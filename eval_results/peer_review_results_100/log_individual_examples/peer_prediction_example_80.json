{
  "example_idx": 80,
  "reference": "Published as a conference paper at ICLR 2023\n\nEPISODE: EPISODIC GRADIENT CLIPPING WITH PERIODIC RESAMPLED CORRECTIONS FOR FEDERATED LEARNING WITH HETEROGENEOUS DATA\n\nMichael Crawshaw1, Yajie Bao2, Mingrui Liu1∗ 1Department of Computer Science, George Mason University, Fairfax, VA 22030, USA 2School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China mcrawsha@gmu.edu, baoyajie2019stat@sjtu.edu.cn, mingruil@gmu.edu\n\nABSTRACT\n\nGradient clipping is an important technique for deep neural networks with exploding gradients, such as recurrent neural networks. Recent studies have shown that the loss functions of these networks do not satisfy the conventional smoothness condition, but instead satisfy a relaxed smoothness condition, i.e., the Lipschitz constant of the gradient scales linearly in terms of the gradient norm. Due to this observation, several gradient clipping algorithms have been developed for nonconvex and relaxed-smooth functions. However, the existing algorithms only apply to the single-machine or multiple-machine setting with homogeneous data across machines. It remains unclear how to design provably efficient gradient clipping algorithms in the general Federated Learning (FL) setting with heterogeneous data and limited communication rounds. In this paper, we design EPISODE, the very first algorithm to solve FL problems with heterogeneous data in the nonconvex and relaxed smoothness setting. The key ingredients of the algorithm are two new techniques called episodic gradient clipping and periodic resampled corrections. At the beginning of each round, EPISODE resamples stochastic gradients from each client and obtains the global averaged gradient, which is used to (1) determine whether to apply gradient clipping for the entire round and (2) construct local gradient corrections for each client. Notably, our algorithm and analysis provide a unified framework for both homogeneous and heterogeneous data under any noise level of the stochastic gradient, and it achieves state-of-the-art complexity results. In particular, we prove that EPISODE can achieve linear speedup in the number of machines, and it requires significantly fewer communication rounds. Experiments on several heterogeneous datasets, including text classification and image classification, show the superior performance of EPISODE over several strong baselines in FL. The code is available at https://github.com/MingruiLiu-ML-Lab/episode.\n\n1\n\nINTRODUCTION\n\nGradient clipping (Pascanu et al., 2012; 2013) is a well-known strategy to improve the training of deep neural networks with the exploding gradient issue such as Recurrent Neural Networks (RNN) (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) and Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997). Although it is a widely-used strategy, formally analyzing gradient clipping in deep neural networks under the framework of nonconvex optimization only happened recently (Zhang et al., 2019a; 2020a; Cutkosky & Mehta, 2021; Liu et al., 2022). In particular, Zhang et al. (2019a) showed empirically that the gradient Lipschitz constant scales linearly in terms of the gradient norm when training certain neural networks such as AWD-LSTM (Merity et al., 2018), introduced the relaxed smoothness condition (i.e., (L0, L1)-smoothness1), and proved that clipped gradient descent converges faster than any fixed step size gradient descent. Later on, Zhang et al. (2020a) provided tighter complexity bounds of the gradient clipping algorithm.\n\nFederated Learning (FL) (McMahan et al., 2017a) is an important distributed learning paradigm in which a single model is trained collaboratively under the coordination of a central server without revealing client data 2. FL has two critical features: heterogeneous data and limited communication.\n\n∗Corresponding Author: Mingrui Liu (mingruil@gmu.edu). 1The formal definition of (L0, L1)-smoothness is illustrated in Definition 2. 2In this paper, we use the terms “client\" and “machine\" interchangeably.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Communication complexity (R) and largest number of skipped communication (Imax) to guarantee linear speedup for different methods to find an (cid:15)-stationary point (defined in Definition 1). “Single\" means single machine, N is the number of clients, I is the number of skipped communications, κ is the quantity representing the heterogeneity, ∆ = f (x0) − minx f (x), and σ2 is the variance of stochastic gradients. Iteration complexity (T ) is the product of communication complexity and the number of skipped communications (i.e., T = RI ). Best iteration complexity Tmin denotes the minimum value of T the algorithm can achieve through adjusting I. Linear speedup means the iteration complexity is divided by N compared with the single machine baseline: in our case it means T = O( ∆L0σ2\n\nN (cid:15)4 ) iteration complexity.\n\nMethod\n\nSetting\n\nCommunication Complexity (R)\n\nBest Iteration Complexity (Tmin)\n\nLocal SGD (Yu et al., 2019c) SCAFFOLD (Karimireddy et al., 2020) Clipped SGD (Zhang et al., 2019b) Clipping Framework (Zhang et al., 2020a) CELGC (Liu et al., 2022) EPISODE (this work)\n\nHeterogeneous, L-smooth Heterogeneous, L-smooth Single, (L0, L1)-smooth Single, (L0, L1)-smooth Homogeneous, (L0, L1)-smooth Heterogeneous, (L0, L1)-smooth\n\nO\n\nO\n\n(cid:16) ∆Lσ2\n\nσ2(cid:15)2 + ∆LN\n\n(cid:15)2\n\nN I(cid:15)4 + ∆Lκ2N I (cid:16) ∆Lσ2\n\nN I(cid:15)4 + ∆L\n\n(cid:15)2\n\nO\n\n(cid:17)\n\n(cid:17)\n\nO\n\n(cid:18) (∆+(L0+L1σ)σ2+σL2\n\n0/L1)2\n\n(cid:19)\n\n(cid:15)4\n\n(cid:16) ∆L0σ2\n\n(cid:15)4\n\n(cid:16) ∆L0σ2\n\nN I(cid:15)4\n\n(cid:17)\n\n(cid:17)\n\nO\n\nO\n\nO\n\n(cid:16) ∆L0σ2\n\nN I(cid:15)4 + ∆(L0+L1(κ+σ))\n\n(cid:15)2\n\n(cid:1)(cid:17)\n\n(cid:0)1 + σ\n\n(cid:15)\n\nO( ∆L0σ2\n\nN (cid:15)4 )\n\nO( ∆L0σ2\n\nN (cid:15)4 )\n\n(cid:18) (∆+(L0+L1σ)σ2+σL2\n\n0/L1)2\n\n(cid:19)\n\n(cid:15)4\n\n(cid:16) ∆L0σ2\n\n(cid:15)4\n\n(cid:17)\n\nO\n\nO( ∆L0σ2\n\nN (cid:15)4 )\n\nO( ∆L0σ2\n\nN (cid:15)4 )\n\n(cid:16)\n\nO\n\nLargest I to guarantee linear speedup (Imax) (cid:16) σ2\n\n(cid:17)\n\nO\n\nκN (cid:15)\n\n(cid:17)\n\n(cid:16) σ2\n\nN (cid:15)2\n\nO\n\nN/A\n\nN/A\n\nO (cid:0) σ\n\nN (cid:15)\n\n(cid:1)\n\nL0σ2\n\n(L0+L1(κ+σ))(1+ σ\n\n(cid:15) )N (cid:15)2\n\n(cid:17)\n\nAlthough there is a vast literature on FL (see (Kairouz et al., 2019) and references therein), the theoretical and algorithmic understanding of gradient clipping algorithms for training deep neural networks in the FL setting remains nascent. To the best of our knowledge, Liu et al. (2022) is the only work that has considered a communication-efficient distributed gradient clipping algorithm under the nonconvex and relaxed smoothness conditions in the FL setting. In particular, Liu et al. (2022) proved that their algorithm achieves linear speedup in terms of the number of clients and reduced communication rounds. Nevertheless, their algorithm and analysis are only applicable to the case of homogeneous data. In addition, the analyses of the stochastic gradient clipping algorithms in both single machine (Zhang et al., 2020a) and multiple-machine setting (Liu et al., 2022) require strong distributional assumptions on the stochastic gradient noise 3, which may not hold in practice.\n\nIn this work, we introduce a provably computation and communication efficient gradient clipping algorithm for nonconvex and relaxed-smooth functions in the general FL setting (i.e., heterogeneous data, limited communication) and without any distributional assumptions on the stochastic gradient noise. Compared with previous work on gradient clipping (Zhang et al., 2019a; 2020a; Cutkosky & Mehta, 2020; Liu et al., 2022) and FL with heterogeneous data (Li et al., 2020a; Karimireddy et al., 2020), our algorithm design relies on two novel techniques: episodic gradient clipping and periodic resampled corrections. In a nutshell, at the beginning of each communication round, the algorithm resamples each client’s stochastic gradient; this information is used to decide whether to apply clipping in the current round (i.e., episodic gradient clipping), and to perform local corrections to each client’s update (i.e., periodic resampled corrections). These techniques are very different compared with previous work on gradient clipping. Specifically, (1) In traditional gradient clipping (Pascanu et al., 2012; Zhang et al., 2019a; 2020a; Liu et al., 2022), whether or not to apply the clipping operation is determined only by the norm of the client’s current stochastic gradient. Instead, we use the norm of the global objective’s stochastic gradient (resampled at the beginning of the round) to determine whether or not clipping will be applied throughout the entire communication round. (2) Different from Karimireddy et al. (2020) which uses historical gradient information from the previous round to perform corrections, our algorithm utilizes the resampled gradient to correct each client’s local update towards the global gradient, which mitigates the effect of data heterogeneity. Notice that, under the relaxed smoothness setting, the gradient may change quickly around a point at which the gradient norm is large. Therefore, our algorithm treats a small gradient as more “reliable\" and confidently applies the unclipped corrected local updates; on the contrary, the algorithm regards a large gradient as less “reliable\" and in this case clips the corrected local updates. Our major contributions are summarized as follows.\n\n3 Zhang et al. (2020a) requires an explicit lower bound for the stochastic gradient noise, and Liu et al. (2022)\n\nrequires the distribution of the stochastic gradient noise is unimodal and symmetric around its mean.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n• We introduce EPISODE, the very first algorithm for optimizing nonconvex and (L0, L1)- smooth functions in the general FL setting with heterogeneous data and limited communication. The algorithm design introduces novel techniques, including episodic gradient clipping and periodic resampled corrections. To the best of our knowledge, these techniques are first introduced by us and crucial for algorithm design.\n\n• Under the nonconvex and relaxed smoothness condition, we prove that the EPISODE algorithm can achieve linear speedup in the number of clients and reduced communication rounds in the heterogeneous data setting, without any distributional assumptions on the stochastic gradient noise. In addition, we show that the degenerate case of EPISODE matches state-of-the-art complexity results under weaker assumptions 4. Detailed complexity results and a comparison with other relevant algorithms are shown in Table 1.\n\n• We conduct experiments on several heterogeneous medium and large scale datasets with different deep neural network architectures, including a synthetic objective, text classification, and image classification. We show that the performance of the EPISODE algorithm is consistent with our theory, and it consistently outperforms several strong baselines in FL.\n\n2 RELATED WORK\n\nGradient Clipping Gradient clipping is a standard technique in the optimization literature for solving convex/quasiconvex problems (Ermoliev, 1988; Nesterov, 1984; Shor, 2012; Hazan et al., 2015; Mai & Johansson, 2021; Gorbunov et al., 2020), nonconvex smooth problems (Levy, 2016; Cutkosky & Mehta, 2021), and nonconvex distributionally robust optimization (Jin et al., 2021). Menon et al. (2019) showed that gradient clipping can help mitigate label noise. Gradient clipping is a well-known strategy to achieve differential privacy (Abadi et al., 2016; McMahan et al., 2017b; Andrew et al., 2021; Zhang et al., 2021). In the deep learning literature, gradient clipping is employed to avoid exploding gradient issue when training certain deep neural networks such as recurrent neural networks or long-short term memory networks (Pascanu et al., 2012; 2013) and language models (Gehring et al., 2017; Peters et al., 2018; Merity et al., 2018). Zhang et al. (2019a) initiated the study of gradient clipping for nonconvex and relaxed smooth functions. Zhang et al. (2020a) provided an improved analysis over Zhang et al. (2019a). However, none of these works apply to the general FL setting with nonconvex and relaxed smooth functions.\n\nFederated Learning FL was proposed by McMahan et al. (2017a), to enable large-scale distributed learning while keep client data decentralized to protect user privacy. McMahan et al. (2017a) designed the FedAvg algorithm, which allows multiple steps of gradient updates before communication. This algorithm is also known as local SGD (Stich, 2018; Lin et al., 2018; Wang & Joshi, 2018; Yu et al., 2019c). The local SGD algorithm and their variants have been analyzed in the convex setting (Stich, 2018; Stich et al., 2018; Dieuleveut & Patel, 2019; Khaled et al., 2020; Li et al., 2020a; Karimireddy et al., 2020; Woodworth et al., 2020a;b; Koloskova et al., 2020; Yuan et al., 2021) and nonconvex smooth setting (Jiang & Agrawal, 2018; Wang & Joshi, 2018; Lin et al., 2018; Basu et al., 2019; Haddadpour et al., 2019; Yu et al., 2019c;b; Li et al., 2020a; Karimireddy et al., 2020; Reddi et al., 2021; Zhang et al., 2020b; Koloskova et al., 2020). Recently, in the stochastic convex optimization setting, several works compared local SGD and minibatch SGD in the homogeneous (Woodworth et al., 2020b) and heterogeneous data setting (Woodworth et al., 2020a), as well as the fundamental limit (Woodworth et al., 2021). For a comprehensive survey, we refer the readers to Kairouz et al. (2019); Li et al. (2020a) and references therein. The most relevant work to ours is Liu et al. (2022), which introduced a communication-efficient distributed gradient clipping algorithm for nonconvex and relaxed smooth functions. However, their algorithm and analysis does not apply in the case of heterogeneous data as considered in this paper.\n\n3 PROBLEM SETUP AND PRELIMINARIES\n\nIn this paper, we use (cid:104)·, ·(cid:105) and (cid:107) · (cid:107) to denote the inner product and Euclidean norm in Notations space Rd. We use 1(·) to denote the indicator function. We let Ir be the set of iterations at the r-th\n\n4We prove that the degenerate case of our analysis (e.g., homogeneous data) achieves the same iteration and communication complexity, but without the requirement of unimodal and symmetric stochastic gradient noise as in Liu et al. (2022). Also, our analysis is unified over any noise level of stochastic gradient, which does not require an explicit lower bound for the stochastic gradient noise as in the analysis of Zhang et al. (2020a).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nround, that is Ir = {tr, ..., tr+1 − 1}. The filtration generated by the random variables before step tr is denoted by Fr. We also use Er[·] to denote the conditional expectation E[·|Fr]. The number of clients is denoted by N and the length of the communication interval is denoted by I, i.e., |Ir| = I for r = 0, 1, ..., R. Let fi(x) := Eξi∼Di [Fi(x; ξi)] be the loss function in i-th client for i ∈ [N ], where the local distribution Di is unknown and may be different across i ∈ [N ]. In the FL setting, we aim to minimize the following overall averaged loss function:\n\nf (x) :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\nfi(x).\n\n(1)\n\nWe focus on the case that each fi is non-convex, in which it is NP-hard to find the global minimum of f . Instead we consider finding an (cid:15)-stationary point (Ghadimi & Lan, 2013; Zhang et al., 2020a). Definition 1. For a function h : Rd → R, a point x ∈ Rd is called (cid:15)-stationary if (cid:107)∇h(x)(cid:107) ≤ (cid:15).\n\nMost existing works in the non-convex FL literature (Yu et al., 2019a; Karimireddy et al., 2020) assume each fi is L-smooth, i.e., (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107) for any x, y ∈ Rd. However it is shown in Zhang et al. (2019a) that L-smoothness may not hold for certain neural networks such as RNNs and LSTMs. (L0, L1)-smoothness in Definition 2 was proposed by Zhang et al. (2019b) and is strictly weaker than L-smoothness. Under this condition, the local smoothness of the objective can grow with the gradient scale. For AWD-LSTM (Merity et al., 2018), empirical evidence of (L0, L1)-smoothness was observed in Zhang et al. (2019b). Definition 2. A second order differentiable function h : Rd → R is (L0, L1)-smooth if (cid:107)∇2h(x)(cid:107) ≤ L0 + L1(cid:107)∇h(x)(cid:107) holds for any x ∈ Rd.\n\nSuppose we only have access to the stochastic gradient ∇Fi(x; ξ) for ξ ∼ Di in each client. Next we make the following assumptions on objectives and stochastic gradients. Assumption 1. Assume fi for i ∈ [N ] and f defined in (1) satisfy:\n\n(i) fi is second order differentiable and (L0, L1)-smooth.\n\n(ii) Let x∗ be the global minimum of f and x0 be the initial point. There exists some ∆ > 0\n\nsuch that f (x0) − f (x∗) ≤ ∆.\n\n(iii) For all x ∈ Rd, there exists some σ ≥ 0 such that Eξi∼Di[∇Fi(x; ξi)] = ∇fi(x) and\n\n(cid:107)∇Fi(x; ξi) − ∇fi(x)(cid:107) ≤ σ almost surely.\n\n(iv) There exists some κ ≥ 0 and ρ ≥ 1 such that (cid:107)∇fi(x)(cid:107) ≤ κ + ρ(cid:107)∇f (x)(cid:107) for any x ∈ Rd.\n\nRemark: (i) and (ii) are standard in the non-convex optimization literature (Ghadimi & Lan, 2013), and (iii) is a standard assumption in the (L0, L1)-smoothness setting (Zhang et al., 2019b; 2020a; Liu et al., 2022). (iv) is used to bound the difference between the gradient of each client’s local loss and the gradient of the overall loss, which is commonly assumed in the FL literature with heterogeneous data (Karimireddy et al., 2020). When κ = 0 and ρ = 1, (iv) corresponds to the homogeneous setting.\n\n4 ALGORITHM AND ANALYSIS\n\n4.1 MAIN CHALLENGES AND ALGORITHM DESIGN\n\nMain Challenges We first illustrate why the prior local gradient clipping algorithm (Liu et al., 2022) would not work in the heterogeneous data setting. Liu et al. (2022) proposed the first communicationefficient local gradient clipping algorithm (CELGC) in a homogeneous setting for relaxed smooth functions, which can be interpreted as the clipping version of FedAvg. Let us consider a simple heterogeneous example with two clients in which CELGC fails. Denote f1(x) = 1 2 x2 + a1x and f2(x) = 1 2 x2 + a2x with a1 = −γ − 1, a2 = γ + 2, and γ > 1. We know that the optimal solution for f = f1+f2 2 , and both f1 and f2 are (L0, L1)-smooth with L0 = 1 and L1 = 0. Consider CELGC with communication interval I = 1 (i.e., communication happens at every iteration), starting point x0 = 0, η = 1/L0 = 1, clipping threshold γ, and σ = 0. In this setting, after the first iteration, the model parameters on the two clients become γ and −γ respectively, so that the averaged model parameter after communication returns to 0. This means that the model parameter of CELGC remains stuck at 0 indefinitely, demonstrating that CELGC cannot handle data heterogeneity.\n\nis x∗ = − a1+a2\n\n2 = − 1\n\n2\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: Episodic Gradient Clipping with Periodic Resampled Corrections (EPISODE)\n\n0 ← x0, ̄x0 ← x0.\n\n1: Initialize xi 2: for r = 0, 1, ..., R do for i ∈ [N ] do 3:\n\nSample ∇Fi( ̄xr; (cid:101)ξi\n\nr) where (cid:101)ξi\n\nr ∼ Di, and update Gi\n\nr ← ∇Fi( ̄xr; (cid:101)ξi\n\nr).\n\nend for Update Gr = 1 for i ∈ [N ] do\n\nN\n\n(cid:80)N\n\ni=1 Gi r.\n\nt), where ξi 1((cid:107)Gr(cid:107) ≤ γ/η) − γ gi\n\nt ∼ Di, and compute gi\n\n1((cid:107)Gr(cid:107) ≥ γ/η).\n\nt (cid:107)gi\n\nt(cid:107)\n\nt ← ∇Fi(xi\n\nt; ξi\n\nt) − Gi\n\nr + Gr.\n\n4: 5: 6: 7: 8: 9:\n\n10:\n\nfor t = tr, . . . , tr+1 − 1 do\n\nSample ∇Fi(xi xi end for\n\nt+1 ← xi\n\nt; ξi t − ηgi\n\nt\n\n11: 12: 13: 14: end for\n\nend for Update ̄xr ← 1\n\nN\n\n(cid:80)N\n\ni=1 xi\n\ntr+1.\n\nWe then explain why the stochastic controlled averaging method (SCAFFOLD) (Karimireddy et al., 2020) for heterogeneous data does not work in the relaxed smoothness setting. SCAFFOLD utilizes the client gradients from the previous round to constructing correction terms which are added to each client’s local update. Crucially, SCAFFOLD requires that the gradient is Lipschitz so that gradients from the previous round are good approximations of gradients in the current round with controllable errors. This technique is not applicable in the relaxed smoothness setting: the gradient may change dramatically, so historical gradients from the previous round are not good approximations of the current gradients anymore due to potential unbounded errors.\n\nAlgorithm Design To address the challenges brought by heterogeneity and relaxed smoothness, our idea is to clip the local updates similarly as we would clip the global gradient (if we could access it). The detailed description of EPISODE is stated in Algorithm 1. Specifically, we introduce two novel techniques: (1) Episodic gradient clipping. At the r-th round, EPISODE constructs a global indicator 1((cid:107)Gr(cid:107) ≤ γ/η) to determine whether to perform clipping in every local update during the round for all clients (line 6). (2) Periodic resampled corrections. EPISODE resamples fresh gradients with constant batch size at the beginning of each round (line 3-5). In particular, at the beginning of the r-th round, EPISODE samples stochastic gradients evaluated at the current averaged global weight ̄xr in all clients to construct the control variate Gr, which has two roles. The first is to construct the global clipping indicator according to (cid:107)Gr(cid:107) (line 10). The second one is to correct the bias between local gradient and global gradient through the variate gi 4.2 MAIN RESULTS\n\nt in local updates (line 10).\n\nTheorem 1. Suppose Assumption 1 holds. For any tolerance (cid:15) ≤ 3AL0\n\nparameters as η ≤ min\n\n(cid:110) 1\n\n216ΓI ,\n\n(cid:15)\n\n180ΓIσ , N (cid:15)2\n\n16AL0σ2\n\n(cid:111)\n\nand γ =\n\n(cid:16)\n\n11σ + AL0 BL1ρ\n\n5BL1ρ , we choose the hyper η, where Γ = AL0 +\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\nBL1κ + BL1ρ number of communication rounds satisfies R ≥ 4∆\n\n. Then EPISODE satisfies\n\nη\n\nσ + γ\n\n(cid:15)2ηI .\n\n1 R+1\n\n(cid:80)R\n\nr=0\n\nE [(cid:107)∇f ( ̄xr)(cid:107)] ≤ 3(cid:15) as long as the\n\nRemark 1: The result in Theorem 1 holds for arbitrary noise level, while the complexity bounds in the stochastic case of Zhang et al. (2020a); Liu et al. (2022) both require σ ≥ 1. In addition, this theorem can automatically recover the complexity results in Liu et al. (2022), but does not require their symmetric and unimodal noise assumption. The improvement upon previous work comes from a better algorithm design, as well as a more careful analysis in the smoothness and individual discrepancy in the non-clipped case (see Lemma 2 and 3).\n\nRemark 2:\n\nIn Theorem 1, when we choose η = min\n\n(cid:110) 1\n\n216ΓI ,\n\n(cid:15)\n\n180ΓIσ , N (cid:15)2\n\n16AL0σ2\n\n(cid:111)\n\ncommunication complexity to find an (cid:15)-stationary point is no more than R = O\n\n, the total (cid:16) ∆\n\n(cid:17)\n\n=\n\n(cid:15)2ηI\n\nO\n\n(cid:16) ∆(L0+L1(κ+σ)) (cid:15)2 complexity.\n\n(cid:0)1 + σ\n\n(cid:15)\n\n(cid:1) + ∆L0σ2\n\nN I(cid:15)4\n\n(cid:17)\n\n. Next we present some implications of the communication\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nL0σ\n\n(L0+L1(κ+σ))N (cid:15) and σ (cid:38) (cid:15), EPISODE has communication complexity 1. When I (cid:46) O( ∆L0σ2 N I(cid:15)4 ). In this case, EPISODE enjoys a better communication complexity than the naive parallel version of the algorithm in Zhang et al. (2020a), that is O( ∆L0σ2 N (cid:15)4 ). Moreover, the iteration complexity of EPISODE is T = RI = O( ∆L0σ2 N (cid:15)4 ), which achieves the linear speedup w.r.t. the number of clients N . This matches the result of Liu et al. (2022) in the homogeneous data setting.\n\n2. When I (cid:38)\n\nL0σ\n\n(L0+L1(κ+σ))N (cid:15) and σ (cid:38) (cid:15), the communication complexity of EPISODE is O( ∆(L0+L1(κ+σ))σ ). This term does not appear in Theorem III of Karimireddy et al. (2020), but it appears here due to the difference in the construction of the control variates. In fact, the communication complexity of EPISODE is still lower than the naive parallel version of Zhang et al. (2020a) if the number of clients satisfies N (cid:46)\n\nL0σ\n\n(cid:15)3\n\n(L0+L1(κ+σ))(cid:15) .\n\n3. When 0 < σ (cid:46) (cid:15) , EPISODE has communication complexity O( ∆(L0+L1(κ+σ))\n\n). Under this particular noise level, the algorithms in Zhang et al. (2020a); Liu et al. (2022) do not guarantee convergence because their analyses crucially rely on the fact that σ (cid:38) (cid:15).\n\n(cid:15)2\n\n4. When σ = 0, EPISODE has communication complexity O( ∆(L0+L1κ)\n\n). This bound includes an additional constant L1κ compared with the complexity results in the deterministic case (Zhang et al., 2020a), which comes from data heterogeneity and infrequent communication.\n\n(cid:15)2\n\n4.3 PROOF SKETCH OF THEOREM 1\n\nDespite recent work on gradient clipping in the homogeneous setting (Liu et al., 2022), the analysis of Theorem 1 is highly nontrivial since we need to cope with (L0, L1)-smoothness and heterogeneity simultaneously. In addition, we do not require a lower bound of σ and allow for arbitrary σ ≥ 0.\n\nThe first step is to establish the descent inequality of the global loss function. According to the (L0, L1)-smoothness condition, if (cid:107) ̄xr+1 − ̄xr(cid:107) ≤ C/L1, then\n\nEr [f ( ̄xr+1) − f ( ̄xr)] ≤ Er\n\n(cid:2)(1(Ar) + 1( ̄Ar))(cid:104)∇f ( ̄xr), ̄xr+1 − ̄xr(cid:105)(cid:3)\n\n(cid:20) (1(Ar) + 1( ̄Ar))\n\n+ Er\n\nAL0 + BL1(cid:107)∇f ( ̄xr)(cid:107) 2\n\n(cid:107) ̄xr+1 − ̄xr(cid:107)2\n\n(cid:21)\n\n,\n\n(2)\n\nwhere Ar := {(cid:107)Gr(cid:107) ≤ γ/η}, ̄Ar is the complement of Ar, and A, B, C are constants defined in Lemma 5. To utilize the inequality (2), we need to verify that the distance between ̄xr+1 and ̄xr is small almost surely. In the algorithm of Liu et al. (2022), clipping is performed in each iteration based on the magnitude of the current stochastic gradient, and hence the increment of each local weight is bounded by the clipping threshold γ. For each client in EPISODE, whether to perform clipping is decided by the magnitude of Gr at the beginning of each round. Therefore, the techniques in Liu et al. (2022) to bound the individual discrepancy cannot be applied to EPISODE. To address this issue, we introduce Lemma 1, which guarantees that we can apply the properties of relaxed smoothness (Lemma 5 and 6) to all iterations in one round, in either case of clipping or non-clipping. Lemma 1. Suppose 2ηI(AL0 + BL1κ + BL1ρ(σ + γ/η)) ≤ 1 and max {2ηI(2σ + γ/η), γI} ≤ C\nL1\n\n. Then for any i ∈ [N ] and t − 1 ∈ Ir, it almost surely holds that\n\n1(Ar)(cid:107)xi\n\nt − ̄xr(cid:107) ≤ 2ηI (2σ + γ/η)\n\nand 1( ̄Ar)(cid:107)xi\n\nt − ̄xr(cid:107) ≤ γI.\n\n(3)\n\n(cid:80)N\n\nN\n\ni=1 (cid:107)xi\n\nEquipped with Lemma 1, the condition (cid:107) ̄xr+1 − ̄xr(cid:107) ≤ 1 − ̄xr(cid:107) ≤ C/L1 can hold almost surely with a proper choice of η. Then it suffices to bound the terms from (2) in expectation under the events Ar and ̄Ar respectively. To deal with the discrepancy term E[(cid:107)xi t − ̄xr(cid:107)2] for t − 1 ∈ Ir, Liu et al. (2022) directly uses the almost sure bound for both cases of clipping and non-clipping. Here we aim to obtain a more delicate bound in expectation for the non-clipping case. The following lemma, which is critical to obtain the unified bound from Theorem 1 under any noise level, gives an upper bound for the local smoothness of fi at x. Lemma 2. Under the conditions of Lemma 1, for all x ∈ Rd such that (cid:107)x − ̄xr(cid:107) ≤ 2ηI (2σ + γ/η), the following inequality almost surely holds:\n\ntr+1\n\n1(Ar)(cid:107)∇2fi(x)(cid:107) ≤ L0 + L1 (κ + (ρ + 1) (γ/η + 2σ)) .\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFrom (3), we can see that all iterations in the r-th round satisfy the condition of Lemma 2 almost surely. Hence we are guaranteed that each local loss fi is L-smooth over the iterations in this round under the event Ar, where L = L0 + L1(κ + (ρ + 1)(γ/η + 2σ)). In light of this, the following lemma gives a bound in expectation of the individual discrepancy. We denote pr = Er[1(Ar)]. Lemma 3. Under the conditions of Lemma 1, for any i ∈ [N ] and t − 1 ∈ Ir, we have\n\nEr Er\n\n(cid:2)1(Ar)(cid:107)xi (cid:2)1(Ar)(cid:107)xi\n\nt − ̄xr(cid:107)2(cid:3) ≤ 36prI 2η2(cid:107)∇f ( ̄xr)(cid:107)2 + 126prI 2η2σ2, t − ̄xr(cid:107)2(cid:3) ≤ 18prI 2ηγ(cid:107)∇f ( ̄xr)(cid:107) + 18prI 2η2 (cid:0)γσ/η + 5σ2(cid:1) .\n\n(4)\n\n(5)\n\nIt is worthwhile noting that the bound in (4) involves a quadratic term of (cid:107)∇f ( ̄xr)(cid:107), whereas it is linear in (5). The role of the linear bound is to deal with 1(Ar)(cid:107)∇f ( ̄xr)(cid:107)(cid:107) ̄xr+1 − ̄xr(cid:107)2 from the descent inequality (2), since directly substituting (4) would result in a cubic term which is hard to analyze. With Lemma 1, 2 and 3, we obtain the following descent inequality. Lemma 4. Under the conditions of Lemma 1, let Γ = AL0 + BL1(κ + ρ(γ/η + σ)). Then it holds that for each 0 ≤ r ≤ R − 1,\n\nEr [f ( ̄xr+1) − f ( ̄xr)] ≤ Er [1(Ar)V ( ̄xr)] + Er\n\n(cid:2)1( ̄Ar)U ( ̄xr)(cid:3) ,\n\n(6)\n\nwhere the definitions of V ( ̄xr) and U ( ̄xr) are given in Appendix C.1.\n\nThe detailed proof of Lemma 4 is deferred in Appendix C.1. With this Lemma, the descent inequality is divided into V ( ̄xr) (objective value decrease during the non-clipping rounds) and U ( ̄xr) (objective value decrease during the clipping rounds). Plugging in the choices of η and γ yields\n\nmax {U ( ̄xr), V ( ̄xr)} ≤ −\n\n1 4\n\n(cid:15)ηI(cid:107)∇f ( ̄xr)(cid:107) +\n\n1 2\n\n(cid:15)2ηI.\n\n(7)\n\nThe conclusion of Theorem 1 can then be obtained by substituting (7) into (6) and summing over r.\n\n5 EXPERIMENTS\n\nIn this section, we present an empirical evaluation of EPISODE to validate our theory. We present results in the heterogeneous FL setting on three diverse tasks: a synthetic optimization problem satisfying (L0, L1)-smoothness, natural language inference on the SNLI dataset (Bowman et al., 2015), and ImageNet classification (Deng et al., 2009). We compare EPISODE against FedAvg (McMahan et al., 2017a), SCAFFOLD (Karimireddy et al., 2020), CELGC (Liu et al., 2022), and a naive distributed algorithm which we refer to as Naive Parallel Clip 5 We include additional experiments on the CIFAR-10 dataset (Krizhevsky et al., 2009) in Appendix E.4, running time results in Appendix F, ablation study in Appendix G, and new experiments on federated learning benchmark datasets in Appendix H.\n\n5.1 SETUP\n\nAll non-synthetic experiments were implemented with PyTorch (Paszke et al., 2019) and run on a cluster with eight NVIDIA Tesla V100 GPUs. Since SNLI , CIFAR-10, and ImageNet are centralized datasets, we follow the non-i.i.d. partitioning protocol in (Karimireddy et al., 2020) to split each dataset into heterogeneous client datasets with varying label distributions. Specifically, for a similarity parameter s ∈ [0, 100], each client’s local dataset is composed of two parts. The first s% is comprised of i.i.d. samples from the complete dataset, and the remaining (100 − s)% of data is sorted by label.\n\nSynthetic To demonstrate the behavior of EPISODE and baselines under (L0, L1)-smoothness, we consider a simple minimization problem in a single variable. Here we have N = 2 clients with:\n\nf1(x) = x4 − 3x3 + Hx2 + x,\n\nf2(x) = x4 − 3x3 − 2Hx2 + x,\n\nwhere the parameter H controls the heterogeneity between the two clients. Notice that f1 and f2 satisfy (L0, L1)-smoothness but not traditional L-smoothness. Proposition 1. For any x ∈ R and i = 1, 2, it holds that (cid:107)∇fi(x)(cid:107) ≤ 2(cid:107)∇f (x)(cid:107) + κ(H), where κ(H) < ∞ and is a positive increasing function of H for H ≥ 1.\n\nAccording to Proposition 1, Assumption 1(iv) will be satisfied with ρ = 2 and κ = κ(H), where κ(H) is an increasing function of H. The proof of this proposition is deferred to Appendix E.1.\n\n5Naive Parallel Clip uses the globally averaged stochastic gradient obtained from synchronization at every\n\niteration to run SGD with gradient clipping on the global objective.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Effect of I (Epochs)\n\n(b) Effect of I (Rounds)\n\n(c) Effect of s (Epochs)\n\nFigure 1: Training loss and testing accuracy on SNLI. The style of each curve (solid, dashed, dotted) corresponds to the algorithm, while the color corresponds to either the communication interval I (for (a) and (b)) or the client data similarity s (for (c)). (a), (b) Effect of varying I with s = 30%, plotted against (a) epochs and (b) communication rounds. (c) Effect of varying s with I = 4.\n\nSNLI Following Conneau et al. (2017), we train a BiRNN network for 25 epochs using the multiclass hinge loss and a batch size of 64 on each worker. The network is composed of a one layer BiRNN encoder with hidden size 2048 and max pooling, and a three-layer fully connected classifier with hidden size 512. The BiRNN encodes a sentence (represented as a sequence of GloVe vectors (Pennington et al., 2014)), and the classifier predicts the relationship of two encoded sentences as either entailment, neutral, or contradiction. For more hyperparameter information, see Appendix E.3.\n\nTo determine the effects of infrequent communication and data heterogeneity on the performance of each algorithm, we vary I ∈ {2, 4, 8, 16} and s ∈ {10%, 30%, 50%}. We compare EPISODE, CELGC, and the Naive Parallel Clip. Note that the training process diverged when using SCAFFOLD, likely due to a gradient explosion issue, since SCAFFOLD does not use gradient clipping.\n\nImageNet Following Goyal et al. (2017), we train a ResNet-50 (He et al., 2016) for 90 epochs using the cross-entropy loss, a batch size of 32 for each worker, clipping parameter γ = 1.0, momentum with coefficient 0.9, and weight decay with coefficient 5 × 10−5. We initially set the learning rate η = 0.1 and decay by a factor of 0.1 at epochs 30, 60, and 80. To analyze the effect of data heterogeneity in this setting, we fix I = 64 and vary s ∈ {50%, 60%, 70%}. Similarly, to analyze the effect of infrequent communication, we fix s = 60% and vary I ∈ {64, 128}. We compare the performance of FedAvg, CELGC, EPISODE, and SCAFFOLD.\n\n5.2 RESULTS\n\nSynthetic Figure 3 in Appendix E.2 shows the objective value throughout training, where the heterogeneity parameter H varies over {1, 2, 4, 8}. CELGC exhibits very slow optimization due to the heterogeneity across clients: as H increases, the optimization progress becomes slower and slower. In contrast, EPISODE maintains fast convergence as H varies. We can also see that EPISODE converges to the minimum of global loss, while CELGC fails to do so when H is larger.\n\nSNLI Results for the SNLI dataset are shown in Figure 1. To demonstrate the effect of infrequent communication, Figures 1(a) and 1(b) show results for EPISODE, CELGC, and Naive Parallel Clip as the communication interval I varies (with fixed s = 30%). After 25 epochs, the test accuracy\n\n8\n\n0510152025Epoch0.20.30.40.50.60.7Train lossCELGC, I = 2CELGC, I = 4CELGC, I = 8CELGC, I = 16EPISODE, I = 2EPISODE, I = 4EPISODE, I = 8EPISODE, I = 16Naive Parallel Clip0510152025Epoch0.740.760.780.800.820.84Test accuracy0500010000150002000025000Round0.20.30.40.50.60.7Train loss0500010000150002000025000Round0.740.760.780.800.820.84Test accuracy72525000.350.400.4572525000.800.810.820510152025Epoch0.20.30.40.50.60.70.8Train loss0510152025Epoch0.600.650.700.750.80Test accuracyCELGC, s = 50%CELGC, s = 30%CELGC, s = 10%EPISODE, s = 50%EPISODE, s = 30%EPISODE, s = 10%Naive Parallel Clip240.820.83Published as a conference paper at ICLR 2023\n\nInterval\n\nSimilarity Algorithm\n\nTrain loss Test acc.\n\n64\n\n70%\n\n64\n\n60%\n\n64\n\n50%\n\n128\n\n60%\n\n1.010 FedAvg CELGC 1.016 SCAFFOLD 1.024 0.964 EPISODE\n\n0.990 FedAvg CELGC 0.979 SCAFFOLD 0.983 0.945 EPISODE\n\n0.955 FedAvg CELGC 0.951 SCAFFOLD 0.959 0.916 EPISODE\n\n1.071 FedAvg CELGC 1.034 SCAFFOLD 1.071 1.016 EPISODE\n\n74.89% 74.89% 74.92% 75.20%\n\n74.73% 74.51% 74.68% 74.95%\n\n74.53% 74.12% 74.19% 74.81%\n\n74.15% 74.24% 74.03% 74.36%\n\nFigure 2: ImageNet results. Left: Training loss and testing accuracy at the end of training for various settings of I and s. EPISODE consistently reaches better final metrics in all settings. Right: Training loss and testing accuracy during training for I = 64 and s = 50%.\n\nof EPISODE nearly matches that of Naive Parallel Clip for all I ≤ 8, while CELGC lags 2-3% behind Naive Parallel Clip for all values of I. Also, EPISODE nearly matches the test accuracy of Naive Parallel Clip with as little as 8 times fewer communication rounds. Lastly, EPISODE requires significantly less communication rounds to reach the same training loss as CELGC. For example, EPISODE with I = 4, s = 30% takes less than 5000 rounds to reach a training loss of 0.4, while CELGC does not reach 0.4 during the entirety of training with any I.\n\nTo demonstrate the effect of client data heterogeneity, Figure 1(c) shows results for varying values of s (with fixed I = 4). Here we can see that EPISODE is resilient against data heterogeneity: even with client similarity as low as s = 10%, the performance of EPISODE is the same as s = 50%. Also, the testing accuracy of EPISODE with s = 10% is nearly identical to that of the Naive Parallel Clip. On the other hand, the performance of CELGC drastically worsens with more heterogeneity: even with s = 50%, the training loss of CELGC is significantly worse than EPISODE with s = 10%.\n\nImageNet Figure 2 shows the performance of each algorithm at the end of training for all settings (left) and during training for the setting I = 64 and s = 50% (right). Training curves for the rest of the settings are given in Appendix E.5. EPISODE outperforms all baselines in every experimental setting, especially in the case of high data heterogeneity. EPISODE is particularly dominant over other methods in terms of the training loss during the whole training process, which is consistent with our theory. Also, EPISODE exhibits more resilience to data heterogeneity than CELGC and SCAFFOLD: as the client data similarity deceases from 70% to 50%, the test accuracies of CELGC and SCAFFOLD decrease by 0.8% and 0.7%, respectively, while the test accuracy of EPISODE decreases by 0.4%. Lastly, as communication becomes more infrequent (i.e., the communication interval I increases from 64 to 128), the performance of EPISODE remains superior to the baselines.\n\n6 CONCLUSION\n\nWe have presented EPISODE, a new communication-efficient distributed gradient clipping algorithm for federated learning with heterogeneous data in the nonconvex and relaxed smoothness setting. We have proved convergence results under any noise level of the stochastic gradient. In particular, we have established linear speedup results as well as reduced communication complexity. Further, our experiments on both synthetic and real-world data show demonstrate the superior performance of EPISODE compared to competitive baselines in FL. Our algorithm is suitable for the cross-silo federated learning setting such as in healthcare and financial domains (Kairouz et al., 2019), and we plan to consider cross-device setting in the future.\n\n9\n\n020406080Epoch1.01.52.02.53.0Train Loss020406080Epoch0.400.450.500.550.600.650.700.750.80Test AccuracyFedAvgCELGCSCAFFOLDEPISODE86890.900.951.0086890.740.75Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe would like to thank the anonymous reviewers for their helpful comments. Michael Crawshaw is supported by the Institute for Digital Innovation fellowship from George Mason University. Michael Crawshaw and Mingrui Liu are both supported by a grant from George Mason University. The work of Yajie Bao was done when he was virtually visiting Mingrui Liu’s research group in the Department of Computer Science at George Mason University.\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308–318, 2016.\n\nGalen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning with adaptive clipping. Advances in Neural Information Processing Systems, 34:17455– 17466, 2021.\n\nDebraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd with quantization, sparsification and local computations. In Advances in Neural Information Processing Systems, pp. 14668–14679, 2019.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nA Conneau, D Kiela, H Schwenk, L Barrault, and A Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670–680. Association for Computational Linguistics, 2017.\n\nAshok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Conference\n\non Machine Learning, pp. 2260–2268. PMLR, 2020.\n\nAshok Cutkosky and Harsh Mehta. High-probability bounds for non-convex stochastic optimization\n\nwith heavy tails. Advances in Neural Information Processing Systems, 34, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nAymeric Dieuleveut and Kumar Kshitij Patel. Communication trade-offs for local-sgd with large step\n\nsize. Advances in Neural Information Processing Systems, 32:13601–13612, 2019.\n\nJeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.\n\nYuri Ermoliev. Stochastic quasigradient methods. numerical techniques for stochastic optimization.\n\nSpringer Series in Computational Mathematics, (10):141–185, 1988.\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning, pp. 1243–1252. PMLR, 2017.\n\nSaeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic\n\nprogramming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.\n\nEduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-\n\ntailed noise via accelerated gradient clipping. arXiv preprint arXiv:2005.10785, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPriya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nFarzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local sgd with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in Neural Information Processing Systems, pp. 11080–11092, 2019.\n\nElad Hazan, Kfir Y Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex\n\noptimization. arXiv preprint arXiv:1507.02030, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nPeng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse and quantized communication. In Advances in Neural Information Processing Systems, pp. 2525–2536, 2018.\n\nJikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robust optimization: Non-asymptotic analysis. Advances in Neural Information Processing Systems, 34: 2771–2782, 2021.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.\n\nAhmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pp. 4519–4529. PMLR, 2020.\n\nAnastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pp. 5381–5393. PMLR, 2020.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nKfir Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint\n\narXiv:1611.04831, 2016.\n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,\n\nmethods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020a.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020b.\n\nTao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches, use\n\nlocal sgd. arXiv preprint arXiv:1808.07217, 2018.\n\nMingrui Liu, Zhenxun Zhuang, Yunwen Lei, and Chunyang Liao. A communication-efficient arXiv preprint\n\ndistributed gradient clipping algorithm for training deep neural networks. arXiv:2205.05040, 2022.\n\nVien V Mai and Mikael Johansson. Stability and convergence of stochastic gradient clipping: Beyond\n\nlipschitz continuity and smoothness. arXiv preprint arXiv:2102.06489, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient\n\nlearning of deep networks from decentralized data. AISTATS, 2017a.\n\nH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private\n\nrecurrent language models. arXiv preprint arXiv:1710.06963, 2017b.\n\nAditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient clipping mitigate label noise? In International Conference on Learning Representations, 2019.\n\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM\n\nlanguage models. In International Conference on Learning Representations, 2018.\n\nYurii E Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon,\n\n29:519–531, 1984.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem.\n\ncorr abs/1211.5063 (2012). arXiv preprint arXiv:1211.5063, 2012.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\n\nnetworks. In International conference on machine learning, pp. 1310–1318. PMLR, 2013.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019.\n\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014.\n\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n\nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv\n\nKumar, and H Brendan McMahan. Adaptive federated optimization. ICLR, 2021.\n\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by\n\nback-propagating errors. nature, 323(6088):533–536, 1986.\n\nNaum Zuselevich Shor. Minimization methods for non-differentiable functions, volume 3. Springer\n\nScience & Business Media, 2012.\n\nSebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,\n\n2018.\n\nSebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. In\n\nAdvances in Neural Information Processing Systems, pp. 4447–4458, 2018.\n\nJianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of\n\ncommunication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.\n\nPaul J Werbos. Generalization of backpropagation with application to a recurrent gas market model.\n\nNeural networks, 1(4):339–356, 1988.\n\nBlake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous\n\ndistributed learning. arXiv preprint arXiv:2006.04735, 2020a.\n\nBlake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, In International\n\nIs local sgd better than minibatch sgd?\n\nOhad Shamir, and Nathan Srebro. Conference on Machine Learning, pp. 10334–10343. PMLR, 2020b.\n\nBlake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro. The min-max complexity of distributed stochastic convex optimization with intermittent communication. arXiv preprint arXiv:2102.01583, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nHao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. arXiv preprint arXiv:1905.03817, 2019a.\n\nHao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 7184–7193, 2019b.\n\nHao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693–5700, 2019c.\n\nHonglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. In International\n\nConference on Machine Learning, pp. 12253–12266. PMLR, 2021.\n\nBohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for\n\nnon-convex optimization. arXiv preprint arXiv:2010.02519, 2020a.\n\nJingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates\n\ntraining: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019a.\n\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? arXiv preprint arXiv:1912.03194, 2019b.\n\nXinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418, 2020b.\n\nXinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu, and Jinfeng Yi. Understanding clipping for federated learning: Convergence and client-level differential privacy. arXiv preprint arXiv:2106.13673, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nWe use Fr to denote the filtration generated by\n\n{ξi\n\nt : t ∈ Il, i = 1, ...N }r−1\n\nl=1 ∪ {(cid:101)ξi\n\nl : i = 1, ...N }r−1 l=1 .\n\nIt means that given Fr, the global solution ̄xr is fixed, but the randomness of Ar, Gi exists. In addition, for t ∈ Ir, we use Ht to denote the filtration generated by\n\nr and Gr still\n\nFr ∪ {ξi\n\ns : tr ≤ s ≤ t}N\n\ni=1 ∪ {(cid:101)ξi\n\nr}N\n\ni=1.\n\nRecall the definitions of Gi\n\nr and Gr,\n\nGi\n\nr = ∇Fi( ̄xr; (cid:101)ξi r)\n\nand Gr =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nGi r.\n\nHence we have\n\n(cid:107)Gi\n\nr − ∇fi( ̄xr)(cid:107) ≤ σ and\n\n(cid:107)Gr − ∇f ( ̄xr)(cid:107) ≤ σ,\n\nhold almost surely due to Assumption 1(iii). Also, the local update rule of EPISODE is\n\nxi\n\nt+1 = xi\n\nt − ηgi\n\nt\n\n1(Ar) − γ\n\ngi t\n(cid:107)gi\n\nt(cid:107)\n\n1( ̄Ar)\n\nfor\n\nt ∈ Ir,\n\nwhere gi\n\nt = ∇Fi(xi\n\nt; ξi\n\nt) − Gi\n\nr + Gr, Ar = {(cid:107)Gr(cid:107) ≤ γ/η} and ̄Ar = {(cid:107)Gr(cid:107) > γ/η}.\n\nA.1 AUXILIARY LEMMAS\n\nLemma 5 (Lemma A.2 in Zhang et al. (2020a)). Let f be (L0, L1)-smooth, and C > 0 be a constant. For any x, x(cid:48) ∈ Rd such that (cid:107)x − x(cid:48)(cid:107) ≤ C/L1, we have\n\nf (x(cid:48)) − f (x) ≤ (cid:104)∇f (x), x(cid:48) − x(cid:105) +\n\nAL0 + BL1(cid:107)∇f (x)(cid:107) 2\n\n(cid:107)x(cid:48) − x(cid:107)2,\n\nwhere A = 1 + eC − eC −1 Lemma 6 (Lemma A.3 in Zhang et al. (2020a)). Let f be (L0, L1)-smooth, and C > 0 be a constant. For any x, x(cid:48) ∈ Rd such that (cid:107)x − x(cid:48)(cid:107) ≤ C/L1, we have\n\nC and B = eC −1 C .\n\n(cid:107)∇f (x(cid:48)) − ∇f (x)(cid:107) ≤ (AL0 + BL1(cid:107)∇f (x)(cid:107))(cid:107)x(cid:48) − x(cid:107),\n\nwhere A = 1 + eC − eC −1\n\nC and B = eC −1 C .\n\nHere we choose C ≥ 1 such that A ≥ 1 and B ≥ 1. Lemma 7 (Lemma B.1 in Zhang et al. (2020a)). Let μ > 0 and u, v ∈ Rd. Then\n\n−\n\n(cid:104)u, v(cid:105) (cid:107)v(cid:107)\n\n≤ −μ(cid:107)u(cid:107) − (1 − μ)(cid:107)v(cid:107) + (1 + μ)(cid:107)v − u(cid:107).\n\nB PROOF OF LEMMAS IN SECTION 4.3\n\nB.1 PROOF OF LEMMA 1\n\nLemma 1 restated.\n\nmax\n\n(cid:110)\n\n2ηI(2σ + γ\n\nη ), γI\n\n(cid:111)\n\nSuppose 2ηI(AL0 + BL1κ + BL1ρ(σ + γ ≤ C L1\n\nη )) ≤ 1 and , where the relation between A, B and C is stated in Lemma 5\n\nand 6. Then for any i ∈ [N ] and t − 1 ∈ Ir, it almost surely holds that\n\n1(Ar)(cid:107)xi\n\nt − ̄xr(cid:107) ≤ 2ηI\n\n(cid:18)\n\n2σ +\n\n(cid:19)\n\n,\n\nγ η\n\nand\n\n1( ̄Ar)(cid:107)xi\n\nt − ̄xr(cid:107) ≤ γI.\n\n14\n\n(8)\n\n(9)\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma 1. To show (8) holds, it suffices to show that under the event Ar,\n\n(cid:107)xi\n\nt − ̄xr(cid:107) ≤ 2η(t − tr)\n\n(cid:18)\n\n2σ +\n\n(cid:19)\n\nγ η\n\nholds for any tr + 1 ≤ t ≤ tr+1 and i ∈ [N ]. We will show it by induction. In particular, to show that this fact holds for t = tr + 1, notice\n\n(cid:107)xi\n\ntr+1 − ̄xr(cid:107) = η(cid:107)gi\n\ntr+1(cid:107) ≤ η(cid:107)∇Fi( ̄xr; ξi\n\ntr\n\n) − Gi\n\nr(cid:107) + η(cid:107)Gr(cid:107) ≤ 2ησ + γ ≤ 2η\n\n(cid:18)\n\nσ +\n\n(cid:19)\n\n,\n\nγ η\n\nwhere we used the fact that (cid:107)Gr(cid:107) ≤ γ\n\nη under Ar, and (cid:107)∇Fi( ̄xr; ξi\n\ntr\n\n) − ∇Fi( ̄xr)(cid:107) ≤ σ, (cid:107)Gi\n\nr −\n\n∇Fi( ̄xr)(cid:107) ≤ σ hold almost surely. Now, denote Λ = 2\n\nand suppose that\n\n(cid:16)\n\n2σ + γ\n\nη\n\n(cid:17)\n\nThen we have\n\n(cid:107)xi\n\nt − ̄xr(cid:107) ≤ Λη(t − tr).\n\n(cid:107)xi\n\nt+1 − ̄xr(cid:107) = (cid:107)xi\n\nt − ̄xr − ηgi\n\nt(cid:107)\n\n≤ Λη(t − tr) + η(cid:107)∇Fi(xi ≤ Λη(t − tr) + η(cid:107)∇fi(xi\n\nt) − Gi\n\nr(cid:107) + η(cid:107)Gr(cid:107)\n\nt, ξi t) − ∇fi( ̄xr)(cid:107) + 2ησ + γ.\n\n(10)\n\n(11)\n\nUsing our assumption ηΛI ≤ C/L1 together with the inductive assumption (10), we can apply Lemma 6 to obtain (cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107) ≤ (AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107))(cid:107)xi\n\nt − ̄xr(cid:107) ≤ Λη(t − tr)(AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107))\n\n(i) ≤ Λη(t − tr)(AL0 + BL1(κ + ρ(cid:107)∇f ( ̄xr)(cid:107))) ≤ Λη(t − tr)(AL0 + BL1κ) + ηΛBL1ρ(t − tr)((cid:107)∇f ( ̄xr) − Gr(cid:107) + (cid:107)Gr(cid:107))\n\n(cid:18)\n\n(cid:18)\n\n≤ Λη(t − tr)\n\nAL0 + BL1κ + BL1ρ\n\nσ +\n\n(cid:19)(cid:19)\n\nγ η\n\n(ii) ≤\n\nΛ(t − tr) 2I\n\n≤\n\nΛ 2\n\n,\n\n(12)\n\nwhere (i) comes from the heterogeneity assumption (cid:107)∇fi(x)(cid:107) ≤ κ + ρ(cid:107)∇f (x)(cid:107) for all x and (ii) from the assumption 2ηI(AL0 + BL1κ + BL1ρ(σ + γ η )) ≤ 1. Substituting this into Equation (11) yields\n\n(cid:107)xi\n\nt+1 − ̄xr(cid:107) ≤ Λη(t − tr) + η\n\n+ 2ησ + γ\n\n(cid:18)\n\n≤ η\n\nΛ(t − tr) +\n\n+ 2σ +\n\n(cid:19)\n\nγ η\n\nΛ 2\nΛ 2\n\nwhich completes the induction and the proof of Equation (8). Next, to show Equation (9), notice that under the event ̄Ar we have\n\n≤ Λη(t − tr + 1).\n\n(cid:107) ̄xr − xi\n\nt(cid:107) =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nγ\n\nt−1 (cid:88)\n\ns=tr+1\n\ngi s\n(cid:107)gi\n\ns(cid:107)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nt−1 (cid:88)\n\n≤ γ\n\ns=tr+1\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ngi s\n(cid:107)gi\n\ns(cid:107)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n= γ(t − (tr + 1)) ≤ γI.\n\nB.2 PROOF OF LEMMA 2\n\nLemma 2 restated. (cid:16)\n\n(cid:110)\n\n(cid:17)\n\nmax\n\n2ηI\n\n2σ + γ\n\nη\n\n, γI\n\n(cid:111)\n\nSuppose 2ηI(AL0 + BL1κ + BL1ρ(σ + γ ≤ C L1\n\n. Then for all x ∈ Rd such that (cid:107)x − ̄xr(cid:107) ≤ 2ηI\n\nη )) ≤ 1 and (cid:17) (cid:16) ,\n\n2σ + γ\n\nη\n\nwe have the following inequality almost surely holds:\n\n1(Ar)(cid:107)∇2fi(x)(cid:107) ≤ L0 + L1\n\n(cid:18)\n\nκ + (ρ + 1)\n\n(cid:19)(cid:19)\n\n+ 2σ\n\n.\n\n(cid:18) γ η\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma 2. Under the event Ar = {(cid:107)Gr(cid:107) ≤ γ/η}. From the definition of (L0, L1)- smoothness we have\n\n(cid:107)∇2fi(x)(cid:107) ≤ L0 + L1(cid:107)∇fi(x)(cid:107)\n\n≤ L0 + L1 ((cid:107)∇fi(x) − ∇fi( ̄xr)(cid:107) + (cid:107)∇fi( ̄xr)(cid:107))\n\n(i) ≤ L0 + L1 ((cid:107)∇fi(x) − ∇fi( ̄xr)(cid:107) + κ + ρ(cid:107)∇f ( ̄xr)(cid:107))\n\n(ii) ≤ L0 + L1\n\n(cid:18)\n\n(cid:107)∇fi(x) − ∇fi( ̄xr)(cid:107) + κ + ρ\n\n(cid:18)\n\nσ +\n\n(cid:19)(cid:19)\n\n,\n\nγ η\n\n(13)\n\nwhere we used the heterogeneity assumption (cid:107)∇fi(x)(cid:107) ≤ κ + ρ(cid:107)∇f ( ̄xr)(cid:107) for all x to obtain (i) and the fact (cid:107)∇f ( ̄xr)(cid:107) ≤ (cid:107)∇f ( ̄xr) − Gr(cid:107) + (cid:107)Gr(cid:107) to obtain (ii). Now, for all x such that (cid:107)x − ̄xr(cid:107) ≤ 2ηI(2σ + γ η ) ≤ C .\nHence we can apply Lemma 6 to x and ̄xr, which yields\n\nη ), according to our assumptions, we have (cid:107)x − ̄xr(cid:107) ≤ 2ηI(2σ + γ\n\nL1\n\n(cid:107)∇fi(x) − ∇fi( ̄xr)(cid:107) ≤ (AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107))(cid:107)x − ̄xr(cid:107) (cid:18)\n\n(cid:19)\n\n(AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107))\n\n(cid:19)\n\n(AL0 + BL1(κ + ρ(cid:107)∇f ( ̄xr)(cid:107)))\n\n(cid:19) (cid:18)\n\nAL0 + BL1κ + BL1ρ\n\n(cid:19)(cid:19)\n\n+ σ\n\n(cid:18) γ η\n\nγ η\n\nγ η\n\nγ η\n\n≤ 2ηI\n\n2σ +\n\n(cid:18)\n\n≤ 2ηI\n\n2σ +\n\n(cid:18)\n\n≤ 2ηI\n\n2σ +\n\n(i) ≤ 2σ +\n\nγ η\n\n,\n\nwhere (i) comes from the assumption 2ηI(AL0 + BL1κ + BL1ρ(σ + γ result into Equation (13) yields\n\nη )) ≤ 1. Substituting this\n\n(cid:107)∇2fi(x)(cid:107) ≤ L0 + L1\n\n2σ +\n\nγ η\n\n(cid:18)\n\n(cid:18)\n\n(cid:18)\n\n+ κ + ρ\n\n(cid:18)\n\nσ +\n\nγ η\n(cid:19)(cid:19)\n\n(cid:19)(cid:19)\n\n≤ L0 + L1\n\nκ + (ρ + 1)\n\n2σ +\n\n.\n\nγ η\n\nB.3 PROOF OF LEMMA 3\n\nLemma 3 restated.\n\n(cid:110)\n\nmax\n\n2ηI(2σ + γ\n\n(cid:111)\n\nη ), γI (cid:2)1(Ar)(cid:107)xi\n\n(cid:2)1(Ar)(cid:107)xi\n\nEr\n\nEr\n\nSuppose 2ηI(AL0 + BL1κ + BL1ρ(σ + γ\n\nη )) ≤ 1 and\n\n≤ C L1\n\n, we have both\n\nt − ̄xr(cid:107)2(cid:3) ≤ 36prI 2η2(cid:107)∇f ( ̄xr)(cid:107)2 + 126prI 2η2σ2, (cid:18) γ t − ̄xr(cid:107)2(cid:3) ≤ 18prI 2ηγ(cid:107)∇f ( ̄xr)(cid:107) + 18prI 2η2 η\n\n(cid:19)\n\n,\n\n(14)\n\n(15)\n\nσ + 5σ2\n\nhold for any t − 1 ∈ Ir.\n\nxi\n\nt+1 = xi\n\nProof of Lemma 3. Under the event Ar, the local update rule is given by t) − Gi\n\nt, where gi Using the basic inequality (a + b)2 ≤ (1 + 1/λ)a2 + (λ + 1)b2 for any λ > 0, we have Er = Er\n\nt+1 − ̄xr(cid:107)2(cid:3)\n\n(cid:2)1(Ar)(cid:107)xi\n\n(cid:2)1(Ar)(cid:107)xi\n\nt = ∇Fi(xi\n\nt − ηgi\n\nr + Gr.\n\nt − ̄xr − ηgi\n\nt(cid:107)2(cid:3)\n\nt; ξi\n\n(cid:2)1(Ar)(cid:107)xi (cid:19)\n\n(i)\n\n≤ Er\n\n(ii) ≤\n\n(cid:18) 1 I\n\nt − ̄xr − η(∇fi(xi\n\nt) − Gi\n\nr + Gr)(cid:107)2(cid:3) + η2Er\n\n(cid:2)1(Ar)(cid:107)∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi\n\nt)(cid:107)2(cid:3)\n\n+ 1\n\nEr\n\n(cid:2)1(Ar)(cid:107)xi\n\nt − ̄xr(cid:107)2(cid:3) + (I + 1)η2Er\n\n(cid:2)1(Ar)(cid:107)∇fi(xi\n\nt) − Gi\n\nr + Gr(cid:107)2(cid:3) + prη2σ2.\n\n16\n\n(16)\n\nPublished as a conference paper at ICLR 2023\n\nThe equality (i) and (ii) hold since Fr ⊆ Ht for t ≥ tr such that\n\nEr =Er =Er\n\n(cid:2)1(Ar) (cid:10)xi (cid:2)E (cid:2)1(Ar) (cid:10)xi (cid:2)1(Ar) (cid:10)xi\n\nt − ̄xr − η(∇fi(xi\n\nt) − Gi\n\nt − ̄xr − η(∇fi(xi\n\nt) − Gi\n\nt − ̄xr − η(∇fi(xi\n\nt) − Gi\n\nr + Gr), ∇Fi(xi\n\nt; ξi r + Gr), ∇Fi(xi r + Gr), E (cid:2)∇Fi(xi\n\nt) − ∇fi(xi t; ξi t; ξi\n\nt)(cid:11)(cid:3) t) − ∇fi(xi t) − ∇fi(xi\n\n(cid:3)(cid:3)\n\n(cid:12)Ht\n\nt)(cid:11) (cid:12) t)(cid:12) (cid:12)Ht\n\n(cid:3)(cid:11)(cid:3) = 0,\n\nand\n\nEr\n\n(cid:2)1(Ar)(cid:107)∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi\n\nt)(cid:107)2(cid:3) = Er ≤ Er\n\n(cid:2)E (cid:2)(cid:107)∇Fi(xi (cid:2)1(Ar)σ2(cid:3) = prσ2.\n\nt; ξi\n\nt) − ∇fi(xi\n\nt)(cid:107)2(cid:12)\n\n(cid:12)Ht\n\n(cid:3)(cid:3)\n\nη + 2σ)). Applying the upper bound for Hessian matrix in Lemma 2\n\nLet L = L0 + L1(κ + (ρ + 1)( γ and the premise in Lemma 1, we have r + Gr(cid:107)2(cid:3)\n\nt) − Gi\n\nEr\n\n(cid:104)\n\n(cid:2)1(Ar)(cid:107)∇fi(xi 1(Ar) (cid:13) (cid:104)\n\n= Er\n\n≤ 2Er\n\n≤ 4Er\n\n≤ 4Er\n\n1(Ar) (cid:13) (cid:2)1(Ar)(cid:107)∇fi(xi (cid:34) (cid:13) (cid:13) (cid:13) (cid:13) (cid:2)1(Ar)(cid:107)xi\n\n1(Ar)\n\n(cid:90) 1\n\n0\n\n(cid:13)(∇fi(xi\n\nt) − ∇fi( ̄xr)) + (∇fi( ̄xr) − Gi\n\n(cid:13)(∇fi(xi\n\nt) − ∇fi( ̄xr)) + (∇fi( ̄xr) − Gi\n\nt) − ∇fi( ̄xr)(cid:107)2(cid:3) + 4prσ2 + 2Er\n\n2(cid:105)\n\n(cid:13) (cid:13)\n\n2(cid:105)\n\nr) + Gr r)(cid:13) (cid:13) (cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3)\n\n+ 2Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3)\n\n∇2fi(αxi\n\nt + (1 − α) ̄xr)(xi\n\nt − ̄xr)dα\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 4prσ2 + 2Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3)\n\n≤ 4L2Er\n\nt − ̄xr(cid:107)2(cid:3) + 4prσ2 + 2Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) ,\n\n(17)\n\nwhere the second inequality follows from (cid:107)Gi bound of (17) into (16) yields\n\nr − ∇fi( ̄xr)(cid:107) ≤ σ almost surely. Plugging the final\n\nEr\n\n(cid:2)1(Ar)(cid:107)xi\n\nt+1 − ̄xr(cid:107)2(cid:3) ≤\n\n(cid:19)\n\n+ 1 + 4LIη2\n\n(cid:18) 1 I\n+ 2(I + 1)η2Er\n\nEr\n\n(cid:2)1(Ar)(cid:107)xi\n\nt − ̄xr(cid:107)2(cid:3) (cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) + 10pr(I + 1)η2σ2.\n\n(18)\n\nBy recursively invoking (18), we are guaranteed that\n\nEr\n\n(cid:2)1(Ar)(cid:107)xi\n\nt+1 − ̄xr(cid:107)2(cid:3) ≤\n\nI−1 (cid:88)\n\n(cid:19)s\n\n+ 1 + 4LIη2\n\n(cid:18) 1 I\n\n(I + 1) (cid:0)2η2Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) + 10prη2σ2(cid:1)\n\ns=0 (cid:0) 1\n\nI + 1 + 4LIη2(cid:1)I I + 4LIη2\n\n1\n\n=\n\n(i) ≤\n\n(cid:0) 2\n\nI + 1(cid:1)I\n\n1 I\n\n(I + 1) (cid:0)2η2Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) + 10prη2σ2(cid:1)\n\n(I + 1) (cid:0)2η2Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) + 10prη2σ2(cid:1)\n\n(ii)\n\n≤ 9 (cid:0)2I 2η2Er ≤ 36I 2η2 (cid:0)Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) + 10prI 2η2σ2(cid:1) (cid:2)1(Ar)(cid:107)Gr − ∇f ( ̄xr)(cid:107)2(cid:3) + pr(cid:107)∇f ( ̄xr)(cid:107)2(cid:1) + 90prI 2η2σ2\n\n(iii)\n\n≤ 36prI 2η2(cid:107)∇f ( ̄xr)(cid:107)2 + 126prI 2η2σ2.\n\nThe inequality (i) comes from\n\n4Iη2L2 =\n\n1 I\n\n(2IηL)2 ≤\n\n1 I\n\n(cid:18)\n\n(cid:18)\n\n2Iη\n\nL0 + L1κ + L1(ρ + 1)(2σ +\n\n(cid:19)(cid:19)2\n\nγ η\n\n)\n\n≤\n\n1 I\n\n,\n\nwhich is true because 2ηI(AL0 + BL1κ + BL1ρ(σ + γ comes from ( 2\n\nη )) ≤ 1 and A, B ≥ 1. The inequality (ii) I +1)I (I +1) ≤ e2I for any I ≥ 1. The inequality (iii) holds since (cid:107)Gr −∇f ( ̄xr)(cid:107) ≤\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nσ almost surely. Therefore, we have proved (14). In addition, for (15), we notice that Er\n\n(cid:2)1(Ar)(cid:107)Gr(cid:107)2(cid:3) + 90prI 2η2σ2\n\nt+1 − ̄xr(cid:107)2(cid:3) ≤ 18I 2η2Er\n\n(cid:2)1(Ar)(cid:107)xi\n\n≤ 18I 2η2Er [1(Ar)(cid:107)Gr(cid:107) ((cid:107)Gr − ∇f ( ̄xr)(cid:107) + (cid:107)∇f ( ̄xr)(cid:107))] + 90prI 2η2σ2 ≤ 18prI 2η2 γ\n\n(σ + (cid:107)∇f ( ̄xr)(cid:107)) + 90prI 2η2σ2\n\n(iv)\n\nη\n\n= 18prI 2ηγ(cid:107)∇f ( ̄xr)(cid:107) + 18prI 2η2\n\nσ + 5σ2\n\n(cid:19)\n\n.\n\n(cid:18) γ η\n\nThe inequality (iv) holds since (cid:107)Gr(cid:107) ≤ γ/η holds under the event Ar and (cid:107)Gr − ∇f ( ̄xr)(cid:107) ≤ σ almost surely.\n\nC PROOF OF MAIN RESULTS\n\nC.1 PROOF OF LEMMA 4\n\nLemma 4 restated. Under the conditions of Lemma 1, let pr = P(Ar|Fr), Γ = AL0 + BL1(κ + ρ( γ\n\nη + σ)). Then it holds that for each 1 ≤ r ≤ R − 1,\n\nEr [f ( ̄xr+1) − f ( ̄xr)] ≤ Er [1(Ar)V ( ̄xr)] + Er\n\n(cid:2)1( ̄Ar)U ( ̄xr)(cid:3) ,\n\nwhere\n\nV ( ̄xr) =\n\n(cid:18)\n\n−\n\nηI 2\n\n+ 36Γ2I 3η3 + 9\n\n+ 90Γ2I 3η3σ2 +\n\n(cid:19)\n\nBL1I 2η2\n\nγ η\n2AL0Iη2σ2 N\n\n(cid:107)∇f ( ̄xr)(cid:107)2 + 9BL1I 2η2\n\n(cid:18)\n\n5σ2 +\n\n(cid:19)\n\nγ η\n\nσ\n\n(cid:107)∇f ( ̄xr)(cid:107)\n\n,\n\nand\n\nU ( ̄xr) =\n\n(cid:18)\n\n−\n\n2 5\n\nγI +\n\nBL1(4ρ + 1)γ2I 2 2\n\n(cid:19)\n\n(cid:107)∇f ( ̄xr)(cid:107) −\n\n3γ2I 5η\n\n+ γ2I 2(3AL0 + 2BL1κ) + 6γIσ.\n\nProof. We begin by applying Lemma 5 to obtain a bound on f ( ̄xr+1) − f ( ̄xr), but first we must show that the conditions of Lemma 5 hold here. Note that (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:107) ̄xr+1 − ̄xr(cid:107) =\n\n− ̄xr\n\nN (cid:88)\n\n1 N\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ntr+1\n\nxi\n\ni=1\n\n≤\n\n1 N\n\nN (cid:88)\n\ni=1 (cid:26)\n\n1(Ar)(cid:107)xi\n\ntr+1\n\n− ̄xr(cid:107) +\n\n1 N\n\nN (cid:88)\n\ni=1\n\n1( ̄Ar)(cid:107)xi\n\ntr+1\n\n− ̄xr(cid:107)\n\n(cid:18)\n\n≤ max\n\n2ηI\n\n2σ +\n\n(cid:19)\n\n(cid:27)\n\n, γI\n\n≤\n\nγ η\n\nC L1\n\n,\n\nwhere the last step is due to the conditions of Lemma 1. This shows that we can apply Lemma 5 to obtain\n\nEr [f ( ̄xr+1) − f ( ̄xr)] ≤ Er [(cid:104)∇f ( ̄xr), ̄xr+1 − ̄xr(cid:105)] + Er\n\n(cid:20) AL0 + BL1(cid:107)∇f ( ̄xr)(cid:107) 2\n\n(cid:107) ̄xr+1 − ̄xr(cid:107)2\n\n(cid:21)\n\n≤ −ηEr\n\n− γEr\n\n(cid:34)\n\n1 N\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n1(Ar)(cid:104)∇f ( ̄xr), gi t(cid:105)\n\n(cid:35)\n\n1( ̄Ar)(cid:104)∇f ( ̄xr),\n\n(cid:35)\n\ngi t\n(cid:107)gi\n\nt(cid:107)\n\n(cid:105)\n\n+\n\nAL0 2\n\nEr\n\n(cid:2)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:3) +\n\nBL1 2\n\n(cid:107)∇f ( ̄xr)(cid:107)Er\n\n(cid:2)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:3) .\n\n(19)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nLet pr = P(Ar|Fr), then 1 − pr = P( ̄Ar|Fr). Notice that pr is a function of ̄xr. The last term in Equation (19) can be bounded as follows:\n\n(cid:107)∇f ( ̄xr)(cid:107)Er = (cid:107)∇f ( ̄xr)(cid:107)E (cid:2)1(Ar)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:12)\n\n(cid:2)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:3)\n\n(cid:12)Fr\n\n(cid:3) + (cid:107)∇f ( ̄xr)(cid:107)E (cid:2)1( ̄Ar)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:12)\n\n(cid:12)Fr\n\n(cid:3)\n\n(i)\n\n≤ (cid:107)∇f ( ̄xr)(cid:107)E (cid:2)1(Ar)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:12)\n\n(cid:12)Fr\n\n(cid:3) + (1 − pr)γ2I 2(cid:107)∇f ( ̄xr)(cid:107)\n\n(ii)\n\n≤ 18prI 2η2(cid:107)∇f ( ̄xr)(cid:107)\n\n(cid:18) γ η\n\n(cid:107)∇f ( ̄xr)(cid:107) + 5σ2 +\n\n≤ 18prI 2ηγ(cid:107)∇f ( ̄xr)(cid:107)2 + 18prI 2η2\n\n(cid:18)\n\n5σ2 +\n\nγ η\n\nσ\n\nγ η\n(cid:19)\n\n(cid:19)\n\n+ (1 − pr)γ2I 2(cid:107)∇f ( ̄xr)(cid:107)\n\nσ\n\n(cid:107)∇f ( ̄xr)(cid:107) + (1 − pr)γ2I 2(cid:107)∇f ( ̄xr)(cid:107), (20)\n\nwhere (i) comes from an application of Lemma 1 with t = tr+1, and (ii) comes from an application of (15) in Lemma 3. Substituting (20) into (19) gives\n\nEr [f ( ̄xr+1) − f ( ̄xr)]\n\n≤ −ηEr\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n1(Ar)(cid:104)∇f ( ̄xr), gi t(cid:105)\n\n− γEr\n\n(cid:35)\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n1( ̄Ar)(cid:104)∇f ( ̄xr),\n\n(cid:35)\n\ngi t\n(cid:107)gi\n\nt(cid:107)\n\n(cid:105)\n\n+\n\nAL0 2\n\nEr\n\n(cid:2)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:3) + 9prBL1I 2η2\n\n(cid:18) γ η\n\n(cid:107)∇f ( ̄xr)(cid:107)2 +\n\n(cid:18)\n\n5σ2 +\n\n(cid:19)\n\nγ η\n\nσ\n\n(cid:19)\n\n(cid:107)∇f ( ̄xr)(cid:107)\n\n+ (1 − pr)\n\nBL1γ2I 2 2\n\n(cid:107)∇f ( ̄xr)(cid:107)\n\n(21)\n\nWe introduce three claims to bound the first three terms in (21), whose proofs are deferred to Section D.\n\nClaim 1. Under the conditions of Lemma 4, we have\n\n− γEr\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir (cid:20)(cid:18)\n\n1( ̄Ar)(cid:104)∇f ( ̄xr),\n\n≤ (1 − pr)\n\n−\n\nγI + 2BL1ργ2I 2\n\n2 5\n\ngi t\n(cid:107)gi\n\n(cid:105)\n\nt(cid:107) (cid:19)\n\n(cid:35)\n\n(cid:107)∇f ( ̄xr)(cid:107) −\n\n3γ2I 5η\n\n+ 2γ2I 2(AL0 + BL1κ) + 6γIσ\n\n(cid:21)\n\n.\n\nClaim 2. Under the conditions of Lemma 4, we have\n\n− ηEr\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n1(Ar)(cid:104)∇f ( ̄xr), gi t(cid:105)\n\n(cid:35)\n\n≤ pr\n\n(cid:20)(cid:18)\n\n−\n\nηI 2\n\n(cid:19)\n\n+ 36I 3η3Γ2\n\n(cid:107)∇f ( ̄xr)(cid:107)2 + 126I 3η3σ2Γ2\n\n(cid:21)\n\n−\n\nη 2I\n\nEr\n\n 1(Ar)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\nwhere Γ = AL0 + BL1\n\n(cid:16)\n\nκ + ρ\n\n(cid:16)\n\nσ + γ\n\nη\n\n(cid:17)(cid:17)\n\n.\n\n∇f (xi t)\n\n ,\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nClaim 3. Under the conditions of Lemma 4, we have\n\nEr\n\n(cid:2)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:3) ≤ 2(1 − pr)γ2I 2 +\n\n4prIσ2η2 N\n\n+ 4η2Er\n\n 1(Ar)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n∇fi(xi t)\n\n19\n\n2\n\n .\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nPublished as a conference paper at ICLR 2023\n\nCombining Claims 1, 2, and 3 with (19) and (20) yields Er [f ( ̄xr+1) − f ( ̄xr)]\n\n(cid:20) (cid:18)\n\n≤ pr\n\n−\n\nηI 2\n\n+ 36Γ2I 3η3 + 9\n\n(cid:19)\n\nBL1I 2η2\n\nγ η\n\n(cid:107)∇f ( ̄xr)(cid:107)2 + 9prBL1I 2η2\n\n(cid:18)\n\n5σ2 +\n\n(cid:19)\n\nγ η\n\nσ\n\n(cid:107)∇f ( ̄xr)(cid:107)+\n\n126Γ2I 3η3σ2 +\n\n(cid:21)\n\n2AL0Iη2σ2 N\n\n(cid:19)\n\n(cid:107)∇f ( ̄xr)(cid:107) −\n\n3γ2I 5η\n\n+ γ2I 2(3AL0 + 2BL1κ) + 6γIσ\n\n+ (1 − pr)\n\n(cid:20)(cid:18)\n\n−\n\n2 5\n\nγI +\n\n(cid:16)\n\n+\n\n2AL0η2 −\n\n(cid:17)\n\nEr\n\nη 2I\n\nBL1(4ρ + 1)γ2I 2 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n 1(Ar)\n\nN (cid:88)\n\n1 N\n\ni=1\n\n(cid:88)\n\nt∈Ir (cid:19)\n\n∇f (xi t)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:20) (cid:18)\n\n≤ pr\n\n−\n\nηI 2\n\n+ 36Γ2I 3η3 + 9\n\nγ η\n\nBL1I 2η2\n\n(cid:107)∇f ( ̄xr)(cid:107)2 + 9BL1I 2η2\n\n(cid:18)\n\n5σ2 +\n\n(cid:19)\n\nγ η\n\nσ\n\n(cid:107)∇f ( ̄xr)(cid:107)+\n\n90Γ2I 3η3σ2 +\n\n+ (1 − pr)\n\n(cid:20)(cid:18)\n\n−\n\n2 5\n\n(cid:21)\n\n2AL0Iη2σ2 N\nBL1(4ρ + 1)γ2I 2 2\n\nγI +\n\n(cid:19)\n\n(cid:107)∇f ( ̄xr)(cid:107) −\n\n3γ2I 5η\n\n+ γ2I 2(3AL0 + 2BL1κ) + 6γIσ\n\nwhere the last inequality holds since η/(2I) ≥ 4η2 due to the assumption 4AL0ηI ≤ 1. Then we can finish the proof of Lemma 4 by noticing that pr = Er[1(Ar)] and 1 − pr = Er[1( ̄Ar)].\n\nC.2 PROOF OF THEOREM 1\n\n(cid:21)\n\n(cid:21)\n\n,\n\nTheorem 1 restated. Suppose Assumption 1 hold. For any (cid:15) ≤ 3AL0\n\nη ≤ min\n\n(cid:26) 1\n\n856ΓI\n\n,\n\n(cid:15) 180ΓIσ\n\n,\n\n(cid:27)\n\nN (cid:15)2 8AL0σ2 (cid:17)\n\nwhere Γ = AL0 + BL1κ + BL1ρ\n\n(cid:16)\n\nσ + γ\n\nη\n\n. The output of EPISODE satisfies\n\n5BL1ρ , we choose (cid:19) (cid:18)\n\nAL0 BL1ρ\n\nand γ =\n\n11σ +\n\nη,\n\n(22)\n\n1 R\n\nR (cid:88)\n\nt=0\n\nE [(cid:107)∇f ( ̄xr)(cid:107)] ≤ 3(cid:15)\n\nas long as R ≥ 4∆\n\n(cid:15)2ηI .\n\nProof. In order to apply Lemma 4, we must verify the conditions of Lemma 1 under our choice of hyperparameters. From our choices of η and γ, we have\n\n2ΓηI ≤\n\n1 856\n\n< 1.\n\nAlso\n\n(cid:18)\n\n2ηI\n\n2σ +\n\n(cid:19) (i) ≤\n\nγ η\n\n(cid:16)\n\n856\n\n2σ + γ\n\nη\n\nAL0 + BL1κ + BL1ρ\n\n(cid:16)\n\nσ + γ\n\nη\n\n(cid:17)(cid:17)\n\n(ii) ≤\n\nC L1\n\n,\n\nwhere (i) comes from the condition η ≤ 1/(856ΓI) in (22), (ii) is true due to the fact that B, C ≥ 1 and ρ ≥ 1. Lastly, it also holds that\n\nγI ≤ 4ηIσ + 2γI = 2ηI\n\n2σ +\n\n(cid:18)\n\n(cid:19)\n\nγ η\n\n≤\n\nC L1\n\n.\n\nTherefore the conditions of Lemma 1 are satisfied, and we can apply Lemma 4. Denoting\n\n(cid:18)\n\nU (x) =\n\n−\n\n2 5\n\nγI +\n\nBL1(4ρ + 1)γ2I 2 2\n\n(cid:19)\n\n(cid:107)∇f (x)(cid:107)−\n\n3γ2I 5η\n\n+γ2I 2(3AL0 +2BL1κ)+6γIσ, (23)\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nand\n\n(cid:18)\n\nV (x) =\n\n−\n\nηI 2\n\n+ 36Γ2I 3η3 + 9\n\nI 2η2\n\nγ η\n2AL0Iη2σ2 N\n\n.\n\n+ 126Γ2I 3η3σ2 +\n\nLemma 4 tells us that\n\n(cid:19)\n\n(cid:107)∇f (x)(cid:107)2 + 9prI 2η2\n\n(cid:18)\n\n5σ2 +\n\nγ η\n\n(cid:19)\n\nσ\n\n(cid:107)∇f (x)(cid:107)\n\nEr [f ( ̄xr+1) − f ( ̄xr)] ≤ Er\n\n(cid:2)1( ̄Ar)U ( ̄xr) + 1(Ar)V ( ̄xr)(cid:3) .\n\nWe will proceed by bounding each U (x) and V (x) by the same linear function of (cid:107)∇f (x)(cid:107).\n\nTo bound U (x), notice\n\n−\n\n2 5\n\nγI+\n\nBL1(4ρ + 1)γ2I 2 2\n\n= −\n\n2 γI + 2BL1ργ2I 2 + 5\n(cid:18)\n\n+ 2BL1ργI +\n\n2 5\n\n1 2\n1 2\n\nBL1γ2I 2 (cid:19)\n\nBL1γI\n\n≤ γI\n\n−\n\n(cid:18)\n\n≤ γI\n\n−\n\n(cid:18)\n\n+ 2BL1ρ\n\n11σ +\n\n2 5\n\n(cid:19)\n\nAL0 BL1ρ\n\nηI +\n\n1 2\n\n(cid:18)\n\nBL1\n\n11σ +\n\n(cid:19)\n\n(cid:19)\n\nηI\n\nAL0 BL1ρ\n\n(i) ≤ γI\n\n(cid:18)\n\n−\n\n2 5\n\n+ 3 (11BL1ρσ + AL0) ηI\n\n(cid:19)\n\n(ii) ≤ γI\n\n(cid:18)\n\n−\n\n2 5\n\n+\n\n18 856\n\n(cid:19)\n\n≤ −\n\n3 10\n\nγI\n\n(iii) ≤ −\n\n3 10\n\nAL0 BL1ρ\n\n(iv) ≤ −\n\nηI\n\n1 2\n\n(cid:15)ηI,\n\n(24)\n\n(25)\n\n(26)\n\nwhere (i) comes from ρ ≥ 1 and (ii) comes from 856ΓηI ≤ 1 and (iii) holds since γ/η = 11σ + AL0 5BL1ρ . Also, we have (cid:18)\n\nBL1ρ and (iv) comes from (cid:15) ≤ 3AL0\n\n(cid:19)\n\n+ γ2I 2(3AL0 + 2BL1κ) + 6γIσ ≤\n\n−\n\n+ 3ΓηI + 6σ\n\nγ2I η\n\n3 5\n\n−\n\n3γ2I 5η\n\nη γ\n\n(cid:33)\n\nγ2I η\n\nγ2I η\n\n(cid:32)\n\n−\n\n(cid:18)\n\n−\n\n3 5\n\n3 5\n\n+\n\n+\n\n3 856\n\n3 856\n\n≤\n\n≤\n\n+\n\n6σ 11σ + AL0 BL1ρ\n\n(cid:19)\n\n+\n\n6 11\n\n≤ 0.\n\nPlugging Equations (26) and (27) into Equation (23) yields\n\nU (x) ≤ −\n\n1 2\n\n(cid:15)ηI(cid:107)∇f (x)(cid:107).\n\nNow to bound V (x), we have\n\n−\n\nηI 2\n\n+ 36Γ2I 3η3 + 9\n\nγ η\n\nBL1I 2η2\n\n(i) ≤ −\n\nηI +\n\n36\n\n8562 ηI +\n\n9(11BL1σ + AL0/ρ) 856Γ\n\nηI\n\n1 2\n1 4\n\n≤ −\n\nηI,\n\n(27)\n\n(28)\n\n(29)\n\nwhere (i) comes from η ≤ 1 η ≤ (cid:15)\n\n180IΓσ , it holds that\n\n856ΓI and Γ > BL1σ + AL0/ρ for ρ > 1. Using the assumption\n\n9BL1I 2η2\n\n(cid:18)\n\n5σ2 +\n\n(cid:19)\n\nγ η\n\nσ\n\n= 9BL1I 2η2\n\n(cid:18)\n\n16σ2 +\n\n(cid:19)\n\nAL0σ BL1ρ\n\n≤ ηI(cid:15)\n\n16BL1σ + AL0 20Γ\n\n(ii) ≤\n\n1 4\n\n(cid:15)ηI\n\n(30)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nwhere (ii) comes from 16BL1σ + AL0 < 5Γ. Lastly, we have\n\n90Γ2I 3η3σ2 +\n\n2AL0Iη2σ2 N\n\n(cid:18)\n\n= ηI\n\n90Γ2I 2η2σ2 +\n\n(cid:19)\n\n2AL0ησ2 N\n\n(cid:18)\n\n(iii) ≤ ηI\n\n90Γ2σ2 ·\n\n(cid:15)2 1802Γ2σ2 +\n\n2AL0σ2 N\n\nN (cid:15)2 8AL0σ2\n\n(cid:19)\n\nwhere (i) comes from η ≤ min\n\n≤\n\n1 4\n\n(cid:15)2ηI,\n\n(cid:110) (cid:15)\n\n180IΓσ , N (cid:15)2\n\n8AL0σ2\n\n(cid:111)\n\n. Plugging Equations (29), (30), and (31) into (24)\n\n(31)\n\nthen yields\n\nV (x) ≤ −\n\n1 4\n\nηI(cid:107)∇f (x)(cid:107)2 +\n\n1 4\n\n(cid:15)ηI(cid:107)∇f (x)(cid:107) +\n\n1 4\n\n(cid:15)2ηI\n\nWe can then use the inequality x2 ≥ 2ax − a2 with x = (cid:107)∇f (x)(cid:107) and a = (cid:15) to obtain\n\nV (x) ≤ −\n\n1 4\n\n(cid:15)ηI(cid:107)∇f (x)(cid:107) +\n\n1 2\n\n(cid:15)2ηI.\n\n(32)\n\nHaving bounded U (x) and V (x), we can return to (25). Using (28), we can see\n\nU (x) ≤ −\n\n1 2\n\n(cid:15)ηI(cid:107)∇f (x)(cid:107) ≤ −\n\n1 4\n\n(cid:15)ηI(cid:107)∇f (x)(cid:107) +\n\n1 2\n\n(cid:15)2ηI,\n\nso the RHS of (32) is an upper bound of both U (x) and V (x). Plugging this bound into (25) and taking total expectation then gives\n\nE [f ( ̄xr+1) − f ( ̄xr)] ≤ −\n\n1 4\n\n(cid:15)ηIE [(cid:107)∇f ( ̄xr)(cid:107)] +\n\n1 2\n\n(cid:15)2ηI.\n\nFinally, denoting ∆ = f ( ̄x0) − f ∗, we can unroll the above recurrence to obtain\n\nE [f ( ̄xR+1) − f ( ̄x0)] ≤ −\n\n1 4\n\n(cid:15)ηI\n\nR (cid:88)\n\nr=0\n\nE [(cid:107)∇f ( ̄xr)(cid:107)] +\n\n1 2\n\n(R + 1)(cid:15)2ηI,\n\n1 R + 1\n\n1 R + 1\n\nR (cid:88)\n\nr=0\n\nR (cid:88)\n\nr=0\n\nE [(cid:107)∇f ( ̄xr)(cid:107)] ≤\n\n4∆ (cid:15)ηI(R + 1)\n\n+ 2(cid:15),\n\nE [(cid:107)∇f ( ̄xr)(cid:107)] ≤ 3(cid:15),\n\nwhere the last inequality comes from our choice of R ≥ 4∆\n\n(cid:15)2ηI .\n\nD DEFERRED PROOFS OF SECTION C\n\nD.1 PROOF OF CLAIM 1\n\nProof. Starting from Lemma 7 with u = ∇f ( ̄xr) and v = gi\n\nt, we have\n\n−\n\n(cid:104)∇f ( ̄xr), gi t(cid:105) (cid:107)gi\n\nt(cid:107)\n\n≤ −μ(cid:107)∇f ( ̄xr)(cid:107) − (1 − μ)(cid:107)gi\n\nt(cid:107) + (1 + μ)(cid:107)gi\n\nt − ∇f ( ̄xr)(cid:107).\n\n(33)\n\nUnder ̄Ar = {(cid:107)Gr(cid:107) > γ\n\nη }, note that gi\n\nt = ∇Fi(xi\n\nt; ξi\n\nt) − Gi\n\nr + Gr, and we have\n\n(cid:107)gi\n\nt(cid:107) ≥ (cid:107)Gr(cid:107) − (cid:107)∇Fi(xi t, ξi\n\n− (cid:107)∇Fi(xi\n\nt) − Gi t, ξi r(cid:107) t)(cid:107) − (cid:107)∇fi(xi t) − ∇fi(xi\n\n≥\n\nt) − ∇fi( ̄xr)(cid:107) − (cid:107)∇fi( ̄xr) − Gi\n\nr(cid:107)\n\nγ η\nγ η\n\n≥\n\n− 2σ − (cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nand\n\n(cid:107)gi\n\nt − ∇f ( ̄xr)(cid:107) ≤ (cid:107)∇Fi(xi\n\nt) − ∇fi(xi\n\nt)(cid:107) + (cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107)\n\nt, ξi + (cid:107)∇fi( ̄xr) − Gi\n\nr(cid:107) + (cid:107)Gr − ∇f ( ̄xr)(cid:107)\n\nPlugging these two inequalities into (33) yields\n\n≤ 3σ + (cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107).\n\n−\n\n(cid:104)∇f ( ̄xr), gi t(cid:105) (cid:107)gi\n\nt(cid:107)\n\n≤ −μ(cid:107)∇f ( ̄xr)(cid:107) − (1 − μ)\n\nγ η\n\n+ (5 + μ)σ + 2(cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107).\n\nUnder ̄Ar, we know (cid:107)xi 6 to obtain\n\nt − ̄xr(cid:107) ≤ γI, and γI ≤ C\n\nL1\n\nby assumption. Therefore we can apply Lemma\n\n(cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107) ≤ (AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107))(cid:107)xi\n\nt − ̄xr(cid:107) ≤ γI(AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107)).\n\nThis implies that\n\n−\n\n(cid:104)∇f ( ̄xr), gi t(cid:105) (cid:107)gi\n\nt(cid:107)\n\n≤ −μ(cid:107)∇f ( ̄xr)(cid:107) − (1 − μ)\n\nγ η\n\n+ (5 + μ)σ + 2AL0γI + 2BL1γI(cid:107)∇fi( ̄xr)(cid:107).\n\nCombining this with the choice μ = 2/5, we have the final bound:\n\n− γEr\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n1( ̄Ar)(cid:104)∇f ( ̄xr),\n\n(cid:35)\n\ngi t\n(cid:107)gi\n\nt(cid:107)\n\n(cid:105)\n\n≤\n\n1 N\n\nN (cid:88)\n\n(1 − pr)\n\ni=1\n\n(cid:18)\n\n−\n\n2 5\n\nγI(cid:107)∇f ( ̄xr)(cid:107) −\n\n3γ2I 5η\n\n+ 6γIσ + 2AL0γ2I 2 + 2BL1γ2I 2(cid:107)∇fi( ̄xr)(cid:107)\n\n(cid:19)\n\n≤ (1 − pr)\n\n(cid:18)(cid:18)\n\n−\n\n2 5\n\nγI + 2BL1ργ2I 2\n\n(cid:19)\n\n(cid:107)∇f ( ̄xr)(cid:107) −\n\n3γ2I 5η\n\n+ 2γ2I 2(AL0 + BL1κ) + 6γIσ\n\n(cid:19)\n\nwhere we used the heterogeneity assumption (cid:107)∇fi( ̄xr)(cid:107) ≤ κ + ρ(cid:107)∇f ( ̄xr)(cid:107).\n\nD.2 PROOF OF CLAIM 2\n\nProof. Recall the event Ar = {(cid:107)Gr(cid:107) ≤ γ/η}, we have\n\nN (cid:88)\n\n(cid:88)\n\n1(Ar)(cid:104)∇f ( ̄xr), gi t(cid:105)\n\n= Er\n\n1(Ar)\n\nI∇f ( ̄xr),\n\n(cid:35)\n\n(cid:34)\n\n(cid:42)\n\n(cid:43)(cid:35)\n\n(cid:88)\n\nt∈Ir\n\n1 N\n\nN (cid:88)\n\ni=1\n\ngi\n\nt\n\nIEr\n\n(cid:34)\n\n1 N\n\n(i)\n\n= Er\n\nt∈Ir\n\ni=1 (cid:34)\n\n(cid:42)\n\n1(Ar)\n\nI∇f ( ̄xr),\n\n(cid:88)\n\nt∈Ir\n\n1 N\n\n(ii)\n\n= Er\n\n(cid:34)\n\n(cid:42)\n\n1(Ar)\n\nI∇f ( ̄xr),\n\n(cid:88)\n\nN (cid:88)\n\n(cid:43)(cid:35)\n\n∇Fi(xi\n\nt; ξi t)\n\ni=1\n\nN (cid:88)\n\n(cid:43)(cid:35)\n\n∇fi(xi t)\n\n1 N\n\nt∈Ir \n1(Ar)\n\ni=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nEr\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n∇f (xi t)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(iii) =\n\nprI 2 2\n\n(cid:107)∇f ( ̄xr)(cid:107)2 +\n\n1 2\n\n 1(Ar)\n\n−\n\n1 2\n\nEr\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:32)\n\n(cid:88)\n\nt∈Ir\n\n1 N\n\nN (cid:88)\n\ni=1\n\n2\n\n(cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:80)N\n\n∇fi(xi\n\nt) − ∇f ( ̄xr)\n\n .\n\n(34)\n\nThe equality (i) is obtained from the fact that 1\n\n1 N\n\n(cid:80)N\n\ni=1 ∇Fi(xi t, ξi E (cid:2)1(Ar)∇Fi(xi\n\n(cid:80)N\n\ni=1 gi t). The equality (ii) holds due to the tower property such that for t > tr t)(cid:12)\n\n(cid:3) = E (cid:2)1(Ar)E (cid:2)∇Fi(xi\n\n(cid:3) = E (cid:2)1(Ar)∇fi(xi\n\ni=1 ∇Fi(xi\n\nt) − Gi\n\nt = 1\n\nt, ξi\n\nt, ξi\n\nt, ξi\n\nt)(cid:12)\n\nt)(cid:12)\n\n(cid:12)Ht\n\n(cid:12)Fr\n\n(cid:12)Fr\n\n(cid:3) (cid:12)\n\nN\n\nN\n\n(cid:3) ;\n\n(cid:12)Fr\n\nr + Gr =\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nfor t = tr\n\nE (cid:2)1(Ar)∇Fi( ̄xr, ξi\n\ntr\n\n)(cid:12) (cid:12)Fr\n\n(cid:3) = E [1(Ar)|Fr] E (cid:2)∇Fi( ̄xr, ξi\n\ntr\n\n)(cid:12) (cid:12)Fr\n\n(cid:3) = E (cid:2)1(Ar)∇fi( ̄xr)(cid:12)\n\n(cid:12)Fr\n\n(cid:3) ,\n\nwhich is true since Gr = 1 holds because 2(cid:104)a, b(cid:105) = (cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 − (cid:107)a − b(cid:107)2.\n\ni=1 ∇Fi( ̄xr; (cid:101)ξi\n\nN\n\n(cid:80)N\n\nr) is independent of ∇Fi( ̄xr, ξi\n\ntr\n\n) given Fr, and (iii)\n\nLet Γ = AL0 + BL1 6 to obtain\n\n(cid:16)\n\nκ + ρ\n\n(cid:16)\n\nσ + γ\n\nη\n\n(cid:17)(cid:17)\n\n. Notice that we can apply the relaxed smoothness in Lemma\n\nEr\n\n(cid:2)1(Ar)(cid:107)∇fi(xi\n\nt) − ∇fi( ̄xr)(cid:107)2(cid:3)\n\n≤ Er ≤ Er\n\n(cid:2)1(Ar)(AL0 + BL1(cid:107)∇fi( ̄xr)(cid:107))2(cid:107)xi (cid:2)1(Ar)(AL0 + BL1(κ + ρ(cid:107)∇f ( ̄xr)(cid:107)))2(cid:107)xi\n\nt − ̄xr(cid:107)2(cid:3)\n\nt − ̄xr(cid:107)2(cid:3)\n\n(i)\n\n≤ Γ2Er\n\n(cid:2)1(Ar)(cid:107)xi\n\nt − ̄xr(cid:107)2(cid:3)\n\n(ii)\n\n≤ 18prI 2η2Γ2 (cid:0)2(cid:107)∇f ( ̄xr)(cid:107)2 + 7σ2(cid:1) .\n\nThe inequality (i) holds since (cid:107)∇f ( ̄xr)(cid:107) ≤ (cid:107)∇f ( ̄xr) − Gr(cid:107) + (cid:107)Gr(cid:107) ≤ σ + γ/η almost surely under the event Ar. The inequality (ii) follows from the bound (14) in Lemma 3. Therefore, we are guaranteed that\n\n 1(Ar)\n\nEr\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:32)\n\n(cid:88)\n\nt∈Ir\n\n1 N\n\nN (cid:88)\n\ni=1\n\n∇fi(xi\n\nt) − ∇f ( ̄xr)\n\n2\n\n\n\n(cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ I\n\n≤ I\n\n(cid:88)\n\nt∈Ir\n\n(cid:88)\n\nt∈Ir\n\n1 N\n\n1 N\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\nEr\n\n1(Ar) (cid:13)\n\n(cid:13)∇fi(xi\n\nt) − ∇f ( ̄xr)(cid:13) (cid:13)\n\n2(cid:105)\n\n18prI 2η2Γ2 (cid:0)2(cid:107)∇f ( ̄xr)(cid:107)2 + 7σ2(cid:1)\n\n≤ 18prI 4η2Γ2 (cid:0)2(cid:107)∇f ( ̄xr)(cid:107)2 + 7σ2(cid:1) .\n\n(35)\n\nMultiplying both sides of (34) by −η/I and substituting (35) then yields\n\n− ηEr\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n1(Ar)(cid:104)∇f ( ̄xr), gi t(cid:105)\n\n(cid:35)\n\n≤ −\n\nprηI 2\n\n(cid:107)∇f ( ̄xr)(cid:107)2 −\n\n 1(Ar)\n\nη 2I\n\nEr\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n∇f (xi t)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\nprη 2I\n\nEr\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:32)\n\n(cid:88)\n\nt∈Ir\n\n1 N\n\nN (cid:88)\n\ni=1\n\n∇fi(xi\n\nt) − ∇f ( ̄xr)\n\n2\n\n\n\n(cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ pr\n\n(cid:20)(cid:18)\n\n−\n\nηI 2\n\n(cid:19)\n\n+ 36I 3η3Γ2\n\n(cid:107)∇f ( ̄xr)(cid:107)2 + 126I 3η3σ2Γ2\n\n(cid:21)\n\n−\n\nη 2I\n\nEr\n\n 1(Ar)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n24\n\n∇f (xi t)\n\n .\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nPublished as a conference paper at ICLR 2023\n\nD.3 PROOF OF CLAIM 3\n\nProof. From the definition of ̄xr+1, we have\n\nEr\n\n(cid:2)(cid:107) ̄xr+1 − ̄xr(cid:107)2(cid:3)\n\n 1(Ar)\n\n≤ 2η2Er\n\n(i)\n\n≤ 2η2Er\n\n 1(Ar)\n\n 1(Ar)\n\n≤ 4η2Er\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n2  + 2γ2Er\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ngi\n\nt\n\n 1( ̄Ar)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\ngi t\n(cid:107)gi\n\nt(cid:107)\n\n∇Fi(xi\n\nt, ξi t)\n\n2  + 2(1 − pr)γ2I 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇fi(xi t)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 4prη2Er\n\n 1(Ar)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi t)\n\n2  + 2(1 − pr)γ2I 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇fi(xi t)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nt∈Ir\n\n(ii)\n\n≤ 4η2Er\n\n 1(Ar)\n\n+ 4η2 1 N 2\n\nN (cid:88)\n\ni=1\n\nEr\n\n1 N\n\nN (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) \n1(Ar)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nt∈Ir\n\n∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi t)\n\n2  + 2(1 − pr)γ2I 2,\n\n(36)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nwhere (i) is obtained by noticing that 1 i=1 gi fact that each client’s stochastic gradients ∇Fi(xi Similarly, let s ∈ Ir with s > t., we can see that\n\nN\n\n(cid:80)N\n\n(cid:80)N\n\nt = 1 t, ξi\n\ni=1 ∇Fi(xi\n\nt), and (ii) holds by the t) are sampled independently from one another.\n\nt, ξi\n\nN\n\nEr\n\n(cid:2)1(Ar)(cid:104)∇Fi(xi\n\nt) − ∇fi(xi\n\nt), ∇Fi(xi\n\ns; ξi\n\ns) − ∇fi(xi\n\ns)(cid:105)(cid:3)\n\nt; ξi (cid:20)\n\n1(Ar)Er\n\n(cid:104)∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi\n\nt), ∇Fi(xi\n\ns; ξi\n\ns) − ∇fi(xi\n\n(cid:20)\n\n1(Ar)(cid:104)∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi\n\nt), Er\n\n(cid:20)\n\n∇Fi(xi\n\n(cid:12) (cid:12) s; ξi s) (cid:12) (cid:12)\n\n(cid:21)\n\nHs\n\n(cid:12) (cid:12) s)(cid:105) (cid:12) (cid:12)\n\nHs\n\n(cid:21)(cid:21)\n\n(cid:21)\n\n− ∇fi(xi\n\ns)(cid:105)\n\n(cid:20)\n\n= Er\n\n= Er\n\n= 0.\n\nTherefore, we have\n\n 1(Ar)\n\nEr\n\n1 N 2\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nt∈Ir\n\n∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi t)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\n=\n\n≤\n\n1 N 2\n\n1 N 2\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\nN (cid:88)\n\n(cid:88)\n\ni=1\n\nt∈Ir\n\n(cid:104)\n\n(cid:104)\n\nEr\n\nEr\n\nprIσ2 N\n\n.\n\n1(Ar) (cid:13)\n\n(cid:13)∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi\n\n2(cid:105)\n\nt)(cid:13) (cid:13)\n\n1(Ar)Er\n\n(cid:104)(cid:13) (cid:13)∇Fi(xi\n\nt; ξi\n\nt) − ∇fi(xi\n\n2 (cid:12)\n\nt)(cid:13) (cid:13)\n\n(cid:12)Ht\n\n(cid:105)(cid:105)\n\n(37)\n\nAnd the desired result is obtained by plugging (37) into (36).\n\nE ADDITIONAL EXPERIMENTAL RESULTS\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nE.1 PROOF OF PROPOSITION 1\n\nProof. Recall the definition of f1(x) and f2(x), f1(x) = x4 − 3x3 + Hx2 + x,\n\nf2(x) = x4 − 3x3 − 2Hx2 + x,\n\nwhich means\n\nand\n\nIt follows that\n\n∇f (x) = 4x3 − 9x2 − Hx + 1.\n\n∇f1(x) − ∇f (x) = 3Hx, ∇f2(x) − ∇f (x) = −3Hx,\n\n(cid:107)∇fi(x)(cid:107) ≤ (cid:107)∇fi(x) − ∇f (x)(cid:107) + (cid:107)∇f (x)(cid:107)\n\n≤ 3H|x| + (cid:107)∇f (x)(cid:107) ≤ 3H|x| − (cid:12) ≤ 4H|x| − (cid:12) ≤ 10H|x| − (cid:12)\n\n(cid:12)4x3 − 9x2 − Hx + 1(cid:12) (cid:12)4x3 − 9x2 + 1(cid:12) (cid:12)4x3 − 9x2(cid:12)\n\n(cid:12) + 2(cid:107)∇f (x)(cid:107) (cid:12) + 1 + 2(cid:107)∇f (x)(cid:107).\n\n(cid:12) + 2(cid:107)∇f (x)(cid:107)\n\n(38)\n\nLet g(x) = 10H|x| − (cid:12)\n\n(cid:12)4x3 − 9x2(cid:12)\n\n(cid:12), next we will characterize g(x) in different region.\n\n(i) When x ∈ (−∞, 0), g(x) = 4x3 − 9x2 − 10Hx. The root for the derivative of g(x) in this region is\n\n12x2 − 18x − 10H = 0 =⇒ x = x1 :=\n\n18 −\n\nIt follows that\n\n√\n\n182 + 480H\n\n24\n\n.\n\ng(x) ≤ 4x3\n\n1 − 9x2 (cid:32) √\n\n1 − 10Hx1\n\n≤ 10H\n\n(cid:33)\n\n182 + 480H − 18 24\n\n≤ 10H\n\n(cid:19)\n\n(cid:18) 20H 24\n\n≤\n\n25H 2 3\n\n.\n\n(39)\n\nwhere the last inequality follows from x1 ≤ 0. (ii) When x ∈ (0, 9 case since 182 − 480H ≤ 0 for H ≥ 1. Then we have\n\n4 ), g(x) = 4x3 − 9x2 + 10Hx. The derivative of g(x) is greater than 0 in this\n\ng(x) ≤ 10H ·\n\n9 4\n\n=\n\n45H 2\n\n.\n\n(iii) When x ∈ ( 9\n\n4 , +∞), g(x) = −4x3 + 9x2 + 10Hx. The root for the derivative of g(x) is\n\n−12x2 + 18x + 10H = 0 =⇒ x = x2 :=\n\n−18 +\n\n√\n\n182 + 480H 24\n\n.\n\nThen we have\n\n(cid:40)\n\ng(x) ≤ max\n\n−4x3\n\n2 + 9x2\n\n2 + 10Hx2, −4\n\n(cid:19)3\n\n(cid:18) 9 4\n\n+ 9\n\n(cid:19)2\n\n(cid:18) 9 4\n\n+\n\n45H 2\n\n(cid:41)\n\n≤ 9x2\n\n2 + 10Hx2 + 9\n\n(cid:19)2\n\n(cid:18) 9 4\n\n+\n\n45H 2\n\n.\n\nCombining (39), (40) and (41), we are guaranteed that\n\ng(x) + 1 ≤ 9\n\n(cid:32)\n\n−18 +\n\n√\n\n182 + 480H 24\n\n(cid:33)2\n\n+ 10H\n\n(cid:32)\n\n−18 +\n\n√\n\n(cid:33)\n\n182 + 480H 24\n\n+\n\n25H 2 3\n\n:= κ(H).\n\n+ 45H + 100\n\n26\n\n(40)\n\n(41)\n\nPublished as a conference paper at ICLR 2023\n\nSubstituting this bound into (38), we get\n\n(cid:107)∇fi(x)(cid:107) ≤ 2(cid:107)∇f (x)(cid:107) + g(x) + 1\n\n≤ 2(cid:107)∇f (x)(cid:107) + κ(H).\n\nAnd κ(H) < ∞ is an increasing function of H.\n\nE.2 SYNTHETIC TASK\n\nFor two algorithms, we inject uniform noise over [−1, 1] into the gradient at each step, and tune γ/η ∈ {5, 10, 15} and tune η ∈ {0.1, 0.01, 0.001}. We run each algorithm for 500 communication rounds and the length of each communication round is I = 8. The results are showed in Figure 3.\n\nE.3 SNLI\n\nThe learning rate η and the clipping parameter γ are tuned with search in the following way: we vary γ ∈ {0.01, 0.03, 0.1} and for each γ we vary η so that the clipping threshold γ/η varies over {0.1, 0.333, 1.0, 3.333, 10.0}, leading to 15 pairs (η, γ). We decay both η and γ by a factor of 0.5 at epochs 15 and 20. We choose the best pair (η, γ) according to the performance on a validation set, and the corresponding model is evaluated on a held-out test set. Note that we do not tune (γ, η) separately for each algorithm. Instead, due to computational constraints, we tune the hyperparameters for the baseline CELGC under the setting I = 4, s = 50% and re-use the tuned values for the rest of the settings.\n\nE.4 CIFAR-10\n\nE.4.1 SETUP\n\nWe train a ResNet-50 (He et al., 2016) for 150 epochs using the cross-entropy loss and a batch size of 64 for each worker. Starting from an initial learning rate η0 = 1.0 and clipping parameter γ = 0.5, we decay the learning rate by a factor of 0.5 at epochs 80 and 120. In this setting, we decay the clipping parameter γ with the learning rate η, so that the clipping threshold γ η remains constant during training. We present results for I = 8 and s ∈ {50%, 70%}. We include the same baselines as the experiments of the main text, comparing EPISODE to FedAvg, SCAFFOLD, and CELGC.\n\nE.4.2 RESULTS\n\nTraining loss and testing accuracy during training are shown below in Figure 4. In both settings, EPISODE is superior in terms of testing accuracy and nearly the best in terms of training loss.\n\nE.5\n\nIMAGENET\n\nThe training curves (training and testing loss) for each ImageNet setting are shown below in Figure 5.\n\nF RUNNING TIME RESULTS\n\nTo demonstrate the utility of EPISODE for federated learning in practical settings, we also provide a comparison of the running time of each algorithm on the SNLI dataset. Our experiments were run on eight NVIDIA Tesla V100 GPUs distributed on two machines. The training loss and testing accuracy of each algorithm (under the settings described above) are plotted against running time below. Note that these are the same results as shown in Figure 1, plotted against time instead of epochs or communication rounds.\n\nOn the SNLI dataset, EPISODE reaches a lower training loss and higher testing accuracy with respect to time, compared with CELGC and NaiveParallelClip. Table 2 shows that, when I ≤ 8, EPISODE requires significantly less running time to reach high testing accuracy compared with both CELGC and NaiveParallelClip. When I = 16, CELGC and NaiveParallelClip nearly match, indicating that I = 16 may be close to the theoretical upper bound on I for which fast convergence can be guaranteed. Also, as the client data similarity decreases, the running time requirement of EPISODE\n\n27\n\nPublished as a conference paper at ICLR 2023\n\n(a) H = 1\n\n(b) H = 2\n\n(c) H = 4\n\n(d) H = 8\n\nFigure 3: The loss trajectories and converged solutions of CELGC and EPISODE on synthetic task.\n\nto reach high test accuracy stays nearly constant (e.g., when I = 4), while the running time required by CELGC steadily increases. This demonstrates the resilience of EPISODE’s convergence speed to heterogeneity. Training curves for the same experiment are shown in Figure 6.\n\n28\n\n0102030Round-9-8-7-6-5-4-3-2-1Objective valueCELGCEPISODE1234x-20020406080100Objective value0102030Round-12-10-8-6-4-2Objective value1234x-40-20020406080100Objective value0102030Round-18-16-14-12-10-8-6-4-2Objective value1234x-100-50050100150Objective value0102030Round-35-30-25-20-15-10-5Objective value1234x-200-150-100-50050100150200Objective valuePublished as a conference paper at ICLR 2023\n\n(a) I = 8, s = 50%\n\n(b) I = 8, s = 70%\n\nFigure 4: Training curves for CIFAR-10 experiments.\n\nInterval\n\nSimilarity Algorithm\n\n70% 75% 80%\n\n1\n\n2\n\n4\n\n8\n\n16\n\n4\n\n4\n\n100%\n\n30%\n\n30%\n\n30%\n\n30%\n\n50%\n\n10%\n\nNaiveParallelClip\n\n37.30\n\n59.69\n\n118.45\n\nCELGC EPISODE\n\nCELGC EPISODE\n\nCELGC EPISODE\n\nCELGC EPISODE\n\nCELGC EPISODE\n\nCELGC EPISODE\n\n33.57 27.20\n\n23.84 18.34\n\n20.37 13.98\n\n16.57 21.26\n\n18.52 18.37\n\n63.98 N/A 38.07\n\n70.60\n\n42.51 N/A 25.73\n\n55.15\n\n34.06 N/A 22.43\n\n53.43\n\n27.00 N/A 28.39 N/A\n\n31.86 N/A 25.71\n\n47.76\n\n39.75 N/A 18.46\n\n29.71\n\nN/A 55.92\n\nTable 2: Running time (in minutes) for each algorithm to reach test accuracy of 70%, 75%, and 80% on SNLI dataset. We use N/A to denote when an algorithm did not reach the corresponding level of accuracy over the course of training.\n\nG ABLATION STUDY\n\nIn this section, we introduce an ablation study which disentangles the role of the two components of EPISODE’s algorithm design: periodic resampled corrections and episodic clipping. Using the SNLI dataset, we have evaluated several variants of the EPISODE algorithm constructed by removing one algorithmic component at a time, and we compare the performance against EPISODE along with variants of the baselines mentioned in the paper. Our ablation study shows that both components of\n\n29\n\n020406080100120140Epochs0.51.01.52.02.5Train lossFedAvgCELGCSCAFFOLDEPISODE1351401450.20.4020406080100120140Epochs0.30.40.50.60.70.80.9Test accuracyFedAvgCELGCSCAFFOLDEPISODE1351401450.850.90020406080100120140Epochs0.51.01.52.02.5Train lossFedAvgCELGCSCAFFOLDEPISODE1351401450.20.4020406080100120140Epochs0.10.20.30.40.50.60.70.80.9Test accuracyFedAvgCELGCSCAFFOLDEPISODE1351401450.850.90Published as a conference paper at ICLR 2023\n\n(a) I = 64, s = 70%\n\n(b) I = 64, s = 60%\n\n(c) I = 64, s = 50%\n\n(d) I = 128, s = 60%\n\nFigure 5: Training curves for all ImageNet experiments.\n\n30\n\n020406080Epoch1.01.52.02.53.0Train Loss020406080Epoch0.400.450.500.550.600.650.700.750.80Test AccuracyFedAvgCELGCSCAFFOLDEPISODE86890.951.0086890.740.75020406080Epoch1.01.52.02.53.0Train Loss020406080Epoch0.400.450.500.550.600.650.700.750.80Test AccuracyFedAvgCELGCSCAFFOLDEPISODE86890.900.951.0086890.740.75020406080Epoch1.01.52.02.53.0Train Loss020406080Epoch0.400.450.500.550.600.650.700.750.80Test AccuracyFedAvgCELGCSCAFFOLDEPISODE86890.900.951.0086890.740.75020406080Epoch1.01.52.02.53.0Train Loss020406080Epoch0.400.450.500.550.600.650.700.750.80Test AccuracyFedAvgCELGCSCAFFOLDEPISODE86891.0251.0501.07586890.740.75Published as a conference paper at ICLR 2023\n\n(a) Effect of I\n\n(b) Effect of κ\n\nFigure 6: Training loss and testing accuracy on SNLI against running time. (a) Various values of communication intervals I ∈ {2, 4, 8, 16} with fixed data similarity s = 30%. (b) Various values of data similarity s ∈ {10%, 30%, 50%} with fixed I = 4.\n\nEPISODE’s algorithm design (periodically resampled corrections and episodic clipping) contribute to the improved performance over previous work.\n\nOur ablation experiments follow the same setting as the SNLI experiments in the main text. The network architecture, hyperparameters, and dataset are all identical to the SNLI experiments described in the main text. In this ablation study, we additionally evaluate multiple variants of EPISODE and baselines, which are described below:\n\n• SCAFFOLD (clipped): The SCAFFOLD algorithm (Karimireddy et al., 2020) with gradient clipping applied at each iteration. This algorithm, as a variant of CELGC, determines the gradient clipping operation based on the corrected gradient at every iteration on each machine.\n\n• EPISODE (unclipped): The EPISODE algorithm with clipping operation removed.\n\n• FedAvg: The FedAvg algorithm (McMahan et al., 2017a). We include this to show that\n\nclipping in some form is crucial for optimization in the relaxed smoothness setting.\n\n• SCAFFOLD: The SCAFFOLD algorithm (Karimireddy et al., 2020). We include this to show that SCAFFOLD-style corrections are not sufficient for optimization in the relaxed smoothness setting.\n\nWe compare these four algorithm variations against the algorithms discussed in the main text, which include EPISODE, CELGC, and NaiveParallelClip.\n\n31\n\n050100150Minutes0.300.350.400.450.500.550.600.650.70Train loss050100150Minutes0.700.720.740.760.780.800.820.84Test accuracyCELGC, I = 2CELGC, I = 4CELGC, I = 8CELGC, I = 16EPISODE, I = 2EPISODE, I = 4EPISODE, I = 8EPISODE, I = 16Naive Parallel Clip050100150Minutes0.300.350.400.450.500.550.600.650.70Train loss050100150Minutes0.700.720.740.760.780.800.820.84Test accuracyCELGC, s = 50%CELGC, s = 30%CELGC, s = 10%EPISODE, s = 50%EPISODE, s = 30%EPISODE, s = 10%Naive Parallel ClipPublished as a conference paper at ICLR 2023\n\nInterval\n\nSimilarity Algorithm\n\nTrain Loss Test Acc.\n\n1\n\n2\n\n4\n\n8\n\n100%\n\n30%\n\n30%\n\n30%\n\n16\n\n30%\n\n4\n\n4\n\n50%\n\n10%\n\nNaiveParallelClip\n\nCELGC EPISODE SCAFFOLD (clipped) EPISODE (unclipped) FedAvg SCAFFOLD\n\nCELGC EPISODE SCAFFOLD (clipped) EPISODE (unclipped) FedAvg SCAFFOLD\n\nCELGC EPISODE SCAFFOLD (clipped) EPISODE (unclipped) FedAvg SCAFFOLD\n\nCELGC EPISODE SCAFFOLD (clipped) EPISODE (unclipped) FedAvg SCAFFOLD\n\nCELGC EPISODE SCAFFOLD (clipped) EPISODE (unclipped) FedAvg SCAFFOLD\n\nCELGC EPISODE SCAFFOLD (clipped) EPISODE (unclipped) FedAvg SCAFFOLD\n\n0.357\n\n0.579 0.361 0.445 4.51 1.56 1.23\n\n0.564 0.399 0.440 9.82 1.14 4.39\n\n0.539 0.431 0.512 8.02 1.25 10.86\n\n0.525 0.534 0.597 4.71 3.45 4.87\n\n0.490 0.385 0.436 9.08 4.81 2.40\n\n0.667 0.404 0.438 8.54 1.89 5.61\n\n82.4%\n\n75.9% 82.3% 80.5% 33.3% 32.8% 34.1%\n\n77.2% 81.7% 80.7% 33.0% 32.8% 32.8%\n\n78.0% 81.1% 77.1% 34.3% 32.7% 32.8%\n\n78.3% 77.8% 75.7% 33.0% 32.7% 32.7%\n\n79.1% 82.1% 80.7% 34.3% 32.8% 32.9%\n\n73.3% 81.5% 80.7% 33.0% 34.3% 34.3%\n\nTable 3: Results for ablation study of EPISODE on SNLI dataset.\n\nFollowing the protocol outlined in the main text, we train each one of these algorithms while varying the communication interval I and the client data similarity parameter s. Specifically, we evaluate six settings formed by first fixing s = 30% and varying I ∈ {2, 4, 8, 16}, then fixing I = 4 and varying s ∈ {10%, 30%, 50%}. Note that the results of NaiveParallelClip are unaffected by I and s, since NaiveParallelClip communicates at every iteration. For each of these six settings, we provide the training loss and testing accuracy reached by each algorithm at the end of training. Final results for all settings are given in Table 3, and training curves for the setting I = 4, s = 30% are shown in Figure 7.\n\nFrom these results, we can conclude that both components of EPISODE (periodic resampled corrections and episodic clipping) contribute to EPISODE’s improved performance.\n\n• Replacing periodic resampled corrections with SCAFFOLD-style corrections yields the variant SCAFFOLD (clipped). In every setting, SCAFFOLD (clipped) performs slightly\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Training curves SNLI ablation study under the setting I = 4 and s = 30%. Note that the training losses of EPISODE (unclipped), FedAvg, and SCAFFOLD are not visible, since they are orders of magnitude larger than the other algorithms.\n\nbetter than CELGC, but still worse than EPISODE. This corroborates the intuition that SCAFFOLD-style corrections use slightly outdated information compared to that of EPISODE, and this information lag caused worse performance in this ablation study.\n\n• On the other hand, clipping is essential for EPISODE to avoid divergence. By removing clipping from EPISODE, we obtain the variant EPISODE (unclipped), which fails to learn entirely. EPISODE (unclipped) never reached a test accuracy higher than 35%, which is barely higher than random guessing, since SNLI is a 3-way classification problem. In summary, both periodic resampled corrections and episodic clipping contribute to the improved performance of EPISODE over baselines.\n\nIn addition, FedAvg and SCAFFOLD show similar divergence behavior as EPISODE (unclipped). None of these three algorithms employ any clipping or normalization in updates, and consequently none of these algorithms are able to surpass random performance on SNLI. Finally, although NaiveParallelClip appears to be the best performing algorithm from this table, it requires more wall-clock time than any other algorithms due to its frequent communication. For a comparison of the running time results, see Table 2 in Appendix F.\n\nH NEW EXPERIMENTS ON FEDERATED LEARNING BENCHMARK:\n\nSENTIMENT140 DATASET\n\nTo evaluate EPISODE on a real-world federated dataset, we provide additional experiments on the Sentiment140 benchmark from the LEAF benchmark (Caldas et al., 2018). Sentiment140 is a sentiment classification problem on a dataset of tweets, where each tweet is labeled as positive or negative. For this setting, we follow the experimental setup of Li et al. (2020b): training a 2-layer LSTM network with 256 hidden units on the cross-entropy classification loss. We also follow their data preprocessing steps to eliminate users with a small number of data points and split into training and testing sets. We perform an additional step to simulate the cross-silo federated environment (Kairouz et al., 2019) by partitioning the original Sentiment140 users into eight groups (i.e., eight machines). To simulate heterogeneity between silos, we partition the users based on a non-i.i.d. sampling scheme similar to that of our SNLI experiments. Specifically, given a silo similarity parameter s, each silo is allocated s% of its users by uniform sampling, and (100 − s)% of its users from a pool of users which are sorted by the proportion of positive tweets in their local dataset. This way, when s is small, different silos will have a very different proportion of positive/negative samples in their respective datasets. We evaluate NaiveParallelClip, CELGC, and EPISODE in this cross-silo environment with I = 4 and s ∈ {0, 10, 20}. We tuned the learning rate η, and the clipping parameter γ with grid search over the values η ∈ {0.01, 0.03, 0.1, 0.3, 1.0} and γ ∈ {0.01, 0.03, 0.1, 0.3, 1.0}. Results are plotted in Figures 8 and 9.\n\n33\n\n0510152025Epochs0.30.40.50.60.70.80.91.0Train lossNaiveParallelClipCELGCEPISODESCAFFOLD clipped0510152025Epochs0.30.40.50.60.70.8Test accuracyNaiveParallelClipCELGCEPISODESCAFFOLD clippedEPISODE unclippedFedAvgSCAFFOLDPublished as a conference paper at ICLR 2023\n\n(a) I = 4, s = 20%\n\n(b) I = 4, s = 10%\n\n(c) I = 4, s = 0%\n\nFigure 8: Training curves for all Sentiment140 experiments over training steps.\n\n(a) I = 4, s = 20%\n\n(b) I = 4, s = 10%\n\n(c) I = 4, s = 0%\n\nFigure 9: Training curves for all Sentiment140 experiments over running time.\n\nOverall, EPISODE is able to nearly match the training loss and testing accuracy of NaiveParallelClip while requiring significantly less running time, and the performance of EPISODE does not degrade as the client data similarity s decreases. Figure 8 shows that, with respect to the number of training steps, EPISODE remains competitive with NaiveParallelClip and outperforms CELGC. In particular, the gap between EPISODE and CELGC grows as the client data similarity decreases, showing that EPISODE can adapt to data heterogeneity. On the other hand, Figure 9 shows that, with a fixed time budget, EPISODE is able to reach lower training loss and higher testing accuracy than both CELGC\n\n34\n\n0200040006000Training Steps0.500.550.600.650.700.75Train lossCELGCEPISODENaive Parallel Clip0200040006000Training Steps0.620.640.660.680.700.720.740.760.78Test accuracy0200040006000Training Steps0.500.550.600.650.700.75Train loss0200040006000Training Steps0.620.640.660.680.700.720.740.760.78Test accuracy0200040006000Training Steps0.500.550.600.650.700.75Train loss0200040006000Training Steps0.620.640.660.680.700.720.740.760.78Test accuracy100200300400500Time0.500.550.600.650.700.75Train lossCELGCEPISODENaive Parallel Clip100200300400500Time0.620.640.660.680.700.720.740.760.78Test accuracy100200300400500Time0.500.550.600.650.700.75Train loss100200300400500Time0.620.640.660.680.700.720.740.760.78Test accuracy100200300400500Time0.500.550.600.650.700.75Train loss100200300400500Time0.620.640.660.680.700.720.740.760.78Test accuracyPublished as a conference paper at ICLR 2023\n\nand NaiveParallelClip in all settings. This demonstrates the superior performance of EPISODE in practical scenarios.\n\n35",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a communication-efficient distributed gradient clipping algorithm for federated learning, which is called EPISODE. The algorithm works particularly well with the heterogenous data and under the nonconvex and relaxed smoothness setting. The novelty consists in two techniques: episodic gradient clipping and periodic resampled corrections. Another contribution of this paper is to provide a convergence proof for the proposed algorithm under several assumptions. Empirical studies show that EPISODE outperforms other federated learning algorithms on the both synthetic and real datasets, which include SNLI and ImageNet.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. This work seems to be the first to study the federated learning under heterogeneous data setting with relaxed smoothness assumption, and provide a provable algorithm to solve the problem.\n2. The paper is well motivated by giving examples why existing algorithms (CELGC and SCAFFOLD) do not work in the setting.\n3. The theoretical results come with well sounded implications, making the results convincing (theoretically).\n\nWeaknesses:\n\n1. I am not sure if the given setting is realistic. Like many other distributed optimization papers, this paper put quite a few constraints to the scenarios, such as heterogeneity of the data. But in reality, do we really need to solve such a problem?\n2.  While the paper proposes a distributed algorithm, which implies the problem scale is large enough. However, in the experiments, the data/model scale is not that big that a single machine can easily hold them. It is like motivating the paper by solving a very difficult problem but it turns out the problem is not that challenging.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1. The paper is well written and the quality is good. \n\n2. The paper is novel in that it proposes two new techniques to solve the distributed optimization with heterogenous data. And the algorithm has theoretical convergence guarantee.\n\n3. Given that the algorithm is intrinsically simple, it won't be difficult to reproduce the results.\n\n# Summary Of The Review\n\nThe major contribution of this paper is to propose an algorithm to solve the distributed optimization with heterogenous data. Although the setting might not be realistic, the theoretical contribution should still be appreciated.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel algorithm designed for Federated Learning (FL) with heterogeneous data, particularly in nonconvex and relaxed smoothness settings. Key contributions include the implementation of episodic gradient clipping and periodic resampled corrections, which allow for enhanced optimization of nonconvex and (L0, L1)-smooth functions. The authors provide theoretical guarantees that demonstrate a linear speedup in the number of clients and a reduction in communication rounds. Experimental results reveal that EPISODE outperforms several strong baselines, indicating its effectiveness in various applications involving heterogeneous datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the challenges of gradient clipping in FL settings with heterogeneous data, filling a significant gap in the existing literature. The theoretical contributions are well-founded, providing solid guarantees on convergence and communication complexities. However, the paper could benefit from a more comprehensive discussion on the limitations of EPISODE, such as the potential trade-offs between communication efficiency and model accuracy in practical scenarios. Additionally, the empirical evaluations, while robust, could be enhanced by including more diverse datasets to further validate the algorithm's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the problem setup, algorithm development, and experimental results. The quality of the theoretical analysis is commendable, and the authors have successfully articulated the significance of their contributions. The reproducibility aspect is bolstered by the availability of the code on GitHub, allowing other researchers to validate the findings. However, a more detailed description of experimental setups and hyperparameter tuning would further improve reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in Federated Learning by introducing the EPISODE algorithm, which effectively addresses gradient clipping challenges in heterogeneous data settings. The theoretical and empirical results support the efficacy of EPISODE, although there is room for further exploration of its limitations and broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel gradient clipping algorithm specifically designed for Federated Learning (FL) with heterogeneous data. Key contributions include the introduction of Episodic Gradient Clipping, which utilizes the global average gradient sampled at the start of each round, and Periodic Resampled Corrections that adjust client updates based on resampled gradients to tackle data heterogeneity. The authors demonstrate that EPISODE significantly outperforms existing methods in terms of training loss and test accuracy across various heterogeneous datasets while requiring fewer communication rounds, thus enhancing communication efficiency.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to addressing gradient clipping in FL settings characterized by heterogeneous data, which has been a gap in existing literature. The algorithm's flexibility in not requiring strong distributional assumptions on stochastic gradient noise broadens its applicability in practice. Empirically, EPISODE shows robust performance across different datasets and experimental conditions, confirming its effectiveness. However, the paper also presents limitations: the algorithm appears tailored primarily for nonconvex functions, which may restrict its broader applicability. Additionally, the dependence on specific communication structures and the complexity of implementation could pose challenges for practitioners.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and experimental results. The quality of the writing is high, facilitating comprehension of the algorithm's mechanics and its advantages. The novelty lies in the integration of episodic gradient clipping with periodic resampled corrections in the context of FL, a relatively unexplored domain. Reproducibility is supported through detailed descriptions of experimental setups, hyperparameter tuning, and comparisons with baseline methods, although the complexity of implementation may hinder some users.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of Federated Learning by introducing EPISODE, an innovative gradient clipping method tailored for heterogeneous data. While the empirical results are compelling and demonstrate clear improvements over existing methods, the algorithm's specific focus on nonconvex functions and implementation complexities may limit its broader usability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel algorithm designed for Federated Learning (FL) that effectively addresses the challenges posed by heterogeneous data and nonconvex optimization scenarios. The main contributions include the development of an Episodic Gradient Clipping technique, which uses a globally averaged gradient for clipping decisions, and a method for Periodic Resampled Corrections that refines local updates using resampled gradients at the beginning of each communication round. The experimental results demonstrate that EPISODE achieves a linear speedup in training and requires fewer communication rounds compared to existing methods such as FedAvg and SCAFFOLD.\n\n# Strength And Weaknesses\nOne of the main strengths of this paper is its focus on the limitations of existing FL algorithms under heterogeneous data distributions, which is a significant and timely challenge in the field. The proposed methods, particularly the episodic gradient clipping and resampled corrections, are well-motivated and theoretically sound, as evidenced by the performance improvements shown in various datasets. However, a potential weakness is that while the theoretical analysis is solid, the paper could benefit from a more detailed discussion of the practical implications of the assumptions made, particularly regarding the bounded noise in stochastic gradients. Additionally, the experiments, while comprehensive, primarily focus on typical benchmarks; exploring more diverse real-world scenarios could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The clarity of the writing aids in understanding the proposed algorithm and its significance. The algorithm is reproducible, as the authors provide sufficient details about the implementation and experimental setup, including the use of PyTorch and specific architectural choices. The novelty of the EPISODE algorithm lies in its tailored approach to gradient clipping in FL settings, particularly under relaxed smoothness conditions, which is a fresh perspective in the domain.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in Federated Learning by addressing the issues of heterogeneous data through the introduction of the EPISODE algorithm. The contributions are theoretically sound and empirically validated, making a strong case for its adoption in real-world applications where data privacy is paramount.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel algorithm designed specifically for optimizing nonconvex and (L0, L1)-smooth functions in the context of federated learning (FL) with heterogeneous data. The authors present a unified framework that effectively operates on both homogeneous and heterogeneous datasets, demonstrating flexibility in application. EPISODE is theoretically supported by guarantees of linear speedup in client numbers and reduced communication rounds, while extensive empirical evaluations show its superiority over established baselines like FedAvg, SCAFFOLD, and CELGC across various tasks, including text and image classification.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include the novelty of the EPISODE algorithm, which fills a significant gap in the literature concerning nonconvex optimization in federated learning. The unified framework enhances its applicability, and the theoretical guarantees lend credibility to the proposed method. Furthermore, the empirical results indicate robust performance, particularly in maintaining effectiveness across varying levels of data heterogeneity. However, weaknesses include the need for validation across a broader range of datasets to confirm the generalizability of the algorithm, potential limitations arising from assumptions made in the theoretical analysis, and the necessity for more comprehensive documentation of the provided code.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology and findings clearly. The quality of the theoretical analysis is high, but the reliance on certain assumptions requires caution in practical applications. The novelty of EPISODE is a standout feature, as it addresses a previously overlooked area in federated learning. The availability of the code on GitHub enhances reproducibility, although the lack of thorough documentation may pose challenges for some users seeking to replicate the results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to federated learning by introducing the EPISODE algorithm, which effectively addresses optimization challenges in heterogeneous settings. While the theoretical and empirical results are compelling, further validation and improved documentation are needed to enhance the algorithm's applicability and ease of use.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data\" presents a novel algorithm designed to enhance gradient clipping techniques within Federated Learning (FL). The proposed method, EPISODE, introduces two key innovations: episodic gradient clipping and periodic resampled corrections, which are specifically tailored to address the challenges posed by heterogeneous data across clients. The authors provide strong theoretical guarantees demonstrating that EPISODE achieves linear speedup with respect to the number of clients and reduces the required communication rounds compared to existing methods. Extensive empirical validation shows that EPISODE consistently outperforms several strong baseline methods across various datasets, confirming the effectiveness of the theoretical claims.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative algorithm design that effectively tackles the issues of nonconvex optimization in federated settings, especially under data heterogeneity. The theoretical contributions are robust, providing valuable insights into the performance of EPISODE compared to traditional approaches. The extensive empirical results add credibility to the proposed method, showcasing its superiority in communication efficiency and model accuracy. However, a potential weakness lies in the need for clearer explanations regarding the assumptions made about stochastic gradient noise, as this could impact the practical applicability of the method in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates both the theoretical and empirical aspects of the proposed method. The quality of writing is high, making complex ideas accessible. The novelty of the approach is significant, as it introduces a new methodology for gradient clipping in federated learning that differs from traditional techniques. The reproducibility of the results is supported by comprehensive experimental details; however, further elaboration on the assumptions and their implications would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of federated learning by presenting a novel algorithm that addresses critical challenges associated with heterogeneous data. The proposed method is well-supported by both theoretical guarantees and empirical evidence, positioning it as a promising advancement in the area. The paper is recommended for acceptance with minor clarifications regarding assumptions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data\" introduces a novel strategy for adversarial training in deep learning models. The primary contributions include the implementation of episodic gradient clipping and periodic resampled corrections, which aim to enhance the robustness of models against adversarial attacks, especially in the context of non-IID data distributions. The methodology involves dynamically adjusting the clipping threshold based on global indicators and sampling gradients from diverse training examples to stabilize model updates. Empirical results demonstrate that EPISODE significantly improves training speed and robustness, outperforming conventional adversarial training methods across multiple datasets characterized by high adversarial noise.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to mitigating the instability of gradients during adversarial training, particularly through the introduction of episodic gradient clipping and periodic resampling. These strategies effectively enhance model robustness, as evidenced by empirical validation across various datasets showing lower error rates on adversarial examples. However, a notable weakness is the potential need for fine-tuning of hyperparameters related to clipping and resampling, which may hinder its applicability in large-scale scenarios. Additionally, the computational overhead associated with the resampling process could pose challenges in resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to a broad audience. The quality of the writing is high, with a logical flow that guides the reader through the proposed methods and their implications. The novelty is significant, as it presents a fresh perspective on adversarial training that incorporates episodic strategies previously unexplored in this domain. Reproducibility is supported by comprehensive empirical validation, although the paper could benefit from clearer descriptions of experimental setups and parameter choices to facilitate replication.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of adversarial training by introducing innovative techniques that enhance robustness against adversarial attacks. While the methodology is promising, attention must be given to the practical implications of parameter tuning and computational demands in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents EPISODE, a novel algorithm designed to enhance gradient clipping in federated learning (FL) under conditions of data heterogeneity, particularly for nonconvex and (L0, L1)-smooth functions. The authors introduce two key techniques—episodic gradient clipping and periodic resampled corrections—which they claim lead to significant improvements in optimization efficiency. Experimental results indicate that EPISODE outperforms established baselines across various datasets, achieving linear speedup with the number of clients and reducing communication rounds. However, the authors' assertions regarding the revolutionary nature of these contributions and the extent of performance improvements may be overstated.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to tackle the challenges posed by heterogeneous data in federated learning, an area that remains underexplored. The proposed techniques for gradient clipping are innovative, and the experimental validation demonstrates a consistent performance edge over traditional methods. However, several weaknesses are evident: the claims regarding the novelty and efficiency of EPISODE seem exaggerated, as similar concepts exist in the optimization literature, and the experimental results may not reflect a substantial advancement in practice. Additionally, the paper lacks a thorough discussion of the limitations inherent in the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that delineates the contributions and methodology. However, the novelty of the proposed techniques is somewhat diluted by the lack of acknowledgment of prior art. While the experiments are conducted methodically, the reproducibility could be enhanced with more detailed descriptions of the experimental setups and hyperparameter choices. Overall, the clarity and quality of the writing are commendable, but the novelty claims require more substantiation.\n\n# Summary Of The Review\nEPISODE presents interesting contributions to the field of federated learning, particularly in addressing the challenges of heterogeneous data. While the proposed techniques are innovative and the experimental validation is robust, the claims of revolutionary advancements and efficiency gains appear overstated. The paper would benefit from a more cautious interpretation of its findings and a deeper exploration of the limitations of the proposed framework.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel gradient clipping algorithm tailored for federated learning (FL) with heterogeneous data. It employs episodic gradient clipping and periodic resampled corrections to address challenges posed by nonconvex optimization and relaxed smoothness conditions. The authors claim that EPISODE is the first algorithm to effectively manage FL challenges without strong distributional assumptions on stochastic gradient noise, achieving improved communication efficiency and a linear speedup concerning the number of clients. Theoretical results suggest a significant reduction in communication complexity, while experimental results show a comparative performance against existing methods, albeit with some discrepancies in reported outcomes.\n\n# Strength And Weaknesses\nThe primary strength of EPISODE lies in its innovative approach to gradient clipping in federated learning, particularly in heterogeneous data settings. The theoretical contribution regarding communication complexity is noteworthy, as it provides a framework for understanding the algorithm's efficiency. However, several weaknesses are evident, particularly in the empirical results. Adjustments to the experimental findings reveal that EPISODE does not consistently outperform existing algorithms like CELGC and FedAvg, with performance metrics indicating less robustness than initially claimed. This raises concerns about the reliability of the empirical evaluations presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a coherent presentation of methodologies and theoretical results. However, the adjusted experimental results could lead to confusion about the algorithm's actual performance. While the novelty of the algorithm is significant, the reproducibility of results may be compromised due to the discrepancies noted between original and modified findings. The paper would benefit from clearer explanations regarding the experimental setup and the reasons behind the adjustments in reported results.\n\n# Summary Of The Review\nOverall, EPISODE presents a promising advancement in federated learning, particularly in handling heterogeneous data. However, the discrepancies in empirical results raise questions about the robustness and reliability of the claims made. Future research should focus on validating these findings across various scenarios to establish the algorithm's efficacy.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel federated learning framework called EPISODE, which aims to address the limitations of existing methods under heterogeneous data distributions. The authors propose a relaxed smoothness condition (L0, L1-smoothness) to replace the traditional L-smoothness assumption, arguing that this leads to improved convergence guarantees. Additionally, EPISODE employs episodic gradient clipping based on global indicators to enhance communication efficiency and robustness to noisy gradients. The experimental results demonstrate that EPISODE achieves reduced communication rounds and linear speedup with the number of clients, although the generalizability of these findings remains questionable.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to relaxed smoothness conditions, which could potentially broaden the applicability of federated learning algorithms to a wider range of neural network architectures. Moreover, the promise of improved communication efficiency is a significant contribution in the context of federated learning. However, the paper exhibits several weaknesses. The assumptions about data heterogeneity and the efficacy of the proposed methods in varying conditions are insufficiently supported by empirical evidence. Additionally, the reliance on global stochastic gradients raises concerns about the algorithm's robustness in highly heterogeneous environments. Furthermore, the paper does not sufficiently explore the limitations of its baseline comparisons or the implications of its ablation studies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, but it lacks depth in discussing the implications of its assumptions and results. While the technical approach is novel, the claims made regarding the generalizability of the results are not adequately substantiated. Reproducibility may be a concern, as the experimental results are based on specific datasets and settings, leaving open questions about performance in other contexts.\n\n# Summary Of The Review\nOverall, the paper presents a promising new framework for federated learning that seeks to improve upon existing models by relaxing certain assumptions. However, the lack of empirical support for some key claims and the questions raised about the robustness of the approach under various conditions diminish the overall impact of the work.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel algorithm designed for gradient clipping in federated learning (FL) contexts with heterogeneous data. The methodology involves episodic gradient clipping and periodic resampled corrections, which together enhance communication efficiency and convergence rates without imposing stringent distributional assumptions. The findings demonstrate that EPISODE achieves linear speedup in relation to the number of clients, necessitating fewer communication rounds while outperforming established methods like FedAvg and SCAFFOLD across various datasets and neural network architectures.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to addressing the challenges of gradient clipping in heterogeneous FL settings, providing theoretical guarantees that enhance its appeal. The experimental results substantiate the claims of EPISODE's superior performance, showcasing its practical applicability. However, one weakness is that while the paper discusses the theoretical aspects extensively, it could benefit from a deeper exploration of the limitations and potential drawbacks of the proposed method, particularly in scenarios with extreme data heterogeneity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings, making it accessible to readers familiar with the field. The quality of the writing is high, and the theoretical analysis is robust. The novelty of the algorithm is significant, as it addresses a gap in existing literature regarding federated learning with heterogeneous data. The reproducibility of the results is supported by comprehensive empirical evaluations, though sharing code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of federated learning through its development of EPISODE, which effectively tackles the challenges posed by data heterogeneity. The theoretical contributions and empirical results are compelling, though further discussion on limitations would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework designed to enhance the scalability and efficiency of deep learning models in resource-constrained environments. The authors propose a hybrid approach that combines knowledge distillation and model pruning to reduce model size while maintaining accuracy. Through extensive experiments on benchmark datasets, the paper demonstrates that the proposed method not only achieves competitive performance compared to state-of-the-art techniques but also significantly reduces computational overhead.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach**: The combination of knowledge distillation and model pruning offers a unique method for optimizing deep learning models, contributing valuable insights to the field.\n2. **Robust Empirical Results**: The experimental evaluation includes a variety of datasets and tasks, showcasing the method's effectiveness and versatility across different scenarios.\n3. **Theoretical Insights**: The paper provides theoretical explanations for the observed improvements, enhancing the understanding of the underlying mechanisms at play.\n\n**Weaknesses:**\n1. **Limited Generalizability**: The proposed framework may not generalize well to all types of models, particularly those that do not lend themselves easily to pruning.\n2. **Narrow Range of Comparisons**: While the paper includes comparisons with a few existing methods, a broader set of baselines would strengthen the claims about performance improvements.\n3. **Assumption Dependencies**: The framework's performance relies on specific assumptions that could limit its applicability in diverse real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with varying levels of expertise. The methodology is detailed, enabling reproducibility of the experiments. The novelty of the proposed method is noteworthy, though the restricted comparative analysis and reliance on specific assumptions may affect its perceived significance.\n\n# Summary Of The Review\nThis paper presents a promising approach to model optimization that balances efficiency and accuracy through a hybrid framework of knowledge distillation and model pruning. While the contributions are significant, the generalizability and robustness of the findings warrant further exploration and broader validation against a more extensive set of benchmarks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents EPISODE, a novel algorithm designed for gradient clipping in Federated Learning (FL) settings with heterogeneous data. It introduces two key techniques: episodic gradient clipping and periodic resampled corrections, enabling effective handling of nonconvex and relaxed smoothness loss functions. The methodology demonstrates a unified framework applicable to both homogeneous and heterogeneous data, achieving linear speedup and reduced communication rounds. Empirical evaluations indicate that EPISODE outperforms existing gradient clipping methods across various datasets, highlighting its robustness and efficiency.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to gradient clipping specifically tailored for the challenges of heterogeneous data in FL, filling a significant gap in existing research. The proposed algorithm is theoretically sound and practically efficient, as evidenced by the experimental results demonstrating improved performance. However, one potential weakness is the lack of extensive theoretical analysis regarding the conditions under which EPISODE optimally performs, which may limit the understanding of its applicability across different scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and contributions. The writing is concise, making complex concepts accessible. The quality of the experiments supports the claims made, showcasing the algorithm's effectiveness. Nonetheless, the reproducibility of results could be enhanced by providing more details on experimental setups, including hyperparameters and specific datasets used in the evaluations.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of Federated Learning by addressing the challenges of gradient clipping in heterogeneous data scenarios. The proposed EPISODE algorithm is both innovative and effective, though further theoretical exploration and detailed experimental protocols could enhance its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces **EPISODE**, a novel gradient clipping algorithm designed for federated learning (FL) in settings characterized by heterogeneous data. The authors identify limitations of existing algorithms, which predominantly focus on homogeneous data environments, and propose two key techniques: episodic gradient clipping and periodic resampled corrections. Through rigorous theoretical analysis and comprehensive empirical evaluations across multiple datasets, the authors demonstrate that EPISODE achieves linear speedup with respect to the number of machines and requires fewer communication rounds compared to various baseline methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the challenges posed by data heterogeneity in federated learning, making significant contributions to both algorithm development and theoretical understanding. The proposed EPISODE algorithm is well-structured, with clear explanations of its mechanics and theoretical guarantees for convergence. However, a potential weakness is that the experimental validation, while robust, could benefit from additional datasets and real-world scenarios to further establish generalizability. Furthermore, the paper could provide more detailed discussions on the limitations and specific assumptions made about the nature of the data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear exposition of the problem setup and methodology. The organization flows logically, facilitating reader comprehension. The novelty of the approach is significant, as it fills a gap in the literature on gradient clipping in federated learning contexts. The reproducibility of the results is supported by the detailed description of the algorithm and experimental setup, though access to code or supplementary materials would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning by addressing the critical issue of data heterogeneity with a novel algorithm, EPISODE. The theoretical and empirical results are compelling, although further validation across diverse real-world scenarios and additional clarity on assumptions would strengthen the paper.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data\" presents a novel gradient clipping algorithm, EPISODE, specifically designed for Federated Learning (FL) scenarios involving heterogeneous data. The authors introduce two key techniques: episodic gradient clipping and periodic resampled corrections, to enhance performance in nonconvex and relaxed smoothness functions. The experimental results demonstrate that EPISODE outperforms existing methods, such as FedAvg, SCAFFOLD, and CELGC, achieving faster convergence with fewer communication rounds while maintaining competitive accuracy.\n\n# Strength And Weaknesses\nThe key strengths of the paper lie in its innovative approach to addressing gradient clipping in FL settings, particularly in the context of heterogeneous data. The theoretical analysis provided supports the claim of linear speedup in terms of client numbers and communication efficiency. Additionally, the comprehensive experimental evaluation across diverse datasets showcases the robustness of EPISODE. However, a potential weakness is the reliance on the assumptions of relaxed smoothness, which may limit the generalizability of the findings to other types of optimization problems not covered by these assumptions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, with a logical progression from the introduction through methodology to experimental results. The figures and tables effectively convey the performance comparisons and key findings. The novelty of the proposed algorithm is significant, as it fills a gap in existing FL literature regarding gradient clipping. Furthermore, the authors have made the code publicly available, which enhances reproducibility and encourages further exploration of their approach.\n\n# Summary Of The Review\nThe paper offers a valuable contribution to the field of Federated Learning by introducing EPISODE, a novel algorithm that effectively addresses challenges posed by heterogeneous data. Its strong empirical results and theoretical analysis support its significance. Overall, the paper should be accepted with minor revisions to enhance clarity.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents EPISODE, a novel algorithm designed to tackle gradient clipping challenges within the Federated Learning (FL) framework, particularly in scenarios characterized by heterogeneous data distributions. The methodology integrates two key innovations: episodic gradient clipping and periodic resampled corrections, allowing for effective aggregation of stochastic gradients from clients. The findings demonstrate that EPISODE achieves linear speedup relative to the number of clients, while requiring fewer communication rounds and outperforming existing methods across various synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of a new algorithm that addresses the specific challenges of FL with heterogeneous data, a significant gap in the existing literature. The theoretical analysis is rigorous, providing a solid foundation for the claims made regarding convergence and performance. Moreover, the empirical results are compelling, showcasing EPISODE's effectiveness across multiple datasets. However, the paper could benefit from a more detailed discussion on the limitations of the proposed method, particularly in terms of its applicability to extreme cases of data heterogeneity or communication constraints.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem statement, methodology, and contributions. The theoretical background is presented logically, making it accessible for readers familiar with FL and gradient clipping techniques. The quality of writing is high, although some sections could be more concise. The novelty of the approach is significant, particularly as it introduces a new perspective on gradient clipping in the context of FL. Reproducibility is addressed through clear descriptions of the algorithm and experimental setups; however, additional details on hyperparameter tuning and computational resources would enhance this aspect.\n\n# Summary Of The Review\nEPISODE introduces a noteworthy advancement in Federated Learning by effectively combining episodic gradient clipping and periodic resampling to address challenges posed by heterogeneous data distributions. The algorithm demonstrates both theoretical and empirical advantages over existing approaches, although further exploration of its limitations would strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the EPISODE algorithm, which aims to optimize nonconvex and (L0, L1)-smooth functions within federated learning contexts characterized by heterogeneous data. The authors claim that this is the first algorithm of its kind, introducing concepts such as \"episodic gradient clipping\" and \"periodic resampled corrections.\" Their experimental results suggest that EPISODE outperforms baseline methods, like CELGC and Naive Parallel Clip, particularly in terms of linear speedup in communication rounds, although the validity of these claims is heavily grounded in theoretical proofs.\n\n# Strength And Weaknesses\nWhile the paper introduces a novel approach to federated learning, the perceived novelty of the EPISODE algorithm is questionable as it appears to rehash existing concepts without significant innovation. The methodology relies on complex mathematical formulations and specific assumptions regarding stochastic gradient noise that may not hold in practical settings, thus limiting its applicability. The experimental results, although favorable, are based on a limited selection of baselines, raising concerns about the robustness and generalizability of the findings. Additionally, scalability issues and insufficient exploration of data heterogeneity further weaken the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper’s clarity is hindered by the complexity of its mathematical formulations and assumptions, which may obscure the practical implementation of the EPISODE algorithm. While the quality of the writing is generally adequate, the lack of comprehensive ablation studies and limited experimental validation under diverse conditions diminishes reproducibility. The claims regarding the algorithm's novelty are overstated, as they do not convincingly differentiate EPISODE from existing methods.\n\n# Summary Of The Review\nOverall, while the EPISODE algorithm claims to be a significant advancement in federated learning, the evidence supporting its novelty and effectiveness is insufficient. The paper suffers from a lack of thorough validation, limited baseline comparisons, and scalability concerns, which ultimately undermines its contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel algorithm tailored for federated learning (FL) that addresses challenges posed by heterogeneous data. Key contributions include a new approach to gradient clipping suitable for nonconvex and relaxed smoothness settings, achieving linear speedup with increased clients, and significantly reducing communication rounds necessary for model training. The authors present state-of-the-art complexity results and demonstrate EPISODE's robust performance across various datasets in text and image classification tasks, showcasing its applicability in real-world scenarios such as healthcare and finance.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to gradient clipping and the introduction of techniques like episodic gradient clipping and periodic resampled corrections, which effectively manage data heterogeneity and enhance convergence. The algorithm's ability to minimize communication rounds is particularly noteworthy, making it efficient for large-scale applications. However, a potential weakness is the lack of extensive comparative analysis with existing state-of-the-art algorithms in diverse scenarios, which may limit the assessment of its practical advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and results, making it accessible to readers. The quality of the experimental results supports the claims made regarding performance and efficiency. The novelty of the proposed techniques is significant, marking a step forward in federated learning research. However, the reproducibility of the results could be enhanced by providing more details about the experimental setup and hyperparameter tuning.\n\n# Summary Of The Review\nEPISODE stands out as a significant advancement in federated learning, introducing unique methodologies that enhance efficiency and performance across various applications. While the algorithm shows great promise, further comparative analyses could strengthen the understanding of its advantages over existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents EPISODE, an innovative algorithm designed for federated learning that addresses the challenges posed by heterogeneous client data and limited communication rounds. Key contributions include the integration of episodic gradient clipping and periodic resampled corrections, which are theoretically justified to enhance gradient estimation reliability. The authors establish theoretical guarantees for EPISODE, demonstrating its capability to achieve linear speedup with respect to the number of clients while requiring fewer communication rounds, independent of stringent noise assumptions.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical framework that advances the understanding of gradient clipping in the context of federated learning, particularly under relaxed smoothness conditions. The proposed algorithm, EPISODE, is well-justified theoretically and compares favorably to existing methods, such as SCAFFOLD and local SGD, by avoiding reliance on specific distributional assumptions. However, a potential weakness is that while empirical results are mentioned, they are not thoroughly detailed in the paper, which may limit the reader's ability to fully evaluate the practical effectiveness of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, adequately conveying complex theoretical concepts without excessive jargon. The quality of the theoretical analysis is high, and the novelty of introducing relaxed smoothness conditions alongside the algorithmic innovations is significant. However, reproducibility could be improved by providing more extensive empirical results and detailed implementation specifics, which would enable others to replicate the findings more easily.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of federated learning by providing a robust theoretical framework for gradient clipping and introducing the EPISODE algorithm. Its innovative approach and theoretical guarantees position it as a significant advancement, although further empirical validation would strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data\" by Michael Crawshaw, Yajie Bao, and Mingrui Liu presents a novel approach to improve federated learning under heterogeneous data distributions. The authors introduce two key methodologies: Episodic Gradient Clipping, which determines the need for clipping based on resampled global averaged gradient norms at the start of each communication round, and Periodic Resampled Corrections, which utilizes resampled stochastic gradients from clients to create control variates for more efficient gradient updates. The experimental results, validated on datasets like SNLI, CIFAR-10, and ImageNet, demonstrate that EPISODE achieves linear speedup in communication and iteration complexity compared to established baselines, affirming its robustness and efficiency in federated learning scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative algorithmic contributions, particularly the combination of episodic gradient clipping and resampled corrections, which effectively addresses the challenges posed by data heterogeneity in federated learning. The thorough experimental evaluation against strong baselines like FedAvg and SCAFFOLD adds credibility to the claims of improved performance. However, a notable weakness is that the paper focuses heavily on implementation specifics and performance metrics, which may obscure the broader implications of the research and its potential applicability to real-world federated learning environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a well-structured presentation of the methodology and results. The quality of the implementation is supported by the availability of the code on GitHub, allowing for reproducibility of the experiments. However, the novelty of the approach, while significant, could be more effectively articulated in relation to existing methods and their limitations. The paper would benefit from a more comprehensive discussion of the implications of its findings for future research in federated learning.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful advancement in federated learning through the introduction of EPISODE, leveraging novel techniques to enhance communication efficiency under heterogeneous data conditions. While the empirical results are promising, the paper could provide a stronger contextualization of its contributions within the broader research landscape.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces EPISODE, a novel algorithm designed to handle heterogeneous data in federated learning (FL) settings via gradient clipping. The authors claim that EPISODE outperforms existing methods, such as FedAvg and SCAFFOLD, by achieving state-of-the-art complexity results and demonstrating resilience to data heterogeneity. The methodology relies on episodic gradient clipping and periodic resampled corrections, asserting that these innovations enable better performance in nonconvex and relaxed smooth functions.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to address a crucial gap in federated learning related to heterogeneous data, positing EPISODE as a significant advancement over existing methods. However, the paper exhibits weaknesses in its comparative analysis, often dismissing previous works without thorough justification. The lack of context in experimental comparisons raises questions about the fairness of the evaluations. Furthermore, the authors' claims of novelty and performance are not always substantiated by adequate discussions of alternative methods and their potential adaptations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured but suffers from occasional overgeneralizations and a tendency to emphasize novelty without adequate support. While the methodology is presented clearly, the justification for its advantages over existing methods lacks depth, potentially affecting reproducibility. The empirical results are compelling but would benefit from a more nuanced discussion of the limitations and conditions under which EPISODE operates effectively.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to addressing data heterogeneity in federated learning, but it falls short in providing a balanced comparison with existing methods. The claims of novelty and superiority are not sufficiently substantiated, leading to a somewhat biased portrayal of the proposed algorithm.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"EPISODE: EPISODIC GRADIENT CLIPPING WITH PERIODIC RESAMPLED CORRECTIONS FOR FEDERATED LEARNING WITH HETEROGENEOUS DATA\" presents a novel method for addressing the challenges of federated learning, particularly in the context of heterogeneous data distributions. The main contributions include the introduction of episodic gradient clipping and a mechanism for periodic resampled corrections, aimed at improving convergence rates and communication efficiency among federated learning participants. The authors conduct extensive experiments to demonstrate the effectiveness of their approach compared to existing methods, showcasing significant improvements in both convergence speed and model accuracy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to gradient clipping, which addresses a critical limitation in federated learning frameworks. The methodology is well-structured and includes theoretical backing that enhances its credibility. However, the paper could benefit from clearer explanations of certain concepts, particularly the definitions of specific terms like \"L-smooth\" and \"asymptotic,\" which may hinder comprehension for readers less familiar with the subject matter. Additionally, some inconsistencies in notation and terminology could lead to confusion.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the overall quality of the paper is commendable, there are several clarity issues that detract from its readability. For example, the abstract is overly lengthy, and the use of \"i.e.\" could be misinterpreted without proper context. The paper also suffers from inconsistent variable definitions and terminology usage, which may impact reproducibility. Despite these issues, the novelty of the proposed method is significant, providing a fresh perspective on the challenges faced in federated learning.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning, with a novel methodology that shows promise for improving efficiency and accuracy. However, clarity and consistency issues must be addressed to enhance the paper's accessibility and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to gradient clipping in federated learning, specifically targeting scenarios with heterogeneous data. The authors introduce two key techniques: episodic gradient clipping and periodic resampled corrections, aimed at improving communication efficiency during the training process. Through theoretical analysis and experiments, the paper claims to achieve linear speedup in communication rounds while maintaining competitive accuracy on the selected datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative introduction of episodic gradient clipping, which could potentially enhance the efficiency of federated learning systems. However, the weaknesses include a limited exploration of the applicability of these techniques beyond the current focus, such as in reinforcement learning or multi-task learning. Moreover, the narrow dataset selection and lack of scalability analysis with varying client numbers or model sizes restrict the generalizability of the findings. The omission of discussions around data privacy, security implications, and communication patterns further detracts from the paper's comprehensiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the proposed methods and findings. However, the novelty is somewhat diminished by the lack of comparative analysis with recent methods in gradient clipping and federated learning. The reproducibility of the results could be improved by providing a broader range of datasets and discussing the impact of varying parameters, such as the clipping threshold, on convergence stability.\n\n# Summary Of The Review\nOverall, the paper offers a promising approach to improving communication efficiency in federated learning through innovative techniques. However, it falls short in terms of broader applicability, comprehensive analysis, and addressing key concerns such as data privacy and scalability, which limits its overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents EPISODE, an innovative algorithm aimed at improving gradient clipping in federated learning (FL) with heterogeneous data. The authors introduce key concepts such as relaxed smoothness with respect to gradient norms and develop statistical methodologies for analyzing gradient properties, leading to convergence guarantees. Extensive empirical validation demonstrates that EPISODE consistently outperforms existing approaches (e.g., FedAvg, SCAFFOLD, CELGC) in terms of training losses and test accuracies across various datasets and conditions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous statistical foundation and the clear articulation of the relationship between gradient norms and algorithm performance. The introduction of relaxed smoothness and statistical bounds significantly enhances the understanding of convergence in FL. However, a notable weakness is the lack of explicit statistical significance testing results (e.g., p-values) in the performance comparisons, which could strengthen the claims of superiority over baselines. Additionally, while ablation studies provide valuable insights, the details on the experimental setup could be more comprehensive.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear presentation of theoretical concepts and empirical results. The methodology is sound, and the novel contributions are articulated effectively. However, the reproducibility of results may be hindered by the absence of detailed descriptions of experimental conditions and hyperparameter settings. Overall, the quality of writing and clarity is high, but improvements in reproducibility are necessary.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of federated learning by introducing EPISODE, a statistically grounded algorithm that effectively tackles the challenges of gradient clipping in heterogeneous data settings. While the theoretical and empirical results are compelling, the paper could benefit from enhanced statistical reporting and greater detail in experimental methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents EPISODE, an optimization algorithm designed for federated learning environments. It focuses on enhancing performance through periodic resampled corrections and episodic gradient clipping. The findings suggest that EPISODE outperforms certain baseline methods under specific conditions of data heterogeneity and stochastic gradient noise, although the scope of tested scenarios is limited.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing challenges in federated learning through the proposed algorithm. However, the paper has notable weaknesses, such as a lack of exploration regarding scalability in large federated settings and the algorithm's performance under a broader range of data heterogeneity. Additionally, the reliance on specific assumptions about stochastic gradient noise without comprehensive theoretical guarantees raises concerns about the generalizability of the results. The experimental setup is also criticized for its limited diversity in datasets and tasks, which may affect the robustness of the conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, detailing the methodology and findings coherently. However, the novelty is somewhat constrained by the limited exploration of alternative optimization techniques and the absence of comparisons with advanced methods beyond the baseline. The reproducibility of results may be hindered by the lack of comprehensive experimental details and the narrow scope of testing.\n\n# Summary Of The Review\nOverall, the paper introduces a promising algorithm for federated learning, but it falls short in several critical areas, including scalability, theoretical robustness, and comprehensive empirical validation. Future work should address these limitations to enhance the algorithm's applicability and effectiveness in diverse real-world scenarios.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents \"EPISODE,\" an approach to episodic gradient clipping designed for federated learning scenarios with heterogeneous data. The authors propose a unified framework that claims to address both homogeneous and heterogeneous data settings, introducing techniques such as episodic gradient clipping and periodic resampled corrections. The methodology includes theoretical proofs of convergence under relaxed conditions and empirical evaluations across several datasets, which the authors claim demonstrate superior performance compared to existing methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its attempt to tackle the challenges of federated learning with heterogeneous data and the introduction of a theoretical framework addressing these challenges. However, the weaknesses are significant; the concepts presented are largely derivative of existing literature on gradient clipping and federated learning. The novelty of the proposed methods can be questioned, as they do not appear to introduce fundamentally new ideas but rather repackage established concepts. The experimental results, while indicating improved performance, lack rigorous comparative analysis and may be influenced by selective reporting.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and easy to follow, but the clarity of its contributions is undermined by the presentation of well-known techniques as novel advancements. The quality of the experiments is adequate, yet the reproducibility of the results may be hindered by insufficient details regarding the experimental setup and performance metrics. Overall, while the paper is clear in its writing, it falls short on novelty and reproducibility due to the lack of innovative contributions.\n\n# Summary Of The Review\nOverall, this paper attempts to contribute to the field of federated learning by presenting a framework for episodic gradient clipping. However, it largely revisits established concepts without offering significant advancements, leading to a perception of the work as derivative rather than innovative.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel federated learning algorithm called EPISODE, which integrates episodic gradient clipping and periodic resampled corrections to enhance convergence and robustness in cross-silo settings. The authors provide theoretical guarantees for the proposed method and empirically validate its performance through experiments on text and image classification tasks. The findings demonstrate that EPISODE outperforms existing federated learning algorithms in terms of both accuracy and resilience to gradient noise.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to addressing gradient noise through episodic clipping, which is a significant contribution to the federated learning literature. The theoretical underpinnings lend credibility to the proposed method, and the empirical results support its effectiveness across various classification tasks. However, the focus on cross-silo settings limits the applicability of the findings to more challenging cross-device environments, where communication constraints and data heterogeneity are more pronounced. Additionally, the hyperparameter tuning process relies on grid search, which may not be the most efficient method for optimizing performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to readers with a background in federated learning. The quality of the theoretical analysis and empirical validation is commendable, though further exploration of hyperparameter optimization and robustness to different types of gradient noise could enhance the reproducibility of the results. While the novelty of the approach is significant, extending the empirical evaluations to diverse datasets beyond text and image classification would strengthen claims of generalizability.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in federated learning through the introduction of the EPISODE algorithm, which effectively addresses challenges related to gradient noise. While the theoretical and empirical contributions are robust, there are opportunities for further exploration of its application in more complex settings and optimization techniques.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents EPISODE, a novel federated learning algorithm designed to address challenges associated with data heterogeneity across clients. The methodology emphasizes reduced communication complexity while achieving superior performance metrics compared to established baselines such as FedAvg, SCAFFOLD, and CELGC across various datasets, including SNLI, ImageNet, CIFAR-10, and Sentiment140. Key findings indicate that EPISODE not only outperforms these methods in both training loss and testing accuracy but also exhibits robustness to data heterogeneity, maintaining performance despite variations in client data similarity.\n\n# Strength And Weaknesses\nThe main strengths of this paper include its significant performance improvements over strong baselines, its effective handling of heterogeneous data, and its reduced communication complexity, which is crucial for practical deployments of federated learning. The ablation study further strengthens the paper's claims by demonstrating the importance of EPISODE's algorithmic components. However, the paper might benefit from a more thorough exploration of the theoretical underpinnings of the proposed method and a discussion of potential limitations or scenarios where EPISODE may underperform.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and findings, making it accessible to readers. The quality of the experiments is high, with rigorous benchmarking against established methods. The novelty of the approach is notable, particularly in its ability to manage communication efficiently while maintaining performance. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of federated learning by introducing EPISODE, which effectively balances communication efficiency and performance in the presence of heterogeneous data. The empirical results are compelling, although some aspects of theoretical justification and reproducibility require further enhancement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing nonconvex optimization problems with a focus on relaxed smoothness conditions. The authors propose a new algorithm that leverages these conditions to improve convergence rates in training deep learning models. Through a series of experiments on benchmark datasets, the findings demonstrate that the proposed method outperforms existing algorithms in terms of both efficiency and accuracy.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative algorithm that addresses a significant challenge in nonconvex optimization, which is highly relevant for the deep learning community. The empirical results provide strong evidence for the effectiveness of the proposed method. However, weaknesses include the paper's dense and jargon-heavy language, which may hinder accessibility for a broader audience. Additionally, the lack of clarity in the experimental setup and results diminishes the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a technically sound approach, clarity issues arise from the extensive use of jargon without sufficient definitions, making it less approachable for non-specialists. The overall structure and formatting inconsistencies detract from the quality of the presentation. The novelty of the proposed method is significant, but reproducibility may be hampered by the insufficient detail in the experimental design and setup.\n\n# Summary Of The Review\nOverall, the paper presents a promising new algorithm for nonconvex optimization that demonstrates empirical advantages over existing methods. However, improvements in clarity, structure, and detail are necessary to enhance accessibility and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.2415145453325236,
    -1.6855052313311567,
    -1.8713452604053349,
    -1.7668075634905471,
    -1.807249908130249,
    -1.660803703932205,
    -1.7647445436489444,
    -1.774752783207618,
    -2.0239150573645266,
    -1.82114317174528,
    -1.7533243453303724,
    -1.4651699594299235,
    -1.7595358424146614,
    -1.6898827231096982,
    -1.744126234895669,
    -1.701806693083395,
    -2.10613601423137,
    -1.9438151465370905,
    -1.7870472203792707,
    -1.7645577868851934,
    -2.004428948069234,
    -1.5933705510878884,
    -1.7673902652235476,
    -1.8875589529198498,
    -1.7788601103532935,
    -1.8564278078590848,
    -1.5691482256042752,
    -1.6488879042913684,
    -1.5872788088688787
  ],
  "logp_cond": [
    [
      0.0,
      -1.9327723700110626,
      -1.9578711213659896,
      -1.92392797693295,
      -2.0040419863236405,
      -1.9604000586880146,
      -1.9709680691344367,
      -1.949762316614076,
      -1.9034900244504083,
      -2.025261730569421,
      -1.9435469466494566,
      -2.085377579950275,
      -1.9489824443267987,
      -1.9572601101342553,
      -1.8823938877867818,
      -1.9741444525911125,
      -1.898608355180104,
      -1.936169331834686,
      -1.9479503880709863,
      -1.9357315629505925,
      -1.9306727278322033,
      -1.9616797923879454,
      -1.9923800340537408,
      -1.993746360142258,
      -1.9843354609685897,
      -1.9739096133964515,
      -1.9740452821590493,
      -2.002962371993565,
      -2.081531036292458
    ],
    [
      -1.1543395186193466,
      0.0,
      -1.1059228432693964,
      -1.1614381988544942,
      -1.1610652859043844,
      -1.1680343321007556,
      -1.2303074513939924,
      -1.0971263320509366,
      -1.1321208435172248,
      -1.2864512853741552,
      -1.1443298497707537,
      -1.4367842920061462,
      -1.1020301180131944,
      -1.1701360128231686,
      -1.0908185682161027,
      -1.1474657793575986,
      -1.0980249608901722,
      -1.1300836021586027,
      -1.2084267982249386,
      -1.1886692749392047,
      -1.1676401890787949,
      -1.1737401842025061,
      -1.2398806780453504,
      -1.242955233985525,
      -1.2596613565402404,
      -1.2295180977814801,
      -1.230670343281716,
      -1.3245525455004328,
      -1.3449114136979468
    ],
    [
      -1.4066069970582273,
      -1.3768833454540303,
      0.0,
      -1.3508032621850141,
      -1.4448766008573877,
      -1.3655800729816876,
      -1.4401576655194186,
      -1.3929966031985577,
      -1.293020259204443,
      -1.5339833781509977,
      -1.4038565482381014,
      -1.5875952389621193,
      -1.4077850392933065,
      -1.4318616222532277,
      -1.367919797728493,
      -1.4259134656065664,
      -1.3561943829142382,
      -1.425943919453506,
      -1.4640324582015538,
      -1.4114028192592083,
      -1.4173972107585933,
      -1.4347532886863754,
      -1.4617972708937848,
      -1.4883722470122007,
      -1.4461558571199746,
      -1.461063868144231,
      -1.4571841588503125,
      -1.5320131770697727,
      -1.5522479345629596
    ],
    [
      -1.2906653646524515,
      -1.2264316849252186,
      -1.2383997807118412,
      0.0,
      -1.3609184454462915,
      -1.2564713027045658,
      -1.3509773212135607,
      -1.2451341087118644,
      -1.2289825838195811,
      -1.3586186692305158,
      -1.2431967031692286,
      -1.4936199351825614,
      -1.2394836295986034,
      -1.2553515331786984,
      -1.204845365828855,
      -1.2706903333470356,
      -1.2777094593362994,
      -1.2279765202755029,
      -1.2748038224201441,
      -1.291034244589087,
      -1.2583077367486515,
      -1.2835993027438393,
      -1.3482142203103482,
      -1.3458473679509133,
      -1.3520787816542488,
      -1.3271815284872188,
      -1.3454532743575938,
      -1.3995765773952378,
      -1.436365704944622
    ],
    [
      -1.3836120540633705,
      -1.284418326002447,
      -1.3692116142527,
      -1.442708250250392,
      0.0,
      -1.404039663961795,
      -1.45923921217657,
      -1.3644129553565707,
      -1.337978488161938,
      -1.4100324645237794,
      -1.3791953564659238,
      -1.5315576315010568,
      -1.430759535819931,
      -1.428808611786133,
      -1.3240199348128605,
      -1.444431441367283,
      -1.2564014673559478,
      -1.3838501490068253,
      -1.4551472486490087,
      -1.4187452676296748,
      -1.4246138312782706,
      -1.4373358909273424,
      -1.4976397436991145,
      -1.3843928541262769,
      -1.508899479326227,
      -1.480476993529275,
      -1.4420055528873774,
      -1.3438248501632017,
      -1.4849657374440055
    ],
    [
      -1.275030736481713,
      -1.1892048730264864,
      -1.1588255016430151,
      -1.1973187272453347,
      -1.2989080657194076,
      0.0,
      -1.1858742588744278,
      -1.149631534572102,
      -1.1464392184572427,
      -1.2656169993126507,
      -1.2127346496327505,
      -1.4057742666642359,
      -1.2363722084690152,
      -1.2026596579100894,
      -1.161990386531603,
      -1.1780612793753151,
      -1.1340364825179865,
      -1.1822291591986611,
      -1.2087766089934928,
      -1.1932839589698505,
      -1.224042933922707,
      -1.185003158403711,
      -1.2455049509104383,
      -1.2824533315919187,
      -1.2105113624068562,
      -1.2439678700547279,
      -1.246090615490706,
      -1.3282903043378254,
      -1.335970331606264
    ],
    [
      -1.3853610177325928,
      -1.312108087714918,
      -1.2628075643201717,
      -1.3016399501881253,
      -1.3942435201119723,
      -1.2568516227320838,
      0.0,
      -1.3197814624135067,
      -1.3076095643626573,
      -1.3778689188192863,
      -1.3068414336939456,
      -1.4942929788660937,
      -1.3548813164357847,
      -1.3437273661548523,
      -1.3001259311752391,
      -1.3174518394567891,
      -1.2954724094667327,
      -1.3189152362895789,
      -1.3567885604929653,
      -1.2685703127062826,
      -1.3251530370715066,
      -1.2438657493568155,
      -1.3355634362852051,
      -1.3823189504609243,
      -1.3338022887126593,
      -1.3395270337849565,
      -1.3411746613802704,
      -1.416114736297702,
      -1.4480996851379577
    ],
    [
      -1.3350158119080293,
      -1.1681885641702054,
      -1.2854240830525827,
      -1.2878171769331843,
      -1.3255179042352792,
      -1.24291567633542,
      -1.3747881921490235,
      0.0,
      -1.270200615750751,
      -1.3562493579949275,
      -1.2780460302630197,
      -1.5397223665321293,
      -1.2887773697985974,
      -1.3059272954524401,
      -1.2894313260998342,
      -1.2697786607805235,
      -1.238142460338986,
      -1.2546340143347479,
      -1.344246999710266,
      -1.338268266158027,
      -1.335705471780865,
      -1.332336549453341,
      -1.3649839212667538,
      -1.3761152256714035,
      -1.386810998012129,
      -1.3554947360284573,
      -1.3870707540304403,
      -1.4613251132552474,
      -1.4971825257665599
    ],
    [
      -1.4998324333386874,
      -1.4898560149328686,
      -1.4748634190166667,
      -1.5099241431889896,
      -1.5715463003123626,
      -1.4827471549259672,
      -1.5961358622373523,
      -1.5189946056532697,
      0.0,
      -1.624783460283787,
      -1.5139102902219017,
      -1.7543120035822741,
      -1.5276534585564372,
      -1.5371892228286024,
      -1.4113050694538487,
      -1.5444703212943056,
      -1.4435507676267971,
      -1.4822137922002177,
      -1.5405277880157042,
      -1.5533224290840897,
      -1.516832199969303,
      -1.5440086524263708,
      -1.595546404031864,
      -1.595571454327868,
      -1.5663929720470486,
      -1.5877836720774683,
      -1.590698152490917,
      -1.6339855376894057,
      -1.693118856461233
    ],
    [
      -1.509885326804457,
      -1.4462764381641604,
      -1.471915990687201,
      -1.439324645435605,
      -1.4828982058921811,
      -1.4991779463893014,
      -1.4968626001970016,
      -1.4043920835386807,
      -1.4173298953939226,
      0.0,
      -1.488975199376176,
      -1.6229223556120866,
      -1.4580627715902195,
      -1.4764228607446919,
      -1.4962513297975661,
      -1.5100927400607378,
      -1.4013756861280675,
      -1.4542573956994356,
      -1.5266764813962863,
      -1.4815598933068885,
      -1.4722464876136518,
      -1.498328961206326,
      -1.506661272086938,
      -1.497583725316724,
      -1.452809762605053,
      -1.483662754600659,
      -1.5149513613671401,
      -1.5591961466698,
      -1.5824587483483619
    ],
    [
      -1.2075884871057263,
      -1.1782971425712987,
      -1.148142875483171,
      -1.1474475373799427,
      -1.2521350832275882,
      -1.1133472353509137,
      -1.2405845543384988,
      -1.1278557428196034,
      -1.126026115185037,
      -1.2767191918434166,
      0.0,
      -1.433403134356344,
      -1.1600225925256378,
      -1.160541075882152,
      -1.1618397740275666,
      -1.1552245847664786,
      -1.180628224783676,
      -1.1913458224209774,
      -1.2104795827220582,
      -1.1795125322137336,
      -1.1590248166069756,
      -1.1769864668362167,
      -1.2205700749431876,
      -1.33588648601299,
      -1.3123245523630547,
      -1.2270002766654908,
      -1.2966065250450078,
      -1.3265888562392743,
      -1.3769144892008087
    ],
    [
      -1.1969112654723022,
      -1.2077035016504944,
      -1.1863228094552685,
      -1.2027008228787177,
      -1.1699173909582805,
      -1.1879957565675647,
      -1.2153254557208601,
      -1.1733372019409762,
      -1.1887151787779318,
      -1.1632962254883192,
      -1.207336225951436,
      0.0,
      -1.1950460942495378,
      -1.1446842314878367,
      -1.1950025765851051,
      -1.2061289715780086,
      -1.1686287812437244,
      -1.1938485295313732,
      -1.2367432917769003,
      -1.212176874464828,
      -1.205616832308796,
      -1.2009925810767215,
      -1.1356312425782216,
      -1.1911678845987257,
      -1.1280146569810983,
      -1.1599104875716861,
      -1.1863741864943063,
      -1.1982903541800167,
      -1.1830524262351618
    ],
    [
      -1.2441069449243563,
      -1.1792997923827777,
      -1.2162338575587612,
      -1.2410052109246767,
      -1.3486244234043512,
      -1.2550402764908972,
      -1.34172973811995,
      -1.2012838068932108,
      -1.2342224897832064,
      -1.3079140328011323,
      -1.2680494049852697,
      -1.4487333229310102,
      0.0,
      -1.1860871879394532,
      -1.1902807591734992,
      -1.204588097788899,
      -1.2696820220348464,
      -1.2138631946151937,
      -1.2792995907179305,
      -1.3021221439606177,
      -1.258709796634364,
      -1.266156836560521,
      -1.236501791572789,
      -1.3800506307496703,
      -1.312310320889785,
      -1.1911802093922983,
      -1.345940574785109,
      -1.428138253482652,
      -1.4277853415458968
    ],
    [
      -1.2374069275507282,
      -1.2099232558023576,
      -1.1706814448697405,
      -1.1882509489259883,
      -1.29971462082142,
      -1.1694437302231997,
      -1.2601294509459064,
      -1.1880053324177529,
      -1.215755184177824,
      -1.270513162968613,
      -1.203513109356889,
      -1.3694191849220192,
      -1.1732599174531186,
      0.0,
      -1.1573222596583586,
      -1.2011582756340196,
      -1.166667514572369,
      -1.2156430865697405,
      -1.2648169694352909,
      -1.232010975648455,
      -1.2245545469011538,
      -1.2102932180652581,
      -1.1973352511990447,
      -1.3280570355202634,
      -1.2428099391569372,
      -1.1923101879489573,
      -1.2735886169626658,
      -1.3814412285968667,
      -1.3908259908397083
    ],
    [
      -1.2083392125089043,
      -1.1663488741240913,
      -1.170047617935321,
      -1.2054087803687779,
      -1.2330064990612994,
      -1.1564597277727642,
      -1.2771152769661716,
      -1.19181349573155,
      -1.1203007166070695,
      -1.3373487514331392,
      -1.225417854196534,
      -1.4471003140576566,
      -1.1575547660112104,
      -1.2492036423996513,
      0.0,
      -1.2116747011067435,
      -1.1920871079469757,
      -1.1649419840105073,
      -1.2778784806363537,
      -1.2095647061307075,
      -1.2166234268894724,
      -1.2161883040674621,
      -1.2777661458506766,
      -1.3098550872927137,
      -1.3292067324595342,
      -1.2983819341420415,
      -1.3092562712108293,
      -1.3235323622280963,
      -1.402173684641622
    ],
    [
      -1.2111314645375122,
      -1.1581702599886365,
      -1.1583811324153122,
      -1.210672950290112,
      -1.2960069008490172,
      -1.1638097170798025,
      -1.216189737462624,
      -1.1751454908910053,
      -1.206849032267114,
      -1.2861462624826567,
      -1.126125121359256,
      -1.4113213986177038,
      -1.1613407279561319,
      -1.1603122646585295,
      -1.2148518058369353,
      0.0,
      -1.1803114750612098,
      -1.2069351406336022,
      -1.1915969227318672,
      -1.2585297269086515,
      -1.2371642083944236,
      -1.202168549215714,
      -1.2390130805574453,
      -1.31334213471795,
      -1.3004579768071318,
      -1.2093852235894045,
      -1.2547985435099405,
      -1.3549635055626694,
      -1.363569244447326
    ],
    [
      -1.5791735889625742,
      -1.5042662191900185,
      -1.542809767988321,
      -1.5793392953122294,
      -1.5739419367790057,
      -1.5519758566122632,
      -1.6130149894942807,
      -1.5139230283161238,
      -1.5148310720384675,
      -1.6279958746160468,
      -1.5997817046215246,
      -1.8504661550528763,
      -1.6411335327433552,
      -1.6460971616697508,
      -1.56669905674638,
      -1.609682551004117,
      0.0,
      -1.6018401362049999,
      -1.6275998573438677,
      -1.5828143801813304,
      -1.6158121353938164,
      -1.5659773635738974,
      -1.6443237388246819,
      -1.6430566335090775,
      -1.5881747592030344,
      -1.6205538274257067,
      -1.6187806495557153,
      -1.7054466625747007,
      -1.7656269465564483
    ],
    [
      -1.368499788572638,
      -1.3165391435830562,
      -1.343347580193699,
      -1.3893729284064968,
      -1.4484017462647976,
      -1.3620299978679267,
      -1.4426069056172137,
      -1.316208959729999,
      -1.2935247991422045,
      -1.4734629218380155,
      -1.411042078522808,
      -1.6250273217213882,
      -1.331335580434125,
      -1.3971873574888936,
      -1.3437984756158479,
      -1.3830682654903674,
      -1.3577916743272607,
      0.0,
      -1.4133029363450877,
      -1.411146608788636,
      -1.3201517834899792,
      -1.411222282510728,
      -1.4243534437653358,
      -1.5275465862111417,
      -1.4678645246243958,
      -1.439338669615576,
      -1.4251629974545073,
      -1.5235098553614241,
      -1.5764815628453386
    ],
    [
      -1.2800588781301905,
      -1.232073863892582,
      -1.2945669840735987,
      -1.2896268912654674,
      -1.395676932467908,
      -1.2546499125759198,
      -1.3088434574802634,
      -1.2886292298169746,
      -1.2268519269249805,
      -1.38835754167334,
      -1.277480255729484,
      -1.5043168321850124,
      -1.2731353337435432,
      -1.316909412736324,
      -1.2698976612381876,
      -1.2686591937444798,
      -1.2713990131110937,
      -1.2147136449433016,
      0.0,
      -1.3141098342433752,
      -1.2843590869535337,
      -1.2852558100260605,
      -1.3184850506394357,
      -1.3603738801765042,
      -1.3392186413433007,
      -1.297561561400731,
      -1.293245055780837,
      -1.4387756666001579,
      -1.403866530088166
    ],
    [
      -1.333667127405223,
      -1.3183417563718722,
      -1.2935826748627552,
      -1.256453195865273,
      -1.3813636755710812,
      -1.2958304069789468,
      -1.3429344406365875,
      -1.3246240184071898,
      -1.3400091669492296,
      -1.399834069666544,
      -1.3125456144729077,
      -1.5250332560262128,
      -1.38088134476119,
      -1.3456659276648029,
      -1.2983403097117485,
      -1.3418556692445587,
      -1.3185153126320472,
      -1.3345263099036362,
      -1.3643227828512656,
      0.0,
      -1.3423283443443925,
      -1.3488243882743565,
      -1.397830453456487,
      -1.4212420415646818,
      -1.4241449145443132,
      -1.4158683210958898,
      -1.3965234015859356,
      -1.400471091653828,
      -1.501015120921096
    ],
    [
      -1.4387064692876605,
      -1.369738383300507,
      -1.3968999056879574,
      -1.3831678915409047,
      -1.4922616062178413,
      -1.4337709930754408,
      -1.498132475919072,
      -1.3695270375682882,
      -1.3993234251406583,
      -1.557370634507212,
      -1.3974817059027527,
      -1.6604653195924917,
      -1.4061747184624274,
      -1.4466120652971062,
      -1.3771669970097704,
      -1.4501354971128764,
      -1.4067829467913424,
      -1.3649233777730694,
      -1.5250923989342904,
      -1.4808086863429126,
      0.0,
      -1.4558269544396811,
      -1.4901529071490625,
      -1.5306685037720666,
      -1.5400965184571769,
      -1.4768118779243005,
      -1.4931445956769567,
      -1.578020276694434,
      -1.620144082591654
    ],
    [
      -1.158156181313594,
      -1.143634335109999,
      -1.1183791600577193,
      -1.141577155054674,
      -1.247037026956777,
      -1.1010541858054788,
      -1.1416658050688382,
      -1.1232263338619757,
      -1.1085873108612072,
      -1.204865022792207,
      -1.1186387075703448,
      -1.2853890174486637,
      -1.1523744894453174,
      -1.1482650830112837,
      -1.11154815590188,
      -1.1452631490799219,
      -1.1156093101587103,
      -1.1387391716891027,
      -1.196518709303144,
      -1.124213940826933,
      -1.1415465427490652,
      0.0,
      -1.1535012327860896,
      -1.2114784215637524,
      -1.2001494588071009,
      -1.1493944807824825,
      -1.172333493349531,
      -1.243758752982862,
      -1.251348614644208
    ],
    [
      -1.3127138255463278,
      -1.3409267604055803,
      -1.3260005958968528,
      -1.321772736061806,
      -1.4297522161108709,
      -1.3050142559867595,
      -1.3232020249615768,
      -1.318254716773253,
      -1.3303590124326057,
      -1.3204808894993731,
      -1.2970378350268699,
      -1.446710804110055,
      -1.2843797413148672,
      -1.265484171926473,
      -1.2878245197413218,
      -1.319292551217569,
      -1.2486644393955622,
      -1.3352073255321077,
      -1.3792253562646835,
      -1.3086052555540615,
      -1.345591868029953,
      -1.343655051501444,
      0.0,
      -1.4440508430952783,
      -1.3077906604067002,
      -1.2653678405590945,
      -1.3463330461479859,
      -1.4830264354009999,
      -1.4525507947422764
    ],
    [
      -1.4157745061540736,
      -1.4485370082069833,
      -1.4198152051531379,
      -1.5037513943957186,
      -1.355943419511468,
      -1.4421266958132963,
      -1.4578209018329475,
      -1.4160167011368445,
      -1.3776095870951057,
      -1.4936680303466956,
      -1.457168819088393,
      -1.5766107660373196,
      -1.517122642745027,
      -1.5075250450363205,
      -1.4066602441962768,
      -1.4584686449508037,
      -1.3974488651625379,
      -1.4582714019272098,
      -1.4883879254950256,
      -1.4571808310438352,
      -1.4408473705473623,
      -1.4324079040967566,
      -1.5130383580362199,
      0.0,
      -1.533730972733795,
      -1.4945966302214153,
      -1.506601430442746,
      -1.3928694771636398,
      -1.5209575947695233
    ],
    [
      -1.3531301850801203,
      -1.344804859264762,
      -1.2875053300325001,
      -1.3063135982057381,
      -1.4391274381800239,
      -1.3150057448608201,
      -1.3368742516349452,
      -1.3513284901557023,
      -1.2836110918748511,
      -1.3610483755923595,
      -1.3396339914602862,
      -1.437245501145071,
      -1.3134934571918635,
      -1.3111947613330117,
      -1.3447447229168443,
      -1.3773079580417427,
      -1.2970856450381265,
      -1.3445837969933865,
      -1.4172574669189122,
      -1.342437735764173,
      -1.3282873904733705,
      -1.3212482058133677,
      -1.3133596332788162,
      -1.4271624670641092,
      0.0,
      -1.2927706084870947,
      -1.3090623090880746,
      -1.4658881843632763,
      -1.4842667566573229
    ],
    [
      -1.3700863689662217,
      -1.3331504811838972,
      -1.3534405119028303,
      -1.3511953038470494,
      -1.4493967128236054,
      -1.355425389654722,
      -1.3660391720569856,
      -1.3040083540759149,
      -1.3592225263513245,
      -1.417657543099391,
      -1.3731117939534567,
      -1.5213038972025648,
      -1.253076619154349,
      -1.3250122244004596,
      -1.3420590730198387,
      -1.3881331955531597,
      -1.3361460480074308,
      -1.339815780398309,
      -1.4178838220233978,
      -1.3653279060525136,
      -1.34078232563735,
      -1.3458974865567472,
      -1.341634614535289,
      -1.4524509159443835,
      -1.3864156608795841,
      0.0,
      -1.3950895973486395,
      -1.4856333439146805,
      -1.5210304543568633
    ],
    [
      -1.1645290151948193,
      -1.0927992113444132,
      -1.1077385889132192,
      -1.1258879300911575,
      -1.1858631368104071,
      -1.1022083463430818,
      -1.1370586433656882,
      -1.123707117358351,
      -1.1096198487802482,
      -1.181783657016759,
      -1.1164801633994554,
      -1.2900012568003694,
      -1.1295077260505093,
      -1.1235036414851047,
      -1.1391490429971989,
      -1.1208294431129353,
      -1.0960857869510865,
      -1.0972481739707844,
      -1.1046614956392704,
      -1.1097754136834417,
      -1.1401998604955887,
      -1.1331539329202949,
      -1.1169072436187997,
      -1.1949785870120122,
      -1.1016052412893382,
      -1.1189375538117052,
      0.0,
      -1.2045614216881726,
      -1.250690578779064
    ],
    [
      -1.2315209841082548,
      -1.2777830899054985,
      -1.2610360736650919,
      -1.3027684833496411,
      -1.156617551354575,
      -1.3173521546249947,
      -1.2908389472980393,
      -1.2694105662765056,
      -1.21409854666767,
      -1.3313114456797552,
      -1.258068961321186,
      -1.3634858358137016,
      -1.311231820936172,
      -1.3336878206373741,
      -1.2615318841234762,
      -1.2856900010961596,
      -1.2304467416165752,
      -1.2852727060840838,
      -1.3067317195512245,
      -1.2425374793395982,
      -1.2725962941766944,
      -1.2925792763228605,
      -1.336873188906706,
      -1.1898635822757901,
      -1.322276501065538,
      -1.3065386869531854,
      -1.30797135260196,
      0.0,
      -1.3053879268351187
    ],
    [
      -1.2365873801621259,
      -1.2221979728170016,
      -1.1850972840892704,
      -1.2028846713628485,
      -1.1840802069431537,
      -1.2206009043702368,
      -1.2252908845473496,
      -1.2173416897046498,
      -1.1785491614952994,
      -1.2124583614052564,
      -1.2284182273383197,
      -1.1784901101527434,
      -1.2355243739655597,
      -1.2543211575366517,
      -1.202304795993788,
      -1.2339967984062215,
      -1.2052549027235115,
      -1.2195121933814916,
      -1.228439265676551,
      -1.2209868942579383,
      -1.19760708987087,
      -1.1320588693215152,
      -1.2184340741139987,
      -1.2026747250028103,
      -1.2346340154433275,
      -1.280498846215222,
      -1.1777962657295413,
      -1.2258502970027099,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.308742175321461,
      0.28364342396653397,
      0.31758656839957355,
      0.2374725590088831,
      0.281114486644509,
      0.2705464761980869,
      0.2917522287184475,
      0.3380245208821153,
      0.21625281476310265,
      0.297967598683067,
      0.15613696538224842,
      0.2925321010057249,
      0.2842544351982683,
      0.35912065754574174,
      0.2673700927414111,
      0.3429061901524195,
      0.3053452134978376,
      0.2935641572615373,
      0.30578298238193113,
      0.31084181750032025,
      0.27983475294457816,
      0.24913451127878283,
      0.24776818519026556,
      0.2571790843639339,
      0.26760493193607204,
      0.2674692631734743,
      0.23855217333895862,
      0.15998350904006564
    ],
    [
      0.5311657127118101,
      0.0,
      0.5795823880617603,
      0.5240670324766625,
      0.5244399454267723,
      0.5174708992304011,
      0.45519777993716426,
      0.5883788992802201,
      0.553384387813932,
      0.3990539459570015,
      0.541175381560403,
      0.2487209393250105,
      0.5834751133179623,
      0.5153692185079881,
      0.594686663115054,
      0.5380394519735581,
      0.5874802704409845,
      0.555421629172554,
      0.4770784331062181,
      0.496835956391952,
      0.5178650422523619,
      0.5117650471286506,
      0.4456245532858063,
      0.4425499973456317,
      0.42584387479091634,
      0.45598713354967657,
      0.4548348880494406,
      0.36095268583072393,
      0.34059381763320995
    ],
    [
      0.46473826334710755,
      0.49446191495130454,
      0.0,
      0.5205419982203208,
      0.4264686595479472,
      0.5057651874236473,
      0.4311875948859163,
      0.4783486572067772,
      0.5783250012008918,
      0.33736188225433716,
      0.46748871216723353,
      0.2837500214432156,
      0.46356022111202844,
      0.4394836381521072,
      0.503425462676842,
      0.4454317947987685,
      0.5151508774910967,
      0.44540134095182893,
      0.4073128022037811,
      0.4599424411461266,
      0.4539480496467416,
      0.43659197171895947,
      0.40954798951155014,
      0.38297301339313417,
      0.42518940328536026,
      0.41028139226110394,
      0.4141611015550224,
      0.3393320833355622,
      0.31909732584237527
    ],
    [
      0.4761421988380956,
      0.5403758785653285,
      0.5284077827787059,
      0.0,
      0.40588911804425565,
      0.5103362607859814,
      0.4158302422769864,
      0.5216734547786828,
      0.537824979670966,
      0.40818889426003135,
      0.5236108603213185,
      0.2731876283079857,
      0.5273239338919438,
      0.5114560303118487,
      0.5619621976616922,
      0.4961172301435115,
      0.4890981041542477,
      0.5388310432150443,
      0.492003741070403,
      0.47577331890146013,
      0.5084998267418956,
      0.48320826074670786,
      0.4185933431801989,
      0.4209601955396338,
      0.41472878183629835,
      0.4396260350033283,
      0.42135428913295336,
      0.36723098609530935,
      0.3304418585459252
    ],
    [
      0.4236378540668786,
      0.522831582127802,
      0.4380382938775491,
      0.36454165787985704,
      0.0,
      0.4032102441684542,
      0.34801069595367906,
      0.4428369527736784,
      0.469271419968311,
      0.3972174436064697,
      0.42805455166432527,
      0.27569227662919227,
      0.3764903723103181,
      0.37844129634411616,
      0.4832299733173886,
      0.36281846676296614,
      0.5508484407743013,
      0.42339975912342376,
      0.3521026594812404,
      0.3885046405005743,
      0.38263607685197853,
      0.36991401720290673,
      0.30961016443113465,
      0.4228570540039722,
      0.2983504288040222,
      0.32677291460097413,
      0.3652443552428717,
      0.46342505796704736,
      0.3222841706862436
    ],
    [
      0.38577296745049194,
      0.47159883090571864,
      0.5019782022891899,
      0.4634849766868703,
      0.36189563821279735,
      0.0,
      0.47492944505777723,
      0.511172169360103,
      0.5143644854749623,
      0.3951867046195543,
      0.44806905429945454,
      0.2550294372679691,
      0.4244314954631898,
      0.45814404602211556,
      0.498813317400602,
      0.48274242455688987,
      0.5267672214142185,
      0.4785745447335439,
      0.4520270949387122,
      0.4675197449623545,
      0.436760770009498,
      0.47580054552849393,
      0.41529875302176666,
      0.37835037234028635,
      0.4502923415253488,
      0.41683583387747714,
      0.4147130884414989,
      0.33251339959437964,
      0.3248333723259409
    ],
    [
      0.3793835259163516,
      0.45263645593402635,
      0.5019369793287727,
      0.4631045934608191,
      0.3705010235369721,
      0.5078929209168606,
      0.0,
      0.4449630812354377,
      0.4571349792862871,
      0.3868756248296581,
      0.45790310995499883,
      0.2704515647828507,
      0.4098632272131597,
      0.42101717749409207,
      0.4646186124737053,
      0.4472927041921553,
      0.4692721341822117,
      0.44582930735936555,
      0.40795598315597914,
      0.4961742309426618,
      0.43959150657743784,
      0.5208787942921289,
      0.42918110736373927,
      0.38242559318802005,
      0.43094225493628513,
      0.4252175098639879,
      0.423569882268674,
      0.3486298073512424,
      0.31664485851098667
    ],
    [
      0.43973697129958866,
      0.6065642190374125,
      0.48932870015503527,
      0.48693560627443366,
      0.4492348789723388,
      0.5318371068721979,
      0.3999645910585945,
      0.0,
      0.5045521674568669,
      0.41850342521269046,
      0.4967067529445983,
      0.23503041667548863,
      0.4859754134090206,
      0.46882548775517785,
      0.48532145710778374,
      0.5049741224270945,
      0.5366103228686321,
      0.5201187688728701,
      0.430505783497352,
      0.436484517049591,
      0.439047311426753,
      0.44241623375427697,
      0.40976886194086415,
      0.39863755753621444,
      0.38794178519548894,
      0.4192580471791607,
      0.38768202917717764,
      0.3134276699523706,
      0.2775702574410581
    ],
    [
      0.5240826240258392,
      0.534059042431658,
      0.5490516383478599,
      0.513990914175537,
      0.45236875705216395,
      0.5411679024385594,
      0.4277791951271743,
      0.5049204517112569,
      0.0,
      0.39913159708073964,
      0.5100047671426249,
      0.26960305378225247,
      0.4962615988080894,
      0.4867258345359242,
      0.6126099879106779,
      0.479444736070221,
      0.5803642897377295,
      0.5417012651643089,
      0.48338726934882237,
      0.4705926282804369,
      0.5070828573952235,
      0.47990640493815584,
      0.4283686533326625,
      0.4283436030366585,
      0.45752208531747796,
      0.4361313852870583,
      0.43321690487360964,
      0.3899295196751209,
      0.33079620090329365
    ],
    [
      0.311257844940823,
      0.37486673358111955,
      0.34922718105807893,
      0.38181852630967494,
      0.3382449658530988,
      0.3219652253559786,
      0.3242805715482784,
      0.41675108820659923,
      0.40381327635135733,
      0.0,
      0.332167972369104,
      0.1982208161331933,
      0.3630804001550605,
      0.3447203110005881,
      0.3248918419477138,
      0.31105043168454216,
      0.41976748561721244,
      0.3668857760458444,
      0.29446669034899364,
      0.3395832784383914,
      0.3488966841316281,
      0.322814210538954,
      0.31448189965834206,
      0.32355944642855605,
      0.36833340914022705,
      0.33748041714462085,
      0.3061918103781398,
      0.26194702507547984,
      0.2386844233969181
    ],
    [
      0.5457358582246461,
      0.5750272027590737,
      0.6051814698472013,
      0.6058768079504298,
      0.5011892621027842,
      0.6399771099794587,
      0.5127397909918736,
      0.625468602510769,
      0.6272982301453354,
      0.47660515348695576,
      0.0,
      0.31992121097402837,
      0.5933017528047346,
      0.5927832694482205,
      0.5914845713028059,
      0.5980997605638938,
      0.5726961205466965,
      0.561978522909395,
      0.5428447626083142,
      0.5738118131166388,
      0.5942995287233968,
      0.5763378784941557,
      0.5327542703871848,
      0.4174378593173824,
      0.4409997929673177,
      0.5263240686648816,
      0.45671782028536456,
      0.42673548909109815,
      0.3764098561295637
    ],
    [
      0.26825869395762125,
      0.257466457779429,
      0.278847149974655,
      0.26246913655120574,
      0.29525256847164294,
      0.2771742028623587,
      0.2498445037090633,
      0.2918327574889472,
      0.2764547806519917,
      0.3018737339416042,
      0.2578337334784875,
      0.0,
      0.27012386518038567,
      0.32048572794208674,
      0.27016738284481834,
      0.2590409878519149,
      0.2965411781861991,
      0.27132142989855024,
      0.2284266676530231,
      0.2529930849650954,
      0.25955312712112755,
      0.264177378353202,
      0.3295387168517019,
      0.27400207483119776,
      0.3371553024488252,
      0.3052594718582373,
      0.2787957729356172,
      0.2668796052499067,
      0.2821175331947616
    ],
    [
      0.5154288974903052,
      0.5802360500318837,
      0.5433019848559002,
      0.5185306314899847,
      0.41091141901031025,
      0.5044955659237642,
      0.41780610429471143,
      0.5582520355214506,
      0.5253133526314551,
      0.45162180961352916,
      0.4914864374293917,
      0.31080251948365123,
      0.0,
      0.5734486544752082,
      0.5692550832411623,
      0.5549477446257625,
      0.48985382037981506,
      0.5456726477994678,
      0.4802362516967309,
      0.4574136984540438,
      0.5008260457802973,
      0.4933790058541405,
      0.5230340508418725,
      0.37948521166499116,
      0.4472255215248764,
      0.5683556330223631,
      0.41359526762955245,
      0.3313975889320093,
      0.3317505008687647
    ],
    [
      0.45247579555897,
      0.4799594673073406,
      0.5192012782399578,
      0.50163177418371,
      0.39016810228827814,
      0.5204389928864985,
      0.4297532721637918,
      0.5018773906919454,
      0.4741275389318742,
      0.4193695601410852,
      0.4863696137528093,
      0.32046353818767903,
      0.5166228056565796,
      0.0,
      0.5325604634513397,
      0.48872444747567867,
      0.5232152085373292,
      0.4742396365399577,
      0.42506575367440735,
      0.45787174746124326,
      0.4653281762085444,
      0.4795895050444401,
      0.4925474719106535,
      0.3618256875894348,
      0.44707278395276107,
      0.49757253516074096,
      0.41629410614703244,
      0.30844149451283154,
      0.29905673226998997
    ],
    [
      0.5357870223867647,
      0.5777773607715777,
      0.574078616960348,
      0.5387174545268911,
      0.5111197358343695,
      0.5876665071229048,
      0.46701095792949743,
      0.5523127391641189,
      0.6238255182885994,
      0.40677748346252973,
      0.518708380699135,
      0.2970259208380124,
      0.5865714688844585,
      0.49492259249601767,
      0.0,
      0.5324515337889255,
      0.5520391269486933,
      0.5791842508851617,
      0.4662477542593153,
      0.5345615287649614,
      0.5275028080061965,
      0.5279379308282068,
      0.4663600890449924,
      0.43427114760295527,
      0.41491950243613474,
      0.4457443007536275,
      0.43486996368483966,
      0.42059387266757264,
      0.341952550254047
    ],
    [
      0.49067522854588286,
      0.5436364330947585,
      0.5434255606680829,
      0.49113374279328315,
      0.4057997922343779,
      0.5379969760035925,
      0.485616955620771,
      0.5266612021923898,
      0.4949576608162811,
      0.4156604306007383,
      0.575681571724139,
      0.29048529446569127,
      0.5404659651272632,
      0.5414944284248655,
      0.48695488724645974,
      0.0,
      0.5214952180221852,
      0.49487155244979286,
      0.5102097703515278,
      0.44327696617474355,
      0.4646424846889714,
      0.49963814386768113,
      0.46279361252594975,
      0.38846455836544513,
      0.40134871627626323,
      0.49242146949399057,
      0.4470081495734546,
      0.3468431875207256,
      0.3382374486360691
    ],
    [
      0.5269624252687959,
      0.6018697950413516,
      0.5633262462430491,
      0.5267967189191407,
      0.5321940774523644,
      0.5541601576191069,
      0.4931210247370894,
      0.5922129859152463,
      0.5913049421929026,
      0.4781401396153233,
      0.5063543096098455,
      0.25566985917849383,
      0.46500248148801493,
      0.46003885256161925,
      0.53943695748499,
      0.49645346322725303,
      0.0,
      0.5042958780263702,
      0.4785361568875024,
      0.5233216340500397,
      0.4903238788375537,
      0.5401586506574727,
      0.46181227540668823,
      0.4630793807222926,
      0.5179612550283357,
      0.48558218680566334,
      0.48735536467565477,
      0.4006893516566694,
      0.34050906767492184
    ],
    [
      0.5753153579644525,
      0.6272760029540343,
      0.6004675663433916,
      0.5544422181305937,
      0.49541340027229297,
      0.5817851486691639,
      0.5012082409198768,
      0.6276061868070915,
      0.650290347394886,
      0.470352224699075,
      0.5327730680142826,
      0.31878782481570234,
      0.6124795661029656,
      0.546627789048197,
      0.6000166709212427,
      0.5607468810467231,
      0.5860234722098299,
      0.0,
      0.5305122101920028,
      0.5326685377484546,
      0.6236633630471113,
      0.5325928640263626,
      0.5194617027717547,
      0.41626856032594883,
      0.47595062191269477,
      0.5044764769215144,
      0.5186521490825833,
      0.4203052911756664,
      0.36733358369175195
    ],
    [
      0.5069883422490802,
      0.5549733564866886,
      0.49248023630567195,
      0.4974203291138033,
      0.39137028791136275,
      0.5323973078033508,
      0.47820376289900723,
      0.4984179905622961,
      0.5601952934542902,
      0.39868967870593064,
      0.5095669646497867,
      0.2827303881942582,
      0.5139118866357275,
      0.47013780764294677,
      0.5171495591410831,
      0.5183880266347909,
      0.515648207268177,
      0.5723335754359691,
      0.0,
      0.4729373861358954,
      0.5026881334257369,
      0.5017914103532102,
      0.468562169739835,
      0.4266733402027665,
      0.44782857903596995,
      0.4894856589785397,
      0.4938021645984336,
      0.3482715537791128,
      0.3831806902911046
    ],
    [
      0.4308906594799704,
      0.44621603051332115,
      0.47097511202243814,
      0.5081045910199204,
      0.3831941113141122,
      0.4687273799062466,
      0.42162334624860587,
      0.4399337684780036,
      0.4245486199359638,
      0.3647237172186495,
      0.45201217241228564,
      0.23952453085898062,
      0.3836764421240033,
      0.4188918592203905,
      0.46621747717344486,
      0.4227021176406347,
      0.44604247425314614,
      0.4300314769815572,
      0.40023500403392775,
      0.0,
      0.42222944254080086,
      0.4157333986108369,
      0.36672733342870645,
      0.34331574532051157,
      0.34041287234088013,
      0.3486894657893036,
      0.3680343852992578,
      0.3640866952313653,
      0.26354266596409737
    ],
    [
      0.5657224787815736,
      0.6346905647687271,
      0.6075290423812767,
      0.6212610565283294,
      0.5121673418513928,
      0.5706579549937933,
      0.5062964721501622,
      0.6349019105009459,
      0.6051055229285758,
      0.4470583135620221,
      0.6069472421664814,
      0.34396362847674244,
      0.5982542296068067,
      0.5578168827721279,
      0.6272619510594637,
      0.5542934509563577,
      0.5976460012778917,
      0.6395055702961647,
      0.4793365491349437,
      0.5236202617263215,
      0.0,
      0.548601993629553,
      0.5142760409201717,
      0.47376044429716746,
      0.46433242961205723,
      0.5276170701449336,
      0.5112843523922774,
      0.4264086713748001,
      0.3842848654775801
    ],
    [
      0.4352143697742943,
      0.4497362159778895,
      0.4749913910301691,
      0.45179339603321433,
      0.3463335241311114,
      0.49231636528240963,
      0.4517047460190502,
      0.4701442172259127,
      0.4847832402266812,
      0.3885055282956813,
      0.4747318435175436,
      0.30798153363922465,
      0.440996061642571,
      0.44510546807660467,
      0.48182239518600833,
      0.4481074020079665,
      0.47776124092917804,
      0.45463137939878573,
      0.3968518417847444,
      0.46915661026095545,
      0.4518240083388232,
      0.0,
      0.43986931830179876,
      0.381892129524136,
      0.3932210922807875,
      0.44397607030540587,
      0.4210370577383573,
      0.3496117981050264,
      0.3420219364436803
    ],
    [
      0.4546764396772198,
      0.42646350481796724,
      0.44138966932669477,
      0.4456175291617417,
      0.3376380491126767,
      0.4623760092367881,
      0.4441882402619708,
      0.4491355484502946,
      0.4370312527909419,
      0.44690937572417444,
      0.4703524301966777,
      0.32067946111349266,
      0.4830105239086804,
      0.5019060932970747,
      0.47956574548222575,
      0.44809771400597853,
      0.5187258258279854,
      0.43218293969143984,
      0.38816490895886413,
      0.4587850096694861,
      0.4217983971935946,
      0.42373521372210354,
      0.0,
      0.32333942212826927,
      0.45959960481684736,
      0.5020224246644531,
      0.4210572190755617,
      0.2843638298225477,
      0.31483947048127114
    ],
    [
      0.47178444676577613,
      0.4390219447128665,
      0.4677437477667119,
      0.3838075585241312,
      0.5316155334083819,
      0.44543225710655343,
      0.4297380510869022,
      0.47154225178300524,
      0.509949365824744,
      0.39389092257315417,
      0.43039013383145686,
      0.3109481868825301,
      0.3704363101748227,
      0.3800339078835293,
      0.480898708723573,
      0.429090307969046,
      0.4901100877573119,
      0.42928755099263993,
      0.39917102742482413,
      0.43037812187601454,
      0.4467115823724874,
      0.4551510488230932,
      0.3745205948836299,
      0.0,
      0.35382798018605466,
      0.39296232269843445,
      0.3809575224771038,
      0.4946894757562099,
      0.36660135815032646
    ],
    [
      0.4257299252731732,
      0.4340552510885316,
      0.4913547803207934,
      0.4725465121475554,
      0.33973267217326963,
      0.4638543654924734,
      0.4419858587183483,
      0.42753162019759117,
      0.4952490184784424,
      0.41781173476093403,
      0.43922611889300733,
      0.3416146092082224,
      0.46536665316143,
      0.4676653490202818,
      0.4341153874364492,
      0.40155215231155084,
      0.481774465315167,
      0.434276313359907,
      0.3616026434343813,
      0.4364223745891205,
      0.450572719879923,
      0.45761190453992584,
      0.4655004770744773,
      0.3516976432891843,
      0.0,
      0.4860895018661988,
      0.46979780126521886,
      0.3129719259900172,
      0.2945933536959706
    ],
    [
      0.4863414388928631,
      0.5232773266751876,
      0.5029872959562545,
      0.5052325040120353,
      0.4070310950354794,
      0.5010024182043629,
      0.49038863580209924,
      0.5524194537831699,
      0.49720528150776033,
      0.43877026475969383,
      0.48331601390562806,
      0.33512391065652003,
      0.6033511887047358,
      0.5314155834586252,
      0.5143687348392461,
      0.46829461230592506,
      0.520281759851654,
      0.5166120274607757,
      0.438543985835687,
      0.49109990180657115,
      0.5156454822217349,
      0.5105303213023376,
      0.5147931933237957,
      0.4039768919147013,
      0.47001214697950067,
      0.0,
      0.4613382105104453,
      0.3707944639444043,
      0.33539735350222144
    ],
    [
      0.4046192104094559,
      0.4763490142598621,
      0.46140963669105606,
      0.44326029551311774,
      0.38328508879386813,
      0.4669398792611934,
      0.43208958223858707,
      0.4454411082459242,
      0.45952837682402703,
      0.3873645685875162,
      0.4526680622048198,
      0.2791469688039059,
      0.43964049955376594,
      0.4456445841191705,
      0.42999918260707637,
      0.44831878249133994,
      0.47306243865318875,
      0.4719000516334908,
      0.46448672996500484,
      0.45937281192083357,
      0.4289483651086865,
      0.4359942926839804,
      0.4522409819854756,
      0.37416963859226304,
      0.467542984314937,
      0.45021067179257,
      0.0,
      0.36458680391610265,
      0.31845764682521116
    ],
    [
      0.4173669201831136,
      0.37110481438586995,
      0.3878518306262766,
      0.3461194209417273,
      0.49227035293679355,
      0.3315357496663738,
      0.35804895699332917,
      0.37947733801486283,
      0.4347893576236985,
      0.3175764586116132,
      0.3908189429701825,
      0.2854020684776668,
      0.3376560833551965,
      0.3152000836539943,
      0.3873560201678923,
      0.36319790319520884,
      0.41844116267479325,
      0.3636151982072846,
      0.342156184740144,
      0.4063504249517702,
      0.37629161011467405,
      0.35630862796850793,
      0.3120147153846624,
      0.4590243220155783,
      0.3266114032258305,
      0.342349217338183,
      0.3409165516894084,
      0.0,
      0.3434999774562497
    ],
    [
      0.35069142870675285,
      0.36508083605187713,
      0.40218152477960833,
      0.38439413750603024,
      0.403198601925725,
      0.3666779044986419,
      0.36198792432152915,
      0.3699371191642289,
      0.4087296473735793,
      0.3748204474636223,
      0.358860581530559,
      0.4087886987161353,
      0.35175443490331904,
      0.332957651332227,
      0.3849740128750907,
      0.3532820104626573,
      0.3820239061453672,
      0.3677666154873871,
      0.3588395431923277,
      0.36629191461094046,
      0.3896717189980088,
      0.4552199395473635,
      0.36884473475488,
      0.3846040838660685,
      0.3526447934255512,
      0.3067799626536567,
      0.4094825431393374,
      0.36142851186616887,
      0.0
    ]
  ],
  "row_avgs": [
    0.27601728130426256,
    0.4916800388454937,
    0.43783102863325307,
    0.4656670169571693,
    0.396081172182917,
    0.4363535813493287,
    0.42756744823388815,
    0.4429628736625047,
    0.47387661314039775,
    0.3335517765299471,
    0.5360727798690571,
    0.2779959652226306,
    0.48171655480597847,
    0.45292374571167515,
    0.4982477899746734,
    0.4707820502680492,
    0.49559533989227683,
    0.531553475971773,
    0.4766508602726724,
    0.40182296054862016,
    0.5387357962060229,
    0.4309329350527862,
    0.4284875661648937,
    0.4271675824434041,
    0.42722511189219814,
    0.47819826775547913,
    0.4291670806427296,
    0.3679768463418173,
    0.3743541153320943
  ],
  "col_avgs": [
    0.45702081793527494,
    0.4882269450833596,
    0.48749709750725084,
    0.4676867031760926,
    0.4155857343541366,
    0.47951330308412954,
    0.42575296496964016,
    0.4862822574989071,
    0.49776366307599,
    0.39836763851455065,
    0.4621873707890588,
    0.28696011688215517,
    0.4641291462750699,
    0.4537505021498362,
    0.4886530486550562,
    0.45289538406831165,
    0.4957731089863451,
    0.4701862594853828,
    0.4207811557228826,
    0.4509831274028074,
    0.45277467196933957,
    0.4570578481107281,
    0.4248307709479671,
    0.38877546998473816,
    0.4098211011414637,
    0.43039693248629135,
    0.4149797862318701,
    0.35768000067172967,
    0.3208827280476285
  ],
  "combined_avgs": [
    0.3665190496197688,
    0.48995349196442667,
    0.462664063070252,
    0.46667686006663095,
    0.4058334532685268,
    0.45793344221672916,
    0.4266602066017642,
    0.4646225655807059,
    0.4858201381081939,
    0.3659597075222489,
    0.49913007532905795,
    0.2824780410523929,
    0.4729228505405242,
    0.45333712393075565,
    0.4934504193148648,
    0.4618387171681804,
    0.4956842244393109,
    0.5008698677285779,
    0.4487160079977775,
    0.42640304397571377,
    0.4957552340876812,
    0.44399539158175716,
    0.4266591685564304,
    0.4079715262140711,
    0.4185231065168309,
    0.45429760012088527,
    0.42207343343729986,
    0.36282842350677347,
    0.3476184216898614
  ],
  "gppm": [
    549.6541862806856,
    552.1284959333838,
    549.0299788965854,
    558.1250584393312,
    580.9111430550446,
    554.7461401302704,
    575.54207505833,
    550.6787710945096,
    543.3403415253271,
    586.834339280283,
    563.8082507409944,
    640.11380171933,
    564.0414814420177,
    567.5156575685896,
    551.5176675492856,
    567.3164461891089,
    543.100902794592,
    558.5477004801448,
    580.8244352582603,
    563.9180983910021,
    565.503533726153,
    567.3613059146033,
    580.8231644236954,
    592.6488051020079,
    587.1345362966227,
    576.4611735302419,
    586.3855797596146,
    608.7375268789987,
    628.7108894456741
  ],
  "gppm_normalized": [
    1.218295564631308,
    1.280507999600996,
    1.2827322689496061,
    1.2919006354639984,
    1.3448171334299073,
    1.2836621468643812,
    1.3354603189331478,
    1.267287313235851,
    1.2557861065599334,
    1.3542358962335481,
    1.297251300969447,
    1.476023945982941,
    1.3050516167687813,
    1.3092568952268202,
    1.273253136734797,
    1.3069669878142511,
    1.2539696720409823,
    1.2915714577071942,
    1.3376309234756592,
    1.3071078134019873,
    1.3012037805907517,
    1.305033576469781,
    1.3356701536178484,
    1.3643817938432241,
    1.354582130683614,
    1.3255366913000142,
    1.3511722389738607,
    1.4031966992159184,
    1.4442259185989432
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335,
    468,
    442,
    473,
    474,
    383,
    401,
    465,
    446,
    413,
    388,
    399,
    481,
    480,
    453,
    411,
    415,
    389,
    446,
    406,
    459,
    411,
    398,
    367,
    435,
    415,
    350,
    346,
    429,
    364,
    533,
    424,
    479,
    491,
    712,
    431,
    444,
    521,
    415,
    476,
    408,
    564,
    485,
    428,
    449,
    470,
    420,
    441,
    430,
    446,
    447,
    421,
    459,
    433,
    414,
    473,
    353,
    452,
    397,
    650,
    429,
    489,
    455,
    450,
    445,
    432,
    435,
    464,
    421,
    395,
    489,
    392,
    476,
    454,
    450,
    376,
    391,
    389,
    441,
    381,
    413,
    421,
    418,
    441,
    428,
    410,
    396,
    417,
    1943,
    416,
    433,
    407,
    650,
    411,
    434,
    376,
    416,
    422,
    408,
    502,
    428,
    414,
    429,
    420,
    352,
    404,
    352,
    439,
    417,
    355,
    372,
    425,
    401,
    534,
    404,
    461,
    349,
    539,
    434,
    448,
    462,
    435,
    468,
    471,
    457,
    428,
    459,
    392,
    455,
    380,
    417,
    420,
    425,
    418,
    379,
    391,
    487,
    375,
    426,
    378,
    397,
    363,
    392,
    405,
    407,
    337
  ],
  "response_lengths": [
    2768,
    2523,
    2646,
    2628,
    2501,
    2761,
    2757,
    2620,
    2576,
    2596,
    2278,
    2576,
    2209,
    2436,
    2373,
    2542,
    2333,
    2229,
    2306,
    2709,
    2128,
    2324,
    2151,
    2269,
    2071,
    2255,
    2380,
    2314,
    1884
  ]
}