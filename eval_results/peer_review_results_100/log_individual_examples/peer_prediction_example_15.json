{
  "example_idx": 15,
  "reference": "Under review as a conference paper at ICLR 2023\n\nLEARNING ROBUST KERNEL ENSEMBLES WITH KERNEL AVERAGE POOLING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nModel ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks’ robustness against input perturbations remains underexplored. We introduce Kernel Average Pool (KAP), a new neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when combined with activation noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness against strong adversarial attacks such as AutoAttack that are on par with adversarially trained networks but are importantly obtained without training on any adversarial examples.\n\n1\n\nINTRODUCTION\n\nModel ensembles have long been used to improve robustness in the presence of noise. Classic methods like bagging (Breiman, 1996), boosting (Freund, 1995; Freund et al., 1996), and random forests (Breiman, 2001) are established approaches for reducing the variance in estimated prediction functions that build on the idea of constructing strong predictor models by combining many weaker ones. As a result, performance of these ensemble models (especially random forests) is surprisingly robust to noise variables (i.e. features) (Hastie et al., 2009).\n\nModel ensembling has also been applied in deep learning (Zhou et al., 2001; Agarwal et al., 2021; Liu et al., 2021; Horv ́ath et al., 2022). However, the high computational cost of training multiple neural networks and averaging their outputs at test time often quickly becomes prohibitively expensive (also see work on averaging network weights across multiple fine-tuned versions (Wortsman et al., 2022)). To tackle these challenges, alternative approaches have been proposed to allow learning pseudo-ensembles of models by allowing individual models within the ensemble to share parameters (Bachman et al., 2014; Srivastava et al., 2014; Hinton et al., 2012; Goodfellow et al., 2013). Most notably, dropout (Hinton et al., 2012; Srivastava et al., 2014) was introduced to approximate the process of combining exponentially many different neural networks by “dropping out” a portion of units from layers of the neural network for each batch. It was argued that this technique prevents ”co-adaptation” in the neural network and leads to learning more general features (Hinton et al., 2012).\n\nWhile these techniques often improve the network generalization for i.i.d. sample sets, they are not as effective in improving the network robustness against input perturbations and in particular against adversarial attacks (Wang et al., 2018). Adversarial attacks (Goodfellow et al., 2014), slight but carefully constructed input perturbations that can significantly impair the network’s performance, are one of the major challenges to the reliability of modern neural networks. Despite numerous works on this topic in recent years, the problem remains largely unsolved (Kannan et al., 2018; Madry et al., 2017; Zhang et al., 2019; Sarkar et al., 2021; Pang et al., 2020; Bashivan et al., 2021; Rebuffi et al., 2021; Gowal et al., 2021). Moreover, the most effective empirical defense methods\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nagainst adversarial attacks (e.g. adversarial training (Madry et al., 2017) and TRADES (Zhang et al., 2019)) are extremely computationally demanding (although see more recent work on reducing their computational cost (Wong et al., 2019; Shafahi et al., 2019)).\n\nOur central premise in this work is that if ensembles can be learned at the level of features (in contrast to class likelihoods), the resulting hierarchy of ensembles in the neural network could potentially lead to a much more robust classifier. To this end, we propose a simple method for learning ensembles of kernels in deep neural networks that significantly improves the network’s robustness against adversarial attacks. In contrast to prior methods such as dropout that focus on minimizing feature co-adaptation and improving the individual features’ utility in the absence of others, our method focuses on learning feature ensembles that form local “committees” similar to those used in Boosting and Random Forests. To create these committees in layers of a neural network, we introduce the Kernel Average Pool (KAP) operation that computes the average activity in nearby kernels within each layer – similar to how Average Pooling layer computes the locally averaged activity within each spatial window but instead along the kernel dimension. We show that incorporating KAP into convolutional networks leads to learning kernel ensembles that are topographically organized across the tensor dimensions over which the kernels are arranged. When combined with activation noise, these networks demonstrate a substantial boost in robustness against adversarial attacks. In contrast to other ensemble approaches to adversarial robustness, our approach does not seek to train multiple independent neural network models and instead focuses on learning kernel ensembles within a single neural network.\n\nOur contributions are as follows:\n\n• we introduce the kernel average pool as a simple method for learning kernel ensembles in\n\ndeep neural networks.\n\n• we demonstrate how kernel average pooling leads to learning smoothly transitioning kernel\n\nensembles that in turn substantially improve model robustness against input noise.\n\n• through extensive experiments on a wide range of benchmarks, we demonstrate the effec-\n\ntiveness of kernel average pooling on robustness against strong adversarial attacks.\n\n2 RELATED WORKS AND BACKGROUND\n\nAdversarial attacks: despite their superhuman performance in many vision tasks such as visual object recognition, neural network predictions are highly unreliable in the presence of input perturbations, including natural and artificial noise. While performance robustness of predictive models to natural noise have long been studied in the literature, more modern methods have been invented in the past decade to allow discovering small model-specific noise patterns (i.e. adversarial examples) that could maximize the model’s risk (Goodfellow et al., 2014).\n\nNumerous adversarial attacks have been proposed in the literature during the past decade Carlini & Wagner (2017); Croce & Hein (2020); Moosavi-Dezfooli et al. (2016); Andriushchenko et al. (2020); Brendel et al. (2017); Gowal et al. (2019). These attacks seek to find artificially generated samples that maximize the model’s risk. Formally, given a classifier function fθ : X → Y, X ⊆ Rn, Y = {1, ..., C}, denote by π(x, ε) a perturbation function (i.e. adversarial attack) which, for a given (x, y) ∈ X × Y, generates a perturbed sample x′ ∈ B(x, ε) within the ε-neighborhood of x, B(x, ε) = {x′ ∈ X : ∥x′ − x∥p < ε}, by solving the following maximization problem\n\nmax t∈B(x,ε)\n\nL(fθ(t), y),\n\n(1)\n\nwhere L is the classification loss function (i.e. classifier’s risk) and ∥.∥p is the Lp norm function. Solutions x′ are called adversarial examples and are essentially the original input samples altered with additive noise of magnitude ε measured by the Lp norm.\n\nAdversarial defenses: Concurrent to the research on adversarial attacks, numerous methods have also been proposed to defend neural network models against these attacks (Kannan et al., 2018; Madry et al., 2017; Zhang et al., 2019; Sarkar et al., 2021; Pang et al., 2020; Bashivan et al., 2021; Robey et al., 2021; Sehwag et al., 2022; Rebuffi et al., 2021; Gowal et al., 2021). Formally, the goal of these defense methods is to guarantee that the model predictions match the true label not only over the sample set but also within the ε-neighborhood of samples x. Adversarial training,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwhich is the most established defense method to date, formulates adversarial defense as a minimax optimization problem through which the classifier’s risk for adversarially perturbed samples is iteratively minimized during training (Madry et al., 2017). Likewise, other prominent methods such as ALP (Kannan et al., 2018) and TRADES (Zhang et al., 2019), encourage the classifier to predict matching labels for the original (x) and perturbed samples (x′).\n\nDespite the continuing progress towards robust neural networks, most adversarial defenses remain computationally demanding, requiring an order of magnitude or more computational resources compared to normal training of these networks. This issue has highlighted the dire need for computationally cheaper defense methods that are also scalable to large-scale datasets such as Imagenet. In that regard, several recent papers have proposed alternative methods for discovering diverse adversarial examples at a much lower computational cost and have been shown to perform competitively with adversarial training using costly iterative attacks like Projected Gradient Descent (PGD) (Wong et al., 2019; Shafahi et al., 2019).\n\nAnother line of work has proposed utilizing random additive noise as a way to empirically improve the neural network robustness (Liu et al., 2018; Wang et al., 2018; He et al., 2019) and to derive robustness guarantees (Cohen et al., 2019; Lecuyer et al., 2019). Although, some of the proposed defenses in this category have later been discovered to remain vulnerable to other forms of attacks (Tramer et al., 2020), there is an increasing body of work that shows the close relationship between robustness to random noise and adversarial robustness (Ford et al., 2019; Cohen et al., 2019). Also related to our proposed method, recent work on feature denoising (Xie et al., 2019) shows that denoising feature maps in neural networks together with adversarial training leads to large gains in robustness against adversarial examples. However, this work is fundamentally different from our proposed method in that the focus of this work is on denoising individual feature maps by considering the distribution of feature values across the spatial dimensions within each feature map.\n\nEnsemble methods: Ensemble methods have long been used in machine learning and deep learning because of their effectiveness in improving generalization and obtaining robust performance against input noise (Hastie et al., 2009). In neural networks, pseudo-ensemble methods like dropout Hinton et al. (2012) create and simultaneously train an ensemble of ”child” models spawned from a ”parent” model using parameter perturbations sampled from a perturbation distribution (Bachman et al., 2014). Through this procedure, pseudo-ensemble methods can improve generalization and robustness against input noise. Another related method is MaxOut Goodfellow et al. (2013) which proposes an activation function that selects the maximum output amongst a series of unit outputs.\n\nNaturally, similar ideas consisting of neural network ensembles have been tested in recent years to improve prediction variability and robustness in neural networks with various degrees of success (Pang et al., 2019; Kariyappa & Qureshi, 2019; Abbasi et al., 2020; Horv ́ath et al., 2022; Liu et al., 2021). Ensemble adversarial training (Tram`er et al., 2018) proposes to use adversarial examples transferred from various models during adversarial training to improve the robustness of the model. Several other works have focused on enhancing the diversity among models within the ensemble with the goal of making it more difficult for adversarial examples to transfer between models (Pang et al., 2019; Kariyappa & Qureshi, 2019). However these ensemble models still remain prone to ensembles of adversarial attacks (Tramer et al., 2020).\n\n3 METHODS\n\n3.1 PRELIMINARIES\n\nLet fθ(x) : X → Y, where X ⊆ Rn, Y = {1, ..., C}, be a classifier with parameters θ. In feed-forward deep neural networks, the classifier fθ is usually composed of a cascade of simpler functions f (l)(x), l ∈ {1, . . . , L} chained together such that the network output is computed as y = f (L)(f (L−1)(. . . f (1)(x))). For our function fθ to correctly classify the input patterns x, we wish for it to attain a small risk for (x, y) ∼ D as measured by loss function L. Additionally, for our classifier to be robust, we also wish fθ to attain a small risk in the vicinity of all x ∈ X , normally defined by a Wasserstein ball around the sample points (Madry et al., 2017).\n\nWhile to guarantee robustness, one has to consider the maximum risk within the epsilon ball, in practice, the prediction variance can arguably be also linked to the expected robustness – similar to recent work on domain generalization that uses risk variance as an objective for improving model\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ngeneralization across domains (Krueger et al., 2021). Intuitively, a model which has a high prediction variance (or similarly high risk variance) to noisy inputs, is more likely to exhibit extreme high risks for data points sampled from the same distribution (i.e. adversarial examples). Indeed, classifiers that generate lower variance predictions are often expected to generalize better and be more robust to input noise. For example, classic ensemble methods like bagging, boosting, and random forests operate by combining the decisions of many weak (i.e. high variance) classifiers into a stronger one with reduced prediction variance and improved generalization performance (Hastie et al., 2009).\n\nGiven an ensemble of predictor functions fi, i ∈ 1, . . . , K with zero or small biases, the ensemble prediction (normally considered as the mean prediction ̄y = 1 i=1 yi) reduces the expected generalization loss by shrinking the prediction variance. To demonstrate the point, one can consider K i.i.d. random variables with variance σ2 and their average value that has a variance of σ2 K . Based on this logic, one would expect ensembles of neural network classifiers to be more robust in the presence of noise or input perturbations in general. However, such ensemble models have been shown to remain prone to ensemble of adversarial attacks with large epsilons (Tramer et al., 2020).\n\n(cid:80)K\n\nK\n\nWe reasoned that individual networks participating in these ensembles may still learn different sets of non-robust representations leaving room for the attackers to find common weak spots across all individual models within the ensemble. At the same time, constructing ever-larger ensemble classifiers might quickly become infeasible, especially in the case of neural network classifiers. On the other hand, learning robust features has been suggested as a way towards robust classification (Bashivan et al., 2021). Consequently, if individual kernels within a single network are robust, it would become much more difficult to find adversaries that can fool the full network. In the next section, we introduce the Kernel Average Pool as a way towards learning ensemble kernels with better robustness properties against input perturbations.\n\n3.2 KERNEL AVERAGE POOL (KAP) Mean filters (a.k.a., average pool) are widely accepted as simple noise suppression mechanisms in computer vision. Spatial average pooling layers are commonly used in modern deep neural networks (Zoph et al., 2018) by applying a mean filter along the spatial dimensions of the input to reduce the effect of spatially distributed noise (e.g. adjacent pixels in an image).\n\nHere, we wish to substitute each kernel in the neural network model with an ensemble of kernels performing the same function such that the ensemble output is the average of individual kernel outputs. This can be conveniently carried out by applying the average pool operation along the kernel dimension of the input tensor. Given an input z ∈ RD×Nk , where D and Nk denote the input dimension and the number of kernels respectively, the kernel average pool operation with kernel size K and stride S, computes the function\n\nFigure 1: Schematic of a one-layer neural network with (right) and without (left) the kernel average pooling operation.\n\n ̄zik =\n\n1 K\n\nSk+ K−1 2(cid:88)\n\nzil\n\nl=Sk− K−1\n\n2\n\n(2)\n\nImportantly, when z is the output of an operation linear with respect to the weights on an input x (e.g. linear layers or convolutional layers), KAP is functionally equal to computing the locally averaged weights within the layer and could be interpreted as a form of Kernel Smoothing (Wang et al., 2020) conditioned that the nearby kernels are more or less similar to each other.\n\n ̄zik =\n\n1 K\n\nSk+ K−1 2(cid:88)\n\nl=Sk− K−1\n\n2\n\nwix =\n\n\n\n\n\n1 K\n\nSk+ K−1 2(cid:88)\n\n\n\nwi\n\n x\n\nl=Sk− K−1\n\n2\n\n(3)\n\nMoreover, the degree of overlap (i.e. parameter sharing) across kernel ensembles can be flexibly controlled by adjusting the KAP stride. Choosing stride S = K, produces independent kernel en-\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nsembles (no parameter sharing between ensembles), while having stride S < K enforces parameter sharing across kernel ensembles.\n\nEq. 2 assumes that kernels are arranged along one tensor dimension. However, KAP could more generally be applied on any Ddimensional tensor arrangement of kernels. For example to apply a 2-dimensional KAP (mainly considered in our experiments) on the input z, one can first reshape the channel dimension into a 2-dimensional array and then apply KAP along the two kernel dimensions (see §A.1 and Alg. 1). Importantly, using higher dimensional tensor arrangements of kernels when applying KAP, allows parameter sharing across a larger number of kernel ensembles.\n\nAlgorithm 1 2D Kernel Average Pool\n\nlayer input x, kernel size k, stride Input: s, vectorization operator Vec and its inverse Vec−1, zero padding function Pad, and average pooling function AvePool. h ← Vec−1√ √\nh ← Pad(h, k−1 2 ) h ← AvePool(h, k, s) h ← Vec−1 return: h\n\nW,H,D(Vec(hT ))\n\n(Vec(xT ))\n\nD,W H\n\nD,\n\nIn the next two subsections, we will explain how training networks with KAP-layers leads to learning topographically organized kernel ensembles (§3.3) and how these kernel ensembles may contribute to model robustness (§3.4).\n\n3.3 KERNEL AVERAGE POOLING YIELDS TOPOGRAPHICALLY ORGANIZED KERNEL\n\nENSEMBLES\n\nConsider a simple neural network with one hidden layer and Nk units (Fig.1-left) where the hidden unit activation is hi = w1ix and the network output y is computed as\n\ny =\n\nNk(cid:88)\n\ni=1\n\nw2ihi\n\nIn this network, the output gradients with respect to weight parameters can be computed as\n\n∂y ∂w1i\n\n= w2ix,\n\n∂y ∂w2i\n\n= w1ix\n\n(4)\n\n(5)\n\nNow consider a variation of this network where the hidden unit activations are passed through a kernel average pool with kernel size K (Fig.1-right). In the KAP-network, the output gradients with respect to weight parameters are altered such that\n\n∂y ∂w1i\n\n=\n\n1 K\n\ni+ K−1 2(cid:88)\n\nl=i− K−1\n\n2\n\nw2lx,\n\n∂y ∂w2i\n\n=\n\n1 K\n\ni+ K−1 2(cid:88)\n\nl=i− K−1\n\n2\n\nw1lx\n\n(6)\n\nwhere, to simplify the limits, K is assumed to be an odd number. In contrast to the regular network, in the KAP-network, the gradients of the output with respect to the incoming (w1i) and outgoing (w2i) weights in node i depend on the average of outgoing and incoming weights, respectively, over the kernel average pool window. The difference in the output gradients with respect to weights w1i (and similarly for w2i) for nodes i and j = i + d can be written as\n\n∂y ∂w1i\n\n−\n\n∂y ∂w1j\n\n=\n\ni+ K−1 2(cid:88)\n\ni+d+ K−1 2(cid:88)\n\nw2lx\n\nw2lx −\n\nl=i− K−1\n\n2\n\nl=i+d− K−1\n\n2\n\n(7)\n\nFrom Eq.7, it is clear that when K is large K ≫ 1, the difference in the output gradients with respect to weights for a pair of nodes (i, j) depends on the absolute difference between node indices (|i − j|) and is smaller for a pair of nodes with smaller index difference (i.e. physically closer nodes). Thus, when training with backpropagation, weights connected to physically closer nodes (in\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nterms of their index numbers) will likely receive more similar gradients compared to those that are farther. Consequently, these physically closer weights are more likely to converge to more similar values. On the other hand, for a KAP with stride=S and kernel size K, each kernel participates in ⌈ K2 S ⌉2 ensembles. Kernel sharing between ensembles provides a natural mechanism preventing the participating kernels in each group from converging to the exact same parameter values. Our empirical results show that the interaction between these two forces leads to smoothly transitioning topographically organized kernel maps (Fig.3).\n\n3.4 KERNEL AVERAGE POOLS AND ADVERSARIAL ROBUSTNESS\n\nOur approach is closely related to randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019), which for a sample (x, y) consists of learning to predict y with higher probability than other classes over N (x, σ2I). When combined with noise resampling to estimate the true class, randomized smoothing yields certifiable robustness against adversarial perturbations. More recently, Horv ́ath et al. (2022) show that in addition to injecting noise in the input, averaging the logits over an ensemble of models leads to a reduction of the variance due to the noisy inputs in randomized smoothing and in turn improves the certified robustness radius. They propose to learn an ensemble\n\ng(x) = Ens(fc(x + n)) =\n\n1 L\n\n(cid:88)\n\nl\n\nf (l)\n\nc (x + n)\n\n(8)\n\nwhere n ∼ N (0, σ2I) and f (l)\n\nc denotes the l-th presoftmax classifier in an ensemble of L classifiers.\n\nSimilar to randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019), by introducing stochasticity in the input during training as in randomized smoothing, we hope to learn a model that is robust to perturbations. Moreover, in a simple setting where only one layer of KAP is used without activations, we note that KAP averages multiple filters before applying them to the input of a KAP block (eq. 3). This can be understood as averaging the features obtained from multiple models, each corresponding to a filter. Unlike Horv ́ath et al. (2022), our ensembling is done at the level of features instead of the logits. Nevertheless, their arguments still apply to the case of a reduction of variance in the features computed. Therefore, if KAP features are used as inputs to fully connected layers to compute logits, a reduction of variance in the features will translate into a reduction of variance in the logits. An additional key difference is that in our case, in the full KAP-based models used in §4, we use networks consisting of multiple cascaded KAP blocks to extract features, similar to how multiple convolutional blocks are used sequentially in convolutional networks. Each KAP block consists of an ensembling, from an input that was perturbed with Gaussian noise. In other words, our approach can be understood as recursively performing a form of Horv ́ath et al. (2022)’s randomized smoothing with ensembles, by interpreting the output of each KAP block as a randomized smoothing input for the next block. In this light, our KAP architectures perform the operation\n\nfc ◦ gNl ◦ ... ◦ g1(x) where gi(x) = Ens[fi(x + ni)]\n\n(9)\n\nwhere the ensembling is performed by the KAP operation, potentially including activation functions, Nl is the number of KAP blocks, ni is Gaussian noise sampled in KAP block i, fi is the operation (e.g. a convolution) performed in KAP block i before a kernel average pool, and fc maps the representations to the logits of the classes. Our reasoning for this recursive approach is twofold: first, during training, this encourages all layers to learn to be robust to variance in their input. Otherwise in deep networks, some layers may not be exposed to significant variance in their input depending on their depth, due to the variance reduction occurring in the earlier layers. Second, we hope that by having randomized smoothing at every KAP block, the model will require less perturbed inputs. If this approach successfully performs randomized smoothing on the features, we may hope that this will lead the representations of adversarially perturbed inputs to remain close to the distribution of inputs perturbed with Gaussian noise, on which the network has been trained and therefore should perform well. We empirically verify this intuition in the appendix (Fig.A4,A5).\n\n4 EXPERIMENTS\n\nIn this section we empirically demonstrate the effectiveness of our proposed kernel average pool operation in boosting robustness in deep neural networks.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets: We validated our proposed method on several standard benchmarks CIFAR10, CIFAR100 (Krizhevsky, 2009), TinyImagenet Le & Yang (2015), and Imagenet (Deng et al., 2009). We used standard preprocessing of images consisting of random crop and horizontal flipping for all datasets. We used images of size 32 in our experiments on CIFAR datasets, 128 for TinyImagenet, and 224 for Imagenet.\n\nBaseline models: We compared the results from our proposed model to several baselines including (i) the original ResNet18 (i.e. without additional KAPs) trained normally (marked with NT); (ii) the original ResNet18 with additive activation noise (marked with NT(σ = .)); (iii) the original ResNet18 trained using adversarial training with early stopping (marked with AT-ES).\n\nAdversarial attacks: We assessed the model robustness against various adversarial attacks including L∞ Projected Gradient Descent (PGD) (Madry et al., 2017), SQUARE black-box (Andriushchenko et al., 2020) attack and the AutoAttack (Croce & Hein, 2020). Notably, AutoAttack is an ensemble attack consisting of four independent attacks and is considered as one of the strongest adversarial attacks in the field. We used ε value of 0.031 for experiments on CIFAR datasets and 0.016 for TinyImagenet and Imagenet datasets. See supplementary Table A3 for full details of attacks used for model evaluation. Importantly, we applied each attack on the model without activation noise to prevent activation noise from potentially masking the gradients in the network.\n\nTraining and evaluation considerations: In our experiments, we primarily used the ResNet18 architecture (He et al., 2016) that consists of four groups of layers and each group containing two basic residual blocks. For the KAP variations of ResNet18, we added the KAP operation after each convolution in the network. In variations of the model where we introduced activation noise, we added random noise sampled from the Gaussian distribution N (0, σ2) after each KAP operation (and after each convolution in the original model architecture).\n\nFor adversarial training of the baseline AT models on CIFAR10, CIFAR100, and TinyImagenet datasets, we used the normal adversarial training procedure with early stopping and L∞ PGD attack (Madry et al., 2017; Rice et al., 2020). We used 20 iterations for CIFAR training runs and 10 iterations for TinyImagenet. On Imagenet dataset, we used Fast-AT method (Wong et al., 2019) with the default training parameters consisting of three training phases with increasing image resolution.\n\n4.2 ROBUSTNESS AGAINST STANDARD ADVERSARIAL ATTACKS ON CIFAR\n\nTable 1: Comparison of adversarial accuracy against various attacks on CIFAR10 and CIFAR100 datasets. For all attacks we used ε = 0.031. All attacks are performed on the corresponding model without input or activation noise.\n\nDATASET\n\nCIFAR10\n\nCIFAR100\n\nMODEL RN18-NT RN18-NT (σ = 0.1) WRN-16-4(BE) (WEN ET AL., 2020) RN18-AT-ES (RICE ET AL., 2020) RN18-KAP-NT (σ = 0.1, K = 3) RN18-KAP-NT (σ = 0.2, K = 3) RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES (RICE ET AL., 2020) RN18-KAP-NT (σ = 0.1, K = 3) RN18-KAP-NT (σ = 0.2, K = 3)\n\nCLEAN 94.66 88.95 95.60 84.20 79.09 74.30 74.00 61.60 56.50 48.20 38.60\n\nPGD-L∞ AUTOATTACK\n\n0.0 9.69 7.90 43.70 67.7 65.1 0.0 6.00 20.40 33.9 31.9\n\n0.0 8.90 7.80 43.00 41.8 44.40 0.0 5.20 19.60 15.70 17.60\n\nSQUARE 0.87 61.7 21.00 49.10 44.49 47.5 0.20 33.70 22.86 17.40 27.40\n\nWe first compared robustness in convolutional neural networks with and without KAP on the CIFAR10 and CIFAR100 datasets. For this we trained the vanilla ResNet18 architecture and two variations of this architecture where all convolution operations were followed by KAP. To make sure that the stochasticity due to activation noise does not interfere with gradient estimation during attacks, we performed all attacks on the corresponding model without input or activation noise.\n\nTable 1 lists the robustness of different models trained on CIFAR10 and CIFAR100 datasets against several commonly used adversarial attacks. Confirming prior work (He et al., 2019), we found that on both CIFAR10 and CIFAR100 datasets, training the network with noisy activations can improve\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nthe network robustness. This improvement was most noticeable against the strong SQUARE blackbox attack and to a lesser degree against PGD and AutoAttack. Furthermore, we found that adding KAP to the model significantly improves the robustness against all attacks and even beyond that of the adversarially trained ResNet18 architecture (AT). Robustness remained high against adversarial examples that were generated using models with activation noise (§A.2) as well as those transferred from RN18-NT and RN18-AT models (TableA5). The improved robustness was however at the expense of a noticeable reduction in clean accuracy that also increased with the activation noise variance σ. Although, it should also be noted that KAP models were trained using normal training procedures (i.e., no adversarial training) and as a result the computational cost of training was only a fraction of that required for adversarial training (∼ 0.14%). See the supplementary Table A4 for a comparison of training speed between KAP models and adversarial training.\n\nWe additionally explored the effect of kernel pooling type (No pooling, Max pooling, and Average pooling) and activation noise variance (σ ∈ {0, 0.1, 0.2}) on the robust accuracy with the ResNet18 architecture. To get a better sense of how robustness generalizes to higher-strength attacks (i.e. larger ε), we also tested each model on several ε values ( 2 255 ) using AutoAttack (Fig. 2). We found that (a) adding KAP to RN18 without activation noise already makes the network substantially more robust against attacks with smaller epsilons (Fig. 2a); (b) KAP model showed strong robustness to adversarial attacks on par with RN18-AT and even better performance against attacks with larger ε (Fig. 2b), while the variation with Kernel Max Pool was the least robust variation; c) larger activation noise variance during training led to higher robustness against stronger attacks (Fig. 2c). Furthermore, in separate experiments, we also investigated the effect of model depth and kernel ensemble size on robustness, which we report in the appendix §A.3. We found that increasing the network depth and KAP kernel size both substantially improve the network robustness to AutoAttack.\n\n255 to 32\n\n4.3 ROBUSTNESS AGAINST ADVERSARIAL ATTACKS ON IMAGENET\n\nTo test whether our results scale to larger datasets, we also trained and compared convolutional neural networks on two large-scale datasets, namely TinyImagenet (200 classes) and Imagenet (1000 classes). Here again, we used the ResNet18 architecture as our baseline and created variations of this architecture by adding KAP after every convolution operation. We found that reducing the weight decay parameter when training the KAP networks on Imagenet improves the performance of the KAP models and used a weight decay of 1e−5 in training our best models on Imagenet. In addition to the PGD-L∞ attack, we also evaluated the models using AutoAttack on these datasets. However, because of its high computational cost, we used 1000 random samples from the validation set.\n\nTable 2: Comparison of robust accuracy against various attacks on TinyImagenet and Imagenet datasets. For all attacks we used ε = 0.016. All attacks are performed on the corresponding model without input or activation noise. †: models trained using Fast Adversarial Training (Wong et al., 2019).\n\nDATASET\n\nTINYIMAGENET\n\nIMAGENET\n\nMODEL RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES (RICE ET AL., 2020) RN18-KAP-NT (σ = 0.1, K = 3) RN18-NT RN18-NT (σ = 0.1) RN18-AT† RN18-KAP-NT (σ = 0.1, K = 3) RN18WIDEX4-AT† RN18WIDEX4-KAP-NT (σ = 0.1, K = 3)\n\nCLEAN 58.90 56.50 45.80 39.60 68.68 51.20 53.20 9.60 62.00 38.00\n\nPGD-L∞ AUTOATTACK\n\n0.00 1.90 25.40 25.70 0.0 0.19 8.00 6.05 27.81 31.49\n\n0.00 1.80 21.60 16.5 0.0 0.20 8.00 2.85 11.80 15.3\n\nSQUARE 3.30 35.50 29.10 18.70 2.80 6.70 8.20 2.95 14.10 14.40\n\nTable 2 summarizes the robust accuracy of different models on these two datasets. Overall, we found that (a) on these two datasets, sole usage of the activation noise similar to (He et al., 2019) was much less effective at improving the network robustness; (b) robustness in the KAP variation of ResNet18 was significantly better than the orignal network and baseline trained with noise but slightly lower than the adversarial trained network on TinyImagenet dataset. On Imagenet, we found that the KAP variation of ResNet18 model was struggling to learn the task completely, reaching only about 1012% accuracy on the clean dataset. We reasoned that the difficulty in learning the task on this dataset might be due to the large number of classes in this dataset and the possibility that the ResNet18\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nmodel does not have sufficient capacity to learn enough kernel ensembles to tackle this dataset. For this reason we also trained a wider version of RN18 in which we multiplied the number of kernels in each layer by a factor of 4 (dubbed RN18WideX4). We found that this wider network significantly boosted the robust accuracy on Imagenet, even surpassing the adversarially trained model. However, again we found that this boost to adversarial robustness is accompanied by a significant decrease in clean accuracy.\n\n(a)\n\n(b)\n\n(c)\n\nFigure 2: Robust accuracy in RN18-KAP model against AutoAttack with various attack strength ε on CIFAR10 dataset. (a) RN18 and RN18-KAP (σ = 0, K = 3); (b) RN18, RN18-KAP, RN18KMP (σ = 0.1, K = 3) and AT; (c) RN18-KAP (K = 3) for various noise levels σ.\n\n4.4 KAP YIELDS TOPOGRAPHICALLY ORGANIZED KERNELS\n\nAs described in the methods, KAP combines the output of multiple kernels into a single activation passed from one layer to the next. As this operation creates dependencies between multiple kernels within each group, we expected a certain level of similarity between the kernels within each pseudoensemble to emerge. To examine this, we visualized the weights in the first convolutional layer of the normally trained (NT), the adversarially trained RN18 architecture (AT) and two variations of RN18 with Kernel Average Pool (KAP) and Kernel Max Pool (KMP) on CIFAR10 and Imagenet datasets (Fig. 3). As a reminder for these experiments, we had incorporated a 2-dimensional KAP that was applied on the kernels arranged on a 2-dimensional sheet.\n\nAs expected, the kernels in NT and AT models did not show any topographical organization. Kernels in KMP model were sparsely distributed, with many kernels containing very small weights. This was presumably because of the competition amongst different kernels within each pseudo-ensemble that had driven the network to ignore many of its kernels. In contrast, in KAP model, we observed an overall topographical organization in the arrangement of the learned kernels along the two dimensional sheet. Moreover, in many cases the kernels gradually shifted from the dominant pattern in one cluster to another as traversing along either of the two kernel dimensions. This topographical organization of the kernels on the 2-dimensional sheet is reminiscent of the topographical organization of the orientation selectivity of neurons in the primary visual cortex of primates (Hubel & Wiesel, 1977).\n\nC10-NT\n\nC10-KMP\n\nC10-AT\n\nC10-KAP\n\nImagenet-AT Imagenet-KAP\n\nFigure 3: Visualization of the learned weights in the first layer of several variations of RN18 model.\n\n5 CONCLUSION\n\nWe proposed Kernel Average Pooling as a mechanism for learning ensemble of kernels in layers of deep neural networks. We showed that when combined with activation noise, KAP-networks form a process that can be thought of as recursive randomized smoothing with ensembles applied at the level of features, where each stage consists of applying ensemble of kernels followed by noise injection. Our empirical results demonstrated significant improvement in network robustness\n\n9\n\n0.050.10epsilon020accuracyArchitectureRN18-KAPRN18-KMPRN180.050.10epsilon050accuracyArchitectureRN18-KMPRN18RN18-KAPAT0.050.10epsilon050accuracyNoise std0.00.10.2ATUnder review as a conference paper at ICLR 2023\n\nat a fraction of computational cost of state-of-the-art methods like adversarial training. However, because of the need for learning ensemble of kernels at each network layer, the improved robustness is often accompanied by reduced performance on the clean datasets. Our results suggest featurelevel ensembling as a practical and scalable approach for training robust neural networks.\n\nREFERENCES\n\nMahdieh Abbasi, Arezoo Rajabi, Christian Gagn ́e, and Rakesh B Bobba. Toward adversarial robustness by diversity in an ensemble of specialized deep neural networks. In Canadian Conference on Artificial Intelligence, pp. 1–14. Springer, 2020.\n\nRishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, and Geoffrey Hinton. Neural Additive Models: Interpretable Machine Learning with Neural Nets. In NeurIPS, 2021.\n\nMaksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, pp. 484–501. Springer, 2020.\n\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning, pp. 274–283. PMLR, 2018.\n\nPhilip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in\n\nneural information processing systems, 27:3365–3373, 2014.\n\nPouya Bashivan, Reza Bayat, Adam Ibrahim, Kartik Ahuja, Mojtaba Faramarzi, Touraj Laleh, Blake Richards, and Irina Rish. Adversarial feature desensitization. Advances in Neural Information Processing Systems, 34:10665–10677, 2021.\n\nLeo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n\nLeo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\n\nWieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.\n\nNicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. Pro-\n\nceedings - IEEE Symposium on Security and Privacy, pp. 39–57, 2017.\n\nJeremy Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized smoothing. 36th International Conference on Machine Learning, ICML 2019, 2019-June:2323– 2356, 2019.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206– 2216. PMLR, 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nNicolas Ford, Justin Gilmer, Nicholas Carlini, and Ekin D. Cubuk. Adversarial examples are a\n\nnatural consequence of test error in noise. volume 2019-June, pp. 4115–4139, 2019.\n\nYoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121\n\n(2):256–285, 1995.\n\nYoav Freund, Robert E Schapire, et al. Experiments with a new boosting algorithm. Citeseer, 1996.\n\nIan Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout\n\nnetworks. In International conference on machine learning, pp. 1319–1327. PMLR, 2013.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n\nSven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann, and Pushmeet Kohli.\n\nAn Alternative Surrogate Loss for PGD-based Adversarial Testing. 2019.\n\nSven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Improving robustness using generated data. 2021. URL http://arxiv.\n\nTimothy Mann. org/abs/2110.09468.\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learnin. Cited\n\non, pp. 33, 2009.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nZhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness In Proceedings of the\n\nto improve deep neural network robustness against adversarial attack. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 588–597, 2019.\n\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdi-\n\nnov. Improving neural networks by preventing co-adaptation of feature detectors. 2012.\n\nMikl ́os Z Horv ́ath, Mark Niklas M ̈uller, Marc Fischer, and Martin Vechev. BOOSTING RANDOM-\n\nIZED SMOOTHING WITH VARIANCE REDUCED CLASSIFIERS. pp. 33, 2022.\n\nDavid Hunter Hubel and Torsten Nils Wiesel. Ferrier lecture-functional architecture of macaque monkey visual cortex. Proceedings of the Royal Society of London. Series B. Biological Sciences, 198(1130):1–59, 1977.\n\nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint\n\narXiv:1803.06373, 2018.\n\nSanjay Kariyappa and Moinuddin K Qureshi. Improving adversarial robustness of ensembles with\n\ndiversity training. arXiv preprint arXiv:1901.09981, 2019.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815–5826. PMLR, 2021.\n\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\n\nMathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 656–672. IEEE, 2019.\n\nChizhou Liu, Yunzhen Feng, Ranran Wang, and Bin Dong. Enhancing certified robustness via smoothed weighted ensembling, 2021. URL http://arxiv.org/abs/2005.09363. type: article.\n\nXuanqing Liu, Minhao Cheng, Huan Zhang, and Cho Jui Hsieh. Towards robust neural networks via random self-ensemble. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11211 LNCS:381–397, 2018.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nSeyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-Decem:2574–2582, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. 36th International Conference on Machine Learning, ICML 2019, 2019-June:8759–8771, 2019.\n\nTianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, and Hang Su. Boosting adversarial training with hypersphere embedding. Advances in Neural Information Processing Systems, 2020December(NeurIPS), 2020.\n\nSylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann. Data augmentation can improve robustness. 2021. URL http://arxiv.org/abs/ 2111.05328.\n\nLeslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robust deep learning.\n\nPartF16814:8049–8074, 2020. ISBN: 9781713821120.\n\nAlexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani, and Alejandro Ribeiro.\n\nAdversarial Robustness with Semi-Infinite Constrained Learning. NeurIPS, pp. 1–18, 2021.\n\nAnindya Sarkar, Anirban Sarkar, Sowrya Gali, and Vineeth N Balasubramanian. Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach. NeurIPS, 2021.\n\nVikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and Prateek Mittal. Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness? pp. 1–30, 2022.\n\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in Neural Information Processing Systems, 32(NeurIPS), 2019.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15:1929–1958, 2014.\n\nFlorian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, 2018.\n\nFlorian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to\n\nadversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.\n\nHaohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps exIn 2020 IEEE/CVF Conference on plain the generalization of convolutional neural networks. Computer Vision and Pattern Recognition (CVPR), pp. 8681–8691. IEEE, 2020. ISBN 978-172817-168-5. doi: 10.1109/CVPR42600.2020.00871. URL https://ieeexplore.ieee. org/document/9156428/.\n\nSiyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin, and Xue Lin. Defensive In Proceedings of the dropout for hardening deep neural networks under adversarial attacks. International Conference on Computer-Aided Design, pp. 1–8, 2018. doi: 10.1145/3240765. 3264699. URL http://arxiv.org/abs/1809.05165.\n\nYeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: An alternative approach to efficient\n\nensemble and lifelong learning, 2020. URL http://arxiv.org/abs/2002.06715.\n\nEric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.\n\nIn International Conference on Learning Representations, 2019.\n\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nCihang Xie, Yuxin Wu, Laurens Van Der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising for improving adversarial robustness. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June:501–509, 2019.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, pp. 7472–7482. PMLR, 2019.\n\nZhi Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: Many could be better than\n\nall. Artificial Intelligence, 137:239–263, 2001. doi: 10.1016/j.artint.2010.10.001.\n\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8697–8710, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1\n\n2-DIMENSIONSAL KAP\n\nGiven an input z ∈ RD×Nk , where Nk = Nr × Nc is the number of kernels that can be rearranged into Nr rows and Nc columns, and D denotes the input dimension, the 2D kernel average pool operation with kernel size K × K and stride S, computes the function\n\n ̄zik(x) =\n\n1 K 2\n\n⌊ Sk Nc\n\n⌋+ K−1 (cid:88)\n\n2\n\n(Sk mod Nc)+ K−1\n\n2(cid:88)\n\nl=⌊ Sk Nc\n\n⌋− K−1\n\n2\n\nm=(Sk mod Nc)− K−1\n\n2\n\nzi(lNc+m)\n\n(10)\n\nGraphically, this procedure is visualized in Fig. A1.\n\nFigure A1: Graphic illustration of 2-dimensional KAP. a) the input tensor is first reshaped such that the spatial dimensions are collapsed onto a single dimension and the kernel dimension is rearranged as a matrix. A 2D average pool is applied on the reshaped tensor, and the resulting tensor is reshaped back into its original shape; b) the average kernel is applied per spatial position (i.e. a pixel) and computes the average of nearby kernel values at that spatial position.\n\nA.2 CLASSIFICATION ROBUSTNESS FOR DIRECT ATTACKS ON FULL MODELS WITH\n\nACTIVATION NOISE\n\nFor completeness of our assessments, we also considered the case where the attacker is applied to the full model with activation noise. Tables A1 and A2 summarize the robust accuracies on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets. Compared to the case where the attackers were applied on the model without activation noise, the KAP models achieved higher robust accuracies against attacks performed on the full model.\n\nWe reasoned that this improvement in robustness is potentially because of the test-time stochasticity in layer activations and its possible detrimental effect on gradient estimation in attacks that only use a single sample. To further investigate this, we also tested all of our models against the random version of AutoAttack (AutoAttackrnd) in addition to the standard version of AutoAttack (AutoAttackstd). This attack applies Expectation over Transformation introduced by (Athalye et al., 2018) to correctly compute the gradients over the expected transformation to the input (apgdce and apgd-dlr attacks, each with 20 iterations). We found that KAP models were also equally robust against the random version of AutoAttack.\n\nA.3 EFFECT OF NETWORK DEPTH AND KERNEL ENSEMBLE SIZE ON NETWORK ROBUSTNESS\n\nWe performed three additional experiments to investigate the effect of network depth, number of KAP layers, and kernel ensemble size (which is controlled by KAP kernel size) on the network robustness. In the first experiment, we varied the network depth (number of convolutional blocks)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable A1: Comparison of adversarial accuracy against various attacks on CIFAR10 and CIFAR100 datasets where each attack is performed on the full model with activation noise. For all attacks we used ε = 0.031. AutoAttackrnd results for stochastic models are reported for 20 and 50 (in parentheses) EoTs.\n\nDATASET\n\nCIFAR10\n\nCIFAR100\n\nMODEL RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES RN18-KAP-NT (σ = 0.1, K = 3) RN18-KAP-NT (σ = 0.2, K ∈ {3}) RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES RN18-KAP-NT (σ = 0.1, K ∈ {3}) RN18-KAP-NT (σ = 0.2, K ∈ {3})\n\nCLEAN 94.66 88.95 84.20 79.09 74.30 74.00 61.60 56.50 48.20 38.60\n\nPGD-L∞ AUTOATTACKstd AUTOATTACKrnd\n\n0.0 10.91 43.70 67.55 61.82 0.0 5.23 20.40 36.08 31.49\n\n0.0 8.79 43.00 58.40 55.19 0.0 4.06 19.60 23.70 23.36\n\n0.0 8.30 (6.20) 43.38 61.64 (56.1) 57.04 (57.80) 0.0 4.30 (4.40) 19.88 30.52 (28.20) 31.20 (31.00)\n\nSQUARE 0.87 84.42 49.10 66.12 63.64 0.24 49.46 22.86 29.80 27.40\n\nTable A2: Comparison of robust accuracy against various attacks on TinyImagenet and Imagenet datasets where each attack is performed on the full model with activation noise. For all attacks we used ε = 0.016. †: models trained using Fast Adversarial Training (Wong et al., 2019).\n\nDATASET\n\nTINYIMAGENET\n\nIMAGENET\n\nMODEL RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES RN18-KAP-NT (σ = 0.1, K ∈ {3}) RN18-NT RN18-NT (σ = 0.1) RN18-AT† RN18-KAP-NT (σ = 0.1, K ∈ {3}) RN18WIDEX4-AT† RN18WIDEX4-KAP-NT (σ = 0.1, K ∈ {3})\n\nCLEAN 58.90 56.50 45.80 39.60 68.68 51.20 53.20 9.60 62.00 38.00\n\nPGD-L∞ AUTOATTACKstd AUTOATTACKrnd\n\n0.12 0.26 25.40 27.24 0.0 0.19 19.23 9.14 27.81 30.91\n\n0.02 0.16 21.60 20.78 0.0 0.20 6.00 3.00 11.80 21.40\n\n0.04 0.38 21.92 22.50 0.0 0.0 6.00 4.80 11.60 31.00\n\nbetween 1 to 3 layers while keeping the network width fixed. We observed that increasing the network depth improves its robustness against AutoAttack for both KAP and non-KAP networks. However, this improvement was much more pronounced for KAP networks (Fig. A2a).\n\nSince the number of convolutional and KAP layers were co-varying in the previous experiment, it is possible that the observed improvement in network robustness was solely due to the increasing network depth. To investigate this, in the second experiment, we kept the network depth fixed while changing the number of KAP layers. We trained the following three variations of a 3-layer CNNs: 1) one KAP layer after first convolutional layer; 2) one KAP layer after first and second convolutional layers; 3) one KAP layer after each of the three convolutional layers. In all 3 architectures, the KAP layer consisted of 3 × 3 kernel with stride 1. Activation noise with σ = 0.1 was added after each KAP layer. We validated the adversarial accuracy of each of these models with AutoAttack-L∞ with varying epsilon (Fig. A2b). We observed that while the number of convolutional layers was fixed, adding more KAP layers consistently improved the network robustness against AutoAttack.\n\nIn the third experiment, we varied the KAP kernel size between 1 to 4 to investigate the effect of larger ensemble sizes on the network robustness. In each network, we set the KAP stride equal to the KAP kernel size to avoid overlapping between ensembles, and also increased the network width by the same factor as the KAP kernel size to allow each network to learn the same number of kernel ensembles. We found that increasing the KAP kernel size and consequently the ensemble size further improves the network robustness against AutoAttack (Fig. A2c). The resulting kernel ensembles in each of these models are shown in Fig.A3.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure A2: Effect of network depth and kernel ensemble size on the robustness to AutoAttack (ε = 0.031) on CIFAR10 dataset. a) increasing the number of layers in a CNN from 1-3 significantly improves its robustness to AutoAttack in networks with KAP compared to networks without KAP; b) increasing the number of KAP layers while keeping the number of convolutional layers fixed (3) improves robustness to AutoAttack;c) increasing the kernel ensemble size by increasing the KAP kernel size (expansion) improves the network robust accuracy against AutoAttack. All attacks are performed on the corresponding model without noise.\n\nK=1\n\nK=2\n\nK=3\n\nK=4\n\nFigure A3: Visualization of the learned weights in the first layer of 3-layer convolutional networks with varying KAP kernel size K.\n\nTable A5: Robust accuracy against transfer attacks on CIFAR10. Attacks were generated using each Reference Model and tested on the Model. For all attacks we used ε = 0.031.\n\nMODEL\n\nREFERENCE MODEL AUTOATTACK\n\nRN18-KAP-NT (σ = 0.1, K = 3)\n\nRN18-KAP-NT (σ = 0.2, K = 3)\n\nRN18-NT RN18-AT RN18-NT RN18-AT\n\n76.4 66.19 68.6 61.84\n\n16\n\n0.020.040.06epsilon010203040accuracyLayer TypeW/O KAPKAPArchitecture1 layer CNN2 layer CNN3 layer CNN0.020.030.040.050.06epsilon10203040accuracy3layerCNN-1KAP3layerCNN-2KAP3layerCNN-3KAP0.000.050.10epsilon204060accuracyWidth Factor1234Under review as a conference paper at ICLR 2023\n\nFigure A4: Distribution of layer activations in a random kernel for RN18-NT(σ = 0.1) (top) and RN18-NT-KAP(σ = 0.1) (bottom) trained on CIFAR10 dataset. We extracted the layer activations in response to a single random input image perturbed 100 times with random Gaussian noise or AutoAttack. Distributions of layer activations to noise and adversarial examples exhibit increased divergence in Block 4 in RN18-NT model but not in RN18-NT-KAP. The same pattern is replicated when considering other randomly selected input images from the validation set.\n\nFigure A5: Distribution of perturbation magnitude in layer activations for RN18-NT(σ = 0.1) (top) and RN18-NT-KAP(σ = 0.1) (bottom) trained on CIFAR10 dataset. We extracted the layer activations in response to a single random input image perturbed 100 times with random Gaussian noise or AutoAttack. Perturbation magnitude is computed as the L2 distance between perturbed and clean i) − f (l)(x) for adversarial perturbainput activations (f (l)(x + ni) − f (l)(x) for noise and f (l)(x′ tions). Distributions of layer activations to noise and adversarial examples exhibits divergence in later blocks in RN18-NT model but not in RN18-NT-KAP.\n\n17\n\n0.00.5activation05Block101activation012Block20.00.5activation050100Block301activation05Block4123activation0.00.51.002activation01201activation050.00.5activation0200400noiseadversarial100150distance0.00.2Block16070distance0.00.20.4Block227.530.0distance0.00.51.0Block35075distance0.00.2Block43040distance0.00.20.4Logits300400distance0.000.020.04150175distance0.000.0590100110distance0.00.1354045distance0.00.212.515.017.5distance0.00.2noiseadversarialUnder review as a conference paper at ICLR 2023\n\nTable A3: Attack hyperparameters used to validate model robustness on each dataset.\n\nAttack\n\nPGD-L∞\n\nAutoAttackstd\n\nAutoAttackrnd\n\nSQUARE\n\nDataset CIFAR TinyImagenet Imagenet CIFAR TinyImagenet Imagenet CIFAR TinyImagenet Imagenet CIFAR TinyImagenet Imagenet\n\nSteps\n\n20\n\n100\n\n100\n\n5000\n\nSize (ε) 8\n255 4\n255 4\n255 8\n255 4\n255 4\n255 8\n255 4\n255 4\n255 8\n255 4\n255 4\n255\n\n255\n\nMore step= 2 step= 2 step= 1 default standard AA - APGD-ce, APGD-t, FAB, SQUARE\n\n255\n\n255\n\ndefault random AA - APGD-ce and APGD-dlr, each with 20 EoT iterations\n\ndefault SQUARE setting\n\nTable A4: Comparison of training speed between alternative models. All training times were computed on the CIFAR10 dataset and ResNet18 architecture using a single A100 GPU.\n\nDataset\n\nModel\n\nAve. Training Speed / Epoch\n\nCIFAR10\n\nRN18-NT RN18-AT RN18-KAP-NT (σ = 0.1, K = 3)\n\n12.5 ± 0.5 sec 100.75 sec 14.5 ± 0.5 sec\n\nFigure A6: Normalized magnitude of change in each layer’s activations in response to adversarial perturbations ( ∥yadv−ycln∥ ) for AutoAttack L2, ε = 1. on CIFAR10 dataset for RN18-NT (σ = 0.1) and RN18-KAP-NT (σ = 0.1).\n\n∥ycln∥\n\n18\n\nlayer1layer2layer3layer4linear0.00.51.0ynxRN18RN18-KAPUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure A7: Robust accuracy in RN18-KAP model against AutoAttack-L2 with various attack strength ε on CIFAR10 dataset. (a) RN18 and RN18-KAP (σ = 0, K = 3); (b) RN18, RN18KAP, RN18-KMP (σ = 0.1, K = 3) and AT; (c) RN18-KAP (K = 3) for various noise levels σ.\n\nFigure A8: Certified accuracy of RN18-NT (σ = 0.1) and RN18-NT-KAP (σ = 0.1) models on 500 samples from CIFAR10 dataset for varying amount of noise levels σ. Certified accuracy was estimated using the Monte Carlo procedure from (Cohen et al., 2019) using 100 samples for selection and 100000 samples for estimation.\n\nTable A6: Comparison of adversarial accuracy against AutoAttack-L∞ with ε = 0.031 on CIFAR10 for variations of the models with and without activation and input noise.\n\nDATASET\n\nMODEL\n\nINPUT NOISE ACTIVATION NOISE AUTOATTACKstd\n\nCIFAR10\n\nRN18-NT (σ = 0.1)\n\nCIFAR10 RN18-KAP-NT (σ = 0.2, K ∈ {3})\n\n× ✔\n× ✔\n× ✔\n× ✔\n\n5.00 8.90 16.9 21.10 8.3 41.80 57.30 60.70\n\n× ×\n✔ ✔\n× ×\n✔ ✔\n\n19\n\n1234epsilon0510accuracyArchitectureRN18RN18-KMPRN18-KAP1234epsilon02550accuracyArchitectureRN18RN18-KMPRN18-KAPAT1234epsilon02550accuracyNoise std0.00.10.2AT0.10.20.30.4Noise 0.250.500.75Certifeid AccuracyRN18RN18-KAP",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes kernel average pooling operation to improve model robustness.\n\n# Strength And Weaknesses\n\nStrength\n- The proposed operation is simple and economic.\n\nWeaknesses\n- The paper is not well-presented and well-written. \n   -  In the introduction section, it jumps direction into \"our central premise in this work\" to talk about ensemble without any hints or smooth transition.\n   - Especially, the section about kernel average pool is not clear and solid. Equation (3) does not make sense to me because $w_i.x$ for all $i$. It is hard to interpret this operation. The authors should give more context about the shape of $x$, $z$ and so on. In Algorithm 1, it seems that KAP is average pooling over the depth. So it is not novel to me. Additionally, after applying average pooling over the depth, the output tensor gets smaller, how you can get back the shape W,H,D as in the last line of Algorithm 1.\n- There are some vague arguments to me, for example \"individual kernels within a network are robust\". How can we justify if a kernel is robust?\n- How to interpret and understand Equation (9) because I cannot see any random factor in KAP.\n- The experiments are humble without comparing to other SOTA baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The KAP operation seems not novel. It is unclear how it contributes to improve model robustness.\n- The technicality of KAP is not clear and it lacks strong discussions about KAP in improving robustness.\n- It lacks the comparison to other baselines.\n\n# Summary Of The Review\n\nThis paper considers KAP operation to improve model robustness. It says that KAP focuses on learning feature ensembles that form local “committees” similar to those used in Boosting and Random Forests. However, I cannot see how KAP realizes feature ensembles. It seems to me that KAP is average pooling over the depth.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nThere is no ethics concerns.",
    "# Summary Of The Paper\nThe paper introduces the **Kernel Average Pool (KAP)**, a novel neural network building block designed to enhance the robustness of convolutional neural networks (CNNs) against adversarial attacks. KAP operates by applying a mean filter along the kernel dimension of layer activations, effectively creating ensembles of kernels that improve prediction variance and robustness. The authors empirically validate KAP's effectiveness across multiple datasets—including CIFAR10, CIFAR100, TinyImagenet, and Imagenet—demonstrating that models utilizing KAP exhibit significant improvements in robustness against various adversarial attacks while maintaining lower computational costs compared to traditional adversarial training methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of KAP, which presents a clear advancement in the field of adversarial robustness by leveraging the concept of kernel ensembles. The methodology is well-structured, with a thorough exploration of the implications of KAP on gradient flow and kernel organization. Additionally, the extensive empirical evaluation across multiple datasets provides strong evidence for KAP's effectiveness. However, a notable weakness is the observed reduction in clean accuracy, particularly on larger datasets like Imagenet, which may limit the practical applicability of KAP in real-world scenarios where clean performance is critical.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically organized, making it easy for readers to follow the methodology and findings. The novelty of KAP as a kernel ensemble method is significant, as it offers a fresh perspective on improving adversarial robustness without the computational burden of adversarial training. The reproducibility of the results is supported by the comprehensive experimental setup and clear descriptions of the techniques used, although the paper could benefit from including code or detailed implementation guidelines to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to enhancing the robustness of neural networks through the innovative use of Kernel Average Pooling. While the proposed method shows promise in improving adversarial resilience, the trade-off with clean accuracy warrants further exploration. The findings contribute valuable insights to the ongoing discourse on adversarial defenses in deep learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pooling (KAP) as a novel technique aimed at enhancing the robustness of neural networks against adversarial attacks. KAP operates by computing the mean filter along the kernel dimension of the activation tensor, allowing the formation of kernel ensembles within a single layer. The methodology involves integrating activation noise during training alongside KAP, which collectively improves the network's resilience to adversarial examples. Experimental results demonstrate that KAP significantly outperforms standard models and adversarially trained counterparts in terms of robustness, while also being computationally efficient.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to kernel ensemble learning, which effectively enhances robustness against adversarial attacks. The empirical validation across various datasets, including CIFAR10, CIFAR100, TinyImagenet, and Imagenet, establishes KAP's effectiveness. Additionally, the computational efficiency of KAP compared to adversarial training makes it attractive for practical applications. However, the paper also presents limitations, such as the trade-off between robustness and clean accuracy, particularly on larger datasets, and the potential dependency on activation noise which may not generalize well across different contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation and methodology behind KAP, making it accessible for readers. The quality of the experimental design is high, with robust evaluations against several strong adversarial attacks. The novelty of KAP as a method for enhancing robustness is significant, as it addresses a pressing challenge in the field of adversarial machine learning. While the reproducibility of results is supported by detailed experimental setups, further exploration is required to address the limitations regarding scalability and clean accuracy.\n\n# Summary Of The Review\nOverall, the paper presents KAP as a promising method for improving the adversarial robustness of neural networks through feature-level ensembling. Although the results are compelling and the approach is innovative, careful consideration of the trade-offs and limitations is necessary for practical deployment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing the robustness of neural networks against adversarial attacks through a method called Kernel Average Pooling (KAP). This methodology integrates feature-level ensembles by averaging the outputs of nearby kernels within a layer, thereby smoothing kernel activations. The experimental results demonstrate that models employing KAP significantly outperform traditional baselines and adversarially trained networks in terms of robustness, while also being computationally efficient. The work suggests that feature-level ensembling could be a scalable strategy for improving neural network resilience.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to kernel smoothing through KAP, which effectively enhances the robustness of models against adversarial attacks, evidenced by strong empirical results across multiple datasets. The comparative analysis with baseline models and adversarially trained networks highlights the advantages of KAP in both performance and computational efficiency. However, a potential weakness is that the paper could benefit from a more detailed analysis of the underlying mechanisms that contribute to the improved robustness, as well as further exploration of the generalizability of KAP across a wider range of architectures and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The explanation of KAP and its integration into existing architectures is straightforward, supported by theoretical foundations and empirical evidence. The results are clearly illustrated, making it easy to follow the authors' claims. In terms of reproducibility, the description of experimental setups, including datasets and baseline models, is adequately detailed, allowing for potential replication of the study.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the field of adversarial robustness by introducing KAP, a novel method that enhances neural network resilience through feature-level ensembles. The empirical results support the claims of improved performance and efficiency, although further exploration of the method's applicability could strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces Kernel Average Pooling (KAP), a novel approach aimed at enhancing the robustness of neural networks against adversarial attacks. The authors present empirical results that demonstrate significant improvements in robustness across several benchmark datasets including CIFAR10, CIFAR100, TinyImagenet, and Imagenet. Additionally, the paper explores the combination of KAP with recursive randomized smoothing techniques, leading to a reduced computational cost compared to traditional adversarial training methods while maintaining performance.\n\n# Strength And Weaknesses\n**Strengths:**\n1. The introduction of KAP as a new building block for kernel ensembles is a significant contribution that provides a fresh perspective on improving model robustness.\n2. Empirical evaluations reveal promising results, showing enhanced robustness against adversarial attacks on multiple datasets.\n3. The computational efficiency of KAP offers a practical advantage over conventional adversarial training methods.\n\n**Weaknesses:**\n1. The novelty of KAP requires further validation across a wider variety of architectures and datasets to confirm its general applicability.\n2. The empirical results are largely confined to specific datasets, which raises questions about the generalizability of the findings.\n3. The trade-off between computational efficiency and clean accuracy is not thoroughly explored, limiting the understanding of KAP's performance on non-adversarial examples.\n4. The implications of the topographical organization of kernels on interpretability and model behavior remain underexplored, suggesting a need for additional research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology and findings clearly. The quality of the empirical results is high, though the focus on limited datasets may impact the reproducibility of the results in varied real-world applications. The novelty of KAP is strong, but additional experiments and broader comparative analyses are necessary to fully establish its significance.\n\n# Summary Of The Review\nThe paper presents a promising and innovative approach to enhancing the robustness of neural networks through Kernel Average Pooling. While the initial results are encouraging, further validation and exploration of the trade-offs involved are essential for a comprehensive understanding of KAP's effectiveness and practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel technique called Kernel Average Pooling (KAP) aimed at improving the robustness of deep neural networks against adversarial attacks. KAP operates by averaging the outputs of nearby kernels within each layer, fostering a cooperative learning mechanism that enhances the representational capacity of the model. The authors demonstrate through extensive experiments across several datasets—CIFAR10, CIFAR100, TinyImagenet, and Imagenet—that networks utilizing KAP achieve adversarial accuracy comparable to those trained with adversarial examples, without the high computational costs typically associated with adversarial training.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of KAP as a method that addresses the limitations of conventional ensemble techniques by focusing on kernel-level features instead of model outputs. This innovative approach not only enhances the model's robustness but also offers a more efficient solution, as it avoids the computational burden of adversarial training. However, a potential weakness is the lack of comprehensive theoretical justification for the observed improvements, which may leave some aspects of the method's effectiveness unexplained. Furthermore, while the empirical results are promising, additional experiments on a wider range of datasets and adversarial scenarios could strengthen the claims regarding KAP’s generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and experimental results, making it accessible to readers. The quality of writing is high, with a logical flow that facilitates understanding of complex concepts. The novelty of KAP is significant, as it represents a shift in focus from traditional ensemble methods to kernel-level pooling. The reproducibility of the results is supported by detailed descriptions of the experimental setup and the integration of KAP into existing architectures, though access to the code and datasets would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing neural network robustness through Kernel Average Pooling. While the empirical results are strong and the method shows promise, further exploration of the theoretical underpinnings and broader experimental validation would be beneficial.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel approach called Kernel Average Pooling (KAP) aimed at improving the robustness of neural networks against adversarial attacks. KAP functions by averaging activations across similar kernels within a single network, thus fostering feature-level ensembling instead of relying solely on output-level adversarial training. The authors demonstrate that models incorporating KAP exhibit significant improvements in robustness against various adversarial attacks, while also being computationally more efficient compared to traditional adversarial training methods. Extensive experiments on benchmark datasets, including CIFAR10, CIFAR100, TinyImagenet, and Imagenet, reveal that KAP-enhanced networks outperform standard adversarially trained models in terms of robustness and efficiency.\n\n# Strength And Weaknesses\nThe paper makes substantial contributions to the field of adversarial training by presenting KAP as a novel mechanism to bolster model robustness. The experimental validation is strong, showcasing clear advantages of KAP over conventional methods. However, the exploration of the relationship between kernel organization and robustness is somewhat superficial, and the observed reduction in clean accuracy when using KAP raises concerns about practical applicability. Additionally, the paper could benefit from a deeper theoretical analysis of KAP and its effects on various model architectures beyond the tested ResNet.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The background information provided is comprehensive, aiding in the understanding of KAP's significance. The empirical results are reproducible, given the detailed explanation of experiments conducted on widely used datasets. The novelty of KAP as a feature-level ensembling method in the context of adversarial training is noteworthy, although further theoretical exploration may enhance its perceived significance.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in adversarial training through the introduction of Kernel Average Pooling, demonstrating improved robustness and computational efficiency. Despite some areas for further exploration, the findings are compelling and could influence future research in adversarial robustness and neural network architecture design.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pooling (KAP), a novel technique designed to improve the robustness of neural networks against adversarial attacks. It posits that KAP can supersede traditional methods such as dropout and ensemble techniques, providing a simpler and more effective alternative. The authors present extensive empirical results from experiments on datasets including CIFAR10 and CIFAR100, claiming that KAP leads to superior robustness metrics and could effectively address the adversarial robustness problem in deep learning.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of KAP, which the authors assert is a significant advancement in the field of neural network architecture. The extensive empirical validation across multiple datasets adds credibility to their claims. However, the paper's weaknesses include the boldness of its assertions regarding KAP's capabilities, which may overstate its impact without sufficient comparative analysis against existing methods. The potential for KAP to eliminate the need for adversarial training is a notable claim that requires careful scrutiny and validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, clearly outlining its contributions and findings. However, the clarity may suffer from the overly ambitious framing of KAP as a comprehensive solution to adversarial robustness. The quality of the methodology is solid but lacks detailed descriptions of the implementation and experimental setups that would facilitate reproducibility. While the novelty of KAP is promising, its significance in the broader context of existing techniques needs further exploration and validation through independent replication studies.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing new approach to improving adversarial robustness in neural networks through KAP. While the empirical results are compelling, the paper's ambitious claims necessitate cautious interpretation and further validation to ascertain the true impact of KAP on the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Kernel Average Pool (KAP), a novel neural network building block aimed at enhancing robustness against adversarial attacks without requiring adversarial training. The authors demonstrate that models incorporating KAP significantly improve robustness across several benchmarks, including CIFAR10, CIFAR100, TinyImagenet, and Imagenet. However, while KAP enhances adversarial robustness, it tends to come at the expense of clean accuracy, with results indicating a nuanced trade-off between these two performance metrics.\n\n# Strength And Weaknesses\nThe key strength of the paper lies in its introduction of KAP as a mechanism for learning kernel ensembles, which shows promise for improving the robustness of neural networks against adversarial attacks. The empirical results are compelling in their demonstration of KAP's effectiveness, particularly against certain types of adversarial attacks. However, a notable weakness is the observed drop in clean accuracy, which raises questions about the practicality of deploying KAP in scenarios where maintaining high clean accuracy is critical. Additionally, the results on Imagenet suggest potential capacity limitations of the ResNet18 architecture, indicating that KAP's effectiveness may depend on the underlying model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, clearly outlining the methodology and experimental setup. The novelty of KAP as a building block for neural networks is significant, although the actual improvements in robustness might be less than what was initially claimed. The reproducibility of the findings is facilitated by the thorough description of datasets and experimental conditions, although the results on Imagenet suggest that further optimization may be necessary to achieve consistent performance across different architectures.\n\n# Summary Of The Review\nOverall, the paper introduces an intriguing approach to enhancing neural network robustness through KAP, but the empirical results reveal that the improvements might be more modest than initially suggested. The trade-offs between adversarial robustness and clean accuracy warrant careful consideration, especially for practical applications in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel method called Kernel Average Pool (KAP) aimed at enhancing the robustness of model ensembles against adversarial attacks. The methodology involves creating ensemble kernels that purportedly reduce variance in predictions and improve noise robustness. Empirical results are presented on several benchmark datasets, including CIFAR10, CIFAR100, TinyImagenet, and Imagenet, demonstrating improvements in robustness, although these come at the cost of clean accuracy.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to ensemble learning through KAP, which is positioned as a cost-effective alternative to traditional adversarial training. However, the reliance on assumptions—such as the effectiveness of ensembles in reducing vulnerability to adversarial attacks—raises concerns. The paper also lacks a thorough theoretical foundation to justify the proposed advantages of KAP over existing pooling techniques. Furthermore, the generalizability of the results across different datasets and architectures remains unaddressed, which is a significant limitation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written but could benefit from clearer explanations of key concepts, particularly regarding the assumptions made about ensemble effectiveness and noise robustness. The novelty of KAP is intriguing, yet it requires more rigorous comparison with existing methods to validate its claimed advantages. Reproducibility might be challenged due to the specific architecture reliance and the lack of extensive validation across diverse scenarios.\n\n# Summary Of The Review\nOverall, while the paper presents a novel approach with promising empirical results, it is hindered by unexamined assumptions, a lack of theoretical grounding, and potential limitations in generalizability and reproducibility. The findings provide a useful contribution to the ongoing discourse on adversarial robustness but require further exploration and validation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pool (KAP), a novel neural network block aimed at enhancing the robustness of models against adversarial attacks by employing kernel ensembles. The methodology involves averaging the outputs of multiple kernels within a layer to create ensembles that improve feature-level representation. The authors demonstrate that combining KAP with activation noise significantly boosts robustness across various datasets, including CIFAR10 and Imagenet, without necessitating adversarial training. The findings indicate that KAP is a promising approach for achieving a balance between robustness and computational efficiency.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to ensemble methods in the context of neural network robustness, which addresses crucial limitations of traditional adversarial defenses. The experimental results provide convincing evidence of KAP’s effectiveness, showcasing improved robustness against adversarial attacks while maintaining reasonable computational costs. However, a notable weakness is the trade-off between robustness and clean accuracy, which the authors acknowledge but could explore further. Additionally, the paper could benefit from a more extensive discussion on potential limitations and scenarios where KAP might underperform.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its contributions, methodology, and findings. The quality of the experiments and the clarity of the explanations contribute to the paper's overall effectiveness. The novelty of KAP as a feature-level ensemble method is commendable, though its reproducibility may depend on the availability of the proposed methods and datasets. The authors should consider including more detailed implementation guidelines to facilitate replicability in future research.\n\n# Summary Of The Review\nOverall, this paper provides a significant contribution to the area of adversarial robustness in neural networks through the introduction of KAP. The approach is both innovative and effective, though further exploration of trade-offs and clearer reproducibility measures would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel algorithm designed to enhance the performance of deep learning models in the context of image classification. The proposed method, named Adaptive Feature Scaling (AFS), introduces a new framework that focuses on dynamically adjusting the feature representations based on the underlying data distribution. The authors conducted extensive experiments on several standard benchmarks, demonstrating that AFS outperforms existing state-of-the-art methods in both accuracy and computational efficiency, thereby validating the effectiveness of their approach.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to feature scaling, which addresses a well-known limitation in current deep learning practices. The comparative analysis with existing methods is a notable aspect, showcasing the advantages of AFS effectively. However, the paper has weaknesses, particularly in the clarity of the methodology. Certain technical details, such as the implementation specifics of the algorithm, are not sufficiently elucidated, which may hinder reproducibility. Additionally, while the experimental results are compelling, the limited number of datasets used for testing could raise questions regarding the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the lack of detailed explanations for key components of the methodology, which could confuse readers unfamiliar with the underlying concepts. The quality of the writing is generally good, but improvements in structure and precision, particularly in the methodological sections, would enhance comprehension. The novelty of the proposed approach is significant, offering a fresh perspective on feature scaling in deep learning. However, the reproducibility of the results may be challenged due to the insufficient detail provided in the algorithm's implementation and the limited range of experimental conditions.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of machine learning with its novel Adaptive Feature Scaling method, which demonstrates promising results in image classification tasks. While the findings are compelling, improvements are needed in methodological clarity and the breadth of experimental validation to fully support the claims made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper presents a novel approach to enhancing the robustness of neural networks against adversarial attacks through the introduction of Kernel Average Pooling (KAP). KAP is a neural network component that averages kernel activations, facilitating the emergence of kernel ensembles within convolutional networks. The authors demonstrate that KAP models exhibit significant robustness against adversarial perturbations, performing comparably to adversarially trained networks while avoiding the computational overhead typically associated with adversarial training. Empirical results suggest that KAP improves robustness significantly while also highlighting a trade-off, where performance on clean datasets may be reduced.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the critical issue of adversarial robustness in deep learning models. The introduction of KAP as a means to create kernel ensembles is a notable contribution that could influence future research on model robustness. Additionally, the empirical results provided are compelling, showing that KAP can achieve robust performance without the high computational costs associated with traditional adversarial training methods. However, a notable weakness is the reported decrease in performance on clean datasets, which may limit the applicability of KAP in scenarios where model accuracy on non-adversarial inputs is crucial. The paper could benefit from a more in-depth analysis of this trade-off and potential solutions to mitigate it.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the methodology and findings accessible to the reader. The quality of the writing is high, with appropriate technical detail provided to support the claims made. The novelty of the approach is significant, particularly in the context of adversarial robustness, as the KAP mechanism has not been widely explored in existing literature. Reproducibility appears to be adequately addressed, with sufficient details provided regarding the experimental setup and the implementation of KAP.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful contribution to the field of robust machine learning by introducing KAP, a technique that enhances the resilience of neural networks against adversarial attacks. While the findings are promising, the trade-off between robustness and performance on clean data warrants further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel neural network building block called Kernel Average Pool (KAP), designed to enhance robustness against input perturbations and adversarial attacks. The methodology involves using KAP to create organized ensembles of kernel outputs, aimed at reducing prediction variance and improving robustness. Empirical evaluations across several datasets (CIFAR10, CIFAR100, TinyImagenet, Imagenet) demonstrate that models utilizing KAP show significant improvements in robustness against adversarial attacks without requiring adversarial training, while also providing insights into the trade-off between clean accuracy and robustness.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to enhancing robustness through feature-level ensemble learning, which addresses existing limitations of traditional techniques like dropout. The empirical results present compelling evidence of KAP's efficacy, showcasing its performance across various datasets and attack scenarios. However, a potential weakness is the lack of in-depth analysis on the computational costs associated with KAP, particularly in comparison to other defense methods. Additionally, while the empirical results are promising, further exploration of the theoretical underpinnings of KAP could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The methodology is described in sufficient detail, allowing for reproducibility of the experiments conducted; however, the paper could benefit from a more comprehensive description of the experimental setup, including hyperparameters and architecture specifics. In terms of novelty, KAP presents a fresh perspective on ensemble methods in the context of adversarial robustness, marking a significant contribution to the field.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the pursuit of adversarial robustness in neural networks through the introduction of KAP. While the findings are compelling and the methodology is sound, further exploration of computational efficiency and a deeper theoretical grounding could enhance the paper's overall impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Robust Kernel Ensembles with Kernel Average Pooling\" introduces a novel approach to enhance the adversarial robustness of neural networks through Kernel Average Pooling (KAP). The authors propose KAP as a technique to learn ensembles of kernels within a single neural network framework, addressing the critical challenge of improving robustness against adversarial attacks. The methodology is well-detailed, providing a theoretical foundation for KAP and sufficient implementation details for reproducibility. The experimental results demonstrate significant improvements in robustness across standard benchmarks such as CIFAR10, CIFAR100, TinyImagenet, and Imagenet, compared to established baseline models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative methodology and comprehensive empirical validation. KAP represents a significant advancement in the domain of adversarial defenses, and the results indicate its effectiveness in enhancing robustness without substantial degradation in performance. However, the paper acknowledges a trade-off between adversarial robustness and clean accuracy, which might limit its applicability in scenarios where both metrics are crucial. Furthermore, while the authors provide a thorough literature review, additional exploration of model capacity and practical implementations could strengthen the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written with a logical flow of ideas, making it accessible to readers. The clarity of the problem statement and the proposed solution is commendable. The quality of the experiments is high, supported by appropriate statistical analysis, and the methodology is described in sufficient detail to allow for reproducibility. The novelty of the KAP approach stands out in the existing literature, marking it as a meaningful contribution to the field.\n\n# Summary Of The Review\nOverall, the paper presents a novel and well-supported approach to improving adversarial robustness in neural networks. While it showcases significant empirical results, it also identifies key limitations, particularly regarding the trade-off between robustness and clean accuracy. The work is recommended for acceptance with minor revisions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework leveraging Kernel Average Pooling (KAP) to construct robust kernel ensembles within deep neural networks, aimed at enhancing resilience against adversarial attacks. The methodology involves substituting traditional convolutional kernels with ensembles that perform mean filtering across the kernel dimension, thereby improving the model's ability to mitigate prediction variance and adversarial susceptibility. Empirical results across various datasets (CIFAR10, CIFAR100, TinyImagenet, Imagenet) demonstrate that KAP significantly improves robustness metrics against adversarial perturbations, achieving comparable performance to adversarially trained networks without requiring adversarial training.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to ensemble learning through KAP, which not only enhances robustness but also reduces computational burden compared to traditional adversarial training methods. The empirical results are compelling, illustrating the effectiveness of KAP across multiple datasets and adversarial attack scenarios. However, the paper could be strengthened by a more thorough analysis of the computational complexity and runtime efficiency of KAP compared to existing methods. Additionally, while the results are promising, further exploration of the theoretical underpinnings of KAP could provide deeper insights into its robustness claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the proposed KAP method, the experimental setup, and the findings. The methodology is detailed, allowing for reproducibility, especially with the provided mathematical formulations. The novelty of KAP as a mechanism for creating kernel ensembles is significant, contributing to the existing literature on adversarial robustness in deep learning. However, additional details on implementation specifics, including hyperparameter selection and training protocols, would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to enhancing adversarial robustness through Kernel Average Pooling, supported by empirical results across multiple datasets. While the contributions are significant, further clarification on computational efficiency and theoretical insights would bolster the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pool (KAP) as a new component for neural networks, positing it as a potential solution to enhance robustness against adversarial attacks while reducing computational costs associated with ensemble methods. The methodology involves applying KAP in standard architectures and evaluating its performance on datasets such as CIFAR10 and ImageNet. However, the findings suggest that while KAP claims to improve robustness, the empirical results are inconclusive, and the performance trade-offs in clean accuracy raise concerns about its practical applicability.\n\n# Strength And Weaknesses\nThe proposed KAP offers an interesting approach to potentially mitigating adversarial vulnerabilities in neural networks, yet its contributions are undermined by several weaknesses. The novelty of KAP appears limited as it closely resembles existing techniques without a strong theoretical foundation or justification for its necessity. The paper also lacks a thorough comparative analysis with state-of-the-art methods, which obscures the benefits of KAP. Additionally, the experimental results are based on a narrow range of datasets, limiting the generalizability of the findings. The reduction in clean accuracy when employing KAP, coupled with insufficient discussion of limitations, further detracts from the paper's overall impact and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by a lack of depth in addressing its proposed method's theoretical underpinnings. While the authors present their findings clearly, the novelty is questionable given the method's similarity to existing approaches without robust justification. Reproducibility is also a concern due to the superficial experiments and limited diversity in testing environments, which may not provide a comprehensive understanding of KAP's effectiveness across different scenarios.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing concept in KAP, but its contributions are not sufficiently substantiated by rigorous theoretical analysis or comprehensive empirical validation. The claims of improved robustness and efficiency lack compelling evidence, making it difficult to ascertain the practical applicability of KAP in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pooling (KAP), a novel approach to learning kernel ensembles in neural networks that enhances robustness against adversarial attacks. Through empirical evaluations on datasets such as CIFAR10, CIFAR100, TinyImagenet, and Imagenet, the authors demonstrate that KAP models not only achieve comparable performance to adversarially trained networks but also do so without the associated computational burdens. The methodology emphasizes feature-level ensembling, resulting in smoother transitions between kernel ensembles, which contributes to improved model stability and performance. The work illustrates the emergence of topographically organized kernel maps that reflect natural learning processes, suggesting broader implications for future machine learning models.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative introduction of KAP and its demonstrated effectiveness in enhancing robustness and efficiency. The empirical results showcase significant improvements over traditional adversarial training, making a compelling case for KAP's application in various neural network architectures. However, the paper could benefit from a more detailed analysis of potential limitations or drawbacks of the KAP approach, particularly in terms of its applicability to other types of datasets or tasks beyond those tested. Additionally, while the topographical insights are intriguing, further exploration of their implications would strengthen the discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear articulation of the KAP methodology and its contributions to the field. The experimental setup is adequately described, allowing for reproducibility of the results. The novelty of the KAP approach is evident, as it represents a significant shift in how kernel ensembles are learned and utilized. The findings are presented in a logical manner, supported by comprehensive empirical evaluations that validate the claims made.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the learning of robust kernel ensembles through the introduction of Kernel Average Pooling. The empirical results and innovative approach offer valuable insights into improving adversarial robustness in neural networks, laying the groundwork for future research in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pooling (KAP), a novel method aimed at enhancing the robustness of kernel ensembles in deep learning architectures. The main contributions include a theoretical framework that supports the use of KAP to reduce variance and improve generalization under adversarial conditions. The methodology focuses on feature-level ensembles that leverage local averaging among kernels to foster smoother transitions in learned representations. Empirical results demonstrate the effectiveness of KAP in improving model resilience against adversarial attacks while underlining the need for further theoretical exploration of its properties.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its solid theoretical foundation and the integration of established ensemble methods into the context of kernel functions within neural networks. The proposal of KAP reflects a thorough understanding of variance reduction and feature diversity, positioning it as a promising approach for enhancing model robustness. However, the paper could benefit from more detailed empirical evaluations, as the theoretical implications are presented with less emphasis on practical applications and results. Additionally, the exploration of KAP's limitations and potential edge cases in larger models remains an area for further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex theoretical concepts accessible to the reader. The quality of the writing is high, with a logical flow that effectively supports the main arguments. The novelty of KAP is significant, as it combines ensemble learning principles with kernel methods in a unique way. However, the reproducibility of results could be enhanced with more comprehensive details on the experimental setup and datasets used, allowing other researchers to replicate the findings more easily.\n\n# Summary Of The Review\nOverall, the paper presents a theoretically robust and novel approach to enhancing kernel ensembles through KAP, with promising implications for adversarial robustness. While the theoretical contributions are strong, the empirical validations could be expanded to fully substantiate the claims made. Further exploration into the practical applications and limitations of KAP in diverse contexts would also be beneficial.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces the Kernel Average Pool (KAP), a new operation designed for learning kernel ensembles in neural networks. KAP computes a mean filter along the kernel dimension of the activation tensor and can be implemented as an average pooling operation, requiring kernel size (K) and stride (S) as parameters. The authors evaluate KAP within the framework of a ResNet18 architecture across various datasets (CIFAR10, CIFAR100, TinyImagenet, and Imagenet) and compare its performance against baseline models, including adversarially trained ResNet18. The results indicate that KAP enhances robustness against adversarial attacks while also demonstrating faster training times, although with some trade-off in clean accuracy.\n\n# Strength And Weaknesses\nThe main strength of this work lies in its introduction of KAP, which provides a novel approach to improving model robustness against adversarial attacks without the high computational cost associated with adversarial training. The empirical results support the effectiveness of KAP in enhancing adversarial performance while maintaining reasonable training speeds. However, a notable weakness is the trade-off observed with clean accuracy, which could limit the practical applicability of KAP in certain scenarios. Furthermore, the lack of explicit code availability may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and articulates its contributions clearly, making it accessible to readers familiar with neural network architectures and adversarial training. The methodology is described in sufficient detail, with algorithmic clarity provided for the implementation of 2D KAP. Nevertheless, the absence of direct references to code availability could pose challenges for reproducibility, which is an important aspect of empirical research.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of adversarial robustness in neural networks through the introduction of the KAP operation. While it demonstrates promising empirical results and efficient training times, the trade-off with clean accuracy and the lack of explicit code availability are notable concerns.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the Kernel Average Pool (KAP) method, claiming it as a novel approach to improving model robustness in comparison to traditional ensemble methods like bagging and boosting. The authors present empirical evaluations on datasets such as CIFAR10 and CIFAR100, asserting that KAP outperforms adversarially trained networks without the need for adversarial training. However, the paper lacks thorough comparisons with state-of-the-art methods and does not provide sufficient empirical evidence to support its claims.\n\n# Strength And Weaknesses\nWhile the introduction of KAP presents an interesting direction, its reliance on established concepts from ensemble methods diminishes its perceived novelty. The paper's assertion that KAP provides a more robust model than dropout is questionable, as dropout has been extensively validated, and the authors do not provide adequate empirical backing. The assessments of KAP's performance against adversarially trained networks are undermined by a failure to consider the computational costs involved, which could have offered a more balanced perspective. Additionally, the paper does not adequately contextualize its findings against recent advancements in adversarial robustness, nor does it address the limitations of KAP in larger datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably clear, but the novelty of KAP is overstated, and the methodology lacks sufficient rigor in its comparisons with existing methods. The reproducibility of results is not addressed in detail, as the paper does not provide comprehensive experimental setups or parameters that would allow for independent verification of findings.\n\n# Summary Of The Review\nOverall, the paper presents Kernel Average Pool (KAP) as a novel approach for enhancing model robustness, but it falls short in adequately justifying its claims through empirical evidence and comparisons with existing methods. The contributions appear to be overstated, and the work does not sufficiently engage with current advancements in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach termed Kernel Average Pooling (KAP), aimed at enhancing the robustness of kernel ensembles against adversarial attacks. The authors introduce a systematic methodology that integrates KAP within existing neural network architectures, demonstrating its effectiveness through extensive experiments across various datasets, including CIFAR10 and CIFAR100. The findings indicate that KAP, especially when coupled with activation noise, significantly improves model resilience against adversarial perturbations while maintaining competitive accuracy on clean data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining KAP with existing architectures, which is shown to yield improved robustness against adversarial attacks. The experiments are comprehensive and well-structured, providing solid empirical support for the claims made. However, the paper could benefit from a more detailed comparison with state-of-the-art methods to contextualize the significance of the proposed approach. Additionally, some sections lack clarity, particularly when explaining the implications of the findings, which may hinder reader comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe manuscript is generally well-organized, but it suffers from minor issues in clarity that may confuse readers, particularly in the definition and application of technical terms. The quality of writing is adequate, yet the paper would benefit from a thorough proofreading to rectify typographical errors and ensure consistent notation. The novelty of the KAP approach is notable, contributing significantly to the field of adversarial robustness. The reproducibility of the results is supported by detailed descriptions of the methodology and clear presentation of experimental setups, though access to code and datasets would enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the area of adversarial robustness through the introduction of Kernel Average Pooling. While the methodology is compelling and the results promising, improvements in clarity and comparison with existing methods would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel kernel average pooling (KAP) method aimed at enhancing adversarial robustness in neural networks. The authors propose a methodology that utilizes ensemble techniques to aggregate kernel outputs, thereby improving the model's resilience against adversarial attacks. Experimental results demonstrate that KAP outperforms baseline models in terms of robustness, but the paper lacks a comprehensive comparison with state-of-the-art defenses and does not explore the scalability of the approach on more complex datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to adversarial robustness through the use of kernel ensembles. However, several weaknesses are evident. Firstly, the exploration of ensemble methods could have been expanded beyond KAP, incorporating alternative strategies like weighted ensembles or dynamic selection. Additionally, the analysis of hyperparameter sensitivity is limited, and the paper fails to address the potential limitations of KAP against evolving adversarial strategies. The lack of comparison with other established techniques significantly reduces the paper's contextual relevance, and there is a missed opportunity to explore the interpretability and efficiency of KAP.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly. However, the novelty is somewhat diminished by the lack of comprehensive comparisons with existing methods. The reproducibility of results could be improved by providing more detailed information regarding the experimental setup, including data preprocessing and hyperparameter settings. The absence of long-term studies and scalability assessments also raises concerns about the reliability of the findings in real-world applications.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting approach to enhancing adversarial robustness through kernel average pooling, it falls short in several key areas, including comparative analysis, scalability, and interpretability. Addressing these limitations could significantly strengthen the contributions of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel method called Kernel Average Pooling (KAP) aimed at enhancing the robustness of neural network ensembles against input perturbations, particularly adversarial attacks. The authors employ multiple datasets, including CIFAR10, CIFAR100, TinyImagenet, and Imagenet, to assess the generalizability of their approach. The findings indicate that models utilizing KAP exhibit statistically significant improvements in robustness metrics, such as adversarial accuracy, compared to traditional baseline models like ResNet18, although specific statistical validation techniques are not thoroughly detailed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its empirical demonstration of KAP's effectiveness in increasing model robustness, which is crucial in the context of adversarial machine learning. The comprehensive experimental design, featuring multiple datasets and controlled baseline comparisons, further strengthens the paper's contributions. However, a notable weakness is the lack of rigorous statistical methodology for validating the significance of the reported improvements. The absence of explicit statistical tests (e.g., t-tests or ANOVA) and confidence intervals raises concerns about the robustness and reliability of the claims made regarding performance enhancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly, with a logical flow from the introduction to the experimental results. However, the lack of detailed statistical validation detracts from the quality of the findings and their reproducibility. While the concept of using KAP is novel and addresses a significant challenge in the field, the empirical results would benefit from a more structured statistical analysis to bolster their credibility.\n\n# Summary Of The Review\nOverall, the paper presents compelling evidence for the effectiveness of the KAP method in improving neural network robustness; however, it falls short in providing a rigorous statistical framework to substantiate its claims. Addressing this gap would significantly enhance the reliability and impact of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the Kernel Average Pool (KAP) method as an innovative approach aimed at enhancing model robustness against adversarial attacks. The authors evaluate KAP's effectiveness through experiments on datasets including CIFAR10, CIFAR100, TinyImagenet, and Imagenet. The findings suggest that KAP significantly improves robustness; however, it raises concerns regarding its generalization potential, trade-offs between clean accuracy and adversarial robustness, and computational efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of KAP, which appears to provide notable improvements in adversarial robustness when tested across multiple datasets. However, several weaknesses are evident: the generalization of KAP beyond the tested datasets is not thoroughly explored, leading to concerns about its applicability in broader contexts. Additionally, while the paper claims to enhance robustness, it lacks a detailed analysis of the types of adversarial perturbations that KAP effectively addresses. There is insufficient investigation into the trade-offs between clean accuracy and adversarial robustness, scalability to larger models, and the impact of hyperparameter tuning. Furthermore, the paper does not provide a comparative analysis with other state-of-the-art methods or insight into the mechanisms behind KAP’s effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear, but it could benefit from a more structured presentation of its findings and a deeper exploration of its limitations. The quality of the methodology is solid, yet it lacks comprehensive evaluations that would enhance reproducibility. Novelty is present in the proposed KAP method; however, the paper does not sufficiently differentiate its contributions from existing techniques or outline the specific advancements it offers. Overall, while the paper presents intriguing ideas, it falls short in terms of thoroughness and detail.\n\n# Summary Of The Review\nIn conclusion, the paper introduces the KAP method, which demonstrates promise in improving adversarial robustness but lacks a comprehensive evaluation of its generalizability and the trade-offs involved. The paper would benefit from a deeper exploration of hyperparameter impacts, scalability, and a comparative analysis with state-of-the-art methods to solidify its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a method named Kernel Average Pooling (KAP) aimed at enhancing the robustness of model ensembles against adversarial attacks. The authors propose to utilize KAP to average the outputs of kernel functions, claiming it serves as a simple yet effective approach to create \"local committees\" within the neural network architecture. Empirical evaluations conducted on standard datasets, including CIFAR-10, demonstrate improved robustness against adversarial perturbations compared to baseline models, although the trade-off between robustness and clean accuracy is noted.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its empirical evaluation, which shows that the proposed method can produce improvements in robustness against adversarial attacks. However, the methodology appears to lack novelty, as the concept of averaging outputs and the use of ensemble techniques have been well established in the literature. Additionally, the paper does not sufficiently differentiate its contributions from existing approaches to adversarial robustness, leading to concerns about the significance of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the novelty of the proposed method is limited. The authors use terminology like \"local committees\" that seems more marketing-driven than scientifically rigorous. The methodology, while straightforward, lacks detailed insights into implementation nuances that would enhance reproducibility. Overall, the quality of writing is acceptable, but the paper does not advance the field in a substantial way.\n\n# Summary Of The Review\nWhile the paper claims to present a novel approach to enhancing adversarial robustness through Kernel Average Pooling, it ultimately rehashes established concepts without significant innovation. The empirical results are promising but do not sufficiently justify the novelty of the method, leading to a lukewarm overall assessment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Kernel Average Pooling (KAP), a novel mechanism aimed at enhancing ensemble learning at the kernel level while focusing on reducing the computational costs associated with adversarial defense methods. The authors empirically validate KAP's effectiveness on datasets such as CIFAR10 and CIFAR100, demonstrating significant improvements in robustness against adversarial attacks. Additionally, they explore the relationship between kernel size and robustness, suggesting that larger kernels contribute to greater resilience in adversarial scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to pooling at the kernel level, which appears to offer substantial improvements in adversarial robustness while maintaining computational efficiency. However, the paper could benefit from a more thorough exploration of hybrid methods that integrate KAP with adversarial training techniques. Moreover, the empirical results, while promising, would be strengthened by a deeper investigation into the interpretability of the learned kernel ensembles and the potential for hybrid pooling strategies that could further enhance performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly, making it accessible to the reader. The novelty of KAP is evident, although the authors could have provided a more rigorous theoretical grounding for their claims. Reproducibility is enhanced by the empirical validation on standard datasets, yet additional experiments, especially on larger datasets and more complex architectures, would be beneficial to confirm the generalizability of KAP.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to pooling that shows promise in improving adversarial robustness while being computationally efficient. However, there are opportunities for further exploration of hybrid techniques and theoretical justifications that could strengthen the contributions of this work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Kernel Average Pool (KAP) method, a novel approach aimed at enhancing the adversarial robustness of neural networks. The authors conduct empirical evaluations of KAP on several standard benchmarks, including CIFAR10, CIFAR100, TinyImagenet, and Imagenet. Key findings reveal that KAP models, particularly the RN18-KAP architecture, exhibit significant improvements in robustness against adversarial attacks compared to baseline models and adversarially trained counterparts. Notable results include an adversarial accuracy of 67.7% on CIFAR10 against PGD-L∞ attacks, surpassing other methods without the need for adversarial training. The KAP approach maintains robustness across varying attack strengths, making it an attractive alternative to traditional adversarial training methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical validation of the KAP method across multiple datasets and attack vectors, demonstrating consistent performance improvements in adversarial robustness. The results provide compelling evidence for the effectiveness of KAP, highlighting its potential as a cost-efficient alternative to conventional adversarial training. However, a potential weakness is the lack of detailed theoretical justification for the KAP method, which may leave readers questioning the underlying mechanisms that contribute to its enhanced robustness. Additionally, while the paper presents strong empirical results, the challenge of maintaining clean accuracy alongside adversarial robustness is acknowledged but not fully addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents the methodology and findings, making it accessible to a broad audience. The quality of the empirical evaluations is high, with thorough benchmarking on various datasets. However, the novelty of KAP, while significant in the context of its empirical performance, could benefit from a deeper exploration of its theoretical foundations. Reproducibility is likely ensured through the rigorous evaluation protocols and standard benchmarks employed; however, the paper does not explicitly mention the availability of code or data, which would be essential for full reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the Kernel Average Pool method, demonstrating its effectiveness in improving adversarial robustness across multiple datasets. While the empirical results are strong, the lack of theoretical insights into the method's functioning could be a limitation. Nonetheless, KAP offers a promising direction for enhancing neural network resilience against adversarial attacks.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing the performance of convolutional neural networks (CNNs) through the introduction of a Kernel Average Pooling (KAP) operation. The authors conduct a series of experiments to demonstrate that KAP can effectively reduce overfitting while maintaining or improving accuracy on benchmark datasets. The methodology involves a technical exploration of KAP in conjunction with various CNN architectures, providing insights into its implementation and potential benefits.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its introduction of KAP, which addresses a common issue in CNNs—overfitting—by providing a simple yet effective pooling alternative. However, the paper suffers from several weaknesses, including overly complex sentence structures and a lack of clarity in the methodological explanations. Additionally, the paper's engagement with readers could be improved by reducing the use of jargon and enhancing the overall flow of the text. While the experimental results are promising, a more thorough discussion of the implications of the findings would strengthen the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by dense language and inconsistent formatting. The abstract could benefit from simplification, and certain sections lack descriptive headings or smooth transitions. Furthermore, the use of technical jargon without sufficient definitions may alienate some readers. On the quality front, the paper presents a well-structured methodology, but the reproducibility may be compromised due to the lack of detailed explanations of specific steps in the methodology.\n\n# Summary Of The Review\nOverall, the paper introduces a promising new pooling technique that could enhance CNN performance, but it is marred by clarity issues and a lack of reader engagement. Addressing these issues could significantly improve the paper's impact and accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.470819802530174,
    -1.5157970381085637,
    -1.6899711429945274,
    -1.5437870123634667,
    -1.6689186555965094,
    -1.463292394239567,
    -1.6685850111288787,
    -1.704076707666493,
    -1.6420584712064998,
    -1.7510695984017999,
    -1.801296764901809,
    -1.4272480471708542,
    -1.4097347098801223,
    -1.5381133660338753,
    -1.7527744197739186,
    -1.609120596633184,
    -1.8628555384078334,
    -1.7310539348252543,
    -1.665853597533475,
    -1.6825744839646812,
    -1.813304994034306,
    -1.5447283884367828,
    -1.6413742987305182,
    -1.5735973844790268,
    -1.586271328785317,
    -1.732543613608471,
    -1.6748138466718243,
    -1.4932560477738959,
    -1.675724503192144
  ],
  "logp_cond": [
    [
      0.0,
      -2.235315767656882,
      -2.2328847485870797,
      -2.233972194392659,
      -2.2345263328630502,
      -2.2469609261273416,
      -2.250971296279644,
      -2.2569371005982966,
      -2.2212527301751956,
      -2.2173783929116073,
      -2.211347768682113,
      -2.3204173330696123,
      -2.2360790258872534,
      -2.218289632632323,
      -2.2289038519530733,
      -2.2296675003475834,
      -2.230869931324559,
      -2.232116463043006,
      -2.2458288666303106,
      -2.225968066695622,
      -2.2216388194094594,
      -2.237975171076116,
      -2.226419034368185,
      -2.249669006459166,
      -2.234315324257942,
      -2.2227247026127794,
      -2.223409755650733,
      -2.251987072353844,
      -2.261358671804596
    ],
    [
      -1.2079851083413031,
      0.0,
      -1.0231128181001057,
      -1.1533267434965653,
      -1.0815543734023374,
      -1.0853127493417807,
      -1.0845209676980399,
      -1.140405481362659,
      -1.0378936157937577,
      -1.0758996184607144,
      -1.0701141497050577,
      -1.2432451031648641,
      -1.0897384745538483,
      -1.051804277924195,
      -1.096188092889251,
      -1.0462020511692056,
      -1.1402115315628096,
      -1.1045229668698986,
      -1.1625173211331918,
      -1.0318360775977409,
      -1.145950359247887,
      -1.1288889578613979,
      -1.1731892773861186,
      -1.1321649391657669,
      -1.1178436562397998,
      -1.155155628534972,
      -1.1312049941993925,
      -1.108622634131621,
      -1.1819452546774352
    ],
    [
      -1.3782760226102158,
      -1.1856564579518263,
      0.0,
      -1.2584730122660215,
      -1.2633532538318093,
      -1.3114220772624663,
      -1.211811665133907,
      -1.3318410239778016,
      -1.2160810166704077,
      -1.2838772518397708,
      -1.2025290672037983,
      -1.4372182046477904,
      -1.290140793232731,
      -1.2657781299798665,
      -1.2456633839020965,
      -1.2675026831311362,
      -1.3323106191645364,
      -1.2603774893084088,
      -1.3257416680604737,
      -1.21364934439227,
      -1.3571706088665194,
      -1.3406956512673707,
      -1.3673237158249456,
      -1.3553499629132497,
      -1.3058221383460868,
      -1.3628768487185958,
      -1.3103647127379106,
      -1.3039126504459702,
      -1.4113103008543828
    ],
    [
      -1.24818489580737,
      -1.1705341360210544,
      -1.1554444523132719,
      0.0,
      -1.165291186967025,
      -1.1714716715169133,
      -1.133723038313009,
      -1.1780779168453375,
      -1.1583205570622213,
      -1.1851949855282582,
      -1.168643861771963,
      -1.2704046444609733,
      -1.1866034775020133,
      -1.1274506958364272,
      -1.1878564915695036,
      -1.1462250153602906,
      -1.1862436770119311,
      -1.1372050808451408,
      -1.1763353387880595,
      -1.1659147164377428,
      -1.1640778893133559,
      -1.1719799492413943,
      -1.207752179117697,
      -1.178712955749841,
      -1.1935478077038313,
      -1.1840537347235978,
      -1.1758924258146992,
      -1.1836817590816577,
      -1.221771727173513
    ],
    [
      -1.3616965015397526,
      -1.2124813101682468,
      -1.2358199532515841,
      -1.3435215486527623,
      0.0,
      -1.2857582428761851,
      -1.2560077542849313,
      -1.3047708420481203,
      -1.2117064478254937,
      -1.215718635002847,
      -1.2608801108308616,
      -1.434132255883754,
      -1.2736990788141962,
      -1.2379238864088233,
      -1.2566920134565855,
      -1.2859810433266632,
      -1.284701355734163,
      -1.2404164987759279,
      -1.3197466024130722,
      -1.2378689790847623,
      -1.3258387915438754,
      -1.3482499987767476,
      -1.3199166907005764,
      -1.2969449251058371,
      -1.2725967504802187,
      -1.3045307554224266,
      -1.3063644849137392,
      -1.3200224005065693,
      -1.3874399254768428
    ],
    [
      -1.1555716068056943,
      -1.0377956489607307,
      -1.0288383939349806,
      -1.0416103519458237,
      -1.0176664013129824,
      0.0,
      -1.0429469120956307,
      -1.0769346001308466,
      -1.037497028498333,
      -1.0317684787154602,
      -1.0488347106178453,
      -1.1671514180929279,
      -1.111673564451067,
      -1.0328824281109266,
      -1.0849144815112692,
      -1.013930193331436,
      -1.0836100396824082,
      -1.007116948649357,
      -1.09329989945759,
      -1.0964898443517441,
      -1.0478433617663887,
      -1.0686235985415293,
      -1.1425917054824322,
      -1.0601310606723318,
      -1.0583450250485307,
      -1.108489901735759,
      -1.0515718357993857,
      -1.056160589065024,
      -1.1325662108057846
    ],
    [
      -1.3626691793572705,
      -1.2306820100494016,
      -1.2356933521905806,
      -1.2673473922123089,
      -1.2193526258276313,
      -1.2651059521455492,
      0.0,
      -1.3163643931146618,
      -1.2011655606346594,
      -1.2390944351278952,
      -1.2847538242825818,
      -1.4308395844871675,
      -1.2995002049118038,
      -1.2321792796666289,
      -1.2525638346586825,
      -1.265948079201791,
      -1.2809170909085048,
      -1.2147566511541177,
      -1.348375069847621,
      -1.2651854209658873,
      -1.3157037557113038,
      -1.3767786444459078,
      -1.3673354946080942,
      -1.3209666914760978,
      -1.2980057324817476,
      -1.3069361941129227,
      -1.2972393813395693,
      -1.2716758138402324,
      -1.3539460466400104
    ],
    [
      -1.3462918516892166,
      -1.280647214891066,
      -1.2588592760324604,
      -1.2976684799384623,
      -1.2633668079819018,
      -1.3086520511106057,
      -1.3203191316432492,
      0.0,
      -1.2517251173746748,
      -1.278645074740475,
      -1.2526465571670906,
      -1.4255853882433427,
      -1.3156909093100688,
      -1.2568462910453733,
      -1.294485722674969,
      -1.2910071723750678,
      -1.2801090793843752,
      -1.3048860290032755,
      -1.2990908612884489,
      -1.2809182923437676,
      -1.2507693547064995,
      -1.3174051301113818,
      -1.3218575071114096,
      -1.3015488859587412,
      -1.2824073549597985,
      -1.2596947925447048,
      -1.300224053675008,
      -1.2956438391904763,
      -1.2992353344159617
    ],
    [
      -1.3274808278466819,
      -1.1777327622637142,
      -1.161329298101532,
      -1.2356909668646527,
      -1.1708300887184195,
      -1.2047948610378534,
      -1.1962853650078498,
      -1.2226915844783315,
      0.0,
      -1.2095791618040086,
      -1.2120741483851052,
      -1.3858774088118349,
      -1.2736194507689291,
      -1.140911509237232,
      -1.2113762504292167,
      -1.204715574509922,
      -1.2444016941816218,
      -1.1696961600424565,
      -1.2701899090991668,
      -1.1910963582467293,
      -1.2275372964457771,
      -1.2821918286598097,
      -1.2767545390277186,
      -1.2319606570147554,
      -1.2195311586037512,
      -1.2836637913570341,
      -1.2343369384168634,
      -1.1857500615209948,
      -1.3388503409762715
    ],
    [
      -1.4031679697728254,
      -1.2840016669171181,
      -1.289444706592476,
      -1.3804339247419226,
      -1.2487032311720114,
      -1.3134303177201336,
      -1.304113429533061,
      -1.3426350218906824,
      -1.2242122658956054,
      0.0,
      -1.2817734816952597,
      -1.4942497245499404,
      -1.4039668164585897,
      -1.2571762724830315,
      -1.3038773858039965,
      -1.3041287849819303,
      -1.3152179372614454,
      -1.3147451733485265,
      -1.389622440954514,
      -1.3473901917674798,
      -1.3421485119193814,
      -1.3657874782655137,
      -1.4111623031747724,
      -1.3522027127602292,
      -1.270389318983761,
      -1.3200803922145525,
      -1.3216940222084537,
      -1.316310377398661,
      -1.3888821493229655
    ],
    [
      -1.4362065230752827,
      -1.3274813554681586,
      -1.245256373217893,
      -1.3489888472947134,
      -1.3148209450757178,
      -1.3598945630181287,
      -1.3113272511681044,
      -1.3795798052505752,
      -1.294428873815045,
      -1.3600018774167246,
      0.0,
      -1.529090591626969,
      -1.4004515838594973,
      -1.2662991423927554,
      -1.3544580222533122,
      -1.3728070544433877,
      -1.359912125427531,
      -1.3573260309855975,
      -1.383810813767344,
      -1.3441243876569595,
      -1.386711889309783,
      -1.3726747041615859,
      -1.4069283046459746,
      -1.400336398886691,
      -1.392483039871451,
      -1.3971298723884975,
      -1.386305318240679,
      -1.3944518442420115,
      -1.466331001941645
    ],
    [
      -1.1626723847714737,
      -1.1475318149812543,
      -1.1554578159287114,
      -1.1597949136353611,
      -1.159027447649199,
      -1.1437805226008588,
      -1.1351072791557613,
      -1.1204023977856739,
      -1.124653090143102,
      -1.1117017697136449,
      -1.1383248522663596,
      0.0,
      -1.0721288682051093,
      -1.1737579948583654,
      -1.1300785347235287,
      -1.153616152005316,
      -1.108650406090874,
      -1.1524003512853462,
      -1.1292057247672247,
      -1.111428244749049,
      -1.1297415837149962,
      -1.107944427399348,
      -1.148973435603698,
      -1.0911736880507388,
      -1.1357912306050144,
      -1.1090019607544073,
      -1.1397820559726721,
      -1.1665682028832502,
      -1.0925445750194542
    ],
    [
      -1.1344231735959245,
      -1.0800514407411117,
      -1.0648130759413652,
      -1.070499060517819,
      -1.076671694874294,
      -1.1034149223282235,
      -1.1152808950043513,
      -1.1145452566450784,
      -1.06863771377149,
      -1.103307027923459,
      -1.08895750373844,
      -1.1356935210998325,
      0.0,
      -1.1170830336343431,
      -1.0898018238817988,
      -1.084236245859662,
      -1.0932410775485832,
      -1.0566764601689842,
      -1.121549682456488,
      -1.0488825645823434,
      -1.1143958693137994,
      -1.119543374750472,
      -1.1254026552240965,
      -1.102912283584854,
      -1.120420539480331,
      -1.093534698285504,
      -1.1044472888194619,
      -1.1625208609037088,
      -1.1329390053091264
    ],
    [
      -1.1951897805053275,
      -1.0772918768731363,
      -1.1209683094585645,
      -1.121618570597293,
      -1.1054147980426234,
      -1.150972435063459,
      -1.1117482512900005,
      -1.112769638794993,
      -1.0741585559121536,
      -1.092444599390674,
      -1.0672080453079547,
      -1.2977253722634492,
      -1.1925229307804315,
      0.0,
      -1.1416978467957744,
      -1.0717646127295235,
      -1.166150323928489,
      -1.12595683541691,
      -1.1471980535151491,
      -1.110975620127453,
      -1.0914676537178396,
      -1.1995006426778954,
      -1.1799613676251977,
      -1.1489890333748112,
      -1.100060423855533,
      -1.1456506081875648,
      -1.1496831958077356,
      -1.1074402830641925,
      -1.2328732323343783
    ],
    [
      -1.4074585545784455,
      -1.314816966111288,
      -1.2919935907088076,
      -1.4006815076672765,
      -1.2858212609128823,
      -1.3775090180227074,
      -1.332359342867522,
      -1.3645445449591938,
      -1.2471515354836713,
      -1.294200727905196,
      -1.306063461672825,
      -1.4616985195300216,
      -1.3638037632050795,
      -1.3032134452109652,
      0.0,
      -1.297091712061008,
      -1.3569701688559623,
      -1.3173324767959254,
      -1.3879717530856044,
      -1.2857571494121287,
      -1.4061495728670235,
      -1.3670539909596808,
      -1.3621478736774426,
      -1.361990237865461,
      -1.3400289444476177,
      -1.3758184852045765,
      -1.3530670572140115,
      -1.3586041858350641,
      -1.4293549797159222
    ],
    [
      -1.2830847057310255,
      -1.0990775324535413,
      -1.097668308618654,
      -1.1993600350101337,
      -1.160108815107662,
      -1.1340766293387277,
      -1.1849050013680669,
      -1.2005410098807912,
      -1.1598203320086498,
      -1.1626632548578526,
      -1.1387947927948876,
      -1.341135349367497,
      -1.202348752168225,
      -1.0709350557654758,
      -1.1454857485498713,
      0.0,
      -1.2357080372901579,
      -1.131685191386447,
      -1.196079502773181,
      -1.1405672283227906,
      -1.2171454846133962,
      -1.201223833309977,
      -1.2541125635845547,
      -1.2136248419441915,
      -1.2135948851006684,
      -1.2308587537149225,
      -1.1900008149378365,
      -1.2249873563929405,
      -1.2585662242939073
    ],
    [
      -1.5459457867301007,
      -1.4754583750460148,
      -1.4749020289550998,
      -1.4736207910739554,
      -1.4591855282752129,
      -1.4938318686606913,
      -1.4366244428266857,
      -1.4694530214872936,
      -1.444720754003394,
      -1.4332317523665945,
      -1.4396294273123427,
      -1.6181370297967206,
      -1.5189557142947627,
      -1.4713500166092432,
      -1.4733944446135436,
      -1.5186409736120583,
      0.0,
      -1.4775190887173626,
      -1.5389685698654747,
      -1.499416892722957,
      -1.4585983633886799,
      -1.4889227627377868,
      -1.4889110015247449,
      -1.53002827905428,
      -1.4489809028057372,
      -1.5107260367832487,
      -1.4444064993821357,
      -1.516448152850385,
      -1.529481553495755
    ],
    [
      -1.4144111569824611,
      -1.344432324690587,
      -1.3154491009012848,
      -1.3232615939961276,
      -1.3091823533201223,
      -1.3258173808457716,
      -1.2842813676880254,
      -1.3562319904556732,
      -1.3087521980357955,
      -1.3186748832300645,
      -1.3297678083864668,
      -1.4426354672651431,
      -1.3457512070310222,
      -1.2711384222508546,
      -1.3057277206410394,
      -1.281944827254043,
      -1.3646444270357792,
      0.0,
      -1.3296649675718246,
      -1.3047771379573785,
      -1.327581768280989,
      -1.3705399906590316,
      -1.3879964482065696,
      -1.374069925336471,
      -1.3677641794038042,
      -1.3860756283603615,
      -1.3610502436194933,
      -1.3883640108033257,
      -1.3819161646799722
    ],
    [
      -1.3403629301546307,
      -1.3200930760620835,
      -1.2815865919930707,
      -1.2993819699247087,
      -1.3305123371741483,
      -1.313009434674216,
      -1.317555426633757,
      -1.3166684194161444,
      -1.3008270425248047,
      -1.3119116980099002,
      -1.2928339753510993,
      -1.3894840800309998,
      -1.329277165570523,
      -1.3241074027904027,
      -1.3309296513713793,
      -1.3359945437387761,
      -1.299877773549849,
      -1.288938420342306,
      0.0,
      -1.3518569173363026,
      -1.2975900210562967,
      -1.341808657245332,
      -1.3462938071265198,
      -1.3395979733981833,
      -1.354442621367393,
      -1.319789819651346,
      -1.2828711750746356,
      -1.3439008972096564,
      -1.3625675147344114
    ],
    [
      -1.3822903174210313,
      -1.217846710372906,
      -1.2106882360728697,
      -1.3128895024609106,
      -1.2699819537717472,
      -1.299099431495368,
      -1.27067929817857,
      -1.3420800129566735,
      -1.2114690113122528,
      -1.248581192622854,
      -1.2891086588385783,
      -1.4138533094810504,
      -1.263029332929566,
      -1.2384092395011186,
      -1.2656672035623215,
      -1.2503598582064193,
      -1.3339385329307996,
      -1.2379286819827529,
      -1.3539467397264207,
      0.0,
      -1.3577827956211619,
      -1.3660805934530051,
      -1.3724371218256004,
      -1.2656739720672385,
      -1.324575543819327,
      -1.3452438716858433,
      -1.3149235039973406,
      -1.273069099050655,
      -1.3681433060339097
    ],
    [
      -1.4838700526618616,
      -1.4133855013579693,
      -1.4040443190857732,
      -1.4435867156433189,
      -1.404461871159551,
      -1.4281884597303949,
      -1.4139304083532158,
      -1.3746763798834944,
      -1.3769026290598025,
      -1.4370518993176735,
      -1.4094261095972753,
      -1.5744920564605223,
      -1.476909297613449,
      -1.374431372073232,
      -1.4434877425038988,
      -1.4087891349474007,
      -1.4043919863453391,
      -1.3748402152733314,
      -1.4411342563105842,
      -1.4337735780066951,
      0.0,
      -1.4379005308489132,
      -1.4343793447751303,
      -1.4869630468852573,
      -1.4122565059454069,
      -1.4440406371792625,
      -1.4211432630854532,
      -1.4229971548536178,
      -1.4632990115634494
    ],
    [
      -1.236882495047108,
      -1.153929232773019,
      -1.1277126439659824,
      -1.1800330941231973,
      -1.1454131192240007,
      -1.1273326057387103,
      -1.1571704046344733,
      -1.1546555357108477,
      -1.1231652594341066,
      -1.1468027206874982,
      -1.119484017005487,
      -1.2692379968814833,
      -1.1727553757669384,
      -1.14735337527748,
      -1.121402248594546,
      -1.1486663932117764,
      -1.1690416218572102,
      -1.1462151306273052,
      -1.187567705409004,
      -1.1629464822444713,
      -1.1780383781365047,
      0.0,
      -1.1838628608066615,
      -1.1664213520184379,
      -1.1633670743036473,
      -1.14013451909506,
      -1.1363590997379451,
      -1.208789782959999,
      -1.1945009104770936
    ],
    [
      -1.3164521731176178,
      -1.2611971281949967,
      -1.2423013745516711,
      -1.256998858840789,
      -1.231951423918444,
      -1.3094993205579342,
      -1.28393458592241,
      -1.260267113232943,
      -1.2296366170426298,
      -1.237289003088976,
      -1.2340427488627113,
      -1.3545305150564722,
      -1.2760552840804456,
      -1.2435699999410132,
      -1.2265052242187802,
      -1.253619124456564,
      -1.2477925929785993,
      -1.2557477506869477,
      -1.2671288798503935,
      -1.291765833799227,
      -1.2357755233713148,
      -1.2550036350370453,
      0.0,
      -1.2715585168746675,
      -1.2029395818894004,
      -1.2401702987840995,
      -1.2119112597237647,
      -1.2781256938098375,
      -1.2937818936897356
    ],
    [
      -1.3011853861229665,
      -1.2330299180839224,
      -1.2309220630059765,
      -1.2526884464574581,
      -1.2364706875701372,
      -1.2457742001173575,
      -1.2405303362663649,
      -1.250760344017868,
      -1.1953152393687676,
      -1.1936048652859457,
      -1.2407924419130734,
      -1.2344597245756004,
      -1.2268058293941229,
      -1.1848455360498662,
      -1.230770609338979,
      -1.2256920869323598,
      -1.27593316227754,
      -1.2244621393405672,
      -1.264863625143818,
      -1.1926112396114081,
      -1.238132006909165,
      -1.2772005996513491,
      -1.279108617561416,
      0.0,
      -1.1951778752803723,
      -1.252400391917052,
      -1.2716928961924017,
      -1.2247760911055114,
      -1.247045335908739
    ],
    [
      -1.3072963235016237,
      -1.2287370073689552,
      -1.2352305306239761,
      -1.2789163036530924,
      -1.219004365432923,
      -1.2641268546034539,
      -1.231890693809716,
      -1.2530620744992536,
      -1.1864230393121988,
      -1.2342109016744476,
      -1.2351382121442167,
      -1.3585361843161297,
      -1.2884446888474197,
      -1.206836182911141,
      -1.2186947484989659,
      -1.2432383951061177,
      -1.2478987780540438,
      -1.2299226633100397,
      -1.2799708155919431,
      -1.236888626123681,
      -1.2650122912363333,
      -1.2594599331921075,
      -1.2529834536169753,
      -1.2096056341143506,
      0.0,
      -1.2329279948013916,
      -1.254753604878811,
      -1.2461953802755752,
      -1.2775691672229232
    ],
    [
      -1.3477268965672533,
      -1.3071872113755836,
      -1.3002273491581324,
      -1.3177025944402116,
      -1.3044959959289153,
      -1.3281823659297765,
      -1.3264696592191692,
      -1.2774309971586852,
      -1.28519807606936,
      -1.2635967024169092,
      -1.2827373503996455,
      -1.3987488800282115,
      -1.3184212404456623,
      -1.270273572752987,
      -1.255006215753796,
      -1.2869765641441073,
      -1.3182904413379446,
      -1.2954998570443743,
      -1.2782289826596522,
      -1.307024801560112,
      -1.2895027703549842,
      -1.2733011017996834,
      -1.3070473198026398,
      -1.3430543947811053,
      -1.273097338365685,
      0.0,
      -1.2897935984075841,
      -1.3419187044531624,
      -1.3129815669660274
    ],
    [
      -1.325703001746256,
      -1.2717438058884047,
      -1.2849929872947596,
      -1.295018820550516,
      -1.25654328062894,
      -1.2755267866781368,
      -1.2564753089937912,
      -1.2548395041479448,
      -1.2306856578452547,
      -1.2122569178634346,
      -1.2715051542212565,
      -1.404171762346768,
      -1.3405188639749053,
      -1.2595061571971258,
      -1.3100864521447952,
      -1.301160863611287,
      -1.2335031230081663,
      -1.3030896227444193,
      -1.3046870007987317,
      -1.3104815869773634,
      -1.2386714039973599,
      -1.3288838434177046,
      -1.2537300399504188,
      -1.300262657388368,
      -1.2337477011279654,
      -1.271566665256352,
      0.0,
      -1.2911153041052623,
      -1.3057256061689455
    ],
    [
      -1.2721366502790736,
      -1.1539583065776613,
      -1.1634686455445,
      -1.216899061866459,
      -1.164504439380051,
      -1.1665407180449476,
      -1.1706645268856646,
      -1.1815785404818318,
      -1.1277430458904711,
      -1.1264687014969899,
      -1.174730475558504,
      -1.259316606179564,
      -1.2148712882968808,
      -1.1484148424320488,
      -1.1964768427059966,
      -1.1766155468891324,
      -1.167830127301648,
      -1.1810942551710897,
      -1.2148281051581664,
      -1.1402461711786889,
      -1.1677665256365557,
      -1.218816342970447,
      -1.209026251092599,
      -1.1813680924624979,
      -1.133995674087768,
      -1.187788855465479,
      -1.1953893022042157,
      0.0,
      -1.24666579422354
    ],
    [
      -1.3247144209633686,
      -1.3434616739444796,
      -1.3271861998560546,
      -1.302739180207226,
      -1.3120348827053419,
      -1.3297434604486649,
      -1.3303028707815905,
      -1.3074445082223463,
      -1.3098574204525018,
      -1.2857435261193406,
      -1.3053310006037702,
      -1.3486255256470894,
      -1.3249969287286874,
      -1.309672074373944,
      -1.3124770828525123,
      -1.334109712162254,
      -1.2689632388943601,
      -1.3054842223109597,
      -1.329771587791042,
      -1.3062545628667894,
      -1.3114582255441218,
      -1.2937767152274597,
      -1.2986781440464874,
      -1.3214331766731118,
      -1.2884157796636777,
      -1.3030907023532623,
      -1.3155233255944425,
      -1.3381290343012897,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.23550403487329197,
      0.23793505394309422,
      0.23684760813751504,
      0.23629346966712372,
      0.22385887640283242,
      0.21984850625052976,
      0.21388270193187742,
      0.24956707235497833,
      0.25344140961856665,
      0.25947203384806095,
      0.15040246946056168,
      0.2347407766429206,
      0.25253016989785104,
      0.2419159505771007,
      0.24115230218259054,
      0.23994987120561495,
      0.23870333948716782,
      0.2249909358998634,
      0.2448517358345521,
      0.24918098312071457,
      0.23284463145405798,
      0.24440076816198886,
      0.22115079607100796,
      0.2365044782722321,
      0.24809509991739453,
      0.24741004687944113,
      0.21883273017632998,
      0.20946113072557804
    ],
    [
      0.3078119297672606,
      0.0,
      0.492684220008458,
      0.3624702946119984,
      0.4342426647062263,
      0.430484288766783,
      0.43127607041052385,
      0.37539155674590474,
      0.477903422314806,
      0.43989741964784934,
      0.445682888403506,
      0.27255193494369956,
      0.4260585635547154,
      0.46399276018436875,
      0.4196089452193128,
      0.4695949869393581,
      0.37558550654575407,
      0.4112740712386651,
      0.35327971697537186,
      0.4839609605108228,
      0.3698466788606767,
      0.38690808024716583,
      0.34260776072244514,
      0.3836320989427968,
      0.39795338186876394,
      0.3606414095735917,
      0.38459204390917123,
      0.4071744039769427,
      0.33385178343112853
    ],
    [
      0.3116951203843117,
      0.5043146850427012,
      0.0,
      0.43149813072850596,
      0.42661788916271814,
      0.37854906573206115,
      0.4781594778606204,
      0.35813011901672587,
      0.47389012632411975,
      0.4060938911547567,
      0.4874420757907292,
      0.25275293834673707,
      0.39983034976179654,
      0.42419301301466095,
      0.4443077590924309,
      0.4224684598633912,
      0.35766052382999103,
      0.4295936536861187,
      0.3642294749340538,
      0.47632179860225743,
      0.33280053412800803,
      0.3492754917271568,
      0.32264742716958184,
      0.3346211800812777,
      0.3841490046484406,
      0.3270942942759316,
      0.3796064302566169,
      0.3860584925485573,
      0.27866084214014464
    ],
    [
      0.2956021165560967,
      0.37325287634241233,
      0.38834256005019485,
      0.0,
      0.37849582539644167,
      0.3723153408465534,
      0.4100639740504577,
      0.36570909551812925,
      0.38546645530124546,
      0.3585920268352085,
      0.3751431505915037,
      0.2733823679024934,
      0.3571835348614534,
      0.4163363165270395,
      0.35593052079396315,
      0.39756199700317607,
      0.3575433353515356,
      0.4065819315183259,
      0.3674516735754072,
      0.3778722959257239,
      0.37970912305011084,
      0.37180706312207246,
      0.33603483324576966,
      0.3650740566136257,
      0.35023920465963543,
      0.35973327763986895,
      0.3678945865487675,
      0.360105253281809,
      0.3220152851899538
    ],
    [
      0.3072221540567568,
      0.45643734542826264,
      0.4330987023449253,
      0.3253971069437471,
      0.0,
      0.3831604127203243,
      0.41291090131157815,
      0.3641478135483891,
      0.4572122077710157,
      0.4532000205936624,
      0.4080385447656478,
      0.2347863997127555,
      0.39521957678231323,
      0.4309947691876861,
      0.4122266421399239,
      0.38293761226984624,
      0.3842172998623463,
      0.42850215682058157,
      0.34917205318343725,
      0.4310496765117471,
      0.343079864052634,
      0.32066865681976187,
      0.34900196489593305,
      0.3719737304906723,
      0.39632190511629073,
      0.36438790017408285,
      0.3625541706827702,
      0.34889625508994015,
      0.2814787301196666
    ],
    [
      0.30772078743387277,
      0.4254967452788363,
      0.4344540003045865,
      0.4216820422937433,
      0.44562599292658467,
      0.0,
      0.4203454821439363,
      0.3863577941087204,
      0.4257953657412341,
      0.43152391552410685,
      0.41445768362172175,
      0.29614097614663915,
      0.3516188297885001,
      0.43040996612864046,
      0.3783779127282978,
      0.44936220090813106,
      0.3796823545571588,
      0.45617544559021006,
      0.36999249478197704,
      0.3668025498878229,
      0.4154490324731783,
      0.3946687956980377,
      0.32070068875713487,
      0.40316133356723527,
      0.40494736919103635,
      0.354802492503808,
      0.4117205584401813,
      0.40713180517454295,
      0.33072618343378246
    ],
    [
      0.3059158317716082,
      0.4379030010794771,
      0.43289165893829806,
      0.40123761891656984,
      0.4492323853012474,
      0.4034790589833295,
      0.0,
      0.3522206180142169,
      0.4674194504942193,
      0.4294905760009835,
      0.3838311868462969,
      0.23774542664171117,
      0.36908480621707485,
      0.4364057314622498,
      0.4160211764701962,
      0.4026369319270877,
      0.3876679202203739,
      0.453828359974761,
      0.3202099412812578,
      0.40339959016299143,
      0.35288125541757487,
      0.2918063666829709,
      0.30124951652078447,
      0.3476183196527809,
      0.37057927864713114,
      0.36164881701595597,
      0.3713456297893094,
      0.39690919728864626,
      0.3146389644888683
    ],
    [
      0.3577848559772765,
      0.4234294927754272,
      0.4452174316340327,
      0.40640822772803076,
      0.4407098996845913,
      0.39542465655588743,
      0.3837575760232439,
      0.0,
      0.45235159029181826,
      0.42543163292601816,
      0.4514301504994025,
      0.2784913194231504,
      0.3883857983564243,
      0.44723041662111984,
      0.409590984991524,
      0.4130695352914253,
      0.42396762828211787,
      0.39919067866321756,
      0.40498584637804425,
      0.42315841532272547,
      0.4533073529599936,
      0.3866715775551113,
      0.3822192005550835,
      0.4025278217077519,
      0.4216693527066946,
      0.44438191512178826,
      0.40385265399148507,
      0.4084328684760168,
      0.4048413732505314
    ],
    [
      0.31457764335981797,
      0.4643257089427857,
      0.4807291731049679,
      0.4063675043418471,
      0.4712283824880803,
      0.43726361016864645,
      0.44577310619865007,
      0.41936688672816835,
      0.0,
      0.43247930940249124,
      0.42998432282139465,
      0.256181062394665,
      0.3684390204375707,
      0.5011469619692679,
      0.43068222077728313,
      0.4373428966965778,
      0.397656777024878,
      0.47236231116404337,
      0.371868562107333,
      0.4509621129597705,
      0.4145211747607227,
      0.3598666425466901,
      0.3653039321787812,
      0.4100978141917444,
      0.4225273126027487,
      0.3583946798494657,
      0.4077215327896364,
      0.456308409685505,
      0.3032081302302283
    ],
    [
      0.34790162862897445,
      0.46706793148468173,
      0.461624891809324,
      0.37063567365987726,
      0.5023663672297884,
      0.43763928068166624,
      0.4469561688687389,
      0.4084345765111175,
      0.5268573325061945,
      0.0,
      0.46929611670654015,
      0.25681987385185945,
      0.3471027819432102,
      0.49389332591876833,
      0.4471922125978034,
      0.4469408134198696,
      0.43585166114035445,
      0.4363244250532734,
      0.36144715744728595,
      0.4036794066343201,
      0.4089210864824184,
      0.38528212013628615,
      0.33990729522702745,
      0.3988668856415707,
      0.4806802794180389,
      0.43098920618724734,
      0.4293755761933462,
      0.4347592210031388,
      0.3621874490788344
    ],
    [
      0.3650902418265263,
      0.4738154094336504,
      0.5560403916839161,
      0.45230791760709566,
      0.4864758198260912,
      0.4414022018836803,
      0.48996951373370456,
      0.4217169596512338,
      0.5068678910867641,
      0.4412948874850844,
      0.0,
      0.27220617327484,
      0.4008451810423117,
      0.5349976225090536,
      0.44683874264849677,
      0.4284897104584213,
      0.441384639474278,
      0.44397073391621156,
      0.41748595113446507,
      0.4571723772448495,
      0.41458487559202606,
      0.42862206074022313,
      0.3943684602558344,
      0.400960366015118,
      0.40881372503035807,
      0.4041668925133115,
      0.41499144666112997,
      0.4068449206597975,
      0.3349657629601641
    ],
    [
      0.26457566239938046,
      0.27971623218959984,
      0.27179023124214274,
      0.26745313353549305,
      0.2682205995216551,
      0.28346752456999535,
      0.2921407680150929,
      0.30684564938518033,
      0.3025949570277522,
      0.3155462774572093,
      0.2889231949044946,
      0.0,
      0.35511917896574485,
      0.2534900523124888,
      0.2971695124473255,
      0.2736318951655381,
      0.31859764107998023,
      0.27484769588550795,
      0.2980423224036295,
      0.3158198024218051,
      0.29750646345585796,
      0.31930361977150623,
      0.27827461156715616,
      0.3360743591201154,
      0.29145681656583977,
      0.3182460864164469,
      0.28746599119818206,
      0.26067984428760393,
      0.33470347215139995
    ],
    [
      0.2753115362841978,
      0.3296832691390106,
      0.3449216339387571,
      0.3392356493623032,
      0.33306301500582824,
      0.3063197875518988,
      0.29445381487577094,
      0.29518945323504386,
      0.3410969961086323,
      0.30642768195666337,
      0.32077720614168226,
      0.2740411887802898,
      0.0,
      0.29265167624577915,
      0.3199328859983235,
      0.32549846402046034,
      0.31649363233153904,
      0.3530582497111381,
      0.2881850274236344,
      0.36085214529777887,
      0.29533884056632287,
      0.29019133512965034,
      0.28433205465602573,
      0.30682242629526835,
      0.28931417039979124,
      0.3162000115946182,
      0.3052874210606604,
      0.24721384897641352,
      0.2767957045709959
    ],
    [
      0.34292358552854774,
      0.46082148916073895,
      0.41714505657531076,
      0.4164947954365823,
      0.43269856799125184,
      0.38714093097041635,
      0.42636511474387473,
      0.42534372723888225,
      0.4639548101217217,
      0.4456687666432013,
      0.4709053207259206,
      0.24038799377042608,
      0.3455904352534438,
      0.0,
      0.3964155192381009,
      0.46634875330435177,
      0.37196304210538633,
      0.4121565306169652,
      0.39091531251872613,
      0.4271377459064223,
      0.44664571231603567,
      0.3386127233559799,
      0.3581519984086776,
      0.3891243326590641,
      0.43805294217834234,
      0.39246275784631046,
      0.3884301702261397,
      0.43067308296968276,
      0.305240133699497
    ],
    [
      0.3453158651954731,
      0.4379574536626305,
      0.460780829065111,
      0.35209291210664206,
      0.4669531588610363,
      0.37526540175121115,
      0.42041507690639657,
      0.3882298748147248,
      0.5056228842902473,
      0.45857369186872265,
      0.4467109581010935,
      0.29107590024389696,
      0.3889706565688391,
      0.4495609745629534,
      0.0,
      0.45568270771291064,
      0.39580425091795624,
      0.4354419429779932,
      0.3648026666883142,
      0.46701727036178986,
      0.3466248469068951,
      0.38572042881423774,
      0.39062654609647596,
      0.39078418190845765,
      0.4127454753263009,
      0.37695593456934207,
      0.399707362559907,
      0.3941702339388544,
      0.3234194400579964
    ],
    [
      0.32603589090215856,
      0.5100430641796427,
      0.51145228801453,
      0.4097605616230504,
      0.4490117815255221,
      0.4750439672944564,
      0.4242155952651172,
      0.4085795867523929,
      0.4493002646245343,
      0.4464573417753315,
      0.4703258038382965,
      0.2679852472656872,
      0.40677184446495906,
      0.5381855408677083,
      0.4636348480833128,
      0.0,
      0.3734125593430262,
      0.4774354052467371,
      0.4130410938600031,
      0.4685533683103935,
      0.39197511201978785,
      0.4078967633232071,
      0.3550080330486294,
      0.39549575468899256,
      0.3955257115325157,
      0.3782618429182616,
      0.4191197816953476,
      0.38413324024024353,
      0.3505543723392768
    ],
    [
      0.3169097516777326,
      0.3873971633618185,
      0.38795350945273355,
      0.389234747333878,
      0.4036700101326205,
      0.36902366974714207,
      0.4262310955811477,
      0.39340251692053974,
      0.4181347844044394,
      0.42962378604123885,
      0.4232261110954907,
      0.24471850861111277,
      0.34389982411307063,
      0.3915055217985901,
      0.3894610937942897,
      0.34421456479577506,
      0.0,
      0.38533644969047076,
      0.32388696854235866,
      0.3634386456848764,
      0.4042571750191535,
      0.37393277567004657,
      0.3739445368830885,
      0.3328272593535533,
      0.41387463560209614,
      0.3521295016245847,
      0.41844903902569763,
      0.3464073855574483,
      0.33337398491207826
    ],
    [
      0.31664277784279315,
      0.38662161013466734,
      0.4156048339239695,
      0.40779234082912663,
      0.42187158150513193,
      0.40523655397948266,
      0.4467725671372289,
      0.37482194436958105,
      0.4223017367894588,
      0.4123790515951897,
      0.40128612643878747,
      0.28841846756011114,
      0.385302727794232,
      0.45991551257439967,
      0.4253262141842149,
      0.44910910757121125,
      0.3664095077894751,
      0.0,
      0.40138896725342965,
      0.4262767968678758,
      0.4034721665442653,
      0.3605139441662226,
      0.3430574866186846,
      0.3569840094887833,
      0.36328975542145003,
      0.3449783064648928,
      0.37000369120576093,
      0.34268992402192855,
      0.349137770145282
    ],
    [
      0.32549066737884425,
      0.34576052147139147,
      0.38426700554040427,
      0.3664716276087663,
      0.33534126035932665,
      0.3528441628592589,
      0.34829817089971793,
      0.3491851781173305,
      0.3650265550086702,
      0.3539418995235748,
      0.3730196221823756,
      0.2763695175024752,
      0.33657643196295206,
      0.3417461947430722,
      0.33492394616209564,
      0.3298590537946988,
      0.36597582398362594,
      0.37691517719116896,
      0.0,
      0.3139966801971723,
      0.3682635764771782,
      0.3240449402881429,
      0.31955979040695515,
      0.32625562413529163,
      0.31141097616608193,
      0.34606377788212894,
      0.3829824224588394,
      0.3219527003238185,
      0.30328608279906355
    ],
    [
      0.30028416654365,
      0.4647277735917752,
      0.47188624789181155,
      0.3696849815037706,
      0.41259253019293407,
      0.3834750524693131,
      0.4118951857861113,
      0.3404944710080078,
      0.4711054726524284,
      0.4339932913418272,
      0.39346582512610295,
      0.26872117448363086,
      0.41954515103511514,
      0.44416524446356265,
      0.4169072804023597,
      0.43221462575826197,
      0.3486359510338817,
      0.4446458019819284,
      0.3286277442382606,
      0.0,
      0.32479168834351935,
      0.3164938905116761,
      0.31013736213908083,
      0.4169005118974427,
      0.35799894014535427,
      0.337330612278838,
      0.36765097996734064,
      0.4095053849140262,
      0.3144311779307716
    ],
    [
      0.32943494137244445,
      0.39991949267633675,
      0.4092606749485328,
      0.36971827839098714,
      0.4088431228747551,
      0.38511653430391113,
      0.3993745856810902,
      0.43862861415081156,
      0.43640236497450346,
      0.3762530947166325,
      0.4038788844370307,
      0.23881293757378375,
      0.336395696420857,
      0.43887362196107405,
      0.36981725153040723,
      0.40451585908690535,
      0.4089130076889669,
      0.43846477876097456,
      0.3721707377237218,
      0.37953141602761087,
      0.0,
      0.3754044631853928,
      0.3789256492591757,
      0.32634194714904874,
      0.40104848808889915,
      0.36926435685504355,
      0.39216173094885276,
      0.39030783918068823,
      0.35000598247085657
    ],
    [
      0.30784589338967483,
      0.3907991556637638,
      0.4170157444708005,
      0.3646952943135855,
      0.39931526921278215,
      0.41739578269807254,
      0.38755798380230955,
      0.39007285272593517,
      0.42156312900267623,
      0.3979256677492846,
      0.4252443714312959,
      0.27549039155529953,
      0.3719730126698444,
      0.3973750131593028,
      0.42332613984223677,
      0.3960619952250064,
      0.3756867665795727,
      0.39851325780947766,
      0.3571606830277789,
      0.38178190619231156,
      0.36669001030027815,
      0.0,
      0.36086552763012136,
      0.37830703641834496,
      0.38136131413313556,
      0.4045938693417228,
      0.4083692886988377,
      0.3359386054767839,
      0.3502274779596892
    ],
    [
      0.32492212561290046,
      0.38017717053552147,
      0.3990729241788471,
      0.3843754398897292,
      0.4094228748120743,
      0.33187497817258405,
      0.35743971280810816,
      0.38110718549757516,
      0.4117376816878884,
      0.40408529564154216,
      0.40733154986780695,
      0.28684378367404606,
      0.36531901465007266,
      0.39780429878950496,
      0.41486907451173805,
      0.38775517427395423,
      0.39358170575191886,
      0.38562654804357055,
      0.37424541888012475,
      0.34960846493129116,
      0.4055987753592034,
      0.38637066369347295,
      0.0,
      0.36981578185585073,
      0.4384347168411178,
      0.40120399994641875,
      0.42946303900675353,
      0.3632486049206807,
      0.3475924050407826
    ],
    [
      0.2724119983560602,
      0.3405674663951044,
      0.34267532147305024,
      0.3209089380215686,
      0.3371266969088895,
      0.32782318436166924,
      0.33306704821266186,
      0.3228370404611587,
      0.37828214511025915,
      0.3799925191930811,
      0.3328049425659534,
      0.33913765990342637,
      0.3467915550849039,
      0.38875184842916055,
      0.3428267751400478,
      0.347905297546667,
      0.2976642222014867,
      0.3491352451384595,
      0.30873375933520886,
      0.3809861448676186,
      0.3354653775698617,
      0.2963967848276776,
      0.29448876691761083,
      0.0,
      0.3784195091986544,
      0.32119699256197465,
      0.3019044882866251,
      0.3488212933735153,
      0.32655204857028775
    ],
    [
      0.27897500528369323,
      0.35753432141636177,
      0.3510407981613408,
      0.30735502513222457,
      0.36726696335239395,
      0.3221444741818631,
      0.3543806349756009,
      0.3332092542860634,
      0.3998482894731181,
      0.3520604271108694,
      0.35113311664110025,
      0.2277351444691873,
      0.29782663993789726,
      0.37943514587417604,
      0.3675765802863511,
      0.3430329336791993,
      0.33837255073127315,
      0.3563486654752772,
      0.3063005131933738,
      0.34938270266163607,
      0.32125903754898366,
      0.3268113955932095,
      0.3332878751683417,
      0.37666569467096633,
      0.0,
      0.35334333398392537,
      0.3315177239065059,
      0.34007594850974177,
      0.3087021615623937
    ],
    [
      0.3848167170412178,
      0.4253564022328875,
      0.43231626445033866,
      0.41484101916825944,
      0.4280476176795558,
      0.40436124767869464,
      0.40607395438930194,
      0.45511261644978585,
      0.4473455375391111,
      0.46894691119156184,
      0.44980626320882555,
      0.3337947335802596,
      0.41412237316280875,
      0.46227004085548407,
      0.4775373978546751,
      0.44556704946436376,
      0.4142531722705265,
      0.4370437565640968,
      0.45431463094881885,
      0.42551881204835906,
      0.4430408432534869,
      0.4592425118087877,
      0.4254962938058313,
      0.38948921882736576,
      0.45944627524278614,
      0.0,
      0.44275001520088697,
      0.3906249091553087,
      0.4195620466424437
    ],
    [
      0.3491108449255682,
      0.40307004078341957,
      0.3898208593770647,
      0.37979502612130833,
      0.41827056604288426,
      0.3992870599936875,
      0.4183385376780331,
      0.41997434252387955,
      0.44412818882656957,
      0.4625569288083897,
      0.4033086924505678,
      0.2706420843250563,
      0.33429498269691904,
      0.41530768947469854,
      0.36472739452702907,
      0.3736529830605373,
      0.441310723663658,
      0.371724223927405,
      0.37012684587309264,
      0.3643322596944609,
      0.43614244267446445,
      0.3459300032541197,
      0.42108380672140555,
      0.3745511892834563,
      0.4410661455438589,
      0.40324718141547233,
      0.0,
      0.383698542566562,
      0.3690882405028788
    ],
    [
      0.22111939749482223,
      0.33929774119623457,
      0.32978740222939584,
      0.27635698590743685,
      0.3287516083938449,
      0.32671532972894823,
      0.3225915208882313,
      0.3116775072920641,
      0.3655130018834247,
      0.366787346276906,
      0.3185255722153919,
      0.2339394415943319,
      0.278384759477015,
      0.34484120534184703,
      0.29677920506789923,
      0.3166405008847635,
      0.32542592047224783,
      0.3121617926028062,
      0.27842794261572945,
      0.353009876595207,
      0.32548952213734017,
      0.2744397048034488,
      0.2842297966812968,
      0.311887955311398,
      0.35926037368612795,
      0.3054671923084169,
      0.2978667455696802,
      0.0,
      0.2465902535503559
    ],
    [
      0.3510100822287754,
      0.33226282924766437,
      0.3485383033360894,
      0.37298532298491804,
      0.3636896204868021,
      0.3459810427434791,
      0.34542163241055346,
      0.3682799949697977,
      0.3658670827396422,
      0.3899809770728033,
      0.3703935025883738,
      0.32709897754505457,
      0.3507275744634566,
      0.3660524288181999,
      0.36324742033963164,
      0.3416147910298899,
      0.4067612642977838,
      0.3702402808811842,
      0.34595291540110185,
      0.3694699403253545,
      0.3642662776480221,
      0.3819477879646842,
      0.37704635914565654,
      0.35429132651903217,
      0.38730872352846624,
      0.37263380083888165,
      0.3602011775977014,
      0.33759546889085423,
      0.0
    ]
  ],
  "row_avgs": [
    0.2337060351069585,
    0.4014628515367167,
    0.39009508033230006,
    0.3651942885106777,
    0.3781533776213107,
    0.3941189571119164,
    0.37854637915028466,
    0.4099257233482119,
    0.4102388286401344,
    0.4192500266236271,
    0.4316675312981656,
    0.2947036998380045,
    0.30816746880923135,
    0.4009918339825001,
    0.4045367473870147,
    0.4202577380372543,
    0.37430232201525265,
    0.3874144814363442,
    0.34213676383658653,
    0.3825824471297422,
    0.38313522687283197,
    0.3815411943028545,
    0.38174744317411,
    0.33548839535759445,
    0.33687936990239536,
    0.42896780827556535,
    0.3917352795263017,
    0.3089987715073789,
    0.3618166752158519
  ],
  "col_avgs": [
    0.3162306864007299,
    0.4011878724185749,
    0.40886957186042955,
    0.3687180072942343,
    0.4019801979017208,
    0.3750747670642089,
    0.3912176348895833,
    0.370298211856187,
    0.4226127427304097,
    0.4025945016125984,
    0.39913732920912126,
    0.2666669319477567,
    0.36129003850394364,
    0.4140736808461682,
    0.38811327169451326,
    0.3901736858333704,
    0.3728724735620253,
    0.398414389629205,
    0.35290847705806444,
    0.3934266034996267,
    0.37182535103709696,
    0.35255982938896413,
    0.34239136938730635,
    0.36329653616278623,
    0.3855142950629354,
    0.36099698370070454,
    0.37444270516984196,
    0.3663996576666207,
    0.3244749424983904
  ],
  "combined_avgs": [
    0.2749683607538442,
    0.4013253619776458,
    0.3994823260963648,
    0.366956147902456,
    0.3900667877615157,
    0.38459686208806265,
    0.38488200701993397,
    0.39011196760219946,
    0.41642578568527205,
    0.4109222641181127,
    0.41540243025364343,
    0.2806853158928806,
    0.3347287536565875,
    0.40753275741433415,
    0.396325009540764,
    0.40521571193531236,
    0.373587397788639,
    0.3929144355327746,
    0.3475226204473255,
    0.38800452531468443,
    0.37748028895496444,
    0.3670505118459093,
    0.36206940628070816,
    0.34939246576019034,
    0.3611968324826654,
    0.39498239598813495,
    0.3830889923480718,
    0.3376992145869998,
    0.34314580885712115
  ],
  "gppm": [
    571.384867597165,
    551.9585945161703,
    547.1807814889288,
    567.7351028477807,
    549.585955843579,
    564.8441275632966,
    555.072578707874,
    566.0950806049681,
    542.4896858135778,
    551.2872662010279,
    551.3647791688298,
    613.2562004180888,
    570.6906748890693,
    547.8700975725109,
    555.7586938994831,
    556.2656918684031,
    560.915310769885,
    550.8546368740064,
    572.4401989868561,
    553.5006510714239,
    563.4853839090916,
    575.2663259329636,
    578.9895592261234,
    569.5088751216464,
    558.1004028840533,
    570.1702713002779,
    565.3584460411223,
    565.7759106888132,
    587.0083550088783
  ],
  "gppm_normalized": [
    1.346516100917178,
    1.242951860944628,
    1.2364770451335425,
    1.279086818603671,
    1.2354980969324834,
    1.2691938745612947,
    1.2513615090214343,
    1.2737471420485367,
    1.222748180379024,
    1.2412034219418957,
    1.2367896569705237,
    1.3904880655785046,
    1.288987882771135,
    1.2347296578675266,
    1.2454963220110984,
    1.2540880041104103,
    1.26220336609132,
    1.2399281073378248,
    1.2923673278687633,
    1.2510144485356869,
    1.265759017011528,
    1.2834026744680114,
    1.2980127259793623,
    1.2846164119624905,
    1.248127704245362,
    1.280988178171686,
    1.2661498473994517,
    1.2718196856491617,
    1.3200886456938563
  ],
  "token_counts": [
    1207,
    434,
    493,
    429,
    428,
    406,
    454,
    432,
    465,
    440,
    395,
    511,
    454,
    459,
    388,
    451,
    431,
    433,
    459,
    475,
    406,
    332,
    377,
    453,
    375,
    404,
    371,
    411,
    404,
    540,
    455,
    437,
    430,
    521,
    409,
    389,
    478,
    408,
    402,
    425,
    502,
    482,
    414,
    404,
    440,
    399,
    395,
    422,
    465,
    395,
    363,
    418,
    397,
    461,
    389,
    405,
    485,
    416,
    591,
    436,
    437,
    455,
    402,
    510,
    464,
    508,
    466,
    453,
    422,
    488,
    439,
    436,
    446,
    468,
    473,
    418,
    388,
    465,
    435,
    376,
    411,
    407,
    434,
    468,
    401,
    449,
    396,
    492,
    472,
    441,
    427,
    466,
    457,
    445,
    400,
    431,
    393,
    410,
    438,
    478,
    432,
    433,
    448,
    433,
    445,
    436,
    445,
    407,
    418,
    402,
    424,
    470,
    388,
    383,
    515,
    389
  ],
  "response_lengths": [
    2243,
    2612,
    2450,
    2438,
    2578,
    2548,
    2527,
    2219,
    2381,
    2156,
    2332,
    2516,
    2635,
    2406,
    2416,
    2551,
    2422,
    2513,
    2509,
    2356,
    2198,
    2366,
    2304,
    2320,
    2529,
    2123,
    2172,
    2832,
    2102
  ]
}