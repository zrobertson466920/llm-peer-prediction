[
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nAmos: AN ADAM-STYLE OPTIMIZER WITH ADAPTIVE WEIGHT DECAY TOWARDS MODEL-ORIENTED SCALE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe present Amos, a stochastic gradient-based optimizer designed for training deep neural networks. It can be viewed as an Adam optimizer with theoretically supported, adaptive learning-rate decay and weight decay. A key insight behind Amos is that it leverages model-specific information to determine the initial learningrate and decaying schedules. When used for pre-training BERT variants and T5, Amos consistently converges faster than the state-of-the-art settings of AdamW, achieving better validation loss within ≤ 70% training steps and time, while requiring ≤ 51% memory for slot variables. Our code is open-sourced at: https: //anonymous-url.\n\n1\n\nINTRODUCTION\n\nThe Adam (Kingma & Ba, 2015) optimizer is widely used for training deep neural networks, demonstrating fast convergence especially in the early stages of training. Although previous works have found issues regarding the theoretical convergence of Adam as the training proceeds (Reddi et al., 2018), in practice it is remedied by various learning-rate schedules and weight decay (Loshchilov & Hutter, 2019). Specifically, Adam with linear learning-rate decay and constant weight decay is the standard setting for pre-training large language models such as BERT (Devlin et al., 2019). However, these decay settings are usually ad-hoc, increase the number of hyper-parameters, and may introduce additional complexities in usage. For example, the linearly decaying learning-rate schedule requires knowing the number of training steps in advance, which makes it nontrivial to continuously train a model after the learning-rate decays to 0.\n\nIn this work, we present Amos, a new optimizer with a theoretically supported and adaptive schedule for learning-rate and weight decay, which can significantly outperform the state-of-the-art AdamW settings for pre-training language models, provide guidance for hyper-parameter tuning, reduce the memory usage, and train continuously without having to specify the number of training steps a priori.\n\nA key insight behind Amos is a hyper-parameter ̃η to be provided by the model architecture, which indicates the expected scale of the trainable weights ̃θ of the model (§ 2), i.e., theoretically we assume that an optimal point ̃θ∗ exists within the | ̃θ∗| ≤ ̃η diameter. Deep neural networks are likely to satisfy such a constraint without degrading performance, because there exist many good local minima; and we show that an appropriate ̃η for Amos can improve generalization and accelerate convergence (§ 5.2). In this work, ̃η is calculated in a consistent way from the input/output scale of neural network components, which is hinted by the model design (§ A.4). Given ̃η, Amos decides a learning-rate per variable, and its L2 regularization will lead the trained weights to the specified scale. The decay of the learning-rate is then determined by the L2 regularizer. Thus, Amos performs better because it can utilize the model-oriented information ̃η efficiently; the name Amos stands for “Adaptive weight decay towards Model-Oriented Scale”.\n\nEmpirically, we focus on the Transformer architecture (Vaswani et al., 2017) since it is pre-dominant in pre-trained language models (Bommasani et al., 2021), but add additional experiments on LSTM (Gers et al., 2000) and ResNet (He et al., 2016). We apply Amos to the pre-training of 4 models: BERT (Devlin et al., 2019), two Transformer variants with relative position representations (Su et al., 2021; Shaw et al., 2018), and the T5 model (Raffel et al., 2020); some with various model sizes and batch sizes. In all experiments, Amos consistently outperforms the state-of-the-art setting, achieving better validation loss within ≤ 70% training steps and time (§ 5.1). Compared to AdamW,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe memory usage for slot variables is reduced to ≤ 51% in Amos (§ A.8). In addition, Amos does not calculate learning-rate from a maximum number of training steps, so one can seamlessly continue training from any checkpoints, which is not trivial for AdamW with linear learning-rate decay (§ 5.1).\n\n2 THE ALGORITHM\n\nFor notation, we denote model weights by ̃θ, and an online learning algorithm recursively calculates a sequence of weights, ̃θ1, ̃θ2, . . ., from initial weights ̃θ0 and training examples zt at each step t = 0, 1, . . . . An optimizer uses the gradient ̃gt = ∇(cid:96)(zt; ̃θt) to compute a weight update ̃θt+1 ← ̃θt − ̃δt, in order to minimize the loss function (cid:96)(z; ̃θ). In neural network models, the model weights ̃θ is an array of trainable tensors (i.e. variables) collected from all model components; we view a variable and its slices as subsets of the model weights (e.g. θ ⊆ ̃θ is a variable slice that functions in part of the model). We use a bold letter to denote an array (e.g. θt, ̃θt), and the same normal letter to denote a scalar element of that array (e.g. θt) for describing element-wise operations. We use tilde for information of the whole model (e.g. ̃θt), and drop tilde to indicate subsets (e.g. θt).\n\nTo start, we recall the update rule of the RMSProp optimizer (Tieleman & Hinton, 2012), which computes the weight update by δt ← α√ gt, where α is a scalar learning-rate and vt a running vt average of the squared gradients g2 t . Based on this, Adam (Kingma & Ba, 2015) replaces gt with its running average mt (i.e. momentum), and adopts bias correction ˆmt, ˆvt for running averages. Further, AdamW (Loshchilov & Hutter, 2019) allows a schedule for learning-rate αt (depending on (cid:1), where γ is a constant hyper-parameter. the step t) and adds a weight decay: δt ← αt For pre-training Transformer variants, the learning-rate schedule αt is set to linearly decay to 0 after warm-up. Therefore, a maximum number of training steps before the learning-rate decays to 0 has to be set as a hyper-parameter. Amos, with a similar construction, has the following update rule:\n\nˆmt + γθt\n\n(cid:0) 1√ ˆvt\n\nδt ← dt\n\n(cid:16) ξη √\nˆvt\n\ngt +\n\n(cid:17)\n\nγtθt\n\n1 2\n\nwhere γt ← ct\n\nξ2 ˆvt\n\nM2(gt)2.\n\n(1)\n\n(cid:113)\n\n(cid:80)k\n\n1 k\n\ni denotes the quadratic mean of entries of an array a ∈ Rk. The update Here, M2(a) := rule consists of a gradient descent part (the term containing gt) and an L2 regularization part (the term containing θt)1, similar to AdamW. The full Amos is shown in Algorithm 1. We explain several novel aspects below.\n\ni=1 a2\n\nModel-oriented scale: For each variable a ⊆ ̃θ in the model weights, we specify the scale η(a) we expect a to converge to, i.e. M2(a∗) ≈ η for an optimal ̃θ∗ ⊇ a∗. Different variables may have different scale η’s. For a common case of a linear transformation, y = xW + u (W , u ⊆ ̃θ, W ∈ Rm×n, x ∈ Rm), we calculate η(W ) by assuming that x is random Gaussian with standard m) deviation σx, and y random Gaussian with standard deviation σy; so we have η(W ) = σy/(σx in order to satisfy the input/output standard deviation (assuming entries of W to be Gaussian as well). Additionally, we set η(u) = σy/2 to ensure that u has a slightly smaller magnitude than xW . The input/output standard deviation can be hinted by other layers in the model; for example, the activation function GELU (Hendrycks & Gimpel, 2016) usually expects the inputs to have standard deviation ≈ 1, because its non-linearity mostly lies within that range; also the output standard deviation of LayerNormalization (Ba et al., 2016) is expected to be 1. For Transformer variants, we will discuss the input/output standard deviation of all types of non-linear layers and derive ̃η in § A.4.\n\n√\n\nFactored initial learning-rate: In Amos, we use ξη as the initial learning-rate, where η is the modeloriented scale specified for each variable, and ξ is a global learning-rate shared across all variables. For online optimizers, the learning-rate is generally affected by both data and model; by factoring the initial learning-rate into ξ and η, we disentangle the two to some extent: While ξ is tuned and may depend on the data, ̃η is calculated from the model architecture.\n\n1Following Loshchilov & Hutter (2019), we decouple the gradient of an L2 regularization term (taking the form of a weight decay) apart from the adaptive gradient normalization factor . When an adaptive optimizer is used, Loshchilov & Hutter (2019) point out that the decoupled weight decay is not equivalent to the L2 regularization without explicit decoupling, and the former is more appropriate. In this work, we always treat L2 regularization as decoupled weight decay, and use the two terms interchangeably.\n\n1√\n\nˆvt\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nAdaptive L2 regularization: Unlike AdamW which uses a constant γ for weight decay, the Amos weight decay ̃γt is intended to control the scale of trained variables, rather than regularize the loss function; so γt decays to 0 at t → ∞ to be less biased, and it is adaptive in the sense that ̃γt depends on ̃gt so that the variables not getting gradient updates are not regularized. Thus, the L2 regularization is robust to sparse gradients, and it does not introduce any additional hyper-parameter. We will give a heuristic derivation of the form of γt in § 4. Decay factors: ̃dt, ̃ct are per-parameter decay factors such that d0 = c0 = 1 and dt, ct monotonically decrease to 0 at t → ∞. We provide a theoretical derivation of the asymptotic behavior of these factors in § A.2, together with a default form that works well empirically in all our experiments. The decay factors do not depend on a maximum number of training steps, thus enabling arbitrary continuous training.\n\nMemory Reduction: Most previous optimizers operate element-wise, so the slot variables (e.g. the running average ̃vt, ̃mt in Adam) have the same shape as ̃θ, which can be memory consuming. In Amos, two slot variables ( ̃vt, ̃bt in Algorithm 1) are shared by certain slices in the model weights, reducing the memory usage of these slot variables. For example, if Rm×n (cid:51) W ⊆ ̃θ is a linear transformation, the corresponding vt ∈ R1×n is shared by the input dimension of W , reducing the memory usage by m times. As a result, in Equation 1 and Algorithm 1, vt, bt, ct and dt are reduced and become scalars, to be used and updated by vector-valued gt and θt. In this work, we reduce the input dimension of linear transformations, the embed dimension of embedding matrix, and all dimensions for other variables by default. An ablative study with different settings is found in § A.8.\n\nAlgorithm 1 The Amos optimizer at step t. Input ̃gt = ∇(cid:96)(zt; ̃θt): The gradient of loss (cid:96) on a random example zt. Input ̃θt: Trainable model weights at step t. Input ̃vt−1, ̃bt: Slot variables of shape broadcastable to ̃θ, initialized to 0. Input (Optional) ̃mt: Slot variable of the same shape as ̃θ, initialized to 0 for momentum. Hyper-parameter ξ: Global learning-rate. Hyper-parameter ̃η: Expected scale for model weights ̃θ. Hyper-parameter ̃ct: Decay factor for L2 regularization. Defaults to ct = (cid:0)1 + 1 Hyper-parameter ̃dt: Decay factor for learning-rate. Defaults to dt = (cid:0)1 + 1 4\nHyper-parameter β ∈ [0, 1): Exponential decay rate for running average ̃vt.\n\nξbt (cid:1)−1\n\n4 ξηbt\n\n√\n\n√\n\n.\n\n(cid:1)− 1 2 .\n\nχ max(χ, |gt|)\n\n1: (Optional) gt ←\n\ngt\n\n(cid:46) Gradient clipping with hyper-parameter χ > 0.\n\n2: vt ← βvt−1 + (1 − β) M2(gt)2 3: ˆvt ← vt/(1 − βt)\n\nM2(gt)2\n\n4: γt ← ct\n\n5: δt ← dt\n\nξ2 ˆvt (cid:16) ξη √\nˆvt\n\ngt +\n\n(cid:17)\n\nγtθt\n\n1 2\n\n6: bt+1 ← bt + γt(1 + bt)\n\n(cid:46) Running average of squared gradients.\n\n(cid:46) Bias correction.\n\n(cid:46) Adaptive L2 regularization strength.\n\n(cid:46) Amos update rule.\n\n(cid:46) Decay factor update.\n\n7: (Optional) δt ← mt+1 ← μmt + (1 − μ)δt Output: Updated model weights θt+1 ← θt − δt. Output: Updated slot variables ̃rt, ̃bt+1 and optional ̃mt+1.\n\n(cid:46) Momentum with hyper-parameter μ ∈ [0, 1).\n\nHyper-parameter Tuning The running average vt in Amos is a low-cost estimator for E[M2(gt)2], where the expectation is taken over the example zt randomly drawn from the training data. It is similar to vt in Adam except that the mean square M2(gt)2 is used instead of element-wise g2 t , due to the memory reduction. Thus, the hyper-parameter β behaves similarly to β2 in Adam: Since the estimator mostly depends on the previous 1/(1 − β) steps, β should be close enough to 1 to make the estimator accurate, but not too large that the model weights in the previous 1/(1 − β) steps differ too much from the current step. We set β = 0.999 by default (the same as β2 in Adam), and it is found that β should be smaller with larger batch size (Shazeer & Stern, 2018; Liu et al., 2019).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nThe global learning-rate ξ can depend on the step t to follow a warm-up schedule at the beginning of training, but a schedule with learning-rate decay is not necessary, since the decay factor ̃dt is already included in Algorithm 1; most of the time ξ remains a constant. While this constant is the major hyper-parameter to be tuned, a good rule of thumb is to set ξ to the same order of magnitude as N , where N is the number of independent batches in the training set (see § 4 for a justification). 1/ This value is usually larger than the typical learning-rates used for Adam. It also implies that ξ should be in proportion to the square-root of the batch size, which we observe in practice as well (§ A.5).\n\n√\n\nIn addition, Algorithm 1 includes optional gradient clipping and momentum. Momentum in Amos is applied after the main update rule (unlike Adam which applies it before). It can improve performance for pre-training Transformer variants, but consume memory because the slot variable ̃mt must have the same shape as ̃θ. When momentum is applied, its decay rate μ is typically set to 0.9.\n\n3 RELATED WORK\n\nBesides RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2015) and AdamW (Loshchilov & Hutter, 2019), the number of previous works on optimization is vast, so we focus on some directly related alternatives below. Also, we note that Amos is a stochastic first-order optimizer, in contrast to recent progress in the second-order optimization methods (Gupta et al., 2018). The convergence of stochastic optimizers has been studied in terms of stochastic approximation (Bottou, 1998), regret (Hazan, 2019), or nonconvex stochastic programming (Ghadimi & Lan, 2013). In particular, Reddi et al. (2018) observed cases of non-convergence of Adam (with constant learning-rate) and proposed a fix. In our work, we analyze the behavior of Amos in an intuitive and heuristic manner, but leave a rigorous convergence proof (e.g. based on regret) to future work.\n\nAdaGrad: The update rule of AdaGrad (Duchi et al., 2011) is δt ← α√ gt, where bt+1 ← bt + g2 t\nbt is similar to the bt in Algorithm 1, in the sense that both AdaGrad and Amos use a (weighted) sum of squared gradients to decay learning-rates. Such decay is “adaptive” because the learning-rate will decay more for parameters getting more updates, which is suitable for sparse gradients. On the other hand, conventional wisdom is that the learning-rate in AdaGrad might decay “too fast” in some cases, which makes the convergence slow, and Adam mitigates this issue by using a running average of squared gradients instead of the decay factor. However, the AdamW setting suggests that the normalization factor by running average of squared gradients is not a replacement for learning-rate decay; one still needs a linearly decaying learning-rate schedule for better convergence. Thus, Amos integrates both the Adam-style gradient normalization and the AdaGrad-style learning-rate decay; with gradient normalization, the learning-rate can actually decay faster and it converges faster.\n\nSGD with L2 Regularization: For the classic Stochastic Gradient Descent (SGD) algorithm, it is recommended to decay the learning-rate by the factor 1 λt , where λ is the smallest eigen-value of the Hessian (Murata, 1998). Although λ is generally unknown, adopting an L2 regularizer of strength λ(cid:48) guarantees that λ ≥ λ(cid:48), so one can set the learning-rate to 1 λ(cid:48)t (Bottou, 2012). In Amos, we adopt a similar idea to heuristically derive the learning-rate decay (see § A.3 for more detailed discussion), by connecting the decaying speed with the strength of L2 regularization (i.e., the L2 strength γt in Algorithm 1 also appears in the update of bt). Unlike SGD, both the learning-rate and L2 regularization in Amos decay adaptively. The adaptive L2 regularization, in particular, is a novel component unseen in previous optimizers.\n\nLAMB: The LAMB optimizer (You et al., 2020) and its origin LARS (You et al., 2017) share several similar aspects with Amos. The idea of layer-wise learning-rate in LAMB and LARS is similar to the per-variable learning-rate ̃η in Amos; they all normalize the gradients in some way; and they all imply scaling up the learning-rate as the batch size increases. In our experiments, scaling the global learning-rate of Amos in proportion to the square-root of the batch size indeed works (§ A.5), although we leave a systematic study of scaling-up to extremely large batch sizes and comparing with LAMB and LARS to future work.\n\nIn Adam, the slot variable ̃vt for maintaining running average of squared gradients AdaFactor: requires the same amount of memory as the model weights ̃θ. In order to reduce the memory usage, AdaFactor (Shazeer & Stern, 2018) proposes to use nonnegative matrix factorization to decompose any matrix into two vectors. In contrast, Amos reduces memory usage by simply reducing some axes\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nof the slot variables and broadcasting to the shape of model weights. This reduction is more efficient than AdaFactor, and our experiments suggest that it will not degrade performance (§ A.8).\n\n4 DERIVATION OF AMOS\n\nIn this section, we heuristically derive the Amos update rule (Equation 1). We start from a general form of the weight update for a given variable θ,\n\nθt+1 = θt − αtgt where ̃gt = ∇(cid:96)(zt; ̃θt),\n\n(2)\n\nand gradually pin down to the specific form of Equation 1. Here, the step size αt > 0 is a scalar (due to our memory reduction mechanism in § 2) and is shared across the elements of the vector-valued gt, θt ∈ Rk. We are focusing on a subset of model parameters, but furthermore note that αt may differ for different variables.\n\nThen, the following Descent Lemma (Murata, 1998) provides a sanity check for a wide range of possible forms of αt, while also suggests some constraints. Its proof can be found in § A.1.\n\nLemma 4.1 (Descent Lemma). If αt does not depend on zt, then there exists (cid:15)t > 0 such that\n\nEt[Et+1[(cid:96)(zt+1; ̃θt+1)]] ≤ Et[(cid:96)(zt; ̃θt)]\n\nfor any αt < (cid:15)t,\n\nwhere Et[•] denotes the expectation taken over the random example zt drawn from the training data at step t, while conditioned on zt−1, . . . , z0 of the previous steps.\n\nIn light of Lemma 4.1, we require (I) αt does not depend on zt (but may differ for different variables), and (II) αt decays to 0 at t → ∞, so the step-size can be sufficiently small that the Descent Lemma applies and Equation 2 will always make progress on average.\n\nIn the Amos update rule, αt = dt and ˆvt depends on zt, which seems to violate requirement (I) above. However, ˆvt should be regarded as an approximation of E[M2(gt)2], where E[•] denotes the expectation taken over examples randomly drawn from the training data, which is zt independent.\n\nξη√ ˆvt\n\nNext, we add an L2-regularization term to Equation 2:\n\nθt+1 = θt − (αtgt + ρtθt)\n\n(3)\n\nwhere ρt ≥ 0 can depend on gt (hence “adaptive”), but we require (III) E[ρt] does not depend on gt. The intuition behind is that an L2-regularization should have the same strength across all variables, rather than be affected by the typical gradient magnitude on each variable. It is the same intuition that motivates the weight decay decoupled from gradient adaptive factors (Loshchilov & Hutter, 2019).\n\nThe first challenge for Amos is to keep a balance between αt and ρt, so that M2(θt) will converge to the pre-specified, per-variable hyper-parameter η. In order to achieve this, we will declare some intuitions on the largeness of gt, E[gt] and ρtθt, as a guide for our heuristic derivation. For deep neural networks, gt’s upon different zt’s appear to be randomly noisy, so they will cancel out when being averaged to E[gt]; which means that M2(E[gt]) is usually much smaller than M2(gt). On the other hand, θt does not depend on zt, and it changes slowly between different steps, so the update by ρtθt is easier to accumulate than αtgt. This means that the magnitude of ρtθt can be kept smaller than αtgt while still compete with αtE[gt]. In Amos, ρt = dt M2(gt)2 decays to 0 faster than αt (due to the extra decay factor ct), which we assume will make ρtθt small enough compared to αtgt, when t is large. Quantitatively, we consider the error ̃εt = ̃θt − ̃θ∗, where ̃θ∗ is a local minimum. Equation 3 implies\n\n2 ct\n\nξ2 ˆvt\n\n1\n\nM2(εt+1)2 = M2(εt)2 −\n\n≈ M2(εt)2 −\n\n2 k\n2 k\n\n(αtgt + ρtθt) · εt + M2(αtgt + ρtθt)2\n\n(αtgt + ρtθt) · εt + α2\n\nt M2(gt)2,\n\n(4)\n\nwhere we investigate a time point t large enough that the model nearly converges. At this point, ρtθt is small compared to αtgt, so it can be approximately omitted in the third term. And we should have\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nE[gt] ≈ 0 and M2(θt) ≈ η if the trained weights converge to scale η. So taking E[•] of Equation 4, we should get\n\nE[M2(εt+1)2] ≈ M2(εt)2 −\n\n2 k\nFurthermore, in order for the model to converge, we should have E[M2(εt+1)2] ≤ M2(εt)2 from the above. Hence, we should have\n\nE[ρt]θt · εt + α2\n\nE[M2(gt)2].\n\nt\n\nα2\n\nt\n\nE[M2(gt)2] ≤\n\n2 k\n\nE[ρt]θt · εt ≤ 2E[ρt] M2(θt) M2(εt) ≈ 2E[ρt]η M2(εt)\n\nas a necessary condition for the trained weights to converge to scale η. By setting ρt to the smallest possible, we get\n\n2ρtη M2(εt) = α2\n\nt M2(gt)2,\n\n(5)\n\nwhich is an important relation connecting ρt to αt. We require (IV) Equation 5 to be satisfied throughout the course of training, and use it ubiquitously in our derivation. It is out of the scope of this work to prove whether Equation 5 actually makes M2(θt) converge to η; but the requirements so far already determine a basic form of the Amos update rule (as shown in Lemma 4.2 below), and our experiments suggest that Amos indeed brings the trained weights to the specific scale (§ 5.2).\n\nLemma 4.2 (Basic Form of Amos). Assume Equation 5, requiring that αt does not depend on zt and E[ρt] does not depend on gt. Then, we have\n\nαt ∝\n\n1 (cid:112)E[M2(gt)2]\n\nand\n\nρt ∝\n\nM2(gt)2 E[M2(gt)2]\n\n.\n\nThe proof is found in § A.1. It is noteworthy that the Adam-style gradient normalization naturally occurs in αt. Based on Lemma 4.2, Amos is derived by specifying the initial learning-rate and decay schedule. For that, we need the following assumption to quantify the largeness of gt and E[gt].\n\nAssumption 1. A scalar ξ > 0 exists such that\n\nM2(E[gt]) (cid:112)E[M2(gt)2]\n\n≥ ξ for all t and across all variables.\n\nThis assumption formalizes two intuitions, i.e. randomly noisy gt will cancel out when being averaged to E[gt] (so ξ has a small value), and as the training proceeds, M2(gt) will decrease2 along with M2(E[gt]) (so the ratio remains larger than a constant ξ > 0). Assumption 1 is verified by our experiments (§ A.6).\n\nThe value of ξ is related to the global learning-rate in Amos (as shown in Lemma 4.3 below), which is tuned as a hyper-parameter in practice. However, we also provide an intuitive estimation of ξ, which is usually a good start for hyper-parameter tuning. The intuition is to view the canceling out of gt averaged to E[gt] as similar to the average of N i.i.d. samples drawn from a distribution of mean 0. According to the Law of Large Numbers, the variance of the average (i.e. M2(E[gt])2) is about 1/N of the variance of the distribution (i.e. E[M2(gt)2]), so ξ ≈ 1√ . In reality, the gradients of deep neural networks, computed over mini-batches, appear to be highly random. So N is usually of the same order of magnitude as the number of independent batches in the training data.\n\nN\n\nNow, we can derive the optimal initial learning-rate as below, under an ideal condition that g0 points to the same direction as ε0. The proof is found in § A.1. Lemma 4.3 (Initial Learning-rate). Assume Equation 2, Assumption 1, α0 = α/(cid:112)E[M2(g0)2] and that g0 points to the same direction as ε0. Then,\n\nand the RHS achieves minimum at α = ξ M2(ε0) ≈ ξη.\n\nE[M2(ε1)2] ≤ M2(ε0)2 − 2αξ M2(ε0) + α2\n\n2As the training proceeds, E[gt] will converge to ≈ 0, so Assumption 1 is related to the observation that, for highly expressive models, ̃gt = ∇(cid:96)(zt; ̃θ∗) can get close to 0 for every zt in the training data (Ma et al., 2018). However, Assumption 1 only requires that M2(gt) decreases as fast as E[gt], which is empirically verified (§ A.6). Whether M2(gt) actually converges to 0 is not guaranteed (because the training may stop early, or E[gt] not get to exactly 0 due to L2-regularization, etc.) and not used in our theory. On the other hand, M2(gt) is always large compared to E[gt], because ξ is a small value.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 4.3 suggests the initial learning-rate α0 =\n\nξη√\n\nE[M2(g0)2]\n\n. Then, we get ρ0 = 1\n\n2 ξ2 M2(g0)2\n\nE[M2(g0)2]\n\nfrom Equation 5. By adding the decay factors, we reveal the Amos update rule (Equation 1):\n\nδt ← αtgt + ρtθt, where αt = dt\n\nξη (cid:112)E[M2(gt)2]\n\nand ρt = dt\n\n1 2\n\nγt = dt\n\n1 2\n\nctξ2 M2(gt)2 E[M2(gt)2]\n\n.\n\n(6)\n\nHere, dt and ct monotonically decrease to 0 and d0 = c0 = 1. In particular, ct decaying to 0 ensures that ρt decays to 0 faster than αt, so ρtθt can be sufficiently small compared to αtgt for large t, which justifies the approximation of Equation 4. In § A.2, we will further derive that ct = (1 + pbt)− 1 and dt = (1 + qbt)−1, where p, q are constants, together with the update rule of bt. The specific p = 1 4\n\nξη are found through experiments and work well in practice.\n\nξ and q = 1\n\n√\n\n√\n\n4\n\n2\n\n5 EXPERIMENTS\n\nWe focus on the Transformer model (Vaswani et al., 2017), and pre-train several variants as below.\n\nBERT: A Transformer Encoder model with learned position embeddings (Devlin et al., 2019). We experiment with the base (12-layer 768-hidden) and large (24-layer 1024-hidden) model sizes.\n\nRoPE: A Transformer Encoder variant with the Rotary Position Encoding (Su et al., 2021). RoPE is integrated in some recent large-scale language models (Chowdhery et al., 2022). It encodes relative positions but the encoding is not learned. We experiment with the base (12-layer 768-hidden) and large (24-layer 1024-hidden) model sizes.\n\nRelative Position Embeddings (RPE): A Transformer Encoder variant with learned relative position embeddings (Shaw et al., 2018). It achieves better performance but the pre-training is more costly on TPU (Tian et al., 2021). We experiment with the base (12-layer 768-hidden) model size.\n\nT5 Encoder-Decoder (T5): A Transformer Encoder-Decoder model implemented by Raffel et al. (2020). We experiment with the large (24-layer 1024-hidden) model size.\n\nFor encoder only models, we pre-train with the Masked Language Modeling loss (Devlin et al., 2019) on Wikipedia3 and the Books Corpus (Zhu et al., 2015). Following Liu et al. (2019), we use batch size 1024 for base-sized models, and pre-train 200k or 300k steps. For BERT-large, we use batch size 4096 and pre-train 250k steps. For RoPE-large, due to memory limitations we have to use batch size 1024 and pre-train 1M steps. For T5, the batch size is 4096 and we pre-train with the Span Corruption loss on the C4 corpus (Raffel et al., 2020), for 250k steps. More detailed settings are found in § A.5.\n\nAs an additional evaluation, we also applied Amos to the ResNet model (He et al., 2016) on the ImageNet (Deng et al., 2009) dataset. The experiment settings and results are shown in § A.9.\n\nFigure 1: Pre-training 3 models of the base (12-layer 768-hidden) size: (a) BERT, (b) RoPE and (c) RPE. We show training loss on the top and validation loss on the bottom.\n\n3https://en.wikipedia.org/wiki/Main_Page\n\n7\n\n(b) RoPE-base(c) RPEAdamW-200kAdamW-300kAmosAdamW-200kAdamW-300kAmosAmos-*Scale(a) BERT-baseAdamW-200kAdamW-300kAmosAdamW-Cont.AdamW-rsqrtUnder review as a conference paper at ICLR 2023\n\nFigure 2: Pre-training 3 models of the large (24-layer 1024-hidden) size: (a) BERT, (b) RoPE and (c) T5. We show training loss on the top and validation loss on the bottom. For T5, the training loss is different from the cross-entropy loss due to an extra regularization term. See § A.5 for details.\n\n5.1 LEARNING CURVE OF PRE-TRAINING TRANSFORMER VARIANTS\n\nIn Figure 1 and Figure 2, we show training and validation loss of pre-training the Transformer variants. In all experiments, across different model architectures, model sizes, batch sizes, datasets and loss functions, Amos (pink curve) outperforms the state-of-the-art AdamW setting, with the loss always significantly lower beyond 30% of the training procedure4, and the validation loss achieving the final value of AdamW-300k within < 70% training steps or time5. For BERT-base (Figure 1a), Amos achieves the same within only 145k steps (< 50%), and the Amos checkpoint at 150k outperforms the final checkpoint of AdamW-300k in fine-tuning on MNLI (Williams et al., 2018) as well (§ A.7).\n\nIn Figure 1a, we also tried starting from the final checkpoint of AdamW-200k and resetting the learning-rate as if it is linearly decaying to max training step 300k (AdamW-Cont.). The loss spikes higher and does not go further lower than the value at 200k, suggesting that the hyper-parameter of max training steps has to be set a priori, and continuous training is not trivial with AdamW. In addition, we tried a learning-rate schedule (AdamW-rsqrt) that takes the same value at step 10k but adopts a decay in proportion to t−1/2 (where t is the step) beyond. Although this setting does not require max training steps, it converges slower than both AdamW-200k and AdamW-300k.\n\nFor the RPE model (Figure 1c), we tried setting η of the relative position embeddings to a smaller value (Amos-*Scale, see § A.4 for more details), and found significant impact especially on the validation loss. Similar results are observed when we change η for a certain type of layers in the BERT-large model (Figure 2a, Amos-*Scale, see § A.4). It suggests that the model-specific\n\nFigure 3: Plots of the quadratic mean of entries of variables over pre-trained steps.\n\n4We have tried different learning-rates in preliminary experiments and the best was chosen. A learning-rate\n\nsearch for BERT-base is presented in § A.5.\n\n5In our JAX (Bradbury et al., 2018) implementation, the running time per training step for all optimizers\n\n(AdamW, Amos and AdaFactor) are almost the same.\n\n8\n\n(b) RoPE-large(c) T5-large(a) BERT-largeAdamWAmosAdamWAmosAmos-*ScaleAdamWAmosAdaFactor(a) BERT-baseScale of TokenEmbed(b) BERT-baseScale of Layer4/MLP/Dense1/Bias√1/d(c) RPEScale of Layer7/RelPosEmbedAdamW-200kAdamW-300kAmosAmos-*ScaleAdamW-200kAdamW-300kAmosUnder review as a conference paper at ICLR 2023\n\nFigure 4: Training a single layer LSTM on the PTB corpus.\n\ninformation ̃η indeed contributes to the performance of Amos, which according to previous work (Kaplan et al., 2020) is unlikely achieved by tuning the learning-rate schedule alone.\n\n5.2 SCALES OF TRAINED VARIABLES\n\nIn Figure 3 we show how the scale of entries of some variables evolve as the training proceeds. With AdamW, both the token embeddings and the bias converge to similar scales (Figure 3ab); while with Amos the token embeddings converge to ≈ (cid:112)1/d (where d is the hidden size) and the bias to ≈ 0.5, as specified by the hyper-parameter ̃η. It shows that the algorithm of Amos can lead variables to converge to drastically different scales, which is unlikely with AdamW. In Figure 3c, comparing Amos and Amos-*Scale, the relative position embeddings in a typical layer of the RPE model converge to different scales, which shows that the scale is indeed controlled by the hyper-parameter ̃η. Recall that Figure 1c shows this has impact on the performance.\n\nIn order to further illustrate the relation among the optimizer, validation performance and the scale of variables, we train a single layer LSTM on the Penn Tree Bank (PTB) corpus (Marcus et al., 1993). The model size is 256 for hidden states and 1024 for memory. We set dropout rate 0.55 for hidden states (which is important for training on PTB) and 0.1 for memory. Sequence length and batch size are set to 64. We compare Amos, AdamW, and Adam (without weight decay). For Amos, the global (calculated from input scale 1 learning-rate is set to 0.01 and η for the LSTM kernel is set to 1√ 4 , input dimension 512 and output scale 1). For AdamW and Adam, the learning-rate is set to 0.0015 (about the same as Amos for the LSTM kernel), and the weight decay is set to 0.01 for AdamW.\n\n32\n\nThe results are shown in Figure 4. Without weight decay, the scale of the LSTM kernel trained by Adam can keep increasing; so Adam is better than AdamW on training loss but worse on validation perplexity (i.e. the model trained by Adam generalizes worse). On the other hand, Amos achieves the same training loss as Adam, while keeping the scale of the kernel as specified. It results in a much better validation perplexity which matches the state-of-the-art6. Overall, we conclude that controlling the scale of trained variables can help the generalization performance of deep neural networks, and the model-specific information from ̃η enables Amos to do this.\n\n6 CONCLUSION\n\nWe have presented the Amos optimizer, which uses an adaptive L2 regularizer to control learning-rate decay and guides trained weights towards a specified model-oriented scale. It demonstrates faster convergence than the state-of-the-art in pre-training language models, where the training process is long and decaying schedule is crucial. On the other hand, its ability to control the scale of trained weights also brings better generalization to small models such as a single layer LSTM.\n\nBesides pre-training, we expect Amos to have advantages in fine-tuning as well, especially for multi-modal models that combine heterogeneous components of varied scales and/or pre-trained with different recipes. Hopefully, the model-specific information ̃η can help us fine-tune such models that were previously difficult with other optimizers (Liang et al., 2022; Kumar et al., 2022).\n\n6See Melis et al. (2020) for a setting that achieves the state-of-the-art performance for a single layer LSTM on PTB. It uses RMSProp and dynamically decays the learning-rate by watching the performance on the validation set. To our knowledge, no previous work has been able to achieve the state-of-the-art with a straightforward setting of the optimizer as we do with Amos.\n\n9\n\nAdamWAdamAmos(a) Train Loss(b) Validation Perplexity(c) Scale of KernelUnder review as a conference paper at ICLR 2023\n\nEthics Statement This work includes pre-training language models, which have the potential risk of inherited bias from the training data. Our empirical contribution is on accelerating the pre-training process and thus does not focus on addressing such risk. For fair comparison, the pre-training data we have used are the same as previous works, and consequently the models we trained to evaluate our approach are similar to those already open-sourced. We refer to Bommasani et al. (2021) for a discussion of the risks of pre-trained language models.\n\nReproducibility Statement Proof of lemmas in § 4 is given in § A.1. Following the derivation of the Amos update rule, a heuristic derivation of the asymptotic behavior of the Amos decay factors is found in § A.2, and its connection with SGD is discussed in § A.3. Assumption 1 in our derivation is verified by experiments in § A.6. We explain the calculation of ̃η for the Transformer models in § A.4. For the pre-training experiments in § 5.1, we describe detailed settings in § A.5, and present a learning-rate search for BERT-base as well. Fine-tuning experiments on MNLI are shown in § A.7. Furthermore, an ablation test for the memory reduction settings of Amos is found in § A.8. Additional experiment settings and results of training the ResNet50 model on ImageNet are found in § A.9. Our code is open-sourced at: https://anonymous-url.\n\nREFERENCES\n\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,\n\nabs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.\n\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.\n\nL ́eon Bottou. On-line learning and stochastic approximations. In David Saad (ed.), On-line Learning in Neural Networks, Publications of the Newton Institute, pp. 9–42. Cambridge University Press, 1998. URL https://leon.bottou.org/publications/pdf/online-1998.pdf.\n\nL ́eon Bottou. Stochastic gradient tricks. In Gr ́egoire Montavon, Genevieve B. Orr, and Klaus-Robert M ̈uller (eds.), Neural Networks, Tricks of the Trade, Reloaded, Lecture Notes in Computer Science (LNCS 7700), pp. 430–445. Springer, 2012. URL https://leon.bottou.org/papers/ bottou-tricks-2012.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and JAX: composable transformations of Python+NumPy programs, 2018. URL Qiao Zhang. http://github.com/google/jax.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248–255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https://doi.org/10. 1109/CVPR.2009.5206848.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171– 4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011. URL https://jmlr.org/papers/v12/duchi11a.html.\n\nFelix Gers, J ̈urgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with\n\nLSTM. Neural computation, 12:2451–71, 10 2000. doi: 10.1162/089976600300015015.\n\nSaeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM J. Optim., 23(4):2341–2368, 2013. doi: 10.1137/120880811. URL https://doi.org/10.1137/120880811.\n\nVineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimiza-\n\ntion. CoRR, abs/1802.09568, 2018. URL http://arxiv.org/abs/1802.09568.\n\nElad Hazan.\n\nIntroduction to online convex optimization. CoRR, abs/1909.05207, 2019. URL\n\nhttp://arxiv.org/abs/1909.05207.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR recognition. 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\n\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606. 08415.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 448–456. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/ioffe15.html.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015. URL https://arxiv.org/abs/1412.6980.\n\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=UYneFzXSJWh.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWeixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. CoRR, abs/2203.02053, 2022. doi: 10.48550/arXiv.2203.02053. URL https://doi.org/10. 48550/arXiv.2203.02053.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\nSiyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3325–3334. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/ma18a.html.\n\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. URL https://aclanthology.org/J93-2004.\n\nG ́abor Melis, Tom ́aˇs Koˇcisk ́y, and Phil Blunsom. Mogrifier lstm.\n\nIn International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= SJe5P6EYvS.\n\nNoboru Murata. A statistical study on on-line learning. In David Saad (ed.), On-Line Learning in Neural Networks, Publications of the Newton Institute, pp. 63–92. Cambridge University Press, 1998. URL https://www.researchgate.net/publication/2666659_A_ Statistical_Study_on_On-line_Learning.\n\nVinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes F ̈urnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807–814. Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/432.pdf.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020. URL http://jmlr.org/ papers/v21/20-074.html.\n\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=ryQu7f-RZ.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pp. 464–468. Association for Computational Linguistics, 2018. doi: 10.18653/v1/ n18-2074. URL https://doi.org/10.18653/v1/n18-2074.\n\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm ̈assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 4603–4611. PMLR, 2018. URL http://proceedings.mlr.press/v80/shazeer18a.html.\n\nCharles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal disIn Berkeley Symposium on Mathematical Statistics and Probability, 1956. URL\n\ntribution. http://projecteuclid.org/euclid.bsmsp/1200501656.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/ 2104.09864.\n\nRan Tian, Joshua Maynez, and Ankur P. Parikh. Shatter: An efficient transformer encoder with single-headed self-attention and relative sequence partitioning. CoRR, abs/2108.13032, 2021. URL https://arxiv.org/abs/2108.13032.\n\nT. Tieleman and G. Hinton. Lecture 6.5 - RMSProp, COURSERA: Neural networks for machine\n\nlearning. Technical report, 2012.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus In Proceedings of the 2018 Conference of for sentence understanding through inference. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //aclanthology.org/N18-1101.\n\nYang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training.\n\nCoRR, abs/1708.03888, 2017. URL http://arxiv.org/abs/1708.03888.\n\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Syx4wnEtvH.\n\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 19–27. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.11. URL https://doi.org/10.1109/ICCV.2015.11.\n\nA APPENDIX\n\nA.1 PROOF OF LEMMAS\n\nProof of Lemma 4.1. We have\n\nEt+1[(cid:96)(zt+1; ̃θt+1)] = Et+1[(cid:96)(zt+1; ̃θt − ̃αt (cid:12) ̃gt)]\n\n= Et+1[(cid:96)(zt+1; ̃θt) − ( ̃αt (cid:12) ̃gt) · ∇(cid:96)(zt+1; ̃θt)] + o( ̃αt),\n\nwhere (cid:12) denotes element-wise multiplication, o( ̃αt)/(cid:107) ̃αt(cid:107) → 0 at (cid:107) ̃αt(cid:107) → 0, and arrays are flattened to vectors for the dot-product. Since zt+1 and zt are drawn from the same distribution, we have Et+1[(cid:96)(zt+1; ̃θt)] = Et[(cid:96)(zt; ̃θt)] and Et+1[( ̃αt(cid:12) ̃gt)·∇(cid:96)(zt+1; ̃θt)] = ( ̃αt(cid:12) ̃gt)·Et[∇(cid:96)(zt; ̃θt)]. Moreover, because ̃αt does not depend on zt, we have Et[ ̃αt (cid:12) ̃gt] = ̃αt (cid:12) Et[ ̃gt]. Thus,\n\nEt[Et+1[(cid:96)(zt+1; ̃θt+1)]] = Et[(cid:96)(zt; ̃θt)] − ( ̃αt (cid:12) Et[ ̃gt]) · Et[∇(cid:96)(zt; ̃θt)] + o( ̃αt).\n\nNow Et[ ̃gt] = Et[∇(cid:96)(zt; ̃θt)] by definition, so ( ̃αt (cid:12) Et[ ̃gt]) · Et[∇(cid:96)(zt; ̃θt)] > 0, and the lemma follows by taking ̃αt small enough so that o( ̃αt) can be omitted.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Lemma 4.2. In the LHS of Equation 5, only ρt can depend on zt; while in the RHS, M2(gt)2 depends on zt but αt does not. In order to satisfy Equation 5 on every zt, it is necessary that ρt has a M2(gt)2 factor: ρt ∝ M2(gt)2. Moreover, we require that E[ρt] does not depend on gt, so ρt should be normalized by E[M2(gt)2]: ρt ∝ M2(gt)2 E[M2(gt)2] . This, substituted back into Equation 5, implies that αt ∝\n\n1√\n\n.\n\nE[M2(gt)2]\n\nProof of Lemma 4.3. Equation 2 implies ε1 = ε0 − α0g0. Taking E[M2(•)] of this equation, we have\n\nE[M2(ε1)2] = M2(ε0)2 −\n\n= M2(ε0)2 −\n\n2 k\n2 k\n\nαtE[g0] · ε0 + α2\n\n0\n\nE[M2(g0)2]\n\nα\n\nE[g0] · ε0 (cid:112)E[M2(g0)2]\n\n+ α2.\n\nSince g0 and ε0 point to the same direction, we have 1 Assumption 1 we have M2(E[g0])/(cid:112)E[M2(g0)2] ≥ ξ. Hence,\n\nk\n\nE[g0] · ε0 = M2(E[g0]) M2(ε0). By\n\nE[M2(ε1)2] ≤ M2(ε0)2 − 2αξ M2(ε0) + α2.\n\nThe RHS above is a quadratic function of α, which achieves minimum at α = ξ M2(ε0). Finally, since (usually) θ0 is initialized close to 0, and M2(θ∗) ≈ η, we have M2(ε0) ≈ η.\n\nA.2 HEURISTIC DERIVATION OF DECAY FACTORS\n\nSubstituting Equation 6 into Equation 5, we get the following equivalent of Equation 5:\n\nct M2(εt) = dtη.\n\n(7)\n\nWithout knowing any specific relation among gt, θt and εt, we found it difficult to theoretically decide an optimal ct. Given that ct decreases to 0, we set ct to decrease according to M2(εt) in Amos, i.e. ct ∼ r M2(εt), where r is a constant and ∼ denotes asymptotically equal at t → ∞. Thus, by Equation 7 we have dt ∼ r η M2(εt)2. We will analyze the evolution of M2(εt)2 to derive ct and dt.\n\nTaking E[•] of Equation 4, and applying Equation 6 and Equation 7, we get\n\nE[M2(εt+1)2] ≈ M2(εt)2 −\n\n(cid:16)\n\n2 k\n\nctξ M2(εt)\n\nE[gt] (cid:112)E[M2(gt)2]\n\n+\n\ndt 2\n\n(cid:17)\n\nE[γt]θt\n\n·εt + c2\n\nt ξ2 M2(εt)2. (8)\n\nAs in the derivation of the initial learning-rate, we make an optimistic estimation that E[gt] and εt have the same direction. Then, applying Assumption 1 we have\n\n1 k\n\nE[gt] (cid:112)E[M2(gt)2]\n\n· εt =\n\nM2(E[gt]) (cid:112)E[M2(gt)2]\n\nM2(εt) ≥ ξ M2(εt),\n\nand Equation 8 implies\n\nE[M2(εt+1)2] ≤ M2(εt)2 − 2ctξ2 M2(εt)2 −\n\nE[γt]θt · εt + c2\n\nt ξ2 M2(εt)2\n\n≤ M2(εt)2 − ctξ2 M2(εt)2 −\n\nE[γt]θt · εt\n\n(9)\n\ndt k\ndt k\n\nwhere in the last equation we have used the fact that ct ≤ 1. Now, in order to estimate θt · εt, we assume that θt will be evenly distributed on the hypersphere of radius M2(εt) around θ∗ as the training proceeds. Then, if k ≥ 3, for most θt from the distribution we will have θ∗ · εt ≈ 0. In this case, we have 1 k (εt + θ∗) · εt ≈ M2(εt)2, and “on average” it is safe to assume7 that k θt · εt ≥ q M2(εt)2 for some constant q > 0. Then, Equation 9 becomes\n\nk θt · εt = 1\n\n1\n\nE[M2(εt+1)2] ≤ M2(εt)2 − E[γt](1 + dtq) M2(εt)2\n\n(10)\n\n7It not useful in this work to provide a rigorous definition of “on average”. We only point out its deep connection with Stein’s example (Stein, 1956) that if k ≥ 3, an estimator with L2 regularization can be better than the maximum likelihood estimator without L2.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nwhere we have used the fact that E[γt] = ctξ2. In light of Equation 10, we consider the following asymptotic difference equation:\n\net+1 ∼ et − γt(1 + dtq)et\n\n(11)\n\nwhere et is intended to follow the asymptotic behavior of M2(εt)2. Since we have dt ∼ r it is natural to assume dt ∼ r\n\nη et. Then, we transform Equation 11 as the following:\n\nη M2(εt)2,\n\n1 et+1\n\n∼\n\n1 et\n\n·\n\n1 1 − γt(1 + dtq)\n\n∼\n\n1 et\n\n(cid:0)1 + γt(1 + dtq)(cid:1) ∼\n\n1 et\n\n+ γt(\n\n1 et\n\n+\n\nqr η\n\n),\n\nin which we have used the approximation 1/(1 − x) ∼ 1 + x applied to x = γt(1 + dtq). Thus, the\n\nupdate rule of bt in Algorithm 1 can be revealed by setting bt =\n\nbt+1 = bt + γt(bt + 1).\n\nη qr\n\n1 et\n\n:\n\nr η\n\nAnd dt ∼\n\nd0 = 1.\n\net implies dt ∼\n\n1 qbt\n\n, so we set dt =\n\n1 1 + qbt\n\nto satisfy both the asymptotic behavior and\n\nSimilarly, since ct ∼ r M2(εt) we have ct ∼\n\nsatisfy the asymptotic behavior and c0 = 1.\n\nA.3 CONNECTION TO SGD\n\n√\n\n1 pbt\n\nwhere p =\n\nq rη\n\n. So we set ct =\n\n√\n\n1 1 + pbt\n\nto\n\nThe derivation of decay factors in Amos (§ A.2) is largely inspired by SGD (Murata, 1998). In this section, we recall the theory of learning-rate schedule of SGD and discuss its relation with Amos.\n\nThe update rule of SGD is simply δt ← αtgt, where αt is a scalar learning-rate. It is recommended to set the learning-rate schedule to αt = α 1+αλt , where α is the initial learning-rate and λ is the smallest eigen-value of the Hessian (Bottou, 2012). This is based on the following discussion. Lemma A.1. Assume ̃θt is in a neighborhood of a local minimum ̃θ∗, such that the gradient E[ ̃gt] is approximated by H ̃εt via Taylor expansion. Here, H = E[∇2(cid:96)(zt; ̃θ∗)] is the Hessian at ̃θ∗. Let 0 < λ be the smallest eigen-value of H. Then,\n\nE[M2( ̃εt+1)2] ≤ M2( ̃εt)2 − 2λαt M2( ̃εt)2 + α2\n\nt\n\nE[M2( ̃gt)2]\n\nand the minimum of RHS of Equation 12 is achieved by\n\nαt =\n\nλ M2( ̃εt)2 E[M2( ̃gt)2]\n\nand E[M2( ̃εt+1)2] ≤ M2( ̃εt)2 −\n\nλ2 M2( ̃εt)4 E[M2( ̃gt)2]\n\n.\n\n(12)\n\n(13)\n\nProof. Since ̃θ∗ is a local minimum, we have E[∇(cid:96)(zt; ̃θ∗)] = 0 and E[ ̃gt] ≈ H ̃εt, where H is positive definite. Given λ the smallest eigen-value of H, we have E[ ̃gt] · ̃εt ≥ λ(cid:107) ̃εt(cid:107)2. Applying this to E[M2(•)] of Equation 2, we get Equation 12. Now the RHS is a quadratic function of αt, and it takes minimum at Equation 13. So the lemma follows.\n\nNote that both Amos and SGD analyze the evolution of M2(εt)2 by estimating αtgt · εt. For SGD this is achieved by approximating E[ ̃gt] with the Hessian. For Amos, on the other hand, we have to make Assumption 1 due to the gradient normalization factor 1/(cid:112)E[M2(gt)2]. In both cases, the learning-rate decay is derived by setting αt in terms of M2(εt) so that M2(εt)2 decreases fast, then solve the asymptotic behavior of M2(εt).\n\nHeuristic derivation of αt: We assume limt→∞ E[M2( ̃gt)2] = ν > 0. In light of Equation 13, we consider the following asymptotic difference equation:\n\net+1 ∼ et −\n\nλ2 ν\n\ne2\n\nt\n\n15\n\n(14)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere et is intended to follow the asymptotic behavior of M2( ̃εt)2. We transform Equation 14 as:\n\n1 et+1\n\n∼\n\n1 et\n\n·\n\n1 − λ2\n\n(1 +\n\nλ2 ν\n\net) =\n\n1 et\n\n+\n\nλ2 ν\n\n∼\n\n1 et\n\n1 ν et λ M2( ̃εt)2 E[M2( ̃gt)2]\n\nso we have\n\n1 et\n\n∼\n\nλ2 ν\n\nt. Now, since αt =\n\nwe have αt ∼\n\nλ ν\n\net ∼\n\n1 λt\n\n. So αt =\n\nα 1 + αλt\n\nsatisfies both the asymptotic behavior and α0 = α. In the above derivation, the assumption limt→∞ E[M2( ̃gt)2] = ν > 0 states that E[M2( ̃gt)2] will converge to some non-zero value and will not further decrease. This is often described intuitively as “the stochastic noise of sampled gradients does not vanish”, a characteristic feature in the theory of SGD. It is in drastic contrast with Assumption 1: We assume that E[M2(gt)2] decreases along with M2(E[gt]) in Amos. Ma et al. (2018) pointed out that the vanishing of E[M2(gt)2] might lead to faster convergence; but to our knowledge, Amos is the first work to use the vanishing of E[M2(gt)2] to actually develop an optimizer that empirically converges faster.\n\nFor SGD, the hyper-parameter λ is generally unknown; but if we adopt an L2 regularizer of strength λ(cid:48), it is guaranteed that λ ≥ λ(cid:48), so one can safely set the learning-rate to 1+αλ(cid:48)t (Bottou, 2012). In Amos, the strength of L2 regularization ̃γt takes a similar role in controlling the speed of learning-rate decay. We expect this work to inspire more theoretical investigation into this principle.\n\nα\n\nA.4 THE CALCULATION OF ̃η\n\n√\n\nAs explained in § 2, for a linear transformation y = xW + u (W , u ⊆ ̃θ, W ∈ Rm×n, x ∈ Rm), we set η(W ) = σy/(σx m) and η(u) = σy/2, where σx is the standard deviation of entries of x and σy the standard deviation of entries of y. The values of σx and σy are constrained by connected layers, and non-linear layers usually expect entries of input/output tensors from some approximate range. In Table 1, we show 3 types of non-linear layers that occur in Transformer, and specify their input/output range (i.e. expected standard deviation) used for calculating ̃η.\n\nFor activations, e.g. GELU (Hendrycks & Gimpel, 2016) in the Multi-Layer Perceptron (MLP) block, the input range is set to 1 because the non-linearity of the activation function mostly lies within that range; and the output range is set to (cid:112)1/2 because the activation function, as similar to ReLU (Nair & Hinton, 2010), will map negative values (which account for 1/2 of the input dimension) to close to 0 and approximately retain positive values.\n\nFor Softmax of n classes, the input range is set to 1 because the derivative of exp(x) is close to 1 within the |x| ≤ 1 range (so Softmax is most sensitive to values within this range); and the output range is set to (cid:112)1/n because the output is an n-dimension vector of L2 norm ≤ 1 (so the quadratic mean of entries ≤ (cid:112)1/n).\n\nFor LayerNormalization (Ba et al., 2016), the input range is arbitrary because the input will be normalized. The output range is expected to be 1.\n\nWe will discuss the calculation of ̃η for specific models in the next sub-sections.\n\nA.4.1 BERT, ROPE AND RPE\n\nFor BERT, RoPE and RPE, the multi-headed attention layer receives the hidden state x, and the linear transformations xQ (i.e. the query) and xK (i.e. the key) are expected to have standard deviation 1 so that the dot-product (cid:112)1/h(xQ) · (xK) (i.e. attention score) has standard deviation 1 as well\n\nType of Non-linear Layer\n\nInput Range Ourtput Range\n\nActivation in MLP Softmax of n classes LayerNormalization\n\n1 1\nN/A\n\n(cid:112)1/2 (cid:112)1/n 1\n\nTable 1: The input/output range of non-linear layers we specify in this work for calculating ̃η.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nType of Variable\n\nη\n\nRemark\n\nBias in all Linears LayerNormalization Scale Input Embeddings MLP/Dense2/Kernel Other Linear Kernels\n\n0.5 1\n(cid:112)1/d (cid:112)2/m m is the size of intermediate activation in the MLP (cid:112)1/d\n\nd is the size of hidden states\n\nd is the size of hidden states\n\nRelative Position Embeddings\n\n0.5\n\nTable 2: The η calculated for variables in BERT, RoPE and RPE. MLP/Dense2/Kernel is the linear kernel for the output layer of the MLP block. Other linear kernels include e.g. query, key and value kernels in the multi-headed attention layer.\n\n(and this is why there is the scaling factor (cid:112)1/h, where h is the size per-head), which is expected by the Softmax for calculating the attention probability. Therefore, the output ranges of Q and K are 1. For RoPE, the dot-product is replaced by a bi-linear form which encodes relative positions, but this does not change the scale because the bi-linear form is orthogonal.\n\nFor other linear transformations in the model, the outputs are either fed into the activation function of an MLP (which requires input range 1), or serve as a summand in a Residual Connection where the residual part comes from a LayerNormalization (which has range 1). So all the linear transformations have output range 1 in these model architectures.\n\nThus, we set the η of bias in all linear transformations to 0.5, and the η for kernels is categorized by the input range and dimension, as we show in Table 2.\n\nThe input embeddings (i.e. token embeddings, position embeddings and segment-type embeddings) are inputs to LayerNormalization so their scales are not constrained there; but the token embeddings are also used as the linear kernel for producing the logits of token generation, which expects input range 1 (because it comes from LayerNormalization) and input dimension d (where d is the hidden size), so η is set to (cid:112)1/d. For the linear kernel of the MLP output layer (MLP/Dense2/Kernel), the input range is (cid:112)1/2 because it comes from a non-linear activation, and input dimension m is the size of intermediate activation in the MLP, so η is (cid:112)2/m.\n\nFor all other linear kernels, the input range is 1 because it comes from LayerNormalization, and input dimension is the hidden size d. So η is (cid:112)1/d.\n\nThe relative position embeddings in the RPE model is used as input to the key and value transformations at each layer, similar to the hidden state. We set η to 0.5 so its scale is close to the hidden state (which has scale 1) but will not dominate it.\n\nIn § 5.1, we have experimented with pre-training RPE and BERTExperiments with Amos-*Scale large with different ̃η (Amos-*Scale). For RPE (Figure 1c), we tried setting η of the relative\n\nType of Variables\n\nη\n\nRemark\n\nLayerNormalization Scale Query Kernel Input Embeddings MLP/wo/Kernel Other Linear Kernels Relative Attention Bias\n\n1 (cid:112)1/(hd) 1\n\nh is the size per-head and d is the size of hidden states\n\n(cid:112)2/m m is the size of intermediate activation in the MLP (cid:112)1/d 0.5\n\nd is the size of hidden states\n\nTable 3: The η calculated for variables in T5. MLP/wo/Kernel is the linear kernel for the output layer of the MLP block.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nposition embeddings to (cid:112)1/d instead of 0.5. For BERT-large (Figure 2a), we tried setting η of MLP/Dense2/Kernel to (cid:112)1/d instead of (cid:112)2/m. They both had impact on performance. Especially √\nfor BERT-large, (cid:112)1/d and (cid:112)2/m only differ by a 2 factor (because m = 4d), still the performance gap is significant. It illustrates the importance of setting ̃η appropriately.\n\nA.4.2 T5\n\nFor the T5 model, η is set as in Table 3. It is different from Table 2, due to several differences between the T5 architecture and BERT, as discussed below.\n\n1. Linear transformations do not have bias terms in T5.\n\n2. Attention score is calculated by (xQ) · (xK) in T5, without the scaling factor. Instead, the query kernel Q is initialized to a smaller scale (cid:112)1/(hd), with an extra (cid:112)1/h factor compared to K. Thus, we accordingly set η of the query kernel to (cid:112)1/(hd).\n\n3. The token embeddings are no longer re-used for producing logits of token generation. So\n\nwe set η to 1, which is the same as the scale for initialization.\n\n4. The MLP activation function (i.e. gated-GELU) used in T5 is different from BERT. Still, η\n\nfor the linear kernel of the MLP output (MLP/wo/Kernel) is set to the same.\n\n5. We set η of the relative attention bias to 0.5 so its scale is close to the attention score (which\n\nhas scale 1) but will not dominate it.\n\nA.5 DETAILED EXPERIMENT SETTINGS AND LEARNING-RATE SEARCH\n\nIn this section, we discuss detailed settings of the pre-training experiments in § 5.1. The hyperparameters and required computation resources are shown in Table 4. For pre-training BERT with AdamW, we follow the settings of Liu et al. (2019). For RPE, pre-training on TPU is slow, so we use a different configuration with more TPU cores to train the base-sized model. For T5, we found that using β = 0.98 for Amos and AdamW causes training instability, so we decrease the value to β = 0.95. The settings of AdaFactor follow Raffel et al. (2020) and Shazeer & Stern (2018).\n\nFor encoder-only models (i.e. BERT, RoPE and RPE) trained on the Wikipedia+Books corpus, we use the Penn TreeBank corpus (Marcus et al., 1993) as the validation set. The training precision is float32. Number of warm-up steps is set to 10k for AdamW and 20k for Amos.\n\nFor T5, the training loss is cross-entropy with an extra regularization term, (log Z)2 (where Z is the normalization factor in Softmax), which makes the logits close to mean 0 and self-normalized. In\n\nBatch Size Optimizer\n\nβ\n\nLearning-rate\n\n#Steps\n\nResource\n\nBERT-base\n\n1024\n\nRoPE-base\n\n1024\n\nRPE\n\n1024\n\nBERT-large\n\n4096\n\nRoPE-large\n\n1024\n\nT5-large\n\n4096\n\nAdamW Amos\n\nAdamW Amos\n\nAdamW Amos\n\nAdamW Amos\n\nAdamW Amos\n\nAdamW Amos AdaFactor\n\n0.98 0.98\n\n0.98 0.98\n\n0.98 0.98\n\n0.98 0.98\n\n0.99 0.99\n\n0.95 0.95 0.8\n\n2e-4 0.01\n\n2e-4 0.01\n\n2e-4 0.01\n\n2e-4 0.01\n\n1e-4 5e-3\n\n1e-3 0.01 0.01\n\n200k/300k 300k\n\n200k/300k 300k\n\nTPUv4 2x2x4 About 2 days\n\n200k/300k TPUv3 8x8 300k\n\nAbout 4 days\n\n250k 250k\n\n1M 1M\n\n250k 250k 250k\n\nTPUv4 4x4x4 About 4 days\n\nTable 4: Hyper-parameter settings and required computational resources. The hyper-parameter β in Amos is corresponding to β2 in AdamW and the (second moment) decay rate in AdaFactor.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Validation loss for pre-training BERT-base. We compare different learning-rates for (a) AdamW with batch size 1024, (b) Amos with batch size 1024 and (c) Amos with batch size 256.\n\nFigure 6: Plot of the ratio M2(E[gt])\n\n√\n\nE[M2(gt)2]\n\nfor variables in the BERT-base model, over pre-training steps.\n\nFigure 2c, we plot cross-entropy for validation loss instead of the loss used for training. The training precision of T5 is bfloat16. Possibly because linear transformations in T5 do not have bias terms, we found the model easier to train than BERT, and Amos can be applied without warm-up of learning-rate. The number of warm-up steps is set to 10k for both AdamW and AdaFactor. Learning-rate decay is in proportion to t−1/2 (where t is the step) for AdaFactor and linear for AdamW.\n\nFor pre-training BERT-base, we present a learning-rate search in Figure 5. For AdamW (Figure 5a), a smaller learning-rate significantly slows down the convergence, while a larger one results in a bumpy validation loss but almost the same performance. On the other hand, both smaller or larger learning-rate can degrade performance for Amos (Figure 5bc). Comparing Figure 5b and Figure 5c, we also verify a theoretical prediction about the global learning-rate of Amos in § 4, i.e. the best learning-rate for Amos is in proportion to the square-root of the batch size: Training with 4× the batch size matches 2× the learning-rate.\n\nA.6 VERIFICATION OF ASSUMPTION 1\n\n√\n\nE[M2(gt)2]\n\nIn Assumption 1, we have assumed that M2(E[gt]) ≥ ξ > 0 for all t and across all variables. E[gt] and E[M2(gt)2] can be estimated by taking the running average of gt and M2(gt)2, respectively; so in Figure 6 we track the pre-training of the BERT-base model, calculate the running averages with exponential decay rate 0.98, and show some typical plots of the ratio. We note two characteristics of the plots: (1) the ratios are increasing as the training proceeds, which suggests that taking a global constant ξ to satisfy Assumption 1 is indeed possible; (2) starting points on the left of these plots are similar across different learning rates, which suggests that it is detectable in the early stage of training whether a learning-rate is too small or too large. In fact, in all plots for all variables we can see that the ratio M2(E[gt]) √\n\n≥ 0.01; the appropriate global learning-rate can be read from these plots.\n\nE[M2(gt)2]\n\nA.7 FINE-TUNING RESULTS\n\nIn Table 5, we show fine-tuning results on the MNLI (Williams et al., 2018) dataset. We compare checkpoints pre-trained for 150k and 300k steps with Amos, and the final checkpoints of AdamW200k and AdamW-300k. We fine-tune all checkpoints using the Adam optimizer with learning-rate 5e-6, batch size 16, and evaluate by the best accuracy on the MNLI dev set among every 1k of 200k training steps. We run each experiment 3 times and report the mean and standard deviation. The checkpoint pre-trained for 150k by Amos already outperforms the final checkpoint of AdamW-300k.\n\n19\n\n(a) AdamW, Batch 1024(b) Amos, Batch 1024AdamW-1e-4AdamW-4e-4AdamW-2e-4Amos-0.01(c) Amos, Batch 256AdamW-1e-4Amos-0.002Amos-0.01Amos-0.005Amos-0.005Amos-0.02Amos-0.01(c) Layer10/AttentionValue/BiasAmos-0.01Amos-0.02(b) Layer10/AttentionValue/Kernel(a) Layer3/AttentionKey/BiasUnder review as a conference paper at ICLR 2023\n\nMNLI-matched MNLI-mismatched\n\nAmos@150k Amos@300k\n\nAdamW-200k AdamW-300k\n\n84.15 ±.40 84.72 ±.15\n\n83.19 ±.37 83.84 ±.14\n\n84.17 ±.37 84.44 ±.26\n\n83.45 ±.41 83.88 ±.17\n\nTable 5: Fine-tuned accuracy on MNLI dev set. We show the mean and standard deviation of 3 runs.\n\nThus, the faster convergence by Amos in pre-training indeed transfers to better performance in fine-tuning; we can save 50% of the pre-training cost by using Amos instead of AdamW.\n\nA.8 ABLATION OF MEMORY REDUCTION\n\nIn this section, we experiment with different settings of the memory reduction. We compare the current setting of reducing the input dimension for linear transformations (Reduce 1Axis), to no memory reduction at all (No Reduce), and the setting of reducing both axes for linear transformations (Reduce Dense). For embedding matrices, no axis is reduced in the No Reduce setting, and the embed dimension is reduced for both Reduce 1Axis and Reduce Dense. We have tried reducing both axes for embedding matrices as well, but found the training unstable in this setting. The comparison of memory usage for slot variables is shown below.\n\nAdaFactor (No Momentum) (cid:28) Reduce Dense < Reduce 1Axis (cid:28) AdamW (cid:28) No Reduce.\n\nWithout memory reduction, Amos (No Reduce) consumes more memory than AdamW because it has more slot variables ( ̃vt, ̃bt, ̃mt vs. ̃vt, ̃mt). When memory reduction is applied, the memory usage of ̃vt, ̃bt becomes negligible compared to the momentum ̃mt, so Amos (Reduce 1Axis and Reduce Dense) requires < 51% memory for slot variables than AdamW. The memory reduction method used by Amos is more efficient than the matrix factorization used by AdaFactor, but in the pre-training of T5 (Figure 2c), AdaFactor achieved favorable performance (although slightly worse in the end than AdamW with linear learning-rate decay) without using momentum, reducing the memory usage further. Whether Amos can achieve a similar performance without using momentum is unclear yet.\n\nFigure 7: Pre-training BERT-base using Amos with different memory reduction settings.\n\nIn Figure 7, we show the training and validation loss of pre-training BERT-base by Amos with different memory reduction settings. Reduce Dense is slightly worse in training loss compared to No Reduce, but not so much in validation loss. On the other hand, Reduce 1Axis is almost the same as No Reduce in training loss, and generalizes even slightly better in validation loss than the other two. So the current Reduce 1Axis setting for Amos is favorable.\n\nA.9 TRAINING RESNET50 ON IMAGENET\n\nIn this section, we apply Amos to the training of ResNet50 (He et al., 2016) on the ImageNet dataset (Deng et al., 2009). ResNet50 is a deep Convolutional Neural Network of 50 layers, with Batch Normalization (Ioffe & Szegedy, 2015) and Residual Connection. ImageNet is a 1000-class image\n\n20\n\nNo ReduceReduce DenseReduce 1AxisUnder review as a conference paper at ICLR 2023\n\nType of Non-linear Layer\n\nInput Range Ourtput Range\n\nReLU Activation BatchNormalization Max-pooling on patch size n\n\n1 N/A 1\n\n(cid:112)1/2 1\n√ 2 ln n\n\n1/\n\nTable 6: The input/output range of non-linear layers we use to calculate ̃η for ResNet.\n\nclassification task with 1.28M traning examples. We train with batch size 1024, on an 8-core TPU machine. The settings for Amos is out-of-the-box: the hyper-parameter β is set to 0.95, warmup steps 5k, and the global learning rate ξ is set to 1√ = 0.028, where N = 1281167/1024 is the number of batches in the traning data. We use the open-sourced init2wint8 codebase to run the experiments.\n\nN\n\nA.9.1 THE CALCULATION OF ̃η FOR RESNET\n\nIn order to calculate the hyper-parameter ̃η for ResNet, we specify the input/output range of 3 types of non-linear layers in Table 6. This is similar to Transformers, with the only specialty that the output 2 ln n, where n is the patch size. This is because the range of a Max-pooling layer is set to 1/ maximum of n normally distributed random variables9 has a standard deviation of about 1/ 2 ln n.\n\n√\n\n√\n\nThe calculated η for different types of variables in ResNet is shown in Table 7.\n\nBatchNormalization is treated the same as LayerNormalization in Transformer.\n\nThe projection kernel of the first residual block is scaled up by max-pooling layer of patch size n.\n\nThe 2nd and 3rd convolution kernels in each residual block is scaled up by come from a ReLU activation.\n\n√\n\n2 ln n because of its previous\n\n√\n\n2 because their inputs\n\nThe variables for bias and other linear kernels are treated the same as in Transformer.\n\nSettings of Amos-*Scale We also tried an Amos-*Scale setting where the η for the projection kernel of the first residual block is set to (cid:112)1/d instead of (cid:112)(2 ln n)/d (in ResNet50, n = 3 × 3 = 9).\n\nA.9.2 RESULTS\n\nIn Figure 8a, we show the validation error rate of Amos and Amos-*Scale, where the error rate for Amos (0.261 lowest) is slightly better than the Amos-*Scale setting (0.263 lowest). Furthermore, it is known that a strong L2 regularization is beneficial for many popular image classification tasks (Loshchilov & Hutter, 2019), but Amos does not have a hyper-parameter to adjust the strength of L2\n\nType of Variable\n\nη\n\nRemark\n\nBias BatchNormalization scale Projection kernel of the first\n\nresidual block\n\nThe 2nd and 3rd convolution\n\nkernels in each residual block\n\nOther linear kernels\n\n0.5 1\n\n(cid:112)(2 ln n)/d n is the patch size of the previous max-pooling;\n\n(cid:112)2/d\n\n(cid:112)1/d\n\nd is the input size d is the input size\n\nd is the input size\n\nTable 7: The η calculated for variables in ResNet. Other linear kernels include convolution kernels and the final linear classification kernel.\n\n8https://github.com/google/init2winit 9https://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_\n\ntheorem#Gumbel_distribution\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Training ResNet50 on ImageNet. We plot error rate of the validation set.\n\nregularization; so we tried an ad hoc setting Amos-Extra, where the Amos update rule (Equation 1) is (cid:17)\n\nreplaced by δt ← dt other constants, but 0.001 was the best). As shown in Figure 8a, Amos-Extra (0.242 lowest error rate) significantly improves the performance on ImageNet.\n\nwith everything else kept the same (we also tried\n\n2 γt + 0.001)θt\n\ngt + ( 1\n\n(cid:16) ξη√\n\nˆvt\n\nIn Figure 8b, we compare the out-of-the-box Amos with Adam (no weight decay). The learning-rate schedule of Adam is set to cosine decay with 5% warmup, and the number of training steps is set to 140k. The base learning-rate is tuned by a random search of log scale between 1e-5 and 1e-2, with 25 runs. Other hyper-parameters are set to the default (i.e. β1 = 0.9 and β2 = 0.999). Amos outperforms all the 25 runs; the best 6 of the 25 are shown in Figure 8b. As alternative settings for Amos, we have also tried β = 0.98, 0.999, or ξ = 0.02, or even changed the decay factors to (cid:1)−1 ct = (cid:0)1 + 1 same validation error rate, sometimes with slightly slower convergence.\n\n. All the other settings converge to almost the\n\n2 and dt = (cid:0)1 + 1\n\nξηbt\n\n(cid:1)− 1\n\nξbt\n\n√\n\n√\n\n16\n\n16\n\nIn Figure 8c, we compare Amos-Extra with the state-of-the-art settings of AdamW. The learning-rate schedule of AdamW is set to cosine decay with 5% warmup, and the number of training steps is set to 187k. The base learning-rate, weight decay strength, and label smoothing rate (defaults to 0.1 for other experiments) are tuned by random search, of log scale between 1e-4 and 1e-2, log scale between 1e-2 and 1.0, and linear scale between 0.0 and 0.2, respectively, with 25 runs. Other hyper-parameters are set to the default (i.e. β1 = 0.9 and β2 = 0.999). Among the 25 runs, 9 of them outperform Amos-Extra, which are shown in Figure 8c. The best performing settings of AdamW gain their advantage close to the end of training, which is probably due to the interaction between weight decay and cosine learning-rate schedule. On the other hand, Amos-Extra demonstrates faster and more stable convergence.\n\nTo conclude, when applied to ResNet50 on ImageNet, Amos can outperform Adam out-of-the-box, and become comparable to the state-of-the-art AdamW settings by adding a small constant weight decay term. However, the extra weight decay term is ad hoc, cannot be covered by our current theory (because we have assumed that the L2 regularization is weak enough and decays to 0, not to bias the loss function but only constrain the scale of trained variables), and probably is not the optimal way to strengthen L2 regularization. It leaves the problem of searching for a more general working theory that enables stronger L2 to future work.\n\n22\n\n(a) Validation Error Rate(b) Amos vs. Adam(c) Amos-Extra vs. AdamWAmos-*ScaleAmosAmos-ExtraAmosAmos-Extra",
    "reference": "# Summary Of The Paper\n\nThe paper introduces Amos, a first-order DL optimizer with adaptive learning rate and decay. The proposed contributions are:\n\n* Outperforming AdamW for pre-training language models\n* Providing guidance for hyperparameter tuning\n* Reducing memory usage\n* Allowing continuous training and resuming from checkpoints\n\nThe proposed optimizer leverages model-specific information, by partitioning the model parameters and adding a per-partition norm constraint. This norm constraint, together with the gradients, is used to adapt the learning rate and weight decay.\nThe parameter update involves a series of newly introduced hyperparameters with different functions, and the paper provides a series of heuristic derivations and experiments to set their values.\n\nIn experiments pre-training current architectures for NLP tasks, Amos is shown to outperform AdamW with 2019 settings in terms of speed, performance and memory usage.\n\n# Strength And Weaknesses\n\nThe idea of using model information to replace the hyperparameters is great, we have good examples of similar strategies from the weight initialization literature (Xavier, He) and extending this to adaptive optimizers sounds exciting. Furthermore, this combines also very nicely with the interplay between learning rate and decoupled weight decay, which has been recently proven useful to solve issues with first-order adaptive DL optimizers like Adam. Finally, it also brings better performance using less parameters in a block-wise regularized fashion.\nCombining these ideas is a very promising and well-grounded line of work.\n\nStill, the question of why specifically this model, is not fully clear to me, for the following reasons:\n* Compared to its counterparts, the model introduces increased complexity, but the motivation (section 1) lists reducing complexity as a goal. This is expected to be achieved through the \"guidance for hyperparameter tuning\", but I'm not convinced of such guidance (see next points).\n* There is a clear effort to justify the choice for heuristics and hyperparametrizations, but the efforts lack clarity both in substance and presentation (see notes on clarity) and the claim that they are \"theoretically supported\" is in many cases not true: many crucial aspects are resolved through trial-and-error (see e.g. end of page 6 and Appendix 7). In many cases, I feel like an ablation study showing the contribution of the added components in an empirical way would be clearer and more compelling.\n* Even the more analytical hyperparameters don't give the impression of being \"easier\" to find. E.g. for \"eta\", Appendix A.3 shows that many factors must be taken into account in a non-automated way, and new architectures may require careful tuning. What if we e.g. have batchnorm instead of layernorm? How is the concept of \"range\" characterized? Will it hold under non-stationary, noisy and/or sparse gradients?\n* Regarding stability: While very promising, the experiments aren't comprehensive enough to convince that the proposed settings (theoretical or empirical) are not highly specific to the results reported, due to the amount of hyperparameters and tuning involved. One thing that could help with this is to analyze the loss landscape stability (see AdamW paper), and/or extend experiments to other tasks and architectures.\n* Regarding experiments, it is unclear how many settings were tried before finding the reported Amos hyperparametrizations. This makes difficult to compare the different optimizers in terms of their hyperparametrization budgets. Furthermore, error bars are not provided: they would be welcome (together with a more publication-friendly plotting mechanism), although the differences between Amos and the rest are very significant.\n\nA couple of thoughts regarding weight decay:\n* Please correct footnote in page 2: Loschilov&Hutter precisely mentions that L2 and weight decay are not equivalent for adaptive optimizers (beyond empirical success), the footnote seems to operate on the opposite idea.\n* We saw that decoupled weight decay helps overcoming convergence issues with Adam. Would re-introducing adaptive weight decay also re-introduce some of those issues?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* Regarding novelty (see my notes from before): I think this is a really nice and promising idea, and it is well embedded in recent and relevant literature.\n\n* Regarding quality (see my notes from before): I think the execution of the idea is heuristic-heavy, and contains a substantial degree of arbitrariness and complexity that is not justified in a compelling way. Experiments are very promising, but in my opinion do not make up for this.\n\n* I had several clarity issues reading the paper, related to notation, formulation, experiments, paper structure and general model clarity:\n  * The explanation for obtaining eta is unclear to me (\"match input/output range\", \"not dominate\"). Concept of \"range\" and its importance is not introduced. It seems that it would depend on weight distribution as well?\n  * Model-oriented scale: overloading of variable W is confusing.\n  * Notation: the lack of distinction between scalar+global variables (eta, c) and per-partition scalars hinders clarity. Indexing partitions would help\n  * Gamma: It is unclear how this component \"makes trained weights empirically converge to eta_tilde\". If eta_tilde is the norm for the full model, how does gamma achieve this without using that global information?\n  * weight decay: Could be expressed in a clearer way. Why is it affected by the decay multiplier \"d\", while it also has its own decay factor \"c\"? The explanation in 4.2 introducing L2 doesn't seem to address this\n  * Section 4.2 aims to provide a \"heuristic derivation\", but it is very difficult to follow since it is not lemma-oriented. At many points, it is not clear what are we expecting to see, and how exactly the discussed equations are related to Amos. E.g. after equation 5: \"now recall that we are given eta such that...\": this was claimed, but I don't see in the paper where this is satisfied. In general, I found the section very confusing as presented. My suggestion would be to resort to a lemma-oriented structure, and whenever the derivations aren't possible or don't lead to watertight conclusions (e.g.  inequalities, broad assumptions), provide experiments that support the ideas presented (e.g. tight bounds), as done in Appendix A.5.\n  * Experimental benchmarks: What is the difference between pre-training and regular training when comparing the observed results? If we are expected to train afterwards, wouldn't the final result (including the training after pre-training) be the actual target to compare? Note that recent results on \"critical learning periods\" (e.g. Achille et al 19). show that issues with pre-training can affect final performance.\n\n* Regarding reproducibility: The paper contains a clear description of the steps to be computed. The experiments are based on well-known, public architectures and benchmarks. Quantitative results are provided, although without error bars. Resources required are high but not unreachable. For the missing details, the authors pledge to provide an open-source (JAX) implementation, upon publication. If that is the case and assuming no lucky seeds, reproducibility issues are expected to be minimal.\n\n# Summary Of The Review\n\nMy opinion is that this contribution clearly deserves attention from the community, but it needs more work: It presents an optimizer of increased complexity, involving a substantial amount (potentially arbitrary) heuristics. Experiments are very promising, but there are methodological concerns.\nThe paper would greatly benefit from improvements in clarity, both in formulation and presentation, especially given the increased amount of details needed to understand and tune this optimizer.\n\nFor those reasons I'm inclined to marginally reject, but I thank the authors for their contribution and encourage them to address/answer some of my points above in order to reconsider my evaluation.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFINE: FUTURE-AWARE INFERENCE FOR STREAMING SPEECH TRANSLATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nA popular approach to streaming speech translation is to employ a single offline model together with a wait-k policy to support different latency requirements. It is a simpler alternative compared to training multiple online models with different latency constraints. However, there is an apparent mismatch in using a model trained with complete utterances on partial streaming speech during online inference. We demonstrate that there is a significant difference between the speech representations extracted at the end of a streaming input and their counterparts at the same positions when the complete utterance is available. Built upon our observation that this problem can be alleviated by introducing a few frames of future speech signals, we propose Future-aware inference (FINE) for streaming speech translation with two different methods to make the model aware of the future. The first method FINE-Mask incorporates future context through a trainable masked speech model. The second method FINE-Wait simply waits for more actual future audio frames at the cost of extra latency. Experiments on the MuSTC EnDe, EnEs, and EnFr benchmarks show that both methods are effective and can achieve better trade-offs between translation quality and latency than strong baselines, and a hybrid approach combining the two can achieve further improvement. Extensive analyses suggest that our methods can effectively alleviate the aforementioned mismatch problem between offline training and online inference.\n\n1\n\nINTRODUCTION\n\nStreaming speech translation (ST) systems consume audio frames incrementally and generate realtime translations, unlike their offline counterparts which have access to the complete utterance before starting to translate. Because of the streaming nature, streaming ST models commonly use unidirectional encoders (Ren et al., 2020; Ma et al., 2020b; Zeng et al., 2021) and are trained with some wait-k policy (Ma et al., 2019) that determines whether to wait for more speech frames or emit target tokens. In real-world applications, however, it is a costly effort to train and maintain multiple models to satisfy different latency requirements (Zhang & Feng, 2021). Recently, some works (Papi et al., 2022; Dong et al., 2022) show that offline models can be adapted to streaming scenarios with waitk policies to meet different latency requirements and achieve comparable or better performance, partially due to their use of more powerful bidirectional encoders. However, there is an inherent mismatch in using a model trained with complete utterances on incomplete streaming speech during online inference (Ma et al., 2019).\n\nIntuitively, speech representations extracted from streaming inputs (Figure 1(b)) are less informative than in the case with full speech encoding (Figure 1(a)). Two questions arise naturally: how much is the difference in speech representations between the two inference modes, and is it significant enough to cause problems? We analyze the gap in speech representations, measured by cosine similarity, at different positions in the streaming input compared to using the full speech (Section 3). We find that there is a significantly greater gap for representations closer to the end of a streaming segment, with an average similarity score as low as 0.2 for the last frame, and the gap quickly narrows for frames further away. Moreover, we observe more degradation in translation quality for utterances with the greatest gap in speech representations between online and offline inference.\n\nBased on the above findings, we hypothesize that the lack of future contexts at the end of streaming inputs could be detrimental to streaming speech translation. To this end, we propose two novel\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Training: Full speech encoding\n\n(b) Testing: Streaming encoding\n\n(c) FINE-Mask\n\n(d) FINE-Wait\n\nFigure 1: (a) and (b) represent the input mismatch between offline training and streaming testing. (c) and (d) denote the proposed methods FINE-Mask and FINE-Wait, respectively. “M” in (c) denotes the mask token. Our methods introduce more informative future context to mitigate the mismatch.\n\nFuture-aware inference (FINE) strategies for streaming speech translation: FINE-Mask and FINEWait, as shown in Figure 1(c) and 1(d). In FINE-Mask, we append a few mask embeddings to the end of the current streaming speech tokens as additional input to the acoustic feature extractor, which based on its masked modeling capability can implicitly estimate and construct future contexts in the corresponding hidden representations and extract more accurate representations for the frames in the streaming input. Since we find that only the speech representations of the last few positions in the streaming input are severely affected by the mismatch problem, the closest future context could provide the most improvement. Thus in FINE-Wait, we simply wait for a few extra speech tokens during streaming encoding and use them as the future context to extract improved representations for the frames in the original streaming segment. FINE-Wait incurs additional latency as the strategy requires waiting for more oracle future context, but it achieves significant improvement in translation quality and leads to a better trade-off.\n\nWe conduct experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks. Experimental results show that our methods outperform several strong baselines on the trade-off between translation quality and latency. In particular, in the lower latency range (when AL is less than 1000ms), we achieve improvements of 8 BLEU in EnDe, 12 BLEU in EnEs, and 6 BLEU in EnFr. Extensive analyses demonstrate that introducing future context reduces the representation gap between the full speech encoding and the partial streaming encoding.\n\n2 BACKGROUND\n\nSpeech translation systems can be roughly categorized into non-streaming (offline) and streaming (online) depending on the inference mode. Regardless of the inference mode, speech translation models typically employ the encoder-decoder architecture and are trained on an ST corpus , where x = (x1, . . . , xT ) denotes an audio sequence, z = (z1, . . . , zI ) and\n\nD y = (y1, . . . , yJ ) the corresponding source transcription and target translation respectively.\n\n(x, z, y)\n\n=\n\n{\n\n}\n\nNon-Streaming Speech Translation For the non-streaming ST task, the encoder maps the entire input audio x to the speech representations h, and the decoder generates the j-th target token yj conditional on the full representations h and the previously generated tokens y<j. The decoding process of non-streaming ST is defined as:\n\np(y\n\n|\n\nx) =\n\nJ (cid:89)\n\nj=1\n\np (yj |\n\nx, y<j) .\n\n(1)\n\nA significant amount of work has focused on non-streaming ST, including pre-training (Wang et al., 2020a; Dong et al., 2021a; Tang et al., 2022; Ao et al., 2022),multi-task learning (Liu et al., 2020; Indurthi et al., 2020; 2021),data augmentation (Pino et al., 2019; Di Gangi et al., 2019b; McCarthy et al., 2020),knowledge distillation (Dong et al., 2021b; Zhao et al., 2021; Du et al., 2022),and cross-modality representation learning (Tang et al., 2021; Fang et al., 2022; Ye et al., 2022).\n\nStreaming Speech Translation A streaming ST model generates the j-th target token yj based on streaming audio prefix x g(j) and the previous tokens y<j , where g(j) is a monotonic non-\n\n≤\n\n2\n\n(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:7)(cid:11)(cid:9)(cid:10)(cid:12)(cid:7)(cid:8)(cid:10)(cid:2)(cid:11)(cid:12)(cid:13)Under review as a conference paper at ICLR 2023\n\ndecreasing function representing the ending timestamp of the audio prefix that needs to be consumed to generate the j-th word. The decoding probability is calculated as:\n\np(y\n\n|\n\nx) =\n\nJ (cid:89)\n\nj=1\n\np (cid:0)yj |\n\nx\n\ng(j), y<j\n\n≤\n\n(cid:1) .\n\n(2)\n\nThus, a streaming ST model requires a policy to determine whether to wait for more source speech or emit new target tokens. Recent studies (Ma et al., 2020b; Ren et al., 2020; Zeng et al., 2021; Dong et al., 2022) on streaming ST make read/write decisions based on a variant of the wait-k policy (Ma et al., 2019) that was initially proposed for streaming text translation, which alternates write and read operations after reading the first k source tokens. Because there is no explicit word boundaries in a streaming audio, several works attempt to detect word boundaries in the audio sequence using methods such as fixed length (Ma et al., 2020b), Connectionist Temporal Classification (Ren et al., 2020; Zeng et al., 2021; Papi et al., 2022), ASR outputs (Chen et al., 2021), and integrate-andfire (Dong et al., 2022). The wait-k policy is applied based on detected words rather than audio frames. In other words, g(j) in Eq.(2) represents the length of audio segment corresponding to the 1 detected words in the streaming ST. Moreover, some studies (Arivazhagan et al., first j + k 2019; Ma et al., 2020c; Zhang et al., 2020; Schneider & Waibel, 2020; Miao et al., 2021; Zhang & Feng, 2022a;b; Zhang et al., 2022; Chang & Lee, 2022; Liu et al., 2021) explore adaptive policies to dynamically decide when to read or write for streaming text and/or streaming speech translation. Several works (Zhang et al., 2021; Zhang & Feng, 2022c) apply knowledge distillation and fill future source positions with positional encoding to introduce future information during training for simultaneous machine translation within the prefix-to-prefix framework. In this paper, we focus on a matter less attended to – how to alleviate the mismatch between offline training and online inference.\n\n−\n\n3 PRELIMINARY ANALYSIS\n\nIn this section, we analyze the major mismatch in Transformer-based (Vaswani et al., 2017) ST architecture between offline training and online decoding. In full-sentence ST, the speech representation of each frame is obtained by attending to all unmasked frames by the multi-head attention in the transformer encoder layers. However, if directly applied to the streaming inference with the model trained offline, the speech representation of the current last frame will deteriorate because it can only attend to its previous frames. Recently, a common approach in speech translation is to stack a pre-trained Wav2Vec2.0 (Baevski et al., 2020) as the acoustic encoder with a semantic NMT encoder-decoder, and achieves SOTA performance in the ST task (Han et al., 2021; Dong et al., 2022; Fang et al., 2022; Ye et al., 2022), because it has been shown that a better speech representation can be learned via Wav2Vev2.0 (Baevski et al., 2020).\n\nTo explore the precise effects of streaming inputs, we first follow MoSST (Dong et al., 2022) to train an offline ST model on the MuST-C EnDe training set, where the acoustic encoder Wav2Vec2.0 is trainable. After the offline ST training, we conduct an analysis on the MuST-C EnDe tst-COMMON set. We remove the outliers and the noisy data, and select audios with a duration between 2s and 10s, resulting in a total of 1829 examples.\n\nFor an input sequence of audio frames x = (x1, . . . , xT ), the convolutional subsampler of Wav2Vec2.0 shrinks the length of the raw audio by a factor 320 and outputs the full speech representation sequence a. In other words, every 320 elements in x become a vector in a. For readability reasons, we uniformly use the notation T to denote the sequence length of a, i.e., a = (a1, . . . , aT ). This simplified notation does not undermine any of our conclusions while at the same time making the equations for readable1. For streaming input T, ˆxt = (x1, . . . , xt), Wav2Vec2.0 will output the representation ˆat = (ˆat,1, . . . , ˆat,t).\n\nt ∀\n\n≤\n\n3.1 WHICH PART OF STREAMING SPEECH REPRESENTATION IS WORSE?\n\nTo measure the gap of the speech representations between the offline and online inputs, we calculate the cosine similarity st,t′ between the speech representation at the t′-th (t′ t) position in the t-th\n\n≤\n\n1Because we can always define x = (x1:T ) such that xt represents consecutive 320 audio frames.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) ̄st,t′ of the first three positions\n\n(b) ̄st,t′ of middle three positions\n\n(c) ̄st,t′ of the last three positions\n\nFigure 2: The average cosine similarity ̄st,t′ of the first three (t′ = 1, 2, 3), middle three (t′ = 1, t) positions for each encoding step t.\n\n+ 1), and last three (t′ = t\n\n2, t\n\n1+t\n\n1+t\n\n1+t\n\n1,\n\n,\n\n⌊\n\n2 ⌋ −\n\n⌊\n\n2 ⌋\n\n⌊\n\n2 ⌋\n\n−\n\n−\n\ny\n\nt i\nr a\nl i\n\nm S\n\ni\n\ne n\n\ni s\no C\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nChimera\n\nSTEMM\n\nMoSST\n\n40\n\n−\n\n30\n\n−\n\n20\n\n−\n\nPosition (beginning →\n(a) Last 40 positions\n\n− end)\n\n30\n\n25.69\n\n19.75\n\n20\n\nU E\nL B\n\n10\n\nOffline ST\n\nStreaming ST\n\n23.79\n\n24.13\n\n23.42\n\n21.91\n\n14.86\n\n13.73\n\n12.01\n\n9.5\n\n10\n\n0\n\nBest\n\nBetter Medium Worse\n\nWorst\n\nDegree (small (b) Degree of deterioration of last representations\n\nlarge)\n\n→\n\nFigure 3: (a). The average cosine similarity ̄st′ of the forty representations at the end positions (position= 1 denotes last position) in the streaming speech. (b). Performance with degree of deterioration of the representation at the last position of the streaming speech.\n\n−\n\nstreaming audio input ˆxt and the speech representation at the same position in the full encoding. Then we average the cosine similarities over the sentences in dataset (cid:88)\n\nto obtain robust statistics.\n\n(cid:88)\n\n1\n\n1\n\nB cos(ˆat,t′, at′),\n\n(3)\n\nFor t′\n\nt,\n\n ̄st,t′ =\n\n≤\n\nst,t′(x) =\n\n|Bt|\n\nx\n\nt\n\n∈B\n\n|Bt|\n\nx\n\nt\n\n∈B\n\nwhere\n\nBt =\n\nx :\n\n{\n\nx |\n\n| ≥\n\nt\n\n}\n\ncontains the audio inputs with length no shorter than t.\n\n,\n\n⌊\n\n⌊\n\n⌊\n\n1,\n\n1+t\n\n1+t\n\n1+t\n\n2 ⌋\n\n2 ⌋\n\n2 ⌋ −\n\n+ 1), and last three (t′ = t\n\nWe empirically compare the averaged cosine similarity at the beginning, middle, and end positions of the speech representations. Figure 2 shows ̄st,t′ of the first three (t′ = 1, 2, 3), middle three 1, t) positions for each encoding (t′ = step t. At the beginning and middle positions, the averaged cosine similarity ̄st,t′ is greater than 0.8 except t′ = 1, indicating that the representations at such positions in the partial streaming input are close to those in the full speech. Note that t′ = 1 with a slightly lower similarity won’t hurt the performance much, because in practice it is almost impossible to apply wait-1 policy in streaming ST. However, the ̄st,t′ declines significantly for the end positions, especially for the last one. In addition, we observe that as t becomes larger, the streaming input will gradually approximate the full speech input, then the gap of the speech representation between the offline and the online input becomes smaller. We conclude that the representations of the end position in the streaming speech are particularly inferior.\n\n2, t\n\n−\n\n−\n\nWe also average the cosine similarity over both dataset and time dimension with reversed index.\n\nt′ =\n\n ̄s\n\n−\n\n1\n\n(cid:88)\n\n|Bt′\n\n|\n\nx\n\n∈Bt′\n\n1 t′ + 1\n\n| −\n\nx |\n\nx |(cid:88) |\n\nt=t′\n\nst,t\n\nt′+1(x)\n\n−\n\n(4)\n\nWe calculate the metric ̄s t′ of the representations at the last 40 positions in the streaming speech for different methods: Chimera (Han et al., 2021), STEMM (Fang et al., 2022) and MoSST, and\n\n−\n\n4\n\n1002003004005000.20.40.60.81StreamingencodingsteptCosineSimilarityt′=1t′=2t′=31002003004005000.20.40.60.81Streamingencodingsteptt′=⌊1+t2⌋−1t′=⌊1+t2⌋t′=⌊1+t2⌋+11002003004005000.20.40.60.81Streamingencodingsteptt′=t−2t′=t−1t′=tUnder review as a conference paper at ICLR 2023\n\nFigure 4: Illustration of offline ST model and proposed methods FINE-Mask and FINE-Wait.\n\nreport the results in the Figure 3(a). The consistent results verify that the conclusion above holds and decide that the low-quality representations at the last 10 positions cannot be ignored.\n\n3.2 DOES THE POOR REPRESENTATION AT THE LAST POSITIONS OF STREAMING SPEECH\n\nAFFECT STREAMING ST PERFORMANCE?\n\nTo answer this question, we only calculate the average cosine similarity in the last position for each sample.\n\nx,\n\n∀\n\n1(x) =\n\n ̄s\n\n−\n\n1 T\n\nt=T (cid:88)\n\nt=1\n\ncos(ˆat,t, at),\n\n(5)\n\n−\n\n1(x) reflects the degree of deterioration of the representation at the last position of the streaming ̄s speech. We sort the dataset by the value of the degree and divide them evenly into 5 groups to ensure enough samples in each group. The translation quality of each group is shown in Figure 3(b). The performance of streaming ST drops close to 10 points as the representation at the last position of the streaming speech becomes worse, while the full-sentence ST fluctuates less than 4 points. In addition, the performance gap between the streaming ST and the full-sentence ST becomes larger as the representation at the last position gets worse. In the worse group, the streaming ST is 12.41 points lower than the full-sentence ST. Therefore, we conclude that the poor representation at the end position of the streaming speech has a strong effect on the translation quality.\n\n4 FINE: FUTURE-AWARE INFERENCE\n\nBased on these analyses, we find that it is only necessary for the offline ST model to be aware of a short future during streaming encoding. Thus, we propose two Future-aware INferencE strategies, FINE-Mask and FINE-Wait, to enhance the representations of streaming speech in Figure 4.\n\n4.1 FINE-MASK\n\nIn this strategy, we use the mask tokens of Wave2Vec2.0 as the pseudo future context and append them to the speech tokens generated from the already consumed speech frames. The mask token embedding is trainable when pre-training Wave2Vec2.0. Particularly, Wav2vec2.0 applies span masks to the speech tokens and reconstructs 2 the corresponding latent features based on unmasked context. By default, the pre-training results in approximately 49% of all time steps being masked with a mean span length of 14.7 (300ms). This pre-training strategy makes the Wav2vec2.0 able to extract better speech representations in offline ST task (Han et al., 2021; Dong et al., 2022; Ye et al., 2022).\n\nWav2Vec2.0 consists of a multi-layer convolutional subsampler fc and a Transformer encoder fe. Concretely, for each audio prefix ˆxt = (x1, . . . , xt) during online inference, the fc first outputs d and d is the dimension of model and τ streaming speech tokens ˆct = (c1, . . . , cτ ), where ˆc is the sequence length after convolutional subsampling. Then, we concatenate the streaming speech Rd along the time dimension, resulting in a longer tokens ˆc and m mask token embeddings e\n\nRτ\n\n∈\n\n×\n\n∈\n\n2Strictly speaking, the task is to identify the quantized latent audio representation rather than reconstruction.\n\n5\n\n(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:2)(cid:3)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:12)(cid:2)(cid:8)(cid:18)(cid:20)(cid:21)(cid:22)(cid:15)(cid:3)(cid:13)(cid:11)(cid:8)(cid:23)(cid:3)(cid:4)(cid:10)(cid:11)(cid:16)(cid:13)(cid:24)(cid:8)(cid:25)(cid:12)(cid:6)(cid:12)(cid:2)(cid:6)(cid:3)(cid:13)(cid:26)(cid:12)(cid:27)(cid:16)(cid:10)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:2)(cid:3)(cid:11)(cid:12)(cid:13)(cid:28)(cid:13)(cid:16)(cid:10)(cid:5)(cid:29)(cid:16)(cid:6)(cid:7)(cid:3)(cid:10)(cid:8)(cid:25)(cid:12)(cid:2)(cid:3)(cid:11)(cid:12)(cid:13)(cid:30)(cid:31)(cid:31)(cid:8)(cid:1)(cid:2)(cid:28)(cid:13)(cid:16)(cid:10)(cid:5)(cid:32)(cid:3)(cid:13)(cid:27)(cid:12)(cid:13)(cid:8)(cid:9)(cid:10)(cid:2)(cid:3)(cid:11)(cid:12)(cid:13)(cid:8)(cid:1)(cid:3)(cid:30)(cid:31)(cid:31)(cid:8)(cid:1)(cid:2)(cid:26)(cid:6)(cid:13)(cid:12)(cid:16)(cid:27)(cid:7)(cid:10)(cid:33)(cid:8)(cid:16)(cid:4)(cid:11)(cid:7)(cid:3)(cid:34)(cid:16)(cid:7)(cid:6)(cid:7)(cid:10)(cid:33)(cid:8)(cid:32)(cid:3)(cid:13)(cid:8)(cid:4)(cid:6)(cid:3)(cid:35)(cid:12)(cid:10)(cid:5)(cid:28)(cid:13)(cid:16)(cid:10)(cid:5)(cid:32)(cid:3)(cid:13)(cid:27)(cid:12)(cid:13)(cid:8)(cid:9)(cid:10)(cid:2)(cid:3)(cid:11)(cid:12)(cid:13)(cid:8)(cid:1)(cid:3)(cid:27)(cid:16)(cid:5)(cid:35)(cid:8)(cid:6)(cid:3)(cid:35)(cid:12)(cid:10)(cid:3)(cid:13)(cid:16)(cid:2)(cid:29)(cid:12)(cid:8)(cid:5)(cid:36)(cid:12)(cid:12)(cid:2)(cid:37)(cid:8)(cid:6)(cid:3)(cid:35)(cid:12)(cid:10)(cid:14)(cid:16)(cid:22)(cid:38)(cid:32)(cid:32)(cid:29)(cid:7)(cid:10)(cid:12)(cid:8)(cid:39)(cid:3)(cid:11)(cid:12)(cid:29)(cid:8)(cid:26)(cid:6)(cid:13)(cid:4)(cid:2)(cid:6)(cid:4)(cid:13)(cid:12)(cid:14)(cid:40)(cid:22)(cid:41)(cid:42)(cid:31)(cid:9)(cid:43)(cid:39)(cid:16)(cid:5)(cid:35)(cid:14)(cid:2)(cid:22)(cid:41)(cid:42)(cid:31)(cid:9)(cid:43)(cid:15)(cid:16)(cid:7)(cid:6)(cid:28)(cid:13)(cid:16)(cid:10)(cid:5)(cid:29)(cid:16)(cid:6)(cid:7)(cid:3)(cid:10)Under review as a conference paper at ICLR 2023\n\nAlgorithm 1 Pseudocode of FINE-Mask inference strategy in a PyTorch-like style.\n\n# model: an offline-trained ST model consists of a acoustic encoder Wav2vec2.0, a token\n\nboundary detector, a semantic encoder, and a decoder # m: mask length, K: wait lagging, audio: audio waveform # mask_emb: pre-trained mask embedding in Wav2vec\n\nN = 0 # the number of source text tokens x = [] # streaming audio prefix y = [] # translations mask_embs = mask_emb.repate(m, 1) # mask embeddings: m while y[-1] != \"<eos>\":\n\nd\n\n×\n\nif x == audio: # audio has been read\n\ny = y + model(a,y) # write new target token\n\nelif N - len(y) < K: # wait K detected source tokens x = x + read(audio) # incrementally read audio c = model.wav2vec2.cnn(x) # audio tokens τ\n\nd\n\n×\n\n# concatenate audio tokens and mask embeddings, (τ + m) c = torch.cat((c, mask_embs), dim=0) a = model.wav2vec2.encoder(c) # audio representations, (τ + m) a = a[:a.shape[0] - m,:] # discard the predicted representations, τ\n\n×\n\n×\n\nd\n\nd\n\nd\n\n×\n\nif model.token_detector(a): # source text token boundary is detected\n\nN += 1\n\nelse:\n\nh = model.semantic_encoder(a) y = y + model.decoder(h, y) # write new target token\n\nd. The new speech tokens are then fed into the Transformer sequence of speech tokens encoder fe, but only the first τ encoder outputs (i.e., speech features) will be kept for the decoder because, as discussed in Section 3.1, the last m speech features are of poor quality and adversely affect translation quality. The FINE-Mask inference strategy is outlined in Algorithm 1.\n\n∈\n\n×\n\nR(τ +m)\n\n4.2 FINE-WAIT\n\nIn the previous section, we conclude that the speech representations of only the last few positions in the streaming input are inferior. Therefore, a straightforward method is to discard the poor representation at the end positions. In the FINE-Wait strategy, we read more audio frames until the fc can output m extra speech tokens, resulting in a new sequence of speech features with length τ + m. We then discard the last m speech features as in the FINE-mask strategy above. The FINE-Wait inference strategy is outlined as Algorithm 2 in Appendix. The FINE-Wait strategy incurs additional latency as the model waits for more actual audio frames during each streaming encoding. However, this strategy results in significant improvement, and motivates us to reconsider the trade-off between translation quality and latency. The detailed analysis is given in Section 5.3.\n\n5 EXPERIMENTS\n\n5.1 EXPERIMENTAL SETTINGS\n\nDatasets We evaluate our approach on MuST-C English-German (EnDe), English-Spanish (EnEs) datasets (Di Gangi et al., 2019a). Because limited previous works discussed the MuST-C EnglishFrench streaming ST with BLEU-latency curve, we present the EnFr results in Appendix. All the corpora contain source audios, source transcriptions, and target translations, and the results reported are conducted on the corresponding tst-COMMON set. For speech data, we normalize the raw audio wave to the range of [ 1, 1). For text data, we keep punctuation and remove non-printing characters, and remain case-sensitive. For each translation direction, the unigram SentencePiece3 model (Kudo & Richardson, 2018) is used to learn a shared vocabulary of size 10k.\n\n−\n\nModel Configuration We follow MoSST (Dong et al., 2022) to train the offline model. For the acoustic encoder, we use Wav2vec2.04 (Baevski et al., 2020) following the base configurations, which use a self-supervised learning framework to pre-train on large-scale audio data from the LibriSpeech (Panayotov et al., 2015) corpus. We use the continuous integrate-and-fire (CIF) module\n\n3https://github.com/google/sentencepiece 4https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n24\n\n20\n\nU E\nL B\n\n16\n\n12\n\n8\n\n28\n\n24\n\n20\n\n16\n\n12\n\n8\n\n4\n\noffline (greedy)\n\nRealTrans †\n\nFINE-Wait\n\nMU-ST † MoSST ‡\n\nFINE-Mask\n\noffline (greedy)\n\nRealTrans †\n\nFINE-Wait\n\nSimulSpeech † MoSST ‡\n\nFINE-Mask\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\n5,000\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\nAverage Lagging (ms)\n\n(a) EnDe\n\nAverage Lagging (ms)\n\n(b) EnEs\n\nFigure 5: The translation quality (BLEU) against the latency metrics (AL) on the tst-COMMON set of MuST-C EnDe and EnEs dataset. † denotes that the results are obtained from corresponding papers. ‡ denotes that the results are from our improved MoSST inference method.\n\n22\n\nU E\nL B\n\n20\n\n18\n\n16\n\n28\n\n26\n\n24\n\n22\n\noffline (greedy)\n\nFINE-Wait\n\nFINE-Mask\n\nFINE-Hybrid\n\noffline (greedy)\n\nFINE-Wait\n\nFINE-Mask\n\nFINE-Hybrid\n\n1,000\n\n2,000\n\n3,000\n\n1,000\n\n1,500\n\n2,000\n\n2,500\n\n3,000\n\n3,500\n\nAverage Lagging (ms) (a) EnDe\n\nAverage Lagging (ms)\n\n(b) EnEs\n\nFigure 6: The translation quality (BLEU) against the latency metrics (AL) on the tst-COMMON set of MuST-C EnDe and EnEs dataset.\n\n(Yi et al., 2021) as the word boundary detector. We use 8 and 6 Transformer layers for the semantic encoder and the translation decoder respectively, with 4 attention heads and 768 hidden units.\n\nInference We use the offline-trained ST model to perform streaming-testing with the wait-k policy. k here means k detected source text tokens by the CIF word boundary detector. The length of future context tokens (m) is 50 and 10 for FINE-Mask and FINE-Wait, respectively. For consistency with previous works (Zeng et al., 2021; Dong et al., 2022), we do not rewrite the tokens that have already been generated during inference. All hyper-parameters are tuned on EnDe and applied to other language pairs. In all experiments, we use our re-implemented MoSST inference method for a better performance (see Appendix B.2 for a detailed explanation).\n\nEvaluation Metrics We use SacreBLEU5 to measure the translation quality. The latency is evaluated with Average Latency (AL) (Ma et al., 2019), Average Proportion (AP) (Cho & Esipova, 2016), and Differentiable Average Lagging (DAL) (Cherry & Foster, 2019) in the SimulEval toolkit6 (Ma et al., 2020a).\n\nBaselines We compare our method with several strong end-to-end streaming ST approaches. (i) SimulSpeech (Ren et al., 2020) and RealTranS (Zeng et al., 2021) use uni-directional encoder rather than bidirectional to simulate streaming inputs. (ii) MoSST (Dong et al., 2022) applies an offline-trained model with a monotonic segmentation module for streaming testing and achieves\n\n5https://github.com/mjpost/sacrebleu 6https://github.com/facebookresearch/SimulEval\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n22\n\n18\n\n14\n\n10\n\n6\n\n2\n\nU E\nL B\n\n18\n\n14\n\n10\n\n6\n\n2\n\nm = 50\n\nm = 30\n\nm = 15\n\nm = 5\n\nm = 40\n\nm = 20\n\nm = 10\n\nm = 0\n\nm = 70\n\nm = 60\n\nm = 50\n\nm = 30\n\nm = 20\n\nm = 10\n\nm = 5\n\nm = 0\n\n500\n\n1,000\n\n1,500\n\n2,000\n\n500\n\n1,000\n\n1,500\n\nAverage Lagging(ms)\n\n(a) FINE-Wait\n\nAverage Lagging(ms)\n\n(b) FINE-Mask\n\nFigure 7: Effect from different lengths of future context. The observed points of FINE-Wait and FINE-Mask in the plots represent wait-k policy with k = 3, 5, 7.\n\ncompetitive performance. For fair comparisons, we use MoSST as our baseline model. (iii) MU-ST (Zhang et al., 2022) learns an adaptive segmentation policy to detect meaningful units, which makes read/write decisions.\n\n5.2 MAIN RESULTS\n\nWe presents the results on the MuST-C EnDe and EnEs tst-COMMON set in Figure 5 7. Compared with the online models SimulSpeech and RealTranS, the offline model MoSST achieves higher translation quality with high latency as it encodes bidirectional context information during training. But in the low latency region, it performs poorly one reason for which is the input mismatch between offline-training and online-decoding. With the ability to reduce this mismatch, FINE-Mask and FINE-Wait achieve higher BLEU in all latency regions. In particular, our methods outperform our most compatible base model MoSST by large margins in lower latency regions (when AL is less than 1000ms), with improvements over 8 BLEU in EnDe and 12 BLEU in EnEs. This indicates that FINE-Mask and FINE-Wait can effectively mitigate the input mismatch between offline-training and online-decoding. FINE-Wait only requires waiting for a small amount of oracle future context 8, but brings higher translation quality. In addition, our strategies achieve comparable translation quality with full-speech translation at middle latency (at AL around 2000ms), especially for EnEs. Compared with FINE-Wait, FINE-Mask achieves a better trade-off as it can introduce more future context without additional latency. Moreover, we find that FINE-Wait and FINE-Mask can be further combined into a new strategy – FINE-Hybrid. Specifically, given a streaming speech with length τ , we first wait for m extra oracle speech tokens and add m′ mask tokens to the end of the streaming speech. The total length of the streaming speech tokens will be τ + m + m′, but like FINE-Mask and FINE-Wait, only the first τ encoder outputs will be kept for decoding. We set m = 10 and m′ = 50 for FINE-Hybrid, which are the optimal settings of FINE-Wait and FINE-Mask, respectively. The experimental results are shown in Figure 6. We observe that FINE-Hybrid achieves a better trade-off.\n\n5.3 ABLATION STUDY\n\nIn this section, we describe experiments to evaluate the effectiveness of our methods from various aspects. All ablation results are obtained from the MuST-C EnDe tst-COMMON set.\n\n5.3.1 HOW MUCH FUTURE CONTEXT IS NEEDED?\n\nTo answer this question, we compare FINE-Mask and FINE-Wait with different lengths of future context m. Figure 7 shows the results. The system that inherits the mismatch problem, i.e. uses the offline model directly for online decoding, is shown by setting m = 0. For FINE-Wait, increasing m obtains better translation quality, but generally brings higher latency. Our results in Figure 7(a) shows that it achieves the better trade-off between quality and latency when m = . If\n\n10, 15, 20\n\n{\n\n}\n\n7The extended results for other latency metrics (AP and DAL) are described in Appendix B.6. 8The results are reported with m = 10 for FINE-Wait. It only corresponds to 200ms oracle audio.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\ny\n\nt i\nr a\nl i\n\nm S\n\ni\n\ne n\n\ni s\no C\n\nFINE-Wait\n\nFINE-Mask\n\n60\n\n70\n\n2 6 10\n\n20\n\n50 30 (a) Future Context Length\n\n40\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\ny\n\nt i\nr a\nl i\n\nm S\n\ni\n\ne n\n\ni s\no C\n\nFINE-Wait\n\nFINE-Mask\n\nMoSST\n\n40\n\n−\n\n30\n\n−\n\n20\n\n−\n\n10\n\n0\n\n−\n\n(b) Last 40 positions of consumed speech\n\nFigure 8: (a). Effect of future context length on the average cosine similarity ̄st,t at the last position of streaming speech. (b). Effect on the average cosine similarity ̄s t′ of the forty representation at the end position in the streaming speech. After applying FINE-Mask and FINE-Wait, the representation at the end position is improved.\n\n−\n\nm > 20, the large latency becomes prohibitive. Unlike FINE-Wait, FINE-Mask can attend to more future context without waiting for future audio. It achieves the best trade-off between quality and latency at m = 50 (Figure 7(b)). Since the predicted context further into the future will likely introduce more noise, FINE-Mask obtains similar performance when m is increased from 30 to 70. We also investigate the impact of various future context lengths on the representation of the last position by calculating the average cosine similarity in Eq. (4). The results are shown in Figure 8(a). We observe that 1) as m increases, the representation of the last position in the streaming speech becomes better. 2) the curves of the average cosine similarity for FINE-Wait and FINEMask becomes flattened when m > 10 and m > 50, respectively. This is consistent with the results in Figure 7. Therefore, we set future context length m = 10 and m = 50 respectively for FINE-Wait and FINE-Mask in our other experiments.\n\n5.3.2 WHY DOES FINE WORK?\n\n−\n\nFigure 8(b) plots the changes of average cosine similarity ̄s t′ (Eq. (4)) of the last 40 positions in the streaming speech after applying the FINE strategies. FINE-Mask and FINE-Wait achieve at least 0.6 and 0.8 cosine similarity at the last position. In MoSST, however, the cosine similarity is less than 0.6 for the last 4 positions and only 0.2 for the last position. Thus, the speech representations with FINEWait and FINE-Mask are closer to those of the full speech input, especially at the end positions. The cosine similarities in FINE-Mask are lower than those in FINE-Wait because the predicted future context is less accurate by consuming mask tokens. However, FINE-Mask achieves better balance (Figure 5) as it does not incur additional latency. In sum, introducing future context significantly reduces the representation gap between full and partial speech input, improves streaming speech representations, and achieves a better balance between quality and latency.\n\nMore ablation studies are included in the Appendix. In Appendix B.3 we find the least monotonic examples are mostly improved by our strategies. Appendix B.4 analyzes the difference of the pretrained and fine-tuned Wav2Vec2.0 with respect to future context representations. Appendix B.5 demonstrates why all predicted features should be discarded in FINE-Mask.\n\n6 CONCLUSION\n\nIn this paper, we examine streaming speech translation from a new perspective. We investigate the effects of the input mismatch between offline-training and online-decoding. We find that the representations at the end positions in the streaming input are particularly poor, directly impacting the translation quality. We propose FINE-Mask and FINE-Wait to improve these representations by introducing, respectively, predicted and real future context. Experiments and analysis demonstrate their effectiveness in bridging the representation gap between full speech encoding and partial streaming encoding. Furthermore, our strategies can be generally beneficial to streaming speech translation models that are based on Wav2Vec2.0. In the future, we will experiment with other methods to improve the accuracy of predicting future information. We hope the work and perspective presented in this paper can engender further innovations in general streaming translation.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJunyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei. SpeechT5: Unified-modal encoderdecoder pre-training for spoken language processing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5723–5738, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.393. URL https://aclanthology.org/2022.acl-long.393.\n\nNaveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei Li, and Colin Raffel. Monotonic infinite lookback attention for simultaneous machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1313–1323, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1126. URL https://aclanthology.org/P19-1126.\n\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.\n\nA framework for self-supervised learning of speech representations. M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural formation Processing Inc., 92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf.\n\nwav2vec 2.0: In H. Larochelle, In12449–12460. Curran Associates, URL https://proceedings.neurips.cc/paper/2020/file/\n\nSystems,\n\nvolume\n\n2020.\n\npp.\n\n33,\n\nChih-Chiang Chang and Hung-yi Lee. Exploring continuous integrate-and-fire for efficient and\n\nadaptive simultaneous speech translation. arXiv preprint arXiv:2204.09595, 2022.\n\nJunkun Chen, Mingbo Ma, Renjie Zheng, and Liang Huang. Direct simultaneous speech-toIn Findings of the Association for text translation assisted by synchronized streaming ASR. Computational Linguistics: ACL-IJCNLP 2021, pp. 4618–4624, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.406. URL https: //aclanthology.org/2021.findings-acl.406.\n\nColin Cherry and George Foster. Thinking slow about latency evaluation for simultaneous machine\n\ntranslation. arXiv preprint arXiv:1906.00048, 2019.\n\nChung-Cheng Chiu and Colin Raffel. Monotonic chunkwise attention.\n\nIn International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= Hko85plCW.\n\nKyunghyun Cho and Masha Esipova. Can neural machine translation do simultaneous translation?\n\narXiv preprint arXiv:1606.02012, 2016.\n\nMattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. MuSTIn Proceedings of the 2019 Conference of the C: a Multilingual Speech Translation Corpus. North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2012–2017, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. doi: 10.18653/v1/N19-1202. URL https: //aclanthology.org/N19-1202.\n\nMattia A. Di Gangi, Matteo Negri, Viet Nhat Nguyen, Amirhossein Tebbifakhr, and Marco Turchi. Data augmentation for end-to-end speech translation: FBK@IWSLT ‘19. In Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong, November 2-3 2019b. Association for Computational Linguistics. URL https://aclanthology.org/ 2019.iwslt-1.14.\n\nQian Dong, Yaoming Zhu, Mingxuan Wang, and Lei Li. Learning when to translate for streaming speech. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 680–694, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.50. URL https://aclanthology. org/2022.acl-long.50.\n\nQianqian Dong, Mingxuan Wang, Hao Zhou, Shuang Xu, Bo Xu, and Lei Li. Consecutive decoding for speech-to-text translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 12738–12748, 2021a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nQianqian Dong, Rong Ye, Mingxuan Wang, Hao Zhou, Shuang Xu, Bo Xu, and Lei Li. Listen, understand and translate: Triple supervision decouples end-to-end speech-to-text translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 12749–12759, 2021b.\n\nYichao Du, Zhirui Zhang, Weizhi Wang, Boxing Chen, Jun Xie, and Tong Xu. Regularizing endto-end speech translation with triangular decomposition agreement. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10590–10598, 2022.\n\nQingkai Fang, Rong Ye, Lei Li, Yang Feng, and Mingxuan Wang. STEMM: Self-learning with speech-text manifold mixup for speech translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7050–7062, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 486. URL https://aclanthology.org/2022.acl-long.486.\n\nA. Graves. Sequence transduction with recurrent neural networks. Computer Science, 58(3):235–\n\n242, 2012.\n\nChi Han, Mingxuan Wang, Heng Ji, and Lei Li. Learning shared semantic space for speech-to-text translation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 2214–2225, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.findings-acl.195. URL https://aclanthology.org/2021.findings-acl. 195.\n\nSathish Indurthi, Houjeung Han, Nikhil Kumar Lakumarapu, Beomseok Lee, Insoo Chung, Sangha Kim, and Chanwoo Kim. End-end speech-to-text translation with modality agnostic metalearning. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7904–7908. IEEE, 2020.\n\nSathish Indurthi, Mohd Abbas Zaidi, Nikhil Kumar Lakumarapu, Beomseok Lee, Hyojung Han, Seokchan Ahn, Sangha Kim, Chanwoo Kim, and Inchul Hwang. Task aware multi-task learning In ICASSP 2021-2021 IEEE International Conference on Acoustics, for speech to text tasks. Speech and Signal Processing (ICASSP), pp. 7723–7727. IEEE, 2021.\n\nTaku Kudo and John Richardson.\n\nSentencePiece: A simple and language independent subIn Proceedings of the 2018 Conword tokenizer and detokenizer for neural text processing. ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.\n\nDan Liu, Mengge Du, Xiaoxi Li, Ya Li, and Enhong Chen. Cross attention augmented transducer networks for simultaneous translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 39–55, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 4. URL https://aclanthology.org/2021.emnlp-main.4.\n\nYuchen Liu, Jiajun Zhang, Hao Xiong, Long Zhou, Zhongjun He, Hua Wu, Haifeng Wang, and Chengqing Zong. Synchronous speech recognition and speech-to-text translation with interactive decoding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8417– 8424, 2020.\n\nMingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3025–3036, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1289. URL https://aclanthology.org/P19-1289.\n\nXutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu, and Juan Pino. SIMULEVAL: An evaluation toolkit for simultaneous translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 144–150, Online, October 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos. 19. URL https://aclanthology.org/2020.emnlp-demos.19.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXutai Ma, Juan Pino, and Philipp Koehn. SimulMT to SimulST: Adapting simultaneous text transIn Proceedings of the 1st Conference of lation to end-to-end simultaneous speech translation. the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pp. 582–587, Suzhou, China, December 2020b. Association for Computational Linguistics. URL https://aclanthology. org/2020.aacl-main.58.\n\nXutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon, and Jiatao Gu. Monotonic multihead attention. In International Conference on Learning Representations, 2020c. URL https:// openreview.net/forum?id=Hyg96gBKPS.\n\nArya D McCarthy, Liezl Puzon, and Juan Pino. Skinaugment: Auto-encoding speaker conversions for automatic speech translation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7924–7928. IEEE, 2020.\n\nYishu Miao, Phil Blunsom, and Lucia Specia. A generative framework for simultaneous maIn Proceedings of the 2021 Conference on Empirical Methods in Natural chine translation. Language Processing, pp. 6697–6706, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.536. URL https://aclanthology.org/2021.emnlp-main.536.\n\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5206–5210. IEEE, 2015.\n\nSara Papi, Marco Gaido, Matteo Negri, and Marco Turchi. Does simultaneous speech translation\n\nneed simultaneous models? arXiv preprint arXiv:2204.03783, 2022.\n\nJuan Pino, Liezl Puzon, Jiatao Gu, Xutai Ma, Arya D. McCarthy, and Deepak Gopinath. Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade. In Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong, November 2-3 2019. Association for Computational Linguistics. URL https://aclanthology. org/2019.iwslt-1.18.\n\nYi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu. SimulIn Proceedings of the 58th AnSpeech: End-to-end simultaneous speech to text translation. nual Meeting of the Association for Computational Linguistics, pp. 3787–3796, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.350. URL https://aclanthology.org/2020.acl-main.350.\n\nFelix Schneider and Alexander Waibel. Towards stream translation: Adaptive computation time for simultaneous machine translation. In Proceedings of the 17th International Conference on Spoken Language Translation, pp. 228–236, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.iwslt-1.28. URL https://aclanthology.org/2020. iwslt-1.28.\n\nYun Tang, Juan Pino, Xian Li, Changhan Wang, and Dmitriy Genzel. Improving speech translation by understanding and learning from the auxiliary text translation task. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4252–4261, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.328. URL https://aclanthology.org/2021.acl-long.328.\n\nYun Tang, Hongyu Gong, Ning Dong, Changhan Wang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski, Xian Li, Abdelrahman Mohamed, Michael Auli, and Juan Pino. Unified speech-text pre-training for speech translation and recognition. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1488–1499, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.105. URL https://aclanthology.org/2022.acl-long.105.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar,\n\nJakob Uszkoreit, Llion Jones, Aidan N In ProceedGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. ings of the 31st International Conference on Neural Information Processing Systems, pp. 6000–6010, 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\nChengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and Zhenglu Yang. Curriculum pre-training for end-to-end speech translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 3728–3738, Online, July 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.344. URL https://aclanthology.org/ 2020.acl-main.344.\n\nChengyi Wang, Yu Wu, Liang Lu, Shujie Liu, Jinyu Li, Guoli Ye, and Ming Zhou. Low Latency End-to-End Streaming Speech Recognition with a Scout Network. In Proc. Interspeech 2020, pp. 2112–2116, 2020b. doi: 10.21437/Interspeech.2020-1292. URL http://dx.doi.org/10. 21437/Interspeech.2020-1292.\n\nRong Ye, Mingxuan Wang, and Lei Li. Cross-modal contrastive learning for speech translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5099–5113, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.naacl-main.376.\n\nCheng Yi, Shiyu Zhou, and Bo Xu. Efficiently fusing pretrained acoustic and linguistic encoders for\n\nlow-resource speech recognition. IEEE Signal Processing Letters, 28:788–792, 2021.\n\nXingshan Zeng, Liangyou Li, and Qun Liu. RealTranS: End-to-end simultaneous speech transIn Findings of the Association for lation with convolutional weighted-shrinking transformer. Computational Linguistics: ACL-IJCNLP 2021, pp. 2461–2474, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.218. URL https: //aclanthology.org/2021.findings-acl.218.\n\nRuiqing Zhang, Chuanqiang Zhang, Zhongjun He, Hua Wu, and Haifeng Wang. Learning adaptive segmentation policy for simultaneous translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2280–2289, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.178. URL https://aclanthology.org/2020.emnlp-main.178.\n\nRuiqing Zhang, Zhongjun He, Hua Wu, and Haifeng Wang. Learning adaptive segmentation policy for end-to-end simultaneous translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7862–7874, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.542. URL https://aclanthology.org/2022.acl-long.542.\n\nShaolei Zhang and Yang Feng. Universal simultaneous machine translation with mixture-of-experts In Proceedings of the 2021 Conference on Empirical Methods in Natural Lanwait-k policy. guage Processing, pp. 7306–7317, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.581. URL https://aclanthology.org/2021.emnlp-main.581.\n\nShaolei Zhang and Yang Feng. Gaussian multi-head attention for simultaneous machine translation. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 3019–3030, Dublin, Ireland, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022. findings-acl.238. URL https://aclanthology.org/2022.findings-acl.238.\n\nShaolei Zhang and Yang Feng. Modeling dual read/write paths for simultaneous machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2461–2477, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.176. URL https://aclanthology. org/2022.acl-long.176.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nShaolei Zhang and Yang Feng. Reducing position bias in simultaneous machine translation with In Proceedings of the 60th Annual Meeting of the Association for length-aware framework. Computational Linguistics (Volume 1: Long Papers), pp. 6775–6788, Dublin, Ireland, May 2022c. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.467. URL https://aclanthology.org/2022.acl-long.467.\n\nShaolei Zhang, Yang Feng, and Liangyou Li. Future-guided incremental transformer for simultaneous translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14428–14436, 2021.\n\nJiawei Zhao, Wei Luo, Boxing Chen, and Andrew Gilman. Mutual-learning improves end-to-end In Proceedings of the 2021 Conference on Empirical Methods in Natural speech translation. Language Processing, pp. 3989–3994, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.325. URL https://aclanthology.org/2021.emnlp-main.325.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Pseudocode of FINE-Wait inference strategy in a PyTorch-like style.\n\n# model: an offline-trained ST model consists of a acoustic encoder Wav2vec2.0, a token\n\nboundary detector, a semantic encoder, and a decoder # m: mask length, K: wait lagging, audio: audio waveform\n\nN = 0 # the number of source text tokens x = [] # streaming audio prefix y = [] # translations while y[-1] != \"<eos>\":\n\nif x == audio: # audio has been read\n\ny = y + model(a,y) # write new target token\n\nelif N - len(y) < K: # wait k detected source tokens x = x + read(audio) # incrementally read audio c = model.wav2vec2.cnn(x) # audio tokens if c.shape[0] <= m: # waiting for m audio tokens\n\ncontinue\n\na = model.wav2vec2.encoder(c) # audio representations, (τ + m) a = a[:a.shape[0] - m,:] # discard last m representations, τ\n\n× d\n\n×\n\nd\n\nif model.token_detector(a): # source text token boundary is detected\n\nN += 1\n\nelse:\n\nh = model.semantic_encoder(a) y = y + model.decoder(h, y) # write new target token\n\nA ALGORITHM OF FINE-WAIT\n\nThe pseudo code of FINE-Wait are described in Algorithm 2\n\nB ADDITIONAL EXPERIMENTS\n\nB.1 WHY WE USE AL RATHER THAN k?\n\nIn our presented results, we plot the BLEU v.s. AL rather than k. We argue that k is not a fair metric to evaluate the latency. In text streaming translation, different tokenization (e.g., different number of BPE operations) will lead to different token boundaries for the same sentence. It indicates the k tokens do not necessarily represent the same partial sentence for different BPE methods. This situation becomes even severer for speech streaming translation. As we have a source text token boundary detector in our model, the first k detected text tokens will represent different lengths of audio frames for different input audios. To be precise, the wait-k policy used in our streaming speech translation is actually wait-k detected tokens policy. Therefore, we prefer to use AL rather than k as the latency metric in our experiments.\n\nB.2 WHY WE USED OUR IMPLEMENTED MOSST?\n\nFor streaming speech translation, when the allowed AL is increasing, the performance of streaming ST will gradually converge to the offline model. However, in the original implementation of MoSST, there is still a nonnegligible gap between the SST and offline ST for the large AL. We re-implement the MoSST inference method and build our inference strategies on top of the improved MoSST inference. The difference of the inference performance between the original MoSST and our improved MoSST can be seen from Figure 9. We can see the inference BLEU of our improved MoSST can approximate the offline model as the AL increases. The detailed implementation can refer to the submitted code.\n\n24\n\n20\n\n16\n\n12\n\n8\n\nU E\nL B\n\noffline (greedy)\n\nimporved-MoSST\n\nMoSST-reproduce\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\nAverage Lagging (ms)\n\nFigure 9: Difference between original and improved MoSST.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Performance (BLEU) on different monotonic levels on test set of MuST-C EnDe.\n\nMonotonic Level\n\n# example\n\nOffline (greedy) MoSST FINE-Wait FINE-Mask\n\nEasy\n\n668\n\n29.36 24.29 26.61+2.32 26.03+1.74\n\nMedium\n\n1013\n\n23.32 15.32 19.36+4.04 19.35+4.03\n\nHard\n\n895\n\n22.38 9.22 17.24+8.02 17.40+8.18\n\nAL\n\n-\n\n- 1295 1251 1143\n\n(a) EnDe\n\n(b) EnEs\n\n(c) EnFr\n\nFigure 10: The source-to-target alignment position shift on MuST-C EnDe, EnEs, and EnFr tstCOMMON set.\n\nB.3 WHAT EXAMPLES ARE IMPROVED BY OUR STRATEGIES?\n\nFor tst-COMMON on MuST-C EnDe, we use fast-align9 to identify the token-level alignment between source transcription and target translation following Zhang & Feng (2022c). First, we define , where the ith source token is aligned the source-to-target alignment position shift as max to the jth target token. If i j is large, it means in order to translate the jth target token, the model may need to read more until seeing the ith source token. Then we calculate the monotonic level of each example as the averaged alignment position shift over the number of aligned tokens, i.e.,\n\n0, i\n\n−\n\n−\n\n}\n\n{\n\nj\n\nmonotonic level =\n\n1 aligned pairs |\n|\n\n(cid:88)\n\n(i,j)\n\n∈\n\naligned pairs\n\nmax\n\n{\n\n0, i\n\nj\n\n}\n\n−\n\n(6)\n\nWe divide the test set into 3 groups according to different monotonic levels: easy (= 0), medium (< 3) and hard ( 3). For each group, we evaluate different inference methods and report the results in Table 1. As we explained in B.1, it is almost impossible to guarantee the same AL for different inference methods. For a fair comparison, we try our best to set the AL of different methods to be approximately equal. We can see our inference strategies show a significant advantage on the non-monotonic examples (hard group).\n\n≥\n\nB.4 HOW IMPORTANT OF THE WAV2VEC2.0?\n\nAs we mentioned in the main text, the special audio token “mask” in Wav2Vec2.0 is pre-trained on the Librispeech dataset to reconstruct the corresponding feature conditional on unmasked context via the contrastive task. In our experiments, we didn’t include contrastive learning as the auxiliary task in the downstream ST training. And in our FINE-Mask inference, we directly leverage the mask embeddings as the future context by appending them to the streaming input. However, we found the speech representations after ST training becomes even better. Particularly, we calculate the cosine similarity between every predicted future representation and full speech representations at the same position, and the results are illustrated in Figure 11. On either the Librispeech or the MuST-C audio test set, the fine-tuned Wav2Vec2.0 can produce better speech representations from the masking inputs.\n\n9https://github.com/clab/fast_align\n\n16\n\n5((cid:3)(cid:4)(cid:1)(cid:5)6((cid:3)(cid:7)(cid:4)(cid:5)(cid:8)(cid:6) (cid:2)(cid:3)(cid:6)(cid:9)(cid:5)3((cid:4)(cid:11)(cid:11)(cid:5)4((cid:4)(cid:3)(cid:3)(cid:5)2((cid:12)(cid:12)(cid:11)(cid:5)1((cid:1)(cid:6)(cid:1)(cid:5)0((cid:6)(cid:6)(cid:11)(cid:5)(cid:1)(cid:2)(cid:3)(cid:4)(cid:4)(cid:2)(cid:5)(cid:22)(cid:11)(cid:4)(cid:7)(cid:8)(cid:5)(cid:23)(cid:11)(cid:4)(cid:9)(cid:6)(cid:5)(cid:21)(cid:11)(cid:2)(cid:6)(cid:9)(cid:5)1(cid:11)(cid:11)(cid:8)(cid:9)(cid:5)(cid:19)(cid:11)(cid:7)(cid:4)(cid:9)(cid:5)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:3)(cid:2)(cid:8)(cid:5)(cid:7)(cid:4)(cid:3)(cid:8)(cid:6)(cid:8)(cid:7)(cid:9)(cid:3)(cid:4)(cid:10)(cid:11)(cid:10)(cid:7)Under review as a conference paper at ICLR 2023\n\ny\n\nt i\nr a\nl i\n\nm S\n\ni\n\ne n\n\ni s\no C\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\nfinetuned-w2v2\n\npretrained-w2v2\n\ny\n\nt i\nr a\nl i\n\nm S\n\ni\n\ne n\n\ni s\no C\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\nfinetuned-w2v2\n\npretrained-w2v2\n\n10\n\n−\n\n0\n\n10\n\n20\n\n30\n\n40\n\nDistance from consumed speech\n\n(a) MuST-C\n\n10\n\n−\n\n0\n\n10\n\n20\n\n30\n\n40\n\nDistance from consumed speech (b) LibriSpeech\n\nFigure 11: We measure the accuracy of predicted context by calculating the cosine similarity between every predicted future representation and full speech representations at the same position.\n\nB.5 WHY ARE ALL PREDICTED FEATURES DISCARDED?\n\n24\n\n20\n\n16\n\nU E\nL B\n\nIn FINE-Mask strategy, all the output representations corresponding to the m = 50 masking tokens will be discarded, because we have demonstrated that the representations at the ending positions are inferior. However, as shown in 11, the first 10 predicted representations are not as bad as the next 40. Therefore, on the EnDE test set, we also conduct another streaming ST inference by appending different numbers of predicted context to the original speech representations. We use discard rate p to measure the number of appending features. When p = 1.0, all predicted features are discarded and it reduces to the standard FINE-Mask inference. In Figure 12, we compare the streaming speech translation quality between regular FINE-Mask and its variant. It is concluded that the predicted future context is too noisy and harmful to the performance.\n\nFigure 12: BLEU v.s. AL on different p.\n\nAverage Lagging(ms)\n\np = 1.0\n\np = 0.0\n\np = 0.8\n\np = 0.4\n\n3,000\n\n2,000\n\n1,000\n\n12\n\n8\n\n4\n\nB.6 ADDITIONAL RESULTS ON ENDE/ES AND ENFR\n\nIn this section, we evaluate our inference methods with other latency metrics AP and DAL. The AP-BLEU and DAL-BLEU curves on the MuST-C EnDe, EnEs, and EnFr tst-COMMON sets are shown in Figure 14. For both EnDe and EnEs, our proposed inference strategies can consistently improve the baseline by a large margin.\n\nEnFr In general, we found very limited previous works evaluating on the MuST-C EnFr dataset with BLEU-latency curves. We apply our strategies on the EnFr datasets on different latency metrics and present the results in Figure 13 and Figure 14(e) and 14(f). The FINEMask strategy can achieve significant improvement. Note that the y-axis of EnFr has a larger measuring scale than EnDe or EnEs, so the gap\n\nU E\nL B\n\n30\n\n24\n\n18\n\n12\n\n6\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\nAverage Lagging (ms)\n\nFigure 13: BLEU v.s. AL on the tst-COMMON set of MuST-C EnFr dataset.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n24\n\n20\n\nU E\nL B\n\n16\n\n12\n\n8\n\nU E\nL B\n\n28\n\n24\n\n20\n\n16\n\nU E\nL B\n\n30\n\n24\n\n18\n\n12\n\n6\n\n24\n\n20\n\n16\n\n12\n\n8\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\n5,000\n\nAverage Proportion (a) EnDe\n\nDifferentiable Average Lagging\n\n(b) EnDe\n\n28\n\n24\n\n20\n\n16\n\n12\n\n8\n\n4\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\nAverage Proportion (c) EnEs\n\nDifferentiable Average Lagging\n\n(d) EnEs\n\n30\n\n24\n\n18\n\n12\n\n6\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\noffline (greedy) MoSST ‡\n\nFINE-Wait\n\nFINE-Mask\n\n0.4\n\n0.6\n\n0.8\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\nAverage Proportion (e) EnFr\n\nDifferentiable Average Lagging\n\n(f) EnFr\n\nFigure 14: The translation quality (BLEU) against the latency metrics (AP, DAL) on the tstCOMMON set of MuST-C EnDe, EnEs and EnFr dataset. ‡ denotes that the results are from our improved MoSST.\n\nis not as visible as the other two language pairs, e.g., the absolute BLEU gain on AL (1000ms) from MoSST to FINE-Mask is about 6 BLEUs. We observe that FINE-Wait does not bring as significant improvement over the baseline improved MoSST. We also count the distributions of monotonic levels of EnFr test set as shown in Figure 10(c), and find the majority (94.8%) of the averaged position shift is < 3. Most of the EnFr examples fall into the easy and medium groups. According to the finding in Table 1, it should be common that there is a less significant difference between our strategy and the baseline MoSST.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nB.7 HOW DO FINE-TUNING EPOCHS AFFECT THE FINE-MASK STRATEGY?\n\n}\n\n{\n\n23, 25, 27, 29, 31\n\nFor FINE-Mask strategy, we use a mask token that is well-trained during pre-training of the Wav2Vec 2.0 model to predict future speech features. Now we investigate the performance of different finetuning steps on FINE-Mask strategy. We evaluate the FINE-Mask inference and regular streaming inference (MoSST) at different checkpoints with epoch e = after training converges, as illustrated in Figure 15. Although different checkpoints achieve different performances, FINE-Mask consistently brings improvement (about 6 BLEU at low latency regimes) over the baseline. Moreover, we can observe that the performance gap between different checkpoints in MoSST is larger than that in FINE-Mask. Specif- [1000, 1500], the performance ically, when AL gap between the best and the worst checkpoints in MoSST is 3 BLEU, while the gap in FINE-Mask is only 2 BLEU. All these indicate that the number of fine-tuning steps is not sensitive to the performance improvement of FINE-Mask.\n\n∈\n\nFINE-Mask (e = 23)\n\nFINE-Mask (e = 25)\n\nFINE-Mask (e = 27)\n\nFINE-Mask (e = 29)\n\nFINE-Mask (e = 31)\n\nMoSST (e = 23)\n\nMoSST (e = 25)\n\nMoSST (e = 27)\n\nMoSST (e = 29)\n\nMoSST (e = 31)\n\nU E\nL B\n\n20\n\n18\n\n16\n\n14\n\n12\n\n10\n\n8\n\n1,000\n\n1,500\n\n2,000\n\n2,500\n\n3,000\n\nAverage Lagging (ms)\n\nFigure 15: Performance of different finetuning epochs on FINE-Mask strategy.\n\nB.8 FUTURE WORK: UPPER BOUNDER OF FINE-MASK\n\nIf we infer with FINE-Wait without counting on the latency, the result will be the upper bound of the FINE-Mask, as illustrated in Figure 16. We can observe that the translation quality is further improved at all latency regimes when the accuracy of the predicted contexts becomes fully correct. In particular, the translation quality remains 16 BLEU at a very low latency regime (AL is about only 200ms), exceeding the FINE-Mask by about 10 BLEU. Thus it motivates us to predict the future contexts or representations as accurate as possible and we will explore this direction in the future work.\n\nU E\nL B\n\n24\n\n20\n\n16\n\n12\n\n8 6\n\noffline (greedy)\n\nm = 50 (oracle)\n\nm = 50 (mask)\n\n200 500\n\n1,000\n\n1,500\n\n2,000\n\n2,500\n\nAverage Lagging (ms)\n\nFigure 16: Upper Bound of FINE-Mask.\n\nC DIFFERENCES FROM STREAMING SPEECH RECOGNITION TECHNIQUES\n\nWe briefly describe the differences between our work and other streaming speech processing techniques for other readers from other backgrounds to understand our work. Most streaming recognition techniques still fall into the category to train the model in a streaming manner, e.g., MoChA (Chiu & Raffel, 2018). Similarly, CAAT (Liu et al., 2021) follows the RNN-T (Graves, 2012) method to build a streaming speech translation model. However, what we are researching is completely different from these streaming recognition techniques. We want to reuse the pre-trained offline ST model for streaming inference without further streaming-related training. In general, this is a difficult problem for two reasons. First, the streaming inference takes the partial speech sequence as input, while the offline model always consumes the full speech sequence. Second, the reordering issue prevents partial speech sequence from seeing future information. Our proposed method attempts to relieve such mismatches via the FINE-Mask strategy.\n\nMoreover, the word boundary detector in our model is similar to that in SN model (Wang et al., 2020b) However, there are two significant differences. First, in our pseudo word detector, we use CIF module. For detector architecture, SN and CIF are almost identical with a linear layer and the sigmoid activation. The only difference is the label used. The label for CIF is the total number of words in the transcription. During inference, the CIF module will segment the speech sequence\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\ninto several possibly overlapped segments, whose number should be roughly equal to the number of words if the model is well-trained. However, for SN, it uses actual word-frames alignment information as the labels to learn the real word boundary. So CIF is almost unsupervised or weakly supervised learning, while SN is supervised learning. Second, our methods are mainly used to adapt a single pre-trained offline ST model to streaming inference without further streaming training, and alleviate the input mismatch between offline training and streaming inference. However, for SN, it is trained conditioned on the previous states. It indicates SN is a streaming-trained model with a unidirectional encoder.\n\nIn summary, our method is different from traditional streaming techniques.\n\nD WHY MOSST AS A BASE MODEL\n\nMoSST (Dong et al., 2022) consists of an acoustic encoder, a monotonic segmentation module (i.e., CIF module), a semantic encoder, and a translation decoder. First, the acoustic encoder is pre-trained Wav2Vec2.0. The mask speech embedding is well-trained and is helpful for our FINE-Mask strategy. Moreover, the CIF module can be trained to roughly segment the speech sequence into words level with very weak signal (the total number of words). After training, the number of resulted segments is roughly equal to the number of words. So it can be used as a pseudo word detector for streaming inference. Thus, we use MoSST as our base model. Details of Offline Training The offline ST model is first trained for about 20 epochs by a multi-task learning, including ASR and ST tasks. A language identity tag is prepended to the target sentence for indicating which task is learned. In this stage, the CIF module which is used for monotonic segmentation is deactivated, in other words, the CIF module is not trained. The main purpose is to learn a better decoder, i.e., a well-trained language model. Then, we activate the CIF module such that its parameters are trainable, and continue to train for another several epochs. In this stage, only the ST task is learned. Usually, it will only take 5 or 6 epochs to converge.\n\nE NUMERIC RESULTS FOR THE FIGURES\n\nModel\n\nMoSST\n\nFINE-Mask\n\nFINE-Wait\n\nTable 2: Numeric results for Figure 5, 13, 14.\n\nEnDe\n\nEnEs\n\nEnFr\n\nAL\n\nAP\n\nDAL\n\nBLEU\n\nAL\n\nAP\n\nDAL\n\nBLEU\n\nAL\n\nAP\n\nDAL\n\nBLEU\n\n867 1295 1939 2505 3312 4410\n\n796 1143 1534 2109 2647 3404 4457\n\n1045 1453 1857 2414 2926 3628 4590\n\n0.51 0.65 0.78 0.85 0.92 0.97\n\n0.63 0.70 0.76 0.83 0.88 0.93 0.97\n\n0.68 0.75 0.81 0.86 0.90 0.94 0.98\n\n1032 1531 2234 2788 3559 4576\n\n1223 1559 1928 2476 2974 3678 4625\n\n1455 1849 2227 2750 3220 3875 4737\n\n7.79 13.31 18.72 20.67 22.33 23.16\n\n16.10 19.19 21.15 22.23 23.15 23.65 23.42\n\n18.43 20.39 21.43 22.45 22.91 23.33 23.39\n\n802 1327 1832 2571 3188 4047\n\n740 1101 1532 1990 2647 3228 4017\n\n965 1409 1895 2357 2975 3516 4232\n\n0.38 0.64 0.76 0.86 0.91 0.95\n\n0.57 0.68 0.77 0.82 0.88 0.92 0.96\n\n0.65 0.74 0.81 0.86 0.91 0.93 0.96\n\n20\n\n981 1606 2189 2944 3543 4327\n\n1123 1499 1954 2404 3035 3568 4287\n\n1368 1808 2291 2734 3323 3815 4466\n\n3.54 16.99 23.30 26.30 27.40 27.58\n\n17.15 21.15 25.43 27.24 27.45 27.44 27.60\n\n20.40 24.15 26.45 27.19 27.49 27.57 27.95\n\n743 1376 1886 2532 3083 3811\n\n414 1009 1499 1937 2554 3087 3797\n\n841 1390 1892 2314 2884 3374 4017\n\n0.4 0.66 0.78 0.87 0.91 0.95\n\n0.4 0.59 0.72 0.8 0.87 0.92 0.95\n\n0.49 0.68 0.79 0.85 0.9 0.93 0.96\n\n866 1576 2150 2798 3341 4017\n\n593 1205 1724 2200 2821 3335 3996\n\n957 1558 2116 2563 3119 3594 4197\n\n4.59 21.07 28.59 31.47 32.54 33.08\n\n6.81 17.88 26.69 30.89 32.78 33.41 33.70\n\n10.07 22.93 28.83 31.26 32.70 33.20 33.57\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Numeric results for Figure 5, 6. The results of MU-ST are obtained from (Zhang et al., 2022). The results of SimulSpeech and RealTrans are obtained from (Zeng et al., 2021).\n\nMU-ST\n\nAL 1023 BLEU 17.94\n\n1424 20.85\n\n1953 22.78\n\n2642 24.3\n\n3621 24.82\n\n4453 24.99\n\n5089 25.05\n\n5754 25.9\n\nEnDe\n\nRealTrans\n\n1355 AL BLEU 16.54\n\n1838 18.49\n\n2290 19.84\n\n2720 20.05\n\n3106 20.41\n\n- -\n\nFINE-Hybrid\n\n1056 AL BLEU 18.66\n\n1396 20.94\n\n1778 22.26\n\n2328 22.76\n\n2840 23.29\n\n3557 23.60\n\nSimulSpeech\n\nAL BLEU 15.02\n\n694\n\n1336 19.92\n\n2169 21.58\n\n2724 22.42\n\n3331 22.49\n\nEnEs\n\nRealTrans\n\nAL 1047 BLEU 18.54\n\n1554 22.74\n\n2043 24.89\n\n2514 25.54\n\n2920 25.97\n\nFINE-Hybrid\n\n- -\n\n- -\n\nAL 1108 BLEU 22.53\n\n1468 24.14\n\n1779 26.62\n\n2234 27.35\n\n2854 27.37\n\n3399 27.70\n\n- -\n\n- -\n\n- -\n\n- -\n\n- -\n\n- -\n\n- -\n\n- -\n\n- -\n\n- -\n\nTable 4: Numeric results for Figure 7.\n\nFINE-Mask\n\nFINE-Wait\n\nm AL\n\nBLEU m AL\n\nBLEU m AL\n\nBLEU m AL\n\nBLEU\n\n0\n\n5\n\n10\n\n20\n\n178 483 867\n\n198 577 977\n\n324 706 1088\n\n409 783 1144\n\n0.10 1.68 7.79\n\n1.66 5.98 12.58\n\n6.39 11.29 16.61\n\n10.99 14.61 18.36\n\n30\n\n50\n\n60\n\n70\n\n442 785 1146\n\n475 796 1143\n\n473 811 1152\n\n494 801 1148\n\n12.38 16.06 18.83\n\n12.65 16.10 19.19\n\n12.65 16.23 19.09\n\n12.33 16.28 18.77\n\n0\n\n5\n\n10\n\n15\n\n178 483 867\n\n484 891 1295\n\n641 1045 1453\n\n768 1166 1575\n\n0.10 1.68 7.79\n\n10.88 16.19 18.87\n\n14.59 18.43 20.39\n\n16.11 19.31 20.67\n\n20\n\n30\n\n40\n\n50\n\n882 1286 1686\n\n1098 1504 1890\n\n1316 1710 2082\n\n1511 1893 2253\n\n17.00 19.79 21.22\n\n17.99 20.18 21.59\n\n18.06 20.48 21.81\n\n18.62 20.92 21.95\n\nWe also provide the numeric results for Figures 5, 13, and 14 in Tables 2, for Figure 7 in Table 3, and for Figures 5 and 6 in Table 4.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper addresses the differences between the training and inference conditions of streaming models. They first present a thorough analysis of the problem by finding correlation between representations during offline and streaming modes and plot that against the model performance in terms of BLEU score. They show that the deterioration happens from the last representations of the input. \n\nIn order to fix this problem they propose a simple strategy during inference to match the conditions that wav2vec 2.0 based models are trained towards. They propose two strategies FINE-Wait and FINE-Mask and also discuss a combination of it called FINE-Hybrid. These techniques essentially create a pseudo longer context using mask tokens (matching the wav2vec 2.0 training criterion) or using actual speech tokens for better contextualization and then dropping it during inference. \n\nOverall, the technical contribution is low but the idea is neat and simple with a good motivational analysis. The drawback is that it's quite constrained towards models trained on wav2vec 2.0 criterion.\n\n# Strength And Weaknesses\n\n**Strengths**\n1) Strong motivation with supported analysis for the motivation and a simple solution towards this problem. \n2) The paper is well written and clear to understand. \n\n**Weaknesses**\n1) It seems like the approach is limited to wav2vec type models from its motivation. But in my opinion this approach should work for any kind of models, specially the FINE-Wait. A study or discussion on this would have made the paper stronger. \n2) The paper is also missing details/analysis on how does number of fine-tuning steps affects the FINE-Mask strategy. If the model is reliant on the MASK tokens that are used during self-supervised pre-training I would be curious to learn if amount of fine-tuning causes a change in the performance.\n3) Similarly, the approach could have been stronger if the authors also studied how these model training could be modified to match the inference conditions. For example, maybe providing future biased masking during fine-tuning would allow the models to match the inference conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1) **Clarity and Quality** : Very Clear and Well Presented \n2) **Novelty** : Low, yet a simple and effective approach\n3) **Reproducibility** : Low, no details on the model was trained\n\n# Summary Of The Review\n\nI think the paper is well motivated, simple yet strong, if the authors are able to comment on the weaknesses I pointed out, this paper can be quite insightful.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMOLEBM: MOLECULE GENERATION AND DESIGN BY LATENT SPACE ENERGY-BASED MODELING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGeneration of molecules with desired chemical and biological properties such as high drug-likeness, high binding affinity to target proteins, is critical in drug discovery. In this paper, we propose a probabilistic generative model to capture the joint distribution of molecules and their properties. Our model assumes an energybased model (EBM) in the latent space. Given the latent vector sampled from the latent space EBM, both molecules and molecular properties are conditionally sampled via a molecule generator model and a property regression model respectively. The EBM in a low dimensional latent space allows our model to capture complex chemical rules implicitly but efficiently and effectively. Due to the joint modeling with chemical properties, molecule design can be conveniently and naturally achieved by conditional sampling from our learned model given desired properties, in both single-objective and multi-objective optimization settings. The latent space EBM, molecule generator, and property regression model are learned jointly by approximate maximum likelihood, while optimization of properties is accomplished by gradual shifting of the model distribution towards the region supported by molecules with high property values. Our experiments show that our model outperforms state-of-the-art models on various molecule design tasks.\n\n1\n\nINTRODUCTION\n\nIn drug discovery, it is of vital importance to find or design molecules with desired pharmacologic or chemical properties such as high drug-likeness and binding affinity to a target protein. It is challenging to directly optimize or search over the drug-like molecule space since it is discrete and enormous, with an estimated size is on the order of 1033 (Polishchuk et al., 2013).\n\nRecently, a large body of work attempts to tackle this problem. The first line of work leverages deep generative models to map the discrete molecule space to a continuous latent space, and optimizes molecular properties in the latent space with methods like Bayesian optimization (G ́omezBombarelli et al., 2018; Kusner et al., 2017; Jin et al., 2018). The second line of work recruits reinforcement learning algorithms to optimize properties in the molecular graph space directly (You et al., 2018; De Cao & Kipf, 2018; Zhou et al., 2019; Shi et al., 2020; Luo et al., 2021). A number of other efforts have been made to optimize molecular properties with genetic algorithms (Nigam et al., 2020), particle-swarm algorithms (Winter et al., 2019) specialized MCMC methods (Xie et al., 2021).\n\nIn this work, we propose a method along the first line mentioned above, by learning a probabilistic latent generative model of molecule distributions and optimizing chemical properties in the latent space. Given the central role of latent variables in this approach, we emphasize that it is critical to learn a latent space model that captures the data regularities of the molecules. Thus, instead of assuming a simple Gaussian distribution in the latent space as in prior work (G ́omez-Bombarelli et al., 2018; Jin et al., 2018), we assume a flexible and expressive energy-based model (EBM) (LeCun et al., 2006; Ngiam et al., 2011; Kim & Bengio, 2016; Xie et al., 2016; Kumar et al., 2019; Nijkamp et al., 2019; Du & Mordatch, 2019; Grathwohl et al., 2019; Finn et al., 2016) in latent space. This leads to a latent space energy-based model (LSEBM) as studied in Pang et al. (2020); Nie et al. (2021), where LSEBM has been shown to model the distributions of natural images and text well. For molecule modeling, without any explicit validity constraints in generation, our model generates molecules with high validity with simple SMILES representation (Weininger, 1988).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nGiven our goal of property optimization, we learn a joint distribution of molecules and their properties. Our model consists of 1) an EBM in a low-dimensional continuous latent space, 2) a generator mapping from the latent space to the observed molecule space, and 3) a property regression model mapping from the latent space to the property values (see Figure 1). We call our model as MolEBM. All three components in our model are learned jointly by approximate maximum likelihood. A learned model generates a molecule with a high property value in two steps: 1) given the property value, sample the latent vector; 2) given the sampled latent vector, generate a molecule (see the topto-bottom path in Figure 1a). Since the learned model approximates the data distribution, directly sampling from the learned model conditional on a high property value does not work well since a molecule with a high property value is most likely not in the original data distribution. We thus design a method to gradually shift the learned distribution towards the region supported by molecules with high property values, and sample molecules with desirable properties from the shifted distribution.\n\nIn drug discovery, most often we need to consider multiple properties simultaneously. Our model can be extended to this setting straightforwardly. With our method, we only need to add a regression model for each property, while the learning and sampling methods remain the same. Learning the model involves inferring the latent vector given both the molecule and the property value, and we recruits Langevin dynamics instead of amortized inference network for inference computation. This design makes our approach versatile in dealing with varying number of properties.\n\nWe evaluate our method in various settings including single-objective optimization and multiobjective optimization. Our method outperforms prior methods by significant margins.\n\nIn summary, our contributions are as follows:\n\n• We propose to learn a latent space energy-based model for the joint distribution of molecules and\n\nmolecular properties.\n\n• We develop a sampling with gradual distribution shifting method, enabling us to extrapolate the data distribution and sample from the region supported by molecules with high property values.\n\n• Our methods are versatile enough to be extended to optimizing multiple properties together.\n\n• Our model achieves state-of-the-art performances on a wide range of molecule optimization tasks.\n\n2 RELATED WORK\n\nOptimization with Generative Models. Deep generative models approximate the distribution of molecules with desired biological or non-biological properties. Existing approaches for generating molecules include applying variational autoencoder (VAE) (Kingma & Welling, 2014) and generative adversarial network (GAN) (Goodfellow et al., 2014) etc. to molecule data (G ́omez-Bombarelli et al., 2018; Jin et al., 2018; De Cao & Kipf, 2018; Honda et al., 2019; Madhawa et al., 2019; Shi et al., 2020; Zang & Wang, 2020; Kotsias et al., 2020; Chen et al., 2021; Fu et al., 2020; Liu et al., 2021; Bagal et al., 2021; Eckmann et al., 2022; Segler et al., 2018). After learning continuous representations for molecules, they are further able to optimize using different methods. (Segler et al., 2018) proposes to optimize by simulating design-synthesis-test cycles. (G ́omez-Bombarelli et al., 2018; Jin et al., 2018; Eckmann et al., 2022) propose to learn a surrogate function to predict properties, and then use Bayesian optimization to optimize the latent vectors. However, the performance of this latent optimization is not satisfactory due to three major issues. First, it is difficult to train an accurate surrogate predictor especially for those novel molecules with high properties along the design trajectories. Second, as the learned latent space tries to cover the fixed data space, its ability to explore the targets out of the distribution is limited (Brown et al., 2019; Huang et al., 2021). Third, those methods are heavily dependent on the quality of learned latent space, which requires non-trivial efforts to design encoders when dealing with multiple properties. To address above issues, (Eckmann et al., 2022) use VAE to learn the latent space and train predictors separately using generated molecules, and then leverage latent inceptionism, which involves the decoder solely, to optimize the latent vector with multiple predictors. In this paper, we propose an encoder-free model in both training and optimization to learn the joint distribution of molecules and properties, and make it possible to obtain several adequate predictors. We then design an efficient algorithm to shift the learned distribution iteratively.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nControlled\n\nGeneration\n\npα(z)\n\npγ(y|z)\n\npβ(x|z)\n\ny\n\nz\n\nx\n\ny1\n\n· · ·\n\nym\n\npγ1 (y1|z)\n\npγm (ym|z)\n\npα(z)\n\npβ (x|z)\n\nz\n\nx\n\n(a) Single-Objective Optimization.\n\n(b) Multi-Objective Optimization.\n\nFigure 1: An illustration of MolEBM. x represents a molecule, z is the latent vector, y is a molecular property of interest, {yj}m\n\nj=1 indicates m properties.\n\nOptimization with Reinforcement Learning and Evolutionary Algorithms. Reinforcement learning (RL) based methods directly optimize and generate molecules in an explicit data space (You et al., 2018; Zhou et al., 2019; Jin et al., 2020; Gottipati et al., 2020). By formulating the property design as a discrete optimization task, they can modify the molecular substructures guided by an oracle reward function. However, the training of those RL-based methods can be viewed as rejection sampling which is difficult and inefficient due to the random-walk search behavior in the discrete space. Evolutionary algorithms (EA) also formulate the optimization in a discrete manner (Nigam et al., 2020; Jensen, 2019; Xie et al., 2021; Fu et al., 2021a;b). By leveraging carefully-crafted combinatorial algorithms, they can search the molecule graph space in a flexible and efficient way. However, the design of those algorithms is non-trivial and domain specific.\n\n3 METHODS\n\n3.1 PROBLEM SETUP AND OVERVIEW\n\nWe use the SELFIES representation for molecules (Krenn et al., 2020). It encodes each molecule as a string of characters and ensures validity of all SELFIES strings. Let x = (x(1), ..., x(t), ..., x(T )) be a molecule string encoded in SELFIES, where x(t) ∈ V is the t-th character and V is the vocabulary. Suppose y ∈ R represents a molecular property of interest. Then the problem we attempt to tackle is to optimize x such that its property y = y∗ where y∗ is some desirable value for y. We take a probabilistic approach and treat the optimization problem as a sampling problem, that is,\n\nx∗ ∼ p(x|y = y∗).\n\n(1)\n\nThis is a single-objective optimization problem since only one property is targeted. In real-world drug design settings, we are more likely to need to optimize multiple properties simultaneously, that is, multi-objective optimization. Suppose we optimize for {yj ∈ R}m j=1, then our task is to sample,\n\nx∗ ∼ p(x|y1 = y∗\n\n1, ..., ym = y∗\n\nm).\n\n(2)\n\nTo address these problems, we propose a solution under a unified probabilistic framework. As a first step, we need to model the data distribution of molecules, pdata(x). To this end, we recruit latent space energy-based model (Pang et al., 2020; Nie et al., 2021) for its expressiveness. LSEBM assumes latent vector z ∈ Rd in a low dimensional latent space follows an energy-based model, while the observed x is generated by a generator conditional z, that is, p(x, z) = p(x|z)p(z). We introduce LSEBM within the context of molecule data in §3.2.\n\nFor the purpose of property optimization, we propose to model the joint distribution of molecules and molecular properties (§3.3). We first consider the single-objective optimization problem (eq. 1). Given the expressiveness of LSEBM, we assume that the latent vector z captures data regularities in x (evaluated in §4.2 and §A.4). Thus, we can learn a simple regression model from the lowdimensional latent space, that is, p(y|z), and the joint distribution is p(x, y, z) = p(z)p(x|z)p(y|z). The model is learned with maximum likelihood (see §3.4 and Algorithm 1).\n\nWith the learned model, we can optimize x given y = y∗ by ancestral sampling z∗ ∼ p(z|y = y∗) and x∗ ∼ p(x|z = z∗). However, if y∗ deviates from the observed data distribution of y, this naive\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nsolution involves sampling in an extrapolated regime (or out of distribution regime) where y∗ is not in the effective support of the learned distribution. We propose a Sampling with Gradual Distribution Shifting (SGDS) approach where we 1) sample on the boundary of the effective support of the learned distribution, and 2) gradually shift the learned distribution with these boundary samples to a region where it is supported by high property values (see §3.5 and Algorithm 2).\n\nOur model is designed to be versatile such that it admits straightforward extension to multi-objective optimization. To optimize x given {yj = y∗ j=1, we can simply augment the joint distribution with more regression models, i.e., p(x, z, y1, ..., ym) = p(z)p(x|z) (cid:81)m j=1 p(yj|z). The sampling procedure follows the same SGDS approach. See §3.6 for more details on multi-objective optimization.\n\nj }m\n\n3.2 LATENT SPACE ENERGY-BASED MODEL\n\nSuppose x = (x(1), ..., x(t), ..., x(T )) is a molecule string in SELFIES and z ∈ Rd is the latent vector. Consider the following model,\n\n(3) where pα(z) is a prior model with parameters α, and pβ(x|z) is a generation model with parameters β. In VAE (Kingma & Welling, 2014), the prior is simply assumed to be an isotropic Gaussian distribution. In our model, pα(z) is formulated as an energy-based model,\n\nx ∼ pβ(x|z),\n\nz ∼ pα(z),\n\npα(z) =\n\n1 Z(α)\n\nexp(fα(z))p0(z),\n\n(4)\n\nwhere p0(z) is a reference distribution, assumed to be isotropic Gaussian as in VAE. fα : Rd → R is the scalar-valued negative energy function and is parameterized by a small multi-layer perceptron (MLP) with parameters α. Z(α) = (cid:82) exp(fα(z))p0(z)dz = Ep0[exp(fα(z))] is the partition function.\n\nThe generation model pβ(x|z) is a conditional autoregressive model,\n\npβ(x|z) =\n\nT (cid:89)\n\nt=1\n\npβ(x(t)|x(1), ..., x(t−1), z)\n\n(5)\n\nwhich is parameterized by a one-layer LSTM with parameters β. Note that the latent vector z controls every step of the autoregressive model.\n\n3.3\n\nJOINT DISTRIBUTION OF MOLECULE AND MOLECULAR PROPERTY\n\nGiven a molecule x, suppose y is the chemical property of interest, such as QED or protein affinity binding. The property value can be computed from an input x via open-sourced software RDKit (Landrum et al., 2013) or AutoDock-GPU (Santos-Martins et al., 2021). We assume that given z, x and y are conditionally independent.\n\npθ(x, y, z) = pα(z)pβ(x|z)pγ(y|z), (6) where pα(z) is the EBM prior, pβ(x|z) is the generation model, and pγ(y|z) is the property regression model, and θ = (α, β, γ). We use the model pθ(x, y, z) to approximate the data distribution of (x, y). See Appendix §A.1 for details.\n\nThe property regression model can be written as\n\npγ(y|z) =\n\n√\n\n1\n\n2πσ2\n\n(cid:18)\n\nexp\n\n−\n\n1\n\n2σ2 (y − sγ(z))2\n\n(cid:19)\n\n,\n\n(7)\n\nwhere sγ(z) is a small MLP, with parameters γ, predicting y based on the latent z. The variance σ2 is set as a constant or hyperparameter in our work.\n\n3.4 LEARNING ALGORITHM\n\nSuppose we observe training examples {(xi, yi), i = 1, ..., n}. The log-likelihood function is L(θ) = (cid:80)n\n\ni=1 log pθ(xi, yi). The learning gradient can be calculated according to ∇θ log pθ(x, y) = Epθ(z|x,y) [∇θ log pθ(x, y, z)]\n\n(8)\n\n(9)\n\n= Epθ(z|x,y) [∇θ(log pα(z) + log pβ(x|z) + log pγ(y|z))] .\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFor the prior model, ∇α log pα(z) = ∇αfα(z)−Epα(z)[∇αfα(z)]. Thus the learning gradient given an example (x, y) is\n\nδα(x, y) = ∇α log pθ(x, y) = Epθ(z|x,y)[∇αfα(z)] − Epα(z)[∇αfα(z)].\n\n(10)\n\nα is updated based on the difference between z inferred from empirical observation (x, y), and z sampled from the current prior. For the generation model,\n\nδβ(x, y) = ∇β log pθ(x, y) = Epθ(z|x,y)[∇β log pβ(x|z)].\n\nSimilarly, for the regression model,\n\nδγ(x, y) = ∇γ log pθ(x, y) = Epθ(z|x,y)[∇γ log pγ(y|z)].\n\n(11)\n\n(12)\n\nEstimating expectations in equations 17, 18, and 19 requires MCMC sampling of the prior model pα(z) and the posterior distribution pθ(z|x, y). We recruit Langevin dynamics (Neal, 2011). For a target distribution π(z), the dynamics iterates\n\nzk+1 = zk + s∇z log π(zk) +\n\n√\n\n2sεk,\n\n(13)\n\nwhere k indexes the time step of the Langevin dynamics, s is a small step size, and εk ∼ N (0, Id) is the Gaussian white noise. π(z) can be either pα(z) or pθ(z|x, y). In either case, ∇z log π(z) can be efficiently computed by back-propagation. See Appendix §A.1 for more details.\n\nAlgorithm 1: Learning MolEBM.\n\ninput : Learning iterations T , learning rates for the prior, generation, and regression model {η0, η1, η2}, i=1, batch size m, number of\n\ninitial parameters θ0 = (α0, β0, γ0), observed examples {(xi, yi)}n prior and posterior sampling steps {K0, K1}, and prior and posterior sampling step sizes {s0, s1}.\n\noutput: θT = (αT , βT , γT ). for t = 0 : T − 1 do\n\n1. Mini-batch: Sample observed examples {(xi, yi)}m 2. Prior sampling: For each i, sample z− π(z) = pαt (z), and s = s0, K = K0.\n\n3. Posterior sampling: For each (xi, yi), sample z+\n\ni=1.\n\ni ∼ pαt (z) using equation (13), where the target distribution\n\ni ∼ pθt (z|xi, yi) using equation (13), where the\n\ntarget distribution π(z) = pθt (z|xi, yi), and s = s1, K = K1.\n\n4. Update prior model: αt+1 = αt + η0 5. Update generation model: βt+1 = βt + η1 6. Update regression model: γt+1 = γt + η2\n\n1 m\n\n(cid:80)m\n\ni=1[∇αfαt (z+\n\ni ) − ∇αfαt (z−\n\ni )].\n\n1 m\n1 m\n\n(cid:80)m (cid:80)m\n\ni=1 ∇β log pβt (xi|z+ i ). i=1 ∇γ log pγt (yi|z+ i ).\n\n3.5 SAMPLING WITH GRADUAL DISTRIBUTION SHIFTING\n\nTo tackle the single-objective optimization problem (eq. 1), one naive approach is to perform ancestral sampling with two steps, given some desirable property value y∗,\n\n(1) z∗ ∼ pθ(z|y = y∗) ∝ pα(z)pγ(y = y∗|z)\n\n(2) x∗ ∼ pβ(x|z = z∗),\n\n(14)\n\nwhere (1) is an application of Bayes’ rule, with pα(z) as the prior and pγ(y|z) as the likelihood.\n\nOur model pθ(x, y, z) is learned to capture the data distribution. In real-world settings, y∗ might not be within the support of the data distribution. Therefore, sampling following equation 14 does not work well since it involves extrapolating the learned distribution. We propose a method called sampling with gradual distribution shifting (SGDS) to address this issue. In particular, we find examples from the training data on the boundary of the distribution support by sorting them according to the values of y and taking top-k samples, {(x(old) i=1, with high property values. We shift the support slightly by adding some small ∆y to all y’s, and sample x conditional on shifted y’s, following equation 14. Given the generated x’s, we compute their groundtruth y’s with RDKit or AutoDock-GPU. We then shift the learned model by finetuning it with the new data, {(x(new) i=1, for a few steps (e.g., 10). This completes one iteration of distribution shift. The molecule with desired high property value is sampled after T shifting iterations. We summarize the algorithm in Algorithm 2.\n\n, y(new)\n\n, y(old)\n\n)}k\n\n)}k\n\ni\n\ni\n\ni\n\ni\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2: Sampling with Gradual Distribution Shifting (SGDS). input : Shift iterations T , initial parameters θ0 = (α0, β0, γ0), initial examples {(x0\n\ni , y0\n\ni )}k\n\ni=1 from the\n\ndata distribution boundary, shift magnitude ∆y, PropertyComputeEngine = RDKit or AutoDock-GPU, LearningAlgorithm = Algorithm 1.\n\ni )}k output: {(xT for t = 0 : T − 1 do\n\ni , yT\n\ni=1.\n\ni = yt\n\ni , ̃yt+1 1. Property shift: For each yt 2. Latent sampling: For each ̃yt+1 3. Molecule sampling: For each zt+1 4. Property computation: For each xt+1 5. Distribution shift: θt+1 = LearningAlgorithm({(xt+1\n\ni + ∆y. , sample zt+1\n\n, compute yt+1\n\n, sample xt+1\n\ni ∼ pθt (z| ̃yt+1\n\n). i ∼ pθt (x|zt+1\n\n, yt+1\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n).\n\ni = PropertyComputeEngine(xt+1\n\ni\n\n).\n\n)}k\n\ni=1, θt).\n\nFor constrained optimization, in step 3, we only keep the sampled molecules that satisfy the given constraints.\n\n3.6 MULTI-OBJECTIVE OPTIMIZATION\n\nWe next consider the multi-objective optimization problem. Suppose we optimize for a set of properties {yj}m\n\nj=1, then we learn a property regression model for each property yj,\n\npγj (yj|z) =\n\n(cid:113)\n\n(cid:32)\n\nexp\n\n−\n\n1 2σ2 j\n\n1\n\n2πσ2 j\n\n(cid:33)\n\n(yj − sγj (z))2\n\n,\n\nwhere each sγj is a small MLP with parameters γj. Then the joint distribution is,\n\npθ(x, z, y1, ..., ym) = pα(z)pβ(x|z)\n\nm (cid:89)\n\nj=1\n\npγj (yi|z).\n\n(15)\n\n(16)\n\nUnder our framework, the learning algorithm and the sampling algorithm for the single-objective problem can be straightforwardly extended to the multi-objective setting. In both settings, the same types of properties are provided in both the initial training stage and the distribution shifting stage.\n\n4 EXPERIMENTS\n\nTo demonstrate the effectiveness of our proposed model, we compare our model with previous SOTA methods for unconditional generation (§4.2) and molecule design including single-objective optimization (§4.3) and multi-objective optimization (§4.4). In molecule design experiments, we consider both non-biological and biological properties.\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets. For the unconditional generation task, we report results on ZINC (Irwin et al., 2012) and MOSES datasets (Polykovskiy et al., 2020). ZINC consists of around 250k molecules, and MOSES comprises around 2 million molecules. For optimization tasks, we conduct experiments with ZINC.\n\nTraining Details. There are three modules in our model, the top-down generation model pβ(x|z), the EBM prior pα(z), and the regression model pγ(y|z). pβ(x|z) is parameterized by a single-layer LSTM with 1024 hidden units. The dimension of latent vector z is 100. pα(z) is a 3-layer MLP with 100 input dimension and 200 hidden units. The property regression model pγ(y|z) is a 3layer MLP with 100 input dimension and 100 hidden units. In multi-objective optimization settings, we recruit one MLP for each property and these MLPs have the same architecture as mentioned above. It is worth mentioning that compared to most previous models, our model is characterized by its simplicity without adding inference networks, RL-related modules, and graph neural networks. Adam (Kingma & Ba, 2015) optimizer is used to train our models with learning rates 0.0001 for EBM and 0.001 for the rest. We train our models for 30 epochs. 10, 000 (non-biological) and 2, 000\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(biological) boundary examples are selected for distribution shifting. The numbers of shift iterations are 50 (non-biological single-objective), 20 (non-biological single-objective) and 10 (multiobjective). Within each SGDS iteration, the model is finetuned with only 10 parameter updates. All experiments are conducted on Nvidia Titan XP GPU. See more details in Appendix §A.6.\n\n4.2 UNCONDITIONAL GENERATION\n\nThree types of encoding systems are used to encode molecules in prior work: SMILES (Weininger, 1988), SELFIES (Krenn et al., 2020), and graph. SMILES and SELFIES linearize a molecule graph into a string of characters. Most previous models using SMILES struggle to generate molecules with high validity, which is the percentage of molecules that satisfy the chemical valency rule. Thus graph representations become popular since explicit valency constraints can be imposed. However, perfect validity in this approach does not imply that the model captures the chemical rules since it is achieved with external constraints. Recently, SELFIES is developed where every SELFIES string corresponds to a valid molecule due to the nature of the encoding system. Thus, validity for generated SMILES strings is a good indicator on how well a learned model captures the basic chemical rules implicitly. Besides validity, we also compare models on uniqueness (the percentage of unique molecules in all generated samples) and novelty (the percentage of generated molecules that are not in the training set).\n\nFollowing previous work, we randomly sample 10k molecules for ZINC and 30k for MOSES, and compare on the three aforementioned metrics. Generations results on ZINC and MOSES are shown in Table 1 and Table 2 respectively. str-smi denotes string-based SMILES representations and str-sfi denotes string-based SELFIES representations. In Table 1, we show generation results for both SMILES and SELFIES. SMILES does not have a validity constraint during generation. However, our model, MolEBM, still achieves 95.5% validity, which outperforms other SMILESbased methods and is also comparable to those with valency check. This result demonstrates that our model can capture those valency rules effectively and implicitly (also see Appendix §A.4.). Samples from our model also achieve perfect uniqueness and novelty.\n\nModel\n\nRepresentation Validity Novelty Uniqueness\n\nJT-VAE (Jin et al., 2018) GCPN (You et al., 2018) GraphNVP (Madhawa et al., 2019) GraphAF (Shi et al., 2020) GraphDF (Luo et al., 2021)\n\nChemVAE (G ́omez-Bombarelli et al., 2018) GrammarVAE (Kusner et al., 2017) SDVAE (Dai et al., 2018) MolEBM MolEBM\n\nGraph Graph Graph Graph Graph\n\nstr-smi str-smi str-smi str-smi str-sfi\n\n1.000⋆ 1.000⋆ 0.426 1.000⋆ 1.000⋆\n\n0.170 0.310 0.435 0.955 1.000\n\n1.000 1.000 1.000 1.000 1.000\n\n0.980 1.000 -\n1.000 1.000\n\n1.000 1.000 0.948 0.991 1.000\n\n0.310 0.108 -\n1.000 1.000\n\nTable 1: Unconditional generation on ZINC. ⋆ denotes valency check.\n\nModel JT-VAE (Jin et al., 2018) GraphAF (Shi et al., 2020) GraphDF (Luo et al., 2021) LIMO (Eckmann et al., 2022) MolEBM\n\nRepresentation Validity Novelty Uniqueness\n\nGraph Graph Graph str-sfi str-sfi\n\n1.000⋆ 1.000⋆ 1.000⋆ 1.000 1.000\n\n0.914 1.000 1.000 1.000 1.000\n\n1.000 0.991 1.000 0.976 1.000\n\nTable 2: Unconditional generation on MOSES. ⋆ denotes valency check. Results obtained from (Polykovskiy et al., 2020; Eckmann et al., 2022).\n\n4.3 SINGLE-OBJECTIVE OPTIMIZATION\n\nNon-Biological Property Optimization. For non-biological properties, we are interested in Penalized logP and QED, both of which can be calculated by RDKit (Landrum et al., 2013). Since we know the Penalized logP scores have a positive relationship with the length of the molecules, we\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Generated molecules with high binding affinities. Left: Top-3 single-objective optimized molecules. The number denotes KD in nmol/L. Right: Top-2 multi-objective optimized molecules. Numbers denote KD in nmol/L, QED and SA respectively.\n\nmaximize Penalized logP either with or without maximum length limit. Following (Eckmann et al., 2022), the maximum length is set to be the maximum length of molecules in ZINC using SELFIES. From Table 9, we can see that with length limit, MolEBM outperforms previous methods by a large margin. Maximizing penalized logP without length limit leads to the highest Penalized logP, 158.0. We also achieve the highest QED with/without length limit. These observations demonstrate the effectiveness of our method. We also illustrate our distribution shifting method in Appendix §A.3.\n\nMethod\n\nLL\n\n✗ JT-VAE ✓\nGCPN ✓\nMolDQN ✗\nMARS ✗\nGraphDF ✓\nLIMO MolEBM ✓ MolEBM ✗\n\nPenalized logP (↑) 3rd 2rd 1st\n\n5.30 7.98 11.8 45.0 13.7 10.5 26.4 158.0\n\n4.93 7.85 11.8 44.3 13.2 9.69 25.1 157.8\n\n4.49 7.80 11.8 43.8 13.2 9.60 24.4 157.5\n\nQED (↑) 2rd\n\n0.911 0.947 0.943 0.948 0.948 0.946 0.948 0.948\n\n3rd\n\n0.910 0.946 0.943 0.948 0.948 0.945 0.948 0.948\n\n1st\n\n0.925 0.948 0.948 0.948 0.948 0.947 0.948 0.948\n\nTable 3: Non-biological single-objective optimization. Report top-3 highest scores found by each model. LL (Length Limit) denotes whether the model has the limit of maximum length. Baseline results obtained from (Eckmann et al., 2022; You et al., 2018; Luo et al., 2021; Xie et al., 2021).\n\nBiological Property Optimization. ESR1 and ACAA1 are two human proteins. We aim to design ligands (molecules) that have the maximum binding affinities towards those target proteins. ESR1 is well-studied, which has many existing binders, while ACAA1 does not. However, we did not use any binder-related information in the design process. Binding affinity is measured by the estimated dissociation constants KD, which can be computed with AutoDock-GPU (Santos-Martins et al., 2021) given a molecule. Large binding affinities corresponds to small KD. That is, we aim to minimize KD. Table 4 shows that our model outperforms previous methods on both ESR1 and ACAA1 binding affinity maximization tasks. Producing those ligands with high binding affinity plays a vital role in the early stage of drug discovery.\n\nMethod\n\nESR1 KD (↓) 2rd\n\n3rd\n\n1st\n\nACAA1 KD (↓) 3rd 2rd 1st\n\n6.4 GCPN 373 MolDQN 25 MARS 17 GraphDF LIMO 0.72 MolEBM 0.52\n\n6.6 588 47 64 0.89 0.54\n\n8.5 1062 51 69 1.4 0.54\n\n75 240 370 163 37 6.71\n\n83 337 520 203 37 6.94\n\n84 608 590 236 41 8.50\n\nTable 4: Biological single-objective optimization. Report top-3 lowest KD (in nanomoles/liter) found by each model. Baseline results obtained from (Eckmann et al., 2022).\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n4.4 MULTI-OBJECTIVE OPTIMIZATION\n\nWe next consider optimizing binding affinity, QED and SAS simultaneously for multi-objective optimization. Following Eckmann et al. (2022), we exclude molecules with abnormal behaviors 1 in the generation process to make sure the learned distribution shifts towards a desirable region in terms of pharmacologic and synthetic properties.\n\nTable 5 shows our multi-objective binding affinity maximization results comparing to LIMO (Eckmann et al., 2022) and GCPN (You et al., 2018). From the results, we can see that MolEBM is able to find the ligands with desired properties while keeping the pharmacologic structures. For ESR1, we have two existing binders on the market, Tamoxifen and Raloxifene. Our designed ligands have similar QED and SA, with rather low KD. Compared to existing methods, MolEBM obtains better results in overall adjustments. For ACAA1, we do not have any existing binders. Compared with prior SOTA methods, our optimized ligands outperform those by a large margin. When comparing ACAA1 multi-objective setting with its corresponding single-objective one, we find multiobjective results even outperforms the single-objective one, which may be counter-intuitive since multi-objective optimization is assumed to be a harder task than single-objective optimization. However, our current results indicate that with proper prior knowledge (e.g. multiple objectives are positively aligned), multi-objective settings could be more plausible in de novo design, and those complicated prior knowledge can indeed be captured by our expressive latent space EBM. While we still need domain expertise to determine the effectiveness of those ligands discovered by MolEBM, we believe our model could shed light on the optimization procedure in the early drug discovery.\n\nLigand\n\nGCPN 1st GCPN 2nd LIMO 1st LIMO 2nd Tamoxifen Raloxifene MolEBM 1st MolEBM 2nd\n\nKD (↓) 810 2.7 × 104 4.6 2.8 87 7.9 × 106 1.71 2.28\n\nESR1 QED (↑) 0.43 0.80 0.43 0.64 0.45 0.32 0.42 0.56\n\nACAA1 SA (↓) KD (↓) QED (↑)\n\n4.2 3.7 4.8 4.9 2.0 2.4 3.85 2.56\n\n8500 8500 28 31 −\n− 5.67 6.60\n\n0.69 0.54 0.57 0.44 −\n− 0.60 0.56\n\nSA (↓) 4.2 4.3 5.5 4.9 −\n− 4.58 4.07\n\nTable 5: Muli-Objective Binding Affinity Maximization for both ESR1 and ACAA1. Report Top-2 average scores related to KD (in nmol/L), QED and SA. Baseline results obtained from (Eckmann et al., 2022).\n\nFigure 2 shows generated molecules with high binding affinities. See Appendix for more examples.\n\n5 CONCLUSION AND DISCUSSION\n\nWe propose a deep generative model, MolEBM, which models the joint distribution of molecules and molecular properties. It assumes an energy-based prior for a low-dimensional continuous latent space, which effectively captures data regularities of the discrete molecule data. We then design a distribution shifting method (SGDS) to shift the learned distribution to a region with high property values. Molecule design can then be achieved by conditional sampling. Our experiments demonstrate that our method outperforms previous SOTA methods by a significant margin.\n\nA limiting factor is that the sampling with gradual distribution shifting (SGDS) requires that the properties of interest can be easily computed. In our future work, we shall explore semi-supervised learning methods using our model where the properties are available only for a small sample of molecules.\n\nAnother limitation is that, for our optimization method, good property scores may not translate to useful molecules for drug design. This problem may be partially addressed by multi-objective optimization as studied in this paper. In our work, we focus on optimizing given objectives. Designing good objectives or metrics is an equally or perhaps even more important problem that deserves careful investigation.\n\n1In particular, we exclude molecules with QED (↑) smaller than 0.4, SA (↓) larger than 5.5, and too small\n\n(less than 5 atoms) or too large (more than 6 atoms) chemical rings.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nViraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Liggpt: Molecular generation\n\nusing a transformer-decoder model. 2021.\n\nNathan Brown, Marco Fiscato, Marwin H.S. Segler, and Alain C. Vaucher. Guacamol: Benchmarking models for de novo molecular design. Journal of Chemical Information and Modeling, 59 (3):1096–1108, 2019. doi: 10.1021/acs.jcim.8b00839. URL https://doi.org/10.1021/ acs.jcim.8b00839. PMID: 30887799.\n\nBinghong Chen, Tianzhe Wang, Chengtao Li, Hanjun Dai, and Le Song. Molecule optimization by explainable evolution. In International Conference on Learning Representation (ICLR), 2021.\n\nHanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoen-\n\ncoder for structured data. In International Conference on Learning Representations, 2018.\n\nNicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs.\n\narXiv preprint arXiv:1805.11973, 2018.\n\nYilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. CoRR,\n\nabs/1903.08689, 2019. URL http://arxiv.org/abs/1903.08689.\n\nPeter Eckmann, Kunyang Sun, Bo Zhao, Mudong Feng, Michael K Gilson, and Rose Yu. Limo: Latent inceptionism for targeted molecule generation. arXiv preprint arXiv:2206.09010, 2022.\n\nChelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. CoRR, abs/1611.03852, 2016. URL http://arxiv.org/abs/1611.03852.\n\nTianfan Fu, Cao Xiao, and Jimeng Sun. Core: Automatic molecule optimization using copy & refine strategy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 638–645, 2020.\n\nTianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differen-\n\ntiable scaffolding tree for molecular optimization. arXiv preprint arXiv:2109.10469, 2021a.\n\nTianfan Fu, Cao Xiao, Xinhao Li, Lucas M Glass, and Jimeng Sun. Mimosa: Multi-constraint molecule sampling for molecule optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 125–133, 2021b.\n\nRafael G ́omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos ́e Miguel Hern ́andez-Lobato, Benjam ́ın S ́anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al ́an Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, In Advances in NeuAaron C. Courville, and Yoshua Bengio. Generative adversarial nets. ral Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680, 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.\n\nSai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Yashaswi Pathak, Haoran Wei, Shengchao Liu, Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, et al. Learning to navigate the synthetically accessible chemical space using reinforcement learning. In International Conference on Machine Learning, pp. 3668–3679. PMLR, 2020.\n\nWill Grathwohl, Kuan-Chieh Wang, J ̈orn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.\n\nShion Honda, Hirotaka Akita, Katsuhiko Ishiguro, Toshiki Nakanishi, and Kenta Oono. Graph\n\nresidual flow for molecular graph generation. arXiv preprint arXiv:1909.13521, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. arXiv preprint arXiv:2102.09548, 2021.\n\nJohn J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52 (7):1757–1768, 2012.\n\nJan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for\n\nthe exploration of chemical space. Chemical science, 10(12):3567–3572, 2019.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola.\n\nJunction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323–2332. PMLR, 2018.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using In International conference on machine learning, pp. 4849–4859.\n\ninterpretable substructures. PMLR, 2020.\n\nTaesup Kim and Yoshua Bengio. Deep directed generative models with energy-based probability estimation. CoRR, abs/1606.03439, 2016. URL http://arxiv.org/abs/1606.03439.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes.\n\nIn 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6114.\n\nPanagiotis-Christos Kotsias, Josep Ar ́us-Pous, Hongming Chen, Ola Engkvist, Christian Tyrchan, and Esben Jannik Bjerrum. Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Nature Machine Intelligence, 2(5):254–265, 2020.\n\nMario Krenn, Florian H ̈ase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Selfreferencing embedded strings (selfies): A 100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020.\n\nRithesh Kumar, Anirudh Goyal, Aaron C. Courville, and Yoshua Bengio. Maximum entropy generators for energy-based models. CoRR, abs/1901.08508, 2019. URL http://arxiv.org/ abs/1901.08508.\n\nMatt J Kusner, Brooks Paige, and Jos ́e Miguel Hern ́andez-Lobato. Grammar variational autoen-\n\ncoder. In International Conference on Machine Learning, pp. 1945–1954, 2017.\n\nGreg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and\n\npredictive modeling. Greg Landrum, 2013.\n\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based\n\nlearning. Predicting structured data, 1(0), 2006.\n\nMeng Liu, Keqiang Yan, Bora Oztekin, and Shuiwang Ji. Graphebm: Molecular graph generation\n\nwith energy-based models. arXiv preprint arXiv:2102.00546, 2021.\n\nYouzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In International Conference on Machine Learning, pp. 7192–7203. PMLR, 2021.\n\nKaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An invert-\n\nible flow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019.\n\nRadford M Neal. MCMC using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo,\n\n2, 2011.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJiquan Ngiam, Zhenghao Chen, Pang Wei Koh, and Andrew Y. Ng. Learning deep energy models. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 1105–1112, 2011. URL https://icml.cc/ 2011/papers/557_icmlpaper.pdf.\n\nWeili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-space energy-based models. Advances in Neural Information Processing Systems, 34:13497–13510, 2021.\n\nAkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Al ́an Aspuru-Guzik. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. ICLR 2020, 2020.\n\nErik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent nonpersistent short-run MCMC toward energy-based model. Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, Canada, 2019.\n\nBo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space\n\nenergy-based prior model. arXiv preprint arXiv:2006.08205, 2020.\n\nPavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of druglike chemical space based on gdb-17 data. Journal of computer-aided molecular design, 27(8): 675–679, 2013.\n\nDaniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models. Frontiers in pharmacology, 11:565644, 2020.\n\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-\n\ncal statistics, pp. 400–407, 1951.\n\nDiogo Santos-Martins, Leonardo Solis-Vasquez, Andreas F Tillack, Michel F Sanner, Andreas Koch, and Stefano Forli. Accelerating autodock4 with gpus and gradient-based local search. Journal of chemical theory and computation, 17(2):1060–1073, 2021.\n\nMarwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1): 120–131, 2018.\n\nChence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382, 2020.\n\nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36, 1988.\n\nRobin Winter, Floriane Montanari, Andreas Steffen, Hans Briem, Frank No ́e, and Djork-Arn ́e Clevert. Efficient multi-objective molecular optimization in a continuous latent space. Chemical science, 10(34):8016–8024, 2019.\n\nJianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 2635–2644, 2016. URL http://proceedings.mlr. press/v48/xiec16.html.\n\nYutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: In International ConferMarkov molecular sampling for multi-objective drug discovery. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= kHSu4ebxFXY.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nJiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional polIn Advances in neural information\n\nicy network for goal-directed molecular graph generation. processing systems, pp. 6410–6421, 2018.\n\nChengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 617–626, 2020.\n\nZhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of\n\nmolecules via deep reinforcement learning. Scientific reports, 9(1):1–10, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 DETAILS ABOUT MODEL AND LEARNING\n\nOur model is of the form pα(z)pβ(x|z)pγ(y|z). The marginal distribution of (x, y) is\n\n(cid:90)\n\npθ(x, y) =\n\npθ(x, y, z)dz =\n\n(cid:90)\n\npα(z)pβ(x|z)pγ(y|z)dz.\n\nWe use pθ(x, y) to approximate the data distribution of (x, y).\n\nFor the data distribution of (x, y), y is a deterministic function of x. However, a machine learning method usually cannot learn the deterministic function exactly. Instead, we can only learn a probabilistic pθ(y|z). Our model pθ(x, y) seeks to approximate the data distribution p(x, y) by maximum likelihood. A learnable and flexible prior model pα(z) helps to make the approximate more accurate than a fixed prior model such as that in VAE.\n\nLet (cid:80)n\n\nthe training data be {(xi, yi), i = 1, ..., n}. i=1 log pθ(xi, yi). The learning gradient is L′(θ) = (cid:80)n\n\nThe log-likelihood function is L(θ) = i=1 ∇θ log pθ(xi, yi). In the following, we provide details for calculating ∇θ log pθ(x, y) for a single generic training example (x, y) (where we drop the subscript i for notation simplicity).\n\n∇θpθ(x, y)\n\n∇θpθ(x, y, z)dz\n\n∇θ log pθ(x, y) =\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\n1 pθ(x, y) 1\npθ(x, y) 1\npθ(x, y) (cid:90) pθ(x, y, z) pθ(x, y)\n\n(cid:90)\n\n(cid:90)\n\npθ(z | x, y)∇θ log pθ(x, y, z)dz\n\npθ(x, y, z)∇θ log pθ(x, y, z)dz\n\n∇θ log pθ(x, y, z)dz\n\n= Epθ(z|x,y) [∇θ log pθ(x, y, z)] = Epθ(z|x,y) [∇θ(log pα(z) + log pβ(x|z) + log pγ(y|z))] .\n\nFor the prior model,\n\n∇α log pα(z) = ∇αfα(z) − ∇α log Z(α)\n\n1 Z(α) 1\nZ(α) (cid:90)\n\n= ∇αfα(z) −\n\n= ∇αfα(z) −\n\n∇αZ(α)\n\n(cid:90)\n\n∇α exp(fα(z))p0(z)dz\n\n= ∇αfα(z) −\n\n1 Z(α) = ∇αfα(z) − Epα(z)[∇αfα(z)].\n\n∇αfα(z)\n\nexp(fα(z))p0(z)dz\n\nThus the learning gradient for α given an example (x, y) is\n\nδα(x, y) = ∇α log pθ(x, y) = Epθ(z|x,y)[∇αfα(z)] − Epα(z)[∇αfα(z)].\n\n(17)\n\nThe above equation has an empirical Bayes nature. pθ(z|x, y) is based on the empirical observation (x, y), while pα is the prior model. For the generation model,\n\nδβ(x, y) = ∇β log pθ(x, y) = Epθ(z|x,y)[∇β log pβ(x|z)].\n\nSimilarly, for the regression model,\n\nδγ(x, y) = ∇γ log pθ(x, y) = Epθ(z|x,y)[∇γ log pγ(y|z)].\n\n(18)\n\n(19)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nEstimating expectations in the above equations requires Monte Carlo sampling of the prior model pα(z) and the posterior distribution pθ(z|x, y). If we can draw fair samples from the two distributions, and use these Monte Carlo samples to approximate the expectations, the the gradient ascent algorithm based on the Monte Carlo samples is the stochastic gradient ascent algorithm or the stochastic approximation algorithm of Robbins and Monro (Robbins & Monro, 1951), who established the convergence of such an algorithm to a local maximum of the log-likelihood.\n\nFor MCMC sampling using Langevin dynamics, the finite step or short run Langevin dynamics may cause bias in Monte Carlo sampling. The bias was analyzed in Pang et al. (2020). The resulting algorithm is an approximate maximum likelihood learning algorithm.\n\nA.2 ABLATION STUDIES\n\nWe conduct ablations on the key components of our method: 1) EBM Prior (EBM vs. standard Gaussian N (0, I)), (2) SGDS (shift distributions with SGDS vs. no shift), and (3) joint training (joint training of molecule and molecular property distribution vs. train molecule with LSEBM and learn property regression model in a second step). The ablation results are displayed in Table 6. It is clear that all the proposed components contribute significantly to the good performance of our method.\n\nEBM Prior\n\nSGDS\n\nJoint Training\n\nPenalized logP 2nd\n\n1st\n\n3rd\n\n✗ ✓\n✓ ✓\n\n✓ ✗\n✓ ✓\n\n✓ ✓\n✗ ✓\n\n13.81 12.27 12.79 26.37\n\n13.78 12.02 12.70 25.05\n\n13.75 11.93 12.41 24.38\n\nTable 6: Ablation studies.\n\nA.3\n\nILLUSTRATION OF SAMPLING WITH GRADUAL DISTRIBUTION SHIFTING (SGDS)\n\nFigure 3 shows property score densities of sampled molecules from our MolEBM in the distribution shifting process. We can see the model distribution is gradually shifting towards the region supported by molecules with high property values.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Illustration of SGDS in a single-objective penalized logP optimization experiment.\n\nA.4 UNCONDITIONAL GENERATION\n\nFor unconditional generation task, we use uniqueness, novelty and validity to compare our generation results with existing methods. Meanwhile, for SELFIES-based ZINC dataset, we split this dataset into train split ( 240k) and test split (10k samples). Here, we randomly sample 10k molecules from the learned latent EBM and calculate their logP, QED and SA scores using RDKit. We compare these property densities with molecule property densities in test split. The results are shown in Figure 4. We can see that the marginal distributions (property score densities) of the learned model match those of the data quite well, implying that our model indeed captures the chemical rules implicitly.\n\nFigure 4: Property score distributions.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nA.5 GENERATION WITH DIFFERENT LENGTH OF MARKOV CHAIN\n\nIn our experiments, we use short-run MCMC (i.e. s = 20) in Equation 13 for all experiments. From Figure 5, we can see with the increasing length of Markov chains, the molecules change accordingly, showing that Markov chain doesn’t get stuck in the local mode.\n\nFigure 5: Sampled molecules with the different length of Markov chain.\n\nA.6 TRAINING TIME\n\nThe joint training of MolEBM takes 4 hours with 30 iterations on a single Nvidia Titan XP GPU with batch size 2048. For non-biological single-objective property optimization, it takes around 0.5 hours to do 50 distribution shifting (SGDS) iterations. For biological binding affinity maximization, the optimization time is mainly dependent on the number of queries of AutoDock-GPU. We do 20 and 10 SGDS iterations for the single-objective and multi-objective tasks, respectively, which cost 10 hours and 5 hours. For biological property optimization tasks, we use two Nvidia Titan XP GPUs, one for running our code and another one for running AutoDock-GPU. We have added a table to compare with previous methods.\n\nModel JT-VAE GCPN MolDQN GraphDF Mars LIMO MolEBM\n\nPenalized-logP/QED 24 8\n24 8\n12 1\n4.5\n\nTable 7: Comparison of molecule generation time in (hrs). Results obtained from (Eckmann et al., 2022).\n\nEven if we intensively use MCMC sampling-based methods, our training speed is affordable comparing to existing methods. That’s due to our designed latent space EBM is low-dimensional (i.e. dim(z)=100) and we use short-run MCMC (i.e. with fixed iteration steps s = 20) in our experiments. The sampling results with respect to the length of Markov chain is discussed in previous section.\n\nA.7 OPTIMIZATION WITH CONSTRAINTS\n\nIn unconditional generation task, even with SMILES representation, our model can capture the valency constraints in chemical space. This idea can further be extended to constrained optimizations, and those constraints in the chemical space can be directly imposed by keeping the sampled molecules that satisfy the constraints during the SGDS molecule sampling step in Algorithm 2. We include logP targeting and similarity-constrained optimization experiments in the following.\n\nFor example, in logP targeting experiments, during each iteration, we only keep the molecules within the target logP range; in similarity-constrained experiments, we use those molecules which satisfy similarity constraints to update the model. By iteratively updating the model with those selected molecules, we shift the joint distribution towards the region that satisfies the constraints.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nA.7.1\n\nLOGP TARGETING\n\nComparing to previous methods, MolEBM is able to get competitive diversity scores with significantly better success rate in both ranges. That’s because after SGDS, our model is shifted towards the region that is supported by molecules satisfying the logP constraints. Due to the flexibility of our EBM prior, MolEBM achieves rather high diversity scores while keeping most of the sampled molecules within the logP range.\n\nMethod\n\n−2.5 ≤ logP ≤ −2 Success Diversity\n\n5 ≤ logP ≤ 5.5 Success Diversity\n\nZINC\n\n0.4%\n\n11.3% JT-VAE 0\nORGAN 85.5% GCPN 10.4% LIMO MolEBM 86.0%\n\n0.919\n\n0.846 −\n0.392 0.914 0.874\n\n1.3%\n\n7.6% 0.2% 54.7% −\n62.2%\n\n0.901\n\n0.907 0.909 0.855 −\n0.858\n\nTable 8: logP targeting to a certain range (Eckmann et al., 2022; You et al., 2018; Luo et al., 2021; Xie et al., 2021).\n\nA.7.2 SIMILARITY-CONSTRAINED OPTIMIZATION\n\nFollowing previous procedures in JT-VAE (Jin et al., 2018), we select 800 molecules with the lowest penalized-logP scores in ZINC250k dataset. This experiment aims to generate novel molecules with high penalized-log while similarity to the target molecules. We first randomly select one molecule as the target, and then optimize the p-logP. For each SGDS step, we only keep the molecules that have similarity score greater than minimum value δ.\n\nδ\n\n0.0 0.2 0.4\n\nGCPN\n\nGraphDF\n\nLIMO\n\nMolEBM\n\nImprov.\n\n% Succ.\n\nImprov.\n\n% Succ.\n\nImprov.\n\n% Succ.\n\nImprov.\n\n% Succ.\n\n4.2 ± 1.3 4.1 ± 1.2 2.5 ± 1.3\n\n100 100 100\n\n5.9 ± 2.0 5.6 ± 1.7 4.1 ± 1.4\n\n100 100 100\n\n10.1 ± 2.3 5.8 ± 2.6 3.6 ± 2.3\n\n100 99.0 93.7\n\n19.11 ± 2.12 7.41 ± 1.89 3.80 ± 1.44\n\n100 100 97.5\n\nTable 9: Similarity-constrained optimization results. Baseline results obtained from (Eckmann et al., 2022; Luo et al., 2021).\n\nA.8 ADDITIONAL EXPERIMENTS ON GUACAMOL BENCHMARKS\n\nWe further evaluate our MolEBM on several distribution learning and goal-directed optimization tasks in GuacaMol benchmark (Brown et al., 2019).\n\nTo be specific, for distribution learning, we use the validity, uniqueness and novelty to evaluate our model. The results are shown in Table 10. Comparing to existing methods, our MolEBM trained using SELFIES representations achieves highest scores among all three tasks.\n\nFor goal-directed benchmarks, we select five multiple property optimization tasks (MPO). Due to the time limit, we did not have time to tune our code for the new experiments. We expect to obtain improved results with fine-tuning. However, our MolEBM is still able to achieve comparable results to the strong baseline as Graph GA.\n\nAgain, GuacaMol is an extremely useful benchmark since it provides the normalized scoring functions which are weighed sum of multiple diverse properties of interest. The normalized scoring functions enable the fast convergence of MolEBM. With GuacaMol, we can also investigate the design choice between single property regression network to predict the pre-defined scores and multiple property regression networks to predict original chemical properties individually. We leave this question for future studies.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nA.8.1 DISTRIBUTION-LEARNING BENCHMARKS\n\nBenchmark Validity Uniqueness Novelty\n\nAAE Graph MCTS Random Sampler 0.822 1.000 0.998\n\n1.000 1.000 0.994\n\n1.000 0.997 0.000\n\nSMILES LSTM VAE MolEBM\n\n1.000 1.000 0.912\n\n0.959 0.999 0.971\n\n1.000 1.000 0.999\n\nTable 10: Distribution learning results on GuacaMol benchmarks (Brown et al., 2019).\n\nA.8.2 GOAL-DIRECTED BENCHMARKS\n\nBenchmark Osimertinib MPO Fexofenadine MPO Ranolazine MPO Sitagliptin MPO\n\nBest of Dataset 0.839 0.817 0.792 0.509\n\nSMILES GA Graph MCTS Graph GA SMILES LSTM MolEBM 0.953 0.998 0.920 0.891\n\n0.933 0.971 0.924 0.829\n\n0.784 0.695 0.616 0.458\n\n0.886 0.931 0.881 0.689\n\n0.907 0.959 0.855 0.545\n\nTable 11: Goal-directed optimization results on GuacaMol benchmarks (Brown et al., 2019). Top-2 results are highlighted as bold and italic respectively.\n\nA.9 GENERATED SAMPLES\n\nFigure 6 and figure 7 show generated molecules with high binding affinities towards ESR1 and ACAA1 respectively in single-objective property design experiments.\n\nFigure 8 and Figure 9 show generated molecules with high binding affinities towards ESR1 and ACAA1 respectively in multi-objective property design.\n\nComparing to the previous state-of-the-art methods, our MolEBM is able to produce more high quality molecules than top-3 molecules because after sampling with gradual distribution shifting (SGDS), MolEBM locates at the area supported by molecules with high binding affinities.\n\nIn single-objective design, we find that few generated molecules may be of less practical use due the first one in Figure 7 has a large circle). This observation is in to undesired properties (e.g. accordance with (Eckmann et al., 2022), which is the case where the single-objective optimization is not sufficient. Thus we need multi-objective binding affinities design settings because in contrast to non-biological properties, binding affinities are hard to compute and optimize.\n\nIn multi-objective design settings, we find that those issues mentioned above can be partially addressed by optimizing binding affinities, QED and SA at the same time.\n\nMeanwhile, compared to previous generative model based methods, we use Langevin dynamics to infer the posterior distribution p(z|x, y1, . . . , yn) without bothering to design different encoders when facing different combination of properties.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Generated molecules in singe-objective esr1 binding affinity maximization experiments with corresponding KD(↓) in nmol/L.\n\nFigure 7: Generated molecules in singe-objective acaa1 binding affinity maximization experiments with corresponding KD(↓) in nmol/L.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Generated molecules in multi-objective esr1 binding affinity maximization experiments with corresponding KD(↓) in nmol/L.\n\nFigure 9: Generated molecules in multi-objective acaa1 binding affinity maximization experiments with corresponding KD(↓) in nmol/L.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nA.10 TOP-3 MOLECULES IN P-LOGP AND QED OPTIMIZATION\n\nFigure 10: Top-3 molecules in single-objective QED maximization.\n\nFigure 11: Top-3 molecules in single-objective p-logP maximization.\n\nA.11 REPRODUCIBILITY\n\nOur code and saved checkpoints can be found here 2.\n\n2https://drive.google.com/drive/folders/1UQcXrLWo20wuBocCIEIq7RRm1p2-bb8H?usp=sharing\n\n22",
    "reference": "# Summary Of The Paper\n\nThe aim of this paper is the generation of molecules with desired chemical and biological properties learning the joint distribution of molecules and properties . In order to achieve this the authors propose an energy based generative model that augments a top down generative model (conditional autoregressive model) with a latent space learnt via EBM and a property regression model for each of the properties. Then, to sample molecules with desired properties they design an algorithm to shift the learned distribution iteratively.\n\n# Strength And Weaknesses\n\nStrength: \nThe paper seems to perform better than state of the art models for unconditional generations using smiles. Especially for unconditional generations it achieves 95.5% validity with 100% novelty using SMILES with the ZINC dataset. Moreover, the sampling with gradual distribution shifting (SGDS) is an interesting suggestion for out of distribution sampling. \n\nWeaknesses: \nThe paper is clearly written but is missing some more explanation about the sampling procedure and the theory behind it.  Even though to estimate the expirations MCMC sampling of the prior and the posterior is required the method is described as \"maximum likelihood”.  \nResults are not compelling enough. Event though the model is proposed for conditional generations given desired properties in the experimental results we see results only for property optimisation. So more experiments should be added to prove the capability for conditional molecule generation like\n- experiments with properties targeted to a predefined range, providing the percent of generated molecules within the target range and the diversity\n- experiments aiming to generate new molecules with a optimised property but similar to the original molecules (similarity-constrained Optimisation)\n\nMoreover, a comparison of the generation time of the proposed model with the other models would be useful. \n\nThe authors should not bold the best values only when they correspond to your model (Table3 GCPN, MolDQN, MARS, GraphDF achieve same QED 1st with the proposed model, MARS, GraphDF achieve same QED 2nd and 3rd with the proposed model)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper is well written, but in order to understand some parts of theory one needs to go back to the [Pang2020] paper. \n\nNovelty : The paper is an extension of the  LLSEBprior model [Pang2020] with which parts of the text are shared. This work differs by adding a property regression model on the latent space of the LLSEBprior model which is trained jointly and proposing the gradual distribution shifting (SGDS) to extrapolate the data distribution and sample from the region supported by molecules with high property values. Moreover, the method is applied to molecular data instead of images and text.  The use of the property predictor in the latent space has also used in [Goméz-Bombarelli2018] and [Jin2018] where also is trained jointly as in the proposed model.\n\nReproducibility: Yes\n\nPang2020]:  Learning Latent Space Energy-Based Prior Model, NeurIPS 2020 \n[Gomez-Bombarelli2018]: Automatic chemical de- sign using a data-driven continuous representation of molecules, ACS Cent. Sci. 2018\n[Jin2018]: Learning multimodal graph-to-graph translation for molecular optimisation, ICLR 2019\n\n# Summary Of The Review\n\nThe paper extends the LLSEBprior model with a property predictor on the latent space in order to generate molecules with desired chemical and biological properties. \nThe contribution of the paper is limited and more experiments are needed in order to illustrate the capability of the proposed model to generate molecules with specific properties (see Weaknesses).\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSCALING PARETO-EFFICIENT DECISION MAKING VIA OFFLINE MULTI-OBJECTIVE RL\n\nBaiting Zhu, Meihua Dang, Aditya Grover University of California, Los Angeles, CA, USA baitingzbt@g.ucla.edu, mhdang@cs.ucla.edu, adityag@cs.ucla.edu\n\nABSTRACT\n\nThe goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent’s preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)datasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends return-conditioned offline methods including Decision Transformers (Chen et al., 2021) and RvS (Emmons et al., 2021) via a novel preference-and-return conditioned policy. Empirically, we show that PEDA closely approximates the behavioral policy on the D4MORL benchmark and provides an excellent approximation of the Pareto-front with appropriate conditioning, as measured by the hypervolume and sparsity metrics.\n\n1\n\nINTRODUCTION\n\nWe are interested in learning agents for multi-objective reinforcement learning (MORL) that optimize for one or more competing objectives. This setting is commonly observed in many real-world scenarios. For instance, an autonomous driving car might trade off high speed and energy savings depending on the user’s preferences. If the user has a relatively high preference for speed, the agent will move fast regardless of power usage; on the other hand, if the user tries to save energy, the agent will keep a more steady speed. One key challenge with MORL is that different users might have different preferences on the objectives and systematically exploring policies for each preference might be expensive, or even impossible. In the online setting, prior work considers several approximations based on scalarizing the vector-valued rewards of different objectives based on a single preference (Lin, 2005), learning an ensemble of policies based on enumerating preferences (Mossalam et al., 2016, Xu et al., 2020), or extensions of single-objective algorithms such as Q-learning to vectorized value functions (Yang et al., 2019).\n\nWe introduce the setting of offline multi-objective reinforcement learning for high-dimensional state and action spaces, where our goal is to train an MORL policy agent using an offline dataset of demonstrations from multiple agents with known preferences. Similar to the single-task setting, offline MORL can utilize auxiliary logged datasets to minimize interactions, thus improving data efficiency and minimizing interactions when deploying agents in high-risk settings. In addition to its practical utility, offline RL (Levine et al., 2020) has enjoyed major successes in the last few years (Kumar et al., 2020, Kostrikov et al., 2021, Chen et al., 2021) on challenging high-dimensional environments for continuous control and game-playing. Our contributions in this work are two-fold in introducing benchmarking datasets and a new family of MORL, as described below.\n\nWe introduce Datasets for Multi-Objective Reinforcement Learning (D4MORL), a collection of 1.8 million trajectories on 6 multi-objective MuJoCo environments (Xu et al., 2020). Here, 5 environ-\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nments consist of 2 objectives and 1 environment consists of 3 objectives. For each environment in D4MORL, we collect demonstrations from 2 pretrained behavioral agents: expert and amateur, where the relative expertise is defined in terms of the Pareto-efficiency of the agents and measured empirically via their hypervolumes. Furthermore, we also include 3 kinds of preference distributions with varying entropies to expose additional data-centric aspects for downstream benchmarking. Lack of MORL datasets and large-scale benchmarking has been a major challenge for basic research (Hayes et al., 2022), and we hope that D4MORL can aid future research in the field.\n\nNext, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that extends return-conditioned methods including Decision Transformer (DT) (Chen et al., 2021) and RvS (Emmons et al., 2021) to the multi-objective setting. These methods learn a returnconditioned policy via a supervised loss on the predicted actions. In recent work, these methods have successfully scaled to agents that demonstrate broad capabilities in multi-task settings (Lee et al., 2022 Reed et al., 2022). For MORL, we introduce a novel preference and return conditioned policy network and train it via a supervised learning loss. At test time, naively conditioning on the default preferences and maximum possible returns leads to out-of-distribution behavior for the model, as neither has it seen maximum returns for all objectives in the training data nor is it possible to simultaneously maximize all objectives under competition. We address this issue by learning to map preferences to appropriate returns and hence, enabling predictable generalization at test-time.\n\nEmpirically, we find PEDA performs exceedingly well on D4MORL and closely approximates the reference Pareto-frontier of the behavioral policy used for data generation. In the multi-objective HalfCheetah environment, compared with an average upper bound on the hypervolume of 5.79ˆ106 achieved by the behavioral policy, PEDA achieves an average hypervolume of 5.77 ˆ 106 on the Expert and 5.76 ˆ 106 on the Amateur datasets.\n\n2 RELATED WORK\n\nMulti-Objective Reinforcement Learning Predominant works in MORL focus on the online setting where the goal is to train agents that can generalize to arbitrary preferences. This can be achieved by training a single preference-conditioned policy (Yang et al., 2019; Parisi et al., 2016), or an ensemble of single-objective policies for a finite set of preferences (Mossalam et al., 2016; Xu et al., 2020; Zhang & Li, 2007). Many of these algorithms consider vectorized variants of standard algorithms such as Q-learning (Mossalam et al., 2016; Yang et al., 2019), often augmented with strategies to guide the policy ensemble towards the Pareto front using evolutionary or incrementally updated algorithms (Xu et al., 2020; Zhang & Li, 2007; Mossalam et al., 2016; Roijers et al., 2014; Huang et al., 2022). Other approaches have also been studied, such as framing MORL as a meta-learning problem (Chen et al., 2019), learning the action distribution for each objective (Abdolmaleki et al., 2020), and learning the relationship between objectives (Zhan & Cao, 2019) among others. In contrast to these online MORL works, our focus is on learning a single policy that works for all preferences using only offline datasets.\n\nThere are also a few works that study decision-making with multiple objectives in the offline setting and sidestep any interaction with the environments. Wu et al., 2021 propose a provably efficient offline MORL algorithm for tabular MDPs based on dual gradient ascent. Thomas et al., 2021 study learning of safe policies by extending the approach of Laroche et al., 2019 to the offline MORL setting. Their proposed algorithm assumes knowledge of the behavioral policy used to collect the offline data and is demonstrated primarily on tabular MDPs with finite state and action spaces. In contrast, we are interested in developing dataset benchmarks and algorithms for scalable offline policy optimization in high-dimensional MDPs with continuous states and actions.\n\nMulti-Task Reinforcement Learning MORL is also closely related to multi-task reinforcement learning, where every task can be interpreted as a distinct objective. There is an extensive body of work in learning multi-task policies both in the online and offline setups (Wilson et al., 2007; Lazaric & Ghavamzadeh, 2010; Teh et al., 2017) inter alia. However, the key difference is that typical MTRL benchmarks and algorithms do not consider solving multiple tasks that involve inherent trade-offs. Consequently, there is no notion of Pareto efficiency and an agent can simultaneously excel in all the tasks without accounting for user preferences.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nReinforcement Learning Via Supervised Learning A body of recent works have formulated offline reinforcement learning as an autoregressive sequence modeling problem using Decision Transformers (DT) or Trajectory Transformers ( Chen et al., 2021, Janner et al., 2021) The key idea in DT is to learn a transformer-based policy that conditions on the past history and a dynamic estimate of the returns (a.k.a. returns-to-go). Follow-up works consider online learning (Zheng et al., 2022) as well as simpler variants that rely only on multi-layer perceptrons (Emmons et al., 2021). Such agents are generally more stable and robust to optimize due to the simplicity of loss function and easier to scale to more complex settings such as environments with high-dimensional actions or states, as shown in recent works in multi-task RL (Lee et al., 2022; Reed et al., 2022).\n\n3 PRELIMINARIES\n\nSetup and Notation. We operate in the general framework of a multi-objective Markov decision process (MOMDP) with linear preferences (Wakuta, 1995). An MOMDP is represented by the tuple xS, A, P, R, Ω, f, γy. At each timestep t, the agent with a current state st P S takes an action at P A to transition into a new state st`1 with probability Ppst`1|st, atq and observes a reward vector rt “ Rpst, atq P Rn. Here, n is the number of objectives. The vector-valued return R P Rn t γtrt. We of an agent is given by the discounted sum of reward vectors over a time horizon, R “ also assume that there exists a linear utility function f and a space of preferences Ω that can map the reward vector rt and a preference vector ω P Ω to a scalar reward rt, i.e., rt “ f prt, ωq “ ω⊺rt. ns⊺ where the expected The expected vector return of a policy π is given an Gπ “ rGπ i “ Eat`1„πp ̈|st,ωqr return of the ith objective is given as Gπ t Rpst, atqis for some predefined time horizon and preference vector ω. The goal is to train a multi-objective policy πpa|s, ωq such that ř\nthe expected scalarized return ω⊺ Gπ “ Erω⊺\n\nt Rpst, atqs is maximized.\n\n2 , . . . , Gπ\n\n1 , Gπ\n\nř\n\nř\n\nPareto Optimality. In MORL, one cannot optimize all objectives simultaneously, so policies are evaluated based on the Pareto set of their vector-valued expected returns. Consider a preference-conditioned policy πpa|s, ωq that is evaluated for m distinct preferences ω1, . . . , ωm, and let the resulting policy set be represented as tπpup“1,...,m, where πp “ πpa|s, ω “ ωpq, and Gπp is the corresponding unweighted expected return. We say the solution Gπp is dominated by Gπq when there is no objective for which πq is worse than πp, i.e., Gπp i ă Gπq for @i P r1, 2, . . . , ns. If a solution is not dominated, it is part of the Pareto set denoted as P . The curve traced by the solutions in a Pareto set is also known as the Pareto front. In MORL, our goal is to define a policy such that its empirical Pareto set is a good approximation of the true Pareto front. While we do not know the true Pareto front for many problems, we can define metrics for relative comparisons between different algorithms. Specifically, we evaluate a Pareto set P based on two metrics, hypervolume and sparsity that we describe next.\n\ni\n\nDefinition 1 (Hypervolume). Hypervolume HpP q measures the space or volume enclosed by the solutions in the Pareto set P : ż\n\nHpP q “\n\nRm\n\n1HpP qpzq dz,\n\nwhere HpP q “ tz P Z|Di : 1 ď i ď |P |, r ĺ z ĺ P piqu. P piq is the ith solution in P , ĺ is the dominance relation operator, and 1HpP qpzq equals 1 if z P HpP q and 0 otherwise. Higher hypervolumes are better. Definition 2 (Sparsity). Sparsity SpP q measures the density of the Pareto front covered by a Pareto set P : |P | ́1ÿ\n\nnÿ\n\nSpP q “\n\n1 |P | ́ 1\n\ni“1\n\nk“1\n\np ̃Pipkq ́ ̃Pipk ` 1qq2,\n\nwhere ̃Pi represents a list sorted as per the values of the ith objective in P and ̃Pipkq is the kth value in the sorted list. Lower sparsity is better.\n\nFigure 1: Illustration of the Hypervolume and Sparsity Metrics. Only undominated solutions (i.e., the Pareto set) are used for calculating the evaluation metrics.\n\nSee Figure 1 for an illustration and Appendix F for discussion on other possible metrics.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n4 D4MORL: DATASETS FOR OFFLINE MULTI-OBJECTIVE REINFORCEMENT\n\nLEARNING\n\nIn offline RL, the goal of an RL agent is to learn the optimal policy using a fixed dataset without any interactions with the environment (Levine et al., 2020). This perspective brings RL closer to supervised learning, where the presence of large-scale datasets has been foundational for further progress in the field. Many such data benchmarks exist for offline RL as well; a notable one is the D4RL (Fu et al., 2020) benchmark for continuous control which has led to the development of several state-of-the-art offline RL algorithms (Kostrikov et al., 2021; Kumar et al., 2020; Chen et al., 2021) that can scale favorably even in high dimensions. To the best of our knowledge, there are no such existing benchmarks for offline MORL. Even for the online setting, most works in MORL conduct evaluations on toy MDPs (e.g., gridworld) with a few exceptions that include continuous control, e.g., Chen et al. (2019); Xu et al. (2020). This calls for a much-needed push towards more challenging benchmarks for reliable evaluation of MORL, especially in the offline setting.\n\nWe introduce Datasets for Multi-Objective Reinforcement Learning (D4MORL), a large-scale benchmark for offline MORL. Our benchmark consists of offline trajectories from 6 multiobjective MuJoCo environments including 5 environments with 2 objectives each (MO-Ant, MOHalfCheetah, MO-Hopper, MO-Swimmer, MO-Walker2d), and one environment with three objectives (MO-Hopper-3obj). The objectives are conflicting for each environment; for instance, the two objectives in MO-Hopper correspond to jumping and running; in MO-HalfCheetah, MO-Swimmer, and MO-Walker2d, they correspond to the speed and energy savings of the agent. See Appendix A for more details on the semantics of the target objectives for each environment. These environments were first introduced in Xu et al. (2020) for online MORL, and as such, we use their pretrained ensemble policies as building blocks for defining new behavioral policies for dataset collection, which we discuss next.\n\n4.1 TRAJECTORY SAMPLING\n\nThe quality of the behavioral policy used for sampling trajectories in the offline dataset is a key factor for benchmarking downstream offline RL algorithms. In existing benchmarks for single-objective RL such as D4RL (Fu et al., 2020), the quality of a behavioral policy can be ascertained and varied based on its closeness to a single expert policy, as measured by its scalar-valued returns. For a MOMDP, we do not have the notion of a scalar return and hence, a reference expert policy (or set of policies) should reflect the optimal returns for all possible preferences in the preference space.\n\nWe use Prediction-Guided Multi-Objective Reinforcement Learning (PGMORL), a state-of-the-art MORL algorithm for defining reference expert policies. PGMORL (Xu et al., 2020) uses evolutionary algorithms to train an ensemble of policies to approximate the Pareto set. Each reference policy in the ensemble is associated with a unique preference; as for any new preference, it is mapped to the closest preference in the reference set. The number of policies in the ensemble can vary significantly; for instance, we have roughly 70 reference policies for MO-Antand 2445 policies for harder environments such as MO-Hopper-3obj. Given a desired preference, we define two sets of behavioral policies:\n\n1. Expert Dataset: We find the best reference policy in the policy ensemble, and always\n\nfollow the action taken by the selected reference policy.\n\n2. Amateur Dataset: As before, we first find the best reference policy in the policy ensemble. With a fixed probability p, we randomly perturb the actions of the reference policies. Otherwise, with probability 1 ́ p, we take the same action as the reference policy. In D4MORL, we set p “ 0.65.\n\nFurther details are described in Appendix C. In Figure 2, we show the returns of the trajectories rolled out from the expert and amateur policies for the 2 objective environments evaluated for a uniform sampling of preferences. We can see that the expert trajectories typically dominate the amateur trajectories, as desired. For the amateur trajectories, we see more diversity in the empirical returns for both objectives under consideration. The return patterns for the amateur trajectories vary across different environments providing a diverse suite of datasets in our benchmark.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) MO-Ant\n\n(b) MO-HalfCheetah\n\n(c) MO-Hopper\n\n(d) MO-Swimmer\n\n(e) MO-Walker2d\n\nFigure 2: Empirical returns for expert and amateur trajectory datasets for the two-objective environments in D4MORL. For each environment and dataset, we randomly plot returns for 300 trajectories.\n\n(a) High Entropy, H “ ́0.2\n\n(b) Medium Entropy, H “ ́0.35\n\n(c) Low Entropy, H “ ́1.54\n\nFigure 3: Illustration of the preference distributions for 3 objectives. Entropy is estimated on 50K preference samples using the Vasicek estimator in Scipy (Vasicek, 1976, Virtanen et al., 2020).\n\n4.2 PREFERENCE SAMPLING\n\nThe coverage of any offline dataset is an important factor in dictating the performance of downstream offline RL algorithms (Levine et al., 2020). For MORL, the coverage depends on both the behavioral MORL policy as well as the distribution of preferences over which this policy is evaluated. We use the following protocols for sampling from the preference space Ω. First, we restrict our samples to lie within a physically plausible preference space Ω ̊ Ď Ω covered by the behavioral policy πβ. For instance, MO-Hopper has two objectives: jumping and running. Since the agent can never gain running rewards without leaving the floor. Thus, the preference of 100% running and 0% jumping is not achievable and excluded from our preference sampling distribution.\n\nSecond, we are primarily interested in offline trajectories that emphasize competition between multiple objectives rather than focusing on a singular objective. To enforce this criterion, we define 3 sampling distributions concentrated around the centroid of the preference simplex. The largest spread distribution samples uniformly from Ω ̊ and is denoted as High-Entropy (High-H). Next, we have a Medium-Entropy (Med-H) distribution specified via samples of Dirichlet distributions with large values of their concentration hyperparameters (aka α). Finally, we have a Low-Entropy (Low-H) distribution that is again specified via samples of Dirichlet distributions but with low values of their concentration hyperparameters. We illustrate the samples for each of the preference distributions along with their empirical entropies in Figure 3. Further details on the sampling distributions are deferred to Appendix B. By ensuring different levels of coverage, we can test the generalizability of an MORL policy to preferences unseen during training. In general, we expect Low-H to be the hardest of the three distributions due to its restricted coverage, followed by Med-H and High-H.\n\nOverall Data Generation Pipeline. The pseudocode for generating the dataset is described in Algorithm 1. Given a preference distribution, we first sample a preference ω and query the closest behavioral policy in either the amateur/expert ensemble matching ω. We roll out this policy for T time steps (or until the end of an episode if sooner) and record the state, action, and reward information. Each trajectory in our dataset is represented as:\n\nτ “ă ω, s1, a1, r1, . . . , sT , aT , rT ą\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Data Collection in D4MORL\n\nprocedure COLLECT(prefDist, nTraj, env, pretrainedAgents, T)\n\nagents = pretrainedAgents prefs = prefDist(nTraj) all trajs = [] for ω in prefs do\n\nagent = closestAgent(agents, ω) s = env.reset() done = False τ = [ω] t = 0 while (NOT done) AND (t ă T) do\n\na = agent.get action(s) s1, done, r = env.step(a) append s, a, s1, r to τ s = s1 t = t + 1\n\nappend τ to all trajs\n\nreturn all trajs\n\nFor every environment in D4MORL, we collect 50K trajectories of length T “ 500 for both expert and amateur trajectory distributions under each of the 3 preference distributions. Overall, this results in a total of 1.8M trajectories over all 6 environments, which corresponds to roughly 867M time steps. We refer the reader to Table 5 in Appendix B for additional statistics on the dataset.\n\n5 PARETO-EFFICIENT DECISION AGENTS (PEDA)\n\nIn this section, we propose Pareto-Efficient Decision Agents (PEDA), a new family of offline multi-objective RL agents. PEDA aims to achieve Pareto-efficiency by extending Decision Transformers (Chen et al., 2021) into multi-objective setting. We first introduce the architecture of Decision Transformers (DT) and its variant, Reinforcement Learning Via Supervised Learning (RvS), followed by our modifications extending them to the multi-objective setting.\n\nT\n\nř\n\nDT casts offline RL as a conditional sequence modeling problem that predicts the next action by conditioning a transformer on past states, actions, and desired returns. The desired returns are defined as returns-to-go (RTG) gt “ t1“t rt1, the future returns that this action is intended to achieve. Therefore, the trajectory is represented by τ “ă s1, a1, g1, . . . , sT , aT , gT ą. In practice, we use a causally masked transformer architecture such as GPT (Radford et al., 2019) to process this sequence and predict the actions by observing the past K timesteps consisting of 3K tokens. DT and its variants have been shown to be more stable and robust to optimize due to the simplicity of loss function; easier to scale to more complex settings such as environments with high-dimensional actions or states, and agents with broad capabilities such as multitask settings (Lee et al., 2022). Hence, we adopt Decision Transformers (Chen et al., 2021) as the representative base algorithm on which we build our work.\n\nIn follow-up work, Emmons et al. (2021) extend DT and shows that even multi-layer perceptrons conditioned on the average returns-to-go can achieve similar performance without the use of transformers. They call their model Reinforcement Learning Via Supervised Learning (RvS). However, RvS is generally not very stable when conditioned on very large returns, unlike DT.\n\n5.1 MULTI-OBJECTIVE REINFORCEMENT LEARNING VIA SUPERVISED LEARNING\n\nIn PEDA, our goal is to train a single preference-conditioned agent for offline MORL. By including preference conditioning, we enable the policy to be trained on arbitrary offline data, including trajectories collected from behavioral policies that are associated with alternate preferences. To parameterize our policy agents, we extend the DT and RvS architectures to include preference tokens and vector-valued returns. We refer to such preference-conditioned extensions of these architectures as MODT(P) and MORVS(P) respectively, which we describe next.\n\nPreference Conditioning. Naively, we can easily incorporate the preference ω into DT by adding this token for each timestep and feeding it a separate embedding layer. However, empirically we find that such a model design tends to ignore ω and the correlation between the preferences and predicted actions is weak. Therefore, we propose to concatenate ω to other tokens before any layers À\nÀ in MODT(P). Concretely, we define s ̊ “ s denotes the concatenation operator. Hence, triples of s ̊, a ̊, g ̊ form the new trajectory. As for MORVS(P), we concatenate the preference with the states and the average RTGs by default and the network interprets everything as one single input.\n\nω, and g ̊ “ g\n\nω, a ̊ “ a\n\nω where\n\nÀ\n\nÀ\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nř\n\nMulti-Objective Returns-to-Go. Similar to RTG for the single objective case, we can define vector-valued RTG as gt “ t1“t rt1 Given a preference vector ω, we can scalarize the total returns-to-go as ˆgt “ ωT gt. In principle, the scalarized RTG ˆgt can be recovered given the preference vector ω and the vector-valued RTG gt. However, empirically we find that directly feeding MODT/MORVS with the preference-weighted RTG vector gt d ω is slightly preferable for stable training, where d denotes the elementwise product operator.\n\nT\n\nAnother unique challenge in the MORL setting concerns the scale of different objectives. Since different objectives can signify different physical quantities (e.g., energy and speed), the choice of scaling can influence policy optimization. We adopt a simple normalization scheme, where the returns for each objective are normalized by subtracting the minimum observed value for that objective and dividing it by the range of values (max-min). Note that the maximum and minimum are computed based on the offline dataset and hence, they are not necessarily the true min/max objective values. Post this normalization, the values for every objective in the trajectory are on the same scale between 0 and 1. For evaluating the hypervolume and sparsity, we use the unnormalized values so that we can make comparisons across different datasets that may have different min/max boundaries.\n\nTraining. We follow a simple supervised training procedure where we train the policies on randomly sampled mini-batches with MSE loss (for continuous actions). In MODT and MODT(P), the input states, actions, and returns-to-go (with concatenated preferences) are treated as tokens and embedded through one layer of MLP. We apply a layer of MLP and Tanh on the last hidden state of GPT-2 transformer to predict next action. In MORVS and MORVS(P), we use only information from the current timestep and MLP layers to predict the next action.\n\n6 EXPERIMENTS\n\nIn this section, we evaluate the performance of PEDA on D4MORL benchmark. First, we investigate the benefits of preference conditioning by evaluating on decision transformers (DT) and RvS (MORVS) where no preference information is available and we scalarize multi-objective vector returns into weighted sums. We denote our methods with preference conditioning as MODT(P) and MORVS(P). Second, we compare our methods with classic imitation learning and temporal difference learning algorithms with preference conditioning.\n\nImitation learning. Imitation learning simply uses supervised loss to train a mapping from states (w/ or w/o concatenating preferences) to actions. We use behavioral cloning (BC) here and train multi-layer MLPs as models named BC (w/o preference) and BC(P) (w/ preference).\n\nTemporal difference learning. Conservative Q-Learning (CQL) (Kumar et al., 2020) is the stateof-the-art standard offline RL method, which learns a conservative Q-function f : S ˆ A Ñ R through neural networks. We modify the network architecture such that it also takes preference vectors as inputs to learn a preference-conditioned Q-function f ̊ : S ˆ A ˆ Ω Ñ R. We denote this method as CQL(P).\n\n6.1 MULTI-OBJECTIVE OFFLINE BENCHMARK\n\nTable 1: Hypervolume performance on High-H-Expert dataset. PEDA variants MODT(P) and MORVS(P) always approach the expert behavioral policy. (B: Behavioral policy)\n\nEnvironments\n\nMO-Ant (106) MO-HalfCheetah (106) MO-Hopper (107) MO-Hopper-3obj (1010) MO-Swimmer (104) MO-Walker2d (106)\n\nB\n\n6.32 5.79 2.09 3.73 3.25 5.21\n\nMODT(P) MORVS(P) BC(P)\n\nCQL(P) MODT\n\nMORVS\n\nBC\n\nCQL\n\n6.21 ̆.01 5.73 ̆.00 2.00 ̆.02 3.38 ̆.05 3.15 ̆.02 4.89 ̆.05\n\n6.41 ̆.01 5.78 ̆.00 2.02 ̆.02 3.42 ̆.10 3.24 ̆.00 5.14 ̆.01\n\n4.88 ̆.17 5.54 ̆.05 1.23 ̆.10 2.29 ̆.07 3.21 ̆.00 3.74 ̆.11\n\n5.76 ̆.10 5.63 ̆.04 0.33 ̆.39 0.78 ̆.24 3.22 ̆.08 3.21 ̆.32\n\n5.52 ̆.16 5.59 ̆.03 1.68 ̆.03 1.05 ̆.43 2.49 ̆.19 0.65 ̆.46\n\n5.52 ̆.02 4.19 ̆.74 1.73 ̆.07 2.53 ̆.06 3.19 ̆.01 5.10 ̆.02\n\n0.84 ̆.60 1.53 ̆.09 0.28 ̆.21 0.06 ̆.02 1.68 ̆.38 0.07 ̆.02\n\n3.52 ̆.45 3.78 ̆.46 0.02 ̆.02 0.00 ̆.00 2.08 ̆.08 0.82 ̆.62\n\nHypervolume. We compare hypervolume of our methods with all baselines on expert datasets in Table 1 as well as amateur dataset in Table 2. For the two-objective environments, we evaluate the\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Hypervolume performance on High-H-Amateur dataset. PEDA variants still approach or even exceed the behavioral policy even when a considerable portion of data is suboptimal. MODT(P) and MORVS(P) still present to be the strongest models and outperform other baselines. (B: Behavioral policy)\n\nEnvironments\n\nMO-Ant (106) MO-HalfCheetah (106) MO-Hopper (107) MO-Hopper-3obj (1010) MO-Swimmer (1) MO-Walker2d (104)\n\nB\n\n5.61 5.68 1.97 3.09 2.11 4.99\n\nMODT(P) MORVS(P) BC(P)\n\nCQL(P) MODT\n\nMORVS\n\nBC\n\nCQL\n\n5.92 ̆.04 5.69 ̆.01 1.81 ̆.05 1.04 ̆.16 1.67 ̆.22 3.10 ̆.34\n\n6.07 ̆.02 5.77 ̆.00 1.76 ̆.03 2.77 ̆.24 2.79 ̆.03 4.98 ̆.01\n\n4.37 ̆.06 5.46 ̆.02 1.35 ̆.03 2.42 ̆.18 2.82 ̆.04 3.42 ̆.42\n\n5.62 ̆.23 5.54 ̆.02 1.64 ̆.01 0.59 ̆.42 1.69 ̆.93 1.78 ̆.33\n\n4.88 ̆.60 5.51 ̆.01 1.54 ̆.08 1.64 ̆.23 0.96 ̆.19 3.76 ̆.34\n\n4.37 ̆.56 4.66 ̆.05 1.57 ̆.01 1.30 ̆.22 2.93 ̆.03 4.32 ̆.05\n\n2.34 ̆.15 2.92 ̆.38 0.01 ̆.01 0.03 ̆.01 0.46 ̆.15 0.91 ̆.36\n\n2.80 ̆.68 4.41 ̆.08 0.00 ̆.06 0.10 ̆.16 0.74 ̆.47 0.76 ̆.81\n\nTable 3: Sparsity (Ó) performance on High-H-Expert dataset. MODT(P) and MORVS(P) have a lower density. BC(P) also has a competitive sparsity in smaller environments such as Swimmer.\n\nEnvironments\n\nMODT(P) MORVS(P)\n\nBC(P)\n\nCQL(P)\n\nMO-Ant (ˆ104) MO-HalfCheetah (ˆ104) MO-Hopper (ˆ105) MO-Hopper-3obj (ˆ105) MO-Swimmer (ˆ1) MO-Walker2d (ˆ104)\n\n8.26 ̆2.22 1.24 ̆0.23 16.3 ̆10.6 1.40 ̆0.44 15.0 ̆7.49 0.99 ̆0.44\n\n6.50 ̆0.81 0.67 ̆0.05 3.03 ̆0.36 2.72 ̆1.93 4.39 ̆0.07 3.22 ̆0.73\n\n46.2 ̆16.4 1.78 ̆0.39 52.5 ̆4.88 0.72 ̆0.09 4.50 ̆0.39 75.6 ̆52.3\n\n0.58 ̆0.10 0.10 ̆0.00 2.84 ̆2.46 2.60 ̆3.14 13.6 ̆5.31 6.23 ̆10.7\n\nmodels on 501 equally spaced preference points in the range [0, 1]; on the three-objective environment MO-Hopper-3obj, models are evaluated on 325 equally spaced points. Each point is evaluated 5 times with random environment re-initialization, and the median value is recorded. Finally, all the results are based on 3 random seeds and we report the mean performance along with the standard error. In Table 1 and Table 2, we can see that MODT(P) and MORVS(P) outperform other baselines and has a relatively very low standard error. Also, PEDA variants including MODT(P) and MORVS(P) approaches the behavioral policy upper-bound.\n\nSparsity. We also evaluate sparsity performance. Since sparsity comparison is only meaningful between models that are sensitive to preference and have a relatively similar hypervolume performance, we only show results for models that concatenate preference. Overall, MORVS(P) has the lowest sparsity in most environments, while at the same time featuring an outstanding hypervolume.\n\n6.2 ABLATION STUDY\n\nPareto front approximation. We ablate how well the MODT(P) and MORVS(P) can approximate the Pareto front through conditioning on different preference points. We show the results in Figure 4, where we can see that the models can approximate the Pareto front while having some dominated points colored in pink mostly in the MO-Hopper and MO-Walker2d environments. The results are based on the average of 3 seeds, and the full plot can be found in Appendix G.\n\nTable 4: Sparsity (Ó) performance on High-H-Amateur dataset. We can see that all models still have a similar or stronger sparsity performance when trained on amateur datasets. Furthermore, MORVS(P) still presents the strongest performance. While BC(P) has strong performance in MOHopper-3obj and MO-Swimmer, it also fails to give a dense solution in other environments and has a higher standard error.\n\nEnvironments\n\nMODT(P)\n\nMORVS(P)\n\nBC(P)\n\nCQL(P)\n\nMO-Ant (ˆ104) MO-HalfCheetah (ˆ104) MO-Hopper (ˆ105) MO-Hopper-3obj (ˆ105) MO-Swimmer (ˆ1) MO-Walker2d (ˆ104)\n\n8.72 ̆.77 1.16 ̆.42 1.61 ̆.29 10.23 ̆2.78 2.87 ̆1.32 164.2 ̆13.5\n\n5.24 ̆.52 0.57 ̆.09 3.50 ̆1.54 1.03 ̆.11 1.03 ̆.20 1.94 ̆.06\n\n25.9 ̆16.4 2.22 ̆.91 2.42 ̆1.08 0.87 ̆.29 5.05 ̆1.82 53.1 ̆34.6\n\n1.06 ̆.28 0.45 ̆.27 3.30 ̆5.25 2.00 ̆1.72 8.87 ̆6.24 7.33 ̆5.89\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: We show that MODT(P) and MORVS(P) can be good approximator to the Pareto front. There are relatively more dominated points in MO-Hopper and MO-Walker2d colored in red.\n\nFigure 5: We show that MORVS(P) Model can follow the given target reward that is within the dataset’s minimum and maximum record. The plots are for all the two-objective environments. In addition, MO-Hopper and MO-Walker2d present to be the most challenging environments for PEDA variants, featuring more dominated solutions than other environments.\n\nReturn distribution. We ablate how well MODT(P) and MORVS(P) follow their given target return, based on a normalized and weighted value. We present the results in Figure 5 for MORVS(P) under High-H-Expert datasets and refer to Appendix H for full settings. Here, we see that the models follow the oracle line nicely when conditioned on the target within the dataset distribution, and generalize to targets outside of the dataset distribution as well.\n\n7 CONCLUSION\n\nWe proposed a new problem setup for offline Multi-Objective Reinforcement Learning to scale Pareto-Efficient decision-making using offline datasets. To characterize progress, we introduced D4MORL, a dataset benchmark consisting of offline datasets generated from behavioral policies of different fidelities (expert/amateur) and rolled out under preference distributions with varying entropies (high/medium/low). Then, we propose PEDA, a family of offline MORL policy optimization algorithms based on decision transformers. To our knowledge, the PEDA variants are the first offline MORL policies that support continuous action and preference spaces. We showed that by concatenating and embedding preferences together with other inputs, our policies can effectively approximate the Pareto front of the underlying behavioral policy as measured by the hypervolume and sparsity metrics. Our proposed family includes MLP and transformer-based variants, viz. the MORVS(P) and MODT(P), with MORVS(P) performing the best overall. In some scenarios, the learned policies can also generalize to higher target rewards that exceed the data distribution.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nOur code is available at: https://github.com/baitingzbt/PEDA.\n\nACKNOWLEDGEMENTS\n\nAG’s research is supported by a Meta Research Award and a Cisco grant.\n\nREFERENCES\n\nAbbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Francis Song, Martina Zambelli, Murilo Martins, Nicolas Heess, Raia Hadsell, and Martin Riedmiller. A distributional view on multi-objective policy optimization. In International Conference on Machine Learning, pp. 11–22. PMLR, 2020.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.\n\nXi Chen, Ali Ghadirzadeh, M ̊arten Bj ̈orkman, and Patric Jensfelt. Meta-learning for multi-objective reinforcement learning. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 977–983. IEEE, 2019.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning, 2020.\n\nConor F Hayes, Roxana R ̆adulescu, Eugenio Bargiacchi, Johan K ̈allstr ̈om, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M Zintgraf, Richard Dazeley, Fredrik Heintz, et al. A practical guide to multi-objective reinforcement learning and planning. Autonomous Agents and Multi-Agent Systems, 36(1):26, 2022.\n\nSandy Huang, Abbas Abdolmaleki, Giulia Vezzani, Philemon Brakel, Daniel J Mankowitz, Michael Neunert, Steven Bohez, Yuval Tassa, Nicolas Heess, Martin Riedmiller, et al. A constrained multi-objective reinforcement learning framework. In Conference on Robot Learning, pp. 883– 893. PMLR, 2022.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273–1286, 2021.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-\n\nlearning. arXiv preprint arXiv:2110.06169, 2021.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nRomain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with baseline bootstrapping. In International conference on machine learning, pp. 3652–3661. PMLR, 2019.\n\nAlessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In\n\nICML-27th international conference on machine learning, pp. 599–606. Omnipress, 2010.\n\nKuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. arXiv preprint arXiv:2205.15241, 2022.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-\n\nrial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJiGuan G Lin. On min-norm and min-max methods of multi-objective optimization. Mathematical\n\nprogramming, 103(1):1–33, 2005.\n\nHossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon Whiteson. Multi-objective\n\ndeep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016.\n\nSimone Parisi, Matteo Pirotta, and Marcello Restelli. Multi-objective reinforcement learning through continuous pareto manifold approximation. Journal of Artificial Intelligence Research, 57:187–227, 2016.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n\nDiederik M Roijers, Shimon Whiteson, Frans A Oliehoek, et al. Linear support for multi-objective coordination graphs. In AAMAS’14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, pp. 1297–1304, 2014.\n\nYee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. Advances in neural information processing systems, 30, 2017.\n\nPhilip S Thomas, Joelle Pineau, Romain Laroche, et al. Multi-objective spibb: Seldonian offline policy improvement with safety constraints in finite mdps. Advances in Neural Information Processing Systems, 34:2004–2017, 2021.\n\nOldrich Vasicek. A test for normality based on sample entropy. Journal of the Royal Statistical\n\nSociety: Series B (Methodological), 38(1):54–59, 1976.\n\nPauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 17(3):261–272, 2020.\n\nKazuyoshi Wakuta. Vector-valued markov decision processes and the systems of linear inequalities.\n\nStochastic processes and their applications, 56(1):159–169, 1995.\n\nAaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning, pp. 1015–1022, 2007.\n\nRunzhe Wu, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. Offline constrained multi-objective reinforcement learning via pessimistic dual value iteration. In Advances in Neural Information Processing Systems, volume 34, pp. 25439–25451, 2021.\n\nJie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik. Prediction-guided multi-objective reinforcement learning for continuous robot control. In Proceedings of the 37th International Conference on Machine Learning, 2020.\n\nRunzhe Yang, Xingyuan Sun, and Karthik Narasimhan. A generalized algorithm for multi-objective In Advances in Neural Information Processing\n\nreinforcement learning and policy adaptation. Systems 32. 2019.\n\nHuixin Zhan and Yongcan Cao. Relationship explainable multi-objective optimization via vector\n\nvalue function based reinforcement learning. arXiv preprint arXiv:1910.01919, 2019.\n\nQingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposi-\n\ntion. IEEE Transactions on evolutionary computation, 11(6):712–731, 2007.\n\nQinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer.\n\nIn International\n\nConference on Machine Learning, pp. 27042–27059. PMLR, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA ENVIRONMENT DESCRIPTION\n\nAll environments are the same as in Xu et al., 2020, except for when resetting the environment, each parameter is uniformly sampled from the rx ́ 10 ́3, x ` 10 ́3s with x being the default value. Except we always reset height as 1.25 for MO-Hopper and MO-Hopper-3objsince this parameter directly relates to the reward function. All environments have a max episode length of 500 steps per trajectory, but the agent may also die before reaching the maximum length.\n\nA.1 MO-ANT\n\nThe two objectives in MO-Ant are achieved distance in x and y axes respectively, denoted as r “ rrvx\n\n, rvy\n\nt s⊺.\n\nt\n\nConsider the position of the agent is represented as pxt, ytq at time t and takes the action at. The agent has a fixed survival reward rs “ 1.0, dt “ 0.05, and an action cost of ra “ 1 k. The rewards are calculated as:\n\nk a2\n\nř\n\n2\n\nt “ pxt ́ xt ́1q { dt ` rs ́ ra rvx rvy t “ pyt ́ yt ́1q { dt ` rs ́ ra\n\n(1)\n\nA.2 MO-HALFCHEETAH\n\nThe two objectives in MO-HalfCheetah are running speed, and energy saving, denoted as r “ rrv\n\nt , re\n\nt s⊺.\n\nConsider the position of the agent is represented as pxt, ytq at time t and takes the action at. The agent has a fixed survival reward rs “ 1.0, fixed dt “ 0.05, and an action cost of ra “ k. The rewards are calculated as:\n\nk a2\n\nř\n\nt “ mint4.0, pxt ́ xt ́1q { dtu ` rs rv t “ 4.0 ́ ra ` rs re\n\n(2)\n\nA.3 MO-HOPPER\n\nThe two objectives in MO-Hopper are running and jumping, denoted as r “ rrr, rjs⊺.\n\nConsider the position of the agent is represented as pxt, htq at time t and takes the action at. The agent has a fixed survival reward rs “ 1.0, a fixed initial height as hinit “ 1.25, a fixed dt “ 0.01, and an action cost of ra “ 2 ˆ 10 ́4\n\nk. The rewards are calculated as:\n\nk a2\n\nř\n\nrr t “ 1.5 ˆ pxt ́ xt ́1q { dt ` rs ́ ra rj t “ 12 ˆ pht ́ hinitq { dt ` rs ́ ra\n\n(3)\n\nA.4 MO-HOPPER-3OBJ\n\nThe physical dynamics are the same in MO-Hopper and MO-Hopper-3obj, while this environment has 3 objectives: running, jumping, and energy saving. The rewards are denoted as r “ rrr, rj, res⊺.\n\nConsider the position of the agent is represented as pxt, htq at time t and takes the action at. The agent has a fixed survival reward rs “ 1.0, a fixed initial height as hinit “ 1.25, a fixed dt “ 0.01, and an action cost of ra “ k. The rewards are calculated as:\n\nk a2\n\nř\n\nrr t “ 1.5 ˆ pxt ́ xt ́1q { dt ` rs rj t “ 12 ˆ pht ́ hinitq { dt ` rs t “ 4.0 ́ ra ` rs re\n\n12\n\n(4)\n\nPublished as a conference paper at ICLR 2023\n\nA.5 MO-SWIMMER\n\nThe two objectives in MO-Swimmer are speed and energy saving, denoted as r “ rrv, res⊺.\n\nConsider the position of the agent is represented as pxt, ytq at time t and takes the action at. The agent has a fixed dt “ 0.05, and an action cost of ra “\n\nk. The rewards are calculated as:\n\nk a2\n\nř\n\nrv t “ pxt ́ xt ́1q { dt t “ 0.3 ́ 0.15 ˆ ra re\n\n(5)\n\nA.6 MO-WALKER2D\n\nThe objectives in MO-Walker2d are speed and energy saving, denoted as r “ rrv, res⊺.\n\nConsider the position of the agent is represented as pxt, ytq at time t and takes the action at. The agent has a fixed survival reward rs “ 1.0, a fixed dt “ 0.008, and an action cost of ra “ k a2 k. The rewards are calculated as:\n\nř\n\nrv t “ pxt ́ xt ́1q { dt ` rs t “ 4.0 ́ ra ` rs re\n\n(6)\n\nTo uniformly sample the High-H data from the entire preference space, the problem is equivalent to sampling from a n-dimensional simplex, where n is the number of objectives. The resulting sampling is:\n\nωhigh „ ||fexpp ̈ , λ “ 1q||1\n\n(7)\n\nWe take the 1-norm following the exponential distribution to make sure each preference add up to 1. When Ω ̊ ‰ Ω, we perform rejection sampling to restrict the range.\n\nTo sample the Med-H and Low-H data, we first sample α from a non-negative uniform distribution, then sample the corresponding Dirichlet preference. Here, we sample a different alpha to make sure the center of the Dirichlet changes and thus allows more variation.\n\nωmed „ fDirichletpαq ; where α „ Unifp0, 106q ωlow „ fDirichletpαq ; where α „ Unifp1{3 ˆ 106, 2{3 ˆ 106q\n\n(8)\n\nFor sampling from behavioral policy consists of a group of single-objective policies πβ “ tπ1, . . . , πBu with B being the total number of candidate policies, we recommend first find the expected unweighted raw rewards Gπ1 , . . . , GπB . Then, find the estimated ˆωπ1 , . . . , ˆωπB by letting ˆωπb , which represents the estimated preference on ith objective of bth candidate\n\nπb\n\ni “\n\nG iř n\nj“1 G\n\nπb j\n\npolicy. For a sampled preference ω „ Ω ̊, use the policy that provides the smallest euclidean distance dpω, ˆωπb q. Empirically, this means picking the candidate policy that has the expected reward ratio closest to ω.\n\nB DATASET DETAILS\n\nTo uniformly sample the High-H data from the entire preference space, we can equivalently sample from a n-dimensional simplex, where n is the number of objectives. The resulting sampling scheme is:\n\nωhigh „ ||fexpp ̈ , λ “ 1q||1\n\n(9)\n\nThe 1-norm following the exponential distribution makes sure each preference vector have entries add up to 1. When Ω ̊ ‰ Ω, we perform rejection sampling to restrict the range.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: A comprehensive view of the dataset. All datasets have a 500 maximum step per trajectory, and 50K trajectories are collected under each setting. As indicated by average step per trajectory, we can see Amateur trajectories are always shorter or same as Expert, thus leading to a lower return.\n\nMO-Ant MO-HalfCheetah MO-Hopper MO-Hopper-3obj MO-Swimmer MO-Walker2d\n\nmax step per traj 500 500 500 500 500 500\n\nexpert avg. step per traj 500 499.91 499.94 499.99 500 500\n\namateur avg. step per traj 500 482.11 387.72 442.87 500 466.18\n\ntrajectories per dataset 50K 50K 50K 50K 50K 50K\n\nTo sample the Med-H and Low-H data, we first sample α from a non-negative uniform distribution, then sample the corresponding Dirichlet preference. Here, we sample a different alpha to make sure the mode of the Dirichlet changes and thus allows more variation.\n\nωmed „ fDirichletpαq ; where α „ Unifp0, 106q ωlow „ fDirichletpαq ; where α „ Unifp1{3 ˆ 106, 2{3 ˆ 106q\n\n(10)\n\nSince our behavioral policy is consists of a group of single-objective policies πβ “ tπ1, . . . , πBu with B being the total number of candidate policies, we first find the expected unweighted raw rewards Gπ1, . . . , GπB . Then, we find the estimated preferences ˆωπ1, . . . , ˆωπB by letting ˆωπb i “ on ith objective of bth candidate policy. For each sampled preference ω „ Ω ̊ following (9)\n\nπb\n\nG iř n\nj“1 G\n\nπb j\n\nor (10), we sample a complete trajectory using the single-objective behavioral policy that provides the smallest euclidean distance min dpω, ˆωπb q. Empirically, this means picking the candidate policy that has the expected reward ratio closest to ω.\n\nC EXPERT & AMATEUR DATASETS\n\nIn Expert collection, we sample trajectories using the fully-trained behavioral policy πβ. In this paper, we use PGMORL by Xu et al., 2020 as our behavioral policy πβ\n\naexpert\n\nt`1 “ πβpa|s “ st, ω “ ωtq\n\n(11)\n\nIn the Amateur collection, the policies has a 35% chance being stochastic on top of the expert collection. Actions has a chance being stochastic, during which it is scaled from the expert action, as following:\n\naamateur\n\nt`1 “\n\n\"\n\nt`1\n\naexpert aexpert\n\n35% t`1 ˆ Unifp0.35, 1.65q 65%\n\n(12)\n\nIn the MO-Swimmer environment only, we let actions has a 35% chance to be a uniform random sample from the entire action space rather than being the same as expert to increase variance and achieve a performance similar to amateur. The resulting strategy for MO-Swimmer is:\n\naamateur\n\nt`1 “\n\n\"\n\nUnifpAq aexpert\n\n35% t`1 ˆ Unifp0.35, 1.65q 65%\n\n(13)\n\nD FINDING APPROPRIATE MULTI-OBJECTIVE RTG\n\nIn Decision Transformer Chen et al. (2021) and Emmons et al. (2021), RTG denotes the future desired reward. In MORL, however, designing appropriate multi-objective RTG is necessary. On top\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nof discounting each objective’s desired reward separately, we empirically find that since some objectives are inherently conflicting, setting RTG high for one objective means we should accordingly lower RTG for other objectives (i.e. we shouldn’t use maximum RTG for both). In this way, our test-time RTG can follow closer to the training distribution.\n\nIn this paper, we use linear regression G “ f pωq to find corresponding RTG conditioned on the given preference. Figure 6 demonstrates the weighted RTG of the “running” objective as a function of its preference in MO-Hopper where the conflicting objectives are “running” and “jumping”. It is clear that RTG closely correlates with the conditioned preference for running and we should adjust the initial RTG during test-time accordingly.\n\nFinally, we only use the learned linear regression model from Expert dataset. This is because regression models fitted on sub-optimal data can easily produce an RTG lower than optimal. In practice, we can easily achieve a similar result by training the regression model only on the best-performing trajectories for respective preferences. Other regression or clustering methods to find appropriate RTG can also work, and we leave it as future works especially when not assuming linearly weighted objectives.\n\nE TRAINING DETAILS\n\nFigure 6: RTG fitted through linear regression (lr) models stay closer to the dataset distribution. Furthermore, linear regression models can also efficiently generalize to unseen preferences during test time.\n\nIn this section, we list our hyper-parameters and model details. In specific, we use the same hyperparameters for all algorithms, except for the learning rate scheduler and warm-up steps. In MODT family, inputs are first embedded by a 1-layer fully-connected network, and n layer represents the number of transformer blocks; in BC family, n layer represents the number of MLP layers to embed each input; in MORVS and MORVS(P), we leverage the same embedding strategy in Emmons et al. (2021). Additionally, we consider MORVS and MORVS(P) both have context length of 1 because they only use the current state to predict the next action, whereas MODT and BC use the past 20.\n\nE.1 PARAMETERS\n\nHyperparameter\n\nMODT MORvS\n\nContext Length - K Batch Size Hidden Size Learning Rate Weight Decay Dropout n layer Optimizer Loss Function LR Scheduler Warm-up Steps Activation\n\n1 64 512 1e-4 1e-3 0.1 3\nAdamW MSE None N/A ReLU\n\n20\n\nlambda 10000\n\n15\n\nBC\n\n20\n\nlambda 4000\n\nPublished as a conference paper at ICLR 2023\n\nE.2 TRAINING STEPS\n\nDataset Name\n\nMODT Steps RvS/BC Steps\n\nMO-Ant MO-HalfCheetah MO-Hopper MO-Hopper-3obj MO-Swimmer MO-Walker2d\n\n20K 80K 400K 400K 260K 360K\n\n200K 200K 200K 200K 200K 200K\n\nE.3 OTHER ATTEMPTED ARCHITECTURES FOR MODT AND MODT(P)\n\nWe tried the following MODT architectures in our preliminary experiments. We picked Case 4 eventually as it gave the best performance in our experiments.\n\n1. Consider ω as an independent token of the transformer.\n\n2. Train a separate embedding for ω, concatenate the embeddings to get fφspsq\n\nÀ\n\nÀ\n\nfφapaq\n\nfφω pωq, and fφg pgq\n\nfφω pωq then pass into the transformer.\n\nÀ\n\nfφω pωq,\n\n3. Add another MLP layer on top of Case 2 after concatenation, then pass output into the\n\ntransformer.\n\n4. Concatenate ω to other tokens before any layers. This means we have s ̊ “ s\n\nÀ\n\nÀ\n\na ̊ “ a\n\nω, and g ̊ “ g\n\nω.\n\nÀ\n\nω,\n\nF OTHER EVALUATION METRICS\n\nAmong a variety of metrics for MORL, we use Hypervolume (HV) and Sparsity (SP) to benchmark models for several reasons. First, many metrics such as the ε-metric require prior knowledge of the true Pareto Fronts, which are not available for our MuJoCo Environments. Second, we only assume linear reward function and cannot collect real-time user feedback, thus utility-based metrics such as expected utility metric (EUM) are not applicable. Finally, using the same metric as in the original behavioral policy paper facilitate algorithm comparisons.\n\nG PARETO SET VISUALIZATIONS\n\nWe present the Pareto Set visualizations for all of our models trained under each High-H dataset in Figure 7. Each point in each subplot is based on the average result of 3 seeds. In 2 objective environments, we evaluate the model using 501 equally spaced preference points. In 3 objective environments, we use 351 equally spaced preference points instead. Since the environments are stochastically initialized, we evaluate 5 times at each preference point and take the mean value. This makes each point the average value of 15 runs. We here allow a small tolerance for coloring the dominated points.\n\nIf a preference point is within the achievable preference Ω ̊ but the solution is dominated, we color it in red. Since our models are conditioned on continuous preference points and environments are initialized stochastically, we give a small tolerance (3%-8%) for points to be colored in blue. The hypervolume and sparsity metrics, on the other hand, are based on strictly undominated solutions without tolerance.\n\nH MEDIUM & LOW ENTROPY DATASET TRAINING\n\nWe train on the Medium-Entropy and Low-Entropy datasets for the MO-HalfCheetah environment. Overall, models have a similar performance under Med-H and High-H datasets but suffer when only trained on Low-H. We present the results in Table 6, in which we illustrate that the Low-H dataset has a worse expert and amateur performance due to reduced variability on preference. However, MODT(P) and MORVS(P) are still able to get close to or exceed in hypervolume on all\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Pareto Visualization for all PEDA variants and baselines on High-H datasets. Notice that MO-Hopper and MO-Walker2d are more challenging and have significantly more dominated points for all variants. In other environments, however, the PEDA variants produce better results while other baselines fail at higher chances. Since BCwithout preference is completely single objective, we don’t show the result here.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: We ablate how well PEDA variants perform and generalize under a different preference distribution in the MO-HalfCheetah environment. We can see that PEDA can still perform well when trained under the partially-clustered Med-H dataset. However, performance drops when it is trained under the entirely clustered Low-H dataset. (B: Behavioral Policy)\n\nDataset (ˆ106)\n\nMed-H-Amateur Low-H-Amateur High-H-Amateur\n\nB\n\n5.69 4.21 5.68\n\nMODT(P) MORVS(P)\n\nBC(P)\n\nMODT\n\nMORVS\n\nBC\n\n5.68 ̆.01 4.86 ̆.05 5.69 ̆.01\n\n5.77 ̆.01 4.80 ̆.03 5.77 ̆.00\n\n5.44 ̆.26 4.75 ̆.04 5.46 ̆.02\n\n5.61 ̆.01 4.72 ̆.05 5.54 ̆.02\n\n4.67 ̆.03 4.39 ̆.04 4.66 ̆.05\n\n3.21 ̆.33 4.05 ̆.07 2.92 ̆.38\n\nTable 7: We ablate the importance of using multi-dimensional rtgs instead of a one-dimensional rtgs by taking the weighted sum of objectives on the MO-HalfCheetah environment. We see multiobjective rtgs provide a variance reduction for MORVS(P) and hypervolume performance improvement in other models. (B: Behavioral Policy)\n\nSetting\n\nDataset (ˆ106)\n\nmo rtg 1-dim rtg\n\nHigh-H-Expert High-H-Expert\n\nB\n\n5.79 5.79\n\nMODT(P) MORVS(P) MODT\n\nMORVS\n\n5.73 ̆.00 5.70 ̆.02\n\n5.78 ̆.00 5.78 ̆.00\n\n5.59 ̆.03 4.43 ̆.09\n\n4.19 ̆.74 2.89 ̆.06\n\ndatasets, which showcases the effectiveness of PEDA as an efficient MORL policy. Results are based on an average of 3 seeds with the standard error given.\n\nI TRAINING WITH 1-DIM RTG\n\nWe attempted to train MODT and RvS with 1-dim return-to-go rather than a separate rtg for each objective. According to results on MO-HalfCheetah and the High-H datasets in 7, using multidimensional RTG enhances the performance of MODT(P), and are about the same for MORVS(P) when preference is concatenated to states. However, it reduces standard error significantly in both MODT(P) and MORVS(P). In the naive models when preferences are not concatenated to states, using a multi-dimensional RTG helps to achieve a much more competitive hypervolume. We thus believe multi-dimensional RTG conveys important preference information when the model doesn’t directly take preference as an input. Results are based on an average of 3 seeds with the standard error given.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis manuscript did two things. One is to propose a new dataset that collects agent trajectories from multiple independent agents with different preferences on the objectives. This dataset contains trajectories from both well-trained agents and semi-trained agents. HalfCheetah is a typical example where actions that are large in absolute value will consume more energy while the agent wants the cheetah to run as fast as possible. Two is to propose a decision transformer-like algorithm that handles an offline multi-objective dataset (which is possibly the one proposed). This algorithm is quite intuitive and the main idea is to condition its output on the preference of the newly tested task. Several experiments are presented.\n\n# Strength And Weaknesses\n\nPro:\n\n1. This paper introduces a new dataset that targets the multi-objective RL with offline data.\n\n2. This paper introduces a decision transformer-like algorithm which is intuitive and works in experiments.\n\nCons:\n\nThe baselines in the experiments seem lacking as only two methods (BC/CQL) are compared with. Is any offline RL and inverse RL approach relevant?\n\nQuestions\n\n1. One question is how the two contributions - namely the dataset and the algorithm - are entangled to each other. With the presentation of this paper one would expect the contributions to be independent, i.e. we expect the algorithm to work on other datasets as well. Is there any way to validate this? I understand a similar dataset might not be present in the community for now.\n\n2. Following the last question, is the dataset general enough to be an independent contribution? From the generation of this dataset I could many variants in the process. Should the community use this dataset in the future or it's good only for this work. \n\n3. What is the difference of this work and multi-objective inverse RL? I'm not seeing much difference despite the work follows the vein of offline RL mostly.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe presentation is in high clarity and quality. See the previous part for novelty. I have no tell on its reproducibility.\n\n# Summary Of The Review\n\nI think it's quite an interesting problem and the work is solid (intuitive, works well, and is presented well). I believe it's a borderline paper and I lean slightly towards acceptance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nORTHOREG: IMPROVING GRAPH-REGULARIZED MLPS VIA ORTHOGONALITY REGULARIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph Neural Networks (GNNs) are currently dominating in modeling graphstructure data, while their high reliance on graph structure for inference significantly impedes them from widespread applications. By contrast, Graph-regularized MLPs (GR-MLPs) implicitly inject the graph structure information into model weights, while their performance can hardly match that of GNNs in most tasks. This motivates us to study the causes of the limited performance of GR-MLPs. In this paper, we first demonstrate that node embeddings learned from conventional GR-MLPs suffer from dimensional collapse, a phenomenon in which the largest a few eigenvalues dominate the embedding space, when a linear encoder is used. As a result of this the expressive power of the learned node representations is constrained. We further propose ORTHO-REG, a novel GR-MLP model, to mitigate the dimensional collapse issue. Through a soft regularization loss on the correlation matrix of node embeddings, ORTHO-REG explicitly encourages orthogonal node representations and thus can naturally avoid dimensionally collapsed representations. Experiments on traditional transductive semi-supervised classification tasks and inductive node classification for cold-start scenarios demonstrate its effectiveness and superiority.\n\n1\n\nINTRODUCTION\n\nGraph Machine Learning (GML) has been attracting increasing attention due to its wide applications in many realworld scenarios, like social network analysis (Fan et al., 2019), recommender systems (van den Berg et al., 2017; Wu et al., 2019b), chemical molecules (Wang et al., 2021; Stärk et al., 2022) and biology structures. Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018; Xu et al., 2019) are currently the dominant models for GML thanks to their powerful representation capability through iteratively aggregating information from neighbors. Despite their successes, such an explicit utilization of graph structure information hinders GNNs from being widely applied in industry-level tasks. On the one hand, GNNs rely on layer-wise message passing to aggregate features from the neighborhood, which is computationally inefficient during inference, especially when the model becomes deep (Zhang et al., 2021). On the other hand, recent studies have shown that GNN models can not perform satisfactorily in cold-start scenarios where the connections of new incoming nodes are few or unknown (Zheng et al., 2021). By contrast, Multi-Layer Perceptrons (MLPs) involve no dependence between pairs of nodes, indicating that they can infer much faster than GNNs (Zhang et al., 2021). Besides, they can predict for all nodes fairly regardless of the numbers of connections, thus can infer more reasonably when neighborhoods are missing (Zheng et al., 2021). However, it remains challenging to inject the knowledge of graph structure information into learning MLPs.\n\nFigure 1: As an MLP model, our method performs even better than GNN models on Pubmed, but with a much faster inference speed. GRAND (Feng et al., 2020) is one of the SOTA GNN models on task. Circled markers denote MLP baselines, and squared markers indicate GNN baselines.\n\nOne classical and popular method to mitigate this issue is Graph-Regularized MLPs (GR-MLPs in short). Generally, besides the basic supervised loss (e.g., cross-entropy), GR-MLPs employ\n\n1\n\n2.55.07.510.012.515.017.520.0Inference Time (ms)6870727476788082Test AccuracyOrtho-Reg(ours)GLNNMLPLap-RegP-RegGCNGATGRANDUnder review as a conference paper at ICLR 2023\n\nan additional regularization term on the final node embeddings or predictions based on the graph structure (Ando & Zhang, 2006; Zhou et al., 2003; Yang et al., 2021; Hu et al., 2021). Though having different formulations, the basic idea is to make node embeddings/predictions smoothed over the graph structure. Even though these GR-MLP models can implicitly encode the graph structure information into model parameters, there is still a considerable gap between their performance compared with GNNs (Ando & Zhang, 2006; Yang et al., 2021). Recently, another line of work, GNN-to-MLP knowledge distillation methods (termed by KD-MLPs) (Zhang et al., 2021; Zheng et al., 2021), have been explored to incorporate graph structure with MLPs. In KD-MLPs, a student MLP model is trained using supervised loss and a knowledge-distillation loss from a well-trained teacher GNN model. Empirical results demonstrate that with merely node features as input, the performance of KD-MLPs can still match that of GNNs as long as they are appropriately learned. However, the 2-step training of KD-MLPs is undesirable, and they still require a well-trained GNN model as a teacher. This motivates us to rethink the failure of previous GR-MLPs to solve graph-related applications and study the reasons that limit their performance.\n\nPresented work: In this paper, we first demonstrate that node embeddings learned from existing GR-MLPs suffer from dimensional collapse (Hua et al., 2021; Jing et al., 2022), a phenomenon that the embedding space of nodes is dominated by the largest (a few) eigenvalue(s). Our theoretical analysis demonstrates that the dimensional collapse in GR-MLP is due to the irregular feature interaction caused by the graph Laplacian matrix (see Lemma 1). We then propose Orthogonality Regularization (ORTHO-REG in short), a novel GR-MLP model, to mitigate the dimensional collapse issue in semi-supervised node representation learning tasks. The key design of ORTHO-REG is to enforce an additional regularization term on the output node embeddings, making them orthogonal so that different embedding dimensions can learn to express various aspects of information. Besides, ORTHO-REG extends the traditional first-order proximity preserving target to a more flexible one, improving the model’s expressive power and generalization ability to non-homophily graphs. We provide a thorough evaluation for ORTHO-REG on various node classification tasks. The empirical results demonstrate that ORTHO-REG can achieve competitive or even better performance than GNNs. Besides, using merely node features to make predictions, ORTHO-REG can infer much faster on large-scale graphs and make predictions more reasonable for new nodes without connections. In Fig. 1 we present the performance of ORTHO-REG compared with GNNs and other MLPs on Pubmed, where ORTHO-REG achieves SOTA performance with the fastest inference speed.\n\nWe summarize our contributions as follows:\n\n1) We are the first to examine the limited representation power of existing GR-MLP models from the perspective of dimensional collapse. We provide theoretical analysis and empirical studies to justify our claims.\n\n2) To mitigate the dimensional collapse problem, we design a novel GR-MLP model named ORTHO-REG. ORTHO-REG encourages the node embeddings to be orthogonal through explicit soft regularization, thus can naturally avoid dimensional collapse.\n\n3) We conduct experiments on traditional transductive semi-supervised node classification tasks and inductive node classification under cold-start scenarios on public datasets of various scales. The numerical results and analysis demonstrate that by learning orthogonal node representations, ORTHO-REG can outperform GNN models on these tasks.\n\n2 BACKGROUNDS AND RELATED WORKS\n\n2.1 PROBLEM FORMULATION\n\nWe mainly study a general semi-supervised node classification task on a single homogeneous graph where we only have one type of node and edge. We denote a graph by G = (V, E), where V is the node set, and E is the edge set. For a graph with N nodes (i.e., |V| = N ), we denote the node feature matrix by X ∈ RN ×D, the adjacency matrix by A ∈ RN ×N . In semi-supervised node classification tasks, only a small portion of nodes are labeled, and the task is to infer the labels of unlabeled nodes using the node features and the graph structure. Denote the labeled node set by V L and the unlabeled node set by V U , then we have V L ∩ V U = ∅ and V L ∪ V U = V.\n\nDenote the one-hot ground-truth labels of nodes by ˆY ∈ RN ×C, and the predicted labels by Y . One can learn node embeddings H using node features X and adjacency matrix A, and use the\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nembeddings to generate predicted labels ˆY . For example, GNNs generate node representations through iteratively aggregating and transforming the embeddings from the neighbors and could be generally formulated as H = fθ(X, A). Then a linear layer is employed on top of node embeddings to predict the labels Y = gθ(H). The model could be trained in an end-to-end manner by optimizing the cross-entropy loss between predicted labels and ground-truth labels of labeled nodes: Lsup = lxent(Y L, ˆY L) = (cid:80) lxent(yi, ˆyi). Note that GNNs explicitly utilize the graph\n\ni∈V L\n\nstructure information through learning the mapping from node features and graph adjacency matrix to predicted labels. However, due to the limitations introduced in Sec. 1 (inefficiency at inference and poor performance for cold-start nodes), we seek to learn an MLP encoder, i.e., H = fθ(X) that only takes node features for making predictions.\n\n2.2 GRAPH-REGULARIZED MLPS\n\nGraph-Regularized MLPs (GR-MLPs in short) implicitly inject the graph knowledge to the MLP model with an auxiliary regularization term on the node embeddings/predictions over the graph structure (Zhou et al., 2003; Yang et al., 2021; Hu et al., 2021), whose objective function could be generally formulated as: L = Lsup + λLreg, where Lreg = l(H, A) or l(Y , A). The most representative graph regularization method, Graph Laplacian Regularization (Zhou et al., 2003; Ando & Zhang, 2006), enforces local smoothness of embeddings/predictions between two connected nodes: l(Y , A) = tr[Y ⊤LY ], where L = I − ̃A = I − D−1/2AD−1/2 is the (symmetric normalized) Laplacian matrix of the graph. Note that Y can be replaced with H if one would like to regularize node embeddings instead of predicted labels.\n\nLater works apply advanced forms of regularization, like propagation regularization (P-Reg, Yang et al. (2021)), contrastive regularization (Hu et al., 2021), etc. Regardless of the minor differences, they are all based on the graph homophily assumption that connected nodes should have similar representations/labels. With the graph structure information implicitly encoded into the model parameters, GR-MLPs can improve the representative power of MLP encoders. However, their performances are still hard to match compared to those of GNN models. Remark 1. (Differences from Graph-Augmented MLPs). Though sound similar, Graph-regularized MLPs(GR-MLPs) are totally different from Graph-augmented MLPs (GA-MLPs). Although trained with implicit graph structure regularization, GR-MLPs make predictions directly through the MLP model. By contrast, GA-MLPs, such as SGC (Wu et al., 2019a), APPNP (Klicpera et al., 2019), GFNN (NT & Maehara, 2019) and SIGN (Rossi et al., 2020) explicitly employs the graph structure to augment the node representation generated from an MLP model.\n\n2.3 DIMENSIONAL COLLAPSE\n\nDimensional collapse (also known as spectral collapse in some work (Liu et al., 2019)) is a phenomenon in representation learning where the embedding space is dominated by the largest a few singular values (other singular values decay significantly as the training step increases). As the actual embedding dimension is usually large, the dimensional collapse phenomenon prevents different dimensions from learning diverse information, limiting their representation power and ability to be linearly discriminated. Jing et al. (2022) has analyzed the dimensional collapse phenomenon from a theoretical perspective and attributed it to the effect of strong data augmentation and implicit regularization effect of neural networks (Arora et al., 2019; Ji & Telgarsky, 2019). Previous methods usually adopt whitening operation (Hua et al., 2021; Ermolov et al., 2021) to mitigate this issue, while such explicit whitening methods are usually computationally inefficient and thus are not applicable to GR-MLP where efficiency is much more important. In this paper, we demonstrate that node embeddings learned from conventional Graph-Regularized MLPs also suffer from dimensional collapse. We provide a theoretical analysis on how it is caused and develop a computationally efficient soft regularization term to mitigate it.\n\n3\n\nISSUE DETECTION: DIMENSIONAL COLLAPSE IN GR-MLPS\n\nIn this section, we investigate the reasons behind the weak representation power of previous GRMLPs. In short, we find that node embeddings learned through traditional GR-MLPs (e.g., with graph Laplacian regularization (Ando & Zhang, 2006)) suffer from dimensional collapse, a phenomenon\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nwhich is first investigated in contrastive self-supervised learning (He & Ozay, 2022; Hua et al., 2021; Jing et al., 2022). Generally speaking, dimensional collapse indicates that the embedding space is dominated by the largest few eigenvalues, limiting their representation powers. We then show that the dimensional collapse phenomenon does exist in the typical GR-MLP model, Graph Laplacian Regularization (Ando & Zhang, 2006), from both theoretical analysis and empirical justification. We provide further discussion on the impacts of dimensional collapse on downstream classification tasks in Appendix E. The objective function of Graph Laplacian Regularization for semi-supervised node classification tasks could be formulated as follows:\n\nL = lxent(Y L, ˆY L) + λtr[H ⊤LH] =\n\n(cid:88)\n\ni∈V L\n\nlxent(yi, ˆyi) + λ\n\n(cid:88)\n\ni,j∈V\n\nAij∥\n\nhi√ Dii\n\n−\n\nhj (cid:112)Djj\n\n∥2. (1)\n\nWe would like to study the effect of the Laplacian regularization term Lreg = tr[H ⊤LH] on H’s embedding space through analyzing the eigenspectra of its auto-correlation matrix C = {Ckk′}, ∈ Rd×d, where ckk′ is defined as:\n\nCkk′ =\n\n√\n\nΣkk′ ΣkkΣk′k′\n\n, and Σ =\n\n(hi − h)(hi − h)⊤ |V|\n\n(cid:88)\n\ni∈V\n\n(2)\n\nNote that h = (cid:80)|V| H, and we denote C’s eigenvalues in a descending order by {λC\n\ni=1 hi/|V| is the average node embedding vector, so Σ is the covariance matrix of\n\n1 , λC\n\n2 , · · · , λC\n\nD}.\n\nTheoretical Analysis. To simplify our analysis, we consider a simple single-layer Perceptron (linear) model as the encoder to learn node embeddings, i.e., H = XW (note that we validate the non-linear case empirically in the empirical justification part below), where W ∈ RF ×D is the weight matrix (we further assume F = D in this part for simplicity). The model (i.e., the weight matrix W ) is optimized using stochastic gradient descent. Then we have the following lemma on the evolvement of the weight matrix’s singular values. Lemma 1. (Shrinking singular-space of weight matrix.) Consider the linear model above which is optimized with Lreg = tr[H ⊤LH]. Let P = X ⊤LX = (cid:80) j and denote its non-ascending\n\nLijxi·x⊤\n\nij\n\n1 , λP\n\n2 , · · · , λP\n\neigenvalues by {λP D}. Denote the randomly initialized weight matrix by W (0) and the updated weight matrix at time t by W (t), respectively. We further denote the non-ascending singular values of W at time t by {σW i=1. Then the relative value of the smaller eigenvalues to the i (t′) larger ones will decrease as t increases. Formally, σW j (t′) , ∀ t < t′, i ≤ j. Furthermore, d > λP if the following condition holds: λP\n\nj (t) ≤ σW d+1 ≥ · · · ≥ λP\n\nD, then\n\n(t)}D\n\ni (t)\n\nσW\n\nσW\n\ni\n\n= 0, ∀ i ≤ d and j ≥ d + 1.\n\n(3)\n\n1 ≥ · · · ≥ λP σW i\nσW\n\n(t) j (t)\n\nlim t→∞\n\nSee proof in Appendix A.1. Lemma 1 indicates that the singular values of W (in proportional to the larger ones) shrink as the training step increases. With Lemma 1, we can conclude the following theorem that reveals a dimensional collapse phenomenon under this condition: Theorem 1. (Laplacian regularization leads to dimensional collapse.) For the linear model above optimized with Graph Laplacian Regularization, the embedding space of nodes tends to be dominated by the largest a few eigenvalues. Specifically, if the covariance matrix of input features is an identity matrix, we have:\n\nlim t→∞\n\nλC λC\n\ni (t) j (t)\n\n= 0, ∀ i ≤ d and j ≥ d + 1.\n\n(4)\n\nSee proof in Appendix A.2. Theorem 1 reveals that with the effect of Graph Laplacian Regularization, the node embeddings suffer from dimensional collapse.N As the eigenspectrum is dominated by its largest few eigenvalues, the embedding space is narrow, leading to poor robustness and generalization ability when classified with linear classifiers (see more discussions in Appendix E).\n\nDespite the analysis above, the exact evolving dynamics of node embeddings should be more complicated, as 1) the supervised cross-entropy loss forces nodes of different classes to have distinguishable embeddings; 2) the encoder is usually an MLP with non-linear activations instead of a simple linear model. Therefore we further provide empirical results to show that dimensional collapse does exist.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Eigenspectra for node embeddings with different strengths of Laplacian regularization λ (the upper three figures), at different training epochs (the lower three figures). x-axis represents the index of sorted eigenvalues and y-axis is the normalized eigenvalue (the ratio to the largest one). The results are averaged over 10 random initialization with 95% confidence intervals.\n\nFigure 3: Evolving of NESum as the training epoch increases, with different regularization factors.\n\nEmpirical justification. We train a 3-layer MLP model using Eq. 1 on Cora dataset. The embedding dimension is set as 512 and the regularization works on the output of the penultimate layer. To study the dimensional collapse phenomenon, we plot the decay of the eigenvalues of node embeddings’ correlation matrix with different strengths of regularization as the training epochs increase in Fig. 2 (here we only plot the top-8 eigenvalues for better visualization, while deferring the original results in Appendix C.4), from which we can easily see that: without Laplacian regularization, the top-C largest eigenvalues (where C is the number of classes, and C = 7 for Cora) are easily preserved by the model. This means the model learns to discriminate between different classes. However, when we increase the Laplacian regularization factor λ, we notice an increasing decay rate of top eigenvalues. As the top eigenvalues indicate the realistic embedding dimension that takes effect, we conclude that a large decay rate degrades the importance of the spaces induced by other eigenvalues, thus leading to feature collapse issues.\n\nBesides, we employ normalized eigenvalue sum (NESum) introduced in He & Ozay (2022) as a metric to measure the extent of dimensional collapse. Formally, NESum is defined as the ratio between the summation of eigenvalue and the largest one: NESum({λC 1 . Intuitively, a large NESum value indicates that the eigenvalues are fluently distributed, while a very small one indicates the dimensional collapse phenomenon (the largest eigenvalue becomes dominant).\n\ni }) ≜ (cid:80)d\n\ni=1 λC\n\ni /λC\n\nIn Fig. 3, we plot the evolution of NESum with different regularization strengths. It is observed that 1) NESum decreases as training goes on because the model learns to pay more attention to important features for downstream classification tasks. 2) NESum trained with purely cross-entropy loss converges to a high value., which is because the top-C eigenvalues are preserved. 3) With additional Laplacian regularization, NESum decreases quickly and converges to a small value even if the regularization factor λ is small. The above observations demonstrate that Laplacian regularization leads to a larger decay rate of top eigenvalues. The significant decay rate will make the learned representations less informative as the model focuses much more on the dominant eigenvalue rather than equal to the top eigenvalues.\n\n4 PROPOSED REMEDY: OVERCOMING DIMENSIONAL COLLAPSE VIA\n\nORTHOGONALITY REGULARIZATION\n\n4.1 EXPLICIT REGULARIZATION ON THE CORRELATION MATRIX\n\nOur thorough analysis in Sec. 3 reveals that the poor performance of GR-MLPs could be attributed to less-expressive node representations (due to dimensional collapse). Specifically, we establish that the eigenspectrum of the embeddings’ correlation matrix is dominated by the largest eigenvalue (different dimensions are over-correlated).\n\n5\n\n12345678Eigenvalue rank102101100=0Epoch02040608010012345678Eigenvalue rank102101100=0.001Epoch02040608010012345678Eigenvalue rank102101100=0.1Epoch02040608010012345678Eigenvalue rank102101100Epoch = 0Epoch=0.0=0.001=0.112345678Eigenvalue rank102101100Epoch = 20Epoch=0.0=0.001=0.112345678Eigenvalue rank102101100Epoch = 100Epoch=0.0=0.001=0.120406080100Training epochs02468101214NESum=0=0.001=0.01=0.1Under review as a conference paper at ICLR 2023\n\nIn contrast to dimensional collapse, whitened representations have an identity correlation matrix with equally distributed eigenvalues. Motivated by this, a natural idea should be enforcing a soft regularization term on the correlation matrix of node embeddings, e.g., minimizing the distance between C and the identity matrix I:\n\nlcorr_reg = ∥C − I∥2\n\nF =\n\nd (cid:88)\n\ni=1\n\n(1 − Cii)2 +\n\nC 2\n\nij =\n\n(cid:88)\n\ni̸=j\n\nC 2\n\nij.\n\n(cid:88)\n\ni̸=j\n\n(5)\n\nNote that the on-diagonal terms Cii = 1 for all i, so Eq. 5 is essentially forcing the off-diagonal terms of the correlation matrix to become zero, or in other words, making the embeddings orthogonal, so that different dimensions of node embeddings can capture orthogonal information. One may directly equip Eq. 5 with existing GR-MLPs for alleviating the dimensional collapse issue. However, we would like to design a more general, flexible, and elegant formulation that can handle high-order connectivity and non-homophily graphs (Pei et al., 2020; Zheng et al., 2022). We then introduce ORTHO-REG, a powerful and flexible GR-MLP model, step by step.\n\n4.2 GRAPH-REGULARIZED MLP WITH ORTHO-REG\n\nSimilar to previous GR-MLPs, we first use an MLP encoder to map raw node features to the embeddings. This process can be formulated as H = MLPθ(X), where X = {xi}|V| i=1 is raw node features while H = {hi}|V|\n\ni=1 is the embedding matrix.\n\nThe next question is what kind of graph structure information is more beneficial. Previous GR-MLPs either resort to edge-centric smoothing (Zhou et al., 2003; Ando & Zhang, 2006) or node-centric matching (Yang et al., 2021; Hu et al., 2021). While recent studies indicate that the node-centric method is more appropriate for node-level tasks as edge-centric methods overemphasize the ability to recover the graph structure (Yang et al., 2021). Inspired by this, we employ a neighborhood abstraction operation to summarize the neighborhood information as guidance of the central node. Formally, for a node i ∈ V and the embeddings of its (up-to) T -hop neighbors {hj}(1:T )(i), we can get the summary if its T -hop neighborhoods through a pooling function si = Pool({hj}(1:T )(i)). The exact formulation of the pooling function could be flexible to fit graphs with different properties. However, here we consider a simple average pooling of node embeddings from different order’s neighborhoods for simplicity, which can work in most cases:\n\nS =\n\nT (cid:88)\n\nt=1\n\n ̃AtH/L, where ̃A = AD−1.\n\n(6)\n\nTo make the node embeddings aware of structural information, we employ the following regularization term on the cross-correlation matrix of node embeddings H and summary embeddings S:\n\nLreg = −α\n\nD (cid:88)\n\nk=1\n\nCkk + β\n\n(cid:88)\n\nk̸=k′\n\nC 2\n\nkk′,\n\n(7)\n\nwhere C = {Ckk′} ∈ RD×D is the cross-correlation matrix of H and S. We show in the following theorem that with Eq. 7, the node embeddings will be locally smoothed and at the same time, prevent dimensional collapse: Theorem 2. Assume T = 1 and H are free vectors. Let H ∗ be a global optimizer of Eq. 7, then H ∗ is smoothed over the graph structure and is orthogonal.\n\nSee proof in Appendix A.3. Finally, we can employ an additional linear layer to make predictions Y = LINφ(H). Then the final objective function to be optimized is:\n\nL = lxent(Y L, ˆY L) − α\n\nD (cid:88)\n\nk=1\n\nCkk + β\n\n(cid:88)\n\nk̸=k′\n\nC 2\n\nkk′,\n\n(8)\n\nwhere α, β are trade-off hyperparameters to balance the strengths of regularization. Remark 2. With a well-trained model, we can give prediction for an upcoming node with feature x with y = Linφ(MLPθ(x)) quickly, and without the help of graph structure.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Prediction accuracy of semi-supervised node classification tasks on the seven benchmark graphs. ORTHO-REG outperforms powerful GNN models and competitive MLP-architectured baselines on 6 out of 7 datasets.\n\nMethods\n\nSGC GCN GAT\n\nGNNs\n\nCora\n\nCiteseer\n\nPubmed\n\nComputer\n\nPhoto\n\nCS\n\nPhysics\n\n81.0±0.5 82.2±0.5 83.0±0.7\n\n71.9±0.5 71.6±0.4 72.5±0.7\n\n78.9±0.4 79.3±0.3 79.0±0.3\n\n80.6±1.9 82.9±2.1 82.5±1.6\n\n90.3±0.8 91.8±0.6 91.4±0.8\n\n87.9±0.7 89.9±0.7 90.5±0.8\n\n90.3±1.4 91.9±1.2 92.3±1.5\n\nKD-MLPs GLNN\n\n82.6±0.5\n\n72.8±0.4\n\n80.2±0.6\n\n82.1±1.9\n\n91.3±1.0\n\n92.6±1.0\n\n93.3±0.5\n\nGR-MLPs\n\nMLP Lap-Reg P-Reg GraphMLP N2N\n\n59.7±1.0 60.3±2.5 64.4±4.5 79.5±0.6 83.2±0.4\n\n57.1±0.5 58.6±2.4 61.1±2.1 73.1±0.4 73.3±0.5\n\n68.4±0.5 68.7±1.4 72.3±1.7 79.7±0.4 80.9±0.4\n\n62.6±1.8 62.6±2.0 68.9±3.3 79.3±1.7 81.4±1.6\n\n76.2±1.4 76.4±1.1 79.7±3.7 90.1±0.5 90.9±0.7\n\n86.9±1.0 87.9±0.6 90.9±1.9 90.3±0.6 91.5±0.7\n\n89.4±0.7 89.5±0.5 91.6±0.7 91.6±0.8 91.8±0.7\n\nOurs\n\nORTHO-REG\n\n84.7±0.4\n\n73.5±0.4\n\n82.8±0.5\n\n83.7±1.5\n\n92.3±1.0\n\n92.9±1.1\n\n92.8±0.8\n\n5 EXPERIMENTS\n\nIn this section, we conduct experiments to evaluate ORTHO-REG by answering the following research questions:\n\n• RQ1: What’s the performance of ORTHO-REG on common transductive node classification\n\ntasks compared with GNN models and other MLP models? (Sec. 5.2)\n\n• RQ2: On cold-start settings where we do not know the connections of testing nodes, can\n\nORTHO-REG demonstrate better performance than other methods? (Sec. 5.3)\n\n• RQ3: Does ORTHO-REG mitigate the dimensional collapse issue, and is each design of\n\nORTHO-REG really necessary to its success? (Sec. 5.4)\n\n• RQ4: Can ORTHO-REG demonstrates better robustness against structural perturbations\n\ncompared with Graph Neural Networks? (Sec. 5.5)\n\nDue to space limits, we defer the experiments on heterophily graphs and scalability comparison in Appendix C.2 and Appendix C.3, respectively. A brief introduction of the baselines is given in Appendix B.3.\n\n5.1 EXPERIMENT SETUPS\n\nDatasets. We consider 7 benchmark graph datasets and their variants in this section: Cora, Citeseer, Pubmed, Amazon-Computer, Amazon-Photo, Coauthor-CS, and Coauthor-Physics as they are representative datasets used for semi-supervised node classification (Kipf & Welling, 2017; Hu et al., 2021; Zhang et al., 2021; Zheng et al., 2021). The detailed introduction and statistics of them are presented in Appendix B. To evaluate ORTHO-REG on large-scale graphs, we further consider two OGB datasets (Hu et al., 2020): Ogbn-Arxiv and Ogbn-Products. Note that the two OGB datasets are designed for fully-supervised node classification tasks, so we defer their results to Appendix C.\n\nImplementations. If not specified, we use a two-layer MLP model as the encoder to generate node embeddings, then another linear layer takes node embeddings as input and outputs predicted node labels. We use Pytorch to implement the model and DGL (Wang et al., 2019) to implement the neighborhood summarizing operation in Eq. 6. If not specified, all our experiments are conducted on an NVIDIA V100 GPU with 16G memory with Adam optimizer (Kingma & Ba, 2015).\n\n5.2 TRANSDUCTIVE SEMI-SUPERVISED NODE CLASSIFICATION (RQ1)\n\nWe first evaluate our method on transductive semi-supervised node classification tasks. For comparison, we consider three types of baseline models: 1) Graph Neural Networks (GNNs), including SGC (Wu et al., 2019a), GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018). 2) Representative knowledge distillation (KD-MLP) method GLNN (Zhang et al., 2021). 3) Basic MLP and GR-MLP models, including Laplacian Regularization (Lap-Reg, Zhou et al. (2003), Ando & Zhang\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Test accuracy on the isolated nodes.\n\nTable 3: Effects of different components of ORTHO-REG\n\nMethods\n\nGCN GraphSAGE\n\nColdBrew GLNN\n\nGNNs\n\nKD-MLPs\n\nCora\n\nCiteseer\n\nPubmed\n\n53.02±1.78 47.09±1.38 71.50±2.21 55.38±1.92 41.46±1.57 69.87±2.13\n\n58.75±2.11 53.17±1.41 72.31±1.99 59.34±1.97 53.64±1.51 73.19±2.31\n\nGR-MLPs\n\n52.35±1.83 53.26±1.41 65.84±2.08 MLP 59.32±1.81 53.17±1.48 72.33±2.11 GraphMLP ORTHO-REG (Ours) 61.93±1.77 56.31±1.54 73.42±1.99\n\nVariants Cora Citeseer Pubmed\n\nBaseline 84.7\n\nα = 0 β = 0\n\nT = 1 T = 2 T = 3\n\n54.7 79.3\n\n83.9 84.7 84.3\n\n73.5\n\n51.4 68.7\n\n72.9 73.5 73.3\n\n82.8\n\n47.2 76.8\n\n82.1 82.8 82.5\n\n(2006)), Propagation Regularization (P-Reg, Yang et al. (2021)), GraphMLP (Hu et al., 2021), and Node-to-Neighborhood Mutual Information Maximization (N2N, Dong et al. (2022))\n\nFor each dataset, we use 20 nodes per class for training, 500 nodes for validation, and another 1000 nodes for testing. For Cora, Citeseer, and Pubmed we use the public split, while for the remaining datasets, we split randomly. We report the average prediction accuracy with standard deviation over 20 random trials in Table 1.\n\nAs demonstrated in the table, ORTHO-REG outperforms previous GR-MLPs by a large margin, which greatly validates the importance and effectiveness of orthogonal node embeddings. Compared with the competitive knowledge distillation method GLNN, ORTHO-REG also demonstrates better performance on 6 out of 7 graphs. It is also worth noting that our method even outperforms powerful GNN models such as GCN and GAT, which indicates that node features of the graphs are less exploited by these GNN models. In contrast, our method can fully exploit the potential of node features.\n\n5.3\n\nINDUCTIVE NODE CLASSIFICATION FOR COLD-START SCENARIOS (RQ2)\n\nTo evaluate the performance of ORTHO-REG on cold-start scenarios where the connections between newly encountered nodes and existing nodes are missing, we follow the setups in ColdBrew that selects a proportion of nodes as isolated nodes which will be removed from the original graph. Then the model is evaluated on the isolated nodes in the testing set. Due to the space limit, we present the detailed setups and evaluation methods in Appendix B.2. Besides the baselines used in Zheng et al. (2021), we also include GLNN for a fair comparison.\n\nIn Table. 2, we report the experimental results of ORTHO-REG and baseline methods on the isolation nodes. As demonstrated in the table, for isolated nodes whose connectivity in the graph is unknown, GNN models perform poorly as they require both the node features and graph structure for accurate inference. By contrast, MLP-based models generalize better on isolated nodes as they make the best of the available node features. The proposed ORTHO-REG outperforms both GNNs and MLPs (including KD MLPs and GR-MLPs) baselines.\n\n5.4 STUDIES OF ORTHO-REG (RQ3)\n\n5.4.1 DOES ORTHO-REG MITIGATE DIMENSIONAL COLLAPSE?\n\nIn Sec. 3 we have attributed the limitation of previous GR-MLPs to the dimensional collapse phenomenon, and in Sec. 4.2 we have proposed ORTHO-REG to mitigate such a problem from a theoretical perspective. In this part, we would like to empirically show that ORTHO-REG can avoid the dimensional collapse issue by keeping node embeddings’ eigenspectra.\n\nIn consistency with the settings in Sec. 3, we evaluate the embeddings learned from ORTHO-REG at different training epochs (we take both Cora and Pubmed for illustrations). The decay of eigenvalues of node embeddings’ correlation matrix at different epochs is plotted in Fig. 4 (a) and (c). It is observed that the top eigenvalues are well-reserved thanks to the explicit regularization of node embeddings’ correlation matrix. In Fig. 4 (b) and (d) we also plot the change of testing accuracy as well as the NESum value as the training epoch increases, from which we could observe a positive relationship between the NESum value and the test accuracy: neglecting the initial oscillations, we notice the test accuracy will grow smoothly as the NESum value increases and will reach its peak\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Visualization of ORTHO-REG’s impact on node embeddings’ Eigenspectra on Cora and Pubmed.\n\nFigure 5: Performance with different strengths of edge masking ratios.\n\nwhen NESum overwhelms (Cora) or converges (Pubmed). The above observations demonstrate that ORTHO-REG does mitigate the dimensional collapse problem and lead to a more powerful model.\n\n5.4.2 ABLATION STUDIES\n\nWe then conduct ablation studies to study the effect of different components of ORTHO-REG, and we present the results in Table 3. We first study the impact of the two regularization terms by setting the corresponding factors (α and β) to 0, respectively. When α = 0 (i.e., only decorrelating different dimensions), we observe that the model’s performance is even worse than the pure MLP model (see in Table 1). This indicates that adding orthogonal regularization is not always beneficial (e.g., for vanilla MLP), but is indeed beneficial for GR-MLPs. By contrast, without orthogonal regularization (i.e., β = 0), the power of structure regularization is restricted, and decorrelating different dimensions can boost performance greatly. We further investigate whether considering a larger neighborhood would improve the model’s performance. The empirical results demonstrate that considering a larger neighborhood improves the performance compared to only using first-order neighborhoods, but T = 2 is already optimal for most datasets.\n\n5.5 ROBUSTNESS AGAINST STRUCTURAL PERTURBATIONS (RQ4)\n\nFinally, we study the robustness of ORTHO-REG against attacks on the graph structures compared with GNN models. As ORTHO-REG uses node features rather than a combination of node features and edges for prediction, we expect it to demonstrate better robustness under mild structural perturbations. To reach this target, we randomly mask a fraction of the edges of the graph and evaluate the performance of ORTHO-REG and GCN under different edge-masking ratios. In Fig. 5, we plot how the model’s performance changes (with standard deviation) as the masking ratio increases with 20 random trials. As demonstrated in Fig. 5, our method demonstrates better robustness against moderate-level edge perturbations. This is because we do not explicitly use the graph structure for generating predictions, making ORTHO-REG less sensitive to perturbations on the graph structure.\n\n6 CONCLUSIONS\n\nIn this paper, we have proposed ORTHO-REG, a novel Graph-Regularized MLP method for node representation learning. We show that simple graph regularization methods can cause dimensionally collapsed node embeddings both theoretically and empirically. We show that the proposed ORTHOREG, which enforces the orthogonality of the correlation matrix of node embeddings, can naturally avoid the feature collapse phenomenon. We have conducted extensive experiments, including traditional transductive semi-supervised node classification tasks and inductive node classification for cold-start nodes, demonstrating the superiority of ORTHO-REG.\n\n9\n\n100101102Eigenvalue rank109106103100Normalized eigenvalue(a) Cora: Eigenvalue distributionEpoch04008001200160020000500100015002000Training Epoch0.20.40.60.8Accuray(b) Cora: Evolvement of NESum51015202530NESum100101102103Eigenvalue rank1010107104101Normalized eigenvalue(c) Pubmed: Eigenvalue distributionEpoch0200400600800100002004006008001000Training Epoch0.40.50.60.70.8Accuray(d) Pubmed: Evolvement of NESum102030NESumAccuracyNESumAccuracyNESum0.00.10.20.30.40.5Masking ratio747678808284AccuracyCoraGCNOrtho-Reg0.00.10.20.30.40.5Masking ratio747678808284PubmedGCNOrtho-RegUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRie Kubota Ando and Tong Zhang. Learning on graph with laplacian regularization. In NIPS, pp.\n\n25–32. MIT Press, 2006.\n\nSanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix\n\nfactorization. In NeurIPS, pp. 7411–7422, 2019.\n\nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank\n\ngraph neural network. In ICLR, 2021.\n\nValerio Ciotti, Moreno Bonaventura, Vincenzo Nicosia, Pietro Panzarasa, and Vito Latora. Homophily\n\nand missing links in citation networks. EPJ Data Sci., 5(1):7, 2016.\n\nWei Dong, Junsheng Wu, Yi Luo, Zongyuan Ge, and Peng Wang. Node representation learning in graph via node-to-neighbourhood mutual information maximization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16620–16629, 2022.\n\nAleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for selfsupervised representation learning. In ICML, volume 139 of PRML, pp. 3015–3024. PMLR, 2021.\n\nWenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In The world wide web conference, pp. 417–426, 2019.\n\nWenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. In NeurIPS, 2020.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\n\ngraphs. In NIPS, pp. 1024–1034, 2017.\n\nBobby He and Mete Ozay. Exploring the gap between collapsed & whitened features in selfsupervised learning. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 8613–8634, 2022.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS, 2020.\n\nYang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, and Yue Gao. Graph-mlp: Node classification without message passing in graph. CoRR, abs/2106.04051, 2021. URL https://arxiv.org/abs/2106.04051.\n\nTianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On feature\n\ndecorrelation in self-supervised learning. In ICCV, pp. 9578–9588, 2021.\n\nZiwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In ICLR,\n\n2019.\n\nLi Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in\n\ncontrastive self-supervised learning. In ICLR, 2022.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\nIn ICLR, 2017.\n\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph\n\nneural networks meet personalized pagerank. In ICLR, 2019.\n\nKanglin Liu, Guoping Qiu, Wenming Tang, and Fei Zhou. Spectral regularization for combating\n\nmode collapse in gans. In ICCV, pp. 6381–6389, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMiller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social\n\nnetworks. Annual review of sociology, 27(1):415–444, 2001.\n\nHoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.\n\narXiv preprint arXiv:1905.09550, 2019.\n\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric\n\ngraph convolutional networks. In ICLR, 2020.\n\nEmanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael M. Bronstein, and Federico Monti. SIGN: scalable inception graph neural networks. arXiv preprint arXiv: abs/2004.11198, 2020.\n\nHannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, pp. 20479–20502. PMLR, 2022.\n\nRianne van den Berg, Thomas N. Kipf, and Max Welling. Graph convolutional matrix completion.\n\nCoRR, abs/1706.02263, 2017.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua\n\nBengio. Graph attention networks. In ICLR, 2018.\n\nMinjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J. Smola, and Zheng Zhang. Deep graph library: Towards efficient and scalable deep learning on graphs. arXiv, 1909.01315, 2019.\n\nYuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molclr: Molecular contrastive learning of representations via graph neural networks. arXiv preprint arXiv:2102.10056, 2021.\n\nFelix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. In ICML, volume 97 of Proceedings of Machine\n\nSimplifying graph convolutional networks. Learning Research, pp. 6861–6871, 2019a.\n\nQitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, and Guihai Chen. Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems. In WWW, pp. 2091–2102, 2019b.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? In ICLR, 2019.\n\nHan Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks.\n\nIn AAAI, pp. 4573–4581. AAAI Press, 2021.\n\nShichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. Graph-less neural networks: Teaching old\n\nmlps new tricks via distillation. In ICLR, 2021.\n\nWenqing Zheng, Edward W. Huang, Nikhil Rao, Sumeet Katariya, Zhangyang Wang, and Karthik Subbian. Cold brew: Distilling graph node representations with incomplete or missing neighborhoods. In ICLR, 2021.\n\nXin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S. Yu. Graph neural networks for\n\ngraphs with heterophily: A survey. arXiv prepint arXiv:2202.07082, 2022.\n\nDengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf.\n\nLearning with local and global consistency. In NIPS, pp. 321–328. MIT Press, 2003.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 PROOFS FOR LEMMA 1\n\nProof. First, let’s take the gradient of the regularization loss Lreg with respect to the weight matrix W :\n\n∂Lreg ∂W\n\n=\n\n=\n\n∂ tr(H ⊤LH) ∂W\n\n∂ tr((XW )⊤L(XW )) ∂W\n\n=2X ⊤LXW =2P W\n\n(9)\n\nTreat the weight matrix as a function of the training step t, i.e., W = W (t), then we can derive the gradient of W (t) with respect to t by dW (t) dt = 2P W . As both X and L are fixed, we can solve the equation analytically,\n\nW (t) = exp(P t) · W (0).\n\n(10)\n\nAs we have the non-ascending eigenvalues of P as λP D, we can define an auxiliary j t) = e(λP function f (t; λP j ) = exp(λP j ) is monotonically decreasing for all j ≥ i. As W (t) is a transformation of its initial state W (0) up to exp(P t), we can easily conclude that\n\nj )t. It is obvious that f (t; λP\n\n2 ≥ · · · ≥ λP\n\ni t)/ exp(λP\n\n1 ≥ λP\n\ni , λP\n\ni , λP\n\ni −λP\n\nσW i\nσW\n\n(t) j (t)\n\n≤\n\nσW i\nσW\n\n(t′) j (t′)\n\n, ∀ t < t′ and i ≤ j.\n\n(11)\n\nIf we further have the condition that λP lim t→∞\n\nf (t; λP\n\ni , λP\n\nj ) = 0, ∀i ≤ d, j ≥ d + 1. Then we are able to complete the proof.\n\n1 ≥ · · · ≥ λP\n\nd > λP\n\nd+1 ≥ · · · ≥ λP\n\nD, we have\n\nA.2 PROOFS FOR THEOREM 1\n\nProof. The embedding space is identified by the eigenspectrum of the correlation (covariance) matrix C of node embeddings H. As H = XW , its correlation matrix can be (simply) identified as:\n\nC =\n\n=\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n(hi − h)⊤(hi − h)/N\n\nW ⊤(xi − x)⊤(xi − x)W /N.\n\n(12)\n\nAccording to Lemma 1, W has shrinking singular values, so C has vanishing eigenvalues, indicating collapsed dimensions.\n\nSpecially, when the input features have an identity matrix, we have:\n\nC =\n\nN (cid:88)\n\ni=1\n\nW ⊤(xi − x)⊤(xi − x)W /N.\n\n(xi − x)⊤(xi − x) N\n\nW\n\n=W ⊤\n\nN (cid:88)\n\ni=1\n\n=W ⊤W .\n\n(13)\n\nThus, for C’s eigenvalues {λC with Lemma 1.\n\ni }D\n\ni=1, we have λC\n\ni = (σW\n\ni\n\n)2. Then Theorem 1 can be easily concluded\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 PROOFS FOR THEOREM 2\n\nProof. Note that we aim to optimize the following objective function (Eq. 7):\n\nL = −α\n\nD (cid:88)\n\nk=1\n\nCkk + β\n\n(cid:88)\n\nk̸=k′\n\nC 2\n\nkk′.\n\nAs the first term applies only on the on-diagonal terms of the correlation matrix and the second term applies only on the off-diagonal terms, we are able to study the effects of two terms respectively:\n\nLon−diag = −α\n\nD (cid:88)\n\nk=1\n\nCkk\n\nLof f −diag = β\n\nD (cid:88)\n\nk̸=k′\n\nC 2\n\nkk′\n\nFor the on-diagonal terms Lon−diag, as we have set T = 1, we have C =\n\nN (cid:80)\n\nCkk =\n\n(hi)k · (si)k/N (the subscript k denotes the k-th dimension). Then,\n\ni=1\n\nAs a result,\n\n∂Ckk ∂(hi)k\n\n=\n\n1 N\n\n(si)k, and\n\n∂Ckk ∂(hi)′\n\nk\n\n= 0, ∀k′ ̸= k.\n\n∂\n\nD (cid:80)\n\nk=1\n\nCkk\n\n∂hi\n\n=\n\n1 N\n\nsi =\n\n1 N\n\n(cid:80)\n\nj∈N (i) hj |N (i)|\n\n.\n\n(14)\n\nN (cid:80)\n\ni=1\n\nhis⊤\n\ni /N , and\n\n(15)\n\n(16)\n\nEq. 16 indicates that the on-diagonal terms force each node embedding to be smoothed within its first-order neighborhoods.\n\nThen we turn to the off-diagonal terms Lof f −diag. Similarity, we have Ckk′ =\n\nN (cid:80)\n\n(hi)k · (si)k′/N .\n\nAs for both H and S, the diagonal term of their correlation matrixes for each dimension should be equal to 1, formally,\n\ni=1\n\nN (cid:80)\n\n(hi)2\n\nk\n\ni=1\n\nN\n\nN (cid:80)\n\n(si)2\n\nk\n\n=\n\ni=1\n\nN\n\n= 1.\n\n(17)\n\nThen according to Cauchy–Schwarz inequality, we have:\n\n(cid:34) N\n\n(cid:88)\n\n(hi)k · (si)k\n\n(cid:35)2\n\n≤\n\n(cid:34) N\n\n(cid:88)\n\n(hi)2\n\nk\n\n(cid:35) (cid:34) N\n\n(cid:88)\n\n(cid:35)\n\n(si)2\n\nk\n\n= N 2\n\ni=1\n\ni=1\n\n,\n\n(18)\n\ni=1\n\nN (cid:80)\n\n(hi)k · (si)k\n\ni=1\n\nN\n\n≤ 1\n\nand the equality holds if and only if (hi)k = (si)k, ∀i. As a result, the global optimizer of Lon−diag will induce hi = si, ∀i. Then, when Ckk′ = 0, k ̸= k′, we have:\n\nN (cid:88)\n\ni=1\n\n(hi)k · (si)k′/N =\n\nN (cid:88)\n\ni=1\n\n(hi)k · (hi)k′/N = C auto\n\nkk′ = 0,\n\n(19)\n\nwhere Cauto is the auto-correlation matrix of H. So we’ve completed the proof.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB EXPERIMENT DETAILS IN SECTION 5\n\nB.1 DATASETS DETAILS\n\nWe present the statistics of datasets used in transductive node classification tasks in Table 4, including the number of nodes, number of edges, number of classes, number of input node features as well as the number of training/validation/testing nodes.\n\nTable 4: Statistics of benchmarking datasets in transductive settings\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Classes\n\n#Features\n\n#Train/Val/Test\n\nCora Citeseer Pubmed Coauthor-CS Coauthor-Physics Amazon-Computer Amazon-Photo\n\n2,708 3,327 19,717 18,333 34,493 13,752 7,650\n\n10,556 9,228 88,651 327,576 991,848 574,418 287,326\n\n7 6\n3 15 5\n10 8\n\n1,433 3,703 500 6,805 8,451 767 745\n\n140/500/1000 120/500/1000 60/500/1000 300/500/1000 100/500/1000 200/500/1000 160/500/1000\n\nFor the inductive node classification tasks in cold-start settings, we also present the statistics of Cora, Citeseer and Pubmed in Table 5, where we provide the number of isolated nodes, the number of tail nodes as well as the number of edges left after removing the isolated nodes from the graph.\n\nTable 5: Statistics of benchmarking datasets in inductive settings\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Isolated\n\nCora Citeseer Pubmed\n\n2,708 3,327 19,717\n\n10,556 9,228 88,651\n\n534 676 4,547\n\n#Tail\n\n534 676 4,547\n\n#Edges left\n\n9,516 7,968 79,557\n\nB.2 DETAILS OF EXPERIMENTS ON COLD-START SCENARIOS (SEC. 5.3)\n\nIn this section, we detailedly elaborate how the inductive isolated nodes are selected. Note that our processing directly follows the officially-implemented codes1 in ColdBrew (Zheng et al., 2021). For each dataset, we first rank among the nodes according to their node degrees, through which we are able to get the degree of the bottom 3th percentile node, termed by d3th. Then we screen out nodes whose degree is smaller than or equal to d3th as isolated nodes, which are subsequently removed from the original graph.\n\nNote that in these datasets most of nodes only have a few connections (e.g. 1 or 2), the actually numbers of isolated nodes and tail nodes are usually much larger than the expected 3%. See Table 5 for details.\n\nFor each dataset, we use the fixed 20 nodes per class (as in the public split) for training and all the remaining nodes for testing.\n\nB.3\n\nINTRODUCTION OF BASELINES\n\nIn Sec. 5.1 we’ve briefly introduced the baselines for comparison, here we’d like to detailedly introduce the MLP-based baselines.\n\nKD-MLPs: We have covered two KD-MLP models: GLNN (Zhang et al., 2021) and ColdBrew (Zheng et al., 2021).\n\n1https://github.com/amazon-research/gnn-tail-generalization\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n• GLNN: As a typical GNN-to-MLP knowledge distillation method, given the predicted soft labels from a well-learned GNN model {zi}, GLNN learns an MLP model through jointly optimizing the supervised loss on labeled nodes and the cross-entropy loss between MLP’s predictions and GNN’s predictions over all nodes:\n\nLglnn =Lsup + λLkd\n\nLsup =\n\n(cid:88)\n\ni∈V L\n\nlxent(y, ˆy) and Lkd =\n\n(cid:88)\n\ni∈V\n\nDKL( ˆyi, zi),\n\n(20)\n\nwhere λ is a trade-off hyperparameter.\n\n• ColdBrew: As another KD-MLP model, ColdBrew is specially designed to handle cold-start problems and has a totally different formulation compared with GLNN. First, it equips the teacher GNN model with structural embedding so that it can overcome the oversmoothing issue. Then, besides the knowledge distillation loss, it discovers the virtual neighborhood of each node using the embedding learned from the student MLP. With this operation, the model is able to estimate the possible neighbors of each node and thus can generalize better in inductive cold-start settings.\n\nGR-MLPs: We then introduce the covered GR-MLP models, including Lap-Reg (Zhou et al., 2003; Ando & Zhang, 2006), P-Reg (Yang et al., 2021) and GraphMLP (Hu et al., 2021) detailedly. Besides the basic supervised cross-entropy loss, GR-MLPs employ a variety of regularization losses to inject the graph structure knowledge into the learning of MLPs implicitly.\n\n• Lap-Reg: Based on the graph homophily assumption, Lap-Reg enforces Laplacian smoothing on the predicted node signals over the graph structure. Its regularization target could be formulated as:\n\nLlap−reg = tr(Y ⊤LY ),\n\n(21)\n\nwhere Y the predicted node signals and L is the Laplacian matrix of the graph.\n\n• P-Reg: Similar to Lap-Reg, P-Reg is also built on top of the graph homophily assumption. However, instead of using edge-centric smoothing regularization, P-Reg employs a nodecentric proximity preserving term that maximizes the similarity of each node and the average of its neighbors. The regularization objective could be formulated as:\n\n1 N\nwhere ̃AH is the propagated node embedding matrix, and φ is a function that measures the difference between H and ̃AH, which could be implemented with a variety of measures like Square Error, Cross Entropy, Kullback-Leibler Divergence, etc.\n\nφ(H, ̃AH),\n\nLP −reg =\n\n(22)\n\n• Graph-MLP: Inspired by the success of contrastive learning, Graph-MLP tries to get rid of\n\nGNN models by contrasting between connected nodes. Formally:\n\nLgraph−mlp =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n− log\n\n(cid:80)\n\nexp(sim(hi, hj)/τ )\n\nj∈N (i) (cid:80)\n\nexp(sim(hi, hk)/τ )\n\nk∈V\n\n.\n\n(23)\n\nThe final objective function is also a trade of between the supervised cross-entropy loss and the regularization loss.\n\nC ADDITIONAL EXPERIMENTS\n\nC.1 EXPERIMENTS ON OGB-GRAPHS\n\nTo study the effectiveness of ORTHO-REG on large-scale graphs, we conduct experiments on two large-scale graphs: Ogbn-Arxiv and Ogbn-Products, and we present the results in this section.\n\nThe statistics of the two datasets in the transductive setting and the inductive cold-start setting are presented in Table 6 and Table 7. Note that the official split of OGB datasets is different from other datasets in Sec. 5 and does not follow the semi-supervised setting.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Statistics of OGB datasets in transductive settings\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Classes\n\n#Features\n\n#Train/Val/Test\n\nOgbn-Arxiv Ogbn-Products\n\n169,343 2,449,029\n\n2,332,486 123,718,024\n\n40 47\n\n128 100\n\n90,941 / 29,799 / 48,603 196,615 / 39,323 / 2,213,091\n\nTable 7: Statistics of OGB datasets in inductive settings\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Isolated\n\n# Nodes left\n\n# Edges left\n\nOgbn-Arxiv Ogbn-Products\n\n169,343 2,449,029\n\n2,332,486 123,718,024\n\n16,934 244,902\n\n152,409 2,204,127\n\n2,298,618 123,661,058\n\nFor fair comparison, we use the same model size (i.e., the number of parameters) for each model. We train each model for 10 times on each dataset and report the average accuracy with standard deviation on transductive setting and inductive cold-start setting in Table 8 and Table 9 respectively.\n\nTable 8: Test accuracy on OGB datasets in transductive settings.\n\nTable 9: Test accuracy on the isolated nodes of OGB datasets.\n\nMethods\n\nOgbn-Arxiv Ogbn-Products\n\nGNNs\n\nGCN SAGE\n\n71.74±0.29 71.49±0.27\n\n75.26±0.21 78.61±0.23\n\nKD-MLPs GLNN\n\n69.37±0.25\n\n75.19±0.34\n\nGR-MLPs\n\nMLP Lap-Reg P-Reg GraphMLP\n\n56.28±0.37 57.83±0.52 58.41±0.45 61.11±0.36\n\n61.06±0.08 65.91±0.31 65.32±0.28 68.54±0.33\n\nOurs\n\nORTHO-REG 70.35±0.22\n\n74.35±0.19\n\nGNNs\n\nKD-MLPs\n\nGR-MLPs\n\nMethods\n\nGCN GraphSAGE\n\nColdBrew GLNN\n\nMLP Lap-Reg P-Reg GraphMLP ORTHO-REG (Ours)\n\nOgbn-Arxiv Ogbn-Products\n\n44.51±0.85 47.32±0.89\n\n52.36±0.84 53.18±1.05\n\n51.03±0.75 51.87±0.81 51.79±0.88 52.21±0.91 54.51±0.77\n\n56.62±1.12 57.88±1.01\n\n61.64±0.98 63.09±0.87\n\n60.18±0.84 60.47±0.77 60.59±0.91 61.12±0.98 63.95±0.74\n\nAs demonstrated in Table 8, though under-performing GNN models, ORTHO-REG gives quite good performance (which are very close to that of GNNs) on these two datasets. On inductive cold-start prediction tasks, ORTHO-REG also outperforms both GNN models and other MLP models.\n\nC.2 EXPERIMENTS ON HETEROPHILY GRAPHS\n\nThen we study the generalization ability of ORTHO-REG on heterophily (non-homophily) graphs. We first give the formal definition of graph homophily ratio as follows:\n\nDefinition 1. (Graph Homophily Ratio) For a graph G = (V, E) with adjacency matrix A, its homophily ratio φ is defined as the probability that two connected nodes share the same label:\n\nφ =\n\n(cid:80)\n\ni,j∈V Aij · 1[yi = yj] i,j∈V Aij\n\n(cid:80)\n\n(cid:80)\n\ni,j∈V Aij · 1[yi = yj] |E|\n\n=\n\n(24)\n\nThe evaluated datasets previously are all homophily graphs, where connected nodes tend to share the same labels. To evaluate ORTHO-REG more extensively we consider three more widely used heterophily graphs: Chameleon, Squirrel and Actor. We provide statistics of the three datasets in Table 10.\n\nFor heterophily graphs where the graph homophily assumption does not hold (McPherson et al., 2001; Ciotti et al., 2016), it might be improper to enforce node embeddings/predictions to be smoothed over the graph structure. As a result, we modify the neighborhood abstraction function so that it can better adjust to heterophily graphs. Specifically, we adopt the following function to construct summary embedding:\n\nS = ̃A2H/T.\n\n(25)\n\nCompared with Eq. 6, we use only the second neighbors for heterophily graphs. We present the results on the three heterophily graphs in Table 11. For comparison with GNN models, we cover additional\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 10: Statistics of heterophily graphs\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Classes\n\n#Features Heterophily ratio φ\n\nChameleon Squirrel Actor\n\n2,277 5,201 7,600\n\n36,101 217,073 33,544\n\n5 5\n5\n\n2,325 2,089 931\n\n0.25 0.22 0.24\n\nTable 11: Performance on heterophily graphs\n\nMethods\n\nGeom-GCN GPRGNN\n\nGNNs\n\nChameleon\n\nSquirrel\n\nActor\n\n67.32±1.76 66.31±2.05\n\n46.01±1.27 50.56±1.51\n\n30.59±0.76 30.78±0.83\n\nKD-MLPs GLNN\n\n60.58±1.72\n\n43.72±1.16\n\n34.12±0.77\n\nGR-MLPs\n\nMLP Lap-Reg ORTHO-REG (ours)\n\n47.59±0.73 48.72±1.52 63.55±0.83\n\n31.67±0.61 30.44±0.97 48.72±0.93\n\n35.93±0.61 33.71±0.59 36.64±0.67\n\ntwo GNN models that specifically designed for handle heterophily graphs: Geom-GCN (Pei et al., 2020) and GPRGNN (Chien et al., 2021)\n\nAs demonstrated in the Table, though can’t match the performance of advanced GNN models on heterophily graphs, our method greatly narrows the gap between MLPs and GNNs. Specifically, on heterophily graphs where GNNs even perform worse than the vanilla MLP, our model is able to achieve even better performance, thanks to the MLP-based encoder and the regularization loss.\n\nC.3 SCALABILITY TEST\n\nFinally, we show that with an MLP model as encoder, ORTHO-REG is able to perform inference fast without the reliance on graph structure. We plot the inference time of ORTHO-REG and GraphSAGE on Ogbn-Products with different model depths in Fig. 6. The result demonstrates the superiority of the inference benefit of ORTHO-REG over GNN models.\n\nFigure 6: The inference time comparison of GraphSAGE and ORTHO-REG on Ogbn-Products. ORTHO-REG is able to perform inference much faster than GraphSAGE.\n\nC.4 COMPLETE EMPIRICAL RESULTS FOR SEC. 3\n\nIn Fig. 2, we only plot the evolving of top-8 eigenvalues for better visualization. Here, we plot the evolving of all 512 eigenvalues. We also provide the result when λ = 0.01 for better comparison.\n\n17\n\n1234Number of Layers0.00.51.01.52.02.53.0Inference Time (ms)1e4Ortho-Reg (ours)GraphSAGEUnder review as a conference paper at ICLR 2023\n\nFigure 7: Eigenspectra for node embeddings with different strengths of Laplacian regularization (λ). The node embeddings are from the second last layer, with a dimension d = 512. x-axis represents the index of sorted eigenvalues while y-axis is the corresponding normalized values.\n\nC.5 SENSITIVITY ANALYSIS OF TRADE-OFF HYPERPARAMETERS\n\nFigure 8: Performance heat map when using different α, β combinations in Eq. 7, on Cora, Citeseer and Pubmed.\n\nIn this section, we study how the two trade-off hyperparameters α and β affects the performance of ORTHO-REG. We try different combinations of α and β on Cora, Citeseer, and Pubmed, and plot the performance heatmap in Fig. 8.\n\nThe conclusion is very interesting: the performance of ORTHO-REG is not very sensitive to a specific value of α or β. In other words, for a reasonable value of α (β), we can easily find another value of β (α) that can achieve similarly high performance. The ratio between α and β seems much more important. From Fig. 8, we can observe that for Cora, α/β = 2 ∗ 103, and for Pubmed, α/β = 1 ∗ 103 can lead to the optimal performance; changing the value of α while fixing α/β will not change the performance very much.\n\nD REPRODUCIBILITY\n\nPlease check the supplementary material.\n\nE RELATIONSHIP BETWEEN DIMENSIONAL COLLAPSE AND LINEAR\n\nCLASSIFICATION PERFORMANCE\n\nIn Sec. 3, we mainly demonstrate that the dimensional collapse phenomenon does exist in the typical GR-MLP method Laplacian Regularization. In this section, we’d like to use a simple example to explain why dimensional collapse may lead to limited expression power and sub-optimal linear classification performance, which will better support the motivation of this work.\n\nTo simplify the analysis, we consider a two-class classification problem where each data point is embedded in 2-dimensional space R2. The two dimensions are denoted by e1 and e2, respectively. In Fig. 9, we consider three cases: 1) complete dimensional collapse where the embeddings fall on a line\n\n18\n\n100101102Eigenvalue rank106105104103102101100=0Epoch020406080100100101102Eigenvalue rank106105104103102101100=0.001Epoch020406080100100101102Eigenvalue rank106105104103102101100=0.01Epoch020406080100100101102Eigenvalue rank106105104103102101100=0.1Epoch0204060801001e-072e-075e-071e-062e-065e-061e-050.00050.0010.0020.0050.01Cora0.6750.7000.7250.7500.7750.8000.8251e-072e-075e-071e-062e-065e-061e-050.00050.0010.0020.0050.01Citeseer0.660.670.680.690.700.710.720.731e-072e-075e-071e-062e-065e-061e-050.00050.0010.0020.0050.01Pubmed0.600.650.700.750.80Under review as a conference paper at ICLR 2023\n\nFigure 9: How dimensional collapse affects the performance of linear classification. Left: complete dimensional collapse; Mid: weak dimensional collapse; Right: perfectly decorrelated features.\n\n(the left case); 2) weak dimensional collapse where the variance of the larger eigenvalue’s direction is much larger than the other one (the middle case); 3) perfectly decorrelated features where the two eigenvalues’ direction are equally important (the right case). We assume that the labels of training data are generated according to ˆy = sgn(e2 − e1), i.e., the data point will be labeled as class 1 when e2 > e1, and class 2 otherwise. As a result, an obvious linear decision boundary will be e2 − e1.\n\nIn the first case (complete dimensional collapse), the data points cannot be linearly classified as they all fall on the same line. In the remaining two cases, data points belonging to two different classes can be easily separated with the decision boundary illustrated above. However, the above results only hold when the data points and labels are generated exactly following the above assumption, which is hard to meet. In many cases, the input data might be noisy, leading to shifting in its final representation. As shown in the circled areas in Fig. 9, when the embedding shift phenomenon occurs (from the brown triangle to the brown triangle) in the weak dimensional collapse case, the shifted embedding can be easily misclassified by the original linear classifier, even if the original embedding is already far from the decision boundary. By contrast, for perfectly decorrelated features, the embeddings can show better tolerance for data noise. Besides, decorrelated features can also show better robustness to attacks and better generalization ability to testing data, thanks to a wider embedding space for each class separated by the decision boundary.\n\n19\n\ne1e2e1e2e1e2DimensionalcollapseDecorrelatedFeaturesclass2class1decisionboundarydecorrelated(whitened)featuresdemonstratebettergeneralizationabilitytonoisydataembeddingspace",
    "reference": "# Summary Of The Paper\n\nThis paper finds that the conventional Graph Regularized MLPs(GR-MLPs) suffer from the dimensional collapse phenomenon that a few large eigenvalues dominate the embedding space and proposes ORTHO-REG, a GR-MLP model that encourages orthogonal embeddings to mitigate the issue.\n\n# Strength And Weaknesses\n\nThis paper demonstrates that the conventional GR-MLPs suffer from the dimensional collapse phenomenon in which a few large eigenvalues dominate the embedding space, thus restricting the representation power of these models through theoretical analysis and empirical results. To solve this problem, the authors propose ORTHO-REG, a GR-MLP model that adds regularization terms to encourage orthogonal embeddings and tackle high-order connectivity and non-homophily graphs. The model shows improved performance compared with conventional GR-MLPs and some GNNs on semi-supervised datasets and cold-start scenarios. In general, the paper explains the failure of existing GR-MLPs and is of good structure. However, there are some issues the authors need to address.\n\n1. The theoretical analysis in Sec.3 is questionable. Lemma 1 and Theorem 1 only consider the laplacian smoothing term as the loss function and do not consider the effect of classification loss, which makes the whole theoretical analysis fundamentally problematic. In fact, the proof of lemma 1 implicitly assumes that the laplacian smoothing loss will be optimized to zero. However, in most cases, the smoothing term will not converge to zero because of the classification loss’s influence; thus, the relative value of the smaller eigenvalues to the largest one may not always decrease. Therefore, the lemma, as well as the theorem, does not hold.\n\n2. In the experiment of Sec.3, the eigenspectra for node embeddings do not change much as the weight of the smoothing term increases, which may indicate the dimensional collapse phenomenon is not too severe. It also suggests that classification loss dominates the training process instead of smoothing loss, which contradicts the assumption of the above theoretical analysis.\n\n3. The relation between the dimensional collapse phenomenon and the poor performance of existing GR-MLPs should be explained. Sec.3 mainly demonstrates the existence of the dimensional collapse phenomenon. However, there is no evidence that this phenomenon causes performance degradation of existing models. Intuitively, a modest dimensional collapse might help improve model performance because it might filter noise from input features.\n\n4. Furthermore, the final model is inconsistent with the previous analysis because it introduces higher-order connectivity information, so it cannot be demonstrated that mitigating the dimensional collapse phenomenon alone will improve model performance. To verify this, experiments using the model based on eq.4 are suggested.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOn the whole, these are OK.\n\n# Summary Of The Review\n\nThis paper tries to show that the dimensional collapse phenomenon leads to poor performance of the existing GR-MLP model, but there are defects in both the experiment and the theory, which makes a claim not strong enough.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nJUMP-START REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nReinforcement learning (RL) provides a theoretical framework for continuously improving an agent’s behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks that present exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that it is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.\n\n1\n\nINTRODUCTION\n\nA promising aspect of reinforcement learning (RL) is the ability of a policy to iteratively improve via trial and error. Often, however, the most difficult part of this process is the very beginning, where a policy that is learning without any prior data needs to randomly encounter rewards to further improve. A common way to side-step this exploration issue is to aid the policy with prior knowledge. One source of prior knowledge might come in the form of a prior policy, which can provide some initial guidance in collecting data with non-zero rewards, but which is not by itself fully optimal. Such policies could be obtained from demonstration data (e.g., via behavioral cloning), from sub-optimal prior data (e.g., via offline RL), or even simply via manual engineering. In the case where this prior policy is itself parameterized as a function approximator, it could serve to simply initialize a policy gradient method. However, sample-efficient algorithms based on value functions are notoriously difficult to bootstrap in this way. As observed in prior work (Peng et al., 2019; Nair et al., 2020; Kostrikov et al., 2021; Lu et al., 2021), value functions require both good and bad data to initialize successfully, and the mere availability of a starting policy does not by itself readily provide an initial value function of comparable performance. This leads to the question we pose in this work: how can we bootstrap a value-based RL algorithm with a prior policy that attains reasonable but sub-optimal performance?\n\nThe main insight that we leverage to address this problem is that we can bootstrap any RL algorithm by gradually “rolling in” with the prior policy, which we refer to as the guide-policy. In particular, the guide-policy provides a curriculum of starting states for the RL exploration-policy, which significantly simplifies the exploration problem and allows for fast learning. As the exploration-policy improves, the effect of the guide-policy is diminished, leading to an RL-only policy that is capable of further autonomous improvement. Our approach is generic, as it can be applied to any RL method that explores its environment for policy improvement, though we focus on value-based methods in this work. The only requirements of our method are that the guide-policy can select actions based on observations of the environment, and its performance is reasonable (i.e., better than a random\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: We study how to efficiently bootstrap value-based RL algorithms given access to a prior policy. In vanilla RL (left), the agent explores randomly from the initial state until it encounters a reward (gold star). JSRL (right), leverages a guide-policy (dashed blue line) that takes the agent closer to the reward. After the guide-policy finishes, the exploration-policy (solid orange line) continues acting in the environment. As the exploration-policy improves, the influence of the guide-policy diminishes, resulting in a learning curriculum for bootstrapping RL.\n\npolicy). Since the guide-policy significantly speeds up the early phases of RL, we call this approach Jump-Start Reinforcement Learning (JSRL). We provide an overview diagram of JSRL in Fig. 1.\n\nJSRL can utilize any form of prior policy to accelerate RL. It is also compatible with RL algorithms that involve rolling out a policy to explore an environment. Thus, JSRL can easily be combined with existing offline and/or online RL methods. In addition, we provide a theoretical justification of JSRL by deriving an upper bound on its sample complexity compared to RL alternatives. Finally, we demonstrate that JSRL outperforms previously proposed imitation and reinforcement learning approaches on a set of benchmark tasks as well as more challenging vision-based robotic problems.\n\n2 RELATED WORK\n\nImitation learning combined with reinforcement learning (IL+RL). Several previous works on leveraging a prior policy to initialize RL focus on doing so by combining imitation learning and RL. Some methods treat RL as a sequence modelling problem and train an autoregressive model using offline data Zheng et al. (2022); Janner et al. (2021); Chen et al. (2021). One well-studied class of approaches initializes policy search methods with policies trained via behavioral cloning Schaal et al. (1997); Kober et al. (2010); Rajeswaran et al. (2017). This is an effective strategy for initializing policy search methods, but is generally ineffective with actor-critic or value-based methods, where the critic also needs to be initialized (Nair et al., 2020), as we also illustrate in Section 3. Methods have been proposed to include prior data in the replay buffer for a value-based approach (Nair et al., 2018; Vecerik et al., 2018), but this requires prior data rather than just a prior policy. More recent approaches improve this strategy by using offline RL Kumar et al. (2020); Nair et al. (2020); Lu et al. (2021) to pre-train on prior data, then finetune. We compare to such methods, showing that our approach not only makes weaker assumptions (requiring only a policy rather than a dataset), but also performs comparably or better.\n\nCurriculum learning and exact state resets for RL. Many prior works have investigated efficient exploration strategies in RL that are based on starting exploration from specific states. Commonly, these works assume the ability to reset to arbitrary states in simulation (Salimans & Chen, 2018). Some methods uniformly sample states from demonstrations as start states (Hosu & Rebedea, 2016; Peng et al., 2018; Nair et al., 2018), while others generate curriculas of start states. The latter includes methods that start at the goal state and iteratively expand the start state distribution, assuming reversible dynamics (Florensa et al., 2017; McAleer et al., 2019) or access to an approximate dynamics model (Ivanovic et al., 2019). Other approaches generate the curriculum from demonstration states (Resnick et al., 2018) or from online exploration (Ecoffet et al., 2021). In contrast, our method does not control the exact starting state distribution, but instead utilizes the implicit distribution\n\n0A project webpage is available at https://jumpstartrl.github.io\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nnaturally arising from rolling out the guide-policy. This broadens the distribution of start states compared to exact resets along a narrow set of demonstrations, making the learning process more robust. In addition, our approach could be extended to the real world, where resetting to a state in the environment is impossible.\n\nProvably efficient exploration techniques. Online exploration in RL has been well studied in theory (Osband & Van Roy, 2014; Jin et al., 2018; Zhang et al., 2020b; Xie et al., 2021; Zanette et al., 2020; Jin et al., 2020). The proposed methods either rely on the estimation of confidence intervals (e.g. UCB, Thompson sampling), which is hard to approximate and implement when combined with neural networks, or suffer from exponential sample complexity in the worst-case. In this paper, we leverage a pre-trained guide-policy to design an algorithm that is more sample-efficient than these approaches while being easy to implement in practice.\n\n“Rolling in” policies. Using a pre-existing policy (or policies) to initialize RL and improve exploration has been studied in past literature. Some works use an ensemble of roll-in policies or value functions to refine exploration Jiang et al. (2017); Agarwal et al. (2020). With a policy that models the environment’s dynamics, it is possible to look ahead to guide the training policy towards useful actions (Lin, 1992). Similar to our work, an approach from Smart & Pack Kaelbling (2002) rolls out a fixed controller to provide bootstrap data for a policy’s value function. However, this method does not mix the prior policy and the learned policy, but only uses the prior policy for data collection. We use a multi-stage curriculum to gradually reduce the contribution of the prior policy during training, which allows for on-policy experience for the learned policy. Our method is also conceptually related to DAgger (Ross & Bagnell, 2010), which also bridges distributional shift by rolling in with one policy and then obtaining labels from a human expert, but DAgger is intended for imitation learning and rolls in the learned policy, while our method addresses RL and rolls in with a sub-optimal guide-policy.\n\n3 PRELIMINARIES\n\nWe define a Markov decision process M = (S, A, P, R, p0, γ, H), where S and A are state and action spaces, P : S × A × S → R+ is a state-transition probability function, R : S × A → R is a reward function, p0 : S → R+ is an initial state distribution, γ is a discount factor, and H is the task horizon. Our goal is to effectively utilize a prior policy of any form in value-based reinforcement learning (RL). The goal of RL is to find a policy π(a|s) that maximizes the expected discounted reward over trajectories, τ , induced by the policy: Eπ[R(τ )] where s0 ∼ p0, st+1 ∼ P (·|st, at) and at ∼ π(·|st). To solve this maximization problem, value-based RL methods take advantage of state or state-action value functions (Q-function) Qπ(s, a), which can be learned using approximate dynamic programming approaches. The Q-function, Qπ(s, a), represents the discounted returns when starting from state s and action a, followed by the actions produced by the policy π. In order to leverage prior data in value-based RL and continue fine-tuning, researchers commonly use various offline RL methods (Kostrikov et al., 2021; Kumar et al., 2020; Nair et al., 2020; Lu et al., 2021) that often rely on pre-trained, regularized Qfunctions that can be further improved using online data. In the case where a pre-trained Q-function is not available and we only have access to a prior policy, value-based RL methods struggle to effectively incorporate that information as depicted in Fig. 2. In this experiment, we train an actor-critic method up to step 0, then we start from a fresh Q-function and\n\nFigure 2: Na ̈ıve policy initialization. We pre-train a policy to medium performance (depicted by negative steps), then use this policy to initialize actor-critic fine-tuning (starting from step 0), while initializing the critic randomly. Actor performance decays, as the untrained critic provides a poor learning signal, causing the good initial policy to be forgotten. In Figures 7 and 8, we repeat this experiment but allow the randomly initialized critic to ”warm up” before fine-tuning.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ncontinue with the pre-trained actor, simulating the case where we only have access to a prior policy. This is the setting that we are concerned with in this work.\n\n4\n\nJUMP-START REINFORCEMENT LEARNING\n\nIn this section, we describe our method, Jump-Start Reinforcement Learning (JSRL), that we use to initialize value-based RL algorithms with a prior policy of any form. We first describe the intuition behind our method then lay out a detailed algorithm along with theoretical analysis.\n\n4.1 ROLLING IN WITH TWO POLICIES\n\nWe assume access to a fixed prior policy that we refer to as the “guide-policy”, πg(a|s), which we leverage to initialize an RL algorithm. It is important to note that we do not assume any particular form of πg; it could be learned with imitation learning, RL, or it could be manually scripted. We will refer to the RL policy that is being learned via trial and error as the “exploration-policy” πe(a|s), since, as it is commonly done in RL literature, this is the policy that is used for exploration as well as online improvement. The only requirement for πe is that it is an RL policy that can adapt with online experience. Our approach and the set of assumptions is generic in that it can handle any downstream RL method that rolls out a policy for exploring an environment, though we focus on the case where πe is learned via a value-based RL algorithm.\n\nThe main idea behind our method is to leverage the two policies, πg and πe, executed sequentially to learn tasks more efficiently. During the initial phases of training, πg is significantly better than the untrained policy πe, so we would like to collect data using πg. However, this data is out of distribution for πe, since exploring with πe will visit different states. Therefore, we would like to gradually transition data collection away from πg and toward πe. Intuitively, we would like to use πg to get the agent into “good” states, and then let πe take over and explore from those states. As it gets better and better, πe should take over earlier and earlier, until all data is being collected by πe and there is no more distributional shift. We can employ different switching strategies to switch from πg to πe, but the most direct curriculum simply switches from πg to πe at some time step h, where h is initialized to the full task horizon and gradually decreases over the course of training. This naturally provides a curriculum for πe. At each curriculum stage, πe needs to master a small part of the state-space that is required to reach the states covered by the previous curriculum stage.\n\n4.2 ALGORITHM\n\nWe provide a detailed description of JSRL in Algorithm 1. Given an RL task with horizon H, we first choose a sequence of initial guide-steps to which we roll out our guide-policy, {H1, H2, · · · , Hn}, where Hi ∈ {1, 2, · · · , H} denotes the number of steps that the guide-policy at the ith iteration acts for. Let h denote the iterator over such a sequence of initial guide-steps. At the beginning of each training episode, we roll out πg for h steps, then πe continues acting in the environment for the additional H − h steps until the task horizon H is reached. We can write the combination of the two policies as the combined policy, π, where π1:h = πg and πh+1:H = πe. After we roll out π to collect online data, we use the new data to update our exploration-policy πe and combined policy π by calling a standard training procedure TRAINPOLICY. The TRAINPOLICY updates both the Q function and the corresponding evaluation policy. For example, the training procedure may be updating the exploration-policy via a Deep Q-Network (Mnih et al., 2013) with ε-greedy as the exploration technique (i.e. πe(a|s) = 1 − ε if a = arg maxa Q(s, a) and ε/|A| otherwise). The new combined policy is then evaluated over the course of training using a standard evaluation procedure EVALUATEPOLICY(π). Once the performance of the combined policy π reaches a threshold, β, we continue the for loop with the next guide step h.\n\nWhile any guide-step sequence could be used with JSRL, we focus on two specific strategies for determining guide-step sequences: curriculum and random-switching. With the curriculum strategy, we start with a large guide-step (ie. H1 = H) and use policy evaluations of the combined policy π to progressively decrease Hn as πe improves. Intuitively, this means that we train our policy in a backward manner by first rolling out πg to the last guide-step and then exploring with πe, and then rolling out πg to the second to last guide-step and exploring with πe, and so on. With the randomswitching strategy, we sample each h uniformly and independently from the set {H1, H2, · · · , Hn}.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nIn the rest of the paper, we refer to the curriculum variant as JSRL, and the random switching variant as JSRL-Random. Algorithm 1 Jump-Start Reinforcement Learning 1: Input: guide-policy πg, performance threshold β, task horizon H, a sequence of initial guide-steps\n\nH1, H2, · · · , Hn, where Hi ∈ {1, 2, · · · , H} for all i ≤ n.\n\n2: Initialize exploration-policy from scratch or with the guide-policy πe ← πg. Initialize Q-function ˆQ and\n\ndataset D ← ∅.\n\n3: for current guide step h = H1, H2, · · · , Hn do 4: 5:\n\nSet the non-stationary policy π1:h = πg, πh+1:H = πe Roll out the policy π to get trajectory {(s1, a1, r1), · · · , (sH , aH , rH )}; Append the trajectory to the\n\ndataset D.\n\nπe, ˆQ ← TRAINPOLICY(πe, ˆQ, D) if EVALUATEPOLICY(π) ≥ β then\n\nContinue\n\nend if\n\n6: 7: 8: 9:\n\n10: end for\n\n4.3 THEORETICAL ANALYSIS\n\nIn this section, we provide theoretical analysis of JSRL, showing that the roll-in data collection strategy that we propose provably attains polynomial sample complexity. The sample complexity refers to the number of samples required by the algorithm to learn a policy with small suboptimality, where we define the suboptimality for a policy π as Es∼p0[V ⋆(s) − V π(s)]. In particular, we aim to answer two questions: Why is JSRL better than other exploration algorithms which start exploration from scratch? Under which conditions does the guide-policy provably improve exploration? To answer these questions, we study upper and lower bounds for the sample complexity of exploration algorithms. We first provide a lower bound showing that simple non-optimism-based exploration algorithms like ε-greedy suffer from a sample complexity that is exponential in the horizon. Then, we show that with the help of a guide-policy with good coverage of important states, the JSRL algorithm with ε-greedy as the exploration strategy can achieve polynomial sample complexity.\n\nWe focus on comparing JSRL with standard non-optimism-based exploration methods, e.g. εgreedy (Langford & Zhang, 2007) and FALCON+ (Simchi-Levi & Xu, 2020). Although the optimismbased RL algorithms like UCB (Jin et al., 2018) and Thompson sampling (Ouyang et al., 2017) turn out to be efficient strategies for exploration from scratch, they all require uncertainty quantification, which can be hard for vision-based RL tasks with neural network parameterization. Note that the cross entropy method used in the vision-based RL framework Qt-Opt (Kalashnikov et al., 2018) is also a non-optimism-based method. In particular, it can be viewed as a variant of ε-greedy algorithm in continuous action space, with the Gaussian distribution as the exploration distribution.\n\nWe first show that without the help of a guide-policy, the non-optimism-based method usually suffers from a sample complexity that is exponential in horizon for episodic MDP. We adapt the combination lock example in Koenig & Simmons (1993) to show the hardness of exploration from scratch for non-optimism-based methods.\n\nTheorem 4.1 (Koenig & Simmons (1993)). For 0-initialized ε-greedy, there exists an MDP instance such that one has to suffer from a sample complexity that is exponential in total horizon H in order to find a policy that has suboptimality smaller than 0.5.\n\nWe include the construction of combination lock MDP and the proof in Appendix A.4.2 for completeness. This lower bound also applies to any other non-optimism-based exploration algorithm which explores uniformly when the estimated Q for all actions are 0. As a concrete example, this also shows that iteratively running FALCON+ Simchi-Levi & Xu (2020) suffers from exponential sample complexity. With the above lower bound, we are ready to show the upper bound for JSRL under certain assumptions on the guide-policy. In particular, we assume that the guide-policy πg is able to cover good states that are visited by the optimal policy under some feature representation:\n\nAssumption 4.2 (Quality of guide-policy πg). Let dπ(s) be the marginalized state occupancy distribution when we follow policy π. Assume that the state is parametrized by some feature mapping φ : S (cid:55)→ Rd such that for any policy π, Qπ(s, a) and π(s) depend on s only through φ(s), and that in the feature space, the guide-policy πg cover the states visited by the optimal policy:\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ndπ⋆ dπg\n\n≤ C.\n\nsup s,h\n\nh (φ(s)) h (φ(s)) We provide formal definition of the marginalized state occupancy distribution in Appendix A.4. In other words, the guide-policy visits only all good states in the feature space. A policy that satisfies Assumption 4.2 may be far from optimal due to wrong choice of actions in each step. Assumption 4.2 is also much weaker than the single policy concentratability coefficient assumption, which requires the guide-policy visits all good state and action pairs and is a standard assumption in the literature in offline learning Rashidinejad et al. (2021); Xie et al. (2021). The ratio in Assumption 4.2 is also sometimes referred to as the distribution mismatch coefficient in the literature of policy gradient methods Agarwal et al. (2021).\n\nWe show via the following theorem that given Assumption 4.2, a simplified JSRL algorithm which only explores at current guide step h + 1 gives good performance guarantees for both tabular MDP and MDP with general function approximation. The simplified JSRL algorithm coincides with the Policy Search by Dynamic Programming (PSDP) algorithm in Bagnell et al. (2003), although our method is mainly motivated by the problem of fine-tuning and efficient exploration in value based methods, while PSDP focuses on policy-based methods. Theorem 4.3 (Informal). Under Assumption 4.2 and an appropriate choice of TrainPolicy and EvaluatePolicy, JSRL in Algorithm 1 guarantees a suboptimality of O(CH 5/2S1/2A/T 1/2) for tabular MDP; and a near-optimal bound up to factor of C · poly(H) for MDP with general function approximation.\n\nTo achieve a polynomial bound for JSRL, it suffices to take TrainPolicy as ε-greedy. This is in sharp contrast to Theorem 4.1, where ε-greedy suffers from exponential sample complexity. As is discussed in the related work section, although polynomial and even near-optimal bound can be achieved by many optimism-based methods Jin et al. (2018); Ouyang et al. (2017), the JSRL algorithm does not require constructing a bonus function for uncertainty quantification, and can be implemented easily based on na ̈ıve ε-greedy methods. Furthermore, although we focus on analyzing the simplified JSRL which only updates policy π at current guide steps h + 1, in practice we run a JSRL algorithm as in Algorithm 1, which updates all policies after step h + 1. This is the main difference between our proposed algorithm and PSDP. For a formal statement and more discussion related to Theorem 4.3, please refer to Appendix A.4.3.\n\n5 EXPERIMENTS In our experimental evaluation, we study the following questions: (1) How does JSRL compare with competitive IL+RL baselines? (2) Does JSRL scale to complex vision-based robotic manipulation tasks? (3) How sensitive is JSRL to the quality of the guide-policy? (4) How important is the curriculum component of JSRL? (5) Does JSRL generalize? That is, can a guide-policy still be useful if it was pre-trained on a related task?\n\n5.1 COMPARISON WITH IL+RL BASELINES\n\nTo study how JSRL compares with competitive IL+RL methods, we utilize the D4RL (Fu et al., 2020) benchmark tasks, which vary in task complexity and offline dataset quality. We focus on the most challenging D4RL tasks: Ant Maze and Adroit manipulation. We consider a common setting where the agent first trains on an offline dataset (1m transitions for Ant Maze, 100k transitions for Adroit) and then runs online fine-tuning for 1m steps. We compare against algorithms designed specifically for this setting, which include AWAC (Nair et al., 2020), IQL (Kostrikov et al., 2021), CQL (Kumar et al., 2020), and behavior cloning (BC). While JSRL can be used in combination with any initial guide-policy or fine-tuning algorithm, we show the combination of JSRL with the strongest baseline, IQL. IQL (Implicit Q-Learning) is an actor-critic method that completely avoids estimating the values of actions that are not seen in the offline dataset. This is a recent state-of-the-art method for the IL+RL setting we consider. In Table 1, we see that across the Ant Maze environments and Adroit environments, IQL+JSRL is able to successfully fine-tune given an initial offline dataset, and is competitive with baselines. We will come back for further analysis of Table 1 when discussing the sensitivity to the size of the dataset.\n\n1The AWAC, BC, and CQL performance scores for D4RL are taken from Kostrikov et al. (2021) which only\n\nevaluated settings with full-sized datasets.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: We evaluate the importance of guide-policy quality for JSRL on Instance Grasping, the most challenging task we consider. By limiting the initial demonstrations, JSRL is less sensitive to limitations of initial demonstrations compared to baselines, especially in the small-data regime. For each of these initial demonstration settings, we find that Qt-Opt+JSRL is more sample efficient than Qt-Opt+JSRL-Random in early stages of training, but converge to the same final performances. A similar analysis for Indiscriminate Grasping is provided in Fig. 10 in the Appendix.\n\nFigure 4: fine-tuning, but Qt-Opt+JSRL is more sample efficient and attains higher final performance.\n\nIL+RL methods on two simulated robotic grasping tasks. The baselines show improvement with\n\n5.2 VISION-BASED ROBOTIC TASKS Utilizing offline data is challenging in complex tasks such as vision-based robotic manipulation. The high dimensionality of both the continuous control action space as well as the pixel-based state space present unique scaling challenges for IL+RL methods. To study how JSRL scales to such settings, we focus on two simulated robotic manipulation tasks: Indiscriminate Grasping and Instance Grasping. In these tasks, a simulated robot arm is placed in front of a table with various categories of objects. When the robot lifts any object, a sparse reward is given for the Indiscriminate Grasping task; for the more challenging Instance Grasping task, the sparse reward is only given when a sampled target object is grasped. An image of the task is shown in Fig. 5 and described in detail in Appendix A.1.2. We compare JSRL against methods that have been shown to scale to such complex vision-based robotics settings: Qt-Opt (Kalashnikov et al., 2018), AW-Opt (Lu et al., 2021), and BC. Each method has access to the same offline dataset of 2,000 successful demonstrations and is allowed to run online fine-tuning for up to 100,000 steps. While AW-Opt and BC utilize offline successes as part of their original design motivation, we allow a more fair comparison for Qt-Opt by initializing the replay buffer with the offline demonstrations, which was not the case in the original Qt-Opt paper. Since we have already shown that JSRL can work well with an offline RL algorithm in the previous experiment, to demonstrate the flexibility of our approach, in this experiment we combine JSRL with an online Q-learning method: Qt-Opt. As seen in Fig. 4, the combination of Qt-Opt+JSRL (both versions of the curricula) outperforms the other methods in both sample efficiency as well as final performance.\n\n5.3\n\nINITIAL DATASET SENSITIVITY\n\nWhile most IL+RL methods are improved by more data and higher quality data, there are often practical limitations that restrict initial offline datasets. JSRL is no exception to this dependency, as\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nEnvironment\n\nDataset\n\nAWAC1\n\nBC1\n\nCQL1\n\nIQL\n\nIQL+JSRL (Ours)\n\nantmaze-umaze-v0\n\nantmaze-umaze-diverse-v0\n\nantmaze-medium-play-v0\n\nantmaze-medium-diverse-v0\n\nantmaze-large-play-v0\n\nantmaze-large-diverse-v0\n\npen-binary-v0\n\ndoor-binary-v0\n\nrelocate-binary-v0\n\n1k 10k 100k 1m (standard) 1k 10k 100k 1m (standard) 1k 10k 100k 1m (standard) 1k 10k 100k 1m (standard) 1k 10k 100k 1m (standard) 1k 10k 100k 1m (standard) 100 1k 10k 100k (standard) 100 1k 10k 100k (standard) 100 1k 10k 100k (standard)\n\n– –\n– 59.0 –\n– –\n49.0 –\n– –\n0.0 –\n– –\n0.3 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n70.3 –\n– –\n30.1 –\n– –\n2.7\n\n– –\n– 54.6 –\n– –\n45.6 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n0.0\n\n– –\n– 99.4 –\n– –\n99.4 –\n– –\n0.0 –\n– –\n32.3 –\n– –\n0.0 –\n– –\n0.0 –\n– –\n9.9 –\n– –\n0.0 –\n– –\n0.0\n\n0.2 ± 0.5 55.5 ± 12.5 74.2 ± 25.6 97.6 ± 3.2 0.0 ± 0.0 33.1 ± 10.7 29.9 ± 23.1 53.0 ± 30.5 0.0 ± 0.0 0.1 ± 0.3 32.8 ± 32.6 92.8 ± 2.7 0.0 ± 0.0 0.0 ± 0.0 15.7 ± 17.7 92.4 ± 4.5 0.0 ± 0.0 0.0 ± 0.0 2.6 ± 8.2 62.4 ± 12.4 0.0 ± 0.0 0.0 ± 0.0 4.1 ± 10.4 68.3 ± 8.9 18.8 ± 11.6 30.1 ± 10.2 38.4 ± 11.2 65.0 ± 2.9 0.8 ± 3.8 0.5 ± 1.5 10.6 ± 14.1 50.2 ± 2.5 0.0 ± 0.0 0.0 ± 0.0 0.2 ± 0.3 8.6 ± 7.7\n\nCurriculum 15.6 ± 19.9 71.7 ± 14.5 93.7 ± 4.2 98.1 ± 1.4 3.1 ± 8.0 72.6 ± 12.2 81.3 ± 23.0 88.6 ± 16.3 0.0 ± 0.0 16.7 ± 12.9 86.7 ± 3.7 91.1 ± 3.9 0.0 ± 0.0 16.6 ± 11.7 81.5 ± 18.8 93.1 ± 3.1 0.0 ± 0.0 0.1 ± 0.2 36.3 ± 16.4 62.9 ± 11.3 0.0 ± 0.0 0.1 ± 0.2 34.4 ± 23.0 68.3 ± 8.8 24.3 ± 12.1 36.7 ± 7.9 44.3 ± 6.2 62.6 ± 3.6 0.4 ± 1.8 0.7 ± 1.0 4.3 ± 8.4 28.5 ± 19.5 0.0 ± 0.1 0.0 ± 0.1 0.6 ± 1.6 0.0 ± 0.1\n\nRandom 10.4 ± 9.6 52.3 ± 26.7 92.1 ± 2.8 95.0 ± 3.0 1.9 ± 4.8 39.4 ± 20.1 82.3 ± 14.2 89.8 ± 10.0 0.0 ± 0.0 3.8 ± 5.0 56.2 ± 28.8 87.8 ± 4.2 0.0 ± 0.0 5.1 ± 8.2 67.0 ± 17.4 86.3 ± 5.9 0.0 ± 0.0 0.0 ± 0.0 17.7 ± 13.4 48.6 ± 10.0 0.0 ± 0.0 0.0 ± 0.0 22.4 ± 15.4 58.3 ± 6.5 29.1 ± 7.6 46.3 ± 6.3 52.1 ± 3.3 60.6 ± 2.7 0.1 ± 0.2 0.45 ± 1.2 22.3 ± 11.6 24.3 ± 11.5 0.0 ± 0.0 0.0 ± 0.0 0.5 ± 0.7 4.7 ± 4.2\n\nTable 1: Comparing JSRL with IL+RL baselines on D4RL tasks by using averaged normalized scores for D4RL Ant Maze and Adroit tasks. Each method pretrains on an offline dataset and then runs online finetuning for 1m steps. Our method IQL+JSRL is competitive with IL+RL baselines in the full dataset setting, but performs significantly better in the small-data regime. For implementation details and more detailed comparisons, see Appendix A.2.\n\nEnvironment Indiscriminate Grasping Indiscriminate Grasping Indiscriminate Grasping Indiscriminate Grasping Instance Grasping Instance Grasping Instance Grasping Instance Grasping\n\n# Demos 20 200 2k 20k 20 200 2k 20k\n\nQt-Opt 0.0 ± 0.0 0.93 ± 0.01 0.94 ± 0.01 0.94 ± 0.01 0.23 ± 0.20 0.47 ± 0.04 0.15 ± 0.26 0.28 ± 0.25\n\nAW-Opt 0.0 ± 0.0 0.96 ± 0.02 0.97 ± 0.01 0.98 ± 0.01 0.47 ± 0.04 0.49 ± 0.02 0.43 ± 0.03 0.57 ± 0.01\n\nBC 0.19 ± 0.04 0.23 ± 0.00 0.44 ± 0.05 0.91 ± 0.01 0.05 ± 0.04 0.15 ± 0.02 0.28 ± 0.04 0.49 ± 0.02\n\nQt-Opt+JSRL (Ours) 0.92 ± 0.00 0.92 ± 0.01 0.94 ± 0.03 0.95 ± 0.00 0.50 ± 0.09 0.54 ± 0.03 0.57 ± 0.07 0.58 ± 0.02\n\nTable 2: Limiting the initial number of demonstrations is challenging for IL+RL baselines on the difficult robotic grasping tasks. Notably, only Qt-Opt+JSRL is able to learn in the smallest-data regime of just 20 demonstrations, 100x less than the standard 2,000 demonstrations.\n\nthe quality of the guide-policy πg directly depends on the offline dataset when utilizing JSRL in an IL+RL setting (i.e., when the guide-policy is pre-trained on an offline dataset). We study the offline dataset sensitivity of IL+RL algorithms and JSRL on both D4RL tasks as well as the vision-based robotic grasping tasks. The two settings presented in D4RL and Robotic Grasping are quite different: IQL+JSRL in D4RL pretrains with an offline RL algorithm from a mixed quality offline dataset, while Qt-Opt+JSRL pretrains with BC from a high quality dataset.\n\nFor D4RL, methods typically use 1 million transitions from mixed-quality policies from previous RL training runs; as we reduce the size of the offline datasets in Table 1, IQL+JSRL performance degrades less than the baseline IQL performance. For the robotic grasping tasks, we provided 2,000 highquality demonstrations. As we reduce the number of demonstrations, we find that JSRL efficiently\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nlearns better policies. Across both D4RL and the robotic grasping tasks, JSRL outperforms baselines in the low-data regime, as shown in Table 1 and Table 2. In the high-data regime, when we increase the number of demonstrations by 10x to 20,000 demonstrations, we notice that AW-Opt and BC perform much more competitively, suggesting that the exploration challenge is no longer the bottleneck. While starting with such large numbers of demonstrations is not typically a realistic setting, this results suggests that the benefits of JSRL are most prominent when the offline dataset does not densely cover good state-action pairs. This aligns with our analysis in Appendix A.1 that JSRL does not require such assumptions about the dataset, but solely requires a prior policy.\n\n5.4\n\nJSRL-CURRICULUM VS. JSRL-RANDOM SWITCHING\n\nIn order to disentangle these two components, we propose an augmentation of our method, JSRLRandom, that randomly selects the number of guide-steps every episode. Using the D4RL tasks and the robotic grasping tasks, we compare JSRL-Random to JSRL and previous IL+RL baselines and find that JSRL-Random performs quite competitively, as seen in Table 1 and Table 2. However, when considering sample efficiency, Fig. 4 shows that JSRL is better than JSRL-Random in early stages of training, while converged performance is comparable. These same trends hold when we limit the quality of the guide-policy by constraining the initial dataset, as seen in Fig. 3. This suggests that while a curriculum of guide-steps does help sample efficiency, the largest benefits of JSRL may stem from the presence of good visitation states induced by the guide-policy as opposed to the specific order of good visitation states, as suggested by our analysis in Appendix A.4.3. For analyze hyperparameter sensitivity of JSRL-Curriculum and provide the specific implementation of hyperparameters chosen for our experiments in Appendix A.3.\n\n5.5 GUIDE-POLICY GENERALIZATION\n\nIn order to study how guide-policies from easier tasks can be used to efficiently explore more difficult tasks, we train an indiscriminate grasping policy and use it as the guide-policy for JSRL on instance grasping (Figure 13). While the performance when using the indiscriminate guide is worse than using the instance guide, the performance for both JSRL versions outperform vanilla Qt-Opt. We also test JSRL ’s generalization capabilities in the D4RL setting. We consider two variations of Ant mazes: ”play” and ”diverse”. In antmaze-*-play, the agent must reach a fixed set of goal locations from a fixed set of starting locations. In antmaze-*-diverse, the agent must reach random goal locations from random starting locations. Thus, the diverse environments present a greater challenge than the corresponding play environments. In Figure 14, we see that JSRL is able to better generalize to unseen goal and starting locations compared to vanilla IQL.\n\n6 CONCLUSION\n\nIn this work, we propose Jump-Start Reinforcement Learning (JSRL), a method for leveraging a prior policy of any form to bolster exploration in RL to increase sample efficiency. Our algorithm creates a learning curriculum by rolling in a pre-existing guide-policy, which is then followed by the self-improving exploration policy. The job of the exploration-policy is simplified, as it starts its exploration from states closer to the goal. As the exploration policy improves, the effect of the guidepolicy diminishes, leading to a fully capable RL policy. Importantly, our approach is generic since it can be used with any RL method including value-based RL approaches, which have traditionally struggled in this setting. We showed the benefits of JSRL in a set of offline RL benchmark tasks as well as more challenging vision-based robotic simulation tasks. Our experiments indicate that JSRL is more sample efficient than more complex IL+RL approaches while being compatible with other approaches’ benefits. In addition, we presented theoretical analysis of an upper bound on the sample complexity of JSRL , which showed from-exponential-to-polynomial improvement in time horizon from non-optimism exploration methods. In the future, we plan on deploying JSRL in the real world in conjunction with various types of guide-policies to further investigate its ability to bootstrap data efficient RL.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed exploration\n\nfor provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.\n\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1–76, 2021.\n\nJ Andrew Bagnell. Learning decisions: Robustness, uncertainty, and approximation. Carnegie\n\nMellon University, 2004.\n\nJames Bagnell, Sham M Kakade, Jeff Schneider, and Andrew Ng. Policy search by dynamic\n\nprogramming. Advances in neural information processing systems, 16, 2003.\n\nJinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.\n\narXiv preprint arXiv:1905.00360, 2019.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.\n\nWei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 208–214. JMLR Workshop and Conference Proceedings, 2011.\n\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then\n\nexplore. Nature, 590(7847):580–586, 2021.\n\nCarlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generation for reinforcement learning. In Conference on robot learning, pp. 482–495. PMLR, 2017.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nIonel-Alexandru Hosu and Traian Rebedea. Playing atari games with deep reinforcement learning\n\nand human checkpoint replay. arXiv preprint arXiv:1607.05077, 2016.\n\nBoris Ivanovic, James Harrison, Apoorva Sharma, Mo Chen, and Marco Pavone. Barc: Backward reachability curriculum for robotic reinforcement learning. In 2019 International Conference on Robotics and Automation (ICRA), pp. 15–21. IEEE, 2019.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. Advances in neural information processing systems, 34, 2021.\n\nNan Jiang. On value functions and the agent-environment boundary. arXiv preprint arXiv:1905.13341,\n\n2019.\n\nNan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In International Conference on Machine Learning, pp. 1704–1713. PMLR, 2017.\n\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 4868–4878, 2018.\n\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143. PMLR, 2020.\n\nSham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In\n\nICML, volume 2, pp. 267–274, 2002.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.\n\nJens Kober, Betty Mohler, and Jan Peters. Imitation and reinforcement learning for motor primitives with perceptual coupling. In From motor learning to interaction learning in robots, pp. 209–225. Springer, 2010.\n\nSven Koenig and Reid G Simmons. Complexity analysis of real-time reinforcement learning. In\n\nAAAI, pp. 99–107, 1993.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nq-learning. arXiv preprint arXiv:2110.06169, 2021.\n\nAkshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual bandits with continuous actions: Smoothing, zooming, and adapting. In Conference on Learning Theory, pp. 2025–2027. PMLR, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\n\nreinforcement learning. arXiv preprint arXiv:2006.04779, 2020.\n\nJohn Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits.\n\nAdvances in neural information processing systems, 20(1):96–1, 2007.\n\nPeng Liao, Zhengling Qi, and Susan Murphy. Batch policy learning in average reward Markov\n\ndecision processes. arXiv preprint arXiv:2007.11771, 2020.\n\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\n\nMachine learning, 8(3-4):293–321, 1992.\n\nBoyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimization\n\nattains globally optimal policy. In Neural Information Processing Systems, 2019.\n\nYao Lu, Karol Hausman, Yevgen Chebotar, Mengyuan Yan, Eric Jang, Alexander Herzog, Ted Xiao, Alex Irpan, Mohi Khansari, Dmitry Kalashnikov, and Sergey Levine. Aw-opt: Learning robotic skills with imitation andreinforcement at scale. In 2021 Conference on Robot Learning (CoRL), 2021.\n\nStephen McAleer, Forest Agostinelli, Alexander Shmakov, and Pierre Baldi. Solving the rubik’s cube\n\nwithout human knowledge. 2019.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nAshvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 6292–6299. IEEE, 2018.\n\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online\n\nreinforcement learning with offline datasets. 2020.\n\nIan Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.\n\narXiv preprint arXiv:1406.1853, 2014.\n\nYi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning unknown markov decision\n\nprocesses: A thompson sampling approach. arXiv preprint arXiv:1709.04570, 2017.\n\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills. ACM Trans. Graph., 37(4), July 2018.\n\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\n\nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021, 2021.\n\nCinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, and Joan Bruna. Backplay:” man muss immer umkehren”. arXiv preprint arXiv:1807.06919, 2018.\n\nSt ́ephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 661–668, 2010.\n\nTim Salimans and Richard Chen. Learning montezuma’s revenge from a single demonstration. arXiv\n\npreprint arXiv:1812.03381, 2018.\n\nStefan Schaal et al. Learning from demonstration. Advances in neural information processing\n\nsystems, pp. 1040–1046, 1997.\n\nBruno Scherrer. Approximate policy iteration schemes: A comparison. In International Conference\n\non Machine Learning, pp. 1314–1322, 2014.\n\nDavid Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm\n\nfor contextual bandits under realizability. Available at SSRN 3562765, 2020.\n\nW.D. Smart and L. Pack Kaelbling. Effective reinforcement learning for mobile robots. In Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292), volume 4, pp. 3404–3410 vol.4, 2002. doi: 10.1109/ROBOT.2002.1014237.\n\nMel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth ̈orl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards, 2018.\n\nLingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. In International Conference on Learning Representations, 2019.\n\nTengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. arXiv preprint arXiv:2106.04895, 2021.\n\nAndrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning, pp. 10978–10989. PMLR, 2020.\n\nJunyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020a.\n\nZihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-advantage decomposition. Advances in Neural Information Processing Systems, 33, 2020b.\n\nQinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. arXiv preprint\n\narXiv:2202.05607, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Example ant maze (left) and adroit dexterous manipulation (right) tasks.\n\nA APPENDIX\n\nA.1 EXPERIMENT IMPLEMENTATION DETAILS\n\nA.1.1 D4RL: ANT MAZE AND ADROIT\n\nWe evaluate on the Ant Maze and Adroit tasks, the most challenging tasks in the D4RL benchmark Fu et al. (2020). For the baseline IL+RL method comparisons, we utilize implementations and reported results from Kostrikov et al. (2021): we use the open-sourced version of IQL and the reported results from for AWAC, BC, and CQL. While the standard initial offline datasets contain 1m transitions for Ant Maze and 100k transitions for Adroit, we additionally ablate the datasets to evaluate settings with 100, 1k, 10k, and 100k transitions provided initially.\n\nFor the implementation of IQL+JSRL, we build upon the open-sourced IQL implementation Kostrikov et al. (2021). First, to obtain a guide-policy, we use IQL without modification for pretraining on the offline dataset. Then, we follow Algorithm 1 when finetuning online and use the IQL online update as the TRAINPOLICY step from Algorithm 1. The IQL neural network architecture follows the original implementation of Kostrikov et al. (2021). For finetuning, we maintain two replay buffers for offline and online transitions. The offline buffer contains all the demonstrations, and the online buffer is FIFO with a fixed capacity of 100k transitions. For each gradient update during finetuning, we sample minibatches such that 75% of samples come from the online buffer, and 25% of samples come from the offline buffer.\n\nFigure 5: In the simulated vision-based robotic grasping tasks, a robot arm must grasp various objects placed in bins in front of it. Full implementation details are described in Appendix A.1.2.\n\nOur implementation of IQL+JSRL focused on two settings when switching from offline pretraining to online finetuning: Warm-starting and Cold-starting. When Warm-starting, we copy the actor, critic, target critic, and value networks from the pre-trained guide-policy to the exploration-policy. When Cold-starting, we instead start training the exploration-policy from scratch. Results for both variants are shown in Appendix A.2. We find that empirically, the performance of these two variants is highly dependent on task difficulty as well as the quality of the initial offline dataset. When initial datasets are very poor, cold-starting usually performs better; when initial datasets are dense and high-quality, warm-starting seems to perform better. For the results reported in Table 1, we utilize Cold-start results for both IQL+JSRL-Curriculum and IQL+JSRL-Random.\n\nFinally, the curriculum implementation for IQL+JSRL used policy evaluation every 10,000 steps to gauge learning progress of the exploration-policy πe. When the moving average of πe’s performance\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nincreases over a few samples, we move on to the next curriculum stage. For the IQL+JSRL-Random variant, we randomly sample the number of guide-steps for every single episode.\n\nA.1.2 SIMULATED ROBOTIC MANIPULATION\n\nWe simulate a 7 DoF arm with an over-the-shoulder camera (see Figure 5) Three bins in front of the robot are filled with various simulated objects to be picked up by the robot and a sparse binary reward is assigned if any object is lifted above a bin at the end of an episode. States are represented in the form of RGB images and actions are continuous Cartesian displacements of the gripper’s 3D positions and yaw. In addition, the policy commands discrete gripper open and close actions and may terminate an episode.\n\nFor the implementation of Qt-Opt+JSRL, we build upon the Qt-Opt algorithm described in Kalashnikov et al. (2018). First, to obtain a guide-policy we use a BC policy trained offline on the provided demonstrations. Then, we follow Algorithm 1 when finetuning online and use the Qt-Opt online update as the TRAINPOLICY step from Algorithm 1. The demonstrations are not added to the QtOpt+JSRLreplay buffer. The Qt-Opt neural network architecture follows the original implementation in Kalashnikov et al. (2018).\n\nFinally, similar to Appendix A.1.1, the curriculum implementation for Qt-Opt+JSRLused policy evaluation every 1,000 steps to gauge learning progress of the exploration-policy πe. When the moving average of πe’s performance increases over a few samples, the number of guide-steps is lowered, allowing the JSRL curriculum to continue. For the Qt-Opt+JSRL-Random variant, we randomly sample the number of guide-steps for every single episode.\n\nA.2 ADDITIONAL EXPERIMENTS\n\nEnvironment pen-binary-v0 door-binary-v0 relocate-binary-v0\n\nJSRL: Random Switching Cold-start 29.12 ± 7.62 0.06 ± 0.23 0.00 ± 0.00\n\nWarm-start 27.18 ± 7.77 0.01 ± 0.04 0.00 ± 0.00\n\nJSRL: Curriculum\n\nWarm-start 25.10 ± 8.73 1.45 ± 4.67 0.00 ± 0.00\n\nCold-start 24.31 ± 12.05 0.40 ± 1.80 0.01 ± 0.06\n\nIQL 18.80 ± 11.63 0.84 ± 3.76 0.01 ± 0.03\n\nTable 3: Adroit 100 Offline Transitions\n\nEnvironment pen-binary-v0 door-binary-v0 relocate-binary-v0\n\nJSRL: Random Switching Cold-start Warm-start 46.30 ± 6.34 47.23 ± 3.96 0.45 ± 1.22 0.15 ± 0.25 0.01 ± 0.04 0.06 ± 0.08\n\nJSRL: Curriculum\n\nWarm-start 34.23 ± 7.22 0.44 ± 0.89 0.05 ± 0.09\n\nCold-start 36.74 ± 7.91 0.68 ± 1.02 0.04 ± 0.10\n\nIQL 30.11 ± 10.22 0.53 ± 1.46 0.01 ± 0.03\n\nTable 4: Adroit 1k Offline Transitions\n\nEnvironment pen-binary-v0 door-binary-v0 relocate-binary-v0\n\nIQL+JSRL: Random Switching Warm-start 51.78 ± 3.00 10.59 ± 11.78 1.99 ± 3.15\n\nCold-start 52.11 ± 3.30 22.32 ± 11.61 0.50 ± 0.65\n\nIQL+JSRL: Curriculum\n\nWarm-start 38.04 ± 12.71 5.08 ± 7.60 4.39 ± 8.17\n\nCold-start 44.31 ± 6.22 4.33 ± 8.38 0.55 ± 1.60\n\nIQL 38.41 ± 11.18 10.61 ± 14.11 0.19 ± 0.32\n\nTable 5: Adroit 10k Offline Transitions\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: A policy is first pre-trained on 100k offline transitions. Negative steps correspond to this pre-training. We then roll out the pre-trained policy for 100k timesteps, and use these online samples to warm-up the critic network. After warming up the critic, we continue with actor-critic fine-tuning with the pre-trained policy and the warmed up critic.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: A policy is first pre-trained on one million offline transitions. Negative steps correspond to this pre-training. We then roll out the pre-trained policy for 100k timesteps, and use these online samples to warm-up the critic network. After warming up the critic, we continue with actor-critic fine-tuning with the pre-trained policy and the warmed up critic. Allowing the critic to warm up provides a stronger baseline to compare JSRL to, since in the case where we have a policy, but no value function, we could use that policy to train a value function.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: QT-Opt+JSRL using guide-policies trained from-scratch online vs. guide-policies trained with BC on demonstration data in the indiscriminate grasping environment. For each experiment, the guide-policy trained offline and the guide-policy trained online are of equivalent performance.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Comparing IL+RL methods with JSRL on the Indiscriminate Grasping task while adjusting the initial demonstrations available. In addition, compare the sample efficiency\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: Comparing IL+RL methods with JSRL on the Instance Grasping task while adjusting the initial demonstrations available.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nIQL+JSRL: Random Switching\n\nIQL+JSRL: Curriculum\n\nEnvironment pen-binary-v0 door-binary-v0 relocate-binary-v0\n\nWarm-start 60.06 ± 2.94 27.23 ± 8.90 5.09 ± 4.39\n\nCold-start 60.58 ± 2.73 24.27 ± 11.47 4.69 ± 4.16\n\nWarm-start 62.81 ± 2.79 38.70 ± 17.25 11.18 ± 11.69\n\nCold-start 62.59 ± 3.62 28.51 ± 19.54 0.04 ± 0.14\n\nIQL 64.96 ± 2.87 50.21 ± 2.50 8.59 ± 7.70\n\nTable 6: Adroit 100k Offline Transitions\n\nEnvironment antmaze-umaze-v0 antmaze-umaze-diverse-v0 antmaze-medium-play-v0 antmaze-medium-diverse-v0 antmaze-large-play-v0 antmaze-large-diverse-v0\n\nIQL+JSRL: Random Switching Warm-start 0.10 ± 0.31 0.10 ± 0.31 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nCold-start 10.35 ± 9.59 1.90 ± 4.81 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nIQL+JSRL: Curriculum Cold-start 15.60 ± 19.87 3.05 ± 7.99 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nWarm-start 0.40 ± 0.94 0.45 ± 1.23 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nIQL 0.20 ± 0.52 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nTable 7: Ant Maze 1k Offline Transitions\n\nIQL+JSRL: Random Switching\n\nIQL+JSRL: Curriculum\n\nEnvironment antmaze-umaze-v0 antmaze-umaze-diverse-v0 antmaze-medium-play-v0 antmaze-medium-diverse-v0 antmaze-large-play-v0 antmaze-large-diverse-v0\n\nWarm-start 56.00 ± 13.70 23.05 ± 10.96 0.05 ± 0.22 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nCold-start 52.70 ± 26.71 39.35 ± 20.07 3.75 ± 4.97 5.10 ± 8.16 0.00 ± 0.00 0.00 ± 0.00\n\nWarm-start 57.25 ± 15.86 26.80 ± 12.03 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nCold-start 71.70 ± 14.49 72.55 ± 12.18 16.65 ± 12.93 16.60 ± 11.71 0.05 ± 0.22 0.05 ± 0.22\n\nIQL 55.50 ± 12.51 33.10 ± 10.74 0.10 ± 0.31 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00\n\nTable 8: Ant Maze 10k Offline Transitions\n\nEnvironment antmaze-umaze-v0 antmaze-umaze-diverse-v0 antmaze-medium-play-v0 antmaze-medium-diverse-v0 antmaze-large-play-v0 antmaze-large-diverse-v0\n\nIQL+JSRL: Random Switching Warm-start 73.35 ± 22.58 40.95 ± 13.34 9.55 ± 14.42 14.05 ± 13.30 0.35 ± 0.93 1.25 ± 2.31\n\nCold-start 92.05 ± 2.76 82.25 ± 14.20 56.15 ± 28.78 67.00 ± 17.43 17.70 ± 13.35 22.40 ± 15.44\n\nIQL+JSRL: Curriculum\n\nWarm-start 71.35 ± 26.36 38.80 ± 21.96 22.15 ± 29.82 15.75 ± 16.48 0.45 ± 1.19 0.75 ± 1.16\n\nCold-start 93.65 ± 4.21 81.30 ± 23.04 86.85 ± 3.67 81.50 ± 18.80 36.30 ± 16.41 34.35 ± 22.97\n\nIQL 74.15 ± 25.62 29.85 ± 23.08 32.80 ± 32.64 15.70 ± 17.69 2.55 ± 8.19 4.10 ± 10.37\n\nTable 9: Ant Maze 100k Offline Transitions\n\nEnvironment antmaze-umaze-v0 antmaze-umaze-diverse-v0 antmaze-medium-play-v0 antmaze-medium-diverse-v0 antmaze-large-play-v0 antmaze-large-diverse-v0\n\nIQL+JSRL: Random Switching Warm-start 95.35 ± 2.23 65.95 ± 27.00 82.25 ± 4.88 83.45 ± 4.64 50.35 ± 9.74 56.80 ± 9.15\n\nCold-start 94.95 ± 2.95 89.80 ± 10.00 87.80 ± 4.20 86.25 ± 5.94 48.60 ± 10.01 58.30 ± 6.54\n\nIQL+JSRL: Curriculum\n\nWarm-start 96.70 ± 1.69 59.95 ± 33.90 92.20 ± 2.84 91.65 ± 2.98 72.15 ± 9.66 70.55 ± 17.43\n\nCold-start 98.05 ± 1.43 88.55 ± 16.37 91.05 ± 3.86 93.05 ± 3.10 62.85 ± 11.31 68.25 ± 8.76\n\nIQL 97.60 ± 3.19 52.95 ± 30.48 92.75 ± 2.73 92.40 ± 4.50 62.35 ± 12.42 68.25 ± 8.85\n\nTable 10: Ant Maze 1m Offline Transitions\n\nA.3 HYPERPARAMETERS OF JSRL\n\nJSRL introduces three hyperparameters: (1) the initial number of guide-steps that the guide-policy takes at the beginning of fine-tuning (H1), (2) the number of curriculum stages (n), and (3) the\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nMoving Average Horizon\n\n1\n\n0% 79.66 5% 51.12 15% 56.41\n\n5 56.66 78.8 47.46\n\n10 74.83 79.78 59.52\n\nTolerance\n\nTable 11: We fix the number of curriculum stages at n = 10 for antmaze-large-diverse-v0, then vary the moving average horizon and tolerance. Each number is the average reward after 5 million training steps of one seed. As tolerance increases, the reward decreases since curriculum stages are not fully mastered before moving on.\n\nperformance threshold that decides whether to move on to the next curriculum stage (β). Minimal tuning was done for these hyperparameters.\n\nIQL+JSRL: For offline pre-training and online fine-tuning, we use the same exact hyperparameters as the default implementation of IQL [6].\n\nOur reported results for vanilla IQL do differ from the original paper, but this is due to us running more random seeds (20 vs. 5), which we also consulted with the authors of IQL. For Indiscriminate and Instance Grasping experiments we utilize the same environment, task definition, and training hyperparameters as Qt-Opt and AW-Opt.\n\nInitial Number of Guide-Steps: H1:\n\nFor all X+JSRLexperiments, we train the guide-policy (IQL for D4RL and BC for grasping) then evaluate it to determine how many steps it takes to solve the task on average. For D4RL, we evaluate it over one hundred episodes. For grasping, we plot training metrics and observe the average episode length after convergence. This average is then used as the initial number of guide-steps. Since H1 is directly computed, no hyperparameter search is required.\n\nCurriculum Stages: n\n\nn . Then h varies from H1 − H1\n\nOnce the number of curriculum stages was chosen, we computed the number of steps between curriculum stages as H1 n , 0. To decide on an appropriate number of curriculum stages, we decreased n (increased H1 n and Hi − Hi−1), starting from n = H, until the curriculum became too difficult for the agent to overcome (i.e., the agent becomes ”stuck” on a curriculum stage). We then used the minimal value of n for which the agent could still solve all stages. In practice, we did not try every value between H and 1, but chose a very small subset of values to test in this range.\n\nn , . . . , H1 − (n − 1) H1\n\nn , H1 − 2 H1\n\nPerformance Threshold β: For both grasping and D4RL tasks, we evaluated π between fixed intervals and computed the moving average of these evaluations (5 for D4RL, 3 for grasping). If the current moving average is close enough to the best previous moving average, then we move from curriculum stage i to i + 1. To define ”close enough”, we set a tolerance that let the agent move to the next stage if the current moving average was within some percentage of the previous best. The tolerance and moving average horizon were our ”β”, a generic parameter that is flexible based on how costly it is to evaluate the performance of π. In Figure 12 and Table 11, we perform small studies to determine how varying β affects JSRL’s performance.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Ablation study for β in the indiscriminate grasping environment. We find that the moving average horizon does not have a large impact on performance, but larger tolerance slightly hurts performance. A larger tolerance around the best moving average makes it easier for JSRL to move on to the next curriculum stage. This means that experiments with a larger tolerance could potentially move on to the next curriculum stage before JSRL masters the previous curriculum stage, leading to lower performance.\n\nFigure 13: First, an indiscriminate grasping policy is trained using online QT-Opt to 90% indiscriminate grasping success and 5% instance grasping success (when the policy happens to randomly pick the correct object). We compare this 90% indiscriminate grasping guide policy with a 8.4% success instance grasping guide policy trained with BC on 2k demonstrations. While the performance for using the indiscriminate guide is slightly worse than using the instance guide, the performance for both JSRL versions are much better than vanilla Qt-Opt.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: First, a policy is trained offline on a simpler antmaze-*-play environment for one million steps (depicted by negative steps). This policy is then used for initializing fine-tuning (depicted by positive steps) in a more complex antmaze-*-diverse environment. We find that IQL+JSRL can better generalize to the more difficult antmazes compared to IQL even when using guide-policies trained on different tasks.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 THEORETICAL ANALYSIS FOR JSRL\n\nA.4.1 SETUP AND NOTATIONS\n\nConsider a finite-horizon time-inhomogeneous MDP with a fixed total horizon H and bounded reward rh ∈ [0, 1], ∀h ∈ [H]. The transition of state-action pair (s, a) in step h is denoted as Ph(· | s, a). Assume that at step 0, the initial state follows a distribution p0.\n\nFor simplicity, we use π to denote the policy for H steps π = {πh}H marginalized state occupancy distribution in step h when we follow policy π.\n\nh=1. We let dπ\n\nh(s) be the\n\nA.4.2 PROOF SKETCH FOR THEOREM 4.1\n\nFigure 15: Lower bound instance: combination lock\n\nWe construct a special instance, combination lock MDP, which is depicted in Figure 15 and works as follows. The agent can only arrive at the red state s⋆ h at the red state s⋆ h at step h. Once it leaves state s⋆ h, the agent stays in the blue states and can never get back to red states again. At the last layer, one receives reward 1 when the agent is at state s⋆ H and takes action a⋆ H . For all other cases, the reward is 0. In exploration from scratch, before seeing rH (s⋆, a⋆), one only sees reward 0. Thus 0-initialized ε-greedy always takes each action with probability 1/2. H with uniform actions is 1/2H , which means that one needs at The probability of arriving at state s⋆ least 2H samples in expectation to see rH (s⋆, a⋆).\n\nh+1 in step h + 1 when it takes action a⋆\n\nA.4.3 UPPER BOUND OF JSRL\n\nIn this section, we restate Theorem 4.3 and its assumption in a formal way. First, we make assumption on the quality of the guide-policy, which is the key assumption that helps improve the exploration from exponential to polynomial sample complexity. One of the weakest assumption in theory of offline learning literature is the single policy concentratability coefficient Rashidinejad et al. (2021); Xie et al. (2021)1. Concretely, they assume that there exists a guide-policy πg such that\n\nh (s, a) h (s, a) This means that for any state action pair that the optimal policy visits, the guide-policy shall also visit with certain probability.\n\nsup s,a,h\n\n≤ C.\n\n(1)\n\ndπ⋆ dπg\n\nIn the analysis, we impose a strictly weaker assumption. We only require that the guide-policy visits all good states in the feature space instead of all good state and action pairs. Assumption A.1 (Quality of guide-policy πg). Assume that the state is parametrized by some feature mapping φ : S → Rd such that for any policy π, Qπ(s, a) and π(s) depends on s only through φ(s). We assume that in the feature space, the guide-policy πg cover the states visited by the optimal policy:\n\ndπ⋆ dπg\n\nh (φ(s)) h (φ(s))\n\nsup s,h\n\n≤ C.\n\n1The single policy concentratability assumption is already a weaker version of the traditional concentratability coefficient assumption, which takes a supremum of the density ratio over all state-action pairs and all policies (Scherrer, 2014; Chen & Jiang, 2019; Jiang, 2019; Wang et al., 2019; Liao et al., 2020; Liu et al., 2019; Zhang et al., 2020a).\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nNote that for the tabular case when φ(s) = s, one can easily prove that equation 1 implies Assumption A.1. In real robotics, the assumption implies that the guide-policy at least sees the features of the good states that the optimal policy also see. However, the guide-policy can be arbitrarily bad in terms of choosing actions.\n\nBefore we proceed to the main theorem, we need to impose another assumption on the performance of the exploration step, which requires to find an exploration algorithm that performs well in the case of H = 1 (contextual bandit). Assumption A.2 (Performance guarantee for ExplorationOracle CB). In (online) contextual bandit with stochastic context s ∼ p0 and stochastic reward r(s, a) supported on [0, R], there exists some ExplorationOracle CB which executes a policy πt in each round t ∈ [T ], such that the total regret is bounded:\n\nT (cid:88)\n\nt=1\n\nEs∼p0[r(s, π⋆(s)) − r(s, πt(s))] ≤ f (T, R).\n\nThis assumption is usually given for free since it is implied by a rich literature in contextual bandit, including tabular Langford & Zhang (2007), linear Chu et al. (2011), general function approximation with finite action Simchi-Levi & Xu (2020), neural networks and continuous actions Krishnamurthy et al. (2019), either via optimism-based methods (UCB, Thompson sampling etc.) or non-optimismbased methods (ε-greedy, inverse gap weighting etc.).\n\nNow we are ready to present the algorithm and guarantee. The JSRL algorithm is summarized in Algorithm 1. For the convenience of theoretical analysis, we make some simplification by only considering curriculum case, replacing the step of EvaluatePolicy with a fixed iteration time, and set the TrainPolicy in Algorithm 1 as follows: at iteration h, fix the policy πh+1:H unchanged, set πh = ExplorationOracle CB(D), where the reward for contextual bandit is the cumulative reward (cid:80)\n\nt=h:H rt. For concreteness, we show the pseudocode for the algorithm below.\n\nAlgorithm 2 Jump-Start Reinforcement Learning for Episodic MDP with CB oracle\n\n1: Input: guide-policy πg, total time step T , horizon length H 2: Initialize exploration policy π = πg, online dataset D = ∅. 3: for iteration h = H − 1, H − 2, · · · , 0 do 4:\n\nExecute ExplorationOracle CB for ⌈T /H⌉ rounds, with the state-aciton-reward tuple for l, at l )}l∈[H−1] l } as the state-action-reward samples for\n\ncontextual bandit derived as follows: at round t, first gather a trajectory {(st by rolling out policy π, then take {st contextual bandit. Let πt be the executed policy at round t.\n\nh, (cid:80)H\n\nl+1, rt\n\nl=h rt\n\nh, at\n\nl, st\n\nSet policy πh = Unif({πt}T\n\nt=1}).\n\n5: 6: end for\n\nNote that the Algorithm 2 is a special case of Algorithm 1 where the policies after current step h is fixed. This coincides with the idea of Policy Search by Dynamic Programming (PSDP) in Bagnell et al. (2003). Notably, although PSDP is mainly motivated from policy learning while JSRL is motivated from efficient online exploration and fine-tuning, the following theorem follows mostly the same line as that in Bagnell (2004). For completeness we provide the performance guarantee of the algorithm as follows. Theorem A.3. Under Assumption A.1 and A.2, the JSRL in Algorithm 2 guarantees that after T rounds,\n\nEs0∼p0 [V ∗\n\n0 (s0) − V π\n\n0 (s0)] ≤ C ·\n\nH−1 (cid:88)\n\nh=0\n\nf (T /H, H − h).\n\nTheorem A.3 is quite general, and it depends on the choice of the exploration oracle. Below we give concrete results for tabular RL and RL with function approximation. Corollary A.4. For tabular case, when we take ExplorationOracle CB as ε-greedy, the rate achieved is O(CH 7/3S1/3A1/3/T 1/3) ; when we take ExplorationOracle CB as FALCON+, the rate becomes O(CH 5/2S1/2A/T 1/2). Here S can be relaxed to the maximum state size that πg visits among all steps.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nThe result above implies a polynomial sample complexity when combined with non-optimism exploration techniques, including ε-greedy Langford & Zhang (2007) and FALCON+ Simchi-Levi & Xu (2020). In contrast, they both suffer from a curse of horizon without such a guide-policy.\n\nNext, we move to RL with general function approximation.\n\nCorollary A.5. For general function approximation, when we take ExplorationOracle CB as FALCON+, the rate becomes ̃O(C (cid:80)H\n\n(cid:112)AEF (T /H)) under the following assumption.\n\nh=1\n\nh, sj h, aj\n\nAssumption A.6. Let π be an arbitrary policy. Given n training trajectories of the form {(sj h, aj h)}j∈[n],h∈[H] drawn from following policy π in a given MDP, according to sj h), sj h ∼ dπ h), there exists some offline regression oracle which returns a family of predictors (cid:98)Qh : S × A → R, h ∈ [H], such that for any h ∈ [H], we have\n\nh ∼ πh(sh), rj\n\nh) ∼ Ph(·|sj\n\nh+1, rj h|sj\n\nh) ∼ Rh(sj\n\nh+1|(sj\n\nh|(sj\n\nh, aj\n\nh, aj\n\nh, aj\n\nh, aj\n\nE\n\n(cid:104) ( (cid:98)Qh(s, a) − Qπ\n\nh(s, a))2(cid:105)\n\n≤ EF (n).\n\nAs is shown in Simchi-Levi & Xu (2020), this assumption on offline regression oracle implies our Assumption on regret bound in Assumption A.2. When EF is a polynomial function, the above rate matches the worst-case lower bound for contextual bandit in Simchi-Levi & Xu (2020), up to a factor of C · poly(H).\n\nThe results above show that under Assumption A.1, one can achieve polynomial and sometimes near-optimal sample complexity up to polynomial factors of H without applying Bellman update, but only with a contextual bandit oracle. In practice, we run Q-learning based exploration oracle, which may be more robust to the violation of assumptions. We leave the analysis for Q-learning based exploration oracle as a future work. Remark A.7. The result generalizes to and is adaptive to the case when one has time-inhomogeneous C, i.e.\n\n∀h ∈ [H], sup\n\ns\n\ndπ⋆ dπg\n\nh (φ(s)) h (φ(s))\n\n≤ C(h).\n\nThe rate becomes (cid:80)H−1\n\nh=0 C(h) · f (T /H, H − h) in this case.\n\nIn our current analysis, we heavily rely on the assumption of visitation and applied contextual bandit based exploration techniques. In our experiments, we indeed run a Q-learning based exploration algorithm which also explores the succinct states after we roll out the guide-policy. This also suggests why setting K > 1 and even random switching in Algorithm 1 might achieve better performance than the case of K = 1. We conjecture that with a Q-learning based exploration algorithm, JSRL still works even when Assumption A.1 only holds partially. We leave the related analysis for JSRL with a Q-learning based exploration oracle for future work.\n\nA.4.4 PROOF OF THEOREM A.3 AND COROLLARIES\n\nProof. The analysis follows a same line as Bagnell (2004). For completeness we include here. By the performance difference lemma Kakade & Langford (2002), one has\n\nEs0∼d0[V ⋆\n\n0 (s0) − V π\n\n0 (s0)] =\n\nH−1 (cid:88)\n\nh=0\n\nEs∼d⋆\n\nh\n\n[Qπ\n\nh(s, π⋆\n\nh(s)) − Qπ\n\nh(s, πh(s))].\n\n(2)\n\nAt iteration h, the algorithm adopts a policy π with πl = πg l , ∀l < h, and fixed learned πl for l > h. The algorithm only updates πh during this iteration. By taking the reward as (cid:80)H l=h rl, this presents a contextual bandit problem with initial state distribution dπg h , reward bounded in between [0, H − h], and the expected reward for taking state action (s, a) is Qπ h be the optimal policy for this contextual bandit problem. From Assumption A.2, we know that after T /H rounds at iteration h,\n\nh(s, a). Let ˆπ⋆\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\none has\n\nH−1 (cid:88)\n\nh=0\n\nEs∼d⋆\n\nh\n\n[Qπ\n\nh(s, π⋆\n\nh(s)) − Qπ\n\nh(s, πh(s))]\n\nH−1 (cid:88)\n\n(i) ≤\n\nEs∼d⋆\n\nh\n\n[Qπ\n\nh(s, ˆπ⋆\n\nh(s)) − Qπ\n\nh(s, πh(s))]\n\nh=0\n\nH−1 (cid:88)\n\nh=0\n\n(ii) =\n\nEs∼d⋆\n\nh\n\n[Qπ\n\nh(φ(s), ˆπ⋆\n\nh(φ(s))) − Qπ\n\nh(φ(s), πh(φ(s)))]\n\n(iii) ≤ C ·\n\nH−1 (cid:88)\n\n(iv) ≤ C ·\n\nh=0\n\nH−1 (cid:88)\n\nh=0\n\nE\n\ns∼dπg\n\nh\n\n[Qπ\n\nh(φ(s), ˆπ⋆\n\nh(φ(s))) − Qπ\n\nh(φ(s), πh(φ(s)))]\n\nf (T /H, H − h).\n\nHere the inequality (i) uses the fact that ˆπ⋆ is the optimal policy for the contextual bandit problem. The equality (ii) uses the fact that Q, π depends on s only through φ(s). The inequality (iii) comes from Assumption A.1. The inequality (iv) comes from Assumption A.2. From Equation equation 2 we know that the conclusion holds true.\n\nthe rate in Assumption A.2 becomes f (T, R) = When ExplorationOracle CB is ε-greedy, R · ((SA/T )1/3) Langford & Zhang (2007), which gives JSRL as O(CH 7/3S1/3A1/3/T 1/3); when we take ExplorationOracle CB as FALCON+ in tabular case, the rate in Assumption A.2 becomes f (T, R) = R · ((SA2/T )1/2) Simchi-Levi & Xu (2020), the final rate for JSRL becomes O(CH 5/2S1/2A/T 1/2). When we take ExplorationOracle CB as FALCON+ in general function approximation under Assumption A.6, the rate in Assumption A.2 becomes f (T, R) = R · (AEF (T ))1/2, the final rate for JSRL becomes ̃O(C (cid:80)H\n\n(cid:112)AEF (T /H)).\n\nthe rate for\n\nh=1\n\n27",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose Jump-Start Reinforcement Learning (JSRL) which utilizes a pre-trained guide policy to form a curriculum of starting states for a different exploration policy. Theoretical analysis shows that with a properly chosen training and evaluation algorithm, JSRL achieves a polynomial sample complexity. Empirical evaluations on D4RL and vision-based robotic tasks are provided to show the effectiveness of JSRL.\n\n# Strength And Weaknesses\n\nStrength: JSRL is well-motivated to address the sample complexity requirement in training RL agents from scratch. The proposed algorithm is easy to understand and empirical evaluations show some effectiveness, especially in the low demo data regime. \n\nWeakness:\nThe empirical evaluation section could be improved. In Table 2, three baseline methods were not tested on low demo data regimes, only the 1 million standard setting. As some of them achieve better results than JSRL in the standard setting, we need to see the comparisons in the low data setting to evaluate how effective JSRL really is.\n\nMoreover, the improvement margins JSRL has over baseline methods are relatively small. In fact, in Table 1, for Instance Grasping, the result confidence intervals of 20 and 20k demos overlap with those of AW-Opt. The improvement in D4RL tasks is also small. So I am not convinced JSRL, in its current form, is really better than existing baselines.\n\nThere are some imprecise writings in the theoretical analysis, see my questions in the next section.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is relatively easy to read. However, in the theoretical analysis, some languages appear imprecise.\n\n1. For Theorem 4.3, the authors wrote “To achieve a polynomial bound for JSRL, it suffices to take TrainPolicy as $\\epsilon$-greedy.”\n\nThe TrainPolicy procedure updates a policy and a Q function. $\\epsilon$-greedy normally refers to an exploration method for policy. The authors should provide a more precise explanation of what an $\\epsilon$-greedy policy update procedure is.\n\n2. The distribution mismatch coefficient in Assumption 4.2 uses undefined quantities $d$. \n\n3. In Section 4.2, I think it should be $H_i \\in \\{1, 2, \\cdots, H\\}$.\n\n# Summary Of The Review\n\nThe major weakness is the evaluation results, which did not convince me that JSRL is stronger than baseline IL + RL methods.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEQUIVARIANT DESCRIPTOR SE(3)- EQUIVARIANT ENERGY-BASED MODELS FOR ENDTO-END VISUAL ROBOTIC MANIPULATION LEARNING\n\nFIELDS:\n\nHyunwoo Ryu1 Hong-in Lee1 1Department of Artificial Intelligence, Yonsei University 2Samsung Research {tomato1mule,theorist17,jongeunchoi}@yonsei.ac.kr jh 0921.lee@samsung.com\n\nJeong-Hoon Lee2,3\n\n3School of Mechanical Engineering, Yonsei University\n\nJongeun Choi1,3\n\nABSTRACT\n\nEnd-to-end learning for visual robotic manipulation is known to suffer from sample inefficiency, requiring large numbers of demonstrations. The spatial rototranslation equivariance, or the SE(3)-equivariance can be exploited to improve the sample efficiency for learning robotic manipulation. In this paper, we present SE(3)-equivariant models for visual robotic manipulation from point clouds that can be trained fully end-to-end. By utilizing the representation theory of the Lie group, we construct novel SE(3)-equivariant energy-based models that allow highly sample efficient end-to-end learning. We show that our models can learn from scratch without prior knowledge and yet are highly sample efficient (5∼10 demonstrations are enough). Furthermore, we show that our models can generalize to tasks with (i) previously unseen target object poses, (ii) previously unseen target object instances of the category, and (iii) previously unseen visual distractors. We experiment with 6-DoF robotic manipulation tasks to validate our models’ sample efficiency and generalizability. Codes are available at: https://github.com/tomato1mule/edf\n\n1\n\nINTRODUCTION\n\nLearning robotic manipulation from scratch often involves learning from mistakes, making realworld applications highly impractical (Kalashnikov et al., 2018; Levine et al., 2016; Lee & Choi, 2022). Learning from demonstration (LfD) methods (Ravichandar et al., 2020; Argall et al., 2009) are advantageous because they do not involve trial and error, but expert demonstrations are often rare and expensive to collect. Therefore, auxiliary pipelines such as pose estimation (Zeng et al., 2017; Deng et al., 2020), segmentation (Simeonov et al., 2021), or pre-trained object representations (Florence et al., 2018; Kulkarni et al., 2019) are commonly used to improve data efficiency. However, collecting sufficient data for training such pipelines is often burdensome or unavailable in practice.\n\nRecently, roto-translation equivariance has been explored for sample-efficient robotic manipulation learning. Transporter Networks (Zeng et al., 2020) achieve high sample efficiency in end-to-end visual robotic manipulation learning by exploiting SE(2)-equivariance (planar roto-translation equivariance). However, the efficiency of Transporter Networks is limited to planar tasks due to the lack of the full SE(3)-equivariance (spatial roto-translation equivariance). In contrast, Neural Descriptor Fields (NDFs) (Simeonov et al., 2021) can achieve few-shot level sample efficiency in learning highly spatial tasks by exploiting the SE(3)-equivariance. Moreover, the trained NDFs can generalize to previously unseen object instances (in the same category) in unseen poses. However, unlike Transporter Networks, NDFs cannot be end-to-end trained from demonstrations. The neural networks of NDFs need to be pre-trained with auxiliary self-supervised learning tasks and cannot be fine-tuned for the demonstrated tasks. Furthermore, NDFs can only be used for well-segmented point cloud inputs and fixed placement targets. These limitations make it difficult to apply NDFs when 1) no public dataset is available for pre-training on the specific target object category, 2) when well-segmented object point clouds cannot be expected, or when 3) the placement target is not fixed.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Given few (5∼10) demonstrations of a mug pick-and-place task, EDFs can be trained fully end-to-end without requiring any pre-training, object segmentation, or pose estimation pipelines. In addition, we show that EDFs can generalize to A) unseen poses, B) unseen instances of the target object category, and C) the presence of unseen visual distractors.\n\nTo overcome such limitations, we present Equivariant Descriptor Fields (EDFs), the first end-to-end trainable and SE(3)-equivariant visual robotic manipulation models. EDFs can be fully end-to-end trained to solve highly spatial tasks from only a few (5∼10) demonstrations without requiring any pre-training, object keypoint annotation, or segmentation. EDFs can generalize to previously unseen target object instances in unseen poses as NDFs. Furthermore, EDFs can generalize to unseen distracting objects and unseen placement poses (See Figure 1). Our contributions are as follows:\n\n1. To enable end-to-end training, we reformulate the energy minimization problem of NDFs into a probabilistic learning framework with energy-based models on the SE(3)-manifold.\n\n2. We generalize the invariant descriptors of NDFs into representation-theoretic equivariant descriptors. Using equivariant descriptors significantly improves generalizability owing to their orientational sensitivity.\n\n3. We propose a novel energy function and end-to-end trainable query point models to achieve\n\nthe SE(3)-equivariance regarding both the target object and placement target.\n\n4. EDFs do not resort to non-local mechanisms to achieve the SE(3)-equivariance. This specific design enables our method to work well without object segmentation pipelines.\n\n2 BACKGROUND AND RELATED WORKS\n\nEquivariant Robotic Manipulation Equivariant models have emerged as a promising approach for robotic manipulation learning, with growing evidence indicating they can significantly improve both sample efficiency and generalizability (Wang & Walters, 2022; Wang et al., 2022). Transporter Networks and their variants (Zeng et al., 2020; Seita et al., 2021) are end-to-end models for visual robotic manipulation tasks that exploit the planar roto-translation equivariance, or the SE(2)- equivariance for the sample efficiency. Equivariant Transporter Networks (ETNs) (Huang et al., 2022) exploit the representation theory of discrete rotation groups to further improve the sample efficiency. However, the efficiency of SE(2)-equivariant models is limited to planar tasks and cannot be extended to highly spatial tasks. Neural Descriptor Fields (NDFs) (Simeonov et al., 2021) overcome this limitation by leveraging the spatial roto-translation equivariance, or the SE(3)-equivariance.\n\nEnergy-Based Models Energy-based models (EBMs) are probabilistic models that are derived from energy functions. EBMs are widely used for image and video generation (Zhu & Mumford, 1998; Xie et al., 2016; 2017; Du & Mordatch, 2019), 3D geometry generation (Xie et al., 2018b; 2021a), internal learning (Zheng et al., 2021), and control (Xu et al., 2022; Florence et al., 2022). Due to the intractability of the integral in the denominator of EBMs, Markov chain Monte Carlo (MCMC) methods are commonly used to estimate the gradient of the log-denominator to maximize the log-likelihood (Hinton, 2002; Carreira-Perpinan & Hinton, 2005). The Metropolis-Hastings algorithm (MH) (Hastings, 1970) and the Langevin dynamics (Langevin, 1908; Welling & Teh, 2011) are widely used MCMC methods for EBMs on Euclidean spaces. However, typical Langevin dynamics cannot be used for non-Euclidean manifolds such as the SE(3) manifold. The Langevin dynamics on the SE(3) group and general Lie groups are studied by Brockett (1997); Chirikjian\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(2011); Davidchack et al. (2017). The Langevin dynamics and their convergence on general Riemannian manifold have been studied by Girolami & Calderhead (2011); Gatmiry & Vempala (2022). In this paper, we propose SE(3)-equivariant EBMs on the SE(3) manifold, which should be distinguished from SE(3)-equivariant EBMs on Euclidean spaces (Jaini et al., 2021; Wu et al., 2021).\n\nRepresentation Theory of Lie Groups A representation D of a group G is a map from G to the space of linear operators acting on a vector space V that has the following property:\n\nD(g)D(h) = D(gh) ∀g, h ∈ G\n\n(1) Any representation of SO(3) group D(R) for R ∈ SO(3) can be block-diagonalized into the direct sum of (real) Wigner D-matrices Dl(R) ∈ R(2l+1)×(2l+1) of degree l ∈ {0, 1, 2, · · · }, which are orthogonal matrices (Aubert, 2013). The (2l+1) dimensional vectors that are transformed by Dl(R) are called type-l (or spin-l) vectors. Type-0 vectors are invariant to rotations such that D(R) = I. Type-1 vectors are the familiar 3-dimensional space vectors with D(R) = R. Type-l vectors are identical to themselves when rotated by θ = 2π/l. A type-l vector field f : R3 × X → R2l+1 is SE(3)-equivariant if\n\nDl(R)f (x|X) = f (T x|T X) ∀T = (R, v) ∈ SE(3), X ∈ X , x ∈ R3\n\n(2) and T x = Rx + v. Tensor Field Networks where X is some set equipped with a group action (TFNs) (Thomas et al., 2018) and SE(3)-Transformers (Fuchs et al., 2020) are used to implement SE(3)-equivariant vector fields in this work. We provide details of these networks in Appendix G.\n\n3 PROBLEM FORMULATION\n\nLet a colored point cloud with M points given by X = {(x1, c1), · · · , (xM , cM )} ∈ P where xi ∈ R3 is the position, ci ∈ R3 is the color vector of the i-th point, and P is the set of all possible : SE(3) × P → P is then defined as colored point clouds. The action of SE(3) on point clouds\n\nT X = {(T x1, c1), (T x2, c2), · · · , (T xM , cM )} ∀T = (R, v) ∈ SE(3) Consider a problem where a robot has to learn to grasp and place objects in specific locations from human demonstrations. To solve this problem, the placement pose T ∈ SE(3) must be inferred such that the relative pose between the grasped object and placement target remains consistent with the demonstrations, regardless of changes in their postures. This can be achieved by requiring the bi-equivariance to T such that 1) it follows changes in the posture of the placement target, and 2) it compensates for changes in the posture of the grasp (See Appendix C for more explanation). In contrast, uni-equivariant methods such as NDFs, which can only account for changes in one object, are not well-suited for problems where both postures may change.\n\n(3)\n\nNow let X be the point cloud of the scene (where the placement target belongs), and Y be the point cloud of the end-effector (with the grasped object) observed in the end-effector frame T ∈ SE(3). The formal definition of bi-equivariance is as follows: Definition 1. A differential probability distribution dP (T |X, Y ) on SE(3) conditioned by two point clouds X, Y ∈ P is bi-equivariant if for all Borel subsets Ω ⊆ SE(3),\n\n(cid:90)\n\nT ∈Ω\n\ndP (T |X, Y ) =\n\n(cid:90)\n\nT ∈SΩ\n\ndP (T |S X, Y ) =\n\n(cid:90)\n\nT ∈ΩS\n\ndP (T |X, S−1 Y ) ∀S ∈ SE(3)\n\n(4)\n\nwhere SΩ = {ST |T ∈ Ω}, ΩS = {T S|T ∈ Ω}, and S−1 denotes the group inverse of S. Definition 2. A scalar function f : SE(3) × P × P → R is bi-equivariant if\n\nf (T |X, Y ) = f (ST |S X, Y ) = f (T S|X, S−1 Y ) ∀S ∈ SE(3) (5) Proposition 1. A probability distribution P (T |X, Y )dT is bi-equivariant if dT is the bi-invariant volume form (See Appendix A) on the SE(3) manifold and P (T |X, Y ) is a bi-equivariant probability density function (PDF).\n\nThe proof of Proposition 1 can be found in Appendix F.1. The bi-equivariance condition of Definition 1 can be viewed as a generalization of Huang et al. (2022) to non-commutative groups like SE(3) and a probabilistic generalization of Ganea et al. (2021). By Proposition 1, our goal boils down to constructing a bi-equivariant PDF P (T |X, Y ) such that\n\nP (T |X, Y ) = P (ST |S X, Y ) = P (T S|X, S−1 Y ) ∀S ∈ SE(3)\n\n(6)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: A) The model is globally equivariant if the grasp pose is equivariant to the transformations of the whole scene (the target object and background). B) The model is locally equivariant to the target object if the grasp pose is equivariant to the localized transformations of the target object.\n\nHowever, we want our policy to be not only globally equivariant as Equation (6) but also to be locally equivariant. That is, we want our models to be equivariant only to the target object, not the backgrounds. We illustrate the local equivariance and the global equivariance in Figure 2. To achieve the local equivariance, the model should only rely on local mechanisms to achieve the equivariance. For example, Transporter Networks achieve translational equivariance by using convolutional neural networks, which are well-known for their locality (Battaglia et al., 2018; Goodfellow et al., 2016). On the other hand, NDFs (Simeonov et al., 2021) rely on the centroid subtraction method to obtain translational equivariance, which is highly nonlocal. As a result, unlike Transporter Networks, NDFs cannot be used without object segmentation pipelines.\n\n4 BI-EQUIVARIANT ENERGY BASED MODELS ON SE(3)\n\nIn this section, we present EDFs and the corresponding bi-equivariant energy-based models on SE(3). We also provide practical implementations for the proposed models. We illustrate the overview of our method in Figure 3.\n\n4.1 EQUIVARIANT DESCRIPTOR FIELD\n\nWe define the EDF φ(x|X) as a direct sum of N vector fields\n\nφ(x|X) =\n\nN (cid:77)\n\nn=1\n\nφ(n)(x|X)\n\n(7)\n\nwhere φ(n)(x|X) : R3 × P → R2ln+1 is an SE(3)-equivariant type-ln vector field. Therefore, the EDF φ(x|X) transforms according to a rigid body transformation T ∈ SE(3) as\n\nφ(T x|T X) = D(R)φ(x|X) ∀ T = (R, v) ∈ SE(3)\n\n(8)\n\nwhere D(R) = (cid:76)N real basis. Therefore, D(R) is an orthogonal representation of the SO(3) group.\n\nn=1 Dln(R) is the direct sum of the real Wigner D-Matrices of degree ln in the\n\nNote that NDFs (Simeonov et al., 2021) only use type-0 descriptors, which are invariant to rotations such that D(R) = I. In contrast, EDFs also use type-1 or higher descriptors, which are highly sensitive to rotations. As a result, NDFs require at least three non-collinear query points (and much more in practice) to represent the orientation of a rigid body, whereas EDFs require only one point.\n\n4.2 EQUIVARIANT ENERGY-BASED MODEL ON SE(3)\n\nNaively minimizing the energy function like Simeonov et al. (2021) cannot be used to simultaneously train the descriptors, as this would result in all the descriptors collapsing to zero (or some other constants). Therefore, we use the EBM approach for the end-to-end training of descriptors.\n\nAn energy-based model on the SE(3) manifold conditioned by X, Y ∈ P can be defined as\n\nP (T |X, Y ) =\n\nexp [−E(T |X, Y )] SE(3) dT exp [−E(T |X, Y )]\n\n(cid:82)\n\n(9)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: A) Query points and query EDF are generated from the point cloud of the grasp. Query EDF values at the query points are used as the query descriptors. We visualized three type-0 descriptors in colors (RGB) and type-1 descriptors as arrows. We only visualized type-1 descriptors in important locations. We did not visualize higher-type descriptors. B) The key descriptors are generated from the point cloud of the scene. C) The query descriptors are transformed and matched to the key descriptors to produce the energy of the pose. For simplicity, we only visualized the query descriptor for a single query point. Note that the query and key descriptors are better aligned in the low energy case than in the high energy case for both the type-0 and type-1 descriptors (The orange query points are near the orange region, and the black arrow is well aligned to the gray arrows).\n\nProposition 2. The EBM P (T |X, Y ) in Equation (9) is bi-equivariant if the energy function E(T |X, Y ) is bi-equivariant.\n\nWe prove Proposition 2 in Appendix F.2. We now propose the following energy function:\n\nE(T |X, Y ) =\n\n(cid:90)\n\nR3\n\nd3xρ(x|Y )∥φ(T x|X) − D(R)ψ(x|Y )∥2\n\n(10)\n\nwhere φ(x|X) is the key EDF, ψ(x|Y ) is the query EDF, and ρ(x|Y ) is the query density. Note that T = (R, v). The query density is an SE(3)-equivariant non-negative scalar field such that\n\nρ(x|Y ) = ρ(T x|T Y ) ∀T ∈ SE(3)\n\n(11)\n\nIntuitively, the energy function in Equation (10) can be thought as a query-key matching between the key EDF and the query EDF which is analogous to (Zeng et al., 2020; Huang et al., 2022). Proposition 3. The energy function E(T |X, Y ) in Equation (10) is bi-equivariant.\n\nWe prove Proposition 3 in Appendix F.3. As a result, the EBM in Equation (9) with the energy function in Equation (10) is also bi-equivariant. Lastly, we provide an important consequence of Equation (11), whose proof can be found in Appendix F.4: Proposition 4. Non-constant query densities that satisfy Equation (11) must be grasp-dependent.\n\n4.3\n\nIMPLEMENTATION\n\nOur method consists of two models, viz. the pick-model and the place-model. The pick-model is a simplified version of the place-model. Therefore, we only demonstrate here the components of the place-model. The pick-model is demonstrated in Appendix E. We show in Appendix E that the energy function used in Simeonov et al. (2021) is a special case of our pick-model’s energy function. For the following sections, we denote all the learnable parameters as θ. Therefore, all the functions with θ as a subscript are to be understood as trainable models. We visualized the key EDF of a trained pick-model in Figure 4.\n\nQuery Density To make the integral in Equation (10) tractable, we model the query density as weighted query points by taking the weighted sum of Dirac delta functions δ(3)(x) = (cid:81)3\n\ni=1 δ(xi)\n\nρθ(x|Y ) =\n\nNq (cid:88)\n\ni=1\n\nwθ (qi;θ(Y )|Y ) δ(3) (x − qi;θ(Y ))\n\n(12)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: The key EDF of a trained pick-model is illustrated for the scenes with a mug in A) upright pose and B) lying pose. Note that the colors (type-0 descriptors) are invariant to the rotation of the mug. On the other hand, the arrows (type-1 descriptors) are equivariant to the rotation. We only visualized type-1 descriptors in important locations. Higher-type descriptors are not visualized.\n\nwhere qi;θ(Y ) : P → R3 is the i-th query point function and wθ(x|Y ) : R3 × P → R+ is the query weight field. These maps are SE(3)-equivariant such that\n\nqi;θ(T Y ) = T qi;θ(Y ) wθ(T x|T Y ) = wθ(x|Y )\n\n(13)\n\nProposition 5. The query density ρθ(x|Y ) in Equation (12) is SE(3)-equivariant.\n\nWe prove Proposition 5 in Appendix F.5. Note that as a consequence of Proposition 4, the grasp dependence is inevitable for non-constant query point models like Equation (12). In this case, the integral in Equation (10) can be written in the following tractable summation form:\n\nEθ(T |X, Y ) =\n\nNq (cid:88)\n\ni=1\n\n(cid:101)Eθ (T |X, Y, wθ (qi;θ(Y )|Y ) , qi;θ(Y ))\n\n(cid:101)Eθ(T |X, Y, w, q) = w∥φθ(T q|X) − D(R)ψθ(q|Y )∥2\n\n(14)\n\n(15)\n\nIn Appendix B, we provide practical implementations of qi;θ(Y ) and wθ (x|Y ) that are continuously parameterized and differentiable.\n\nEDFs As was argued in Section 3, only the local operations should be used in our models for the local equivariance. We use Tensor Field Networks (TFNs) (Thomas et al., 2018) for the last layer and SE(3)-Transformers (Fuchs et al., 2020) for the other layers. The convolution operations that are used in these networks are highly local when their radial functions (See Appendix G) have short cutoff distances. We used simple radius clustering, which is locally SE(3)-equivariant within the clustering radius, to make the point clouds into graphs. For computational efficiency, only the last layer is used to evaluate the field values at the query points. All the other layers’ outputs only depend on the point cloud and not the query points. Therefore, during the MCMC steps, only the last layer (TFN) has to be recalculated, and the outputs of the other layers (SE(3)-Transformers) can be reused. We use the E3NN package (Geiger et al., 2022) to implement the equivariant layers.\n\n5 SAMPLING AND TRAINING\n\nFor the sampling, we first run the Metropolis-Hastings algorithm (MH) with IGSO(3) distribution (Nikolayev & Savyolov, 1970; Savyolova, 1994; Leach et al., 2022) for the orientation proposal and typical Gaussian distribution for the translation proposal. Next, we run the Langevin dynamics on the SE(3) manifold using the samples from MH as initial seeds. The Lie derivatives (Brockett, 1997; Chirikjian, 2011) for the Langevin dynamics are calculated in quaternion-translation parameterization as Davidchack et al. (2017) to avoid singularity issues. We provide details in Appendix D.\n\nFor the training, we estimate the gradient of the log-likelihood of Equation (9) at Ttarget as\n\n∇θ log Pθ(Ttarget|X, Y ) ≈ −∇θEθ(Ttarget|X, Y ) +\n\n1 N\n\nN (cid:88)\n\nn=1\n\n[∇θEθ(Tn|X, Y )]\n\n(16)\n\nwhere Tn ∼ P (T |X, Y ) is the n-th negative sample (Carreira-Perpinan & Hinton, 2005). However, naively maximizing the log-likelihood is highly unstable. If the query EDF and the key EDF are\n\n6\n\nφ(x|X)B)A)φ(x|X)Published as a conference paper at ICLR 2023\n\ninitially very different at some important sites, the learning algorithm tends to lower the query density of these sites rather than make the two EDFs closer. Therefore, all the query points in essential locations (such as contact points) are being pushed away. As a result, the training diverges.\n\nTo avoid this instability, we propose using the following surrogate query model during the early stage of training. We first decompose the EBM P (T |X, Y ) in Equation (14) into\n\nP (T |X, Y ) =\n\n(cid:90)\n\n(cid:90)\n\ndw\n\ndQP (T |X, Y, w, Q)P (w, Q|Y )\n\nP (T |X, Y, w, Q) =\n\n(cid:82)\n\n(cid:104)\n\nexp\n\n− (cid:80)Nq i=1 (cid:101)E(T |X, Y, wi, qi) (cid:104) − (cid:80)Nq\n\ni=1 (cid:101)E(T |X, Y, wi, qi)\n\n(cid:105)\n\nSE(3) dT exp\n\n(17)\n\n(18)\n\n(cid:105)\n\nP (w, Q|Y ) =\n\nNq (cid:89)\n\ni=1\n\nPi(wi, qi|Y ) =\n\nNq (cid:89)\n\n(cid:104)\n\ni=1\n\nδ(wi − w(qi|Y )) × δ(3)(qi − qi(Y ))\n\n(cid:105)\n\n(19)\n\nwhere Q = (q1, · · · , qNq ) and w = (w1, · · · , wNq ). We temporarily hide θ for brevity. Proposition 6. The marginal EBM P (T |X, Y ) in Equation (17) is bi-equivariant if\n\nP (w, Q|Y ) = P (w, SQ|S Y ) ∀S ∈ SE(3)\n\nWe prove Proposition 6 in Appendix F.6. We now relax this deterministic query model into a stochastic model by adding Gaussian noise to the logits of the query weights li = log wi as follows.\n\nˆP (w, Q|Y ) =\n\nNq (cid:89)\n\ni=1\n\nˆPi(wi, qi|Y ) =\n\nNq (cid:89)\n\ni=1\n\ndli dwi\n\nN (li; log w(qi|Y ), σH )δ(3)(qi − qi(Y ))\n\n(20)\n\nNow we propose the following surrogate query model\n\nH(w, Q|X, Y, T ) =\n\nHi(wi, qi|X, Y, T ) =\n\nNq (cid:89)\n\ni=1\n\nHi(wi, qi|X, Y, T )\n\n(cid:26) ˆPi(wi, qi|Y )\n\n(dli/dwi)N (li; α, σH )δ(3)(qi − qi(Y ))\n\n(21)\n\nif dmin(T qi, X) < r else\n\nwhere σH ∈ R+, r ∈ R+, and α ∈ R are hyperparameters and dmin(x, X) : R3 × P → R+ is the shortest Euclidean distance between x and the points in X. We set α to be sufficiently small so that query points without neighboring points in X can be suppressed.\n\nTo train our models using the surrogate query model in Equation (21), we maximize the following variational lower bound (Kingma & Welling, 2013) instead of the marginal log-likelihood.\n\nLθ(T |X, Y ) = Ew,Q∼Hθ [log Pθ(T |X, Y, w, Q)] − DKL\n\n(cid:104)\n\nHθ(w, Q|X, Y, T )\n\n(cid:13) (cid:13) (cid:13)\n\nˆPθ(w, Q|Y )\n\n(cid:105)\n\n(22)\n\nProposition 7. The variational lower bound Lθ(T |X, Y ) in Equation (22) is bi-equivariant.\n\nWe provide the proof of Proposition 7 in Appendix F.7. The Kullback-Leibler divergence term in Equation (22) is provided in Appendix B.2. Once the query model has been sufficiently trained, we remove the surrogate query model and return to the maximum likelihood training in Equation (16).\n\n6 EXPERIMENTAL RESULTS\n\nWe design the experiments to assess the generalization performance of our approach (EDFs) when the number of demonstrations is very limited. Similar to Simeonov et al. (2021), we evaluate the generalization performance of models with a mug-hanging task and bowl/bottle pick-and-place tasks. In the mug-hanging task, a mug has to be picked by its rim and then hung on a hanger by its handle. In the bowl/bottle pick-and-place task, a bowl/bottle should be picked and placed on a tray. As opposed to Simeonov et al. (2021), we randomize the pose of the hanger and tray to evaluate\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: A) Only ten demonstrations with objects in upright poses are provided during the training. B) The models are evaluated with unseen object instances in unseen poses with unseen distractors.\n\nTable 1: Pick-and-place success rates in various out-of-distribution settings.\n\nMug\n\nBowl Pick Place Total Pick Place Total Pick Place Total\n\nBottle\n\n0.36 0.97\n\n0.36 0.97\n\nUnseen Instances SE(3)-TNs (Zeng et al., 2020) 1.00 EDFs (Ours) 1.00 Unseen Poses SE(3)-TNs (Zeng et al., 2020) 0.00 N/A 0.00 1.00 EDFs (Ours) Unseen Distracting Objects SE(3)-TNs (Zeng et al., 2020) 1.00 EDFs (Ours) 1.00 Unseen Instances, Arbitrary Poses & Distracting Objects SE(3)-TNs (Zeng et al., 2020) 0.25 1.00 EDFs (Ours)\n\n0.04 0.95\n\n0.01 0.95\n\n0.63 0.98\n\n0.63 0.98\n\n1.00\n\n1.00\n\n0.76 0.98\n\n1.00 1.00\n\n0.76 0.98\n\n0.20 1.00\n\n1.00 1.00\n\n0.20 1.00\n\n0.00 N/A 0.00 1.00 1.00 1.00\n\n0.00 N/A 0.00 0.95 0.95 1.00\n\n1.00 1.00\n\n1.00 1.00\n\n1.00 1.00\n\n0.96 0.99\n\n0.92 1.00\n\n0.88 0.99\n\n0.09 0.95\n\n1.00 1.00\n\n0.09 0.95\n\n0.26 0.95\n\n0.88 1.00\n\n0.23 0.95\n\nwith changing target placement poses. We use ten demonstrations of upright poses generated by probabilistic oracles for the training. We then evaluate the success rate for (1) unseen instances, (2) unseen poses (lying poses), (3) unseen distracting objects, and (4) unseen instances in arbitrary poses (50% lying, 50% upright but in arbitrary elevation) with unseen distracting objects. We illustrate the experimental setups in Figure 5. Details are provided in Appendix H.\n\nFor baselines, we use SE(3)-Transporter Networks (SE(3)-TNs) (Zeng et al., 2020) as the state-of-the-art method for end-to-end visual manipulation. However, for the mug hanging task, we find that SE(3)-TNs entirely fail due to the multimodality of the demonstrations. Therefore, we instead train SE(3)-TNs using unimodal, low-variance demonstrations only for the mug-hanging task. For fair comparison, we provide the result for EDFs trained with the same demonstrations in Table 3 of Appendix I. The experimental results for SE(3)-TNs and EDFs are summarized in Table 1. For EDFs, we provide the learning curves for the mug task in Figure 6.\n\nNote that we do not directly compare EDFs with NDFs because (1) NDFs require the target placement poses to be fixed, and (2) NDFs require object segmentation pipelines (Performance of NDFs without object segmentation is provided in Appendix J). Instead, we perform an ablation study by using only type-0 descriptors as NDFs. One may consider this as the end-to-end trainable and bi-equivariant modification of NDFs (See Section 4.1 and Appendix E). We control the ablated model’s query point number such that the wall-clock inference time is similar to or slightly longer than EDFs. Note that the query point number serves a similar role to the batch size of inputs. Details on the ablated model and EDFs are provided in Appendix H. All the times were\n\nFigure 6: The success rate of EDFs for the mug-hanging task with respect to the number of demonstrations.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Success rate and inference time of the ablated model and EDFs. All the evaluations are done in the unseen instances, poses & distracting objects setting.\n\nDescriptor Type\n\nNDF-like (Type-0 Only) Inference Time Success Rate EDFs (Type-0∼3) Inference Time Success Rate\n\nPick\n\n5.7s 0.84\n\n5.1s 1.00\n\nMug Place Total\n\n8.6s 0.77\n\n8.3s 0.95\n\n14.3s 0.65\n\n13.4s 0.95\n\nPick\n\n6.1s 0.60\n\n5.2s 0.95\n\nBowl Place Total\n\n9.9s 0.95\n\n16.0s 0.57\n\n10.4s 1.00\n\n15.6s 0.95\n\nBottle Place Total\n\n17.3s 0.95\n\n23.0s 0.63\n\n11.5s 1.00\n\n16.7s 0.95\n\nPick\n\n5.8s 0.66\n\n5.2s 0.95\n\nmeasured using an Intel i9-12900k CPU (P-core only) with an Nvidia RTX3090 GPU. Experimental results for the ablated model with more query points can be found in Table 4 of Appendix I.\n\nAnalysis As can be seen in Table 1, EDFs outperform SE(3)- TNs (Zeng et al., 2020) for all three tasks. It is obvious that EDFs generalize much better than SE(3)-TNs for unseen poses, as EDFs are SE(3)-equivariant, whereas SE(3)-TNs are only SE(2)-equivariant. We provide a qualitative example in Figure 7. EDFs also generalize much better than SE(3)-TNs for unseen instances and/or unseen distracting objects. For example, SE(3)- TNs often fail to correctly regress the z-axis of the grasp pose of an unseen instance when the object height differs a lot from the trained objects. In contrast, EDFs rarely fail under such height differences due to the SE(3)-equivariance. For the distracting objects, we presume that the reason why EDFs perform better is that the point cloud inputs provide better geometric information than the orthographic RGB-D inputs that SE(3)-TNs take.\n\nFigure 7: A) SE(3)-TNs fail to pick the object in an unseen pose due to the lack of SE(3)- equivariance. B) Type-0 only descriptors fail to place the object in a proper orientation due to the lack of orientational sensitivity.\n\nThe ablation study shows that using higher-type descriptors significantly increases generalization performance when the number of query points is highly limited due to computational constraints. As can be seen in Table 2, EDFs outperform the ablated model that only uses type-0 descriptors as NDFs. As was discussed in Section 4.1, we presume that this is due to the orientational insensitivity of type-0 descriptors. As type-0 descriptors cannot represent orientations alone, query points are crucial in representing orientations for the ablated model. In contrast, the higher-type descriptors of EDFs can represent the orientation without the help of query points. As a result, EDFs can maintain orientational accuracy in unseen situations in which low-quality query points are expected. We illustrate the failure case of the ablated model in Figure 7.\n\n7 DISCUSSION AND CONCLUSION\n\nThere are several limitations to EDFs that should be resolved in future works. First, faster sampling methods are required for real-time manipulations. Cooperative learning (Xie et al., 2018a; 2021b; 2022) and amortized sampling (Wang & Liu, 2016; Xie et al., 2021c) can be applied to accelerate the MCMC sampling. In addition, EDFs are not intended for tasks with significant occlusions to the target object. Future work may also encompass 3D reconstruction methods. Lastly, EDFs cannot solve problems at the trajectory level, which is a shared problem with NDFs. Future work should define the adequate equivariance condition for full trajectory-level manipulation tasks.\n\nTo summarize, we introduce EDFs and the corresponding energy-based models, which are SE(3)- equivariant end-to-end models for robotic manipulations. We propose novel bi-equivariant energybased models, which provably allow highly sample efficient and generalizable learning. We show by experiment that our method is highly sample efficient and generalizable to unseen object instances, unseen object poses, and unseen distracting objects. Lastly, we show by the ablation study that higher-degree equivariance (type1 or higher) is important for generalizability.\n\n9\n\nA)B)Published as a conference paper at ICLR 2023\n\nAcknowledgement This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2021R1A2B5B01002620). This work was partially supported by the Korea Institute of Science and Technology (KIST) intramural grants (2E31570).\n\nREFERENCES\n\nBrenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning\n\nfrom demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\n\nG Aubert. An alternative to wigner d-matrices for rotating real spherical harmonics. AIP Advances,\n\n3(6):062121, 2013.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nRoger Brockett. Notes on stochastic processes on manifolds. In Systems and Control in the Twenty-\n\nfirst Century, pp. 75–100. Springer, 1997.\n\nMiguel A Carreira-Perpinan and Geoffrey Hinton. On contrastive divergence learning. In Interna-\n\ntional workshop on artificial intelligence and statistics, pp. 33–40. PMLR, 2005.\n\nGregory S Chirikjian. Stochastic models, information theory, and Lie groups, volume 2: Analytic\n\nmethods and modern applications, volume 2. Springer Science & Business Media, 2011.\n\nGregory S Chirikjian. Partial bi-invariance of se (3) metrics. Journal of Computing and Information\n\nScience in Engineering, 15(1), 2015.\n\nEthan Chun, Yilun Du, Anthony Simeonov, Tomas Lozano-Perez, and Leslie Kaelbling. Local neural descriptor fields: Locally conditioned object representations for manipulation. arXiv preprint arXiv:2302.03573, 2023.\n\nErwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,\n\nrobotics and machine learning. http://pybullet.org, 2016–2021.\n\nRuslan L Davidchack, Thomas E Ouldridge, and Michael V Tretyakov. Geometric integrator for langevin systems with quaternion-based rotational degrees of freedom and hydrodynamic interactions. The Journal of chemical physics, 147(22):224103, 2017.\n\nXinke Deng, Yu Xiang, Arsalan Mousavian, Clemens Eppner, Timothy Bretl, and Dieter Fox. Selfsupervised 6d object pose estimation for robot manipulation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 3665–3671. IEEE, 2020.\n\nRosen Diankov.\n\nPhD thesis, Carnegie Mellon University, Robotics Institute, August 2010. URL http://www. programmingvision.com/rosen_diankov_thesis.pdf.\n\nAutomated Construction of Robotic Manipulation Programs.\n\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances\n\nin Neural Information Processing Systems, 32, 2019.\n\nPete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian In\n\nImplicit behavioral cloning.\n\nWong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Conference on Robot Learning, pp. 158–168. PMLR, 2022.\n\nPeter R Florence, Lucas Manuelli, and Russ Tedrake. Dense object nets: Learning dense visual\n\nobject descriptors by and for robotic manipulation. arXiv preprint arXiv:1806.08756, 2018.\n\nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3d rototranslation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970–1981, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nOctavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein docking. arXiv preprint arXiv:2111.07786, 2021.\n\nCaelan Reed Garrett.\n\nPybullet\n\nplanning.\n\nhttps://pypi.org/project/\n\npybullet-planning/, 2018.\n\nKhashayar Gatmiry and Santosh S Vempala. Convergence of the riemannian langevin algorithm.\n\narXiv preprint arXiv:2204.10818, 2022.\n\nMario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller, Wouter Boomsma, Bradley Dice, Kostiantyn Lapchevskyi, Maurice Weiler, Michał Tyszkiewicz, Simon Batzner, Dylan Madisetti, Martin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel Rød, and Michael Bailey. Euclidean neural networks: e3nn, apr 2022. URL https: //doi.org/10.5281/zenodo.6459381.\n\nMark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2): 123–214, 2011.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n\nDavid J Griffiths and Darrell F Schroeter. Introduction to quantum mechanics. Cambridge university\n\npress, 2018.\n\nW. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. ISSN 0006-3444. doi: 10.1093/biomet/57.1.97. URL\n\nBiometrika, 57(1):97–109, 04 1970. https://doi.org/10.1093/biomet/57.1.97.\n\nGeoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\n\ncomputation, 14(8):1771–1800, 2002.\n\nHaojie Huang, Dian Wang, Robin Walter, and Robert Platt. Equivariant transporter network. arXiv\n\npreprint arXiv:2202.09400, 2022.\n\nPriyank Jaini, Lars Holdijk, and Max Welling.\n\nLearning equivariant energy based models with equivariant stein variational gradient descent. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, pp. 16727–16737. Curran Associates, URL https://proceedings.neurips.cc/paper/2021/file/ Inc., 8b9e7ab295e87570551db122a04c6f7c-Paper.pdf.\n\nvolume 34,\n\n2021.\n\nD Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation (2018). arXiv preprint arXiv:1806.10293, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nTejas D Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, and Volodymyr Mnih. Unsupervised learning of object keypoints for perception and control. Advances in neural information processing systems, 32, 2019.\n\nPaul Langevin. Sur la th ́eorie du mouvement brownien. Compt. Rendus, 146:530–533, 1908.\n\nAdam Leach, Sebastian M Schmon, Matteo T Degiacomi, and Chris G Willcocks. Denoising diffusion probabilistic models on SO(3) for rotational alignment. In ICLR 2022 Workshop on Geometrical and Topological Representation Learning, 2022.\n\nJeong-Hoon Lee and Jongeun Choi. Hierarchical primitive composition: Simultaneous activation of skills with inconsistent action dimensions in multiple hierarchies. IEEE Robotics and Automation Letters, 7(3):7581–7588, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-\n\nmotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.\n\nYi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic\n\ngraphs. arXiv preprint arXiv:2206.11990, 2022.\n\nQiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference\n\nalgorithm. Advances in neural information processing systems, 29, 2016.\n\nNicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087–1092, 1953.\n\nMikio Nakahara. Geometry, topology and physics. CRC press, 2018.\n\nDmitry I Nikolayev and Tatjana I Savyolov. Normal distribution on the rotation group SO(3). Tex-\n\ntures and Microstructures, 29, 1970.\n\nHarish Ravichandar, Athanasios S Polydoros, Sonia Chernova, and Aude Billard. Recent advances in robot learning from demonstration. Annual Review of Control, Robotics, and Autonomous Systems, 3:297–330, 2020.\n\nTM Ivanova TI Savyolova. Normal distributions on SO(3).\n\nIn Programming And Mathematical Techniques In Physics-Proceedings Of The Conference On Programming And Mathematical Methods For Solving Physical Problems, pp. 220. World Scientific, 1994.\n\nDaniel Seita, Pete Florence, Jonathan Tompson, Erwin Coumans, Vikas Sindhwani, Ken Goldberg, and Andy Zeng. Learning to rearrange deformable cables, fabrics, and bags with goalconditioned transporter networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 4568–4575. IEEE, 2021.\n\nWeijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1711–1719, 2020.\n\nAnthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, and Vincent Sitzmann. Neural descriptor fields: SE(3)-equivariant object representations for manipulation. arXiv preprint arXiv:2112.05124, 2021.\n\nAnthony Simeonov, Yilun Du, Lin Yen-Chen, Alberto Rodriguez, Leslie Pack Kaelbling, Tomas Lozano-Perez, and Pulkit Agrawal. Se (3)-equivariant relational rearrangement with neural descriptor fields. arXiv preprint arXiv:2211.09786, 2022.\n\nMichael Spivak. Calculus on manifolds: a modern approach to classical theorems of advanced\n\ncalculus. CRC press, 2018.\n\nGusi Te, Wei Hu, Amin Zheng, and Zongming Guo. Rgcnn: Regularized graph cnn for point cloud segmentation. In Proceedings of the 26th ACM international conference on Multimedia, pp. 746– 754, 2018.\n\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.\n\nDian Wang and Robin Walters. So (2) equivariant reinforcement learning. In International Confer-\n\nence on Learning Representations, 2022.\n\nDian Wang, Jung Yeon Park, Neel Sortur, Lawson LS Wong, Robin Walters, and Robert Platt. The surprising effectiveness of equivariant models in domains with latent symmetry. arXiv preprint arXiv:2211.09231, 2022.\n\nDilin Wang and Qiang Liu. Learning to draw samples: With application to amortized mle for\n\ngenerative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5): 1–12, 2019.\n\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.\n\nIn Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681–688. Citeseer, 2011.\n\nJiaxiang Wu, Tao Shen, Haidong Lan, Yatao Bian, and Junzhou Huang. SE(3)-equivariant energy-\n\nbased models for end-to-end protein folding. bioRxiv, 2021.\n\nJianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet.\n\nIn\n\nInternational Conference on Machine Learning, pp. 2635–2644. PMLR, 2016.\n\nJianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatialIn Proceedings of the ieee conference on computer vision and\n\ntemporal generative convnet. pattern recognition, pp. 7093–7101, 2017.\n\nJianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Cooperative training of descriptor and generator networks. IEEE transactions on pattern analysis and machine intelligence, 42(1):27–45, 2018a.\n\nJianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8629–8638, 2018b.\n\nJianwen Xie, Yifei Xu, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Generative pointnet: Deep energy-based learning on unordered point sets for 3d generation, reconstruction and classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14976–14985, 2021a.\n\nJianwen Xie, Zilong Zheng, Xiaolin Fang, Song-Chun Zhu, and Ying Nian Wu. Cooperative training of fast thinking initializer and slow thinking solver for conditional learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):3957–3973, 2021b.\n\nJianwen Xie, Zilong Zheng, and Ping Li. Learning energy-based model with variational autoencoder as amortized sampler. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):10441–10451, 2021c.\n\nJianwen Xie, Yaxuan Zhu, Jun Li, and Ping Li. A tale of two flows: cooperative learning of langevin flow and normalizing flow toward energy-based model. arXiv preprint arXiv:2205.06924, 2022.\n\nYifei Xu, Jianwen Xie, Tianyang Zhao, Chris Baker, Yibiao Zhao, and Ying Nian Wu. Energybased continuous inverse optimal control. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\nAnthony Zee. Group theory in a nutshell for physicists, volume 17. Princeton University Press,\n\n2016.\n\nAndy Zeng, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker, Alberto Rodriguez, and Jianxiong Xiao. Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 1386– 1383. IEEE, 2017.\n\nAndy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprint arXiv:2010.14406, 2020.\n\nZilong Zheng, Jianwen Xie, and Ping Li. Patchwise generative convnet: Training energy-based models from a single natural image for internal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2961–2970, 2021.\n\nSong Chun Zhu and David Mumford. Grade: Gibbs reaction and diffusion equations.\n\nIn Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271), pp. 847–854. IEEE, 1998.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA BI-INVARIANT VOLUME FORM\n\nA bi-invariant volume form dg of a n-dimensional Lie group G is a differential n-form that satisfies\n\ndg = d(hg) = d(gh) ∀h ∈ G\n\nsuch that for all Borel subsets Ω ⊆ G and for all well-behaved function f (g) : G → R\n\n(cid:90)\n\ng∈Ω\n\n(cid:90)\n\ng∈Ω\n\n(cid:90)\n\ndgf (g) =\n\ndgf (g) =\n\nhg∈hΩ\n\n(cid:90)\n\ngh∈Ωh\n\nd(hg)f (cid:0)h−1(hg)(cid:1)\n\n∀h ∈ G\n\n(Left invariance)\n\nd(gh)f (cid:0)(gh)h−1(cid:1)\n\n∀h ∈ G\n\n(Right invariance)\n\n(23)\n\n(24)\n\nwhere hΩ = {hg|g ∈ Ω} and Ωh = {gh|g ∈ Ω}. Let z = φ(g) be some coordinatization of G that for some function J(z) : Rn → R, dg can be explicitly written as\n\ndg = J(z)dnz\n\nLet the left/right group translations in coordinates be z(l)(z) = φ(hg(z)) and z(r)(z) = φ(g(z)h). The bi-equivariance condition in Equation (23) can then be expressed in the coordinate form as\n\nd(hg) = J(z(l))dnz(l) = J(z(l)) det\n\nd(gh) = J(z(r))dnz(r) = J(z(r)) det\n\n(cid:21)\n\n(cid:20) ∂z(l) ∂z (cid:20) ∂z(r) ∂z\n\ndnz = J(z)dnz = dg\n\n(cid:21)\n\ndnz = J(z)dnz = dg\n\nthus leading to the following equations:\n\nJ(z(l)) det\n\n(cid:21)\n\n(cid:20) ∂z(l) ∂z\n\n= J(z) = J(z(r)) det\n\n(cid:21)\n\n(cid:20) ∂z(r) ∂z\n\n(25)\n\n(26)\n\n(cid:90)\n\nDetailed introduction to invariant volume forms on Lie groups can be found in (Chirikjian, 2011; Zee, 2016). Readers interested in differential forms may find differential geometry textbooks (Spivak, 2018; Nakahara, 2018) useful. For example, the translation group (R, +) admits a bi-invariant volume form dx because (cid:26)dx (cid:26) (cid:26) d(x + ε) (cid:26)\n\n(27) Note that the right invariance in Equation (27) is sufficient to prove the bi-invariance of dx because (R, +) is commutative, that is x + ε = ε + x ∀x, ε ∈ R. On the other hand, for the multiplicative group (R̸=0, ×), dx is not a bi-invariant volume form:\n\nd(x + ε)f ((x + ε) − ε)\n\nf ((x + ε) − ε) =\n\ndxf (x) =\n\nd(x + ε)\n\nx+ε∈Ω+ε\n\nx+ε∈Ω+ε\n\nx∈Ω\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\ndxf (x) =\n\n(cid:90)\n\nx∈Ω\n\nd(εx)\n\ndx d(εx)\n\nf (ε−1(εx))\n\nεx∈εΩ (cid:90)\n\n=\n\n1 ε\n\nεx∈εΩ\n\nd(εx)f (ε−1(εx)) ̸=\n\n(cid:90)\n\nεx∈εΩ\n\nd(εx)f (ε−1(εx))\n\n∀ε ∈ R̸=0\n\n(28)\n\nHowever, d(log |x|) = dx/x is a bi-invariant volume form: (cid:90)\n\n(cid:90)\n\ndx x\n\nf (x) =\n\nx∈Ω\n\nεx∈εΩ\n\nd(εx) x\n\ndx d(εx)\n\nf (ε−1(εx))\n\n(cid:90)\n\nεx∈εΩ\n\nd(εx) εx\n\nf (ε−1(εx))\n\n(29)\n\nAgain, we only show the left invariance because the multiplicative group is commutative.\n\nNote that not every Lie group does admit bi-invariant volume form. Nevertheless, the Lie groups that we are concerned in this paper, the SO(3) group and the SE(3) group, have bi-invariant volume forms. We reproduce here the bi-invariant volume forms of SO(3) and SE(3) in coordinate forms provided in Chirikjian (2011). The bi-invariant volume form on SO(3) can be written in Euler angles α, β,and γ as\n\n1\n\ndR =\n\n8π2 sin βdαdβdγ\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nThe bi-invariant volume form on SE(3) can be written in rotation-translation coordinate as\n\ndT = dRd3v\n\nwhere v ∈ R3 denotes the translation vector with respect to the space frame. Lastly, the bi-invariant volume form or bi-invariant integral measure of SE(3) should not be confused with the bi-invariant metric, which does not exist for SE(3) (Chirikjian, 2015).\n\nB QUERY MODELS\n\nB.1 EQUIVARIANT QUERY POINTS\n\nWe use Stein variational gradient descent (SVGD) (Liu & Wang, 2016; Jaini et al., 2021) method to equivariantly draw query points [qi;θ(Y )]i=Nq in Equation (13) from the query weight field wθ(x|Y ). In this case, wθ(x|Y ) can be interpreted as an unnormalized probability distribution on R3. The SVGD equation (Liu & Wang, 2016) is given by\n\ni=1\n\nqt+1\n\ni = qt\n\ni + ε\n\n1 Nq\n\nNq (cid:88)\n\n(cid:104)\n\nj=1\n\nk(qt\n\nj, qt\n\ni) ∇x log wθ(x|Y )|x=qt\n\nj\n\nk(x, x′) = exp\n\n(cid:20)\n\n−\n\n1 h\n\n(cid:21)\n\n∥x − x′∥2\n\n+ ∇xk(x, qt\n\ni)(cid:12)\n\n(cid:12)x=qt\n\nj\n\n(cid:105)\n\n(30)\n\n(31)\n\nNote that SVGD is fully deterministic given initial points qt=0 t / log Nq as (Liu & Wang, 2016) where medt denotes the median of the distances between all the points in (cid:110)\n\n. We use ht = med2\n\n(cid:111)\n\ni\n\n. We take the final output of SVGD as the query points, that is\n\nqt\n\n1, qt\n\n2, · · · , qt\n\nNq\n\nfor some tf in ≥ 1. We take ε and tf in as hyperparameters. In our work, we uses ε = 0.005 and tf in = 100.\n\nqi(Y ) = qt=tf in\n\ni\n\n(32)\n\nWe now show that the query point qi(Y ) in Equation (32) is SE(3)-equivariant. Proposition 8. The query points {qi(Y )}Nq (cid:8)qt=0 i=1 are SE(3)-equivariant, that is\n\n(Y )(cid:9)Nq\n\ni\n\ni=1 are SE(3)-equivariant if the initial query points\n\nT qt=0\n\ni\n\n(Y ) = qt=0\n\ni\n\n(T Y ) ∀i ⇒ T qi(Y ) = qi(T Y ) ∀i\n\n(33)\n\nTo prove Proposition 8, we first explicitly denote the query points’ dependence on Y as qt We then propose the following lemma. Lemma 1. If qt\n\ni(Y ) ∀T ∈ SE(3), the following equations hold:\n\ni(T Y ) = T qt\n\ni = qt\n\ni(Y ).\n\nk(qt\n\nj(Y ), qt\n\ni(Y ))R ∇x log wθ(x|Y )|x=qt\n\nj (Y )\n\n= k(qt\n\nj(T Y ), qt\n\ni(T Y )) ∇x log wθ(x|T Y )|x=qt\n\nj (T Y )\n\nR ∇xk(x, qt\n\ni(Y ))(cid:12)\n\n(cid:12)x=qt\n\nj (Y ) = ∇xk(x, qt\n\ni(T Y ))(cid:12)\n\n(cid:12)x=qt\n\nj (T Y )\n\nSince ∥x − x′∥2 = ∥T x − T x′∥2 ∀T ∈ SE(3), it is straightforward to prove that\n\nk(x, x′) = k(T x, T x′) ∀T ∈ SE(3)\n\n(34)\n\n(35)\n\n(36)\n\nTo prove the equivariance of the gradient terms in Equation (34) and Equation (35), we prove the following lemma. Lemma 2. If f (x|Y ) = f (T x|T Y ) for some function f : R3 × P → R, the following holds.\n\n∇xf (x|T Y )|x=T x0\n\n= R ∇xf (x|Y )|x=x0\n\n(37)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nProof.\n\n∇xf (x|T Y )|x=T x0\n\nwhere in the last line we used\n\n(cid:12)x=T x0\n\n= ∇xf (T −1x|Y )(cid:12) = ∇xf (x′|Y )|x=T x0 = R ∇x′f (x′|Y )|x′=x0\n\n(∵ f (x|Y ) = f (T x|T Y ))\n\n(Change of variables x′ = T −1x)\n\n= R ∇xf (x|Y )|x=x0\n\n(x′ → x)\n\n∇x =\n\n(cid:19)T\n\n(cid:18) ∂x′ ∂x\n\n∇x′ =\n\n(cid:32)\n\n∂ (cid:0)R−1x − (cid:24)(cid:24)(cid:24) R−1v(cid:1)\n\n(cid:33)T\n\n∂x\n\n∇x′ = (R−1)T ∇x′ = R∇x′\n\nOne can prove Lemma 1 using Lemma 2 and Equation (36).\n\nWe now prove Proposition 8 using Lemma 1. Let the sum on the right-hand side of Equation (30) be s(Y ) such that\n\ns(Y ) =\n\nNq (cid:88)\n\nj=1\n\n(cid:104)\n\nk(qt\n\nj(Y ), qt\n\ni(Y )) ∇x log wθ(x|Y )|x=qt\n\nj (Y ) + ∇xk(x, qt\n\ni(Y ))(cid:12)\n\n(cid:12)x=qt\n\nj (Y )\n\n(cid:105)\n\n(38)\n\nWe first consider the case with t = 0. One can prove that s(T Y ) = Rs(Y ) using Lemma 1 and the equivariance of the initial points T qt=0 (Y ) = qt=0 (T Y ), which was assumed in Proposition 8. It is then straightforward to prove that qt=1\n\n(Y ) is also equivariant.\n\ni\n\ni\n\ni\n\nProof.\n\nqt=1\n\ni\n\n(T Y ) = qt=0\n\ni\n\n(T Y ) + εs(T Y )\n\n(Y ) + εRs(Y )\n\n(Y ) + v + εRs(Y )\n\n(Y ) + εs(Y )) + v\n\ni\n\n= T qt=0 = Rqt=0 = R(qt=0 = T qt=1\n\ni\n\ni\n\ni\n\n(Y )\n\n(39)\n\nWe now recursively apply this relation to t = 1, 2, 3, · · · , tf in to conclude that the final query point is also equivariant, that is qt=tf in (T Y ). Therefore, the only requirement for our query points to be equivariant is the equivariance of the initial points: T qt=0 (T Y ). We provide a simple (and clearly not the best) deterministic method that we used to sample the initial point qt=0\n\n(Y ) in Algorithm 1.\n\n(Y ) = qt=tf in\n\n(Y ) = qt=0\n\ni\n\ni\n\ni\n\ni\n\ni\n\nAlgorithm 1 Simple algorithm for deterministic and equivariant initial point sampling\n\nInput: Y = {(y1, c1), · · · , (yM , cM )}, w(x|Y ), Nmax, rcluster Output: q1, q2, · · ·\n\ni ← 1 Q ← {y1, · · · , yM } while i ≤ Nmax do\n\n▷ Initialize set Q with the set of all the points in Y\n\nif Q is not empty then qi ← arg max Q ← Q − (cid:8)z ∈ Q (cid:12)\n\ny∈Q\n\nw(y|Y )\n\n(cid:12) ∥z − qi∥2 ≤ rcluster\n\n▷ Take the point with largest weight in Q\n\n(cid:9)\n\n▷ Remove the neighbors from Q\n\nend if i ← i + 1\n\nend while\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.2 SURROGATE QUERY MODEL\n\nLet A(r) be the set of all the indices of the query points whose shortest distance to the point cloud X is farther than some radius r such that A(r) = {i ∈ {1, 2, · · · , Nq}|dmin(T qi, X) ≥ r}. The Kullback-Leibler divergence term in Equation (22) can be calculated as follows:\n\nDKL(H(w, Q|X, Y, T )∥ ˆP (w, Q|Y ))\n\n(cid:90)\n\ndwi\n\nR3\n\n(cid:90)\n\nNQ (cid:89)\n\n(cid:90)\n\n\n\n\n\nR+\n\n(cid:90)\n\ni=1\n\n(cid:88)\n\ni∈A(r)\n\nR+\n\n(cid:90)\n\n(cid:88)\n\ni∈A(r)\n\nR+\n\n=\n\n=\n\n=\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nR\n\n(cid:90)\n\nR\n\n(cid:88)\n\ni∈A(r)\n\n(cid:88)\n\ni∈A(r)\n\n(cid:88)\n\ni∈A(r)\n\n(cid:88)\n\ni∈A(r)\n\n\n\ndqiHi(wi, qi|X, Y, T )\n\n\n\nNQ (cid:88)\n\nj=1\n\nlog\n\nHj(wj, qj|X, Y, T ) ˆPj(wj, qj|Y )\n\ndwi\n\ndqiHi(wi, qi|X, Y, T ) log\n\nHi(wi, qi|X, Y, T ) ˆPi(wi, qi|Y )\n\nR3\n\n(cid:8)(cid:8)dwi\n\n(cid:90)\n\ndli\n\n(cid:8)(cid:8)dwi(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)\n\nR3\n\ndqiδ(3)(qi − qi(Y ))N (li; α, σH )\n\n× log\n\nN (li; α, σH )(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40) N (li; log w(qi|Y ), σH )(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)\n\nδ(3)(qi − qi(Y ))\n\nδ(3)(qi − qi(Y )) N (li; α, σH ) N (li; log w(qi(Y )|Y ), σH )\n\ndliN (li; α, σH ) log\n\ndliN (li; α, σH )\n\n−\n\n(cid:20)\n\n1 2σ2 H\n\n(cid:110)\n\n(li − α)2 − (li − log w(qi(Y )|Y ))2(cid:111)(cid:21)\n\nEε∼N0,1\n\n(cid:20)\n\n−\n\n1 2σ2 H\n\n(cid:110)\n\n(εσH )2 − (εσH + α − log w(qi(Y )|Y ))2(cid:111)(cid:21)\n\n1 2\n\n(cid:18) log w(qi(Y )|Y ) − α σH\n\n(cid:19)2\n\n(40)\n\nwhere in the third line we used\n\nlog\n\nHj(wj, qj|X, Y, T ) ˆPj(wj, qj|Y )\n\n= log (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24) ˆPj(wj, qj|Y ) (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24) ˆPj(wj, qj|Y )\n\n= 0 ∀j /∈ A(r)\n\nand in the last line, we used the reparameterization trick for Gaussian distributions (Kingma & Welling, 2013).\n\nB.3 QUERY ATTENTION\n\nDue to the computational limitations, it is desirable to have as few query points as possible during the inference time. Therefore, instead of directly taking wi = wθ(qi;θ(Y )), we normalize the query weights by taking\n\nwi =\n\nwθ(qi(Y ))\n\n(cid:80)Nq\n\nj=1 wθ(qj(Y ))\n\n(41)\n\nsuch that the query points compete with each other during the training. As a result of this competition, only a few query points have non-negligible weights. Therefore, during the inference time, we can calculate for only a few query points with non-negligible weights instead of calculating the whole query points to save the computation. Note that the normalized query weight is still equivariant because only scalar addition and division were used in Equation (41).\n\nC INTUITION BEHIND THE BI-EQUIVARIANCE CONDITION\n\nTo illustrate the bi-equivariance condition in Equation (6), consider an object placing task where Tgo ∈ SE(3) is the object pose (o) in the gripper frame (g) and Tsd ∈ SE(3) is the desired object\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: A) The end-effector should follow the transformation of the placement target to keep the relative pose between the object and the placement target invariant. B) The end-effector should transform contravariantly to compensate for the transformation of the grasped object such that the relative pose between the object and the placement target is invariant.\n\npose (d) that is to be placed in the scene frame (s). Consequently, the gripper pose in the scene frame Tsg should satisfy the following equation\n\nTsd = TsgTgo\n\n(42)\n\nNow, let the desired pose of the object to be placed has been transformed as Tsd → T ′ sd = STsd for some transformation S ∈ SE(3). In order to keep the relation in Equation (42) invariant such that for the new gripper pose T ′ sg = STsg. Similarily, if the pose of the grasped object is transformed as Tgo → T ′′ go = STgo, the gripper pose should also be transformed as Tsg → T ′′ sg = TsgS−1 to keep the relation in Equation (42) invariant. Since Tsd and Tgo are implicitly encoded in X and Y individually, one can naively substitute Tsd and Tgo into X and Y to get the equation\n\nsgTgo holds, it should be that T ′\n\nsg the equation T ′\n\nsd = T ′\n\nP (Tsg|X, Y ) = P (T ′\n\nsg = STsg|S X, Y ) = P (T ′′\n\nsg = TsgS|X, S−1 Y )\n\nThis is the intuition behind Equation (6). We illustrate the bi-equivariance condition in Figure 8\n\nD SAMPLING DETAILS\n\nFor the sampling, we use the Metropolis-Hastings (MH) algorithm (Hastings, 1970; Metropolis et al., 1953) and Langevin algorithm on the SE(3) manifold (Brockett, 1997; Chirikjian, 2011; Davidchack et al., 2017). Unlike the MH, the Langevin algorithm does not suffer from high rejection ratios and converges with much fewer iterations. However, the Langevin algorithm requires the gradient of the energy function and thus is computationally inefficient. In addition, the time step for the Langevin algorithm cannot be arbitrarily high to maintain the precision of the dynamics. Therefore, we first run MH for rapid exploration and then run the Langevin algorithm using the MH samples as initial seeds. Note that the differential geometric aspects of the SE(3) manifold must be considered in implementing these methods.\n\nFor the following sections, we provide details of the sampling methods that we used. We first explain the proposal distributions that we used to run the MH algorithm on the SE(3) manifold. We then introduce the Langevin algorithm on SE(3). We calculate the Langevin dynamics in quaterniontranslation parameterization as Davidchack et al. (2017) to avoid singularity while benefiting from commonly used autograd packages.\n\nD.1 PROPOSAL DISTRIBUTION FOR MH\n\nThe Metropolis-Hastings (MH) algorithm (Hastings, 1970) is a propose-and-reject algorithm used for sampling from some probability distribution dP (T ). First, a proposal point Tp is sampled from the proposal distribution dQ(Tp|Tt). The proposed point Tp is stochastically accepted or rejected\n\nby the acceptance ratio A = min . If the proposal is accepted, the next point is the proposed point, that is Tt+1 = Tp. If rejected, the point remains the same, that is Tt+1 = Tt. It is known that the steady-state distribution dP∞(T∞) converges to dP (T ).\n\n1, dP (Tp)dQ(Tt|Tp) dP (Tt)dQ(Tp|Tt)\n\n(cid:105)\n\n(cid:104)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nWe decompose the proposal distribution dQ(T |Tt) = Q(T |Tt)dT into 1) the orientation proposal distribution QR(R|Rt)dR and 2) the position proposal distribution Qv(v|vt)d3v such that\n\nQ(T |Tt)dT = QR(R|Rt)dR × Qv(v|vt)d3v\n\nwhere d3v is the Euclidean volume element and dR is the bi-invariant volume form of SO(3) (See Appendix A). We use Gaussian distribution for the position proposal, that is Qv(vp|vt) = N (vp; vt, σI). For the orientation proposal QR(Rp|Rt), we used IGSO(3) which is the normal distribution on SO(3) (Nikolayev & Savyolov, 1970; Savyolova, 1994; Leach et al., 2022). Concrete calculation and sampling methods for IGSO(3) are provided in Appendix D.2.\n\nD.2 NORMAL DISTRIBUTION ON SO(3)\n\nWe follow the method in Leach et al. (2022) to calculate and sample from IGSO(3), the normal distribution on SO(3) (Nikolayev & Savyolov, 1970; Savyolova, 1994). In the axis-angle parameterization, our orientation proposal distribution QR(Rp|Rt)dR, which is IGSO(3), can be written as\n\nQR(Rp|Rt)dR = (1 − cos ω)/π × fε(ω)dωdΩ where ω ∈ [0, π) is the rotation angle, dΩ = sin θ/π × dθdφ is the uniform volume element over the sphere S2 where the rotation axis lies, and fε(ω) is as follows:\n\nfε(ω) =\n\n(cid:34) ∞ (cid:88)\n\nl=0\n\n(2l + 1)e−εl(l+1) sin ((2l + 1)ω/2)\n\nsin ω/2\n\n(cid:35)\n\n(43)\n\nWe approximate the infinite sum in Equation (43) by summing up to sufficiently high l. Note that the summand in Equation (43) decays exponentially fast to the square of l. Therefore, the approximation is justified. The rotation axis vector can be easily sampled by first sampling from three-dimensional Gaussian and then normalizing it. For the sampling of ω, one may use numerical inverse transform sampling. As noted by Leach et al. (2022), the volume element (1 − cos ω)/π should be multiplied to fε(ω) for the inverse transform sampling.\n\nD.3 LANGEVIN MCMC ON SE(3)\n\nLet Vi be the i-th basis of the Lie algebra of an unimodular Lie group G. Consider the following stochastic process g(t) ∈ G generated by a Lie algebra δX(t) = (cid:80) i δXi(t)Vi ∈ TeG such that g(t) = g(0) exp [δX(0)] exp [δX(dt)] · · · exp [δX(t − dt)]. The Langevin dynamics for G is then\n\n2dwi dt is the standard Wiener process and LV f = (cid:0) d where dwi ∼ N0; Lie derivative of a function f on G along V. It is known that this process converges to\n\nδXi(t) = −LVi [E(g)] dt +\n\nds f (g exp[sV])(cid:1)(cid:12)\n\n√\n\n(44)\n\n(cid:12)s=0 is the (left)\n\n√\n\nwhen t → ∞ where dg is the (left) invariant volume form of G (Brockett, 1997; Chirikjian, 2011).\n\ndP∞(g) ∝ exp [−E(g)] dg\n\nDavidchack et al. (2017) provide concrete ways to calculate the Lie derivative and the Langevin dynamics on SE(3) in quaternion-translation parameterization. Quaternion-translation parameterization is convenient because it has no singularities. Therefore, the gradients from commonly used autograd packages can be easily used to calculate the dynamics. For SE(3), Vi is the Lie algebra basis of SO(3) for i = 1, 2, 3 and the translation generator for i = 4, 5, 6. Let the quaterniontranslation parameterization be z = (q, v) ∈ S3 × R3 ⊂ R7. Let L ∈ R7×6 be the Lie derivative matrix whose (μ, i)’th element is [L]μ\n\ni = LVizμ. The matrix can be calculated as (cid:21)\n\nL =\n\nLSO(3) =\n\n03×3 \n\n(cid:20)LSO(3) 04×3 I3×3 −q2 −q3 −q4 q1 −q4 q3 q1 −q2 q4 q1 q2 −q3\n\n1 2\n\n \n\n19\n\n\n\n \n\n(45)\n\nPublished as a conference paper at ICLR 2023\n\nwhere q = q1 + q2i + q3j + q4k. Derivations of Equation (45) can be found in (Davidchack et al., 2017). Since the chain rule holds for the Lie derivatives (Chirikjian, 2011), the Equation (44) can be written in the parameterized form as\n\ndz =\n\n(cid:19)\n\n(cid:18)dq dv\n\n= −G−1∇zE(z)dt +\n\n√\n\n2Ldw\n\n(46)\n\nwhere G−1 = LLT . We calculate the gradient of the energy ∇zE(z) using typical autograd packages. Note that dq in Equation (46) satisfies the unit-quaternion constraint q · dq = 0 such that q + dq ∈ S3 (Davidchack et al., 2017). In practice, however, we reproject q + dq onto S3 by a normalization because of the inaccuracy in numerical integration.\n\nE PICK-MODEL AND THE RELATIONSHIP TO NDFS\n\nPick-model For the place model, the point cloud of the end-effector Y is always different because of the grasped object. On the other hand, Y is always the same for the pick-model because no object has been grasped yet. Therefore, we remove the Y -dependence of the query EDF and the query density by taking ψθ(x|Y ) to ψθ(x) and ρθ(x|Y ) to ρθ(x). In this case, the energy function Eθ(T |X, Y ) in Equation (14) becomes\n\nEθ(T |X, Y ) = Eθ(T |X) =\n\nNq (cid:88)\n\nwi∥φθ(T qi|X) − D(R)ψi∥2\n\n(47)\n\ni=1 where wi, qi and ψi are not the outputs of some functions anymore but just parameters that are either predefined or learned.\n\nRelationship to NDFs We now illustrate the relation of Equation (47) to the energy function of NDFs (Simeonov et al., 2021). Let the energy function in Simeonov et al. (2021) be\n\nEN DF (T |X) =\n\nNq (cid:88)\n\ni=1\n\n∥φ(T qi|X) − ψi∥1\n\nψi =\n\n1 Ndemo\n\nNdemo(cid:88)\n\nn=1\n\nφ( ˆTnqi| ˆXn)\n\n(48)\n\n(49)\n\nwhere ˆTn and ˆXn are the grasp pose and the point cloud input of the n’th demonstration. The Equation (48) can be understood as a special case of Equation (47) with (i) the L1 error instead of the square error, (ii) all the query weights being constant wi = 1, and (iii) all the components of the feature EDF φ(x|X) being the invariant scalars (type-0 vectors) such that D(R) = I.\n\nRelationship to other variants of NDFs Other recent works derived from NDFs (Simeonov et al., 2022; Chun et al., 2023) also are closely related to EDFs. Relational Neural Descriptor Fields (RNDFs) (Simeonov et al., 2022) extend NDFs’ fixed placement target tasks to object rearrangement tasks in which a placement target is also an object with varying poses. Instead of using fixed target descriptors as NDFs, R-NDFs utilize another descriptor field to represent the target placement object. The resulting R-NDFs’ energy function is very similar to EDFs’ bi-equivariant energy function. This is a natural consequence due to the bi-equivariant nature of object rearrangement tasks.\n\nOn the other hand, Local Neural Descriptor Fields (L-NDFs) (Chun et al., 2023) focus on imposing locality on the descriptor fields. While EDFs and L-NDFs both try to exploit locality, the motivation is largely different. Chun et al. (2023) focus on locality to improve generalization and transferability to novel objects. This is distinguished from the major motivation for imposing locality on EDFs: removing the necessity of object segmentation.\n\nOne thing to note is that these studies are not mutually exclusive but complement each other. EDFs can be pre-trained with a similar method to NDFs and R-NDFs. The strict SE(3)-equivariance of EDFs can be relaxed by imposing the SE(3)-equivariance on the loss function as L-NDFs instead of imposing it on the model itself. Conversely, these methods may benefit from the end-to-end trainability and generative nature of EDFs’ energy-based model. The orientational sensitivity of higher-type (equivariant) descriptors should also be beneficial to these methods.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nIrrepwise L1 Norm For closer analogy with the energy function of NDFs, we propose using irrepwise L1 norm. Let an equivariant vector f be given by f = (cid:76)N n=1 f (n) where f (n) is a type-ln vector. We then define the irrepwise L1 norm as\n\n∥f ∥I\n\n1 =\n\nN (cid:88)\n\nn=1\n\n∥f (n)∥2\n\nIf we use irrepwise L1 norm in Equation (47) and confine all the vectors in EDFs to be of type-0, Equation (47) and Equation (48) are exactly identical. Although we did not use irrepwise L1 norm in our work, we expect that this modification would be more robust to outliers than using the square error term.\n\nF PROOFS\n\nF.1 PROOF OF PROPOSITION 1\n\nWe first prove the left equivariance.\n\nProof. (cid:90)\n\nT ∈SΩ\n\ndP (T |S X, Y ) =\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nT ∈SΩ\n\n(cid:90)\n\nT ∈SΩ\n\n(cid:90)\n\ndT P (T |S X, Y )\n\ndT P (S−1T |X, Y )\n\n(∵ P (ST |S X, Y ) = P (T |X, Y ))\n\nd(S−1T )P (S−1T |X, Y )\n\n(∵ bi-invariance of dT )\n\nS−1T ∈Ω\n\n(cid:90)\n\nT ∈Ω\n\n(cid:90)\n\nT ∈Ω\n\ndT P (T |X, Y )\n\ndP (T |X, Y )\n\n(S−1T → T )\n\nThe right equivariance can be similarly proved.\n\nProof. (cid:90)\n\nT ∈ΩS\n\ndP (T |X, S−1 Y ) =\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nT ∈ΩS\n\n(cid:90)\n\nT ∈ΩS\n\n(cid:90)\n\ndT P (T |X, S−1 Y )\n\ndT P (T S−1|X, Y )\n\n(∵ P (T S|X, S−1 Y ) = P (T |X, Y ))\n\nd(T S−1)P (T S−1|X, Y )\n\n(∵ bi-invariance of dT )\n\nT S−1∈Ω\n\n(cid:90)\n\nT ∈Ω\n\n(cid:90)\n\nT ∈Ω\n\ndT P (T |X, Y )\n\ndP (T |X, Y )\n\n(T S−1 → T )\n\nF.2 PROOF OF PROPOSITION 2\n\nLet the partition function (the denominator) of Equation (9) be Z(X, Y ).\n\nLemma 3. For a bi-equivariant energy function E(T |X, Y ), the following equation holds.\n\nZ(X, Y ) = Z(S X, Y ) = Z(X, S−1 Y )\n\n(50)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nProof.\n\nZ(S X, Y ) =\n\n(cid:90)\n\ndT exp [−E(T |S X, Y )]\n\nSE(3)\n\n(cid:90)\n\nSE(3)\n\n(cid:90)\n\nSE(3)\n\n(cid:90)\n\nSE(3)\n\n=\n\n=\n\n=\n\ndT exp (cid:2)−E(S−1T |X, Y )(cid:3)\n\n(∵ E(ST |S X, Y ) = E(T |X, Y ))\n\nd(S−1T ) exp (cid:2)−E(S−1T |X, Y )(cid:3)\n\n(∵ bi-invariance of dT )\n\ndT exp [−E(T |X, Y )] = Z(X, Y )\n\n(S−1T → T )\n\nZ(X, S−1 Y ) =\n\n(cid:90)\n\ndT exp (cid:2)−E(T |X, S−1 Y )(cid:3)\n\nSE(3)\n\n(cid:90)\n\nSE(3)\n\n(cid:90)\n\nSE(3)\n\n(cid:90)\n\nSE(3)\n\n=\n\n=\n\n=\n\ndT exp (cid:2)−E(T S−1|X, Y )(cid:3)\n\n(∵ E(T S|X, S−1 Y ) = E(T |X, Y ))\n\nd(T S−1) exp (cid:2)−E(T S−1|X, Y )(cid:3)\n\n(∵ bi-invariance of dT )\n\ndT exp [−E(T |X, Y )] = Z(X, Y )\n\n(T S−1 → T )\n\nNow we prove the bi-equivariance of Equation (9) using Lemma 3.\n\nProof.\n\nP (ST |S X, Y ) = exp [−E(ST |S X, Y )]/Z(S X, Y )\n\n= exp [−E(T |X, Y )]/Z(X, Y ) = P (T |X, Y ) = exp (cid:2)−E(T S|X, S−1 Y )(cid:3)/Z(X, S−1 Y ) = P (T S|X, S−1 Y )\n\nF.3 PROOF OF PROPOSITION 3\n\nWe first show that the energy function in Equation (10) satisfies E(ST |S X, Y ) = E(T |X, Y ) where T = (R, v) ∈ SE(3) and S = (RS, vS) ∈ SE(3).\n\nProof.\n\nE(ST |S X, Y )\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\nd3xρ(x|Y )∥φ(ST x|S X) − D(RSR)ψ(x|Y )∥2\n\nd3xρ(x|Y )∥D(RS)φ(T x|X) − D(RSR)ψ(x|Y )∥2\n\n(∵ Equation (8))\n\nd3xρ(x|Y )∥D(RS)φ(T x|X) − D(RS)D(R)ψ(x|Y )∥2\n\n(∵ Equation (1))\n\nd3xρ(x|Y )∥φ(T x|X) − D(R)ψ(x|Y )∥2 = E(T |X, Y )\n\nwhere the orthogonality of the representation D(R) is used in the last line. Note that the inner product of two vectors is invariant to orthogonal transformations.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nWe now prove that E(T S|X, S−1 Y ) = E(T |X, Y ).\n\nProof.\n\nE(T S|X, S−1 Y )\n\n=\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\n(cid:90)\n\nR3\n\nd3xρ(x|S−1 Y )∥φ(T Sx|X) − D(RRS)ψ(x|S−1 Y )∥2\n\nd3xρ(x|S−1 Y )∥φ(T Sx|X) − D(R)D(RS)ψ(x|S−1 Y )∥2\n\n(∵ Equation (1))\n\nd3xρ(Sx|Y )∥φ(T Sx|X) − D(R)ψ(Sx|Y )∥2\n\nd3(Sx)ρ(Sx|Y )∥φ(T Sx|X) − D(R)ψ(Sx|Y )∥2\n\n(∵ d3(T x) = d3x ∀T ∈ SE(3))\n\nd3xρ(x|Y )∥φ(T x|X) − D(R)ψ(x|Y )∥2\n\n(Sx → x)\n\nIn the fourth line, we used ρ(T x|T Y ) = ρ(x|Y ) and ψ(T x|T Y ) = D(R)ψ(x|Y ) by the definition of the query density and the query EDF. Note that in the fifth line we used the SE(3)- invariance of the Euclidean volume element d3x, that is\n\nd3(T x) = det [∂(Rx + v)/∂x]d3x\n\n= det [∂(Rx)/∂x]d3x = (cid:24)(cid:24)(cid:24)\n\ndet R(cid:24)(cid:24)(cid:24)\n\ndet I d3x = d3x ∀T = (R, v) ∈ SE(3)\n\n(51)\n\nTherefore, the energy function E(T |X, Y ) in Equation (10) is indeed bi-equivariant.\n\nF.4 PROOF OF PROPOSITION 4\n\nProof. Let a query density satisfies Equation (11) such that ρ(x|Y ) = ρ(T x|T Y ) ∀T ∈ SE(3). If this query density is grasp-independent such that ρ(x|Y ) = ρ(x), then ρ(x) = ρ(T x) ∀T ∈ SE(3) by Equation (11). Since there always exists some T ∈ SE(3) such that T x = x′ for any x′ ∈ R3, ρ(x) must be a constant function. In other words, there exists no grasp-independent and non-constant query density that satisfies Equation (11).\n\nF.5 PROOF OF PROPOSITION 5\n\nProof.\n\nρθ(T x|T Y ) =\n\n=\n\n=\n\n=\n\nNQ (cid:88)\n\ni=1\n\nNQ (cid:88)\n\ni=1\n\nNQ (cid:88)\n\ni=1\n\nNQ (cid:88)\n\ni=1\n\nwθ (qi;θ(T Y )|T Y ) δ(3) (T x − qi;θ(T Y ))\n\nwθ (T qi;θ(Y )|T Y ) δ(3) (T x − T qi;θ(Y ))\n\nwθ (qi;θ(Y )|Y ) δ(3) (T x − T qi;θ(Y ))\n\n(52)\n\nwθ (qi;θ(Y )|Y ) δ(3) (x − qi;θ(Y )) = ρθ(x|Y )\n\nwhere Equation (13) was used in the second and the third lines.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nF.6 PROOF OF PROPOSITION 6\n\nLet the query model P (w, Q|Y ) be SE(3)-equivariant such that\n\nP (w, Q|Y ) = P (w, SQ|S Y ) ∀S ∈ SE(3)\n\nWe first show that P (T |X, Y, w, Q) satisfies\n\nP (T |X, Y, w, Q) = P (ST |S X, Y, w, Q)\n\n= P (T S|X, S−1 Y, w, S−1Q) ∀S = (RS, vS) ∈ SE(3)\n\n(53)\n\n(54)\n\nTo prove Equation (54), we first show that (cid:101)E(T |X, Y, w, q) in Equation (15) satisfies the following:\n\n(cid:101)E(ST |S X, Y, w, q) = (cid:101)E(T |X, Y, w, q) = (cid:101)E(T S|X, S−1 Y, w, S−1q)\n\nProof. We first prove the left equivariance.\n\n(cid:101)Eθ(ST |S X, Y, w, q) = w∥φθ(ST q|S X) − D(RS)D(R)ψθ(q|Y )∥2 = w∥D(RS)φθ(T q|X) − D(RS)D(R)ψθ(q|Y )∥2 = w∥φθ(T qi|X) − D(R)ψθ(qi|Y )∥2 = (cid:101)Eθ(T |X, Y, w, q)\n\nWe now prove the right equivariance.\n\nSS−1qi|X) − D(R)D(RS)ψθ(S−1qi|S−1 Y )∥2\n\n(cid:101)Eθ(T S|X, S−1 Y, w, S−1q) = w∥φθ(T(cid:24)(cid:24)(cid:24) = w∥φθ(T qi|X) − D(R)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40) D(RS)D(R−1 = w∥φθ(T qi|X) − D(R)ψθ(qi|Y )∥2 = (cid:101)Eθ(T |X, Y, w, q)\n\nS )ψθ(qi|Y )∥2\n\n(∵ Equation (8))\n\n(∵ D(R)T = D(R)−1)\n\n(∵ Equation (8))\n\n(∵ D(R−1) = D(R)−1)\n\nOne may simply replace the energy function E(T |X, Y ) in Appendix F.2 with the new energy function E(T |X, Y, v, Q) = (cid:80)Nq\n\ni=1 (cid:101)E(T |X, Y, wi, qi) to find that Equation (54) indeed holds.\n\nNow we show that the marginal PDF P (T |X, Y ) is bi-equivariant,\n\nProof.\n\nP (ST |S X, Y )\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n=\n\n=\n\n=\n\n=\n\n=\n\ndwdQP (ST |S X, Y, w, Q)P (w, Q|Y )\n\ndwdQP (T |X, Y, w, Q)P (w, Q|Y ) = P (T |X, Y )\n\n(∵ Equation (53) and Equation (54))\n\ndwdQP (T S|X, S−1 Y, w, S−1Q)P (w, S−1Q|S−1 Y )\n\ndwd(S−1Q)P (T S|X, S−1 Y, w, S−1Q)P (w, S−1Q|S−1 Y )\n\n(∵ Equation (51))\n\ndwdQP (T S|X, S−1 Y, w, Q)P (w, Q|S−1 Y ) = P (T S|X, S−1 Y )\n\n(S−1Q → Q)\n\nIn the fourth line, we used the SE(3)-invariance of the Eulcidean volume element in Equation (51):\n\n(cid:90)\n\ndQ =\n\nNq (cid:89)\n\n(cid:90)\n\nR3\n\ni=1\n\nd3qi =\n\nNq (cid:89)\n\n(cid:90)\n\nR3\n\ni=1\n\n(cid:90)\n\nd3(T qi) =\n\nd(T Q) ∀T ∈ SE(3)\n\n(55)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nF.7 PROOF OF PROPOSITION 7\n\nWe first show that ˆPi(wi, qi|Y ) in Equation (20) is SE(3)-equivariant:\n\nˆPi(wi, T qi|T Y ) = ˆPi(wi, qi|Y ) ∀T ∈ SE(3)\n\n(56)\n\nProof.\n\nˆPi(wi, T qi|T Y )\n\n=\n\n=\n\ndli dwi dli dwi\n\nN (li; log w(T qi|T Y ), σH )δ(3)(T qi − qi(T Y ))\n\nN (li; log w(qi|Y ), σH )δ(3)((cid:0)T qi −(cid:0)T qi(Y ))\n\n(∵ Equation (13))\n\n= ˆPi(wi, qi|Y )\n\nAs a result, ˆP (w, Q|Y ) = (cid:81)Nq\n\ni=1\n\nˆPi(wi, qi|Y ) in Equation (20) also satisfies\n\nˆP (w, T Q|T Y ) = ˆP (w, Q|Y ) ∀T ∈ SE(3)\n\n(57)\n\nWe now show that Hi(wi, qi|X, Y, T ) in Equation (21) satisfies the following equation:\n\nHi(wi, qi|X, Y, T ) = Hi(wi, qi|S X, Y, ST ) = Hi(wi, S−1qi|X, S−1 Y, T S)\n\n(58)\n\nProof.\n\nHi(wi, qi|S X, Y, ST )\n\n=\n\n=\n\n(cid:26) ˆPi(wi, qi|Y )\n\n(dli/dwi)N (li; α, σH )δ(3)(qi − qi(Y ))\n\n(cid:26) ˆPi(wi, qi|Y )\n\n(dli/dwi)N (li; α, σH )δ(3)(qi − qi(Y ))\n\nif dmin(ST qi, S X) < r else\n\nif dmin(T qi, X) < r else\n\n= Hi(wi, qi|X, Y, T )\n\n=\n\n=\n\n(cid:26) ˆPi(wi, S−1qi|S−1 Y )\n\n(dli/dwi)N (li; α, σH )δ(3)(S−1qi − S−1qi(Y ))\n\n(cid:26) ˆPi(wi, S−1qi|S−1 Y )\n\n(dli/dwi)N (li; α, σH )δ(3)(S−1qi − qi(S−1 Y ))\n\n(cid:0)T (SS−1)qi, X(cid:1) < r\n\nif dmin else\n\n(cid:0)(T S)(S−1qi), X(cid:1) < r\n\nif dmin else\n\n= Hi(wi, S−1qi|X, S−1 Y, T S)\n\n(A)\n\n(B)\n\n(C)\n\nWe used dmin(T x, T X) = dmin(x, X) ∀T ∈ SE(3) in (A). This is because the Euclidean distance is preserved under SE(3) transformations. We used δ3(x1 − x2) = δ3(T x1 − T x2) ∀T ∈ SE(3) and Equation (56) in (B). Lastly, we used Equation (13) in (C).\n\nTherefore, H(w, Q|X, Y, T ) = (cid:81)Nq\n\ni=1 Hi(wi, qi|X, Y, T ) also satisfies\n\nH(w, Q|X, Y, T ) = H(w, Q|SX, Y, ST ) = H(w, S−1Q|X, S−1 Y, T S)\n\n(59)\n\nWe now propose the following lemma.\n\nLemma 4. Let a scalar function f (T |X, Y ) be defined as follows:\n\nf (T |X, Y ) =\n\n(cid:90)\n\n(cid:90)\n\ndw\n\ndQ h1(T, X, Y, w, Q)h2(T, X, Y, w, Q)\n\nf (T |X, Y ) is bi-equivariant if h1(T, X, Y, w, Q) and h2(T, X, Y, w, Q) satisfies\n\nhi(T, X, Y, w, Q) = hi(ST, S X, Y, w, Q)\n\n= hi(T S, X, S−1 Y, w, S−1Q) ∀S ∈ SE(3), i ∈ {1, 2}\n\n(60)\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nProof.\n\nf (ST |SX, Y )\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n=\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\ndw\n\ndw\n\ndw\n\ndw\n\ndw\n\ndQ h1(ST, SX, Y, w, Q)h2(ST, SX, Y, w, Q)\n\ndQ h1(T, X, Y, w, Q)h2(T, X, Y, w, Q) = f (T |X, Y )\n\n(∵ Equation (60))\n\ndQ h1(T S, X, S−1 Y, w, S−1Q)h2(T S, X, S−1 Y, w, S−1Q) (∵ Equation (60))\n\nd(S−1Q) h1(T S, X, S−1 Y, w, S−1Q)h2(T S, X, S−1 Y, w, S−1Q)\n\n(∵ Equation (55))\n\ndQ h1(T S, X, S−1 Y, w, Q)h2(T S, X, S−1 Y, w, Q)\n\n(S−1Q → Q)\n\n= f (T S|X, S−1Y )\n\nWe now prove the bi-equivariance of Lθ(T |X, Y ) in Equation (22).\n\nProof. We first define h(T, X, Y, w, Q) as follows:\n\nh(T, X, Y, w, Q) = log Pθ(T |X, Y, w, Q) + ˆPθ(w, Q|Y ) − Hθ(w, Q|X, Y, T ) Using Equation (54), Equation (57) and Equation (59), one can prove that h(T, X, Y, w, Q) satisfies Equation (60). In addition, Hθ(w, Q|X, Y, T ) satisfies Equation (60) as was shown in Equation (59). Because Lθ(T |X, Y ) can be written as\n\n(61)\n\nLθ(T |X, Y ) =Ew,Q∼Hθ [log Pθ(T |X, Y, w, Q)] − DKL\n\n(cid:104)\n\nHθ(w, Q|X, Y, T )\n\n(cid:13) (cid:13) (cid:13)\n\nˆPθ(w, Q|Y )\n\n(cid:105)\n\n(cid:90)\n\n=\n\n(cid:90)\n\ndw\n\ndQ Hθ(w, Q|X, Y, T )h(T, X, Y, w, Q)\n\n(62)\n\nwe prove the bi-equivariance of Lθ(T |X, Y ) in Equation (22) using Lemma 4.\n\nG EQUIVARIANT GRAPH NEURAL NETWORKS\n\nGraph neural networks are often used to model point cloud data (Wang et al., 2019; Te et al., 2018; Shi & Rajkumar, 2020). SE(3)-equivariant graph neural networks (Thomas et al., 2018; Fuchs et al., 2020; Liao & Smidt, 2022) exploit the roto-translation symmetry of graphs with spatial structures. In this work, we use Tensor Field Networks (TFNs) (Thomas et al., 2018) and the SE(3)-transformers (Fuchs et al., 2020) as the backbone networks for our models.\n\nTensor Product and Spherical Harmonics Given two vectors u and v of type-l1 and -l2, the tensor product u ⊗ v transforms according to a rotation R ∈ SO(3) as\n\nu ⊗ v → (Dl1 (R)u) ⊗ (Dl2 (R)v)\n\n(63)\n\nTensor products are important because they can be used to construct new vectors of different types. By a change of basis the tensor product u⊗v can be decomposed into the direct sum of type-l vectors using the Clebsch-Gordan coefficients (Thomas et al., 2018; Zee, 2016; Griffiths & Schroeter, 2018). Let this type-l vector be (u ⊗ v)(l). The m’th components of this vector is calcuated as:\n\n(u ⊗ v)(l)\n\nm =\n\nl1(cid:88)\n\nl2(cid:88)\n\nm1=−l1\n\nm2=−l2\n\nC (l,m)\n\n(l1,m1)(l2,m2)um1vm2\n\n(64)\n\nwhere C (l,m) for |l1 − l2| ≤ l ≤ l1 + l2.\n\n(l1,m1)(l2,m2) is the Clebsch-Gordan coefficients in real basis, which can be nonzero only\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nThe (real) spherical harmonics Y (l) m (x/∥x∥) are orthonormal functions that form the complete basis of the Hilbert space on the sphere S2. l ∈ {0, 1, 2, · · · } is called the degree and m ∈ {−l, · · · , l} is called the order of the spherical harmonic function.\n\nm=−l, · · · , Y l Consider the following (2l + 1)-dimensional vector field Y(l) = (Y l dimensional rotation R ∈ SO(3), Y(l) transforms like a type-l vector field such that\n\nm=l). By a 3-\n\nY(l)(R(x/∥x∥)) = Dl(R)Y(l)(x/∥x∥)\n\n(65)\n\nTensor Field Networks Tensor field networks (TFNs) (Thomas et al., 2018) are SE(3)- equivariant models for generating representation-theoretic vector fields from a point cloud input. TFNs construct equivariant output feature vectors from equivariant input feature vectors and spherical harmonics. Spatial convolutions and tensor products are used for the equivariance.\n\nConsider a featured point cloud input with M points given by X = {(x1, f1), · · · , (xM , fM )} where xi ∈ R3 is the position and fi is the equivariant feature vector of the i-th point. Let fi be decomposed into N vectors such that fi = (cid:76)N is a type-ln vector, which is (2ln + 1) dimensional. Therefore, we define the action of T = (R, v) ∈ SE(3) on X as\n\n, where f (n)\n\nn=1 f (n)\n\ni\n\ni\n\nT X = {(T x1, D(R)f1), · · · , (T xM , D(R)fM )}\n\nwhere R ∈ SO(3), v ∈ R3 and D(R) = (cid:76)N\n\nn=1 Dln (R).\n\nConsider the following input feature field f(in)(x|X) generated by the point cloud input X as\n\nf(in)(x|X) =\n\nM (cid:88)\n\nj=1\n\nfjδ(3)(x − xj)\n\n(66)\n\nwhere δ(3)(x − y) = (cid:81)3 Note that this input feature field is an SE(3)-equivariant field, that is:\n\nμ=1 δ(xμ − yμ) is the three-dimensional Dirac delta function centered at xj.\n\nf(in)(T x|T X) = D(R)f(in)(x|X) ∀ T = (R, v) ∈ SE(3)\n\nNow consider the following output feature field by a convolution\n\nf(out)(x|X) =\n\nN ′ (cid:77)\n\nf (n′) (out)(x|X)\n\nn′=1 (cid:90)\n\nd3yW(x − y)f(in)(y|X) =\n\n=\n\n(cid:88)\n\nj\n\nW(x − xj)fj\n\n(67)\n\nwith the convolution kernel W(x − y) ∈ Rdim(f(out))×dim(f(in)) whose (n′, n)-th block Wn′n(x − y) ∈ R(2ln′ +1)×(2ln+1) is defined as follows:\n\n(cid:104)\n\nWn′n(x)\n\n(cid:105)\n\nm′m\n\n=\n\nln′ +ln (cid:88)\n\nJ=|ln′ −ln|\n\nφn′n\n\nJ (∥x∥)\n\nJ (cid:88)\n\nk=−J\n\nC (ln′ ,m′)\n\n(J,k)(ln,m)Y (J)\n\nk\n\n(x/∥x∥)\n\n(68)\n\nHere, φn′n Equation (67) is proven to be SE(3)-equivariant (Thomas et al., 2018; Fuchs et al., 2020).\n\nJ (∥x∥) : R → R is some learnable radial function. The output feature field f(out)(x|X) in\n\nSE(3)-Transformers The SE(3)-Transformers (Fuchs et al., 2020) are variants of TFNs with self-attention. Consider the case in which the output field is also a featured sum of Dirac deltas\n\nf(out)(x|X) =\n\nM (cid:88)\n\nj=1\n\nf(out),jδ(3)(x − xj)\n\n(69)\n\nwhere xi is the same point as that of the point cloud input X. The SE(3)-Transformers apply type-0 (scalar) self-attention αij to Equation (67):\n\nf(out),i =\n\n(cid:88)\n\nj̸=i\n\nαijW(x − xj)fj +\n\nN ′ (cid:77)\n\nN (cid:88)\n\nn′\n\nn=1\n\n27\n\nWn′n\n\n(S) f (n)\n\nj\n\n(70)\n\nPublished as a conference paper at ICLR 2023\n\n(S)\n\nterm is called the self-interaction (Thomas et al., 2018). Wn′n\n\nwhere the Wn′n (S) is nonzero only when l′ n = ln. The self-interaction occurs where i = j such that W(xi − xj) = W (0). The selfinteraction term is needed because W is a linear combination of the spherical harmonics, which are not well defined in x = 0. Details about the calculation of the self-attention αij can be found in Fuchs et al. (2020).\n\nH EXPERIMENTAL DETAILS\n\nFor the test environment, we use PyBullet (Coumans & Bai, 2016–2021) simulator for the experiments. We use the Franka Panda manipulator with a custom end-effector. We use IKFast (Diankov, 2010) with Pybullet-Planning (Garrett, 2018) for the inverse kinematics. Three simulated depth cameras are used to observe the point cloud of the scene. Six simulated depth cameras are used to observe the point cloud of the grasp. We downsample the point clouds using a voxel filter. We illustrate the downsampled point clouds in Figure 9. Since motion planning is not in the scope of our work, we assume no collision between the environment and the robot links except for the hand link. We also allow the robot to teleport to reach pre-grasp and pre-place poses to eliminate the unnecessary influence of the motion planners. However, we fully simulate the trajectories of all the task-relevant primitives (e.g., grasping, releasing, lifting).\n\nWe experiment with three tasks, the mug-hanging task and the bowl/bottle pick-and-place task. For the mug hanging task and the bowl pick-and-place task, we demonstrate with a single target object instance in upright poses only. For the bottle pick-and-place task, we use five object instances in upright poses only. For evaluations, we tested in four different unseen setups: (A) Unseen instances, (B) Unseen poses, (C) Unseen distractors, and (D) Unseen poses, instances, and distractors. Note that in the unseen poses setup (B), we only use lying poses, which were not presented during the training. On the other hand, in the unseen poses, instances, and distractors setup (D), we both use lying and upright poses but in unseen elevations. The reason why we also use upright poses in setup (D) is that the baseline model already completely fails in setup (B). Therefore, the result would be trivial if we only use lying poses in (D). Therefore, we mix both upright and lying poses in setup (D). Note that the upright poses are also unseen poses because of the unseen elevations.\n\nFor the inference, we run the MH for 1000 steps and then run the Langevin algorithm for 300 steps. Lastly, we optimize the samples for 100 steps using Equation (46) but without noise. We empirically find that only at most three query points have significant weights after training. Therefore, we only use three query points with the highest weights to save computations. Instead of directly taking the lowest-energy sample pose, we check the feasibility of the pose before going into action. For example, if a collision is found or no inverse kinematics solution can be found for the sample pose, we deny that pose and move to the next best sample. We provide the details in Algorithm 2.\n\nTen demonstrations are generated by a probabilistic oracle for each task, with the default instances in upright poses, with only the x, y, and yaw being randomized. In the unseen poses setup, the default (trained) instances are provided in unseen (lying) poses. The poses are completely randomized, including the elevation z. In the unseen instances setup, ten unseen instances of target objects are provided in the trained poses (upright), again with only the x, y, and yaw being randomized. In the unseen distractors setup, four unseen visual distractors are located near the target objects. We randomize the poses and the colors of these distractors. To separate the effect of motion planning, we disable the collision between the distractors and the robot. Lastly, in the unseen instances, poses, and distractors setup, we combine all the prior setups. We experiment with ten unseen instances in unseen poses (50% upright and 50% lying, arbitrary elevation) with four randomized visual distractors. In this setup, we use supports to give arbitrary elevation to the target objects. Note that we both used upright and lying poses, unlike the unseen poses setup. This is to test the case with unseen distractors and unseen instances for not only the lying poses but the upright poses as well. Note that upright poses are also unseen poses because we give arbitrary elevations. For the mug and bowl tasks, only a single instance was used as the default instance for training. For the bottle task, five instances were used due to the high variance of the shape. All the models are sufficiently trained such that the total success rate in the trained setup (no unseen situations) exceed at least 90% for all three tasks. The experimental settings for the three tasks are illustrated in Figure 10. We also illustrate the ten unseen instances of mug, bowl, and bottle in Figures 11, 12, and 13, respectively.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 Pick-and-place algorithm\n\nInput: Pick-model Ppick(T |X), Place-model Pplace(T |X, Y ), Number of iterations N\n\nX ← observe scene() [Ti]N [Ti]N for T in [Ti]N\n\ni=1 ← sample(Ppick(·|X), N ) i=1 ← sort (cid:0)[Ti]N\n\ni=1, Ppick(·|X)(cid:1)\n\ni=1 do\n\nif feasible(T ) then\n\npick(T ) break\n\nend if\n\n▷ Observe the point cloud of the scene ▷ Sample N poses from the pick-model ▷ Sort samples by their probabilities (descending order)\n\n▷ Pick if the configuration is feasible\n\ni=1 ← sample(Pplace(·|X, Y ), N ) i=1 ← sort (cid:0)[Ti]N\n\n▷ Observe the point cloud of the scene ▷ Observe the point cloud of the gripper ▷ Sample N poses from the place-model i=1, Pplace(·|X, Y )(cid:1) ▷ Sort samples by their probabilities (descending order)\n\nend for X ← observe scene() Y ← observe gripper() [Ti]N [Ti]N for T in [Ti]N\n\ni=1 do\n\nif feasible(T ) then\n\nplace(T ) break\n\nend if\n\nend for\n\n▷ Place if the configuration is feasible\n\nFor EDFs, we use a single query point for the pick inference and three query points for the place inference. For the ablated model, we use ten query points for the pick inference and five query points for the place inference. The reason we use more query points for the ablated model is that the type0 descriptors cannot encode orientations alone. Unlike EDFs, type-0 descriptor fields (the ablated model) require at least three non-collinear query points and much more in practice to determine orientations. This is in direct comparison with EDFs, which can determine the orientations even with a single point. The computational benefit of using only type-0 descriptors is compensated by the increased number of query points. We set the number of query points to make the inference time of the ablated model to be similar to or slightly longer than EDFs. We run all the experiments on an Nvidia RTX3090 GPU and an Intel i9-12900k CPU with 16Gb RAM. We turned off all the E-cores of the CPU and only used P-cores with a fixed clock of 5100Mhz. We found that turning off the E-core is crucial for the inference speed.\n\nThe models were trained for 600 steps (60 epochs) using Adam optimizer (Kingma & Ba, 2014) where the learning rates range from 0.005 to 0.001. We randomly perturb the target pose and apply jitters on input point clouds to augment the training data. It takes around 5.5 hours to train the pick-model and 8.5 hours to train the place-model, where most of the time is spent on MCMC sampling. We run 10000 iterations of the MH and 3000∼6000 iterations (linearly increasing as training proceeds) of the Langevin algorithm to draw negative samples for the training.\n\nFigure 9: A) Downsampled point cloud of the scene. B) Downsampled point cloud of the gripper with a grasped object.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Test environment for the mug-hanging task, the bowl pick-and-place task, and the bottle pick-and-place task with various unseen setups.\n\nFigure 11: The ten mug instances that were used as unseen mug instances are illustrated.\n\nFigure 12: The ten bowl instances that were used as unseen bowl instances are illustrated.\n\nFigure 13: The ten bottle instances that were used as unseen bottle instances are illustrated.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nI ADDITIONAL EXPERIMENTAL RESULTS\n\nTable 3: Success rate of EDFs for mug-hanging task with different demonstrations\n\nSetup Unseen Poses (P) Unseen Instances (I) Unseen Distractors (D) Unseen P+I+D\n\nLow Var. & Unimodal Demo Total Place Pick 0.96 0.96 1.00 0.89 0.90 0.99 1.00 1.00 1.00 0.82 0.83 0.99\n\nMixed Grasp (Handle & Rim) Place 0.99 0.92 0.99 0.89\n\nTotal 0.99 0.92 0.95 0.80\n\nPick 1.00 1.00 0.96 0.90\n\nIn Table 3, we list the success rates of EDFs for (1) low variance and unimodal demonstrations and (2) highly multimodal demonstrations for the mug-hanging task. In the highly multimodal demonstrations, the mug is picked both using the rim grasp and the handle grasp. The experimental results indicate that EDFs are both robust to low variance or high variance demonstrations, whereas SE(3)-TNs can only be trained from low variance ones.\n\nTable 4: Success rate and inference time of the ablated model and EDFs. All the evaluations are done in the unseen instances, poses & distracting objects setting.\n\nMug\n\nBowl\n\nBottle\n\nDescriptor Type (# Pick Query / # Place Query) Pick Place Total Pick Place Total Pick Place Total Type-0 Only (10 / 5) Inference Time Success Rate Type-0 Only (30 / 10) Inference Time Success Rate EDFs (1 / 3) Inference Time Success Rate\n\n10.0s 14.5s 24.5s 13.9s 19.4s 33.4s 10.9s 24.2s 35.1s 0.73 0.99 0.90\n\n17.3s 23.0s 0.63 0.95\n\n10.4s 15.6s 0.95 1.00\n\n11.5s 16.7s 0.95 1.00\n\n14.3s 0.65\n\n16.0s 0.57\n\n13.4s 0.95\n\n9.9s 0.95\n\n5.8s 0.66\n\n5.7s 0.84\n\n8.6s 0.77\n\n6.1s 0.60\n\n8.3s 0.95\n\n5.2s 0.95\n\n5.1s 1.00\n\n5.2s 0.95\n\n0.96\n\n0.90\n\n0.81\n\n0.72\n\n0.76\n\n0.71\n\nIn Table 4, we compare EDFs with the ablated models with only type-0 descriptors. We evaluate the ablated model for different query point numbers. The result shows that EDFs still outperform the ablated model even though much more query points and longer inference time is used.\n\nTable 5: Success rate of experimented methods in the trained setups\n\nMug\n\nMethod\n\nBowl Pick Place Total Pick Place Total Pick Place Total 0.98 1.00 EDFs 1.00 0.92 1.00 SE(3)-TNs (Zeng et al., 2020) 1.00 Type-0 Only (Fast) 0.97 1.00 1.00 Type-0 Only (Slow) 1.00 1.00 1.00\n\n0.98 0.99 1.00 1.00\n\n0.99 0.91 0.98 0.98\n\n1.00 1.00 1.00 1.00\n\n0.99 0.91 0.98 0.98\n\n1.00 0.93 0.97 1.00\n\n1.00 1.00 1.00 1.00\n\nBottle\n\nLastly, in Table 5, we list the success rate of all the methods in the trained setup. Note that all the methods used in the experiments were sufficiently trained to achieve at least 91% total success rate.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nJ EXPERIMENTAL RESULTS ON NEURAL DESCRIPTOR FIELDS WITH\n\nUNSEGMENTED POINT CLOUD INPUTS\n\nIn this section, we show by experiment that object segmentation is critical to the performance of Neural Descriptor Fields (NDFs) (Simeonov et al., 2021). We compare the success rate of NDFs with unsegmented point cloud input to the same NDFs with segmented point cloud input. All the experiments are done using the official implementations of Simeonov et al. (2021). The results are summarized in Table 6. As can be seen in Table 6, the performances significantly drop for both tasks when NDFs are provided with unsegmented inputs. We provide qualitative examples in Figure 14. Therefore, it can be concluded that object segmentation is essential for NDFs.\n\nTable 6: Success rate of NDFs with and without object segmentation\n\nObject Segmentation With Segmentation Without Segmentation Difference\n\nPick 0.94 0.00 0.94\n\nBottle Place 0.86 0.00 0.86\n\nTotal 0.81 0.00 0.81\n\nPick 0.97 0.37 0.60\n\nMug Place 0.72 0.00 0.72\n\nTotal 0.70 0.00 0.70\n\nFigure 14: A) NDFs successfully infer the pick position of a well-segmented mug point cloud. B) NDFs fail to successfully infer the pick position of an unsegmented mug point cloud. C) NDFs successfully infer the pick position of a well-segmented bottle point cloud. D) NDFs fail to successfully infer the pick position of an unsegmented bottle point cloud. The black dots are the query points attached to the gripper.\n\n32",
    "reference": "# Summary Of The Paper\n\nIn this paper, the author explores the direction of how to utilize SE(3)-equivariance as a novel inductive bias to facilitate sample efficiency. Specifically, it focuses on the same robotic manipulation task as previous work, Neural Descriptor Field. The authors derive an energy function based on the representation of the Lie group, which can be used to characterize the distance/discrepancy between SE(3) pose in the demonstration and the current state. By approximating the gradient of the function with sampling, the SE(3)-equivariance demonstration following problems are formulated as an energy minimization problem. Experiments on the object placement task show that the proposed method outperforms the modified-Transporter Network and Invariant Descriptor Fields method.\n\n# Strength And Weaknesses\n\nThis paper is trying to solve a very important problem: how to encode the SE-(3) equivariance property into the network as a powerful inductive bias. It is an important technique for many 3D vision and robotics applications. Although the TransporterNetwork shows that such kind of network design is helpful for manipulation tasks, it still relies on the translational invariance of CNN but does not explore this problem in a more general SE(3) space and does not deal with the hard rotation part. The authors leverage the representation theory of third-order Lie groups to define the bi-equivariant energy using the probability distribution on SE(3). The formulated equivariant descriptor field can then be used in a key-query system, as previous NDF. I feel that the energy-based model in 4.2 makes much sense. But the implementation choice of using Dirac delta functions to shape the query density makes this design not so useful in many applications where continuous parameterization is more preferred.\n\nOne weakness of this paper is the gradient of EBM, which requires a special sampling strategy described in Section 5. It seems that this design makes the overall complexity of the method very high in applications and leads to a very long inference time. As the author discussed in the final section, this method cannot solve problems at the trajectory level.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nFor the paper writing, the relationship between the bi-equivariant EBM and the bi-equivariant energy function is not so clear to me. For novelty, many previous works also explored the same SE(3)-equivariance, and many formulations are similar to previous work, e.g. NDF. The code is attached with well-written documentation, so I believe that it is easy to reproduce.\n\n# Summary Of The Review\n\nI feel that the problem solved in this paper is well-motivated, and the problem formulation looks good to me. However, the implementation choice can be improved to better support the theoretical power of EBM. The difficulty level of the experiments is on-par with previous work in this field. So I recommend a weak acceptance for this paper but the rating may be lower or higher after the rebuttal phase.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "UnderreviewasaconferencepaperatICLR2023RADIALSPIKEANDSLABBAYESIANNEURALNET-WORKSFORSPARSEDATAINRANSOMWAREATTACKSAnonymousauthorsPaperunderdouble-blindreviewRansomwareattacksareincreasingatanalarmingrate,leadingtolargefinanciallosses,unrecov-erableencrypteddata,dataleakage,andprivacyconcerns.Thepromptdetectionofransomwareattacksisrequiredtominimizefurtherdamage,particularlyduringtheencryptionstage.However,thefrequencyandstructureoftheobservedransomwareattackdatamakesthistaskdifficulttoaccomplishinpractice.Thedatacorrespondingtoransomwareattacksrepresentstemporal,high-dimensionalsparsesignals,withlimitedrecordsandveryimbalancedclasses.Whiletraditionaldeeplearningmodelshavebeenabletoachievestate-of-the-artresultsinawidevarietyofdomains,BayesianNeuralNetworks,whichareaclassofprobabilisticmodels,arebettersuitedtotheissuesoftheransomwaredata.ThesemodelscombineideasfromBayesianstatisticswiththerichexpres-sivepowerofneuralnetworks.Inthispaper,weproposetheRadialSpikeandSlabBayesianNeuralNetwork,whichisanewtypeofBayesianNeuralnetworkthatincludesanewformoftheapprox-imateposteriordistribution.Themodelscaleswelltolargearchitecturesandrecoversthesparsestructureoftargetfunctions.Weprovideatheoreticaljustificationforusingthistypeofdistribution,aswellasacomputationallyefficientmethodtoperformvariationalinference.Wedemonstratetheperformanceofourmodelonarealdatasetofransomwareattacksandshowimprovementoveralargenumberofbaselines,includingstate-of-the-artmodelssuchasNeuralODEs(ordinarydif-ferentialequations).Inaddition,weproposetorepresentlow-leveleventsasMITREATT&CKtactics,techniques,andprocedures(TTPs)whichallowsthemodeltobettergeneralizetounseenransomwareattacks.1INTRODUCTIONRansomwareattacksareincreasingrapidlyandcausingsignificantlossestogovernments,corpora-tions,non-governmentalorganizations,andindividuals.Thelossesmayincludefinancialcostsduetoransomspaidtodecryptassets,unrecoverablefileswhentheransomisnotpaidortheattackerfailstoprovidethedecryptionkey,privacyandintellectualpropertytheftwhenassetsareexported,andevensignificantinjurywhenransomwareimpairshealthcaredevicesorpatientrecordsinhospitals.Itisclearthatthetimelydetectionofransomwareincidentsisnecessaryinordertominimizethenumberofassetsthatareencryptedorexfiltrated(Uroojetal.,2021).Toimprovetheransomwareresponse,thisworkproposesanewBayesianNeuralNetworkmodelthatoffersimproveddetectionratesfororganizationswhichemployanalyststoprotecttheirassetsandnetworks.Theproblemisusuallyconsideredasadetectiontask,wherethetwoclassesareransomwareornot.Thetraditionalmethodsofstatisticsandmachinelearninghavebeenproposedtodetectsecuritythreatsingeneralandspecificallyransomwareinsomecases.Fromthestatisticalperspective,acommonapproachistheapplicationofBayesianNetworks(Perusqu ́ıaetal.,2020;Oyenetal.,2016;Shinetal.,2015),whosemaingoalistomodeltherelationshipbetweentheobservedsignalandtheclassoftheattackasagraphicalmodel.Fromthemachinelearningperspective,arangeofmodelswereusedtodetectransomware(Alhawietal.,2018;Poudyaletal.,2018;Zhangetal.,2019;Larsenetal.,2021),suchasNaiveBayes,GradientBoosting,andRandomForests.Bottleneck.Toobtaintherichexpressivepoweroftraditionaldeeplearningmodels,trainingusu-allyrequireshavingaccesstoalargenumberofrecordstosuccessfullyobtainrobustgeneralizedresults.Unfortunately,thefrequencyandstructureofcommonlyobserveddatacorrespondingtoran-somwareattacksmakesthistaskmoredifficulttoaccomplish.Inparticular,ransomwareattackdatacanberepresentedastemporalhigh-dimensionalsparsesignals,withalimitednumberofrecordsandveryimbalancedclasses.Inourdata,thepercentageofransomwareattackstonon-ransomwareattacksis1%versus99%,respectively.1UnderreviewasaconferencepaperatICLR2023Mainideasandcontributions.Toaddresstheseuniquefeaturesoftheransomwaredata,wefirstproposetorepresentransomwaresignalsaccordingtheirMITREATT&CKtactics,techniques,andprocedures(TTPs)whichallowsustogeneralizeransomwareandotherattacksatahigher-levelinsteadofthelow-leveldetectionsassociatedwithanindividualattack.Inaddition,thisallowsforthedetectionofbothhumanoperatedandautomatedransomwareattacksacrossmultiplestagesinthekillchainwithinanorganization’snetwork.Next,weproposeanewprobabilisticmodelwhichiscalledtheRadialSpikeandSlabBayesianNeuralNetwork.ItisaBayesianNeuralNetwork,wheretheapproximateposteriorisrepresentedbyamixtureofdistributions,resultinginaRadialSpikeandSlabdistribution.Ourmodelprovidesthefollowingbenefitsincluding:(1)theSpikeandSlabcomponenthandlesmissingorsparsedata,(2)theRadialcomponentscaleswellwiththegrowthofthenumberofparametersinthedeepneuralnetwork,and(3)theBayesiancomponentpreventsoverfittinginthelimiteddatasetup.Fromthetheoreticalperspective,weprovidethejustificationforusingthistypeofdistribution,aswellasacomputationallyefficientmethodtoperformvariationalinference.Intheresultssection,wedemonstratetheperformanceofourmodelonasetofactualransomwareattacksandshowimprovementoveranumberofbaselines,includingthestate-of-the-arttemporalmodelssuchasRNNs(Choetal.,2014)andNeuralODEs(ordinarydifferentialequations)(Chenetal.,2018).Thus,theproposedmodelisanimportanttoolforthecriticalproblemofransomwaredetection.2INCIDENTDATADESCRIPTIONThisworkutilizesthreatdataprovidedby‘ourindustrypartner’todetectransomwareandothertypesofcybersecurityattacks.Low-leveleventgeneratorsaremanuallycreatedbyanalysts(i.e.,signatures)andareprovidedwithaUUID(UniversallyUniqueIdentifier).Features.Giveneachincident,featuresneedtobeextractedwhichcapturetherangeofattackbe-haviorsobservedacrossthekillchainandrepresentcommonbehaviorsacrossthedifferentfamiliesofransomwareattacks.Thelow-leveleventscannotbeuseddirectlybecausetherearetoomanytotrainourmodel,giventhenumberoflabeledexamples,andtheydonotgeneralizewellindi-vidually.Toovercometheseproblems,wemapasubsetofthelow-leveleventsintoahigher-levelrepresentationusingtheMITREATT&CKframework(MITRE).WechosetheMITREATT&CKframeworkforthemappingbecauseitprovidesaknowledgebaseofadversarytactics,techniques,andprocedures(TTPs)andiswidelyusedacrosstheindustryforclassifyingattackbehaviorsandunderstandingthelifecycleofanattack.UsingtheMITREATT&CKTTPsisanaturalchoiceforfeaturesasitisgeneralizable,interpretable,andeasytoacquireforthisdataaseachlow-leveleventfrom‘theanonymizedcompany’istaggedwiththeMITREtechniqueassociatedwiththealertedbehavior(MITRE).Forexample,oneofthefeaturescanrepresentwhether‘OSCredentialDump-ing’happenedornot.AdditionalMITREATT&CKfeaturesareincludedinTable2,andtheentiresetisprovidedbytheMITREcorporation(MITRE,2022a).Theverbosedefinitionofthesefeaturescanbefoundin(MITRE).Forexample,featureT1059.001“CommandandScriptingInterpreter,Powershell”correspondsto“AdversariesmayabusePowerShellcommandsandscriptsforexecu-tion”(MITRE,2022b).Intotal,ourdataisasparsebinary,high-dimensionalvectorofsize706,whichcontains298MITREATT&CKfeaturesand408additionalsignature-basedfeatures,ateachtimepoint.Oneoftheprimarycharacteristicsofthedataissparsitybecauseonlyveryfewactionsarecompletedateachtimestepduringtheattack.Labels.Usingmanualinvestigation,ananalystprovidesalabelforeachincidentindicatingwhetheritisduetoaransomwareattackoranothertypeofattack.Theransomwareincidentsincludebothhumanoperatedransomware(HumOR)andautomatedransomwareattacksdescribedinAppendixBintheSupplementaryMaterial.However,ourpositiveclasslabelonlyindicatesthatanattackisransomwareanddoesnotdistinguishbetweenthetwoclassesofransomware(i.e.,HumOR,Auto-mated).Ourgoalistobuildanalarm-recommendationsystem,whichcannotonlydetectapossibleransomwareattack,butalsoprovideanestimateoftheuncertaintyaboutthedecision.WeprovideadditionaldetailsaboutthetrainingandtestingdatainSection4.Ethics.Aspartoftheproductiondatacollectionprocess,alldatahasbeenprocessedtoremoveallpersonalidentifiableinformation.ThedatasetswereceivedforthisanalysisonlyincludedarandomlyassignedUUIDfortheorganization,andtheincidentsthatincludedtheMITREevents,2UnderreviewasaconferencepaperatICLR2023signatureUUIDidentifiersandthelabels.ThedatawascollectedandadherestotheGDPRstandard.Therearenonegativesocietalimpactsforcreatingmodelstoprotectusersfromransomware.3METHODOLOGYImportantfeaturesofprobabilisticmodels,suchasprovidinganotionofuncertainty,dealingwithmissingdata,andpreventingoverfittinginalimiteddataregime,havegeneratedastronginterestindeepBayesianlearning.Inthissection,weprovidemoredetailsregardingBayesianNeuralNetworks,includingdifferentaspectsofinitializingandtrainingthemodel.WethenproposetheRadialSpikeandSlabBayesianNeuralNetworkmodeltoaddresstheproblemsoftheransomwaredata.BayesianNeuralNetworks.ThemainideabehindtheBayesianNeuralNetworkistoconsiderallweightsasbeingsamplesfromarandomdistribution.Formally,wedenotetheobserveddataas(x,y),wherexisaninputtothenetwork,andyisacorrespondingresponse.LetallweightsofaBNN,W=(W1,...,WD),bearandomvector,whereDisthedepth(i.e.,numberoflayers)oftheBNNandeachWj=(wj,1,...,wj,lj)isarandomvectoritselfofallweightswj,kperlayerWjofsizelj.Togenerateuncertaintyoftheprediction,weneedtobeabletocomputep(y|x).However,sinceallweightsofaBNNareconsideredtoberandomvariables,wecanrewritetheconditionalprobabilityasp(y|x)=!wp(y,W|x)dW=!Wp(y|W,x)p(W|x)dW.Typically,thelikelihoodtermp(y|W,x)isdefinedbytheproblemsetup,e.g.,ifweconsiderclassification,asinransomwareincidentdetection,y∼Bern(g(W,x))forsomefunctiong.Then,themainproblemoftrainingaBNNistocomputetheposteriorprobabilityp(W|x),giventheobserveddataxandasuitablepriorforW.Insomesimplecasesofsmallneuralnetworks,itmaybepossibletoobtainaclosed-formsolutionfortheposteriorifthepriorandposteriorareconjugatedistributions.Inothercases,ifaclosed-formsolutionisunavailable,sampling-basedstrategiesarerequiredsuchasMarkovChainMonteCarloschemesbasedonGibbsorMetropolisHastingsamplers.Whilesuchanapproachprovidesexcellentstatisticalbehaviorwiththeoreticalsupport,scalabilityasafunctionofthedimensional-ityoftheproblemisknowntobeaseriousissue.ThealternativeformachinelearningandvisionproblemsisVariationalInference(VI)(Graves,2011).ThecoreconceptofVIisbasedonthefactthatapproximatingthetrueposteriorwithanotherdistributionmayoftenbeacceptableinpractice.ThecomputationaladvantagesofVIpermitestimationproceduresincaseswhichwouldnotother-wisebefeasible.VIisnowamaturetechnology,anditssuccesshasledtoanumberoffollow-updevelopmentsfocusedontheoreticalaswellaspracticalaspects(Blundelletal.,2015).WhenusingVIinBayesianNeuralNetworks,weapproximatethetrueunknownposteriordistri-butionP(W|x)withanapproximateposteriordistributionQθofourchoice,whichdependsonlearnedparametersθ.LetWθ=(W1θ,...,WDθ)denotearandomvectorwithdistributionQθandprobabilitydistributionfunction(pdf)qθ.VIseekstofindθsuchthatQθisascloseaspos-sibletothereal(unknown)posteriorP(W|x),andthisisaccomplishedbyminimizingtheKull-back–Leibler(KL)divergencebetweenQθandP(W|x).Givenapriorpdfofweights,p,withalikelihoodtermp(y|W,x),andthecommonmeanfieldassumptionofindependenceforWdandWdθford∈1,...,D,i.e.,p(W)=\"Dd=1pd(Wd)andqθ(Wθ)=\"Dd=1qdθ(Wdθ),θ∗=argminθKL(qθ||p)−Eqθ[lnp(y|W,x)](1)KL(qθ||p)=D#d=1Eqdθ$lnqdθ(w)%−Eqdθ$lnpd(w)%.(2)BydefinitionoftheexpectedvalueEqθ,itisnecessarytocomputethemulti-dimensionalintegralw.r.tw∼Qθtosolveequation1.Ifsuchintegralsareimpossibletocomputeinaclosed-form,anumericalapproximationisused(Ranganathetal.,2014;Paisleyetal.,2012;Milleretal.,2017).Forexample,MonteCarlo(MC)samplingyieldsanasymptoticallyexact,unbiasedestimatorwith3UnderreviewasaconferencepaperatICLR2023varianceO(1M),whereMisthenumberofsamples.Forafunctiong(·):Eqθ[g(w)]=&g(w)qθ(w)dw≈1MM#i=1g(wi),wherewi∼Qθ.(3)Theexpectedvaluetermsinequation1andequation2canbeestimatedbyapplyingthemethodinequation3,andinfact,evenifaclosed-formexpressioncanbecomputed,anMCapproximationmayperformsimilarlygivenenoughsamples(Blundelletal.,2015).Givenamechanismtosolveequation1,themainconsiderationinVIisthechoiceofpriorpandtheapproximateposteriorqθ.AcommonchoiceforpandqθisGaussian,whichallowscalculatingequation2inaclosed-form.However,thistypeofdistributionismainlyusedforcomputationalpur-posesanddoesnotreflectthenatureofthedata.Choosingthecorrectdistribution,especiallytheonewhichcanincorporatethefeaturesoftheanalyzeddata,isanopenproblem(Ghosh&Doshi-Velez,2017;Farquharetal.,2020;McGregoretal.,2019;Krishnanetal.,2019).Inthenextsection,wediscussourproposeddistribution,whichnaturallyfitsthedataencounteredinransomwareincidentdetection.WhilewegivethedescriptionoftheanalyzeddatainSection2,wenextdescribethefeaturesofthedata,whichareimportanttoencapsulateinthemodeldesign.SpikeandSlabdistribution.Thesparsityofthedataisacommonprobleminmanyareas(Kang,2013)andwaspreviouslyapproachedfromdifferentperspectives.Forexampleinthestatisticscom-munity,sparsitycanbeaddressedwithbothStochasticRegressionImputationandLikelihoodBasedApproaches(Lakshminarayanetal.,1999).Inthemachinelearningcommunity,methodsbasedonk-nearestneighbor(Batista&Monard,2003)anditerativetechniques(Buuren&Groothuis-Oudshoorn,2010)havebeendeveloped,includingapproacheswithneuralnetworks(Sharpe&Solly,1995; ́Smiejaetal.,2018).Anotherwaytotacklesparsitycomesfromregularizationthe-oryviaL1regularization,e.g.,groupLASSO(Meieretal.,2008),sparsegroupLASSO(Simonetal.,2013)andgraphLASSO(Jacobetal.,2009).However,weareinterestedinaprobabilisticapproachtoaddressthesparsityinourdata.Fromtheprobabilisticperspective,acommonwaytoaccountforsparsityofthedatainthemodelistoconsideranappropriatedistribution.Forexample,thedistributioncanbetheHorseshoedis-tribution(Carvalhoetal.,2009)orderivativesoftheLaplacedistribution(Babacanetal.,2009;Bhattacharyaetal.,2015).Namely,inourcase,wewouldliketomodelsparsedatawithasparseprobabilisticBayesianneuralnetwork.Sinceonlyaportionoftheinputvariablesarerelevanttotheresponsevariable,wewanttheweightstoberepresentedason/offswitchestounderstandwhetherweshouldaccountfortheinputvariables.SuchasparseBayesianneuralnetworkcanberepre-sentedbya‘sparse‘distributiononitsweights,e.g.,themixtureofpriorswithSpikeandSlabcomponentswhichhavebeenwidelyusedforBayesianvariableselection(Mitchell&Beauchamp,1988;George&McCulloch,1997).Ingeneral,theformoftheSpikeandSlabdistributionforran-domvariablewcanbewrittenas:w∼(1−π)δξ+πg,whereπdeterminestheprobabilityforeachmixturecomponent,δisspikecomponent,whichismodeledwithaDiracdeltafunctionsuchthatδ(w)=’+∞,w=ξ0,w∕=ξand!∞−∞δ(w)dw=1,andgistheslabcomponent,whichisageneraldistributionofthepractioner’schoice.Thegeneralideaistoexplicitlyintroducethesparsitycom-ponentinthedistributionofthedata,allowingtheprobabilitymasstofullyconcentrateonξ=0withprobability1−π,andwithprobabilityπspreadtheremainingmassoverthedomainoftheslabcomponentg.Notice,thatπcanbeconsideredasarandomvariableitself,e.g.,π∼Bern(λ),whereλiseitheralearnedparameterorafixedvaluethatisprovidedbyaspecialist.Thenextquestionstoconsiderinclude:(1)howcanthe‘SpikeandSlab’distributionbeappliedinaBNN,and(2)whichslabcomponentgshouldweconsider?SpikeandSlabBNN.IntheBNN,alloftheneuralnetwork’sweightsWareconsideredtoberandomvariables,andtouseVItosolveequation1foreachlayer’ssetofweightsWjinW=(W1,...,WD),itisnecessarytoprovidethepriorpjandtheapproximateposteriorqjθ.Withoutlossofgenerality,weconsiderasingleweightw:=wj,k,droppingtheindicesjandk,andonlyworkwiththepriorpandtheapproximateposteriorqfortheremainderofthissection.4UnderreviewasaconferencepaperatICLR2023IncorporatingaSpikeandSlabdistributiononboththepriorpandtheapproximateposteriorq,sampleswpfrompandwqfromqhavethefollowingdistribution:wp|πp∼(1−πp)δ0+πpgpandwq|πq∼(1−πq)δ0+πqgq,(4)whereπp∼Bern(λp),πq∼Bern(λq),andgp,gqaredistributionsofourchoice.Aswediscussedpreviously,themaingoalofVIistolearnparametersθofanapproximateposteriorqθ,byminimizingequation2.Inthecaseofequation4,θ=(λq,θq),whereλqistheprobabilityoftheBernoullidistributionassociatedwithπq,andθqaretheparametersoftheSlabcomponentgq.First,westateTheorem3.1,whichallowsustocomputetheKLtermbetweentwogeneralSpikeandSlabdistributions.Theorem3.1.GiventwogeneralSpikeandSlabdistributionssuchthat:p(w|πp)=(1−πp)δ0(w)+πpgp(w),q(w|πq)=(1−πq)δ0(w)+πqgq(w),πp∼p(π)=Bern(λp),andπq∼q(π)=Bern(λq),withδ0beingadiracdeltafunctionat0andgp,gqarethepdfsofthedistributionsofourchoice,theKL(q(w,π)’p(w,π))isequalto:KL(Bern(λq)’Bern(λp))+λqKL(gq’gp).(5)TheproofisshowninAppendixF.Choiceofgqandgp:Radialdistribution.Sofar,wehaveshownresultsforageneralSpikeandSlabdistribution.Theimportantquestioniswhichslabcomponentsgshouldweconsiderforourapproach,andifgqandgpshouldbefromthesamefamily?Authorsin(Baietal.,2020)consid-eredbothgqandgptobetheGaussiandistribution.However,thereisemergingevidence(Farquharetal.,2020;Fortuinetal.,2020)thattheGaussianassumptionresultsinpoorperformanceofthemediumtolarge-scaleBayesianNeuralNetworks.Authorsregardthisasbeingcausedbytheprob-abilitymassinahigh-dimensionalGaussiandistributionconcentratinginanarrow“soap-bubble”farfromthemean.Forthisreason,(Farquharetal.,2020)proposedusingaRadialdistributionwithparameters(μ,σ),wheresamplescanbegeneratedas:μ+σ∗ξ||ξ||∗|r|∼Radial(μ,σ),whereξ∼MVN(0,I),r∼N(0,1).(6)Following(Farquharetal.,2020),wesetupourapproximateposteriorgqtobetheRadialdistri-bution(μ,σ),whilethepriorgpisNormal(0,1).Givenequation5,itisnecessarytodefinetheKL(gq’gp)term.Unfortunately,aclosed-formsolutionforourchoiceofgqandgpisnotavailable,andweapproximatetheKLtermusingMonteCarlosamplingfromequation3withMsamples.Thisprocessleadsto(uptoaconstant):KL(gq’gp)≈−logσ−1M(Mi=1log[p(wi)],wherewiissampledfromtheRadialdistribution(μ,σ)asdescribedinequation6andpistheLikelihoodofN(0,1).NotethatrunninganMCapproximationforlargeMcanleadtorunningoutofmemoryineitheraGPUorRAM,(Nazarovsetal.,2021).Totacklethisissue,wefollow(Nazarovsetal.,2021)andapplyagraphparameterizationforourRadialSpikeandSlabdistribution,allowingustosetM=1000withoutexhaustingthememory.Reparameterizationtrick:Gumbel-Softmax.GivenTheorem3.1,wecanrewriteequation1as:θ∗=argminθ=(λq,θq)KL(Bern(λq)’Bern(λp))+λqKL(gq’gp)−Eqθ[lnp(y|W,x)].(7)Recall,wecancomputetheKL(Bern(λq)’Bern(λp))inaclosed-form(insidetheproofofThe-orem3.1)andapproximatetheKL(gq’gp)termwithMCsampling.Next,therearetwomainaspectsleftforourattention:(1)computingEqθ[lnp(y|W,x)],whichisusuallyapproximatedwithMonte-Carlosampling(Kingma&Welling,2013)becauseoftheintractabilityissue,and(2)howtodoback-propagationforoptimization.Theproblemwithback-propagationinthissettingisthatsamplingdirectlyfrom,e.g.,w∼N(μ,σ)withlearnableparametersμandσ,doesnotallowustoback-propagatethroughthoseparameters,andthus,theycannotbelearned.Thisissueisaddressedbyapplyingalocal-reparameterizationtrick(Kingmaetal.,2015).Forexample,insteadofsam-plingfromw∼N(μ,σ)directly,wesamplez∼N(0,1)andcompute:w=μ+σz.Thisallowsback-propagationtooptimizethelossw.r.t.μandσ.5UnderreviewasaconferencepaperatICLR2023Whilethelocal-reparameterizationtrickisobviousformembersofalocation-scalefamily,liketheGaussiandistribution,andevenfortheselectedRadialdistribution,itisnotclearhowtoapplythistricktotheBernoullidistribution,Bern(λ).OnewaytoaddressthisissueistoapproximatesamplesfromtheBernoullidistributionwiththeGumbel-Softmax(Maddisonetal.,2016;Jangetal.,2016;Baietal.,2020).Thatis,π∼Bern(λ)isapproximatedby)π∼Gumbel-Softmax(λ,τ),where)π=(1+exp(−η/τ))−1,η=logλ1−λ+logu1−u,andu∼U(0,1).Here,τistheparameterwhichisreferredasthetemperature.Whenτapproaches0, ̃πconvergesindistributiontoπ.However,inpractice,τisusuallychosennosmallerthan0.5fornumericalstability(Baietal.,2020).ApplyingtheGumbel-Softmaxapproximationinsteadofoptimizingthelossforparameterλq,weconsideranewparameterθπ=logλq1−λq.Thus,λq=S(θπ)=11+e−θπ,resultinginthefinallearnedparameters:θ=(θπ,θq).FinalLossandMethodSummary.Astep-by-stepsummaryofthemethodinprovidedinAlgo-rithm1.ThefinallossisgiveninAlgorithm2.Algorithm1:LearningtheposteriordistributionofaBNNp(W|x)withaRadialSpikeandSlabapproximateposterior,toaccountforsparsityofthedata.Input:1:NeuralNetworkofdepthDwith2:WeightsWθ=(W1θ,...,WDθ),whichhave3:SpikeandSlabRadialdistributionQθwithpdfqθ,s.t.•q(w|πq)=(1−πq)δ0(w)+πqgq(w;μ,σ),•gq(w;μ,σ)ispdfofRadial(μ,σ)•πq∼Bern(S(θπ)),whereSisthesoftmax,and4:PriorSpikeandSlabdistributionPθwithpdfp,s.t.•p(w|πp)=(1−πp)δ0(w)+πpgp(w;μp,σp),•gp(w;μp,σp)ispdfofGaussiandistribution•πp∼Bern(πp)Output:Learnedparametersθ=(θπ,μ,σ)Require:Priordistribution’sparameters(πp,μp,σp)5:whileθhasnotconvergeddo6:MinimizeVIlossinequation8,byusinggradientdescentalgorithms(e.g.,SGDorAdam)anddoing:7:Forwardpass:tocompute•ywithlocalreparameterizationtrickforbothRadialandBernoulli(usingGumbel-Softmax)•KLtermsandexpectedlog-likelihoodterm,usingcombinationofclosed-formandMC8:Backwardpass:computegradientsofθ9:endwhileAlgorithm2:FinallossusedforoptimizationinAlgorithm1.Original:KL(Bern(λq)#Bern(λp))+λqKL(gq#gp)−EQθ[lnp(y|W,x)]Final:L=!j=1,...,D,k=1,...,ljKLj,k−EQθ[lnp(y|W,x)],where(8)KLj,k=(1−S(θj,kπ))log1−S(θj,kπ)1−λj,kp+S(θj,kπ)logS(θj,kπ)λj,kp+S(θj,kπ)\"−logσj,k−1M#Mi=1log[p(wj,ki)]$NotethatbasedonthemeanfieldassumptionofaBNN,thefinallossLincludesthesumoverallKLj,kterms,whicharecomputedforeachk-thweightwj,kofthej-thlayeroftheBNNwithparametersθj,k=(θj,kπ,μj,k,σj,k).Inthiscase,thefinalsetoftrainableparametersisθ={θj,k}forj=1,...,Dandk=1,...,lj.Inaddition,EQθcanbecomputedeitherinaclosed-formorapproximatedbyMC,dependingonthecomplexityoftheBNN.6UnderreviewasaconferencepaperatICLR20234EXPERIMENTSDatadescription.AsdescribedpreviouslyinSection2,eachincidentisrepresentedbyatemporalsequenceofeventsfromaknowledgebaseofTTPswithanassignedlabel,whichindicateswhetheritisransomwareoranothertypeofattack.First,thecompanyprovided201incidentslabeledasRansomwareand24,913withNon-Ransomwarelabelsfortheinitialdataset.Allofthesamplesinthisdatasetwerededuplicatedandincluded706sparsebinaryfeatures.Thisfirstdatasetwasrandomlysplitwith80%oftheexamplesassignedtothetrainingset,whiletheremainderwereusedtocreateavalidationset.Second,forthetestset,wereceivedanewer,deduplicateddatasetmakingitindependentofthetrainingandvalidationsets.Thisdatasetincluded644Ransomwareincidentsand14,696Non-Ransomwareincidents.Preprocessingoftemporalinformation.SomeofthemodelssuchastheNeuralODEbenefitfromknowledgeoftheactualtimeassociatedwiththerecordedevent,whileothers,includingtheRNNwithaGRUcell,canbetrainedontheeventsequencebasedsolelyontheeventindex(i.e.,t=1,2,...).Finally,othermodelssuchasthefullyconnectedandBayesianNeuralNetworkscanbetrainedandtestedusingtheaggregationofalloftheeventsintheeventsequence.Toreducethenumberoftimestepsforthetime-basedmodelsforourstudy,weaggregatedallTTPeventsobservedwithinaoneminutewindow.Wesettheaggregationtimetooneminuteafterdoinghyperparametertuningonthisvalue.Thisresultsinveryfewsignalsbeingrecordedperaggregatedtimestep,whichisrepresentedinFigure1inAppendixD.Weseethatthemajorityofthedatahaveasmallnumberoffeaturesthatareset,namelylessthan10outof706possible.Fortheneuralnetworkmodels,weaggregatedalloftheTTPfeaturesintoasingleinputvector.Allofthesequencesforthetrainingandtestingdatasetsweretruncatedafteronehourfromthetimeofthefirstevent.Models.Intheexperiments,weconsiderseveralbaselinemodelsfromthetraditional,temporal,andprobabilisticdeeplearningsettings,inadditiontoourproposedmodel.Fromthetemporalperspec-tive,weconsidertwomodelsincludingtheRecurrentNeuralNetworkwithaGRUcell(RNN)andtheNeuralODE(NODE).Aswementionedearlier,thetraditionalrecurrentneuralnetworkmodels(e.g.,SimpleRNN,GRU,LSTM)ignorethevalueofthetimestepsandonlyconsidertheorder(i.e.,index),incontrasttotheNeuralODEwhichaccountsforthetimestepvalue.Note,weoriginallyconsideredseveraltemporalmodels,whichdonotaccountforthetimevalue,likethetraditional(i.e.,Simple)RNN,theRNNwithaGRUcell,theLSTM,andtheBi-directionalLSTM.However,amongallofthesemodels,theRNNwiththeGRUcellperformedthebest,andweonlyincludethismodelintheanalysisbelow.Inadditionweconsiderthetraditionalfullyconnectedneuralnetwork(FC),andfourBNNmodels.ThefirsttwoBNNsarethestandardBNNswhichhaveaGaussianorRadialapproximateposterior(BNN:Gaus,BNN:Radial),andtheothertwoarethecorrespondingSpikeandSlabversions,BNN:Spike-SlabGaussianandourproposedBNN:Spike-SlabRadial.Forthesenetworks,weignorethetemporalaspectofthedatabyaggregatingallavailablefeaturesperentrywiththe‘logicalor’operator.Sinceourfeaturesarebinary,aggregationcorrespondstosummarizingtheinformationintothesetofeventswhichoccurredduringthetimeperiod.Inaddition,wealsoconsideredanapproachwithaBayesianNetwork(i.e.,notaBNN).However,theBNmodelfailedtoconvergeduetothesparsityandhighdimensionalityofthedata.Furthermore,wealsotrainedmanyvariantsofXGBoost(Chen&Guestrin,2016),butalloftheboosteddecisiontreemodelsproducedrandomresults.Therefore,wedidnotincludetheresultsforXGBoostbelow.Parametersettings/hardware.AllexperimentswererunonanNVIDIAP100.ThecodewasimplementedinPyTorch,usingtheAdamoptimizer(Kingma&Ba,2014)forallmodels,andtrainedfor400epochs.Themodelwiththelowestvalidationlosswasselectedforevaluation.ThefinalhyperparametersettingsarespecifiedinAppendixA.Ablationstudy.TounderstandtheeffectofthedistributionontheBNN,weconductanablationstudybetweentheGaussianBNN,RadialBNN,andtheirSpikeandSlabversions.WeprovideresultsinTable1.Clearlyourproposedmethodprovidesbetterresultsinanumberofmetrics,includingSpecificity,Precision,F1,andFPR,whichareimportantforRansomwaredetection.ModelEvaluation.InFigure1,weprovidetheROCcurvesfortheproposedmodelandseveralbaselines.ForourRadialSpikeandSlabBNNmethodandtheGaussianBNNmethod,wedisplaythedistributionofeachmodel’sROCcurves,shadedingreen,togetherwithitsmeanvalue(e.g.,7UnderreviewasaconferencepaperatICLR2023ValidationSetTestSet:FutureTimePeriodStatisticsRNN-GRUNeuralODEFCBNN:GaussianBNN:RadialBNN:GaussianSpike&SlabBNN:RadialSpike&SlabRNN-GRUNeuralODEFCBNN:GaussianBNN:RadialBNN:GaussianSpike&SlabBNN:RadialSpike&SlabAUC0.850.830.830.830.880.870.870.700.730.770.750.810.790.77Specificity0.900.800.880.890.910.900.930.900.790.820.890.910.900.92Precision0.060.030.050.050.060.060.080.090.060.060.100.120.120.13FPR0.100.200.120.110.090.100.070.100.210.180.110.090.100.08FNR0.250.230.270.270.250.230.270.530.380.420.420.410.400.46FDR0.940.970.950.950.940.940.920.910.940.940.900.880.880.87Accuracy0.900.800.870.890.910.900.930.890.790.810.880.900.890.91BalancedAccuracy0.820.780.800.810.830.830.830.680.710.700.740.750.750.73F10.110.060.080.100.120.110.140.150.110.120.180.200.190.21G-Mean0.820.780.800.810.830.830.830.650.700.690.720.730.730.70Table1:Ablationstudyforbothvalidationsetandthetestset,whichcontainsdatafromthefuture.greenline).Figure1ashowsthat,withrespecttothedistributionoftheROCcurves,ourmodelout-performsthebaselinesonaverage,particularlyintheregionofsmallfalsepositiverates,Figure1b,whichisthemostimportantforransomwaredetection.Inaddition,theRadialSpikeandSlabBNNisabletoprovidearangeofROCcurveswhicharesignificantlyhigherthantheotherbaselines,ifweconsiderthemarginsoftheROCdistribution.LookingatthecolumnsforthevalidationandtestingsetinTable1,weseethatproposedBNNmodeloutperformsthebaselinemethods,w.r.t.toAUC,accuracy,G-Mean,andotherstatistics.(a)TestSet(b)TestSet-ZoomedInFigure1:WepresenttheROCcurvesforthenewdatafromthefuturetimeperiodintheTestSet.BecausetheBNNisaprobabilisticmodel,weshowthedistributionoftheindividualROCcurves(greenshade)withthemeanofthisdistribution(greenline).TrainingandTestTimes.TrainingtheRadialSpikeandSlabBayesianNeuralNetworkinasingleAzure-hostedLinuxVMwithanNVIDIAP100for400epochsrequired1hour,32minutesand53seconds.Thetimerequiredtoevaluatethe15,340samplesinthetestsetwas19seconds.How-ever,tocreateaconfidenceinterval(CI),theevaluationisrepeated100times.Thus,19secondscorrespondsto100evaluations.Were-raninferenceonanNVIDIAA100forallofthemodelstocompareto1runofdeterministicmodels.Theresultsinclude4.07sec(1iteration)and10.37sec(100iterationsforCI)fortheBNN:RadialSpike&Slab(ours),4.02secfortheBNN:Gaussian,7.72secforFullyConnectedNN(FC),3.12secfortheNeuralODE,and2.35secfortheRNN-GPU.FeatureImportanceandInterpretation.WewouldliketounderstandwhichTTPfeaturesoftheattackareconsideredtobeimportantbyourmodelwhenmakingapredictionwhetheranattackisransomwareornot.OnewaytodothisistoinvestigatetheposteriorprobabilitiesforthefirstlayerweightsoftheBNN.However,whileunderstandingtheimportanceoftheTTPfeaturesbasedontheBNN’strainedweightsconceptuallymakessense,weinsteadfollowamorewell-knownandestablishedwaytointerpretthefeaturesofageneralneuralnetwork,calledIntegratedGradients(Sundararajanetal.,2017).BothmethodsarediscussedinAppendixE.InTable2,wepresentthesubsetoffeatureswhicharethemostimportantforourmodeltoidentifywhetheranattackisransomwareorsomeothertypeofattackbasedonIntegratedGradients.SortingthevaluesofIntegratedGradients,wefindthattheMITREATT&CK8UnderreviewasaconferencepaperatICLR2023featuresaresignificantlymoreimportantthanthesignature-basedfeatures.The“signature”inTa-ble2isalow-leveleventgeneratorfromananalyst.AsTable2shows,theMITREeventsaremuchmoreimportantthanthelow-levelsignatures.5RELATEDWORKIdFeaturerepresentationT1059.001CommandandScriptingInterpreter,PowershellT1105IngressToolTransferT1087AccountDiscoverySignatureSuspiciousactivitywasobservedonthisdeviceT1049SystemNetworkConnectionsDiscoveryT1027.002ObfuscatedFilesorInformation:SoftwarePackingT1566.001Phishing:SpearphishingAttachmentT1546.001EventTriggeredExecution:ChangeDefaultFileAssociationT1218.003SignedBinaryProxyExecution:CMSTPT1055.004ProcessInjection:AsynchronousProcedureCallTable2:TheIntegratedGradientsmethodproducesascoreforeachoftheTTPfeatureswhichindicatestheimportanceofthefeatureforpredictingwhethertheattackisransomware(top)oranothertype(bottom).FeaturesarerankedfromthehighesttolowestIntegratedGradientsscores.Recently,ransomwarehasbecomeanactiveresearcharea(Ozetal.,2022;McIn-toshetal.,2021).Machinelearningapproacheshavebeenproposedforthede-tectionofransomwareat-tacks.Astacked,varia-tionalautoencoderisusedtodetectransomwareintheindustrialIoT(IIoT)set-ting(Al-Hawawreh&Sit-nikova,2019).SystemAPIcallsareusedtode-tectransomwareusingDe-cisionTrees,aK-NearestNeighborclassifier,andaRandomForestin(Sheen&Yadav,2018).Takeuchietal.(Takeuchietal.,2018)alsoproposedusinganSVMtodetectransomwareusingSystemAPIcalls.Agrawaletal.(Agrawaletal.,2019)proposedanewattentionmechanismontheinputvectorofanLSTM,anRNNandaGRUtoimprovethedetectionofransomwareattacksfromAPIcalls.AnensembleofnetworktrafficclassifiersareusedtodetectnetworkpacketsandflowsfortheLockyfamilyofransomwarein(Almashhadanietal.,2019).ABayesianNetworkwasthebestperformingflow-basedclassifierinthisworkwhileaRandomTreewasthebestfordetectingpacketsinthiswork.HelDroid(Andronioetal.,2015)usesnaturallanguageprocessingtechniques,alongwithstaticanddynamicanalysis,todetectransomwareonmobilecomputingdevices.AdamovandCarlsson(Adamov&Carlsson,2020)usereinforcementlearningtosimulateransomwareattacksfortestingransomwaredetectors.Uroojetal.(Uroojetal.,2021)proposedanonlineclassifiertopredictearlystageransomware,buttheydonotprovideanydetailsfortheclassifieritself.6LIMITATIONSANDCONCLUSIONInthiswork,weproposethenewRadialSpikeandSlabBayesianNeuralNetworkanddemonstratethatitoutperformsthestandardBayesianNeuralNetworkandotherdeeplearningmethodsforthetaskofdetectingransomwareattackswithinthegeneralclassofallattacks,suchasthedroppingofcommoditymalware.Theresultscanprovideanearlyindicatorofapotentialransomwareattackforanalyststobeabletoconfirmwithadditionalinvestigation.Whilethemodelisabletolearntodistinguishbetweenransomwareattacksandotherattacks,theROCcurveindicatesthatitcannotbeusedbyafullyautomatedsystemtocompletelydisablecom-putersorblocknetworkaccessduetoapotentialransomwareoutbreak.However,sincetheseattacksarebeingdiagnosedbyanalysts,webelievethatthemodelcanalerttheseanalystsaboutpossibleactiveransomwareattackontheirnetwork.Giventhatransomwareattacksarerelativelyrarecomparedtothedownloadingofcommoditymal-ware,theamountoflabeleddataforthesetypesofattacksissmall.Thesizeofourdatasetsfromaproductionsecurityservicereflectsthislimitation.Fortunately,BayesiancomputationalmethodssuchasBayesianNeuralNetworkscanbeusedfortrainingandinferencewithoutoverfittinginscenarioswheretheamountoflabeleddataislimited.Inadditiontosecurityproblems,twosignificantareas,whichfocusontheanalysisofhigh-dimensionalsparsedata,areBiostatisticsandGenetics.Whiletherearenotabledevelopmentsinsparsemethodsinthoseareas,ourproposedmethodisnovelforthemaswell.Oncethesourcecodeispublic,wehopethatresearchersfromotherfieldswillfindtheproposedmodeluseful.9UnderreviewasaconferencepaperatICLR2023REFERENCESAlexanderAdamovandAndersCarlsson.Reinforcementlearningforanti-ransomwaretesting.In2020IEEEEast-WestDesignTestSymposium(EWDTS),pp.1–5,2020.doi:10.1109/EWDTS50664.2020.9225141.RakshitAgrawal,JackW.Stokes,KarthikSelvaraj,andMadyMarinescu.Attentioninrecur-rentneuralnetworksforransomwaredetection.InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.3222–3226,2019.doi:10.1109/ICASSP.2019.8682899.MunaAl-HawawrehandElenaSitnikova.Industrialinternetofthingsbasedransomwaredetectionusingstackedvariationalneuralnetwork.InProceedingsofthe3rdInternationalConferenceonBigDataandInternetofThings,BDIOT2019,pp.126–130,NewYork,NY,USA,2019.AssociationforComputingMachinery.ISBN9781450372466.doi:10.1145/3361758.3361763.URLhttps://doi.org/10.1145/3361758.3361763.OmarMKAlhawi,JamesBaldwin,andAliDehghantanha.Leveragingmachinelearningtechniquesforwindowsransomwarenetworktrafficdetection.InCyberthreatintelligence,pp.93–106.Springer,2018.AhmadO.Almashhadani,MustafaKaiiali,SakirSezer,andPhilipO’Kane.Amulti-classifiernetwork-basedcryptoransomwaredetectionsystem:Acasestudyoflockyransomware.IEEEAccess,7:47053–47067,2019.doi:10.1109/ACCESS.2019.2907485.Nicol ́oAndronio,StefanoZanero,andFedericoMaggi.HelDroid:DissectingandDetectingMobileRansomware.InProceedingsofthe18thinternationalconferenceonResearchinAttacks,Intru-sions,andDefenses,LectureNotesinComputerScience,pp.382–404.SpringerInternationalPublishing,November2015.ISBN978-3-319-26361-8978-3-319-26362-5.doi:10.1007/978-3-319-26362-518.URLhttps://doi.org/10.1007/978-3-319-26362-518.SDerinBabacan,RafaelMolina,andAggelosKKatsaggelos.Bayesiancompressivesensingusinglaplacepriors.IEEETransactionsonimageprocessing,19(1):53–63,2009.JinchengBai,QifanSong,andGuangCheng.Efficientvariationalinferenceforsparsedeeplearningwiththeoreticalguarantee.arXivpreprintarXiv:2011.07439,2020.GustavoBatistaandMariaCarolinaMonard.Astudyofk-nearestneighbourasanimputationmethod.InInHIS.Citeseer,2003.AnirbanBhattacharya,DebdeepPati,NateshSPillai,andDavidBDunson.Dirichlet–laplacepriorsforoptimalshrinkage.JournaloftheAmericanStatisticalAssociation,110(512):1479–1490,2015.CharlesBlundell,JulienCornebise,KorayKavukcuoglu,andDaanWierstra.Weightuncertaintyinneuralnetworks.arXivpreprintarXiv:1505.05424,2015.SvanBuurenandKarinGroothuis-Oudshoorn.mice:Multivariateimputationbychainedequationsinr.Journalofstatisticalsoftware,pp.1–68,2010.CarlosMCarvalho,NicholasGPolson,andJamesGScott.Handlingsparsityviathehorseshoe.InArtificialIntelligenceandStatistics,pp.73–80.PMLR,2009.RickyTQChen,YuliaRubanova,JesseBettencourt,andDavidKDuvenaud.Neuralordinarydifferentialequations.Advancesinneuralinformationprocessingsystems,31:6571–6583,2018.TianqiChenandCarlosGuestrin.Xgboost:Ascalabletreeboostingsystem.InProceedingsofthe22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,KDD’16,pp.785–794,NewYork,NY,USA,2016.AssociationforComputingMachinery.ISBN9781450342322.doi:10.1145/2939672.2939785.URLhttps://doi.org/10.1145/2939672.2939785.KyunghyunCho,BartVanMerri ̈enboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Hol-gerSchwenk,andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation.arXivpreprintarXiv:1406.1078,2014.10UnderreviewasaconferencepaperatICLR2023SebastianFarquhar,MichaelAOsborne,andYarinGal.Radialbayesianneuralnetworks:Beyonddiscretesupportinlarge-scalebayesiandeeplearning.stat,1050:7,2020.VincentFortuin,Adri`aGarriga-Alonso,FlorianWenzel,GunnarRatsch,RichardETurner,MarkvanderWilk,andLaurenceAitchison.Bayesianneuralnetworkpriorsrevisited.In”ICan’tBelieveIt’sNotBetter!”NeurIPS2020workshop,2020.EdwardIGeorgeandRobertEMcCulloch.Approachesforbayesianvariableselection.Statisticasinica,pp.339–373,1997.SoumyaGhoshandFinaleDoshi-Velez.Modelselectioninbayesianneuralnetworksviahorseshoepriors.arXivpreprintarXiv:1705.10388,2017.AlexGraves.Practicalvariationalinferenceforneuralnetworks.InAdvancesinneuralinformationprocessingsystems,pp.2348–2356,2011.LaurentJacob,GuillaumeObozinski,andJean-PhilippeVert.Grouplassowithoverlapandgraphlasso.InProceedingsofthe26thannualinternationalconferenceonmachinelearning,pp.433–440,2009.EricJang,ShixiangGu,andBenPoole.Categoricalreparameterizationwithgumbel-softmax.arXivpreprintarXiv:1611.01144,2016.HyunKang.Thepreventionandhandlingofthemissingdata.Koreanjournalofanesthesiology,64(5):402,2013.DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.DiederikPKingmaandMaxWelling.Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114,2013.DurkPKingma,TimSalimans,andMaxWelling.Variationaldropoutandthelocalreparameteri-zationtrick.InAdvancesinNeuralInformationProcessingSystems,pp.2575–2583,2015.RanganathKrishnan,MaheshSubedar,andOmeshTickoo.Efficientpriorsforscalablevariationalinferenceinbayesiandeepneuralnetworks.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops,pp.0–0,2019.KamakshiLakshminarayan,StevenAHarp,andTariqSamad.Imputationofmissingdatainindus-trialdatabases.Appliedintelligence,11(3):259–275,1999.ErikLarsen,DavidNoever,andKoreyMacVittie.Asurveyofmachinelearningalgorithmsfordetectingransomwareencryptionactivity.arXivpreprintarXiv:2110.07636,2021.ChrisJMaddison,AndriyMnih,andYeeWhyeTeh.Theconcretedistribution:Acontinuousrelaxationofdiscreterandomvariables.arXivpreprintarXiv:1611.00712,2016.FelixMcGregor,ArnuPretorius,JohanduPreez,andSteveKroon.Stabilisingpriorsforrobustbayesiandeeplearning.arXivpreprintarXiv:1910.10386,2019.TimothyMcIntosh,A.S.M.Kayes,Yi-PingPhoebeChen,AlexNg,andPaulWatters.Ransomwaremitigationinthemodernera:Acomprehensivereview,researchchallenges,andfuturedirections.ACMComput.Surv.,54(9),oct2021.ISSN0360-0300.doi:10.1145/3479393.URLhttps://doi.org/10.1145/3479393.LukasMeier,SaraVanDeGeer,andPeterB ̈uhlmann.Thegrouplassoforlogisticregression.JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology),70(1):53–71,2008.AndrewMiller,NickFoti,AlexanderD’Amour,andRyanPAdams.Reducingreparameterizationgradientvariance.InAdvancesinNeuralInformationProcessingSystems,pp.3708–3718,2017.TobyJMitchellandJohnJBeauchamp.Bayesianvariableselectioninlinearregression.Journaloftheamericanstatisticalassociation,83(404):1023–1032,1988.11UnderreviewasaconferencepaperatICLR2023MITRE.EnterpriseTechniques.https://attack.mitre.org/techniques/enterprise/.[Online;accessed20-January-2022].MITRE.Mitreatt&ck.https://attack.mitre.org/,2022a.MITRE.Commandandscriptinginterpreter:Powershell.https://attack.mitre.org/techniques/T1059/001/,2022b.JurijsNazarovs,RonakRMehta,VishnuSureshLokhande,andVikasSingh.Graphreparameteriza-tionsforenabling1000+montecarloiterationsinbayesiandeepneuralnetworks.InUncertaintyinArtificialIntelligence,pp.118–128.PMLR,2021.DianeOyen,BlakeAnderson,andChristineAnderson-Cook.Bayesiannetworkswithpriorknowl-edgeformalwarephylogenetics.InWorkshopsattheThirtiethAAAIConferenceonArtificialIntelligence,2016.HarunOz,AhmetAris,AlbertLevi,andA.SelcukUluagac.Asurveyonransomware:Evolution,taxonomy,anddefensesolutions.ACMComput.Surv.,jan2022.ISSN0360-0300.doi:10.1145/3514229.URLhttps://doi.org/10.1145/3514229.JustAccepted.JohnPaisley,DavidBlei,andMichaelJordan.Variationalbayesianinferencewithstochasticsearch.arXivpreprintarXiv:1206.6430,2012.Jos ́eAPerusqu ́ıa,JimEGriffin,andCristianoVilla.Bayesianmodelsappliedtocybersecurityanomalydetectionproblems.arXivpreprintarXiv:2003.10360,2020.SubashPoudyal,KulPrasadSubedi,andDipankarDasgupta.Aframeworkforanalyzingran-somwareusingmachinelearning.In2018IEEESymposiumSeriesonComputationalIntelligence(SSCI),pp.1692–1699.IEEE,2018.RajeshRanganath,SeanGerrish,andDavidBlei.Blackboxvariationalinference.InArtificialIntelligenceandStatistics,pp.814–822,2014.PeterK.SharpeandRJSolly.Dealingwithmissingvaluesinneuralnetwork-baseddiagnosticsystems.NeuralComputing&Applications,3(2):73–77,1995.ShinaSheenandAshwithaYadav.Ransomwaredetectionbyminingapicallusage.In2018Inter-nationalConferenceonAdvancesinComputing,CommunicationsandInformatics(ICACCI),pp.983–987,2018.doi:10.1109/ICACCI.2018.8554938.JinsooShin,HanseongSon,GyunyoungHeo,etal.Developmentofacybersecurityriskmodelusingbayesiannetworks.ReliabilityEngineering&SystemSafety,134:208–217,2015.NoahSimon,JeromeFriedman,TrevorHastie,andRobertTibshirani.Asparse-grouplasso.Journalofcomputationalandgraphicalstatistics,22(2):231–245,2013.Marek ́Smieja,ŁukaszStruski,JacekTabor,BartoszZieli ́nski,andPrzemysławSpurek.Processingofmissingdatabyneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pp.2719–2729,2018.MukundSundararajan,AnkurTaly,andQiqiYan.Axiomaticattributionfordeepnetworks.InInternationalConferenceonMachineLearning,pp.3319–3328.PMLR,2017.YukiTakeuchi,KazuyaSakai,andSatoshiFukumoto.Detectingransomwareusingsupportvec-tormachines.InProceedingsofthe47thInternationalConferenceonParallelProcessingCompanion,ICPP’18,NewYork,NY,USA,2018.AssociationforComputingMachinery.ISBN9781450365239.doi:10.1145/3229710.3229726.URLhttps://doi.org/10.1145/3229710.3229726.UmaraUrooj,MohdAizainiBinMaarof,andBanderAliSalehAl-rimy.Aproposedadaptivepre-encryptioncrypto-ransomwareearlydetectionmodel.In20213rdInternationalCyberResilienceConference(CRC),pp.1–6,2021.doi:10.1109/CRC50527.2021.9392548.HanqiZhang,XiXiao,FrancescoMercaldo,ShiguangNi,FabioMartinelli,andArunKumarSan-gaiah.Classificationofransomwarefamilieswithmachinelearningbasedonn-gramofopcodes.FutureGenerationComputerSystems,90:211–221,2019.12UnderreviewasaconferencepaperatICLR2023AHYPERPARAMETERSETTINGSForreproducibility,thisappendixprovidesthehyperparametersettingsusedfortheproposedmodelaswellasthoseforthebaselinemodels.ThebestgeneralhyperparametersettingsfromtuningareprovidedinTable3.OthernetworkhyperparameterareincludedinTable4.ThehyperparametersfortheRNN,ODE,FullyConnected,andBayesianmodelsareprovidedinTables,5,6,7,and8,respectively.ParameterValuetrainbatchsize100maximumnumtrainepochs400hiddensize706learningrate1e-4learningrateforprobabilities1e-3Adamβ10.5Adamβ20.999Table3:GeneralhyperparametersusedfortrainingtheproposedBayesianneuralnetworksandbaselinemod-els.ParameterValueIncludebinaryclassificationlossTrueParameterforpositiveweightinthebinarylosstorepresentimbalanceofthedata0.0068stdusedinlikelihoodterm(orMSE)0.1dropoutretentionratefordiscriminator0.9slopeofleakyrelufunction0.2Table4:Networkhyperparameters.ParameterValueNumberoflayersinODEfuncinrecognitionODE100NumberofunitsperlayerinODEfunc0.0068ODEsolverEulerODEfuncunits300ODEfuncrecnumlayers300Table5:ODEhyperparameters.BRANSOMWAREATTACKSRansomwareattacksfallintotwomaincategories,automatedransomwarewhichincludeinfamouscasessuchasWannaCry,andhumanoperatedransomware(HumOR)conductedbyactorgroupssuchasREvilandamyriadofothers.Althoughautomatedransomwareinvolveshumans,thedis-tributionofthepayloadusuallydoesnotinvolvehumaninteraction.HumORattacks,however,involvehands-on-keyboardactivity,whereanactivehumanadversaryhasgainedaccesstoanet-work–whetherthroughpurchasedaccess,malware,vulnerabilities,orothermeans–andprogressesthroughthekillchaintoescalateprivileges,movelaterallyifpossible,anddistributeransomwareintheenvironment.Humanoperatedattackstendtobemoresevere,astheadversaryisabletotakestepstobypassprotectionsandworktoensuretheransomwarepayloadisexecutedsuccessfully.Securitysolutionswillactivelymonitorforthesesuspiciouseventsacrossthedifferentkillchainstagesinaransomwareattacktodetectandalertonthemaliciousbehaviors.Ransomwareattackerswilltypicallyutilizemultipletoolkits,custommalware,andscriptstocon-ducttheiractivitymoreeffectively.Oftenthiscanalsoentailmultipleoperatorsfordifferentstagesinthekillchain,suchaswithRansomware-as-a-Service(RaaS)attacks.RaaSinvolvesoperatorswhoworktocreatetoolsandprovideaccessforvettedattackers–knownasaffiliates-toconductthemajorityoftheransomwareattack.Complicatingmatters,manyofthetoolsransomwareattack-ersfrequentlyuseareopen-sourceandhavelegitimatepurposes,preventingoutrightdetectionandblockingunlessthemethodofusingthetoolscanspecificallybeclassifiedasmalicious.13UnderreviewasaconferencepaperatICLR2023ParameterValueRNNcelltypeGRUGRUunits300Table6:RNNhyperparameters.ParameterValueNumberofhiddenlayersinFCnetwork1NumberofhiddenunitsinFCnetworkinFCnetwork300Table7:Fullyconnectedneuralnetworkhyperparameters.Thereareseveralchallengesfordetectingandblockingransomwareattacks.First,thereisatimecriticalityrequiredfordetectionpriortothedistributionandencryptionofdevices.Ideally,agoodransomwaredetectionservicecandetectaransomwareattackpriortotheencryptionofanyassets.Thisnecessitatesdetectingthecompromiseasearlyinthekillchainaspossible.However,theearlystagesofanattackdonotnecessarilyhaveclearandspecificimplicationsofransomwareandcanoftenmirrorattacksthatarenotransomwareinnature.Second,althoughransomwareattacksareincreasingandregularlyreportedinthenews,theyarestillrare,andthelabeleddataislimited.Therefore,aransomwaredetectormustnotoverfittosparsedata.Inaddition,aransomwaredetectionservicemusthaveaccesstosignalsfromalargenumberofcomputersormobiledevicesinordertocreatedatasetsthatcanlearntodetectimportantbehaviors.Third,thesystemmustgeneralizetohandlepolymorphismsincethesignalsarepolymorphicbytheirnature.Attackersmaydelayorreordertheiractivity,utilizeopen-sourcelegitimatetoolsformaliciouspurposes,usepolymorphicmalware(e.g.,backdoors)orscripts,orfastfluxnetworksforcommandandcontroltoavoiddetection.Finally,theinputsignalsareoftenweakandoftendonotindicatearansomwareattackontheirown.Aneffectiveransomwaredetectionservicemustbeabletocombinetheselow-levelsignalsinordertoproduceasuccessfulhigh-leveldetection.CTHREATMODELTheBNNransomwaredetectoroperatesondatacollectedfromthe<theanonymizedcompany’s>currentlyoperationalbackendsecuritysystem.Thissystemprocessesthelow-leveleventswhicharegeneratedbythedeviceandstoredinacloudservice,oralternativelyon-premise,and,likeallsecurityservices,thiscreatesseveralareaswhichmustbeprotectedfromattack.Thelow-leveleventsaregeneratedbythedeviceitselfinkernelmodeoftheoperatingsystem.Thesystemassumesthattheeventsaresuccessfullygenerated,transmitted,andreceivedbythecloudoron-premisebackendservice,andthattheeventshavenotbeenalteredbytheactorusingaperson-in-the-middleattack.Next,thesystemassumesthattheeventshavenotbeenalteredoncetheyhavereceivedandstoredinthebackendserviceitself.Thus,thesystemassumesthattherearenosuccessfuldatapoisoningorinsiderthreatattacks.Finally,thesystemassumesthattheransomwarealertsaresuccessfullytransmittedtoandcorrectlyreceivedbythecustomer’sandthe<theanonymizedcompany’s>analysisportals.Itshouldbenotedthatallofthesesystemcomponentsareoperationaltoday,andtheproposedmodelonlyaffectstheprocessingcomponentinthebackendservice.DTEMPORALPREPROCESSINGInthisappendix,weinvestigatethesparsityoftherawdataset.InFigure2,weshowthedistributionofthenumberoffeaturesthataresetduringeachsingleoneminutetimestep(i.e.,timeinterval)intherawdata.Ingeneral,tenorfewerfeaturesaresetduringasingleoneminutetimeinterval.Afewoneminutetimestepshavebetween10and20featuresthatareset,whileasmallnumberofothershavebetween40and55featuresset.Interestingly,wefoundthatnoneofthetimestepshadbetween20and40featuresset.14UnderreviewasaconferencepaperatICLR2023ParameterValueKLcoefficientforVIGravesMethodtocomputeKLreparameterizationNumberofsamplestoevaluatethetest100Table8:Bayeshyperparameters.Figure2:Countofthenumberoffeaturesavailablefor1,oneminutetimestepoftheransomwaredata.Themainmasscontainslessthan10featurespertimepoint.EFEATUREIMPORTANCEANDINTERPRETATIONDETAILSInthisappendix,wediscusstwomethodstodeterminethemostimportantfeaturesfortheRadialSpikeandSlabBayesianNeuralNetworkransomwaredetectionmodel.Thefirstmethodwecon-sideristoranktheposteriorprobabilitieswhicharefoundinthefirstlayerweightsoftheBNN.RecallthatthecoreideabehindaBNNwithSpikeandSlabdistributionsistolearnaparameterθπ,whichmodelstheprobabilityS(θπ)ofeachnodeintheneuralnetworktobeincluded.GiventhatthefirstlayeroftheBNNisfullyconnected,wecanconsiderS(θπ)ofthefirstlayerastheimpor-tanceofeachTTPforournetwork.SincewesuspectthatnotallTTPsareequallyimportant,weexpecttoobservespikesinthelearnedS(θπ).TheresultsinFigure3confirmthishypothesis,wherewestartfromtheuninformative,uniformprior(left)andgeneratespikesinthelearnedS(θπ)(right)aftertraining.However,whileunderstandingwhichTTPfeaturesareimportantbasedontheBNN’s−→Figure3:OneoftheapproachestoevaluatetheimportanceoftheinputfeaturesforaSpikeandSlabBNNistoevaluatethelearnedposteriorprobabilityS(θπ)(right).Notehowdifferentitisfromthenon-informativepriorprobabilities(left).trainedweightsconceptuallymakessense,weinsteadfollowamorewell-knownandestablishedwaytointerpretthefeaturesofageneralneuralnetwork,calledIntegratedGradients(Sundararajanetal.,2017)asthesecondmethodtorankthefeatures.15UnderreviewasaconferencepaperatICLR2023Basedonthisprocedure,wecangenerateanimportancescoreforeachfeaturegivenatrainednetwork,andthesescoresarerepresentedinFigure4forourmodel.IncontrasttotheBayesianap-proach,thismethodalsoincludesthesignsofthefeaturescores.Apositiveattributionscoremeansthataparticularfeaturepositivelycontributedtothefinalpredictionofanattackbeingransomwareandanegativescoreindicatesthefeaturewasimportantforpredictingnon-ransomwareattacks.Themagnitudeoftheattributionscoresignifiesthestrengthofthecontribution.Afeaturewhichdoesnotmeaningfullycontributetothefinaloutputhasascoreofnearzero.Figure4:ApplyingtheIntegratedGradientsmethodgeneratesscoreswhichindicatetheimportanceoftheinputfeatures.Ahigher,positivescoremeansthatthefeatureisrelevantforpredictingdatabelongingtotheransomwareclass,whileanegativescoremeansthefeatureismorerelevanttopredictingthenon-ransomwareattackclass.FPROOFOFTHEOREM3.1Inthisappendix,weprovidetheproofofTheorem3.1.16UnderreviewasaconferencepaperatICLR2023Proof.KL(q(w,π)’p(w,π))=&π&wlogq(w,π)p(w,π)q(w,π)dwdπgiventhatq(w,π)=q(w|π)q(π)andp(w,π)=p(w|π)p(π)=&π’&wlogq(w,π)p(w,π)q(w|π)dw*q(π)dπgiventhatq(π)=Bern(λq)andp(π)=Bern(λp)=q(π=0)’&wlogq(w|0)q(π=0)p(w|0)p(π=0)q(w|0)dw*+q(π=1)’&wlogq(w|1)q(π=1)p(w|1)p(π=1)q(w|1)dw*=(1−λq)’log1−λq1−λp&wδ0(w)dw*+λq’logλqλp+&wloggq(w)gp(w)gq(w)dw*=(1−λq)log1−λq1−λp+λqlogλqλp+λq&wloggq(w)gp(w)gq(w)dw=KL(Bern(λq)’Bern(λp))+λqKL(gq’gp).17",
    "reference": "# Summary Of The Paper\n\nIn this work, the authors propose to detect ransomware-based attacks amongst a general class of malware attacks using a Bayesian Neural Network (BNN). In such settings, the feature set available to subsequently perform inference or detection of vulnerabilities is quite sparse, and imbalanced. In order to address this, the paper proposes to utilize a new architecture for the BNN, namely using a Radial Spike and Slab BNN. Here, the weights are assumed to arise from a mixture distribution, with the first component being a dirac mass (or “spike”) and the second having an atomless support (or “slab”), namely a Radial distribution (Farquhar et al., 2020).\n\n# Strength And Weaknesses\n\nStrengths:\n1) The setting of malware detection through the use of variational inference with BNNs is indeed an important topic of study, with possible immediate implications towards real-world security.\n2) The proposed Radial Spike and Slab distribution for the BNN is indeed interesting, and helps simultaneously model a mixture distribution of discrete and atomless components to address the specific issues that are commonly seen to arise from the feature sets available in the setting of malware detection. The use of the Radial distribution (proposed by Farquhar et al., 2020) for the atomless component also helps circumvent the concentration of measure within an annulus as seen for multivariate gaussians in large dimensional settings.\n\nWeaknesses:\n1) The scope of the paper as presented is quite limited to the particular application of BNNs for detecting ransomware attacks amongst a set of more general malware attacks. This is quite disparate from that required in practical scenarios, wherein real-world security systems need to raise alerts amongst feature sets corresponding to (rare) occurrences of abnormal activity, amongst the general set of normal functioning parameters.\n2) Furthermore, it is unclear why ransomware attacks alone are studied, given that other rare modes of malware attacks exist with corresponding sparse feature sets and comparable severity in ramifications arising from the security breach, all of which could potentially be addressed using BNNs as well. Thus, it is unclear why the detection of ransomware attacks alone amongst a set of malware attacks with equivalent or greater severity is required.\n3)  The empirical evaluations presented are fairly limited, and could be significantly improved. For example, the paper states that variants of XGBoost produced random results. However, given that it is known that out of the sparse binary feature set of dimension 706, the 298 MITRE ATT&CK features are known to form the most crucial subset. Thus, for several models of lower complexity, this reduced feature set could be used to attain significantly improved baselines, by utilizing this domain-expert specific information to exclude near-irrelevant features.\n4) Furthermore, the improvement in detection performance achieved using the proposed method as compared to baseline approaches such as BNN-Radial are extremely marginal, if any, as observed from Table-1. It is unclear if any improvement in detection performance can be established in a statistically significant manner, without performance statistics reported over multiple training instances of these networks with different random seeds. \n\nMinor Typos:\nThe  abstract needs to be modified to follow the ICLR 2023 guidelines for typesetting and formatting.\nThe citation scheme used in the paper results in several instances where author names appear contiguously within sentences without demarcation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is fairly well written, and presents its key ideas in a lucid manner. The overall scope and originality of the paper is fairly limited however.\n\n# Summary Of The Review\n\nThe paper studies the very narrow problem of the isolated detection of ransomware attacks amongst a larger class of malware attacks. Furthermore, the results obtained using the proposed Radial Spike and Slab BNN do not seem statistically significant in achieving improved detection over the baseline of Radial BNNs.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSIMULTANEOUSLY LEARNING STOCHASTIC AND ADVERSARIAL MARKOV DECISION PROCESS WITH LINEAR FUNCTION APPROXIMATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nReinforcement learning (RL) has been commonly used in practice. To deal with the numerous states and actions in real applications, the function approximation method has been widely employed to improve the learning efficiency, among which the linear function approximation has attracted great interest both theoretically and empirically. Previous works on the linear Markov Decision Process (MDP) mainly study two settings, the stochastic setting where the reward is generated in a stochastic way and the adversarial setting where the reward can be chosen arbitrarily by an adversary. All these works treat these two environments separately. However, the learning agents often have no idea of how rewards are generated and a wrong reward type can severely disrupt the performance of those specially designed algorithms. So a natural question is whether an algorithm can be derived that can efficiently learn in both environments but without knowing the reward type. In this paper, we first consider such best-of-both-worlds problem for linear MDP with the known transition. We propose an algorithm and prove it can simultaneously achieve O(poly log K) regret in the stochastic setting and ̃O( K) regret in the adversarial setting where K is the horizon. To the best of our knowledge, it is the first such result for linear MDP.\n\n√\n\n1\n\nINTRODUCTION\n\nReinforcement learning (RL) studies the problem where a learning agent interacts with the environment over time and aims to maximize its cumulative rewards in a given horizon. It has a wide range of real applications including robotics (Kober et al., 2013), games (Mnih et al., 2013; Silver et al., 2016), etc. The environment dynamics are usually modeled by the Markov Decision Process (MDP) with a fixed transition function. We consider the general episodic MDP setting where the interactions last for several episodes and the length of each episode is fixed (Jin et al., 2018; 2020b; Luo et al., 2021; Yang et al., 2021). In each episode, the agent first observes its current state and would decide which action to take. After making the decision, it receives an instant reward and the environment will then transfer to the next state. The cumulative reward in an episode is called the value and the objective of the agent is equivalent to minimizing the regret defined as the cumulative difference between the optimal value and its received values over episodes.\n\nMany previous works focus on the tabular MDP setting where the state and action space are finite and the values can be represented by a table (Azar et al., 2017; Jin et al., 2018; Chen et al., 2021; Luo et al., 2021). Most of them study the stochastic setting with the stationary reward in which the reward of a state-action pair is generated from a fixed distribution (Azar et al., 2017; Jin et al., 2018; Simchowitz & Jamieson, 2019; Yang et al., 2021). Since the reward may change over time in applications, some works consider the adversarial MDP where the reward can be arbitrarily generated among different episodes (Yu et al., 2009; Rosenberg & Mansour, 2019; Jin et al., 2020a; Chen et al., 2021; Luo et al., 2021). All of these works pay efforts to learn the value function table to find the optimal policy and the computation complexity highly depends on the state and action space size.\n\nHowever, in real applications such as the Go game, there are numerous states and the value function table is huge, which brings a great challenge to the computation complexity for traditional algorithms in the tabular case. To cope with the dimensionality curse, a rich line of works employ the function\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\napproximation methods, such as the linear function and deep neural networks, to approximate the value functions or the policies to improve learning efficiency. These methods also achieve great success in practical scenarios such as the Atari and Go games (Mnih et al., 2013; Silver et al., 2016). Despite their great empirical performances, it also brings a series of challenges in deriving theoretical analysis. To build a better theoretical understanding of these approximation methods, lots of works start from deriving regret guarantees for linear function classes.\n\nThe linear MDP is a popular model which assumes both the transition and reward at a state-action pair are linear in the corresponding d-dimensional feature (Jin et al., 2020b; He et al., 2021; Hu et al., 2022). There are also mainly two types of the reward. For the stochastic setting, Jin et al. (2020b) provides the first efficient algorithm named Least-Square Value Iteration UCB (LSVI-UCB) and show that the its regret over K episodes can be upper bounded by O( K). To seek for a tighter result with respect to the specific problem structure, He et al. (2021) provide a new analysis for LSVI-UCB and show it achieves an O(poly log K) instance-dependent regret upper bound. The adversarial setting is much harder than the stochastic one since the reward can change arbitrarily but the agent can only observe the rewards on the experienced trajectory. For this more challenging case, a regret upper bound of order O( K) is only obtained in the case with known transition by Neu & Olkhovskaya (2021). All these works separately treat two environment types.\n\n√\n\n√\n\nHowever, the learning agent usually has no idea of how the reward is generated. And once the reward type is wrong, the specially designed algorithm for a separate setting may suffer great loss. Thus deriving an algorithm that can adapt to different environment types becomes a natural solution for this problem. This direction has attracted great research interest in simpler bandit (Bubeck & Slivkins, 2012; Zimmert et al., 2019; Lee et al., 2021; Kong et al., 2022) and tabular MDP settings (Jin & Luo, 2020; Jin et al., 2021b) but still remains open in linear MDP.\n\nIn this paper, we try to answer the question of deriving best-of-both-worlds (BoBW) guarantees for linear MDP. Due to the challenge of learning in the adversarial setting, we also consider the known transition case. We propose an algorithm that continuously detects the real environment type and adjusts its strategy. It has been shown that our algorithm can simultaneously achieve O(poly log K) regret in the stochastic setting and ̃O( K) regret in the adversarial setting. To the best of our knowledge, these are the first BoBW results for linear MDP. It is also worth noting that our BoBW algorithm relies on an algorithm that can achieve a high-probability guarantee for the adversarial setting, which previous works fail to provide. And we propose the first analysis for a high-probability regret bound in the adversarial linear MDP.\n\n√\n\n2 RELATED WORK\n\n√\n\nLinear MDP. Recently, deriving theoretically guaranteed algorithms for RL with linear function approximation has attracted great interests. The linear MDP model is one of the most popular one. Jin et al. (2020b) develop the first efficient algorithm LSVI-UCB both in sample and computation d3H 3K) regret where d is the complexity for this setting. They show that the algorithm achieves O( feature dimension and H is the length of each episode. This result is recently improved to the optimal order O(dH K) by Hu et al. (2022) with a tighter concentration analysis. Apart from UCB, the TS-type algorithm has also been proposed for this setting (Zanette et al., 2020a). All these results do not consider the specific problem structure. In the stochastic setting, deriving an instance-dependent regret is more attractive to show the tighter performances of algorithms in a specific problem. This type of regret has been widely studied under the tabular MDP setting (Simchowitz & Jamieson, 2019; Yang et al., 2021). He et al. (2021) is the first to provide this type of regret in linear MDP. Using a different proof framework, they show that the LSVI-UCB algorithm can achieve O(d3H 5 log K/∆) where ∆ is the minimum value gap in the episodic MDP.\n\n√\n\nAll these works consider the stochastic setting with stationary rewards. Neu & Olkhovskaya (2021) first attempts to analyze the more challenging adversarial environment. They consider a simplier setting with known transition and provide an O( dHK) regret upper bound. For unknown transition case, Luo et al. (2021) provide an O(d2/3H 2K 2/3) upper bound with the help of a simulator and O(d2H 4K 14/15) guarantee for the general case. Above all, even in the separate adversarial setting, K) regret is only derived for known transition case. We also study the known transition O(\n\n√\n\n√\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nsetting and try to provide ̃O( O(poly log K) regret if the environment is truly stochastic.\n\nK) regret in the adversarial setting while simultaneously achieving\n\n√\n\n√\n\nBest-of-both-worlds. The question of reaching best-of-both-worlds results is first proposed by Bubeck & Slivkins (2012) for bandit setting, a special case of episodic MDP with H = 1. Their proposed algorithm assumes the setting is stochastic and continuously detects whether the assumption is satisfied. Such a detection-based method is shown to achieve O(poly log K) regret in the stochastic setting and O( K) in the adversarial setting, which is later improved by Auer & Chiang (2016). Similar detection-based techniques have also been adopted in more general linear bandit (Lee et al., 2021) and graph feedback (Kong et al., 2022) settings to achieve BoBW guarantees. Another line of works consider using Follow-the-Regularized-Leader (FTRL) to adapt to different environment types (Zimmert & Seldin, 2019; 2021). This type of algorithm is shown to be tighter than Bubeck & Slivkins (2012); Auer & Chiang (2016) in the bandit setting and also attracts lots of interest in more complex problems such as combinatorial bandits (Zimmert et al., 2019; Chen et al., 2022).\n\nThe first BoBW result in the MDP setting is provided by Jin & Luo (2020) in the tabular case. Due to the challenge of the problem, they first study the known transition setting. Their approach to achieving BoBW is the FTRL algorithm with a newly designed regularizer, which result is later improved by Jin et al. (2021b) and also generalized to the unknown transition case. To the best of our knowledge, we are the first to consider the BoBW problem in linear MDP. We also start from the known transition setting and our algorithm is based on detection.\n\nRL with general function approximation The linear mixture MDP is another popular RL model with linear function approximation. It assumes the transition function can be approximated by a weighted average over several transition kernels. In the stochastic setting, both instance-independent (Ayoub et al., 2020; Zhou et al., 2021) and dependent regret bound (He et al., 2021) have been derived. And in the adversarial setting, only full information case has been studied, where the agent has access to the rewards of all state-action pairs (Cai et al., 2020; He et al., 2022). Apart from linear function approximation, there is also a rich line of works considering general function classes, such as the setting with low Bellman rank (Jiang et al., 2017; Zanette et al., 2020b), low Eluder dimension (Wang et al., 2020; Kong et al., 2021) and low Bellman Eluder dimension (Jin et al., 2021a).\n\n3 SETTING\n\nWe consider the episodic MDP setting where the agent interacts with the environment for K episodes with known transition. The episodic MDP can be represented by M(S, A, H, {rk}K k=1 , P ) where S is the state space, A is the action space, H is the length of each episode, rk = (cid:8)rk,h h=1 is the reward function and P = {Ph}H h=1 is the known transition probability function. Specifically, at each episode k and step h ∈ [H], rk,h(s, a) ∈ [0, 1] and Ph(· | s, a) ∈ [0, 1]|S| are the reward and transition probability at state s ∈ S by taking action a ∈ A, respectively.\n\n(cid:9)H\n\nWe focus on stationary policies. Denote π = {πh}H h=1 as a policy mapping from the state space to an action distribution where πh : S → ∆A. For each episode k ∈ [K], the agent would start 1 and determine the policy πk. Then at each step h ∈ [H] of episode from the first state sk,1 := s1 k, it first observes the current state sk,h and then perform the action ak,h ∼ πk,h(· | sk,h). The agent would receive a random reward yk,h := rk,h(sk,h, ak,h) + εk,h, where εk,h is an independent zero-mean noise. The environment then transfers to the next state sk,h+1 based on the transition function P (· | sk,h, ak,h). The episode ends when the last state sk,H+1 is reached.\n\nWe focus on linear MDP with known transition where the reward functions are linear in a given feature mapping (Jin et al., 2020b; He et al., 2021). The formal definition is as follows. Assumption 1. (Linear MDP with known transition) M(S, A, H, {rk}K k=1 , P ) is a linear MDP with a known feature mapping φ : S × A → Rd such that for each step h ∈ [H], there exists an unknown vector θh where for each (s, a) ∈ S × A, rk,h(s, a) = (cid:10)φ(s, a), θk,h\n\n(cid:11).\n\n1The deterministic starting state is only for expositional convenience. Our algorithm and analysis can directly\n\nhandle random starting states with a distribution.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nIn the stochastic setting, the reward function {rk}K fixed over different episodes k ∈ [K]. And in the adversarial setting, the reward parameter {θk}K k=1 can be chosen arbitrarily by an adversary (which may be possibly dependent on previous policies).\n\nk=1, or namely the reward parameter {θk}K\n\nk=1, is\n\nWe evaluate the performance of a policy π by its value functions. Specifically, for any episode k and step h, denote the Q-value function Qπ k,h(s, a) as the expected reward that will be obtained by the agent starting from (sk,h, ak,h) = (s, a) with policy π, which is formally defined as\n\nQπ\n\nk,h(s, a) = E\n\n\n\n\n\nH (cid:88)\n\nh′=h\n\nyk,h′ | π, sk,h = s, ak,h = a\n\n .\n\n\n\nSimilarly, the value function V π\n\nk,h(s) of any state s is defined as\n\n\n\nV π\n\nk,h(s) = E\n\n\n\nH (cid:88)\n\nh′=h\n\n\n\nyk,h′ | π, sk,h = s\n\n .\n\nk := V π\n\nIn the following paper, we abuse a bit notation by using V π k,1(s1) to represent the value of policy π at the starting state s1 and episode k. Define φπ,h = E (cid:2)φ(sh, ah) | π(cid:3) as the expected feature vector that the policy π visits at step h. It is worth noting that this recovers the state-action visitation probability in the tabular setting. And according to the definition, the value function of k = (cid:80)H policy π at episode k can be represented as V π In this paper, we consider optimizing both the stochastic and adversarial environments within a finite policy set Π. Given a policy set Π, the learning agent would determine the policy πk ∈ Π in each episode k ∈ [K]. Let π∗ ∈ arg maxπ k be one optimal policy in Π that maximizes the cumulative value functions over K episodes, which is assumed to be unique in the stochastic setting similar to previous works in tabular MDP (Jin & Luo, 2020; Jin et al., 2021b) and bandit setting (Lee et al., 2021; Zimmert & Seldin, 2019; 2021). Denote the cumulative regret compared with π∗ ∈ Π over K episodes as\n\n(cid:10)φπ,h, θk,h\n\nk=1 V π\n\n(cid:80)K\n\nh=1\n\n(cid:11).\n\nReg(K; Π) =\n\nK (cid:88)\n\n(cid:16)\n\nk=1\n\n4 ALGORITHM\n\nV π∗\n\nk − V πk\n\nk\n\n(cid:17)\n\n.\n\n(1)\n\nIn this section, we propose a detection-based algorithm to optimize both stochastic and adversarial environments for linear MDP with a given policy set Π. Our algorithm is mainly inspired by the detection technique of Lee et al. (2021) for BoBW in linear bandits. At a high level, the algorithm first assumes the environment is truly adversarial and continuously detect whether it could be a stochastic one. Its design relies on a new linear MDP algorithm that can return well-concentrated estimators for values of policies and also achieve sub-linear regret in the adversarial setting with high probability.\n\nPrevious works on adversarial linear MDP fail to provide a high-probability guarantee and thus no existing algorithms satisfy this property. In Appendix D, we propose a variant of Geometric Hedge (Algorithm 4), which is initially designed for the simple bandit case (Bartlett et al., 2008), and provide a new analysis for it in the linear MDP setting. We show that this algorithm satisfies the following properties and can be used to derive the BoBW results. It is also worth noting that this algorithm is the first to achieve a high-probability regret guarantee for adversarial linear MDP.\n\nTheorem 1. Given a policy set Π, the regret of our proposed Algorithm 4 in the adversarial setting can be upper bounded by\n\nReg(K; Π) ≤ O\n\n(cid:18)(cid:113)\n\ndH 3K log (cid:0)|Π|/δ(cid:1)\n\n(cid:19)\n\n(2)\n\nwith probability at least 1 − δ.\n\nFurther, at each episode k, Algorithm 4 returns a value estimator ˆV π k for each policy π ∈ Π. Choosing constant L0 = 4dH log (cid:0)|Π|/δ(cid:1), C1 ≥ 215dH 3 log (cid:0)K|Π|/δ(cid:1) and C2 ≥ 20, it holds that for any\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nk0 ≥ L0 and policy π ∈ Π,\n\nk0(cid:88)\n\nk=1\n\n(cid:0)V π\n\nk − V πk\n\nk\n\n(cid:112)\n\n(cid:1) ≤\n\nC1k0 − C2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk0(cid:88)\n\nk=1\n\n(cid:16) ˆV π\n\nk − V π\n\nk\n\n(cid:17)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nwith probability larger than 1 − δ.\n\nOur main BoBW algorithm is a phased algorithm and is presented in Algorithm 1. It takes Algorithm 4 satisfying Theorem 1 with parameter L0 and C1 as input. The first epoch is of length L0 and the length of the following epochs would grow exponentially as Line 4. During each epoch, it executes Algorithm 2, which we refer to as the BoBW main body (Line 3).\n\nAlgorithm 1 BoBW for linear MDP\n\n1: Input: Algorithm 4 with parameter C1 and L0; Set L := L0. Maximum duration K. 2: while number of episodes k ≤ K do 3: 4: 5: end while\n\nExecute Algorithm 2 (BoBW main body) with parameter L and receive output k0 Set L = 2k0\n\nAlgorithm 2 (BoBW main body) takes the Algorithm 4 with parameter C1 and L as input. Here L can just be regarded as the minimum number of episodes that Algorithm 4 needs to run to collect enough observations. The algorithm first assumes the environment is adversarial and executes Algorithm 4 in at least L episodes (which we refer to as the first phase). As shown in Theorem 1, Algorithm 4 guarantees that when running for more than L episodes, the regret compared with any policy π and the distance between its estimated V value and the real V value would be no larger. Based on these concentration properties, if a policy ˆπ shows consistent better performance than all of the other policies (Line 5), we have the reason to believe that the environment is truly stochastic. Being aware of this, as shown in Line 6, Algorithm 2 would transfer to the stochastic phase (Line 9-18, which we refer to as the second phase) with the estimated V values returned by Algorithm 4.\n\nSince the estimated values by Algorithm 4 can well approximate the real values of policies, the exploration in the stochastic setting can be conducted by the estimated value gaps to obtain a problemdependent regret bound. The objective is to identify the optimal policy and maximize the collected rewards, which can be implemented by an optimization problem (Algorithm 3). Taking the estimated value gap ˆ∆ as input, Algorithm 3 would return a probability distribution p∗ over the policy set Π. Intuitively, p∗ maximizes the expected values of policies while ensuring the uncertainty of all policies to be smaller than its sub-optimality gap. In Appendix A, we show that when ˆ∆ is estimated accurately, selecting policies based on p∗ can reach a problem-dependent regret upper bound.\n\nBack to the main body of Algorithm 2, after computing the distribution pk based on the current estimated ˆ∆ (Line 10), it also mixes this policy distribution with a one-hot vector eˆπ to ensure that the estimated optimal policy ˆπ can be observed for enough times and the variance of its following estimators can thus be low (Line 11). The algorithm then samples a policy πk according to this mixed distribution and executes it in this episode. Then based on the received rewards yk,h at each step h and the total reward Yk = (cid:80)H h=1 yk,h, the value estimations of policies can be further updated. Due to the technical reason, here for the estimated optimal policy ˆπ and other policies π, we use different estimators. Specifically, we use the importance weighted estimator to approximate the value of ˆπ. The reason for using mixed policy distribution ̃p is just to ensure the low variance of this estimator. And for other policies, we use the standard least square estimators (Line 13). Based on these newly estimated values of policies, the algorithm can then update the estimation for their sub-optimality gaps as Line 14. To get a tighter analysis, when computing the value gap of π, we use the traditional estimator of it by Algorithm 4 in the first k0 episodes and the Catoni estimator for the recent k − k0 episodes. Formally speaking, the Catoni estimator i=1 Φ (cid:0)α (Xi − z)(cid:1), where Catoniα Φ(y) = log (cid:0)1 + y + y2/2(cid:1) if y ≥ 0 and Φ(y) = − log (cid:0)1 − y + y2/2(cid:1) otherwise. The hyper-\n\n(cid:0){X1, X2, · · · Xn}(cid:1) is defined as the unique root of f (z) = (cid:80)n\n\nparameter απ\n\nk is set as\n\n4 log (cid:0)k|Π|/δ(cid:1)/(cid:80)k\n\nκ=k0+1\n\n(cid:16)\n\n2κ ˆ∆2\n\nπ/βκ + 9dH\n\n(cid:17)\n\nwhich mainly follows Lee\n\n(cid:114)\n\net al. (2021) but with the careful consideration in MDP setting.\n\n5\n\n(3)\n\n(4)\n\n(5)\n\n(6)\n\n(cid:17)\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 BOBW main body\n\n1: Input: A new instance of Algorithm 4 with parameter C1, parameter L 2: Define: fK = log K 3: for each episode k = 1, 2, · · · do 4: 5:\n\nExecute and update Algorithm 4, receive value estimators ˆV π if k ≥ L and there exists a policy ˆπ ∈ Π such that\n\nk for each π ∈ Π\n\nk (cid:88)\n\ns=1\n\nk (cid:88)\n\ns=1\n\nYs ≤\n\nYs ≥\n\nk (cid:88)\n\ns=1\n\nk (cid:88)\n\ns=1\n\ns + 5(cid:112)fKC1k , ˆV ˆπ\n\ns + 25(cid:112)fKC1k , ∀π ̸= ˆπ . ˆV π\n\nthen\n\n6:\n\nk0 = k, ˆ∆π = 1\n\nk0\n\n(cid:16)(cid:80)k\n\ns=1\n\nˆV ˆπ\n\ns − ˆV π\n\ns\n\n(cid:17)\n\n, ˆ∆ =\n\n(cid:26)(cid:16) ˆ∆π\n\n(cid:17)\n\n(cid:27)\n\n; break\n\nπ∈Π\n\nend if 7: 8: end for 9: for episode k = k0 + 1, k0 + 2, · · · do\n\n10: 11: 12:\n\n13:\n\nCompute pk = OP(k, ˆ∆) Compute ̃pk(π) = 1 Sample πk ∼ ̃pk and execute πk Receive rewards Yk = (cid:80)H\n\n2 eˆπ + 1\n\n2 pk(π), where eˆπ is a one-hot vector with 1 only at policy ˆπ\n\nh=1 yk,h and calculate ˆV π\n\nk for each π ∈ Π as follows\n\n∀π ̸= ˆπ : ˆV π\n\nk =\n\nH (cid:88)\n\nφ⊤\n\nπ,hΣ−1\n\nk,hφπk,hyk,h , where Σk,h =\n\nˆV ˆπ\n\nk =\n\nh=1 Yk ̃pk (ˆπ)\n\n1{πk = ˆπ} .\n\n14:\n\nFor each π ̸= ˆπ, compute ˆ∆π as\n\n(cid:88)\n\nπ\n\n ̃pk(π)φπ,hφ⊤\n\nπ,h ;\n\nˆ∆π\n\nk =\n\n1 k\n\n15:\n\nif\n\n\n\n\n\nk0(cid:88)\n\ns=1\n\nˆV π\n\ns + (k − k0)Robk,π −\n\nk (cid:88)\n\ns=1\n\nˆV ˆπ\n\ns\n\n\n\n , with Robk,π = Catoniαπ\n\nk\n\n(cid:16)\n\n{ ˆV π\n\ns }k\n\ns=k0+1\n\n∃π ̸= ˆπ , ˆ∆π\n\nk /∈\n\n(cid:104)\n\n0.39 ˆ∆π, 1.81 ˆ∆π\n\n(cid:105)\n\nor\n\nk (cid:88)\n\ns=k0+1\n\n(cid:16) ˆV ˆπ\n\ns − Ys\n\n(cid:17)\n\n≥ 20(cid:112)fKC1k0\n\n(7)\n\n(8)\n\n(9)\n\nthen\n\nReturn k0\n\n16: end if 17: 18: end for\n\nIs is worth noting that an adversarial setting may be disguised as stochastic scenarios and fool the algorithm to enter in the stochastic phase. Thus the agent still needs to be vigilant about the possible change of the environment. The detection conditions (Line 15) are set for this objective. To be specific, if the estimated sub-optimality gap of a policy changes obviously compared with the original estimation by Algorithm 4 in the adversarial phase or the regret compared with ˆπ is large, the algorithm can determine that the environment may not be stochastic and would terminiate the current epoch and enter in the next epoch with parameter k0 (Line 16).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3 Optimization problem (OP) 1: Define βk = 215H log (cid:0)|Π|k/δ(cid:1) 2: Return the minimizer p∗ of the following constrained optimization problem\n\n(cid:16)\n\n(cid:17)\n\nk, ˆ∆\n\nmin p\n\n(cid:88)\n\nπ∈Π\n\npπ ˆ∆π\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\ns.t.\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nh (p) ≤\n\nk ˆ∆π βk\n\n+ 4dH and\n\n(cid:88)\n\nπ∈Π\n\npπ = 1 ,\n\n(10)\n\n(11)\n\nwhere Σh(p) = (cid:80)\n\nπ pπφπ,hφ⊤\n\nπ,h.\n\n5 THEORETICAL ANALYSIS\n\nIn this section, we provide the theoretical guarantee and the analysis of Algorithm 1 in both stochastic and adversarial settings\n\nThe first is about the stochastic setting. Since the value function remains the same for different episodes, we simplify the notation and use V π to represent the real value of policy π ∈ Π. Before presenting the main results, we first introduce the sub-optimality gaps that will be used. Definition 1. For each policy π ∈ Π, define ∆π = V π∗ − V π as the sub-optimality gap of π compared with the optimal policy π∗ ∈ arg maxπ∈Π V π. Further let ∆min = minπ:∆π>0 ∆π be the minimum non-negative value gap.\n\nTheorem 2 provides a regret upper bound for Algorithm 1 in the stochastic setting. Theorem 2. (Regret bound in the stochastic setting) With probability at least 1 − δ, Algorithm 1 guarantees that\n\nReg(K; Π) ≤ O\n\n.\n\n(12)\n\n(cid:32)\n\ndH 2 log(K) log (cid:0)|Π|K/δ(cid:1) ∆min\n\n(cid:33)\n\nAnd if the environment is adversarial, the regret of Algorithm 1 can be upper bounded as Theorem 3. Theorem 3. (Regret bound in the adversarial setting) With probability at least 1 − δ, Algorithm 1 guarantees that\n\nReg(K; Π) ≤ O\n\ndH 3K log(K) log (cid:0)|Π|K/δ(cid:1)\n\n.\n\n(13)\n\n(cid:18)(cid:113)\n\n(cid:19)\n\nDue to the space limit, the full proof of these two theorems are deferred to Appendix B and C. We will provide a proof sketch for them in later sections.\n\nTechnique challenge and novelty. There are mainly two types of algorithms to deal with the BoBW problem: the switch-based method which actively detects the environment type, e.g., Bubeck & Slivkins (2012), and the FTRL-based method which adapts to different environments, e.g. Zimmert & Seldin (2019). The approach in Bubeck & Slivkins (2012) first assumes the setting to be stochastic and would detect whether a policy’s value has changed. Such an approach in our setting brings an O((cid:112)|Π|) dependence in the regret for adversarial setting which is not idealistic as the policy set size can be large. And the success of FTRL for BoBW mainly relies on a self-bounding inequality that bounds the regret by the chosen probabilities of policies. But such a technique is challenging with linear structure. As discussed by Lee et al. (2021), even for the single-state linear bandit setting, connecting FTRL with OP is hard.\n\nOur approach relies on a new observation that the value of each policy can be written as the inner product between the expected state-action visitation feature φπ and the unknown reward feature θ. From this view, we are able to reduce the problem to linear optimization and existing techniques for linear optimization can be used. To the best of our knowledge, we are the first to introduce this type of linear optimization for the regret minimization problem in MDP and such reduction may be of independent interest.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nRelationship between our ∆π and the gapmin in He et al. (2021). To compare our ∆π with gapmin in He at al. (2021), we assume the optimal policy π∗ ∈ Π is just the global optimal policy. h (s)− Recall that gapmin is defined as minh,s,a h(s, a). In general, ∆π can be decomposed as ∆π = V ∗ Q∗ 1 (s1, a′) ≥ V ∗ 1 (s1) − Q∗ 1(s1, a′) = gap1(s1, a′) where the first equality follows He et al. (2021, Eq. (B.2)) and pπ h(s, a) is the visitation probability of state-action (s, a) at step h by following π. This shows that ∆π ≥ pπgapmin in the worst case where pπ is the minimum none-zero visitation probability of policy π at some state-action pair.\n\n(cid:8)gaph(s, a) : gaph(s, a) > 0(cid:9) where gaph(s, a) = V ∗\n\n1 (s1) = V ∗\n\n1 (s1) − V π\n\n1 (s1) − Qπ\n\nAnd there are also cases where our defined ∆π is larger than that in He et al. (2021). When both the policy and transition are deterministic (Ortner, 2010; Tranos & Proutiere, 2021; Dann et al., 2021; Tirinzoni et al., 2022), we have ∆π = (cid:80)H h=1 gaph(sh, ah) ≥ gapmin. And in the stochastic transition case, if all sub-optimal policies happen to not select the optimal action in arg maxa Q∗ 1(s1, a), 1(s1, a′) = gap1(s1, a′) ≥ gapmin, ∆π = V ∗ where a′ is the action selected by π at the first step and the last inequality is due to gap1(s1, a′) > 0. In the above two cases, our our sub-optimality gap is larger than previously defined gap and our dependence on the gap is better.\n\n1 (s1, a′) ≥ V ∗\n\n1 (s1) = V ∗\n\n1 (s1) − V π\n\n1 (s1) − Qπ\n\n1 (s1) − Q∗\n\nTo the best of our knowledge, Algorithm 1 is the first that can simultaneously achieve O(poly log K) regret in the stochastic setting and ̃O( K) regret in the adversarial setting for linear MDP problem. It is also worth noting that previous works on the separate adversarial setting only provide the upper bound for the expected regret (Neu & Olkhovskaya, 2021), and we are the first to provide a high-probability guarantee.\n\n√\n\n5.1 REGRET ANALYSIS IN THE STOCHASTIC SETTING\n\nIn the stochastic setting, we consider the regret in two phases of each epoch separately. We first show that, the first phase (which we call as the adversarial phase, Line 3-8 in Algorithm 2) will terminate after k0 episodes where k0 ∈ (cid:2)64fKC1/∆2 (cid:3) with high probability, and the optimal policy π∗ ∈ Π can be identified. Lemma 1 summarizes the formal claims. Lemma 1. In the stochastic setting, in each epoch, the following 4 claims hold.\n\nmin, 900fKC1/∆2\n\nmin\n\n1. With probability at least 1 − 4δ, k0 ≤ max 2. With probability at least 1 − δ, ˆπ = π∗. 3. With probability at least 1 − 2δ, k0 ≥ 64fK C1 4. With probability at least 1 − 3δ, ˆ∆π ∈ [0.7∆π, 1.3∆π] , ∀π ̸= π∗.\n\n, L\n\n∆2\n\nmin\n\nmin\n\n.\n\n(cid:110) 900fK C1 ∆2\n\n(cid:111) .\n\nThe detailed proof of Lemma 1 is deferred to Appendix B. We next will give a proof sketch of Theorem 2 based on the results of Lemma 1. According to claim 1 in Lemma 1, we know that (cid:1), so we can bound the regret in the first phase using the guarantees of k0 = O (cid:0)fKC1/∆2 Algorithm 4 in Theorem 1 by\n\n(cid:112)log(K)/∆min\n\nC1k0 = O\n\nC1\n\nmin\n\n√\n\n(cid:16)\n\n(cid:17)\n\n.\n\nAnd after k0 episodes, the algorithm would transfer to the second phase, which we call as the stochastic phase (Line 9-18 in Algorithm 2). If the environment is truly stochastic, the values of all policies would remain stationary and the detection condition (Line 15 in Algorithm 2) would never be satisfied. Thus the stochastic phase will not end (proved in Lemma 8 in Appendix B). As for the regret suffered in this phase, we can analyze it using the properties of OP. According to claim 4 in Lemma (cid:105) 1, the estimated sub-optimality gap is close to the real sub-optimality gap as ˆ∆π ∈ .\nThus performing policies based on the solution of OP with ˆ∆ can reach the real instance optimality.\n\n∆π/\n\n3∆π\n\n√\n\n√\n\n3,\n\n(cid:104)\n\nSpecifically, we can first bridge the gap between the expected regret and the regret that occurred using Freedman inequality (Lemma 12).\n\nk (cid:88)\n\n∆πs ≤ 2\n\nk (cid:88)\n\n(cid:88)\n\ns=k0+1\n\ns=k0+1\n\nπ\n\n ̃ps(π)∆π + 2H log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n.\n\nRecall that ̃p is computed based on OP under ˆ∆, which is close to the real sub-optimality gap ∆. The regret occurred in phase 2 in episodes k larger than a problem dependent constant M =\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nO (cid:0)dHβK/∆2 7 with r = 3:\n\nmin\n\n(cid:1), which is the dominating part in the expected regret, can be bounded using Lemma\n\nk (cid:88)\n\n(cid:88)\n\ns=M\n\nπ\n\n ̃ps(π)∆π ≤\n\nk (cid:88)\n\ns=M\n\n72dHβs ∆mins\n\n= O\n\n(cid:18) dHβk log(k) ∆min\n\n(cid:19)\n\n.\n\nAbove all, we can conclude that with probability at least 1 − δ, the regret can be upper bounde as\n\nReg (K; Π) = O\n\n(cid:18) dHβK log(K) ∆min\n\n(cid:19)\n\n= O\n\n(cid:32)\n\ndH 2 log(K) log (cid:0)|Π|K/δ(cid:1) ∆min\n\n(cid:33)\n\n.\n\n5.2 REGRET ANALYSIS IN THE ADVERSARIAL SETTING\n\nIn the adversarial setting, the regret in the first phase can be guaranteed with the property of Algorithm 4 in Theorem 1. Here we will mainly analyze the second phase. We first show that in the second phase of each epoch, the returned policy ˆπ is actually the optimal policy in Π.\n\nLemma 2. For any episode k in the second phase, the policy ˆπ has the most accumulated value in Π during episodes 1 to k. That is, ˆπ ∈ arg maxπ∈Π\n\nκ=1 V π κ .\n\n(cid:80)k\n\nSince ˆπ is the optimal policy in Π, the regret can be written as the sum of the deviation between the value of the selected policy πs and V ˆπ\n\ns\n\nk (cid:88)\n\n(cid:16)\n\ns=k0+1\n\nV ˆπ\n\ns − V πs\n\ns\n\n(cid:17)\n\n=\n\nk (cid:88)\n\n(cid:20)(cid:16) ˆV ˆπ\n\ns − Ys\n\n(cid:17)\n\n(cid:16)\n\n+\n\nYs1{πs = ˆπ} + V ˆπ\n\ns\n\n1{πs ̸= ˆπ} − ˆV ˆπ\n\ns\n\n(cid:17)\n\ns=k0+1\n\n(cid:18)(cid:16)\n\n+\n\nV ˆπ\n\ns\n\n1{πs = ˆπ} − Ys1{πs = ˆπ}\n\n(cid:17)\n\n+ (Ys − V πs s )\n\n(cid:19)(cid:35)\n\n.\n\nfKC1k0\n\nAccording to the detection condition (equation 9) in Algorithm 2, the first term can be upper bounded by O (cid:0)√ (cid:1). As for the second and last term, Freedman inequality (Lemma 12) also provides an upper bound O (C1k0) for them. Above all, the regret in a single epoch can be upper bounded by O (cid:0)√ (cid:1). According to the choice of the minimal duration L in each epoch, the length k0 of the first phase in an epoch is at most half of that in the next epoch. Thus the final regret can be bounded as\n\nfKC1k0\n\nReg (K; Π) = O\n\n(cid:16)(cid:112)C1K log(K)\n\n(cid:17)\n\n= O\n\n(cid:16)(cid:112)dH 2KβK log(K)\n\n(cid:17)\n\n.\n\n6 CONCLUSION\n\nIn this paper, we propose the first BoBW algorithm for linear MDP that can simultaneously achieve O(poly log K) regret in the stochastic setting and ̃O( K) regret in the adversarial setting. Our approach relies on the new observation that the value function of a policy can be written as the sum of the inner products between the expected state-action visitation feature φπ,h and the unknown reward parameter θh at different steps h ∈ [H]. And the problem can thus be regarded as an online linear optimization problem.\n\n√\n\nApart from these BoBW results, we also propose a new analysis that can reach a high-probability regret guarantee for adversarial linear MDP, which is also the first such result in the literature.\n\nAn important future direction is to remove the assumption of unique optimal policy in the stochastic setting. This assumption also appears in previous BoBW works for tabular MDP (Jin & Luo, 2020; Jin et al., 2021b) and linear bandits (Lee et al., 2021) which destroys the generality of the results but is challenging to be removed due to the hardness of the BoBW objective and the complex structure of MDP. Extending the current results to the unknown transition setting is also prospective. This is hoped to be solved by some new techniques since the current approach highly depends on the known state-action visitation feature. Deriving an FTRL-type algorithm for this objective is also an interesting future direction. It still remains open even in the simpler linear bandit setting without the transition between different states.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nPeter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic\n\nand adversarial bandits. In Conference on Learning Theory, pp. 116–120. PMLR, 2016.\n\nAlex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning, pp. 463–474. PMLR, 2020.\n\nMohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pp. 263–272. PMLR, 2017.\n\nPeter Bartlett, Varsha Dani, Thomas Hayes, Sham Kakade, Alexander Rakhlin, and Ambuj Tewari. High-probability regret bounds for bandit online linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory-COLT 2008, pp. 335–342. Omnipress, 2008.\n\nSébastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In Conference on Learning Theory, pp. 42–1. JMLR Workshop and Conference Proceedings, 2012.\n\nQi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimiza-\n\ntion. In International Conference on Machine Learning, pp. 1283–1294. PMLR, 2020.\n\nCheng Chen, Canzhe Zhao, and Shuai Li. Simultaneously learning stochastic and adversarial bandits\n\nunder the position-based model. In AAAI, 2022.\n\nLiyu Chen, Haipeng Luo, and Chen-Yu Wei. Minimax regret for stochastic shortest path with adversarial costs and known transition. In Conference on Learning Theory, pp. 1180–1215. PMLR, 2021.\n\nChristoph Dann, Teodor Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert. Beyond valuefunction gaps: Improved instance-dependent regret bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 34:1–12, 2021.\n\nDavid Freedman. On tail probabilities for martingales. 1975.\n\nJiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. In International Conference on Machine Learning, pp. 4171–4180. PMLR, 2021.\n\nJiafan He, Dongruo Zhou, and Quanquan Gu. Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps. In International Conference on Artificial Intelligence and Statistics, pp. 4259–4280. PMLR, 2022.\n\nPihe Hu, Yu Chen, and Longbo Huang. Nearly minimax optimal reinforcement learning with linear function approximation. In International Conference on Machine Learning, pp. 8971–9019. PMLR, 2022.\n\nNan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In International Conference on Machine Learning, pp. 1704–1713. PMLR, 2017.\n\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient?\n\nAdvances in neural information processing systems, 31, 2018.\n\nChi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial markov decision processes with bandit feedback and unknown transition. In International Conference on Machine Learning, pp. 4860–4869. PMLR, 2020a.\n\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143. PMLR, 2020b.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nChi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. Advances in neural information processing systems, 34:13406–13418, 2021a.\n\nTiancheng Jin and Haipeng Luo. Simultaneously learning stochastic and adversarial episodic mdps with known transition. Advances in neural information processing systems, 33:16557–16566, 2020.\n\nTiancheng Jin, Longbo Huang, and Haipeng Luo. The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. Advances in Neural Information Processing Systems, 34: 20491–20502, 2021b.\n\nJens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The\n\nInternational Journal of Robotics Research, 32(11):1238–1274, 2013.\n\nDingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for reinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203, 2021.\n\nFang Kong, Yichi Zhou, and Shuai Li. Simultaneously learning stochastic and adversarial bandits with general graph feedback. In International Conference on Machine Learning, pp. 11473–11482. PMLR, 2022.\n\nChung-Wei Lee, Haipeng Luo, Chen-Yu Wei, Mengxiao Zhang, and Xiaojin Zhang. Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In International Conference on Machine Learning, 2021.\n\nHaipeng Luo, Chen-Yu Wei, and Chung-Wei Lee. Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. Advances in Neural Information Processing Systems, 34:22931– 22942, 2021.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nGergely Neu and Julia Olkhovskaya. Online learning in mdps with linear function approximation and bandit feedback. Advances in Neural Information Processing Systems, 34:10407–10417, 2021.\n\nRonald Ortner. Online regret bounds for markov decision processes with deterministic transitions.\n\nTheoretical Computer Science, 411(29-30):2684–2695, 2010.\n\nAviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes. In International Conference on Machine Learning, pp. 5478–5486. PMLR, 2019.\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\n\nMax Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular\n\nmdps. Advances in Neural Information Processing Systems, 32, 2019.\n\nAndrea Tirinzoni, Aymen Al-Marjani, and Emilie Kaufmann. Near instance-optimal pac reinforcement learning for deterministic mdps. Advances in neural information processing systems, 2022.\n\nDamianos Tranos and Alexandre Proutiere. Regret analysis in deterministic reinforcement learning. In 2021 60th IEEE Conference on Decision and Control (CDC), pp. 2246–2251. IEEE, 2021.\n\nRuosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information Processing Systems, 33:6123–6135, 2020.\n\nChen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. CoRR,\n\nabs/1801.03265, 2018. URL http://arxiv.org/abs/1801.03265.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKunhe Yang, Lin Yang, and Simon Du. Q-learning with logarithmic regret.\n\nIn International\n\nConference on Artificial Intelligence and Statistics, pp. 1576–1584. PMLR, 2021.\n\nJia Yuan Yu, Shie Mannor, and Nahum Shimkin. Markov decision processes with arbitrary reward\n\nprocesses. Mathematics of Operations Research, 34(3):737–757, 2009.\n\nAndrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In International Conference on Artificial Intelligence and Statistics, pp. 1954–1964. PMLR, 2020a.\n\nAndrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning, pp. 10978–10989. PMLR, 2020b.\n\nDongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory, pp. 4532–4576. PMLR, 2021.\n\nJulian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. In International Conference on Artificial Intelligence and Statistics, pp. 467–475. PMLR, 2019.\n\nJulian Zimmert and Yevgeny Seldin. Tsallis-inf: An optimal algorithm for stochastic and adversarial\n\nbandits. Journal of Machine Learning Research, 22:1–49, 2021.\n\nJulian Zimmert, Haipeng Luo, and Chen-Yu Wei. Beating stochastic and adversarial semi-bandits optimally and simultaneously. In International Conference on Machine Learning, pp. 7683–7692. PMLR, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA ANALYSIS OF OP\n\nThe analysis of OP mainly follows the op problem for linear bandits (Lee et al., 2021) but with the consideration of the special structure of linear MDP. In this section, we provide some useful lemmas for OP in linear MDP.\n\nLemma 3. First, consider the constrained optimization problem:\n\nmin p∈∆Π\n\n(cid:88)\n\nπ\n\npπ ˆ∆π −\n\n2 ξ\n\nH (cid:88)\n\nh=1\n\n(− ln(det(Σh(p)))) ,\n\nwhere Σh(p) = (cid:80)\n\nπ pπφπ,hφ⊤\n\nπ,h.\n\nThe optimal choice of p = p∗ yields that:\n\n(cid:88)\n\nπ∈Π\n\np∗\n\nπ\n\nˆ∆π ≤\n\n2dH ξ\n\n,\n\nH (cid:88)\n\nh=1\n\n∥φπ,h∥2\n\nh (p∗) ≤\n\nΣ−1\n\nξ ˆ∆π 2\n\n+ dH , ∀π ∈ Π .\n\nProof. Here we relaxed the constraint that pπ be a valid distribution on Π to (cid:80) π pπ ≤ 1 since there must be ˆπ∗ where ˆ∆ˆπ∗ = 0, so we can always add up the probability on ˆπ∗ to make it a distribution. Applying the KKT conditions and setting the derivatives with respect to each pπ in the Lagrangian to zero, we got:\n\n0 = ˆ∆π −\n\n2 ξ\n\nH (cid:88)\n\nh=1\n\n∥φπ,h∥2\n\nh (p∗) − λπ + λ ,\n\nΣ−1\n\nwhere λπand λ are the respective Lagrange multipliers for the constraints of pπ ≥ 0 and (cid:80) thus we have λ ≥ 0 and (cid:80) π ∈ Π, we got:\n\nπ λπpπ = 0 . Multiplying the above equation with p∗\n\nπ pπ ≤ 1, π and summing over\n\n0 =\n\n=\n\n(cid:88)\n\nπ∈Π\n\n(cid:88)\n\nπ∈Π\n\n p∗\n\nπ\n\nˆ∆π −\n\n2 ξ\n\nH (cid:88)\n\nh=1\n\np∗ π∥φπ,h∥2\n\nh (p∗) − λπp∗\n\nΣ−1\n\nπ + λp∗\n\nπ\n\n\n\n\n\np∗\n\nπ\n\nˆ∆π −\n\n2 ξ\n\nH (cid:88)\n\n(cid:88)\n\nh=1\n\nπ∈Π\n\nπ∥φπ,h∥2 p∗\n\nh (p∗) + λ .\n\nΣ−1\n\nNotice that:\n\n(cid:88)\n\nπ∈Π\n\nPlugging in this result, we got:\n\n∥φπ,hp∗\n\nπ∥2\n\nh (p∗) =\n\nΣ−1\n\n(cid:88)\n\nπ∈Π (cid:88)\n\nπ∈Π\n\nπφπ,hΣ−1 p∗\n\nh (p∗)φ⊤\n\nπ,h\n\n(cid:16)\n\nTr\n\nπφπ,hφ⊤ p∗\n\nπ,hΣ−1\n\nh (p∗)\n\n=\n\n(cid:17)\n\n\n\nπφπ,hφ⊤ p∗\n\nπ,hΣ−1\n\nh (p∗)\n\n\n\n\n\n\n\n(cid:88)\n\nπ∈Π\n\n= Tr\n\n= d .\n\n0 =\n\n≥\n\n(cid:88)\n\nπ∈Π (cid:88)\n\nπ∈Π\n\np∗\n\nπ\n\nˆ∆π −\n\np∗\n\nπ\n\nˆ∆π −\n\n2 ξ\n\n2 ξ\n\ndH + λ\n\ndH .\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nSo that\n\nand λ ≤ 2 Above all,\n\nξ dH, since (cid:80)\n\nπ∈Π p∗\n\nπ\n\n(cid:88)\n\nπ∈Π\n\np∗\n\nπ\n\nˆ∆π ≤\n\n2 ξ\n\ndH ,\n\nˆ∆π ≥ 0.\n\nH (cid:88)\n\nh=1\n\n∥φπ,h∥2\n\nh (p∗) =\n\nΣ−1\n\nξ 2\n\n( ˆ∆π − λπ + λ) ≤\n\nξ 2\n\n( ˆ∆π + λ) ≤\n\nξ ˆ∆π 2\n\n+ dH .\n\nLemma 4. Suppose that for any h ∈ {1, 2, · · · , H}, (cid:8)φπ,h |π ∈ Π (cid:9) spans Rd. Denote pΠ as a uniform distribution on Π and let κ ∈ (0, 1 2 ). For any G ⊂ Π, there exists distribution on q ∈ PG such that (cid:80)H ≤ 2dH, where qG,κ = κpΠ + (1 − κ)q and Σh(qG,κ) = (cid:80) π,h.\n\nh=1∥φπ,h∥2\n\nπ φπ,hφ⊤\n\nπ∈Π qG,κ\n\nh (qG,κ)\n\nΣ−1\n\nProof. Denote P G,κ = (cid:8)κpΠ + (1 − κ)q |q ∈ PG\n\n(cid:9).\n\n∥φπ,h∥2\n\nΣ−1\n\nh (q)\n\nmin q∈P G,κ\n\nmax π∈G\n\n= min\n\nq∈P G,κ\n\nmax p∈PG\n\n= max p∈PG\n\nmin q∈P G,κ\n\n≤ max p∈PG\n\nH (cid:88)\n\nh=1\n\nTr\n\nH (cid:88)\n\nh=1\n\nH (cid:88)\n\nh=1\n\nH (cid:88)\n\nh=1 \n\n (\n\n\n\n(cid:88)\n\nTr\n\n\n\nπ∈G\n\n\n\n(cid:88)\n\nTr\n\n\n\nπ∈G\n\n\n\n\n\n\n\n−1\n\npπφπ,hφ⊤\n\nπ,h\n\n\n\n\n\n(cid:88)\n\nqπφπ,hφ⊤\n\nπ,h\n\n\n\nπ∈Π\n\n\n\n\n\n\n\n−1\n\npπφπ,hφ⊤\n\nπ,h\n\n\n\n\n\n(cid:88)\n\nqπφπ,hφ⊤\n\nπ,h\n\n\n\nπ∈Π\n\n(14)\n\n(cid:88)\n\npπφπ,hφ⊤\n\nπ,h)\n\n\n\n(cid:88)\n\nπ∈G\n\nπ∈Π\n\n\n\n(cid:18) κ |Π|\n\n(cid:19)\n\n+ (1 − κ)pπ\n\n−1\n\n\n\nφπ,hφ⊤\n\nπ,h\n\n\n\n \n\n\n\n\n\n \n\n\n\nH (cid:88)\n\nh=1\n\nTr\n\n(cid:18) κ |Π|\n\n(cid:88)\n\nπ∈Π\n\n+ (1 − κ)pπ\n\n(cid:19)\n\n\n\n\n\nφπ,hφ⊤\n\nπ,h\n\n\n\n\n\n(cid:88)\n\nπ∈Π\n\n(cid:18) κ |Π|\n\n(cid:19)\n\n+ (1 − κ)pπ\n\n−1\n\n\n\nφπ,hφ⊤\n\nπ,h\n\n\n\n \n\n≤2 max p∈PG\n\n=2dH .\n\nWhere the second inequality is due to Sion’s minimax theorem as equation 14 is linear in p and convex in q. The last inequality is due to 1 − κ ≥ 1 2 .\n\nLemma 5. Let pπ be the solution of OP(k, ˆ∆), then we have : (cid:19)\n\npπ ˆ∆π ≤ O\n\n(cid:88)\n\nπ∈Π\n\n(cid:18) dHβk√\n\nk\n\n.\n\n(15)\n\n√\n\nProof. We transform p∗ in Lemma 3 to a solution satisfying OP. Choosing ξ as 2 p∗ + 1 q = 1 π ∈ G,\n\n2 qG,κ, where qG,κ is the distribution stated above with κ = 1√\n\nin Lemma 4, and let G = {π ∈ Π : ˆ∆π ≤ 1√\n\nk βk\n\nk\n\nk\n\n}, construct the distribution\n\nin Lemma 4, we have for\n\nH (cid:88)\n\nh=1\n\n∥φπ,h∥2\n\nh (q) ≤ 4dH;\n\nΣ−1\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nand for π /∈ G,\n\nH (cid:88)\n\n∥φπ,h∥2\n\nh (q) ≤ 2\n\nΣ−1\n\nh=1\n\n(cid:32)\n\nξ ˆ∆π 2\n\n(cid:33)\n\n√\n\n+ dH\n\n≤\n\nk ˆ∆π βk\n\n+ 2dH ≤\n\nk ˆ∆2 π\nβk\n\n+ 4dH .\n\nSo distribution q satisfy the constraints of OP. For the optimal solution p of OP, we have:\n\npπ ˆ∆π\n\n(cid:88)\n\nπ∈Π\n\n≤qπ ˆ∆π =\n\n(cid:18) 1 2\n\np∗ +\n\n(cid:19)\n\nˆ∆π\n\nqG,κ\n\n1 2\n\n≤\n\ndHβk√ k\n\n+\n\nH √\n\n2\n\nk\n\n+\n\n1 √\n\n2\n\nk\n\n= O\n\n(cid:18) dHβk√\n\nk\n\n(cid:19)\n\n.\n\nLemma 6. Given{ ˆ∆π}π∈Π, suppose there exists unique ˆπ such that ˆ∆ˆπ = 0, and ˆ∆min = π∈Π pπ ˆ∆π ≤ min ˆ∆π>0\n\nˆ∆π. Let pπ be the solution of OP(k, ∆), when k ≥ 16dHβk\n\n, we have (cid:80)\n\nˆ∆2\n\nmin\n\n24dHβk ˆ∆mink\n\n.\n\nProof. let Gi = {π ∈ Π : 2i−1 ˆ∆2 empty. Define zi = dHβk 2i−2 ˆ∆2 For π ̸= ˆπ,\n\nmink\n\nmin ≤ ˆ∆2\n\nπ ≤ 2i ˆ∆2\n\nmin } and n be the largest index that Gi is not\n\nand κ = 1\n\nn2n . Define the distribution ̃p as follows:\n\nfor ˆπ,\n\nWe show it’s a valid distribution over Π:\n\n ̃pπ =\n\n(cid:88)\n\ni≥1\n\nziqGi,κ\n\nπ\n\n;\n\n ̃pˆπ = 1 −\n\n(cid:88)\n\nπ̸=ˆπ\n\n ̃pπ .\n\n ̃pˆπ =1 −\n\n=1 −\n\n≥1 −\n\n=1 −\n\n(cid:88)\n\nπ̸=ˆπ (cid:88)\n\nπ̸=ˆπ (cid:88)\n\ni≥1 (cid:88)\n\ni≥1\n\n ̃pπ\n\n(cid:88)\n\ni\n\nziqGi,κ\n\nπ\n\nzi −\n\nzi −\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ni≥1 (cid:88)\n\nπ∈Gi (cid:88)\n\nj̸=i (cid:88)\n\ni≥1\n\nπ∈Gi\n\nj̸=i\n\nzjqGj ,κ\n\nπ\n\nzj n2n|Π|\n\n≥1 − 2\n\n(cid:88)\n\ni≥1\n\nzi\n\n≥\n\n1 2\n\n.\n\nWhere the last inequality is due to k ≥ 16dHβk For π ̸= ˆπ and π ∈ Gi,\n\nˆ∆2\n\nmin\n\n.\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nh ( ̃p) ≤\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\nh=1\n\n(cid:13) 2\n(cid:13) (cid:16)(cid:80)\n\nπ∈Π ziqGi,κ\n\nπ\n\n(cid:17)−1 ≤\n\nφπ,hφ⊤\n\nπ,h\n\n2dH zi\n\n≤\n\nk ˆ∆2 π\nβk\n\n+ 4dH .\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFor ˆπ,\n\nH (cid:88)\n\n(cid:13) (cid:13)φˆπ,h\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nh ( ̃p)\n\nH (cid:88)\n\n≥\n\nh=1\n\nH (cid:88)\n\n≥\n\nh=1\n\n(cid:13) (cid:13)Σ−1 (cid:13)\n\nh ( ̃p)φˆπ,h\n\n(cid:13) (cid:13)Σ−1 (cid:13)\n\nh ( ̃p)φˆπ,h\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nΣ−1\n\nh ( ̃p)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:16) 1\n\n2 φˆπ,hφ⊤\n\nˆπ,h\n\n(cid:17)\n\nH (cid:88)\n\n(cid:13) (cid:13)φˆπ,h\n\n≥\n\n1 2\n\n(cid:13) 4\n(cid:13) Σ−1\n\nh ( ̃p)\n\nh=1 \n\n≥\n\n1 2H\n\nH (cid:88)\n\n(cid:13) (cid:13)φˆπ,h\n\n\n\nh=1\n\n 2\n\n(cid:13) 2\n(cid:13) Σ−1\n\nh ( ̃p)\n\n\n\n,\n\nwhere the last inequality is due to Cauchy inequality. So that:\n\nH (cid:88)\n\nh=1\n\n(cid:13) (cid:13)φˆπ,h\n\n(cid:13) 2\n(cid:13) Σ−1\n\nh ( ̃p) ≤ 2H .\n\nThus, ̃p satisfy the constraints of OP. Now we bound the result of OP. By the optimality of pπ:\n\n(cid:88)\n\npπ ˆ∆π ≤\n\n(cid:88)\n\n ̃pπ ˆ∆π\n\nπ∈Π\n\nπ∈Π\n\n ziqGi,κ +\n\n(cid:88)\n\n(cid:88)\n\ni≥1\n\nπ∈Gi\n\nzj\n\n1 n2n|Π|\n\n(cid:88)\n\nj̸=i\n\n\n\n 2\n\ni\n\n2 ˆ∆min\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nn2n+j− i\n\ndHβk 2 −2|Π| ˆ∆mink\n\n+\n\n(cid:88)\n\ni≥1\n\ndHβk 2 −2 ˆ∆mink\n\n2 i\n\n=\n\n≤\n\ni≥1\n\nπ∈Gi\n\nj̸=i dHβk 2 −2 ˆ∆mink\n\n2 i\n\n≤2\n\n(cid:88)\n\ni≥1\n\n≤\n\n24dHβk ˆ∆mink\n\n.\n\nLemma 7. Suppose that ˆ∆π ∈ yields that :\n\n(cid:16) 1√\n\nr ∆π,\n\n√\n\n(cid:17)\n\nr∆π\n\n, then the solution pπ of OP(k, ˆ∆) for k ≥ 16rdHβk\n\n∆2\n\nmin\n\n(cid:88)\n\nπ∈Π\n\npπ∆π ≤\n\n24rdHβk ∆mink\n\n.\n\nProof. By the condition on ∆π, we have t ≥ 16rdHβk\n\n∆2\n\nmin\n\n(cid:88)\n\nπ∈Π\n\npπ∆π ≤\n\n√\n\nr\n\n(cid:88)\n\npπ ˆ∆π ≤\n\n√\n\nr\n\nπ∈Π\n\nWhere the inequality is due to Lemma 6.\n\nmin\n\n≥ 16dHβk ˆ∆2 24dHβk ˆ∆mink\n\n, and that ∆π∗ = ˆ∆π∗ = 0. Thus,\n\n≤\n\n24rdHβk ∆mink\n\n.\n\nB ANALYSIS IN THE STOCHASTIC SETTING\n\nProof of Lemma 1. First, we show the following properties, for any k in phase 1:\n\nC2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nˆV π\n\ns − V π\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nV πs\n\ns − V π\n\ns ≤\n\n(cid:112)\n\nC1k + k∆π .\n\n(cid:112)\n\nC1k +\n\n≤\n\nk (cid:88)\n\ns=1\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:12) (cid:80)k (cid:12) (cid:12)\n\nˆV π\n\ns − V π\n\n(cid:12) (cid:12) (cid:12).\n\ns=1\n\nHere we denote DEVk,π = Claim 1’s proof: Let k = max{ 900fK C1 , L} and assume that phase 1 has not finished at episode k. Set ˆπ = π∗, and we show that the termination conditions hold with high probability at episode k. According to the Azuma-Hoeffding inequality, since Ys − V πs is a martingale sequence and that |Ys − V πs\n\n∆2\n\ns | ≤ H,\n\nmin\n\ns\n\ns\n\nk (cid:88)\n\ns=1\n\nYs ≤\n\n≤\n\n≤\n\n≤\n\nk (cid:88)\n\ns=1\n\nK (cid:88)\n\ns=1\n\nK (cid:88)\n\ns=1\n\nK (cid:88)\n\ns=1\n\nV πs\n\ns +\n\n(cid:112)\n\nC1k\n\nV π∗\n\ns +\n\n(cid:112)\n\nC1k\n\nˆV π∗\n\ns + 2\n\n(cid:112)\n\nC1k\n\ns + 3(cid:112)fKC1k , ˆV π∗\n\nso equation 3 is satisfied. For all π ̸= π∗, we have:\n\nk (cid:88)\n\ns=1\n\nˆV π\n\ns − Ys =\n\nk (cid:88)\n\ns=1\n\n(cid:16) ˆV π\n\ns − V π\n\ns\n\n(cid:17)\n\n(cid:16)\n\n+\n\ns − V π∗ V π\n\ns\n\n(cid:17)\n\n(cid:16)\n\n+\n\nV π∗\n\ns − V πs\n\ns\n\n(cid:17)\n\n+ (V πs\n\ns − Ys)\n\nC1k − C2DEVk,π∗ +\n\nC1k + k∆π\n\n(cid:17)\n\n− k∆π\n\n(cid:112)\n\nC1k\n\n(cid:112)\n\n(cid:16)(cid:112)\n\n≤DEVk,π − k∆π + ≤2(cid:112)fKC1k + ≤ − 0.95k∆π + 2.1(cid:112)fKC1k . √\n\n1 C2\n\n, thus k∆π ≥ 30\n\nfKC1k for all π ̸= π∗. So −0.95k∆π + 2.1\n\n√\n\nSince k ≥ 900fK C1 −25 fKC1k. Thus:\n\n∆2 π\n\nk (cid:88)\n\ns=1\n\nYs ≥\n\nk (cid:88)\n\ns=1\n\ns + 25(cid:112)fKC1k . ˆV π\n\nSo equation 4 is satisfied. Claim 2’s proof: Using equation 3 and equation 4, we got:\n\nk0(cid:88)\n\ns=1\n\nˆV ˆπ\n\ns − ˆV π\n\ns ≥ 20(cid:112)fKC1k0 , ∀π ̸= ˆπ .\n\nHowever, with probability at least 1 − δ, for any π ̸= π∗\n\nk0(cid:88)\n\ns=1\n\ns − ˆV π∗ ˆV π\n\ns ≤\n\nk0(cid:88)\n\ns=1\n\n(cid:16) ˆV π\n\ns − V π\n\ns\n\n(cid:17)\n\n(cid:16)\n\n+\n\ns − V π∗ V π\n\ns\n\n(cid:17)\n\n(cid:16)\n\n+\n\ns − ˆV π∗ V π∗\n\ns\n\n(cid:17)\n\n≤DEVk0,π + DEVk0,π∗ − k0∆π (cid:112)\n\n(cid:16)(cid:112)\n\n(cid:17)\n\nC1k0 + k0∆π\n\n+\n\n≤\n\n1 C2\n\n1 C2\n\n≤5(cid:112)fKC1k0 .\n\nC1k0 − k0∆π\n\nSo we must have ˆπ = π∗.\n\n17\n\n√\n\nfKC1k ≤\n\n(16)\n\nUnder review as a conference paper at ICLR 2023\n\nClaim 3’s proof: Suppose that k0 ≤ 64fK C1 is: ∆π = ∆min.\n\n∆2\n\nmin\n\n. Let π be the policy with minimal estimated gap, that\n\nk0(cid:88)\n\ns=1\n\nˆV π∗\n\ns − ˆV π\n\ns ≤k0∆min + DEVk0,π + DEVk0,π∗\n\n(cid:17)\n\nC1k0 + k0∆min\n\n(cid:16)\n\n2\n\n(cid:112)\n\n1 ≤k0∆min + C2 ≤2k0∆min + 2(cid:112)fKC1k0 ≤16(cid:112)fKC1k0 + 2(cid:112)fKC1k0 =18(cid:112)fKC1k0 .\n\nWhich contradicts with equation 16.\n\nClaim 4’s proof:\n\n(cid:12) (cid:12)\n\n(cid:12)k0 ˆ∆π − k0∆π\n\n(cid:12) (cid:12) (cid:12) ≤DEVk0,π + DEVk0,π∗\n\n(cid:16)\n\n(cid:112)\n\n2\n\n≤\n\n1 C2\n\nC1k0 + k0∆π\n\n(cid:17)\n\n≤(cid:112)fKC1k0 +\n\n1 C2\n\nk0∆π\n\n≤0.3k0∆π .\n\n.\n\nThe last inequality is due to k0 ≥ 64fK C1 So we have:\n\n∆2\n\nmin\n\nˆ∆π ∈ [0.7∆π, 1.3∆π] , ∀π ̸= π∗ .\n\nLemma 8. With probability at least 1 − δ, phase 2 never ends.\n\nProof. For equation 9, we decompose it as\n\nk (cid:88)\n\ns=k0+1\n\nˆV ˆπ\n\ns − Ys =\n\nk (cid:88)\n\n(cid:16)\n\nV ˆπ\n\ns − V πs\n\ns\n\n(cid:17)\n\n+\n\n(cid:32)\n\nV πs\n\ns − Ys +\n\nYs − V ˆπ s\n ̃ps(ˆπ)\n\n(cid:33)\n\n1{πs = ˆπ}\n\ns=k0+1 (cid:32)\n\n+\n\nV ˆπ s\n ̃ps(ˆπ)\n\n(cid:33)\n\n1{πs = ˆπ} − V ˆπ\n\ns\n\n.\n\nFirst we deal with the second term, which is a martingale difference sequence. It’s variance is bounded as:\n\n\n\n(cid:32)\n\nE\n\n\n\nV πs\n\ns − Ys +\n\nYs − V ˆπ s\n ̃ps(π)\n\n1{πs = ˆπ}\n\n(cid:33)2 \n\n(cid:34)\n\n= ̃ps(ˆπ)E\n\n(V πs\n\ns − Ys)2\n\n(cid:19)2(cid:35)\n\n(cid:18)\n\n1 −\n\n1 ̃ps(ˆπ)\n\n+ (cid:0)1 − ̃ps(ˆπ)(cid:1) E\n\n(cid:104)\n\ns − Ys)2(cid:105)\n\n(V πs\n\n≤2H 2 (cid:0)1 − ̃ps(ˆπ)(cid:1) .\n\nThe third term is also a martingale difference sequence, whose variance can be bounded as:\n\n\n\n(cid:32)\n\nE\n\n\n\nV ˆπ s\n ̃ps(ˆπ)\n\n1{πs = ˆπ} − V ˆπ\n\ns\n\n(cid:33)2\n\n ≤ 2H 2 (cid:0)1 − ̃ps(ˆπ)(cid:1) .\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nThus, using the Freedman inequality for the second and third term, we got:\n\n(cid:32)\n\nk (cid:88)\n\nV πs\n\ns − Ys +\n\nYs − V ˆπ s\n ̃ps(ˆπ)\n\n(cid:33)\n\n(cid:32)\n\n1{πs = ˆπ}\n\n+\n\nV ˆπ s\n ̃ps(ˆπ)\n\n(cid:33)\n\n1{πs = ˆπ} − V ˆπ\n\ns\n\ns=k0+1 (cid:118) (cid:117) (cid:117) (cid:116)\n\n≤8H\n\nk (cid:88)\n\ns=k0+1\n\n(cid:0)1 − ̃ps (ˆπ)(cid:1) log\n\n(cid:19)\n\n(cid:18) K δ\n\n+ 6H log\n\n(cid:19)\n\n.\n\n(cid:18) K δ\n\nFor the first term, we bound it’s variance against it’s expectation (cid:80)\n\nπ̸=ˆπ ̃ps(π) (cid:0)V ˆπ\n\ns − V π\n\ns\n\n(cid:1) as follows:\n\n\n\n \n\n V ˆπ\n\ns − V πs\n\ns −\n\nE\n\n(cid:16)\n\n ̃ps(π)\n\n(cid:88)\n\nπ̸=ˆπ\n\nV ˆπ\n\ns − V π\n\ns\n\n(cid:17)\n\n2\n\n\n\n\n\n \n\n= ̃ps (ˆπ) E\n\n\n\n\n\n \n\n\n\n(cid:88)\n\nπ̸=ˆπ\n\n(cid:16)\n\n ̃ps(π)\n\n(cid:17)\n\n2\n\n\n\n\n\n\n\n + (cid:0)1 − ̃ps (ˆπ)(cid:1) E\n\nV ˆπ\n\ns − V π\n\ns\n\n\n\n \n\n V ˆπ\n\ns − V πs̸=ˆπ\n\ns\n\n(cid:88)\n\n−\n\n ̃ps(π)\n\n(cid:16)\n\nπ̸=ˆπ\n\nV ˆπ\n\ns − V π\n\ns\n\n(cid:17)\n\n\n\n\n\n2\n\n \n\n≤ ̃ps (ˆπ) H 2 (cid:0)1 − ̃ps (ˆπ)(cid:1)2 ≤4H 2 (cid:0)1 − ̃ps (ˆπ)(cid:1) . Using Freedman inequality for the martingale difference sequence above:\n\n+ (cid:0)1 − ̃ps (ˆπ)(cid:1) H 2 (cid:0)2 − ̃ps (ˆπ)(cid:1)2\n\nk (cid:88)\n\ns=k0+1\n\nV ˆπ\n\ns − V πs\n\ns\n\nk (cid:88)\n\n(cid:88)\n\n(cid:16)\n\n ̃ps(π)\n\nV ˆπ\n\ns − V π\n\ns\n\n≤\n\n≤\n\ns=k0+1\n\nπ̸=ˆπ\n\nk (cid:88)\n\n(cid:88)\n\ns=k0+1\n\nπ̸=ˆπ\n\n(cid:17)\n\n+ 4H\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nk (cid:88)\n\ns=k0+1\n\n(cid:0)1 − ̃ps (ˆπ)(cid:1) log\n\n(cid:19)\n\n(cid:18) K δ\n\n+ 2H log\n\n(cid:19)\n\n(cid:18) K δ\n\n ̃ps(π) (∆π − ∆ˆπ) + 4H\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nk (cid:88)\n\ns=k0+1\n\n(cid:0)1 − ̃ps (ˆπ)(cid:1) log\n\n(cid:19)\n\n(cid:18) K δ\n\n+ 2H log\n\n(cid:19)\n\n(cid:18) K δ\n\n≤\n\n1 2\n\nk (cid:88)\n\n(cid:88)\n\ns=k0+1\n\nπ̸=ˆπ\n\nps(π)∆π + 4H\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nk (cid:88)\n\ns=k0+1\n\n(cid:0)1 − ̃ps (ˆπ)(cid:1) log\n\n(cid:19)\n\n(cid:18) K δ\n\n+ log\n\n(cid:19)\n\n,\n\n(cid:18) K δ\n\nwhere the last inequality is conditioned on that π∗ = ˆπ. Using Lemma 7 and equation 31, we have:\n\n1 2\n\n(cid:88)\n\nπ̸=ˆπ\n\nps(π)∆π ≤\n\n1 − ̃ps (ˆπ) ≤\n\n36dHβs ∆mins\n\n12dHβs ˆ∆2 mins\n\n,\n\n.\n\nFinally, we have:\n\nk (cid:88)\n\ns=k0+1\n\nˆV ˆπ\n\ns − Ys\n\n≤\n\n36dHβk log(k) ∆min\n\n+ 12H\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nk (cid:88)\n\ns=k0+1\n\n12dHβs ˆ∆2 mins\n\nlog\n\n(cid:19)\n\n(cid:18) K δ\n\n+ 8H log\n\n(cid:19)\n\n(cid:18) K δ\n\n≤\n\n52dHβk log(k) ˆ∆min\n\n≤20dHβK log(k)\n\n≤20(cid:112)fKC1k0 ,\n\ndH log(k) ˆ∆min\n\nβK√ 215\n\n+ 72\n\n(cid:115)\n\nk0 fKC1\n\n19\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere inequality 21 is due to claim 3 that k0 ≥ 64fK C1 [0.7∆, 1.3∆] which is claim 4; and fKC1 ≥ dH 2βK log(k). So equation 9 is never satisfied. According to equation 28 and equation 30, we have:\n\n∆2\n\nmin\n\n≥ 37fK C1 ˆ∆2\n\nmin\n\n, inequality 20 is due to ˆ∆ ∈\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nV π\n\ns −\n\nk0(cid:88)\n\ns=1\n\nˆV π\n\ns − (k − k0)Robk,π\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n1.4k ˆ∆π 10\n\n,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nˆV ˆπ\n\ns − V ˆπ\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 2(cid:112)fKC1k ≤ 0.1k ˆ∆π .\n\nWhere the last inequality is due to equation 27. So:\n\n(cid:12) (cid:12)\n\n(cid:12)k ˆ∆k,π − k∆π\n\n(cid:12) (cid:12) (cid:12) ≤\n\n(cid:12) (cid:12) k\n(cid:88) (cid:12) (cid:12) (cid:12) (cid:12)\n\ns=1\n\nV π\n\ns −\n\nk0(cid:88)\n\ns=1\n\nˆV π\n\ns − (k − k0)Robk,π\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\n+\n\nThus:\n\nˆV ˆπ\n\ns − V ˆπ\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 0.24k ˆ∆π .\n\nˆ∆k,π ≤ ∆π + 0.24 ˆ∆π ≤\n\nˆ∆k,π ≥ ∆π − 0.24 ˆ∆π ≥\n\n1 0.7 1\n1.3\n\nˆ∆π + 0.24 ˆ∆π ≤ 1.81 ˆ∆π ,\n\nˆ∆π − 0.24 ˆ∆π ≥ 0.39 ˆ∆π .\n\nSo equation 8 is never satisfied.\n\nProof of Theorem 2 . Now, we bound the deviation between the actual regret and the real regret in phase 2. Using Freedman inequality on the martingale difference sequence ∆πs − (cid:80) (cid:118) (cid:117) (cid:117) (cid:116)log\n\n ̃ps(π)∆π + 2\n\nπ ̃ps(π)∆π:\n\n(cid:3) + H log\n\nE (cid:2)∆2\n\n(cid:19) k\n\nk (cid:88)\n\nk (cid:88)\n\n(cid:88)\n\n(cid:88)\n\n∆πs ≤\n\n(cid:19)\n\nπs\n\n(cid:18) 1 δ\n\n(cid:18) 1 δ\n\ns=k0+1\n\ns=k0+1\n\ns=k0+1\n\nπ\n\nk (cid:88)\n\n(cid:88)\n\ns=k0+1\n\nπ\n\nk (cid:88)\n\n(cid:88)\n\n≤\n\n≤\n\n ̃ps(π)∆π + 2\n\n ̃ps(π)∆π + 2\n\n(cid:118) (cid:117) (cid:117) (cid:116)log\n\n(cid:118) (cid:117) (cid:117) (cid:116)log\n\n(cid:19)\n\n(cid:18) 1 δ\n\nH\n\n(cid:19)\n\n(cid:18) 1 δ\n\nH\n\nk (cid:88)\n\ns=k0+1\n\nE [∆πs] + H log\n\n(cid:19)\n\n(cid:18) 1 δ\n\nk (cid:88)\n\n(cid:88)\n\ns=k0+1\n\nπ\n\n ̃ps(π)∆π + H log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n(22)\n\ns=k0+1\n\nπ\n\nk (cid:88)\n\n(cid:88)\n\n≤2\n\ns=k0+1\n\nπ\n\n ̃ps(π)∆π + 2H log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n.\n\n.\n\nDenote M be the episode that first satisfy M ≥ 48dHβM For k ≥ M , we have:\n\n∆2\n\nmin\n\nk (cid:88)\n\n(cid:88)\n\ns=M\n\nπ\n\n ̃ps(π)∆π =\n\nk (cid:88)\n\n(cid:88)\n\ns=M\n\nπ\n\n1 2\n\nps(π)∆π\n\n≤\n\nk (cid:88)\n\ns=M\n\n36dHβs ∆mins (cid:18) dHβk log(k) ∆min\n\nwhere the inequality is due to Lemma 7 with r = 3.\n\n≤O\n\n20\n\n(23)\n\n(cid:19)\n\n,\n\nUnder review as a conference paper at ICLR 2023\n\nFor k < M , according to Lemma 5 we have:\n\nM (cid:88)\n\n(cid:88)\n\n ̃ps(π)∆π =\n\nM (cid:88)\n\n(cid:88)\n\ns=k0+1\n\nπ\n\ns=k0+1\n\nπ\n\n1 2\n\nps(π)∆π\n\nM (cid:88)\n\n≤\n\nO\n\n(cid:19)\n\n(cid:18) dHβs√\n\ns\n\ns=k0+1 (cid:16)\n\n≤O\n\ndHβM\n\n√\n\n(cid:17)\n\n.\n\nM\n\nDuring phase 1, we have:\n\nk0(cid:88)\n\ns=1\n\nV π∗\n\ns − V πs\n\ns ≤\n\n(cid:115)\n\nC1\n\n900fKC1\n\n∆2\n\nmin\n\n≤ O\n\n(cid:32)\n\nC1\n\n(cid:112)log(K) ∆min\n\n(cid:33)\n\n.\n\nSince we condition on that phase 2 never ends, we have:\n\n∆πs ≤O\n\n(cid:32)\n\nC1\n\n(cid:33)\n\n(cid:112)log(K) ∆min\n\n+ O\n\n(cid:18) dHβK log(K) ∆min\n\n(cid:19)\n\nK (cid:88)\n\ns=1\n\n(cid:16)\n\n+ O\n\ndHβM\n\n√\n\n(cid:17)\n\nM\n\n\n\n \n\n≤O\n\ndH 2 log(K) log\n\n∆min\n\n(cid:16) |Π|K δ\n\n\n\n(cid:17)\n\n  .\n\nC ANALYSIS IN THE ADVERSARIAL SETTING\n\nProof of Lemma 2. First, we bound the deviation of estimation in Phase 1. For any episode k in phase 1, we have:\n\nk (cid:88)\n\ns=1\n\nV π\n\ns − V πs\n\ns ≤\n\n(cid:112)\n\nC1K − C2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nˆV π\n\ns − V π\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:112)\n\nC1K − (C2 − 1)\n\n≤\n\nˆV π\n\ns − V π\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\n(cid:16)\n\n+\n\ns=1\n\nV π\n\ns − ˆV π\n\ns\n\n(cid:17)\n\n.\n\nThus:\n\nAt time k0:\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nˆV π\n\ns − V π\n\ns\n\n≤\n\n1 C2 − 1\n\n\n\n(cid:112)\n\n\n\nC1K +\n\nk (cid:88)\n\ns=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nV πs\n\ns − ˆV π\n\ns\n\n\n\n .\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk0(cid:88)\n\ns=1\n\nˆV π\n\ns − V π\n\ns\n\n≤\n\n1 C2 − 1\n\n\n\n(cid:112)\n\n\n\nC1K +\n\nk0(cid:88)\n\ns=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nV πs\n\ns − ˆV π\n\ns\n\n\n\n\n\n\n\n2\n\n\n\n2\n\n(cid:112)\n\nC1K +\n\n\n\nYs − ˆV π\n\ns\n\n\n\nk0(cid:88)\n\ns=1\n\n(cid:112)\n\nC1K + 5(cid:112)fKC1k0 +\n\nk0(cid:88)\n\ns=1\n\n(24)\n\n\n\n\n\nˆV ˆπ\n\ns − ˆV π\n\ns\n\n≤\n\n1 C2 − 1\n\n1 C2 − 1\n\n≤\n\n≤\n\n(cid:16)\n\n1 C2 − 1\n\n7(cid:112)fKC1k0 + k0 ˆ∆π\n\n(cid:17)\n\n.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nNext, we bound the deviation of (k − k0)Robk,π for π ̸= ˆπ. The variance of ˆV π κ is bounded as follows:\n\n(cid:20)(cid:16) ˆV π\n\nκ\n\nE\n\n(cid:17)2(cid:21)\n\n=E\n\n\n\n\n\n \n\n\n\nH (cid:88)\n\n(cid:16)\n\nh=1\n\nˆrπ\n\nκ,h\n\n(cid:17)\n\n2\n\n\n\n\n\n \n\nH (cid:88)\n\nE\n\n(cid:20)(cid:16)\n\nˆrπ\n\nκ,h\n\n(cid:17)2(cid:21)\n\n≤H\n\n≤ H\n\nH (cid:88)\n\nh=1\n\n≤2H\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nκ,h\n\nUsing the properties of the Catoni estimator, we have:\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\nκ=k0\n\nV π\n\nκ − (k − k0)Robk,π\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:13) (cid:13)φπ,h\n\n(cid:13) 2\n(cid:13) ̃Σ−1\n\nκ,h\n\nh=1 (cid:32)\n\nκ ˆ∆2 π\nβκ\n\n≤ 2H\n\n(cid:33)\n\n+ 4dH\n\n.\n\nk (cid:88)\n\n≤απ\n\nk\n\nκ=k0+1\n\n\n\nE\n\n \n\n(cid:20)(cid:16) ˆV π\n\nκ − V π\n\nκ\n\n(cid:17)2(cid:21)\n\n V π\n\nκ −\n\n+\n\n1 k − k0\n\nk (cid:88)\n\nκ=k0\n\n2\n\n\n\nV π\n\nκ\n\n\n\n  +\n\n2 log\n\n(cid:17)\n\n(cid:16) k2|Π| δ\n\nαπ k\n\nk (cid:88)\n\n≤απ\n\nk\n\nκ=k0+1\n\n(cid:32)\n\nH\n\n2\n\nκ ˆ∆2 π\nβκ\n\n(cid:33)\n\n+ 9dH\n\n+\n\n(cid:118) (cid:117) (cid:117) (cid:116)H log\n\n≤4\n\n(cid:18) k|Π| δ\n\n(cid:19) k\n\n(cid:88)\n\nκ=k0+1\n\n(cid:32)\n\n2\n\nκ ˆ∆2 π\nβκ\n\n4 log\n\n(cid:17)\n\n(cid:16) k|Π| δ\n\nαπ k\n\n(cid:33)\n\n+ 9dH\n\n.\n\nWhere equation 26 is by our choice of απ k . Since equation 3 and equation 4 provides that:\n\nˆ∆π =\n\n1 k0\n\nk0(cid:88)\n\ns=1\n\nˆV ˆπ\n\ns − ˆV π\n\ns ≥ 20\n\n(cid:114) fKC1 k0\n\n(cid:115)\n\n= 20\n\nfKβKdH 2 k0\n\n,\n\nwe have 9dH ≤ k0 ˆ∆2 Thus we have:\n\nβK\n\nπ\n\n≤ 2κ ˆ∆2\n\nπ\n\nβκ\n\n.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\nκ=k0\n\nV π\n\nk − (k − k0)Robk,π\n\n(cid:118) (cid:117) (cid:117) (cid:116)H log\n\n≤4\n\n(cid:18) k|Π| δ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:19) k\n\n(cid:88)\n\nκ=k0+1\n\n4\n\nκ ˆ∆2 π\nβκ\n\n(cid:18) k|Π| δ\n\n(cid:19) 4k βk\n\nk (cid:88)\n\nˆ∆2\n\nπ\n\nκ=k0+1\n\n(cid:118) (cid:117) (cid:117) (cid:116)H log\n\n≤4\n\n≤\n\n1 16\n\nk ˆ∆π .\n\nCombining terms, we have:\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nV π\n\ns −\n\nk0(cid:88)\n\ns=1\n\nˆV π\n\ns − (k − k0)Robk,π\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n1.4k ˆ∆π 10\n\n.\n\nFinally, we bound the deviation of ˆV ˆπ. In the first k0 episodes, since ˆ∆ˆπ = 0, according to equation 24, we have that:\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk0(cid:88)\n\ns=1\n\nV ˆπ\n\ns − ˆV ˆπ\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ (cid:112)fkC1k0 .\n\n22\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\nUnder review as a conference paper at ICLR 2023\n\nIn phase 2, since E\n\n(cid:105)\n\n(cid:104) ˆV ˆπ\n\nk\n\n= V ˆπ\n\nFreedman inequality, and that E\n\nk is an unbiased estimator of the true value function, according to (cid:20)(cid:16) ˆV ˆπ\n\n ̃pk(ˆπ) ≤ 2H 2, we have:\n\n1{πk = ˆπ}\n\n(cid:104) Y 2 ̃p2\n\n≤ H 2\n\n= E\n\n(cid:17)2(cid:21)\n\n(cid:105)\n\nk\n\nk\n\nk(ˆπ)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=k0+1\n\nˆV ˆπ\n\ns − V ˆπ\n\ns\n\n(cid:114)\n\n≤ 2\n\n2H 2k log\n\nk|Π| δ\n\n+ 2H log\n\nk|Π| δ\n\n(cid:112)\n\n≤\n\nC1k .\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nCombining the two terms, we have:\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nk (cid:88)\n\ns=1\n\nIn sum,\n\nˆV ˆπ\n\ns − V ˆπ\n\ns\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 2(cid:112)fKC1k .\n\n(30)\n\nk (cid:88)\n\ns=1\n\nk−1 (cid:88)\n\ns=1\n\nk0(cid:88)\n\ns=1\n\n≥\n\n≥\n\nV ˆπ\n\ns − V π\n\ns\n\nV ˆπ\n\ns − V π\n\ns − 2H\n\n(cid:16) ˆV ˆπ\n\ns − ˆV π\n\ns\n\n(cid:17)\n\n\n\n+\n\n\n\nk−1 (cid:88)\n\nˆV ˆπ\n\ns − (k − k0 − 1)Robk−1,π\n\n − 2(cid:112)fKC1(k − 1) −\n\n\n\n≥(k − 1) ˆ∆k−1,π −\n\ns=k0+1\n\n3(k − 1) ˆ∆π 10\n\n≥ 0 ,\n\n1.4(k − 1) ˆ∆π 10\n\n− 2H\n\nwhere the second to last inequality is due to equation 27, and the last one is due to equation 8.\n\nProof of Theorem 3. Finally, we proof the regret bound in adversarial setting. For the regret in phase 2, we can decompose it as follows.\n\nk (cid:88)\n\ns=k0+1\n\nk (cid:88)\n\ns=k0+1\n\n=\n\nV ˆπ\n\ns − V πs\n\ns\n\n(cid:16) ˆV ˆπ\n\ns − Ys\n\n(cid:17)\n\n+\n\n(cid:16)\n\nYs1{πs = ˆπ} + V ˆπ\n\ns\n\n1{πs ̸= ˆπ} − ˆV ˆπ\n\ns\n\n(cid:17)\n\n(cid:20)(cid:16)\n\n+\n\nV ˆπ\n\ns\n\n1{πs = ˆπ} − Ys1{πs = ˆπ}\n\n(cid:17)\n\n(cid:21)\n\n+ (Ys − V πs s )\n\n.\n\nThe first term is bounded by equation 9:\n\nk (cid:88)\n\ns=k0+1\n\nˆV ˆπ\n\ns − Ys ≤ O\n\n(cid:16)(cid:112)fKC1k0\n\n(cid:17)\n\n.\n\nThe second term is a martingale difference sequence since:\n\n(cid:104)\n\nE\n\n1{πs ̸= ˆπ} − ˆV ˆπ Ys1{πs = ˆπ} + V ˆπ s\n(cid:21)\n\n(cid:20)\n\ns\n\n= ̃ps (ˆπ) E\n\nYs −\n\n+ (cid:0)1 − ̃ps (ˆπ)(cid:1) V ˆπ\n\ns\n\n(cid:105)\n\nYs ̃ps (ˆπ) (cid:19) 1\n ̃ps (ˆπ)\n\n(cid:18)\n\n= ̃ps (ˆπ)\n\n1 −\n\n=0 .\n\nE [Ys] + (cid:0)1 − ̃ps (ˆπ)(cid:1) V ˆπ\n\ns\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nIt’s variance is bounded as:\n\n(cid:20)(cid:16)\n\nE\n\nYs1{πs = ˆπ} + V ˆπ\n\ns\n\n1{πs ̸= ˆπ} − ˆV ˆπ\n\ns\n\n(cid:17)2(cid:21)\n\n= ̃ps (ˆπ) E\n\n(cid:34)(cid:18)\n\nYs −\n\nYs ̃ps (ˆπ)\n\n(cid:19)2(cid:35)\n\n+ (cid:0)1 − ̃ps (ˆπ)(cid:1) (cid:16)\n\nV ˆπ\n\ns\n\n(cid:17)2\n\n(cid:18)\n\n1 −\n\n≤ ̃ps (ˆπ)\n\n1 ̃ps (ˆπ) ≤2H 2 (cid:0)1 − ̃ps (ˆπ)(cid:1) ,\n\n(cid:19)2\n\nH 2 + (cid:0)1 − ̃ps (ˆπ)(cid:1) H 2\n\nwhere the last inequality is due to ̃ps (ˆπ) ≥ 1 2 . The third term is also a martingale difference sequence, whose variance is bounded as:\n\n(cid:34)(cid:18)(cid:16)\n\nE\n\nV ˆπ\n\ns\n\n1{πs = ˆπ} − Ys1{πs = ˆπ}\n\n(cid:17)\n\n+ (Ys − V πs s )\n\n(cid:19)2(cid:35)\n\n≤ (cid:0)1 − ̃ps (ˆπ)(cid:1) E ≤ (cid:0)1 − ̃ps (ˆπ)(cid:1) 4H 2 .\n\n(cid:104)\n\n(Ys − V πs\n\ns )2(cid:105)\n\nAlso, we have:\n\n1 − ̃ps (ˆπ) ≤\n\n1 2\n\nps (ˆπ) =\n\n1 2\n\n(cid:88)\n\nπ̸=ˆπ\n\nps (π) ≤\n\n1 2\n\n(cid:88)\n\nπ̸=ˆπ\n\nps (π)\n\nˆ∆π ˆ∆min\n\n≤\n\n12dHβs ˆ∆2 mins\n\n.\n\n(31)\n\nThus, using Freedman inequality on the last two terms, we got:\n\nV ˆπ\n\ns − V πs\n\ns\n\nk (cid:88)\n\ns=k0+1 \n\n≤O\n\n(cid:112)fKC1k0 +\n\n \n\n\n\n≤O\n\n(cid:112)fKC1k0 +\n\n \n\n\n(cid:16)(cid:112)fKC1k0\n\n(cid:17)\n\n.\n\n≤O\n\n(cid:118) (cid:117) (cid:117) (cid:116)log\n\nk δ\n\nH 2\n\nk (cid:88)\n\ns=1\n\ndHβs ˆ∆2 mins\n\n+ H log\n\n\n\n \n\nk δ\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nlog\n\n(cid:16) k\n\n(cid:17)\n\nδ\n\ndH 3 log(k)βkk0\n\nfKC1\n\n+ H log\n\n\n\nk δ\n\n \n\n\n(32)\n\nCombining with the regret in phase 1, the regret in one epoch is bounded as:\n\nV ˆπ\n\ns − V πs\n\ns ≤ O\n\n(cid:16)(cid:112)fKC1k0\n\n(cid:17)\n\n.\n\nk (cid:88)\n\ns=1\n\nSince the duration time k0 of phase 1 is at least twice as the length of phase 1 in the previous epoch, k0 in all the epochs is bounded by an constant factor of the square root of we have that the sum of duration time in phase 1 of the last epoch, which is bounded by Summation over all the epochs, we have :\n\nK.\n\n√\n\n√\n\nReg (K; Π) = O\n\n(cid:16)(cid:112)C1K log(K)\n\n(cid:17)\n\n\n\n(cid:115)\n\n= O\n\n\n\ndH 3K log(K) log\n\n\n\n(cid:19)\n\n .\n\n(cid:18) |Π|K δ\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 4 Geometric Hedge for Linear Adversarial MDP Policies \n\n\n \n\ndH log\n\n(cid:114)\n\n(cid:17)\n\n(cid:16) |Π| δ\n\n1: Input: policy set Π, γ = min\n\n, η = γ\n\n4dH 2\n\n1\n\n2 ,\n\n\n\nK\n\n\n\n2: Initialize: ∀π ∈ Π, w1 (π) = 1, W1 := |Π|. ∀h from 1 to H, compute the G-optimal design\n\ngh(π) on the set of feature visitations: {φπ,h, π ∈ Π}. Denote g(π) = 1\n\n(cid:80)H\n\nh=1 gh(π)\n\nH\n\n3: for each episode k = 1 to K do 4:\n\n∀π ∈ Π,\n\npk (π) = (1 − γ)\n\nwk (π) Wk\n\n+ γg(π)\n\n5:\n\n6:\n\n7:\n\n8:\n\nSelect πk ∈ Π according to the probability pk(π) and collect rewards yk,h Calculate reward estimators: ˆθk,h = Σ−1 where Σk,h = (cid:80) Compute the optimistic estimate of the value function:\n\nk,hφπk,hyk,h, and ˆrπ\n\nπ pk(π)φπ,hφ⊤\n\nk,h = φ⊤\n\nπ,h\n\nπ,h\n\nˆθk,h, ˆV π\n\nk = (cid:80)H\n\nh=1 ˆrπ\n\nk,h,\n\n ̃V π\n\nk =\n\nH (cid:88)\n\nh=1\n\n\n\n\n\nˆrπ\n\nk,h + 2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\nH\n\n\n\n \n\nAnd we transform it into loss functions: lk,h(sh, ah) = 1 − rk,h(sh, ah), lπ\n\nk,h = 1 − rπ\n\nk,h\n\nLπ\n\nk,h =\n\nˆLπ\n\nk,h =\n\nH (cid:88)\n\nh=1\n\nH (cid:88)\n\nh=1\n\nlπ k,h = H − V π\n\nk,h\n\nˆlπ k,h =\n\nH (cid:88)\n\n(cid:16)\n\nh=1\n\n(cid:17)\n\n1 − ˆrπ\n\nk,h\n\n= H − ˆV π\n\nk,h\n\n ̃Lπ\n\nk,h = H − ̃V π\n\nk,h\n\n9:\n\nupdate using the loss estimators:\n\n∀π ∈ Π, wk+1(π) = wk(π) exp\n\n(cid:16)\n\n−η ̃Lπ\n\nk\n\n(cid:17)\n\n, Wk+1 =\n\n(cid:88)\n\nπ∈Π\n\nwk+1(π)\n\n10: end for\n\nD HIGH PROBABILITY GUARANTEE FOR ADVERSARIAL LINEAR MDP\n\nFirst, we bound the deviation between the estimated value and the true value of policy π.\n\nLemma 9. Denote DEVK,π =\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:80)K\n\nk=1\n\nˆV π\n\nk − V π\n\nk\n\n(cid:12) (cid:12) (cid:12), then we have:\n\nDEVK,π =\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:88)\n\nk=1\n\nˆV π\n\nk − V π\n\nk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:88)\n\nk=1\n\nˆLπ\n\nk − Lπ\n\nk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n1 C2\n\nK (cid:88)\n\nH (cid:88)\n\nk=1\n\nh=1\n\n(cid:13) (cid:13)φπ,h\n\n(cid:13) 2\n(cid:13) Σ−1\n\nk,h\n\n(cid:115)\n\n(cid:1)\n\nH log (cid:0) 1 dK\n\nδ\n\n(cid:115)\n\n+ C2\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2\n\n(cid:32)\n\ndH 2 γ\n\n(cid:33)\n\n+ H\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n.\n\nProof. First, we show that ˆV π\n\nk is an unbiased estimate of V π k :\n\nE\n\n(cid:105)\n\n(cid:104) ˆV π\n\nk\n\n=\n\nH (cid:88)\n\nh=1\n\nE\n\n(cid:105)\n\n(cid:104) ˆV π\n\nk,h\n\n=\n\nH (cid:88)\n\nh=1\n\n25\n\nφ⊤\n\nπ,hΣ−1\n\nk,h\n\nE (cid:2)φπk,hyk,h\n\n(cid:3) ,\n\nUnder review as a conference paper at ICLR 2023\n\nusing the tower rule of expectation, we have: E (cid:2)φπk,hyk,h\n\n(cid:104) φπk,hrπk\n\n(cid:3) = E\n\nk,h\n\n(cid:105)\n\n(cid:104)\n\n= E\n\nφπk,hφ⊤\n\nπk,hθk,h\n\n(cid:105)\n\n= Σk,hθk,h ,\n\nso\n\nE\n\n(cid:105)\n\n(cid:104) ˆV π\n\nk\n\n=\n\nH (cid:88)\n\nh=1\n\nφ⊤\n\nπ,hθk,h = V π k .\n\nDenote σ2 = (cid:80)K\n\nk=1 Var\n\n(cid:17)\n\n(cid:16) ˆV π\n\nk\n\n, then:\n\nK (cid:88)\n\nE\n\nσ2 ≤\n\n\n\n\n\n \n\n\n\nh=1\n\nH (cid:88)\n\nφ⊤\n\nπ,hΣ−1\n\nk,hφπk,hyk,h\n\n2\n\n\n\n\n\n \n\nH (cid:88)\n\nE\n\n(cid:20)(cid:16)\n\nH\n\nφ⊤\n\nπ,hΣ−1\n\nk,hφπk,hyk,h\n\n(cid:17)2(cid:21)\n\nk=1\n\nK (cid:88)\n\n≤\n\nk=1\n\nh=1\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\n≤H\n\nk=1\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nk,h\n\n.\n\nAlso, due to the properties of G-optimal design, we have:\n\n(cid:13) (cid:13)φπ,h\n\n(cid:13) 2\n(cid:13) (cid:16)(cid:80)\n\nπ gh(π)φπ,hφ⊤\n\nπ,h\n\n(cid:17)−1 ≤ d ,\n\n(cid:80)\n\nand Σk,h ⪰ γ Freedman inequality, the sum of the martingale difference sequence ˆV π\n\nπ gh(π)φπ,hφ⊤\n\nπ,h. Thus we have(cid:13)\n\n(cid:13) 2\n(cid:13) Σ−1\n\n(cid:13)φπ,h\n\n≤ dH\n\nk,h\n\nH\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:88)\n\nk=1\n\nˆV π\n\nk − V π\n\nk\n\n(cid:118) (cid:117) (cid:117) (cid:116)H\n\n≤ 2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\nk=1\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nk,h\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n(cid:32)\n\ndH 2 γ\n\nk ≤ dH 2 k is bounded as:\n\nγ , ∀π ∈ Π, so ˆV π k − V π (cid:33)\n\nγ . Using\n\n+ H\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n≤\n\n1 C2\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:13) (cid:13)φπ,h\n\nk=1\n\nh=1\n\n(cid:13) 2\n(cid:13) Σ−1\n\nk,h\n\n(cid:115)\n\n(cid:1)\n\nH log (cid:0) 1 dK\n\nδ\n\n(cid:115)\n\n+ C2\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2\n\n(cid:32)\n\ndH 2 γ\n\n(cid:33)\n\n+ H\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n,\n\nwhere the last inequality is due to the geometric mean-arithmetic mean inequality.\n\nLemma 10.\n\nLπk\n\nk −\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π) ˆLπ\n\nk\n\nK (cid:88)\n\n(cid:88)\n\n=\n\nk=1\n\nπ\n\npk(π) ˆV π\n\nk −\n\nK (cid:88)\n\nk=1\n\nV πk\n\nk\n\n(cid:16)√\n\n≤H\n\nd + 1\n\n(cid:115)\n\n(cid:17)\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n4 3\n\n(cid:32)\n\nH +\n\n(cid:33)\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n.\n\nProof.\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π) ˆV π\n\nk −\n\nK (cid:88)\n\nk=1\n\nV πk\n\nk =\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:32)\n\n(cid:88)\n\nk=1\n\nh=1\n\nπ\n\nUsing lemma 6 in Bartlett et al. (2008), we have:\n\npk(π)ˆrπ\n\nk,h − rπk\n\nk,h\n\n(cid:33)\n\n.\n\n(33)\n\nK (cid:88)\n\nrπk k,h −\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)ˆrπ\n\nk,h\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:16)√\n\n≤\n\nd + 1\n\n(cid:115)\n\n(cid:17)\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n(cid:18) dH γ\n\n4 3\n\n(cid:19)\n\n+ 1\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n.\n\nk=1 (cid:12) (cid:12) (cid:12)\n\nSince\n\nˆθ⊤ k,hφπ,h\n\n(cid:12) (cid:12) ≤ dH (cid:12) γ .\n\nPlugging it into equation 33, we finish the proof.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 11. With probability at least 1 − δ, we have :\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ̃Lπ\n\nk\n\n≤ 2(d + 1)KH 2 + 2\n\n(cid:115)\n\ndH 3 γ\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n(cid:1)\n\n8dH 3 log (cid:0) 1 γ\n\nδ\n\n.\n\nProof. Since ˆLπ\n\nk = H − ˆV π k ,\n\n(cid:17)2\n\n(cid:16) ˆLπ\n\nk\n\n(cid:17)2\n\n(cid:16) ˆV π\n\nk\n\n≤\n\n+ H 2, we have:\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ˆLπ\n\nk\n\n≤\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ˆV π\n\nk\n\n+ KH 2 .\n\nUsing Cauchy-Schwarz inequality, we have:\n\nSo,\n\n(cid:17)2\n\n(cid:16) ˆV π\n\nk\n\n≤ H\n\nH (cid:88)\n\n(cid:16)\n\n(cid:17)2\n\n.\n\nˆrπ\n\nk,h\n\nh=1\n\nK (cid:88)\n\n(cid:88)\n\npt(π)\n\n(cid:17)2\n\n(cid:16)\n\nˆrπ\n\nk,h\n\nπ\n\n(cid:88)\n\nπ\n\n(cid:88)\n\nπ\n\npt(π)ˆθ⊤\n\nk,hφπ,hφ⊤\n\nπ,h\n\nˆθk,h\n\nˆθ⊤ k,hΣk,h\n\nˆθk,h\n\nφ⊤\n\nπk,hΣ−1\n\nk,hφπk,h .\n\nk=1\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\n=\n\n=\n\n≤\n\nNotice that using the definition of Σk,h and properties of the G-optimal design,\n\nE\n\n(cid:104)\n\nφ⊤\n\nπk,hΣ−1\n\nk,hφπk,h\n\n(cid:105)\n\n= d ,\n\nφ⊤\n\nπk,hΣ−1\n\nk,hφπk,h ≤\n\ndH γ\n\n.\n\nApplying the Hoeffding bound, we got:\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npt(π)\n\n(cid:17)2\n\n(cid:16)\n\nˆrπ\n\nk,h\n\n≤ dK +\n\n(cid:115)\n\ndH γ\n\n2K log\n\n(cid:19)\n\n.\n\n(cid:18) 1 δ\n\nThus:\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ˆV π\n\nk\n\nH (cid:88)\n\nK (cid:88)\n\n(cid:88)\n\n≤H\n\npt(π)\n\n(cid:17)2\n\n(cid:16)\n\nˆrπ\n\nk,h\n\nh=1\n\nk=1\n\nπ\n\n≤dKH 2 +\n\n(cid:115)\n\ndH 3 γ\n\n2K log\n\n(cid:19)\n\n.\n\n(cid:18) 1 δ\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nAnd:\n\n(cid:17)2\n\n(cid:16) ̃Lπ\n\nk\n\n=\n\n\n\n \n\nˆLπ\n\nk −\n\nH (cid:88)\n\nh=1\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\nH\n\n 2\n\n \n\n(cid:17)2\n\n(cid:16) ˆLπ\n\nk\n\n≤2\n\n+ 2\n\n\n\n \n\n(cid:17)2\n\n(cid:16) ˆLπ\n\nk\n\n≤2\n\n+ 2H\n\nH (cid:88)\n\nh=1\n\nH (cid:88)\n\nh=1\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\nH\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\n 2\n\n \n\n4(cid:13) (cid:13)φπ,h\n\n(cid:13) 2\n(cid:13) Σ−1\n\nk,h\n\nφ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:1)\n\nH log (cid:0) 1 dK\n\nδ\n\n(cid:17)2\n\n(cid:16) ˆV π\n\nk\n\n≤2\n\n+ 2H 2 + 8\n\ndH γ\n\nH (cid:88)\n\nh=1\n\nSo we conclude that:\n\nφ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:1)\n\nH log (cid:0) 1 dK\n\nδ\n\n.\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ̃Lπ\n\nk\n\n≤2\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ˆLπ\n\nk\n\n+\n\n8H 2 log 1 δ\nγK\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nk=1\n\nh=1\n\nπ\n\npk(π)φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n≤2(d + 1)KH 2 + 2\n\n(cid:115)\n\n2K log\n\ndH 3 γ\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n(cid:1)\n\n8dH 3 log (cid:0) 1 γ\n\nδ\n\n.\n\nProof of Theorem 1 . Now we analyze the potential function. Using classical techniques, we got that counter part of equation (2) in Bartlett et al. (2008):\n\nlog\n\n(cid:19)\n\n(cid:18) Wk Wk−1\n\nlog\n\n(cid:19)\n\n(cid:18) WK W1\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\n=\n\n=\n\n≤\n\n≤\n\nk=1\n\nπ\n\nlog\n\nlog\n\n(cid:88)\n\n(cid:32)\n\n(cid:88)\n\nπ\n\n(cid:32)\n\n(cid:88)\n\nπ\n\nwk(π) Wk−1\n\nexp\n\n(cid:16)\n\n−η ̃Lπ\n\nk\n\n(cid:33)\n\n(cid:17)\n\n(cid:18)\n\npk(π) − γgπ 1 − γ\n\n1 − η ̃Lπ\n\nk + η2 (cid:16) ̃Lπ\n\nk\n\n(cid:17)2(cid:19)(cid:33)\n\n(34)\n\n(cid:18)\n\npk(π) − γgπ 1 − γ\n\n−η ̃Lπ\n\nk + η2 (cid:16) ̃Lπ\n\nk\n\n(cid:17)2(cid:19)\n\n≤\n\nη 1 − γ\n\n\n\n\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\n−pk(π) ̃Lπ\n\nk + γ\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\ng(π) ̃Lπ\n\nk + η\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\npk(π)\n\n(cid:17)2\n\n(cid:16) ̃Lπ\n\nk\n\n\n\n , (35)\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:12) (cid:12)\n\n(cid:12)η ̃Lπ\n\nk\n\n(cid:12) (cid:12) (cid:12) ≤ 1.\n\nwhere inequality 34 is due to the constraint that\n\nUsing Lemma 10, we have:\n\nK (cid:88)\n\n(cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\nπ\n\n−pk(π) ̃Lπ\n\nk\n\npk(π)φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\nH\n\n−pk(π) ˆLπ\n\nk + 2\n\nK (cid:88)\n\nH (cid:88)\n\n(cid:88)\n\nh=1\n\nk=1 (cid:115)\n\nπ\n\n−pk(π) ˆLπ\n\nk + 2H\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\nπ\n\n=\n\n=\n\n≤\n\n−Lπk\n\nk + H\n\n(cid:16)√\n\nd + 1\n\n(cid:115)\n\n(cid:17)\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n4 3\n\n(cid:32)\n\nH +\n\n(cid:33)\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n(cid:115)\n\n+ 2H\n\ndKH log\n\n(cid:19)\n\n.\n\n(cid:18) 1 δ\n\nK (cid:88)\n\nk=1\n\nUsing Lemma 9 and choosing C2 = 1\n\n2 , we have:\n\nK (cid:88)\n\nk=1\n\n ̃Lπ\n\nk ≤\n\n≤\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\nLπ\n\nk + DEVk,π −\n\nK (cid:88)\n\nH (cid:88)\n\nk=1\n\nh=1\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\nLπ\n\nk +\n\n1 2\n\n(cid:115)\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2\n\n(cid:32)\n\ndH 2 γ\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\n(cid:115)\n\nH\n\n(cid:33)\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ H\n\nlog\n\n≤KH +\n\n(cid:115)\n\n1 2\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2\n\n(cid:32)\n\ndH 2 γ\n\n(cid:33)\n\n+ H\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n.\n\nThus equation 35 becomes:\n\nlog\n\n(cid:19)\n\n(cid:18) WK W1\n\n≤\n\nη 1 − γ\n\n\n\n−\n\nK (cid:88)\n\nk=1\n\n(cid:16)√\n\nd + 1\n\n(cid:115)\n\n(cid:17)\n\n2K log\n\nLπk\n\nk + H\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n4 3\n\n(cid:32)\n\nH +\n\n(cid:33)\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n(cid:115)\n\n+ 2H\n\ndKH log\n\n(cid:19)\n\n\n\n\n\n(cid:18) 1 δ\n\n+ 2ηγKH + η\n\n(cid:115)\n\ndKH log\n\n\n\n\n\n1 2\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2\n\n(cid:32)\n\ndH 2 γ\n\n(cid:33)\n\n+ H\n\nlog\n\n(cid:19)\n\n\n\n\n\n(cid:18) 1 δ\n\n 2(d + 1)KH 2 + 2\n\n+ 2η2\n\n(cid:115)\n\ndH 3 γ\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n8dH 3 log (cid:0) 1 γ\n\nδ\n\n(cid:1)\n\n\n\n\n\n\n\n≤η\n\n−\n\nK (cid:88)\n\nk=1 \n\n+ 2η\n\nH\n\n  + 2η2\n\n 2(d + 1)KH 2 + 2\n\nLπk\n\nk\n\n(cid:115)\n\ndH 3 γ\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n8dH 3 log (cid:0) 1 γ\n\nδ\n\n(cid:1)\n\n\n\n\n\n(cid:16)√\n\nd + 1\n\n(cid:115)\n\n(cid:17)\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n4 3\n\n(cid:32)\n\nH +\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n(cid:115)\n\n+ 2H\n\ndKH log\n\n(cid:19)\n\n\n\n\n\n(cid:18) 1 δ\n\n(cid:33)\n\n(cid:33)\n\n+ H\n\nlog\n\n\n\n(cid:19)\n\n .\n\n(cid:18) 1 δ\n\n(36)\n\n+ 2ηγKH + η\n\n\n\n\n\n1 2\n\n(cid:115)\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2\n\n(cid:32)\n\ndH 2 γ\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nAnd we also have for ∀π ∈ Π,\n\nlog\n\n(cid:19)\n\n(cid:18) WK W1\n\n\n\n≥η\n\n\n\n\n\n \n\n≥η\n\nK (cid:88)\n\nk=1\n\nK (cid:88)\n\nk=1\n\n  − log (cid:0)|Π|(cid:1)\n\n− ̃Lπ\n\nk\n\n−Lπ\n\nk − DEVK,π +\n\nK (cid:88)\n\nH (cid:88)\n\nk=1\n\nh=1\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\nH\n\n\n\n\n\n − log (cid:0)|Π|(cid:1) ,\n\n(37)\n\nwhere the last inequality is due to Lemma 9. Plugging equation 36 and equation 37 together, we have that for ∀π ∈ Π:\n\nK (cid:88)\n\nk=1\n\nLπk\n\nk − Lπ\n\nk ≤DEVk,π −\n\n(cid:32)\n\ndH 2 γ\n\n+ 4\n\nK (cid:88)\n\nH (cid:88)\n\nk=1\n\nh=1 (cid:33)\n\n+ H\n\nlog\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\nH\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\nlog|Π| η\n\n(cid:115)\n\n+ 5H\n\ndKH log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n(cid:16)√\n\nd + 1\n\n+ 2H\n\n(cid:115)\n\n(cid:17)\n\n2K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n8 3\n\n(cid:32)\n\nH +\n\n(cid:33)\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ 2γKH + 4η\n\n (d + 1)KH 2 +\n\n(cid:115)\n\n2K log\n\n\n\n(cid:19)\n\n +\n\n(cid:18) 1 δ\n\ndH 3 γ\n\n16ηdH 3 log 1 δ\nγ\n\n.\n\n(38)\n\nWhen K ≥ L0 = 4dH log\n\n(cid:17)\n\n(cid:16) |Π| δ\n\nand γ ≤ 1 2 ,\n\n(cid:12) (cid:12) (cid:12)\n\n ̃Lπ\n\nk\n\n(cid:12) (cid:12) (cid:12) ≤ H +\n\n(cid:12) ̃V π (cid:12) (cid:12)\n\nk\n\n(cid:12) (cid:12) (cid:12) ≤ H +\n\ndH 2 γ\n\n 1 + 2\n\n\n\n(cid:115)\n\n(cid:1)\n\nH log (cid:0) 1 dK\n\nδ\n\n\n\n  ≤\n\n4dH 2 γ\n\n.\n\nSo the choice of η = γ becomes:\n\n4dH 2 ensures\n\n(cid:12) (cid:12)\n\n(cid:12)η ̃Lπ\n\nk\n\n(cid:12) (cid:12) (cid:12) ≤ 1. Plugging in our choice of η, equation 38 then\n\nK (cid:88)\n\nk=1\n\nLπk\n\nk − Lπ\n\nk ≤\n\n(cid:18) 1 C2\n\n(cid:19)\n\n− 2\n\n\n\n(cid:115)\n\n\n\n \n\nK (cid:88)\n\nH (cid:88)\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\nk=1\n\nh=1\n\n(cid:115)\n\nH\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\n\n\n \n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) |Π| δ\n\n\n\n+ γKH\n\n .\n\n+ O\n\n\n\ndH 3K log\n\nChoosing C2 = 1\n\n2 , we have:\n\nK (cid:88)\n\nk=1\n\nLπk\n\nk − Lπ\n\nk ≤ O\n\n\n\n(cid:115)\n\n\n\ndH 3K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) |Π| δ\n\n\n\n+ γKH\n\n .\n\n(39)\n\nAnd by our choice of γ = min\n\n \n\n\n\n1\n\n2 ,\n\n(cid:114)\n\ndH log\n\n(cid:17)\n\n(cid:16) |Π| δ\n\nK\n\n \n\n\n\n=\n\n(cid:114)\n\ndH log\n\n(cid:17)\n\n(cid:16) |Π| δ\n\nK\n\n, we have:\n\nReg (K; Π) ≤\n\nK (cid:88)\n\nk=1\n\nLπk\n\nk − Lπ\n\nk ≤ O\n\n\n\n(cid:115)\n\n\n\ndH 3K log\n\n\n\n(cid:19)\n\n .\n\n(cid:18) |Π| δ\n\n(40)\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nFor K ≤ L0 = 4dH log\n\n(cid:17)\n\n(cid:16) |Π| δ\n\n, we have:\n\nReg (K; Π) ≤ KH ≤\n\n(cid:112)\n\nKL0H = O\n\n\n\n(cid:115)\n\n\n\ndH 3K log\n\n\n\n(cid:19)\n\n .\n\n(cid:18) |Π| δ\n\n(41)\n\nCombining equation 41 and equation 40, we prove the regret bound. Moreover, when K ≥ L0, choosing C2 ≥ 20 and recalling the definition of DEVk,π in Lemma 9,\n\nK (cid:88)\n\nk=1\n\nV π\n\nk − V πk\n\nk =\n\nK (cid:88)\n\nk=1\n\nLπk\n\nk − Lπ\n\nk\n\nK (cid:88)\n\nH (cid:88)\n\n≤ −\n\n2φ⊤\n\nπ,hΣ−1\n\nk,hφπ,h\n\n(cid:115)\n\n(cid:1)\n\nlog (cid:0) 1 δ\ndK\n\nH\n\nk=1\n\nh=1 (cid:115)\n\n\n\n+ O\n\n\n\ndH 3K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) |Π| δ\n\n\n\n\n\n+ γKH\n\n\n\n(cid:115)\n\n≤ − C2DEVK,π + O\n\n\n\ndH 3K log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\ndH 2 γ\n\nlog\n\n(cid:19)\n\n(cid:18) |Π| δ\n\n\n\n+ γKH\n\n .\n\nApplying a union bound for all the possible k0 ∈ {1, 2, · · · K}, we conclude with at least probability 1 − δ, we have:\n\nk0(cid:88)\n\nk=1\n\nV π\n\nk − V πk\n\nk ≤\n\n(cid:112)\n\nC1k0 − C2DEVk0,π .\n\nWith the constant C1 = O\n\n(cid:18)\n\ndH 3 log\n\n(cid:17)(cid:19)\n\n(cid:16) |Π|K δ\n\n≥ dH 2βK, proving Theorem 1.\n\nE CONCENTRATION INEQUALITIES\n\nLemma 12. [Freedman inequality(Freedman, 1975)] Let F0 ⊂ F1 ⊂ · · · ⊂ FT be a filtration and let X1, X2, · · · XT be random variables such that Xt is Ft measurable, E (cid:2)Xt|Ft−1 (cid:3) = 0, (cid:3) ≤ V for some fixed V > 0 and b > 0. Then, for any |Xt| ≤ b almost surely, and (cid:80)T δ ∈ (0, 1), we have with probability at least 1 − δ,\n\nE (cid:2)X 2\n\nt |Ft−1\n\nt=1\n\n(cid:113)\n\nXt ≤ 2\n\nV log (cid:0)1/δ(cid:1) + b log (cid:0)1/δ(cid:1) .\n\nT (cid:88)\n\nt=1\n\n(cid:3) = μi for some fixed μi, and (cid:80)n\n\nLemma 13 (Concentration inequality for Catoni estimators (Wei & Luo, 2018; Lee et al., 2021)). Let F0 ⊂ F1 ⊂ · · · ⊂ Fn be a filtration and let X1, X2, · · · Xn be random variables such that Xi is Fi measurable, E (cid:2)Xi|Fi−1 ≤ V for some fixed V . Denote μ = 1 i=1 μi and let ˆμn,α be the Catoni’s robust mean estiman tor of X1, X2, · · · Xn with a fixed parameter α, that is, ˆμn,α is the unique root of the funci=1 Φ (cid:0)α (Xi − z)(cid:1), where Φ(y) = log (cid:0)1 + y + y2/2(cid:1) if y ≥ 0 and Φ(y) = tion: f (z) = (cid:80)n − log (cid:0)1 − y + y2/2(cid:1) otherwise. Then for any δ ∈ (0, 1), as long as n is large enough that n ≥ α2 (cid:16) 2 log (cid:0)1/δ(cid:1), we have with probability at least 1 − 2δ,\n\ni=1 (μi − μ)2(cid:17)\n\n(Xi − μi)2 |Fi−1\n\nV + (cid:80)n\n\n(cid:80)n\n\ni=1\n\nE\n\n+\n\n(cid:105)\n\n(cid:104)\n\n(cid:12)ˆμn,α − μ(cid:12) (cid:12)\n\n(cid:12) ≤\n\n(cid:16)\n\nα\n\nV + (cid:80)n\n\ni=1 (μi − μ)2(cid:17)\n\nn\n\n31\n\n2 log (cid:0)1/δ(cid:1) αn\n\n.\n\n+\n\nUnder review as a conference paper at ICLR 2023\n\nChoosing α optimally, we have:\n\n(cid:12)ˆμn,α − μ(cid:12) (cid:12)\n\n(cid:12) ≤\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:116)2\n\n2 n\n\n\n\nV +\n\nn (cid:88)\n\ni=1\n\n(μi − μ)2\n\n  log (cid:0)1/δ(cid:1) .\n\nIn particular, if μ1 = μ2 = · · · = μn, we have:\n\n(cid:12)ˆμn,α − μ(cid:12) (cid:12)\n\n(cid:12) ≤\n\n(cid:113)\n\n2 n\n\n2V log (cid:0)1/δ(cid:1) .\n\n32",
    "reference": "# Summary Of The Paper\n\nThis paper studies linear MDP with possibly adversarial rewards. The authors propose a detection-based algorithm that can simultaneously learn stochastic and adversarial linear MDP. Assuming a known probability transition, it is shown that the proposed algorithm can achieve logarithmic regret in the stochastic case and sublinear regret in the adversarial case.\n\n# Strength And Weaknesses\n\nStrength:\n\n- The authors have done a good job reviewing the related literature.\n- This paper studies an interesting problem of simultaneously learning stochastic and adversarial linear MDP, and the result presents a best-of-both-worlds guarantee.\n- The theoretical analysis provides a high-probability regret guarantee for adversarial linear MDP.\n\nWeaknesses:\n\n- Given the existing literature on RL with function approximation, I believe the setting of linear MDP is already well-motivated. So for the introduction, the authors should discuss more the motivation of best-of-both-world type algorithm and, more importantly, explain the challenges in the algorithm design and theoretical analysis, compared with existing results.\n- Following the previous point, please explain the technical challenges induced by the setting of linear MDP and the novelty of the proposed method.\n- The description of the proposed algorithm in Section 4 is a bit difficult to follow. I would suggest the authors put more effort into reorganizing this section for readability. More specifically, Algorithm 4 seems to be an important subroutine, but it is not clearly explained what this algorithm is doing and why we need it. The description on page 5 could be more carefully structured using some highlighted words/sentences. Also, please provide references to existing related methods in other settings and compare them.\n- It seems crucial to assume a finite policy set. Why is this a reasonable assumption for linear MDP? For example, consider the LSVI-UCB algorithm where the policy is given by the greedy policy w.r.t. the estimated Q-function, and in this case, the policy set is infinite. Even if using a covering argument for the policy set, it would introduce additional dependence on the dimension. Please justify this assumption and discuss its limitation in detail.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis work is original, extending previous works on best-of-both-worlds results for bandits and tabular MDP to the setting of linear MDP. For novelty, the authors should discuss the unique challenges in the setting of linear MDP and the corresponding solutions. For clarity and quality, I suggest revising the algorithm section for better readability. Also, the authors should explicitly and clearly specify the assumptions for each lemma and theorem.\n\n# Summary Of The Review\n\nI think this paper studies a meaningful setting and the results are solid. But the overall writing can be further improved, and see detailed comments above. I didn't carefully check the proof in the appendix.\n\nBesides, I have a few more questions as follows:\n\n- In the definition of the Q-function, normally people would include the reward received at the current step, i.e., $r_{k,h}(s,a)$\n- The loops in Algorithm 1 and 2 are a bit weird. Should provide a stopping criterion, otherwise it's not clear when the algorithm ends.\n- If assuming a known transition, do we still need to require the transition probability to be linear? I think without the linear assumption on the probability transition, the value function can still be written as a summation of inner products between the expected feature vectors and the parameters of the rewards.\n\n---\nOther minor problems:\n\n- Near the bottom of page 5, should be 'standard least square estimators'\n- Under Algorithm 3, should be 'fool the algorithm'\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPOLARITY IS ALL YOU NEED TO LEARN AND TRANSFER FASTER\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNatural intelligences (NIs) thrive in a dynamic world – they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligence (AI) has achieved supra (-human) level performance in certain AI settings, typically dependent on a prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle based on a simple observation from NIs: post-development, neuronal connections in the brain rarely see polarity switch. We demonstrate with simulations that if weight polarities are adequately set a priori, then networks learn with less time and data. We extend such findings onto image classification tasks and demonstrate that fixed polarity, not weight, is a more effective medium for knowledge transfer between networks. We also explicitly illustrate situations in which a priori setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.\n\n1\n\nINTRODUCTION\n\nNatural intelligences (NIs), including animals and humans, thrive in a dynamic world. Often, NIs learn quickly with just a few samples. Artificial intelligences (AIs), specifically deep neural networks (DNNs), can now compete with or even surpass humans in certain tasks, e.g., GO playing (Silver et al., 2017), object recognition (Russakovsky et al., 2015), protein folding analysis (Jumper et al., 2021), etc. However, DNN is only capable of achieving such when a prohibitive amount of data and training resources are available. Such a gap on learning speed and data efficiency between NI and AI has baffled and motivated many AI researchers. A subfield of AI is dedicated to achieving few-shot learning using DNNs (Hoffer & Ailon, 2015; van der Spoel et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017). Many research teams have achieved amazing performances on benchmark datasets (Lazarou et al., 2022; Bendou et al., 2022). However, the products of such engineering efforts greatly deviate from the brain. What are the design principle differences between NIs and AIs that contribute to such a learning efficiency gap? In this paper, we propose one possiblility of such a design difference - we could move AI one step closer to NI-level learning efficiency by applying just one simple design principle from NI.\n\nNIs are blessed with hundreds of millions of years of optimization through evolution. Through trial and error, the most survival-advantageous circuit configurations emerge, refine, and slowly come into the form that can thrive in an ever-changing world. Such circuit configurations get embedded into our genetic code, establishing a blueprint to be carried out by development. Among the many configurations, circuit rules, and principles that formed through evolution, one theme stands out, one that neuroscientists celebrate and yet is overlooked by the machine learning community: postdevelopment, neuronal connections in the brain rarely see polarity switch (Spitzer, 2017). After development, NIs learn and adapt through synaptic plasticity – a connection between a pair of neurons can change its strength but rarely its excitatory or inhibitory nature; on the contrary, a connection (weight) between a pair of units in a DNN can freely change its sign (polarity). In fact, polarity change in the adult brain is hypothesized to be associated with depression, schizophrenia, and other illnesses (Spitzer, 2017). For the rare times such phenomenon have been observed, they never appeared in sensory and motor cortices (Spitzer, 2017) where visual, auditory and motor processing take place. It seems a rather rigid design choice to fix a network’s connection polarity. We wonder why the biological networks settled into such a learning strategy: Is it a mere outcome of an implementation-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nlevel constraint? It could be just hard for synapses to switch polarities. Or could it be that NIs found out polarity pattern is an effective medium for transferring knowledge across generations?\n\nThis paper provides some thoughts and evidence relevant to these questions. We first investigate why brains pre-set neuronal connection polarities by assessing what we gain through setting weight polarity a priori (Sec 2-4). We discuss in theory the trade-off between representation capacity and learning speed when weight polarity is fixed for networks. We then propose an SGD-based polarity-fixed learning algorithm: Freeze-SGD. We experimentally show that if the weight polarities are adequately set a priori, then networks can learn with less time and data (simulated task (Sec 2) + two image classification tasks (Sec 3)). We call a network with fixed polarity Frozen-Net, and we discuss how the quality of the polarity configuration affects a DNN’s learning efficiency (SufficientPolarity vs. RAND-Polarity). We further find transferring and fixing polarities is even superior to transferring weights (Sec 4). Our results point to an unexplored direction in the machine learning community: polarity, not weight, may be the more effective and compressed medium for transferring knowledge between networks. To complete our discussion, we further discuss what we lose when weight polarities are set a priori (Sec 5). Frozen-Nets have reduced representation capacity; therefore, a randomly configured network may not even have the capacity to represent a simple problem such as XOR (Def 2.2). We theoretically prove and experimentally show that if the polarities are set randomly, the probability of a single-hidden-layer network learning XOR increases exponentially as a function of its size (i.e., number of hidden units). We also experimentally show that a sufficiently sized network, even when its polarities are randomly picked, can learn with less time and data than an equally sized network without fixed polarities.\n\nBy discussing both the advantages and disadvantages of fixing weight polarity, we provide some insights on how to make AI more statistically and computationally efficient; we also show polarity pattern is an effective medium for transferring knowledge.\n\n2 WHAT DO WE GAIN BY SETTING WEIGHT POLARITY a priori?\n\nNetworks need both positive and negative weights to funcion (Wang et al., 2022) - a DNN with all non-negative weights is not a universal approximator. Constraining a network’s weight polarity pattern limits its representation capacity: when only half of the range is available to each connection, the reduction in total possible network patterns grows exponentially with more edges in the network. It seems counter-intuitive for any network to have willingly chosen to give up on a vast portion of its representation capacity. Are they gaining elsewhere? Our thought is: maybe they learn faster. Below we provide theoretical discussions and experimental evidence. Definition 2.1 (DNN). Let W (l) ∈ Rnl×nl−1 be the input weights of layer l, nl be the number of units in layer l, b(l) ∈ Rnl be the bias terms of layer l, for l ∈ {1, . . . , L}. l = 0 is the input layer, n0 = dim(x). Let layer l be σ(l)(x) = σ(W (l)x + b(l)).\n\nP (x) = σ(L) ◦ σ(L−1) ◦ ... ◦ σ(1)(x), P (x) ∈ RnL ; F (x) = softmax(P (x))\n\nDNN is F (x). For the non-linearity σ, we define it to be ReLU throughout the paper.\n\nσ(xj) =\n\n(cid:26)0 xj < 0 xj xj ⩾ 0\n\nσ(x) = (σ(x1), . . . , σ(xnl))\n\nLemma 2.1 (capacity-speed trade-off). If the weight polarities are set a priori, such that the function is still representable, then the network can learn faster.\n\nWe prove Lemma 2.1 for single-hidden-layer networks, with the following assumptions:\n\nAssumption 1: The weights take on discrete values. This is essentially true for all DNNs implemented on silicon chips where all continuous variables are discretized;\n\nAssumption 2: Exhaustive search as the learning algorithm.\n\nSee proof on page 20.\n\nThe theory, albeit proved under constrained settings, argues for a trade-off between network representation capacity and learning speed. Next, we test with simulation to show that networks\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nlearn more quickly when polarities are set a priori in such a way that the function is still representable (Figure 1 panel B). With input space X , a function f is representable by a DNN F when ∀x ∈ X , ε > 0, |f (x) − F (x)| < ε.\n\nWe design our freeze training procedure to be exactly the same as SGD (Adam optimizer) except for one single step: after each batch, all weights are compared to the preset polarity template, and if any of the weight has switched polarity in the last batch, it is reverted back to the desired sign (see algorithm 1). As our goal is to see the pure effect of fixing weight polarity, we did not adopt any bio-plausible learning algorithms as they may introduce confounding factors.\n\nWe compared four training procedures in general:\n\n1. Frozen-Net sufficient-Polarity: Weight polarities were set a priori. The polarity pattern was chosen based on a rule that ensures the configuration is adequate for learning the task, i.e., the polarity pattern carries expert knowledge about the data.\n\n2. Frozen-Net RAND-Polarity: Weight polarities were set a priori randomly Bernoulli(0.5).\n\nThe polarity pattern does not carry any information about the data.\n\n3. Fluid-Net RAND-Polarity: Weights (containing polarities) were initialized randomly;\n\nweight polarities were free to change throughout the training procedure.\n\n4. Fluid-Net sufficient-Polarity Weight polarities (not magnitudes) were initialized with prior knowledge; weight polarities were free to change throughout the training procedure. This scenario will only be discussed in the next section 3 on image classification tasks.\n\nWe used 5-dimensional XOR (XOR-5D) as our simulation binary classification task; only the first two dimensions are relevant to the task, and the remaining three dimensions are noise following a normal distribution N (1, 1) (Figure 1 panel A). For Frozen-Net sufficient-Polarity, the polarity template is pre-set in this way: for each hidden unit, the polarity of the output edge is the sign product of the first two dimensions’ input weights. Definition 2.2 (XOR). Let x ∈ R2 and t1, t2 ∈ R,\n\nf (x) =\n\n(cid:26)0 1\n\n[x1 ≥ t1 and x2 ≥ t2] [x1 ≥ t1 and x2 < t2]\n\nor or\n\n[x1 < t1 and x2 < t2] [x1 < t1 and x2 ≥ t2].\n\n(1)\n\nTo have the best controlled experiments, we fixed the magnitude distribution, architecture, training samples (n varies, see figure), batch sequence of the training data, and the validation samples (n=1000) across all scenarios. More details about the experiments can be found in the appendix Sec C. We first tried four different weight reset methods (details in Algo 1) and found they give us similar results and thus did not matter for our primary questions (supplementary figure B.1). For the rest of this paper, we chose the posRand method whereby we set weights to a small random number of the correct sign.\n\nWhen the polarities are fixed in such a way that it is sufficient to learn the task (red), networks always learn faster than networks without polarity constrains (blue) (Fig 1 panel B). This advantage is true across all data scarcity levels and is particularly valuable when data is scarce. When only 60 or 72 training samples were available, Frozen-Net sufficient-polarity on average take 58% and 48% of the standard Fluid-Net training time respectively to reach the same level of accuracy. When weight polarities are randomly chosen (green), networks learn as fast as their Fluid-Net counterparts (blue), and sometimes faster (e.g., training samples = 80 and 92).\n\nFrozen-Net sufficient-Polarity not only saves time, but also saves data (Fig 1 panel C). At convergence, Frozen-Net sufficient-Polarity (red) takes fewer samples to reach the same level of accuracy when compared to the standard Fluid-Net (blue). Randomly configured Frozen-Net (green) uses similar and oftentimes less data than Fluid-Net (blue) (e.g., to reach 80% accuracy, Frozen-Net RAND-polarity uses less data than Fluid-Net).\n\nWe showed that setting weight polarity a priori makes networks learn in less time, with less data, provided the function is still representable. Even randomly configured Frozen-Nets show comparable and sometimes better performance than Fluid-Nets. Such a result is striking from an optimization perspective: Frozen-Net is at a disadvantage by design because the weight resetting step in FreezeSGD 1 fights against the gradient update - part of the error update information is lost in this process. Regardless of this disadvantage during optimization, our Frozen-Net sufficient-polarity consistently\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\noutperforms Fluid-Net; even Frozen-Net RAND-Polarity is never worse than Fluid-Net. Combined, these results show we may help AIs to learn quickly and with fewer samples by doing two things: 1) fix weight polarities, and 2) choose polarity pattern wisely. We will tease apart the effect of these two factors in the next section.\n\nFigure 1: Adequately setting weight polarity a priori makes networks learn more quickly (fewer epochs) and with less data (fewer training samples). Single hidden layer networks with 64 hidden units are trained to learn XOR-5D. In all scenarios, networks were trained for 100 epochs. A) XOR-5D, a binary classification problem where only the first two dimensions are relevant to the task (XOR), and the other three dimensions are noise following a normal distribution N (1, 1). B) Computational efficiency: Setting weight polarity in a data-informed way (red) makes networks learn more quickly – this is true across all data-scarcity levels; when weight polarities are randomly fixed (green), networks learn as fast as their fluid counterparts (blue), and sometimes faster (e.g., training sample sizes = 80 and 92). C) Statistical efficiency: At convergence, adequately configured freeze networks (red) achieve the same level of performance (% error) with less data; randomly configured freeze networks (green) use similar and oftentimes less data than fluid (blue), e.g., to reach 80% accuracy (20% error). For all experiments, n=50 trials. All curves correspond to medians with shaded regions 25th-75th percentiles.\n\n3 EFFECTIVENESS OF FROZEN-NET IN IMAGE CLASSIFICATION TASKS\n\nTo prove the effectiveness of our Frozen-Net strategy for more complex tasks, we extended the experiments in Fig 1 to image classification tasks (Fig 2). Such complex learning tasks do not have simple and explicit polarity configuration rules as in XOR-5D. We exploited an alternative strategy by using ImageNet trained polarities (IN-Polarity). A Frozen-Net IN-Polarity has its weight magnitudes initialized randomly, its weight polarities initialized and fixed to match the IN-polarity pattern. We also tested Fluid IN-Polarity where networks were initialized to the exact same pattern as Frozen-Net IN-Polarity, except polarities are free to switch in this scenario. This pair of comparison help us to understand which of the two factors contributed more to the performance gain: fixing polarities or knowledge transfer through polarity pattern. We trained and tested networks on the Fashion-MNIST (grayscale) and CIFAR-10 (RGB-color) datasets, using AlexNet network architecture (Krizhevsky et al., 2017). For both datasets, we trained for 100 epochs, with lr=0.001. The AlexNet IN-weights were obtained here. Worth emphasizing, we controlled all conditions to follow the same weight magnitude distribution at initialization (supp Fig B.6) Specifically, we randomly initialized the networks following normal procedures: Glorot Normal for conv layers, Glorot Uniform for fc layers; then either fixed the polarities as is (RAND-Polarity) or flipped the polarities according to IN template (IN-Polarity), introducing no change to the magnitude distributions.\n\nAcross the board, Frozen-Nets IN-polarity (red) always learn with fewer samples than Fluid-Net (blue) (Fig 2 first column). When only 100 training images were available (across the 10 classes), Frozen-Net IN-Polarity (red) yields 7% less validation error at convergence compared to Fluid-Net (blue) in the Fashion-MNIST task and 9.4% less error for CIFAR-10. Such a gain is mostly brought by knowledge transferred through polarity pattern (pink vs. blue, 6% gain for Fashion-MNIST; 8.4% gain for CIFAR-10). Fixing the polarities can further bring performance gain: for CIFAR-10, 1% gain at 100 training samples, and up to 3% at 50000 training samples. The gain from Fluid IN-Polarity to\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFreeze-Net IN-Polarity can be statistically significant (Fig 3, pink line). When the polarities are fixed randomly (RAND-polarity, green), networks never perform worse than Fluid-Net (blue).\n\nFigure 2: DNNs with frozen IN-Polarity learn more quickly and with less data in image classification tasks. A) Experiments on Fashion-MNIST image classification dataset. From left to right: 1) Statistical efficiency: Frozen-Nets with IN-polarity (red) always learn with fewer samples than Fluid-Net (blue); the majority of the gain is contributed by the knowledge transferred from initial polarity configuration (pink vs. blue); Frozen-Nets RAND-Polarity (green) never perform worse than Fluid-Net (blue). 2) Frozen-Net IN-Polarity always have a higher chance of reaching 80% validation accuracy than Fluid-Net; Frozen-Net RAND-Polarity have comparable and sometimes a higher chance of reaching 80% validation accuracy than Fluid-Net. 3) Computational efficiency: Frozen-Net IN-Polarity always take less time than Fluid-Net to reach 80% validation accuracy; again, effective knowledge transfer from preset polarity pattern is the major contributing factor (pink vs. blue);Frozen-Net RAND-Polarity takes a similar number of computational iterations as Fluid-Net. B) Same as A except experiments were on CIFAR-10 dataset and the validation accuracy threshold is at 50%. Gray lines in the first column correspond to the validation accuracy thresholds used to plot the next two columns. For a comprehensive view of performances at different thresholds, see Supp Fig B.2. For statistical significance of the difference, see Fig 3. Both datasets: n=20 trials, 100 epochs, lr=0.001. No data augmentation was performed.\n\nAcross the board, Frozen-Net IN-Polarity always learns with less time than Fluid-Net (Fig 2 third column). Majority of such a gain is brought by polarity pattern knowledge transfer. Frozen-Net RAND-Polarity takes a comparable number of computational iterations as Fluid-Net. Furthermore, not every network is able to reach the specified accuracy threshold (Fashion-MNIST: 80%, CIFAR10: 50%; Fig 2 second column). Across the board, Frozen-Net IN-Polarity has a higher chance of passing the specified accuracy threshold than Fluid-Net; Frozen-Net RAND-Polarity has an equal and sometimes higher chance than Fluid-Net (Fashion-MNIST 1000 & 2000 samples; CIFAR-10 10000 & 25000 samples). These observations are true across different validation accuracy thresholds (Supp Fig B.2).\n\nWorth noting, IN-Polarity initialized networks in general shows more consistent performance across trials, compared to the Fluid setting, for both Frozen-Net and Fluid-Net. This is especially obvious for the statistical efficiency plots: across the board, IN-Polarity always shows less variation in its performance on validation error (shaded area marks 25th-75th percentiles).\n\nTo provide a lens into the dynamic of polarity switch throughout learning, we analyzed the ratio of weight parameters (excluding bias terms) that had polarity switch between two epochs for Fluid RAND-Polarity for the first 50 epochs - indeed, there are more polarity switch early on in training\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nand the ratio decays throughout training. Such a trend is true across layers and across training sample sizes. These suggest polarities are mostly learnt early on during training but also remain dynamic throughout the learning process.\n\nIn sum, we are able to extend our simulation results in Sec 2 to more complex image classification tasks: Frozen-Net IN-polarity consistently learns with less data and time and does so with a higher probability of success compared to Fluid-Net; the majority of the performance gain is brought by knowledge embedded in the initialized polarity pattern, with further gain possible by fixing weight polarities; Frozen-Net RAND-polarity perform as well as Fluid-Net, sometimes better.\n\n4 TRANSFERRING AND FIXING POLARITY IS SUPERIOR TO TRANSFERRING\n\nWEIGHTS\n\nFrom a transfer learning perspective, our Frozen-Net strategy essentially transfers weight polarities instead of weights per se. How does polarity transfer compare to the traditional finetune (weight transfer) strategy? This time, instead of randomly initializing the weight magnitudes, we initialized the weight magnitudes based on the ImageNet-trained weights (IN-Weight). We compared them with Frozen-Net IN-Polarity by plotting their differences (∆={LEGEND} - {Freeze IN-Polarity}) in Fig 3 (the original curves before taking the differences can be found in Supp Fig B.3).\n\nThe orange curves compare weight transfer (Fluid-Net IN-Weight) with our polarity transfer strategy (Frozen-Net IN-Polarity). Across almost all data scarcity levels, our polarity transfer strategy achieves higher accuracy than finetune (first column, orange⩾ 0). Such a superiority is statistically significant\n\nFigure 3: Across all sample sizes, Frozen-Net with ImageNet-polarities (IN-polarities) learns more quickly than Fluid-Net with ImageNet-weight (IN-weight) initialization. In first and third columns, curves are the median difference ∆={LEGEND} - {Freeze IN-Polarity} (the second term is the red curve in Fig 2). Asterisks (*) indicate statistical significance with Mann-Whitney U two-tail test (α=0.05). 1) First column: Transferring and fixing polarity is more effective than transferring weights in terms of data efficiency. This is indicated by orange curve above zero across most sample sizes, and it is statistically significant for small sample sizes. Note the zero line here means LEGEND = Freeze IN-Polarity. 2) Second column: Frozen-Net IN-Polarity has a higher chance of reaching 80% (Fashion-MNIST) / 50% (CIFAR-10) validation accuracy compared to weight transfer (orange curve); 3) Third column: Frozen-Net IN-Polarity always takes less epochs to reach high validation accuracy than weight transfer (orange curve). Both datasets: n=20 trials, 100 epochs, lr=0.001. For the right two columns, the validation accuracy thresholds are the same as in Fig 2\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(Mann-Whitney U two-tail) when training data is limited (Fashion-MNIST: ⩽ 2000 samples except 750, CIFAR-10: ⩽ 1000 samples). Polarity transfer also allows the networks to learn with higher probability (second column) and fewer epochs (third column, curve⩾ 0). Such faster learning is true regardless of the performance threshold value (Supp Fig B.4). In sum, transferring and fixing polarity is almost always superior to weight transfer both in terms of statistical efficiency and computational efficiency. Such an observation suggests polarity configuration is an effective medium, if not superior to weight pattern, for transferring knowledge between networks.\n\nWhen networks were transferred with polarities but not frozen (Fluid IN-Polarity pink), they almost always perform better than Fluid IN-Weight (orange, Fig 3 & Supp Fig B.3 to see variance), this is true except rare cases (e.g. CIFAR-10 2500&5000 samples) where the difference is not significant due to the wide performance variation of Fluid IN-Weight.\n\nSurprisingly, when we initialized the Frozen-Net with IN-weight (cyan), there is some gain in performance, but to a limited extent; in fact, it can sometimes be worse. When training data is limited (Fashion-MNIST⩽ 1000, CIFAR-10⩽ 1000), Frozen-Net IN-weight gained little performance (cyan≈0) and could be detrimental at times (CIFAR-10 500&1000 samples). When training data is more abundant, there is a more consistent accuracy gain by initializing Frozen-Net with IN-weight (Fashion-MNIST gain ∼ 1%, CIFAR-10 gain ∼ 4%). Such a gain is discounted by more training iterations (third column), and sometimes less likelihood of reaching a high level of performance (second column).\n\nFurthermore, similar to random initialization, weight transfer tend to have wider performance variations compared to polarity transfer, for both Frozen and Fluid networks (Fig B.3). The exact reason behind this observation remains to be explored, our current hypothesis is the stochasticity of sample batching: by only transferring polarity while initializing the magnitudes randomly, the learning process is more robust against such stochasticity. The difference in initialized magnitude distribution between polarity transfer vs. weight transfer could also be a potential contributing factor (supp Fig B.6).\n\nIn sum, polarity transfer is superior to weight transfer in most of the scenarios we tested and such a superiority is further secured by fixing the polarities throughout training. To a large extent, weight polarity alone, not weight per se, is enough to transfer knowledge between networks. Giving the additional magnitude information to Frozen-Net can give some performance gain, but only when data and time is abundant; in all other scenarios (i.e., data-limited or time-limited), initializing Frozen-Net with stereotypical weight magnitudes could be detrimental to the learning performance.\n\n5 WHAT DO WE LOSE BY SETTING WEIGHT POLARITY a priori?\n\nIntelligent agents have limited resources in 1) data, 2) time, 3) space (number of hidden units or other network size parameters), and 4) power (∝time×space, number of flops). In Sec 2,3,4, we showed that fixing weight polarity helps to save on two of these resources, time and data, but with a condition – the polarity configuration must be adequately set such that the network can still represent the function. With fixed polarity, certain configurations will result in networks never being able to learn the task. What is the probability of such unfortunate events happening? We investigate this direction with our simulated task XOR-5D.\n\nAssuming unlimited resources (i.e., perfect learning algorithm, unlimited data and time), we deduced the theoretical probability limit for a single-hidden-layer network to be able to represent XOR (and its high-dimensional variants with task-irrelevant higher dimensions) as a function of the number of hidden units (Supp Theorem D.1). The theoretical results are plotted in Fig 4 panel A (top). In theory, it takes at least 3 units for Fluid-Net and Frozen-Net sufficient-Polarity to be able to learn XOR-5D on every single trial. Such a probability will never be 100% for Frozen-Net RAND-Polarity, no matter how large the networks are. Luckily, the probability grows exponentially with the network size: having 15 hidden units is already sufficient for a randomly configured Frozen-Net to learn XOR with > 99% probability.\n\nThis is exactly what we see in our simulation results (Fig 4 panel A bottom). For a generous amount of data (500 training samples) and learning time (100 epochs), the Frozen-Net RAND-Polarity curve nicely matches the theoretical curve - networks with more than 15 hidden units learn XOR-5D with very high probability, even though their polarities are fixed randomly.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: A network has to be sufficiently large for it to learn more quickly, with less data, if its weight polarity is configured randomly. A) For a randomly configured Frozen-Net to learn XOR-5D, it has to be sufficiently large. In both the theoretical limit and simulation results, it takes at least 15 hidden units for a Frozen-Net RAND-Polarity (green) to have > 99% chance of learning XOR-5D; it takes three hidden units, in theory, for an adequately configured Frozen-Net (red) and Fluid-Net (blue) to learn XOR-5D. B) When Frozen-Net RAND-Polarity is not large enough, it has a much lower chance of learning XOR-5D, and when it does learn, it uses more time and data. C) When Frozen-Net RAND-Polarity is sufficiently large, it at least shows the same level of performance as standard Fluid-Nets. D) Frozen-Net sufficient-Polarity shows an advantage over Fluid-Net (red + yellow) in almost all training sample sizes (especially when data is limited) and across all network sizes. Randomly configured Frozen-Nets have an advantage over Fluid-Nets (green + yellow), mostly when the network is sufficiently sized (> 15 hidden units). We ran 50 trials for all experiments. All curves represent medians with shaded regions denoting 25th-75th percentiles.\n\nBoth theory and simulation results show that we will lose all advantages if weight polarities are fixed but not configured adequately; an example of such is a small, randomly configured Frozen-Net (e.g., 10 hidden units, Fig 4 panel B). Notice that for the same network size, if we ensure the configuration is adequately set (red), then the network learns quickly and is data-efficient. By allowing more space (network size), Frozen-Net RAND-Polarity starts to pick up the advantages in time and data efficiency (Fig 4 panel C). In summary, we gain from setting weight polarity a priori only if the polarity is configured adequately (Fig 4 panel D); the adequacy can either be made more probable by having a larger network or can be guaranteed by an explicit configuration rule (e.g., through development for NIs, or explicit rules in our simulations, or transferred polarities from previously trained networks).\n\n6 DISCUSSION\n\nWe showed in this paper that 1) if the weight polarities are adequately set a priori, then networks can learn with less data, time, space, and power; 2) polarity may be a more effective and compressed medium of network knowledge transfer; and 3) while we lose all advantages when the polarities are\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nfixed but not set adequately, we can regain these advantages by increasing the size of the networks. Below we discuss the novelty of our work and some future directions.\n\nIn the literature of bio-plausible artificial neural networks (ANNs), the most related work is on Dale’s principle: a single unit’s output weights are exclusively excitatory or inhibitory. An exciting attempt of applying such a principle to ANNs achieved performance comparable to multi-layer perceptrons (gray-scale image classification tasks) and VGG16 (CIFAR-10) (Cornford et al., 2021). Our approach differs in several ways; the most fundamental one is ours does not require exclusivity of a unit’s weight polarity, we only ask the polarity configuration to stay fixed throughout training. Because we made fewer assumptions on the architecture and network properties, we were able to reveal the true power of weight polarities - polarity-fixed networks can not only perform as well as the traditional approaches when the polarities are set adequately, but they can also learn more quickly with less samples. Additionally, we revealed that polarities, not weights, may be a more effective and compressed form of knowledge transfer medium. Furthermore, our Freeze-SGD algorithm 1 is easily applicable to any existing network architecture and any learning mode, be it transfer learning or de novo learning, thus enjoy a wider application range.\n\nOur hypothesis is also novel in the machine learning literature. In a statistical learning framework, it is counter-intuitive to disassemble a single parameter into magnitude and sign. To the best of our knowledge, we are the first to think in this way and prove that connection polarities themselves serve as an effective form of inductive bias. Previous work that vaguely took on a connectionist’s view mostly focused on the existence or range of connections (e.g., adding skip connections (He et al., 2016)), but the polarity of such connections were essentially left out of the discussion. Our work broke the stereotypical view and brought weight polarity into the playing field. A lesson we learned from this research is that when designing network architectures, we should not only focus on the existence of connections, but also pay attention to the polarities of connections. In the transfer learning literature, to the best of our knowledge, there has been no indication that transferring polarity alone is sufficient for knowledge transfer – ours is the first demonstration on the importance of polarity.\n\nOur work is in sync with the framework of learning theory. Our way of explicitly setting the weight polarities provides a strong prior for the networks. This is in line with the bias-variance trade-off theorem (Geman et al., 1992; Tibshirani & Friedman, 2001). Frozen-Net sufficient-Polarity has a high inductive bias – by trading representation capacity, they gain in time and data efficiency through discounting parameter variances. Indeed, the performance gain we observe from fixing polarities might be explained by preventing over-fitting thus better generalization to the validation data.\n\nOur work is also in sync with the lottery ticket hypothesis (Frankle & Carbin, 2019). Our observations on larger networks enjoying a high probability of learning XOR-5D even when their polarities are randomly configured is a realization of the lottery ticket hypothesis.\n\nWe engineered our Freeze-SGD algorithm entirely based on SGD because we are interested in the pure effect of fixing polarity during learning. As discussed in Sec 2, such an approach intrinsically put Frozen-Net in a disadvantageous position as while resetting weights to the correct polarity after each learning iteration, this procedure effectively fights against the gradient update direction. It is an interesting next step to adopt a more polarity-freeze compatible learning algorithm, possibly allowing us to further improve learning performance. One possibility is adapting the primal-dual interior-point method (Vogelstein et al., 2010) or Hebbian learning (Amit, 2019), as well as a host of bio-plausible learning algorithms (Miconi, 2017; Boopathy & Fiete, 2022; Dellaferrera & Kreiman, 2022).\n\nFrom this work, we have strong experimental evidence (simulation and image classification) and some theoretical discussion suggesting that if the weight polarities are adequately set a priori, then networks can learn with less data, time, space, and power. We look forward to applying our FrozenNet approach to more tasks to see if we can empirically extend our results to more diverse scenarios. We also look forward to extending our results theoretically (e.g., including SGD assumption in our Lemma 2.1).\n\nREFERENCES\n\nYali Amit. Deep learning with asymmetric connections and hebbian updates. Frontiers in Computational Neuroscience, 13:18, 2 2019. ISSN 16625188. doi: 10.3389/FNCOM.2019.00018/BIBTEX.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nYassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup, Stéphane Pateux, and Vincent Gripon. Easy: Ensemble augmented-shot y-shaped learning: State-of-the-art few-shot classification with simple ingredients. arXiv, 1 2022. doi: 10.48550/arxiv.2201.09699. URL http://arxiv.org/abs/2201.09699.\n\nAkhilan Boopathy and Ila Fiete. How to train your wide neural network without backprop: An input-weight alignment perspective. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2178–2205. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ boopathy22a.html.\n\nJonathan Cornford, Damjan Kalajdzievski, Marco Leite, Amélie Lamarquette, Dimitri Michael Kullmann, and Blake Aaron Richards. Learning to live with dale’s principle: {ANN}s with separate excitatory and inhibitory units. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=eU776ZYxEpz.\n\nGiorgia Dellaferrera and Gabriel Kreiman. Error-driven input modulation: Solving the credit assignment problem without a backward pass. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 4937–4955. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ dellaferrera22a.html.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/ finn17a.html.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 7th International Conference on Learning Representations, ICLR 2019, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.\n\nStuart Geman, Elie Bienenstock, and René Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4:1–58, 1 1992. ISSN 0899-7667. doi: 10.1162/neco.1992.4.1.1. URL https://doi.org/10.1162/neco.1992.4.1.1.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.\n\nElad Hoffer and Nir Ailon. Deep metric learning using triplet network. In Aasa Feragen, Marcello Pelillo, and Marco Loog (eds.), Similarity-Based Pattern Recognition, pp. 84–92, Cham, 2015. Springer International Publishing. ISBN 978-3-319-24261-3.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A.A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino RomeraParedes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583–589, 2021. ISSN 14764687. doi: 10.1038/s41586-021-03819-2. URL http://dx.doi.org/10.1038/s41586-021-03819-2.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep conISSN 15577317. doi:\n\nvolutional neural networks. Communications of the ACM, 60, 2017. 10.1145/3065386.\n\nMichalis Lazarou, Tania Stathaki, and Yannis Avrithis. Iterative label cleaning for transductive ISSN 15505499. doi:\n\nICCV, pp. 8731–8740, 2022.\n\nand semi-supervised few-shot learning. 10.1109/iccv48922.2021.00863.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nThomas Miconi. Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife, 6, 2017. ISSN 2050084X. doi: 10.7554/eLife. 20899.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211–252, 2015. ISSN 15731405. doi: 10.1007/s11263-015-0816-y.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George Van Den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354–359, 2017. ISSN 14764687. doi: 10.1038/nature24270. URL http://dx.doi.org/10.1038/nature24270.\n\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ cb8da6767461f2812ae4290eac7cbc42-Paper.pdf.\n\nNicholas C. Spitzer. Neurotransmitter switching in the developing and adult brain. Annual Review of\n\nNeuroscience, 40, 2017. ISSN 15454126. doi: 10.1146/annurev-neuro-072116-031204.\n\nRobert Tibshirani and Jerome H Friedman. The elements of statistical learning [electronic resource]:\n\ndata mining, inference, and prediction: with 200 full-color illustrations. Springer, 2001.\n\nEvie van der Spoel, Maarten P. Rozing, Jeanine J. Houwing-Duistermaat, P. Eline Slagboom, Marian Beekman, Anton J M de Craen, Rudi G J Westendorp, and Diana van Heemst. Siamese neural networks for one-shot image recognition. ICML - Deep Learning Workshop, 7, 2015. ISSN 19454589.\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. MatchIn D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and ing networks for one shot learning. R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/ 90e1357833654983612fb05e3ec9148c-Paper.pdf.\n\nJoshua T. Vogelstein, Adam M. Packer, Timothy A. Machado, Tanya Sippy, Baktash Babadi, Rafael Yuste, and Liam Paninski. Fast nonnegative deconvolution for spike train inference from population calcium imaging. Journal of Neurophysiology, 104:3691–3704, 12 2010. ISSN 00223077. doi: 10.1152/JN.01073.2009/ASSET/IMAGES/LARGE/Z9K0081002840010.JPEG. URL https: //journals.physiology.org/doi/full/10.1152/jn.01073.2009.\n\nQingyang Wang, Michael A. Powell, Ali Geisa, Eric Bridgeford, and Joshua T. Vogelstein. Why do\n\nnetworks have inhibitory/negative connections?, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA FREEZE-SGD ALGORITHM\n\nAlgorithm 1 Freeze-SGD for l = 1, 2, . . . , L do\n\nGet weight polarity template T (l) based on the configuration rules. Match W (l) to T (l)\n\nend for\n\nfor epoch = 1, 2, . . . do\n\nfor batch = 1, 2, . . . do\n\nSGD updates all weights. for l = 1, 2, . . . , L do\n\nCompare signs of the weights W (l) to the template T (l), get checker = T (l) ∗ sign(W (l)). for (i, j) where T (l)(i, j) < 0 do\n\nMake Wl(i, j) in compliance with T (l)(i, j) by one of the following four ways: Case posCon W (l)(i, j) = T (l)(i, j) ∗ ε where ε > 0 Case posRand W (l)(i, j) = T (l)(i, j) ∗ rand([0, ε]) where ε > 0 Case zero W (l)(i, j) = 0 Case flip W (l)(i, j) = −W (l)(i, j)\n\nend for\n\nend for\n\nend for\n\nend for\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nB SUPPLEMENTARY FIGURES\n\nFigure B.1: Different reset methods in the Freeze-SGD algorithm yields similar results. The meaning of different reset methods is described in algorithm 1. A) Freeze-informed consistently reaches higher validation accuracy after the same amount of training time. This is especially evident when training sample is scarce. B) Same curves as in Fig 4, panels B & C first column, plotted for different reset methods. C) & D) Same curves as in Fig 1, panels B & C, plotted for different reset methods. We run 20 trials for all experiments in this figure. All curves correspond to medians with shaded regions representing the 25th and 75th percentiles.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure B.2: Related to Fig 2. Regardless of validation accuracy threshold, Frozen-Net INPolarity always learn more quickly. Same curves as in Fig 2 right two columns, plotted for different validation accuracy thresholds.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure B.3: Related to Fig 3. All scenarios plotted separately instead of plotting only for the differences.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure B.4: Related to Fig 3. Regardless of validation accuracy threshold, transferring and fixing polarities help networks learn faster than traditional weight transfer strategy. Same curves as in Fig 3 right two columns, plotted for different validation accuracy thresholds.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure B.5: Tracking number of polarity flips. Weight polarity flips were analyzed for Fluid RAND-Polarity (SGD with random initialization) and measured by the ratio of weight parameters (excluding bias terms) flipped sign between two consecutive epochs. The first 50 epochs were analyzed and plotted, separately across layers and training data size. Curves are median with shaded area representating the 25th and 75th percentiles out of the 20 trials.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure B.6: Weight magnitude distribution at initialization across all experimental conditions for image classification tasks. All scenarios follow the exact same magnitude initialization magnitude except IN-weight transfer.\n\nC METHODS\n\nXOR-5D Data were prepared by sampling from the XOR-5D distribution as described in the main text, the training data size varied, validation set is always 1000 samples across all scenarios. A single hidden layer network (64 hidden units for Fig 1, various for Fig 4) was trained for 100 epochs; in total 50 randomly seeded initializations (trials) were ran. To have the best controlled experiments, we fixed the magnitude distribution, architecture, training samples (n varies), learning rate, batch sequence of the training data, and the validation samples across all tested scenarios. Statistical efficiency was quantified by plotting validation error rate at convergence across different training data scarcity levels. Computational efficiency was quantified by plotting the number of epochs to reach certain level of validation accuracy. We note not all trails could reach the same level of validation accuracy cut-off, to present the whole picture, we 1) plotted the percentage of trials that reached the validation accuracy cut-off (success rate); 2) showed results across a range of cut-off selections (supp Fig B.2, B.3, B.4).\n\nImage classification Experiments were done essentially the same as XOR-5D in a tightly controlled fashion, same magnitude distribution, batch sequence (batch size = 1000), learning rate, and training data across all scenarios. lr=0.001 was chosen from [0.03, 0.01, 0.005] based on the most accuracy gain after 50 epochs across all scenarios. Networks were trained for 100 epochs, 20 trials in total. We did not do any image augmentation. All performance quantification follow the above description.\n\nAll experiments were run on 2 RTX-8000 GPUs.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nD XOR THEOREMS RELATED TO SEC 5\n\nLemma D.1 (minimum XOR solution, 2D). For any single-hidden-layer network to be able to solve XOR, it is sufficient to have 3 different XOR-compatible hidden units. The weight pattern for each unit can be given by a triplet (w(1) j,1 ), where j is the unit index, j ∈ {1, . . . , n}, n is the number of hidden units. An XOR compatible hidden unit is one where the weight pattern triplet have their polarities satisfy sign(w(2)\n\n1,j , w(1)\n\n2,j , w(2)\n\nj,1 ) = sign(w(1)\n\n1,j ) × sign(w(1)\n\n2,j ).\n\nSee proof on page 20.\n\nLemma D.2 (can learn XOR probability, 2D). For a single hidden layer network with n hidden units (n ∈ Z+), randomly initialize each weight with its polarity following P (polarity) ∼ Bernoulli(0.5), then the probability that this randomly initialized network can learn XOR without changing any of the weight polarity is lower bounded by\n\nΩ(P (n)) = 1 −\n\n(cid:80)2\n\nk=0 Q(k, 4, 4, n)\n\n8n\n\n, n ∈ Z+\n\nQ(k, m, M, n) = [(M + k)n −\n\nk−1 (cid:88)\n\np=0\n\nQ(p, k, M, n)]\n\n(cid:19)\n\n(cid:18)m k\n\nQ here is a helper function for counting, it is defined in more detail in the proof.\n\nSee proof on page 21.\n\nTheorem D.1 (can learn XOR probability, high dimensional). For a single hidden layer network with n hidden units, randomly initialize each weight with its polarity following P (polarity) ∼ then the probability that this randomly initialized network can learn highBernoulli(0.5), dimensional XOR (first two dimensions are relevant, the rest (d − 2) dimensions are irrelevant) without changing any of the weight polarities is lower bounded by\n\nΩ(P (n, d)) = 1 −\n\n(cid:80)2\n\nk=0 Q(k, 2d, 2d, n)\n\n(2d+1)n\n\n, n, d ∈ Z+, d ≥ 2\n\nQ(k, m, M, n) = [(M + k)n −\n\nk−1 (cid:88)\n\np=0\n\nQ(p, k, M, n)]\n\n(cid:19)\n\n(cid:18)m k\n\nQ here is a helper function for counting, it is defined in more detail in the proof.\n\nSee proof on page 22.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nE PROOFS\n\nLemma 2.1 (capacity-speed trade-off). If the weight polarities are set a priori, such that the function is still representable, then the network can learn faster.\n\nProof of Lemma 2.1. A feedforward DNN can be described by as a graph\n\nG = (V, E), w : E → D.\n\nNodes of the graph correspond to neurons (units), where each neuron is a function σ(l) j (x) = σ(W (l) j x + b(l)), j ∈ [nl]. All the weights for the network take on values from the set D = {−d, . . . , 0, . . . , d} for some d ∈ N. The network is organized in layers. That is, the set of nodes can be decomposed into a union of (nonempty) disjoint subsets, V = ̇∪L l=0Vl, such that every edge in E connects some node in Vl−1 to some node in Vl, for some l ∈ [L]. Assume we have fully connected layers. Then the number of incoming edges per node is |Vl−1|. Let |V0| be the input space dimensionality. All nodes in V0 (input layer) and VL (output layer) are distinct. Let |G| denote the total number of distinct weight patterns of the graph G. Then for a single hidden layer network where L = 2, we have:\n\nThen\n\n|G| = |D||E| = |D|(|V0|∗|V1|+|V1|∗|V2|)\n\n|G| |GpolarityF rozen|\n\n= (\n\n2d + 1 d + 1\n\n)\n\n(|V0|∗|V1|+|V1|∗|V2|)\n\n(2)\n\n(3)\n\nAssume a different weight pattern represents a different function (trivially holds in linear + full rank weight case), then for every single representable function, there always exists a set of weight polarity configurations Gcorrect such that Gcorrect ⊆ GpolarityF rozen, therefore\n\n|Gcorrect| |GpolarityFrozen|\n\n≪\n\n|Gcorrect| |G|\n\n(4)\n\nThis means setting weight polarity a priori in an adequate way constraints the combinatorial search space to have much higher proportion of correct solutions, hence easier to learn under exhaustive search algorithm (Lemma 2.1).\n\nLemma D.1 (minimum XOR solution, 2D). For any single-hidden-layer network to be able to solve XOR, it is sufficient to have 3 different XOR-compatible hidden units. The weight pattern for each unit can be given by a triplet (w(1) j,1 ), where j is the unit index, j ∈ {1, . . . , n}, n is the number of hidden units. An XOR compatible hidden unit is one where the weight pattern triplet have their polarities satisfy sign(w(2)\n\n1,j , w(1)\n\n2,j , w(2)\n\nj,1 ) = sign(w(1)\n\n1,j ) × sign(w(1)\n\n2,j ).\n\nProof of Lemma D.1. For each hidden unit, we can enumerate all of its 8 possible weight polarity configurations, with the index set AP olarities = {1, 2, 3, 4, 5, 6, 7, 8}:\n\n# 1\n2 3\n4 5\n6 7\n8\n\n1,j\n\nw(1) +\n+ +\n+ -\n- -\n-\n\n2,j\n\nw(1) +\n+ -\n- +\n+ -\n-\n\nj,1\n\nw(2) +\n- +\n- +\n- +\n-\n\nThe set BXOR−P olarities = {2, 3, 5, 8} ⊂ AP olarities contains indexes of polarity patterns that follow XOR. To prove Lemma D.1, we first consider the case of 3 hidden unit single layer network. To prove a network of certain polarity pattern is capable of solving XOR, it suffices to give a working solution. Below, we exhaust all possible 3-unit network that satisfy the statement of Lemma D.1, i.e. having 3 different XOR-compatible units; and list out their corresponding working network solutions.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n1,j w(1)\n\n2,j\n\nj,1 working network F (x, y)=\n\n{}\n\n{2,3,5}\n\n{2,3,8}\n\n{2,5,8}\n\n{3,5,8}\n\n# w(1) +1 2\n1 3\n0 5\n+1 2\n+1 3\n0 8\n0 2\n-1 5\n-1 8\n0 3\n-1 5\n-1 8\n\n+1 0\n1 0\n-1 -1 +1 +1 0\n-1 0\n-1\n\nb(1) j\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n\nw(2) -1 1\n1 -1 +1 -1 -1 +1 -1 +1 +1 -1\n\nsigmoid(−σ(x + y) + σ(x) + σ(y))\n\nsigmoid(−σ(x) + σ(x − y) − σ(−y))\n\nsigmoid(−σ(y) + σ(y − x) − σ(−x))\n\nsigmoid(σ(−y) + σ(−x) − σ(−x − y))\n\nFor the case that single-hidden-layer network has more than 3 hidden units, if it satisfies the rule in Lemma D.1, we can always construct a XOR-solvable network solution by setting all other weights to zero except for 3 different XOR-compatible units, then we arrive at one of the four situations in the above table and we just proved they are XOR solutions.\n\nTherefore, for any single hidden layer network to solve XOR, it is sufficient to have at least 3 of the 4 XOR polarity patterned units.\n\nLemma D.2 (can learn XOR probability, 2D). For a single hidden layer network with n hidden units (n ∈ Z+), randomly initialize each weight with its polarity following P (polarity) ∼ Bernoulli(0.5), then the probability that this randomly initialized network can learn XOR without changing any of the weight polarity is lower bounded by\n\nΩ(P (n)) = 1 −\n\n(cid:80)2\n\nk=0 Q(k, 4, 4, n)\n\n8n\n\n, n ∈ Z+\n\nQ(k, m, M, n) = [(M + k)n −\n\nk−1 (cid:88)\n\np=0\n\nQ(p, k, M, n)]\n\n(cid:19)\n\n(cid:18)m k\n\nQ here is a helper function for counting, it is defined in more detail in the proof.\n\nProof of Lemma D.2. For any network randomly initialized, its weight polarity pattern is a set of n draws with replacement from the polarities indexed by the set AP olarities, |AP olarities| = 8. Define H = (h1, . . . , hm, . . . , hn), hm ∈ AP olarities, |H| = n, to be the tuple of the indices of the observed weight patterns for the hidden layer. hm is the index of the weight polarity pattern for unit m. For any network to be able to solve XOR, it needs to have at least 3 units whose weight patterns are distinct members of set BXOR−P olarities (Lemma D.1). That is, let J = {hm : hm ∈ H, hm ∈ BXOR−P olarities}. Then we need that |J| ≥ 3. We can define the probability of having exactly k of the 4 XOR compatible weight patterns present within the hidden layer as following (for brevity, A = AP olarities, B = BXOR−P olarities): None of the members in set B appears is given by:\n\nP (|J| = 0) =\n\n|B|n |A|n = (\n\n4 8\n\n)n = (\n\n1 2\n\n)n\n\nOnly one of the member in set B appeared in H, and that member can appear more than once in H is given by:\n\nP (|J| = 1) =\n\n(cid:0)|B| 1\n\n(cid:1)((|B| + 1)n − |B|n)\n\n|A|n\n\n(cid:0)4 1\n\n(cid:1)(5n − 4n) 8n\n\n=\n\nThis is explained by choosing one of 4 members of B (cid:0)|B| appears at least once ((|B| + 1)n − |B|n)\n\n1\n\n(cid:1), then multiply by the chosen member\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nOnly two of the members in set B appeared in H, and both can appear more than once in H:\n\nP (|J| = 2) =\n\n=\n\n(cid:0)|B| 2\n\n(cid:1)((|B| + 2)n − (cid:0)2\n\n(cid:1)((|B| + 1)n − |B|n) − |B|n)\n\n1\n\n(cid:0)4 2\n\n(cid:1)(6n − (cid:0)2\n\n1\n\n|A|n (cid:1)(5n − 4n) − 4n)\n\n8n\n\n(cid:1) ways of choosing the 2 members To count two members appear at least once in H, we have (cid:0)|B| from set B, and there are (|B| + 2)n ways of choosing with replacement to populate the tuple H, where each hidden unit can choose from in total (|B| + 2) possible patterns, and subtract the situation where only one of the two members appeared (cid:0)2 (cid:1)((|B| + 1)n − |B|n) and the situation where neither appeared |B|n The above equations can be put into a compact form\n\n2\n\n1\n\nP (|J| = k) =\n\nQ(k, |B|, |A| − |B|, n) |A|n\n\n(cid:1), k ∈ {0, . . . , m}, m ∈ where Q(k, m, M, n) = [(M + k)n − (cid:80)k−1 1, . . . , |B|, M ∈ 1, . . . , |A| − |B|, n ∈ Z+ is a helper function that gives exactly k of the m different XOR-compatible polarity patterns appeared in n units, and in total m + M options are considered for each unit.\n\np=0 Q(p, k, M, n)](cid:0)m\n\nk\n\nThen, P (n) is counting at least 3 of the 4 set B patterns appear:\n\nΩ(P (n)) = P (|J| ≥ 3) = 1 − P (|J| = 0) − P (|J| = 1) − P (|J| = 2)\n\n(cid:80)2\n\nk=0 Q(k, 4, 4, n)\n\n= 1 −\n\n= 1 − (\n\n1 2\n\n)n −\n\n8n (cid:0)4 1\n\n(cid:1)(5n − 4n) 8n\n\n−\n\n(cid:0)4 2\n\n(cid:1)(6n − (cid:0)2\n\n1\n\n(cid:1)(5n − 4n) − 4n)\n\n8n\n\n.\n\nTheorem D.1 (can learn XOR probability, high dimensional). For a single hidden layer network with n hidden units, randomly initialize each weight with its polarity following P (polarity) ∼ Bernoulli(0.5), then the probability that this randomly initialized network can learn highdimensional XOR (first two dimensions are relevant, the rest (d − 2) dimensions are irrelevant) without changing any of the weight polarities is lower bounded by\n\nΩ(P (n, d)) = 1 −\n\n(cid:80)2\n\nk=0 Q(k, 2d, 2d, n)\n\n(2d+1)n\n\n, n, d ∈ Z+, d ≥ 2\n\nQ(k, m, M, n) = [(M + k)n −\n\nk−1 (cid:88)\n\np=0\n\nQ(p, k, M, n)]\n\n(cid:19)\n\n(cid:18)m k\n\nQ here is a helper function for counting, it is defined in more detail in the proof.\n\nProof of Theorem D.1. We solve Theorem D.1 with the exact same counting algorithm as in Lemma D.2, the only difference is now |AP olarities| = 2d+1 and |BXOR−P olarities| = 2d. We prove these two equalities below.\n\n|AP olarities| = 2d+1 because we have d input weights and 1 output weight for each unit.\n\nThe weight pattern of a single hidden unit in this case is given by a tuple (w(1) j,1 ), j ∈ {1, . . . , n} and n is the number of hidden units. Our conclusion in Lemma D.1 trivially extends to the high dimensional case. This is because high-dimensional-XOR-solvable network solutions can be trivially constructed from Lemma D.1 by setting the irrelevant input dimension weights to 0. We can restate Lemma D.1 for high-dimensional XOR as following:\n\n1,j , . . . , w(1)\n\nd,j , w(2)\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFor any single-hidden-layer network to be able to solve high dimensional XOR (only first two dimensions are relevant), it is sufficient to have 3 different XOR-compatible hidden units. A high dimensional XOR compatible unit polarity pattern is ruled by sign(w(2) 1,j ) × sign(w(1)\n\nj,1 ) = sign(w(1)\n\n2,j ).\n\nTherefore the irrelevant dimension input weights can be of either polarities and there are 2d−2 different combinations of them. Therefore |BXOR−P olarities| = 4 × 2d−2 = 2d.\n\nFollow the steps in Lemma D.2, we have (for brevity, A = AP olarities, B = BXOR−P olarities)\n\nP (|J| = 0) =\n\nP (|J| = 1) =\n\nP (|J| = 2) =\n\n|B|n |A|n = (\n\n2d 2d+1 )n (cid:1)((|B| + 1)n − |B|n)\n\n(cid:0)|B| 1\n\n|A|n (cid:1)((|B| + 2)n − (cid:0)2\n\n1\n\n(cid:0)|B| 2\n\n(cid:0)2d 1\n\n=\n\n(cid:1)((2d + 1)n − (2d)n)\n\n(2d+1)n\n\n(cid:1)((|B| + 1)n − |B|n) − |B|n)\n\n|A|n\n\n(cid:0)2d 2\n\n=\n\n(cid:1)((2d + 2)n − (cid:0)2\n\n(cid:1)((2d + 1)n − (2d)n) − (2d)n)\n\n1\n\n(2d+1)n\n\n(cid:1)((2d + 2)n − (cid:0)2\n\n(cid:1)((2d + 1)n − (2d)n) − (2d)n)\n\n1\n\n(2d+1)n\n\n,\n\nTherefore, we have\n\nΩ(P (n, d)) =1 −\n\n(cid:80)2\n\nk=0 Q(k, 2d, 2d, n)\n\n(2d+1)n\n\n=1 − (\n\n1 2\n\n)n −\n\n(cid:0)2d 1\n\n(cid:1)((2d + 1)n − (2d)n)\n\n(2d+1)n\n\n(cid:0)2d 2\n\n−\n\nn, d ∈ Z+, d ≥ 2\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper explores the computational features that result from a neural network with weights of fixed polarity. The authors demonstrate through proofs in constrained settings, and experiments in both constrained and more general settings, that networks with fixed and appropriately initialized weight polarity learn faster than randomly initialized networks. They also demonstrate that for randomly initialized networks of sufficient size, constraining the polarity of the weights in the update function does not compromise learning.\n\nThe paper proposes  a learning algorithm which is a variant on stochastic gradient descent but which does not permit polarity changes in the weights (Freeze-SGD). The paper compares networks trained under this algorithm (frozen nets) with networks trained under regular SGD, including networks that have previously had some pretraining. They present a theoretical trade-off between network representational capacity and learning speed, as a function of the network’s polarity being fixed.\n\nIn experiment 1 they consider single-layer networks on the 5D-XOR problem (the first 2 dims are classic XOR and the final 3 dims are noise). They compare learning rates of 3 networks to consider the influence of two factors: fixed weight polarities and well-chosen polarity patterns. The first net is initialised with the weights set to have the correct polarity combination for the task (a priori knowledge) and which uses a polarity-frozen update rule the authors propose. The second network uses this same update rule but is initialised with random small weights (presumably of the same scale), and the final network uses random weights and vanilla SGD. The network which had both the correct % polarity and used the polarity-preserving update rule learned the fastest.\n\nIn experiment 2 they consider image classification with AlexNet on the datasets Fashion-MNIST and CIFAR-10, and consider the same three initialisation and update conditions.  \n\nIn experiment 3 they compare initialising the network with ImageNet-trained weights vs Imagenet-trained polarity %s, to assess whether transfer between tasks is more effective when transferring the weights magnitudes and polarities or simply the  polarity alone (both under the polarity-preserving update rule). This yielded a very surprising result that polarity alone resulted in faster learning on the new tasks. \n\nThe paper then calculates the probability that a randomly initialised network with a fixed polarity update rule will never be able to learn the XOR task. The experimental results closely match the theoretical results.\n\n# Strength And Weaknesses\n\n*Strengths:*\n- A very interesting idea of broad and general interest to both the computational neuroscience community and the ML community.\n- The authors explore the idea of fixed weight polarity is some depth, discussing the theoretical benefits and costs of fixed-polarity networks (costs: limits on representational capacity, pros: learning speed).\n- Nicely designed experiments in both toy and more realistic settings (however see comment below on an important way they can be improved).\n- Beautifully written.\n\n*Weaknesses:*\n- In all experiments, but most crucially in experiments 1 and 2: The authors should do the final control experiment in which they initialize the network with the same weights as Freeze-sufficient polarity (set to be the correct % polarity) but without the frozen learning algorithm. This will truly isolate the two factors of (1) initially setting the weights % with the correct polarity, and (2) keeping that weight polarity throughout training. This yields a fully factorized experimental design which is tidier for isolating the two contributing factors and their interactions. It may be that initializing the network with the correct polarities but without constraining these polarities via the update rule, is sufficient for the network to gain the same benefits. This experiment would tell us these sufficiency conditions.\n\n- I couldnt see whether in each experiment the initial weights were set with the same generative process for the different experimental conditions (and so have the same magnitude distribution). To ensure that different weight magnitude distributions (which are known to affect learning rates) are not playing a role in these results, the Freeze- in polarity vs Freeze Rand, vs Fluid weights should all start with the same magnitude distribution, despite the differences in polarity. For these results to be interpretable this must be true - and so I assume this is the case - but I could not see this mentioned in the text (perhaps I missed it) and it’s quite an important feature to highlight.\n\n- Could the authors comment on whether randomly initialised networks trained with SGD learn polarity early on in training? I am aware that weight magnitudes are typically learned very early on in training but am not sure whether polarity is also a commonly learned early feature of weights with SGD. If this is known, it would help to complete the picture and could be added to the discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper communicated its ideas very clearly and these ideas (to my knowledge) appear very novel. There is one point on weight initialisation that the authors should clarify for reproducibility and interpretability to ensure this paper has as much impact as it has the potential to have.\n\n# Summary Of The Review\n\nThis paper explores the computational features that result from a neural network with weights of fixed polarity. The authors demonstrate through proofs in constrained settings, and experiments in both constrained and more general settings, that networks with fixed and appropriately initialized weight polarity learn faster than randomly initialized networks, and that for randomly initialized networks of sufficient size, constraining the polarity of the weights in the update function does not compromise learning. In my opinion the findings are novel and very interesting, but subject to a few caveats that need confirming.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDFLOW: LEARNING TO SYNTHESIZE BETTER OPTICAL FLOW DATASETS VIA A DIFFERENTIABLE PIPELINE\n\nKwon Byung-Ki1, Nam Hyeon-Woo1, Ji-Yun Kim1, Tae-Hyun Oh1,2,3 1Department of Electrical Engineering, POSTECH 3Institute for Convergence Research and Education in Advanced Technology, Yonsei University {byungki.kwon, hyeonw.nam, junekim, taehyun}@postech.ac.kr\n\n2Graduate School of AI, POSTECH\n\nABSTRACT\n\nComprehensive studies of synthetic optical flow datasets have attempted to reveal what properties lead to accuracy improvement in learning-based optical flow estimation. However, manually identifying and verifying the properties that contribute to accurate optical flow estimation require large-scale trial-and-error experiments with iteratively generating whole synthetic datasets and training on them, i.e., impractical. To address this challenge, we propose a differentiable optical flow data generation pipeline and a loss function to drive the pipeline, called DFlow. DFlow efficiently synthesizes a dataset effective for a target domain without the need for cumbersome try-and-errors. This favorable property is achieved by proposing an efficient dataset comparison method that uses neural networks to approximately encode each dataset and compares the proxy networks instead of explicitly comparing datasets in a pairwise way. Our experiments show the competitive performance of our DFlow against the prior arts in pre-training. Furthermore, compared to competing datasets, DFlow achieves the best fine-tuning performance on the Sintel public benchmark with RAFT.\n\n1\n\nINTRODUCTION\n\nOptical flow is a fundamental computer vision problem to find dense pixel-wise correspondences between two subsequent frames in a video. Optical flow is indeed a key building block in many practical applications, including video understanding, action analysis, video enhancement, editing, 3D vision, etc. Recently, optical flow has been significantly advanced by learning-based approaches with deep neural networks (Fischer et al., 2015; Ilg et al., 2017; Ranjan & Black, 2017; Hui et al., 2018; Sun et al., 2018; Teed & Deng, 2020) in terms of accuracy and efficiency.\n\nA driving force of these prior arts is large-scale supervised datasets. However, it is difficult to collect a reasonable amount of real-world optical flow labels. Thus, they exploited large-scale synthetic datasets, e.g., Fischer et al. (2015); Mayer et al. (2016), which has become the standard in optical flow, e.g., training on FlyingChairs (Fischer et al., 2015) followed by FlyingThings3D (Mayer et al., 2016). After the seminal studies, there have been various efforts to build different synthetic datasets (Gaidon et al., 2016; Richter et al., 2017; Lv et al., 2018; Oh et al., 2018; Aleotti et al., 2021). Despite the vast efforts of these studies, it remains unclear which factors are important for an effective synthetic dataset construction against the target domain.\n\nInstead of manually identifying important design criteria, AutoFlow (Sun et al., 2021) pioneers the first learning-based approach to go beyond being heuristic by posing data generation as a hyperparameter optimization problem maximizing validation performance on a target dataset. AutoFlow generates data samples by composing simple 2D layers with non-differentiable hyperparameters, which are optimized by sampling-based evolutionary search. The use of evolutionary search requires large resources, which is burdensome because each target scenario requires to re-generate different datasets.\n\nTo address this challenge, we propose DFlow, which is an efficient synthetic optical flow dataset generation method. We compose each data sample by simple differentiable graphic operations, such as warping layer and real-world effects, so that each sample can be parameterized in a learnable manner. This allows us to exploit efficient gradient descent methods to generate each sample, and\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthereby DFlow is more than an order of magnitude efficient than AutoFlow in GPU hours when constructing the same amount of training data.\n\nWe also introduce a new loss function that learns the data parameters by contrasting a target dataset from a base dataset, e.g., FlyingChairs. Since directly using large datasets in the contrastive learning process is cumbersome, we approximate the base and target datasets by two neural networks trained on respective datasets as proxies. This approximation allows an end-to-end differentiable pipeline from the data parameters to the loss function.\n\nThrough comprehensive experiments, we show that DFlow is effective in both pre-training and finetuning. The DFlow data has a size of 512 × 384, which is the same as FlyingChairs, but the RAFT network (Teed & Deng, 2020) pre-trained on DFlow achieves comparable performance compared to the high-resolution competing datasets (Sun et al., 2021; Mayer et al., 2016). In addition, compared to competing datasets, the RAFT model initially pre-trained on DFlow achieves the best fine-tuning performance on the Sintel public benchmark. We summarize our contributions as follows:\n\n• A simple and efficient differentiable data generation pipeline for optical flow (refer to Table 1);\n\n• A contrastive-style learning scheme and its loss function by approximating expensive dataset-to-\n\ndataset comparison by leveraging proxy neural networks (refer to Sec 3);\n\n• Compared to competing datasets, DFlow achieves the best fine-tuning performance on the Sintel\n\npublic benchmark with RAFT. (refer to Table 4).\n\n2 RELATED WORK\n\nOptical Flow. Dense optical flow estimation is to find pixel-wise correspondences from the brightness patterns of images (Gibson, 1950; Gibson & Carmichael, 1966; Horn & Schunck, 1981). After conventional optimization algorithms (Black & Anandan, 1993; Zach et al., 2007), deep-learning algorithms (Fischer et al., 2015; Ilg et al., 2017) become dominant due to their computational efficiency and reasonable performance. Prior arts (Xu et al., 2017; Bailer et al., 2017; Wulff et al., 2017; Sun et al., 2018) have attempted to implement explicit neural modules that are suitable for optical flow estimation. Recently, RAFT (Teed & Deng, 2020) adopts recurrent architectures and achieves a notable performance improvement, which is represented as state-of-the-art.\n\nThose recent advances in learning-based approaches require large-scale data with ground-truth, but labeling dense optical flow is a highly undetermined task, i.e., challenging (Fischer et al., 2015). The previous real-world datasets have been built under sophisticated labeling conditions, including the special sensor hardware, controlled environment, or limited objects (Scharstein & Szeliski, 2002; Scharstein & Pal, 2007; Geiger et al., 2012; Kondermann et al., 2014). It leads to the limitation of the size of datasets. To relieve this issue, synthetic datasets (Fischer et al., 2015; Mayer et al., 2016) have been proposed and achieved remarkable accuracy despite the gap between real and synthetic datasets. After that, previous arts endeavor to construct more realistic synthetic datasets (Gaidon et al., 2016; Richter et al., 2017; Lv et al., 2018). The prior arts (Aleotti et al., 2021; Han et al., 2022) generate the subsequent frames and ground-truth optical flow by warping the previous frame. These do not handle the photometric inconsistency that is common in real-world scenes. In this work, we propose a differentiable synthetic data generation pipeline with the target knowledge so that the generated dataset could improve its performance more.\n\nLearning-based Optical Flow Dataset. AutoFlow (Sun et al., 2021) is the first learning-based data generation approach in optical flow, but with the sampling-based evolutionary search for nondifferentiable optimization. It is our closest related work in the sense that they learn the data generation parameters for performance improvement on the specific target dataset. However, distinctively, our method is the first differentiable method to learn data generation parameters, which leads to a more efficient pipeline than AutoFlow in terms of computation cost and GPU hours for data generation. We list other differences in Table 1. Recently, RealFlow (Han et al., 2022) proposes an iterative learning framework that learns enhanced flow estimation and pseudo ground-truth generation, alternatively. Different from our work, this work suggests a framework including iterative model training and dataset generation, and does not suggest a goodness measure of a resulting dataset, which we address. Other than optical flow, there are also interesting attempts to generate synthetic data in learnable\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nAutoFlow (Sun et al., 2021)\n\nDFlow (Ours)\n\nLearning method Computational resource GPU hours for constructing a dataset Data resolution\n\nEvolutionary search\n\nGradient descent 48 P100 GPUs A single V100 GPU 9.3 days / V100 512 × 384\n\n336 days / P100 1280 × 720\n\nTable 1: Comparison of dataset generation methods of AutoFlow and DFlow (ours).\n\nways (Sixt et al., 2018; Yang & Deng, 2020; Kaspar et al., 2019; Zhao et al., 2021) for other computer vision applications.\n\n3 DIFFERENTIABLE DATA GENERATION PIPELINE\n\nOur data generation pipeline synthesizes subsequent frames It and It+1, and a flow (motion) map FGT between those two frames. We concisely denote a pair of subsequent frames and a flow map as a data sample. We parameterize each data sample with a learnable parameter vector θ that characterizes a composition of elemental graphic operations, such as color perturbation and geometric warping. We refer to the composition process as a data generator, which is designed to be differentiable.\n\nWith this differentiable data generator part, we specifically seek to synthesize optical flow data improving the accuracy on the target dataset. We define a loss function Ltotal = Ltask + Lreg to effectively update the learnable parameters, which drives the data generation to an improving direction. With this loss, we efficiently learn the parameters {θ} in a fully-differentiable way.\n\nThe overview of our data generation pipeline is illustrated in Fig. 1, called DFlow. In Sec. 3.1, we first describe the design of the loss function. In Sec. 3.2, we describe the data generator method from the data parameters {θ}. Further detailed descriptions and implementation details can be found in Appendix A.\n\nFigure 1: An overview of DFlow. We parameterize data samples to be synthesized. The learnable data parameters {θ} are updated by our proposed loss function.\n\n3.1 TASK LOSS FUNCTION\n\nGiven our differentiable data generator, a sample parameter θ is rendered to a data sample, i.e., a pair of subsequent frames It(θ) and It+1(θ) and those corresponding flow map FGT (θ). By rendering a collection of data samples with the data generator and a set of {θ}, we can construct a dataset. To find an updated parameter θ∗ that improves the accuracy of the optical flow model, we need an effective criterion to drive the parameter update.\n\nAs a tractable criterion to efficiently find such a dataset, we are motivated by learning-to-augment and dataset condensation approaches (Sixt et al., 2018; Kaspar et al., 2019; Zhao et al., 2021). They synthesize data to have characteristics similar to target data under the motivation that learning with two similar datasets is likely to yield accuracy improvement (Ben-David et al., 2010; Kaspar et al., 2019), which is typically achieved by distribution matching. However, directly comparing combinatorial pairs of samples in the target dataset and synthesized one is computationally expensive, and even intractable during iteratively updating a synthesized dataset.\n\nTo efficiently deal with this issue, we propose to encode the target dataset into a proxy neural network, called target network. With an optical flow neural network ftarget(It, It+1) → F, we pre-train the network with a given small target optical flow dataset, which yields the trained target network. We deem the target network as a differentiable proxy of the target dataset that encodes the knowledge of target data. Compared to using the target data as a set in set-to-set comparison, leveraging this differentiable proxy allows the whole procedure to be tractable. With the target network, we define a target loss Ltarget (ftarget (It(θ), It+1(θ)) , FGT (θ)) that measures flow errors for a data sample associated with θ in the view of the target domain. We use the same loss used for pre-training the target network, e.g., the sequence loss for RAFT (Teed & Deng, 2020), for the target loss Ltarget(·).\n\n3\n\nData parameterL!\"#$+L%&’Loss{θ(}()*+Update {θ(}()*+Differentiable data generation pipeline (DFlow)DifferentiabledatageneratorPublished as a conference paper at ICLR 2023\n\nWith the target network and loss, we could first attempt to update the parameters {θ} by\n\n{θ∗} = arg min{θ}\n\n(cid:88)\n\ni\n\nLtarget (ftarget (It(θi), It+1(θi)) , FGT (θi)) .\n\n(1)\n\nIf the target loss is small for a data sample, it means that the target network is familiar with the given data sample, and the sample is close to one of the target data, i.e., similar characteristics to the target data. In this sense, Eq. (1) distills the target knowledge into generated data. However, in our empirical preliminary study, we observed instability and under-fitting issues with Eq. (1), where optimizing the loss is stuck at a high value. We assume that target networks trained on the popular benchmarks (Butler et al., 2012; Menze & Geiger, 2015) may not produce sufficiently rich training signals for synthesizing data, which might be due to limited scales of those real-world benchmarks.\n\nTo complement more informative training signals, we employ the contrastive-style learning scheme by using another comparator network, called base network, which is trained with a common large-scale synthetic dataset, e.g., (Fischer et al., 2015; Mayer et al., 2016), as a base dataset. Similar to the target network, we pre-train the base network but on a large-scale dataset, FlyingChairs (Fischer et al., 2015), which is randomly generated. We use the base network similarly to the target network except for maximizing flow errors Lbase to implement a contrastive behavior, so that a resulting data sample should be closer to the target dataset while being different from the base (common) dataset. This contrastive behavior can generate combinatorial diversity of gradient signals by pairwise contrasting, which may more strongly drive the data generation to an improving direction in favor of a target domain than using the target network alone. To implement the contrastive behavior, we define a wrapping loss function, Ltask(Ltarget, Lbase): R2→R, called task loss.\n\nMultiplication\n\nForm\n\nWhat Function Type is Suitable for Ltask . There are many candidates for Ltask satisfying the aforementioned criteria. As a function form, we are first motivated by sample-wise weighting distillation (Zhou et al., 2021), where the loss function is dynamically weighted according to each data sample. The loss function is weighted by the target and base losses in a multiplication form. We additionally examine addition forms to select the best one. We consider tanh, sigmoid, and exponential functions to implement bounded contrasting behaviors.1 Table 2 shows the accuracy comparison of different forms of loss functions using 1, 000 data samples generated with the Sintel dataset (Butler et al., 2012) as a target. The addition form with the exponential function shows promising results in terms of performance, which corresponds to\n\nAddition\n\nLtask\n\nExponential Sigmoid Tanh\n\nExponential Sigmoid Tanh\n\nDataset\n\nSintel clean\n\nSintel final\n\n4.26 4.12 2.02\n\n1.85 2.00 1.95\n\n4.49 4.35 3.35\n\n3.09 3.13 3.21\n\n*Bold denotes the best.\n\nTable 2: AEPE of different forms of Ltask. We train the RAFT network with different loss forms. The addition form with exponential shows promising results. Refer to Appendix A.2 for details.\n\nLtask(Ltarget, Lbase) = Ltarget + α exp(−β Lbase\n\nLtarget+ε + γ),\n\n(2)\n\nwhere we set the balance parameters {β, γ} to {1, 0}, and especially set α to 20 for the experiments in Table 2. We found that, depending on α, generated data characteristics are distinctive. When generating our final dataset, we first generate subsets of data with different α values, and ensemble the subsets into a single dataset by taking union, denoted as {α} combination. This is found to be notably effective. More details of the total loss can be found in Appendix.\n\n3.2 DATA GENERATOR\n\nIn this section, we describe the differentiable data generator that synthesizes a data sample from a parameter θ. As shown in Fig. 2, the data generator is composed of geometric warping, flow field translation, layer composition, color perturbation, and real-world effects. The data generator takes N pairs of layer images and masks, {Ll l=1. Similar to Oh et al. (2018), We randomly sample the layer images from public image datasets (Kuznetsova et al., 2020; Perazzi et al., 2016; Mayer et al., 2016; Lin et al., 2014) and layer masks from a segmentation dataset (Everingham et al.,\n\nl=1 and {Ml\n\n0}N\n\n0}N\n\n1We observe that contrasting without bounds may lead to instability and divergence. We list the specific\n\nequations of the used loss functions in Appendix A.2.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Schematic of data generator. The data generator is composed of color perturbation, two steps of geometric warpings, flow field translation, layer composition, and real-world effects. The whole pipeline is differentiably parameterized.\n\ngeometric warping parameters\n\nAlgorithm 1: PyTorch-style pseudo-code for data generator. # W1, W2: # Ds: translation parameters # C: white balance parameters # R: Real-world effect parameters # images0, masks0:\n\ninitial layer images and masks\n\n# Applying color perturbation images0 = ColorPerturbation(images0, C)\n\n# Warping the images and masks images1, masks1 = GeometricWarping(images0, masks0, W1) W2s = FlowFieldTranslation(W2, Ds) images2, masks2 = GeometricWarping(images1, masks1, W2s)\n\n# Superimposing the images, masks, and flow fields image1 = LayerComposition(images1, masks1) image2 = LayerComposition(images2, masks2) flow = LayerComposition(W2s, masks1)\n\n# Applying real-world effects to the subsequent images image1, image2 = RealworldEffect(image1, image2, R)\n\n2010). The data generator then outputs synthesized subsequent frames It and It+1 with ground-truth optical flow FGT . All the following pipeline is parameterized by θ.\n\n0}N\n\nt+1}N\n\nl=1 and {Ml\n\nGeometric Warping. Geometric warping consists of rigid transformation, perspective warping, and grid warping. The geometric warping generates a warping field. In the first geometric warping, we 0}N apply the same warping field W0→t+1 to {Ll l=1 and generate the layer images and t+1}N masks at the frame t + 1, {Ll l=1 and {Ml l=1. In addition to this globally shared geometric warping, we model the local movement of each layer, i.e., segmented objects, in the second geometric warping step. To generate complex optical flows of the frame t, we can apply independent warping fields to each layer image and mask. However, we observe poor optimization behaviors when we use all independent warping fields on each layer. Thus, we propose to use decomposed warping parameters for time t to reduce the number of parameters. We use a anchor geometric warping, Wt+1→t, shared across all layers and flow field translation (∆x, ∆y), {Dl ∈ R2}N l=1, for each layer, t+1→t = (Dl ◦ Wt+1→t), where ◦ denotes the warping and construct each warping of layers by Wl operation. This strategy is also beneficial in terms of optimization stability. Finally, the operations of geometric warping and flow field translation are as follows:\n\nLl\n\nt+1 = W0→t+1 ◦ Ll\n\n0, Ll\n\nt = Wl\n\nt+1→t ◦ Ll\n\nt+1 = (Dl ◦ Wt+1→t) ◦ Ll\n\nt+1\n\nMl\n\nt+1 = W0→t+1 ◦ Ml\n\n0, Ml\n\nt = Wl\n\nt+1→t ◦ Ml\n\nt+1 = (Dl ◦ Wt+1→t) ◦ Ml\n\nt+1.\n\n(3)\n\n(4)\n\n5\n\nGeometric warpingL!\"#$L!\"##M!\"##L!$L!#I!I!\"#F%&Layer compositionReal-world effects!L’$L’#M’$M’#Color perturbationStep containing learnable parameterStep not containing learnable parameterLearnable parameters (dimension)White balance R!⋯×NFlow field translationTranslation R(#$%)×(M!$M!#Texture noiseFogMotion blurFog color R)×)×!Anisotropic Gaussian kernel R!NoiseN*+%R,×-×!NoiseN*R,×-×!Rigid transformation R.Grid-warping R/×%(×!Perspective-warping R/⋅Geometric warpingRigid transformation R.Grid-warping R/×%(×!Perspective-warping R/M!\"#$⋯⋯⋯Published as a conference paper at ICLR 2023\n\nFigure 3: Generated data samples. (top) current frames; (bottom) optical flow visualizations.\n\nLayer Composition. After the geometric warping, we have N pairs of layer images, masks, and warping fields in order of depth. We superimpose each stack of layers to generate subsequent frames It and It+1, with ground-truth optical flow FGT . We can leverage alpha blending and softmax splatting (Niklaus & Liu, 2020). Both strategies show comparable performance, but we mainly use the softmax splatting strategy in our experiments. The comparison of alpha blending and softmax splatting can be found in Sec. 4.\n\nColor Perturbation and Real-world Effects. Prior studies (Mayer et al., 2018; Sun et al., 2021) have shown that synthetic data containing real-world effects, such as texture noises, fog, and motion blur, often brings the performance improvement of optical flow networks in generalization. Inspired by the observations, we introduce color perturbation and real-world effects into our data generator as well. Color perturbation adjusts the white balance of each layer image, and our real-world effects apply texture noises, fog, and motion blur, which are all parameterized to be controlled and updated.\n\nWe synthesize optical flow data by applying the above components: {Ll l=1 → {It, It+1, FGT } . Note that we apply regularizations to the grid warping and texture noise, i.e., grid and noise regularizations. The detail of each component, including regularizations, can be found in Appendix A.3-A.6. The overall differentiable data generator pipeline is summarized in Algorithm 1 as a pseudo-code. Each component has its own parameters, which are updated by our task loss and regularizations. See generated samples in Fig. 3.\n\nl=1, {Ml\n\n0}N\n\n0}N\n\nSummarization. We provide the summarization of DFlow below:\n\n1. We train base and target networks on base and target datasets, respectively. 2. We fix both networks and update the data parameter θ using our loss function. 3. We train optical flow networks with the generated dataset.\n\n4 RESULTS\n\nIn this section, we analyze the effects of our method in pre-training and fine-tuning perspectives with different optical flow models. We report the average end-point error (AEPE) for Sintel (Butler et al., 2012) and AEPE & F1 for KITTI 2015 (Menze & Geiger, 2015). From this section, we refer to our dataset generated by targeting Sintel with RAFT proxy models as DFlow, unless specified otherwise. Other details of experiment setups and implementation can be found in Appendix B.\n\nPre-training RAFT Results. Pre-training performance is one of the key factors in evaluating the applicability of optical flow datasets (Fischer et al., 2015; Mayer et al., 2016; 2018; Aleotti et al., 2021; Sun et al., 2021) by testing on benchmarks, i.e., Sintel and KITTI 2015 datasets. We train the RAFT networks (Teed & Deng, 2020) from scratch on respective competing datasets, including our DFlow. Table 3-(a) shows the pre-training results of each dataset. Compared to FlyingChairs which has the same data resolution as DFlow, the model trained on DFlow outperforms the one trained on FlyingChairs in both Sintel and KITTI 2015 datasets. Despite the fact that DFlow is around 1 4\nresolution of AutoFlow, DFlow achieves the best performance on the KITTI 2015 and Sintel clean datasets. We postulate that the real-world textures used in DFlow led to performance improvement on KITTI 2015. Also, we use both Sintel clean and final as the target datasets to generate DFlow, unlike AutoFlow using Sintel final only. It might affect performance on the Sintel clean and final datasets. Figure 4 shows the qualitative results obtained from FlyingChairs and DFlow. These results show that DFlow is effective for learning an accurate model in challenging scenes, such as shaded, foggy, and motion-blurred scenes.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Pre-training results. We train the (a) RAFT (Teed & Deng, 2020) and (b) FlowNet (Fischer et al., 2015) (c) GMA (Jiang et al., 2021) networks on respective pre-training datasets, and evaluate on the evaluation datasets, i.e., Sintel and KITTI 2015. Chairs→Things denotes the heterogeneous dataset experiment pre-training on FlyingChairs followed by FlyingThings3D.\n\nModel\n\nPre-training dataset\n\nSintel clean\n\nSintel final\n\nKITTI 2015\n\nEvaluation dataset\n\n(a) RAFT\n\n(b) FlowNet\n\n(c) GMA\n\nFlyingChairs AutoFlow DFlow (Ours)\n\nChairs→Things\n\nFlyingChairs AutoFlow DFlow (Ours)\n\nChairs→Things DFlow-GMA (Ours)\n\nAEPE\n\nAEPE\n\n2.28 2.08 1.81\n\n1.43\n\n4.71 5.43 3.45\n\n- -\n\n4.51 2.75 2.93\n\n2.71\n\n6.22 6.03 4.73\n\n- -\n\nAEPE / F1\n\n9.85 / 37.56 4.66 / 4.59 / 15.03\n\n-\n\n5.04 / 17.40\n\n20.36 / 62.20 19.64 / 43.95 12.94 / 38.95\n\n4.69 / 17.10 4.47 / 13.10\n\n*Bold denotes the best.\n\nFigure 4: Qualitative results. Top: pre-training results on the Sintel final pass. Bottom: pre-training results on KITTI 2015. The RAFT network pre-trained on DFlow shows robust results in challenging shaded, foggy, and motion-blurred scenes.\n\nModel Generalization Property. Since DFlow is generated with RAFT proxy networks, a question naturally arises about the effectiveness of the generated dataset and data generation pipeline on other model architectures. To evaluate the generalization ability of the generated dataset, i.e., DFlow, we pre-train the Flownet network on the DFlow dataset generated with the RAFT network. As shown in Table 3-(b), FlowNet pre-trained on DFlow shows noticeable improvement over the ones pre-trained on the competing datasets. We further investigate the generalization ability of the data generation pipeline. In this experiment, we select the KITTI 2015 dataset as the target dataset and generate the DFlow-GMA dataset using the GMA network (Jiang et al., 2021). Using our DFlow-GMA dataset, we train the GMA network and list the performance on KITTI 2015 in Table 3-(c). The GMA network trained on our DFlow-GMA achieves higher performance than the one trained on FlyingChairs followed by FlyingThings3D. From these observations, we postulate that our generated data and generation pipeline are agnostic to the deep network structure.\n\n7\n\nFlyingChairsGround TruthInput Image ItDFlowGround truthFlyingChairsDFlowPublished as a conference paper at ICLR 2023\n\nTable 4: Fine-tuning results on public benchmarks. We report F1 scores for KITTI 2015 and AEPE for Sintel public benchmarks. C, K, S, T, and H denote FlyingChairs, KITTI 2015, Sintel, FlyingThings3D, and HD1K (Kondermann et al., 2014). The results of Sintel public benchmarks are evaluated without the warm-start initialization (Teed & Deng, 2020).\n\nDataset schedule\n\nSintel clean\n\nSintel final\n\nKITTI 2015\n\nC → T → TSKH/K (Teed & Deng, 2020) AutoFlow → TSKHV (Sun et al., 2021) DFlow (Ours) → TSKH/K\n\n1.94 2.01 1.62\n\n3.18 3.14 3.07\n\n5.10 4.78 5.03\n\n*Bold denotes the best.\n\nTable 5: Analysis of data generation components. We analyze the effects of each generation component. The experiments are conducted by adding or removing one of the generation components. The default setting is with all components and the number of foregrounds parameter in the range of 8 to 12 except the {α} combination, which is annotated with underlines.\n\nEvaluation dataset\n\nSintel clean\n\nSintel final\n\nKITTI 2015\n\nExperiments\n\nData update\n\n{α} combination\n\nLayer composition\n\nNumber of foregrounds\n\nColor perturbation\n\nMotion blur\n\nFog\n\nTexture noise\n\nNoise regularization\n\nGrid regularization\n\nTarget dataset\n\nState\n\nOn Off\n\nOff On\n\nsoftmax splatting alpha blending\n\n0 2\n4 8-12\n\nOn Off\n\nOn Off\n\nOn Off\n\nOn Off\n\nOn Off\n\nOn Off\n\nSintel KITTI 2015\n\nAEPE\n\nAEPE\n\n1.86 9.70\n\n1.86 1.90\n\n1.86 1.95\n\n4.10 2.15 2.00 1.86\n\n1.86 2.07\n\n1.86 1.95\n\n1.86 1.98\n\n1.86 2.02\n\n1.86 1.91\n\n1.86 1.88\n\n1.86 2.14\n\n3.04 9.67\n\n3.04 3.02\n\n3.04 2.83\n\n4.89 3.32 3.15 3.04\n\n3.04 3.20\n\n3.04 3.57\n\n3.04 3.20\n\n3.04 3.15\n\n3.04 3.09\n\n3.04 3.10\n\n3.04 3.79\n\nAEPE / F1\n\n5.21 / 16.27 17.31 / 46.84\n\n5.21 / 16.27 4.90 / 15.78\n\n5.21 / 16.27 5.23 / 16.06\n\n7.56 / 23.76 5.73 / 17.60 5.41 / 16.33 5.21 / 16.27\n\n5.21 / 16.27 4.87 / 16.09\n\n5.21 / 16.27 5.06 / 16.24\n\n5.21 / 16.27 5.04 / 15.68\n\n5.21 / 16.27 5.47 / 16.56\n\n5.21 / 16.27 4.97 / 15.87\n\n5.21 / 16.27 5.09 / 16.35\n\n5.21 / 16.27 4.31 / 14.29\n\nFine-tuning Results on Public Benchmarks. Table 4 shows the fine-tuning results on the public benchmark test sets with corresponding dataset schedules. DFlow improves the original RAFT recipe (C→T→TSKH/K) on both benchmarks by replacing the conventional initial dataset schedule, i.e., FlyingChairs followed by FlyingThings3D (C→T), with our DFlow. In particular, compared to the competing datasets, DFlow achieves the best fine-tuning performance on the Sintel with RAFT. This result validates that DFlow as a pre-training dataset effectively affects fine-tuning results.\n\nAnalysis of Components. We analyze the effects of each generation pipeline component: {α} combination, the number of foregrounds, color perturbation, real-world effects, regularizations, and target dataset. For the fair and quick experiments, we generate 2k training data and train RAFT networks with the same training details. As shown in Table 5, we add or remove each of the components from a default setting to measure their effects with the pre-training experiment. The default setting indicates the dataset with all components and the number of foregrounds parameter in the range of 8 to 12 except the {α} combination. We observe that applying each component brings consistent performance improvement on the target dataset, i.e., Sintel, which is used for generating\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nDFlow; whereas the results on KITTI 2015, which is not a target data of DFlow, are inconsistent with the results of Sintel.\n\n• Data update. This analysis compares DFlow with randomly initialized data without any optimization, i.e., randomly generated by our pipeline. We train and compare the RAFT networks with those datasets, which show the effectiveness gap between the two datasets.\n\n• {α} Combination. We collect the same amount of data by combining subsets of data, which are obtained from diverse α values. This balances the performance between evaluation datasets because of the regularization effect.\n\n• Layer composition. The softmax splatting and alpha blending show comparable performance on Sintel. The softmax splatting improves the performance in the Sintel clean pass, but shows the performance drop in the Sintel final pass.\n\n• Number of foregrounds. Without any foreground, we observe the significant performance drop on Sintel and KITTI 2015. Adding only 2 foregrounds brings notable performance improvement.\n\n• Color perturbation and real-world effects. Without the color perturbation, the performance on Sintel drops moderately. Removing the motion blur significantly affects the accuracy on Sintel, especially in the Sintel final pass. The effects of fog and texture noise are moderate.\n\n• Regularizations. Removing the regularizations shows a slight performance drop in Sintel.\n\n• Target dataset. The generated dataset optimized to KITTI 2015 shows a notable performance gain on KITTI 2015, while the performance on Sintel significantly drops. This may be caused by the distribution gap between the two datasets.\n\nDataset\n\nRainy scene AEPE / F1\n\nRobustness to Corrupted Data (Whether Artifact). The photometric inconsistency degrades the robustness of optical flow estimation. For example, the rainy scene occurs photometric inconsistency. We evaluate the performance of RAFT on rainy scenes of Virtual KITTI (Gaidon et al., 2016). We choose FlyingChairs and RealFlow (Han et al., 2022) as baselines. Using the rainy scenes of Virtual KITTI as a target dataset, we generate an additional dataset, DFlow-V. Note that the target of DFlow is the Sintel dataset. Table 6 lists the performance of rainy scenes of virtual KITTI. As Han et al. (2022) have mentioned their limitation of discontinuous illumination, the performance of RealFlow is lower than others. DFlow outperforms the baselines, and DFlowV, which of the target is Virtual KITTI, shows much higher performance rather than others. From these results, we assume that our method can distill the characteristics of photometric inconsistency.\n\nTable 6: Analysis of photometric inconsistency scenario.\n\nFlyingChairs RealFlow DFlow DFlow-V\n\n7.22 / 24.95 7.75 / 27.57 5.87 / 22.89 3.11 / 12.61\n\n*Bold note the best.\n\n5 CONCLUSION AND DISCUSSION\n\nWe propose a new data generation pipeline for training optical flow networks. Our pipeline consists of geometric warping, real-world effects, etc., which are all parameterized differentiably. We propose a new objective function that drives our data optimization by leveraging the compressed knowledge of the proxy networks pre-trained on target and base datasets, respectively. Optical flow models trained on our datasets achieve favorable or superior performance against the competing datasets on pre-training and fine-tuning experiments. We conclude our paper with a discussion section.\n\nDiscussion. We use the pre-defined elementary data generation operations, e.g., fog, geometric warping, etc. While DFlow shows effectiveness in the real-world dataset, i.e., KITTI 2015, the pre-defined and restricted operations might not span all the real-world effects. Thus, diverse and complementary operations would further improve expressiveness and may lead to additional performance improvement. Our method also aims at a specific target dataset. For that, we need a target network trained on the target dataset, which requires at least some amount of optical flow annotations. To mitigate the requirement of the supervised data, it would be an interesting future direction to investigate the way to train the target network in an unsupervised method.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nThis paper develops an optical flow data generation method and uses the public datasets (Kuznetsova et al., 2020; Perazzi et al., 2016; Mayer et al., 2016; Lin et al., 2014; Everingham et al., 2010) for data generation. Therefore, the author does not expect any potential ethical issues related to sensitive information of the dataset.\n\nREPRODUCIBILITY STATEMENT\n\nWe introduce the whole data generation pipeline in Sec. 3. In Sec. 3.1, we present the loss function for data optimization. In Sec. 3.2, we describe the data generator method from the parameters. We list more details of the loss function and data generation pipeline in Appendix A.\n\nACKNOWLEDGEMENT\n\nThis work was partially supported by the National Research Foundation of Korea (NRF) grant (No. NRF-2021R1C1C1006799), and Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities; No.2021-0-02068, Artificial Intelligence Innovation Hub) funded by the Korea government (MSIT). This project is the result of “HPC Support” Project supported by the “Ministry of Science and ICT” and NIPA.\n\nREFERENCES\n\nFilippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning optical flow from still images. In\n\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nChristian Bailer, Kiran Varanasi, and Didier Stricker. Cnn-based patch matching for optical flow with thresholded hinge embedding loss. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151–175, 2010.\n\nMichael J Black and Padmanabhan Anandan. A framework for the robust estimation of optical flow.\n\nIn IEEE International Conference on Computer Vision (ICCV), 1993.\n\nD. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical\n\nflow evaluation. In European Conference on Computer Vision (ECCV), pp. 611–625, 2012.\n\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303–338, 2010.\n\nPhilipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H ̈ausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In IEEE International Conference on Computer Vision (ICCV), 2015.\n\nA Gaidon, Q Wang, Y Cabon, and E Vig. Virtual worlds as proxy for multi-object tracking analysis.\n\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nJames J Gibson. The perception of the visual world. 1950.\n\nJames Jerome Gibson and Leonard Carmichael. The senses considered as perceptual systems.\n\nHoughton Mifflin Boston, 1966.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch ̈olkopf, and Alexander Smola. A\n\nkernel two-sample test. volume 13, pp. 723–773, 2012.\n\nYunhui Han, Kunming Luo, Ao Luo, Jiangyu Liu, Haoqiang Fan, Guiming Luo, and Shuaicheng Liu. Realflow: Em-based realistic optical flow dataset generation from videos. In European Conference on Computer Vision, pp. 288–305. Springer, 2022.\n\nBerthold KP Horn and Brian G Schunck. Determining optical flow. Artificial intelligence, 17(1-3):\n\n185–203, 1981.\n\nTak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Liteflownet: A lightweight convolutional neural network for optical flow estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nEddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nShihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and Richard Hartley. Learning to estimate hidden motions with global motion aggregation. In IEEE International Conference on Computer Vision (ICCV), pp. 9772–9781, 2021.\n\nAlexandre Kaspar, Tae-Hyun Oh, Liane Makatura, Petr Kellnhofer, and Wojciech Matusik. Neural inverse knitting: From images to manufacturing instructions. In International Conference on Machine Learning (ICML), 2019.\n\nDaniel Kondermann, Rahul Nair, Stephan Meister, Wolfgang Mischler, Burkhard G ̈ussefeld, Sabine Hofmann, Claus Brenner, and Bernd J ̈ahne. Stereo ground truth with error bars. In Asia Conference on Computer Vision (ACCV), 2014.\n\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. In International Journal of Computer Vision (IJCV), 2020.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014.\n\nZhaoyang Lv, Kihwan Kim, Alejandro Troccoli, Deqing Sun, James M Rehg, and Jan Kautz. Learning rigidity in dynamic scenes with a moving camera for 3d motion field estimation. In European Conference on Computer Vision (ECCV), pp. 468–484, 2018.\n\nNikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nNikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. What makes good synthetic training data for learning disparity and optical flow estimation? International Journal of Computer Vision, 126(9):942–960, 2018.\n\nMoritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In IEEE Conference\n\non Computer Vision and Pattern Recognition (CVPR), pp. 3061–3070, 2015.\n\nSimon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE Conference\n\non Computer Vision and Pattern Recognition (CVPR), pp. 5437–5446, 2020.\n\nTae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Fr’edo Durand, William T In European\n\nFreeman, and Wojciech Matusik. Learning-based video motion magnification. Conference on Computer Vision (ECCV), pp. 633–648, 2018.\n\nFederico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAnurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In\n\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nStephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks.\n\nIn IEEE\n\nInternational Conference on Computer Vision (ICCV), pp. 2213–2222, 2017.\n\nDaniel Scharstein and Chris Pal. Learning conditional random fields for stereo. In IEEE Conference\n\non Computer Vision and Pattern Recognition (CVPR), 2007.\n\nDaniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo\n\ncorrespondence algorithms. International journal of computer vision, 2002.\n\nLeon Sixt, Benjamin Wild, and Tim Landgraf. RenderGAN: Generating realistic labeled data.\n\nFrontiers in Robotics and AI, 5:66, 2018.\n\nDeqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using In IEEE Conference on Computer Vision and Pattern\n\npyramid, warping, and cost volume. Recognition (CVPR), 2018.\n\nDeqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin, Huiwen Chang, Ramin Zabih, William T Freeman, and Ce Liu. Autoflow: Learning a better training set for optical flow. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European\n\nConference on Computer Vision (ECCV), 2020.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\n\nLearning Research, 9(86):2579–2605, 2008.\n\nJonas Wulff, Laura Sevilla-Lara, and Michael J Black. Optical flow in mostly rigid scenes. In IEEE\n\nConference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nJia Xu, Ren ́e Ranftl, and Vladlen Koltun. Accurate optical flow via direct cost volume processing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5807–5815, 2017.\n\nDawei Yang and Jia Deng. Learning to generate 3d training data through hybrid gradient. In IEEE\n\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 779–789, 2020.\n\nChristopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime tv-l 1\n\noptical flow. In Joint pattern recognition symposium, 2007.\n\nBo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with distribution matching.\n\nInternational Conference on Learning Representations (ICLR), 2021.\n\nHelong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective. In International Conference on Learning Representations (ICLR), 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nWe present implementation details and additional experimental results. The contents are listed as follows:\n\nCONTENTS\n\nA Details of Differentiable Data Generation Pipeline\n\nA.1 Base and Target Networks\n\nA.2 The Form of Task Loss Functions\n\nA.3 Geometric Warping and Flow Field Translation\n\nA.4 Layer Composition\n\nA.5 Color Perturbation and Real-world Effects\n\nA.6 Regularization Loss\n\nA.7 Details of Learnable Parameter θ\n\nB Details of Experiment Setup\n\nB.1 Generation Details of the DFlow Dataset\n\nB.2 Details of Pre-training Results\n\nB.3 Details of Model Generalization Property Results\n\nB.4 Details of Fine-tuning Results\n\nB.5 Details of Robustness against Corrupted Data\n\nC Additional Experiments\n\nC.1 Contrasting Effect of Base and Target Networks\n\nC.2 Validation Results\n\nC.3 Dataset Size\n\nC.4 Pre-training Performance according to Motion Magnitudes\n\nC.5 Analysis of Multi-target Datasets\n\nC.6 Additional Qualitative Results\n\nD Additional Generated Data Samples\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA DETAILS OF DIFFERENTIABLE DATA GENERATION PIPELINE\n\nIn this section, we describe the implementation details of the differentiable data generation pipeline.\n\nA.1 BASE AND TARGET NETWORKS\n\nBase Network. For the architecture, we use the same architecture for both base and target networks. We use the published weight of RAFT (Teed & Deng, 2020) as base network2, which is pre-trained on FlyingChairs (Fischer et al., 2015) for 100k iterations with a batch size of 12, 496 × 368 image size, and learning rate 4 × 10−4.\n\nTarget Network. We fine-tune the base network on the Sintel datasets (Butler et al., 2012) for the target network. During fine-tuning, we only use the image crop as data augmentation and fine-tune the base network for 20k iterations with a batch size of 6, 768 × 368 image size, and learning rate 1.25 × 10−4.\n\nA.2 THE FORM OF TASK LOSS FUNCTIONS\n\nWe introduce our task loss function Ltask(Ltarget, Lbase) for contrastive-style learning and evaluate the candidate of the loss function in Sec. 3.1. Table 7 shows the used task loss functions in experiments of Table 2. We use the sequence loss of flow estimates of RAFT (Teed & Deng, 2020) for L{target,base}. Algorithm 2 is the pseudo-code for updating the parameters of each data sample.\n\nTable 7: The forms of task loss function. We list the used forms of task loss function in Sec. 3.1. The form is the combination of {Multiplication, Addition} and {Exponential, Sigmoid, Tanh}. The task loss functions have the hyperparameters, α, β, and γ.\n\nTask loss function Ltask(Ltarget, Lbase)\n\nMultiplication & Exponential Multiplication & Sigmoid Multiplication & Tanh Addition & Exponential Addition & Sigmoid Addition & Tanh\n\n(1 − α exp(βLbase/(Ltarget + ε) + γ))Ltarget (1 + αsigmoid(βLtarget/(Lbase + ε) + γ))Ltarget (1 + αtanh(βLtarget/(Lbase + ε) + γ))Ltarget Ltarget + α exp(−βLbase/(Ltarget + ε) + γ) Ltarget + αsigmoid(βLtarget/(Lbase + ε) + γ) Ltarget + αtanh(βLtarget/(Lbase + ε) + γ)\n\nA.3 GEOMETRIC WARPING AND FLOW FIELD TRANSLATION.\n\nGiven N pairs of layer images and masks, {Ll 0}N It and It+1, with ground-truth optical flow FGT ,\n\nl=1 and {Ml\n\n0}N\n\nl=1, we generate subsequent frames,\n\n{Ll\n\n0}N\n\nl=1, {Ml\n\n0}N\n\nl=1 → {It, It+1, FGT } .\n\n(5)\n\nl=1.\n\n0}N\n\n0}N\n\nWe apply two steps of geometric warping with warping fields W0→t+1 and Wt+1→t which are computed from combinations of rigid transformation, perspective warping, and grid warping. First, l=1 and {Ml we warp the layer images and masks, {Ll l=1 and masks {Ml l=1 and 0}N {Ml\n\nl=1 at the t + 1 frame by applying the same warping field W0→t+1 to {Ll\n\nl=1, to be the layer images {Ll\n\nt+1}N 0}N\n\nt+1}N\n\nt and mask Ml\n\nt+1 = W0→t+1 ◦ Ll\n\nt+1 = W0→t+1 ◦ Ml 0,\n\n0, Ml (6) where ◦ denotes the geometric warping operation according to a given warping field. In the next step, each layer image Ll t at the t frame is generated from the ones at the t + 1 frame, i.e., Ll t+1 and Ml t+1. To simulate complex optical flows with minimal parameters, we introduce the flow field translation {Dl}N l=1, which translates the warping field Wt+1→t and obtains each warping of layers: Wl t+1→t, we warp the layer images and masks at the t + 1 time. Ll\n\nt+1→t = (Dl ◦ Wt+1→t). Using each warping of layers Wl\n\nt = (Dl ◦ Wt+1→t) ◦ Ml\n\nt = (Dl ◦ Wt+1→t) ◦ Ll\n\nt+1, Ml\n\nt+1.\n\n(7)\n\nLl\n\n2https://github.com/princeton-vl/RAFT\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: PyTorch-style pseudo-code for DFlow. # G: data generator # B: base network # T: target network # L: our task loss function # Lt: target loss function # Lb: base loss function # θ: data parameters for l, m in loader:\n\n# Generate optical flow data # l: layer images, m: layer masks image1, image2, label = G(l, m, θ)\n\n# loss lt = Lt(B(image1, image2), label) lb = Lb(T(image1, image2), label) loss = L(lt, lb)\n\n# Update θ loss.backward() optimizer.step()\n\nThe above parameterization is efficient for approximating multi-layer geometric warpings because it only introduces two additional parameters per layer to generate the local movement of each layer from a single warping field Wt+1→t. As aforementioned, we propose to use the two steps of warping with W0→t+1 and Wt+1→t. This is distinctive from the existing works, including Fischer et al. (2015); Sun et al. (2021), where they only use a single step of warping from a randomly generated image to a synthetic next frame. This only parameterizes the next frames, not the reference frames randomly generated. This could not update the textures and mask shapes of the reference frames, whereas our method parameterizes both frames with two warping fields, W0→t+1 and Wt+1→t. It allows us to jointly update subsequent frames.\n\nA.4 LAYER COMPOSITION\n\nFrom the geometric warping, we have N pairs of layer images, masks, and warping fields in order of depth. We adopt the softmax splatting (Niklaus & Liu, 2020) to compose the layer images at subsequent frames and warping fields with flow field translation, i.e., Ll t+1→t. We get the importance image Z and weight image K of each layer and frame,\n\nt+1, and Wl\n\nt, Ll\n\nZl\n\nt = Ml\n\ntal − maxl(Ml\n\ntal),\n\nKl\n\nt =\n\nexp(Zl t) l=1 exp(Zl t)\n\n(cid:80)N\n\n.\n\n(8)\n\n(9)\n\nwhere al = cl and c is a constant value which we set 6 for layer composition. As the value of c increases, sharper boundaries can be obtained. The subsequent frames and ground-truth optical flow are as follows:\n\n(cid:40) N\n\n(cid:88)\n\nl=1\n\nKl\n\nt+1 ⊙ Ll\n\nt+1,\n\nN (cid:88)\n\nl=1\n\nKl\n\nt ⊙ Ll t,\n\nN (cid:88)\n\nl=1\n\nKl\n\nt ⊙ Wl\n\nt+1→t\n\n(cid:41)\n\n→ {It+1, It, FGT } ,\n\n(10)\n\nA.5 COLOR PERTURBATION AND REAL-WORLD EFFECTS\n\nColor Perturbation. Color perturbation consists of N white balances to adjust the color intensity of layer images. Each white balance has three-channel values from zero to one and has half of the chance to be applied. Color perturbation is the same for subsequent frames and optimized.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Color perturbation and real-world effects. Real-world effects are composed of texture noises, fog, and motion blur. From left to right: (a) color perturbation, (b) texture noises, (c) fog, (d) motion blur. (a) is an overlaid image before (upper right) and after color perturbation (lower left).\n\nTexture Noises. We simulate the shot noise to mimic real-world photos. We apply three-channel texture noises to all of the pixels while masking the area where the noises are actually applied. Two different texture noises, Nt and Nt+1, are applied to subsequent frames and optimized.\n\nFog and Motion Blur. Inspired by AutoFlow (Sun et al., 2021), we introduce two real-world effects; fog and motion blur. To generate a random fog, we follow the implementation of AutoFlow and superimpose the fog on It and It+1. However, one difference is that we adjust the three-channel values of fog, which yields the colored fog. The fog is the same for the subsequent frames and is optimized as well. For the motion blur, we randomly sample object masks from PASCAL VOC (Everingham et al., 2010) and combine them to generate a motion blur mask. We use the 2D gaussian blur to approximate the motion blur kernel, and the motion blurred frames are alpha-blended to It and It+1. The standard deviation of each axis and angle of rotation are parameters to be updated.\n\nFigure 5 shows the color perturbation and real-world effects including texture noise, fog, and motion blur.\n\nA.6 REGULARIZATION LOSS\n\nThe grid warping and texture noise may take shortcuts only to minimize task loss. These behaviors are undesirable, and the generated data might not properly train the optical flow network. To handle this potential issue, we propose a regularization term Lreg consisting of grid and noise regularization losses, i.e., Lreg = Lgrid + Lnoise. The grid regularization Lgrid prevents from producing infeasible 2D motion. We define the grid regularization loss Lgrid as follows:\n\nLgrid = max(0,\n\nw (cid:88)\n\nh−1 (cid:88)\n\n[Ct(k, j) − Ct(k, j + 1)] +\n\nk=1\n\nj=1\n\nw−1 (cid:88)\n\nh (cid:88)\n\nk=1\n\nj=1\n\n[Ct(k, j) − Ct(k + 1, j)]),\n\n(11)\n\nwhere Ct is the warped coordinate by the geometric warping Wt+1→t. Lgrid gives a penalty when the warped coordinates of the previous grid exceed those of the next grid. The noise regularization Lnoise prevents from producing too noisy images. We define the noise regularization loss Lnoise as follows:\n\nLnoise = (∥Nt∥1+∥Nt+1∥1),\n\n(12)\n\nwhere Nt and Nt+1 are texture noises of the current and next frames, respectively. Finally, we obtain our total loss by adding the regularization loss to the task loss as:\n\nLtotal = Ltask(Ltarget, Lbase) + Lreg.\n\n(13)\n\nWith the total loss Ltotal, we update the data parameters {θ}.\n\nA.7 DETAILS OF LEARNABLE PARAMETER θ\n\nLearning Rate of θ. We set the learning rate of real-world effects as 3 × 10−2. and the others as {1 × 100, 1 × 10−1, 2 × 10−2} depending on whether they are pixel-unit operations (e.g., translation parameters) or not. We decay the learning rates linearly over the update iterations; the decay factor is (1 − iteration/80), where 80 is the maximum iteration of updates. We distinguish the pixel-unit operations and the others as:\n\n16\n\n(b)(c)(d)(a)Published as a conference paper at ICLR 2023\n\nTable 8: Generation details of DFlow. Data augmentation used in generating DFlow.\n\nData augmentation\n\nColor jitter\n\nbrightness 0.1\n\ncontrast 0.1\n\nsaturation 0.1\n\nhue 0.04\n\nmin scale 0.93\n\nRandom resize and crop max scale 2.30\n\nimage crop 496 × 368\n\n• Pixel-unit operation (learning rate of 1 × 100: translation of the rigid transformation, grid warping,\n\nperspective warping, and flow field translation\n\n• Non-pixel-unit operation (learning rates of {1 × 10−1, 2 × 10−2}): rotation and scaling of the rigid\n\ntransformation\n\nInitialization of θ. Following the implementation of AutoFlow (Sun et al., 2021), we sample the initial data parameters θ from the uniform distributions ranging from a to b, i.e., [a, b].\n\n• White balance: [0, 1]\n\n• Rigid transformation (translation, rotation, scaling): ([-80, 80], [-5, 5], [0.75, 1.13])\n\n• Grid warping: [0, 0]\n\n• Perspective warping: [-25, 25]\n\n• Flow field translation: [-50, 50]\n\n• Texture noise: [-0.01, 0.01]\n\n• Fog color: [0, 1]\n\n• Motion blur (std of axis x, std of axis y, angle of rotation): ([1, 2], [3, 11], [0, 90])\n\nB DETAILS OF EXPERIMENT SETUP\n\nIn this section, we present the generation details of the DFlow dataset. Then, we present the details of pre-training and fine-tuning results in Sec. 4 of the main paper.\n\nB.1 GENERATION DETAILS OF THE DFLOW DATASET\n\nTo generate the DFlow dataset, the data augmentation is included in the proposed differentiable data generation pipeline. We randomly augment each data sample before inputting them into the base and target networks. For stability of data generation, we stack 6 randomly augmented data samples and input them into networks. Each data sample of DFlow is updated up to 80 times and saved depending on the threshold 25 of the target network loss. The DFlow data has a size of 512 × 384, which is the same as FlyingChairs. Table 8 summarizes the data augmentation applied to each data sample.\n\nB.2 DETAILS OF PRE-TRAINING RESULTS\n\nExcept for the data augmentation, we set the training parameter settings by following the training details of RAFT (Teed & Deng, 2020) on FlyingChairs (Fischer et al., 2015), and pre-train the RAFT network on DFlow with a data size of 15k. We use the same data augmentation applied for generating the DFlow dataset. We early stop the training at 95k iteration.\n\nB.3 DETAILS OF MODEL GENERALIZATION PROPERTY RESULTS\n\nFor the FlowNet results, we use the Pytorch implementation of FlowNet3. We pre-train the FlowNet model for 2700 epochs with batch size 8. Following the Pytorch Implementation of FlowNet, we use the proposed data augmentation including translation, rotation, image crop, vertical flip, and horizontal flip. For the AutoFlow result, we use the published AutoFlow dataset (Sun et al., 2021). To generate the DFlow-GMA dataset, we replace the RAFT proxy networks with GMA networks (Jiang et al., 2021) while selecting the KITTI 2015 dataset as the target dataset. The set α to 13 for the DFlow-GMA dataset and pre-train the GMA network on DFlow-GMA with a data size of 2k.\n\n3https://github.com/ClementPinard/FlowNetPytorch\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Data samples of the rainy scene of Virtual KITTI.\n\nTable 9: Contrasting analysis in pre-training. The dataset generated with the contrasting effect achieves comparable results at the target dataset, KITTI 2015. The contrasting effect leads to a notable performance improvement on Sintel, which is not used for data generation.\n\nContrasting effect\n\n✗ ✓\n\nEvaluation dataset\n\nSintel clean\n\nSintel final\n\nKITTI 2015\n\nAEPE\n\n4.38 2.14\n\nAEPE\n\n5.03 3.79\n\nAEPE / F1\n\n4.61 / 13.86 4.31 / 14.29\n\n*Bold denotes the best.\n\nB.4 DETAILS OF FINE-TUNING RESULTS\n\nTo achieve fine-tuning results on the public benchmarks, we fine-tune the RAFT model pre-trained on DFlow. Except for the initial data schedule, i.e., FlyingChairs followed by FlyingThings3D, we follow the same dataset schedule and training details of the original implementation (Teed & Deng, 2020).\n\nB.5 DETAILS OF ROBUSTNESS AGAINST CORRUPTED DATA\n\nThe photometric inconsistency is the main cause of the error in optical flow estimation. In real-world images, real-world effects, such as blur, fog, and illumination change, cause photometric inconsistency, and these effects frequently occur. Since DFlow generates a dataset using the compressed knowledge of the target dataset, we verify DFlow’s ability to generate the dataset for training a robust optical flow network against photometric inconsistency. We use rainy scenes of Virtual KITTI (Gaidon et al., 2016) because the photometric inconsistency is dominant, as shown in Fig. 6. To obtain the target network, we further train the base network on the rainy scenes for 20k iterations and generate an additional dataset, DFlow-V, by targeting the rainy scenes. We use the published weight of RAFT trained on the RF-AB dataset for the result of RealFlow (Han et al., 2022).\n\nC ADDITIONAL EXPERIMENTS\n\nWe analyze the contrasting effect of base and target networks in Sec. C.1, and evaluate the validation performance with DFlow in Sec. C.2.\n\nC.1 CONTRASTING EFFECT OF BASE AND TARGET NETWORKS\n\nTo evaluate the contrasting effect of base and target networks, we measure the pre-training performance depending on the contrasting effect. The details will be presented in the following paragraphs.\n\nContrasting Analysis in Pre-training. For the contrasting analysis, we generate two types of dataset with a sample size of 2k. For the first dataset, we use KITTI 2015 (Menze & Geiger, 2015) as the target dataset, i.e., the target network is obtained by fine-tuning the base network on the KITTI 2015 dataset. To synthesize the first dataset with contrasting effect, we minimize and maximizes the loss of the target and base networks, respectively. For the other dataset, we generate a new target network that is pre-trained on the KITTI 2015 dataset. With the new target network, we generate a new pre-training dataset that is synthesized to minimize the loss of the new target data network only, i.e., the contrasting effect is not applied.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: t-SNE plot (van der Maaten & Hinton, 2008) of different datasets. We use the context encoder of the base network trained on FlyingChairs as a feature extractor and extract the features of several datasets. (Red): target datasets, e.g., KITTI 2015, (Blue): base dataset, e.g., FlyingChairs, (Sky): our initial dataset before optimization, (green): updated our dataset without the contrasting effect, (Yellow): updated our dataset with the contrasting effect. The features of our dataset with the contrasting effect are getting closer to the features of the target dataset through the optimization process than the features of other datasets.\n\nTable 9 shows the pre-training results depending on the contrasting effect. With the contrasting effect, the RAFT network achieves comparable results on the target dataset, i.e., KITTI 2015, compared to the other network trained on the dataset generated with the new target network only. However, without the contrasting effect, the network shows notable performance degradation on Sintel (Butler et al., 2012), which is not used for data generation.\n\nTable 10: Distance of feature mean between ours and KITTI 2015.\n\nWe also visualize data features to identify the effect of the contrasting effect. We embed the features of several datasets, such as FlyingChairs, KITTI 2015, our datasets with and without the contrasting effect, and our dataset before optimization. We use the context encoder of the base network as a feature extractor. As shown in Fig. 7, ours with the contrasting effect is closer to the target than one without the contrasting effect. We additionally compute distances of feature mean between ours and KITTI 2015. As shown in Table 10, the maximum mean discrepancy (MMD) (Gretton et al., 2012) of ours with the contrasting effect is lower than ours without the contrasting effect. These results show that the contrasting effect drives the data generation toward an improving direction in a target domain.\n\nOurs wo. contrasting Ours w. contrasting\n\n6.6892 6.1454\n\nDatasets\n\nMMD\n\nC.2 VALIDATION RESULTS\n\nThe advantage of the proposed pipeline is that the generated data can be used to improve the validation performance on the unseen target dataset. We split the KITTI 2015 dataset into 50 training samples and 150 validation samples; KITTI-50 and KITTI-150. Using the target network fine-tuned on KITTI-50, we synthesize a dataset; OursKITTI-50. Therefore, any data or information on KITTI-150 is not included in the data generation process. We fine-tune the base network on several combinations of datasets with 20k iterations and evaluate the validation performance on KITTI-150. As shown in Table 11, fine-tuning on the combination of Sintel and KITTI-50 shows comparable performance with the one of fine-tuning on KITTI-50 alone. In contrast, we observe notable improvement in validation performance when adding the same number of OursKITTI-50 to the training dataset. As our data generation pipeline is not limited to the number of samples that can generate, we analyze the effect of our data size and observe the improvement of validation accuracy.\n\n19\n\ntargetbaseours (w. contrasting)ours (wo. contrasting)ours (initial)Published as a conference paper at ICLR 2023\n\nTable 11: Comparison of the validation performance of networks fine-tuned on different dataset combinations. We fine-tune the base network on the combination of KITTI-50 and another dataset. OursKITTI-50 notably improves the validation accuracy on the unseen KITTI dataset, KITTI-150.\n\nFine-tune dataset\n\nData size\n\nKITTI-50 KITTI-50+Sintel KITTI-50+OursKITTI-50 KITTI-50+OursKITTI-50\n\n50 50+2082 50+2082 50+12003\n\nValidation dataset\n\nKITTI-150\n\nAEPE / F1\n\n3.41 / 11.47 3.39 / 11.21 3.07 / 10.99 2.98 / 10.70\n\n*Bold denotes the best, and underline denotes the second best.\n\nTable 12: Pre-training results depending on the amounts of DFlow data. We train the RAFT networks on FlyingChairs (Fischer et al., 2015) and various amounts of DFlow dataset. We evaluate the performance on the Sintel and KITTI 2015 datasets.\n\nDataset\n\nData size\n\nSintel clean\n\nSintel final\n\nKITTI 2015\n\nEvaluation dataset\n\nAEPE\n\nAEPE\n\nFlyingChairs DFlow DFlow DFlow DFlow DFlow DFlow DFlow\n\n22873 100 500 1000 2000 4000 8000 15000\n\n2.28 2.72 2.20 2.02 1.90 1.87 1.81 1.81\n\n4.51 4.03 3.48 3.24 3.02 2.92 2.91 2.93\n\nAEPE / F1\n\n9.85 / 37.56 9.15 / 23.09 5.40 / 17.54 5.22 / 17.06 4.90 / 15.78 4.78 / 15.64 4.88 / 15.51 4.59 / 15.03\n\nC.3 DATASET SIZE\n\nThe number of the dataset is a key factor for training accurate optical flow networks, and synthetic optical flow data have an advantage because of the annotation efficiency compared to the real-world data. We analyze the effect of data size in the optical flow network, RAFT (Teed & Deng, 2020) We train the RAFT network on the various amount of DFlow data and evaluate the performance on both Sintel and KITTI 2015 datasets. We determine FlyingChairs (Fischer et al., 2015) as a competing dataset because FlyingChairs and DFlow have the same data resolution. As shown in Table 12, the RAFT network trained on the 500 DFlow dataset outperforms the model trained on the full FlyingChairs dataset. This result shows that DFlow data has more information with the same data resolution. The overall performance on Sintel and KITTI 2015 datasets is also improved as the data size increase, which shows that diverse texture and motion is effective for training the optical flow network.\n\nC.4 PRE-TRAINING PERFORMANCE ACCORDING TO MOTION MAGNITUDES\n\nTable 13 summarizes AEPEs of RAFT pre-trained on respective FlyingChairs (Fischer et al., 2015) and our DFlow according to different motion ranges. The model trained on DFlow shows higher accuracy than that of FlyingChairs except for the minimal motion range. Performance gaps between the models tend to be enlarged in larger motion. These results may be explained by human intuition that the small motion hardly contributes to the error of optical flow networks. For the same reason, most motion magnitudes of DFlow are in the middle and high motion ranges, as shown in Fig. 8.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 13: Motion accuracy of the RAFT networks in different magnitude ranges. We evaluate the AEPE score of models pre-trained on FlyingChairs and DFlow in different motion ranges. As the motion magnitude increases, the model trained on DFlow achieves more considerable performance improvements than that trained on FlyingChairs.\n\nPre-training dataset\n\nFlyingChairs DFlow (Ours)\n\nFlyingChairs DFlow (Ours)\n\nEvalutaion dataset\n\nSintel Final\n\nKITTI 2015\n\n< 1\n\n0.57 0.69\n\n0.74 1.16\n\nMotion magnitude ranges\n\n[1, 10]\n\n(10, 20]\n\n(20, 30]\n\n1.15 0.80\n\n1.67 0.81\n\n3.31 2.04\n\n2.65 1.31\n\n6.47 3.59\n\n3.86 2.10\n\n> 30\n\n24.08 15.18\n\n21.32 9.72\n\n*Bold denotes the best.\n\nFigure 8: Motion magnitude histograms. DFlow focuses more on the mid-high ranges of motion than the other datasets, because small motion hardly contributes to errors of networks.\n\nTable 14: Analysis of multi-target datasets. The target of DFlow is Sintel, DFlow-V rainy scenes of Virtual KITTI. We denote DFlow+DFlow-V as the multi-target datasets. Multi-target datasets show the trade-off in general but achieve the highest performance on KITTI 2015.\n\nDataset\n\nDFlow DFlow-V\n\nDFlow + DFlow-V\n\nEvaluation dataset\n\nSintel clean\n\nSintel final\n\nKITTI 2015\n\nRainy scene\n\nAEPE\n\n1.81 2.57\n\n1.99\n\nAEPE\n\nAEPE / F1\n\nAEPE / F1\n\n2.93 3.86\n\n3.14\n\n4.59 / 15.03 5.98 / 15.16\n\n5.87 / 22.89 3.11 / 12.61\n\n4.39 / 14.84\n\n4.30 / 16.91\n\n*Bold and underline note the best and second best, respectively.\n\nC.5 ANALYSIS OF MULTI-TARGET DATASETS\n\nWe focus on generating the dataset close to the target dataset. It is questionable whether multi-target datasets can be used to generate the DFlow dataset consisting of diverse data samples. The targets of DFlow and DFlow-V are Sintel and rainy scenes of Virtual KITTI. Table 14 lists the optical flow performance on Sintel clean, Sintel final, KITTI 2015, and the rainy scene of Virtual KITTI. As expected, DFlow-V achieves higher performance than DFlow on the rainy scene; but DFlow-V has inferior performance on the other datasets. When combining DFlow and DFlow-V, we observe that there is a trade-off; the performance of DFlow with DFlow-V is usually between the performance of DFlow and DFlow-V. However, the mixture of DFlow and DFlow-V outperforms in KITTI 2015. It indicates that exploiting multi-target datasets is a promising research direction.\n\n21\n\n0.0%0.1%1.0%10.0%100.0%FlyingChairsFlyingThings3DSintelKITTIDflow_all0255075100Motion Magnitude (pixels)Percentage of PixelsSintelDFlowKITTI 2015FlyingChairsFlyingThings3DPublished as a conference paper at ICLR 2023\n\nC.6 ADDITIONAL QUALITATIVE RESULTS\n\nFigure 9: Additional qualitative results. Top: pre-training results on the KITTI 2015 dataset. Bottom: pre-training results on Sintel Final pass.\n\n22\n\nGround truthFlyingChairsDFlowGround truthFlyingChairsDFlowInput ImageGround truthFlyingChairsDFlowPublished as a conference paper at ICLR 2023\n\nD ADDITIONAL GENERATED DATA SAMPLES\n\nFigure 10 shows additional data samples. Color perturbation, texture noise, fog, and motion blur can be found in the data samples.\n\nFigure 10: Additional generated data samples. Top: current frames; Middle: next frames; Bottom: visualizations of ground truth optical flow.\n\n23",
    "reference": "# Summary Of The Paper\n\nThe paper proposed a differentiable optical flow data generation pipeline and a loss function to drive the pipeline. The proposed modules enable automatic and efficient synthesis of a dataset effectively to a target domain, given a snippet of target data. This distinctiveness is achieved by proposing an efficient data comparison method. Experiments show the competitive performance of DFlow against the prior arts in pre-training.\n\n# Strength And Weaknesses\n\nStrength:\n\n+ A simple and efficient differentiable data generation pipeline for optical flow.\n\n+ A contrastive-style learning scheme and its loss function by approximating expensive dataset-todataset comparison to leverage proxy neural networks.\n\nWeaknesses:\n\n- The claim that \"the RAFT model pre-trained with DFlow achieves state-of-the-art performance on the Sintel public benchmark in fine-tuning\" is questionable. According to the benchmark in Sintel. The performance of \"DF-RAFT\" is even not as good as the orignal RAFT.\n\n- The paper did not discuss the limitations of the proposed approach.\n\n- There are some recent work generating optical flow dataset in a rather cheap way such as :\nRealFlow: EM-Based Realistic Optical Flow Dataset Generation from Videos, ECCV 2022.\nYunhui Han, Kunming Luo, Ao Luo, Jiangyu Liu, Haoqiang Fan, Guiming Luo, Shuaicheng Liu\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper writing is general clear.\n\nQuality: The paper quality is good in general.\n\nNovelty: The paper owns its novelty in generating optical flow dataset. However the benefits of the new dataset has not been fully validated.\n\nReproducibility: The proposed method is relatively complex. It is not very easy to reproduce the results.\n\n# Summary Of The Review\n\nThe paper proposed a differentiable optical flow data generation pipeline (DFlow) and a loss function to drive the pipeline. This is a simple and efficient differentiable data generation pipeline for optical flow. It also proposed a contrastive-style learning scheme and its loss function by approximating expensive dataset-todataset comparison to leverage proxy neural networks.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEFFICIENT NEURAL REPRESENTATION IN THE COGNITIVE NEUROSCIENCE DOMAIN: MANIFOLD CAPACITY IN ONE-VS-REST RECOGNITION LIMIT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe structure in neural representations as manifolds has become a popular approach to study information encoding in neural populations. One particular interest is the connection between object recognition capability and the separability of neural representations for different objects, often called \"object manifolds.\" In learning theory, separability has been studied under the notion of storage capacity, which refers to the number of patterns encoded in a feature dimension. Chung et al. (2018) extended the notion of capacity from discrete points to manifolds, where manifold capacity refers to the maximum number of object manifolds that can be linearly separated with high probability given random assignment of labels. Despite the use of manifold capacity in analyzing artificial neural networks (ANNs), its application to neuroscience has been limited. Due to the limited number of \"features\", such as neurons, available in neural experiments, manifold capacity cannot be verified empirically, unlike in ANNs. Additionally, the usage of random label assignment, while common in learning theory, is of limited relevance to the definition of object recognition tasks in cognitive science. To overcome these limits, we present the Sparse Replica Manifold analysis to study object recognition. Sparse manifold capacity measures how many object manifolds can be separated under one versus the rest classification, a form of task widely used in both in cognitive neuroscience experiments and machine learning applications. We demonstrate the application of sparse manifold capacity allows analysis of a wider class of neural data - in particular, neural data that has a limited number of neurons with empirical measurements. Furthermore, sparse manifold capacity requires less computations to evaluate underlying geometries and enables a connection to a measure of dimension, the participation ratio. We analyze the relationship between capacity and dimension, and demonstrate that both manifold intrinsic dimension and the ambient space dimension play a role in capacity.\n\n1\n\nINTRODUCTION\n\nThe approach to study neural populations as manifolds and their geometry has become a popular method to uncover important structural properties in neural encoding and understand the mechanisms behind the ventral stream, the motor cortex, and cognition (Kriegeskorte & Kievit, 2013)(Sengupta et al., 2018) (Gallego et al., 2017) (Sohn et al., 2019)(Ebitz & Hayden, 2021)(Kriegeskorte & Wei, 2021) (Chung & Abbott, 2021). In the ventral stream, the invariant ability for humans and animals to recognize an object despite changes in pose, position, and orientation has motivated a definition of object manifold as the underlying representation of neural responses to a distinct object class. A long-standing hypothesis in visual neuroscience posits that the visual cortex untangles these object manifolds for invariant object recognition (Dicarlo & Cox, 2007), relating object recognition to the separation of manifolds by some linear hyperplane.\n\nThere is a well developed theory of linear separability given by Gardner (1988) that studies the separation of points by a perceptron. The theory quantifies a capacity load that describes the maximum number of points that can be linearly separated given a random dichotomy (a random assignment of binary labels to the manifolds). The capacity load also encodes the number of points stored per feature dimension required to have linear separability. This theory of separation, however, does not connect to the geometries of the underlying representations.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Dense vs Sparse labels (left) Computer science theories such as VC dimension and Cover’s Theorem are interested in capacity under all random dichotomies. Under this regime, manifolds are densely labeled where more than one manifold has a positive label. (right) Conversely, neuroscience studies cognitive tasks such as object recognition, where one-vs-rest dichotomies are more relevant. Under this regime, manifolds are sparsely labeled where only one manifold is assigned a positive labeling at a time. In each dichotomy, (red) is the positive label 1 and (blue) is the negative label -1.\n\nChung et al. (2018) extended the notion of capacity from points to manifolds and related capacity to manifold geometric properties. Manifold capacity, defined as the maximum number of object manifolds that can be linearly separated given a random dichotomy, has been used to show how classification emerges along layers of deep neural network (DNN) undertaking visual object recognition Cohen et al. (2020) and speech recognition (Stephenson et al., 2019). Furthermore, it has been shown that geometric properties of the manifold, defined as manifold radius and dimension, decrease in magnitude while capacity increases. This observation suggests feature representations with a low intrinsic dimension allows invariance and robustness in classification.\n\nDespite the use of manifold capacity in analyzing DNN, its applicability to neuroscience has been limited due to the way manifold capacity is defined. The current theory following Gardner’s framework considers all random dichotomies, similar to other theoretical computer science studies. VC dimension in learning theory returns the largest number of points in a set such that every dichotomy of the set can be learned by a given hypothesis function (Vapnik & Chervonenkis, 2015; Abu-Mostafa et al., 2012). Cover’s theorem returns the number of linearly separable sets given P points and a D dimensional space (Cover, 1965). The regime of object recognition for a classification model and for a monkey performing a delayed matching Majaj et al. (2015b) or oddity task, however, is equivalent to the separation of manifolds on a one-vs-rest basis (Figure 1). In other words, the only relevant dichotomies are those where only one manifold has a positive labeling. This relates to the notion of sparse labeling in Chung et al. (2018).\n\nSparse labeling also overcome the technical restrictions of using manifold capacity to analyze biological neurons like previous works have for artificial neurons. The current manifold capacity theory for random dichotomies falls outside the regime of most available neural datasets, namely, data with limited number of simultaneously recorded neurons (Gao et al., 2017). Hence, as modern large scaled probing techniques improve and become publicly available (Jia et al., 2019; Steinmetz et al., 2021), analyzing current data requires further innovations in theoretical and analysis framework. Under sparse labeling, capacity is greater than in the traditional regime (Chung et al., 2018; Gardner, 1988). It follows that, under the sparse label regime, we can verify capacity in datasets with fewer number of features, or neurons, which was not previously possible (Froudarakis et al., 2021). Thus, the sparse labeling regime allows us to apply the theory of manifold capacity to real neural data and use capacity as a measure of recognition and similarity between DNN and the biological brain.\n\nIn this paper, we extend the work presented in Chung et al. (2018) to analyze neural data by estimating manifold capacity in the one-vs-rest recognition limit. We define sparse manifold\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ncapacity as the ratio of P/N where P is the number of manifolds in a given system, and N is the degree of freedom required such that the probability of linearly separating a one-vs-rest dichotomy is 0.5. Our contribution is as follow:\n\n• We present the Sparse Replica Manifold analysis for estimating sparse manifold capacity in the one-vs-rest recognition limit. Our analysis overcomes the limitations in Chung et al. (2018) by taking into account of correlation between manifolds and the effects of a heterogeneous mixture of manifold geometries, thereby making the analysis applicable to real complex data.1\n\n• We show that the application of sparse manifold capacity allows analysis of a wider class of neural data that has a limited number of neurons with empirical measurements. We demonstrate, for the first time to our knowledge, the match between the theoretical and empirical manifold capacity in real neural data, and suggest capacity as a reliable measurement of linear separability in the biological brain.\n\n• We show that the manifold geometries under the sparse label regime is faster to compute\n\nthan in the classical regime, allowing efficient analyses via manifold geometries.\n\n• We explicitly illustrate the effects of ambient dimension and manifold intrinsic dimension on sparse linear separability. In particular, we show that better linear separability is related to both high ambient dimension and low manifold intrinsic dimension, filling in the gap of knowledge on the role of high and low dimensional representations in neural encoding.\n\n2 REVIEW OF MEAN FIELD THEORY MANIFOLD ANALYSIS\n\nWe will first summarize the Mean Field Theory Manifold Analysis (MFTMA) for object manifolds first introduced in Chung et al. (2018). This framework extends the calculation of perceptron capacity of discrete points (Gardner, 1988) to capacity of manifolds. Consider a system of P manifolds in a N -dimensional ambient space, where each manifold has an affine dimension of D. We define the capacity load αc = P/Nc where Nc is the critical number of feature dimensions required such that 0.5 of all manifold dichotomies are separable by a hyperplane. αc can also be interpreted as the number of categories encoded per feature dimension. Chung et al. (2018) calculated capacity as an average of αM , for each manifold M . To determine αM , equation 12 involves finding manifold anchor points, ̃s(⃗t, t0), uniquely determined by Gaussian vectors T ∈ RD+1, whose components Ti ∽ N (0, 1). For the rest of this paper, we denote T = (⃗t, t0) (⃗t ∈ RD, t0 ∈ R). Given some dichotomy and orientation of the manifolds, the anchor point is a point on the manifold or its convex hull that contributes to the separating linear hyperplane. The notion of anchor point emerges from the solution to the KKT interpretation of the mean field equations for capacity, discussed in depth in Chung et al. (2018) (see also appendix A.8). Similar to support vectors in a Support Vector Machine, these anchor points change based on the location and labels of other manifolds. The Gaussian vectors represent the variability in orientations, locations, and labels of other manifolds.\n\nα−1\n\nM =\n\n(cid:28) [−t0 − ⃗t · ̃s(⃗t, t0)]2 (∥ ̃s(⃗t, t0)∥2 + 1)2\n\n(cid:29)\n\n⃗t,t0\n\n(1)\n\nMFTMA links the connection between this capacity load with the manifold geometric properties. In particular, Chung et al. (2018) defined for each manifold, the manifold anchor radius (RM ) squared as the mean squared length of all ̃s(⃗t, t0), i.e. R2 . In addition, the manifold anchor dimension (DM ) measures the mean angular spread between the unit vector of an anchor point (ˆs) and the Gaussian vector that determines the anchor point, i.e. DM = ⟨(⃗t · ˆs(⃗t, t0))2⟩⃗t,t0 Chung et al. (2018) also showed that one can estimate αM using manifold anchor radius and dimension by αBall(κ, RM , DM ) with a margin of κ,\n\nM = ⟨∥ ̃s(⃗t, t0)∥2⟩⃗t,t0\n\nBall(κ, R, D) = (cid:82) ∞ α−1\n\n0 dtχD(t) · where Dt is the Gaussian measure and χD(t) = 21− D 2 ) Γ( D\n\nκ−tR−1 Dt0\n\n2\n\n(cid:104)(cid:82) κ+tR\n\ntD−1e− 1\n\n2 t2\n\n,\n\nt ≥ 0.\n\n(−t0+tR+κ)2\n\n(1+R2) + (cid:82) κ−tR−1\n\n−∞\n\n(cid:16)\n\n(t0 − κ)2 + t2(cid:17)(cid:105)\n\nDt0\n\n(2)\n\n1Upon publication, the Sparse Replica Manifold Analysis will be made publicly available for the use of the\n\nneuroscience and machine learning community to analyze artificial neural networks and biological data.\n\n2⟨·⟩ denotes taking the average in this paper.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2.1 GAUSSIAN RADIUS AND GAUSSIAN DIMENSION\n\nShown in Chung et al. (2018), for small manifold sizes, the anchor point ( ̃s) depends only on ⃗t. This motivates a definition of manifold radius similar to the Gaussian mean width. Chung et al. (2018) additionally defined the Gaussian radius (Rg) and Gaussian dimension (Dg) as\n\ng = ⟨∥ ̃sg(⃗t)∥2⟩⃗t , Dg = ⟨(⃗t · ˆsg(⃗t))2⟩⃗t where ̃sg(⃗t) is the anchor point determined by ⃗t, and ˆsg(⃗t) is its unit vector. These measures play important roles when considering classification tasks with sparse labels (see next section).\n\nR2\n\n(3)\n\n3 MANIFOLD CAPACITY FOR ONE-VS-REST OBJECT RECOGNITION\n\nThe MFTMA estimates the capacity load when 0.5 of all 2P dichotomies in system of P manifolds are separable. In neuroscience and learning models, however, a more practical usage of this theory is to estimate capacity where manifolds are separated on a one-vs-rest basis. This falls under the notion of sparse labeling, in contrast to dense labeling. In other words, we want to consider only the P dichotomies, in each which only one manifold has a positive labeling. Assuming that each manifold represents a class of object, estimating capacity on a one-vs-rest basis is equivalent to estimating the capacity for a system undertaking an object recognition task.\n\nIn the following sections, we present the Sparse Replica Manifold Analysis that computes manifold in the one-vs-rest recognition limit, also called sparse manifold capacity. Sparse manifold capacity was previously considered in Chung et al. (2018) in the context of random uncorrelated manifolds with identical geometries (i.e. every manifold has the same manifold radius and dimension). We extend this work by taking into account of heterogeneous geometries and correlations between manifolds. In the following sections, we first review the theory for sparse capacity considered in previous work. Then, we provide the theorem for calculating sparse manifold capacity in the general case with a mixture of different manifold geometries. Lastly, we discuss the case where manifold center correlation are taken into account.\n\n3.1 SPARSE MANIFOLD CAPACITY\n\nDefine f , the sparsity parameter, as the fraction of positively labeled manifolds in a dichotomy. Then, f = 1/P corresponds to the one-vs-rest dichotomies. Under the regime of sparse labeling, capacity depends on maximizing a bias parameter, where the hyperplane is not constrained to pass through the origin. This bias parameter has a positive contribution to the margin for the positively labeled manifold and a negative contribution for the negatively labeled manifolds. The resulting effect of the bias parameter, shown in Chung et al. (2018), puts the manifolds in regime where t0 becomes negligible to the anchor point of a contributing manifold. These observations allow us to use equation 4 to estimate the sparse manifold capacity assuming every manifold behaves as a ball with the same Gaussian radius (Rg) and Dimension (Dg) (Chung et al., 2018).\n\nαc,HOM OG(κ, Rg, Dg) = max\n\n[f · α−1\n\nBall(κ + b, Rg, Dg) + (1 − f ) · α−1\n\nBall(κ − b, Rg, Dg)]−1 (4)\n\nb\n\n3.1.1 SPARSE MANIFOLD CAPACITY WITH MIXTURES OF SHAPES\n\nIn this work, to account for the different Gaussian Radii and Dimensions for each manifold in a heterogeneous system, we derive and employ Theorem 1. The derivation of this theorem is provided in the appendix A.1, and a short version is given in the next section. For each manifold i = 1, 2, ...P , we calculate αM i from Equation 5, where ⃗R is the array of all manifold Gaussian Radii in the system, and ⃗D is the array of all manifold Gaussian dimensions in the system. The sparse manifold capacity is the average of all αM i. Theorem 1. Given P manifolds. Let ⃗Dg = [Dg1, ...Dgp] be the array of their corresponding Gaussian dimensions. And let ⃗Rg = [Rg1, ...Rgp] be the array of their corresponding Gaussian radii. Then sparse manifold capacity, αc,SP ARSE, is computed as ⟨αM i⟩i={1,2...P } where\n\nαM i (κ, ⃗Rg, ⃗Dg) = max\n\n[1/P · α−1\n\nBall(κ + b, Rgi, Dgi) +\n\nb\n\n1/P · α−1\n\nBall(κ − b, Rgj, Dgj)]−1 (5)\n\n(cid:88)\n\nj̸=i\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.1.2 DERIVATION TO THEOREM 1\n\nGiven ⃗Dg and ⃗Rg, by (Chung et al., 2018), the capacity of such ensemble of manifolds is the average of αM , computed for each manifold by equation 1 or estimated as αBALL(κ, R, D) (equation 2). Sparse capacity for a system of manifolds with identical shapes, is computed by equation 4. Observe that equation 4 is averaging across αBALL’s, each with inputs from a manifold with either positive labeling or negative labeling indicated by +b and −b. Since all R and D are the same in a homogeneous system, equation 4 sufficiently accounts for all possible assignments of positive and negative labeling. In the case of heterogeneous geometries, it follows that we need to consider all possible dichotomies given by the sparse parameter. In order to conserve equation 4, averaging across all such dichotomies suffices. For the one-vs-rest regime discussed in this paper, there are only P dichotomies to consider, within each only one manifold is positively labeled. Thus, for each manifold i, compute equation 5. Then, averaging across all αM i, i.e. across all relevant dichotomies, gives us the sparse manifold capacity.\n\n3.1.3 SPARSE MANIFOLD CAPACITY WITH CENTER CORRELATION\n\nMFTMA assumes that the manifold centers of the system are randomly related. Realistic data, however, tend to exhibit correlation. Cohen et al. (2020) took center correlation into account by projecting the manifolds into a space where they will have low-correlated centers. This low correlation space is determined via the Euclidean centroids of each manifolds. We follow a similar approach, but instead of using the Euclidean centroids of each manifold to recover the space of low correlations, our approach for computing sparse manifold capacity with center correlation uses categorical local linear differences (CLLD), which are sampled differences between the local linear centers of the manifolds.\n\nSuppose manifold M ∈ RD. We define a manifold’s local linear centroid as the D-dimensional projection of its Euclidean centroid in the LLE space. The LLE space is the lower d ≪ D dimensional space that resulted from the local linear embedding (LLE) (Roweis & Saul, 2000) of a manifold3. We describe the algorithm to recover the appropriate low correlations space using CLLD and calculate sparse manifold capacity in figure 2. In appendix A.4, we show our method is better than the technique in Cohen et al. (2020) for estimating sparse manifold capacity with correlations.\n\nComputing sparse manifold capacity with center correlation using CLLD.\n\n1: Use LLE to project each manifold into a low dimensional space 2: For each manifold, compute its Euclidean center in the new space and project the center back to\n\nthe original, high dimensional space to obtain local linear centroids. 3: Sample CLLD by sampling differences between local linear centroids. 4: Use CLLD to recover the space of low center correlation via the technique in Cohen et al. (2020). 5: Project each manifold into the low center correlation space and compute capacity via Theorem 1\n\nFigure 2: Pseudocode for sparse manifold capacity with correlation. See appendix A.2 for details.\n\n4 RESULTS\n\nBy relaxing the homogeneous and random correlations assumptions in Chung et al. (2018), our method estimates manifold capacity for real data in the one-vs-rest recognition limit, the relevant domain for machine learning and neuroscience. While previous works (Cohen et al., 2020; Stephenson et al., 2019) estimate manifold capacity in ANNs under the dense label regime, we demonstrate for the first time the application of manifold capacity in the neuroscience domain, enabled by sparse labeling. We use our Sparse Replica Manifold Analysis to measure the sparse manifold capacity of deep neural network manifolds and neural manifolds collected from the ventral streams of primates presented with visual stimulus in experiments done by Majaj et al. (2015a) and Freeman et al. (2013). Details of these datasets are provided in the appendix A.3. We show that sparse manifold capacity requires fewer number of features, ideal for neural data where limited number (e.g., order of hundreds) of neurons are recorded. With this, we demonstrate the match between the theoretical and empirical manifold capacity in real neural data for the first time (to our knowledge). We then study the manifold\n\n3For the experiments in this paper, we chose d=2 to minimize the reconstruction error of LLE projection\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ngeometries in relation to sparse manifold capacity and show explicitly that smaller manifold intrinsic dimension is desirable for larger capacity and better linear separability.\n\n4.1 SPARSE MANIFOLD CAPACITY THEORY PREDICTS NUMERICS IN REAL DATA\n\nIn this section, we show how well theorem 1 predicts the sparse manifold capacity. Figure 3 demonstrates the match between the theoretical sparse manifold capacity (determined by our theorem) and simulated sparse manifold capacity (the ground truth determined empirically) for responses in layers of a CIFAR-100 trained VGG-16 and brain regions of Macaque monkeys to given stimuli. The ground truth sparse manifold capacity is determined by interpolating the critical number of features (Nc) over which the probability a one-vs-rest dichotomy is linearly separable drops from 1 to 0. In practice, we use a bisection search for Nc where the probability is nearly 0.5. The linear separability of each dichotomy is determined by finding a consistent SVM model. The full algorithm to determine simulated capacity is given in the appendix A.6. Interestingly, it appears that the neural data in figure 3 is better estimated by theorem 1. A possible explanation is that neural data have existing neural noise that smooths out the manifold geometries and makes them more amenable to our analysis.\n\nFigure 3: Theoretical vs simulated sparse manifold capacity (a) on smooth manifolds in layers of a VGG16 (b) on pointcloud manifolds in layers of a VGG16 (c) on neural data from the ventral stream of Macaque monkeys engaged in a classification task. Each dot is a layer or a brain region.\n\n4.2 BROADER APPLICATION OF MANIFOLD ANALYSIS ENABLED BY SPARSITY\n\nFigure 4: (a) Sparse Replica Manifold Analysis enables simulation-theory validation in real neural data with limited number of the neurons. Given N0 neurons in the data, capacity under dense labels tends to be outside the testable regime (top). Conversely, under sparse labels, the theoretical capacity can be validated empirically (bottom). In (b) and (c), we demonstrate this phenomenon. We estimate critical number of features Nc = P/αc from the neural data in V4 and IT regions (Majaj et al., 2015a) and the V1 and V2 regions (Freeman et al., 2013) under dense labels (left) and sparse labels (right). The testable regime (highlighted blue) depends on the number of neurons/features available. Sample images of each dataset are shown on the bottom of their respective panel. Observe that under dense labeling, there is no simulated data available for validation.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nSparse labeling allows analysis of capacity on a wider class of neural data that has a limited number of neurons measured. Recall that capacity is the ratio of P/Nc (refer to section 2). Figure 4a (top) demonstrates that under dense labels and limited number of neurons/features, N0, capacity often cannot be validated empirically using the bisection search because Nc ≫ N0. On the other hand, the verification for capacity under sparse labeling requires less number of features (Figure 4a bottom). For the Majaj dataset (Figure 4b), the estimation for Nc under dense labeling requires almost a thousand neurons in the IT region where only 168 neurons were recorded. Under sparse labeling, the estimation is reduced to 65 neurons. Similarly, for the Freeman dataset (Figure 4c), dense labeling estimates 185 and 159 neurons for the V1 and V2 regions while sparse labeling estimates just 32 and 27 neurons, well within the number of neurons available.\n\n4.3 COMPUTATIONAL EFFICIENCY\n\nWe observe that the computational time for the relevant manifold geometries is faster under sparse labeling compared to dense labeling by an order of magnitude. Table 1 compares the CPU time for computing manifold radius and dimension under dense labeling and sparse labeling for layers of the VGG-16. We note that the total sequential computation time for sparse manifold capacity is dominated by the optimization of the b parameter, and it is typically not faster compared to capacity under dense labels (see appendix A.5). Nonetheless, sparse manifold capacity enables the use of Gaussian radius and Dimension as a convenient measure of manifold geometry during object recognition.\n\nTable 1: Sparse Replica Manifold Analysis Computation Time (seconds): Manifold Radius and Dimension computed sequentially for 40 manifolds.\n\nVGG-16 Layers Sparse Labels Dense Labels\n\nInput 1.01 51.68\n\nReLU-4 ReLU-7 ReLU-14 ReLU-21 ReLU-28 ReLU-37 1.16 50.00\n\n1.14 48.53\n\n1.15 46.34\n\n0.76 45.76\n\n1.23 50.30\n\n1.07 45.10\n\n4.4 MANIFOLD GEOMETRY AND SPARSE CAPACITY IN ARTIFICIAL AND NEURAL DATA\n\nIn this section, we study the behavior between capacity, manifold Gaussian radius and Dimension, along with the manifold effective dimension and radius. Imposing hierarchy on layers of the DNN and the ventral stream, figure 5 shows that capacity generally increases while average manifold Gaussian dimension (Dg) and Gaussian radius decrease (Rg). This follows the similar observations made in Cohen et al. (2020) and suggests that compressing the manifold allows better manifold linear separability.\n\nFigure 5 also compares the Gaussian geometries we have introduced to the notion of effective dimension and effective radius. Effective dimension and effective radius correspond to measuring the manifold dimension and radius on an ellipsoid imposed on the manifold, whereas the Gaussian geometries are measured on the convex hull of each manifold. Under sparse labeling, the effective dimension is also known as the participation ratio in most literature (Chung et al., 2018; Gao et al., 2017; Litwin-Kumar et al., 2017; Elmoznino & Bonner, 2022; Jozwik et al., 2019; Sorscher et al., 2021). In figure 5(a), mean effective dimension and radius behave similarly to mean Gaussian dimension and radius for layers of the DNN. Hence, the manifold intrinsic dimension, measured by the participation ratio of each manifold, also decreases as capacity increases. We note that figure 5(b-c)(ii,iv) shows that mean effective dimension and mean Gaussian dimension do not decrease across the hierarchy imposed on the biological ventral stream as evidently as mean effective radius and mean Gaussian radius. Instead, the manifold Gaussian dimension and effective dimension appear to exhibit the opposite behavior relative to the artificial network. These suggestive differences between the artificial and the biological brain points toward using manifold geometries as a metric to finding and fitting an artificial brain model.\n\n4.5 ROLE OF DIFFERENT DIMENSIONS IN CAPACITY AND LINEAR SEPARABILITY\n\nWe have shown that capacity increases while manifold Gaussian radius and Dimension decrease. This suggests that compressing the manifolds and reducing their dimensions allow better separation. This may seem to contradict the intuitive belief in deep learning that higher dimensions allow better\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: This figure plots (i) capacity, (ii) mean manifold Gaussian dimension, (iii) mean manifold radius, (iv) mean manifold effective dimension , and (v) mean manifold effective radius for smooth manifolds in ReLU layers of the (a) VGG16, (b) neural data in the V4 and IT regions, and (c) in the V1 and V2 regions. Effective dimension is computed as ∥λ∥2 1/∥λ∥2 where λ is the eigenspectrum of the covariance matrix of manifold. Effective radius is computed as ∥λ∥2/∥λ∥1.\n\nclassification. In Figure 6, we give experimental evidence how linear separability is related to both lower manifold intrinsic dimension (D) and larger ambient dimension (N ). Figure 6a summarizes our point: ambient dimension allows more expressivity for a separating hyperplane, but when the manifold intrinsic dimension is high, the manifolds may not be separable.\n\nFor these experiments, we examine the interaction between manifold intrinsic dimension (as gaussian mean width and effective dimension), ambient dimension (N ), the sparse manifold capacity, and the margin of linear separability in the SVM (i.e. the distance between the anchor point and separating hyperplane). We sample 20 manifolds, consisting of 1000 examples in the CIFAR100 dataset. We vary the intrinsic dimension by projecting each manifold to a number of their PCA components. We vary the ambient dimension by random projection. In figure 6c,d,f,g, we illustrate that at a fixed ambient dimension, increasing intrinsic dimension leads to decreasing capacity and margin. On the other hand, at a fixed manifold intrinsic dimension, increasing ambient dimension increases capacity and margin (Figure 6b,e). Hence, even though higher ambient dimension may enable better linear separability, the intrinsic dimension still plays a role.\n\n5 DISCUSSION\n\nWe devised the Sparse Replica Manifold analysis for estimating sparse manifold capacity that is applicable to neuroscience experiments. As Gardner’s original perceptron capacity demonstrated that the sparse labeling regime increases capacity (Gardner, 1988), our new theory allows applications of the replica mean field theory analysis on neural data with few feature dimensions or neurons recorded. Thus, we can begin analyzing the dynamics of the neural system where only sufficient samples of neurons were drawn (Gao et al., 2017). Our theory can estimate sparse manifold capacity in layers of a deep neural network and in real neural data of the ventral stream (Figures 3,4). We are able to estimate the number of features required to separate a one-vs-rest dichotomy with 0.5 success rate within the regime of available number of features.\n\nIn contrast to the capacity used in Cohen et al. (2020), sparse manifold capacity depends only on the Gaussian radius and Dimension of each manifold, which is faster to compute than in the classical regime (Table 1). We note, however, that the time takes to compute sparse manifold capacity is longer, attributed to the optimization of the bias parameter, which warrants future studies. To account for correlation between manifolds, we also differ from Cohen et al. (2020) by\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: (a) illustrates the relationship between ambient dimension N (shown as orange axes), manifold intrinsic dimension D (shown as green axes), and linear separability. Manifolds are most easily separable in the regime of high ambient dimension and low manifold dimension. (b) shows that with a fixed α, increasing Nc (critical number of features) will increase the population capacity, which is the number of separable manifolds, P . Recall that we only need N ≫ Nc. Otherwise α is not dependent on N . (b) holds true as long as N ≫ Nc. Refer to section 2 for notations. (c)-(g) are empirical results on sparse manifold capacity and the mean margin achieved by a SVM when linear separability is plausible. (c)-(d) demonstrate that the per neuron capacity, which is the sparse manifold capacity, decreases as effective dimension (participation ratio) and Gaussian mean (cid:112)Dg) increase respectively. (e) illustrates that the margin of the SVM increases as the width (Rg ambient dimension increases. We increase ambient dimension using random projections. (f)-(g) on the other hand demonstrate that margin decreases as effective dimension and the Gaussian mean width increases.\n\nusing categorical local linear differences (CLLD) to compute a space of low correlation. Thus, we introduce another method to deal with the center correlation for manifold analysis.\n\nincreasing capacity corresponds to decreasing manifold radius and dimension Generally, (Figure 5), and figure 6 shows that linear separability is associated with both low manifold dimension and high ambient dimension. The focus on the effective ambient dimension has been used in the literature to describe the encoding capability and generalization performance of neural networks (Elmoznino & Bonner, 2022; Jozwik et al., 2019) with claims that higher ambient dimensional measures benefit performance (Elmoznino & Bonner, 2022). Our work studies the manifold intrinsic dimension and illustrates the two types of dimension may behave inversely, filling in the gap of knowledge in the argument between the role of high and low dimensional representations in neural encoding.\n\nSparse manifold capacity can be related to other work involving sparseness in neural encoding. Previous work associates sparse neural connectivity with better readout performance (Babadi & Sompolinsky, 2014; Litwin-Kumar et al., 2017). These ideas of sparseness are associated with the number of active neurons, where in our work could relate to the number of critical features needed to achieve the given capacity of manifolds, which extends to the representation’s efficiency. Sparse manifold capacity is used for the regime of one-vs-rest class discrimination, where higher sparse manifold capacity is associated with improvement in manifold separability, equivalent to better readout performance.\n\nSparse Our theory opens many future directions in neuroscience and machine learning. manifold capacity broadens the operating regime of the replica manifold analysis framework to lower ambient dimensions, enabling analysis of both artificial and biological neural networks occupying low dimensional space. Among other things, we hope this work will (a) enable broader accessibility of the present manifold analysis approach to a wider experimental neuroscience community; (b) motivate the usage of present geometric approach to measure and train a broader size range of machine learning models; and (c) inspire future studies in using geometric approaches like ours as method to compare representations between biological neural data and artificial neural network models of the brain, by providing theoretically grounded metrics in representation geometry and task performance.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nYaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning From Data. AMLBook,\n\n2012.\n\nBaktash Babadi and Haim Sompolinsky. Sparseness and expansion in sensory representations.\n\nNeuron, 83(5):1213–1226, 2014.\n\nSueYeon Chung. Statistical Mechanics of Neural Processing of Object Manifolds. PhD thesis,\n\nHarvard University, 2017.\n\nSueYeon Chung and LF Abbott. Neural population geometry: An approach for understanding biological and artificial neural networks. Current opinion in neurobiology, 70:137–144, 2021.\n\nSueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Classification and geometry of general\n\nperceptual manifolds. Physical Review X, 8(3):031003, 2018.\n\nUri Cohen, SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Separability and geometry of\n\nobject manifolds in deep neural networks. Nature communications, 11(1):1–13, 2020.\n\nThomas M. Cover. Geometrical and statistical properties of systems of linear inequalities with IEEE Transactions on Electronic Computers, EC-14(3):\n\napplications in pattern recognition. 326–334, 1965. doi: 10.1109/PGEC.1965.264137.\n\nJames Dicarlo and David Cox. Untangling invariant object recognition. Trends in cognitive sciences,\n\n11:333–41, 09 2007. doi: 10.1016/j.tics.2007.06.010.\n\nR Becket Ebitz and Benjamin Y Hayden. The population doctrine in cognitive neuroscience. Neuron,\n\n109(19):3055–3068, 2021.\n\nEric Elmoznino and Michael F Bonner. High-performing neural network models of visual cortex\n\nbenefit from high latent dimensionality. bioRxiv, 2022.\n\nJeremy Freeman, Corey M. Ziemba, David J. Heeger, Eero P. Simoncelli, and J. Anthony Movshon. A functional and perceptual signature of the second visual area in primates. Nature Neuroscience, 16(7):974–981, Jul 2013. ISSN 1546-1726. doi: 10.1038/nn.3402. URL https://doi.org/ 10.1038/nn.3402285.\n\nEmmanouil Froudarakis, Uri Cohen, Maria Diamantaki, Edgar Y Walker, Jacob Reimer, Philipp Berens, Haim Sompolinsky, and Andreas S Tolias. Object manifold geometry across the mouse cortical visual hierarchy. BioRxiv, pp. 2020–08, 2021.\n\nJuan A Gallego, Matthew G Perich, Lee E Miller, and Sara A Solla. Neural manifolds for the control\n\nof movement. Neuron, 94(5):978–984, 2017.\n\nPeiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna Shenoy, and Surya Ganguli. A theory of multineuronal dimensionality, dynamics and measurement. BioRxiv, pp. 214262, 2017.\n\nElizabeth Gardner. The space of interactions in neural network models. Journal of physics A:\n\nMathematical and general, 21(1):257, 1988.\n\nXiaoxuan Jia, Joshua H Siegle, Corbett Bennett, Samuel D Gale, Daniel J Denman, Christof Koch, and Shawn R Olsen. High-density extracellular probes reveal dendritic backpropagation and facilitate neuron classification. Journal of neurophysiology, 121(5):1831–1847, 2019.\n\nKamila Maria Jozwik, Hyo-Dong Lee, Nancy Kanwisher, and James DiCarlo. Are topographic deep convolutional neural networks better models of the ventral visual stream? 2019 Conference on Cognitive Computational Neuroscience, 2019.\n\nNikolaus Kriegeskorte and Rogier A Kievit. Representational geometry: integrating cognition,\n\ncomputation, and the brain. Trends in cognitive sciences, 17(8):401–412, 2013.\n\nNikolaus Kriegeskorte and Xue-Xin Wei. Neural tuning and representational geometry. Nature\n\nReviews Neuroscience, 22(11):703–718, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAshok Litwin-Kumar, Kameron Decker Harris, Richard Axel, Haim Sompolinsky, and LF Abbott.\n\nOptimal degrees of synaptic connectivity. Neuron, 93(5):1153–1164, 2017.\n\nNajib J. Majaj, Ha Hong, Ethan A. Solomon, and James J. DiCarlo. Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance. Journal of Neuroscience, 35(39):13402–13418, 2015a. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.5181-14.2015. URL https://www.jneurosci.org/content/ 35/39/13402.\n\nNajib J Majaj, Ha Hong, Ethan A Solomon, and James J DiCarlo. Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance. Journal of Neuroscience, 35(39):13402–13418, 2015b.\n\nSam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. doi: 10.1126/science.290.5500.2323. URL https://www.science.org/doi/abs/10.1126/science.290.5500.2323.\n\nMartin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, Kailyn Schmidt, Daniel L. K. Yamins, and James J. DiCarlo. Brain-score: Which artificial neural network for object recognition is most brain-like? bioRxiv preprint, 2018. URL https://www.biorxiv.org/content/ 10.1101/407007v2.\n\nMartin Schrimpf, Jonas Kubilius, Michael J Lee, N Apurva Ratan Murty, Robert Ajemian, and James J DiCarlo. Integrative benchmarking to advance neurally mechanistic models of human intelligence. Neuron, 2020. URL https://www.cell.com/neuron/fulltext/S0896-6273(20) 30605-X.\n\nAnirvan Sengupta, Cengiz Pehlevan, Mariano Tepper, Alexander Genkin, and Dmitri Chklovskii. Manifold-tiling localized receptive fields are optimal in similarity-preserving neural networks. Advances in neural information processing systems, 31, 2018.\n\nHansem Sohn, Devika Narain, Nicolas Meirhaeghe, and Mehrdad Jazayeri. Bayesian computation\n\nthrough cortical latent dynamics. Neuron, 103(5):934–947, 2019.\n\nBen Sorscher, Surya Ganguli, and Haim Sompolinsky. The geometry of concept learning. bioRxiv,\n\n2021.\n\nNicholas A. Steinmetz, Cagatay Aydin, Anna Lebedeva, Michael Okun, Marius Pachitariu, Marius Bauza, Maxime Beau, Jai Bhagat, Claudia Böhm, Martijn Broux, Susu Chen, Jennifer Colonell, Richard J. Gardner, Bill Karsh, Fabian Kloosterman, Dimitar Kostadinov, Carolina Mora-Lopez, John O’Callaghan, Junchol Park, Jan Putzeys, Britton Sauerbrei, Rik J. J. van Daal, Abraham Z. Vollan, Shiwei Wang, Marleen Welkenhuysen, Zhiwen Ye, Joshua T. Dudman, Barundeb Dutta, Adam W. Hantman, Kenneth D. Harris, Albert K. Lee, Edvard I. Moser, John O’Keefe, Alfonso Renart, Karel Svoboda, Michael Häusser, Sebastian Haesler, Matteo Carandini, and Timothy D. Harris. Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings. Science, 372(6539):eabf4588, 2021. doi: 10.1126/science.abf4588. URL https://www.science.org/doi/abs/10.1126/science.abf4588.\n\nCory Stephenson, Jenelle Feather, Suchismita Padhy, Oguz Elibol, Hanlin Tang, Josh McDermott, and SueYeon Chung. Untangling in invariant speech recognition. Advances in neural information processing systems, 32, 2019.\n\nVladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of\n\nevents to their probabilities. In Measures of complexity, pp. 11–30. Springer, 2015.\n\nA APPENDIX\n\nA.1 DERIVATION TO THEOREM 1\n\nSuppose we have a heterogeneous system of P manifolds. Let ⃗Dg = [Dg1, ...Dgp] be the array of their corresponding Gaussian dimensions. And let ⃗Rg = [Rg1, ...Rgp] be the array of their corresponding\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nGaussian radii. By Chung et al. (2018), the capacity of such ensemble of manifolds is the average of αM , computed for each manifold by equation 1 or estimated as αBALL(κ, R, D) (equation 2) where R and D are the manifold’s respective measure of dimension and radius. Furthermore, by Chung et al. (2018), sparse capacity for a homogeneous system of manifolds (i.e.assuming all manifolds have the same Gaussian radius and Gaussian dimension), is computed as\n\nαHOM OG = max\n\n[f · α−1\n\nBall(κ + b, Rg, Dg) + (1 − f ) · α−1\n\nBall(κ − b, Rg, Dg)]−1\n\nb\n\n(⋆)\n\nwhere f is the sparse parameter (i.e. the fraction of positively labeled manifolds), and b is the bias parameter that has positive contributions to positively labeled manifolds and negative contributions to negatively label manifolds. Observe that equation (⋆) is thus averaging across αBALL’s, each with inputs from a manifold with either positive labeling or negative labeling indicated by +b and −b. Since all R and D are the same in a homogeneous system, equation (⋆) sufficiently accounts for all possible assignments of positive and negative labeling.\n\nIn the case of heterogeneous geometries, it follows that we need to consider all possible dichotomies given by the sparse parameter. In order to conserve equation (⋆), averaging across all such dichotomies suffices. For the one-vs-rest regime discussed in this paper, there are only P dichotomies to consider, within each only one manifold is positively labeled. Thus, for each manifold i, compute\n\nαM i(κ, ⃗Rg, ⃗Dg) = max\n\n[1/P · α−1\n\nBall(κ + b, Rgi, Dgi) +\n\nb\n\n1/P · α−1\n\nBall(κ − b, Rgj, Dgj)]−1\n\n(cid:88)\n\nj̸=i\n\nThen, averaging across all αM i, i.e. across all relevant dichotomies, gives us the sparse manifold capacity. Observe that this general theorem does not deviate from the case of a homogeneous system. In the case of f = 1/P and we have a homogeneous system,\n\nαHOM OG = αM i = i̸=j\n\nαM j =< αM i >\n\nA.2 COMPUTING SPARSE MANIFOLD CAPACITY VIA CLLD\n\nAlgorithm 1 gives the detailed pseudocode to calculate sparse manifold capacity with center correlation. For the experiments in this paper, the number of components chosen for local linear embedding of each manifold was heuristically chosen as the default value of 2 to minimize the reconstruction error of the dimensional reduction. See section 3.1.3 for definitions.\n\nA.2.1 ALGORITHMS FOR LOCALIZING LOCAL LINEAR CENTROID\n\nDiscovering the local linear centroid involves projecting each manifold low dimensional center to the high dimensional space. Since this problem involves solving an overdetermined system, we use algorithms to localize for the local linear centroid. Here we describe two of these algorithms. Results of the paper use the Nearest Neighbor algorithm.\n\nThe Nearest Neighbor algorithm tracks the K nearest points on the manifold closest to the Euclidean centroid in the LLE space. The D-dimensional local linear center is the average of these K points in the higher D-dimensional space. On the other hand, the Weighted Average algorithm determines a weighted average in the D-dimensional space based on the distance of each example of the manifold from the Euclidean centroid in the LLE space. Points on the manifold closer to the LLE space Euclidean centroid will have a greater weight. The algorithms to determine local linear centroids given a manifold and its LLE equivalent are provided in Algorithms 2 and 3\n\nA.3 DATASETS\n\nWe used three datasets for our experimental analysis. In figure 3, capacity is measured across layers of a trained VGG-16 and brain regions of Macaque monkeys. For the DNN, we extract the activation from each layer to form manifolds corresponding to a given input. Responses to inputs of the same class form an object manifold. A pointcloud manifold consists of distinct examples from the same class. A smooth manifold is generated by affine transformations (translation) of an image belonging to a distinct class. Neural data were taken from Majaj et al. (2015a) and Freeman et al. (2013) where images were shown to Macaque monkeys, and responses from the primate ventral stream were\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Calculate sparse manifold capacity with Center Correlation Function: sparse_capacity_with_correlation (X) Input data: Set of manifolds X = {M 1, M 2, ..M P }, M i ∈ RDxm Output data: sparse manifold capacity αc\n\n1: {μ1, μ2, ..μP } ← local_linear_centroids(X) 2: Sample ∆ ⊂ {μi − μj|i ̸= j, i, j ∈ {1, ..P }} 3: Compute common component matrix, V , using ∆ ((Cohen et al., 2020) Suppl) 4: for manifold M i, i ∈ {1, 2...P } do Compute residual manifold M i 5: Compute Dgi, Rgi of M i 6: 7: end for 8: for manifold M i, i ∈ {1, 2...P } do 9:\n\nCompute αi(κ, ⃗Dg, ⃗Rg) (by equation 5)\n\nr = M i − V (V T M i)\n\nr (by equation 3)\n\n10: end for 11: Compute αc = ⟨αi⟩i\n\nFunction: local_linear_centroids (X) Input data: Set of manifolds X = {M 1, M 2, ..M P }, M i ∈ RDxm Output data: Local linear centroids {μ1, μ2, ..μP }, μi ∈ RDx1 1: for manifold M i, i ∈ {1, 2...P } do 2: 3: 4: 5: end for\n\nPerform LLE on M i and project M i to its LLE space. Let Li be the projected manifold, and compute the center νi = ⟨Li⟩ Compute μi, the projection of νi in the original, higher dimensional space. (See A.2.1)\n\nAlgorithm 2 Nearest Neighbor Algorithm for Computing Local Linear Centroids Input data: Manifold M i ∈ RDxm; the projection of M i after LLE Li ∈ Rdxm; the mean of Li, νi ∈ Rdx1 Output data: Local linear centroid μi ∈ RDx1 Parameters: K ∈ Z\n\n1: Compute the set of {δj = ∥νi − Li 2: δj1, ...δjK ← minf irstK{δj} 3: μi = ⟨M i\n\nj ⟩j∈{j1,j2,...jK}\n\nj∥|∀j ∈ {1, ...m}}\n\nAlgorithm 3 Weighted Average Algorithm for Computing Local Linear Centroids Input data: Manifold M i ∈ RDxm; the projection of M i after LLE Li ∈ Rdxm; the mean of Li, νi ∈ Rdx1 Output data: Local linear centroid μi ∈ RDx1 Parameters: s ∈ R+\n\n1: ∀j ∈ {1, ...m}, compute δj = ∥νi − Li 2: ∀j ∈ {1, ...m}, compute wj = 1/δs 3: ∀j ∈ {1, ...m}, compute ˆwj = wj/ (cid:80) 4: μi = (cid:80)m\n\nj=1 ˆwjM i\n\nj\n\nj\n\nj∥\n\nk wk\n\nrecorded. Again, responses to inputs of the same class form an object manifold. The Majaj dataset contains neural responses to 64 different classes from the V4 and IT regions. The Freeman dataset contains neural responses to 30 different classes from the V1 and V2 regions.\n\nA.3.1 DNN TRAINING\n\nA regular VGG-16 was trained using pytorch on the Cifar100 dataset. The model was trained on a stochastic gradient descent optimizer with a learning rate of 0.01 and a momentum rate of 0.9.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA learning scheduler was used to reduce learning rate by 0.5 every 10 epochs. The final training accuracy was nearly 0.90.\n\nA.3.2 GENERATING SMOOTH AND POINTCLOUD MANIFOLDS\n\nSmooth manifolds are generated by affine transformations of an image. For a Cifar100 image, a 24x24 frame is translated randomly from the center of the image by a maximum of 4 pixels horizontally and vertically. Each pointcloud manifold consists of the top 50 well-trained examples from that class. For Figure 3, we extract smooth manifolds from the following layers of the VGG-16\n\n[’layer_00_Input’, ’layer_01_Conv2d’, ’layer_02_ReLU’, ’layer_03_Conv2d’, ’layer_04_ReLU’, ’layer_07_ReLU’, ’layer_09_ReLU’, ’layer_10_MaxPool2d’, ’layer_12_ReLU’, ’layer_13_Conv2d’, ’layer_14_ReLU’, ’layer_16_ReLU’, ’layer_21_ReLU’, ’layer_23_ReLU’, ’layer_24_MaxPool2d’, ’layer_26_ReLU’, ’layer_27_Conv2d’, ’layer_28_ReLU’, ’layer_30_ReLU’, ’layer_37_ReLU’]\n\nWe extract our pointcloud manifolds from the following layers of the VGG-16\n\n[’layer_00_Input’, ’layer_02_ReLU’, ’layer_04_ReLU’, ’layer_07_ReLU’, ’layer_09_ReLU’, ’layer_14_ReLU’, ’layer_16_ReLU’, ’layer_21_ReLU’, ’layer_23_ReLU’, ’layer_28_ReLU’, ’layer_30_ReLU’, ’layer_37_ReLU’]\n\nA.3.3 NEURAL DATA\n\nThe data from Majaj et al. (2015a) are made available by Brainscore (Schrimpf et al., 2020) (Schrimpf et al., 2018). The data consists of neural responses from the V4 and IT regions of four Macaque monkeys. The stimulus set consists of 64 classes, and there are a total of 88 neurons recorded in the V4 region and 168 neurons recorded in the IT region. The data from Freeman et al. (2013) are made available by the authors of the work. There are a total of 102 and 103 neurons recorded from the V1 and V2 regions of 13 Macaque monkeys respectively. The set of stimuli contains 15 texture classes. For the purpose of the author’s experiments, the 15 texture classes are used to generate corresponding 15 noise classes. We consider the neural data corresponding to texture and noise images together. Hence, in total, we had 30 classes for this dataset.\n\nA.4 COMPUTING SPARSE MANIFOLD CAPACITY VIA CLLD VS METHOD IN COHEN ET AL\n\nThe calculation for sparse manifold capacity assumes that manifolds are uncorrelated. Cohen et al. (2020) takes into account of correlation for the classical notion of manifold capacity by finding a space of low correlation. This space is computed using the centroids of each manifold. Compared to Cohen et al. (2020), our method outlined by algorithm 1 uses CLLD, categorical local linear differences, to compute the space of low correlation. Figure 7 shows that the method via CLLD is in fact superior to the method in Cohen et al. (2020) for predicting sparse manifold capacity in our neural datasets. Using CLLD allows a better match between theoretical and simulated sparse capacity.\n\nA.5 SPARSE REPLICA MANIFOLD ANALYSIS COMPUTATION TIME\n\nTable 2 shows the computational time of Sparse Replica Manifold Analysis for layers of VGG-16 for 40 smooth manifolds. Observe that radius and dimension are computed simultaneously as capacity in the dense label regime. Also, observe that in the regime of sparse label, the compute time of capacity dominates the total compute time.\n\nA.6 SIMULATING CAPACITY\n\nThe algorithm for simulating sparse manifold capacity uses a bisection search to find Nc, the number of feature dimensions the input data can be reduced to such that half of the one-vs-rest dichotomies are linearly separable. The full algorithm is in Algorithm 4.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: This figure shows the match between simulated and theoretical sparse manifold capacity on neural datasets when different methods are used to take into account of manifold correlation. In (a), the method presented in Cohen et al. (2020) is employed. The manifold centroids are used to recover the space of low correlation. In (b), CLLD are used to recover the space of low correlation. Refer to appendix A.2.\n\nTable 2: Sparse Replica Manifold Analysis Computation Time (seconds) for 40 smooth manifolds in VGG-16 Layers\n\nDense Labels\n\nSparse Labels\n\nVgg16 Layers R,D Capacity Total Time Rg,Dg Capacity Total Time 159.99 Input 111.09 ReLU-2 99.41 ReLU-4 84.51 ReLU-7 82.18 ReLU-9 68.31 ReLU-14 69.76 ReLU-16 69.98 ReLU-21 61.31 ReLU-23 62.43 ReLU-28 81.88 ReLU-30 105.36 ReLU-37\n\n771.75 762.17 846.37 832.91 768.40 715.54 798.93 1336.58 1153.42 1290.43 1086.01 1198.15\n\n857.46 851.72 932.27 918.44 856.76 802.85 893.71 1424.74 1245.36 1379.53 1177.79 1284.02\n\n51.68 50.21 50.00 50.30 50.92 48.53 48.34 46.34 46.34 45.76 45.97 45.10\n\n51.68 50.21 50.00 50.30 50.92 48.53 48.34 46.34 46.34 45.76 45.97 45.10\n\n1.01 1.24 1.16 1.23 1.20 1.14 1.21 1.15 1.28 0.76 0.82 1.07\n\nA.7 CAPACITY FOR ELLIPICAL MANIFOLDS\n\nFigure 6 measures simulated capacity while varying manifold effective dimension. Effective dimension is equivalent to the manifold Gaussian dimension if the manifold is an ellipsoid (Chung et al., 2018). Hence, simulated capacity for figure 6(c,d) is calculated assuming manifolds have an elliptical structure. As such, we use the Maximum Margin Manifold Machines algorithm described in (Chung, 2017).\n\nA.8 MEAN FIELD THEORY OF MANIFOLD SEPARATION\n\nIn this section, we elucidate the emergence of anchor points in the solution for manifold capacity. Following Gardner’s framework, Chung et al. (2018) determined that in the max margin solution for M = ⟨F ( ⃗T )⟩ ⃗T , where a homogeneous system of manifolds, inverse manifold capacity is given by α−1 F ( ⃗T ) = min⃗V {∥⃗V − ⃗T ∥2|gS(⃗V ) − κ ≥ 0, ∀⃗S ∈ S} and gS(⃗V ) = min ⃗S{⃗V · ⃗S|⃗S ∈ S}.\n\nαM is interpreted for a single manifold. ⃗T are sampled Gaussian vectors while ⃗Vi = y ⃗w · ⃗ui (i = 1, ...D + 1), where ⃗w is a solution hyperplane, ⃗ui is the manifold’s ith basis vector, and y is the\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nend if\n\nFound← True\n\nif ∥prob − 0.5∥ < tol then\n\nN ← Nnext prob ← is_linearly_separable(X, N )\n\nAlgorithm 4 Simulated Capacity Function: bisection_search (X) Input data: Set of manifolds X = {M 1, M 2, ..M P }, M i ∈ RDxm Output data: sparse manifold capacity αc 1: Initialize N = 0 and F ound = F alse 2: Initialize Nmax = D and Nmin = 2 3: Nnext ← (Nmax + Nmin)/2 4: probmax ← is_linearly_separable(X, Nmax) 5: probmin ← is_linearly_separable(X, Nmin) 6: 7: while not Found and N ! = Nnext do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end while 28: 29: if Found then 30: 31: 32: else 33: 34: 35: end if 36: αc = P/Nc\n\nend if Nnext ← (Nmax + Nmin)/2\n\nif (probmin − 0.5) ∗ (prob − 0.5) < 0 then\n\nNmax ← N probmax ← prob\n\nNmin ← N probmin ← prob\n\nInterpolate for Nc\n\nNc ← Nnext\n\nelse\n\nmanifold’s label. ⃗V is described as the signed fields induced by the solution hyperplane, and S can be thought of as the set of points on the manifold. The KKT interpretation is to find ⃗V given by\n\nsuch that the following holds\n\n⃗V = ⃗T + λ ̃S( ⃗T )\n\nλ ≥ 0\n\ngS(⃗V ) − κ ≥ 0\n\nλ[gS(⃗V ) − κ] = 0\n\n(6)\n\nand ̃S = arg min ⃗S∈S ⃗V · ⃗S. The mean field interpretation of equation 6 is that ⃗V is a decomposition of contributions from other manifolds represented by ⃗T and the fixed manifold represented by ̃S. Furthermore, equation 6 implies that the resulting F ( ⃗T ) is precisely ∥λ ̃S( ⃗T )∥2, with ̃S( ⃗T ) denoted as the anchor point of the manifold.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFunction: is_linearly_separable (X,N ) Input data: Set of manifolds X = {M 1, M 2, ..M P }, M i ∈ RDxm; Number of feature dimensions N\nOutput data: Probability of separation given dichotomy, prob\n\nAssign the set Y of binary labels for each manifold, where manifold i gets the positive label. Find consistent SVM solution for input X and label Y\n\n1: Initialize sep = 0 2: Project D-dimensional manifolds to N dimensions using random projection. 3: 4: for i ∈ {1, ..., P } do 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: prob ← sep/P\n\nif solution exists then sep = sep + 1\n\nend if\n\nA.9 HOMOGENEOUS VS HETEROGENEOUS GEOMETRIES\n\nWe’ve made the claim in the previous section, Derivation to Theorem 1, that the equation 4 is insufficient for a system of manifolds with varying radii and dimensions. We demonstrate the disparity in the match between the theoretical and simulated sparse manifold capacity when different assumptions of the geometries are made in figure 8 and 9. Using the smooth manifold and the pointcloud manifold data from the VGG-16 dataset used throughout this paper, we compute the capacity using homogeneous geometries by averaging across the Gaussian radii and dimensions of all manifolds in the system. Comparing the left and the right plots in each picture, it is evident that assuming homogeneous geometries is insufficient in a system where manifolds have varying Gaussian radii and dimensions.\n\nFigure 8: Heterogeneous vs Homogeneous geometries in estimating sparse manifold capacity for smooth manifolds. (a) is the result of incorporating the heterogeneous geometries in sparse manifold capacity as demonstrated in Fig 3a of the paper (i.e. using theorem 1). (b) is the result of assuming homogeneous geometries to estimate sparse manifold capacity (i.e. equation 4 in the paper). If we simply use the average dimension and radius of all manifolds to estimate the sparse capacity, we have the resulting (mis)match in (b)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Heterogeneous vs Homogeneous geometries in estimating sparse manifold capacity for pointcloud manifolds. (a) is the result of incorporating the heterogeneous geometries in sparse manifold capacity as demonstrated in Fig 3b of the paper (i.e. using theorem 1). (b) is the result of assuming homogeneous geometries to estimate sparse manifold capacity (i.e. equation 4 in the paper). If we simply use the average dimension and radius of all manifolds to estimate the sparse capacity, we have the resulting (mis)match in (b)\n\nA.10 SPARSE REPLICA MANIFOLD ANAYSIS ON IMAGENET-TRAINED RESNET 101\n\nWe used an imagenet-trained Resnet101 and create 20 object smooth manifolds from the following extracted convolutional layers. Results of the match between theoretical and simulated sparse manifold capacity is shown in figure 10\n\n[ ’layer_123_Conv2d’, ’layer_014_Conv2d’, ’layer_155_Conv2d’, ’layer_181_Conv2d’, ’layer_202_Conv2d’, ’layer_232_Conv2d’, ’layer_028_Conv2d’, ’layer_048_Conv2d’ ]\n\nFigure 10: Theoretical vs simulated sparse manifold capacity measured on 20 manifolds from an imagenet-trained Resnet101.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nA.11 CLLD SENSITIVITY TO NUMBER OF NEURONS\n\nEstimating sparse manifold capacity requires accounting for manifold correlation. Cohen et al. (2020) successfully account for manifold center correlation in their work that applied dense labeled manifold capacity to artificial neural network. In appendix A.4 and figure 7 using the method in Cohen et al. (2020) is insufficient for sparse manifold capacity. Instead, we show that the method we presented in this paper using CLLD does a better job. In figure 11, we show that using CLLD always outperform Cohen et al regardless of the number of neurons present in the data. We vary the number of neurons by random projection. The results show that the estimation for sparse manifold capacity is robust.\n\nFigure 11: In (a) and (b), each color represents the capacity at various layers of the ventral stream. Each point of the same color is the capacity measured from randomly projecting the neural manifold to some number of features. (a1) and (b1) show explicitly the number of neurons vs the resulting estimated capacity. We vary the number of neurons by random projection. The resulting theoretical capacity changes little as a result. Furthermore, compared to the method in Cohen et al (b1), the method via CLLD is closer to the ground truth capacity (represented by x).\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper extends Chung et al. (2018), which proposes Sparse Replica Manifold analysis to estimate manifold capacity. The authors show that the application of sparse manifold capacity requires a smaller number of features and is faster compared to dense labeling. The authors also illustrate the effects of ambient dimension and manifold intrinsic dimension on sparse separability.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1.The proposed Sparse Replica Manifold analysis overcomes the limitation of size of neuroscience dataset and is close to real tasks in cognitive science.\n\n2.The authors analyze the manifold capacity of both deep neural network manifolds and neural manifolds, making the conclusions more convincing.\n\nWeaknesses:\n\n1.The authors follow the work of Chung et al. (2018), but they don’t clearly claim the contributions of previous work.\n\n2.The datasets this paper uses are relatively old and limited.\n\n3.More comprehensive and thorough experiments are necessary to prove the conclusions.\n\nQuestions for the Authors\n\n1.In chapter 3, the authors claims that sparse manifold capacity has been previously considered in Chung et al. (2018) and that they extend this work k by taking into account of heterogeneous geometries and correlation between manifolds. But in “RESULTS”, they don’t clearly claim the advances of Sparse Replica Manifold compared with previous works and the contributions of taking different Gaussian Radii and Dimensions into account.\n\n2.The dataset this paper uses are from Majaj et al. (2015), Freeman et al. (2013) and VGG-16 trained by a CIFAR-100. More experiments on latest and large-scale datasets are needed.\n\n3.In Figure 5, the change ranges of Dg in both neural dataset and the tendency of Rg and Deff is not so evident. More experiments are necessary to prove the conclusion.\n\n4.The broader application of manifold analysis is meaningful; however, the scale of neuroscience dataset is larger recently. The total sequential computation time of Spares Replica Manifold Analysis has also no advantage. The influence of this method needs to prove again.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe novelty of this paper is limited and some description is not clear.\n\n# Summary Of The Review\n\nThis paper is the following work of Chung et al. (2018) and many conclusions have been proposed before.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSAMPLED TRANSFORMER FOR POINT SETS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe sparse transformer can reduce the computational complexity of the selfattention layers to O(n), whilst still being a universal approximator of continuous sequence-to-sequence functions. However, this permutation variant operation In this paper, we proposed an is not appropriate for direct application to sets. O(n) complexity sampled transformer that can process point set elements directly without any additional inductive bias. Our sampled transformer introduces random element sampling, which randomly splits point sets into subsets, followed by applying a shared Hamiltonian self-attention mechanism to each subset. The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles. This mechanism implements a Monte Carlo simulation of the O(n2) dense attention connections. We show that it is a universal approximator for continuous set-to-set functions. Experimental results for classification and few-shot learning on point-clouds show comparable or better accuracy with significantly reduced computational complexity compared to the dense transformer or alternative sparse attention schemes.\n\n1\n\nINTRODUCTION\n\nEncoding structured data has become a focal point of modern machine learning. In recent years, the defacto choice has been to use transformer architectures for sequence data, e.g., in language (Vaswani et al., 2017) and image (Dosovitskiy et al., 2020) processing pipelines. Indeed, transformers have not only shown strong empirical results, but also have been proven to be universal approximators for sequence-to-sequence functions (Yun et al., 2019). Although the standard transformer is a natural choice for set data, with permutation invariant dense attention, its versatility is limited by the costly O(n2) computational complexity. To decrease the cost, a common trick is to use sparse attention, reducing the complexity from O(n2) to O(n) (Yun et al., 2020; Zaheer et al., 2020; Guo et al., 2019). However, in general this results in an attention mechanism that is not permutation invariant – swapping two set elements change which elements they attend. As a result, sparse attention cannot be directly used for set data.\n\nRecent work has explored the representation power of transformers in point sets as a plug-in module (Lee et al., 2019), a pretraining-finetuning pipeline (Yu et al., 2022; Pang et al., 2022), and with a hierarchical structure (Zhao et al., 2021). However, these set transformers introduced additional inductive biases to (theoretically) approach the same performance as the densely connected case in language and image processing applications. For example, to achieve permutation invariance with efficient computational complexity, previous work has required nearest neighbor search (Zhao et al., 2021) or inducing points sampling (Lee et al., 2019). Following the above analysis, a research question naturally arises to avoid introducing unneeded inductive bias:\n\nCan O(n) complexity sparse attention mechanisms be applied directly to sets?\n\nWe propose the sampled transformer to address this question, which is distinguished from the original sparse transformer by mapping the permutation of set elements to the permutation of attention matrix elements. Viewing this permutation sampling as attention matrix sampling, the proposed sampled attention approximates O(n2) dense attention. This is achieved with the proposed random element sampling and Hamiltonian self-attention. To be specific, in random element sampling the input point set is first randomly split into several subsets of ns points (Fig. 1b), each of which will be processed by shared self-attention layers. In addition, a sparse attention mechanism – namely\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Dense Attention\n\n(b) After Sampling\n\n(c) Hamiltonian Attention\n\n(d) Cycle Attention\n\n(e) Swap Points\n\n(f) Sampled Attention\n\nFigure 1: Attention mechanisms: (a) original dense attention; (b) the attention matrix after random element sampling; (c) a special case of sparse attention – Hamiltonian (self-)attention – for each subset; (d) combining all subsets (which have overlapping element per (b)) connects the individual Hamiltonian attention sub-matrices, gives cycle attention which is a Hamiltonian cycle; (e) permutation of points permutes the elements in cycle attention matrix; (f) the resulting sampled attention, viewed as a sampled Hamiltonian cycle from the edges of the complete attention graph.\n\nHamiltonian self-attention (Fig. 1c) – is applied to reduce complexity of the subset inputs, so that ns point connections are sampled from O(n2 s) connections. The combination of all Hamiltonian self-attention mechanism for all subsets – namely cycle attention (Fig. 1d) – can be viewed as a Hamiltonian cycle in the complete attention graph. As a result, the permutation of set elements is equivalent to the permutation of nodes in a Hamiltonian cycle (Fig. 1e), which is in fact randomly sampling Hamiltonian cycles from the complete graph – thereby yielding the proposed sampled attention (Fig. 1f). Finally, viewing this randomization as a Monte Carlo sample of attention pairs, repeated sampling can be used to approximate the complete O(n2) dense connections. Furthermore, our proposed sampled transformer is proven to be a universal approximator for set data – any continuous set-to-set functions can be approximated to arbitrary precision.\n\nThe contributions of this paper are summarized as follows.\n\n• We propose the sampled attention mechanism which maps the random permutation of set elements to the random sampling of Hamiltonian cycle attention matrices, permitting the direct processing of point sets.\n\n• We prove that the proposed sampled transformer is a universal approximator of continuous\n\nset-to-set functions, see Corollary 1.\n\n• Compared to previous transformer architectures, the empirical results show that our proposed sampled transformer achieves comparable (or better) performance with less inductive bias and complexity.\n\n2 RELATED WORK\n\nThe transformer (Vaswani et al., 2017) is widely used in languages (Raffel et al., 2020; Dai et al., 2019; Yang et al., 2019b) and images (Dosovitskiy et al., 2020; Liu et al., 2021; Touvron et al., 2021; Ramachandran et al., 2019). For example, Raffel et al. (2020) explored the transformer by unifying a\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nsuite of text problems to a text-to-text format; Dai et al. (2019) modeled very long-term dependency by reusing previous hidden states; Dosovitskiy et al. (2020) demonstrated that the pure transformer can be effectively applied directly to a sequence of image patches; and Liu et al. (2021) proposed a transformer with hierarchical structure to learn various scales with linear computational complexity. In addition, the representation power of the transformer has been explored by the pre-training and fine-tuning models (Devlin et al., 2018; Bao et al., 2021; Yu et al., 2022; He et al., 2022).\n\nRecently, an increasing number of researchers begin to explore the representation power of the transformer in 3D point clouds (sets) data. Xie et al. (2018) applied multi-layered dense transformers to small-scale point clouds directly; Yang et al. (2019a) further proposed the Group Shuffle attention to deal with size-varying inputs by furthest point sampling; Han et al. (2022) aggregated point-wise and channel-wise features by directly adding two self-attention layers. To avoid the tricky tokenization step, Lee et al. (2019) tried to deal with points directly with O(nm) complexity by introducing inducing points, and proved universal approximation; Mazur & Lempitsky (2021) further proposed a hierarchical point set mapping, grouping, and merging structure with nearest neighbors defining the sparse attention mechanism. Yu et al. (2022) and Pang et al. (2022) further introduced the transformers to the pre-training and fine-tuning pipelines in the area of 3D point clouds. Last but not the least, transformers have also been widely used in other such works on 3D (point cloud) data as Liu et al. (2019a); Misra et al. (2021); Mao et al. (2021); Fuchs et al. (2020); Sander et al. (2022)\n\nAnother important line of work seeks to theoretically demonstrate the representation power of the transformer by showing the universal approximation of continuous sequence-to-sequence functions (Yun et al., 2019; 2020; Zaheer et al., 2020; Shi et al., 2021; Kratsios et al., 2021). To be specific, Yun et al. (2019) demonstrated the universal approximation property of the transformer; Yun et al. (2020) and Zaheer et al. (2020) demonstrated that the transformer with sparse attention matrix remains a universal approximator; Shi et al. (2021) claimed that the transformer without diag-attention is still a universal approximator. Kratsios et al. (2021) proposed that the universal approximation under constraints is possible for the transformer.\n\nIn comparison with the above works, we proposes the O(n) sampled transformer – a universal approximator of continuous set-to-set functions. To our knowledge, the use of approximating dense attention by sampling Hamiltonian cycle attention matrices is new.\n\n3 PRELIMINARY\n\n3.1 NOTATION\n\n. = {1, . . . , a}. For a matrix M ∈ Rn×m, for a k ∈ [m] the k-th Given an integer a we define [a] column is denoted by Mk. Given an (ordered) index set A ⊂ [m] the submatrix MA ∈ Rn×|A| consists of the matrix generated by concatenating the columns determined by indices in A. See the notation guide in §A in the supplementary material.\n\n3.2 TRANSFORMER\n\nThe transformer X (cid:55)→ t(X) (Vaswani et al., 2017; Dosovitskiy et al., 2020) implements a function from point clouds to point clouds with input points X ∈ Rd×n. It is formally defined by a multi-head self-attention layer and a feed-forward layer: Headj(X) = (W j\n\n(1a)\n\nV X) · σS[(W j\n\nQX]\n\nAttn(X) = X + WO\n\nKX)T W j \n\n\n\n \n\nHead1(X) ... Headh(X)\n\n \n\nTB(X) = Attn(X) + W2 · ReLU(W1Attn(X)),\n\nwhere n is the number of points and d is the feature dimension. Head(·) is the self-attention layer, and Attn(·) is the multi-head self-attention layer with the parameter WO ∈ Rd×mh. W i Q ∈ Rm×d are value, key, and query parameters; W1 ∈ Rr×d and W2 ∈ Rd×r are feed-forward layer parameters. We utilize a positional embedding E in the input X, defined by E = WpP , where P ∈ R3×n is the (xyz) coordinate, and WP ∈ Rd×3 is an MLP layer. To\n\nK, W i\n\nV , W i\n\n3\n\n(1b)\n\n(1c)\n\nUnder review as a conference paper at ICLR 2023\n\nsimplify the notation, here we use X = X + E so that all the inputs X in this paper will include the positional embedding unless specifically stated otherwise. The attention mechanism for a dense transformer is the n × n attention matrix (W i QX in Eq. 1a, which is in fact a similarity matrix for n elements/tokens, or a complete attention graph. Sparse Attention also refers to the same similarity matrix/attention graph but with sparse connections instead. As tokenization may not be necessary in dealing with point clouds, for clarity we use the terminology points, elements, and tokens are all to refer to points (which may be thought of as tokens in a traditional transformer context) in a point cloud (set).\n\nKX)T W i\n\n3.3 UNIVERSAL APPROXIMATION\n\nLet F be the class of continuous sequence-to-sequence functions f : Rd×n (cid:55)→ Rd×n defined on any compact domain. Further define T h,m,r as the set of transformer blocks t(·) with h attention heads of each of size m, and with hidden layer width r (Yun et al., 2019; 2020). To measure the distance between functions in F, we define the standard lp distance function by the corresponding norm:\n\n(cid:18)(cid:90)\n\ndp(f1, f2) =\n\n∥f1(X) − f2(X)∥p\n\np dX\n\n(cid:19)1/p\n\n,\n\n(2)\n\nwhich is element-wise continuous (w.r.t the lp norm) for 1 ≤ p < ∞. Theorem 1 (Universal Approximation, Yun et al. (2019)). Let 1 ≤ p < ∞ and ε > 0, then for any given f ∈ F, there exist a Transformer network g ∈ T 2,1,4, such that dp(f, g) ≤ ε.\n\nThe proof of Theorem 1 makes three stages of approximations, which are chained together via the triangle inequality to give the ε bound (Yun et al., 2019). In particular, 1 any f ∈ F is approximated by a piece-wise linear function f ∈ F (over a discretized input space). Then 2 the piece-wise linear function is approximated by a modified transformer T , where the widely used ReLU and σS activation functions (as per Eq. 1) are replaced by the hardmax function σH . Finally, 3 it is shown that the class of transformer T The key step comes in the proof of the second approximation 2. In Yun et al. (2019), the approximation is proved by showing that multi-head self-attention layers of the modified transformer can implement any contextual map qc : Rd×n (cid:55)→ Rn. Definition 3.1 (Contextual Mapping). Consider a finite set L ⊂ Rd×n. A map q: L (cid:55)→ R1×n defines a contextual map if the map satisfies the following:\n\ncan approximate any regular transformer g ∈ T 2,1,4.\n\n2,1,4\n\n2,1,4\n\n1. For any L ∈ L, the n entries in q(L) are all distinct.\n\n2. For any L, L′ ∈ L, with L ̸= L′, all entries of q(L) and q(L′) are distinct.\n\nIntuitively, a contextual map can be thought of as a function that outputs unique “id-values”. The only way for a token (column) in L ⊂ Rd×n to share an “id-value” (element of q(L)) is to map the exact same sequence. As each token in the sequence is mapped to a unique value, an appropriately constructed feed-forward neural network can map a sequence to any other desired sequence, providing a universal approximation guarantee. In Yun et al. (2020), such a contextual map is implemented via selective shift operators and all-max-shift operators through careful construction of multi-head self-attention layers.\n\n4 METHODOLOGY\n\nWe propose a variation of the sparse attention transformer – sampled sparse attention transformer – applicable to point sets. We deviate from the typical sparse attention transformer in two ways. First, we randomly sub-sample the input point set l times, with each sub-sample being evaluated through a shared multi-head self-attention layer. Secondly, we propose a simple Hamiltonian self-attention mechanism, a special case of the sparse attention mechanism, to reduce the computation complexity of considering point sets. This ultimately yields the variant of the typical sparse transformer (Eq. 1) which can be interpreted as using a sampled attention mechanism, as depicted in Fig. 1. To study the approximation capabilities of our proposed architecture, we prove that our sampled sparse attention transformer is a universal approximator of set-to-set functions.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4.1 RANDOM ELEMENT SAMPLING\n\nFor a point set input X ∈ Rd×n, instead of directly applying the transformer attention layer to n tokens, we process l many sub-sampled inputs Xi ∈ Rd×ns for i ∈ [l] and 2 ≤ ns ≤ n. For simplicity, we assume that (ns − 1) · l = n. The sub-sampled inputs Xi can be defined by taking various column submatrices:\n\nXi = XRi∪Rγ(i)\n\n1\n\n;\n\nγ(v) = 1 + (v mod l),\n\n(3)\n\nwhere R1, . . . Rl are randomly selected ordered index sets, such that |Ri| = ns −1 and Ri ∩Rj = ∅ for i ̸= j. The index element Ri 1 denotes the first index in the ordered set Ri. The cycle function γ : [l] → [l] ensures that the edge-case of Xl is well defined, i.e., γ(l) = 1.\n\nIntuitively, the sequence of sub-sampled inputs X1, . . . , Xl can be interpreted as a rolling window of (ns − 1) · l = n many sampled point set elements. Indeed, by concatenating the index sets in order, Xi is a sliding window of the elements with size ns and stride ns − 1 (with wrapping).\n\nIt should be noted that Xi can be treated as a random variable. As such a singular realization of the sampled elements can be viewed as a Monte Carlo sample over the set of ordered point sequences (Metropolis & Ulam, 1949). Computationally, by applying a dense self-attention layer to each of the sub-sampled elements Xi, the total complexity of evaluating l many self-attention layer is O(l · n2 s). We however note that the l self-attention layers can be evaluated in parallel, which yields a trade-off between individual self-attention complexity O(n2\n\ns) and computation time.\n\nTo gain intuition, consider the “limiting behaviours” of our random element sampling: taking ns = n + 1 can be interpreted as taking the whole sequence with l = 1, i.e., X1 = X which under dense attention would result in complexity O(n2). On the other end, if we take ns = 2, we get l = n pairs of points |Xi| = 2; processing every such pair with dense self-attention results in n many O(1) self-attention evaluations. Random element sampling with dense attention layers can be interpreted as an instance of sparse attention, see Fig. 1b.\n\n4.2 HAMILTONIAN SELF-ATTENTION\n\nThe random element sampling discussed in the previous section reduces the computational coms) = O(n2/l) (as (ns − 1) · l = n) plexity of dense self-attention-layers from O(n2) to O(l · n2 by processing each sampled set of points Xi through individual self-attention layers. Despite this improved computational complexity, the quadratic scaling of n can still be costly for point clouds.\n\nAs such, instead of evaluating each sampled element X1, . . . , Xl with a dense self-attention layer, we propose a sparse attention layer. Sparse attention mechanisms can be formally defined via the attention patterns {Ak}k∈[ns], where j ∈ Ak implies that the j-th token will attend to the k-th token. We propose the use of an attention mechanism, dubbed as Hamiltonian self-attention, which is defined by the following attention patterns:\n\nAk =\n\n(cid:26){k, k + 1}\n\n{k}\n\nif 1 ≤ k < ns otherwise k = ns\n\n,\n\n(4)\n\nwhich ensures that the set of attention patterns {Ak}k∈[ns] define a Hamiltonian path. Indeed, if we fix a subset of elements Xi, by starting at Xi 1 and following the attended elements (ignoring self-attention k ∈ Ak), we visit every token exactly once. Fig. 1c shows the corresponding attention matrix, where the Hamiltonian path corresponds to off-diagonal elements and self-attention corresponds to the diagonal elements, respectively.\n\nFor Hamiltonian self-attention, computing the attention mechanism according to Eq. 4 only requires 2ns = O(ns) many evaluations. Thus by using our proposed sparse attention for each X1, . . . , Xl, in comparison to dense attention, the computational complexity reduces from O(n2/l2) to O(n/l).\n\nThe proposed Hamiltonian self-attention mechanism is rather simple and general. For instance, in the general case sparsity patterns can be defined for each individual layer (resulting in an addition superscript for each Ak). Despite this, the attention patterns {Ak}k∈[ns] satisfy important key assumptions for proving that the attention pattern will result in a sparse transformer that is a universal\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\napproximator (Yun et al., 2020, Assumption 1). In particular, by stacking (ns − 1) many attention layers, our Hamiltonian self-attention will allow any element to indirectly or directly attend all other element in a Xi. The proposed Hamiltonian self-attention could also be viewed as a special case of window attention in Zaheer et al. (2020), where elements are linked undirectedly.\n\n4.3 SAMPLED SPARSE ATTENTION TRANSFORMER\n\nGiven the setup of random element sampling and Hamiltonian self-attention, we can define our proposed sampled transformer for continuous set-to-set function approximation:\n\nSHeadj\n\nk(Xi) = (W j\n\n)T W j\n\nQXi k]\n\nAk\n\ngi(Xi) = Xi + WO\n\nV Xi \n\n) · σS[(W j KXi \n\nAk SHead1(Xi) ... SHeadh(Xi)\n\n \n\n \n\n(5a)\n\n(5b)\n\n(5c)\n\nSAttn(X) = gl(Xl) ◦ gl−1(Xl−1) ◦ · · · ◦ g1(X1) STB(X) = SAttn(X) + W2 · ReLU (W1SAttn(X)) .\n\n(5d) (5e) In Eq. 5c, composition is w.r.t. the induced linear maps from matrices given by Eq. 5b. The learnable parameters of the sampled transformer are the same as the usual dense transformer in Eq. 1.\n\n1\n\nAs the attention pattern of each Xi forms a Hamiltonian path, and each Xi shares an element with the proceeding Xγ(i), the joint attention map makes a Hamiltonian cycle path. In other words, the shared index Rγ(i+1) in Eq. 3 links each individual Hamiltonian path given by Eq. 4, leading the attention matrix to form a cycle attention as shown in Fig. 1d. Furthermore, the permutation of elements in cycle attention corresponds to the swapping of nodes in the Hamiltonian cycle, with corresponding links and swapping of element values in the attention matrix, see in Fig. 1e. As a result, the combined randomization from using random element sampling and Hamiltonian selfattention can be thought of as sampling from the set of Hamiltonian cycle graphs from the complete attention graph, resulting in the sampled attention depicted in Fig. 1f.\n\nUnlike dense attention, sparse attention patterns are not generally permutation invariant. Indeed, if we permute the columns of Xi, the elements attended according to {A}k∈[ns] are not the same. As such, applying {Ak}k∈[ns] directly to X is not valid for point clouds, which requires a permutation invariant operation. However, in our case the sparse attention heads are being applied to randomized sub-sampled element sets Xi. Ignoring computation, if we continue to sample the randomized elements Xi and average the resulting attention (w.r.t. the entire point set X), the attention will converge to dense attention – through randomization of Xi, the event that any non-self-edge appears in a sampled attention graph (as per Eq. 4) is equiprobable. This also holds when fixing the order of elements while applying randomly sampled Hamiltonian cycle attention. As such, the sampled transformer can be used to approximate a permutation invariant operator, and thus be used to approximate set-to-set functions.\n\nOf course, sampling sufficiently many realizations of Hamiltonian cycle attention to converge to dense attention is impractical. Instead, in practice, we re-sample the attention pattern only for each batch and epoch. Although this may seem like a crude approximation to dense attention, similar methods are successful in Dropout (Srivastava et al., 2014), which even induces desirable model regularization. Furthermore, our empirical results indicate that sampled sparse attention closely approximates the more expensive (and infeasible at the typical point set scales) dense attention.\n\n4.4 SAMPLED TRANSFORMER AS A UNIVERSAL APPROXIMATOR\n\nWe formally guarantee the representation power of the proposed sampled transformer by proving universal approximation for set-to-set functions. As our sampled transformer Eq. 5c is similar to dense / sparse transformers presented by Yun et al. (2019; 2020), we follow their framework (Sec. 3.3) to prove our universal approximation property. Corollary 1 (Sampled Transformer is a Universal Approximator). There exist sampled (sparse) Transformers that are universal approximators in the sense of Theorem 1.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Object classification on ModelNet40. Here [ST] denotes that model adopts the standard (dense) transformer, while [T] denotes all other transformers.\n\nSupervised Methods\n\nAccuarcy\n\nPointNet (Qi et al., 2017a) PointNet++ (Qi et al., 2017b) PointCNN (Li et al., 2018) KPConv (Thomas et al., 2019) DGCNN (Wang et al., 2021) RS-CNN (Liu et al., 2019b) [T] PCT (Guo et al., 2021) [T] PVT (Zhang et al., 2021) [T] PointTransformer (Zhao et al., 2021) [T] Transformer (Yu et al., 2022)\n\n89.2% 90.7% 92.5% 92.9% 92.9% 92.9% 93.2% 93.6% 93.7% 91.4%\n\nSelf-Supervised Methods\n\nAccuarcy\n\nOcCo (Wang et al., 2021) STRL (Huang et al., 2021) IAE (Yan et al., 2022) [ST]Transformer-OcCo (Yu et al., 2022) [ST]Point-BERT (Yu et al., 2022) [ST]Point-MAE (Pang et al., 2022)\n\n[ST]MAE-dense (ours) [T]MAE-sampled (ours)\n\n93.0% 93.1% 93.7% 92.1% 93.2% 93.8%\n\n93.6% 93.7%\n\nTo prove our Corollary, we extend the proof of Yun et al. (2019; 2020) by showing that our sparse attention mechanisms with random element sampling can also implement a selective shift operator. As a result, we show that the proposed sampled sparse attention transformer is a universal approximator in the context of set-to-set functions. See §E in the supplementary material for the full proof of the universal approximation property.\n\n5 EXPERIMENTS\n\nWe evaluate our proposed sampled attention in popular transformer-based frameworks as well as a basic setting. We compare our sampled attention (Fig. 1f) with dense attention via the pre-training and fine-tuning framework (Yu et al., 2022; Pang et al., 2022), where we pre-train our model on ShapeNet (Chang et al., 2015) via the reconstruction task, and further evaluate the performance on two downstream fine-tuning tasks: classification and few-shot learning in ModelNet40 (Wu et al., 2015). In addition, to eliminate the influence of other factors, we compared the dense, sparse, sampled, and kNN attention (Definition B.1) in a basic classification setting consisting of a transformer block with a single attention layer for feature aggregation, as well as a minimal number of MLPs for feature mapping. Finally, we compare the sampled attention with the kNN attention in the hierarchical grouping and merging structure following the Point-Transformer (Zhao et al., 2021).\n\n5.1 COMPARSION ON PRE-TRAINING AND FINE-TUNING FRAMEWORK\n\nPre-training. We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data, denoted as MAE-dense, for pre-training. This framework is the concurrent work with Point-MAE (Pang et al., 2022). Note that MAE-dense adopts dense-attention layers in its encoder and decoder network. To evaluate the effectiveness of our claimed contribution, we replace the dense-attention layer in MAE-dense with our sampled-attention layer (Fig. 1f) while keeping the other components fixed. It is denoted as MAE-sampled.\n\nTo pre-train the MAE-dense and MAE-sampled, we follow the standard train-test split of ShapeNet (Chang et al., 2015), which is also adopted by Pang et al. (2022); Yu et al. (2022), and develop the following training strategy. To begin with, each input point cloud consisting of 1024 points was divided into 64 groups / tokens of size 32 points each. The Furthest Points Sampling (FPS) and nearest neighbour search were adopted in tokenization (Yu et al., 2022). Tokens were further mapped to 256-dimensional latent vectors by MLP layers and max-pooling. In addition, we have 12 stacked transformers in the encoder (masking ratio of 70%) and 1 single transformer in the decoder, both with h = 8, d = 32 and r = 256. The batch size is 64 and the epoch number is 300. We used the AdamW (Loshchilov & Hutter, 2017) optimizer with cosine learning rate decay (Loshchilov & Hutter, 2016), an initial learning rate of 0.0005, and weight decay of 0.05.\n\nClassification The pre-trained MAE-dense and MAE-sampled models are first evaluated on the classification task in ModelNet40 (Wu et al., 2015), with the standard training and testing splits defined in Yu et al. (2022); Pang et al. (2022). Specifically, we build the classifier by keeping the encoder structure and weights of the pre-trained MAE-dense and MAE-sampled models, followed by max-pooling as well as a fully connected layer of dimension [256, 256, 40] to map the global\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Mean ± std. dev. accuracy (%) for 10 independent Few-shot classification experiments.\n\nMethods\n\n5-way, 10-shot\n\n5-way,20-shot\n\n10-way,10-shot\n\n10-way, 20-shot\n\nDGCNN-rand (Wang et al., 2021) DGCNN-OcCo (Wang et al., 2021) Transformer-rand (Yu et al., 2022) Transformer-OcCo (Yu et al., 2022) Point-BERT (Yu et al., 2022) Point-MAE (Pang et al., 2022)\n\nMAE-dense (ours) MAE-sampled (ours)\n\n31.6 ± 2.8 90.6 ± 2.8 87.8 ± 5.2 94.0 ± 3.6 94.6 ± 3.1 96.3 ± 2.5\n\n95.9 ± 3.1 97.0 ± 2.3\n\n40.8 ± 4.6 92.5 ± 1.9 93.3 ± 4.3 95.9 ± 2.3 96.3 ± 2.7 97.8 ± 1.8\n\n97.2 ± 2.1 98.3 ± 1.6\n\n19.9 ± 2.1 82.9 ± 1.3 84.6 ± 5.5 89.4 ± 5.1 91.0 ± 5.4 92.6 ± 4.1\n\n90.8 ± 5.0 92.7 ± 5.4\n\n16.9 ± 1.5 86.5 ± 2.2 89.4 ± 6.3 92.4 ± 4.6 92.7 ± 5.1 95.0 ± 3.0\n\n92.8 ± 3.9 93.8 ± 3.5\n\nTable 3: Object classification accuracy (%) for different attention mechanisms in the basic setting. OM denotes out of memory.\n\n#Points\n\n256\n\n512\n\n768\n\n1024\n\n2048\n\n3072\n\n4096\n\n8192\n\nMLP + FC (no attention) Dense Attention Sparse Attention Sampled Attention kNN Attention\n\n85.96 87.78 87.09 87.34 85.80\n\n86.24 88.72 88.03 87.93 84.74\n\n85.43 88.11 87.54 87.66 85.35\n\n85.96 88.47 87.74 88.03 84.70\n\n85.84 88.39 87.58 87.82 82.95\n\n86.61 OM 87.42 87.18 82.58\n\n86.32 OM 87.42 87.46 82.26\n\n86.13 OM 87.58 87.73 OM\n\ntoken of a dimension of 256 to the 40 categories. Similar to Yu et al. (2022), we further dataaugment the point cloud training set via random scaling and translation during training. As shown in Tab. 1, the proposed method achieved the second best performance compared with the most recent state-of-the-art alternatives. Our sampled attention can achieve an accuracy improvement of 0.1% when compared to dense attention, while reducing the complexity from O(n2) to O(n).\n\nFew Shot Learning The pre-trained MAE-dense and MAE-sampled models are also evaluated on a few shot learning task. Following Sharma & Kaul (2020); Wang et al. (2021); Yu et al. (2022); Pang et al. (2022), the few-shot learning adopted an k-way, m-shot training setting on the ModelNet40 (Wu et al., 2015) dataset, where k represents the number of randomly sampled classes and m the number of randomly sampled examples per class. The testing split is 20 randomly sampled unseen examples from each class. We set k ∈ {5, 10} and m ∈ {10, 20}, and report the mean accuracy with standard deviation for 10 independent experiments. As shown in Tab. 2, our proposed MAE-sampled model outperformed all state-of-the-art methods on 3 out of 4 settings, while MAE-sampled consistently outperformed MAE-dense.\n\n5.2 COMPARSION ON BASIC CLASSIFICATION SETTING\n\nOur inputs are clouds of n points with 3D coordinates as position and its normal information as features. The feature and position are first transformed by two separate MLP layers with hidden dimensions [64, 256], and then added together as the input of a single layer transformer with h = 8, r = 256, and d = 32, as per Eq. 1 and Eq. 5. The transformer output of Rn×256 is then summarized by max-pooling to obtain a global feature with a dimension of 256, followed by a fully connected layer to map it to the category vector. Here we tested this basic pipeline with n ∈ {256, 512, 768, 1024, 2048, 3072, 4096, 8192} for each of the dense, sparse, kNN, and the proposed sampled attention layers, including an additional case without attention layer (MLP+Full Connected layer) as the baseline.\n\nAs shown in Tab. 3 and Tab. 4, the model with dense attention layers achieves the best performance as it considers all O(n2) connections directly with relatively few parameters to train. However, it runs out of the 24 Gigabytes memory when the number of points n ≥ 3072, due to the quadratic complexity. While both sparse and sampled transformers have a computational complexity of O(n), our model with sampled attention outperformed the sparse one, in line with the strong theoretical guarantees we provide. We conjecture that the improvements of sampled transformer over the sparse transformer may indicate that the additional randomness (randomly shuffling points, w / o attention)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Memory usage (Gb) for different attention mechanisms in the basic classification setting. All are trained on a single RTX 3090 with 24 Gb on board RAM. OM denotes out of memory.\n\n#Points\n\n256\n\n512\n\n768\n\n1024\n\n2048\n\n3072\n\n4096\n\n8192\n\nMLP + FC (no transformer) Dense Attention Sparse Attention Sampled Attention kNN Attention\n\n0.9 1.2 1.0 1.0 1.9\n\n0.9 1.8 1.1 1.1 2.8\n\n0.9 2.7 1.2 1.2 3.7\n\n1.0 3.9 1.3 1.3 4.4\n\n1.1 11.9 1.7 1.7 8.5\n\n1.1 1.5 1.2 OM OM OM 4.3 2.5 2.1 4.2 2.5 2.1 OM 16.5 11.4\n\nTable 5: Classification accuracy (%) for sampled and kNN attention with hierarchical model structure.\n\n#Layers\n\n1\n\nsampled attention kNN attention\n\n74.55 66.23\n\n2\n\n88.0 82.8\n\n3\n\n90.5 90.1\n\n4\n\n91.0 91.0\n\n5\n\n91.8 91.4\n\nleads to a better approximation of the O(n2) connections in a manner analogous to Dropout (Srivastava et al., 2014). In addition, the transformer with kNN attention layers has the worst performance, as the permutation could not extend its receptive field. Finally, the memory usage comparison in Tab. 4 shows that the dense transformer has the largest memory usage due to its O(n2) complexity. The sparse transformer and sampled transformer have comparable memory usage due to the same O(n) complexity.\n\n5.3 COMPARSION ON HIERARCHICAL TRANSFORMER STRUCTURE\n\nWe further compare our sampled attention with kNN attention by adopting the hierarchical structure for the classification task under the framework of Zhao et al. (2021). Each hierarchical layer is obtained by FPS, followed by the nearest neighbour search for the grouping, using MLPs with maxpooling for feature merging, and transformers for feature mapping. The grouping stage within each hierarchical layer summarizes the point cloud into key (subset) points.\n\nThe total hierarchical layer number is t = 5, the parameters for which we chose the number k of nearest neighbours {8, 16, 16, 16, 16}, strides {4, 4, 4, 4, 4}, self-attention feature dimensions {32, 64, 128, 256, 512}, and transformer blocks {2, 3, 4, 6, 3}. The scalar attention (Eq. 1 or Eq. 5) is adopted specifically for comparison. Results shown in Tab. 5 demonstrate that our sampled attention outperforms the kNN attention in line with our randomly sampled receptive field. Furthermore, the performance of the kNN layer improved greatly from t = 1 to t = 2 and from t = 2 to t = 3 as its receptive field extends due to the multiple hierarchical layers. Finally, kNN with vector attention (Yu et al., 2022) (reported in Tab. 1 on the PointTransformer row) achieved a better performance, in line with the observation that replacing the softmax with learnable MLPs γ in the transformer can easier make kNN attention a universal approximator of continuous functions. Detailed analysis is provided in §B.1 in the supplementary material. The performance difference between scalar attention and vector attention is shown in the Tab. 7 of Yu et al. (2022), and is also analyzed in Yun et al. (2020).\n\n6 CONCLUSION\n\nIn this paper, we present an O(n) complexity sparse transformer – sampled transformer – which directly handles point set data. By relating the permutation of set elements to the sampling of Hamiltonian cycle attention, we relieve the model of inappropriate permutation variance. The result is a sampled attention scheme that implements Monte Carlo simulation to approximate a dense attention layer with a prohibitive O(n2) number of connections. To guarantee the representation power of the proposed sampled transformer, we showed that it is a universal approximator of set-toset functions. Motivated also by the strong empirical performance that our model achieves, we hope this work will help to shed light on the sparse transformer in dealing with set data.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Ga ̈el Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pp. 108–122, 2013.\n\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rototranslation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970–1981, 2020.\n\nMeng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu.\n\nPct: Point cloud transformer. Computational Visual Media, 7(2):187–199, 2021.\n\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-\n\ntransformer. arXiv preprint arXiv:1902.09113, 2019.\n\nXian-Feng Han, Yi-Fei Jin, Hui-Xian Cheng, and Guo-Qiang Xiao. Dual transformer for point cloud\n\nanalysis. IEEE Transactions on Multimedia, 2022.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nSiyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised representation learning for 3d point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6535–6545, 2021.\n\nAnastasis Kratsios, Behnoosh Zamanlooy, Tianlin Liu, and Ivan Dokmani ́c. Universal approximation under constraints is possible with transformers. arXiv preprint arXiv:2110.03303, 2021.\n\nXin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya In Proceedings of the IEEE/CVF\n\nJia. Stratified transformer for 3d point cloud segmentation. Conference on Computer Vision and Pattern Recognition, pp. 8500–8509, 2022.\n\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International conference on machine learning, pp. 3744–3753. PMLR, 2019.\n\nYangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolu-\n\ntion on x-transformed points. Advances in neural information processing systems, 31, 2018.\n\nXinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 8778–8785, 2019a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural In Proceedings of the IEEE/CVF Conference on Computer\n\nnetwork for point cloud analysis. Vision and Pattern Recognition, pp. 8895–8904, 2019b.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\n\npreprint arXiv:1608.03983, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nJiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3164–3173, 2021.\n\nKirill Mazur and Victor Lempitsky. Cloud transformers: A universal approach to point cloud processing tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10715–10724, 2021.\n\nNicholas Metropolis and Stanislaw Ulam. The monte carlo method. Journal of the American statis-\n\ntical association, 44(247):335–341, 1949.\n\nIshan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2906–2917, 2021.\n\nYatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. arXiv preprint arXiv:2203.06604, 2022.\n\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660, 2017a.\n\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017b.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. Advances in Neural Information Processing Systems, 32, 2019.\n\nMichael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr ́e. Sinkformers: Transformers with doubly stochastic attention. In International Conference on Artificial Intelligence and Statistics, pp. 3515–3530. PMLR, 2022.\n\nCharu Sharma and Manohar Kaul. Self-supervised few-shot learning on point clouds. Advances in\n\nNeural Information Processing Systems, 33:7212–7221, 2020.\n\nHan Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in self-attention. In International Conference on Machine Learning, pp. 9547–9557. PMLR, 2021.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc ̧ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6411–6420, 2019.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\n\nMikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1588– 1597, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nHanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt J Kusner. Unsupervised point cloud pre-training via occlusion completion. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9782–9792, 2021.\n\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912–1920, 2015.\n\nSaining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. Attentional shapecontextnet for point cloud recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4606–4615, 2018.\n\nYifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point In Proceedings of the European Conference on\n\nsets with parameterized convolutional filters. Computer Vision (ECCV), pp. 87–102, 2018.\n\nSiming Yan, Zhenpei Yang, Haoxiang Li, Li Guan, Hao Kang, Gang Hua, and Qixing Huang. Implicit autoencoder for point cloud self-supervised representation learning. arXiv preprint arXiv:2201.00785, 2022.\n\nJiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. In Proceedings of the\n\nModeling point clouds with self-attention and gumbel subset sampling. IEEE/CVF conference on computer vision and pattern recognition, pp. 3323–3332, 2019a.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019b.\n\nXumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: In Proceedings of the\n\nPre-training 3d point cloud transformers with masked point modeling. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19313–19322, 2022.\n\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. arXiv preprint\n\nAre transformers universal approximators of sequence-to-sequence functions? arXiv:1912.10077, 2019.\n\nChulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. O (n) connections are expressive enough: Universal approximability of sparse transformers. Advances in Neural Information Processing Systems, 33:13783–13794, 2020.\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017.\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nCheng Zhang, Haocheng Wan, Xinyi Shen, and Zizhao Wu. Pvt: Point-voxel transformer for point\n\ncloud learning. arXiv preprint arXiv:2108.06076, 2021.\n\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16259–16268, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA NOTATIONS\n\nf\n\ng\n\ng\n\nF\n\nFS\n\nF\n\nF S T h,m,r\n\nh,m,r\n\nT\n\nσS\n\nσH\n\nlp\n\nGδ G+\n\nδ\n\nn\n\nd\n\nm\n\nh\n\nr\n\nδ\n\nX Xi\n\nP\n\nE\n\nL\n\nAL W i\n\nV\n\nW i\n\nK\n\nW i\n\nQ\n\nWO\n\nW1\n\nW2\n\nWp\n\na continuous function\n\ntransformer\n\nmodified transformer\n\nthe class of continuous sequence-to-sequence function\n\nthe class of continuous set-to-set function\n\nthe class of piece-wise constant sequence-to-sequence function\n\nthe class of piece-wise constant set-to-set function\n\nthe class of (sparse) transformers with h attention heads, m head size, and hidden layer width r\n\nthe class of the modified transformers with h attention heads, m head size, and hidden layer width r\n\nsoftmax activation\n\nhardmax activation\n\np norm\n\ngrid {0, δ, . . . , 1 − δ}d×n\n\nextend grid {−δ−nd, 0, δ, . . . , 1 − δ}d×n\n\nnumber of points/elements/tokens\n\npoint/element/token feature size\n\nhead size\n\nheads number\n\nhidden layer width\n\nstep size\n\ntransformer input\n\ni-th subset of transformer input\n\nxyz coordinates for point cloud (set)\n\npositional embedding\n\nquantized transformer input\n\ndesired output for the input L\n\nvalue parameter in i-th single-head attention layer\n\nkey parameter in i-th single-head attention layer\n\nquery parameter in i-th single-head attention layer\n\nmulti-head attention parameter\n\nfeed-forward layer parameter\n\nfeed-forward layer parameter\n\nparameter for position embedding\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nu\n\ne(1)\n\n1n\n\n0n\n\nHeadi(·) SHeadi(·)\n\nAttn(·)\n\nSAttn(·)\n\nTB(·)\n\nSTB(·)\n\nt(·)\n\nqc(·) Ψ(·; bQ, b′\n\nQ)\n\nψ(·; bQ)\n\ndc(·, ·)\n\nquery, key, and value parameter used in universal approximation proof\n\nindicator vector (1, 0, 0, . . . , 0) ∈ Rd\n\nvector with all ones (1, . . . , 1) ∈ Rn\n\nvector with all zeros (0, . . . , 0) ∈ Rn\n\ni-th single-head attention layer\n\ni-th sparse/sampled single-head attention layer\n\nmulti-head attention layer\n\nmulti-head attention layer with sampled sparse attention\n\ntransformer block\n\nsampled transformer block\n\na series of any number of transformer blocks\n\ncontextual mapping\n\nselective shift operation\n\na single-head attention in selective shift operation\n\ndistance between two functions\n\nB ADDITIONAL INFORMATION ON THE BASIC CLASSIFICATION SETTING\n\nB.1 kNN TRANSFORMER\n\nDefinition B.1 (kNN Attention). For k ∈ [n], kNN attention has the attention pattern Ak = kNN(k) for all points, where kNN(·) represents the Euclidean k-nearest neighbourhood of the input.\n\nDefinition B.2 (kNN Transformer). The kNN transformer is the transformer defined as in Eq. 1, but with the kNN attention of definition B.1.\n\nIn addition, in the case of vector attention (Eq. 3 in (Zhao et al., 2021)), universal approximation holds as the learnable mapping γ(·) (an MLP) is a universal approximator. This may helps to explain why vector attention could outperform scalar attention in Tab. 7 of (Zhao et al., 2021).\n\nFinaly, in Tab. 3, the performance of the kNN transformer drops with the increasing number of points. This is because as the point number increase, the fix k nearest neighbor number is relatively reduced. As a result, the receptive field shrink. So the performance drops.\n\nB.2\n\nIN COMPARISON WITH INDUCTING POINTS (SET TRANSFORMER)\n\nWe additionally compared the proposed sampled attention with learnable inducting points strategy (Lee et al., 2019) in Tab.6. The inducting points here are implemented by simply replacing the multi-heads self-attention transformer block in Eq. 5e with the Induced Set Attention Block (ISAB) in Eq. (9) of Lee et al. (2019). And the positional embedding is added in the key and value input as per our sampled attention. Our implementation of the basic classification in Sec. 5.2 is different from the one in (Lee et al., 2019) with respect to the data pre-processing: our data pre-processing is in line with Zhao et al. (2021); Yu et al. (2022), while Lee et al. (2019) follow Zaheer et al. (2017) without positional embedding. As we can see in Tab. 6, our proposed sampled attention outperformance the inducting point strategy (Lee et al., 2019) with linear complexity in the attention matrix.\n\nAs the performance of Lee et al. (2019) on the two implementations is quite different, we further compared the sampled attention and inducting points strategy in the implementation provided by the official implementation of Lee et al. (2019). To begin with, our proposed sampled attention could be applied to the inducting points strategy directly to reduce its complexity from O(mn) to O(n),\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Additional object classification accuracy (%) for different attention mechanisms in the basic setting. OM denotes out of memory.\n\n#Points\n\n256\n\n512\n\n768\n\n1024\n\n2048\n\n3072\n\n4096\n\n8192\n\nMLP + FC (no attention) Inducting Points (Lee et al., 2019) Stratified Strategy (Lai et al., 2022) Sampled Attention\n\n85.96 84.21 87.21 87.34\n\n86.24 81.25 87.62 87.93\n\n85.43 82.55 86.69 87.66\n\n85.96 81.57 85.99 88.03\n\n85.84 80.96 85.34 87.82\n\n86.61 76.18 84.32 87.18\n\n86.32 75.13 OM 87.46\n\n86.13 75.65 OM 87.73\n\nTable 7: Object classification in the setting of Lee et al. (2019) measured accuracy (%).\n\n#Points\n\n100\n\n200\n\n1000\n\n2000\n\n3000\n\n5000\n\nISAB(16) + PMA ISAB(16) + PMA + sampled attention (ours)\n\n80.52 81.25\n\n85.38 82.65\n\n84.43 84.15\n\n85.99 85.04\n\n85.49 84.48\n\n86.99 86.49\n\nwhere n is the number of input points and m is a learnable inducting points number. Specifically, we use the sampled attention to replace the dense attention in the Induced Set Attention Block(ISAB) from Eq. 9 of Lee et al. (2019). However, as the inducting points and points have different physical meanings, also as the inducting points number m (query in the self-attention) is not equal to the input points number n (key and value), our Hamiltonian cycle attention could not be applied directly. We instead applied a different version of sampled attention by randomly sampling two elements per row in the dense attention matrix. This is a loose version of sampled attention as no Hamiltonian cycle is constructed. The results could be found in Tab. 7. As we can see, our proposed sampled attention is still comparable with the set transformer but with less computational complexity.\n\nB.3\n\nIN COMPARISON WITH STRATIFIED STRATEGY\n\nThe window-based transformer is another important branch of exploring the representation power of the transformer. Combined with the hierarchical backbone, it has been widely used in processing 2D images, languages, and 3D point clouds, such as Liu et al. (2021); Lai et al. (2022). The windowbased transformer is proposed to learn the cross-window relationships as well as the non-overlapping local relationship.\n\nHere we compared our proposed sampled attention with the Stratified strategy from Figure 3 of Lai et al. (2022) in Tab. 6. The Stratified strategy could be viewed as a combination of dense and sparse keys obtained by the window partition of different sizes. It is an efficient design for learning token relationships in the hierarchical backbone. However, in the single-layer setting, directly learning O(n2) connections in the attention matrix may be a better solution as it could reach the full receptive field. As our proposed sampled attention mechanism could estimate O(n2) connections by implementing the Monto Carlo simulation, we outperformed the Stratified strategy in the basic classification setting as per Tab. 6.\n\nC AMORTIZED CLUSTERING WITH MIXTURE OF GAUSSIANS\n\nWe additionally tested the proposed sampled attention in 2D set datasets in the encoding-decoding framework introduced by (Lee et al., 2019). And the task is about using a neural network to learn the parameters of the mixture Gaussian distribution from the input set data.\n\nTo begin with, the mixture Gaussian distribution is defined by a weighted sum of k number of Gaussian distribution. Given a dataset X = {x1, . . . , xn}, the log-likelihood of the mixture Gaussian distribution is defined as follows:\n\nlog p(X; θ) =\n\nn (cid:88)\n\nlog\n\nk (cid:88)\n\ni=1\n\nj=1\n\nπjN (xi; μi; diag(σ2\n\nj )).\n\n(6)\n\nGenerally, the parameters of the mixture Gaussian distribution are inferred by maximizing the loglikelihood θ∗(X) = arg maxθ log p(X; θ) using Expectation-Maximisation (EM) algorithm as the\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Amortized clustering results. The number in ISAB(·) indicates the number of learnable inducting points used in ISAB as per Lee et al. (2019). The evaluation metric LLO/data is the average log-likelihood value, and LL1/data is the average log-likelihood value after a single EM update (implemented by scikit-learn package (Buitinck et al., 2013)).\n\nArchitecture\n\nrFF + Pooling ISAB(16) + PMA\n\nLL0/data\n\nLL1/data\n\n-2.0006 ± 0.0123 -1.5034 ± 0.0072\n\n-1.6186 ± 0.0042 -1.4908 ± 0.0044\n\nISAB(16) + PMA + sampled attention (ours)\n\n-1.5663 ± 0.0074\n\n-1.5272 ± 0.0052\n\nclosed-form solution could not be inferred directly by setting the gradient equals to zero. Here we instead use the transformer to infer θ∗(X). Specifically, given the input, the neural network f outputs mixture Gaussian parameters f (X) = {π(X), {μj(x), σj(X)}k j=1} by maximing the log likelihood in Eq. 6 (and replacing all parameters as functions of X).\n\nThe 2D set data X is randomly sampled from a given mixture Gaussian distribution with k = 4. And the number of elements n is randomly sampled from [100, 500]. Namely, when setting the dimension of Gaussian distribution as 2, each sampled point could be viewed as a 2D data point, so the sampled collection is a 2D set dataset.\n\nThe baseline we compared with is the Set transformer (Lee et al., 2019) with two Induced Set Attention Block(ISAB) in the encoder, one Multi-head Attention (PMA) and two Set Attention Block (SAB) in the decoder, as per the official implementation. The inducting points refer to the additional learnable points I ∈ Rm×d proposed in Eq. 9 of (Lee et al., 2019), with d dimension and m number of inducting points. Here we have a mixture usage of points, tokens, and elements to represent a single sampled data point xi.\n\nAs the computation complexity of the inducting points block (ISAB) is O(nm), our sampled attention may be adopted in the ISAB to reduce the computation complexity to O(n). However, as the number of inducting points m (regarded as the query in Lee et al. (2019)) is not equal to the number of input points n (regarded as key and value) (in fact inducting points and points have different physical meanings), our Hamiltonian cycle attention could not be applied directly. In fact, the dense attention matrix in the inducting points layer is m × n rather than n × n. We instead applied a different version of sampled attention by randomly sampling two elements per row in the attention matrix. This is a loose version of sampled attention as no Hamiltonian cycle is constructed. As we can see in Tab. 8, the sampled attention could be a plug-in module to replace the dense attention in the inducting points structure with competitive performance but theoretically less computational complexity.\n\nD TRANSFER LEARNING\n\nWe additionally included the transfer learning as a fine-tuning classification task with respect to the pre-training and fine-tuning framework in Sec. 5.1, to demonstrate that the proposed sampled transformer also has a good transfer ability. The fine-tuning task is implemented on the ScanObjectNN (Uy et al., 2019) dataset with 2902 point clouds from 15 categories. We follow the data pre-processing and fine-tuning setting from Point-BERT (Yu et al., 2022) with the same three variants: OBJ-BG, OBJ-ONLY, and PB-T50-RS. As we can see in Tab. 9, our sampled attention layer achieved a competitive performance in comparison with dense attention while reaching state-of-theart performance.\n\nE UNIVERSAL APPROXIMATOR PROOF\n\nA proof of Corollary 1 follows the steps described in § 3.3. As we only changed the dense/sparse attention to the sampled attention, the steps 1 and 3 in § 3.3 remain the same as Yun et al. (2019; 2020) and found in the §C and F in Yun et al. (2020). Here we need only cover the proof of step 2.\n\nFirst, we have FS(·) is the class of continuous set-to-set function, and F S(·) is the class of piecewise constant set-to-set function.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Transfer learning on the classification task, measured by the Accuracy (%).\n\nMethods\n\nOBJ-BG OBJ-ONLY PB-T50-RS\n\nPointNet (Qi et al., 2017a) SpiderCNN (Xu et al., 2018) PointNet++ (Qi et al., 2017b) PointCNN (Li et al., 2018) DGCNN(Wang et al., 2021) BGA-DGCNN (Uy et al., 2019) BGA-PN++ (Uy et al., 2019) Point-BERT (Yu et al., 2022) Point-MAE (Pang et al., 2022)\n\nMAE-dense (ours) MAE-sampled (ours)\n\n73.3 77.1 82.3 86.1 82.8 -\n- 87.43 90.02\n\n90.36 89.68\n\n79.2 79.5 84.3 85.5 86.2 -\n- 88.12 88.29\n\n88.50 88.81\n\n68.0 73.7 77.9 78.5 78.1 79.7 80.2 83.07 85.18\n\n83.41 82.44\n\nLemma 2 (Modified Universal Approximation.). For each f ∈ F S(δ) and 1 ≤ q < ∞, ∃g ∈ T such that f (X) = g(X) for all X ∈ D.\n\n2,1,1\n\nWithout loss of generality, here D ∈ [0, 1)d×n. As in (Yun et al., 2019; 2020) The proof of Lemma 2 could then be separated into four steps:\n\n1. Use the positional embedding E in § 3.2 such that each column of the input Xk + Ek are\n\nin disjoint intervals.\n\n2. The input X +E is quantized into L with values in {0, δ, . . . , n−δ} by a series of modified\n\nfeed-forward layers.\n\n3. The contextual mapping q defined in Definition 3.1 is implemented by a series of modified sampled multi-head self-attention layers (modified version of Eq. 5c) with the input of L .\n\n4. Another series of modified feed-forward layers implements the value mapping such that\n\neach element in the unique id q(L) is mapped to the desired output AX .\n\nAs modified feed-forward layers are all the same as in Yun et al. (2020), the definition and proof of step 2 is available in §D.2 and E.1 in (Yun et al., 2020), while the definition and proof of step 4 could be found in the §D.4 and E.3 in (Yun et al., 2020). Here we mainly explain steps 1 and 3.\n\nE.1 POSITIONAL EMBEDDING\n\nThe positional input for point sets in its xyz coordinate P ∈ R3×n. We adopted a matrix Wp ∈ Rd×3 (a permutation invariant operation) such that the input of the sampled transformer will be X + E = X + WpP . And there exists a case such that:\n\nE1 = (n − 1)1n, and E = (i − 2)1n, for i ∈ [2 : n].\n\n(7)\n\nIn this case, the first column will be (X + E)1 ∈ [n − 1, n)d, and (X + E)i ∈ [i − 2, i − 1)d for i ∈ [2 : n]. So the requirement of step 1 is satisfied, that each column lies in disjoint intervals.\n\nE.2 CONTEXTUAL MAPPING FOR STACKED MULTI-HEADS SELF-ATTENTION LAYERS\n\nAfter the step 2, the quantized input L will be in the set Hδ ⊂ Rd×n, such that:\n\nHδ := {G + E ∈ Rd×n|G ∈ Gδ},\n\n(8)\n\nwith Gδ := {0, δ, . . . , 1 − δ}. Then the adaptive selective shift operation Ψ is defined so that the learnable parameter uT ∈ Rd could map uT Ψ(L) into unique scalars (ids). Finally, with the help of the all-max-shift operation Ω, the output of a series of those two operations will be a scalar in disjoint intervals w.r.t each column of L, as well as different inputs L and L′, thereby implementing the contextual mapping in Definition. 3.1.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAdaptive Selective Shift Operation. With a 2 heads and 1 hidden layer width modified multiheads attention layer, the adaptive selective shift operation Ψ(·) may be defined as:\n\nΨl(Ll; c, bQ, b′\n\nQ) : = Ll + c[11\n\nns\n\n− 11 ns\n\n]\n\n(cid:21) (cid:20)ψl(Ll; bQ) ψl(Ll; b′ Q)\n\nψl(Ll; bQ)k = uT LAl\n\nk\n\n(cid:104)\n\nσH\n\n(uT LAl\n\nk\n\n)T (uT Ll\n\n(cid:105) k − bQ)\n\n=\n\n(cid:40)\n\nmaxj∈Al minj∈Al\n\nk\n\nk\n\nuT Ll j\nuT Ll j\n\nif uT Ll if uT Ll\n\nk > bQ k < bQ,\n\n(9a)\n\n(9b)\n\nwhere we assign query, key, and value parameters as uT , and we introduced the superscript l to denote different attention layers of self-attention layer l. With the help of hardmax, the k-th row of the attention matrix will be one-hot vectors to select the max or min vector in Al k. WO = ] ∈ Rns×2 is used to make sure only the first element in feature dimension are changed c[11 ns in selective shift operation. Specifically, the 1, k-entity of the self-attention output reads:\n\n− 11 ns\n\nΨl(Ll; c, bQ, b′\n\nl\n\n1,k + c (cid:0)ψl(Ll; bQ)k − ψl(Ll; b′\n\nQ)k\n\n(cid:1)\n\n(10)\n\nQ)1,k = L \n\n\n(cid:16)\n\n1,k + c\n\nl L\nl L\n1,k\n\nmaxj∈Al\n\nk\n\nuT Ll\n\nj − minj∈Al\n\nk\n\n(cid:17)\n\nuT Ll j\n\nif bQ < uT Ll\n\nk < b′\n\nQ,\n\nif uT Ll\n\nk /∈ [bQ, b′\n\nQ].\n\n=\n\n\n\n(11)\n\nWithout loss of generality, the sampled transformer in §. 4.3 may be viewed as a series of stacked masked attention Ai for i ∈ [n], such that:\n\nAi\n\nk=1+(i−2+n mod n) = {i, 1 + (i − 2 + n mod n)}\n\nAi Ai\n\nk=i = {i} k̸⊂{i,i−1 mod n} = {},\n\n(12a)\n\n(12b)\n\n(12c)\n\nfor k ∈ [n]. This is in fact the n point pairs in the Hamiltonian cycle. So the stack of all the masked attention is the cycle attention in Fig. 1d reflected across the diagonal line. Then the Eq. 5d will be SAttn(L) = gn(LAn) ◦ gn−1(LAn−1) ◦ · · · ◦ g1(LA1 ), noting that the updated column for previous gi will be applied to the next gi+1. In conclusion, the contextual mapping holds as the masked attention Ai is designed to aggregate information from all n elements / tokens by applying the g(·) about O(n) times, which matches the design of (Yun et al., 2020).\n\n(13)\n\nNow consider uT = (1, δ−1, δ−2, . . . , δ−d+1), the mapping li = ps(Li) = uT Li is bijective as all input point features Li are different with at least one element having a gap of δ. In addition, without loss of generality, the order l2 < l3 < . . . < ln < l1 holds as in (Yun et al., 2020) because of the positional embedding E. Further, as each li has δ−d intervals, and as the n tokens are disjoint with each other, we need nδ−d adaptive selective operations to achieve the bijective mapping of unique ids.\n\nFirst δ−d selective shift operations. The first δ−d layers are all applied to the second column (token) within l2 ∈ (cid:2)0 : δ : δ−d+1 − δ(cid:3), and each selective shift operation will match one interval within bQ = b − δ 1 = {1}, 2 = {1, 2}, and is empty otherwise. So all δ−d layers are only applied on the first two token A2 embeddings, then the maximum value is l1 and the minimum value is l2. We have the output after those selective shift operations:\n\n2 for b ∈ (cid:2)0 : δ : δ−d+1 − δ(cid:3). Also A2 is in fact A2\n\nQ = b + δ\n\n2 , b′\n\n ̃l2 = l2 + δ−d(max\n\nj∈A1 2\n\nlj − min j∈A1 2\n\nlj) = l2 + δ−d(l1 − l2),\n\n(14)\n\nwhere with constant value c = δ−d in Eq. 9b. Note that ̃l2 > l1 because\n\nl2 + δ−d(l1 − l2) > l1 ⇔ (δ−d − 1)(l1 − l2) > 0, (15) which is true. So the current order becomes l3 < l4 < . . . < ln < l1 < ̃l2. So in the next δ−d selective shift operations, the maximum value will be ̃l2 and the minimum will be l3.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nSecond δ−d selective shift operations. The next δ−d layers will be applied on the third column (token embedding) within intervals l3 ∈\n\nwhich results in\n\ni=0 δ−i : δ : (cid:80)d−1\n\ni=0 δ−i + δ−d+1 − δ\n\n(cid:104)(cid:80)d−1\n\n(cid:105)\n\n ̃l3 = l3 + δ−d( ̃l2 − l3) = l3 + δ−d(l2 − l3) + δ−2d(l1 − l2),\n\nwhich is again ̃l3 > ̃l2 because\n\nl3 + δ−d( ̃l2 − l3) > ̃l2 ⇔ (δ−d − 1)( ̃l2 − l3) > 0.\n\nSo we have a new maximum ̃l3 and new minimum l4.\n\n(16)\n\n(17)\n\nRepeat after (n − 1)δ−d operations. The next δ−d will operate on the fourth column. After all (n − 1)δ−d operations we have\n\n(n − 1)\n\nd−1 (cid:88)\n\ni=0\n\nδ−i ≤ l1 < ̃l2 < . . . < ̃ln.\n\nFor j-th column, we will have the output\n\n ̃l1 = l1, ̃l2 = l2 + δ−d(l1 − l2),\n\n ̃lj = lj +\n\nj−2 (cid:88)\n\nk=1\n\nδ−kd(lj−k − lj−k+1) + δ−(j−1)d(l1 − l2).\n\nAnd we also know the interval of each li\n\nl1 ∈ [(n − 1)∆ : δ : (n − 1)∆ + δ−d+1 − δ] li ∈ [(i − 2)∆ : δ : (i − 2)∆ + δ−d+1 − δ],\n\n(18)\n\n(19a)\n\n(19b)\n\n(19c)\n\n(20)\n\n(21)\n\nwith δ−d+1 − δ < ∆ := (cid:80)d−1\n\ni=0 δ−i = δ−d−1\n\nδ−1−1 ≤ δ−d − 1 ⇒ 0 < δ ≤ 1\n\n2 . So we have\n\nl1 − l2 ∈ [(n − 1)∆ − δ−d+1 + δ : δ : (n − 1)∆ + δ−d+1 − δ] li − li+1 ∈ [−∆ − δ−d+1 + δ : δ : −∆ + δ−d+1 − δ] for i ∈ {2, 3, . . . , n − 1}.\n\n(22)\n\n(23)\n\nThen the interval of outputs are\n\n ̃l1 ∈ [(n − 1)∆, (n − 1)∆ + δ−d+1 − δ] ̃l2 ∈ [(n − 1)∆δ−d − δ−2d+1 + δ−d+1, (n − 1)∆δ−d + δ−2d+1 − δ]\n\n(24)\n\n(25)\n\n ̃li ∈ [(i − 2)∆ −\n\ni−2 (cid:88)\n\nk=1\n\nδ−kd∆ −\n\ni−2 (cid:88)\n\nk=1\n\nδ−kd(δ−d+1 − δ) + δ−(i−1)d(n − 1)∆ − δ−(i−1)d(δ−d+1 − δ),\n\n(i − 2)∆ + δ−d+1 − δ −\n\ni−2 (cid:88)\n\nk=1\n\nδ−kd∆\n\n+\n\ni−2 (cid:88)\n\nk=1\n\nδ−kd(δ−d+1 − δ) + δ−(i−1)d(n − 1)∆ + δ−(i−1)d(δ−d+1 − δ)],\n\n(26)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nand to check whether intervals are disjoint or not, we take the difference between the lower bound of ̃li+1 and the upper bound of ̃li\n\n(27)\n\n(28)\n\n(29)\n\ni+1 − ̃lu ̃ll\n\ni = ∆ − δ−(i−1)d∆ + (δ−id − δ−(i−1)d)(n − 1)∆ − (δ−d+1 − δ)\n\n− δ−(i−1)d(δ−d+1 − δ) − 2\n\ni−2 (cid:88)\n\nk=1\n\nδ−kd(δ−d+1 − δ)\n\n=\n\n− δ−id(δ−d+1 − δ) − δ−(i−1)d(δ−d+1 − δ) 1 − nδ−(i−1)d + (n − 1)δ−id(cid:105) (cid:104) (cid:18) 1 + δ−d\n\n∆\n\n−\n\n1 − δ−d −\n\n≥\n\n(cid:20) 2δ−d\n\nδ−d − 1\n\n−\n\n2δ−d 1 − δ−d δ−(i−2)d + 2δ−(i−1)d + δ−id 2δ−d δ−d − 1\n\nδ−(i−2)d − (n + 2)δ−(i−1)d + (n − 2)δ−id\n\n(cid:19)\n\n(δ−d+1 − δ)\n\n(30)\n\n(cid:21)\n\n(δ−d+1 − δ)\n\n≥ δ−(i−2)d\n\n(cid:20)\n\n−\n\n2δ−d δ−d − 1\n\n− (n + 2)δ−d + (n − 2)δ−2d\n\n(cid:21)\n\n(δ−d+1 − δ)\n\n≥ δ−(i−2)d (cid:2)−4 − (n + 2)δ−d + (n − 2)δ−2d(cid:3) (δ−d+1 − δ),\n\n(31)\n\n(32)\n\n(33)\n\nwhich is not guaranteed to be above 0, so the addition operations should be introduced.\n\nFurther, the adaptive shift operation is a one-to-one map as the map Lk (cid:55)→ uT Lk is one-to-one, and the permutation of columns is one-to-one, and so it sufficies to prove that the map [l1 · · · ln] (cid:55)→ ̃lk is also one-to-one. See the detailed analysis in §E.2.3 in (Yun et al., 2020).\n\nPreliminaries. As in (Yun et al., 2020), the upper bound for the unique id ̃li is:\n\n ̃li := li +\n\ni−2 (cid:88)\n\nj=1\n\n≤ li + δ−d\n\nδ−jd(li−j − li+1−j) + δ−(i−1)d(l1 − l2)\n\ni−2 (cid:88)\n\nj=1\n\n(li−j − li+1−j) + δ−(i−1)d(l1 − l2)\n\n= li + δ−d(l2 − li) + δ−(i−1)d(l1 − l2) = δ−(i−1)dl1 − (δ−(i−1)d − δ−d)l2 − (δ−d − 1)li ≤ δ−(i−1)dl1 ≤ δ−(i−1)d (cid:0)(n − 1)∆ + δ−d+1 − δ(cid:1) ≤ δ−(i−1)d(i − 1 + δ)(δ−d − 1) ≤ nδ−id − δ.\n\nSimilarly, we have\n\nAlso, for any n ≥ 1, we have\n\nln ≤ nδ−nd − δ.\n\n(cid:19)\n\n(cid:18) 2n + 1 2n\n\n≤\n\n(cid:18) 2n + 1 2n\n\n(cid:19)2\n\n≤ · · · ≤\n\n(cid:19)n\n\n(cid:18) 2n + 1 2n\n\n≤ 2\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\nAll-max-shift operations. Following (Yun et al., 2020), to make the interval between lk are disjoint with each other, the all-max-shift operation Ωl : Rd×n → Rd×n is a self-attention layer defined as follows:\n\nThe (1, k)-th entry of Ωl(Z; c) reads\n\nΩl(L; c) = L + ce(1)ψl(L; 0).\n\nΩl(L; c)1,k = L1,k + cψl(L; 0)k = L1,k + c max j∈Al k\n\nuT Lj.\n\n(39)\n\n(40)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nThe main idea of all-max-shift operation is that, in the i-th layer, we will ’replace’ the current ’column’ by the maximum column within reach of sparse attention pattern Ai. In the next layer, the shifted max column will again be ’replaced’ by the new maximum value within reach of the shifted column. After n steps or layers, all the first elements of each column will be replaced by the one in the maximum column, which is the dominated value. The steps within the dominated element are greater than the intervals of the whole ln. So, for two different inputs L, they n entries are distinct, and the requirement 2 in Definition 3.1 satisfied.\n\nWithout loss of generality, in contrast with the case of the cycle attention Eq. 12 in the adaptive selective operation, the case of the stacked sampled attention is the same as in Fig. 1d, with l = 1.\n\nFirst layer of all-max-shift. The input of the first all-max-shift operation is ̃L ∈ Rd×n. Recall that uT ̃L = [l1, ̃l2, ̃l3, . . . , ̃ln] and each element is 0 < l1 < ̃l2 < ̃l3 < . . . < ̃ln < nδ−nd − δ. The last inequality holds as in Eq. 36. Let the output of the first layers be M 1. The k-th element in the first row reads\n\nM 1\n\n1,k := ̃L1,k + 2n2δ−nd−1 max\n\nj∈A1 k\n\nuT ̃Lj = ̃L1,k + 2n2δ−nd−1uT ̃Lk+1 mod n,\n\n(41)\n\nwhere with constant value c = 2n2δ−nd−1 in Eq. 40, and for each column we will have\n\nuT M 1\n\nk = uT ̃Lk + 2n2δ−nd−1uT ̃Lk+1 mod n,\n\n(42)\n\nas the first element of u is 1. Next, we see that uT M 1 2n2δ−nd−1uT ̃Lk+1 mod n, which is defined by for any k, k′ ∈ [n],\n\nk is dominated by the right term\n\nuT ̃Lk+1 mod n < uT ̃Lk′+1 mod n ⇒ uT Mk < uT Mk′.\n\nThis is because the minimum gap between uT ̃Lk+1 is δ, and we have\n\nuT ̃Lk < nδ−nd < 2n2δ−nd−1 · δ,\n\n(43)\n\n(44)\n\nso if we have uT ̃Lk+1 mod n < uT ̃Lk′+1 mod n, it could determine the order uT Mk < uT Mk′, because uT ̃Lk is within the minimum gap of the right term of Eq. 42, and so cannot change the overall value.\n\nSecond layer of all-max-shift. As in the first layer, we define the output of this layer as M 2, and the k-th element in the first row reads\n\nM 2\n\n1,k := M 1\n\n1,k + 2n2δ−nd−1 max\n\nj∈A2 k\n\nuT M 2\n\nj = M 1\n\n1,k + 2n2δ−nd−1uT M 2\n\nk+1 mod n,\n\n(45)\n\nso for each column, we have\n\nuT M 2\n\nk = uT M 1\n\nk + 2n2δ−nd−1uT M 2 = uT ̃Hk + 2n2δ−nd−1uT ̃Hk+1 mod n\n\nk+1 mod n\n\n+ 2n2δ−nd−1(uT ̃Hk+1 mod n + 2n2δ−nd−1uT ̃Hk+2 mod n)\n\n= uT ̃Hk + 4n2δ−nd−1uT ̃Hk+1 mod n + (2n2δ−nd−1)2uT ̃Hk+2 mod n.\n\n(46)\n\nThe last term domains uT M 2\n\nk , because the minimum gap of uT M 2\n\nk+1 mod n is at least δ, and\n\nk − (2n2δ−nd−1)2uT ̃Hk+2 mod n = uT ̃Hk + 4n2δ−nd−1uT ̃Hk+1 mod n\n\nuT M 2 < (1 + 4n2δ−nd−1)nδ−nd ≤ (1 + 4n)n2δ−2nd−1 ≤ (2n2δ−nd−1)2 · δ.\n\nThe last inequality holds due to\n\nfrom Eq. 38.\n\n(cid:19)2\n\n(cid:18) 1 + 2n 2n\n\n≤ 2 ⇔ 1 + 4n ≤ 4n2,\n\n22\n\n(47)\n\n(48)\n\nUnder review as a conference paper at ICLR 2023\n\nRepeat all-max-shifts. After all n layers we get M n, and uT M n\n\nk is dominated by\n\n(2n2δ−nd−1)n max j∈An k\n\nuT ̃Hj = (2n2δ−nd−1)n ̃ln.\n\nBecause the remains in uT M n\n\nk have strictly upper-bound\n\nuT M n\n\nk − (2n2δ−nd−1)n ̃ln <\n\n≤\n\n(cid:32)n−1 (cid:88)\n\ni=0 (cid:32)n−1 (cid:88)\n\ni=0\n\n(cid:19)\n\n(cid:18)n i\n\n(cid:19)\n\n(cid:18)n i\n\n(cid:33)\n\n(2n2δ−nd−1)i\n\nnδ−nd\n\n(cid:33)\n\n(2n)i\n\n(nδ−nd−1)n−1nδ−nd\n\n(49)\n\n(50)\n\n(51)\n\nThe last inequality used (1 + 2n)n − (2n)n ≤ (2n)n from Eq. 38.\n\n= ((1 + 2n)n − (2n)n) (nδ−nd−1)n · δ ≤ (2n2δ−nd−1)n · δ.\n\n(52)\n\nVerifying Contextual Mapping. This matches the analysis in §E.2.5 of (Yun et al., 2020). As all u selective-shift operations and all-max operations are bijective, and u map each column (token) of the input to the unique id, the requirement 1 in the Definition 3.1 holds. As uT M n k are all dominated by (2n2δ−nd−1) ̃ln, and different inputs L have different ̃ln as ̃ln is influenced by all [l1, l2, . . . , ln], not all columns are the same for different inputs L, and uT is the unique mapping. The interval may be written\n\nuT M n\n\nk ∈ [(2n2δ−nd−1)n ̃ln, (2n2δ−nd−1)n( ̃ln + δ)].\n\n(53)\n\nThe upper bound holds as other terms are less than (2n2δ−nd−1)n · δ in total (not the dominated term). So as we can see the interval for all uT M n k are disjoint for different inputs, and the requirement 2 in the Definition 3.1 holds.\n\n23",
    "reference": "# Summary Of The Paper\n\nThis work proposes a novel transformer model for dealing with point set data, with the main focus on achieving linear time complexity in the attention mechanism. By utilizing the idea of random sampling-based approximation to the vanilla attention from sampled Hamiltonian cycle attention, the authors show that the proposed model can be a universal function approximator of set-to-set functions. Experiments on point cloud data are studied with comparison to several baseline methods, showing that the proposed model can achieve comparable performance with better efficiency.\n\n# Strength And Weaknesses\n\n- This paper is well written and easy to follow. The motivation is clear and the overall structure is well organized.\n- It seems like the theoretical discussion on the universal approximation property is heavily based on the work of [1,2], while the authors also briefly touched on the role of contextual map. However, the definition 3.1 and the paragraph followed do not seem very self-explanatory to me and it's in fact a bit confusing about what are the novel part exactly in the proof. I wonder if the authors can provide more details on this part.\n- One major contribution of this work focuses on achieving linear time complexity on set data. However, the experiments are only conducted on point cloud data, which is a special case of set data. I wonder if the authors can provide more experiments on other set data, such as graph data, to show the generalization of the proposed model. Also, there exists a lot of works on efficient attention in the past few years, but in recent benchmarks and scaling studies, the vanilla transformer is still the best performing one. In a similar spirit, how do the authors think about the contributions of the proposed model in this context?\n\n[1] Yun, Chulhee, et al. \"Are transformers universal approximators of sequence-to-sequence functions?.\" arXiv preprint arXiv:1912.10077 (2019).\n[2] Yun, Chulhee, et al. \"O (n) connections are expressive enough: Universal approximability of sparse transformers.\" Advances in Neural Information Processing Systems 33 (2020): 13783-13794.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease see above comments.\n\n# Summary Of The Review\n\nI believe the problem studied in this work is interesting and this work made some contributions.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTARGETED HYPERPARAMETER OPTIMIZATION WITH LEXICOGRAPHIC PREFERENCES OVER MULTIPLE OBJECTIVES\n\nShaokun Zhang1, Feiran Jia1, Chi Wang2, Qingyun Wu1 1 Pennsylvania State University, State College, PA, USA 2 Microsoft Research, Redmond, Washington, USA {shaokun.zhang, feiran.jia@psu.edu, qingyun.wu}@psu.edu, wang.chi@microsoft.com\n\nABSTRACT\n\nMotivated by various practical applications, we propose a novel and general formulation of targeted multi-objective hyperparameter optimization. Our formulation allows a clear specification of an automatable optimization goal using lexicographic preference over multiple objectives. We then propose a randomized directed search method named LexiFlow to solve this problem. We demonstrate the strong empirical performance of the proposed algorithm in multiple hyperparameter optimization tasks.\n\n1\n\nINTRODUCTION\n\nHyperparameter optimization (HPO) of machine learning models, as a core component of AutoML, is a process of finding a good choice of hyperparameter configuration that optimizes the model “performance”. In the context of practical ML systems, there are typically more than one metrics to evaluate the model “performance” on which one desires to optimize. For instance, latency (He et al., 2018), fairness (Brookhouse & Freitas, 2022), and explainability (Gonzalez et al., 2021) are important complementary metrics of interest in addition to prediction accuracy in many application scenarios. Typical multi-objective HPO (MO-HPO) approaches (Knowles, 2006; Daulton et al., 2020) seek to find wide-spread Pareto frontiers for users to choose from. This type of method can only establish a partial ordering of the configurations. The final choice on which Pareto frontier to use is typically done manually and is opaque to the optimization algorithm. We call such optimization “untargeted”. An automated approach is desirable, especially in repetitive tuning scenarios such as continuous integration and delivery (CI/CD) of machine learning models or MLOps in general (Garg et al., 2021; M ̈akinen et al., 2021; Symeonidis et al., 2022). This automation is possible if the criteria for selecting the final choice is specified explicitly. In this scenario, untargeted HPO can be inefficient as the optimization algorithm may waste resources on finding Pareto frontiers that are far from the desired final choice, i.e., the target.\n\nIn this work, we consider a targeted HPO scenario: practitioners have a priority order over the objectives, which enables a total ordering of all the configurations. We formalize a general notion of priority order rigorously as a lexicographic preference (Fishburn, 1975) over multiple objectives in an HPO task. It allows users to specify a clear optimization target across multiple objectives before the optimization starts and removes the need for manual post hoc selection. Such a priority structure is found in HPO tasks from various application domains. For example, in many bioinformatics applications, besides the primary objective of finding model hyperparameter configurations with low prediction error, minimizing feature numbers via a feature selection step is found to be helpful in avoiding overfitting and discovering relevant features for domain experts and thus is suggested to be used as an auxiliary objective in HPO (Bommert et al., 2017; Gonzalez et al., 2021). When both objectives are included, the auxiliary objective is considered less important than the minimization of the prediction error, which naturally forms a lexicographic structure.\n\nDespite its appealing practical importance, we find this type of targeted HPO problem remarkably under-explored. In this work, we first provide a rigorous problem formulation for the targeted HPO\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Results in an XGBoost tuning task to find accurate and fair models, in which validation loss is specified as an objective of a higher priority, and DSP (fairness-related objective) of a lower priority. Lexi-Target∗ = l∗ +0.05, in which l∗ is the optimal loss value which is unknown before the optimization starts and 0.05 is the user-specified tolerance onloss degradation. The circles represent proposed configurations from different methods in the objective space (darker color indicates later iteration). Both objectives are smaller the better. SO-HPO easily achieves the target loss value but performs poorly regarding fairness. MO-HPO is able to achieve a better performance in terms of fairness compared to SO-HPO. However, it also wastes resources on finding points outside the desired loss target as it seeks a wide spread of the Pareto front. Compared to MO-HPO, a larger fraction of the configurations in LexiFlow are within the loss target. This allows LexiFlow to try more configurations within the desired loss range and achieves better fairness performance.\n\ntask with lexicographic preference over multiple objectives. This formulation provides a general and flexible way for the users to specify customized targets expressed via a priority order on the objectives and a list of optional goals and tolerances on the objectives. Based on the problem formulation, we propose an algorithm named LexiFlow as a general solution. Specifically, LexiFlow conducts the optimization by leveraging pairwise comparisons between hyperparameter configurations in a randomized direct search framework. The pairwise comparisons are supported by a suite of targeted lexicographic relations, which allow us to navigate toward the more promising region of the search space considering the lexicographic structure in the objective space. By doing so, the algorithm is able to efficiently optimize the objectives with a strong any-time performance.\n\nWe perform extensive empirical evaluation on four different machine learning model tuning tasks, including a tuning task to find accurate and fast/small neural network models, a tuning task to find accurate and fair Xgboost models, a tuning task on random forest combined with feature selection for gene expression prediction, and a tuning task to mitigate overfitting. Our method has promising performance on all the evaluated tasks. The good empirical performance verified the unique advantages of our proposed algorithm. We demonstrate different performance patterns of methods including our method LexiFlow, a single objective method (SO-HPO), and a multiple objective method (MO-HPO) in Figure 1.\n\n1.1 RELATED WORK\n\nThere are a number of works trying to address MO-HPO tasks on machine learning models, including evolutionary algorithms (Deb et al., 2002; Srinivas & Deb, 1994; Zhang & Li, 2007; Binder et al., 2020), Bayesian optimization (Knowles, 2006; Daulton et al., 2020; Emmerich & Klinkenberg, 2008; Hern ́andez-Lobato et al., 2016) and multi-fidelity methods (Schmucker et al., 2020; 2021). All the aforementioned methods treat all the objectives equally important and seek an approximation of the Pareto front in the objectives space. Some recent work proposes to incorporate different types of user preferences into MO-HPO. Paria et al. (2020) allow users to encode their preferences as a prior on a weight vector to scalarize multiple objectives. The prior will induce a posterior distribution over the set of Pareto optimal values. Setting a proper prior in practice is non-trivial as the relation between the prior and posterior is difficult, if not impossible, to derive for an average practitioner. Abdolshah et al. (2019) regard preferences as the stability of objectives, using a constrained Bayesian optimization method. An earlier multi-objective optimization method (Zitzler et al., 2008) incorporates preference information into the multiple objective evolutionary frameworks by defining relations between different populations. However, this preference information is defined upon different populations rather than individual configurations.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 PROBLEM FORMULATION\n\nIn this section, we first provide a rigorous formulation of the targeted HPO problem with lexicographic preferences over multiple objectives together with a motivating example. We then analyze the challenges in approaching the problem.\n\n2.1 PROBLEM FORMULATION\n\nThroughout the paper, we use [K] to denote the integers from 1 to K. The studied problem needs the following inputs from the user,\n\n• A d-dimensional hyperparameter search space X. Any hyperparameter configuration x ∈ X is\n\nconsidered a feasible configuration.\n\n• A list of K (K > 1) objective functions regarding x ∈ X, denoted by F (x) = [f 1(x), f 2(x), ..., f K(x)]. Specifically, each f k (∀k ∈ [K]) is a real-valued objective function of x ∈ X, and the superscript k in f k indicates the priority order (the smaller the index, the high the priority) of the objective. Without loss of generality, we assume minimization problems regarding all the objectives here and throughout this paper.\n\n• A K-dimensional tolerance vector on the objectives denoted by T = [τ 1, τ 2, ..., τ K]. ∀k ∈ [K], each τ k is a non-negative number representing an optimality tolerance on the k-th objective. It can be intuitively considered as the amount of performance degradation the user is willing to compromise in order to find choices with better performance on the objectives of lower priorities. • C = [c1, c2, ..., cK] a K-dimensional goal vector, in which ck ∈ R denotes a goal value on the\n\nk-th objective desired by the user.\n\nWith the inputs F, T, C specified above, ∀k ∈ [K], we can define a lexi-target value zk target value zk important objectives, i.e., the first (k − 1) objectives. We define X 0\n\n∗ . The lexi- ∗ of the k-th objective is defined recursively based on the lexi-target values of the more\n\n∗ = X and for k ∈ [K],\n\n∗ := max{ck, f k zk\n\n∗ + τ k}, f k\n\n∗ := inf\n\nx∈X k−1\n\n∗\n\nf k(x), X k\n\n∗ := {x ∈ X k−1\n\n∗\n\n|f k(x) ≤ zk\n\n∗ }\n\n(1)\n\n∗ ⊇ X 2\n\n∗ ⊇ ... ⊇ X K\n\nwhere X 1 1, 2, ..., K objectives respectively; and f k objective. Note that the cardinality of {F (x)}x∈XK vectors, we define lexicographic relations as follows:\n\n∗ is a series of nested sets dubbed lexi-frontiers considering the first ∗ is the inferior limit of lexi-frontiers in X k ∗ on the k-th can be larger than one. To compare these F (x)\n\n∗\n\nF (x′) =l F (x) ⇔ f k(x′) = f k(x) ∀k ∈ [K]\n\nF (x′) ≺l F (x) ⇔ ∃k ∈ [K] : f k(x′) < f k(x) ∧ (∀k′ < k, f k F (x′) ⪯l F (x) ⇔ F (x′) ≺l F (x) ∨ F (x′) =l F (x)\n\n′\n\n(x′) = f k\n\n′\n\n(x))\n\n(2)\n\n(3)\n\n(4)\n\nOur aim is to find a lexi-optimal configuration, i.e., any element in X ∗ = {x ∈ X K x, F (x) ⪯l F (x′)}, using minimal cost. We call this problem a Lexi-HPO problem in short. Remark 1 (Lexi-optimal vs Pareto frontiers). The lexi-optimal solutions in X ∗ are always a subset of the Pareto frontiers in the K-dimensional objective space. The relative location of X K ∗ in the Pareto frontiers depends on the lexicographic preferences and the relaxations imposed via tolerance and/or goals. In this way, the users could specify their own ‘target’ of interest in the Pareto frontiers in a straightforward way before the optimization starts.\n\n∗ |∀x′\n\n̸=\n\nRemark 2 (On tolerances and goals). Note that the two inputs tolerances T and goals C are both optional. They are introduced to help impose customized needs on the targeted optimum in flexible ways. When there is no tolerance and no particular goal on a particular objective k, one can just set τ k = 0 and ck = − inf without affecting the nature of the problem.\n\nTo facilitate understanding of the Lexi-HPO problem and better motivate the studied scenario, we include a concrete example for gene expression prediction from the bio-informatics domain below.\n\nExample: In the HPO task for gene expression prediction from (Bommert et al., 2017), three minimization objectives are considered during the HPO process, including the model’s prediction\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nerror, the resulting feature numbers after feature selection, and feature selection instability. The three objectives are in descending priority order. Despite the priority order, when trying to find the optimal choice, it is okay to consider the configurations whose loss does not exceed l∗ + 0.05, in which l∗ denotes the best loss one can achieve by any feasible hyperparameter configuration. In addition, a feature number of 500 is considered good enough for the second objective. Say in this example l∗ = 0.1 and xA, xB, xC, xD are whole Pareto frontiers with performance F (xA) = [0.2, 100, 0.1], F (xB) = [0.1, 600, 0.2], F (xC) = [0.13, 500, 0.2], and F (xD) = [0.1, 300, 0.5].\n\nThis example can be formulated as a Lexi-HPO problem, where the objectives are F = tolerances are T = [0.05, 0, 0], and goals are C = [loss, feature number, instability], [inf, 500, − inf]. Under the Lexi-HPO problem, we have the following conclusions: (1) xC is the optimal choice among the whole frontiers; (2) Combining the first conclusion with Remark 1, we can also conclude that xC is also the lexi-optimal choice in the whole space. This example also shows that the tolerances T and goals C are, in general, intuitive to understand and easy to set by practitioners because they are directly about each single objective value.\n\n2.2 CHALLENGES\n\nWe want to first remind the readers that in the context of HPO of ML models, the objectives are mostly black-box, and it is also expensive to get function evaluations because it involves training and validating an ML model. The black-box nature of this problem makes methods that require analytic forms of the objectives, e.g., gradient-based methods (Gong et al., 2021) inapplicable. We also urge methods that are efficient and cost-frugal because of the expensive function evaluations and practical resource limitations. One potential solution for the Lexi-HPO problem is to use MO-HPO approaches to find Pareto frontiers and add a post-processing step to narrow them down to the targeted optimal points. This type of method can be inefficient because they strive to find a wide spread of Pareto frontiers while the users are actually only interested in a subset of them in a particular region. Another potential solution is to perform single objective HPO or constrained single objective HPO regarding each objective sequentially following the preference order. More specifically, when optimizing the k-th objective, one can impose constraints of the form f i(x) ≤ zi ∗, ∀i ∈ [k − 1] and estimate zk ∗ according to the result. However, one tricky problem in this approach is how to allocate limited resources to the optimization of different objectives. The resource allocation directly affects how reliable the constraints are. For example, if we allocate insufficient resources to the first objective, we will not reach the optimal value on the first objective which loosens the constraints imposed on the first objective when trying to optimize objectives of lower priorities. If we allocate excessive resources to the first objective, the optimization of other objectives can be hindered due to a lack of resources. We include this approach as a baseline in the experiment section.\n\n3 ALGORITHM\n\nWe propose an algorithm named LexiFlow to solve the Lexi-HPO problem. We adopt a randomized direct search framework that could direct the search to the targeted optimum based on lexicographic comparisons over pairs of configurations. During the iterative optimization process, the algorithm automatically adjusts its focus on the different objectives based on their priorities and room for improvement. For example, when there is no room for improvement on the first objective in a particular iteration, the algorithm will direct the search toward the region where the second objective could improve without hurting the optimality of the first objective. Since such adjustment is made at every iteration, the algorithm keeps the chance to improve every objective when the optimality has not been reached. By doing so, the algorithm is able to adaptively allocate optimization resources over multiple objectives in a flexible order while respecting the priority. This strategy can help the algorithm achieve a good any-time performance.\n\nLexiFlow is presented in Alg. 1. It takes as inputs the objectives F , a goal vector C (optionally), and a tolerance vector T (optionally). When C and T are not provided, we just set ck to −∞, and τ k to 0, ∀k ∈ [K]. After an initialization step, which sets an initial hyperparameter configuration x0 and an initial stepsize δinit, the algorithm proceeds as follows: At each iteration i, LexiFlow maintains an ‘incumbent’ point, denoted by xi. The ‘incumbent’ point is the point in whose neighboring area the algorithm samples randomized directions to get new points to try next. More specifically, a new direction u is sampled uniformly at random from a unit sphere (Line 4 in Alg. 1), which leads to\n\n4\n\nPublished as a conference paper at ICLR 2023\n\ntwo newly proposed points xi ± δu (considering both the sampled direction and its opposite direction). After evaluating the newly proposed point(s), i.e., obtaining F (xi + δu) and potentially also F (xi − δu), it decides whether to update the incumbent with one of the newly proposed points, i.e., xi + δu or xi − δu. In LexiFlow, this decision is made via a carefully designed Update function. Following the same spirit of an existing randomized direct search-based single-objective HPO method (Wu et al., 2021), LexiFlow includes the restart and dynamic stepsize techniques to free the algorithm from local optimum and manual configuration of stepsize (line 9-11 of Alg. 1). We also adopt the same setting of the initial stepsize δinit and stepsize lower bound δlower with the aforementioned work. The algorithm terminates if it runs out of resource budget and outputs a lexi-optimal configuration x∗, which is maintained in a way such that it is lexi-optimal among all the evaluated configurations. Following the general principles of direct search (Kolda et al., 2003), the incumbent point shall be maintained in a way such that it is the best or at least a very promising point among the historically evaluated points (within a local search thread without considering restart). This general principle, together with an iterative random sampling of new points around the incumbent, could gradually lead the search to more promising region (Kolda et al., 2003; Wu et al., 2021). The algorithm makes a decision on whether to update the incumbent with a newly proposed point via the Update procedure attached at the end of Alg. 1 (line 12-18).\n\nAlgorithm 1: LexiFlow\n\nInput: Objectives F (·), goals C (optional) and tolerances T (optional).\n\n1 Initialization: Initial configuration x0, i′ = r = s = 0, δ = δinit; 2 Obtain F (x0), and x∗ ← x0, H ← {x0}, ZH ← F (x0) 3 while i = 0, 1, ... do 4\n\nSample u uniformly from unit sphere S if Update(F (xi + δu), F (xi), ZH) then xi+1 ← xi + δu, i′ ← i; else if Update(F (xi − δu), F (xi), ZH) then xi+1 ← xi − δu, i′ ← i ; else xi+1 ← xi, s ← s + 1 ; H ← H ∪ {xi+1}, and update ZH according to Eq. (8) if s = 2d−1 then s ← 0, δ ← δ(cid:112)(i′ + 1)/(i + 1) ; if δ < δlower then\n\nr ← r + 1, xi+1 ← N (x0, I), δ ← δinit + r // Random Restart\n\n12 Procedure Update(F (x′), F (x), ZH):\n\nif F (x′) ≺(ZH) F (x) Or (cid:0)F (x′) =(ZH) F (x) and F (x′) ≺l F (x(cid:1)) then\n\nif F (x′) ≺(ZH) F (x∗) Or (cid:0)F (x′) =(ZH) F (x∗) and F (x′) ≺l F (x∗)(cid:1) then\n\nx∗ ← x′ // Using x∗ to keep track of the lexi-optimal solution\n\nReturn True\n\nelse\n\n// Accept x′\n\nReturn False\n\n// Discard x′\n\n19 Output: A lexi-optimal configuration x∗\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\nUpdate Function. Existing work on randomized direct search (Kolda et al., 2003; Wu et al., 2021), especially the one about cost-frugal HPO (Wu et al., 2021) indicates the following desiderata for doing cost-efficient randomized direct search: (1) Whenever a newly proposed point is better than the incumbent, it should be accepted as the new incumbent. This property can help achieve a good anytime performance of the algorithm and avoids getting stuck into local optimum while maintaining a good convergence rate. (2) The algorithm should not accept a point that is strictly worse than the incumbent. This property can help control the evaluation cost in each individual trial according to theoretical analysis of the cost of a randomized direct search method (Proposition 4 and 5 in (Wu et al., 2021)). Our Update function is designed following these desiderata.\n\nOne seemingly straightforward idea for the Update function is to directly make comparisons between the incumbent and the newly proposed points according to the vanilla lexicographic relations defined in Eq. (2), Eq. (3) and Eq. (4). However, one problem with vanilla lexicographic relations is that they cannot accommodate user-specified tolerances and goals on the objectives. To address this challenge, we introduce the targeted lexicographic relations, which are parameterized by a Kdimensional real-valued target vector Z = [z1, z2, ..., zK] and denoted by =(Z), ≺(Z), ⪯(Z),\n\nF (x′) =(Z) F (x) ⇔ f k(x′) = f k(x) ∨ (f k(x′) ≤ zk ∧ f k(x) ≤ zk) ∀k ∈ [K] (5) F (x′) ≺(Z) F (x) ⇔ ∃k ∈ [K] : f k(x′) < f k(x) ∧ f k(x) > zk ∧ F k−1(x) =(Z) F k−1(x′) (6) F (x′) ⪯(Z) F (x) ⇔ F (x′) ≺(Z) F (x) ∨ F (x′) =(Z) F (x) (7)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nin which F k−1(x) denotes a vector with the first k − 1 dimensions of F (x), i.e., F k−1(x) = [f 1(x), ..., f k−1(x)] (We define F 0(x) = 0, ∀x ∈ X). It is easy to verify that the relation ⪯(Z) is reflexive in X and we prove in Lemma 1 (in Appendix A) that it is also transitive in X. With these properties, it can be verified that when Z is set to the lexi-targets Z∗ = [z1 ∗, ..., zK ∗ ], the targeted lexicographic relations can be used to find points that are in X K ∗ , i.e., the lexi-frontiers, through exhaustive pairwise comparisons of configurations from the search space X.\n\n∗, z2\n\nAnother caveat in this idea is that the comparisons depend on the knowledge of lexi-targets zk ∗\n∀k ∈ [K], which are generally unknown during the optimization process. To address this difficulty, we propose to maintain an online approximate of the lexi-targets based on historical observations. Specifically, we maintain all historically evaluated points in H and introduce the following statistics of H based on Eq. (1): X (0)\n\nH := max{ck, f k zk\n\nf k(x), X k\n\nH := {x ∈ X k−1\n\nH |f k(x) ≤ zk\n\nH}\n\n(8)\n\nH = H and ∀k ∈ [K], H + τ k}, f k\n\nH := min\n\nx∈X k−1\n\nH\n\n∗ , f k\n\nH, f k\n\n2 , ..., zK\n\n∗ , and X k\n\nH, and X k\n\nH can be considered online versions of zk\n\nHere zk ∗ respectively, calculated based on historical observations in H. With the approximated lexi-targets ZH = [z1 H, zH H ], we decide whether to accept a newly proposed point x′ comparing the the current incumbent x according to the Update procedure in Alg. 1. Specifically, we accept x′ as the new incumbent in the following two cases: (1) When F (x′) ≺(ZH) F (x) is true. This condition allows us to leverage the targeted lexicographic relation ≺(ZH) to direct the search toward the approximated lexifrontiers based on the approximated lexi-targets ZH. (2) When F (x′) =(ZH) F (x) and F (x′) ≺l F (x(cid:1). This condition allows us to move toward lexi-optimal points (also approximated) when the proposed point is already an approximated lexi-frontier. The update rules on the one hand respect the lexicographic relations considering the existence of lexi-targets and on the other hand follow the established desiderata. In Alg. 1 we use x∗ to keep track of the lexi-optimal solution and update it whenever needed in a similar way in the Update procedure (line 13-15). Note that when there is no random start, x∗ is simply the current incumbent, i.e., xi at iteration. The extra update steps in line 13-15 are essential considering the potential random restarts.\n\n4 EXPERIMENTS\n\nWe perform evaluations on multiple hyperparameter tuning tasks, which fall into the following two categories of use case scenarios: (1) To directly perform lexicographic optimization in cases where the users do have lexicographic preferences over multiple objectives. (2) To optimize a particular objective that is inaccessible during the optimization, by finding lexi-optimal solutions in terms of a list of proxy objectives. For the first type of use case, we include an efficient NN tuning task, an unfairness mitigation task, and a high-dimensional feature selection task in Section 4.1. For the second type of use case, we include a task about overfitting mitigation in Section 4.2. We tune three types of models including Xgboost, Random Forest and Neural Networks on different classification tasks. In all the experiments, if not otherwise specified, we show the results of the lexi-optimal solutions calculated based on the historical observations in each method at each particular concerned time point. All reported results are averaged over five runs with different random seeds.\n\n4.1 HPO WITH LEXICOGRAPHIC PREFERENCES OVER MULTIPLE OBJECTIVES\n\nSince there are no existing HPO methods that are directly applicable to the Lexi-HPO problem, we include three general HPO methods which could be extended to the Lexi-HPO problem: (1) A single objective HPO algorithm (SO-HPO), named CFO (Wu et al., 2021), which only optimizes the objective of the highest priority. The method CFO is selected as it is a strong SO-HPO method showing superior performance over other alternatives, e.g., BO-based methods, multi-fidelity-based methods as reported in (Wu et al., 2021). (2) A constrained optimization method (C-HPO) as described and constructed in Section 2.2. This method allocates even resources to different objectives. (3) A popular multiple objective HPO algorithm (MO-HPO) qEHVI (Daulton et al., 2020). We choose qEHVI in this category because it is a state-of-the-art MO-HPO method with ready-to-use opensource implementation. We use both CFO and its constrained optimization version implementation from the AutoML library FLAML (Wang et al., 2021), and use the implementation for qEHVI from the HPO library Optuna (Akiba et al., 2019). In all the experiments, when presenting the anytime\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nperformance, we use a horizontal dashed line labeled Lexi-Target∗ to represent the lexi-target calculated based on user inputs and the configurations found by all the methods (unknown before the optimization) according to Eq. (8).\n\n4.1.1 TUNING ACCURATE AND EFFICIENT NEURAL NETWORKS\n\nDeploying neural networks in practical applications typically requires a balance between model accuracy and efficiency. Lexico-HPO can be a good way to find models achieving the desired balance. For example, as mentioned in (He et al., 2018), for AI applications such as Google Photos, model accuracy is more important than latency, and it is desirable to find smaller models under the condition of not sacrificing accuracy. There is a natural lexicographic preference over the two objectives of improving model accuracy and reducing latency. In this experiment, we evaluate our method for the task of tuning accurate and efficient neural networks. More specifically, following a similar search space setting with (Abdolshah et al., 2019; Hern ́andez-Lobato et al., 2016), we tune neural network models on a subsampled Fashion MNIST dataset (Xiao et al., 2017) with two minimization objectives under lexicographic preference. The 1st objective is the error rate. The 2nd objective is efficiency-related objectives including FLOPs and parameter numbers in two different experiments respectively. For both experiments, we provide tolerance vector T = [τ 1 = 1/ S, τ 2 = 0.0], where S represents the number of validation data points since the error metric calculated from validation S. We do not set a specific goal vector data can deviate from the true test error in the scale of 1/ for this task, as mentioned in (He et al., 2018), latency is typically not a hard constraint.\n\n√\n\n√\n\nWe perform tuning for 2 hours with each method and show the anytime performance of the methods regarding the two objectives in Figure 2. All four methods eventually achieve good performance in terms of the objective of the highest priority, i.e., the error rate: all of them are within the calculated lexi-target. LexiFlow, SO-HPO, and C-HPO achieve the target value earlier than MO-HPO because they put more priority on the first objective. In terms of the second objective, LexiFlow shows a significant performance improvement in both two experiments compared to all the baselines. MOHPO finds models with lower FLOPs and lower parameter numbers than SO-HPO and C-HPO. But the FLOPs and parameter numbers in MO-HPO are still 2-4 times larger than that in LexiFlow when the error rate is within the tolerance range.\n\n(a) Finding accurate and fast NN\n\n(b) Finding accurate and small NN\n\nFigure 2: Results from the neural networks tuning task on Fashion-MNIST. Each of the curves represents averaged result of a particular method from 5 runs with different random seeds. The shaded area corresponds to 95% confidence intervals.\n\n4.1.2 TUNING ACCURATE AND FAIR XGBOOST\n\nDue to increasing evidence on fairness-related harms (Angwin et al., 2016; Barocas & Selbst, 2016) in ML, there is an emerging need to build ML models that is not only accurate but also fair. One way to achieve this goal is to optimize the predictive performance of a model (1st objective) while minimizing the unfair bias (2nd objective) under a lexicographic preference in model tuning (Brookhouse & Freitas, 2022). Following the same setting with (Brookhouse & Freitas, 2022), we perform the Xgboost tuning task with two minimization objectives, including validation loss (1st objective) according to (Brookhouse & Freitas, 2022), and DSP (2nd objective), which an unfairness-related metric calculated based on statistical disparity (Perrone et al., 2021). We do the evaluation on datasets including Adult, Compas, and German, which are commonly used datasets for fairness-\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nrelated tasks (Perrone et al., 2021). We set a tolerance of 0.05 on the validation loss according to (Brookhouse & Freitas, 2022), and we do not provide the goal value .\n\nWe perform tuning for 1 hour with each method and show the final performance in Table 1. The results show that LexiFlow can indeed achieve the best performance considering the lexicographic preference over the objectives compared to baselines: LexiFlow has a similar performance with baselines considering optimality tolerance (0.05) in terms of the 1st objective and achieves a better performance in terms of the 2nd objective across all three datasets. Regarding the performance on the 2nd objective, i.e., the DSP value, 0.05 is a commonly used reference value for good DSP (Perrone et al., 2021). LexiFlow reaches 0.016 in Compas and 0.034 in German, where most of the other baselines got larger than 0.05.\n\nTable 1: Results from tuning Xgboost on tasks that involve fairness considerations. All four methods are within the optimality tolerance range in 1st objective. In terms of the 2nd objective, LexiFlow achieves a significant performance improvement compared to baselines.\n\nMethod\n\nCompas\n\nGerman\n\nAdult\n\nDSP\n\nVal Loss\n\nDSP SO-HPO 0.344 (+0.004) 0.067 0.431 (+0.007) 0.064 0.368 (+0.003) 0.082 MO-HPO 0.340 (+0.000) 0.064 0.472 (+0.048) 0.078 0.408 (+0.043) 0.198 C-HPO 0.354 (+0.014) 0.029 0.424 (+0.000) 0.100 0.369 (+0.004) 0.088 LexiFlow 0.345 (+0.005) 0.016 0.434 (+0.010) 0.034 0.365 (+0.000) 0.060\n\nVal Loss\n\nVal loss\n\nDSP\n\n4.1.3 FEATURE SELECTION IN BIOINFORMATICS\n\nFigure 3: The results in tuning Xgboost on biological dataset AP Colon Kidney.\n\nIn many applications of ML in bioinformatics, the primary goal is to find a model with good predictive performance. For high-dimension biological data, due to a large number of features, it is also critical to integrate feature selection into the model fitting process, to make the selected features few and stable (Bommert et al., 2017), which could help provide insights for domain experts.\n\nIn this experiment, we verify LexiFlow in a feature selection task following a similar setting with (Bommert et al., 2017) which implicitly formulates the optimization objectives as lexicographic preference. Specifically, we perform the Xgboost tuning on biological dataset AP colon Kidney with three minimizing objectives including error rate, feature number, and the instability of the selected features (with descending priority order). We set targets C = [c1 = 0, c2 = 500, c3 = 0] and tolerances T = [τ 1 = 0.01, τ 2 = 0.0, τ 3 = 0.0] according to(Bommert et al., 2017) We use the filter feature selection method from Scikit-learn (Pedregosa et al., 2011) for feature selection.\n\nWe perform tuning for 104 seconds and report the anytime performance regarding the three objectives in Figure 3. From the results, we observe that regarding the 1st objective, all four methods are within the optimality tolerance range. In terms of the 2nd objective, only LexiFlow reaches the goal in the end. Moreover, LexiFlow achieves significant performance improvement compared to baselines in the 3rd objective. We also include results from all five random seeds in Appendix B.\n\n4.2 LEXIFLOW AS AN OPTIMIZATION TOOL TO MITIGATE OVERFITTING\n\nAlthough low loss on the final test data is the objective of interest in machine learning, validation loss is commonly used as a proxy metric for test loss since the final test data is inaccessible in\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Test-time error rate (%) achieved by different methods on different datasets.\n\nMethod\n\nGisette\n\nMadelon Ginal prior Ginal agnostic Bioresponse Hiva agnostic\n\nScene\n\nChristine\n\nSO-HPO w/o FS 2.46 ± 0.08\n\n17.1 ± 0.7\n\n4.54 ± 0.3\n\n5.00 ± 0.2\n\n2.15 ± 0.7\n\n2.41 ± 0.2\n\n2.49 ± 0.0\n\n26.4 ± 0.5\n\nSO-HPO w/ FS\n\n2.30 ± 0.2\n\n18.5 ± 1.2\n\n5.01 ± 0.4\n\n4.71 ± 0.1\n\n2.14 ± 0.7\n\n2.39 ± 0.1\n\n2.52 ± 0.07 26.5 ± 0.9\n\nLexiFlow w/ FS\n\n2.08 ± 0.3\n\n16.3 ± 2.5\n\n4.50 ± 0.4\n\n4.45 ± 0.2\n\n2.19 ± 0.6\n\n2.36 ± 0.1\n\n2.49 ± 0.0\n\n26.1 ± 1.0\n\nthe model building and tuning stage. It is well known that there may exist a gap between an ML model’s performance on validation data and test data. Overfitting is a major factor that causes such a performance gap. Evidence from existing work show that optimizing machine learning models with fewer but key features could mitigate overfitting problems (Jovi ́c et al., 2015; Ying, 2019; Abdel-Aal, 2005). These findings inspire us to explore the possibility of incorporating the objective of minimizing feature numbers as an additional proxy objective in ML model tuning. Specifically, we consider the commonly used validation loss as the primary objective and feature number as a secondary proxy objective during HPO. We tune Random Forest (RF) with training and validation data and evaluate the best model’s test loss on a reserved test dataset as the final performance metric.\n\nMore specifically, we include the following methods in comparison: (1) LexiFlow w/ FS, which is LexiFlow with two minimization objectives, including validation loss (1st), and feature number (2nd). (2) SO-HPO w/ FS, which is the single objective HPO algorithm CFO (Wu et al., 2021) with validation loss as its objective. (3) SO-HPO w/o FS, which is the single objective HPO algorithm CFO with validation loss as the objective and without a feature selection process. Both LexiFlow w/ FS and SO-HPO w/ FS include a feature selection step (indicated by w/ FS) using the same feature selection method as that in Section 4.1.3, in addition to RF model hyperparameter tuning. We include eight classification datasets, which are datasets from two previous feature selection studies (Gonzalez et al., 2021; Bommert et al., 2020) satisfying the following two conditions: feature number larger than 100 and available on Openml. We use 0.01 or 0.001 as the tolerance on validation loss according to empirical studies from (Gonzalez et al., 2021): for each dataset, if the mean validation loss from a default RF model (from sklearn) is larger than 0.1, 0.01 is used as the tolerance, otherwise 0.001. No tolerance or target is imposed on the 2nd objective.\n\nWe show the final test result from different methods in Table 2. LexiFlow w/ FS achieves the best performance on most of the datasets (7/8). We also investigate the number of features selected in each method and find that LexiFlow w/ FS achieves the smallest reserved feature ratio (30.1%) compared with SO-HPO w/o FS (100%) and SO-HPO w/ FS (66.8%). There are two important takeaways from this experiment, considering the nature of the compared methods and their performance: (1) The good test performance of SO-HPO w/o FS and LexiFlow w/ FS (over SO-HPO w/o FS) indicates that reducing feature number can indeed help mitigate overfitting and thus improve test performance, which is consistent with findings from existing work mentioned; (2) The dominating performance of LexiFlow w/ FS over SO-HPO w/ FS indicates that leveraging feature number as a secondary objective in addition to validation loss during model tuning is an effective way to further mitigate overfitting. Our method serves as an ideal optimization tool in this important endeavor.\n\n5 CONCLUSION\n\nIn this paper, we propose an HPO algorithm named LexiFlow, which could easily incorporate users’ lexicographic preferences across multiple objectives in HPO tasks. LexiFlow is simple and effective, showing a strong empirical performance over a wide spectrum of tuning tasks and application domains. LexiFlow is of good practical importance especially considering the ubiquitous existence of potentially conflicting objectives in modern machine learning systems, such as accuracy, latency, model size, robustness, and ethics-related objectives. We also made an interesting finding on overtuning certain proxy objectives with a lexicographic structure could help find fitting mitigation: models that are less likely to overfit. The implementation of our method is available in the opensource AutoML library FLAML1.\n\n1Link to the documentation page of LexiFlow in FLMAL: https://microsoft.github.io/ FLAML/docs/Use-Cases/Tune-User-Defined-Function#lexicographic-objectives. code example demonstrating the use of LexiFlow to find accurate and fast neural networks: https: //microsoft.github.io/FLAML/docs/Examples/Tune-Lexicographic-objectives.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRE Abdel-Aal. Gmdh-based feature ranking and selection for improved classification of medical\n\ndata. Journal of Biomedical Informatics, 38:456–468, 2005.\n\nMajid Abdolshah, Alistair Shilton, Santu Rana, Sunil Gupta, and Svetha Venkatesh. Multi-objective bayesian optimisation with preferences over objectives. Advances in Neural Information Processing Systems, 2019.\n\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: In Proceedings of the 25th ACM\n\nA next-generation hyperparameter optimization framework. SIGKDD international conference on knowledge discovery & data mining, 2019.\n\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias there’s software used across the country to predict future criminals. and it’s biased against blacks. ProPublica, 2016.\n\nSolon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671, 2016.\n\nMartin Binder, Julia Moosbauer, Janek Thomas, and Bernd Bischl. Multi-objective hyperparameter In Proceedings of the 2020 Genetic and\n\ntuning and feature selection using filter ensembles. Evolutionary Computation Conference, 2020.\n\nAndrea Bommert, J ̈org Rahnenf ̈uhrer, and Michel Lang. A multicriteria approach to find predictive and sparse models with stable feature selection for high-dimensional data. Computational and mathematical methods in medicine, 2017, 2017.\n\nAndrea Bommert, Xudong Sun, Bernd Bischl, J ̈org Rahnenf ̈uhrer, and Michel Lang. Benchmark for filter methods for feature selection in high-dimensional classification data. Computational Statistics & Data Analysis, 143:106839, 2020.\n\nJames Brookhouse and Alex Freitas. Fair feature selection with a lexicographic multi-objective genetic algorithm. In International Conference on Parallel Problem Solving from Nature, 2022.\n\nSamuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization. Advances in Neural Information Processing Systems, 2020.\n\nKalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6:182–197, 2002.\n\nMichael Emmerich and Jan-willem Klinkenberg. The computation of the expected improvement in dominated hypervolume of pareto front approximations. Rapport technique, Leiden University, 34:7–3, 2008.\n\nPeter C. Fishburn. Axioms for lexicographic preferences. The Review of Economic Studies, 42(3):\n\n415–419, 1975.\n\nSatvik Garg, Pradyumn Pundir, Geetanjali Rathee, PK Gupta, Somya Garg, and Saransh Ahlawat. On continuous integration/continuous delivery for automated deployment of machine learning In IEEE Fourth International Conference on Artificial Intelligence and models using mlops. Knowledge Engineering, 2021.\n\nChengyue Gong, Xingchao Liu, and Qiang Liu. Automatic and harmless regularization with constrained and lexicographic optimization: A dynamic barrier approach. Advances in Neural Information Processing Systems, 2021.\n\nJesus Gonzalez, Julio Ortega, Juan Jose Escobar, and Miguel Damas. A lexicographic cooperative\n\nco-evolutionary approach for feature selection. Neurocomputing, 463:59–76, 2021.\n\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDaniel Hern ́andez-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive entropy search for multi-objective bayesian optimization. In International conference on machine learning, 2016.\n\nAlan Jovi ́c, Karla Brki ́c, and Nikola Bogunovi ́c. A review of feature selection methods with applications. In international convention on information and communication technology, electronics and microelectronics, 2015.\n\nJ. Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10:50–66, 2006.\n\nTamara G Kolda, Robert Michael Lewis, and Virginia Torczon. Optimization by direct search: New\n\nperspectives on some classical and modern methods. SIAM review, 45:385–482, 2003.\n\nSasu M ̈akinen, Henrik Skogstr ̈om, Eero Laaksonen, and Tommi Mikkonen. Who needs mlops: What data scientists seek to accomplish and how can mlops help? In 1st Workshop on AI Engineering - Software Engineering for AI, pp. 109–112, 2021.\n\nBiswajit Paria, Kirthevasan Kandasamy, and Barnab ́as P ́oczos. A flexible framework for multiobjective bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence, pp. 766–776, 2020.\n\nFabian Pedregosa, Ga ̈el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.\n\nValerio Perrone, Michele Donini, Muhammad Bilal Zafar, Robin Schmucker, Krishnaram Kenthapadi, and C ́edric Archambeau. Fair bayesian optimization. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021.\n\nRobin Schmucker, Michele Donini, Valerio Perrone, Muhammad Bilal Zafar, and C ́edric Archambeau. Multi-objective multi-fidelity hyperparameter optimization with application to fairness. In NeurIPS Workshop on Meta-Learning, 2020.\n\nRobin Schmucker, Michele Donini, Muhammad Bilal Zafar, David Salinas, and C ́edric Archambeau. Multi-objective asynchronous successive halving. arXiv preprint arXiv:2106.12639, 2021.\n\nNidamarthi Srinivas and Kalyanmoy Deb. Muiltiobjective optimization using nondominated sorting\n\nin genetic algorithms. Evolutionary computation, 2:221–248, 1994.\n\nGeorgios Symeonidis, Evangelos Nerantzis, Apostolos Kazakis, and George A Papakostas. Mlops–\n\ndefinitions, tools and challenges. arXiv preprint arXiv:2201.00162, 2022.\n\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\n\nlibrary. Proceedings of Machine Learning and Systems, 2021.\n\nQingyun Wu, Chi Wang, and Silu Huang. Frugal optimization for cost-related hyperparameters. In\n\nProceedings of the AAAI Conference on Artificial Intelligence, 2021.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\n\ning machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nXue Ying. An overview of overfitting and its solutions. In Journal of physics: Conference series,\n\nvolume 1168, pp. 022022, 2019.\n\nQingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposi-\n\ntion. IEEE Transactions on evolutionary computation, 11:712–731, 2007.\n\nEckart Zitzler, Lothar Thiele, and Johannes Bader. Spam: Set preference algorithm for multiobjective optimization. In International Conference on Parallel Problem Solving from Nature, 2008.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA PROOF\n\nLemma 1. The binary relation ⪯(Z) defined in Eq. (7) is transitive, i.e., ∀x, y, w ∈ X, F (x) ⪯(Z) F (y) ∧ F (y) ⪯(Z) F (w) ⇒ F (x) ⪯(Z) F (w).\n\nProof of Lemma 1. According to Eq. (7),\n\nF (x) ⪯(Z) F (y) ∧ F (y) ⪯(Z) F (w) ⇒ (cid:0)F (x) ≺(Z) F (y) ∨ F (x) =(Z) F (y)(cid:1) ∧ (cid:0)F (y) ≺(Z) F (w) ∨ F (y) =(Z) F (w)(cid:1) ⇒ (cid:0)F (x) =(Z) F (y) ∧ F (y) =(Z) F (w)(cid:1) (cid:124) (cid:125) (cid:123)(cid:122) i\n∨ (cid:0)F (x) =(Z) F (y) ∧ F (y) ≺(Z) F (w)(cid:1) (cid:125) (cid:123)(cid:122) iii\n\n∨ (cid:0)F (x) ≺(Z) F (y) ∧ F (y) ≺(Z) F (w)(cid:1) (cid:124) (cid:123)(cid:122) (cid:125) ii ∨ (cid:0)F (x) ≺(Z) F (y) ∧ F (y) =(Z) F (w)(cid:1) (cid:125) (cid:123)(cid:122) iv\n\n(cid:124)\n\n(cid:124)\n\nStatement I: F (x) =(Z) F (y) ∧ F (y) =(Z) F (w) ⇒ F (x) =(Z) F (w).\n\nProof of Statement I. From Eq. (5), we have,\n\nF (x) =(Z) F (y) ∧ F (y) =(Z) F (w) ⇒ (cid:16)\n\nf k(x) = f k(y) ∨ (cid:0)f k(x) ≤ zk ∧ f k(y) ≤ zk(cid:1), ∀k ∈ [K] (cid:16)\n\nf k(y) = f k(w) ∨ (cid:0)f k(y) ≤ zk ∧ f k(w) ≤ zk, ∀k ∈ [K](cid:1)(cid:17)\n\n∧\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\n⇒ ∀k ∈ [K] :\n\nf k(x) = f k(y) = f k(w) (cid:16)(cid:0)f k(x) = f k(y)(cid:1) ∧ (cid:0)f k(y) ≤ zk ∧ f k(w) ≤ zk(cid:1)(cid:17) (cid:16)(cid:0)f k(x) ≤ zk ∧ f k(y) ≤ zk(cid:1) ∧ (cid:0)f k(y) = f k(w)(cid:1)(cid:17) f k(x) ≤ zk ∧ f k(y) ≤ zk ∧ f k(w) ≤ zk(cid:17) (cid:16)(cid:0)f k(x) = f k(w)(cid:1) ∨ (cid:0)f k(x) ≤ zk ∧ f k(w) ≤ zk(cid:1), ∀k ∈ [K]\n\n(cid:16)\n\n∨\n\n∨\n\n∨\n\n⇒\n\n(cid:17)\n\n(9)\n\n(10)\n\n⇒ F (x) =(Z) F (w)\n\nStatement II: F (x) ≺(Z) F (y) ∧ F (y) ≺(Z) F (w) ⇒ F (x) ≺(Z) F (w)\n\nProof of Statement II. According to Eq. (6), we have\n\nF (x) ≺(Z) F (y) ⇔ ∃k1 ∈ [K] : f k1(x) < f k1(y) ∧ f k1 (y) > zk1 ∧ F k1−1(x) =(Z) F k1−1(y) F (y) ≺(Z) F (w) ⇔ ∃k2 ∈ [K] : f k2(y) < f k2(w) ∧ f k2 (w) > zk2 ∧ F k2−1(y) =(Z) F k2−1(w)\n\n(11)\n\nLet k′ = min{k1, k2}, according to Statement I and Eq. (11), we have:\n\n(cid:16)\n\nF k′−1(x) =(Z) F k′−1(y)\n\n(cid:17)\n\n(cid:16)\n\n∧\n\nF k′−1(y) =(Z) F k′−1(w)\n\n(cid:17)\n\n⇒ F k′−1(x) =(Z) F k′−1(w)\n\nAccording to Eq. (12), if k′ = k1 < k2, we have\n\n(cid:16)\n\nf k′\n\n(x) < f k′\n\n(y) = f k′\n\n(w)\n\n(cid:16)\n\n(cid:17)\n\n∧\n\nzk′\n\n< f k′\n\n(y) = f k′\n\n(w)\n\n(cid:17)\n\nIf k′ = k2 < k1, we have (cid:16)\n\nf k′\n\n(x) = f k′\n\n(y) < f k′\n\n(w)\n\n(cid:17)\n\n(cid:16)\n\n∧\n\nzk′\n\n< f k′\n\n(cid:17)\n\n(w)\n\n(12)\n\n(13)\n\n(14)\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nOtherwise when k′ = k1 = k2, we have (cid:16)\n\nf k′\n\n(x) < f k′\n\n(y) < f k′\n\n(cid:17)\n\n(w)\n\n(cid:16)\n\n∧\n\nzk′\n\n< f k′\n\n(y) < f k′\n\n(cid:17)\n\n(w)\n\n(15)\n\nCombing Eq. (12), Eq. (13), Eq. (14) and Eq. (15), we have found existing k = k′ such that (cid:0)f k(x) < f k(w)(cid:1) ∧ (cid:0)f k(w) > zk(cid:1) ∧ (cid:0)F k−1(x) =(Z) F k−1(w)(cid:1).\n\nStatement III: F (x) =(Z) F (y) ∧ F (y) ≺(Z) F (w) ⇒ F (x) ≺(Z) F (w).\n\nProof of Statement III. According to the definitions of =(Z) and ≺(Z) in Eq. (5) and Eq. (6),\n\n(cid:16)\n\n(cid:16)\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\nF (x) =(Z) F (y) ∀k1 ∈ [K] : (cid:0)f k1(x) = f k1 (y)(cid:1) ∨ (cid:0)f k1 (x) ≤ zk1 ∧ f k1 (y) ≤ zk1 (cid:1)(cid:17)\n\nF (y) ≺(Z) F (w)\n\n⇒\n\n∧\n\n(cid:16)\n\n∧\n\n∃k2 ∈ [K] : f k2(y) < f k2(w) ∧ f k2(w) > zk2 ∧ F k2−1(y) =(Z) F k2−1(w)\n\n(cid:17)\n\nThen, we find a k = k2, such that,\n\n(cid:16)(cid:0)f k(x) = f k(y) < f k(w)(cid:1) ∨ (cid:0)f k(x) ≤ zk < f k(w)(cid:1)(cid:17)\n\n(cid:16)\n\n∧\n\nzk < f k(w)\n\n(cid:17)\n\n∧\n\n(cid:16)\n\n(cid:16)\n\n⇒\n\nf k(x) < f k(w)\n\nF k−1(x) =(Z) F k−1(w) (cid:17)\n\n(cid:16)\n\n(cid:16)\n\n(cid:17)\n\n∧\n\nzk < f k(w)\n\n∧\n\n(cid:17)\n\nF k−1(x) =(Z) F k−1(w)\n\n(cid:17)\n\n(16)\n\n(17)\n\nin which indicates F (x) ≺(Z) F (w).\n\nStatement IV: F (x) ≺(Z) F (y) ∧ F (y) =(Z) F (w) ⇒ F (x) ≺(Z) F (w).\n\nProof of Statement IV. According to the definitions of =(Z) and ≺(Z) in Eq. (5) and Eq. (6),\n\n(cid:16)\n\nF (x) ≺(Z) F (y)\n\n(cid:16)\n\n(cid:17)\n\n∧\n\nF (y) =(Z) F (w)\n\n(cid:17)\n\n(cid:16)\n\n⇒\n\n∃k1 ∈ [K] : f k1(x) < f k1 (y) ∧ f k1 (y) > zk1 ∧ F k1−1(x) =(Z) F k1−1(y)\n\n(cid:17)\n\n∧\n\n(18)\n\n(cid:16)\n\n∀k2 ∈ [K] : (cid:0)f k2(y) = f k2 (w)(cid:1) ∨ (cid:0)f k2 (y) ≤ zk2 ∧ f k2 (w) ≤ zk2 (cid:1)(cid:17)\n\nThen, we can find a k = k1, such that,\n\n(cid:16)\n\nf k(x) < f k(y) = f k(w)\n\n(cid:17)\n\n(cid:16)\n\n∧\n\nzk < f k(w) = f k(y)\n\n(cid:17)\n\n(cid:16)\n\n∧\n\nF k−1(x) =(Z) F k−1(w)\n\n(cid:17)\n\n(cid:16)\n\n⇒\n\nf k(x) < f k(w)\n\n(cid:17)\n\n(cid:16)\n\n∧\n\nzk < f k(w)\n\n(cid:17)\n\n(cid:16)\n\n∧\n\nF k−1(x) =(Z) F k−1(w)\n\n(cid:17)\n\n⇒ F (x) ≺(Z) F (w)\n\n(19)\n\nBy substituting the conclusions in statement (I), (II), (III) and (IV) into (i), (ii), (iii), and (iv) in Eq. (9) respectively, we get:\n\nF (x) ⪯(Z) F (y) ∧ F (y) ⪯(Z) F (w) ⇒\n\n(cid:16)\n\nF (x) ≺(Z) F (w)\n\n(cid:17)\n\n(cid:16)\n\n∨\n\nF (x) =(Z) F (w)\n\n(cid:17)\n\n(20)\n\n⇒ F (x) ⪯(Z) F (w)\n\nwhich concludes the proof.\n\nB SUPPLEMENTARY RESULTS\n\nWe provide the results for the feature selection task in bio-informatics (Section 4.1.3) under different random seeds in Figure 4.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n(a) seed = 1\n\n(b) seed = 2\n\n(c) seed = 3\n\n(d) seed = 4\n\n(e) seed = 5\n\nFigure 4: The detailed results in tuning Xgboost on biological dataset AP Colon Kidney with different random seeds.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nC EXPERIMENTATION DETAILS\n\nC.1 SEARCH SPACE\n\nThe detailed search space in tuning Neural Networks, Random Forest, and XGboost are shown in Table 3, Table 4 and Table 5, respectively.\n\nTable 3: Hyperparameters tuned in Neural Network\n\nhyperparameter\n\ntype\n\nepoch num int layer num int int float float\n\nhidden units num per layer dropout value per layer learning rate per layer\n\nrange [1, 20] [1 , 3] [4, 128] [0.2, 0.5] [1e-5, 1e-1]\n\nTable 4: Hyperparameters tuned in Random Forest\n\nhyperparameter max features estimators number max leaves\n\ntype float int int\n\n√\n\nrange [min(0.1, 1/ [4, min(2048, train datasize)] [4, train size]\n\ndata features), 1.0]\n\nTable 5: Hyperparameters tuned in XGboost\n\nhyperparameter estimators number max leaves max depth min child weight learning rate subsample colsample by tree colsample by level reg alpha reg lambda\n\ntype int int int float float float float float float float\n\nrange [4, min(32768, train datasize)] [4, min(32768, train datasize)] [0, 6, 12] [0.001, 128] [1/1024, 1.0] [0.1, 1.0] [0.01, 1.0] [0.01, 1.0] [1/1024, 1024] [1/1024, 1024]\n\nC.2 DATE STATISTICS INFORMATION\n\nAll datasets used in our experiment are available in OpenML. In Table 6, we show the detailed statistics information of the datasets used in Section 4.2.\n\nTable 6: Date statistics information\n\nDataset statistics\n\nGisette Christine Scene Ginal prior Ginal agnostic Bioresponse Hiva agnostic Madelon\n\n# of train instance\n\n4900\n\n4063\n\n1805\n\n2601\n\n# of val instance\n\n2098\n\n1355\n\n# full feature dimension\n\n5000\n\n1636\n\n602\n\n299\n\n867\n\n784\n\n2601\n\n867\n\n970\n\n2813\n\n938\n\n1776\n\n3171\n\n1058\n\n1617\n\n1950\n\n650\n\n500\n\nC.3 DETAILS OF THE VALIDATION LOSS CALCULATION IN SECTION 4.1.2\n\nIn section 4.1.2, in order to keep the same experiment setting with the paper (Brookhouse & Freitas, 2022), we use the accuracy metric named the geometric mean of Sensitivity and Specificity from\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n(Brookhouse & Freitas, 2022) to calculate the 1st objective validation loss in the experiment section. More specifically, the accuracy metric geometric mean of Sensitivity and Specificity GMSen×Spe is calculated by:\n\nSensitivity =\n\nT P T P + F N\n\n, Specificity =\n\nT N T N + F P\n\n(21)\n\nGMSen×Spe = (cid:112)\n\nSensitivity × Specificity\n\nWhere T P , T N , F N and F P represent the number of true positive instances, the number of true negative instances, the number of false negative instances and the number of false positive instances, respectively. The validaiton loss reported in Table 1 is calculated by 1 − GMSen×Spe. More information of this metric could be found in (Brookhouse & Freitas, 2022).\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper \"Targeted Hyperparameter Optimization with Lexicographic Preference Over Multiple Objectives\" proposes a novel algorithm for optimizing hyperparameters with multiple objective functions. In their approach, the authors assume a total order over the objective functions to be given and accordingly sort candidate hyperparameter configurations in lexicographic order. To optimize the objectives effectively the authors propose a directed search algorithm and find their method LexiFlow to produce strong results.\n\n# Strength And Weaknesses\n\nStrengths:\n- Interesting method with a lot of potential\n- Strong performance compared to existing state-of-the-art multi-objective HPO methods\n- Robust and strong anytime performance\n\nWeaknesses:\n- Requires the order of objective functions as an input by the user (who might not be able to express such an order)\n- Requires the user to specify goals/thresholds which might be difficult to provide for black-box objectives. Meaning to provide such information the user needs extensive knowledge about the black-box objectives.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is overall well structured and well written. The explanations are easy to follow and the logic flows are reasonable.\n\nThe quality of the paper is very good. The method is well motivated, however, still imposing some assumptions which might be non-trivial. Still, for such a first work, such strong assumptions might be reasonable and be resolved in follow-up work.\n\nThe methodology is novel to the best of my knowledge.\n\nA good level of detail is given in the paper such that the work should be reproducible.\n\n# Summary Of The Review\n\nAll in all, I think that this paper makes an interesting contribution and the proposed method shows reasonable performance across tuning tasks and also within tuning tasks compared to state-of-the-art methods.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nGRAPH CONVOLUTIONAL NORMALIZING FLOWS FOR SEMI-SUPERVISED CLASSIFICATION & CLUSTERING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph neural networks (GNNs) are discriminative models that directly model the class posterior p(y|x) for semi-supervised classification of graph data. While being effective for prediction, as a representation learning approach, the node representations extracted from a GNN often miss useful information for effective clustering, because that is not necessary for a good classification. In this work, we replace a GNN layer by a combination of graph convolutions and normalizing flows under a Gaussian mixture representation space, which allows us to build a generative model that models both the class conditional likelihood p(x|y) and the class prior p(y). The resulting neural network, GC-Flow, enjoys two benefits: it not only maintains the predictive power because of the retention of graph convolutions, but also produces well-separated clusters in the representation space, due to the structuring of the representation as a mixture of Gaussians. We demonstrate these benefits on a variety of benchmark data sets. Moreover, we show that additional parameterization, such as that on the adjacency matrix used for graph convolutions, yields additional improvement in clustering.\n\n1\n\nINTRODUCTION\n\nSemi-supervised learning (Zhu, 2008) refers to the learning of a classification model by using typically a small amount of labeled data with possibly a large amount of unlabeled data. The presence of the unlabeled data, together with additional assumptions (such as the manifold and smoothness assumptions), may significantly improve the accuracy of a classifier learned even with few labeled data. A typical example of such a model in the recent literature is the graph convolutional network (GCN) of Kipf & Welling (2017), which capitalizes on the graph structure (considered as an extension of a discretized manifold) underlying data to achieve effective classification. GCN, together with other pioneering work on parameterized models, have formed a flourishing literature of graph neural networks (GNNs), which excel at node classification (Zhou et al., 2020; Wu et al., 2021).\n\nHowever, driven by the classification task, GCN and other GNNs may not produce node representations with useful information for goals different from classification. For example, the representations do not cluster well in some cases. Such a phenomenon is of no surprise. For instance, when one treats the penultimate activations as the data representations and uses the last dense layer as a linear classifier, the representations need only be close to linearly separable for an accurate classification; they do not necessarily form well-separated clusters.\n\nThis observation leads to a natural question: can one build a representation model for graphs that not only is effective for classification but also unravels the inherent structure of data for clustering? The answer is affirmative. One idea is to, rather than construct a discriminative model p(y|x) as all GNNs do, build a generative model p(x|y)p(y) whose class conditional likelihood is defined by explicitly modeling the representation space, for example by using a mixture of well-separated unimodal distributions. Indeed, the recently proposed FlowGMM model (Izmailov et al., 2020) uses a normalizing flow to map the distribution of input features to a Gaussian mixture, resulting in wellstructured clusters. This model, however, is not designed for graphs and it underperforms GNNs that leverage the graph structure for classification.\n\nIn this work, we present graph convolutional normalizing flows (GC-Flows), a generative model that not only classifies well, but also yields node representations that capture the inherent structure of data, as a result forming high-quality clusters. We can relate GC-Flows to both GCNs and\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) FlowGMM Silhouette = 0.725, F1 = 0.518\n\n(b) GCN Silhouette = 0.341, F1 = 0.808\n\n(c) GC-Flow (this work) Silhouette = 0.733, F1 = 0.817\n\nFigure 1: Representation space of the data set Cora under different models, visualized by t-SNE. Coloring indicates groud-truth labeling. Silhouette coefficients measure cluster separation. MicroF1 scores measure classification accuracy.\n\nFlowGMMs. On the one hand, GC-Flows incorporate each GCN layer with an invertible flow. Such a flow parameterization allows training a model through maximizing the likelihood of data representations being a Gaussian mixture, mitigating the poor clustering effect of GCNs. On the other hand, GC-Flows augment a usual normalizing flow model (such as FlowGMM) that is trained on independent data, with one that incorporates graph convolutions as an inductive bias in the parameterization, boosting the classification accuracy. In Figure 1, we visualize for a graph data set the nodes in the representation space using t-SNE. It suggests that GC-Flow inherits the clustering effect of FlowGMM, while being similarly accurate to GCN for classification.\n\nA few key characteristics of GC-Flows are as follows:\n\n1. A GC-Flow is a GNN, because being applied to graph data, it computes node representations by\n\nusing the graph structure. In contrast, a FlowGMM is not a GNN.\n\n2. A GC-Flow is a generative model, admitting FlowGMMs as a special case when graph is absent.\n\n3. As a generative model, the training loss function of GC-Flows involves both labeled and unla-\n\nbeled data, similar to FlowGMMs, while that of GNNs involves only the labeled data.\n\nSignificance. While classification is the dominant node-level task that concerns the current literature on GNNs, the importance of clustering in capturing the inherent structure of data is undeniable. This work addresses a weakness of the current GNN literature—particularly, the separation of clusters. A Gaussian mixture representation space properly reflects this goal. The normalizing flow is a vehicle to parameterize the feature transformation, so that it encourages the formation of separated Gaussians. It is a generative model that can return data densities. The likelihood training is organically tied to a generative model, whereas existing methods based on clustering or contrastive losses externally encourage the GNN to produce clustered representations, without a notation of densities.\n\n2 RELATED WORK\n\nGraph neural networks (GNNs) are machineries to produce node-level and graph-level representations, given graph-structured data as input (Zhou et al., 2020; Wu et al., 2021). A popular class of GNNs are message passing neural networks (MPNNs) (Gilmer et al., 2017), which treat information from the neighborhood of a node as messages and recursively update the node representation through aggregating the neighborhood messages and combing the result with the past node representation. Many popularly used GNNs can be considered a form of MPNNs, such as GG-NN (Li et al., 2016), GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GAT (Veliˇckovi ́c et al., 2018), and GIN (Xu et al., 2019).\n\nNormalizing flows are invertible neural networks that can transform a data distribution to a typically simple one, such as the normal distribution (Rezende & Mohamed, 2015; Kobyzev et al., 2021; Papamakarios et al., 2021). Because of invertibility, one may navigate the input and output distributions for purposes such as estimating densities and sampling new data. The densities of the two distributions are related by the change-of-variable formula, which involves the Jacobian determinant of the flow. Computing the Jacobian determinant is costly in general; thus, many proposed neural\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nnetworks exploit constrained structures, such as the triangular pattern of the Jacobian, to reduce the computational cost. Notable examples include NICE (Dinh et al., 2015), IAF (Kingma et al., 2016), MAF (Papamakarios et al., 2017), RealNVP (Dinh et al., 2017), Glow (Kingma & Dhariwal, 2018), and NSF (Durkan et al., 2019). While these network mappings are composed of discrete steps, another class of normalizing flows with continuous mappings have also been developed, which use parameterized versions of differential equations (Chen et al., 2018b; Grathwohl et al., 2019).\n\nNormalizing flows can be used for processing or creating graph-structured data in different ways. For example, GraphNVP (Madhawa et al., 2019) and GraphAF (Shi et al., 2020) are graph generative models that use normalizing flows to generate a graph and its node features in a one-shot and a sequential manner, respectively. GANF (Dai & Chen, 2022) uses an acyclic directed graph to factorize the otherwise intractable joint distribution of time series data and uses the estimated data density to detect anomalies. GNF (Liu et al., 2019) is both a graph generative model and a graph neural network. For the latter functionality, GNF is relevant to our model, but its purpose is to classify rather than to cluster, thus missing the representation-space modeling and a training objective suitable for clustering. Furthermore, the architecture of GNF differs from ours in the role the graph plays. For our method, the graph adjacency matrix is part of the flow mapping, hence provoking a determinant calculation with respect to the matrix; whereas for GNF, the graph is used in the parameterization of an affine coupling layer and it incurs no determinant calculation. CGF (Deng et al., 2019) extends the continuous version of normalizing flows to graphs, where the dynamics of the differential equation is parameterized as a message passing layer. The difference between our model and CGF inherits the general difference between discrete and continuous flows in how different parameterizations transform distributions.\n\nUnder the focus on clustering, several graph-based methods were developed based on the use of GNNs for feature extraction. For example, Fettal et al. (2022) use a combination of reconstruction and clustering losses to train the GNN; whereas Zhu et al. (2021); Li et al. (2022); Jing et al. (2022) use contrastive losses. Different from ours, these methods do not model the data (or representation) space with distributions as generative methods do. We empirically compare with several contrastive methods and demonstrate that our model significantly outperforms them in cluster separation.\n\n3 PRELIMINARIES\n\nIn this section, we review a few key concepts and familiarize the reader with notations to be used throughout the paper.\n\n3.1 NORMALIZING FLOW\n\nLet x ∈ RD be a D-dimensional random variable. A normalizing flow is a vector-valued invertible mapping f (x) : RD → RD that normalizes the distribution of x to some base distribution, whose density is easy to evaluate. Let such a base distribution have density π(z), where z = f (x). With the change-of-variable formula, the density of x, p(x), can be computed as\n\np(x) = π(f (x))| det ∇f (x)|,\n\n(1)\n\nIn general, such a flow f may be the composition of T where ∇f denotes the Jacobian of f . In notation, we write f = fT ◦ fT −1 ◦ · · · ◦ f1, constituent flows, all of which are invertible. where fi(x(i−1)) = x(i) for all i, and x(0) ≡ x and x(T ) ≡ z. Then, the chain rule expresses the Jacobian determinant as a product of the Jacobian determinants of each constituent flow: det ∇f (x) = (cid:81)T\n\ni=1 det ∇fi(x(i−1)).\n\nIn practical uses, the Jacobian determinant of each constituent flow needs be easy to compute, so that the density p(x) in (1) can be evaluated. One example that serves such a purpose is the affine coupling layer of Dinh et al. (2017). For notational simplicity, we denote such a coupling layer by g(x) = y, which in effect computes\n\ny1:d = x1:d,\n\nyd+1:D = xd+1:D (cid:12) exp(s(x1:d)) + t(x1:d),\n\nwhere d = (cid:98)D/2(cid:99) and s, t : Rd → RD−d are any neural networks. It is simple to see that the Jacobian is a triangular matrix, whose diagonal has value 1 in the first d entries and exp(s) in the remaining D − d entries. Hence, the Jacobian determinant is simply the product of the exponential of the outputs of the s-network; that is, det ∇g(x) = (cid:81)D−d\n\ni=1 exp(si).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 GAUSSIAN MIXTURE AND FLOWGMM\n\nDifferent from a majority of work that take the base distribution in a normalizing flow to be a single Gaussian, we consider it to be a Gaussian mixture, because this is a natural probabilistic model for clustering. Using k to index mixture components (K in total), we express the base density π(z) as\n\nπ(z) =\n\nK (cid:88)\n\nk=1\n\nφkN (z; μk, Σk) with N (z; μk, Σk) =\n\nexp(− 1\n\n2 (z − μk)T Σ−1 (2π)D/2(det Σk)1/2\n\nk (z − μk))\n\n,\n\n(2)\n\nwhere φk ≥ 0 are mixture weights that sum to unity and μk and Σk are the mean vector and the covariance matrix of the k-th component, respectively.\n\nA broad class of semi-supervised learning models specifies a generative process for each data point x through defining p(x|y)p(y), where p(y) is the prior class distribution and p(x|y) is the class conditional likelihood for data. Then, by the Bayes’ Theorem, the class prediction model p(y|x) is proportional to p(x|y)p(y). Among them, FlowGMM (Izmailov et al., 2020) makes use of the flow transform z = f (x) and defines p(x|y = k) = N (f (x); μk, Σk)| det ∇f (x)| with p(y = k) = φk. This definition is valid, because marginalizing over the class variable y, one may verify that p(x) = (cid:80) y p(x|y)p(y) is consistent with the density formula (1), when the base distribution follows (2).\n\n3.3 GRAPH CONVOLUTIONAL NETWORK\n\nThe GCNs (Kipf & Welling, 2017) are a class of parameterized neural network models that specify the probability of class y of a node x, p(y|x), collectively for all nodes x in a graph, without defining the data generation process as in FlowGMM. To this end, we let A ∈ Rn×n be the adjacency matrix of the graph, which has n nodes, and let X = [x1, · · · , xn]T ∈ Rn×D be the input feature matrix, with xi being the feature vector for the i-th node. We further let P ∈ Rn×K be the output probability matrix, where K is the number of classes and Pik ≡ p(y = k|xi). An L-layer GCN is written as\n\nX(i) = σi( (cid:98)AX(i−1)W(i−1)),\n\ni = 1, . . . , L,\n\n(3)\n\nwhere X ≡ X(0) and P ≡ X(L). Here, σi is an element-wise activation function, such as ReLU, for the intermediate layers i < L, while σL is the row-wise softmax activation function for the final layer. The matrices W(i), i = 0, . . . , L − 1, are learnable parameters and (cid:98)A denotes a certain normalized version of the adjacency matrix A. The standard definition of (cid:98)A for an undirected graph is (cid:98)A = (cid:101)D− 1 , but we note that many other variants of (cid:98)A are used in practice as well (such as (cid:98)A = (cid:101)D−1 (cid:101)A).\n\n2 , where (cid:101)A = A + I and (cid:101)D = diag\n\n2 (cid:101)A (cid:101)D− 1\n\nj (cid:101)Aij\n\n(cid:16)(cid:80)\n\n(cid:17)\n\n4 METHOD\n\nThe proposed graph convolutional normalizing flow (GC-Flow) extends a usual normalizing flow acting on data points separately to one that acts on all graph nodes collectively. Following the notations used in Section 3.3, starting with X(0) ≡ X, where X is an n × D input feature matrix for all n nodes in the graph, we define a GC-Flow F(X) : Rn×D → Rn×D that is a composition of T constituent flows F = FT ◦ FT −1 ◦ · · · ◦ F1, where each constituent flow Fi computes\n\nX(i) = Fi( (cid:98)AX(i−1) (cid:125)\n\n(cid:124)\n\n),\n\n(cid:123)(cid:122) (cid:101)X(i)\n\ni = 1, . . . , T.\n\n(4)\n\nThe final representation of the nodes is the matrix Z ≡ X(T ).\n\nGC-Flow is a normalizing flow. Similar to other normalizing flows, each constituent flow preserves the feature dimension; that is, each Fi is an Rn×D → Rn×D function. Furthermore, we let Fi act on each row of the input argument (cid:101)X(i) separately and identically. In other words, from the functionality perspective, Fi can be equivalently replaced by some function fi : R1×D → R1×D that computes x(i) j ) for a node j. The main difference between GC-Flow and a usual flow is that the input argument of fi contains not only the information of node j but also that of its\n\nj = fi((cid:101)x(i)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nneighbors. One may consider a usual flow to be a special case of GC-Flows, when (cid:98)A = I (e.g., the graph contains no edges).\n\nMoreover, GC-FLow is a GNN. In particular, a constituent flow Fi of (4) resembles a GCN layer of (3) by making use of graph convolutions—multiplying (cid:98)A to the flow/layer input X(i−1). When (cid:98)A results from the normalization defined by GCN, such a graph convolution approximates a lowpass filter (Kipf & Welling, 2017). In a sense, the GC-Flow architecture is more general than a GCN architecture, because one may interpret the dense layer (represented by the parameter matrix W(i−1)) followed by a nonlinear activation σi in (3) as an example of the constituent flow Fi in (4). However, such a conceptual connection needs a few adjustments to make a GC-Flow and a GCN mathematically equivalent, because W(i−1) in GCN is not required to preserve the feature dimension and σi of GCN has a zero derivative on the negative axis, compromising invertibility. The nearest adjustment can be made via using the Sylvester flow (van den Berg et al., 2018), which adds a residual connection and uses an additional parameter matrix U(i−1) to preserve the feature dimension:1 X(i) = X(i−1) + σi( (cid:98)AX(i−1)W(i−1))U(i−1). However, the Sylvester flow generally has a limited capacity (Kobyzev et al., 2021) and a more sophisticated flow is instead used as Fi, such as the affine coupling layer introduced in Section 3.1.\n\nA major distinction between the GC-Flow and a usual GNN lies in the training objective. To encourage a good clustering structure of the representation Z, we use a maximum-likelihood kind of objective for all graph nodes, because it is equivalent to maximizing the likelihood that Z forms a Gaussian mixture:\n\nmax L :=\n\n1 − λ |Dl|\n\n(cid:88)\n\n(x,y=k)∈Dl\n\nlog p(x, y = k) +\n\nλ |Du|\n\n(cid:88)\n\nx∈Du\n\nlog p(x),\n\n(5)\n\nwhere Dl denotes the set of labeled nodes, Du denotes the set of unlabeled nodes, and λ ∈ (0, 1) is a tunable hyperparameter balancing labeled and unlabeled information. It is useful to compare L with the usual (negative) cross-entropy loss for training GNNs. First, for training a usual GNN, no loss is incurred on the unlabeled nodes, because their likelihoods are not modeled. Second, for a labeled node x with true label k, the negative cross-entropy is log p(y = k|x), while the likelihood term over labeled data in (5) is a joint probability of x and y: log p(x, y = k) = log p(y = k|x) + log p(x). Fundamentally, GC-Flow belongs to the class of generative classification models, while GNNs belong to the class of discriminative models. Under Bayesian paradigm, the former models the class prior and the class conditional likelihood, while the latter models only the posterior.\n\nIn what follows, we will define the proposed probability model p(x|y)p(y) for a node x, so that the loss L can be computed and the label y can be predicted via argmaxk p(y = k|x). We first need an important lemma on the Jacobian determinant when a graph convolution is involved in the flow.\n\n4.1 DETERMINANT LEMMA\n\nThe Jacobian determinant of each constituent flow Fi defined in (4) is needed for training a GCFlow. The Jacobian is an nD × nD matrix, but it admits a special block structure that allows the determinant to be computed as a product of determinants on D matrices of size n × n, after rearrangement of the QR factorization factors of the Jacobians of fi. The following lemma summarizes this finding; the proof is given in the appendix. For notational convenience, we remove the flow index and use G to denote a generic constituent flow. Lemma 1. Let X ∈ Rn×D and (cid:98)A ∈ Rn×n. Let Y = G( (cid:101)X), where (cid:101)X ≡ (cid:98)AX and G : Rn×D → Rn×D acts on each row of the input matrix independently and identically. Let g : RD → RD be functionally equivalent to G; that is, yi = g((cid:101)xi) where yi and (cid:101)xi are the i-th row of Y and (cid:101)X, respectively. Then, (cid:12)\n\n(cid:1)(cid:12) (cid:12) = | det (cid:98)A|D (cid:81)n\n\n(cid:12)det (cid:0) dY\n\ni=1 | det ∇g((cid:101)xi)|.\n\ndX\n\nPutting back the flow index, the above lemma suggests that, by the chain rule, the Jacobian determinant of the entire GC-Flow F is\n\n| det ∇F(X)| = | det (cid:98)A|T D\n\nT (cid:89)\n\nn (cid:89)\n\nj=1\n\ni=1\n\n| det ∇fj((cid:101)x(j)\n\ni )|.\n\n(6)\n\n1For notational convenience and consistency with the GNN literature, here we omit the often-used bias term.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nNote that to maintain invertibility of the flow, the matrix (cid:98)A must be nonsingular. We will define the probability model for GC-Flow based on equality (6).\n\n4.2 PROBABILITY MODEL\n\nDifferent from a usual normalizing flow, where the representation zi for the i-th data point depends on its input feature vector xi, in a GC-Flow, zi depends on (a possibly substantial portion of) the entire node set X, because of the (cid:98)A-multiplication. To this end, we use p(X) and π(Z) to denote the joint distribution of the node feature vectors and that of the representations, respectively. We still have, by the change-of-variable formula,\n\np(X) = π(Z)| det ∇F(X)|,\n\n(7)\n\nwhere the Jacobian determinant has been derived in (6). Under the freedom of modeling and for convenience, we opt to let π(Z) be expressed as π(Z) = π(z1)π(z2) · · · π(zn), where each π(zi) is an independent and identically distributed Gaussian mixture (2). Similarly, we assume the nodes to be independent to start with; that is, p(X) = p(x1)p(x2) · · · p(xn).\n\nFor generative modeling, a task is to model the class prior p(y) and the class conditional likelihood p(x|y), such that the posterior prediction model p(y|x) can be easily obtained as proportional to p(x|y)p(y), by Bayes’ Theorem. To this end, we define\n\np(xi|yi = k) := N (zi; μk, Σk)| det (cid:98)A|T D/n (cid:81)T\n\nj=1 | det ∇fj((cid:101)x(j)\n\ni )|\n\nand p(yi = k) = φk. (8)\n\nSuch a definition is self-consistent. First, marginalizing over the label yi and using the Gaussian mixture definition (2) for π(zi), we obtain the marginal likelihood\n\np(xi) = π(zi)| det (cid:98)A|T D/n (cid:81)T\n\nj=1 | det ∇fj((cid:101)x(j)\n\ni )|.\n\n(9)\n\nThen, by the modeling of π(Z) and p(X), taking the product for all nodes and using the Jacobian determinant formula derived in (6), we exactly recover the density formula (7). We will use (8) and (9) to compute the labeled part and the unlabeled part of the loss (5), respectively.\n\nThe modeling of π(Z) as a product of π(zi)’s reflects independence, which may seem conceptually at odds with graph convolutions, where a node’s representation depends on the information of nodes in it’s T -hop neighborhood. However, nothing prevents the convolution results to be independent, just like the case that a usual normalizing flow can decorrelate the input features and make each transformed feature independent, when postulating a standard normal distribution output. It is the aim of the independence of the zi’s that enables finding the most probable GC-Flow.\n\n4.3 TRAINING AND COSTS\n\nDespite inheriting the generative characteristics of FlowGMMs (including the training loss), GCFlows are by nature a GNN, because the graph convolution operation ( (cid:98)A-multiplication) involves a node’s neighbor set when computing the output of a constituent flow for this node. Due to space limitation, we discuss the complication of training and inference owing to neighborhood explosion in Appendix C; these discussions share great similarities with the GNN case. Additionally, we compare the full-batch training costs of GC-Flow and GCN in Appendix D, which suggests that they are comparable and admit the same scaling behavior.\n\n4.4\n\nIMPROVING PERFORMANCE THROUGH PARAMETERIZING (cid:98)A\n\nSo far, we have treated (cid:98)A as the normalization of the graph adjacency matrix A defined by GCN (see Section 3.3). One convenience of doing so is that det (cid:98)A is a constant and can be safely omitted in the loss calculation. One may improve the quality of GC-Flow through introducing parameterizations to (cid:98)A. One approach, which we call GC-Flow-p, is to parameterize the edge weights. This approach is similar to GAT (Veliˇckovi ́c et al., 2018) that uses attention weights to redefine (cid:98)A. Another approach, which we call GC-Flow-l, is to learn (cid:98)A in its entirety without resorting to the (possibly unknown) graph structure. For this purpose, several approaches have been developed; see, e.g., Franceschi et al. (2019); Wu et al. (2020); Shang et al. (2021); Fatemi et al. (2021); Dai & Chen (2022).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison of GMM-based generative models, GNN-based discriminative models, and GC-Flow for semi-supervised classification and clustering. Standard deviations are obtained by repeating model training ten times. For each data set and metric, the two best cases are boldfaced.\n\nCora\n\nCiteseer\n\nPubmed\n\nSilhouette 0.739 ± 0.015 FlowGMM GMM on X 0.162 ± 0.000 GMM on (cid:98)AX 0.144 ± 0.000 0.340 ± 0.003 0.346 ± 0.004 0.383 ± 0.003 0.734 ± 0.006\n\nGCN GraphSAGE GAT GC-Flow\n\nMicro-F1 0.504 ± 0.021 0.163 ± 0.000 0.173 ± 0.000 0.813 ± 0.007 0.801 ± 0.005 0.825 ± 0.005 0.815 ± 0.011\n\nSilhouette 0.609 ± 0.034 0.071 ± 0.000 0.089 ± 0.000 0.314 ± 0.016 0.278 ± 0.007 0.304 ± 0.003 0.538 ± 0.022\n\nMicro-F1 0.512 ± 0.044 0.085 ± 0.000 0.182 ± 0.000 0.700 ± 0.025 0.697 ± 0.007 0.702 ± 0.007 0.714 ± 0.011\n\nSilhouette 0.653 ± 0.031 0.062 ± 0.000 0.183 ± 0.000 0.453 ± 0.006 0.440 ± 0.018 0.435 ± 0.010 0.669 ± 0.021\n\nMicro-F1 0.734 ± 0.014 0.581 ± 0.000 0.411 ± 0.000 0.791 ± 0.004 0.769 ± 0.011 0.774 ± 0.005 0.791 ± 0.009\n\nComputers\n\nPhoto\n\nWiki-CS\n\nSilhouette 0.540 ± 0.024 FlowGMM GMM on X -0.018 ± 0.00 GMM on (cid:98)AX -0.021 ± 0.00 0.357 ± 0.026 0.434 ± 0.030 0.431 ± 0.015 0.487 ± 0.012\n\nGCN GraphSAGE GAT GC-Flow\n\nMicro-F1 0.614 ± 0.026 0.102 ± 0.000 0.062 ± 0.000 0.812 ± 0.016 0.761 ± 0.024 0.814 ± 0.023 0.847 ± 0.007\n\nSilhouette 0.704 ± 0.027 -0.024 ± 0.00 -0.041 ± 0.00 0.388 ± 0.003 0.386 ± 0.007 0.425 ± 0.020 0.655 ± 0.013\n\nMicro-F1 0.599 ± 0.089 0.120 ± 0.000 0.098 ± 0.000 0.891 ± 0.012 0.839 ± 0.020 0.900 ± 0.009 0.917 ± 0.004\n\nSilhouette 0.677 ± 0.011 0.088 ± 0.000 0.026 ± 0.000 0.264 ± 0.005 0.233 ± 0.009 0.278 ± 0.008 0.717 ± 0.010\n\nMicro-F1 0.671 ± 0.011 0.124 ± 0.000 0.188 ± 0.000 0.775 ± 0.005 0.771 ± 0.003 0.773 ± 0.003 0.775 ± 0.002\n\nIn a later experiment, we will give examples for GC-Flow-p and GC-Flow-l (see Appendix B for details) and investigate the performance improvement over GC-Flow. Note that the parameterization may lead to a different (cid:98)A for each constituent flow. Hence, we add the flow index (j) to (cid:98)A and rewrite the Jacobian determinant (6) as | det ∇F(X)| = (cid:81)T . The probability models (8) and (9) are correspondingly rewritten, respectively, as\n\ni=1 | det ∇fj((cid:101)x(j)\n\n| det (cid:98)A(j)|D (cid:81)n\n\ni )|\n\nj=1\n\n(cid:17)\n\n(cid:16)\n\np(xi|yi = k) := N (zi; μk, Σk) (cid:81)T\n\nj=1 | det (cid:98)A(j)|D/n| det ∇fj((cid:101)x(j)\n\ni )|\n\nand p(yi = k) = φk,\n\np(xi) = π(zi) (cid:81)T\n\nj=1 | det (cid:98)A(j)|D/n| det ∇fj((cid:101)x(j)\n\ni )|.\n\nThese two formulas are used to substitute the labeled and unlabeled parts of the loss (5), respectively.\n\n5 EXPERIMENTS\n\nIn this section, we conduct a comprehensive set of experiments to evaluate the performance of GCFlow on graph data and demonstrate that it is competitive with GNNs for classification, while being advantageous in learning representations that extract the clustering structure of the data.\n\nData sets. We use six benchmark node-classification data sets. Data sets Cora, Citeseer, and Pubmed are citation graphs, where each node is a document and each edge represents the citation relation between two documents. We follow the predefined splits in Kipf & Welling (2017). Data sets Computers and Photo are subgraphs of the Amazon co-purchase graph (McAuley et al., 2015). They do not have a predefined split. We randomly sample 200/1300/1000 nodes for training/validation/testing for Computers and 80/620/1000 for Photo. The data set Wiki-CS is a web graph where nodes are Wikipedia articles and edges are hyperlinks (Mernyei & Cangea, 2020). We use one of the predefined splits. For statistics of the data sets, see Table 4 in Appendix E.\n\nBaselines. We compare GC-Flow with both discriminative and generative models. For discriminative models, we use three widely used GNNs: GCN, GraphSAGE, and GAT. For generative models, besides FlowGMM, we use the basic Gaussian mixture model (GMM). GMM is not parameterized; it takes either the node features X or the graph-transformed features (cid:101)X = (cid:98)AX as input.\n\nMetrics. For measuring classification quality, we use the standard micro-averaged F1 score. For evaluating clustering, we mainly use the silhouette coefficient. This metric does not require groundtruth cluster labels and it measures the separation of clusters. Better separation indicates a more interpretable structure. We additionally use NMI (normalized mutual information) and ARI (adjusted rand index) to measure clustering quality with known ground truths.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) FlowGMM\n\n(b) GCN\n\n(c) GC-Flow\n\nFigure 2: Representation space of the data set Pubmed under different models.\n\nImplementation details and hyperparameter information may be found in Appendix E.\n\nClassification and clustering performance. Table 1 lists the F1 scores and the silhouette coefficients for all data sets and all compared models. GNNs are always better than GMMs for classification; while the flow version of GMM, FlowGMM, beats all GNNs on cluster separation. Our model, GC-Flow, is competitive with the better of the two and is always the best or the second best. When being the best, some of the improvements are rather substantial, such as the F1 score for Computers and the silhouette coefficient for Wiki-CS. It is interesting to note that the basic GMMs perform rather poorly. This phenomenon is not surprising, because without any neural network parameterization, they cannot compete with other models that allow feature transformations to encourage class separation or cluster separation.\n\nTable 2: Clustering performance of various GNN methods. The two best cases are boldfaced. Data set: Cora.\n\nTo further illustrate the clustering quality of GC-Flow, we compare it with several contrastive-based methods that produce competitive clusterings: DGI (Veliˇckovi ́c et al., 2019), GRACE (Zhu et al., 2020), GCA (Zhu et al., 2021), GraphCL (You et al., 2020), and MVGRL (Hassani & Khasahmadi, 2020). Table 2 lists the results for Cora and Table 5 in Appendix F includes more data sets. For Cora, GC-Flow delivers the best performance on all metrics, with a silhouette score more than double of the second best. Compared with NMI and ARI, silhouette is a metric that takes no knowledge of the ground truth but measures solely the cluster separation in space. This result suggests that the clusters obtained from GC-Flow are more structurally separated, albeit improving less the cluster agreement.\n\nNMI 0.592 ± 0.001 DGI 0.475 ± 0.028 GRACE 0.418 ± 0.053 GCA 0.577 ± 0.002 GraphCL 0.612 ± 0.026 MVGRL GC-Flow 0.621 ± 0.013\n\nARI 0.570 ± 0.002 0.394 ± 0.047 0.259 ± 0.058 0.482 ± 0.003 0.576 ± 0.062 0.631 ± 0.008\n\nSilhouette 0.330 ± 0.000 0.153 ± 0.011 0.301 ± 0.005 0.297 ± 0.002 0.369 ± 0.013 0.734 ± 0.006\n\nTraining behavior. Figure 5 (see Appendix F) plots the convergence behavior of the training loss for FlowGMM, GCN, and GC-Flow. All methods converge favorably, with GCN reaching the plateau earlier, while FlowGMM and GC-Flow converge at a rather similar speed.\n\nVisualization of the representation space. To complement the numerical metrics, we visualize the representation space of FlowGMM, GCN, and GC-Flow by using a t-SNE plot (Van der Maaten & Hinton, 2008), for qualitative evaluation. The representations for FlowGMM and GC-Flow are the zi’s, while those for GCN are extracted from the penultimate activations. The results for Cora are given earlier in Figure 1; we additionally give the results for Pubmed in Figure 2. From both figures, one sees that similar to FlowGMM, GC-Flow exhibits a better clustering structure than does GCN, which produces little separation for the data. More visualizations are provided in Appendix F.\n\nAnalysis on depth. Figure 3 plots the performance of FlowGMM, GCN, and GC-Flow as the number of layers/flows increases. One sees that the classification performance of GCN deteriorates with more layers, in agreement with the well-known oversmoothing phenomenon (Li et al., 2018), while the clustering performance is generally stable. On the other hand, the classification performance of FlowGMM and GC-Flow does not show a unique pattern: for Cora, it degrades, while for Pubmed, it stabilizes. The clustering performance of FlowGMM and GC-Flow generally degrades, except for the curious case of GC-Flow on Cora, where the silhouette coefficient shows a V-shape. Nevertheless, generally a smaller depth is preferred for all models.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cora\n\n(b) Pubmed\n\n(a) Cora\n\nFigure 3: Performance variation with respect to the network depth/number of flows.\n\nFigure 4: Performance variation with respect to the labeling rate.\n\nTable 3: Effect of modeling (cid:98)A. Boldfaced numbers indicate improvement over GC-Flow.\n\nPubmed\n\nComputers\n\nPhoto\n\nGC-Flow GC-Flow-p GC-Flow-l\n\nSilhouette 0.669 ± 0.021 0.804 ± 0.010 0.856 ± 0.029\n\nMicro-F1 0.791 ± 0.009 0.790 ± 0.007 0.783 ± 0.009\n\nSilhouette 0.487 ± 0.012 0.706 ± 0.019 0.582 ± 0.020\n\nMicro-F1 0.847 ± 0.007 0.841 ± 0.006 0.851 ± 0.009\n\nSilhouette 0.655 ± 0.013 0.874 ± 0.011 0.842 ± 0.006\n\nMicro-F1 0.917 ± 0.004 0.914 ± 0.008 0.911 ± 0.005\n\nAnalysis on labeling rate. Figure 4 plots the performance of FlowGMM, GCN, and GC-Flow as the number of training labels per class increases. One sees that for all models, the performance generally improves with more labeled data. The improvement is more steady and noticeable for classification, while being less significant for clustering. Additionally, GC-Flow classifies significantly better than does GCN at the low-labeling rate regime, achieving a 10.04% relative improvement in the F1 score when there are only two labeled nodes per class.\n\nImproving performance with additional parameterization. We experiment with two variants of GC-Flow by introducing parameterizations to (cid:98)A. The variant GC-Flow-p uses an idea similar to GAT, through embedding flow inputs and computing an additive attention on the graph edges to redefine their weights. Another variant GC-Flow-l also computes weights, but rather than using them to define (cid:98)A, it treats each weight as a probability of edge presence and samples the corresponding Bernoulli distribution to obtain a binary sample (cid:98)A. The details are given in Appendix B.\n\nTable 3 lists the performance of GC-Flow-p and GC-Flow-l on three selected data sets, where the improvement over GC-Flow is notable. The improvement predominantly appears for clustering, with the most striking increase from 0.487 to 0.706. The increase of silhouette coefficients generally come with a marginal decrease in the F1 score, but the decrement amount is below the standard deviation. In one occasion (Computers), the F1 score even increases, despite also being marginal.\n\n6 CONCLUSIONS\n\nWe have developed a generative GNN model which, rather than directly computing the class posterior p(y|x), computes the class conditional likelihood p(x|y) and applies the Bayes rule together with the class prior p(y) for prediction. A benefit of such a model is that one may control the representation of the data (e.g., a clustering structure) through modeling the representation distribution (e.g., optimizing it toward a mixture of well-separated unimodal distributions). We achieve so by designing the GNN as a normalizing flow that additionally incorporates graph convolutions. Interestingly, the graph adjacency matrix appears in the density computation of the normalizing flow as a stand-alone term, which could be ignored if it is a constant, or easily optimized if it is parameterized. We demonstrate that the proposed model not only maintains the predictive power of the past GNNs, but also produces high-quality clusters in the representation space.\n\nREFERENCES\n\nJie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks\n\nvia importance sampling. In ICLR, 2018a.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-\n\nferential equations. In NeurIPS, 2018b.\n\nWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks. In KDD, 2019.\n\nEnyan Dai and Jie Chen. Graph-augmented normalizing flows for anomaly detection of multiple\n\ntime series. In ICLR, 2022.\n\nZhiwei Deng, Megha Nawhal, Lili Meng, and Greg Mori. Continuous graph flow. Preprint\n\narXiv:1908.02436, 2019.\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components\n\nestimation. In ICLR Workshop, 2015.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In\n\nICLR, 2017.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows.\n\nIn\n\nNeurIPS, 2019.\n\nBahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. SLAPS: Self-supervision improves\n\nstructure learning for graph neural networks. In NeurIPS, 2021.\n\nChakib Fettal, Lazhar Labiod, and Mohamed Nadif. Efficient graph convolution for joint node\n\nrepresentation learning and clustering. In WSDM, 2022.\n\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.\n\nRLGM@ICLR, 2019.\n\nLuca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures\n\nfor graph neural networks. In ICML, 2019.\n\nJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural\n\nmessage passing for quantum chemistry. In ICML, 2017.\n\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\n\nnetworks. In AISTATS, pp. 249–256, 2010.\n\nWill Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. In ICLR,\n\nFFJORD: Free-form continuous dynamics for scalable reversible generative models. 2019.\n\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec.\n\nInductive representation learning on large\n\ngraphs. In NIPS, 2017.\n\nKaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on\n\ngraphs. In ICML, 2020.\n\nPavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi-supervised learn-\n\ning with normalizing flows. In ICML, 2020.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\n\nPreprint arXiv:1611.01144, 2016.\n\nBaoyu Jing, Shengyu Feng, Yuejia Xiang, Xi Chen, Yu Chen, and Hanghang Tong. X-GOAL:\n\nMultiplex heterogeneous graph prototypical contrastive learning. In CIKM, 2022.\n\nTim Kaler, Nickolas Stathas, Anne Ouyang, Alexandros-Stavros Iliopoulos, Tao B. Schardl, Charles E. Leiserson, and Jie Chen. Accelerating training and inference of graph neural networks with fast sampling and pipelining. In MLSys, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.\n\nIn NeurIPS, 2018.\n\nDiederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.\n\nImproving variational inference with inverse autoregressive flow. In NeurIPS, 2016.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. In ICLR, 2017.\n\nIvan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964–3979, 2021.\n\nBolian Li, Baoyu Jing, and Hanghang Tong. Graph communal contrastive learning. In WWW, 2022.\n\nQimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for\n\nsemi-supervised learning. In AAAI, 2018.\n\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\n\nnetworks. In ICLR, 2016.\n\nJenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. In\n\nNeurIPS, 2019.\n\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\n\nl 0 regularization. Preprint arXiv:1712.01312, 2017.\n\nDongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang. Learning to drop: Robust graph neural network via topological denoising. In WSDM, pp. 779– 787, 2021.\n\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\n\nrelaxation of discrete random variables. Preprint arXiv:1611.00712, 2016.\n\nKaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. GraphNVP: An in-\n\nvertible flow model for generating molecular graphs. Preprint arXiv:1905.11600, 2019.\n\nJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based rec-\n\nommendations on styles and substitutes. In SIGIR, pp. 43–52, 2015.\n\nP ́eter Mernyei and C ̆at ̆alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural net-\n\nworks. Preprint arXiv:2007.02901, 2020.\n\nGeorge Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density\n\nestimation. In NIPS, 2017.\n\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1–64, 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. NeurIPS, 32, 2019.\n\nFabian Pedregosa, Ga ̈el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows.\n\nIn ICML,\n\n2015.\n\nChao Shang, Jie Chen, and Jinbo Bi. Discrete graph structure learning for forecasting multiple time\n\nseries. In ICLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nChence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. GraphAF: a\n\nflow-based autoregressive model for molecular graph generation. In ICLR, 2020.\n\nRianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, and Max Welling. Sylvester nor-\n\nmalizing flows for variational inference. In UAI, 2018.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\n\nLearning Research, 9(11), 2008.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\n\nBengio. Graph attention networks. In ICLR, 2018.\n\nPetar Veliˇckovi ́c, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. In ICLR, 2019.\n\nZonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting the dots: Multivariate time series forecasting with graph neural networks. In KDD, 2020.\n\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4–24, 2021.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? In ICLR, 2019.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In KDD, 2018.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. In NeurIPS, 2020.\n\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-\n\nSAINT: Graph sampling based inductive learning method. In ICLR, 2020.\n\nJie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1: 57–81, 2020.\n\nXiaojin Zhu. Semi-supervised learning literature survey. Technical Report TR1530, University of\n\nWisconsin-Madison, 2008.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. Preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning\n\nwith adaptive augmentation. In WWW, 2021.\n\nDifan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In NeurIPS, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOF OF LEMMA 1\n\ndX is an nD × nD matrix. By the chain rule, an entry of it is dYij\n\nThe Jacobian dY where Ji := ∇g((cid:101)xi) ∈ RD×D. Hence, dY permutation and transpose that do not affect the absolute value of the determinant):\n\njq, dX can be expressed as the following block matrix (up to\n\n= (cid:98)AipJi\n\ndXpq\n\n\n\n \n \n\n(cid:98)A11J1 (cid:98)A21J2 ... (cid:98)An1Jn\n\n(cid:98)A12J1 (cid:98)A22J2 ... (cid:98)An2Jn\n\n· · · · · · . . . · · ·\n\n\n\n \n \n\n.\n\n(cid:98)A1nJ1 (cid:98)A2nJ2 ... (cid:98)AnnJn\n\nSince any matrix admits a QR factorization, let Ji = QiRi for all i, where Qi is unitary and Ri is upper-triangular. Then, the above block matrix is equal to\n\n\n\nQ1\n\n \n\n\nQ2\n\n\n\n \n\n\n. . .\n\nQn\n\n\n\n \n \n\n(cid:98)A11R1 (cid:98)A21R2 ... (cid:98)An1Rn\n\n(cid:98)A12R1 (cid:98)A22R2 ... (cid:98)An2Rn\n\n· · · · · · . . . · · ·\n\n\n\n \n \n\n.\n\n(cid:98)A1nR1 (cid:98)A2nR2 ... (cid:98)AnnRn\n\nBecause left block matrix is unitary, it does not change the determinant. Hence, we only need to compute the determinant of the right block matrix. We may rearrange this matrix into the following form, while maintaining the absolute value of the determinant: \n\n\n\n\n\n\n \n \n\n(cid:98)A (cid:12) S11 (cid:98)A (cid:12) S21 ... (cid:98)A (cid:12) Sn1\n\n(cid:98)A (cid:12) S12 (cid:98)A (cid:12) S22 ... (cid:98)A (cid:12) Sn2\n\n· · · · · · . . . · · ·\n\n(cid:98)A (cid:12) S1n (cid:98)A (cid:12) S2n ... (cid:98)A (cid:12) Snn\n\n \n \n\nwhere Sij =\n\n \n \n\nij\n\nij\n\nR1 ij R1 ij R2 R2 ... ... ij Rn Rn\n\nij\n\n· · · R1 ij · · · R2 ij ... . . . · · · Rn ij\n\nfor all i, j pairs.\n\n \n \n\nBecause each Rk is upper-triangular, the matrix Sij is zero whenever i > j. Therefore, the block matrix above is block upper-triangular and its absolute determinant is equal to\n\nD (cid:89)\n\ni=1\n\n| det( (cid:98)A (cid:12) Sii)|.\n\nNote that Sii is a matrix with identical rows, for each i. Then, by the definition of determinant (see the Leibniz formula), det( (cid:98)A (cid:12) Sii) = det( (cid:98)A) · R1\n\nii. Therefore,\n\nii · · · Rn\n\niiR2\n\nD (cid:89)\n\ni=1\n\n| det( (cid:98)A (cid:12) Sii)| = | det (cid:98)A|D · |R1\n\n11R2\n\n11 · · · Rn\n\n11R1\n\n22R2\n\n22 · · · Rn\n\n22 · · · R1\n\nDDR2\n\nDD · · · Rn\n\nDD|\n\n= | det (cid:98)A|D · | det J1| · | det J2| · · · | det Jn|,\n\nwhich concludes the proof.\n\nB PARAMETERIZATIONS OF (cid:98)A\n\nWith parameterization, (cid:98)A may differ in the constituent flows. Hence, we use the flow index j to distinguish them; i.e., (cid:98)A(j). Let a graph be denoted by G = (V, E), where V is the node set and E is the edge set.\n\nB.1 GC-FLOW-P VARIANT\n\nThe GC-Flow-p variant parameterizes (cid:98)A(j) by using an idea similar to GAT (Veliˇckovi ́c et al., 2018), where an existing edge (i, k) ∈ E is reweighted by using attention scores. Let E1, E2 : RD → Rd be two embedding networks that map a D-dimensional flow input to a d-dimensional vector, and let M : R2d → R1 be a feed-forward network. We compute two sets of vectors\n\nh(j−1)\n\ni\n\n= ReLU\n\n(cid:16)\n\nE1(x(j−1)\n\ni\n\n(cid:17) )\n\n,\n\ng(j−1)\n\nk\n\n= ReLU\n\n(cid:16)\n\nE2(x(j−1)\n\nk\n\n)\n\n(cid:17)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nand concatenate them to compute a pre-attention coefficient α(j) ik for all (i, k) ∈ E: (cid:17) ]\n\n|| g(j−1)\n\n[h(j−1)\n\nα(j)\n\n(cid:16)\n\n.\n\nik = M\n\ni\n\nk\n\nThen, constructing a matrix S(j) where\n\nS(j)\n\nik =\n\n(cid:40)\n\nLeakyReLU(α(j) ik ) −∞\n\nif(i, k) ∈ E, otherwise,\n\nwe define (cid:98)A(j) through a row-wise softmax:\n\n(cid:98)A(j) = softmax(S(j)). This parameterization differs from GAT mainly in using more complex embedding networks E1 and E2 than a single feed-forward layer to compute the vectors h(j−1) . Moreover, we do not use multiple heads.\n\nand g(j−1)\n\nk\n\ni\n\nB.2 GC-FLOW-L VARIANT\n\nThe GC-Flow-l variant learns a new graph structure. For computational efficiency, the learning of the structure is based on the given edge set E; that is, only edges are removed from E but no edges are inserted outside E. The method follows Luo et al. (2021), which hypothesizes that the existing edge set is noisy and aims at removing the noisy edges.\n\nThe basic idea uses a prior work on differentiable sampling (Maddison et al., 2016; Jang et al., 2016), which states that the random variable (cid:18)(cid:16)\n\n(cid:19)\n\n(cid:17)\n\nlog (cid:15) − log (cid:0)1 − (cid:15)(cid:1) + ω\n\n/τ\n\nwhere\n\n(cid:15) ∼ Uniform (cid:0)0, 1(cid:1)\n\ne = σ\n\n(10)\n\nfollows a distribution that converges to a Bernoulli distribution with success probability p = (1 + e−ω)−1 as τ > 0 tends to zero. Hence, we if parameterize ω and specify that the presence of an edge between a pair of nodes has probability p, then using e computed from (10) to fill the corresponding entry of (cid:98)A will produce a matrix (cid:98)A that is close to binary. We can use this matrix in GC-Flow, with the hope of improving classification/clustering performance due to the ability of denoising edges. Moreover, because (10) is differentiable with respect to ω, we can train the parameters of ω like in a usual gradient-based training. To this end, we let E1, E2 : RD → Rd be two embedding networks that embed the pairwise flow inputs as\n\na(j)\n\nik = tanh\n\nb(j)\n\nik = tanh\n\n(cid:16)\n\n(cid:16)\n\nE1\n\n(cid:0)x(j)\n\ni\n\nE2\n\n(cid:0)x(j)\n\ni\n\n(cid:1)(cid:17)\n\n(cid:1)(cid:17)\n\n(cid:12) tanh\n\n(cid:12) tanh\n\n(cid:16)\n\n(cid:16)\n\nE2\n\n(cid:0)x(j)\n\nk\n\nE1\n\n(cid:0)x(j)\n\nk\n\n(cid:1)(cid:17)\n\n(cid:1)(cid:17)\n\n, ∀ (i, k) ∈ E\n\n, ∀ (i, k) ∈ E\n\nwhere a(j)\n\nik , b(j)\n\nik ∈ Rd and (cid:12) is the Hadamard product. Then, we take their difference and compute\n\nω(j)\n\nik = tanh\n\n(cid:16)\n\n1T (a(j)\n\nik − b(j) ik )\n\n(cid:17)\n\n,\n\nfollowed by\n\n(cid:18)(cid:16)\n\n(cid:98)e(j) ik = σ\n\nlog (cid:15) − log (cid:0)1 − (cid:15)(cid:1) + ω(j)\n\nik\n\n(cid:19)\n\n(cid:17)\n\n/τ\n\nwhere\n\n(cid:15) ∼ Uniform (cid:0)0, 1(cid:1),\n\nwhich returns an approximate Bernoulli sample for the edge (i, k). When τ is not sufficiently close to zero, this sample may not be close enough to binary, and in particular, it is strictly nonzero. To explicitly zero out an edge, we follow Louizos et al. (2017) and introduce two parameters, γ < 0 and ξ > 1, to remove small values of (cid:98)e(j) ik : ik , 0(cid:1)(cid:17) 1, max (cid:0)e(j) This definition of (cid:98)A(j) does not insert new edges to the graph (i.e., when (i, k) /∈ E, (cid:98)A(j) only removes (denoises) some edges (i, k) originally in E.\n\nik (ξ − γ) + γ.\n\nik = (cid:98)e(j) e(j)\n\nik = 0), but\n\nik = min\n\n(cid:98)A(j)\n\nwhere\n\n(cid:16)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC TRAINING AND INFERENCE\n\nDespite inheriting the generative characteristics of FlowGMMs (including the training loss), GCFlows are by nature a GNN, because the graph convolution operation ( (cid:98)A-multiplication) involves a node’s neighbor set when computing the output of a constituent flow for this node. Across all constituent flows, the evaluation of the loss on a single node will require the information of the entire T -hop neighborhood, causing scalability challenges for large graphs. One may perform fullbatch training (the deterministic gradient decent method), which minimizes the multiple evaluations on a node in any constituent flow. Such an approach is the most convenient to implement in the current deep learning frameworks; typical GPU memory can afford handling a medium-scale graph and CPU memory can afford even larger graphs. If one opts to perform mini-batch training (the stochastic gradient decent method), neighborhood sampling for GNNs (e.g., node-wise (Hamilton et al., 2017; Ying et al., 2018), layer-wise (Chen et al., 2018a; Zou et al., 2019), or subgraphsampling (Chiang et al., 2019; Zeng et al., 2020)) is a popular approach to reducing the computation within the T -hop neighborhood.\n\nInference is faced with the same challenge as training, but since it requires only a single pass on the test set, doing so in full batch or by using large mini-batches may suffice. If one were to use normal mini-batches with neighborhood sampling (for reasons such as being consistent with training), empirical evidence of success has been demonstrated in the GNN literature (Kaler et al., 2022).\n\nD COMPLEXITY ANALYSIS\n\nLet us analyze the cost of computing the likelihood loss (5) for GC-Flows. Based on (8) and (9), the cost consists of three parts: that to compute (cid:101)X(j) = (cid:98)AX(j−1) for each constituent flow indexed by j, that to compute the Jacobian determinant det ∇fj((cid:101)x(j) i ) for each node i, and that to compute the graph-related determinant det (cid:98)A. For a fixed graph, the third part is a constant and is omitted in training. The cost of the second part varies according to the type of the flow. If we use an affinecoupling flow as exemplified in Section 3.1, let the cost of the s and t networks be Cst. Then, the cost of computing the overall loss can be summarized as\n\nO(cid:0) nz( (cid:98)A)DT + nT Cst where nz( (cid:98)A) denotes the number of nonzeros of (cid:98)A and recall that D and T are the feature dimension and the number of flows, respectively. An affine-coupling flow can be implemented with varying (cid:1), where h0 = (cid:98)D/2(cid:99); architectures. For example, for a usual MLP, Cst = O(cid:0) (cid:80)L−1 h1 . . . hL−1 are hidden dimensions; and hL = 2 (cid:100)D/2(cid:101). Note that O(Cst) dominates the cost of computing a matrix determinant, which is only O(D), because the Jacobian matrix is triangular in an affine-coupling flow.\n\ni=0 hihi+1\n\n(11)\n\n(cid:1),\n\nIt would be useful to compare the above cost with that of the cross-entropy loss for a usual GCN:\n\nO(cid:0) nz( (cid:98)A) (cid:80)T −1\n\nj=0 dj + n (cid:80)T −1\n\nj=0 djdj+1\n\n(cid:1),\n\n(12)\n\nwhere d0 = D; d1 . . . dT −1 are hidden dimensions; and dT = K, the number of classes. Here, we assume that the GCN has T layers, comparable to GC-Flow. The two costs (11) and (12) are comparable, part by part. For the first part, DT is comparable to (cid:80)T −1 j=0 dj, if all the dj’s are similar. In some data sets, the input dimension of GCN is much higher than the hidden and output dimensions, but we correspondingly perform a dimension reduction on the input features when running GC-Flows (as is the practice in the experiments), reducing DT to D(cid:48)T for some D(cid:48) (cid:28) D. For the second part, the number L of hidden layers in each flow is typically a small number (say, 5), and the hidden dimensions hj’s are comparable to the input dimension h0, rendering comparable terms T Cst versus (cid:80)T −1 j=0 djdj+1. Overall, the computational costs of GC-Flow and GCN are similar and their scaling behaviors are the same.\n\nIt is worth noting that when one parameterizes (cid:98)A, the cost of computing (cid:98)A(j) for each flow/layer j will need to be added to the loss computation, for both GC-Flows and GCNs. The cost depends on the specific parameterization and it can be either cheap or expensive. Additionally, GC-Flows require the computation of det (cid:98)A(j), whose cost depends on the structure of the matrix, which in turn is determined by the parameterization.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nE EXPERIMENT DETAILS\n\nData sets. Table 4 summarizes the statistics of the benchmark data sets used in this paper.\n\nTable 4: Data set statistics.\n\nData set Cora Citeseer Pubmed Computers Photo Wiki-CS\n\n# Nodes 2,708 3,327 19,717 13,381 7,487 11,701\n\n# Edges 5,429 4,732 44,338 245,778 119,043 216,123\n\n# Features 1,433 3,703 500 767 745 300\n\n# Classes 7\n6 3\n10 8\n10\n\n# Train/val/test 140 / 500 / 1,000 120 / 500 / 1,000 60 / 500 / 1,000 200 / 1,300 / 1,000 80 / 620 / 1,000 580 / 1,769 / 5,847\n\nComputing environment. We implemented all models using PyTorch (Paszke et al., 2019), PyTorch Geometric (Fey & Lenssen, 2019), and Scikit-learn (Pedregosa et al., 2011). All data sets used in the experiments are obtained from PyTorch Geometric. We conduct the experiments on a server with four NVIDIA RTX A6000 GPUs (48GB memory each).\n\nImplementation details. For fair comparison, we run all models on the entire data set under the transductive semi-supervised setting. All models are initialized with Glorot initialization (Glorot & Bengio, 2010) and are trained using the Adam optimizer (Kingma & Ba, 2015). For reporting the silhouette coefficient, k-means is run for 1000 epochs. For all models on all data sets, the (cid:96)2 weight decay factor is set to 5 × 10−4 and the number of training epochs is set to 400. For all models, we use the early stopping strategy on the F1 score on validation set. In all experiments, we use (cid:98)A = (D + I)−1(A + I), where D = diag((cid:80) j Aij). For FlowGMM, GC-Flow, and its variants, we clip the norm of the gradients to with the range [−50, 50]. For GC-Flow-l and GCFlow-p, since (cid:98)A(j) in each flow may not has a full rank, we add to (cid:98)A(j) a diagonal matrix with damping value 10−3. Moreover, the slope in LeakyReLU is set to 0.2. In all models involving normalizing flows, we use RealNVP (Dinh et al., 2017) with coupling layers implemented by using MLPs. Following Izmailov et al. (2020), the mean vectors are parameterized as some scalar multiple of the vector of all ones, and the covariance matrices are parameterized as some scalar multiple of the identity matrix. On the other hand, the GMM models are implemented by using Scikit-learn with full covariance matrices. The number of training epochs for GMMs is set to 200.\n\nToo large a feature dimension renders a challenge on the training of a normalizing flow. Hence, we perform dimension reduction in such a case. For Cora, Pubmed, Computers, and Photo, we use PCA to reduce the feature dimension to 50; and for Citeseer, to 100. We keep the dimension 300 on Wiki-CS without feature reduction.\n\nHyperparameters. We use grid search to tune the hyperparameters of FlowGMM, GC-Flow, and its variants. The search spaces are listed in the following:\n\n• Number of flow layers: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20;\n\n• Number of dense layers in each flow: 6, 10, 14;\n\n• Hidden size of flow layers: 128, 256, 512, 1024;\n\n• Weighting parameter λ: 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5;\n\n• Gaussian mean and covariance scale: [0.5, 10];\n\n• Initial learning rate: 0.001, 0.002, 0.003, 0.005;\n\n• Dropout rate: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6.\n\nAdditionally, for GC-Flow-p and GC-Flow-l:\n\n• Number of dense layers in E1 and E2: 4, 6;\n\n• Hidden size of dense layers in E1 and E2: 128, 256;\n\n• Embedding dimension d: 8, 16.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFor GCN and GraphSAGE, we set the hidden size to 128, dropout rate to 0.5, and learning rate to 0.01. For GAT, we follow Veliˇckovi ́c et al. (2018) to set the hidden size to 8, the number of attention heads to 8, droupout rate to 0.6, and the learning rate to 0.005.\n\nThe results in Table 1 for GC-Flow are obtained by using different number of flows and number of layers per flow for different data sets. For Computers, Photo, and Wiki-CS, we use 4, 2, and 2 flows, respectively, with 6 dense layers in each flow. For Cora, Citeseer and Pubmed, we use 4, 10 and 10 flows, respectively, with 10 dense layers in each flow.\n\nThe results in Table 3 also use different number of flows and layers. For GC-Flow-p, on Cora, Citeseer, Pubmed, Photo, and Wiki-CS, we use 10 flows; while on Computers, we use 6 flows. For GC-Flow-1, we use 2 flows for Wiki-CS and 4 flows for the rest of the data sets. For both For GCFlow-p and GC-Flow-l, there are 6 dense layers per flow on Wiki-CS while 10 per flow on the rest of the data sets.\n\nF ADDITIONAL EXPERIMENT RESULTS\n\nClustering performance. Table 5 compares the clustering performance between GC-Flow and various contrastive-based GNN methods, for Citeseer and Pubmed. GC-Flow maintains the attractively best performance on the silhouette score, which measures cluster separation. For the cluster assignment metrics, GC-Flow remains competitive on ARI, which measures cluster similarity, while falling back on NMI, which measures cluster agreement.\n\nTable 5: Clustering performance of various GNN methods. The two best cases are boldfaced. Data sets: Citeseer (top) and Pubmed (bottom).\n\nNMI 0.427 ± 0.001 DGI 0.380 ± 0.017 GRACE 0.336 ± 0.008 GCA 0.417 ± 0.001 GraphCL 0.468 ± 0.002 MVGRL GC-Flow 0.405 ± 0.013\n\nNMI 0.379 ± 0.001 DGI 0.305 ± 0.075 GRACE 0.488 ± 0.016 GCA 0.337 ± 0.001 GraphCL 0.391 ± 0.001 MVGRL GC-Flow 0.384 ± 0.011\n\nARI 0.399 ± 0.002 0.378 ± 0.023 0.279 ± 0.005 0.372 ± 0.002 0.445 ± 0.004 0.426 ± 0.014\n\nARI 0.361 ± 0.002 0.261 ± 0.111 0.511 ± 0.030 0.308 ± 0.002 0.361 ± 0.001 0.460 ± 0.015\n\nSilhouette 0.314 ± 0.000 0.219 ± 0.014 0.251 ± 0.015 0.299 ± 0.001 0.305 ± 0.000 0.538 ± 0.022\n\nSilhouette 0.426 ± 0.001 0.149 ± 0.008 0.354 ± 0.014 0.412 ± 0.002 0.441 ± 0.001 0.655 ± 0.013\n\nTraining behavior. Figure 5 plots the convergence of the training loss for various methods. The loss for GCN is the cross-entropy while the loss for FlowGMM and GC-Flow is the likelihood. All the curves are smooth and exhibit reasonable decay, suggesting convergence as expected. GC-Flow converges similarly to FlowGMM, while GCN converges faster.\n\nVisualization of the representation space. Figure 6 shows the t-SNE plots for data sets Citeseer, Computers, Photo, and Wiki-CS, one per row.\n\nFigure 5: Convergence of the training loss (Cora).\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) FlowGMM\n\n(b) GCN\n\n(c) GC-Flow\n\n(d) FlowGMM\n\n(e) GCN\n\n(f) GC-Flow\n\n(g) FlowGMM\n\n(h) GCN\n\n(i) GC-Flow\n\n(j) FlowGMM\n\n(k) GCN\n\n(l) GC-Flow\n\nFigure 6: Representation space of several data sets under different models. Data set from top to bottom: Citeseer, Computers, Photo, and Wiki-CS.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a method to replace a GNN layer by a combination of graph convolutions and normalizing flows under a Gaussian mixture representation space. The proposed method not only classifies well, but also yields node representations that capture the inherent structure of data, as a result forming high-quality clusters (i.e. cluster well).\n\n# Strength And Weaknesses\n\nStrength: \nThe paper proposes a coherent story, etc\n\nWeakness:\n1 replace GNN layer with graph convolutions & normalizing flows will significantly add more computational burden? especially inverse of Jacobians? Will the normalizing flow harder to train due to  intermediate stage of graph functions' ill behaviour (due to graph convolution), leading to instability and divergence of the algorithm? \nHave the author consider other priors than gaussian mixture?\n\n2. The paper reports microF1 score in Table 1\n\nHow does the paper’s performance compare with leaderboard here\nhttps://paperswithcode.com/sota/node-classification-on-pubmed\nfor example, pubmed acc is 91+ \n\nhow does performance compare with https://arxiv.org/pdf/2109.05641v1.pdf\nhttps://arxiv.org/pdf/2012.06113.pdf (Table 3)\nseems much better than Table 1 of the paper. \nCan the author clarify?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe author provide a quite complete story\n\n# Summary Of The Review\n\nI am not fully on board on the motivation of this and the results section.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCORRELATIVE INFORMATION MAXIMIZATION BASED BIOLOGICALLY PLAUSIBLE NEURAL NETWORKS FOR CORRELATED SOURCE SEPARATION\n\nBariscan Bozkurt1,2 Ates Isfendiyaroglu3 Cengiz Pehlevan4 Alper T. Erdogan1,2 1KUIS AI Center, Koc University, Turkey 3 Uskudar American Academy 4John A. Paulson School of Engineering & Applied Sciences and Center for Brain Science, Harvard University, Cambridge, MA 02138, USA {bbozkurt15, alperdogan}@ku.edu.tr cpehlevan@seas.harvard.edu\n\n2EEE Department, Koc University, Turkey\n\nABSTRACT\n\nThe brain effortlessly extracts latent causes of stimuli, but how it does this at the network level remains unknown. Most prior attempts at this problem proposed neural networks that implement independent component analysis, which works under the limitation that latent causes are mutually independent. Here, we relax this limitation and propose a biologically plausible neural network that extracts correlated latent sources by exploiting information about their domains. To derive this network, we choose the maximum correlative information transfer from inputs to outputs as the separation objective under the constraint that the output vectors are restricted to the set where the source vectors are assumed to be located. The online formulation of this optimization problem naturally leads to neural networks with local learning rules. Our framework incorporates infinitely many set choices for the source domain and flexibly models complex latent structures. Choices of simplex or polytopic source domains result in networks with piecewise-linear activation functions. We provide numerical examples to demonstrate the superior correlated source separation capability for both synthetic and natural sources.\n\n1\n\nINTRODUCTION\n\nExtraction of latent causes, or sources, of complex stimuli sensed by sensory organs is essential for survival. Due to absence of any supervision in most circumstances, this extraction must be performed in an unsupervised manner, a process which has been named blind source separation (BSS) (Comon & Jutten, 2010; Cichocki et al., 2009).\n\nHow BSS may be achieved in visual, auditory, or olfactory cortical circuits has attracted the attention of many researchers, e.g. (Bell & Sejnowski, 1995; Olshausen & Field, 1996; Bronkhorst, 2000; Lewicki, 2002; Asari et al., 2006; Narayan et al., 2007; Bee & Micheyl, 2008; McDermott, 2009; Mesgarani & Chang, 2012; Golumbic et al., 2013; Isomura et al., 2015). Influential papers showed that visual and auditory cortical receptive fields could arise from performing BSS on natural scenes (Bell & Sejnowski, 1995; Olshausen & Field, 1996) and sounds (Lewicki, 2002). The potential ubiquity of BSS in the brain suggests that there exists generic neural circuit motifs for BSS (Sharma et al., 2000). Motivated by these observations, here, we present a set of novel biologically plausible neural network algorithms for BSS.\n\nBSS algorithms typically derive from normative principles. The most important one is the information maximization principle, which aims to maximize the information transferred from input mixtures to separator outputs under the restriction that the outputs satisfy a specific generative assumption about sources. However, Shannon mutual information is a challenging choice for quantifying information transfer, especially for data-driven adaptive applications, due to its reliance on the joint and conditional densities of the input and output components. This challenge is eased by the independent component analysis (ICA) framework by inducing joint densities into separable forms based on the assumption of source independence (Bell & Sejnowski, 1995). In particular scenarios,\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthe mutual independence of latent causes of real observations may not be a plausible assumption (Tr ̈auble et al., 2021). To address potential dependence among latent components, Erdogan (2022) recently proposed the use of the second-order statistics-based correlative (log-determinant) mutual information maximization for BSS to eliminate the need for the independence assumption, allowing for correlated source separation.\n\nIn this article, we propose an online correlative information maximization-based biologically plausible neural network framework (CorInfoMax) for the BSS problem. Our motivations for the proposed framework are as follows:\n\n• The correlative mutual information objective function is only dependent on the second-order statistics of the inputs and outputs. Therefore, its use avoids the need for costly higher-order statistics or joint pdf estimates,\n\n• The corresponding optimization is equivalent to maximization of correlation, or linear depen-\n\ndence, between input and output, a natural fit for the linear inverse problem,\n\n• The framework relies only on the source domain information, eliminating the need for the source independence assumption. Therefore, neural networks constructed with this framework are capable of separating correlated sources. Furthermore, the CorInfoMax framework can be used to generate neural networks for infinitely many source domains corresponding to the combination of different attributes such as sparsity, nonnegativity etc.,\n\n• The optimization of the proposed objective inherently leads to learning with local update rules. • CorInfoMax acts as a unifying framework to generate biologically plausible neural networks for various unsupervised data decomposition methods to obtain structured latent representations, such as nonnegative matrix factorization (NMF) (Fu et al., 2019), sparse component analysis (SCA) (Babatas & Erdogan, 2018), bounded component analysis (BCA) (Erdogan, 2013; Inan & Erdogan, 2014) and polytopic matrix factorization (PMF) (Tatli & Erdogan, 2021).\n\n(a)\n\n(b)\n\nFigure 1: CorInfoMax BSS neural networks for two different canonical source domain representations. xi’s and yi’s represent inputs (mixtures) and (separator) outputs , respectively, W are feedforward weights, ei’s are errors between transformed inputs and outputs, By, the inverse of output autocorrelation matrix, represents lateral weights at the output. For the canonical form (a), λi’s are Lagrangian interneurons imposing source domain constraints, AP (AT P ) represents feedforward (feedback) connections between outputs and interneurons. For the canonical form (b), interneurons on the right impose sparsity constraints on the subsets of outputs.\n\nFigure 1 illustrates CorInfoMax neural networks for two different source domain representation choices, which are three-layer neural networks with piecewise linear activation functions.We note that the proposed CorInfoMax framework, beyond solving the BSS problem, can be used to learn structured and potentially correlated representations from data through the maximum correlative information transfer from inputs to the choice of the structured domain at the output.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n1.1 RELATED WORK AND CONTRIBUTIONS\n\n1.1.1 BIOLOGICALLY PLAUSIBLE NEURAL NETWORKS FOR BSS\n\nThere are different methods to solve the BSS problem through neural networks with local learning rules. These methods are differentiated on the basis of the observation models they assume and the normative approach they propose. We can list biologically plausible ICA networks as an example category, which are based on the exploitation of the presumed mutual independence of sources (Isomura & Toyoizumi, 2018; Bahroun et al., 2021; Lipshutz et al., 2022). There exist alternative approaches which exploit different properties of the data model to replace the independence assumption with a weaker one. As an example, Pehlevan et al. (2017a) uses the nonnegativeness property along with the biologically inspired similarity matching (SM) framework (Pehlevan et al., 2017b) to derive biologically plausible neural networks that are capable of separating uncorrelated but potentially dependent sources. Similarly, Erdogan & Pehlevan (2020) proposes bounded similarity matching (BSM) as an alternative approach that takes advantage of the magnitude boundedness property for uncorrelated source separation. More recently, Bozkurt et al. (2022) introduced a generic biologically plausible neural network framework based on weighted similarity matching (WSM) introduced in Erdogan & Pehlevan (2020) and maximization of the output correlation determinant criterion used in the NMF, SCA, BCA and PMF methods. This new framework exploits the domain structure of sources to generate two- / three-layer biologically plausible networks that have the ability to separate potentially correlated sources. Another example of biologically plausible neural networks with correlated source separation capability is offered in Simsek & Erdogan (2019), which also uses the determinant maximization criterion for the separation of antisparse sources.\n\nOur proposed framework differs significantly from Bozkurt et al. (2022): Bozkurt et al. (2022) uses the similarity matching criterion, which is not employed in our framework, as the main tool for generating biologically plausible networks. Therefore, the resulting network structure and learning rules are completely different from Bozkurt et al. (2022). For example, the lateral connections of the outputs in Bozkurt et al. (2022) are based on the output autocorrelation matrix, while the lateral connections for our proposed framework are based on the inverse of the output autocorrelation matrix. Unlike our framework, neurons in Bozkurt et al. (2022) have learnable gains. The feedforward weights of the networks in Bozkurt et al. (2022) correspond to a cross-correlation matrix between the inputs and outputs of a layer, whereas, for our proposed framework, the feedforward connections correspond to the linear predictor of the output from the input. The feedback connection matrix in Bozkurt et al. (2022) is the transpose of the feedforward matrix which is not the case in our proposed framework. Compared to the approach in Simsek & Erdogan (2019), our proposed framework is derived from information-theoretic grounds, and its scope is not limited to antisparse sources, but to infinitely many different source domains.\n\n1.1.2\n\nINFORMATION MAXIMIZATION FOR UNSUPERVISED LEARNING\n\nThe use of Shannon’s mutual information maximization for various unsupervised learning tasks dates back to a couple of decades. As one of the pioneering applications, we can list Linsker’s work on self-organizing networks, which proposes maximizing mutual information between input and its latent representation as a normative approach (Linsker, 1988). Under the Gaussian assumption, the corresponding objective simplifies to determinant maximization for the output covariance matrix. Becker & Hinton (1992) suggested maximizing mutual information between alternative latent vectors derived from the same input source as a self-supervised method for learning representations. The most well-known application of the information maximization criterion to the BSS problem is the ICA-Infomax approach by Bell & Sejnowski (1995). The corresponding algorithm maximizes the information transferred from the input to the output under the constraint that the output components are mutually independent. For potentially correlated sources, Erdogan (2022) proposed the use of a second-order statistics-based correlative (or log-determinant) mutual information measure for the BSS problem. This approach replaces the mutual independence assumption in the ICA framework with the source domain information, enabling the separation of both independent and dependent sources. Furthermore, it provides an information-theoretic interpretation for the determinant maximization criterion used in several unsupervised structured matrix factorization frameworks such as NMF (or simplex structured matrix factorization (SSMF)) (Chan et al., 2011; Lin et al., 2015; Fu et al., 2018; 2019), SCA, BCA, and PMF. More recently, Ozsoy et al. (2022)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nproposed maximization of the correlative information among latent representations corresponding to different augmentations of the same input as a self-supervised learning method.\n\nThe current article offers an online optimization formulation for the batch correlative information maximization method of Erdogan (2022) that leads to a general biologically plausible neural network generation framework for the unsupervised unmixing of potentially dependent/correlated sources.\n\n2 PRELIMINARIES\n\nThis section aims to provide background information for the CorInfoMax-based neural network framework introduced in Section 3. For this purpose, we first describe the BSS setting assumed throughout the article in Section 2.1. Then, in Section 2.2, we provide an essential summary of the batch CorInfoMax-based BSS approach introduced in (Erdogan, 2022).\n\n2.1 BLIND SOURCE SEPARATION SETTING\n\nSOURCES: We assume a BSS setting with a finite number of n-dimensional source vectors, represented by the set S = {s(1), s(2), . . . , s(N )} ⊂ P, where P is a particular subset of Rn. The choice of source domain P determines the identifiability of the sources from their mixtures, the properties of the individual sources and their mutual relations. Structured unsupervised matrix factorization methods are usually defined by the source/latent domain, such as\n\ni. Normalized nonnegative sources in the NMF(SSMF) framework: ∆ = {s | s ≥ 0, 1T s = 1}. Signal processing and machine learning applications such as hyperspectral unmixing and text mining (Abdolali & Gillis, 2021), (Fu et al., 2016).\n\nii. Bounded antisparse sources in the BCA framework: Bl∞ = {s | ∥s∥∞ ≤ 1}. Applications\n\ninclude digital communication signals Erdogan (2013).\n\niii. Bounded sparse sources in the SCA framework: Bl1 = {s | ∥s∥1 ≤ 1}. Applications: modeling efficient representations of stimulus such as vision Olshausen & Field (1997) and sound Smith & Lewicki (2006).\n\niv Nonnegative bounded antiparse sources in the nonnegative-BCA framework: Bl∞,+ = Bl∞ ∩Rn +.\n\nApplications include natural images Erdogan (2013).\n\nv. Nonnegative bounded sparse sources in the nonnegative-SCA framework: Bl1,+ = Bl1 ∩ Rn +.\n\nPotential applications similar to ∆ in (i).\n\nNote that the sets in (ii)-(v) of the above list are the special cases of (convex) polytopes. Recently, Tatli & Erdogan (2021) showed that infinitely many polytopes with a certain symmetry restriction enable identifiability for the BSS problem. Each identifiable polytope choice corresponds to different structural assumptions on the source components. A common canonical form to describe polytopes is to use the H-representation Gr ̈unbaum et al. (1967):\n\nP = {y ∈ Rn|AP y ≼ bP },\n\n(1)\n\nwhich corresponds to the intersection of half-spaces. Alternatively, similar to Bozkurt et al. (2022), we can consider a subset of polytopes, which we refer to as feature-based polytopes, defined in terms of attributes (such as non-negativity and sparseness) assigned to the subsets of components: P = (cid:8)s ∈ Rn | si ∈ [−1, 1] ∀i ∈ Is, si ∈ [0, 1] ∀i ∈ I+, ∥sJl ∥1 ≤ 1, Jl ⊆ Z+ where Is ⊆ Z+ n is the set of indexes for signed sources, and I+ is its complement, sJl is the subvector constructed from the elements with indices in Jl, and L is the number of sparsity constraints imposed on the sub-vector level. In this article, we consider both polytope representations above.\n\nn , l ∈ Z+\n\n(cid:9) ,\n\n(2)\n\nL\n\nMIXING: We assume a linear generative model, that is, the source vectors are mixed through an unknown matrix A ∈ Rm×n, x(i) = As(i), ∀i = 1, . . . , N , where we consider the overdetermined case, that is, m ≥ n and rank(A) = n. We define X = [ x(1)\n\n. . . x(N ) ].\n\nSEPARATION: The purpose of the source separation setting is to recover the original source matrix S from the mixture matrix X up to some scaling and/or permutation ambiguities, that is, the separator output vectors {y(i)} satisfy y(i) = W x(i) = ΠΛs(i), for all i = 1, . . . , N , where W ∈ Rn×m is the learned separator matrix, Π is a permutation matrix, Λ is a full-rank diagonal matrix and y(i) refers to the estimate of the source of the sample index i.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n2.2 CORRELATIVE MUTUAL INFORMATION MAXIMIZATION FOR BSS\n\nErdogan (2022) proposes maximizing the (correlative) information flow from the mixtures to the separator outputs, while the outputs are restricted to lie in their presumed domain P. The corresponding batch optimization problem is given by\n\nI (ε) LD (X, Y ) =\n\nmaximize Y ∈ Rn×N subject to Y:,i ∈ P, i = 1, . . . , N,\n\n1 2\n\nlog det( ˆRy + εI) −\n\n1 2\n\nlog det( ˆRe + εI)\n\n(3a)\n\n(3b)\n\nN 1N 1T\n\nN Y (IN − 1\n\ni.e., ˆRy = 1\n\nxy( ˆRx + εI)−1 ˆRyx where ˆRxy is the sample cross-correlation, i.e., ˆRxy = 1\n\nwhere the objective function I (ε) LD (X, Y ) is the log-determinant (LD) mutual information1 between the mixture and the separator output vectors (see Appendix A.1 and Erdogan (2022) for more information), ˆRy is the sample autocorrelation, N Y Y T , (or autocovariN )Y T ) matrix for the separator output vector, ˆRe equals ance, i.e., ˆRy = 1 ˆRy − ˆRT N XY T (or cross-covariance ˆRxy = 1 N )Y T ) matrix between mixture and output vectors, and ˆRx is the sample autocorrelation (or autocovariance) matrix for the mixtures. As discussed in Appendix A.1, for sufficiently small ε, ˆRe is the sample autocorrelation (covariance) matrix of the error vector corresponding to the best linear (affine) minimum mean square error (MMSE) estimate of the separator output vector y, from the mixture vector x. Under the assumption that the original source samples are sufficiently scattered in P, (Fu et al., 2019; Tatli & Erdogan, 2021), i.e., they form a maximal LD-entropy subset of P, (Erdogan, 2022), then the optimal solution of (3) recovers the original sources up to some permutation and sign ambiguities, for sufficiently small ε.\n\nN X(IN − 1\n\nN 1N 1T\n\nThe biologically plausible CorInfoMax BSS neural network framework proposed in this article is obtained by replacing batch optimization in (3) with its online counterpart, as described in Section 3.\n\n3 METHOD: BIOLOGICALLY PLAUSIBLE NEURAL NETWORKS FOR\n\nCORRELATIVE INFORMATION MAXIMIZATION\n\n3.1 ONLINE OPTIMIZATION SETTING FOR LD-MUTUAL INFORMATION MAXIMIZATION\n\nWe start our online optimization formulation for CorInfoMax by replacing the output and error sample autocorrelation matrices in (3a) with their weighted versions\n\nˆRζy\n\ny (k) =\n\n1 − ζy 1 − ζ k y\n\nk (cid:88)\n\ni=1\n\ny y(i)y(i)T ζ k−i\n\nˆRζe\n\ne (k) =\n\n1 − ζe 1 − ζ k e\n\nk (cid:88)\n\ni=1\n\ne e(i)e(i)T , ζ k−i\n\n(4)\n\nwhere 0 ≪ ζy < 1 is the forgetting factor, W (i) is the best linear MMSE estimator matrix (to estimate y from x), and e(k) = y(i) − W (i)x(i) is the corresponding error vector. Therefore, we can define the corresponding online CorInfoMax optimization problem as\n\nmaximize y(k) ∈ Rn\n\nJ (y(k)) =\n\nsubject to yk ∈ P.\n\n1 2\n\nlog det( ˆRζy\n\ny (k) + εI) −\n\n1 2\n\nlog det( ˆRζe\n\ne (k) + εI)\n\n(5a)\n\n(5b)\n\nNote that the above formulation assumes knowledge of the best linear MMSE matrix W (i), whose update is formulated as a solution to an online regularized least squares problem,\n\nmaximize W (i) ∈ Rm×n\n\nμW ∥y(i) − W (i)x(i)∥2\n\n2 + ∥W (i) − W (i − 1)∥2 F .\n\n(6a)\n\n3.2 DESCRIPTION OF THE NETWORK DYNAMICS FOR SPARSE SOURCES\n\nWe now show that the gradient-ascent-based maximization of the online CorInfoMax objective in (5) corresponds to the neural dynamics of a multilayer recurrent neural network with local learning\n\n1In this article, we use “correlative mutual information” and “LD-mutual information” interchangeably.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nrules. Furthermore, the presumed source domain P determines the output activation functions and additional inhibitory neurons. For an illustrative example, in this section, we concentrate on the sparse special case in Section 2.1, that is, P = Bl1 . We can write the corresponding Lagrangian optimization setting as\n\nminimize λ≥0\n\nmaximize y(k)∈Rn\n\nL(y(k), λ(k)) = J (y(k)) − λ(k)(∥y(k)∥1 − 1).\n\n(7)\n\nTo derive network dynamics, we use the proximal gradient update (Parikh et al., 2014) for y(k) with the expression (A.14) for ∇y(k)J (y(k)), derived in Appendix B, and the projected gradient descent update for λ(k) using ∇λL(y(k), λ(k)) = 1 − ∥y(k; ν + 1)∥1, leading to the following iterations: (8)\n\ne(k; ν) = y(k; ν) − W (k)x(k)\n\n∇y(k)J (y(k; ν)) = γy(k)Bζy\n\ny(k; ν + 1) = STλ(k;ν) (cid:18)\n\ny (k − 1)y(k; ν) − γe(k)Bζe (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))(cid:1)\n\ne (k − 1)e(k; ν),\n\n(cid:19)\n\nλ(k; ν + 1) = ReLU\n\nλ(k; ν) − ηλ(ν)(1 − ∥y(k; ν + 1)∥1)\n\n,\n\n(9)\n\n(10)\n\n(11)\n\ny (k) and Bζe\n\nwhere Bζy e (k −1) respectively, γy(k) and γe(k) are provided in (A.11) and (A.13), ν ∈ N is the iteration index, ηy(ν) and ηλ(ν) are the learning rates for the output and λ, respectively, at iteration ν, ReLU(·) is the rectified linear unit, and STλ(.) is\n\ne (k) are inverses of Rζy\n\ny (k −1) and Rζe\n\nthe soft-thresholding nonlinearity defined as STλ(y)i =\n\n(cid:26)\n\n0\n\n|yi| ≤ λ, yi − sign(yi)λ otherwise\n\n. We rep-\n\nresent the values in the final iteration νf inal, with some abuse of notation, with y(k) = y(k; νf inal) and e(k) = e(k; νf inal).\n\nThe neural dynamic iterations in (8)- (11) define a recurrent neural network, where W (k) represents feedforward synaptic connections from the input x(k) to the error e(k), Bζe e (k) represents feedforward connections from the error e(k) to the output y(k), and Bζy y (k) corresponds to lateral synaptic connections among the output components. Next, we examine the learning rules for this network.\n\nUpdate of inverse correlation matrices Bζy by applying matrix inversion lemma to ( ˆRζy Appendix B to obtain (A.10) and (A.12)\n\ny (k) and Bζe\n\ne (k): We can obtain the update expressions e (k) + εI)−1, as derived in\n\ny (k) + εI)−1 and ( ˆRζe\n\nBζy\n\ny (k + 1) =\n\nBζe\n\ne (k + 1) =\n\n1 − ζ k y\nζy − ζ k y\n1 − ζ k e\nζe − ζ k e\n\n(Bζy\n\ny (k) − γy(k)Bζy\n\ny (k)y(k)y(k)T Bζy\n\ny (k)),\n\n(Bζe\n\ne (k) − γe(k)Bζe\n\ne (k)e(k)e(k)T Bζe\n\ne (k)).\n\n(12)\n\n(13)\n\nHowever, note that (12), and (13) violate biological plausibility, since the multiplier γy and γe depend on all output and error components contrasting the locality. Furthermore, the update in (13) is not local since it is only a function of the feedforward signal ze(k) = Bζe e (k)e(k) entering into output neurons: the update of [Bζe e ]ij, the synaptic connection between the output neuron i and the error neuron j requires [ze]j which is a signal input to the output neuron j. To modify the updates (12) and (13) into a biologically plausible form, we make the following observations and assumptions:\n\n1 ε\n\n• ˆRζe\n\ne (k) + εI ≈ εI ⇒ Bζe\n\ne (k + 1) ≈\n\nI, which is a reasonable assumption, as we expect the\n\nerror e(k) to converge near zero in the noiseless linear observation model,\n\n• If ζy is close enough to 1, and the time step k is large enough, γy(k) is approximately\n\nTherefore, we modify the update equation of Bζy\n\nBζy\n\ny (k + 1) =\n\n1 ζy\n\n(Bζy\n\ny (k) −\n\ny (k + 1) in (12) as 1 − ζy ζy\n\nBζy\n\ny (k)y(k)y(k)T Bζy\n\ny (k)).\n\n1 − ζy ζy\n\n.\n\n(14)\n\nFeed-forward synaptic connections W (k): The solution of online optimization in (6) is given by\n\nW (k + 1) = W (k) + μW (k)e(k)x(k)T ,\n\n(15)\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nwhere μW (k) is the step size corresponding to the adaptive least-mean-squares (LMS) update based on the MMSE criterion (Sayed, 2003). Algorithm 1 below summarizes the Sparse CorInfoMax output and learning dynamics:\n\nAlgorithm 1 Sparse CorInfoMax Algorithm Input: Streaming data {x(k) ∈ Rm}N 1: Initialize ζy, ζe, μ(1)W , W (1), Bζy 2: for k = 1, 2, . . . , N do 3:\n\nrun neural output dynamics until convergence:\n\nk=1, Output: {y(k) ∈ Rn}N e (1).\n\ny (1), Bζe\n\nk=1.\n\ne(k; ν) = y(k; ν) − W (k)x(k)\n\n∇y(k)J (y(k; ν)) = γy(k)Bζy\n\ny (k − 1)y(k; ν) − γe(k)Bζe (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))(cid:1) λ(k; ν + 1) = ReLU(λ(k; ν) − ηλ(ν)(1 − ∥y(k; ν + 1)∥1)),\n\ny(k; ν + 1) = STλ(k;ν)\n\ne (k − 1)e(k; ν),\n\nUpdate feedforward synapses: W (k + 1) = W (k) + μW (k)e(k)x(k)T Update lateral synapses: Bζy\n\ny (k) − 1−ζy\n\n(Bζy\n\nBζy\n\ny (k + 1) = 1 ζy\n\nζy\n\ny (k)y(k)y(k)T Bζy\n\ny (k))\n\n4: 5: 6: end for\n\ne (k) ≈ 1\n\nNeural Network Realizations: Figure 2a shows the three-layer realization of the sparse CorInfoMax neural network based on the network dynamics expressions in (8)- (11) and the approximation Bζe ε I. The first layer corresponds to the error (e(k)) neurons, and the second layer corresponds to the output (y(k)) neurons with soft thresholding activation functions. If we substitute (8) and Bζe where M ζy to the two-layer network shown in Figure 2b.\n\nε I in (9), we obtain ∇y(k)J(y(k; ν)) = M ζy y (k) − γe\n\nε I. Therefore, this gradient expression and (10)- (11) correspond\n\ne (k + 1) = 1 y (k) = γyBζy\n\ny (k)y(k; ν) +\n\nW (k)x(k)\n\nγe ε\n\n(a)\n\n(b)\n\nFigure 2: Sparse CorInfoMax Network: (a) three-layer (b) two-layer. xi’s and yi’s represent inputs (mixtures) and (separator) outputs , respectively, W are feedforward weights, ei’s in the threelayer implementation (on the left) are errors between transformed inputs and outputs, BY in (a), the inverse of output autocorrelation matrix, represents lateral weights at the output. MY in (b), represents the output lateral weights, which is a diagonally modified form of the inverse of output correlation. In both representations, the rightmost interneuron impose the sparsity constraint.\n\nThe neural network derivation examples for other source domains are provided in Appendix C.\n\n3.3 DESCRIPTION OF THE NETWORK DYNAMICS FOR A CANONICAL POLYTOPE\n\nREPRESENTATION\n\nIn this section, we consider the optimization problem specified in (5) for a generic polytope with H-representation in (1). We can write the corresponding online optimization setting in Lagrangian form as\n\nminimize λ(k)≽0\n\nmaximize y(k)∈Rn\n\nL(y(k), λ(k)) = J (y(k)) − λ(k)T (AP y(k) − bP ),\n\n(16)\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nwhich is a Min-Max problem. For the recursive update dynamics of the network output and the Lagrangian variable, we obtain the derivative of the objective in (16) with respect to y(k) and λ(k) as\n\n∇y(k)L(y(k; ν), λ(k; ν)) = γyBζy\n\ny (k − 1)y(k; ν) − γeBζe\n\ne (k − 1)e(k; ν) − AT\n\nP λ(k; ν),\n\n∇λ(k)L(y(k; ν)) = −AP y(k; ν) + bP .\n\n(17)\n\n(18)\n\nRecursive update dynamics for y(k) and λ(k): To solve the optimization problem in (16), the projected gradient updates on the y(k) and λ(k) lead to the following neural dynamic iterations:\n\ny(k; ν + 1) = y(k; ν) + ηy(ν)∇y(k)L(y(k; ν), λ(k; ν)), λ(k, ν + 1) = ReLU (λ(k, ν) − ηλ(ν)(bP − AP y(k; ν))) ,\n\n(19)\n\n(20)\n\nwhere ηλ(ν) denotes the learning rate for λ at iteration ν. These iterations correspond to a recurrent neural network for which we can make the following observations: i) output neurons use linear activation functions since y(k) is unconstrained in (16), ii) the network contains f interneurons corresponding to the Lagrangian vector λ, where f is the number of rows of AP in (16), or the number of (n − 1)-faces of the corresponding polytope, iii) the nonnegativity of λ implies ReLU activation functions for interneurons. The neural network architecture corresponding to the neural dynamics in (19)- (20) is shown in Figure 1a, which has a layer of f interneurons to impose the polytopic constraint in (1). The updates of W (k) and Bζy y (k) follow the equations provided in Section 3.2. Although the architecture in Figure 1a allows implementation of arbitrary polytopic source domains; f can be a large number. Alternatively, it is possible to consider the subset of polytopes in (2), which are described by individual properties and the relations of source components. Appendix C.5 derives the network dynamics for this feature-based polytope representation, and Figure 1b illustrates its particular realization. The number of interneurons in this case is equivalent to the number of sparsity constraints in (2), which can be much less than the number of faces of the polytope.\n\n4 NUMERICAL EXPERIMENTS\n\nIn this section, we illustrate different domain selections for sources and compare the proposed CorInfoMax framework with existing batch algorithms and online biologically plausible neural network approaches. We demonstrate the correlated source separation capability of the proposed framework for both synthetic and natural sources. Additional experiments and details about their implementations are available in Appendix D.\n\n4.1 SYNTHETICALLY CORRELATED SOURCE SEPARATION WITH ANTISPARSE SOURCES\n\nTo illustrate the correlated source separation capability of the online CorInfoMax framework for both nonnegative and signed antisparse sources, i.e. s(i) ∈ Bl∞,+ ∀i and s(i) ∈ Bl∞ ∀i, respectively, we consider a BSS setting with n = 5 sources and m = 10 mixtures. The 5-dimensional sources are generated using the Copula-T distribution with 4 degrees of freedom. We control the correlation level of the sources by adjusting a Toeplitz distribution parameter matrix with a first row of [1 ρ ρ ρ ρ] for ρ ∈ [0, 0.8]. In each realization, we generate N = 5 × 105 samples for each source and mix them through a random matrix A ∈ R10×5 whose entries are drawn from an i.i.d. standard normal distribution. Furthermore, we use an i.i.d. white Gaussian noise (WGN) corresponding to the signal-to-noise ratio (SNR) level of 30dB to corrupt the mixture signals. We use antisparse CorInfoMax network in Section C.1 and nonnegative CorInfoMax network in Appendix C.2 for these experiments. To compare, we also performed these experiments with biologically plausible algorithms: online BCA (Simsek & Erdogan, 2019), WSM (Bozkurt et al., 2022), NSM (Pehlevan et al., 2017a), BSM (Erdogan & Pehlevan, 2020), and batch altgorithms: ICA-Infomax (Bell & Sejnowski, 1995), LD-InfoMax (Erdogan, 2022), PMF (Tatli & Erdogan, 2021).\n\nFigure 3 shows the signal-to-interference-plus-noise ratio (SINR) versus correlation level ρ curves of different algorithms for nonnegative antisparse and antisparse source separation experiments. We observe that the proposed CorInfoMax approach achieves relatively high SINR results despite increasing ρ in both cases. Although the WSM curve has a similar characteristic, its performance falls behind that of CorInfoMax. Moreover, the LD-InfoMax and PMF algorithms typically achieve the best results, as expected, due to their batch learning settings. Furthermore, the performance\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nof NSM, BSM, and ICA-InfoMax degrades with increasing source correlation because these approaches assume uncorrelated or independent sources.\n\n(a)\n\n(b)\n\nFigure 3: The SINR performances of CorInfoMax (ours), LD-InfoMax, PMF, ICA-InfoMax, NSM, and BSM, averaged over 100 realizations, (y-axis) with respect to the correlation factor ρ (x-axis). SINR vs. ρ curves for (a) nonnegative antisparse (Bl∞,+), (b) antisparse (Bl∞) source domains.\n\n4.2 VIDEO SEPARATION\n\n23\n\n12\n\n23\n\n12\n\n13\n\n= −0.1549, ρaverage\n\n= −0.1597, ρaverage\n\n= 0.3811 and ρmaximum\n\nTo provide a visual example and illustrate a real naturally correlated source scenario, we consider the following video separation setup: 3 videos of 10 seconds are mixed to generate 5 mixture videos. The average (across frames) and maximum Pearson correlation coefficients for these three sources are ρaverage =\n= 0.5173, respectively. We use a random mixing matrix A ∈ R5×3 with positive 0.2587, ρmaximum entries (to ensure nonnegative mixtures so that they can be displayed as proper images without loss of generality), which is provided in Appendix D.3.3. Since the image pixels are in the set [0, 1], we use the nonnegative antisparse CorInfoMax network to separate the original videos. The demo video (which is available in supplementary files and whose link is provided in the footnote 2) visually demonstrates the separation process by the proposed approach over time. The first and second rows of the demo are the 3 source videos and 3 of the 5 mixture videos, respectively. The last row contains the source estimates obtained by the CorInfoMax network during its unsupervised learning process. We observe that the output frames become visually better as time progresses and start to represent individual sources. In the end, the CorInfoMax network is trained to a stage of near-perfect separation, with peak signal-to-noise ratio (PSNR) levels of 35.60dB, 48.07dB, and 44.58dB for each source, respectively. Further details for this experiment can be found in the Appendix D.3.3.\n\n= 0.3139, ρmaximum\n\n13\n\n5 CONCLUSION\n\nIn this article, we propose an information-theoretic framework for generating biologically plausible neural networks that are capable of separating both independent and correlated sources. The proposed CorInfoMax framework can be applied to infinitely many source domains, enabling a diverse set of source characterizations. In addition to solving unsupervised linear inverse problems, CorInfoMax networks have the potential to generate structured embeddings from observations based on the choice of source domains. In fact, as a future extension, we consider representation frameworks that learn desirable source domain representations by adapting the output-interneuron connections in Figure 1a. Finally, the proposed unsupervised framework and its potential supervised extensions can be useful for neuromorphic systems that are bound to use local learning rules.\n\nIn terms of limitations, we can list the computational complexity for simulating such networks in conventional computers, mainly due to the loop-based recurrent output computation. However, as described in Appendix D.7, the neural networks generated by the proposed framework have computational loads similar to the existing biologically plausible BSS neural networks.\n\n2https://figshare.com/s/a3fb926f273235068053\n\n9\n\n0.00.10.20.30.40.50.60.70.8ρ05101520253035SINR (dB)Nonnegative Anti-sparse Source Separation SINR ResultsCorInfoMaxWSMICA-InfomaxNSMLD-InfoMaxPMF0.00.10.20.30.40.50.60.70.8ρ051015202530SINR (dB)Anti-sparse Source Separation SINR ResultsCorInfoMaxWSMOnlineBCAICA-InfomaxBSMPMFPublished as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY\n\nTo ensure the reproducibility of our results, we provide\n\ni. Detailed mathematical description of the algorithms for different source domains and their neural network implementations in Section 3.2, Appendix C.1, Appendix C.2, Appendix C.3, Appendix C.4 and Appendix C.5,\n\nii. Detailed information on the simulation settings of the experiments in Section 4 in the main article,\n\nand Appendix D,\n\niii. Full list of hyperparameter sets used in these experiments in Table 3, Table 4 in Appendix D.4, iv. Ablation studies on hyperparameters in Appendix D.5, v. Algorithm descriptions for special source domains in pseudo-code format in Appendix D.1, vi. Python scripts and notebooks for individual experiments to replicate the reported results in the supplementary zip file as well as in https://github.com/BariscanBozkurt/Bio-Plausible-CorrInfoMax.\n\n7 ETHICS STATEMENT\n\nRelated to the algorithmic framework we propose in this article, we see no immediate ethical concerns. In addition, the datasets that we use have no known or reported ethical issues, to the best of our knowledge.\n\n8 ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\n\nThis work was supported by KUIS AI Center Research Award. CP was supported by an NSF Award (DMS-2134157) and the Intel Corporation through the Intel Neuromorphic Research Community.\n\nREFERENCES\n\nMaryam Abdolali and Nicolas Gillis. Simplex-structured matrix factorization: Sparsity-based identifiability and provably correct algorithms. SIAM Journal on Mathematics of Data Science, 3(2):593–623, 2021. doi: 10.1137/20M1354982. URL https://doi.org/10.1137/ 20M1354982.\n\nHiroki Asari, Barak A Pearlmutter, and Anthony M Zador. Sparse representations for the cocktail\n\nparty problem. Journal of Neuroscience, 26(28):7477–7490, 2006.\n\nEren Babatas and Alper T Erdogan. An algorithmic framework for sparse bounded component\n\nanalysis. IEEE Transactions on Signal Processing, 66(19):5194–5205, August 2018.\n\nYanis Bahroun, Dmitri Chklovskii, and Anirvan Sengupta. A normative and biologically plausible algorithm for independent component analysis. Advances in Neural Information Processing Systems, 34:7368–7384, 2021.\n\nSuzanna Becker and Geoffrey E Hinton. Self-organizing neural network that discovers surfaces in\n\nrandom-dot stereograms. Nature, 355(6356):161–163, 1992.\n\nMark A Bee and Christophe Micheyl. The cocktail party problem: what is it? how can it be solved? and why should animal behaviorists study it? Journal of comparative psychology, 122(3):235, 2008.\n\nAnthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separa-\n\ntion and blind deconvolution. Neural computation, 7(6):1129–1159, 1995.\n\nBariscan Bozkurt and Alper T Erdogan. On identifiable polytope characterization for polytopic matrix factorization. 2022 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 2022.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBariscan Bozkurt, Cengiz Pehlevan, and Alper T Erdogan. Biologically-plausible determinant maximization neural networks for blind separation of correlated sources. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=espX_4CLr46.\n\nAdelbert W Bronkhorst. The cocktail party phenomenon: A review of research on speech intelligibility in multiple-talker conditions. Acta Acustica united with Acustica, 86(1):117–128, 2000.\n\nTsung-Han Chan, Wing-Kin Ma, ArulMurugan Ambikapathi, and Chong-Yung Chi. A simplex volume maximization framework for hyperspectral endmember extraction. IEEE Transactions on Geoscience and Remote Sensing, 49(11):4177–4193, May 2011.\n\nZizhong Chen and Jack J Dongarra. Condition numbers of gaussian random matrices. SIAM Journal\n\non Matrix Analysis and Applications, 27(3):603–620, 2005.\n\nAndrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation. John Wiley & Sons, 2009.\n\nPierre Comon and Christian Jutten. Handbook of Blind Source Separation: Independent component\n\nanalysis and applications. Academic press, 2010.\n\nSergio Cruces. Bounded component analysis of linear mixtures: A criterion of minimum convex\n\nperimeter. IEEE Transactions on Signal Processing, 58(4):2141–2154, 2010.\n\nAlper T Erdogan. A class of bounded component analysis algorithms for the separation of both independent and dependent sources. IEEE Transactions on Signal Processing, 61(22):5730–5743, August 2013.\n\nAlper T Erdogan. An information maximization based blind source separation approach for dependent and independent sources. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4378–4382, 2022. doi: 10.1109/ICASSP43922. 2022.9746099.\n\nAlper T Erdogan and Cengiz Pehlevan. Blind bounded source separation using neural networks with local learning rules. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3812–3816, 2020. doi: 10.1109/ICASSP40776. 2020.9053114.\n\nXiao Fu, Kejun Huang, Bo Yang, and N.D. Sidiropoulos. Robust volume minimization-based matrix factorization for remote sensing and document clustering. IEEE Transactions on Signal Processing, 64, 08 2016. doi: 10.1109/TSP.2016.2602800.\n\nXiao Fu, Kejun Huang, and Nicholas D Sidiropoulos. On identifiability of nonnegative matrix\n\nfactorization. IEEE Signal Processing Letters, 25(3):328–332, January 2018.\n\nXiao Fu, Kejun Huang, Nicholas D Sidiropoulos, and Wing-Kin Ma. Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications. IEEE Signal Process. Mag., 36(2):59–80, March 2019.\n\nElana M Zion Golumbic, Nai Ding, Stephan Bickel, Peter Lakatos, Catherine A Schevon, Guy M McKhann, Robert R Goodman, Ronald Emerson, Ashesh D Mehta, Jonathan Z Simon, et al. Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party”. Neuron, 77(5):980–991, 2013.\n\nBranko Gr ̈unbaum, Victor Klee, Micha A Perles, and Geoffrey Colin Shephard. Convex polytopes,\n\nvolume 16. Springer, 1967.\n\nHuseyin A Inan and Alper T Erdogan. Convolutive bounded component analysis algorithms for independent and dependent source separation. IEEE transactions on neural networks and learning systems, 26(4):697–708, 2014.\n\nTakuya Isomura and Taro Toyoizumi. Error-gated hebbian rule: A local learning rule for principal\n\nand independent component analysis. Scientific reports, 8(1):1–11, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTakuya Isomura, Kiyoshi Kotani, and Yasuhiko Jimbo. Cultured cortical neurons can perform blind source separation according to the free-energy principle. PLoS Comput Biol, 11(12):e1004643, 2015.\n\nThomas Kailath, Ali H Sayed, and Babak Hassibi. Linear estimation. Prentice-Hall information and\n\nsystem sciences series. Prentice Hall, 2000. ISBN 9780130224644.\n\nMichael S Lewicki. Efficient coding of natural sounds. Nature neuroscience, 5(4):356–363, 2002.\n\nChia-Hsiang Lin, Wing-Kin Ma, Wei-Chiang Li, Chong-Yung Chi, and ArulMurugan Ambikapathi. Identifiability of the simplex volume minimization criterion for blind hyperspectral unmixing: The no-pure-pixel case. IEEE Transactions on Geoscience and Remote Sensing, 53(10):5530– 5546, May 2015.\n\nRalph Linsker. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988.\n\nDavid Lipshutz, Cengiz Pehlevan, and Dmitri B Chklovskii. Biologically plausible single-layer networks for nonnegative independent component analysis. Biological Cybernetics, pp. 1–12, 2022.\n\nJosh H McDermott. The cocktail party problem. Current Biology, 19(22):R1024–R1027, 2009.\n\nNima Mesgarani and Edward F Chang. Selective cortical representation of attended speaker in\n\nmulti-talker speech perception. Nature, 485(7397):233–236, 2012.\n\nRajiv Narayan, Virginia Best, Erol Ozmeral, Elizabeth McClaine, Micheal Dent, Barbara ShinnCunningham, and Kamal Sen. Cortical interference effects in the cocktail party problem. Nature neuroscience, 10(12):1601–1607, 2007.\n\nBruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by\n\nlearning a sparse code for natural images. Nature, 381(6583):607–609, 1996.\n\nBruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy\n\nemployed by v1? Vision research, 37(23):3311–3325, 1997.\n\nSerdar Ozsoy, Shadi Hamdan, Sercan O Arik, Deniz Yuret, and Alper T Erdogan. Self-supervised learning with an information maximization criterion. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=5MgZAu2NR7X.\n\nNeal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends® in Optimization,\n\n1(3):127–239, 2014.\n\nCengiz Pehlevan, Sreyas Mohan, and Dmitri B Chklovskii. Blind nonnegative source separation\n\nusing biological neural networks. Neural computation, 29(11):2925–2954, 2017a.\n\nCengiz Pehlevan, Anirvan M Sengupta, and Dmitri B Chklovskii. Why do similarity matching objectives lead to hebbian/anti-hebbian networks? Neural computation, 30(1):84–124, 2017b.\n\nAli H Sayed. Fundamentals of adaptive filtering. John Wiley & Sons, 2003.\n\nJitendra Sharma, Alessandra Angelucci, and Mriganka Sur. Induction of visual orientation modules\n\nin auditory cortex. Nature, 404(6780):841–847, 2000.\n\nBerfin Simsek and Alper T Erdogan. Online bounded component analysis: A simple recurrent neural network with local update rule for unsupervised separation of dependent and independent sources. In 2019 53rd Asilomar Conference on Signals, Systems, and Computers, pp. 1639–1643, 2019. doi: 10.1109/IEEECONF44664.2019.9048916.\n\nEvan C Smith and Michael S Lewicki. Efficient auditory coding. Nature, 439(7079):978–982, 2006.\n\nGokcan Tatli and Alper T Erdogan. Polytopic matrix factorization: Determinant maximization based criterion and identifiability. IEEE Transactions on Signal Processing, 69:5431–5447, 2021. doi: 10.1109/TSP.2021.3112918.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFrederik Tr ̈auble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Sch ̈olkopf, and Stefan Bauer. On disentangled representations learned from correlated data. In International Conference on Machine Learning, pp. 10401–10412. PMLR, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1\n\nINFORMATION THEORETIC DEFINITIONS\n\nIn this section, we review the logarithm-determinant (LD) entropy measure and mutual information for the BSS setting introduced in Section 2.1 based on Erdogan (2022). Note that in this article, we refer to LD-mutual information synonymously as correlative mutual information. For a finite set of vectors X = {x(1), x(2), . . . , x(N )} ⊂ Rm with a sample covariance matrix ˆRx = XX T −\n\n1 N\n\n1\n\nN 2 X11T X T , where X is defined as X = [ x(1) x(2)\n\nentropy is defined in Erdogan (2022) as\n\n. . . x(N ) ], the deterministic LD-\n\nH(X)(ε)\n\nLD =\n\n1 2\n\nlog det( ˆRx + εI) +\n\nm 2\n\nlog(2πe)\n\n(A.1)\n\nwhere ε > 0 is a small number to keep the expression away from −∞. If the sample covariance ˆRx in this expression is replaced with the true covariance, then H(x)(0) LD coincides with the Shannon differential entropy for a Gaussian vector x. Moreover, a deterministic joint LD-entropy of two sets of vectors X ⊂ Rm and Y ⊂ Rn can be defined as\n\nH(X, Y )(ε)\n\nLD =\n\n1 2\n\nm + n 2\n\nlog det(2πe)\n\n(cid:21)(cid:19)\n\nˆRxy ˆRy + εI\n\n+\n\nm + n 2\n\nlog det(2πe)\n\ndet( ˆRx + εI) det( ˆRy + εI − ˆRT\n\nxy( ˆRx + εI)−1 ˆRyx)\n\n(cid:17)\n\nlog det( ˆR(cid:34)x\n\n(cid:35) + εI) +\n\ny (cid:18)(cid:20) ˆRx + εI\n\n(cid:16)\n\nlog\n\nˆRyx\n\nlog det\n\n1 2\n1 2\nm + n 2\nlog det( ˆRx + εI) +\n\nlog det(2πe)\n\n1 2\n\nm 2\nLD + H(Y |LX)(ε)\n\nLD\n\n=\n\n=\n\n+\n\n=\n\n= H(X)(ε)\n\nlog(2πe) +\n\n1 2\n\nlog det( ˆRe + εI) +\n\nn 2\n\nlog(2πe)\n\n(A.2)\n\nN XY T = ˆRT\n\nxy( ˆRx + εI)−1 ˆRyx, and ˆRxy = 1 LD ̸= H(Y |X)(ε)\n\nwhere ˆRe = ˆRy − ˆRT yx. In (A.2), the notation Y |LX is used to signify H(Y |LX)(ε) LD as the latter requires the use of ˆRy|x instead of ˆRe. Moreover, H(Y |LX)(ε) LD corresponds to the log-determinant of the error sample covariance of the best linear minimum mean squared estimate (MMSE) of y from x. To verify that in the zero-mean and noiseless case, consider the MMSE estimate ˆy = W x for which the x (Kailath et al., 2000). Then R ˆy = E[ ˆy ˆyT ] = solution is given by W = RyxR−1 E[RT x Rxy. Therefore, if the error is defined as e = y − ˆy, its covariance matrix can be found as desired, i.e., Re = Ry − R ˆy = Ry − RT\n\nx Rxy] = RT\n\nx xxT R−1\n\nx = RT\n\nxyR−1\n\nxyR−1\n\nxyR−1\n\nxyR−1\n\nx Rxy.\n\nThe LD-mutual information for X and Y can be defined based on the equations (A.1) and (A.2) as\n\nI (ε)(X, Y ) = H (ε)\n\nLD (Y ) − H (ε) log det( ˆRy + εI) −\n\nLD (Y |LX) = H (ε) 1\n2 1\n2\n\n=\n\n=\n\n1 2\n1 2\n\nLD (X) − H (ε)\n\nlog det( ˆRy − ˆRT\n\nLD (X|LY ) xy( ˆRx + εI)−1 ˆRyx + εI) + C\n\nlog det( ˆRx + εI) −\n\nlog det( ˆRx − ˆRT\n\nyx( ˆRy + εI)−1 ˆRxy + εI) + C, (A.3)\n\nwhere C =\n\nm 2\n\nlog(2πe) +\n\nn 2\n\nlog(2πe) is a constant.\n\nB GRADIENT DERIVATIONS FOR ONLINE OPTIMIZATION OBJECTIVE\n\nAssuming that the mapping W (k) changes slowly over time, the current output y(k) can be implicitly defined by the projected gradient ascent with neural dynamics. To derive the corresponding neural dynamics for the output y(k), we need to calculate the gradient of the objective 5a with respect\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nto y(k). First, we consider the derivative of both log det( ˆRζy with respect to y(k).\n\ny (k) + εI) and log det( ˆRζe\n\ne (k) + εI)\n\n∂ log det( ˆRζy\n\ny (k) + εI)\n\n∂yi(k)\n\n(cid:32)\n\n= T r\n\n∇\n\n( ˆR\n\nζy y (k)+εI)\n\nlog det( ˆRζy\n\ny (k) + εI)\n\ny (k)\n\n∂ ˆRζy ∂yi(k)\n\n(cid:33)\n\n,\n\nwhere yi(k) is the i-th element of the vector y(k), and\n\n∇\n\n( ˆR\n\nζy y (k)+εI)\n\nlog det( ˆRζy\n\ny (k) + εI) = ( ˆRζy\n\ny (k) + εI)−1,\n\ny (k)\n\n∂ ˆRζy ∂yi(k)\n\n=\n\n1 − ζy 1 − ζ k y\n\n(y(k)e(i)T\n\n+ e(i)y(k)T ).\n\n(A.4)\n\n(A.5)\n\nIn (A.5), e(i) denotes the standard basis vector with a 1 at position i and should not be confused with the error vector e(k). Combining (A.4) and (A.5), we obtain the following result:\n\n∂ log det( ˆRζy\n\ny (k) + εI)\n\n∂yi(k)\n\n= 2\n\n1 − ζy 1 − ζ k y\n\ne(i)T\n\n( ˆRζy\n\ny (k) + εI)−1y(k),\n\nwhich leads to\n\n∇y(k) log det( ˆRζy\n\ny (k) + εI) = 2\n\n1 − ζy 1 − ζ k y\n\n( ˆRζy\n\ny (k) + εI)−1y(k).\n\n(A.6)\n\nIf we apply the same procedure to obtain the gradient of log det( ˆRζe we obtain\n\ne (k) + εI) with respect to e(k),\n\n∇e(k) log det( ˆRζe\n\ne (k) + εI) = 2\n\n1 − ζe 1 − ζ k e\n\n( ˆRζe\n\ne (k) + εI)−1e(k).\n\nUsing the composition rule, we can obtain the gradient of log det( ˆRζe as follows:\n\ne (k)+εI) with respect to y(k)\n\n∇y(k) log det( ˆRζe\n\ne (k) + εI) =\n\n∇e(k) log det( ˆRζe\n\ne (k) + εI)\n\n∂e(k) ∂y(k) (cid:124) (cid:123)(cid:122) (cid:125) In 1 − ζe 1 − ζ k e\n\n= 2\n\n( ˆRζe\n\ne (k) + εI)−1e(k).\n\n(A.7)\n\nFinally, combining the results from (A.6) and (A.7), we obtain the derivative of the objective function J (y(k)) with respect to y(k)\n\n∇y(k)J (y(k)) =\n\n=\n\n1 2\n1 − ζy 1 − ζ k y\n\n∇y(k) log det( ˆRζy\n\ny (k) + εI) −\n\n∇y(k) log det( ˆRζe\n\ne (k) + εI)\n\n( ˆRζy\n\ny (k) + εI)−1y(k) −\n\n( ˆRζe\n\ne (k) + εI)−1e(k)\n\n(A.8)\n\n1 2\n1 − ζe 1 − ζ k e\n\nFor further simplification of (A.8), we define the recursions for ˆRζy based on the recursive definitions of the corresponding correlation matrices. Based on the definition in (4), we can write\n\nand ˆRζe\n\ne (k)\n\ny (k)\n\n−1\n\n−1\n\nˆRζy\n\ny (k) + εI =\n\n≈\n\ny\n\n1 − ζ k−1 1 − ζ k y\n\ny\n\n1 − ζ k−1 1 − ζ k y\n\nζy( ˆRζy\n\ny (k − 1) + εI) +\n\nζy( ˆRζy\n\ny (k − 1) + εI) +\n\n1 − ζy 1 − ζ k y\n\n1 − ζy 1 − ζ k y\n\ny(k)y(k)T +\n\n1 − ζy 1 − ζ k y\n\nεI\n\ny(k)y(k)T\n\n(A.9)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nUsing the assumption in (A.9), we take the inverse of both sides and apply the matrix inversion lemma (similar to its use in the derivation of the RLS algorithm Kailath et al. (2000)) to obtain\n\n( ˆRζy\n\ny (k) + εI)−1 =\n\n( ˆRζy\n\ny (k − 1) + εI)−1\n\n(cid:16)\n\n1 − ζ k y\nζy − ζ k y\n− γy(k)( ˆRζy\n\ny (k − 1) + εI)−1y(k)y(k)T ( ˆRζy\n\ny (k − 1) + εI)−1(cid:17)\n\n,\n\n(A.10)\n\nwhere\n\nγy(k) =\n\n(cid:32)\n\nζy − ζ k y\n1 − ζy\n\n+ y(k)T ( ˆRζy\n\ny (k − 1) + εI)−1y(k)\n\n.\n\n(A.11)\n\n(cid:33)−1\n\nWe apply the same procedure to obtain the inverse of ( ˆRζe\n\ne (k) + εI)−1:\n\n( ˆRζe\n\ne (k) + εI)−1 =\n\n( ˆRζe\n\ne (k − 1) + εI)−1\n\n(cid:16)\n\n1 − ζ k e\nζe − ζ k e\n− γe(k)( ˆRζe\n\ne (k − 1) + εI)−1e(k)e(k)T ( ˆRζe\n\ne (k − 1) + εI)−1(cid:17)\n\n,\n\n(A.12)\n\nwhere\n\nγe(k) =\n\n(cid:18) ζe − ζ k e\n1 − ζe\n\n+ e(k)T ( ˆRζe\n\ne (k − 1) + εI)−1e(k)\n\n(cid:19)−1\n\n.\n\n(A.13)\n\nNote that plugging (A.10) into the first part of (A.8) yields the following simplification:\n\n1 − ζy 1 − ζ k y\n\n( ˆR(k) + εI)−1y(k) =\n\n1 − ζy 1 − ζ k y\n\n1 − ζ k y\nζy − ζ k y\n\n(cid:16)\n\n( ˆR(k − 1) + εI)−1\n\n− γy(k)( ˆR(k − 1) + εI)−1y(k)y(k)T ( ˆR(k − 1) + εI)−1(cid:17)\n\ny(k)\n\n=\n\n−\n\n(cid:18)\n\n( ˆR(k − 1) + εI)−1\n\n1 − ζy ζy − ζ k y\n( ˆR(k − 1) + εI)−1y(k)y(k)T ( ˆR(k − 1) + εI)−1\n\nζy−ζk y\n1−ζy\n\n+ y(k)T ( ˆR(k − 1) + εI)−1y(k)\n\n(cid:19)\n\ny(k)\n\n=\n\n(cid:16)\n\n1 − ζy ζy − ζ k y\n\nζy−ζk y\n1−ζy\n\n( ˆR(k − 1) + εI)−1y(k)\n\n(cid:17)\n\nζy−ζk y\n1−ζy\n\n+ y(k)T ( ˆR(k − 1) + εI)−1y(k)\n\n(cid:16) ( ˆR(k − 1) + εI)−1y(k)y(k)T ( ˆR(k − 1) + εI)−1y(k)\n\n(cid:17)\n\nζy−ζk y\n1−ζy\n\n+ y(k)T ( ˆR(k − 1) + εI)−1y(k)\n\n(cid:16) ( ˆR(k − 1) + εI)−1y(k)y(k)T ( ˆR(k − 1) + εI)−1y(k)\n\n(cid:17)\n\n+\n\n−\n\n1 − ζy ζy − ζ k y\n\n1 − ζy ζy − ζ k y\n\nζy−ζk y\n1−ζy y (k − 1) + εI)−1y(k).\n\n= γy(k)( ˆRζy\n\n+ y(k)T ( ˆR(k − 1) + εI)−1y(k)\n\nA similar simplification can be obtained for (A.12), and incorporating these simplifications into (A.8 yields\n\n∇y(k)J (y(k)) = γy(k)Bζy\n\ny (k)y(k) − γe(k)Bζe\n\ne (k)e(k),\n\n(A.14)\n\nwhere we denote ( ˆRζy simplicity, respectively.\n\ny (k) + εI)−1 and ( ˆRζe\n\ne (k) + εI)−1 by Bζy\n\ny (k + 1) and Bζe\n\ne (k + 1) for\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC SUPPLEMENTARY ON THE NETWORK STRUCTURES FOR THE EXAMPLE\n\nDOMAINS\n\nWe can generalize the procedure for obtaining CorInfoMax BSS networks in Section 3.2 for other source domains. The choice of source domain would affect the structure of the output layer and the potential inclusion of additional interneurons. Table 1 summarizes the output dynamics for special source domains provided in Section 2.1, for which the derivations are provided in the following subsections.\n\nTable 1: Example source domains and the corresponding CorInfoMax network dynamics.\n\nSource Domain Output Dynamics\n\nOutput Activation\n\nP = ∆\n\nP = Bl∞\n\nP = Bl∞,+\n\nP = Bl1,+\n\n∇y(k)J(y(k; ν)) = γyB y(k; ν + 1) = ReLU (cid:0)y(k; ν) + ηy(ν)∇y(k)J(y(k; ν)) − λ(ν)(cid:1), (cid:18)\n\nζy y (k − 1)y(k; ν) − γeBζe\n\ne (k − 1)e(k; ν),\n\n(cid:19)\n\nλ(k; ν + 1) = λ(k; ν) − ηλ(ν)\n\n1 − (cid:0) (cid:80)n\n\ni=1 yi(k; ν + 1)(cid:1)\n\n.\n\n∇y(k)J(y(k; ν)) = γyB y(k; ν + 1) = σ1\n\n(cid:0)y(k; ν) + ηy(ν)∇y(k)J(y(k; ν))(cid:1),\n\nζy y (k − 1)y(k; ν) − γeBζe\n\ne (k − 1)e(k; ν),\n\n∇y(k)J(y(k; ν)) = γyB y(k; ν + 1) = σ+\n\nζy y (k)y(k; ν) − γeBζe (cid:0)y(k; ν) + ηy(ν)∇y(k)J(y(k; ν))(cid:1),\n\ne (k)e(k; ν),\n\n∇y(k)J(y(k; ν)) = γyB y(k; ν + 1) = ReLU (cid:0)y(k; ν) + ηy(ν)∇y(k)J(y(k; ν))(cid:1),\n\nζy y (k − 1)y(k; ν) − γeBζe\n\ne (k − 1)e(k; ν),\n\nλ(k; ν + 1) = ReLU\n\nλ(k; ν) − ηλ(ν)\n\n(cid:18)\n\n(cid:18)\n\n1 − (cid:0) (cid:80)n\n\ni=1 yi(k; ν + 1)(cid:1)\n\n(cid:19)(cid:19)\n\n.\n\nC.1 DESCRIPTION OF THE NETWORK DYNAMICS FOR ANTISPARSE SOURCES\n\nWe consider the source domain P = Bl∞ which corresponds to antisparse sources. Similar to the sparse CorInfoMax example in Section 3.2, we derive the network corresponding to the antisparse CorInfoMax network through the projected gradient ascent method. Since projection onto the P = Bl∞ is an elementwise clipping operation, unlike the sparse CorInfoMax case, we do not require any interneurons related to the projection operation.\n\nRecursive update dynamics for y(k): Based on the gradient of (5a) with respect to y(k) in (A.14), derived in Appendix B, we can write the corresponding projected gradient ascent iterations for (5) as\n\ne(k; ν) = y(k; ν) − W (k)x(k),\n\n∇y(k)J (y(k; ν)) = γyBζy y(k; ν + 1) = σ1\n\ny (k)y(k; ν) − γeBζe\n\ne (k)e(k; ν), (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))(cid:1) ,\n\nwhere Bζy e (k − 1) respectively, ν ∈ N is the index of neural dynamic iterations, σ1(.) is the projection onto the selected domain Bl∞, which is\n\ne (k) are inverses of Rζy\n\ny (k − 1) and Rζe\n\ny (k) and Bζe\n\nthe elementwise clipping function defined as σ1(y)i =\n\n(cid:26)\n\nyi sign(yi)\n\n−1 ≤ yi ≤ 1, otherwise.\n\n.\n\nThe corresponding realization of the neural network is shown in Figure 4.\n\nC.2 DESCRIPTION OF THE NETWORK DYNAMICS FOR NONNEGATIVE ANTISPARSE\n\nSOURCES\n\nWe consider the source domain P = Bl∞,+. The treatment for this case is almost the same as the signed antisparse case in Appendix C.1. The only difference is that the projection to the source domain is performed by applying elementwise nonnegative clipping function to individual outputs.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Two-Layer antisparse CorInfoMax Network. xi’s and yi’s represent inputs (mixtures) and (separator) outputs , respectively, W represents feedforward weights, ei’s are errors between transformed inputs and outputs, BY, the inverse of output autocorrelation matrix, represents lateral weights at the output. The output nonlinearities are clipping functions.\n\nTherefore, we can write the neural network dynamics for the nonnegative antisparse case as\n\ne(k; ν) = y(k; ν) − W (k)x(k),\n\n∇y(k)J (y(k; ν)) = γyBζy y(k; ν + 1) = σ+\n\ny (k)y(k; ν) − γeBζe (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))(cid:1) ,\n\ne (k)e(k; ν),\n\nwhere the nonnegative clipping function is defined as\n\nσ+(y)i =\n\n(cid:40) 0 yi 1\n\nyi ≤ 0, 0 ≤ yi ≤ 1, yi ≥ 1.\n\nThe corresponding neural network realization is shown in Figure 5.\n\nFigure 5: Two-Layer nonnegative antisparse CorInfoMax Network. xi’s and yi’s represent inputs (mixtures) and (separator) outputs , respectively, W represents feedforward weights, ei’s are errors between transformed inputs and outputs, BY, the inverse of output autocorrelation matrix, represents lateral weights at the output. The output nonlinearities are nonnegative clipping functions.\n\nC.3 DESCRIPTION OF THE NETWORK DYNAMICS FOR NONNEGATIVE SPARSE SOURCES\n\nFor the nonnegative sparse CorInfoMax network in Section 3.2, the only change compared to its sparse counterpart is the replacement of the soft-thresholding activation functions at the output layer with the rectified linear unit. Accordingly, we can state the dynamics of output and inhibitory neurons as\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n∇y(k)J (y(k; ν)) = γyBζy\n\ny (k − 1)y(k; ν) − γeBζe y(k; ν + 1) = ReLU (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν)) − λ(k; ν)(cid:1) ,\n\ne (k − 1)e(k; ν),\n\n∇λ(k)L(y(k; ν)) = 1 −\n\nyi(k; ν + 1)\n\n(cid:19) ,\n\n(cid:18) n\n\n(cid:88)\n\ni=1 (cid:18)\n\nλ(k; ν + 1) = ReLU\n\nλ(k; ν) − ηλ(ν)∇λ(k)L(y(k; ν))\n\n(cid:19) .\n\nThe corresponding network structure is illustrated in Figure 6.\n\nFigure 6: Three-Layer nonnegative sparse CorInfoMax Network. xi’s and yi’s represent inputs (mixtures) and (separator) outputs , respectively, W represents feedforward weights, ei’s are errors between transformed inputs and outputs, BY, the inverse of output autocorrelation matrix, represents lateral weights at the output. The output nonlinearities are ReLU functions. The leftmost interneuron imposes sparsity constraints on the outputs through inhibition.\n\nC.4 DESCRIPTION OF THE NETWORK DYNAMICS FOR UNIT SIMPLEX SOURCES\n\nFor the BSS setting for the unit simplex set ∆, which is used for nonnegative matrix factorization, we consider the following optimization problem,\n\nmaximize y(k) ∈ Rn\n\nsubject to\n\nJ (y(k))\n\n||y(k)||1 = 1, y(k) ≽ 0\n\n(A.15a)\n\n(A.15b)\n\nfor which the Lagrangian-based Min-Max problem can be stated as\n\nminimize λ\n\nmaximize y(k)∈Rn\n\nL(y(k),λ(k)) (cid:125)(cid:124) (cid:122) J (y(k)) − λ(k)(∥y(k)∥1 − 1) .\n\n(cid:123)\n\nIn this context, contrary to the optimization problem defined in (7), we do not require that the Lagrangian variable λ be nonnegative, due to the equality constraint in (A.15b). Hence, we write the network dynamics for a simplex source as\n\n∇y(k)J (y(k; ν)) = γyBζy\n\ny (k − 1)y(k; ν) − γeBζe y(k; ν + 1) = ReLU (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν)) − λ(k; ν)(cid:1) ,\n\ne (k − 1)e(k; ν),\n\n∇λ(k)L(y(k; ν)) = 1 −\n\nyi(k; ν + 1)\n\n(cid:19) ,\n\n(cid:18) n\n\n(cid:88)\n\ni=1\n\nλ(k; ν + 1) = λ(k; ν) − ηλ(ν)∇λ(k)L(y(k; ν)).\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7 demonstrates the network structure for simplex sources, which is identical to nonnegative sparse CorInfoMax network except that the linear activation replaces the ReLU activation of the inhibitory neuron.\n\nFigure 7: Three-Layer CorInfoMax Network for unit simplex sources. xi’s and yi’s represent inputs (mixtures) and (separator) outputs , respectively, W represents feedforward weights, ei’s are errors between transformed inputs and outputs, BY, the inverse of output autocorrelation matrix, represents lateral weights at the output. The output nonlinearities are ReLU functions. The leftmost interneuron imposes sparsity constraints on the outputs through inhibition.\n\nC.5 DESCRIPTION OF THE NETWORK DYNAMICS FOR FEATURE BASED SPECIFIED\n\nPOLYTOPES\n\nIn this section, we consider the source separation setting where source samples are from a polytope represented in the form of (2). We expand the derivation in Bozkurt et al. (2022) (see Appendix D.6 in the reference) to obtain a neural network solution to the BSS problem for any identifiable polytope that can be expressed in the form of (2). Accordingly, we consider the following optimization problem\n\nmaximize y(k) ∈ Rn\n\nsubject to\n\nJ (y(k))\n\n−1 ≼ y(k)Is 0 ≼ y(k)I+\n\n≼ 1, ≼ 1,\n\n||y(k)Jl ||1 ≤ 1 ∀l = 1, . . . , L\n\n(A.16a)\n\n(A.16b) (A.16c)\n\n(A.16d)\n\nWe write the online optimization setting in a Lagrangian Min-Max setting as follows:\n\nminimize λl(k)≥0\n\nmaximize y(k) ∈ Rn −1 ≼ y(k)Is 0 ≼ y(k)I+\n\n≼ 1 ≼ 1\n\nJ (y(k)) −\n\n(cid:122)\n\nL(y(k),λ1(k),...,λL(k)) (cid:125)(cid:124)\n\n(cid:123)\n\nL (cid:88)\n\nl=1\n\nλl(k)(∥y(k)Jl ∥1 − 1) .\n\nThe proximal operator corresponding to the Lagrangian term can be written as\n\nproxλ(y) = argmin\n\nq s.t. qI+\n\n≽0\n\n(cid:32)\n\n1 2\n\n∥y − q∥2\n\n2 +\n\n(cid:33)\n\nλl∥qJl ∥1\n\n.\n\nL (cid:88)\n\nl=1\n\n(A.17)\n\nLet q∗ be the output of the proximal operator defined in (A.17). From the first order optimality condition,\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n• If j ̸∈ I+, then q∗\n\nj − yj + (cid:80)\n\nλlsign(yj) = 0. Therefore, q∗\n\nj = yj − (cid:80)\n\nλlsign(yj).\n\nl∈Jl s.t. j∈Jl\n\n• If j ∈ I+, then q∗\n\nj = yj − (cid:80)\n\nλl.\n\nl∈Jl s.t. j∈Jl\n\nl∈Jl s.t. j∈Jl\n\n∁ As a result, defining Ia = (∩lJl) sparsity constraints, we can write the corresponding output dynamics as\n\nas the set of dimension indices which do not appear in the\n\n∇y(k)J (y(k; ν)) = γyBζy\n\ny (k − 1)y(k; ν) − γeBζe\n\ne (k − 1)e(k; ν),\n\n ̄y(k; ν + 1) = y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))\n\nyj(k; ν + 1) = STαj (k,ν) ( ̄yj(k; ν + 1)) where αj(k; ν) =\n\n(A.18) λl(k; ν) ∀j ∈ Is ∩ I ∁\n\na\n\n(cid:88)\n\nl∈Jl s.t. j∈Jl\n\nyj(k; ν + 1) = ReLU\n\n ̄yj(k; ν + 1) −\n\n(cid:18)\n\n(cid:19)\n\nλl(k; ν)\n\n∀j ∈ I+ ∩ I ∁\n\na\n\n(cid:88)\n\nl∈Jl s.t. j∈Jl\n\nyj(k; ν + 1) = σ1( ̄yj(k; ν)) ∀j ∈ Is ∩ Ia, yj(k; ν + 1) = σ+( ̄yj(k; ν)) ∀j ∈ I+ ∩ Ia\n\nFor inhibitory neurons corresponding to Lagrangian variables λ1, . . . , λL, we obtain the update dynamics based on the derivative of L(y(k; ν), λ1(k; ν), . . . , λL(k; ν)) as (cid:12) (cid:12) (cid:12)λl(k;ν)\n\ndL(y(k), λ1(k), . . . , λL(k)) dλl(k)\n\n= 1 − ∥[y(k; ν + 1)]Jl ∥1 ∀l,\n\n ̄λl(k; ν + 1) = λl(k; ν)\n\n− ηλl (ν)\n\ndL(y(k), λ1(k), . . . , λL(k)) dλl(k) (cid:19) .\n\n ̄λl(k; ν + 1)\n\n(cid:18)\n\n(cid:12) (cid:12) (cid:12)λl(k;ν)\n\n,\n\nλl(k, ν + 1) = ReLU\n\nIn Appendix D.2.4, we demonstrate an example setting in which the underlying domain is defined as\n\nPex =\n\n \n\n\n\ns ∈ R5\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ns1, s2, s4 ∈ [−1, 1], s3, s5 ∈ [0, 1], (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:34) s2 s3 s4\n\n(cid:34) s1 s2 s5\n\n(cid:35)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n(cid:35)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n≤ 1,\n\n≤ 1\n\n \n\n\n\n.\n\nWe summarize the neural dynamics for this specific example:\n\ny1(k; ν + 1) = STλ1(k,ν) ( ̄y1(k; ν + 1)) , y2(k; ν + 1) = STλ1(k,ν)+λ2(k,ν) ( ̄y2(k; ν + 1)) , y3(k; ν + 1) = ReLU(cid:0) ̄y3(k; ν + 1) − λ2(k; ν)(cid:1), y4(k; ν + 1) = STλ2(k,ν) ( ̄y4(k; ν + 1)) , y5(k; ν + 1) = ReLU(cid:0) ̄y5(k; ν + 1) − λ1(k; ν)(cid:1), λ1(k; ν + 1) = λ1(k; ν) − ηλ1(ν)(cid:0)1 − |y1(k; ν + 1)| − |y2(k; ν + 1)| − y5(k; ν + 1)(cid:1), λ2(k; ν + 1) = λ2(k; ν) − ηλ1(ν)(cid:0)1 − |y2(k; ν + 1)| − y3(k; ν + 1) − |y4(k; ν + 1)|(cid:1),\n\nwhere ̄y(k; ν) is defined as in (A.18).\n\nD SUPPLEMENTARY ON NUMERICAL EXPERIMENTS\n\nIn this section, we provide more details on the algorithmic view of the proposed approach and the numerical experiments presented. In addition, we provide more examples.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nD.1 ONLINE CORINFOMAX ALGORITHM IMPLEMENTATIONS FOR SPECIAL SOURCE\n\nDOMAINS\n\nAlgorithm 2 summarizes the dynamics of the CorInfoMax network and learning rules. For each of the domain choices, the recurrent and feedforward weight updates follow (14) and (15), respectively, and the learning step is indicated in the 4th and 5th lines in the pseudo-code. The line 3rd expresses the recursive neural dynamics to obtain the output of the network, and its implementation differs for different domain choices. Based on the derivations in Section 3 and Appendix C, Algorithm 3, 4, 5, and 6 summarizes the neural dynamic iterations for some example domains. For example, Algorithm 5 indicates the procedure to obtain the output y(k) of the antisparse CorInfoMax network at time step k corresponding to the mixture vector x(k). As it is an optimization process, we introduce two variables for implementation in digital hardware: 1) numerical convergence tolerance εt, and 2) maximum number of (neural dynamic) iterations νmax. We run the proposed neural dynamic iterations until either a convergence happens, i.e., ||y(k; ν) − y(k; ν − 1)||/||y(k; ν)|| > εt, or the loop counter reaches a predetermined maximum number of iterations, that is, ν = νmax. Differently from the antisparse and nonnegative antisparse networks, the other CorInfoMax neural networks include additional inhibitory neurons due to the Lagrangian Min-Max settings, and the activation of these neurons are coupled with the network’s output. Therefore, inhibitory neurons are updated in neural dynamics based on the gradient of the Lagrangian objective.\n\nk=1.\n\nk=1.\n\nAlgorithm 2 Online CorInfoMax pseudo-code Input: A streaming data of {x(k) ∈ Rm}N Output: {y(k) ∈ Rn}N 1: Initialize ζy, ζe, μ(1)W , W (1), Bζy 2: for k = 1, 2, . . . , N do 3: 4: W (k + 1) = W (k) + μW (k)e(k)x(k)T 1\n5: Bζy Bζy ζy Adjust μW (k + 1) if necessary.\n\n1 − ζy ζy\n\ny (k + 1) =\n\ny (k) −\n\n(Bζy\n\ny (1), Bζe\n\n6: 7: end for\n\ne (1), and select P.\n\ny (k)y(k)y(k)T Bζy\n\ny (k))\n\nrun neural dynamics (in Algorithms 3 to 5 below according to P) until convergence .\n\nAlgorithm 3 Online CorInfoMax neural dynamic iterations: sources in unit simplex\n\ny(k; ν + 1) = ReLU (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν)) − λ(ν)(cid:1)\n\n1: Initialize νmax, εt, ηy(1), ηλ(1), λ(1)( = 0 in general) and ν = 1 2: while (||y(k; ν) − y(k; ν − 1)||/||y(k; ν)|| > εt) and ν < νmax do 3: ∇y(k)J (y(k; ν)) = γyBζy 4: 5: ∇λ(k)L(y(k; ν)) = 1 − ∥y(k; ν + 1)∥1 6: 7: 8: end while\n\nλ(k; ν + 1) = λ(k; ν) − ηλ(ν)∇λ(k)L(y(k; ν)) ν = ν + 1, and adjust ηy(ν), ηλ(ν) if necessary.\n\ny (k)y(k; ν) − γeBζe\n\ne (k)e(k; ν)\n\nAlgorithm 4 Online CorInfoMax neural dynamic iterations: sparse sources\n\n1: Initialize νmax, εt, ηy(1), ηλ(1), λ(1)( = 0 in general) and ν = 1 2: while (||y(k; ν) − y(k; ν − 1)||/||y(k; ν)|| > εt) and ν < νmax do 3: ∇y(k)J (y(k; ν)) = γyBζy y(k; ν + 1) = STλ(ν) 4: 5: ∇λ(k)L(y(k; ν)) = 1 − ∥y(k; ν + 1)∥1 6: 7: 8: end while\n\nλ(k; ν + 1) = ReLU (cid:0)λ(k; ν) − ηλ(ν)∇λ(k)L(y(k; ν))(cid:1) ν = ν + 1, and adjust ηy(ν), ηλ(ν) if necessary.\n\n(cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν)) − λ(ν)(cid:1)\n\ny (k)y(k; ν) − γeBζe\n\ne (k)e(k; ν)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 5 Online CorInfoMax neural dynamic iterations: antisparse sources\n\n1: Initialize νmax, εt, ηy(1) and ν = 1 2: while (||y(k; ν) − y(k; ν − 1)||/||y(k; ν)|| > εt) and ν < νmax do 3: ∇y(k)J (y(k; ν)) = γyBζy 4: 5: 6: end while\n\ny(k; ν + 1) = σ1 ν = ν + 1, and adjust ηy(ν) if necessary.\n\ny (k)y(k; ν) − γeBζe (cid:0)y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))(cid:1)\n\ne (k)e(k; ν)\n\nAlgorithm 6 Online CorInfoMax neural dynamic iterations for Canonical Form\n\ny (k)y(k; ν) − γeBζe y(k; ν + 1) = y(k; ν) + ηy(ν)∇y(k)J (y(k; ν))\n\n1: Initialize νmax, εt, ηy(1), ηλ(1), λ(1)( = 0 in general) and ν = 1 2: while (||y(k; ν) − y(k; ν − 1)||/||y(k; ν)|| > εt) and ν < νmax do 3: ∇y(k)J (y(k; ν)) = γyBζy 4: 5: ∇λ(k)L(y(k; ν)) = −AP y(k; ν) + bP 6: 7: 8: end while\n\nλ(k; ν + 1) = ReLU (cid:0)λ(k; ν) − ηλ(ν)∇λ(k)L(y(k; ν))(cid:1) ν = ν + 1, and adjust ηy(ν), ηλ(ν) if necessary.\n\ne (k)e(k; ν) − AT\n\nP λ(ν)\n\nD.2 ADDITIONAL NUMERICAL EXPERIMENTS FOR SPECIAL SOURCE DOMAINS\n\nD.2.1 SPARSE SOURCE SEPARATION\n\nIn this section, we illustrate blind separation of sparse sources, i.e. s(i) ∈ Bl1 ∀i. We consider n = 5 sources and m = 10 mixtures. For each source, we generate 5 × 105 samples in each realization of the experiments. We examine two different experimental factors: 1) output SINR performance as a function of mixture SNR levels, and 2) output SINR performance for different distribution selections for the entries of the mixing matrix.\n\nFor the first scenario with different mixture SNR levels: the sources are mixed through a random matrix A ∈ R10×5 whose entries are drawn from i.i.d. standard normal distribution, and the mixtures are corrupted by WGN with 30dB SNR. We compare our approach with the WSM (Bozkurt et al., 2022), LD-InfoMax (Erdogan, 2022) and PMF (Tatli & Erdogan, 2021) algorithms and visualize the results in Figure 8. We also use the portion of the mixtures to train batch LD-InfoMax and PMF algorithms. Figure 8a illustrates the SINR performances of these algorithms for different input noise levels. The SINR results of CorInfoMax, LD-InfoMax, and PMF are noticeably close to each other, which is almost equal to the input SNR. Figure 8b illustrates the SINR convergence of the sparse CorInfoMax network for the 30dB mixture SNR level as a function of update iterations. Based on this figure, we can conclude that the proposed CorInfoMax network converges robustly and smoothly.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 8: SINR performances of CorInfoMax (ours), LD-InfoMax, PMF, and WSM (averaged over 50 realizations) for sparse sources: (a) output SINR results (vertical axis) with respect to input SNR levels (horizontal axis), (b) SINR (vertical axis) convergence plot as a function of iterations (horizontal axis) of sparse CorInfoMax for the 30 dB SNR level (mean solid line and standard deviation envelopes).\n\nIn the second experimental setting, we examine the effect of the distribution choice for generating the random mixing matrix. Figure 9 illustrates the box plots of SINR results for CorInfoMax, LD-InfoMax, and PMF for different distribution selections to generate the mixing matrix, which are N (0, 1), U[−1, 1], U[−2, 2], L(0, 1). where N is normal distribution, U is uniform distribution, and L is the Laplace distribution. It is observable that the performance of CorInfoMax is robust against the different distribution selections of the unknown mixing matrix, while its performance is on par with the batch algorithms LD-InfoMax and PMF.\n\nFigure 9: SINR performances of CorInfoMax (ours), LD-InfoMax, and PMF with different distribution selections for the mixing matrix entries and for sparse sources. The horizontal axis represents different distribution choices, and the vertical axis represents the SINR levels.\n\nD.2.2 NONNEGATIVE SPARSE SOURCE SEPARATION\n\nWe replicate the first experimental setup in Appendix D.2.1 for the nonnegative sparse source separation: evaluate the SINR performance of the nonnegative sparse CorInfoMax network for different levels of mixture SNR, compared to the batch LD-InfoMax algorithm and the biologically plausible WSM neural network. In these experiments, n = 5 uniform sources in Bl1,+ are randomly mixed to generate m = 10 mixtures. Figure 10a illustrates the averaged output SINR performances of each algorithm with a standard deviation envelope for different input noise levels. In Figure 10b, we observe the SINR convergence behavior of nonnegative sparse CorInfoMax as a function of update iterations. Note that CorInfoMax outperforms the biologically plausible neural network WSM for all input SNR levels, and its convergence is noticeably stable.\n\n24\n\n10.012.515.017.520.022.525.027.530.0Input SNR (dB)051015202530SINR (dB)Sparse Source Separation SINR ResultsCorInfoMaxWSMLD-InfoMaxPMF0.02.55.07.510.012.515.017.520.0Number of Iterations / 25000051015202530SINR (dB)Online CorInfoMax Sparse SINR Convergence (0,1)[1,1][2,2](0,1)25262728293031323334Online CorInfoMax Results(0,1)[1,1][2,2](0,1)25262728293031323334LD-InfoMax Results(0,1)[1,1][2,2](0,1)25262728293031323334PMF ResultsDistribution of the Entries of Mixing Matrix SINR (dB)Published as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 10: SINR performances of CorInfoMax (ours), LD-InfoMax, and WSM for nonnegative sparse sources: (a) the output SINR (vertical axis) results with respect to the input SNR levels (horizontal axis), (b) the SINR (vertical axis) convergence plot as a function of iterations (horizontal axis) of nonnegative sparse CorInfoMax for the 30dB SNR level (mean solid line and standard deviation envelopes).\n\nD.2.3 SIMPLEX SOURCE SEPARATION\n\nWe repeat both experimental settings in Appendix D.2.1 for the blind separation of simplex sources using the CorInfoMax network in Figure 7. Figure 11a shows the output SINR results of both the online CorInfoMax and the batch LD-InfoMax approaches for different mixture SNR levels. Even though simplex CorInfoMax is not as successful as the other examples (e.g., sparse CorInfoMax), in terms of the closeness of its performance to the batch algorithms, it still has satisfactory source separation capability. Similarly to the sparse network examples, its SINR convergence is fast and smooth, as illustrated in Figure 11b.\n\n(a)\n\n(b)\n\nFigure 11: SINR performances of CorInfoMax (ours) and LD-InfoMax (averaged over 50 realizations) for unit simplex sources: (a) the output SINR (vertical axis) results with respect to the input SNR levels (horizontal axis), (b) SINR (vertical axis) convergence plot as a function of iterations (horizontal axis) of simplex-CorInfoMax for 30dB SNR level (mean solid line and standard deviation envelopes).\n\nFigure 12 shows the box plots of the SINR performances for both CorInfoMax and LD-InfoMax approaches with respect to different distribution selections for generating the random mixing matrix. Based on this figure, we can conclude that the simplex CorInfoMax network significantly maintains its performance for different distributions for the entries of the mixing matrix.\n\n25\n\n10.012.515.017.520.022.525.027.530.0Input SNR (dB)051015202530SINR (dB)Nonnegative Sparse Source Separation SINR ResultsCorInfoMaxWSMLD-InfoMax0.02.55.07.510.012.515.017.520.0Number of Iterations / 25000051015202530SINR (dB)Online CorInfoMax Nonnegative Sparse SINR Convergence 10.012.515.017.520.022.525.027.530.0Input SNR (dB)051015202530SINR (dB)Simplex Source Separation SINR ResultsCorInfoMaxLD-InfoMax0.02.55.07.510.012.515.017.520.0Number of Iterations / 250000510152025SINR (dB)Online CorInfoMax Simplex SINR Convergence Published as a conference paper at ICLR 2023\n\nFigure 12: SINR performances of CorInfoMax (ours) and LD-InfoMax with different distribution selections for the mixing matrix entries (averaged over 50 realizations) and unit simplex sources. The horizontal axis represents different distribution choices, and the vertical axis represents the SINR levels.\n\nD.2.4 SOURCE SEPARATION FOR POLYTOPES WITH MIXED LATENT ATTRIBUTES\n\nIn this section, we demonstrate the source separation capability of CorInfoMax on an identifiable polytope with mixed features, which is a special case of feature-based polytopes in (2). We focus on the polytope\n\nPex =\n\n \n\n\n\ns ∈ R5\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ns1, s2, s4 ∈ [−1, 1], s3, s5 ∈ [0, 1], (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:34) s2 s3 s4\n\n(cid:34) s1 s2 s5\n\n(cid:35)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n(cid:35)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n≤ 1,\n\n≤ 1\n\n \n\n\n\n,\n\n(A.19)\n\nwhose identifiability property is verified by the identifiable polytope characterization algorithm presented in Bozkurt & Erdogan (2022). We experiment with both approaches discussed in Section 3.3 and Appendix C.5. For the feature-based polytope setting introduced in Appendix C.5, the output dynamics corresponding to Pex is also summarized as an example. This polytope can also be represented as the intersection of 10 half-spaces, that is, Pex = {s ∈ Rn|AP s ≼ bP } where AP ∈ R10×5 and bP ∈ R10. Therefore, using AP and bP , we can also employ the neural network illustrated in Figure 1a with 10 inhibitory neurons.\n\nFor this BSS setting, we generated the sources uniformly within this 5 dimensional polytope Pex where the sample size is 5×105. The source vectors are mixed through a random matrix A ∈ R10×5 with standard normal entries. Figure 13a and 13b show the SINR convergence of CorInfoMax networks based on the feature-based polytope representation in (2) and the H-representation in (1) respectively, for the mixture SNR level of 30dB. Moreover, Figure 13c and 13d illustrate their SINR convergence curves for the SNR level of 40dB. In Table 2, we compare both approaches with the batch algorithms LD-InfoMax and PMF.\n\n26\n\n(0,1)[1,1][2,2](0,1)2022242628303234Online CorInfoMax Results(0,1)[1,1][2,2](0,1)2022242628303234LD-InfoMax ResultsDistribution of the Entries of Mixing Matrix SINR (dB)Published as a conference paper at ICLR 2023\n\n(a)\n\n(c)\n\n(b)\n\n(d)\n\nFigure 13: SINR performances of CorInfoMax networks on Pex with mean-solid line with standard deviation envelope (averaged over 50 realization) for the polytope with mixed latent attributes: (a) SINR convergence curve for CorInfoMax feature-based polytope formulation with 30dB mixture SNR, (b) SINR convergence curve for CorInfoMax canonical formulation with 30dB mixture SNR, (c) SINR convergence curve for CorInfoMax feature-based polytope formulation with 40dB mixture SNR, (d) SINR convergence curve for CorInfoMax canonical formulation with 40dB SNR.\n\nTable 2: Source separation averaged SINR results on Pex for the CorInfoMax (ours), CorInfoMax Canonical (ours) LD-InfoMax, and PMF algorithms (averaged for 50 realizations).\n\nAlgorithm\n\nCorInfoMax\n\nCorInfoMax Canonical\n\nSINR (\\w 30dB SNR) SINR (\\w 40dB SNR)\n\n26.55 30.93\n\n24.85 26.19\n\nLD-InfoMax PMF\n\n30.28 38.50\n\n27.68 31.48\n\nD.3 APPLICATIONS\n\nWe present several potential applications of the proposed approach, for which we illustrate the usage of antisparse, nonnegative antisparse and sparse CorInfoMax neural networks. This section demonstrates sparse dictionary learning, source separation for digital communication signals with 4-PAM modulation scheme, and video separation.\n\n27\n\n0.02.55.07.510.012.515.017.520.0Number of Iterations / 25000051015202530SINR (dB)Online CorInfoMax Special Polytope SINR Convergence 0.02.55.07.510.012.515.017.520.0Number of Iterations / 25000051015202530SINR (dB)Online CorInfoMax-Canonical Special Polytope SINR Convergence 0.02.55.07.510.012.515.017.520.0Number of Iterations / 2500005101520253035SINR (dB)Online CorInfoMax Special Polytope SINR Convergence 0.02.55.07.510.012.515.017.520.0Number of Iterations / 2500005101520253035SINR (dB)Online CorInfoMax-Canonical Special Polytope SINR Convergence Published as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 14: (a) Sparse dictionary learned by sparse CorInfoMax from natural image patches, (b) The SINR (vertical axis) convergence curve for the 4-PAM digital communication example as a function of iterations (horizontal axis) which is averaged over 100 realizations: mean-solid line with standard deviation envelope.\n\nD.3.1 SPARSE DICTIONARY LEARNING\n\nWe consider sparse dictionary learning for natural images, to model receptive fields in the early stages of visual processing (Olshausen & Field, 1997). In this experiment, we used 12 × 12 prewhitened image patches as input to the sparse CorInfoMax network. The image patches are obtained from the website http://www.rctn.org/bruno/sparsenet. The inputs are vectorized to the shape 144×1 before feeding to the neural network illustrated in Figure 2. Figure 14a illustrates the dictionary learned by the sparse CorInfoMax network.\n\nD.3.2 DIGITAL COMMUNICATION EXAMPLE: 4-PAM MODULATION SCHEME\n\nOne successful application of antisparse source modeling to solve BSS problems is digital communication systems Cruces (2010); Erdogan (2013). In this section, we verify that the antisparse CorInfoMax network in Figure 4 can separate 4-pulse-amplitude-modulation (4-PAM) signals with domain {−3, −1, 1, 3} (with a uniform probability distribution). We consider that 5 digital communication (4-PAM) sources with 105 samples are transmitted and then mixed through a Gaussian channel to produce 10 mixtures. The mixtures can represent signals received at some base station antennas in a multipath propagation environment. Furthermore, the mixtures are corrupted by WGN that corresponds to SNR level of 30dB. We feed the mixtures to the antisparse CorInfoMax network illustrated as input. For 100 different realization of the experimental setup, Figure 14b illustrates the SINR convergence as a function of update iterations. We note that the proposed approach distinguishably converges fast and each realization of the experiments resulted in a zero symbol error rate.\n\nD.3.3 VIDEO SEPARATION\n\nWe provide more details on the video separation experiment discussed in Section 4.2. Three source videos we used in this experiment are from the website https://www.pexels.com/, which are free to download and use. Videos are mixed linearly through a randomly selected nonnegative 5 × 3 matrix\n\nA =\n\n\n\n \n \n\n1.198 0.367 0.100 1.257 0.957\n\n1.020 1.364 0.869 0.859 0.789\n\n\n\n \n \n\n1.273 0.901 0.627 0.015 0.592\n\n,\n\nto generate 5 mixture videos. Figures 15a and 15b illustrate the last RGB frames of the original and mixture videos, respectively. To train the nonnegative antisparse CorInfoMax network in Figure 5 for the separation of the videos, we followed the procedure below.\n\n28\n\n0.00.51.01.52.02.53.03.54.0Number of Iterations / 25000051015202530SINR (dB)Online CorInfoMax 4-PAM SINR Convergence Published as a conference paper at ICLR 2023\n\n• In each iteration of the algorithm, we randomly choose a pixel location and select pixels from one\n\nof the color channels of all mixture videos to form a mixture vector of size 5 × 1,\n\n• We sample 20 mixture vectors from each frame and perform 20 algorithm iterations per frame\n\nusing these samples.\n\nThe demonstration video contains three rows of frames: the first row contains the source frames, the second row contains three of the five mixture frames, and the last row contains the network outputs for these mixture frames. Demo video is located at (https://figshare.com/s/a3fb926f273235068053). It is also included in supplementary files. If we use the separator matrix of the CorInfoMax network, which is an estimate for the left inverse of A, to predict the original videos after training, we obtain PSNR values of 35.60dB, 48.07dB, and 44.58dB for the videos, which are calculated as the average PSNR levels of the frames. Figure 15c illustrates the final output frames of the CorInfoMax.\n\n(a)\n\n(b)\n\n(c)\n\nFigure 15: Video separation example: outputs of the nonnegative antisparse CorInfoMax network.\n\nthe final frames of (a) sources, (b) mixtures, and (c) the\n\nD.3.4\n\nIMAGE SEPARATION\n\nRelated to the example in the previous section, we consider an example on the blind separation of correlated images from their linear mixtures. In this experiment, we consider 3 RGB natural scenes with sizes 454 × 605 that are illustrated in Figure 16a. The Pearson correlation coefficient for these sources are ρ12 = 0.076, ρ13 = 0.262, ρ23 = 0.240, respectively. The image sources are mixed through a random matrix A ∈ R5×3 whose entries are drawn from i.i.d. standard normal distribution. Moreover, the mixtures are corrupted with WGN corresponding to 40dB SNR. The\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nmixture images are demonstrated in Figure 16b, and the mixing matrix for this particular example is\n\nA =\n\n\n\n \n \n\n1.757 0.650 −0.363 1.568 1.487 1.100 0.032 −0.417 −1.266 1.260 −0.822 0.643 0.661 −0.023 −0.752\n\n\n\n \n \n\n.\n\nWe experiment with the proposed antisparse CorInfomax and biologically plausible NSM and WSM networks, and batch ICA-InfoMax and LD-InfoMax frameworks to separate the original sources, and Figures 16c-16g show the corresponding outputs. We note that the residual interference effects in the output images of ICA-InfoMax algorithm is remarkably perceivable, and the resulting PSNR values are 18.56dB, 20.52dB, and 21.06dB, respectively. The NSM algorithm’s outputs are visually better whereas some interference effects are still noticable, and the resulting PSNR values are 25.30dB, 26.49dB, 26.45dB. The visual interference effects in the WSM algorithm’s outputs are barely visible, and, therefore, they achieve higher PSNR values of 27.99dB, 29.71dB, and 31.92dB. The batch LD-InfoMax algorithm achieves the best PSNR performances which are 33.60dB, 31.99dB, and 33.62dB. Finally, we note that our proposed CorInfoMax method outperforms other biologically plausible neural networks and the batch ICA-InfoMax algorithm, while its performance is on par with the batch LD-InfoMax algorithm, and its output PSNR values are 32.45dB, 29.72dB, and 32.37dB. We note that ICA and NSM algorithms assume independent and uncorrelated sources, respectively, so their performances are remarkably affected by the correlation level of the sources. Typically, the best performance is obtained by LD-InfoMax due to its batch nature and dependent source separation capability. Finally, we notice that the biologically plausible WSM network is able to separate correlated sources whereas CorInfoMax performs better both visually and in terms of the PSNR metric. The hyperparameters used in this experiment for CorInfoMax are included in Appendix D.4, and the codes to reproduce each output are included in our supplementary material.\n\n(a)\n\n(b)\n\n(c)\n\nFigure 16: Image separation example: (a) Original RGB images, (b) mixture RGB images, (c) ICA outputs.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\n(d)\n\n(e)\n\n(f)\n\n(g)\n\nFigure 16: (d) NSM outputs, (e) WSM outputs, (f) LD-InfoMax Outputs, (g) CorInfoMax (ours) Outputs.\n\nD.4 HYPERPARAMETER SELECTIONS\n\nHyperparameter selection has a critical impact on the performance of the neural networks offered in this article. In this section, we provide the list of our hyperparameter selections for the experiments provided in the article. These parameters are selected through ablation studies and some trials, which are discussed in Appendix D.5.\n\nTable 3 summarizes the hyperparameter selections for the special domain choices provided in Section 2.1. Based on this table, we can observe that the hyperparameter sets for different special source domains generally resemble each other. However, there are some noticable domain-specific changes in some parameters, such as the starting learning rate for neural dynamics (ηy(1)) and the learning rate for the Lagrangian variable (ηλ(ν)).\n\nFor the other experiments presented in the Appendices D.2.4 and D.3, Table 4 summarizes the corresponding hyperparameter selections.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: CorInfoMax network hyperparameter selections for special source domains in 2.1.\n\nSource Domain\n\nHyperparameters\n\nP = Bl∞\n\nζy = 1 − 10−2,\n\nP = Bl∞,+\n\nP = B1\n\nW (1) = I, Bζy\n\ny (1) = 5I, Bζe\n\ne (1) = 5000I\n\nζe = 1 − 2 × 10−2, μW = 3 × 10−2\n\nνmax = 500, W (1) = I, Bζy\n\nηy(ν) = 0.9/ν, y (1) = 5I, Bζe\n\nεt = 10−6 e (1) = 2000I\n\nζy = 1 − 10−2, νmax = 500,\n\nζe = 1 − 10−1/3, μW = 3 × 10−2 εt = 10−6\n\nηy(ν) = max{0.9/ν, 10−3}, y (1) = I, Bζe ζe = 1 − 10−2, μW = 3 × 10−2\n\ne (1) = 1000I\n\nW (1) = I, Bζy\n\nζy = 1 − 10−2,\n\nνmax = 500,\n\nηλ(ν) = 1,\n\nW (1) = I, Bζy\n\nηy(ν) = max{0.1/ν, 10−3}, εt = 10−6 y (1) = 5I, Bζe ζe = 1 − 10−2, μW = 3 × 10−2\n\ne (1) = 1000I\n\nηy(ν) = max{0.1/ν, 10−3}, εt = 10−6 y (1) = 5I, Bζe ζe = 1 − 10−2, μW = 3 × 10−2\n\ne (1) = 1000I\n\nηλ(ν) = 1,\n\nW (1) = I, Bζy\n\nηy(ν) = max{0.1/ν, 10−3}, εt = 10−6\n\nηλ(ν) = 0.05,\n\nP = B1,+\n\nζy = 1 − 10−2,\n\nνmax = 500,\n\nP = ∆\n\nζy = 1 − 10−2,\n\nνmax = 500,\n\nTable 4: CorInfoMax network hyperparameter selections for the polytope with mixed latent attributes in (A.19) and application examples in Appendix D.3.\n\nExperiment\n\nHyperparameters\n\nPolytope in (A.19) with Mixed Attributes (Canonical)\n\nPolytope in (A.19) with Mixed Attributes (Feature-based)\n\nSparse Dictionary Learning\n\nDigital Communications\n\nVideo Separation\n\nImage Separation\n\nW (1) = I, Bζy\n\nζy = 1 − 10−2, νmax = 500,\n\nW (1) = I, Bζy\n\nζy = 1 − 10−2, νmax = 500,\n\nηλ(ν) = 0.1,\n\ne (1) = 1000I\n\ny (1) = I, Bζe ζe = 1 − 10−2, μW = 5 × 10−2 ηy(ν) = max{0.25/ν, 10−4}, εt = 10−6\n\ny (1) = 5I, Bζe ζe = 1 − 10−2, μW = 5 × 10−2 ηy(ν) = max{0.1/ν, 10−10},\n\ne (1) = 2500I\n\nηλ1(ν) = ηλ2(ν) = 1,\n\nW (1) = I, Bζy\n\ny (1) = I, Bζe\n\nεt = 10−6 e (1) = 20000I\n\nζy = 1 − 10−3/7,\n\nζe = 1 − 10−3/7, μW = 0.25 × 10−3\n\nνmax = 500,\n\nηy(ν) = max{1.5/(ν × 0.01), 10−8},\n\nηλ(ν) = 3 × 10−2,\n\nεt = 10−6\n\nW (1) = I, Bζy\n\nζy = 1 − 10−2,\n\ny (1) = 5I, Bζe ζe = 1 − 10−2, μW = 3 × 10−2\n\ne (1) = 1000I\n\nνmax = 500,\n\nηy(ν) = max{0.9/ν, 10−3},\n\nεt = 10−6\n\nW (1) = I, Bζy\n\nζy = 1 − 10−1/5,\n\ny (1) = I, Bζe\n\ne (1) = 65I\n\nζe = 0.3, μW = 6 × 10−2\n\nνmax = 500,\n\nηy(ν) = max{0.5/ν, 10−3},\n\nεt = 10−6\n\nW (1) = I, Bζy\n\nζy = 1 − 10−1/15,\n\ny (1) = I, Bζe\n\ne (1) = 100I\n\nζe = 0.5, μW = 5 × 10−2\n\nνmax = 500,\n\nηy(ν) = max{0.5/ν, 10−3},\n\nεt = 10−6\n\nD.5 ABLATION STUDIES ON HYPERPARAMETER SELECTIONS\n\nIn this section, we illustrate an ablation study on selection of two hyperparameters for the nonnegative antisparse CorInfoMax network: we experiment with selection for the learning rate of the\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nfeedforward weights μW and the initialization of the inverse error correlation matrix Bζe hyperparameters, we consider the following selections:\n\ne . For these\n\n• μW ∈ {5 × 10−3, 10−2, 3 × 10−2, 5 × 10−2}, • Bζe\n\ne ∈ {103, 2 × 103, 5 × 103, 104}.\n\nWe consider the experimental setup in Section 4.1 for both uncorrelated sources and correlated sources with correlation parameter ρ = 0.6. For the ablation study for μW , we used fixed Bζe e as indicated in Table 3, and vice versa.\n\nFigures 17a and 17b illustrate the mean SINR with standard deviation envelope results with respect to μW for uncorrelated and correlated source separation settings, respectively. Note that although selection μW = 5 × 10−2 seems to be better for uncorrelated sources, its performance degrades for correlated sources as for some of the realizations, the algorithm diverges. We conclude that the selection μW = 3 × 10−2 is near optimal in this setting and obtains good SINR results for both correlated and uncorrelated sources. For the initialization of Bζe e , Figures 17c and 17d illustrate the SINR performances of CorInfoMax for uncorrelated and correlated source separation experiments, respectively. Note that the selections Bζe e = 2000I and Bζe e = 5000I are suitable considering both settings.\n\n(a)\n\n(c)\n\n(b)\n\n(d)\n\nFigure 17: CorInfoMax ablation studies on the hyperparameter selections of μW and Bζe e : mean SINR with standard deviation envelopes corresponding to 50 realizations. (a) ablation study of μW on uncorrelated nonnegative antisparse sources, (b) ablation study of μW on correlated (with ρ = 0.6) nonnegative antisparse sources, (c) ablation study of Bζe e on uncorrelated nonnegative antisparse sources, (d) ablation study of Bζe e on correlated (with ρ = 0.6) nonnegative antisparse sources,\n\nD.6 ABLATION STUDIES ON THE EFFECT OF NUMBER OF MIXTURES\n\nThe number of mixtures with respect to the number of sources might be crucial for blind source separation problem, and it can affect the overall performance of the proposed method. To explore the impact of the number of mixtures on the SINR performance of our proposed method, we performed experiments with varying number of mixtures and fixed number of sources for nonnegative antisparse, i.e., P = B∞,+, and sparse, i.e., P = B1, source separation settings. In these experi-\n\n33\n\n5e-31e-23e-25e-2μW2224262830SINR (dB)5e-31e-23e-25e-2μW2224262830SINR (dB)1e32e35e31e4Be Gain2224262830SINR (dB)1e32e35e31e4Be Gain2224262830SINR (dB)Published as a conference paper at ICLR 2023\n\nmental settings, we consider 5 sources, and change the number of mixtures gradually from 5 to 10. Figure 18a and 18b illustrate the overall SINR performances with a standard deviation envelope for nonnegative antisparse CorInfoMax and sparse CorInfoMax networks with respect to the number of mixtures, which are averaged over 50 realizations, respectively. For each realization, we randomly generate a mixing matrix with i.i.d. standard normal entries. We observe that the performance of CorInfoMax networks monotonically improves as the number of mixtures increases. This aligns with the theoretical expectations that the condition of the random mixing matrix improve with increasing number of mixtures Chen & Dongarra (2005) which positively impacts the algorithm’s numerical performance.\n\n(a)\n\n(b)\n\nFigure 18: CorInfoMax ablation studies on the impact of number of mixtures with fixed number of sources (5 sources). (a) illustrates the SINR performances of nonnegative antisparse CorInfoMax network as a function of number of mixtures, (b) illustrates the SINR performances of sparse CorInfoMax network as a function of number of mixtures.\n\nD.7 COMPUTATIONAL COMPLEXITY OF THE PROPOSED APPROACH\n\nThe complexity of the proposed approach can be characterized based on the analysis of output dynamics and weight updates. For simplicity, we consider the complexity of the antisparse CorInfoMax network in Figure 4: Let νmax represent the maximum count for the neural dynamic iterations.\n\nNeural Dynamics’ Complexity: For one neural dynamic iteration, we observe the following number of operations\n\n• Error calculation e(k; ν) = y(k; ν) − W (k)x(k) requires nm multiplications, • Bζy\n\ny (k)y(k; ν) requires n2 multiplications,\n\n• Bζe\n\ne (k)e(k; ν) requires n multiplications as Bζe\n\ne (k) =\n\n1 ε\n\nI.\n\nAs a result, νmax neural dynamics iterations require νmax(mn+n2 +n) ≈ νmaxmn multiplications per output computation.\n\nWeight Updates’ Complexity: For the weight updates, we note the following number of operations\n\n• W (k + 1) = W (k) + μW (k)e(k)x(k)T requires 2mn multiplication, 1 − ζy ζy\n\ny (k)y(k)y(k)T Bζy\n\ny (k + 1) =\n\ny (k) −\n\n• Bζy\n\n(Bζy\n\n1 ζy\n\nBζy\n\ny (k)) requires n2 + n2 + n2 + 1 +\n\nn(n + 1) 2\n\n=\n\n7n2 + n + 2 2\n\nmultiplications.\n\nTherefore, weight updates for learning require 2mn + 7n2+n+2\n\n2\n\nmultiplications per input sample.\n\nTaking into account both components of the computations, complexity is dominated by the neural dynamics iterations which require approximately O(νmaxmn) operations. This is in the same order as the complexity reported in Bozkurt et al. (2022) for biologically plausible WSM, NSM, and BSM networks.\n\n34\n\n5678910Number of Mixtures1015202530SINR (dB)5678910Number of Mixtures1015202530SINR (dB)",
    "reference": "# Summary Of The Paper\n\nI thank the authors for providing a thoughtful reply addressing most of my concerns. Together with the clarifications I am now convinced that this work is a good contribution. I have adjusted my score accordingly.\n\nThe paper proposes a biologically plausible source separation method with local updates and batched online learning.\n\nI can see that the paper is based on a recent publication by Erdogan (2022). Generally, this work cites a large number of previous works by that author. I cannot judge how novel the contribution in this paper is compared to those previous works. The framework and mathematical formulation are extremely complex and I was not able, within reasonable time investment, to digest and asses the full breadth of it. The applications seem somewhat niche. And the only real data application does, as far as I can tell, not include the interesting setting of correlated sources. By the way, the same setting (in more interesting nonlinear scenarios) is also investigated in this (https://proceedings.mlr.press/v139/trauble21a.html) work.\n\n# Strength And Weaknesses\n\nThe paper is well written, with a thorough mathematical derivation. It also cites many relevant existing works.\n\nThe paper is extremely dense, possibly incremental on previous work, somewhat niche.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIt is not clear how important the biological plausibility is and that does not seem be further explore by the paper.\n\nNits:\n\nBoth the title and the abstract are very hard to understand and one is left wondering what the paper is about. Latent causes are not defined, domains are not defined, what are presumed sets?\n\nSecond paragraph: The fact that biological receptive fields look like ICA filters should not be confused with the much stronger claim that the brain is doing BSS.\n\nAll figures need more explanation in the caption.\n\n1.1.1 second to last sentence: ‘while’ ? ‘they’; autocorrelation\n\n2.1.iii superfluous dot?\n\n2.1 after list the “” quotation marks are wrongly formatted\n\nFig3 define the axis labels in the caption\n\n# Summary Of The Review\n\nThe work is hard to digest and it is not clear how incremental it is compared to previous work.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBETTER WITH LESS: DATA-ACTIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecently, pre-training on graph neural networks (GNNs) has become an active research area and is used to learn transferable knowledge for downstream tasks with unlabeled data. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training samples and graph datasets do not necessarily lead to better performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: few, but carefully chosen data are fed into a GNN model to enhance pre-training. This novel pretraining pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as the predictive uncertainty. The proposed predictive uncertainty, as feedback from the pre-training model, measures the confidence level of the model to the data. When fed with the chosen data, on the other hand, the pre-training model grasps an initial understanding of the new, unseen data, and at the same time attempts to remember the knowledge learnt from the previous data. Therefore, the integration and interaction between these two components form a unified framework, in which graph pre-training is performed in a progressive way. Experiment results show that the proposed APT framework is able to obtain an efficient pre-training model with fewer training data and better downstream performance.\n\nINTRODUCTION\n\n1 Pre-training Graph Neural Networks (GNNs) shows the potential to be an attractive and competitive strategy for learning graph representations without costly labels. However, its transferability is guaranteed only if the pre-training datasets come from the same or similar domain as the downstream Hu et al. (2019; 2020b); You et al. (2020a;b); Hu et al. (2020c); Li et al. (2021); Lu et al. (2021); Sun et al. (2021). When we have no knowledge of the downstream, an encouraging yet largely unexplored research direction is pre-training GNNs on cross-domain data Qiu et al. (2020); Hafidi et al. (2020). Taking the graphs from multiple domains as the input, graph pre-training is able to learn the transferable structural patterns in graphs (when some semantic meanings are present), or to obtain the capability of discriminating these patterns.\n\nWith diverse and various cross-domain data, the success of a graph pre-training model is often attributed to the massive amount of unlabeled training data, a well-established fact for pre-training in computer vision Girshick et al. (2014); Donahue et al. (2014); He et al. (2020) and natural language processing Mikolov et al. (2013); Devlin et al. (2019). In view of this, contemporary research almost has no controversy on the following issue: Is a massive amount of input data really necessary, or even beneficial, for pre-training GNNs? However, two simple experiments regarding the number of training samples and graph datasets seem to doubt the positive answer to this question. The first observation is that scaling pre-training samples does not result in a one-model-fits-all increase in downstream performance (see the first row of Figure 1). Second, we observe that adding input graphs (while fixing sample size) does not improve and sometimes even deteriorates the generalization of the pre-trained model (see the second row in Figure 1). Furthermore, even if the number of input graphs (the horizontal coordinate) is fixed, the performance of the model pre-trained on different combinations of inputs varies dramatically; see the standard deviation in blue. As the first contribution, we identify the curse of big data phenomenon in graph pre-training: more training samples and graph datasets do not necessarily lead to better downstream performance.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Top row: The effect of scaling up sample size (log scale) on the downstream performance based on a group of GCCs Qiu et al. (2020) under different configurations (the graphs used for pre-training are kept as all eleven pre-training data in Table 3, and the samples are taken from the backbone pre-training model according to its sampling strategy). The results for different downstream graphs (and tasks) are presented in separate figures. To better show the changing trend, we fit a curve to the best performing models (i.e., the convex hull fit as Abnar et al. (2022) does). Bottom row: The effect of scaling up the number of graph datasets on the downstream performance based on GCC. For a fixed horizontal coordinate, we run 5 trials. For each trial, we randomly choose a combination of input graphs. The shaded area indicates the standard deviation over the 5 trials. See Appendix D for more observations on other graph pre-training models and detailed settings.\n\nTherefore, instead of training on massive data, it is more appealing to choose wisely some samples and graphs for pre-training. However, without the knowledge of downstream tasks, the difficulty is how to design new criteria for selecting input data to the pre-training model. To fill this gap, we propose a novel graph selector that is able to provide the most instructive data for the model. The criteria in the graph selector include predictive uncertainty and graph properties. Predictive uncertainty is introduced to measure the level of confidence (or certainty) in the data. On the other hand, some graphs are more informative and representative than others, due to their inherent structure. To this end, some fundamental properties of graphs also help in the selection process.\n\nGiven the selected input data, we take full advantage of the predictive uncertainty as a proxy for measuring the model capability during the training phase. Instead of swallowing data as a whole, the pre-training model is encouraged to learn from the data in a progressive way. After learning a certain amount of training data, the predictive uncertainty gives feedback on what kind of data the model has least knowledge of. Then the pre-training model is able to reinforce itself on highly uncertain data in next training iterations.\n\nPutting together, we propose a data-active graph pre-training (APT) framework, which integrates the graph selector and the pre-training model into a unified framework. The two components in the framework actively cooperate with each other. The graph selector recognizes the most instructive data for the model. Equipped with this intelligent selector, the pre-training model is well-trained and in turn provides better guidance for the graph selector.\n\nThe rest of the paper is organized as follows. In §2 we review the existing works about basic graph pre-training framework commonly used for training cross-domain graph data. Then in §3 we describe in detail the proposed data-active graph pre-training (APT) paradigm. §4 contains numerical experiments, which demonstrate the superiority of APT in different downstream tasks, especially when the test and training graphs come from different domains. Lastly, we also include the applicable scope of our pre-trained model.\n\n2 BASIC GRAPH PRE-TRAINING FRAMEWORK\n\nThis section reviews the basic framework of cross-domain graph pre-training commonly used in related literature. The backbone of our graph pre-training model also follows this framework, and uses GCC Qiu et al. (2020) as an instantiation. In principle, GCC can be substituted by any encoder suitable for training cross-domain graphs.\n\nWe start with a natural question: What does cross-domain graph pre-training actually learn? Previous studies argue that the semantic meaning associated with structural patterns is transferable. ) is For example, both in citation networks and social networks, the closed triangle structure ( ) indicates an unstable relationship. interpreted as a stable relationship, while the open triangle (\n\n2\n\nmsrc-21wisconsinimdb-binaryimdb-binarymsrc-21wisconsinbrazilbrazil# of graphs# of graphs# of graphs# of graphs# of samples# of samples# of samples# of samplesmicro F1micro F1micro F1micro F1micro F1micro F1micro F1micro F1(a) Node classification(b) Graph classificationstd : 3.1std :1.5std : 1.4std :1.4model config.5 layers4 layers3 layersconvex hull fit(all)(all)(all)(all)Under review as a conference paper at ICLR 2023\n\nWhen data comes from other domains like molecular networks, the semantic meaning can be quite different. Nevertheless, we argue that the distinction between different structural patterns is still transferable. Taking the same example, the closed and open triangles might yield different interpretations in molecular networks (unstable vs. stable in terms of chemical property) from those in social networks (stable vs. unstable in terms of social relationship), but the distinction between these two structures remains the same because they indicate opposite (or contrastive) semantic meanings. Therefore, the cross-domain pre-training either learns representative structural patterns (when semantic meanings are present), or more importantly, obtains the capability of distinguishing these patterns. This observation in graph pre-training is not only very different from that in other areas (e.g., computer vision and natural language processing), but may also explain why graph pre-training is effective, especially when some downstream information is absent.\n\nWith the hope to learn the transferable structural patterns or the ability to distinguish them, the crossdomain graph pre-training model is fed with a collection of input graphs (possibly from different domains), and the learnt model, denoted by fθ (or simply f if the parameter θ is clear from context), maps a node to a low-dimensional representation. Unaware of the specific downstream task as well as task-specific labels, one should design a self-supervised task for the pre-training model. Such self-supervised information for a node is usually hidden in its neighborhood pattern, and thus the structure of its ego network is often used as the transferable pattern. Naturally, subgraph instances sampled from the same ego network Γi are considered similar while those sampled from different ego networks are rendered dissimilar. Therefore, the pre-training model attempts to capture the similarities (and dissimilarities) between subgraph instances, and such a self-supervised task is called the subgraph instance discrimination task. More specifically, given a subgraph instance ζi from an ego network Γi centered at node vi as well as its representation xi = f (ζi), the model f aims to encourage higher similarity between xi and the representation of another subgraph instance ζ + sampled from the same ego network. This can be done by minimizing, e.g., the InfoNCE loss Oord et al. (2018):\n\ni\n\nLi = − log\n\nexp (cid:0)x⊤\n\ni f (ζ +\n\nexp (cid:0)x⊤ i f (ζ + i )/τ (cid:1) + (cid:80)\n\ni )/τ (cid:1)\n\nexp (cid:0)x⊤\n\ni f (ζ ′\n\ni)/τ (cid:1) ,\n\ni∈Ω− ζ′\n\ni\n\n(1)\n\ni\n\nis a collection of subgraph instances sampled from different ego networks Γj (j\n\nwhere Ω− = i), and τ is a temperature hyper-parameter. Here the inner product is used as a similarity measure between two instances. One common strategy to sample these subgraph instances is via random walks on graphs, as used in GCC Qiu et al. (2020), but other sampling methods as well as loss functions are also valid.\n\n3 DATA-ACTIVE GRAPH PRE-TRAINING\n\nIn this section we present the proposed APT framework for cross-domain graph pre-training, and the overall pipeline is illustrated in Figure 2. The APT framework consists of two major components, a graph selector and a graph pre-training model. The technical core is the interaction between these two components: The graph selector feeds suitable data for pre-training, and the graph pre-training model learns from the carefully chosen data. The feedback of the pre-training model in turn helps select the needed data tailored to the model.\n\nThe rest of this section is organized as follows. We describe the graph selector in § 3.1 and the graph pre-training model in § 3.2. The overall pre-training and fine-tuning strategy is presented in § 3.3.\n\n3.1 GRAPH SELECTOR In view of the curse of big data phenomenon, it is more appealing to carefully choose data well suited for graph pre-training rather than training on a massive amount of data. Conventionally, the criterion of suitable data, or the contribution of a data point to the model, is defined based on the output predictions on downstream tasks Goodfellow et al. (2016). In graph pre-training where downstream information is absent, new selection criteria or guidelines are needed to provide effective instructions for the model. Here we introduce two kinds of selection criteria, originated from different points of view, to help select suitable data for pre-training. The predictive uncertainty measures the model’s understanding of certain data, and thus helps select the least certain data points for the current model. In addition to the measure of model’s ability, some inherent properties of graphs can also be used to assess the level of representativeness or informativeness of a given graph. Predictive uncertainty. The notion of predictive uncertainty can be explained via an illustrative example, as shown in part (a) of the graph selector component in Figure 2. Consider a query sub-\n\n3\n\n̸ Under review as a conference paper at ICLR 2023\n\nFigure 2: Overview of the proposed data-active graph pre-training paradigm. The graph selector provides the graph and samples suitable for pre-training, while the graph pre-training model learns from the incoming data in a progressive way and in turn better guides the selection process. In the graph selector component, Part (a) provides an illustrating example on the predictive uncertainty, and Part (b) plots the Pearson correlation between the properties of the input graph and the performance of the pre-trained model (using this graph) on different unseen test datasets (see Appendix E for other properties that exhibit little/correlation with performance).\n\nin Figure 2) from the ego network Γi in a graph G. If the pre-training graph instance ζi (denote by model cannot tell its similar instance ζ + i (denoted by ), we say that the current model is uncertain about the query instance ζi. Therefore, the contrastive loss function in Eq. (1) comes in handy as a natural measure for the predictive uncertainty of the instance ζi: φuncertain(ζi) = i. Accordingly, the predictive uncertainty of a graph G L\n(i.e., the graph-level predictive uncertainty) is defined as φuncertain(G) = (1/M ) (cid:80)M i, where M is the number of subgraph instances queried in this graph.\n\n) from its dissimilar instance ζ −\n\n(denoted by\n\ni=1 L\n\ni ∈\n\nΩ−\n\ni\n\nThe proposed selection process is different from strategies used in curriculum learning Bengio et al. (2009). Predictive uncertainty encourages the model to learn more difficult (uncertain) graphs and samples in the first place, while in curriculum learning, the easiest samples are fed first. The choice of difficult-first order is intuitive in our case; see also Appendix G for empirical evidence. Graph properties. As we see above, the predictive uncertainty measures the model’s ability to distinguish (or identify) a given graph (or subgraph instance). However, predictive uncertainty is sometimes misleading, especially when the chosen graph (or subgraph) happens to be an outlier of the entire data collection. Hence learning solely from the most uncertain data might not improve the overall performance, or even worse, lead to overfitting. The inherent properties of the graph turn out to be equivalently important as a selection criterion for graph pre-training. Intuitively, it is preferable to choose those graphs that are good by themselves, those with a better structure, or those containing more information. So here we introduce five inherent properties of graphs (i.e., network entropy, density, average degree, degree variance and scale-free exponent) to help select better data points for pre-training. All these properties exhibit a strong correlation with downstream performance, which is empirically verified and presented in part (b) of the graph selector component in Figure 2. The choice of these properties also has an intuitive explanation, and here we discuss the intuition behind the network entropy as an example.\n\nThe use of network entropy is inspired from the sampling methods used in most cross-domain graph pre-training models (see e.g., Qiu et al. (2020); Hafidi et al. (2020)): Random walks started at a node are employed to construct a subgraph instance as the model input. Random walks can also be used to compute the amount of information contained in a graph. Especially, the amount of information log Pij Cover & Thomas (1999), where P is contained in the move from node vi to node vj is the transition matrix. Thus the network entropy of a connected graph G = (V, E) can be defined as the expected information of individual transitions over the random walk process Burda et al. (2009):\n\n−\n\n4\n\ngraph pre-training modelgraph selectorpre-training datadim2dim1---decision boundarysimilar/dissimilar instance of+-/(b) Graph properties??query instance with most uncertainty(a) Predictive uncertainty Combined for graph selectorThe graph and samples friendly to pre-trainingPearson correlation(cid:237)(cid:19)(cid:17)(cid:25)(cid:237)(cid:19)(cid:17)(cid:23)(cid:237)(cid:19)(cid:17)(cid:21)(cid:19)(cid:17)(cid:19)(cid:19)(cid:17)(cid:21)(cid:19)(cid:17)(cid:23)(cid:19)(cid:17)(cid:25)(cid:19)(cid:17)(cid:27)corapubmed brazil dd242dd68wisconsin cornellmicro F1 on?+graph modelinput data...new dataold dataregularization for old dataloss for new dataforwardpropbackprop@@✓<latexit sha1_base64=\"RsMEzc+g1+yzCRXelJDsSQbwe54=\">AAACCnicbZDLSsNAFIYn9VbrLerSzWgRXJWkCrosuHFZwV6gCWUynbRDJ5MwcyKU0LUbX8WNC0Xc+gTufBsnbUBt/WHg4z/nzMz5g0RwDY7zZZVWVtfWN8qbla3tnd09e/+greNUUdaisYhVNyCaCS5ZCzgI1k0UI1EgWCcYX+f1zj1TmsfyDiYJ8yMylDzklICx+vaxFypCMy8hCjgRePqDHowYkGnfrjo1Zya8DG4BVVSo2bc/vUFM04hJoIJo3XOdBPwsv5UKNq14qWYJoWMyZD2DkkRM+9lslSk+Nc4Ah7EyRwKeub8nMhJpPYkC0xkRGOnFWm7+V+ulEF75GZdJCkzS+UNhKjDEOM8FD7hiFMTEAKGKm79iOiImGzDpVUwI7uLKy9Cu19zzWv32otpwizjK6AidoDPkokvUQDeoiVqIogf0hF7Qq/VoPVtv1vu8tWQVM4foj6yPbxIXmxQ=</latexit>(cid:237)(cid:19)(cid:17)(cid:25)(cid:237)(cid:19)(cid:17)(cid:23)(cid:237)(cid:19)(cid:17)(cid:21)(cid:19)(cid:17)(cid:19)(cid:19)(cid:17)(cid:21)(cid:19)(cid:17)(cid:23)(cid:19)(cid:17)(cid:25)(cid:19)(cid:17)(cid:27)network entropydensityaverage degreedegree variancescale-free exponentother properties@@✓<latexit sha1_base64=\"nKog68C95ytFaqOen/Vj5mu31lM=\">AAACCnicbZDLSsNAFIYnXmu9RV26GS2Cq5JUQd0V3LisYC/QhDKZTtqhk0mYORFK6NqNr+LGhSJufQJ3vo2TNqC2/jDw8Z9zZub8QSK4Bsf5spaWV1bX1ksb5c2t7Z1de2+/peNUUdaksYhVJyCaCS5ZEzgI1kkUI1EgWDsYXef19j1TmsfyDsYJ8yMykDzklICxevaRFypCMy8hCjgRePKDHgwZkEnPrjhVZyq8CG4BFVSo0bM/vX5M04hJoIJo3XWdBPwsv5UKNil7qWYJoSMyYF2DkkRM+9l0lQk+MU4fh7EyRwKeur8nMhJpPY4C0xkRGOr5Wm7+V+umEF76GZdJCkzS2UNhKjDEOM8F97liFMTYAKGKm79iOiQmGzDplU0I7vzKi9CqVd2zau32vFK/KuIooUN0jE6Riy5QHd2gBmoiih7QE3pBr9aj9Wy9We+z1iWrmDlAf2R9fAMUf5sc</latexit>@@✓<latexit sha1_base64=\"nKog68C95ytFaqOen/Vj5mu31lM=\">AAACCnicbZDLSsNAFIYnXmu9RV26GS2Cq5JUQd0V3LisYC/QhDKZTtqhk0mYORFK6NqNr+LGhSJufQJ3vo2TNqC2/jDw8Z9zZub8QSK4Bsf5spaWV1bX1ksb5c2t7Z1de2+/peNUUdaksYhVJyCaCS5ZEzgI1kkUI1EgWDsYXef19j1TmsfyDsYJ8yMykDzklICxevaRFypCMy8hCjgRePKDHgwZkEnPrjhVZyq8CG4BFVSo0bM/vX5M04hJoIJo3XWdBPwsv5UKNil7qWYJoSMyYF2DkkRM+9l0lQk+MU4fh7EyRwKeur8nMhJpPY4C0xkRGOr5Wm7+V+umEF76GZdJCkzS2UNhKjDEOM8F97liFMTYAKGKm79iOiQmGzDplU0I7vzKi9CqVd2zau32vFK/KuIooUN0jE6Riy5QHd2gBmoiih7QE3pBr9aj9Wy9We+z1iWrmDlAf2R9fAMUf5sc</latexit>-1.00.01.0dim3selectadded toprovided to predictive uncertainty?+(cid:16)Under review as a conference paper at ICLR 2023\n\nφentropy = ⟨− log P ⟩P = −\n\n(cid:88)\n\nij\n\nπiPij log Pij,\n\n(2)\n\n⟨·⟩\n\nwhere π is the stationary distribution of a random walk and P denotes the expectation of a random variable according to P . Network entropy (2) is in general difficult to calculate, but for a connected )d unweighted graph, Pij = 1/di, π = (1/2 E\n| |\nV and d = (where di is the degree of node vi (d1, d2, . . .) is the degree vector). Then the network entropy (2) reduces to\n\n∈\n\nφentropy =\n\n1 2|E|\n\nN (cid:88)\n\ni=1\n\ndi log di,\n\n(3)\n\nFigure 3: The illustrative graphs (from bottom left to top right) with increasing network entropy and the other four graph properties.\n\nV\n\nwhere N = is the total number of nodes in G. In this case, the network entropy of a graph depends solely on its degree distribution, and is straightforward to compute.\n\n|\n\n|\n\nAlthough the definition of network entropy originates from random walks on graphs, it is still useful in graph pre-training even when the sampling of subgraph instances does not depend on random walks. Here we provide another intuitive explanation of network entropy from the coding theory. Network entropy can be viewed as the entropy rate of a random walk, and it is known that the entropy rate is the expected number of bits per symbol required to describe a stochastic process Cover & Thomas (1999). Similarly, the network entropy can be interpreted as the expected number of “words” needed to describe the graph. Thus intuitively, the larger the network entropy is, the more information the graph contains.\n\nAs a final remark on network entropy, the connectivity assumption does not limit the usefulness of Eq. (3) in our case. For disconnected input graphs, we can simply compute the network entropy of the largest connected component, since for most real-world networks, the largest connected component contains most of the information Easley & Kleinberg (2010). Alternatively, we can also take some of the largest connected components from the graph and treat them separately as several connected graphs.\n\nFurthermore, the other four graph properties, i.e., density, average degree, degree variance and scalefree exponent, are closely related to the network entropy. Figure 3 presents a clear correlation between the network entropy and the other four graph properties, as well as provides some illustrative (These example graphs are generated by the configuration model proposed in Newman graphs. (2003), and Appendix E contains more results of real-world networks.) Intuitively, graphs with higher network entropy contain a larger amount of information, and so are graphs with larger density, higher average degree, higher degree variance, or a smaller scale-free exponent. The connections between all five graph properties can also be theoretically justified and the motivations of choosing these properties can be found in Appendix A. The detailed empirical justification of these properties and the pre-training performance in included in Appendix E. Time-adaptive selection strategy. The proposed predictive uncertainty and the five graph properties together act as a powerful indicator of a graph’s goodness. Thus the selection of graph can be formulated as the following optimization problem:\n\nmaximize J (G) = (1 − γt) ˆφuncertain + γtMEAN( ˆφentropy, ˆφdensity, ˆφavg deg, ˆφdeg var, - ˆφα),\n\n(4)\n\nwhere the optimization variable is the graph G to be selected, ˆφuncertain, ˆφentropy, ˆφdensity, ˆφavg deg, ˆφdeg var and ˆφα are the z-score normalized value of graph-level predictive uncertainty, network entropy, density, average degree, degree variance and scale-free exponent of graph G respectively, [0, 1] is a parameter to trade off the predictive uncertainty and the graph properties, and t is the γt iteration counter. Note that the pre-training model learns nothing at the beginning, so we initialize γ0 = 0. The balance between the predictive uncertainty and the inherent graph properties ensures that the selected graph is a good supplement to the current pre-training model as well as an effective representative for the entire data distribution.\n\n∈\n\n5\n\n normalized value of structural propertiesnetwork entropy densitynegative scale-free exponentaverage degreedegree variancecommon structural properties(i.e., average degree, density, degree variance or negative scale-free exponent)less more InformativenessinformativenessUnder review as a conference paper at ICLR 2023\n\nWe shall also note that, at the beginning of the pre-training, the outputs of the model are not accurate enough to guide data selection, so the parameter γt should be set smaller so that the graph properties play a leading role. As the training phase proceeds, the graph selector gradually pays more attention to the feedback φuncertain from the model via a larger value of γt. Therefore, the parameter γt is called the time-adaptive parameter, and is set to be a random variable depending on time t. In this Beta(1, βt), where βt decreases over time (training work, we take γt from a Beta distribution γt iterations).\n\n∼\n\nFinally, after a graph is selected, we can further choose subgraph instances with high predictive uncertainty for training, rather than feed the model with random subgraph samples. Connections and differences with hard example mining. Hard example mining learns from the examples that contribute the most to model training, which has been widely applied in computer vision, natural language processing and recommender system Simo-Serra et al. (2014); Loshchilov & Hutter (2015); Shrivastava et al. (2016); Krishnan et al. (2020). Our usage of predictive uncertainty for choosing graphs is conceptually similar to hard example mining. However, existing approaches for hard sample mining can not be directly applied to our setting with the following two requirements. (1) The chosen instances should follow a joint distribution that reflects the topological structures of real-world graphs. This is met by our use of graph-level predictive uncertainty and graph properties. (2) The chosen set of graphs should include informative and sufficiently diverse instances. This goal is achieved by the data-active graph pre-training framework, which enables the interaction between the graph selector and pre-training model. Another line of works in active learning introduce the measure of uncertainty by querying the labels of samples that current model is least certain w.r.t classification prediction Cai et al. (2017); Zhang et al. (2021a); Yang et al. (2015); Zhu et al. (2008), which cannot be adapted in pre-training with unlabeled data.\n\n3.2 GRAPH PRE-TRAINING MODEL\n\nThe graph pre-training model takes the input graphs and samples one by one and enhances itself in a progressive manner. However, such a sequential training process does not guarantee the model to remember all the contributions of previous input data. As shown in the orange curve in Figure 4, the previously learnt graph exhibits a larger predictive uncertainty as the training phase proceeds. The empirical result indicates that the knowledge or information contained in previous input data will be forgotten or covered by newly incoming data. This phenomenon, called catastrophic forgetting Kirkpatrick et al. (2017), was first noticed in continual learning and is also identified here. Intuitively, when the training data is taken in a one-by-one manner, the learnt parameters will cater to the newly incoming data compared with the old, previous data points. One remedy for this issue is adding a proximal term to the objective. The additional proximal term (i.e., the regularization) guarantees the proximity between the new parameters and the model parameters learnt from previous graph. Therefore, the final loss function for our pre-training model in APT is\n\nL(θ) =\n\n(cid:88)\n\ni\n\nLi(θ) +\n\nk (cid:88)\n\n(cid:88)\n\nj=k−1\n\nm\n\nλj 2\n\nF (j)\n\nm ∥θm − θ(j)\n\nm ∥2,\n\n(5)\n\nL\n\nwhere i is given in Eq. (1), the summation in the first term is taken over the subgraph instances sampled from the new input graph, k is the number of previously learnt graphs, θ(j) is the model parameters learnt from the first j graphs, and λj’s are the trade-off parameters between the knowledge learnt form a nonfrom new data and that from previous data. Typically, the trade-off parameters decreasing sequence, i.e., λ1 λk. Inspired from the Elastic weight consolidation (EWC) algorithm Kirkpatrick et al. (2017), we take F (j) as the Fisher information matrix of θ(j), F (j) m is the diagonal element of F (j) and m labels each parameter. When F is set as an identity matrix, the second term degenerates to the L2 regularization (serves as one of our variants). Note that the proximal term in Eq. (5) is absent when the first input graph is introduced to the model.\n\nFigure 4: Predictive uncertainty of a learnt graph (“michigan”) versus training epoch.\n\n≤ · · · ≤\n\nλ2\n\nλj\n\n≤\n\n{\n\n}\n\n3.3 TRAINING AND FINE-TUNING Integrating the graph selector and the pre-training model forms the entire APT framework, and the overall algorithm is presented in Appendix B. After the training phase, the APT framework returns a\n\n6\n\n505560657075808590training epoch4.55.05.56.06.5predictive uncertaintywithout proximal termwith proximal term (L2)Under review as a conference paper at ICLR 2023\n\npre-trained GNN model, and then the pre-trained model can be applied to various downstream tasks from a wide spectrum of domains. In the so-called freezing mode, the pre-trained model outputted from APT is directly applied to downstream tasks, without any changes in parameters. Alternatively, the fine-tuning mode uses the pre-trained graph encoder as initialization, and offers the flexibility of training the graph encoder and the downstream classifier together in an end-to-end manner. 4 EXPERIMENTS In the experiments, we pre-train our model on the incoming data provided by the graph selector, and then evaluate the transferability of our pre-trained model on multiple unseen graphs from different domains in the node classification and graph classification task. Lastly, we include the applicable scope of our pre-trained model. Additional experiments can be found in Appendix G, including analysis on training time, sensitivity analysis of hyper-parameters, ablation study on various combinations of graph properties.\n\n4.1 EXPERIMENTAL SETUP Datasets. The datasets for pre-training and testing, and their detailed statistics are listed in Appendix C. The datasets for pre-training are collected from different domains, including social networks, citation networks, and movie collaboration networks. We then evaluate the pre-trained models on 13 real-world graphs, including large-scale datasets with millions of edges from Open Graph Benchmark Hu et al. (2020a). Some of them are from the similar domain as pre-training (like citation networks), while most of them are from a totally unseen domains (like web networks, transportation networks, protein networks and others).\n\nBaselines. We comprehensively evaluate our model against the following baselines for node classification and graph classification tasks, respectively. For node classification tasks, ProNE Zhang et al. (2019), DeepWalk Perozzi et al. (2014), struc2vec Ribeiro et al. (2017), DGI Velickovic et al. (2019), GAE Kipf et al. (2016), and GraphSAGE Hamilton et al. (2017) are used as baselines, and then the learned representations are fed into the logistic regression for node classification (as most of baselines did). As for graph classification tasks, we take graph2vec Narayanan et al. (2017), InfoGraph Sun et al. (2020), DGCNN Zhang et al. (2018) and GIN Xu et al. (2019) as baselines, and then feed the representations into SVM as the classifier (as most of baselines did). For both tasks, we also compare our model with (1) Random, where random vectors are generated as representations; (2) GraphCL You et al. (2020a), a GNN pre-training scheme based on contrastive learning with augmentations; (3) JOAO You et al. (2021), a GNN pre-training scheme that can automatically select data augmentations; (4) GCC Qiu et al. (2020), the state-of-the-art cross-domain graph pre-training model (the version of our model without data selection scheme, which trains on all pre-training data). GCC, GraphCL and JOAO are trained on the entire collected input data, and the suffix (rand, fine-tune) indicates that the model is trained from scratch. We also include 4 variants of our model: (1) APT-G, which removes the criteria of graph properties in the graph selector; (2) APT-P, which removes the criterion of predictive uncertainty in the graph selector; (3) APT-R, which removes the regularization w.r.t old knowledge in Eq. (5); (4) APT-L2, which degenerates the second term in Eq. (5) to the L2 regularization.\n\nExperimental settings. In the training phase, we iteratively select graphs for pre-training until the predictive uncertainty of any candidate graph is below 3.5. For each selected graph, we choose those samples with predictive uncertainty higher than 3. We include M = 500 query subgraph instances in a graph when measuring the predictive uncertainty of this graph. The time-adaptive parameter γt 0.995t. We set the trade-off parameter in Eq. (4) is drawn from γt λj = 10 for all j for APT-L2, and λj = 500 for APT. The total iteration number is 100. We adopt GCC as the backbone pre-training model with their default hyper-parameters. Note that we can also use other pre-training models like GraphCL as the backbone, but we do not report them due to the non-ideal performance of GraphCL. In the fine-tuning phase, we select logistic regression or SVM as the downstream classifier and adopt the same setting as GCC. See Appendix F for more details.\n\nBeta (1, βt), where βt = 3\n\n−\n\n∼\n\n4.2 EXPERIMENTAL RESULTS\n\nNode classification. Table 1 presents the micro F1 score of different methods over 10 unseen graphs from a wide spectrum of domain for node classification task. We can observe our model beats the graph pre-training competitor by an average of 9.94% and 17.83% under freezing and fine-tuning mode respectively. This suggests that instead of pre-training on all the collected graphs (like GCC), it is better to choose a part of graphs better suited for pre-training (like our model APT). Moreover, compared with the traditional models without pre-training, the performance gain of our\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Micro F1 scores of different models in the node classification task. The column “A.R.” reports the average rank of each model. Asterisk (∗) denotes the best result on each dataset, and bold numbers denote the best result among graph pre-training models in the freezing or fine-tuning setting. The notation “/” means out of memory or no convergence for more than three days.\n\nDataset\n\nMethod\n\nbrazil\n\ndd242\n\ndd68\n\ndd687\n\nwisconsin\n\ncornell\n\ncora\n\npubmed\n\nogbarxiv\n\nogbproteins A.R.\n\n/\n\n/\n\n9.36(3.63)\n\nRandom ProNE DeepWalk struc2vec DGI GAE GraphSAGE GraphCL (freeze) JOAO (freeze) GCC (freeze) APT-G (freeze) APT-P (freeze) APT-R (freeze) APT-L2 (freeze) APT (freeze) GraphCL (rand, fine-tune) 64.43(14.95) 15.04(0.85) 14.69(2.48) 10.99(0.58) 63.85(2.18) 44.21(10.58) 30.45(0.37) 40.73(0.66) JOAO (rand, fine-tune) 7.40(3.48) 45.38(13.30) 45.26(10.31) 29.93(2.84) 42.01(0.68) GCC (rand, fine-tune) GraphCL (fine-tune) JOAO (fine-tune) GCC (fine-tune) APT-G (fine-tune) APT-P (fine-tune) APT-R (fine-tune) APT-L2 (fine-tune) APT (fine-tune)\n\n5.98(2.70) 26.79(8.59) 39.77(7.26) 26.80(1.62) 38.85(0.76) 11.89(4.66) 52.69(5.94) 13.2 32.16(13.65) 6.71(2.44) 8.29(4.35) 3.88(1.66) 44.67(7.49) 47.32(12.14) 80.76(2.92)∗ 78.80(0.98)∗ 65.96(0.06)∗ 76.28(0.34)∗ 7.5 50.24(11.56) 10.04(2.56) 7.73(3.11) 6.17(2.18) 39.61(9.11) 47.67(8.30) 49.85(9.26) 44.99(10.89) 16.04 (2.98) 64.74(0.49) 8.7 43.16(16.78) 8.11(1.45) 6.72(3.04) 25.54(11.74) 13.71(2.66) 10.30(3.23) 7.98(2.74) 45.39(7.36) 38.39(9.18) 36.01(2.41) 44.45(0.78) 10.6 56.44(7.79) 14.35(0.44) 13.57(0.44) 11.04(1.93) 49.46(5.46) 49.85(9.26) 30.02(0.44) 42.39(0.84) 12.93(7.67) 55.98(0.33) 6.6 8.5 57.88(10.68) 14.09(1.52) 13.43(0.96) 10.25(2.63) 45.78(4.18) 49.26(5.24) 30.10(0.31) 40.14(0.68) 7.1 67.93(9.28) 14.33(0.37) 13.55(0.70) 10.39(0.78) 47.03(1.98) 49.20(1.46) 35.93(1.76) 39.94(0.02) 12.2 50.71(5.00) 9.53(2.50) 6.03(1.86) 38.85(10.80) 41.05(5.67) 16.95(2.39) 41.07(1.16) 71.22(7.21) 7.98(2.90) 12.36(2.59) 5.34(1.43) 42.69(8.15) 43.16(5.67) 18.13(2.82) 41.05(0.87) 10.9 67.47(4.09) 15.83(0.80) 11.95(1.13) 9.61(0.94) 52.57(1.69) 46.87(1.73) 35.47(0.51) 46.40(0.18) 14.56(7.60) 59.15(0.35) 7.0 68.69(3.42) 17.21(1.13) 11.98(0.75) 9.54(1.29) 54.45(1.90) 46.53(1.59) 34.89(0.25) 46.49(0.22) 12.32(7.71) 60.38(0.41) 6.3 66.55(2.35) 16.58(0.97) 12.48(0.85) 10.33(0.83) 51.90(1.64) 47.33(2.31) 35.63(0.56) 46.16(0.12) 12.86(7.54) 60.32(0.32) 6.2 68.12(3.07) 16.72(0.72) 12.42(1.24) 11.05(0.88) 54.48(1.77) 46.80(1.08) 34.93(0.36) 46.02(0.11) 18.79(5.87) 62.18(0.46) 5.0 69.82(2.32) 16.79(0.88) 12.68(0.81) 10.34(1.12) 55.11(1.74) 48.76(2.20) 34.27(0.43) 46.21(0.15) 19.64(6.46) 60.23(0.37) 4.4 73.39(2.55) 16.57(0.94) 12.08(0.89) 10.35(1.24) 53.38(1.19) 47.37(1.29) 36.69(0.49) 46.88(0.21) 22.04(0.29) 62.29(0.55) 3.8 8.3 72.14(6.74) 10.93(2.85) 8.08(2.15) 9.6 58.51(3.07) 15.98(1.05) 13.16(1.06) 9.74(0.95) 53.85(2.58) 50.95(2.26) 43.70(0.52) 49.72(0.17) 18.61(1.88) 59.12(0.35) 7.6 7.5 73.57(10.33) 15.35(0.99) 13.51(2.57) 10.66(1.04) 63.85(4.42) 51.05(2.41) 30.81(0.36) 42.91(0.91) 75.00(5.76) 10.54(3.07) 7.56(1.94) 9.5 8.77(2.39) 50.0(12.28) 42.11(10.26) 29.34(3.04) 42.21(0.88) 74.46(3.05) 19.32(0.80) 13.87(1.13) 10.37(1.06) 59.47(1.49) 48.32(2.42) 43.34(0.38) 50.87(0.19) 18.62(1.92) 60.08(0.56) 6.4 77.60(1.48) 25.45(0.60)∗ 17.78(1.14) 11.27(0.76) 66.09(2.28) 53.02(1.51) 45.63(0.66) 50.81(0.18) 27.33(4.80) 60.02(0.32) 3.8 78.99(2.44) 25.19(0.87) 16.40(1.22) 11.69(1.19) 64.24(1.90) 50.05(1.39) 45.53(0.30) 50.66(0.18) 27.20(4.80) 59.86(0.32) 4.8 79.14(1.97) 24.96(0.57) 17.43(1.05) 11.29(1.04) 66.28(1.94) 53.56(2.28)∗ 46.02(0.83) 51.00(0.21) 18.41(1.84) 60.10(0.38) 3.4 78.75(1.63) 24.62(0.90) 17.83(1.35)∗ 12.26(0.78)∗ 67.04(1.50)∗ 52.94(1.95) 47.48(0.46) 51.25(0.21) 27.40(4.97) 60.85(0.46) 2.6 79.67(2.30)∗ 28.62(0.55)∗ 20.30(1.13)∗ 12.80(1.54)∗ 67.08(1.75)∗ 52.15(2.25) 47.51(0.62) 51.30(0.16) 27.40(4.87) 61.64(0.35) 1.3\n\n/ /\n/ /\n\n/ /\n/ /\n\n/ /\n\n/ /\n\n/ /\n\n/ /\n\ndd\n\nDataset\n\nimdb-binary\n\nmsrc-21 A.R.\n\nTable 2: Micro F1 of different models in the graph classification.\n\nmodel is attributed to the transferable knowledge learned by pre-training strategies. We also find that some proximity-based models like ProNE enforce neighboring nodes share similar representations, thus they perform well on graphs with strong homophily rather than weak homophily. Graph classification. The micro F1 score on unseen test data in the graph classification task is summarized in Table 2. Especially, our model is 7.2% and 1.3% on average better than the graph pre-training backbone model under freezing and fine-tuning mode, respectively. Interestingly, we find that the variants of APT perform well under graph classification, indicating that we can apply a version with simpler architecture in practice yet achieve good results. Analysis of the selected graphs. The data sequentially selected via our graph selector are uillinois, soc-sign0811, msu, michigan, wiki-vote, soc-sign0902 and dblp. To further analyze why these graphs are their detailed strucchosen, we present tural properties in Table 4 in the Appendix C. We first observe that uillinois, michigan and msu have the largest value of MEAN( ˆφentropy, ˆφdensity, ˆφavg deg, ˆφdeg var, - ˆφα), while dblp has the smallest. This shows that both criteria, the graph properties and the predictive uncertainty, play an important role in data selection. Moreover, it is also interesting to see that wiki-vote is the smallest graph among all the pre-training graphs, but it still contributes to the performance. This observation again verifies the curse of big data phenomenon in graph pre-training.\n\nMethod 11 49.30(4.82) 52.72(4.34) 4.49(2.14) Random 7.7 56.20(5.33) 59.16(3.47) 8.22(3.67) graph2vec 7.7 66.58(0.63) 58.66(0.23) 6.01(0.59) InfoGraph 9.3 55.10(3.18) 57.82(4.71) 5.44(2.77) GraphCL (freeze) 63.90(3.48) 55.97(3.61) 5.09(2.65) 9.3 JOAO (freeze) 73.09(0.55) 75.16(0.53) 11.61(1.33) 5.3 GCC (freeze) 73.10(0.39) 75.24(0.42) 12.81(0.74) 4.3 APT-G (freeze) 72.83(0.81) 76.38(0.32) 13.30(0.57) 3.0 APT-P (freeze) 73.98(0.21) 75.32(0.34) 12.90(0.57) 3.0 APT-R (freeze) 73.54(0.40) 75.81(0.30) 13.16(0.77) 2.3 APT-L2 (freeze) 73.00(0.50) 75.83(0.31) 13.81(1.06) 3.0 APT (freeze) 71.00(4.69) 58.63(4.46) 6.01(0.59) 11.3 DGCNN 72.00(2.41) 77.61(1.47)∗ 10.54(5.08) 6.0 GIN GraphCL (rand, fine-tune) 63.60(3.61) 58.15(4.60) 8.25(2.94) 12.7 67.70(3.35) 62.10(4.31) 11.40(3.06) 10.0 JOAO (rand, fine-tune) 75.80(1.37) 74.26(0.59) 17.18(1.43) 7.3 GCC (rand, fine-tune) 66.90(4.39) 65.55(5.14) 8.77(2.60) 10.7 GraphCL (fine-tune) 68.50(3.61) 62.61(4.99) 10.18(1.72) 10.0 JOAO (fine-tune) 76.19(0.90) 75.32(1.77) 24.90(1.65) 4.7 GCC (fine-tune) 76.29(0.89) 75.46(0.77) 21.94(0.73) 4.7 APT-G (fine-tune) 76.70(1.01)∗ 75.34(0.88) 24.32(1.22) 3.7 APT-P (fine-tune) 76.60(1.02) 75.64(0.70) 24.09(2.12) 3.3 APT-R (fine-tune) 75.93(0.84) 75.58(1.06) 25.58(1.57)∗ 3.7 APT-L2 (fine-tune) 76.27(1.20) 75.69(1.42) 24.41(1.82) 3.0 APT (fine-tune)\n\n4.3 DISCUSSION: SCOPE OF APPLICATION\n\nThe transferability of the pre-trained model comes from the learnt representative structural patterns and the ability to distinguish these patterns (as discussed in §2). Therefore, our pre-training model is more suitable for the datasets where the target (e.g., labels) is correlated with subgraph patterns or structural properties (e.g., motifs, triangles, betweenness, stars). For example, for node classification on heterophilous graphs (e.g., winconsin, cornell), our model performs very well because in these graphs, nodes with the same label are not directly connected, but share similar structural properties and behavior (or role, position). On the contrary, graphs with strong homophily (like cora, pubmed, ogbarxiv and ogbproteins) may not benefit too much from our models. Similar observation can\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nalso be made on graph classification: our model could also benefit the graphs whose label has a strong relationship with their structure, like molecular, chemical, and protein networks (e.g., dd in our experiments) Vishwanathan et al. (2010); Gardiner et al. (2000).\n\n5 RELATED WORKS\n\nPre-training in CV and NLP. Initially, the CV community benefits from the models like Vision Transformers Liu et al. (2021), MLP-mixers Tolstikhin et al. (2021) and ResNets He et al. (2016), which are supervised pre-trained on large-scale image data. To take full advantage of massive unlabeled data, NLP community adapts self-supervised learning models like Transformer-based encoder Vaswani et al. (2017); Radford & Narasimhan (2018); Devlin et al. (2019) for language pretraining. When pre-training in CV and NLP, researchers find that scaling up the pre-training data size would results in a better or saturating performance in downstream Tan & Le (2019); Kaplan et al. (2020); El-Nouby et al. (2021); Abnar et al. (2022); Raffel et al. (2020). However, this is not true in graph pre-training. In this paper we argue that adding input graphs or pre-training samples does not necessarily improve, but sometimes even deteriorates the downstream performance.\n\nIn view of the above phenomenon in CV and NLP pre-training, data selection is not an active research direction. The only related research we notice focus on domain-specific pre-training models, which select pre-training data that is most similar to the downstream domain Cui et al. (2018); Beltagy et al. (2019); Dai et al. (2019; 2020); Yan et al. (2020); Lee et al. (2020); Chakraborty et al. (2022). The assumption on the knowledge of downstream domain is different from the acrossdomain graph pre-training in our paper, and thus data selection in CV/NLP pre-training is not that relevant to the current work. Graph pre-training. Taking inspiration from the pre-training in CV and NLP, recent efforts have shed the light on pre-training GNNs. Initially, some unsupervised graph representation learning can be used for graph pre-training Tang et al. (2015); He et al. (2016); Grover & Leskovec (2016); Narayanan et al. (2017); Ribeiro et al. (2017); Donnat et al. (2018); Zhang et al. (2019); Hamilton et al. (2017). They are designed based on neighborhood similarity assumption, thus cannot generalize to unseen nodes and graphs. Later, a line of graph self-supervised learning can be also treated as graph pre-training, which are categorized into two folds: graph generative models and contrastive models. Graph generative models capture the universal graph patterns by recovering certain parts of input graph (e.g., masked structure or attributes) Kipf et al. (2016); Wang et al. (2017); Hu et al. (2020c); Cui et al. (2020); Hou et al. (2022). These works rely on specific domain knowledge, for example the node/edge/attribute type should be the same, which makes them difficult to transfer across different types of graphs.ƒ On the other hand, graph contrastive models maximize the agreement between positive pairs and minimize that between negative pairs Velickovic et al. (2019); Hu et al. (2020b); You et al. (2020a); Zhu et al. (2020); Hassani & Khasahmadi (2020); Sun et al. (2020); Li et al. (2021); Lu et al. (2021); Sun et al. (2021); Zhu et al. (2021b); Xu et al. (2021); Zhu et al. (2021a); Lee et al. (2022); Zeng & Xie (2021); Zhang et al. (2021b); Han et al. (2022). One of the technical cores is to design appropriate data augmentation like attribute masking, edge perturbation, node dropping, diffusion, etc., which either performed on node attributes or the whole graph structure. So they only achieve transferability in graphs from similar (or the same) domains, or the downstream task is restricted to graph classification. With the purpose of learning transferable patterns across different domains, some works take subgraph sampling as augmentation, such that the transferable (sampled) subgraph patterns can be captured during pre-training Qiu et al. (2020); You et al. (2020a; 2021). However, these existing works only focus on how to design the pre-training model, rather than how to select data for pre-training. Our paper first points out the necessity of selecting data, and fills the gap of the data selection strategy in graph pre-training.\n\n6 CONCLUSION In this paper, we observe that big data is not a necessity for pre-training GNNs. This motivates us to wisely choose some suitable graphs and samples for pre-training rather than training on a massive amount of data. Without any knowledge of the downstream tasks, we propose a novel graph selector to provide the most instructive data for the model. The pre-training model is then encouraged to learn from the data in a progressive way and reinforce itself on newly selected data. We integrate the graph selector and the graph pre-training model in a unified framework, and form a data-active graph pre-training (APT) paradigm. The two components in APT are able to mutually boost the capability of each other. Extensive experimental results show that the proposed APT framework can enhance model capability with a fewer number of input data.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT\n\nWe provide an open-source implementation of our model APT at https://github.com/ anonymous-APT-ai/Anonymous-APT-code. Hyperparameters necessary for reproducing the experiments can be found in §4.1, Appendix D and Appendix F. Users can run APT on their own datasets.\n\n8 ETHICS STATEMENT\n\nThis paper empirically identifies the curse of big data phenomenon in graph pre-training, and develops a novel data-active graph pre-training framework. All the experiments are conducted on publicly available datasets for reproducibility purposes. Overall, this work inherits some of the risks of the existing works implementing these pre-existing datasets, and the risks are not amplified by the work. Therefore, this present paper likely does not introduce any new ethical or future social concerns.\n\nREFERENCES\n\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of\n\nlarge scale pre-training. In ICLR, 2022.\n\nIz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. In\n\nEMNLP-IJCNLP, pp. 3615–3620, 2019.\n\nYoshua Bengio, J ́erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\n\nICML, pp. 41–48, 2009.\n\nZdzislaw Burda, Jarek Duda, Jean-Marc Luck, and Bartek Waclaw. Localization of the maximal\n\nentropy random walk. Physical review letters, 102(16):160602, 2009.\n\nHongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. Active learning for graph embed-\n\nding. arXiv preprint arXiv:1705.05085, 2017.\n\nShuvam Chakraborty, Burak Uzkent, Kumar Ayush, Kumar Tanmay, Evan Sheehan, and Stefano Ermon. Efficient conditional pre-training for transfer learning. In CVPR, pp. 4241–4250, 2022.\n\nThomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 1999.\n\nGanqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. Adaptive graph encoder for attributed graph\n\nembedding. KDD, 2020.\n\nYin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge J Belongie. Large scale fine-grained\n\ncategorization and domain-specific transfer learning. In CVPR, pp. 4109–4118, 2018.\n\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile Paris. Using similarity measures to select\n\npretraining data for ner. In Proceedings of NAACL-HLT, pp. 1460–1470, 2019.\n\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile Paris. Cost-effective selection of pretraining data: A case study of pretraining bert on social media. In EMNLP (Findings), pp. 1675–1681, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pp. 4171–4186, 2019.\n\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, pp. 647–655, 2014.\n\nClaire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node embed-\n\ndings via diffusion wavelets. In SIGKDD, pp. 1320–1329, 2018.\n\nDavid Easley and Jon Kleinberg. Networks, crowds, and markets: Reasoning about a highly con-\n\nnected world. Cambridge university press, 2010.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv ́e Jegou, and Edouard arXiv preprint\n\nGrave. Are large-scale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021.\n\nEleanor J Gardiner, Peter Willett, and Peter J Artymiuk. Graph-theoretic techniques for macromolecular docking. Journal of Chemical Information and Computer Sciences, 40(2):273–279, 2000.\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-\n\nrate object detection and semantic segmentation. In CVPR, pp. 580–587, 2014.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In SIGKDD,\n\npp. 855–864, 2016.\n\nHakim Hafidi, Mounir Ghogho, Philippe Ciblat, and Ananthram Swami. Graphcl: Contrastive self-\n\nsupervised learning of graph representations. arXiv preprint arXiv:2007.08025, 2020.\n\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec.\n\nInductive representation learning on large\n\ngraphs. In NeurIPS, pp. 1025–1035, 2017.\n\nXiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for\n\ngraph classification. In ICML, pp. 8230–8248, 2022.\n\nKaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on\n\ngraphs. In ICML, pp. 4116–4126, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, pp. 9729–9738, 2020.\n\nZhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang.\n\nGraphmae: Self-supervised masked graph autoencoders. In SIGKDD, pp. 594–604, 2022.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020a.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure\n\nLeskovec. Strategies for pre-training graph neural networks. In ICLR, 2020b.\n\nZiniu Hu, Changjun Fan, Ting Chen, Kai-Wei Chang, and Yizhou Sun. Pre-training graph neural\n\nnetworks for generic structural feature extraction. arXiv preprint arXiv:1905.13728, 2019.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative\n\npre-training of graph neural networks. In SIGKDD, pp. 1857–1867, 2020c.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nThomas N Kipf et al. Variational graph auto-encoders. NIPS Workshop on Bayesian Deep Learning,\n\n2016.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n\nAdit Krishnan, Mahashweta Das, Mangesh Bendre, Hao Yang, and Hari Sundaram. Transfer learning via contextual invariants for one-to-many cross-domain recommendation. In SIGIR, pp. 1081– 1090, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240, 2020.\n\nNamkyeong Lee, Junseok Lee, and Chanyoung Park. Augmentation-free self-supervised learning\n\non graphs. In AAAI, pp. 7372–7380, 2022.\n\nPengyong Li, Jun Wang, Ziliang Li, Yixuan Qiao, Xianggen Liu, Fei Ma, Peng Gao, Seng Song, and Guotong Xie. Pairwise half-graph discrimination: A simple graph-level self-supervised strategy for pre-training graph neural networks. In IJCAI, pp. 2694–2700, 2021.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp. 10012– 10022, 2021.\n\nIlya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks.\n\narXiv preprint arXiv:1511.06343, 2015.\n\nYuanfu Lu, Xunqiang Jiang, Yuan Fang, and Chuan Shi. Learning to pre-train graph neural networks.\n\nIn AAAI, pp. 4276–4284, 2021.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-\n\ntions of words and phrases and their compositionality. In NeurIPS, pp. 3111–3119, 2013.\n\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005, 2017.\n\nMark E. J. Newman. The structure and function of complex networks. SIAM Rev., 45:167–256,\n\n2003.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-\n\ntions. In SIGKDD, pp. 701–710, 2014.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In SIGKDD, pp. 1150–1160, 2020.\n\nAlec Radford and Karthik Narasimhan.\n\nImproving language understanding by generative pre-\n\ntraining. 2018.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nLeonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node\n\nrepresentations from structural identity. In SIGKDD, pp. 385–394, 2017.\n\nAbhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors\n\nwith online hard example mining. In CVPR, pp. 761–769, 2016.\n\nEdgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, and Francesc Moreno-Noguer.\n\nFracking deep convolutional image descriptors. arXiv preprint arXiv:1412.6537, 2014.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In ICLR, 2020.\n\nMengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: Data-driven molecular In SIGKDD, pp.\n\nfingerprint via knowledge-aware contrastive learning from molecular graph. 3585–3594, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-\n\nworks. In ICML, pp. 6105–6114, 2019.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale\n\ninformation network embedding. In WWW, pp. 1067–1077, 2015.\n\nIlya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. NeurIPS, pp. 24261–24272, 2021.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. 2019.\n\nS Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph\n\nkernels. Journal of Machine Learning Research, 11:1201–1242, 2010.\n\nC. Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. Mgae: Marginalized graph\n\nautoencoder for graph clustering. CIKM, 2017.\n\nDongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-\n\naware graph contrastive learning. pp. 30414–30425, 2021.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks. In ICLR, 2019.\n\nXi Yan, David Acuna, and Sanja Fidler. Neural data server: A large-scale search engine for transfer\n\nlearning data. In CVPR, pp. 3893–3902, 2020.\n\nYi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander G Hauptmann. Multi-class active learning by uncertainty sampling with diversity maximization. IJCV, 113:113–127, 2015.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. In NeurIPS, pp. 5812–5823, 2020a.\n\nYuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help\n\ngraph convolutional networks. In ICML, pp. 10871–10880, 2020b.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. In ICML, pp. 12121–12132, 2021.\n\nJiaqi Zeng and Pengtao Xie. Contrastive self-supervised learning for graph classification. In AAAI,\n\npp. 10824–10832, 2021.\n\nJie Zhang, Yuxiao Dong, Yan Wang, Jie Tang, and Ming Ding. Prone: Fast and scalable network\n\nrepresentation learning. In IJCAI, pp. 4278–4284, 2019.\n\nMuhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning\n\narchitecture for graph classification. In AAAI, pp. 4438–4445, 2018.\n\nWentao Zhang, Yu Shen, Yang Li, Lei Chen, Zhi Yang, and Bin Cui. Alg: fast and accurate active\n\nlearning framework for graph convolutional networks. In SIGMOD, pp. 2366–2374, 2021a.\n\nZaixin Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph selfsupervised learning for molecular property prediction. In NeurIPS, pp. 15870–15882, 2021b.\n\nJingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K Tsou. Active learning with sampling by uncertainty and density for word sense disambiguation and text classification. In COLING, pp. 1137–1144, 2008.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nYanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning.\n\nIn NeurIPS D&B, 2021a.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning\n\nwith adaptive augmentation. In WWW, 2021b.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA THEORETICAL CONNECTION BETWEEN NETWORK ENTROPY AND\n\nTYPICAL GRAPH PROPERTIES\n\nMany interesting graph structural properties from basic graph theory give rise to a graph with high network entropy Lynn et al. (2020). We here theoretically show some connections between the proposed network entropy and typical structural properties.\n\nTo make theoretical analysis, we consider connected, unweighted and undirected graph, whose network entropy depends solely on its degree distribution (see Eq. (3)). Considering a random graph G with a fixed node set, we suppose that the degree of any node vi independently follows distribution p, which is a common setting in random graph theory G ́omez-Gardenes & Latora (2008). Then the expected network entropy of G is\n\n| where every di (and d) is an independent random variable follows the distribution p.\n\ni\n\n1\n\n(cid:88)\n\n(G) ⟩\n\n⟨H\n\n=\n\n2 E\n|\n\ndi log di\n\n= ⟨\n\n⟩\n\n⟨\n\nd log d ⟩\nd ⟩\n\n⟨\n\n.\n\n(6)\n\nNow we are ready to discuss the connection between network entropy graph properties (i.e., average degree\n\n(G) ⟩\n\n⟨H\n\nd ⟨\n\n, degree variance Var(d) and the scale-free exponent α). ⟩\n\nand some typical\n\nAverage degree. Given that the function x log x is convex in x, we have\n\nlog d\n⟨ It is clear that average degree is the lower bound of network entropy. Based on our discussion on §3.1, we conclude that when used for pre-training, an input graph with higher average degree would in general result in better performance of the pre-trained model.\n\nd ⟩\n⟨\n\n= log\n\nd ⟨\n\nd ⟨\n\n(G)\n\n⟩ ≥\n\n⟨H\n\n(7)\n\n⟩\n\n⟩\n\n⟩\n\n.\n\nDegree variance. The Taylor expansion of\n\nd log d ⟩\n⟨\n\nin Eq. (6) at (cid:18) 1 d\n⟩\n\n⟨\n\nVar(d) d\n2 ⟨\n\ngives\n\nd ⟨\n⟩ (cid:19)\n\n⟨H\n\n= log\n\n(G) ⟩\n\n2 + o ⟩\nwhere Var(d) is the variance of d. We find that log is exactly the zeroth-order term in the expansion. When average degree is fixed, the network entropy and the degree variance Var(d) are positively correlated. This in turn implies a positive correlation between degree variance and the test performance of the model.\n\nd ⟩\n\nd ⟨\n\n+\n\n⟨\n\n⟩\n\n2\n\n.\n\nScale-free exponent. Most real-world networks exhibit an interesting scale-free property (i.e., only a few nodes have high degrees), and thus the degree distribution often follows a power-law x−α, where α is called distribution. That is, we can just write the degree distribution as p(x) the scale-free exponent. For a real-world network, the scale-free exponent α is usually larger than 2 Clauset et al. (2009). Suppose the degrees of a random graph G with N nodes follows a powerlaw distribution p(x) = Cx−α where C is a normalization constant. When α > 2, we could approximately have G ́omez-Gardenes & Latora (2008)\n\n∼\n\nClearly, a smaller scale-free exponent α results in a higher network entropy.\n\n(G) ⟩\n\n⟨H\n\n=\n\nα\n\n,\n\n2\n\nif N\n\n. → ∞\n\n1\n\n−\n\nRemark 1 (Connection between network entropy and typical structural properties) A graph with high network entropy arises from graphs with typical structural characteristics like large average degree, large degree variance, and scale-free networks with low scale-free exponent.\n\nBesides the above theoretical analysis. The motivation of choosing density, average degree, degree variance and scale-free exponent is similar to that of network entropy. Intuitively, graphs with larger average degree and higher density have more interactions among the nodes, thus providing more topological information to graph pre-training. Also, the larger the diversity of node degrees, the more diverse the subgraph samples. The diversity of node degrees can be measured by degree variance and scale-free exponent. (A smaller scale-free exponent indicates the length of the tail of degree distribution is relatively longer, i.e., the degree distribution spreads out wider. )\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nB ALGORITHM\n\n}\n\nG1, . . . , GN\n\nThe overall algorithm for APT is given in Algorithm 1. Given a collection of graphs\n\n= from various domains, APT aims to pre-train a better generalist GNN (i.e., pre- {\ntraining model) on wisely chosen graphs and samples. Our APT pipeline involves the following three steps. (i) At the beginning, the graph selector chooses a graph for pre-training according to the graph properties (line 1). (ii) Given the chosen graph, the graph selector chooses the subgraph samples whose predictive uncertainty is higher than Ts in this graph (line 3). (iii) The selected samples are then fed into the model for pre-training until the predictive uncertainty of the chosen graph is below Tg or the number of training iterations on this chosen graph reaches F (line 4-5). (iv) The model’s feedback in turn helps select the most needed graph based on predictive uncertainty and graph properties until the predictive uncertainty of any candidate graph is low enough (line 67). The last three steps are repeated until the iteration number reaches a pre-set maximum value T (which can be considered as the total iteration number required to train on all selected graphs).\n\nG\n\nAlgorithm 1 Overall algorithm for APT.\n\n=\n\nInput: A collection of graphs trade-off parameter γt = 0, hyperparameter threshold Tg of moving to a new graph, the predictive uncertainty threshold Ts of choosing training samples, and the maximum iteration number T . Output: Model parameter θ of the pre-trained graph model.\n\n, maximal period F of training one graph, }\n, the learning rate μ, the predictive uncertainty\n\nG1, . . . , GN\n\nβt\n\nG\n\n{\n\n{\n\n}\n\nSample instances with predictive uncertainty higher than Ts from G∗ via the graph selector. Update model parameters θ if φuncertain(G∗) < Tg or the model has been trained on G∗ by F iterations then\n\n(θ).\n\n∇\n\nθ\n\nvia the graph selector, and\n\nG ← G\\{\n\nG∗\n\n.\n\n}\n\nG\n\n1: Choose a graph G∗ from 2: while The iteration number reaches T do 3: 4: 5: 6: 7: 8: 9: end while\n\nUpdate the trade-off parameter γt Choose a graph G∗ from\n\nend if\n\n, and\n\n←\n\n−\n\n∼\n\nμ\n\nG\n\nθ\n\nL Beta (1, βt). G∗\n\n. }\n\nG ← G\\{\n\n|\n\nV\n\nV |\n\nThe time complexity of our model mainly consists of five components: data augmentation, GNN encoder propagation, contrastive loss, sample selection and graph selection. Suppose the maximal , the batch size is B, and D is the representation number of nodes of subgraph instances is |\ndimension. (1) As for the data augmentation, the time complexity of random walk with restart is at 3) Xia et al. (2019). (2) The time complexity of GNN encoder propagation depends least O(B |\non the architectures of the backbone GNN. We denote it as X here. (3) The time complexity of the contrastive loss is O(B2D) Li et al. (2022). (4) Sample selection is conducted by choosing the samples with high contrastive loss (the loss is computed before), which costs O(B). (5) Graph M 2D) (where M the number of samples needed to compute the predictive selection costs O( uncertainty of a graph, and is the number of graphs that have not been selected). This step is executed only in a few epochs (around 6% in our current model), so we ignore its time overhead in 3 + X + graph selection. Therefore, the overall time complexity of APT in each batch is O(B |\nB2D + B).\n\n|G|\n\n|G|\n\nV\n\n|\n\nC DATASET DETAILS\n\nThe graph datasets for pre-training and testing in this paper are collected from a wide spectrum of domains (see Table 3 for an overview). The consideration of the graphs for pre-training and test is as follows. When selecting pre-training data, we hope that the graph size is at least hundreds of thousands to contain enough information for pre-training. When selecting test data, we hope that: (1) some test data is in the same domain as the pre-training data, and some is cross-domain, so as to comprehensively evaluate our model’s in and across-domain transferability. Accordingly, the in-domain test data is selected from the type of movie and citations, and the others test data are across-domain; (2) the size of test graphs can scale from hundreds to millions.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nRegarding the pre-training datasets, arxiv, dblp and patents-main are citation networks collected from Bonchi et al. (2012), Yang & Leskovec (2012) and Hall et al. (2001), respectively. Imdb is the collection of movie from Rossi & Ahmed (2015). As for the social networks, soc-sign0902 and soc-sign0811 are collected from Leskovec et al. (2009), wiki-vote is from Leskovec et al. (2010), academia is from Fire et al. (2011), and michigan, msu and uillions are from Traud et al. (2012). Regarding the test datasets, we collect the protein network dd and ogbproteins from Dobson & Doig (2003) and Hu et al. (2020a). The image network msrc-21 is from Neumann et al. (2016). The movie network imdb-binary is from Yanardag & Vishwanathan (2015). The citation networks, cora, pubmed and ogbarxiv, are from McCallum et al. (2000), Namata et al. (2012) and Hu et al. (2020a). The web networks cornell and wisconsin are collected from Pei et al. (2019). The transportation network brazil is form Ribeiro et al. (2017), and dd242, dd68 and dd687 are from Rossi & Ahmed (2015).\n\nThe detailed graph properties of the pre-training data and test data are presented in Table 4 and Table 5, respectively.\n\nTable 3: Datasets for pre-training and testing, where ∗ denotes the average statistic of multiple graphs under graph classification setting. denote the number of nodes and the number of edges in a graph, respectively.\n\nand\n\nE\n\nV\n\n|\n\n|\n\n|\n\n|\n\nType\n\ncitations\n\nsocial\n\nmovie\n\nprotein\n\nimage movie citations\n\nweb\n\na t\na d\n\ng n\n\ni\n\nn\n\ni\n\na r\nt -\ne r\np\n\na t\na d\n\nt s\ne t\n\nName\n\n|V |\n\n|E|\n\nDescription\n\narxiv dblp\n\n517,563 178,145 560,943 497,672 468,554 100,762 369,692\n\n86,376 93,156 patents-main 240,547 soc-sign0902 81,867 soc-sign0811 77,350 7,115 137,969 30,147 1,176,516 friendships between Facebook users in University of Michigan 32,375 1,118,774 same as above (Michigan State University) 30,809 1,264,428 same as above (University of Uillinois) 896,305 3,782,447 relationships between actors and movies\n\ncitations between papers on the arxiv same as above (dblp) citations between US patents friend/foe links between the users of Slashdot in Feb. 2009 same as above (Nov. 2008) voting relationships between wikipedia users friendships between academics on Academia.edu\n\nwiki-vote academia michigan msu uillinois imdb\n\ndd\n\n284.32∗\n\n715.66∗ molecular interactions between amino acids\n\nogbproteins 132,534 39,561,252 biologically associations between proteins\n\nmsrc-21\n\n77.52∗ adjacency between superpixels of the image segmentations imdb-binary 19.77∗ collaboration relationships between actors/actresses citations between Machine Learning papers 2,708 19,717 citations between scientific papers 169,343 1,166,243 citation network between computer science arXiv papers\n\n198.32∗ 96.53∗ 5,278 88,648\n\ncora pubmed ogbarxiv cornell\n\n183\n\n280\n\ntransportation others\n\nwisconsin brazil dd242 dd68 dd687\n\n251 131 1,284 775 725\n\n466 2,077 3,303 2,093 2,600\n\nhyperlinks between webpages collected from Cornell University same as above (Wisconsin University) commercial flights between airports in Brazil this network dataset is in the category of labeled networks same as above same as above\n\nTable 4: Detailed structural properties of pre-training datasets, where avg properties equals to MEAN( ˆφentropy, ˆφdensity, ˆφavg deg, ˆφdeg var, - ˆφα) in Eq. (4), and nei2 denotes the average number and standard deviation of 2 denote the number of nodes and the number of E\n| edges in a graph, respectively.\n\nhop neighbors,\n\nV |\n\nand\n\n−\n\n|\n\n|\n\nProperties\n\nDataset soc-sign0902 soc-sign0811 imdb patent academia wiki-Vote dblp arxiv michigan msu uillinois\n\nV |\n\n|\n\nE |\n\n|\n\n81867 497672 77350 468554 896305 3782447 240547 560943 137969 369692 7115 103689 93156 178145 86376 517563 30147 1176516 32375 1118774 30809 1264428\n\navg properties avg degree degree var density entropy α\n\nnei2 (avg, std)\n\navg clustering coef\n\n-0.32 -0.32 -0.66 -0.96 -0.89 0.74 -1.12 -0.43 1.40 1.13 1.44\n\n13.16 13.12 9.44 5.66 6.36 29.32 4.82 12.98 79.05 70.11 83.08\n\n1643.20 1.49e-04 1631.77 1.57e-04 9.42e-06 298.27 1.94e-05 34.95 102.14 3.88e-05 3314.79 4.10e-03 4.11e-05 58.05 382.12 1.39e-04 6369.17 2.59e-03 5087.53 2.13e-03 6306.02 2.66e-03\n\n3.91 3.93 3.07 2.04 2.38 4.46 2.16 3.22 4.78 4.62 4.78\n\n1.51 1192.33, 2305.99 1.52 1226.93, 2312.03 1.53 316.16, 614.03 1.57 117.23, 172.40 1.57 101.40, 225.96 1.40 972.03, 1045.13 1.72 1.41 145.21, 309.12 1.23 4683.90, 3655.25 1.23 4567.63, 3465.16 1.22 5267.10, 3831.37\n\n58.85, 90.46\n\n0.06 0.05 5e-05 0.08 0.14 0.14 0.27 0.68 0.21 0.21 0.21\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Detailed structural properties of test datasets, where nei2 denotes the average number and denote the average statistics of standard deviation of 2 −\nmultiple graphs under graph classification setting. denote he number of nodes in a graph, the number of edges in a graph and the number of graphs in graph classification datasets, respectively.\n\nhop neighbors, and the numbers with and\n\n∗ G\n| |\n\n, |\n\nE\n\nV\n\n|\n\n|\n\n|\n\nProperties\n\nDataset imdb-binary msrc-21 dd cora pubmed brazil dd242 dd68 dd687 wiscosin cornell ogbarxiv ogbproteins\n\nV |\n\n|\n\nE |\n\n|\n\n193.06∗ 19.77∗ 198.32∗ 77.52∗ 715.66∗ 284.32∗ 5278 2708 44327 19717 1074 131 3303 1284 2093 775 2600 725 466 251 280 183 16343 1157799 132534 39561252\n\nG |\n| 1000 563 1178 /\n/ /\n/ /\n/ /\n/ /\n/\n\navg degree degree var\n\ndensity\n\nentropy\n\n9.89∗ 6.10∗ 6.00∗ 4.90 5.50 16.85 6.14 6.40 8.17 4.65 4.04 14.67 598.00\n\n116.01∗ 30.26∗ 27.60∗ 42.53 75.44 539.18 28.80 33.42 55.78 76.26 58.48 4898.17\n\n1.04∗ 6.81e-02∗ 2.78e-02∗ 1.44e-03 2.28e-04 1.26e-01 4.01e-03 6.98e-03 9.91e-03 1.49e-02 1.68e-02 8.07e-05 742637.58 4.50e-03\n\n1.07∗ 1.71∗ 1.65∗ 1.71 2.23 3.14 1.68 1.76 2.01 1.84 1.74 3.63 6.84\n\nnei2 (avg, std) 24.89∗,15.91∗ 17.00∗, 5.81∗ 14.30∗,5.68∗ 34.98,47.70 57.10,82.72 92.27,28.50 14.57,4.30 17.40,8.93 25.45,9.96 68.04,58.22 54.09,44.30 3483.08,6711.40 32265.17,19401.46\n\navg clustering coef # of classes\n\n0.95∗ 0.51∗ 0.48∗ 0.24 0.06 0.66 0.47 0.44 0.48 0.23 0.18 0.23 0.28\n\n2 20 2\n7 3\n4 20 20 20 5\n5 40 2\n\nD ADDITIONAL OBSERVATIONS OF Curse of Big Data PHENOMENON\n\nThis section provides more comprehensive observations to support the curse of big data phenomenon in cross-domain graph pre-training, i.e., more training samples and graph datasets do not necessarily lead to better downstream performance.\n\nWe investigate 3630 experiments with GCC Qiu et al. (2020) and GraphCL Hafidi et al. (2020) model with different model configurations (i.e., the number of GNN layers is set to be 3, 4 and 5 respectively), when pre-trained on all training graphs listed in Table 3 and evaluated on different test graphs (annotated in the upper left corner of each figure) under freezing setting. Note that GCC and GraphCL are the only two pre-training models that can be adopted for the cross-domain setting. For each experiment, we calculate the mean and standard deviation over 10 evaluation results of the downstream task with random training/testing splits.\n\nThe observations of GCC and GraphCL model can be found in Figure 5 and Figure 6 respectively. The downstream results of different test data are presented in separate rows. The figures in left three columns present the effect of scaling up the number of graphs on the downstream performance under different model configurations (i.e., the number of GNN layers) respectively. We first pre-train the model with only two input graphs, and the result is plotted in a dotted line. The largest standard deviation among the results w.r.t different graph last is also marked by the blue arrow. The figures in the last column illustrate the effect of scaling up sample size (log scale) on the performance.\n\nTable 6: The value of parameters for fitting the curve according to the function f (x) = a1 ln x/xa2 + a3 (a1, a2, a3 > 0), based on the points in the last column in Figure 5 and Figure 6.\n\nParameter\n\nDataset\n\na1\n\na2\n\na3\n\na1\n\na2\n\na3\n\nGCC\n\nGraphCL\n\ncora pubmed brazil dd242 dd68 dd687 wisconsin cornell imdb-binary dd msrc\n\n33.43 39.63 0\n3.82 3.83 10.31 16.59 45.45 64.97 75.53 4.85\n\n1038.19 3.50 29.59 7.84 2.08e+17 5.21 11.42 7.84 1.06 13.76 4.71\n\n1.24 0.12 0.19 0.12 6.55 0.12 0.21 4.83 0\n0.10 0.13\n\n17.15 36.83 41.83 0\n10.37 1.35 38.24 51.79 51.69 36.33 0\n\n0.45 4.74 39.10 6.60 6.44 968.01 26.78 15.46 7.43 0.81 5.56\n\n0 0.11 0.09 0.08 0.11 1.37 0.11 0.41 0.13 0.31 0.11\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nThe explanation of convex hull fit. In order to better show the changing trend, the blue curve in the last column in Figure 5 and Figure 6 is fitted to the convex hull of the points. The convex hull is proposed to capture the performance of a randomized classifier made by choosing pre-training models with different probabilities Abnar et al. (2022).\n\n1 , cds\n\n1 ) and c2 = (csz\n\nWe first introduce the concept of randomized classifier. Given two classifiers with training sample size and downstream performance c1 = (csz 2 ), a randomized classifier can be made to choose the first classifier with probability p and the second classifier with probability 1 p. Then the output of the randomized classifier is pc1 + (1 p)c2, which is the convex combination of c1 and c2. All the points on this convex combination can be obtained by choosing different p. Extend the notion to the case of multiple classifiers, we can consider the output of such a randomized classifier to be a convex combination of the outputs of its endpoints Abnar et al. (2022). All the points on the convex hull are achievable. Therefore, the output of the randomized classifier is equivalent to the convex hull of our trained classifiers’ performance.\n\n2 , cds\n\n−\n\n−\n\nIn our experiments, we include the upper hull of the convex hull of the model performances, i.e., the highest downstream performance for every given sample size. Such convex hull fit is proved to be robust to the density of the points in each figure Abnar et al. (2022).\n\nA final remark is that our observations on different downstream datasets do not result in a onemodel-fits-all trend. So we propose to fit a complicated curve whose function has form f (x) = a1 ln x/xa2 +a3 (a1, a2, a3 > 0) to the best performing models (i.e., the convex hull fit as discussed above). The fitted parameters a1, a2 and a3 in this function of each curve are given in Table 6.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The additional observations of curse of big data phenomenon, performed on different GCC pre-training models.\n\n20\n\ndd687dd687dd687micro F1micro F1micro F1# of samplesmicro F1dd687# of graphs# of graphs# of graphswisconsinwisconsinbrazilbrazil# of graphs# of samples# of samplesmicro F1micro F1micro F1micro F1std : 3.1std :1.5micro F1# of graphs# of graphs# of graphs# of graphs# of graphswisconsinwisconsinmicro F1micr oF1micro F1brazilbrazilmodel config.5 layers4 layers3 layersconvex hull fitstd : 1.1std :2.1std : 0.8std :5.0std : 4.1std : 1.1std : 1.3# of graphs# of samples# of samplesmicro F1micro F1micro F1dd242dd68dd242dd68dd242# of graphs# of graphs# of graphsdd242micro F1micro F1# of graphsmicro F1micro F1micro F1dd68dd68# of graphsmicro F1micro F1micro F1micro F1# of graphs# of graphs# of graphs# of samplespubmedpubmedpubmedpubmed# of samples# of graphs# of graphs# of graphsmicro F1micro F1micro F1micro F1cornellcornellcornellcornellcoracoramicro F1micro F1micro F1micro F1# of graphs# of graphs# of graphs# of samplescoracorastd : 1.8std : 1.0std : 0.6std : 0.3std : 0.7std : 2.0std : 0.8std : 0.5std : 0.5std : 0.6std : 0.8std : 0.3std : 0.5std : 0.5std : 1.3# of graphs# of samplesmicro F1micro F1dddd# of graphsmicro F1micro F1# of graphsddddmsrc-21imdb-binary# of samples# of samplesmicro F1micro F1msrc-21std : 1.4# of graphs# of graphs# of graphs# of graphs# of graphs# of graphsmicro F1micro F1micro F1micro F1micro F1micro F1msrc-21msrc-21imdb-binarystd :1.4imdb-binaryimdb-binarystd : 1.4std : 0.6std :1.4std : 1.2std : 0.7std : 1.3std : 0.5GCC (5 layers)GCC (5 layers)GCC (4 layers)GCC (3 layers)GCC (5 layers)GCC (4 layers)GCC (3 layers)GCC (4 layers)GCC (3 layers)GCC (5 layers)GCC (5 layers)GCC (5 layers)GCC (5 layers)GCC (5 layers)GCC (5 layers)GCC (5 layers)GCC (5 layers)GCC (4 layers)GCC (4 layers)GCC (4 layers)GCC (4 layers)GCC (4 layers)GCC (4 layers)GCC (4 layers)GCC (4 layers)GCC (3 layers)GCC (3 layers)GCC (3 layers)GCC (3 layers)GCC (3 layers)GCC (3 layers)GCC (3 layers)GCC (3 layers)(b) Graph classification(a) Node classificationUnder review as a conference paper at ICLR 2023\n\nFigure 6: The additional observations of curse of big data phenomenon, performed on different GraphCL pre-training models.\n\n21\n\nmicro F1micro F1# of samplesmicro F1# of graphs# of graphs# of graphs# of graphs# of samples# of samplesmicro F1micro F1micro F1micro F1micro F1# of graphs# of graphs# of graphs# of graphs# of graphswisconsinmodel config.5 layers4 layers3 layersconvex hull fitstd :3.0# of graphs# of samples# of samplesmicro F1micro F1micro F1dd242# of graphs# of graphs# of graphs# of graphsmicro F1micro F1dd68# of graphsmicro F1micro F1# of graphs# of graphs# of graphs# of samplespubmed# of graphs# of graphs# of graphsmicro F1micro F1cornellmicro F1micro F1micro F1# of graphs# of graphs# of graphs# of samplesstd : 2.5std : 1.0std : 1.0std : 0.3# of graphs# of samplesmicro F1# of graphsmicro F1micro F1# of graphsimdb-binary# of samples# of samplesmicro F1micro F1# of graphs# of graphs# of graphs# of graphs# of graphs# of graphsmicro F1micro F1micro F1micro F1std : 1.4std : 1.4msrc-21dd# of samplescornellpubmedcoradd687dd242dd68dd68wisconsinbrazilcorastd : 0.8GraphCL (5 layers)brazilstd :3.3dd687std : 0.8GraphCL (5 layers)GraphCL (5 layers)GraphCL (5 layers)GraphCL (5 layers)GraphCL (5 layers)GraphCL (5 layers)GraphCL (5 layers)imdb-binaryGraphCL (5 layers)GraphCL (5 layers)msrc-21std : 1.4ddGraphCL (5 layers)micro F1micro F1micro F1micro F1micro F1micro F1micro F1micro F1micro F1micro F1micro F1(b) Graph classification(a) Node classificationimdb-binaryGraphC (4 layers)Lmsrc-21GraphC (4 layers)Lmicro F1pubmed GraphCL (4 layers)wisconsindd242 GraphCL (4 layers)dd687ddbrazilcoracornelldd68 GraphCL (4 layers) GraphCL (4 layers) GraphCL (4 layers) GraphCL (4 layers) GraphCL (4 layers) GraphCL (4 layers) GraphCL (4 layers)std : 2.0std : 5.7std : 0.7 std : 1.1std : 0.6std : 0.7std : 4.7std : 1.4std : 1.7std : 1.0std : 1.0 micro F1micro F1micro F1pubmedmsrc-21imdb-binaryddcornellcoradd242dd687dd68wisconsin GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers) GraphCL (3 layers)brazil GraphCL (3 layers)std : 3.2std : 3.1std : 0.8std : 1.2std : 1.3std : 1.0std : 1.7std : 5.1std : 6.1std : 3.4std : 0.7Under review as a conference paper at ICLR 2023\n\nE EMPIRICAL STUDY OF GRAPH PROPERTIES\n\nAdditional properties for part (b) in Figure 2. In Figure 7, we plot the Pearson correlation between the graph properties of the graph used in pre-training (shown in the y-axis) and the performance of the pre-trained model using this graph on different unseen test datasets (shown in the x-axis). Note that the pre-training is performed on each of the input training graphs (in Table 3) via GCC. The results indicate that network entropy, density, average degree and degree variance exhibit a clear positive correlation with the performance, while the scale-free exponent presents an obviously negative relation with the performance. On the contrary, some other properties of graphs, including clique number, transitivity, degree assortativity and average clustering coefficient, do not seem to have connections with downstream performance, and also exhibit little or no correlation with the performance. Therefore, the favorable properties of network entropy, density, average degree, degree variance and the scale-free exponent of a real graph are able to characterize the contribution of a graph to pre-training.\n\nFigure 7: Pearson correlation between the structural features of the graph used in pre-training and the performance of the pre-trained model (using this graph) on different unseen test datasets.\n\nDetailed illustrations of Figure 3. In Figure 3, the illustrative graphs are generated by the configuration model with 15-18 nodes. The shaded area groups the illustrative graphs whose network entropy and graph properties are similar. Each four points on the same horizontal coordinate represent four graph properties of an illustrating graph. Each curve is fitted by least squares and represents the relation between entropy and other graph properties.\n\nAdditional real-world example for Figure 3. In Figure 8, we provide a real-world example of how network entropy correlates with four typical structural properties (in red), as well as the performance of the pre-trained model on test graphs (in blue). Numerical experiments again support our explanation (or intuition) of their strong correlation.\n\n(a) Density\n\n(b) Average degree\n\n(c) Degree variance\n\n(d) Scale-free exponent\n\nFigure 8: The red plot shows the network entropy (left y-axis) versus typical structural properties in a graph (i.e., density, average degree, degree variance, and the parameter α in a scale-free network), and the blue one shows the pre-training performances on wisconsin dataset (right y-axis) versus structural features.\n\n22\n\n−0.6−0.4−0.20.00.20.40.60.8 no correlationpositive correlation negative correlationcorapubmedbrazildd242dd68wisconsincornellDensityAverage degreeDegree varianceClique numberTransivityDegree assortativityClustering coefficientScale-free exponentEntropy1e-32e-33e-312345464850525456Network entropyPerformance (%)20406012345464850525456Network entropyPerformance (%)20406012345464850525456Network entropyPerformance (%)1.351.501.6512345464850525456Network entropyPerformance (%)Under review as a conference paper at ICLR 2023\n\nF IMPLEMENTATION DETAILS\n\nThe number reported in all the experiments are the mean and standard deviation over 10 evaluation results of the downstream task with random training/testing splits. When conducting the downstream task, For each dataset, we consistently use 90% of the data as the training set, and 10% as the testing set. We conduct all experiments on a single machine of Linux system with an Intel Xeon Gold 5118 (128G memory) and a GeForce GTX Tesla P4 (8GB memory). Our codes are available at https://github.com/anonymous-APT-ai/Anonymous-APT-code.\n\nImplementations of our model. The regularization for weights of the model in Eq. (5) is applied to first 2 layers of GIN. The maximal period of training one graph F is 6, the maximum iteration number T is 100, and the predictive uncertainty thresholds Ts and Tg are set to be 3 and 2 respectively. The selected instances are sampled from 20,000 instances each epoch. Since the pre-training model is unable to provide precise predictive uncertainty in the initial training stage, the model is warmed up over the first 20 iterations. Since we adopt GCC as the backbone pre-training model, the other settings are the same as GCC.\n\nOur model is implemented under the following software settings: Pytorch version 1.4.0+cu100, CUDA version 10.0, networkx version 2.3, DGL version 0.4.3post2, sklearn version 0.20.3, numpy version 1.19.4, Python version 3.7.1.\n\nImplementations of baselines. We compare against several graph representation learning methods. For implementation, we directly adopt their public source codes and most of their default hyperparameters. The key parameter settings and code links can be found in Table 7.\n\nTable 7: The source code and major hyper-parameters used in the baselines.\n\nMethod\n\nHyper-parameter\n\nCode\n\nDGI\n\nGAE\n\nstruc2vec\n\ngraph2vec\n\nDeepWalk The dimension of output representations is 64, walk length = 10, number of walks = 80 The dimension of output representations is 32, walk length = 80, number of walks = 10, window size = 5 512 hidden units per GNN layer, learning rate = 0.001 32 hidden units per GNN layer, learning rate = 0.01 The dimension of output representations is 128 32 hidden units per GNN layer, 5 layers 32 hidden units per GNN layer, learning rate = 0.001, batch size = 50 64 hidden units per GNN layer, 5 layers, learning rate = 0.01, sum pooling 300 hidden units per GNN layer, 5 layers, learning rate = 0.001 64 hidden units per GNN layer, 5 layers, learning rate = 0.005, number of samples per epoch = 20000\n\nInfoGraph DGCNN\n\nGraphCL\n\nGCC\n\nGIN\n\nhttps://github.com/shenweichen/GraphEmbedding\n\nhttps://github.com/leoribeiro/struc2vec\n\nhttps://github.com/PetarV-/DGI\n\nhttps://github.com/zfjsail/gae-pytorch\n\nhttps://github.com/benedekrozemberczki/graph2vec\n\nhttps://github.com/fanyun-sun/InfoGraph https://github.com/leftthomas/DGCNN\n\nhttps://github.com/weihua916/powerful-gnns\n\nhttps://github.com/Shen-Lab/GraphCL\n\nhttps://github.com/THUDM/GCC\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nG ADDITIONAL EXPERIMENTAL RESULTS\n\n{\n\n}\n\nλj\n\n. The hyper-parameter\n\nEffects of hyper-parameter is the trade-off parameters between the knowledge learnt from new data and that from previous data in Eq. (5). We simply set λ1 = λ2 = = λk. We use the dataset dd242 as an example to find the suitable values of the hyperparameter under the L2 and EWC regularization setting respectively, and present here for reference (see Figure 9). Clearly, a too small or too large λ would deteriorate the performance. Thus, an appropriate value of λ is preferred to ensure that the graph pre-training model can learn from new data as well as remember previous knowledge. We leave changing\n\nas the future work.\n\n· · ·\n\nλj\n\n{\n\n}\n\nλj\n\n{\n\n}\n\n(a) Ablation study on APT-L2 (freeze)\n\n(b) Ablation study on APT (freeze)\n\nFigure 9: Performance of our model on dd242 w.r.t varying\n\nλj\n\n{\n\n. }\n\n}\n\n∈ {\n\n1, 2, 3\n\nEffects of hyper-parameter F, Tg, Ts. Our model training involves three hyper-parameter F, Tg, Ts, where F controls the largest number of epochs training on each graph, Tg is the predictive uncertainty threshold of moving to a new graph, Ts is the predictive uncertainty threshold of choosing training samples. We use grid search to show F ’s and ’s role in the pre-training. F remains at 5 while studying Tg and Ts, Tg remains at Ts 3.5 while studying F and Ts, and Ts remains at 2 while studying F and Tg. Figure 10 presents the effect of these parameters, We find that if the value of F is set too small or that of Tg is too large, the model cannot learn sufficient knowledge from each graph, leading to suboptimal results. Too large F or small Tg also lead to poor performance. This indicates that instead of training on a graph for a large period, it would be better to switch to training on various graphs in different domains to gain diverse and comprehensive knowledge. Regarding the hyper-parameter Ts, we observe that large Ts would make the model having too few training samples to learn knowledge, and small Ts could not select the most uncertain and representative samples, thus both cases achieve suboptimal performance.\n\n3, 3.5, 4\n\n4, 5, 6\n\n’s, Tg\n\n∈ {\n\n∈ {\n\n}\n\n}\n\n(a) Ablation study on F\n\n(b) Ablation study on Tg\n\n(c) Ablation study on Ts\n\nFigure 10: Performance of our model on dd242 w.r.t varying F, Tg, Ts.\n\nThe choice of βt, its alternatives, and ablation study. At the beginning of the pre-training, the model is less accurate and needs more guidance from graph properties. We therefore set γt as larger at the beginning and gradually decrease it. To simplify this process, we follow Cai et al. (2017) to ct use the exponential formula of βt = c1 2 to set the expectation of γt to be strictly decreasing Beta(1, βt)). (where γt\n\n−\n\nThe parameters c1 and c2 in the exponential formula of βt = c1 0.995 in Cai et al. (2017). We simply perform grid search on c1 in in the Figure 11.\n\n− {\n\nct 2 are suggested as 1.005 and ; see the effect of c1\n\n1.005, 3, 5\n\n}\n\n∼\n\n24\n\n1105010016.516.616.716.8micro F110030050070016.316.416.5micro F156715.016.017.0micro F112315.016.017.0micro F112315.016.017.0micro F1Under review as a conference paper at ICLR 2023\n\nTable 8: Micro F1 of APT-L2 (freeze) with the different decay functions in the node classification task.\n\nDataset\n\nMethod\n\nlinear step exponential\n\nbrazil\n\ndd242\n\ndd68\n\ndd687\n\nwisconsin\n\ncornell\n\ncora\n\npubmed\n\n72.30(1.37) 16.28(0.57) 12.44(0.72) 10.29(0.87) 54.20(1.50) 47.66(1.53) 35.74(0.52) 46.49(0.19) 68.70(3.95) 16.74(0.45) 12.86(1.07) 10.09(0.76) 52.55(2.39) 48.08(1.28) 35.50(0.46) 46.58(0.21) 69.82(2.32) 16.79(0.88) 12.68(0.81) 10.34(1.12) 55.11(1.74) 48.76(2.20) 34.27(0.43) 46.21(0.15)\n\nTable 9: Micro F1 of APT-L2 (freeze) with the different decay functions in the graph classification task.\n\nDataset\n\nMethod linear step exponential\n\nimdb-binary\n\ndd\n\nmsrc-21\n\n73.66(0.34) 75.47(0.26) 13.01(0.78) 72.99(0.40) 75.41(0.41) 14.13(0.56) 73.54(0.40) 75.81(0.30) 13.16(0.77)\n\nWe then illustrate that the choice of the decay function of βt is robust. Table 8 and Table 9 below show the effect of linear decay, step decay and exponential decay on βt. (The function for linear decay and step decay are designed as βt = 2.001 + 0.004t, βt = 2.005 + floor(t/20), respectively. The initial value β1 is set the same as ours.) While there is no universally better decay function, the performance of our method is not significantly impacted by the choice of different decay functions, and our performance is better than the baselines in most cases regardless of the choices of specific decay functions.\n\n(a) Wisconsin on node classification\n\n(b) Imdb-binary on graph classification\n\nFigure 11: Performance of APT-L2 (freeze) w.r.t varying c1.\n\nImpact of five graph properties combination. As a further experimental analysis, we study the effect of the strategy of utilizing only one graph property in Table 10 and Table 11. We find that the five properties used in our model are all indispensable, and the most important one probably varies for different tasks and datasets. That’s why we choose to combine all graph properties.\n\nMoreover, these case studies may provide some clues of how to select pre-training graphs when some knowledge of the downstream tasks is known. For example, if the downstream dataset is extremely dense (like imdb-binary), the density property dominates among the selection criteria (such that the probability of encountering very dense out-of-distribution samples during testing can be reduced). If the entropy of downstream dataset is very high (like brazil), it is perhaps better to choose graphs with high entropy for pre-training. But still, when the downstream task is unknown, using the combination of five metrics often leads to the most satisfactory and robust results.\n\nThe justification of input graphs’ learning order. Table 12 reveals the downstream performance can be affected by the learning order of input training graphs. With the guidance of graph selector, the pre-training model is encouraged to first learn the graphs and samples with higher predictive\n\n25\n\n1.00535535455micro F11.00535727374micro F1Under review as a conference paper at ICLR 2023\n\nuncertainty and graph properties. Such learning order accomplishes better downstream performance compared to the reverse or random one.\n\nThe choice of the “difficult” data. Among all the data, “difficult” samples contribute the most to the loss function, and thus they can yield gradients with large magnitude. Comparatively, training with easy samples may suffer from inefficiency and poor performance as these data points produce gradients with magnitudes close to zero Huang et al. (2016); Sohn (2016). In addition, learning from difficult samples has proven to be able to accelerate convergence and enhance the expressive power of the learnt representations Suh et al. (2019); Schroff et al. (2015). For our model, the importance of learning from difficult samples is also justified empirically, as shown in Table 13.\n\nTraining time. As empirically noted in Table 14, the total training time of APT-L2 and APT is 18321.39 seconds and 18592.01 seconds respectively (including the time consumed in graph selection and regularization term), while the competitive graph pre-training model GCC takes 40161.68 seconds for the same number of training epochs on the same datasets.\n\n• The time spent on the inference on all graphs during graph selection (which is the main time spent for graph selection) only accounts for 3.95% and 3.87% of the total time under APT-L2 and APT respectively. Note that this step is executed only in a few epochs (around 6% in our current model) if and only if the condition in line 5 in Algorithm 1 is satisfied.\n\n• The time cost of the L2 regularization term only accounts for 0.08% of the total time and the EWC regularization term only accounts for 0.45% of the total time, which is calculated by the runtime gap between the models with and without the regularization term. Note that the regularization term is imposed on the first two layers of the GNN encoder, which only accounts for 12.4% of the total number of parameters.\n\nThe efficiency of our model is due to a much smaller number of carefully selected training graphs and samples at each epoch. In addition, the number of parameters in our model is 190,544, which is the same order of magnitude as classical GNNs like GraphSAGE, GraphSAINT, etc. and is relatively small among models in open graph benchmark Hu et al. (2020b).\n\ntraining from scratch.\n\nTime comparison: pre-training vs. Using a pre-trained model can significantly reduce the time required for training from scratch. The reason is that the weights of the pre-trained model have already been put close to appropriate and reasonable values; thus the model converges faster during fine-tuning on a test data. As shown in Figure 12, compared to regular GNN model (e.g. GIN), our model yields a speedup of 4.7× on average (which is measured by the ratio of the training time of GIN to the fine-tuning time of APT). Based on above analysis, we can draw a conclusion that pre-training is beneficial both in effectiveness and efficiency.\n\nFigure 12: The running time of our model and the basic GNN model on graph classification task. Our model achieves a speedup of 4.7× on average compared with GIN.\n\n26\n\nmsrc-21ddimdb-binaryrunning time (sec)APTGIN2004006005.23.94.9⇥<latexit sha1_base64=\"bOTtxKgBv1/Y3YpPvW7uqUlUa+A=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lqQY8FLx4r2A9oQ9lsN+3azSbsToQS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1gNOE+xEdKREKRtFK7T6KiJtBueJW3QXIOvFyUoEczUH5qz+MWRpxhUxSY3qem6CfUY2CST4r9VPDE8omdMR7lipql/jZ4toZubDKkISxtqWQLNTfExmNjJlGge2MKI7NqjcX//N6KYY3fiZUkiJXbLkoTCXBmMxfJ0OhOUM5tYQyLeythI2ppgxtQCUbgrf68jpp16reVbV2X6806nkcRTiDc7gED66hAXfQhBYweIRneIU3J3ZenHfnY9lacPKZU/gD5/MHs0OPKg==</latexit>⇥<latexit sha1_base64=\"bOTtxKgBv1/Y3YpPvW7uqUlUa+A=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lqQY8FLx4r2A9oQ9lsN+3azSbsToQS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1gNOE+xEdKREKRtFK7T6KiJtBueJW3QXIOvFyUoEczUH5qz+MWRpxhUxSY3qem6CfUY2CST4r9VPDE8omdMR7lipql/jZ4toZubDKkISxtqWQLNTfExmNjJlGge2MKI7NqjcX//N6KYY3fiZUkiJXbLkoTCXBmMxfJ0OhOUM5tYQyLeythI2ppgxtQCUbgrf68jpp16reVbV2X6806nkcRTiDc7gED66hAXfQhBYweIRneIU3J3ZenHfnY9lacPKZU/gD5/MHs0OPKg==</latexit>⇥<latexit sha1_base64=\"bOTtxKgBv1/Y3YpPvW7uqUlUa+A=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lqQY8FLx4r2A9oQ9lsN+3azSbsToQS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1gNOE+xEdKREKRtFK7T6KiJtBueJW3QXIOvFyUoEczUH5qz+MWRpxhUxSY3qem6CfUY2CST4r9VPDE8omdMR7lipql/jZ4toZubDKkISxtqWQLNTfExmNjJlGge2MKI7NqjcX//N6KYY3fiZUkiJXbLkoTCXBmMxfJ0OhOUM5tYQyLeythI2ppgxtQCUbgrf68jpp16reVbV2X6806nkcRTiDc7gED66hAXfQhBYweIRneIU3J3ZenHfnY9lacPKZU/gD5/MHs0OPKg==</latexit>Under review as a conference paper at ICLR 2023\n\nTable 10: The effect of different graph properties on downstream performance (micro F1 is reported) under APT-L2 (fine-tune) in the node classification task. The last row is our strategy of combining all the graph properties, and each of the first five rows is the strategy of only utilizing one graph property.\n\nDataset\n\nMethod\n\nbrazil\n\ndd242\n\ndd68\n\ndd687\n\nwisconsin\n\ncornell\n\ncora\n\npubmed\n\n80.04(2.15) 25.79(0.94) 16.31(0.81) 11.08(0.82) 67.01(2.00) 52.80(2.46) 45.41(0.85) 50.85(0.19) Entropy 79.23(1.92) 27.29(0.62) 19.89(0.95) 12.22(1.06) 65.58(1.87) 51.15(1.59) 46.18(0.71) 50.74(0.15) Density 79.22(1.65) 24.99(0.67) 16.56(1.01) 11.67(1.18) 67.02(1.86) 51.43(4.16) 46.38(0.48) 50.99(0.31) Average degree Degree variance 78.44(2.24) 24.94(0.61) 16.62(1.04) 11.51(1.17) 65.65(1.28) 50.45(2.14) 45.76(0.65) 50.70(0.21) Scale-free exponent 79.70(2.71) 24.94(0.68) 17.26(0.63) 12.03(1.41) 64.77(2.31) 51.37(2.70) 45.18(0.52) 50.84(0.26) 78.75(1.63) 24.62(0.90) 17.83(1.35) 12.26(0.78) 67.04(1.50) 52.94(1.95) 47.48(0.46) 51.25(0.21) Our combination\n\nTable 11: The effect of different graph properties on downstream performance (micro F1 is reported) under APT-L2 (fine-tune) in the graph classification task.\n\nDataset\n\nimdb-binary\n\ndd\n\nmsrc-21\n\nMethod Entropy 76.78(0.84) 75.56(0.84) 24.34(1.50) 77.20(0.66) 75.29(0.54) 24.20(1.31) Density Average degree 76.87(0.93) 75.46(0.53) 25.14(1.54) 76.39(1.04) 75.47(0.67) 25.22(1.51) Degree variance Scale-free exponent 75.24(0.62) 75.52(1.24) 23.19(1.39) 75.93(0.84) 75.58(1.06) 25.58(1.57) Our combination\n\nTable 12: The effect of input graphs’ learning order on downstream performance (micro F1 is reported) under freezing mode in the node classification task. The first row is the order learnt from APT-L2, and the second and third rows are the reverse and random order of the first row, respectively.\n\nDataset\n\nMethod\n\nbrazil\n\ndd242\n\ndd68\n\ndd687\n\nwisconsin\n\ncornell\n\ncora\n\npubmed\n\n69.82(2.32) 16.79(0.88) 12.68(0.81) 10.34(1.12) 55.11(1.74) 48.76(2.20) 34.27(0.43) 46.21(0.15) Our order Reverse order 69.60(2.71) 16.00(0.47) 11.41(0.91) 10.65(0.65) 51.46(1.64) 44.36(1.38) 35.66(0.62) 45.92(0.14) Random order 67.25(2.40) 16.11(0.79) 12.57(1.17) 11.06(0.75) 53.06(2.41) 46.76(1.95) 35.90(0.72) 46.36(0.20)\n\nTable 13: The comparison of learning from easy samples and learning from difficult sample in our pipeline (APT-L2 (freeze)) on node classification. Micro F1 is reported in the table. (Under the setting of learning from easy samples, we replace φuncertain with φuncertain in Eq.(4), and only sample instances with predictive uncertainty lower than Ts.)\n\n−\n\nMethod\n\nDataset\n\nbrazil\n\ndd242\n\ndd68\n\ndd687\n\nwisconsin\n\ncornell\n\ncora\n\npubmed\n\n56.34(3.45) 14.38(0.53) 11.76(1.04) 9.90(0.64) 50.65(1.84) 48.09(1.72) 35.74(0.42) 46.03(0.17) Learning from easy samples Learning from difficult samples (ours) 69.82(2.32) 16.79(0.88) 12.68(0.81) 10.34(1.12) 55.11(1.74) 48.76(2.20) 34.27(0.43) 46.21(0.15)\n\nTable 14: Training time (sec) comparison between our model and GCC. All the models are trained under the same number of epochs, which is set as 100 in practice. (The difference in time cost of inference on all graphs is due to different runs.)\n\nTime of the inference on all graphs Time of the regularization term Total time\n\nGCC -\n- 40161.68\n\nAPT-L2 723.92 15.98 18321.39\n\nAPT 719.64 83.58 18592.01\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nH DISCUSSION: THE DESIGN OF PREDICTIVE UNCERTAINTY.\n\nWe here discuss two advantages of using the model loss (i.e., InfoNCE loss) as predictive uncertainty to select samples. First, InfoNCE loss is exactly the objective function of our model, so what we do is actually to select the samples with the greatest contributions to the objective function (i.e., select the samples with the greatest InfoNCE loss). Such strategy has been justified to accelerate convergence and enhance the discriminative power of the learned representations [1-4]. Second, as the loss function of our model, InfoNCE is already computed during the training, and thus no additional computation expense is needed in the data selection phase.\n\nREFERENCES\n\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of\n\nlarge scale pre-training. In ICLR, 2022.\n\nFrancesco Bonchi, Pooya Esfandiar, David F Gleich, Chen Greif, and Laks VS Lakshmanan. Fast matrix computations for pairwise and columnwise commute times and katz scores. Internet Mathematics, 8(1-2):73–112, 2012.\n\nHongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. Active learning for graph embed-\n\nding. arXiv preprint arXiv:1705.05085, 2017.\n\nAaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical\n\ndata. SIAM review, 51(4):661–703, 2009.\n\nPaul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without\n\nalignments. Journal of molecular biology, 330(4):771–783, 2003.\n\nM. Fire, L. Tenenboim, O. Lesser, R. Puzis, L. Rokach, and Y. Elovici. Link prediction in social networks using computationally efficient topological features. In IEEE third international conference on social computing, pp. 73–80, 2011.\n\nJes ́us G ́omez-Gardenes and Vito Latora. Entropy rate of diffusion processes on complex networks.\n\nPhysical Review E, 78(6):065102, 2008.\n\nHakim Hafidi, Mounir Ghogho, Philippe Ciblat, and Ananthram Swami. Graphcl: Contrastive self-\n\nsupervised learning of graph representations. arXiv preprint arXiv:2007.08025, 2020.\n\nBronwyn H Hall, Adam B Jaffe, and Manuel Trajtenberg. The nber patent citations data file: lessons,\n\ninsights and methodological tools. 2001.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020a.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. NeurIPS, 33:22118–22133, 2020b.\n\nChen Huang, Chen Change Loy, and Xiaoou Tang. Local similarity-aware deep feature embedding.\n\nIn NeurIPS, pp. 1262–1270, 2016.\n\nJure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Mahoney. Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. Internet Mathematics, 6(1):29–123, 2009.\n\nJure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in\n\nonline social networks. In WWW, pp. 641–650, 2010.\n\nSihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant\n\nrationale discovery inspire graph contrastive learning. In ICML, pp. 13052–13065, 2022.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nChristopher W Lynn, Lia Papadopoulos, Ari E Kahn, and Danielle S Bassett. Human information\n\nprocessing in complex networks. Nature Physics, 16(9):965–973, 2020.\n\nAndrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the Information Retrieval, 2(3):127–163,\n\nconstruction of internet portals with machine learning. 2000.\n\nGalileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven active surveying for\n\ncollective classification. In MLG, pp. 1, 2012.\n\nMarion Neumann, Roman Garnett, Christian Bauckhage, and Kristian Kersting. Propagation kernels: efficient graph kernels from propagated information. Machine Learning, 102(2):209–245, 2016.\n\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric\n\ngraph convolutional networks. In ICLR, 2019.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In SIGKDD, pp. 1150–1160, 2020.\n\nLeonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node\n\nrepresentations from structural identity. In SIGKDD, pp. 385–394, 2017.\n\nRyan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics\n\nand visualization. In AAAI, pp. 4292–4293, 2015.\n\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face\n\nrecognition and clustering. In CVPR, pp. 815–823, 2015.\n\nKihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS, pp.\n\n1849–1857, 2016.\n\nYumin Suh, Bohyung Han, Wonsik Kim, and Kyoung Mu Lee. Stochastic class-based hard example\n\nmining for deep metric learning. In CVPR, pp. 7244–7252, 2019.\n\nAmanda L Traud, Peter J Mucha, and Mason A Porter. Social structure of Facebook networks.\n\nPhysica A: Statistical Mechanics and its Applications, 391(16):4165–4180, 2012.\n\nFeng Xia, Jiaying Liu, Hansong Nie, Yonghao Fu, Liangtian Wan, and Xiangjie Kong. Random walks: A review of algorithms and applications. IEEE Transactions on Emerging Topics in Computational Intelligence, 4(2):95–107, 2019.\n\nPinar Yanardag and SVN Vishwanathan. Deep graph kernels. In SIGKDD, pp. 1365–1374, 2015.\n\nJaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-\n\ntruth. In SIGKDD, pp. 1–8, 2012.\n\n29",
    "reference": "# Summary Of The Paper\n\nThis paper studies an interesting question in terms of which graph datasets should be selected for GNN pre-training tasks. The authors propose a novel graph selector that is able to provide the most instructive data for the model. The criteria in the graph selector include predictive uncertainty and graph properties (graph entropy, density, degree, etc). Besides, they propose a data-active graph pre-training (APT) framework, which integrates the graph selector and the pre-training model into a unified framework.\n\n# Strength And Weaknesses\n\n**Strength**\n\n- Overall, the problem studied seems to be a novel angle in the context of GNN pre-training. Efficient pre-training is currently a new trend in other domains such as NLP and CVs with a focus on how to select informative training instances. It is important to shed some light in the context of how to better use the pre-training graphs data. \n\n- The core idea the author proposed intuitively makes sense. Indeed, graphs with certain characteristics such as higher network entropy, larger density, higher average degree, higher degree variance, or a smaller scale-free exponent will contain a larger amount of information. Even though the authors do not provide theoretical justification, empirically, this work does provide good insight into GNN pre-training. \n\n**Weaknesses**\n\n- The intuition figure 1 is a bit hard to fully understand. For the first row, when scaling up the sample size, does it mean we keep the same number of pre-train graphs but we vary the same portion of sample size from each pre-train graph dataset? For the bottom row, the total number of pre-train graphs in the paper is eleven and the number of graphs in the figure is only up to ten. The author should provide what is the benchmark results if we use all the possible pre-train datasets to properly compare \n\n- Directly using contrastive loss to define and measure predictive uncertainty is not questionable. \n\n- The notation of this paper is a bit messy. For example, both scaler and vector use non-bolded lowercase (degree vector and node degree) which is not conventional. \n\n- The selection mechanism is a bit complicated and heavily handcrafted. Currently, the graph selection policy uses a combination of 6 loss terms in total. Besides, loss terms work on a very different scale and are combined together with an additional time-adaptive parameter. I am not sure if this design is reasonable. Empirically, there is no proper ablation study on the choice of which graph property to be included in the pre-train graph selection criteria. \n\n- There is an additional proximal regularization term in the model design, which aims to better preserve the knowledge or information contained in previous input data when we train on new incoming data, a similar design component in the continue learning paradigm. I am not able to fully follow the rationale of this design. The pre-training problem is completely different from the catastrophic forgetting setting since we will normally shuffle the training order of the samples after each epoch. The claim of \"previous input data will be forgotten or covered by new incoming data\" is invalid if we shuffle the data training order. Besides, the $\\mathbf{\\theta}$ parameters learned from the first $j$ graphs, does it mean in terms of the memory complexity of the model will be $j$ times larger since we need to store the previous training iteration of the model?\n\n- Experimental results-wise, I have several concerns: 1) It seems the work is directly established on top of the GCC paper, but with completely different suits of pre-training datasets and downstream datasets. The choice of dataset section seems arbitrary and suspicious. Could the author directly work on the precious experiment setting in GCC? or what's the reason behind the complete switch of datasets? \n 2) The baseline uses are quite outdated, using baselines only coming before the year 2020. A lot of recent work in terms of GNN pre-training or graph data augmentation should be included [1] - [8].  3) One suggestion to the authors, since the paper uses a lot of downstream tasks and lots of numbers in the result table, a better way to present the results can be considered. For example, providing an average rank number across all the datasets for each method will be informative and helpful to deliver the message to readers.  3) I will suggest including the full pre-training dataset results in the experiment table to see the effectiveness of the pre-train graph selection scheme. Or should I interpret the GCC results as the full pre-training dataset results?  4) Can the author provide the training time comparison? It seems that to fully  \n\n- Can the author comment on the relationship of this work to some recent adaptive graph positive sample generation work? It seems they all achieve a similar end goal by finding the proper training samples [3] [5] [6] [7].\n\n[1] Hu, Ziniu, et al. \"GPT-GNN: Generative pre-training of graph neural networks.\" KDD 2020.\n\n[2] Xu, Dongkuan, et al. \"Infogcl: Information-aware graph contrastive learning.\" NeurIPS 2021.\n\n[3] Zhu, Yanqiao, et al. \"Graph contrastive learning with adaptive augmentation.\" Proceedings of the Web Conference 2021. 2021.\n\n[4] Zhu, Yanqiao, et al. \"An Empirical Study of Graph Contrastive Learning.\" NeurIPS 2021.\n\n[5] You, Yuning, et al. \"Graph contrastive learning automated.\" ICML 2021.\n\n[6] Lee N, Lee J, Park C. Augmentation-free self-supervised learning on graphs. In Proc. of the AAAI Conference on Artificial Intelligence 2022.\n\n[7] Han et al. \"G-Mixup: Graph Data Augmentation for Graph Classification.\" ICML 2022.\n\n[8] Hou, Zhenyu, et al. \"GraphMAE: Self-Supervised Masked Graph Autoencoders.\" KDD 2022.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease refer to Strength And Weaknesses.\n\n# Summary Of The Review\n\nPlease refer to Strength And Weaknesses.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLARGE LEARNING RATE MATTERS FOR NON-CONVEX OPTIMIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWhen training neural networks, it has been widely observed that a large step size is essential in stochastic gradient descent (SGD) for obtaining superior models. However, the effect of large step sizes on the success of SGD is not well understood theoretically. Several previous works have attributed this success to the stochastic noise present in SGD. However, we show through a novel set of experiments that the stochastic noise is not sufficient to explain good non-convex training, and that instead the effect of a large learning rate itself is essential for obtaining best performance. We demonstrate the same effects also in the noise-less case, i.e. for full-batch GD. We formally prove that GD with large step size—on certain non-convex function classes—follows a different trajectory than GD with a small step size, which can lead to convergence to a global minimum instead of a local one. Finally, we also demonstrate the difference in trajectories for small and large learning rates for real neural networks, again observing that large learning rates allow escaping from a local minimum, confirming this behavior is indeed relevant in practice.\n\n1\n\nINTRODUCTION\n\nWhile using variants of gradient descent (GD), namely stochastic gradient descent (SGD), has become standard for optimizing neural networks, the reason behind their success and the effect of various hyperparameters is not yet fully understood. One example is the practical observation that using a large learning rate in the initial phase of training is necessary for obtaining well performing models (Li et al., 2019). Though this behavior has been widely observed in practice, it is not fully captured by existing theoretical frameworks.\n\nRecent investigations of SGD’s success (Kleinberg et al., 2018; Pesme et al., 2021) have focused on understanding the implicit bias induced by the stochasticity. Note that the effective variance of the trajectory due to the stochasticity of the gradient is moderated by the learning rate (see Appendix F for more intuition). Therefore, using a larger learning rate amplifies the stochasticity and the implicit bias induced by it which can provide a possible explanation for the need for larger learning rates. We show that this explanation is incomplete by demonstrating cases where using stochasticity with arbitrary magnitude but with a small learning rate, can not guarantee convergence to global minimum whereas using a large learning rate can. Furthermore, we provide a practical method to increase stochasticity without changing the learning rate when training neural networks and observe that increased stochasticity can not replace the effects of large learning rates. Therefore, it is important to study how a larger learning rate affects the trajectory beyond increasing the stochasticity.\n\nTo that end, in this work we show that randomly initialized gradient descent with a high learning rate provably escapes local minima and converges to the global minimum over of a class of non-convex functions. In contrast, when using a small learning rate, GD over these functions can converge to a local minimum instead. We note that for brevity, we focus our results on the full-batch GD.\n\nWe further show the positive effect of using a high learning rate to increase the chance of completely avoiding undesirable regions of the landscape such as a local minimum. Note that this behavior does not happen when using the continuous version of GD, i.e. gradient flow which corresponds to using infinitesimal step sizes. The difference remains even after adding the implicit regularization term identified in (Smith et al., 2021) in order to bring trajectories of gradient flow and gradient descent closer.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nWe would like to note that throughout the paper, we sometimes misuse the terms “global” and “local” minimum to refer to desirable and undesirable minima respectively. For example when discussing generalization, a desirable minimum might not have the lowest objective value but enjoy properties such as flatness.\n\nFinally, to show the relevance of our theoretical results in practice, we demonstrate similar effects can happen in neural network training by showing evidence of an escape from local minimum when applying GD with a high learning rate on a commonly used neural network architecture. Our observations signify the importance of considering the effects of high learning rates for understanding the success of GD.\n\nOverall, our contributions can be summarized as follows: (cid:15) Demonstrating the exclusive effects of large learning rates even in the stochastic setting both in theory and in practice, showing that they can not be reproduced by increasing stochasticity and establishing the importance of analyzing them.\n\n(cid:15) Capturing the distinct trajectories of large learning rate GD and small learning rate GD in theory on a class of functions, demonstrating the empowering effect of large learning rate to escape from local minima.\n\n(cid:15) Providing experimental evidence showing that gradient descent escapes from local minima in neural network training when using a large learning rate, establishing the relevance of our theoretical results in practice.\n\n2 RELATED WORK\n\nExtensive literature exists on studying the effect of stochastic noise on the convergence of GD. Several works have focused on the smoothing effect of injected noise (Chaudhari et al., 2017; Kleinberg et al., 2018; Orvieto et al., 2022; Wang et al., 2021a). In (Vardhan & Stich, 2022) it has been shown that by perturbing the parameters at every step (called perturbed GD) it is possible to converge to the minimum of a function f while receiving gradients of f + g, assuming certain bounds on g. Other works use different models for the stochastic noise in SGD and use it to obtain convergence bounds or to show SGD prefers certain type (usually flat) of minima (Wu et al., 2018; Xie et al., 2021). In order to better understand the effect of various hyperparameters on convergence, Jastrzebski et al. (2019); Jastrzbski et al. (2018) show the learning rate (and its ratio to batch size) plays an important role in determining the minima found by SGD. In (Pesme et al., 2021) it was shown that SGD has an implicit bias in comparison with gradient flow and its magnitude depends on the learning rate. While this shows one benefit of using large learning rates, in this work, we provide evidence that the effect of learning rate on optimization goes beyond controlling the amount of induced stochastic noise.\n\nPrior work also experimentally establish existence of different phases during training of a neural network. Cohen et al. (2021) show that initially Hessian eigenvalues tend to grow until reaching the convergence threshold for the used learning rate, a state they call \"Edge of Stability\". This growth is also reported in (Lewkowycz et al., 2020) for the maximum eigenvalue of the Neural Tangent Kernel (Jacot et al., 2018) where it has also been observed that this value decreases later in training, leading to convergence. Recent works have also investigated GD’s behavior at the edge of stability for some settings (Arora et al., 2022) obtaining insights such as its effect on balancing norms of the layers of a two layer ReLU network (Chen & Bruna, 2022). In our results, GD is above the conventional stability threshold while it is escaping from a local minimum but returns to stability once the escape is finished.\n\nIn (Elkabetz & Cohen, 2021) it is conjectured that gradient descent and gradient flow have close trajectories for neural networks. However, the aforementioned observations suggest that gradient descent with a large learning rate visits a different set of points in the landscape than GD with a small learning rate. Therefore, this conjecture might not hold for general networks. The difference in trajectory is also supported by the practical observation that a large learning rate leads to a better model (Li et al., 2019).\n\nTo bridge this gap and by comparing gradient flow and gradient descent trajectories, Barrett & Dherin (2021) identify an implicit regularization term on gradient norm induced by using discrete steps. Still, this term is not enough to remove a local minimum from the landscape. Other implicit regularization terms specific to various problems have also been proposed in the literature (Ma et al., 2020; Razin & Cohen, 2020; Wang et al., 2021b). In this paper, we provide experimental evidence\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand showcase the benefits of using large step sizes that are unlikely to be representable through a regularization term, suggesting that considering discrete steps might be necessary to understand the success of GD.\n\nThe type of obstacles encountered during optimization of a neural network is a long-standing question in the literature. Lee et al. (2016) show that gradient descent with random initialization almost surely avoids saddle points. However it is still unclear whether local minima are encountered during training. In (Goodfellow & Vinyals, 2015) it was observed that the loss decreases monotonically over the line between the initialization and the final convergence points. However, it was later shown that this observation does not hold when using larger learning rates (Lucas et al., 2021). Swirszcz et al. (2017) also show that it is possible to create datasets which lead to a landscape containing local minima. Furthermore, better visualization of the landscape shows non-convexities can be observed on some loss functions (Li et al., 2018). For the concrete case of two layer ReLU networks, Safran & Shamir (2018) show gradient descent converges to local minima quite often without the help of over-parameterization. Also, it was shown that in the over-parameterized setting, the network is not locally convex around any differentiable global minimum and one-point strong convexity only holds in most but not all directions (Safran et al., 2021). These observations show the importance of understanding the mechanisms of escaping local minima. We also use these observations to make assumptions that are practically justifiable.\n\nThere also exists a body of work on which properties of a minimum leads to better generalization (Dinh et al., 2017; Dziugaite & Roy, 2017; Keskar et al., 2017; Tsuzuku et al., 2020). In this work, our goal is to show the ability of gradient descent to avoid certain minima when using a high learning rate. However, the argument about whether these minima offer better or worse generalization is outside the scope of this work.\n\n3 MAIN RESULTS\n\nTheoretical Proof of Escaping From Local Minima with a Large Learning Rate The need for a large learning rate in practice is commonly explained based on the intuition of escaping certain local minima. However, a theoretical setting where GD escapes from a local minimum and converges to a global minimum is lacking. Such settings are necessary both for understanding success of GD and for analyzing the effectiveness of other optimizers. In this work, we introduce a class of functions where such behavior can be observed from GD. This is stated in Theorem 1 which we describe here informally and leave the formal version to Section 4.1. Theorem 1 (Informal). There exists a class of functions having at least two minima xy and x⋆ where GD initialized on a random point, converges to x⋆ with a large learning rate almost surely but might converge to xy with a small learning rate.\n\nTheoretical Analysis of Avoiding Local Minima As an alternative to escaping from minima, we note that due to discrete steps in GD, it may not visit any point in an arbitrary but small part of the landscape X, such as a local minimum. However, note that there may still exist a set of starting points for which GD iterates reach a point in X. Therefore, assuming the starting point is chosen randomly, not visiting any point in X is a probabilistic event. In this work, we provide a lower bound for the probability of this event in Theorem 2 which we state here informally and postpone the formal statement to Section 4.2. Theorem 2 (Informal). For any arbitrary part (subset) of the landscape X sufficiently far from the global minimum, let EX be the probabilistic event that GD, when initialized randomly from a large enough set, will not iterate over any point in X. Then under certain assumptions on the landscape, Pr [EX ] can be lower bounded where the bound depends monotonically increasing on the learning rate and inversely on the size of X (as measured by Lebesgue measure). In particular, if X is finite, this probability is 1.\n\nThe dependence of the lower bound on the learning rate is intuitive as a larger learning rate allows larger steps and makes it less probable (but not impossible) to visit a small part of the landscape as illustrated in Figure 1. We note that avoiding a region is inherently different from escaping from it. In particular, as can be seen in the example, this region can be almost completely flat. In this case, once that GD reaches a point in this region, it will instantly converge. Furthermore, the region can even contain points where the function can not be differentiated. Therefore, such effect can not be compensated for by adding previously identified implicit regularization terms such as the one in\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GD escapes with large LR.\n\n(b) GD escapes with both LRs.\n\n(c) GD does not escape.\n\nFigure 1: Success of GD to avoid a region based on the magnitude of learning rate when initialized from different points. While various cases are possible, it is more likely to avoid the minimum with a higher learning rate.\n\n(Smith et al., 2021). This suggests other methods are needed to bridge the gap between gradient flow and GD. We demonstrate both effects of escaping and avoiding local minima on an example function in Appendix H.\n\nImportance of Large Learning Rate Despite the Effects of Stochastic Noise One possible explanation for the need of large learning rate is the magnification of the effect of stochastic noise (see Appendix F for further intuition), facilitating escaping from local minima. While this explanation can not explain the success of full-batch GD, it is more common to use SGD in practice. Therefore, this explanation makes it questionable whether it is necessary to understand direct effects of learning rate on the trajectory or is it enough to only consider the stochastic noise.\n\nIn this work, we show that the effects of using a large learning rate goes beyond magnifying stochastic noise. To that end, we first provide an example in Section 4.3 where escaping from a local minimum and converging to the global minimum can only be achieved with a large learning rate even in presence of stochastic noise. Furthermore, we demonstrate this result in practice in Section 5.1 by decoupling the effect of stochastic noise on the trajectory and the magnitude of the learning rate when training neural networks. Our experiment results show that the effects of the large learning rate remain crucial for converging to the correct minimum even in presence of (magnified) stochastic noise.\n\nDemonstrating Effects of Large Learning Rate in Neural Networks While escaping local minima is an intuitive explanation, an escape is not clearly observed while training a neural network using GD with a large learning rate even though it converges to a different minimum. In particular, it seems GD automatically avoids cases where it gets close to a local minima and then escapes from it. This makes it hard to verify the relevance of escaping behavior for neural network landscape. We do so in Section 5.2 by deliberately finding a point close to a minimum that GD would converge to with a small learning rate. In contrast, when applying GD with a large learning rate from this point, we can clearly observe an escape both in the trajectory and in the value of the loss.\n\n4 THEORETICAL ANALYSIS\n\nWe now state our results more formally. For our theoretical analysis, we focus on optimizing the minimization problem\n\nf⋆ := min x2Rd using (full-batch) gradient descent with random initialization. For completeness, we provide a pseudo code in the Appendix A, Algorithm 1.\n\nf (x)\n\nAs is done widely in the literature, we assume smoothness (as defined in Definition 1) over regions of the landscape to ensure the gradient does not change too sharply. Definition 1 (L-smoothness). A function f : Rd ! R is L-smooth if it is differentiable and there exists a constant L > 0 such that:\n\n∥∇f (x) (cid:0) ∇f (y)∥ (cid:20) L∥x (cid:0) y∥ ;\n\n8x; y 2 Rd :\n\n(1)\n\nSimilarly, we need to ensure sharpness of certain regions, in particular around a local minima, to obtain our results. Therefore, to ensure a lower bound for sharpness in our analysis, we use onepoint strong convexity assumption on these regions as defined in the following definition which also commonly appears in the literature:\n\n4\n\n678910x0500100015002000f(x)small lrlarge lr678910x0500100015002000f(x)small lrlarge lr678910x0500100015002000f(x)small lrlarge lrUnder review as a conference paper at ICLR 2023\n\nFigure 2: A case where GD keeps returning to a sharp minimum showing that a lower bound on the distance to the global minimum might be necessary to show it can be avoided.\n\n(a) GD with small LR converges to the minimum.\n\n(b) GD with large LR escapes the minimum.\n\nFigure 3: Different behaviors of GD based on the magnitude of learning rate in escaping or converging a sharp minima. GD with a high enough learning rate always escapes the minimum.\n\nDefinition 2 ((cid:22)-one-point-strongly-convex (OPSC) with respect to x⋆ over M ). A function f : Rd ! R is one-point strongly convex with respect to x⋆ if it is differentiable and there exists a constant (cid:22) > 0 such that:\n\n⟨∇f (x); x (cid:0) x⋆⟩ (cid:21) (cid:22)∥x (cid:0) x⋆∥2 ;\n\n8x 2 M :\n\n(2)\n\nAssuming OPSC property is common in the literature. When this assumption is applied over the whole landscape, it has been shown to guarantee convergence to x⋆ (Kleinberg et al., 2018; Lee et al., 2016; Safran et al., 2021). However, in this work we only make this assumption hold on a limited part of the landscape, namely regions around a local minima. Furthermore, we use this assumption to ensure sharpness which we show can result in escaping from the regions where this assumption holds rather than converging to them. Note that recent works have verified both theoretically and empirically that landscapes of neural networks satisfy this property to some extent (Kleinberg et al., 2018; Safran et al., 2021). For example, Safran et al. (2021) show that the condition is satisfied with high probability over the trajectory of perturbed gradient descent on over-parameterized twolayer ReLU networks when initialized in a neighborhood of a global minimum. We also note that there exists other variants of this definition such as quasi-strong convexity (Necoara et al., 2019) or (1; (cid:22))-(strong) quasar convexity (Hinder et al., 2020), which are similar but slightly stronger.\n\n4.1 ESCAPING FROM LOCAL MINIMA WITH A LARGE LEARNING RATE\n\nWe first state a lemma which is the key to proving Theorem 1. In particular, this lemma defines a set of criteria for the region M around a minimum xy as well as the region around M , called P (M ), that ensures GD escapes from M moving toward a different minimum x⋆. To build further intuition, Figure 3 provides an example of how GD with large learning rate may escape a sharp minimum. Figure 4 provides an illustration of different regions defined in the theorem’s statement. Lemma 1. Let f be a function that is Lglobal-smooth and consider running GD with learning rate (cid:13) and randomly initialized over a set W with L(W ) > 0. Let M be a set with diameter r, containing a local minimum xy and define P (M ) := fx =2 M j ∥x (cid:0) xy∥2 (cid:20) r (cid:0) 3g to be the set surrounding M . Assume f is L < Lglobal-smooth and (cid:22)⋆-OPSC over P (M ) with respect to a (global) (cid:0)3)(1(cid:0)(cid:13)(cid:22)⋆) minimum x⋆ that is sufficiently far from M , formally, ∥x⋆ (cid:0) xy∥2 (cid:21) r (cid:1) 1+ Finally, assume f is (cid:22)y-OPSC with respect to xy over M where (cid:22)y > 2L2 learning rate 2 distance to x⋆ of less than ∥xy (cid:0) x⋆∥ (cid:0) r almost surely.\n\n. Then, using a suitable L2 , if GD reaches a point M , it will escape M and reach a point with\n\n(cid:22)y < (cid:13) (cid:20) (cid:22)⋆\n\n((cid:13)2L2 1(cid:0)\n\n(cid:13)2L2\n\n1(cid:0)(cid:13)(cid:22)⋆\n\nglobal\n\n√\n\np global\n\np\n\n(cid:22)⋆\n\n.\n\nNote that this lemma allows for multiple local minima to exist on the landscape and only applies constraints around each local minimum. Furthermore, we point out that the lemma only ensures that GD will exit the local minima after some steps. In order to obtain convergence guarantees to the global minimum, it is necessary to assume a convergence property on the rest of the landscape as well. Indeed, this is how we build the class of functions to prove Theorem 1. We provide a complete proof of Lemma 1 in Appendix D and proceed by discussing some of our assumptions:\n\nOPSC condition inside M We assume f is OPSC with respect to a different minima inside M in order to ensure GD will escape from M . However, other conditions might also ensure the same\n\n5\n\n−3−2−10123x05101520f(x)diverging from Mreturning to MM01234x050100150200250300f(x)falling into minimaconverging into minima01234x050100150200250300f(x)falling into minimaescaping from minimaUnder review as a conference paper at ICLR 2023\n\nFigure 4: Illustration of different regions defined in Lemma 1.\n\nFigure 5: An example where convergence to global minimum can only be achieved by using both stochastic noise and large learning rate but not with either used alone.\n\neffect. The theorem would also hold with those assumptions. Note that the sharpness of M with respect to the rest of landscape is reflected through the lower bound on (cid:22)y and is necessary so we can set the learning rate in the given range. As an example, when f is a quadratic function everywhere except M (such as in Figure 4), we have (cid:22)⋆ = L and the lower bound becomes (cid:22)y > 2L.\n\nOPSC condition around M We combine this assumption with the assumption on M being sufficiently far from the global minimum in order to ensure that once GD escapes from a local minima, the gradient points strongly towards x⋆. This ensures that GD will reach a point closer to the global minimum after escaping M . While the OPSC assumption is not necessary to show GD will never converge to M and may be replaceable by alternatives, an assumption on the distance to x⋆ might be necessary to show GD will not return to M . For example, consider a quadratic function where the region around minimum is replaced by a sharper quadratic function, as plotted in Figure 2. In this case, GD with a high learning rate will keep returning to M though it will never converge to it. As alternatives to OPSC assumption on P (M ), one can assume GD converges in at most a fixed number of steps (which Corollary 1 states can not be to any point in M almost surely) or assume directly that the gradient points strongly away from M . Finding similar assumptions is grounds for future work.\n\nGiven Lemma 1, it can be seen that if it is possible to ensure the iterations of GD get closer to the global minimum on the rest of the landscape, it is possible to ignore existence of the region M . This is because either the iterations would never cross M or if they do, they will eventually reach a point closer to the global minimum according to Lemma 1. We build the class of functions for Theorem 1 based on this observation. We now state the formal statement of Theorem 1 and leave the proof to Appendix E. Theorem 1 (Formal). Consider any function f that is L-smooth and (cid:22)⋆-OPSC with respect to the global minimum x⋆ in its landscape except on a region M containing a local minimum xy satisfying the conditions in Lemma 1. GD initialized randomly inside M converges to xy with a small learning rate (cid:13) < (cid:22)y In contrast, GD initialized randomly over any arbitrary set W with positive Lebesgue measure L(W ) > 0 will instead converge to x⋆ with a large learning rate (cid:22)y < (cid:13) (cid:20) (cid:22)⋆\n\nL2 almost surely.\n\nL2\n\nglobal\n\n2\n\n.\n\n4.2 AVOIDING LOCAL MINIMA\n\nThe following key lemma is used to prove Theorem 2. Lemma 2. Assume gradient descent is initialized randomly on the set W and is run with learning rate (cid:13) (cid:20) 1 2L . Let X 2 Rd be an arbitrary set of points in the landscape. Assume f is L-smooth over Rd n X. Let L(S) denote the Lebesgue measure of any set S. The probability of encountering any points of X in the first T steps of gradient descent, i.e. xi 2 X for some 1 (cid:20) i (cid:20) T is at most 2(T +1)(cid:1)d (cid:1) L(X) L(W ) .\n\nWe provide the complete proof in Appendix B. Note that the lemma makes no assumption on f over X. Furthermore, the dependence of the bound on T seems inevitable in general since the optimization might force the iterations toward certain regions, such as the region around the minimum.\n\n6\n\n−202468x0250500750100012501500f(x)x†x⋆MP(M)−12.5−10.0−7.5−5.0−2.50.02.55.0x050100150200f(x)x*(M*)x†(M†)Under review as a conference paper at ICLR 2023\n\nWhen using standard GD, by making assumptions on the landscape to ensure the iterations of GD move away from the undesired region X, the bound can be instead adapted to depend inversely on the learning rate which controls the speed of progression. We use one such assumption to prove Theorem 2 which we state formally here and leave its proof to Appendix C.\n\nTheorem 2 (Formal). Let X be an arbitrary set of points. Assume f is L-smooth and (cid:22)⋆-OPSC with respect to a minima x⋆ =2 X over Rd n X. Define cX := inff∥x (cid:0) x⋆∥ j x 2 Xg and rW := supf∥x (cid:0) x⋆∥ j x 2 W g. The probability of not encountering any points of X during running gradient descent with (cid:13) (cid:20) (cid:22)⋆ L(W ) when cX (cid:20) rW L2 is at least 1 (cid:0) 2d (cid:1) rW and is 1 otherwise.\n\nlog2(1(cid:0)(cid:13)(cid:22)⋆ ) (cid:1) L(X)\n\ncX\n\n(cid:0)\n\nd\n\n4.3\n\nIMPORTANCE OF LARGE LEARNING RATE DESPITE THE EFFECTS OF STOCHASTIC NOISE\n\nWe now consider the case of SGD where the stochastic noise is applied as an additive term to the gradient. The update step of SGD in this case would be:\n\nxt+1 := xt (cid:0) (cid:13)(∇f (xt) + (cid:24)t) :\n\n(3)\n\nFor example, when the global objective f (x) is a finite sum of n different objectives, e.g. one for each data point, we will have (cid:24)t := ∇frt(x) (cid:0) ∇f (x) where rt is the index of data point used in t-th step.\n\nIn this section we consider the case where the noise (cid:24)t is drawn from a uniform distribution U nif orm((cid:0)(cid:27); (cid:27)) and assess the convergence point of GD on an example function plotted in Figure 5. This function contains a local minima xy and a global minimum x⋆. Optimally, we would like to ensure convergence to the global minimum regardless of the initialization point. We now show that this is not possible using a small learning rate regardless of the magnitude of the noise. However, we show that when using a large enough learning rate alongside the stochastic noise, it is possible to obtain the desired result. We state this in the following proposition and leave a more formal description and its proof to Appendix G.\n\nProposition 3. Consider running SGD on the function plotted in Figure 5. If the learning rate is sufficiently small, starting close to xy, the iterates will never converge to the optimum x⋆ nor to a small region around it regardless of the magnitude of the noise.On the other hand, by using a large learning rate, given that the stochastic noise satisfies certain bounds, GD will succeed to converge to the optimum x⋆ given any starting point.\n\n5 EXPERIMENTS\n\nWe now provide practical evidence to show the effects of high learning rate also apply and are essential in optimization of neural networks. In our experiments we train a ResNet-18 (He et al., 2016) without batch normalization on CIFAR10 (Krizhevsky & Hinton, 2009) dataset.\n\n5.1 DISENTANGLING EFFECTS OF STOCHASTIC NOISE AND LEARNING RATE\n\nAs can be seen from (3), reducing the learning rate would also reduce the variance of the effective stochastic noise (cid:13)(cid:24)t. This entanglement makes it hard to assess the effects of stochastic noise and large learning rate separately. We design the following method to maintain the level of noise when reducing the learning rate.\n\nSGD with Repeats In order to simulate the same magnitude of noise while still using a smaller learning rate, every time a mini-batch is drawn, we use it for k steps before drawing another minibatch. Note that when k = 1, we recover standard SGD. Re-using the same batch k times, allows the bias of the mini-batch to be amplified k times, so when reducing learning rate by 1 k the overall magnitude remains unchanged. This is explained in more detail in Appendix F.\n\nWe compare standard SGD with learning rate 0:01, standard SGD with learning rate 0:001, and SGD with k = 10 and learning rate 0:001. We apply 0:0005 weight decay, 0:9 momentum, and decay the learning rate at epochs 80, 120, and 160 by 0:1. Results without momentum are reported in Appendix M. When training with standard SGD and learning rate 0:001 we train the model for 10 times more epochs (2000 epochs) in order to obtain a fair comparison and rescale its plot to 200 epochs. In this case, learning rate decay happens at epochs 800, 1200, and 1600. Note that\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Comparsion between performance of SGD with different learning rates. The gap in performance between large and small learning rates, even after repeatedly using the same batch to maintain the effect of stochastic noise, suggests that learning rate has an effect on trajectory beyond controlling stochastic noise. Repeating batches is turned off at epoch 200 and 10 additional epochs are performed (green). For the experiment with 2000 epochs (orange), the plot is normalized to 200 epochs.\n\nwhen running SGD with k = 10 repeats, we perform 10 steps using each batch while going through the whole dataset at each epoch. Therefore, the total number of steps in SGD with k = 10 is the same as standard SGD with 2000 epochs. Furthermore, when we have k > 1 we train the model for 10 more epochs at the end and use each batch only once (as in standard SGD) during the additional epochs. We perform these additional steps since training for several steps on one batch at the end of training might lead to overfitting on that batch which is not desirable for test performance. In Appendix K we also experiment with turning off repeats earlier in the training and observe no significant improvement. Finally, we ensure that the experiment with k = 10 uses the same initialization point and the same ordering of batches used for training with learning rate 0:01.\n\nThe results (averaged over 3 runs) are plotted in Figure 6. The first clear observation is that SGD with learning rate 0:01 leads to a much better model than SGD with learning rate 0:001. More importantly, while amplifying the noise through repeats helps lower the gap, it still has a performance below training with the large learning rate.\n\nExplaining the positive effect of using SGD over GD on convergence has been the focus of several prior work. For example, Kleinberg et al. (2018) argue that applying SGD allows optimization to be done over a smoothed version of the function which empirically satisfies good convergence properties, particularly, one-point strong convexity toward a minimum. We argue that our observation provides a more complete overview and suggests that even after applying stochastic noise (which for example can lead to a smoothing of the function), there might be certain regions of the landscape that can only be avoided using a high learning rate. As we described above, one can consider the effect of stochastic noise to be the improvement observed when using repeats with a small learning rate in comparison with training in a standard way which still does not close the gap with training using a high learning rate. Therefore, the effects of using a high learning rate, such as those described in Section 4, are still important in determining the optimization trajectory even in stochastic setting.\n\n5.2 COMPARING TRAJECTORIES OF LARGE AND SMALL LEARNING RATES\n\nIn Section 4, we proved some of the effects of using large step sizes in avoiding or escaping certain minima in the landscape. We now demonstrate that these effects can be observed in real world applications such as training neural networks. To be able to observe the effect of large learning rate more clearly, we first warm-start the optimization by running SGD with a small learning rate 0:001 with k = 10 repeats (as described in Section 5.1) for 20 epochs to obtain parameters xwarm. We do this to get near a minimum that would be found when using the small learning rate. Then, we start full-batch GD from xwarm with two different learning rate 0:001 (small) and 0:01 (large). We do not apply momentum when performing full-batch GD but apply 0.0005 weight decay. However we did not observe any visible difference in the results without weight decay. Similar to (Li et al., 2018), we obtain the first two principal components of the vectors x1 (cid:0) x0; x2 (cid:0) x0; : : : ; xt (cid:0) x0 and plot the trajectory along these two directions in Figure 7. We can clearly observe that GD with a large learning rate changes path and moves toward a different place in the landscape. GD continues on the different path even when the learning rate is reduced back to 0:001 after 400 steps. Furthermore, looking at the loss values, we can observe a peak at the beginning of training that closely resembles\n\n8\n\n050100150200(Normalized) Epoch7075808590Test Accuracyγ=0.01 - Standardγ=0.001 - Standard - 2000epochγ=0.001 - 10 Repeats050100150200(Normalized) Epoch707580859095100Train Accuracyγ=0.01 - Standardγ=0.001 - Standard - 2000epochγ=0.001 - 10 RepeatsUnder review as a conference paper at ICLR 2023\n\n(a) Trajectory of (full-batch) GD with small and large learning rate.\n\n(b) The value of train loss when using different values of learning rate.\n\nFigure 7: Behavior of GD for learning rates 0:001 (small) and 0:01 (large). The initialization is obtained by warm-starting the network using SGD with a small learning rate 0:001. Using a large learning rate changes the trajectory sharply and even if the learning rate is reduced again after several steps (blue line) we move toward a different direction in the landscape. This is accompanied by a sharp increase of loss at the beginning that can be attributed to GD escaping from a local sharp region in the landscape.\n\nwhat we expect to observe when GD is escaping from a local sharp region. This clearly shows that these behaviors of GD are not merely theoretical and are also relevant in real world applications.\n\nNote that while we do not observe similar high spikes in the loss in the next steps, we conjecture that this behavior of escaping sharp regions is constantly happening throughout training. This is also confirmed by observations in (Cohen et al., 2021) which show sharpness increases throughout training until reaching the threshold 2 (cid:22) where (cid:22) is the learning rate. GD will then oscillate between being sharper and smoother than this threshold. As a result of constantly avoiding sharp regions, symptoms of an escape such as a spike in loss is not observed. While we observe smaller increases in the loss, these can be due to oscillations also observed in (Cohen et al., 2021) along the highest eigenvectors which are not the same as escaping. Results in the same work show that in these cases the parameters do not move in these directions and only oscillate around the same center. Developing better visualization techniques or identifying other effects of using a high learning rate on GD’s trajectory can help explain this behavior further and both of these directions are ground for future work.\n\n6 FUTURE WORK\n\nObtaining further insight on how GD avoids locally sharp regions, for example by developing better methods for visualization of the landscape and trajectory, is grounds for future work. Furthermore, there are various extensions possible on the theoretical results obtained in this paper. For example, it might be possible to show other effects of using a large learning rate on trajectory that facilitate escaping from local minima. Finally, obtaining similar results with a relaxed set of assumptions would also be an interesting direction of research.\n\n7 CONCLUSION\n\nIn this paper, we highlight that a high learning rate can provably lead to avoiding or escaping local minima and reaching a global minimum. Based on this result, we argue that analyzing GD with infinitesimally small learning rate is not sufficient to understand its success unlike what was suggested in prior work (Elkabetz & Cohen, 2021). Furthermore, by designing a method to amplify stochastic noise without increasing the learning rate, we disentangle the effects of stochastic noise and high learning rates. We observe that while a higher stochastic noise leads to a better model, it is not enough to close the gap with the model obtained using a high learning rate. Therefore, we argue that the effect of learning rate goes beyond controlling the impact of stochastic noise even in SGD. In contrast, recent works on analyzing success of SGD focus on continuous settings (Xie et al., 2021) and only take step size into account when modeling the noise (Pesme et al., 2021). We hope that our results will encourage future work on large step size regime. Finally, we demonstrate that the escape from sharp regions can happen in training of neural networks, hence signifying the relevance of the effects of large learning rate in real world applications.\n\n9\n\n1.00.80.60.40.20.00.20.40.6PC11.00.80.60.40.20.0PC2=0.001=0.01 reduced to 0.001020406080100Step1234567Train Losssmall lrlarge lrUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding Gradient Descent on Edge of Stability in Deep Learning, June 2022. URL http://arxiv.org/abs/2205.09745. arXiv:2205.09745 [cs, math].\n\nDavid G. T. Barrett and Benoit Dherin. Implicit gradient regularization. In 9th International Conference on\n\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nPratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nLei Chen and Joan Bruna. On Gradient Descent Convergence beyond the Edge of Stability, June 2022. URL\n\nhttp://arxiv.org/abs/2206.04172. arXiv:2206.04172 [cs, math, stat].\n\nJeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1019–1028, 2017.\n\nGintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the ThirtyThird Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017, 2017.\n\nOmer Elkabetz and Nadav Cohen. Continuous vs. discrete optimization of deep neural networks. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 4947–4960, 2021.\n\nIan J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization problems. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.\n\nOliver Hinder, Aaron Sidford, and Nimit Sharad Sohoni. Near-optimal methods for minimizing star-convex functions and beyond. In Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of Machine Learning Research, pp. 1894–1938, 2020.\n\nArthur Jacot, Clément Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization In Advances in Neural Information Processing Systems 31: Annual Conference on in neural networks. Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 8580–8589, 2018.\n\nStanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos J. Storkey. On the relation between the sharpest directions of DNN loss and the SGD step length. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nStanisaw Jastrzbski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos\n\nStorkey. Three factors influencing minima in SGD. arXiv:1711.04623 [cs, stat], 2018.\n\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nRobert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local minima? In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 2703– 2712, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\n\nJason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent converges to mini-\n\nmizers. arXiv:1602.04915 [cs, math, stat], 2016.\n\nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning\n\nrate phase of deep learning: the catapult mechanism. ArXiv preprint, abs/2003.02218, 2020.\n\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 6391–6401, 2018.\n\nYuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11669–11680, 2019.\n\nJames Lucas, Juhan Bae, Michael R. Zhang, Stanislav Fort, Richard Zemel, and Roger Grosse. Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes. ArXiv preprint, abs/2104.11044, 2021.\n\nCong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and blind deconvolution. Foundations of Computational Mathematics, 20(3):451–632, 2020. ISSN 1615-3375, 1615-3383. doi: 10.1007/s10208-019-09429-9.\n\nI. Necoara, Yu. Nesterov, and F. Glineur. Linear convergence of first order methods for non-strongly convex\n\noptimization. Mathematical Programming, 175(1):69–107, 2019. ISSN 1436-4646.\n\nAntonio Orvieto, Hans Kersting, Frank Proske, Francis Bach, and Aurelien Lucchi. Anticorrelated noise injec-\n\ntion for improved generalization. arXiv:2202.02831 [cs, math, stat], 2022.\n\nScott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 29218–29230, 2021.\n\nNoam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nItay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.\n\nIn Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 4430– 4438, 2018.\n\nItay Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the optimization\n\nlandscape of shallow ReLU neural networks. arXiv:2006.01005 [cs, stat], 2021.\n\nSamuel L. Smith, Benoit Dherin, David G. T. Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nGrzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of neural\n\nnetworks. arXiv:1611.06310 [cs, stat], 2017.\n\nYusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using pac-bayesian analysis. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 9636–9647, 2020.\n\nHarsh Vardhan and Sebastian U. Stich. Tackling benign nonconvexity with smoothing and stochastic gradients.\n\narXiv:2202.09052 [cs, stat], 2022.\n\nXingyu Wang, Sewoong Oh, and Chang-Han Rhee. Eliminating sharp minima from SGD with truncated heavy-\n\ntailed noise. arXiv:2102.04297 [cs, math, stat], 2021a.\n\nYuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large Learning Rate Tames Homogeneity: Conver-\n\ngence and Balancing Effect. ArXiv preprint, abs/2110.03677, 2021b.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLei Wu, Chao Ma, and Weinan E. How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 8289–8298, 2018.\n\nZeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA GRADIENT DESCENT WITH RANDOM INITIALIZATION\n\nAlgorithm 1 Gradient Descent with Random Initialization\n\n1: Pick x0 randomly from the set W . 2: for t = 1 : : : T do 3: 4: end for\n\nxt xt(cid:0)1 (cid:0) (cid:13)∇f (xt(cid:0)1)\n\nB PROOF OF LEMMA 2\n\nLemma. Assume gradient descent is initialized randomly on the set W and is run with learning 2L . Let X 2 Rd be the set of points that should not be encountered by GD and assume rate (cid:13) (cid:20) 1 f is L-smooth over Rd n X. Let L(S) denote the Lebesgue measure of any set S. The probability of encountering any points of X in the first T steps of gradient descent, i.e. xi 2 X for some 1 (cid:20) i (cid:20) T is at most 2(T +1)(cid:1)d (cid:1) L(X) L(W ) .\n\nProof. Define g(x) := x (cid:0) (cid:13)∇f (x). When (cid:13) < 1 L , since f is L-smooth over Dg := Rd n X, results of Lee et al. (2016) show g(x) is a diffeomorphism over Dg. As a result, the function gT obtained by applying g for T times is also a diffeomorphism over the set\n\nDgT := Rd n (X [ g(cid:0)1(X) [ : : : [ (g(T (cid:0)1))(cid:0)1(X)) :\n\nAccording to the change of variable formula for Lebesgue measure (for example, see (Bogachev, 2006, Eq. (3.7.2))), for any measurable set Y (cid:26) DgT\n\n∫\n\nL(gT (Y )) =\n\nj det ∇gT (y)jdy :\n\nY\n\nSince (cid:13) (cid:20) 1\n\n2L , we have for any y 2 Dg,\n\nj det ∇g(y)j = j det(I (cid:0) (cid:13)∇2f (y))j (cid:21) 2(cid:0)d :\n\nThe last equality holds because smoothness ensures all eigenvalues of ∇2f (x) are at most L. So for 2 . Using this result, we also can obtain j det ∇gT (y)j (cid:21) 2(cid:0)T d for any eigenvalue (cid:21)i, 1 (cid:0) (cid:13)(cid:21)i (cid:21) 1 any y 2 DgT . Thus, we have\n\nL(gT (Y )) (cid:21) 2(cid:0)T dL(Y ) ;\n\nwhich means,\n\nL((gT )(cid:0)1(X) \\ DgT ) (cid:20) 2T dL(X) : Note that while the above argument works for T (cid:21) 1, the former inequality also trivially holds for T = 0. Hence\n\nL([T\n\nt=0((gt)(cid:0)1(X) \\ W )) (cid:20) L([T = L([T T∑\n\nt=0(gt)(cid:0)1(X)) t=0((gt)(cid:0)1(X) \\ Dgt))\n\n(cid:20)\n\nL((gt)(cid:0)1(X) \\ Dgt)\n\nt=0 T∑\n\nt=0\n\n(cid:20)\n\n2tdL(X)\n\nT∑\n\n(cid:20) L(X)(\n\n2t)d\n\nt=0\n\n(cid:20) 2(T +1)dL(X) ;\n\nwhere in the last inequality we used 20 + 21 + : : : 2T < 2T +1. The theorem follows directly from this result.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nThe following corollary directly follows from Lemma 2. We use this corollary in proving Lemma 1 to avoid cases where we directly land on a minimum with ∇f (x) = 0. Corollary 1. Let f be L smooth. If X is a set with L(X) = 0, for example when it is a finite set of points, the probability of encountering X throughout training with gradient descent using (cid:13) (cid:20) 1 and random initialization over a set W with L(W ) > 0 is 0.\n\n2L\n\nC PROOF OF THEOREM 2\n\nTheorem. Let f be L-smooth and (cid:22)⋆-OPSC with respect to a minima x⋆ over Rd n X. Define cX := inff∥x (cid:0) x⋆∥ j x 2 Xg and rW := supf∥x (cid:0) x⋆∥ j x 2 W g. The probability of encountering any points of X during running gradient descent with (cid:13) (cid:20) (cid:22)⋆ L2 is upper bounded by 2d (cid:1) rW cX\n\nL(W ) when cX (cid:20) rW and is zero otherwise.\n\nlog2 (1(cid:0)(cid:13)(cid:22)⋆ ) (cid:1) L(X)\n\n(cid:0)\n\nd\n\nProof. Due to (cid:22)⋆-OPSC property of the landscape over Rd n X, as long as xt =2 X, we have\n\n∥xt+1 (cid:0) x⋆∥2\n\n2 = ∥xt (cid:0) (cid:13)∇f (xt) (cid:0) x⋆∥2\n\n2\n\n(cid:0) 2(cid:13) ⟨∇f (xt); xt (cid:0) x⋆⟩ + (cid:13)2∥∇f (xt)∥2\n\n2\n\n2\n\n= ∥xt (cid:0) x⋆∥2 (cid:20) (1 (cid:0) 2(cid:13)(cid:22)⋆ + (cid:13)2L2)∥xt (cid:0) x⋆(cid:3)∥2 (cid:20) (1 (cid:0) (cid:13)(2(cid:22)⋆ (cid:0) (cid:13)L2))∥xt (cid:0) x⋆∥2 (cid:20) (1 (cid:0) (cid:13)(cid:22)⋆)∥xt (cid:0) x⋆∥2 2 ;\n\n2\n\n2\n\nwhere the last inequality holds because (cid:13) (cid:20) (cid:22)⋆\n\nL2 . Hence, if xt =2 X for t 2 [T (cid:0) 1], we have\n\n∥xT (cid:0) x⋆∥2\n\n2\n\n(cid:20) (1 (cid:0) (cid:13)(cid:22)⋆)T ∥xt (cid:0) x⋆∥2 (cid:20) (1 (cid:0) (cid:13)(cid:22)⋆)T rW :\n\n2\n\nLet T0 :=\n\nlog2\n\ncX rW\n\nlog2 (1(cid:0)(cid:13)(cid:22)⋆) . For T > T0, we have\n\n∥xT (cid:0) x⋆∥2\n\n2\n\n(cid:20) (1 (cid:0) (cid:13)(cid:22)⋆)cX < cX ;\n\nwhich means xT =2 X. Therefore, if GD does not reach any point in X in the first T0 steps, it will not reach any point in X afterwards neither. Therefore, the probability of encountering any point in X is the same as the probability of encountering such points in the first T0 steps. According to Lemma 2, this value is bounded as:\n\n2(T0+1)d (cid:1)\n\nL(X) L(W )\n\n= 2d (cid:1) cX rW = 2d (cid:1) rW cX\n\nd\n\nlog2 (1(cid:0)(cid:13)(cid:22)⋆ ) (cid:1)\n\nL(X) L(W )\n\n(cid:0)\n\nd\n\nlog2 (1(cid:0)(cid:13)(cid:22)⋆ ) (cid:1)\n\nL(X) L(W )\n\n:\n\nD PROOF OF LEMMA 1\n\nLemma. Let f be a function that is Lglobal-smooth and consider running GD with learning rate (cid:13) randomly initialized over a set W with L(W ) > 0. Let M be a set with diameter r, containing a local minimum xy and define P (M ) := fx =2 M j ∥x (cid:0) xy∥2 (cid:20) r (cid:0) 3g to be the set surrounding M . Assume f is L < Lglobal-smooth and (cid:22)⋆-OPSC over P (M ) with respect to a (global) (cid:0)3)(1(cid:0)(cid:13)(cid:22)⋆) minimum x⋆ that is sufficiently far from M , formally, ∥x⋆ (cid:0) xy∥2 (cid:21) r (cid:1) 1+ Finally, assume f is (cid:22)y-OPSC with respect to xy over M where (cid:22)y > 2L2 learning rate 2 distance to x⋆ of less than ∥xy (cid:0) x⋆∥ (cid:0) r almost surely.\n\n. Then, using a suitable L2 , if GD reaches a point M , it will escape M and reach a point with\n\n(cid:22)y < (cid:13) (cid:20) (cid:22)⋆\n\n((cid:13)2L2 1(cid:0)\n\n(cid:13)2L2\n\n1(cid:0)(cid:13)(cid:22)⋆\n\nglobal\n\n√\n\np global\n\np\n\n(cid:22)⋆\n\n.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Let t be the smallest step where xt 2 M . Using Corollary 1, xt ̸= xy almost surely. Therefore ∥xt (cid:0) xy∥ > 0. Since (cid:13) > 2\n\n(cid:22)y , we have\n\n∥xt+1 (cid:0) xy∥2\n\n2 = ∥xt (cid:0) (cid:13)∇f (xt) (cid:0) xy∥2\n\n2\n\n⟨\n\n⟩\n\n2\n\n2\n\n= ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 = ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2\n\n∇f (xt); xt (cid:0) xy\n\n+ (cid:13)2∥∇f (xt)∥2\n\n(cid:0) 2(cid:13) (cid:0) 2(cid:13)∥∇f (xt)∥2∥xt (cid:0) xy∥2 + (cid:13)2∥∇f (xt)∥2 2 + (cid:13)∥∇f (xt)∥2((cid:13)∥∇f (xt)∥2 (cid:0) 2∥xt (cid:0) xy∥2) 2 + (cid:13)∥∇f (xt)∥2((cid:13)(cid:22)y∥xt (cid:0) xy∥2 (cid:0) 2∥xt (cid:0) xy∥2) 2 + (cid:13)∥∇f (xt)∥2∥xt (cid:0) xy∥2((cid:13)(cid:22)y (cid:0) 2)\n\n2\n\n2\n\n(A)\n\n(cid:21) ∥xt (cid:0) xy∥2\n\n2 + (cid:13)(cid:22)y∥xt (cid:0) xy∥2\n\n2((cid:13)(cid:22)y (cid:0) 2)\n\n(B)\n\n(cid:21) (2(cid:13)(cid:22)y (cid:0) 3)∥xt (cid:0) xy∥2 2 ;\n\nwhere (A) holds because (cid:13)(cid:22)y (cid:0) 2 > 0 and (B) is obtained by using the lower bound (cid:13)(cid:22)y > 2. Therefore, the distance to xy grows at least with the rate 2(cid:13)(cid:22)y (cid:0) 3 > 1. Hence, GD is guaranteed to reach a point xt+k outside M for some k > 0. If ∥xt+k (cid:0) x⋆∥ (cid:20) ∥xy (cid:0) x⋆∥ (cid:0) r, we are done. Otherwise, we verify that this condition holds for xt+k+1.\n\nFirst note that\n\n∥xt+k (cid:0) xy∥2\n\n2 = ∥xt+k(cid:0)1 (cid:0) (cid:13)∇f (xt+k(cid:0)1) (cid:0) xy∥2\n\n2\n\n2\n\n(cid:0) 2(cid:13)\n\n⟨ ∇f (xt+k(cid:0)1); xt+k(cid:0)1 (cid:0) xy global)\n\n2(1 (cid:0) 2(cid:13)(cid:22)y + (cid:13)2L2\n\n⟩\n\n+ (cid:13)2∥∇f (xt+k(cid:0)1)∥2\n\n2\n\n= ∥xt+k(cid:0)1 (cid:0) xy∥2 (cid:20) ∥xt+k(cid:0)1 (cid:0) xy∥2 (cid:20) r2(1 (cid:0) 2(cid:13)(cid:22)y + (cid:13)2L2 (cid:20) r2((cid:13)2L2\n\n(cid:0) 3) ;\n\nglobal\n\nglobal)\n\nwhere the last inequality holds because (cid:13) > 2\n\n(cid:22)y .\n\n∥xt+k+1 (cid:0) x⋆∥2\n\n2 = ∥xt+k (cid:0) (cid:13)∇f (xt+k) (cid:0) x⋆∥2\n\n2\n\n(cid:0) 2(cid:13) ⟨∇f (xt+k); xt+k (cid:0) x⋆⟩ + (cid:13)2∥∇f (xt+k)∥2\n\n2\n\n2\n\n= ∥xt+k (cid:0) x⋆∥2 (cid:20) ∥xt+k (cid:0) x⋆∥2 (cid:20) ∥xt+k (cid:0) x⋆∥2\n\n2(1 (cid:0) 2(cid:13)(cid:22)⋆ + (cid:13)2L2) 2(1 (cid:0) (cid:13)(cid:22)⋆) ; L2 . We can now write\n\nwhere in the last inequality we used (cid:13) (cid:20) (cid:22)⋆\n\n∥xt+k+1 (cid:0) x⋆∥2 (cid:20) (∥xy (cid:0) x⋆∥2 + ∥xt+k (cid:0) xy∥2)\n\n(\n\n√\n\n√\n\n1 (cid:0) (cid:13)(cid:22)⋆ ) √\n\n(cid:20)\n\n∥xy (cid:0) x⋆∥2 + r\n\n(cid:13)2L2\n\nglobal\n\n(cid:0) 3\n\n1 (cid:0) (cid:13)(cid:22)⋆ :\n\nGiven the lower bound on distance ∥xy (cid:0) x⋆∥, we have\n\n√\n\nr(\n\n((cid:13)2L2\n\nglobal\n\n(cid:0) 3)(1 (cid:0) (cid:13)(cid:22)⋆) + 1) (cid:20) ∥xy (cid:0) x⋆∥2(1 (cid:0)\n\n√\n\n1 (cid:0) (cid:13)(cid:22)⋆) :\n\nThis yields\n\ncompleting the proof.\n\n∥xt+k+1 (cid:0) x⋆∥2 (cid:20) ∥xy (cid:0) x⋆∥2 (cid:0) r ;\n\nE PROOF OF THEOREM 1\n\nConsider any function f that is L-smooth and (cid:22)⋆-OPSC with respect to some minimum x⋆ in its landscape except on a region M containing a local minimum xy satisfying the conditions in Lemma 1. GD initialized randomly inside M converges to xy with a small learning rate (cid:13) < (cid:22)y but will instead converge to x⋆ with a large learning rate 2\n\nL2 almost surely.\n\n(cid:22)y < (cid:13) (cid:20) (cid:22)⋆\n\nL2\n\nglobal\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nProof. When GD is initialized inside M and the learning rate is small satisfying (cid:13) < (cid:22)y we have (cid:22)y-OPSC and Lglobal-smoothness inside M , the iterates will satisfy\n\nL2\n\nglobal\n\n, since\n\n∥xt+1 (cid:0) xy∥2\n\n2 = ∥xt (cid:0) (cid:13)∇f (xt) (cid:0) xy∥2\n\n2\n\n⟨\n\n2\n\n(cid:0) 2(cid:13)\n\n= ∥xt (cid:0) xy∥2 (cid:20) (1 (cid:0) 2(cid:13)(cid:22)y + (cid:13)2L2 (cid:20) (1 (cid:0) (cid:13)(2(cid:22)y (cid:0) (cid:13)L2 (cid:20) (1 (cid:0) (cid:13)(cid:22)y)∥xt (cid:0) xy∥2 2 ;\n\n∇f (xt); xt (cid:0) xy global)∥xt (cid:0) xy∥2 global))∥xt (cid:0) xy∥2\n\n2\n\n2\n\n⟩\n\n+ (cid:13)2∥∇f (xt)∥2\n\n2\n\nTherefore, GD will converge to xy. Let us now consider the case when GD is instead applied using a large learning rate satisfying 2 L2 . Furthermore, we allow initialization over any arbitrary set (instead of only subsets of M ) as long as they satisfy L(W ) > 0. In this case, for each iterate, if xt =2 M , similar to above we have\n\n(cid:22)y < (cid:13) (cid:20) (cid:22)⋆\n\n∥xt+1 (cid:0) x⋆∥2\n\n2\n\n(cid:20) (1 (cid:0) (cid:13)(cid:22)⋆)∥xt (cid:0) xstar∥2 2 :\n\nIf xt 2 M , Lemma 1 shows that there exists k > 0 such that xt+k =2 M and ∥xt+k (cid:0) x⋆∥2 2 is less than the distance of any point in M to x⋆. Since xt+k =2 M the above argument holds and the distance to x⋆ decreases. Therefore, for any t′ > t + k this distance ∥xt′ (cid:0) x⋆∥2 2 remains less than the distance of any point in M to x⋆. This guarantees that xt′ =2 M . Hence the distance to x⋆ keeps decreasing which means GD will converge to x⋆.\n\nF EFFECT OF LEARNING RATE ON STOCHASTICITY\n\nLet us focus on the case where f (x) is the finite-sum 1 rate k(cid:13), the iterates would satisfy\n\nN\n\n∑\n\nN\n\ni=1 fi(x). Then, using a large learning\n\nxt+1 (cid:0) xt (cid:13)\n\n= (cid:0)k∇frt(xt) = (cid:0)k∇f (xt) (cid:0) k(∇frt(xt) (cid:0) ∇f (xt)) :\n\n(4)\n\nLet us assume that the deviation direction of each data point from the true gradient changes very the functions fi (cid:0) f are extremely smooth. Then, using a smaller learning rate we slowly, i.e. instead have\n\nxt+k (cid:0) xt (cid:13)\n\n= (cid:0)\n\n= (cid:0)\n\n(cid:25) (cid:0)\n\nk(cid:0)1∑\n\ni=0 k(cid:0)1∑\n\ni=0 k(cid:0)1∑\n\ni=0\n\n∇frt+i(xt+i)\n\n∇f (xt+i) (cid:0)\n\n∇f (xt+i) (cid:0)\n\nk(cid:0)1∑\n\ni=0 k(cid:0)1∑\n\ni=0\n\n(∇frt+i(xt+i) (cid:0) ∇f (xt+i))\n\n(∇frt+i(xt) (cid:0) ∇f (xt)) :\n\nN\n\n∑\n\nN i=1\n\n∥∇fi(xt) (cid:0) ∇f (xt)∥2\n\nTo compare the strength of noise in each case we can for example compare the variance of the right hand side. Let (cid:27)2 := 1 2. Then, the variance when using the large learning rate would be k2(cid:27)2. When using a smaller learning rate and sampling at each step to obtain rt the variance is instead k(cid:27)2 and is therefore reduced. However, using SGD with repeats, i.e. using rt+i = rt for 1 (cid:20) i (cid:20) k(cid:0)1, we recover the same variance as the large learning rate. Therefore, using SGD with repeats, allows maintaining the same level of noise while still using a smaller learning rate.\n\nG PROOF OF PROPOSITION 3\n\nWe first state the following key theorem which describes criteria ensuring escaping from or staying around a minimum:\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 4. Let M be a ball with radius r centered at a minimum xy and assume f is LM -smooth over M and (cid:22)y-OPSC with respect to xy. Consider running SGD with a small learning rate (cid:13) (cid:20)\n\n. Assume that when running SGD such that the oracle noise is bounded as\n\n(cid:22)y 2L2 M\n\nE∥gt (cid:0) ∇f (xt)∥2\n\n(cid:20) (cid:27)2 :\n\nFurthermore assume that for some c (cid:20) 1 we have Pr Assume SGD reaches a point in M . If (cid:27)2 (cid:20) (cid:22)y On the other hand, if c2(cid:27)2 (cid:21) 2LM\n\n(cid:13) r2 it will remain in M with probability at least\n\n(cid:13) r2 + ε for some ε > 0 it will escape M almost surely.\n\n> 0 for all xt 2 M . 2(cid:0)(cid:13)(cid:22)y .\n\n2\n\n2\n\n[ ∥gt (cid:0) ∇f (xt)∥2\n\n2 > c2(cid:27)2\n\n]\n\nProof. Let t denote the parameters at an iteration such that xt 2 M . We have\n\n⟨\n\n⟩\n\nEgt; xt (cid:0) xy ⟨\n∇f (xt); xt (cid:0) xy\n\nE∥xt+1 (cid:0) xy∥2 = ∥xt (cid:0) xy∥2 (cid:0) 2(cid:13) = ∥xt (cid:0) xy∥2 (cid:0) 2(cid:13) (cid:20) ∥xt (cid:0) xy∥2(1 (cid:0) 2(cid:13)(cid:22)y + (cid:13)2L2 (cid:20) r2(1 (cid:0) (cid:13)(cid:22)y + (cid:13)2L2 (cid:20) r2(1 (cid:0) (cid:13)(cid:22)y\n\nM )\n\n) :\n\nM ) + (cid:13)r2(cid:22)y\n\n2\n\n+ (cid:13)2E∥gt∥2\n\n⟩\n\n+ (cid:13)2∥∇f (xt)∥2 + (cid:13)2(cid:27)2\n\nThus, using Markov inequality we have\n\n[ ∥xt+1 (cid:0) xy∥2 > r2\n\n]\n\n(cid:20)\n\nPr\n\nwhich shows the claim. On the other hand, let p := Pr probability at least p > 0 we have\n\n;\n\n1 1 (cid:0) (cid:13)(cid:22)y [\n∥gt (cid:0) ∇f (xt)∥2\n\n2\n\n2 > c2(cid:27)2\n\n]\n\n. Then with\n\n∥xt+1 (cid:0) xy∥2\n\n2 = ∥xt (cid:0) (cid:13)∇f (xt) (cid:0) xy (cid:0) (cid:13)(gt (cid:0) ∇f (xt))∥2 ⟨\n\n⟩\n\n2\n\n2 + (cid:13)2∥gt (cid:0) ∇f (xt)∥2\n\n2\n\n2\n\n2\n\n= ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 = ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2 (cid:21) ∥xt (cid:0) xy∥2\n\n∇f (xt); xt (cid:0) xy\n\n+ (cid:13)2∥∇f (xt)∥2\n\n(cid:0) 2(cid:13) (cid:0) 2(cid:13)∥∇f (xt)∥2∥xt (cid:0) xy∥2 + (cid:13)2∥∇f (xt)∥2\n\n2 + c2(cid:13)2(cid:27)2 2 + (cid:13)∥∇f (xt)∥2((cid:13)∥∇f (xt)∥2 (cid:0) 2∥xt (cid:0) xy∥2) + c2(cid:13)2(cid:27)2 2 + (cid:13)∥∇f (xt)∥2((cid:13)(cid:22)y∥xt (cid:0) xy∥2 (cid:0) 2∥xt (cid:0) xy∥2) + c2(cid:13)2(cid:27)2 2 + (cid:13)∥∇f (xt)∥2∥xt (cid:0) xy∥2((cid:13)(cid:22)y (cid:0) 2) + c2(cid:13)2(cid:27)2 2 + (cid:13)LM ∥xt (cid:0) xy∥2 2 + (cid:13)LM r2((cid:13)(cid:22)y (cid:0) 2) + 2(cid:13)LM r2 + ε 2 + (cid:13)2LM (cid:22)yr2 + ε :\n\n2((cid:13)(cid:22)y (cid:0) 2) + c2(cid:13)2(cid:27)2\n\nThis means the distance to xy grows at least with the constant ε. Let q := r2 ε . Therefore, with probability at least pq, one of xt+1; : : : ; xt+q will be out of M . This holds for any consecutive q iterates. Partitioning the iterates to parts of consecutive iterates of size q, each part has a positive probability of containing a point outside M . Therefore SGD will reach a point outside of M almost surely.\n\nThe function plotted in Figure 5 can be formally defined as follows:\n\nf1(x) := 86400((x (cid:0) (cid:11))3 (cid:0) (2:9 (cid:0) (cid:11))3) + 2:92 f2(x) := (cid:12)((x (cid:0) 3:1)3 + 0:001) + f1(3) f3(x) := (cid:0)300(x (cid:0) 3:1)2 + f2(3:1)\n\nftm(x) :=\n\n8\n\n>>>>>< >>>>>:\n\nx2 f1(x) f2(x) f3(x) 450 (cid:3) ((x (cid:0) 4:1)2 (cid:0) 0:16) + f3(3:7) 3:7 < x\n\nx (cid:20) 2:9 2:9 < x (cid:20) 3 3 < x (cid:20) 3:1 3:1 < x (cid:20) 3:7\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nwith (cid:11) = 2:9 (cid:0)\n\np\n\n2:9\n\n360 and (cid:12) = 8640000(3 (cid:0) (cid:11))2. This function satisfies the following properties:\n\n(cid:15) ftm is 2-smooth over fx j x < 2:9g. (cid:15) ftm is 900-OPSC towards 4:1 and 900-smooth over fx j 3:7 < xg. (cid:15) ftm is 4.5-OPSC towards 4:1 over fx j 3:108 < xg.\n\nWe now proceed to proving Proposition 3, stating it formally here:\n\nProposition 3. Consider running SGD on ftm with stochastic noise (cid:24)t drawn i.i.d. at each step from the uniform distribution U nif orm((cid:0)(cid:27); (cid:27)). If the learning rate is small such that it satisfies (cid:13) < 1 302 the algorithm will not converge to x⋆ for some set of initialization points with positive Lebesgue measure. In contrast, with a large learning rate satisfying 0:4 (cid:20) (cid:13) (cid:20) 0:5 it is possible to choose (cid:27) such that the algorithm will converge to x⋆ almost surely.\n\n900 . Consider the case where the algorithm is initialized inside M1 := fx j Proof. Assume (cid:13) < 1 3:7 < x < 4:5g. Then if (cid:27) satisfies (cid:27)2 (cid:20) 900 (cid:1) 0:42 it will remain in M1 with positive probability according Theorem 4. According to the same theorem, If this bound is not satisfied, then we have (cid:27)2 which means any time SGD reaches a point in M⋆ := fx j (cid:0)2:9 < x < 2:9g, it will 2\nescape from it almost surely within a constant number of steps. This means that the algorithm will never stay close to x⋆ = 0 forever or for an arbitrarily long number of steps.\n\n(cid:21) 2(cid:1)2(cid:1)2:92\n\n(cid:13)\n\n(cid:13)\n\nNow consider the case where the learning rate is large enough. Choose (cid:27) such that 5:1 < (cid:27) < 5:5. Note that if the iterates reach the set fx j (cid:0)2:9 < x < 2:9g they will never exit it since we have\n\njx (cid:0) (cid:13)(2x + (cid:24)t)j = j(1 (cid:0) 2(cid:13))x (cid:0) (cid:13)(cid:24)tj\n\n(cid:20) (1 (cid:0) 2(cid:13))(2:9) + (cid:13)(5:5) (cid:20) (1 (cid:0) 2(cid:13))(2:9) + 2(cid:13)(2:9) (cid:20) 2:9 :\n\nWe will now show that from any other point there is a positive probability of reaching the range [(cid:0)2:9; 2:9]. This fact combined with the almost sure guarantee of not escaping from [(cid:0)2:9; 2:9], proves that the algorithm will converge to this set almost surely. Note that ftm is 4:5-OPSC towards x = 4 over the set fx j 3:108 < xg. Since (cid:13) > 0:45, it can be seen from the proof of Lemma 1 that SGD will continue to get further from x = 4 while it is in this set. Furthermore, given the direction of the gradients it is clear that the iterates would alternate between being less and more than 4. Therefore, at some point, the iterates will exit this set reaching a point xt < 3:108. Note that with a positive probability, the noise will not interfere with this escape since there is at least 0:5 probability that the noise is aligned with the gradient direction.\n\nIf 3:1 < x < 3:108, the gradient value is less than 5. Since (cid:27) > 5:1 there is a positive probability of moving to the region x < 3:1. When 2:9 < x < 3:1, because of the positive probability of alignment between the gradient and the noise, SGD will move to x < 2:9 with positive probability. Finally, given the smoothness of the region x < 2:9, if x < (cid:0)2:9 SGD will converge toward x⋆ = 0, ultimately reaching the region (cid:0)2:9 < x < 2:9 with positive probability. This completes the proof.\n\nH TOY EXAMPLE\n\nIn order to demonstrate the effects discussed in Section 4, we experiment with running GD over a simple function. The landscape of this function is plotted in Figure 8a and its formula is presented in Appendix I. The function has two minima, one near the initialization and one further away. Since the near-init minimum is almost completely flat, i.e. gradient is constant and equal to zero (except for the edges which are extremely sharp lines in order to ensure the function remains continuous), if GD reaches a point in this region, it will remain there. However, as this region is very close to the initialization, Lemma 2 (more particularly Corollary 2) suggests that GD with large enough learning rate, will probably not reach any points in this region. To demonstrate this more clearly, we plot the trajectory of GD from several initialization points in Figure 1. It is worth noting that even with large learning rate it is possible for GD to get stuck in this region while it is possible to avoid this\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Landscape of function\n\n(b) Distribution of convergence to each minima\n\nFigure 8: The function used in the toy experiments which has two local minima, a mostly flat minima near the initialization points and a sharp minima further away. It can be clearly observed how as the learning rate grows the two effects of avoiding parts of the landscape and escaping sharp minimas allow GD to converge to the global minimum.\n\nregion even with a small learning rate. However, as suggested by our theoretical upper bound, the probability of this phenomenon increases with the learning rate.\n\nThe other minimum is much sharper than the rest of the function and therefore we can expect an escaping behavior similar to the one described by Lemma 1. This behavior is demonstrated in Figure 3. Note that unlike the previous case, GD with large learning rate always (except when landing directly at the minimum) escapes the sharp minimum while GD with small learning rate converges.\n\nWe measure rate of convergence of GD for 100 different random initialization to each of these three regions for different learning rates. The results are plotted in Figure 8b. We observe that as the learning increases, the rate of avoiding the near initialization minimum increases. While the learning rate is not still high enough, GD will converge to the sharp minimum while as the learning rate increases further, it is also able to escape the sharp minimum and converge to the global minimum. This behavior is completely compatible with what can be expected based on the results and effects discussed in Section 4.\n\nI FUNCTION FOR TOY EXAMPLE\n\n8\n\n>>>>>< >>>>>:\n\nf (x) :=\n\n(cid:0)1600(x (cid:0) 2:5)5 (cid:0) 2000(x (cid:0) 2:5)4 + 800(x (cid:0) 2:5)3 + 1020(x (cid:0) 2:5)2 1411:2 (cid:2) (1 (cid:0) 104(x (cid:0) 8:4)) 0\n1479:2 (cid:2) (104(x (cid:0) 8:6) + 1) 20x2\n\n2 (cid:20) x (cid:20) 3; 8:4 (cid:20) x (cid:20) 8:40001; 8:40001 (cid:20) x (cid:20) 8:59999; 8:59999 (cid:20) x (cid:20) 8:6; otherwise:\n\nJ\n\n2D TOY EXAMPLE\n\nTo build more intuition and show the effect of large learning rate extends to multi-dimensions, we also provide a toy example on 2D. Figure 9 shows the landscape of our toy example which contains four local minima that are also sharp. Consider GD initialized randomly on the region W := f(x; y) j 3 (cid:20) x; y (cid:20) 4g. Then, using a small learning rate GD will converge to the minimum in the region [1; 2] (cid:2) [1; 2]. However, using a larger learning rate allows escaping that minimum. Increasing the magnitude, GD can also jump over the minimum completely. In these cases, GD will converge towards the global minimum at (0; 0).\n\n19\n\n−20246810x0500100015002000f(x)initialization rangesharp minimumnear-init minimumglobal minima10−410−310−2learning rate (log scale)0.00.20.40.60.81.0ratio of runs convergedglobal minimumsharp minimumnear-init minimumUnder review as a conference paper at ICLR 2023\n\nFigure 9: The landscape of the function f (x; y) := x2 + y2 (cid:0) 200ReLU (jxj (cid:0) 1)ReLU (jyj (cid:0) 1)ReLU (2 (cid:0) jxj)ReLU (2 (cid:0) jyj).\n\nK RESULTS OF STOPPING REPEATS FROM DIFFERENT EPOCHS\n\nIn Section 5.1, we explained that at the end of training we stop using the same batch for k steps and train in the standard way (each batch used just once) for additional 10 epochs. This was done to make sure the model that is used to obtain the accuracy on the test data is not overfitted on one batch which might be more likely to happen at the end of the training. In this section, we also experiment with stopping repeats, i.e. using the same batch for k steps, earlier in the training. The result is plotted in Figure 10. No significant improvement is observed.\n\nFigure 10: Plot of test accuracy when we stop using the same batch several times (doing repeats) at different epochs. It can be clearly observed that the stopping epoch does not affect the final accuracy and the gap with the case of GD with a large learning rate can be clearly observed.\n\nL EXPERIMENTS ON CIFAR100\n\nIn order to make sure our results extend to other scenarios, we repeat the experiments in Section 5.1 on CIFAR100 and observe a similar behavior. The accuracy on the train and test datasets during training are plotted in Figure 11.\n\n20\n\n−3−2−10123−3−2−10123−5051015050100150200405060708090Accuracyγ=0.01 - Standardγ=0.001 - 10 Repeats (Until 201)γ=0.001 - Standard - 2000epochγ=0.001 - 10 Repeats (Until 71)γ=0.001 - 10 Repeats (Until 101)γ=0.001 - 10 Repeats (Until 131)Under review as a conference paper at ICLR 2023\n\nFigure 11: Comparsion between performance of SGD with different learning rates on CIFAR100. Repeating batches is turned off at epoch 200 and 10 additional epochs are performed (green). For the experiment with 2000 epochs (orange), the plot is normalized to 200 epochs. For more explanations refer to Figure 6 and Section 5.1.\n\nM EXPERIMENTS ON SGD WITHOUT MOMENTUM\n\nIn Section 5.1, we designed an experiment to show the effect of large learning rate is important and goes beyond controlling the effect of stochastic noise on the trajectory. Since our goal was to demonstrate the relevance and importance of analyzing these effects for the practical scenarios, we used the standard training settings including momentum and weight decay. For completeness, in this section we also include the results of applying SGD with repeats without momentum and without weight decay. We compare standard SGD with learning rate 0:05. standard SGD with learning rate 0:005, and SGD with k = 10 repeats and learning rate 0:005. Accuracy on test and train datasets throughout training is plotted in Figure 12. The figure also contains the accuracy during training with momentum to allow comparison. As expected, applying SGD without momentum performs worse than SGD with momentum. The gap between small and large learning rate can be observed in this case as well. However, we do not observe an improvement when applying repeats.\n\nFigure 12: Comparsion between performance of SGD without momentum and weight decay and with different learning rates on CIFAR10. Repeating batches is turned off at epoch 200 and 10 additional epochs are performed (green). For the experiment with 2000 epochs (orange), the plot is normalized to 200 epochs. For more explanations refer to Figure 6 and Section 5.1.\n\nN LOSS ON THE LINE BETWEEN LARGE AND SMALL LEARNING RATE\n\nTRAJECTORIES\n\nIn Section 5.2, we observed that GD with a large learning rate shows behavior similar to escaping and follows a different trajectory than GD with the small learning rate. In this section, we plot the loss along the line between the first point in the trajectory of GD with small learning rate (hereafter called the origin) and different points along the trajectory of GD with the large learning rate. Figure 13 shows the loss based on the norm of the distance to the origin. As expected the loss increases along the line between the origin and points at the beginning of the trajectory. This is when GD is showing\n\n21\n\n050100150200(Normalized) Epoch0102030405060Test Accuracyγ=0.01 - Standardγ=0.001 - Standard - 2000epochγ=0.001 - 10 Repeats050100150200(Normalized) Epoch707580859095100Train Accuracyγ=0.01 - Standardγ=0.001 - Standard - 2000epochγ=0.001 - 10 Repeats050100150200(Normalized) Epoch20406080Test Accuracyγ=0.05 - Standardγ=0.005 - Standard - 2000epochγ=0.005 - 10 Repeats050100150200(Normalized) Epoch20406080100Train Accuracyγ=0.05 - Standardγ=0.005 - Standard - 2000epochγ=0.005 - 10 RepeatsUnder review as a conference paper at ICLR 2023\n\nescaping behaviors. However, interestingly, the loss is decreasing along the line between the origin and points encountered later in the trajectory.\n\nFigure 13: The value of loss along the line between the first point in the trajectory of GD with small learning rate and different points in the trajectory of GD with a large learning rate. For more detailed explanation of the settings, refer to Section 5.2. Each line corresponds to the value of loss measured on 30 points along the line between the initialization and the parameters after an step. The step number for each line is written in the box located on the top-right of the plot.\n\nADDITIONAL REFERENCES\n\nV.I. Bogachev. Measure Theory. Springer Berlin Heidelberg, 2006. ISBN 978-3-540-34513-8.\n\n22\n\n0.00.20.40.60.81.01.2Distance Norm123456Loss461030100399",
    "reference": "# Summary Of The Paper\n\nThis paper shows that a large learning rate can provably escape local minima and reach global minima for smooth and one-point-strongly-convex functions, while a small learning rate would not work. The paper further shows that a higher stochastic noise is not enough to close the gap with the model trained with a large learning rate.  Some experiments are conducted to disentangle the effects of stochastic noise and learning rate, to compare small and learning rates when training neural networks.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. The paper is clearly-written and the main message is clear.\n2. The literature review is sufficient.\n\nWeaknesses:\n1. The large learning rate is indeed important in neural network training, but the authors did not analyze the neural network training trajectory of gradient descent theoretically: instead, the authors simply assumed the function is smooth and one-point-strongly-convex, and there are exactly two minima. I am not sure the insight obtained in this paper can be generalized in neural networks since some arguments are made over some contrived functions (e.g., the function plotted in Figure 5).\n\n2. I am wondering what is the real technical contribution of this paper. It seems to be that the proof technique is standard: it uses standard proof roadmaps for smooth functions and combines with the one-point-strong-convexity to obtain the relationship between two consecutive iterates. \n\n3. The experiments are not well-designed. In Section 5.1, to disentangle the effects of learning rate and the stochastic noise, the authors should consider noiseless case instead of noisy case with different scale: the reason is that the noise does not only affect the current solution but also affects the whole trajectory of gradient descent. Why the authors turns off batch normalization? \n\n4. The insight of Section 5.2 is very similar to Figure 1 in [Li et al. NeurIPS 2019]. Although [Li et al. NeurIPS 19] did not plot the principal component but it is clear that small and large learning rate gradient descent have different trajectories (Please refer to Section 4 in [Li et al. NeurIPS 2019]).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Good. The paper is very clear about their findings and contribution.\n\nQuality and Novelty: Low to Medium. The authors only considered an oversimplified case of nonconvex optimization with smooth and one-point-strongly-convex loss functions. The authors did not compare their results with some formal studies on large learning rates in neural networks, such as [Li et al. NeurIPS 2019].\n\nReproducibility: Medium. The code is provided, but there are not multiple runs based on different random seed. In addition, the experiment design has some issues which I mentioned in the Weaknesses section.\n\n# Summary Of The Review\n\nThe paper considered the effects of a large learning rate and showed that under smooth and one-point-strong-convex assumptions large learning rate is better than a small learning rate. However, it is unclear that how relevant these results are because the authors did not analyze the trajectory of a certain neural network but some contrived functions (e.g., Proposition 3). The experimental design has some issues. Overall the insight is not significant compared with previous work, such as [Li et al. NeurIPS 2019], which formally analyzed the effect of the learning rate in a certain neural network.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCALIBRATING THE RIGGED LOTTERY: MAKING ALL TICKETS RELIABLE\n\nBowen Lei Texas A&M University bowenlei@stat.tamu.edu\n\nRuqi Zhang Purdue University ruqiz@purdue.edu\n\nDongkuan Xu North Carolina State University dxu27@ncsu.edu\n\nBani Mallick Texas A&M University bmallick@stat.tamu.edu\n\nABSTRACT\n\nAlthough sparse training has been successfully used in various resource-limited deep learning tasks to save memory, accelerate training, and reduce inference time, the reliability of the produced sparse models remains unexplored. Previous research has shown that deep neural networks tend to be over-confident, and we find that sparse training exacerbates this problem. Therefore, calibrating the sparse models is crucial for reliable prediction and decision-making. In this paper, we propose a new sparse training method to produce sparse models with improved confidence calibration. In contrast to previous research that uses only one mask to control the sparse topology, our method utilizes two masks, including a deterministic mask and a random mask. The former efficiently searches and activates important weights by exploiting the magnitude of weights and gradients. While the latter brings better exploration and finds more appropriate weight values by random updates. Theoretically, we prove our method can be viewed as a hierarchical variational approximation of a probabilistic deep Gaussian process. Extensive experiments on multiple datasets, model architectures, and sparsities show that our method reduces ECE values by up to 47.8% and simultaneously maintains or even improves accuracy with only a slight increase in computation and storage burden.\n\n1\n\nINTRODUCTION\n\nSparse training is gaining increasing attention and has been used in various deep neural network (DNN) learning tasks (Evci et al., 2020; Dietrich et al., 2021; Bibikar et al., 2022). In sparse training, a certain percentage of connections are maintained being removed to save memory, accelerate training, and reduce inference time, enabling DNNs for resource-constrained situations. The sparse topology is usually controlled by a mask, and various sparse training methods have been proposed to find a suitable mask to achieve comparable or even higher accuracy compared to dense training (Evci et al., 2020; Liu et al., 2021; Schwarz et al., 2021). However, in order to deploy the sparse models in real-world applications, a key question remains to be answered: how reliable are these models?\n\nThere has been a line of work on studying the reliability of dense DNNs, which means that DNNs should know what it does not know (Guo et al., 2017; Nixon et al., 2019; Wang et al., 2021). In other words, a model’s confidence (the probability associated with the predicted class label) should reflect its ground truth correctness likelihood. A widely used reliability metric is Expected Calibration Error (ECE) (Guo et al., 2017), which measures the difference between confidence and accuracy, with a lower ECE indicating higher reliability. However, prior research has shown that DNNs tend to be over-confident (Guo et al., 2017; Rahaman et al., 2021; Patel et al., 2022), suggesting DNNs may be too confident to notice incorrect decisions, leading to safety issues in real-world applications, e.g., automated healthcare and self-driving cars (Jiang et al., 2012; Bojarski et al., 2016).\n\nIn this work, we for the first time identify and study the reliability problem of sparse training. We start with the question of how reliable the current sparse training is. We find that the over-confidence\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) Reliability Diag. (0%)\n\n(b) Reliability Diag. (95%)\n\n(c) Test Accuracy and ECE Value\n\nFigure 1: Reliability diagrams for (a) the dense model and (b) the sparse model. The sparse model is more over-confident than the dense model. (c) the scatter plot of test accuracy (%) and ECE value at different sparsities. From the high sparse model to the dense model, the ECE value first decreases, then increases, and then decreases again, showing a double descent pattern.\n\nproblem becomes even more pronounced when sparse training is applied to ResNet-50 on CIFAR100. Figures 1 (a)-(b) show that the gap (blue area) between confidence and accuracy of the sparse model (95% sparsity) is larger than that of the dense model (0% sparsity), implying the sparse model is more over-confident than the dense model. Figure 1 (c) shows the test accuracy (pink curve) and ECE value (blue curve, a measure of reliability) (Guo et al., 2017) at different sparsities. When the accuracy is comparable to dense training (0%-95%), we observe that the ECE values increase with sparsity, implying that the problem of over-confidence becomes more severe at higher sparsity. And when the accuracy decreases sharply (>95%), the ECE value first decreases and then increases again. This leads to a double descent phenomenon (Nakkiran et al., 2021) when we view the ECE value curve from left to right (99.9%-0%) (see Section 6 for more discussion).\n\nTo improve the reliability, we propose a new sparse training method to produce well-calibrated predictions while maintaining a high accuracy performance. We call our method “The Calibrated Rigged Lottery” or CigL. Unlike previous sparse training methods with only one mask, our method employs two masks, including a deterministic mask and a random mask, to better explore the sparse topology and weight space. The deterministic one efficiently searches and activates important weights by exploiting the magnitude of weights/gradients. And the random one, inspired by dropout, adds more exploration and leads to better convergence. When near the end of training, we collect weights & masks at each epoch, and use the designed weight & mask averaging procedure to obtain one sparse model. From theoretical analysis, we show our method can be viewed as a hierarchical variational approximation (Ranganath et al., 2016) to a probabilistic deep Gaussian process (Gal & Ghahramani, 2016), which leads to a large family of variational distributions and better Bayesian posterior approximations. Our contributions are summarized as follows:\n\n• We for the first time identify and study the reliability problem of sparse training and find\n\nthat sparse training exacerbates the over-confidence problem of DNNs.\n\n• We then propose CigL, a new sparse training method that improves confidence calibration\n\nwith comparable and even higher accuracy.\n\n• We prove that CigL can be viewed as a hierarchical variational approximation to a probabilistic deep Gaussian process which improves the calibration by better characterizing the posterior.\n\n• We perform extensive experiments on multiple benchmark datasets, model architectures, and sparsities. CigL reduces ECE values by up to 47.8% and simultaneously maintain or even improve accuracy with only a slight increase in computational and storage burden.\n\n2 RELATED WORK\n\n2.1 SPARSE TRAINING\n\nAs the scale of models continues to grow, there is an increasing attention to the sparse training which maintains sparse weights throughout the training process. Different sparse training methods have been investigated, and various pruning and growth criteria, such as weight/gradient magnitude,\n\n2\n\n0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0AccuracyGapOutputs0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0AccuracyGapGap increase due to sparsityOutputsPublished as a conference paper at ICLR 2023\n\nare designed (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; ̈Ozdenizci & Legenstein, 2021; Zhou et al., 2021; Schwarz et al., 2021; Yin et al., 2022). However, sparse training is more challenging in the weight space exploration because sparse constraints cut off update routes and produce spurious local minima (Evci et al., 2019; Sun & Li, 2021; He et al., 2022). There are some studies that have started to promote exploration, while they primarily pursue high accuracy and might add additional costs (Liu et al., 2021; Huang et al., 2022). Most sparse training methods use only one mask to determine the sparse topology, which is insufficient for achieving adequate exploration, and existing multi-mask methods are not designed for improved exploration (Xia et al., 2022; Bibikar et al., 2022) (more details in Section D.4).\n\n2.2 CONFIDENCE CALIBRATION IN DNNS\n\nMany studies have investigated whether the confidences of DNNs are well-calibrated (Guo et al., 2017; Nixon et al., 2019; Zhang et al., 2020), and existing research has found DNNs tend to be over-confident (Guo et al., 2017; Rahaman et al., 2021; Patel et al., 2022), which may mislead our choices and cause unreliable decisions in real-world applications. To improve confidence calibration, a widely-used method is temperature scaling (Guo et al., 2017), which adds a scaling parameter to the softmax formulation and adjusts it on a validation set. Some other works incorporate regularization in the training, such as Mixup (Zhang et al., 2017) and label smoothing (Szegedy et al., 2016). In addition, Bayesian methods also show the ability to improve calibration, such as Monte Carlo Dropout (Gal & Ghahramani, 2016) and Bayesian deep ensembles (Ashukha et al., 2020). However, they mainly focus on dense training. Studies have been conducted on reliability of sparse DNNs (more details in Section D.5), but they target on pruning, starting with a dense model to gradually increase sparsity, which reduces the exploration challenge (Venkatesh et al., 2020; Chen et al., 2022). They still find that uncertainty measures are more sensitive to pruning than generalization metrics, indicating the sensitivity of reliability to sparsity. Yin et al. (2022) studies sparse training, but it aims to boost the performance and brings limited improvement in reliability. Therefore, how to obtain a well-calibrated DNN in sparse training is more challenging and remains unknown.\n\n3 METHOD\n\nWe propose a new sparse training method, CigL, to improve the confidence calibration of the produced sparse models, which simultaneously maintains comparable or even higher accuracy. Specifically, CigL starts with a random sparse network and uses two masks to control the sparse topology and explore the weight space, including a deterministic mask and a random mask. The former is updated periodically to determine the non-zero weights, while the latter is sampled randomly in each iteration to bring better exploration in the model update. Then, with the designed weight & mask averaging, we combine information about different aspects of the weight space to obtain a single output sparse model. Our CigL method is outlined in Algorithm 1.\n\n3.1 DETERMINISTIC MASK & RANDOM MASK\n\nIn our CigL, we propose to utilize two masks, a deterministic mask M and a random mask Z, to search for a sparse model with improved confidence calibration and SOTA accuracy. We will first describe the two masks in detail and discuss how to set their sparsity.\n\nThe deterministic mask controls the entire sparse topology with the aim of finding a wellperforming sparse model. That is, the mask determines which weights should be activated and which should not. Inspired by the widely-used sparse training method RigL (Evci et al., 2020), we believe a larger weight/gradient magnitude implies that the weight is more helpful for loss reduction and needs to be activated. Thus, CigL removes a portion of the weights with small magnitudes, and activates new weights with large gradient magnitudes at fixed time intervals ∆T .\n\nThe random mask allows the model to better explore the weight space under sparsity constraints. In each iteration prior to backpropagation, the mask is randomly drawn from Bernoulli distribution. In this way, the mask randomly selects a portion of the non-zero weights to be temporarily deactivated and forces the model to explore more in other directions of the weight space, which adds more randomness in the weight update step and leads to a better exploration of the weight space compared\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nto one mask strategy. As a result, the model is more likely to jump out of spurious local minima while avoiding deviations from the sparse topology found by the deterministic mask.\n\nThe sparsity setting of the two masks is illustrated as below. On the one hand, the deterministic mask is responsible for the overall sparsity of the output sparse model. Suppose we want to train a network with 95% sparsity, the deterministic mask will also have the same sparsity, with 5% of the elements being 1. On the other hand, the random mask deactivates some nonzero weights during the training process, producing temporary models with increasing sparsity. Since highly sparse models (like 95% sparsity) are sensitive to further increases in sparsity, we set a low sparsity, such as 10%, for the random mask so that no significant increases in sparsity and no dramatic degradation in performance occurs in these temporary models.\n\n3.2 WEIGHT & MASK AVERAGING\n\nAlgorithm 1 CigL\n\nInput: initialize W (0), M , and WCigL = None, set epoch length m, update interval ∆T , number of iterations T , start iteration for weight & mask averaging T ∗, random mask rate p, and learning rate αt for t = 1 to T do\n\nSample a mini-batch data Bt with size n if t mod ∆T = 0 then\n\nUpdate mask M using weights and gradients Prune and regrow weights W (t) based on M\n\nend if Sample mask Z (t) and Z (t) Update sparse weights: W (t) = W (t−1) − αtM ⊙ Z (t) ⊙ ∇L(M ⊙ Z (t) ⊙ W (t−1); Bt) if t mod m = 0 and t > T ∗ then\n\nij = Bernoulli(p)\n\nif WCigL = None then\n\nWCigL = M ⊙ Z (t) ⊙ W (t) nmodels = 1\n\nelse\n\nWCigL = WCigL·nmodels+M ⊙Z(t)⊙W (t) nmodels = nmodels + 1\n\nnmodels+1\n\nend if\n\nend if\n\nend for Output: Sparse Model Weights WCigL\n\nWith the two masks designed above, we propose a weight & mask averaging procedure to obtain one single sparse model with improved confidence calibration and comparable or even higher accuracy. We formalize this procedure as follows. We first iteratively update the two masks and model weights. Consistent with widely used sparse training methods (Evci et al., 2020; Liu et al., 2021), the deterministic mask stops updating near the end of the training process. While we still continuously draw different random masks from the Bernoulli distribution and collect a pair of sparse weights and random masks {Z(t), W (t)} at each epoch after T ∗-th epoch. Then, we can produce multiple temporary sparse models Z(t) ⊙ W (t) with different weight values and different sparse topologies, which contain more knowledge about the weight space than the single-mask training methods. Finally, inspired by a popular way of combining models (Izmailov et al., 2018; Wortsman et al., 2022), we obtain the single output sparse model by averaging the weights of these temporary sparse models, which can be viewed as a mask-based weighted averaging.\n\n4 MAKING ALL TICKETS RELIABLE\n\n4.1 CIGL WITH BETTER CONFIDENCE CALIBRATION\n\nObtaining reliable DNNs is more challenging in sparse training, and we will show why our CigL provides a solution to this problem. Bayesian methods have shown the ability to improve confidence calibration (Gal & Ghahramani, 2016; Ashukha et al., 2020), but they become more difficult to fit the posterior well under sparse constraints, limiting their ability to solve unreliable problems. We find that CigL can be viewed as a hierarchical Bayesian method (shown in Section 4.2), which improves confidence calibration by performing better posterior approximations in two ways discussed below.\n\nOn the one hand, the model is more challenging to fully explore the weight space due to sparsity constraint, and inappropriate weight values can also negatively affect the mask search. During sparse training, a large percentage of connections are removed, cutting off the update routes and thus narrowing the family of Bayesian proposal distributions. This leads to more difficult optimization and sampling. To overcome this issue, our CigL adds a hierarchical structure to the variational distributions so that we have a larger family of distributions, allowing it to capture more complex marginal distributions and reducing the difficulty of fitting the posterior.\n\nOn the other hand, when sparsity constraints are added, the posterior landscape changes, leading to more complex posterior distributions. One example is the stronger correlation between hidden\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nvariables, such as the random mask Z and weight W (shown in the Appendix C.1). In a dense model, the accuracy does not change much if we randomly draw Z and use Z ⊙ W compared to using W . However, in high sparsity like 95%, we see a significant accuracy drop when Z ⊙ W is used compared to using W . Thus, in CigL, the pairings of Z and W are collected to capture the correlation, leading to a better posterior approximation.\n\n4.2 CIGL AS A HIERARCHICAL BAYESIAN APPROXIMATION\n\nWe prove that training sparse neural networks with our CigL are mathematically equivalent to approximating the probabilistic deep GP (Damianou & Lawrence, 2013; Gal & Ghahramani, 2016) with hierarchical variational inference. We show that the objective of CigL is actually to minimize the Kullback–Leibler (KL) divergence between a hierarchical variational distribution and the posterior of a deep GP. During our study, we do not restrict the architecture, allowing the results applicable to a wide range of applications. Detailed derivation is shown in Appendix B.\n\nWe first present the minimisation objective function of CigL for a sparse neural network (NN) model with L layers and loss function E. The sparse weights and bias of the l-th layer are denoted by Wl ∈ RKi×Ki−1 and bl ∈ RKi (l = 1, · · · , L), and the output prediction is denoted by (cid:98)yi. Given data {xi, yi}, we train the NN model by iteratively update the deterministic mask and the sparse weights. Since the random mask is drawn from a Bernoulli distribution, it has no parameters that need to be updated. For deterministic mask updates, we prune weights with the smaller weight magnitude and regrow weights with larger gradient magnitude. For the weight update, we minimise Eq. (1) which is composed of the difference between (cid:98)yi and the true label yi and a L2 regularisation.\n\nLCigL :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE(yi, (cid:98)yi) + λ\n\nL (cid:88)\n\nl=1\n\n(||Wl||2\n\n2 + ||bl||2\n\n2).\n\n(1)\n\nThen, we derive the minimization objective function of Deep GP which is a flexible probabilistic NN model that can model the distribution of functions (Gal & Ghahramani, 2016). Taking regression as an example, we assume that Wl is a random matrix and w = {Wl}L l=1, and denote the prior by p(w). Then, the predictive distribution of the deep GP can be expressed as Eq. (2) where τ > 0\n\np(y|x, X, Y ) =\n\n(cid:90)\n\np(y|x, w)p(w|X, Y )dw,\n\n(2)\n\np(y|x, w) = N (y; (cid:98)y, τ −1I), (cid:98)y =\n\n(cid:114) 1 KL\n\n(cid:18)\n\nWLσ\n\n· · ·\n\n(cid:114) 1 K1\n\nW2σ(cid:0)W1x + u1\n\n(cid:1)\n\n(cid:19)\n\n.\n\nThe posterior distribution p(w|X, Y ) is intractable, and one way of training the deep GP is variational inference where a family of tractable distributions q(w) is chosen to approximate the posterior. Specifically, we define the hierarchy of q(w) as Eq. (3):\n\nq(Wlij|Zlij, Ulij, Ml) ∼ Zlij · N (MlijUlij, σ2) + (1 − Zlij) · N (0, σ2),\n\nq(Ml|Ul) ∝ exp(Ml ⊙ (|Ul| + |∇Ul|)), Ulij ∼ N (Vlij, σ2), Zlij ∼ Bernoulli(pl),\n\n(3)\n\nwhere l, i and j denote the layer, row, and column index, Ml is a matrix with 0’s and constrained 1’s, Wl is the sparse weights, Ul is the variational parameters, and Vl is the variational hyper parameters.\n\nThen, we iteratively update Ml and Wl to approximate the posterior. For the update of Ml, we obtain a point estimate by maximising q(Ml|Ul) under the sparsity constraint. In pruning step, since the gradient magnitudes |∇Ul| can be relatively small compared to the weight magnitudes |Ul| after training, we can use exp(Ml ⊙ |Ul|) to approximate the distribution. In regrowth step, since the inactive weights are zero, we directly compare the gradient magnitudes exp(Ml ⊙ |∇Ul|). Thus, the update of Ml is aligned with the update in CigL.\n\nFor Wl, we minimise the KL divergence between q(w) and the posterior of deep GP as Eq. (4)\n\n(cid:90)\n\n−\n\nq(w) log p(Y |X, w)dw + DKL(q(w)∥p(w)).\n\n(4)\n\n(cid:82) q(w) log p(yn|xn, w). Then, we can For the first term in Eq. (4), we can first rewrite it as − (cid:80)N approximate each integration in the sum with a single estimate (cid:98)w. For the second term in Eq. (4),\n\nn=1\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwe can approximate it as (cid:80)L\n\nl=1( pl\n\n2 ∥Ul∥2\n\n2 + 1\n\n2 ∥ul∥2\n\n2). As a result, we can derive the objective as\n\nLGP :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\n− log p(yn|xn, (cid:98)w) τ\n\n+\n\nL (cid:88)\n\nl=1\n\n(\n\npl 2\n\n∥Ul∥2\n\n2 +\n\n1 2\n\n∥ul∥2\n\n2),\n\n(5)\n\nwhich is shown to have the same form as the objective in Eq. (1) with appropriate hyperparameters for the deep GP. Thus, the update of Wl is also consistent with the update in CigL. This suggests that our CigL can be viewed as an approximation to the deep GP using hierarchical variational inference. The final weight & mask averaging procedure can be incorporated into the Bayesian paradigm as an approximation to the posterior distribution (Srivastava et al., 2014; Maddox et al., 2019).\n\n4.3 CONNECTION TO DROPOUT\n\nOur CigL can be seen as a new version of Dropout, and our random mask Z is related to the Dropout mask. Dropout is a widely used method to overcome the overfitting problem (Hinton et al., 2012; Wan et al., 2013; Srivastava et al., 2014). Two widely used types are unit dropout and weight dropout, which randomly discard units (neurons) and individual weights at each training step, respectively. Both methods use dropouts only in the training phase and remove them in the testing phase, which is equivalent to discarding Z and only using W for prediction. However, simply dropping Z can be detrimental to the fit of the posterior. Thus, MC dropout collects multiple models by randomly selecting multiple dropout masks, which is equivalent to extracting multiple Z and using one W for prediction. However, only using one W neither fully expresses the posterior landscape nor captures the correlation between Z and W . In contrast, our CigL uses multiple pairings of Z and W , which can better approximate the posterior under sparsity constraints.\n\n4.4 CONNECTION TO WEIGHT AVERAGING\n\nOur weight & mask averaging can be seen as an extension of weight averaging (WA), which averages the weights of multiple model samples to produce a single output model (Izmailov et al., 2018; Wortsman et al., 2022). Compared to deep ensembles (Ashukha et al., 2020), WA outputs only one model, which reduces the forward FLOPs and speeds up prediction. When these model samples are located in one low error basin, it usually leads to wider optima and better generalization. However, although WA can produce better generalization, it does not improve the confidence calibration (Wortsman et al., 2022). In contrast to WA, our weight & mask averaging uses masks for weighted averaging and improves the confidence calibration with similar FLOPs in the prediction.\n\n5 EXPERIMENTS\n\nWe perform a comprehensive empirical evaluation of CigL, comparing it with the popular baseline method RigL (Evci et al., 2020). RigL is a popular sparse training method that uses weights magnitudes to prune and gradient magnitudes to grow connections.\n\nDatasets & Model Architectures: We follow the settings in Evci et al. (2020) for a comprehensive comparison. Our experiments are based on three benchmark datasets: CIFAR-10 and CIFAR100 (Krizhevsky et al., 2009) and ImageNet-2012 (Russakovsky et al., 2015). For model architectures, we used ResNet-50 (He et al., 2016) and Wide-ResNet-22-2 (Zagoruyko & Komodakis, 2016). We repeat all experiments 3 times and report the mean and standard deviation.\n\nSparse Training Settings: For sparse training, we check multiple sparsities, including 80%, 90%, 95%, and 99%, which can sufficiently reduce the memory requirement and is of more interest.\n\nImplementations: We follow the settings in (Evci et al., 2020; Sundar & Dwaraknath, 2021). The parameters are optimized by SGD with momentum. For the learning rate, we use piecewise constant decay scheduler. For CIFAR-10 and CIFAR-100, we train all the models for 250 epochs with a batch size of 128. For ImageNet, we train all the models for 100 epochs with a batch size of 64.\n\n5.1 COMPARISON BETWEEN POPULAR SPARSE TRAINING METHOD\n\nResults on CIFAR-10 and CIFAR-100. We first compare our CigL and RigL by the expected calibration error (ECE) (Guo et al., 2017), a popular measure of the discrepancy between a model’s\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n(a) CIFAR-10, ResNet-50\n\n(b) CIFAR-10, Wide-ResNet-22-2\n\n(c) CIFAR-100, ResNet-50\n\nFigure 2: ECE value comparison between CigL and RigL at different sparsities (80%, 90%, 95%, 99%). Compared to RigL, CigL produces sparse models with smaller ECE values.\n\nTable 1: Testing accuracy (%) comparison between CigL and RigL at different sparsities (80%, 90%, 95%, 99%). Compared to RigL, CigL maintains comparable or higher test accuracy.\n\nCIFAR-10\n\nCIFAR-100\n\nRIGL\n\nCIGL\n\nRIGL\n\nCIGL\n\n80% SPARSITY 90% SPARSITY 95% SPARSITY 99% SPARSITY\n\n94.02 (0.115) 93.84 (0.184) 93.19 (0.198) 91.31 (0.205)\n\n94.75 (0.107) 94.56 (0.189) 94.20 (0.202) 92.42 (0.196)\n\n72.08 (0.109) 71.90 (0.172) 70.90 (0.210) 65.57 (0.208)\n\n76.84 (0.089) 76.24 (0.181) 74.71 (0.197) 66.42 (0.206)\n\nconfidence and true accuracy, with a lower ECE indicating better confidence calibration and higher reliability. In Figure 2, the pink and blue curves represent CigL and RigL, respectively, where the colored ares represent the 95% confidence intervals. We can see that the pink curves are usually lower than the blue curves for different sparsities (80%, 90%, 95%, 99%), which implies that our CigL can reduce the ECE and improve the confidence calibration of the produced sparse models.\n\nApart from ECE value, we also compare our CigL and RigL by the testing accuracy for multiple sparsities (80%, 90%, 95%, 99%). We summarize the results for sparse ResNet-50 in Table 1. It is observed that CigL tends to bring comparable or higher accuracy, which demonstrates that CigL can simultaneously maintain or improve the accuracy.\n\nResults on ImageNet-2012. We also compare the ECE values and test accuracy of our CigL and RigL on a larger dataset, ImageNet-2012, where the sparsity of ResNet-50 is 80% and 90%. As shown in Figure 3, the pink and blue bars represent our CigL and RigL, respectively. For the comparison of ECE values in (a), the pink bars are shorter than the blue bars, indicating an improved reliability of the sparse model produced by CigL. For the test accuracy comparison in (b), the pink and blue bars are very similar in height, implying that the accuracy of CigL is comparable to that of RigL.\n\n(a) ECE value (RM)\n\n(b) Test accuracy (RM)\n\nFigure 3: ECE value and test accuracy(%) of CigL and RigL at 80% & 90% sparsities on ImageNet2012. Compared with RigL, CigL has smaller ECE values and comparable test accuracies.\n\n5.2 COMPARISON BETWEEN DIFFERENT DROPOUT METHODS\n\nIn this section, since our CigL is related to dropout methods, we compare our CigL with RigL using existing popular dropout methods, namely weight dropout (W-DP) and MC dropout (MC-DP). The comparison of test accuracy is shown in Table 2. Our CigL usually provides a comparable or higher accuracy compared to RigL. However, using weight dropout and MC dropout in RigL usually result in a decrease in accuracy. We also summarize the comparison of the ECE value between CigL and different dropout methods in Table 3. For each sparsity and architecture, we have marked in bold those cases where the ECE value is significantly reduced (≥ 15% reduction compared to RigL). Our CigL are always bolded, indicating its ability to reduce ECE value and increase reliability in sparse training. But RigL + weight dropout does not significantly reduce ECE values in almost all cases and RigL + MC dropout also does not improve the calibration in highly sparse cases (99% sparsity).\n\n7\n\n80%90%95%99%Sparsity0.0280.0350.0420.0490.056ECE valueCigLRigL80%90%95%99%Sparsity0.0100.0150.0200.0250.0300.035ECE valueCigLRigL80%90%95%99%Sparsity0.0600.0750.0900.1050.120ECE valueCigLRigL80%90%0.0200.0220.0240.0260.0280.0300.0320.034ECE ValueRigLCigL80%90%010203040506070Testing Accuracy (%)RigLCigLPublished as a conference paper at ICLR 2023\n\nTable 2: Testing accuracy (%) comparison between CigL, RigL + weight dropout (W-DP), and RigL + MC dropout (MC-DP) at different sparsities (80%, 90%, 95%, 99%). Compared to RigL, RigL + W-DP, and RigL + MC-DP, CigL maintains comparable or higher test accuracy.\n\n80% SPARSITY\n\n90% SPARSITY\n\n95% SPARSITY\n\n99% SPARSITY\n\n0 5\n- T\nE N\nS E\nR\n\n2 -\n2 2\n-\n\nN R\nW\n\nRIGL RIGL + W-DP RIGL + MC-DP CIGL\n\nRIGL RIGL + W-DP RIGL + MC-DP CIGL\n\n94.02 (0.115) 93.26 (0.114) 93.39 (0.105) 94.75 (0.107)\n\n93.12 (0.188) 91.77 (0.182) 91.75 (0.149) 93.95 (0.088)\n\n93.84 (0.184) 93.47 (0.186) 93.71 (0.181) 94.56 (0.189)\n\n92.26 (0.187) 91.44 (0.191) 91.49 (0.187) 93.05 (0.219)\n\n93.19 (0.198) 92.71 (0.193) 92.87 (0.205) 94.20 (0.202)\n\n91.02 (0.179) 89.66 (0.183) 89.39 (0.177) 91.34 (0.171)\n\n91.31 (0.205) 89.99 (0.210) 89.84 (0.212) 92.42 (0.196)\n\n83.82 (0.224) 80.42 (0.215) 77.48 (0.198) 83.96 (0.189)\n\nTable 3: Testing ECE comparison between CigL, RigL + weight dropout (W-DP), and RigL + MC dropout (MC-DP) at different sparsities (80%, 90%, 95%, 99%). Compared to RigL + W-DP and RigL + MC-DP, CigL more consistently achieves a significant reduction in the ECE value of RigL.\n\n80% SPARSITY\n\n90% SPARSITY\n\n95% SPARSITY\n\n99% SPARSITY\n\n0 5\n- T\nE N\nS E\nR\n\n2 -\n2 2\n-\n\nN R\nW\n\nRIGL RIGL + W-DP RIGL + MC-DP CIGL\n\nRIGLT RIGL + W-DP RIGL + MC-DP CIGL\n\n0.0423 (0.001) 0.0504 (0.002) 0.0322 (0.001) 0.0356 (0.001)\n\n0.0319 (0.003) 0.0433 (0.003) 0.0159 (0.001) 0.0178 (0.001)\n\n0.0441 (0.001) 0.0438 (0.001) 0.0200 (0.001) 0.0361 (0.001)\n\n0.0272 (0.001) 0.0348 (0.002) 0.0077 (0.002) 0.0159 (0.001)\n\n0.0504 (0.001) 0.0462 (0.002) 0.0121 (0.001) 0.0385 (0.001)\n\n0.0235 (0.001) 0.0256 (0.002) 0.0384 (0.001) 0.0131 (0.001)\n\n0.0571 (0.001) 0.0315 (0.002) 0.0528 (0.002) 0.0298 (0.001)\n\n0.0150 (0.002) 0.0174 (0.003) 0.1502 (0.002) 0.0101 (0.002)\n\n5.3 COMPARISON BETWEEN OTHER CALIBRATION METHODS\n\nIn this section, we compare our CigL with existing popular calibration methods, including mixup (Zhang et al., 2017), temperature scaling (TS) (Guo et al., 2017), and label smoothing (LS) (Szegedy et al., 2016). The testing ECE are depicted in Figure 4, where the pink and blue polygons represent CigL and other calibration methods, respectively. We can see that CigL usually gives smaller polygons, indicating a better confidence calibration.\n\n5.4 ABLATION STUDIES\n\nWe do ablation studies to demonstrate the importance of each component in our CigL, where we train sparse networks using our CigL without random masks (CigL w/o RM) and CigL without weight & mask averaging (CigL w/o WMA), respectively. In CigL w/o RM, we search for sparse topologies using only the deterministic mask. In CigL w/o WMA, we collect multiple model samples and use prediction averaging during testing. Figures 5(a)-(b) show the effect of random masks on the test accuracy and ECE values, where the blue, green, and pink bars represent RigL, CigL w/o RM, and CigL, respectively. We can see that if we remove the random mask, we can still obtain an improvement in accuracy compared to RigL. However, the ECE values do not decrease as much as CigL, indicating that the CigL w/o RM is not as effective as CigL in improving the confidence calibration. Figures 5(c)-(d) further show the effect of weight & mask averaging. We can see that without using weight & mask averaging, the accuracy decreases and the ECE value increases in high sparsity such as 95% and 99%, demonstrating the importance of weight & mask averaging.\n\n6 DISCUSSION & CONCLUSION\n\nWe for the first time identify and study the reliability problem of sparse training and find that sparse training exacerbates the over-confidence problem of DNNs. We then develop a new sparse training\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) CigL vs. Mixup\n\n(b) CigL vs. Temper. Scaling\n\n(c) CigL vs. Label Smoothing\n\nFigure 4: ECE value comparison between CigL and RigL + other calibration methods at different sparsities (80%, 90%, 95%, 99%). The pink polygons (CigL) are smaller than the blue polygons (other calibration methods), indicating a better confidence calibration using CigL compared to (a) Mixup, (b) Temperature scaling, and (c) Label smoothing.\n\n(a) Test accuracy (RM)\n\n(b) ECE value (RM)\n\n(c) Test accuracy (WMA)\n\n(d) ECE value (WMA)\n\nFigure 5: Ablation studies: test accuracy(%) and ECE value comparison between CigL, CigL without random mask (CigL w/o RM), and CigL without weight & mask averaging (CigL w/o WMA) at different sparsities (80%, 90%, 95%, 99%). Compared to (a)-(b) CigL w/o RM and (c)-(d) CigL w/o WMA, CigL more consistently produces sparse models with low ECE values and high accuracy.\n\nmethod, CigL, to produce reliable sparse models, which can simultaneously maintain or even improve accuracy with only a slight increase in computational and storage burden. Our CigL utilizes two masks, including a deterministic mask and a random mask, which allows the sparse model to better explore the weight space. Then, we design weight & mask averaging method to combine multiple sparse weights and random masks into a single model with improved reliability. We prove that CigL can be viewed as a hierarchical variational approximation to the probabilistic deep Gaussian process. Experiments results on multiple benchmark datasets, model architectures, and sparsities show that our CigL reduces ECE values by up to 47.8% with comparable or higher accuracy.\n\nOne phenomenon we find worth discussing is the double descent in reliability of sparse training. Nakkiran et al. (2021) first observed this double descent phenomenon in DNNs, where as the model size, data size, or training time increases, the performance of the model first improves, then gets worse, and then improves again. Consistent with the previous definition, we consider sparsity and reliability as the measures of model size and performance, respectively. Then, as shown in the Figure 1 (c), as the sparsity decreases (model size increases), the reliability (model performance) gets better, then gets worse, and then gets better again. To explain this phenomenon, we divided sparsity into four phases, from the left (99.9%) to the right (0%), by drawing an analogy between the phases and model accuracy and size. (a) The sparse model starts as a poor model, which is too sparse to learn the data well (low reliability & accuracy). (b) It gradually becomes equivalent to a shallow model that can learn some patterns but is not flexible enough to learn all the data well (high reliability & moderate level of accuracy). (c) Then, it moves to a sparse deep model that can accommodate complex patterns but suffers from poor exploration (low reliability & high accuracy). (d) Finally, it reaches a dense deep model with over-confidence issues (moderate level of reliability & high accuracy). It is observed that at around 95% sparsity, the sparse model can achieve comparable accuracy and high sparsity at the same time, which makes it important in practical applications. However, the ECE value is at the peak of the double-descent curve at this point, which implies that the reliability of the sparse model is at a low level. Thus, our CigL smooths the double descent curve and produce reliable models on those important high sparsity levels.\n\n9\n\n80%90%95%99%0.040.080.120.160.20RigL + MixupCigL80%90%95%99%0.0060.0120.0180.0240.030RigL + TSCigL80%90%95%99%0.0120.0240.0320.0480.060RigL + LSCigL80%90%95%99%606264666870727476RigLCigL w/o RMCigL80%90%95%99%0.040.050.060.070.080.090.100.110.120.13RigLCigL w/o RMCigL80%90%95%99%8082848688909294RigLCigL w/o WMACigL80%90%95%99%0.000.010.020.030.040.05RigLCigL w/o WMACigLPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis research was partially supported by NSF Grant No. NSF CCF-1934904 (TRIPODS).\n\nREPRODUCIBILITY STATEMENT\n\nThe implementation code can be found in https://github.com/StevenBoys/CigL. All datasets and code platform (PyTorch) we use are public.\n\nREFERENCES\n\nArsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. arXiv preprint arXiv:2002.06470, 2020.\n\nGuillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. International Conference on Learning Representations (ICLR), 2018.\n\nSameer Bibikar, Haris Vikalo, Zhangyang Wang, and Xiaohan Chen. Federated dynamic sparse training: Computing less, communicating less, yet learning better. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6080–6088, 2022.\n\nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-\n\nume 4. Springer, 2006.\n\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\n\nTianlong Chen, Zhenyu Zhang, Jun Wu, Randy Huang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Can you win everything with a lottery ticket? Transactions of Machine Learning Research, 2022.\n\nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and\n\nstatistics, pp. 207–215. PMLR, 2013.\n\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing\n\nperformance. arXiv preprint arXiv:1907.04840, 2019.\n\nAnastasia Dietrich, Frithjof Gressmann, Douglas Orr, Ivan Chelombiev, Daniel Justus, and Carlo Luschi. Towards structured dynamic sparse pre-training of bert. arXiv preprint arXiv:2108.06277, 2021.\n\nUtku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse\n\nneural networks. arXiv preprint arXiv:1906.10732, 2019.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. International Conference on Learning Representations (ICLR), 2019.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059. PMLR, 2016.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\n\nnetworks. In International conference on machine learning, pp. 1321–1330. PMLR, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZheng He, Zeke Xie, Quanzhi Zhu, and Zengchang Qin. Sparse double descent: Where network In International Conference on Machine Learning, pp. 8635–\n\npruning aggravates overfitting. 8659. PMLR, 2022.\n\nGeoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\n\nShaoyi Huang, Bowen Lei, Dongkuan Xu, Hongwu Peng, Yue Sun, Mimi Xie, and Caiwen Ding. Dynamic sparse training via balancing the exploration-exploitation trade-off. arXiv preprint arXiv:2211.16667, 2022.\n\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon WilarXiv preprint\n\nson. Averaging weights leads to wider optima and better generalization. arXiv:1803.05407, 2018.\n\nSiddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast: Top-k always sparse training. Advances in Neural Information Processing Systems, 33:20744–20754, 2020.\n\nXiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association, 19(2):263–274, 2012.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\nTechnical report, University of Toronto, 2009.\n\nShiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In International Conference on Machine Learning, pp. 6989–7000. PMLR, 2021.\n\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems, 32, 2019.\n\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018.\n\nHesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In International Conference on Machine Learning, pp. 4646–4655. PMLR, 2019.\n\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021.\n\nJeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-\n\ning calibration in deep learning. In CVPR Workshops, volume 2, 2019.\n\nOzan ̈Ozdenizci and Robert Legenstein. Training adversarially robust sparse networks via bayesian In International Conference on Machine Learning, pp. 8314–8324.\n\nconnectivity sampling. PMLR, 2021.\n\nKanil Patel, William Beluch, Kilian Rambach, Michael Pfeiffer, and Bin Yang. Improving uncertainty of deep learning-based object classification on radar spectra using label smoothing. In 2022 IEEE Radar Conference (RadarConf22), pp. 1–6. IEEE, 2022.\n\nRahul Rahaman et al. Uncertainty quantification and deep ensembles. Advances in Neural Informa-\n\ntion Processing Systems, 34:20063–20075, 2021.\n\nRajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International\n\nconference on machine learning, pp. 324–333. PMLR, 2016.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\nJonathan Schwarz, Siddhant Jayakumar, Razvan Pascanu, Peter E Latham, and Yee Teh. Powerpropagation: A sparsity inducing weight reparameterisation. Advances in Neural Information Processing Systems, 34:28889–28903, 2021.\n\nGowthami Somepalli, Liam Fowl, Arpit Bansal, Ping Yeh-Chiang, Yehuda Dar, Richard Baraniuk, Micah Goldblum, and Tom Goldstein. Can neural nets learn the same model twice? investigating reproducibility and double descent from the decision boundary perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13699–13708, 2022.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\nYiyou Sun and Yixuan Li. On the effectiveness of sparsification for detecting the deep unknowns.\n\narXiv preprint arXiv:2111.09805, 2021.\n\nVarun Sundar and Rajat Vadiraj Dwaraknath. [reproducibility report] rigging the lottery: Making all\n\ntickets winners. arXiv preprint arXiv:2103.15767, 2021.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. RethinkIn Proceedings of the IEEE conference on\n\ning the inception architecture for computer vision. computer vision and pattern recognition, pp. 2818–2826, 2016.\n\nBindya Venkatesh, Jayaraman J Thiagarajan, Kowshik Thopalli, and Prasanna Sattigeri. Calibrate and prune: Improving reliability of lottery tickets through prediction calibration. arXiv preprint arXiv:2002.03875, 2020.\n\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International conference on machine learning, pp. 1058–1066. PMLR, 2013.\n\nYezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, and Dongsheng Li. Energy-based open-world uncertainty modeling for confidence calibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9302–9311, 2021.\n\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing In International Conference on Machine Learning, pp. 23965–23998. PMLR, inference time. 2022.\n\nMengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate\n\nmodels. arXiv preprint arXiv:2204.00408, 2022.\n\nLu Yin, Vlado Menkovski, Meng Fang, Tianjin Huang, Yulong Pei, Mykola Pechenizkiy, Decebal Constantin Mocanu, and Shiwei Liu. Superposing many tickets into one: A performance booster for sparse neural network training. arXiv preprint arXiv:2205.15322, 2022.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer-\n\nence, 2016.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nJize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional In International conference on machine\n\nmethods for uncertainty calibration in deep learning. learning, pp. 11117–11128. PMLR, 2020.\n\nXiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang. Effective sparsification of neural networks with global sparsity constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3599–3608, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX: BACKGROUND\n\nIn this section, we briefly summarize CigL, Gaussian processes, and hierarchical variational inference, which will be used to support the main theoretical analysis of this work.\n\nA.1 SPARSE TRAINING: CIGL\n\nWe first review our CigL method for the case of a single hidden layer neural network (NN). This is done for ease of notation, and it is straightforward to generalise to multiple layers (Gal & Ghahramani, 2016). Denote by W1, W2 the sparse weight matrices connecting the first layer to the hidden layer and connecting the hidden layer to the output layer respectively. For the sparse mask controlling the sparse topology, we use M1, M2 to denote the corresponding deterministic masks for W1 and W2, respectively. And we use Z1, Z2 to denote the corresponding random masks for W1 and W2, respectively. These linearly transform the layers’ inputs before applying some elementwise non-linearity σ(). Denote by b the biases by which we shift the input of the non-linearity. We assume the model to output D dimensional vectors while its input is Q dimensional vectors, with K hidden units. Thus W1, M1, and Z1 are Q × K matrices, W2, M1, and Z1 are K × D matrices, and b is a K dimensional vector. A sparse NN model with the two masks would output (cid:98)y = σ(xZ1 ⊙ W1 + b)Z2 ⊙ W2 given some input x. For the update of masks, the deterministic masks is updated by exploiting the magnitude of weights and gradients. The random mask is randomly sampled. For the update of weights, we use E to denote the loss function, which is the euclidean loss for regression problem\n\nE =\n\n1 2N\n\n(cid:88)\n\nn=1\n\nN ||yn − (cid:98)yn||2 2,\n\n(6)\n\nwhere ynis the observed response, (cid:98)yn is the prediction based on input xn for n = 1, · · · , N . For classification task with D classes, we use softmax function to map the output (cid:98)yn to the probability score for each class (cid:98)pnd = exp((cid:98)ynd)/((cid:80)\n\nk exp((cid:98)ynk)), and the loss function will be\n\nE = −\n\n1 N\n\n(cid:88)\n\nn=1\n\nN log((cid:98)pn,cn ),\n\n(7)\n\nwhere cn ∈ [1, 2, · · · , D] is the true class label for xn.\n\nDuring NN optimization process, apart from the loss function mentioned above, l2 regularisation is often used to improve the performance, leading to a minimisation objective:\n\nLCigL :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE(yi, (cid:98)yi) + λ1||W1||2\n\n2 + λ2||W2||2\n\n2 + λ3||b||2 2.\n\n(8)\n\nTherefore, duing the training process of CigL, we iteratively update the sparse weights W by minimising Eq. (8) and update deterministic mask based on the magnitude of weights and gradients.\n\nA.2 GAUSSIAN PROCESS\n\nThe Gaussian process (GP) is a popular non-parametric Bayesian methodto model distributions over functions, which can be applied to bothe regression and classification tasks. It has good performance in various fields, but it will bring huge computation burden when faced with a large number of data. The use of variational inference for GP can make it scalable to large data.\n\nGiven a data {xn, yn}, n = 1, · · · , N , the task is to estimate an unknown function y = f (x), where X ∈ RN ×Q and Y ∈ RN ×D. GP usually put a prior over the function space and we want to fit the posterior distribution over the function space:\n\np(f |X, Y ) ∝ p(Y |X, f )p(f ).\n\nWithin Gaussian process, we usually place a Gaussian prior over the function space, and it equivalent to placing a joint Gaussian distribution over all function values\n\nF |X ∼ N (0, K(X, X)) Y |F ∼ N (F , τ −1)\n\n13\n\n(9)\n\nPublished as a conference paper at ICLR 2023\n\nwhere τ is a precision parameter and IN is the identity matrix with dimensions N × N .\n\nFor classification tasks, we can formulate the model as\n\nF |X ∼ N (0, K(X, X)) Y |F ∼ N (F , τ −1), (cid:18)\n\ncn|Y ∼ Categorical\n\nexp((cid:98)ynd)/(\n\n(cid:88)\n\n(cid:19)\n\nexp((cid:98)ynk))\n\n(10)\n\n(11)\n\nk\n\nAn important aspect of Gaussian process is the choice of the covariance function, which reflects how we believe the similarity between each pair of inputs xi and xj. One widely-used covariance function is stationary squared exponential covariance function. In addition, some non-stationary covariance function are proposed, such as dot-product kernels and more flexible deep network kernels.\n\nA.3 HIERARCHICAL VARIATIONAL INFERENCE\n\nVariational inference (VI) is a broadly-used technique to approximate intractable integrals in Bayesian modeling, which sets up a parameterized family of tractable distributions over the latent variables and then optimizes the parameters to be close to the posterior. More specifically, suppose w is the set of random variables defining our model. Then, the predictive distribution will be formulated as\n\np(y∗|x∗, X, Y ) =\n\n(cid:90)\n\np(y∗|x∗, w)p(w|X, Y )dw,\n\nwhere the posterior p(w|X, Y ) is usually intractable. Thus, we define a family of tractable approximating variational distributions q(w) to approach the posterior.\n\nTo find the closest approximating distribution among the family of q(w), we minimise the Kullback–Leibler (KL) divergence between q(w) and posterior p(w|X, Y ), which is equivalent to maximising the log evidence lower bound (ELBO) with respect to q(w):\n\n(cid:90)\n\nLVI :=\n\nq(w) log p(Y |X, w)dw − DKL(q(w)∥p(w)).\n\nAfter obtaining an good approximation q(w), we can update the predictive distribution to\n\np(y∗|x∗, X, Y ) =\n\n(cid:90)\n\np(y∗|x∗, w)q(w)dw.\n\nHowever, when faced with posterior difficult to fit, q(w) can be limited and not flexible enough to approach the posterior. In this case, VI cannot capture the posterior dependencies between latent variables that both improve the fidelity of the approximation and are sometimes intrinsically meaningful. To solve this limitation of VI, hierarchical variational inference (HVI) is proposed, which can capture both posterior dependencies between the latent variables and more complex marginal distributions (Ranganath et al., 2016). More specifically about HVI, we extend the limited family of VI distribution hierarchically, i.e., by placing a prior on the parameters of the likelihood. Suppose VI uses q(w; λ) to approximate the posterior where λ is the variational parameters to optimise. HVI will add prior on λ and uses q(w|λ)q(λ; θ). The ELBO equivalently will be as:\n\nLHVI := EqHVI(w;λ)[log p(x, w) − log qHVI(w; θ)].\n\nThis ELBO can be further bounded by\n\nLHVI ≤ EqHVI(w;λ)[log p(x, w)] − Eq(w,λ)[log q(λ) + log q(w|λ) − log r(λ|w; θ)].\n\nwhere r(λ|w; θ) is introduced to apply the variational principle.\n\nB APPENDIX: CIGL AS A HIERARCHICAL BAYESIAN APPROXIMATION\n\nWe show that sparse deep NNs trained with CigL are mathematically equivalent to approximate hierarchical variational inference in the deep Gaussian process (marginalised over its covariance\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nfunction parameters). For this, we build on previous work (Gal & Ghahramani, 2016) that proved unit dropout applied before every weight layer are mathematically equivalent to approximate variational inference in the deep Gaussian process. Starting with the full Gaussian process we will develop an approximation that will be shown to be equivalent to the sparse NN optimisation objective with CigL (eq. (8)) with either the Euclidean loss in the case of regression or softmax loss in the case of classification. Our derivation takes regression as an example, which can be extended to classification by Section 4 of the Appendix of Gal & Ghahramani (2016). This view of CigL will allow us to derive new probabilistic results in sparse training.\n\nB.1 A GAUSSIAN PROCESS APPROXIMATION\n\nIn this section, we will re-parameterise the deep GP model and marginalise over the additional auxiliary random variables, which is built on Gal & Ghahramani (2016). To define our covariance function, let σ(.) be some non-linear activation function and K(x, y) can be formulated as:\n\n(cid:90)\n\nK(x, y) =\n\np(w)p(b)σ(w⊤x + b)σ(w⊤y + b)dwdb,\n\nwhere p(w) is a standard multivariate normal distribution in dimension Q.\n\nWe use Monte Carlo integration with K samples to approximate the integral above and get the finite rank covarinace function\n\n(cid:99)K(x, y) =\n\n1 K\n\nK (cid:88)\n\nk=1\n\nσ(w⊤\n\nk x + bk)σ(w⊤\n\nk y + bk),\n\nwhere wk ∼ p(w) and bk ∼ p(b). K is the number of hidden units in our single hidden layer sparse NN approximation. The generative model will be as follow when we use (cid:99)K instead of K:\n\nw ∼ p(w), W1 = [wqk]Q\n\nq=1\n\nbk ∼ p(b), K\n\nk=1,\n\nb = [bk]K\n\nk=1,\n\n(cid:99)K(x, y) =\n\n1 K\n\nK (cid:88)\n\nk=1\n\nσ(w⊤\n\nk x + bk)σ(w⊤\n\nk y + bk),\n\nF |X, W1, b ∼ N (0, (cid:99)K(x, y)),\n\nY |F ∼ N (F , τ −1In),\n\nwhere W1 ∈ RQ×K which parameterise the covariance function.\n\nWe can get the predictive distribution by integrating over the F , W1, and b\n\n(cid:90)\n\np(Y |X) =\n\np(Y |F )p(F |W1, b, X)p(W1)p(b)dW1db.\n\nDenoting a 1 × K row vector\n\nφ(x, W1, b) =\n\n(cid:114) 1 K\n\nσ(W ⊤\n\n1 x + b)\n\nand a N × K feature matrix Φ = [φ(xn, W1, b)]N predictive distribution can be rewritten as\n\nn=1. Then, we can get (cid:99)K(X, X) = ΦΦ⊤ and the\n\n(cid:90)\n\np(Y |X) =\n\nN (Y ; 0, ΦΦ⊤ + τ −1IN )p(W1)p(b)dW1db\n\nThe normal distribution of Y inside the integral above can be written as a joint normal distribution over yd which denoting the d-th columns of the N × D matrix Y (d = 1, · · · , D). For each term in the joint distribution, following Bishop & Nasrabadi (2006), we introduce a K × 1 auxiliary random variable wd ∼ N (0, IK),\n\nN (yd; 0, ΦΦ⊤ + τ −1IN ) =\n\n(cid:90)\n\nN (yd; Φwd, τ −1IN )N (wd; 0, IK)dwd.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nWe use W2 = [wd]D\n\nd=1 ∈ RK×D and we get the predictive distribution as\n\n(cid:90)\n\np(Y |X) =\n\np(Y |X, W1, W2, b)p(W1)p(W2)p(b)dW1dW2db.\n\nB.2 HIERARCHICAL VARIATIONAL INFERENCE IN THE APPROXIMATE MODEL\n\nWe next approximate the posterior over these variables with appropriate hierarchical approximating variational distributions. We define a hierarchical variational distribution as:\n\nq(W1, W2, b) := q(W1)q(W2)q(b) =\n\nq(W1|U1)q(W2|U2)q(U1)q(U2)q(b)dU1dU2,\n\n(cid:90)\n\nwhere we define q(W1) to be a Gaussian mixture distribution with two components, which is factorised over Q and K:\n\nq(W1|U1) =\n\nQ (cid:89)\n\nK (cid:89)\n\nq=1\n\nk=1\n\nq(wqk|uqk),\n\nq(wqk|uqk) = p1N (uqk, σ2) + (1 − p1)N (0, σ2),\n\nq(uqk) = N (vqk, σ2)\n\nwhere p1 ∈ [0, 1], and σ > 0. Similarly, we can define a hierarchical variational distribution over W2\n\nq(W2|U2) =\n\nK (cid:89)\n\nD (cid:89)\n\nk=1\n\nd=1\n\nq(wkd|ukd),\n\nq(wkd|ukd) = p2N (ukd, σ2) + (1 − p2)N (0, σ2),\n\nq(ukd) = N (vkd, σ2)\n\nFor b, we use a simple Gaussian distribution\n\nq(b) = N (u, σ2IK).\n\nB.3 EVALUATING THE LOG EVIDENCE LOWER BOUND FOR REGRESSION\n\nNext we evaluate the log evidence lower bound for the task of regression. The log evidence lower bound is as below\n\n(cid:90)\n\nLGP-VI :=\n\nq(W1, W2, b) log p(Y |X, W1, W2, b) − DKL(q(W1, W2, b)∥p(W1, W2, b)).\n\nwhere the integration is with respect to W1, W2, b.\n\nDuring regression, we can rewrite the integrand as a sum:\n\nlog p(Y |W1, W2, b) =\n\nD (cid:88)\n\nd=1\n\nlog N (yd; Φwd, τ −1IN ),\n\n= −\n\nN D 2\n\nlog(2π) +\n\nN D 2\n\nlog(τ ) −\n\nD (cid:88)\n\nd=1\n\nτ 2\n\n||yd − Φwd||2 2.\n\nas the output dimensions of a multi-output Gaussian process are assumed to be independent. Denote (cid:98)Y = ΦW2. We can then sum over the rows instead of the columns of (cid:98)Y and write\n\nD (cid:88)\n\nd=1\n\nτ 2\n\n||yd − (cid:98)yd||2\n\n2 =\n\nN (cid:88)\n\nn=1\n\nτ 2\n\n||yn − (cid:98)yn||2 2.\n\nHere we have (cid:98)yn = φ(x, W1, b)W2 =\n\n(cid:113) 1\n\nK σ(xnW1 + b)W2, leading to\n\nlog p(Y |W1, W2, b) =\n\nN (cid:88)\n\nn=1\n\nlog N (yn; φ(xn, W1, b)W2, τ −1ID).\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, we can update the log evidence lower bound as\n\nN (cid:88)\n\n(cid:90)\n\nn=1\n\nq(W1, W2, b) log p(Y |xn, W1, W2, b) − DKL(q(W1, W2, b)∥p(W1, W2, b)).\n\nWe re-parametrise the integrands in the sum to not depend on W1, W2, b directly, but instead on the standard normal distribution and the Bernoulli distribution. Let q(ε1) = N (0, IQ×K), q(z1,q,k) = Bernoulli(p1), q(ε2) = N (0, IK×D), q(z2,k,d) = Bernoulli(p2), q(ε) = N (0, IK), q(ε3) = N (0, IQ×K), and q(ε4) = N (0, IK×D). Then, we can have\n\nW1 = Z1 ⊙ (U1 + σε1) + (1 − Z1) ⊙ σε1, W2 = Z2 ⊙ (U2 + σε2) + (1 − Z2) ⊙ σε2,\n\nb = u + σε, U1 = σε3, U2 = σε4\n\nwhere ⊙ means element-wise multiplication. Thus, we can update the above the sum over the integrals\n\nN (cid:88)\n\n(cid:90)\n\nn=1\n\nq(W1, W2, b) log p(yd|xn, W1, W2, b),\n\nN (cid:88)\n\n(cid:90)\n\n=\n\nn=1\n\nq(Z1, ε1, Z2, ε2, ε, ε3, ε4) log p(yd|xn, W1(Z1, ε1, ε3), W2(Z2, ε2, ε4), b(ε)).\n\nFor the first term in LGP-MC, we can estimate the integrals using Monte Carlo integration with a distinct single sample to obtain\n\nLGP-MC :=\n\nN (cid:88)\n\n(cid:90)\n\nn=1\n\nlog p(yd|xn, (cid:99)W n\n\n1 , (cid:99)W n\n\n2 , (cid:98)bn) − DKL(q(W1, W2, b)∥p(W1, W2, b)).\n\nFollowing Gal & Ghahramani (2016), by optimising the stochastic objective LGP-MC, we can converge to the same limit as LGP-VI, which justifies this stochastic approximation.\n\nMoving to the second term in LGP-MC, we use w and u to denote certain component in the weights (W1) and variational parameters (U1), respectively. Then, we can have\n\n(cid:18)\n\n(cid:19)\n\n− DKL\n\nq(w)∥p(w)\n\n= −\n\n(cid:90)\n\nq(w) log\n\nq(w) p(w)\n\ndw\n\n(cid:90) (cid:90)\n\nq(w, u)du log\n\ndw\n\np(w) (cid:82) q(w, u)du p(w) (cid:82) q(w, u)du\n\nq(w|u) log\n\ndw}du\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\nq(u){\n\n(cid:90)\n\nq(u)[\n\n(cid:90)\n\nq(u)[\n\nq(w|u) log p(w)dw]du −\n\nq(w|u) log p(w)dw]du −\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\nq(u){\n\n(cid:90)\n\nq(w|u) log[\n\nq(w, u)du]dw}du\n\n(cid:90) [\n\nq(w, u)du] log[\n\n(cid:90)\n\nq(w, u)du]dw\n\n(12)\n\nFor the first term in Eq. (12), we can first follow the Proposition 1 in (Gal & Ghahramani, 2016) to approximate (cid:82) q(w|u) log p(w)dw as below:\n\n(cid:90)\n\nq(w|u) log p(w)dw ≈ −\n\n1 2\n\n(p · u2 + σ2)\n\nThen, we can approximate the first term in Eq. (12) as\n\n(cid:90)\n\n(cid:90)\n\nq(u)[\n\nq(w|u) log p(w)dw]du ≈\n\n(cid:90)\n\nq(u)[−\n\n1 2\n\n(p · u2 + σ2)]du\n\n= −\n\n1 2\n\nσ2 −\n\n(cid:90)\n\np 2\n\nq(u)u2du = −\n\np + 1 2\n\nσ2 −\n\np 2\n\nv2\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFor the second term in Eq. (12), we can estimate the integral (cid:82) q(w, u)du using Monte Carlo integration with a distinct single sample to obtain\n\n(cid:90)\n\n(cid:90) [\n\nq(w, u)du] log[\n\n(cid:90)\n\nq(w, u)du]dw ≈\n\n(cid:90)\n\nq(w|(cid:98)u) log[q(w|(cid:98)u)]dw\n\nThen we can follow the Proposition 1 in (Gal & Ghahramani, 2016) to approximate it as\n\n(cid:90)\n\nq(w|(cid:98)u) log[q(w|(cid:98)u)]dw ≈\n\n1 2\n\n(log σ2 + 1 + 2 log π) + C\n\nTherefore, we can get the approximation for Eq. (12) as Eq. (13):\n\n(cid:18)\n\n(cid:19)\n\n−DKL\n\nq(w)∥p(w)\n\n= −\n\np + 1 2\n\nσ2 −\n\np 2\n\nv2 +\n\n1 2\n\n(log σ2 + K(1 + 2 log π)) + C\n\n(13)\n\nThen, we can have the following equation based on Eq. (13):\n\nDKL(q(W1)∥p(W1)) ≈\n\nQK(p + 1) 2\n\nσ2 −\n\nQK 2\n\n(log(σ2) + 1) +\n\np1 2\n\nQ (cid:88)\n\nK (cid:88)\n\nq=1\n\nk=1\n\nv2\n\nqk + C,\n\nwhere C is a constant and DKL(q(W2)∥p(W2)) can be approximated in a similar way. For DKL(q(b)∥p(b)), it can be written as\n\nDKL(q(b)∥p(b)) =\n\n1 2\n\n(u⊤u + K(σ2 − log(σ2) − 1)) + C.\n\nB.4 LOG EVIDENCE LOWER BOUND OPTIMISATION FOR CIGL\n\nNext we explain the relation between the above equations with equations for CigL. Ignoring the constant terms τ , σ we obtain the maximisation objective\n\nLGP-MC ∝ −\n\nτ 2\n\nN (cid:88)\n\nn=1\n\n||yn − (cid:98)yn||2\n\n2 −\n\np1 2\n\n||V1||2\n\n2 −\n\np2 2\n\n||V2||2\n\n2 −\n\n1 2\n\n||u||2 2.\n\n(14)\n\nWe will show the equivalence between the iterative update of M1, Z1 and U1 in CigL and the hierarchical variational inference for deep GP. The update for M2, Z2 and U2 will be similar. In the hierarchical variational distribution, the distribution of sparse weights W1 depends on three random variables M1, Z1, and U1. Given Z1 and U1, we can know the variational distribution for M1 as:\n\nq(M1|U1) ∝ exp(M1 ⊙ |U1|)\n\n(15)\n\nwhere M1 is under certain sparsity constraint. Thus, we can update M1 by choosing (cid:99)M1 that maximising Eq. (15), which is aligned with the M1 update procedure in CigL.\n\nGiven M1, we can use (cid:98)V1 to approximate (cid:98)U1. Then, we can let σ tend to zero (Gal & Ghahramani, 1 , (cid:99)W n 2016), and the random variable realisations (cid:99)W n\n\n2 , (cid:98)bn can be\n\n(cid:99)W n\n\n1 ≈ (cid:98)Z1 ⊙ (cid:98)U1, (cid:99)W n\n\n2 ≈ (cid:98)Z2 ⊙ (cid:98)U2,\n\n(cid:98)bn ≈ (cid:98)u\n\nThen, we can get\n\n(cid:114) 1 K\n\n(cid:98)yn ≈\n\nσ(xn( (cid:98)Z1 ⊙ (cid:98)U1) + (cid:98)u)( (cid:98)Z2 ⊙ (cid:98)U2).\n\nWe scale the optimisation objective by a positive constant 1\n\nτ N and get the objective:\n\nLGP-MC ∝ −\n\n1 2N\n\nN (cid:88)\n\nn=1\n\n||yn − (cid:98)yn||2\n\n2 −\n\np1 2τ N\n\n||U1||2\n\n2 −\n\np2 2τ N\n\n||U2||2\n\n2 −\n\n1 2τ N\n\n||u||2 2.\n\nSo, we recover equation for CigL. With correct stochastic optimisation scheduling, both will converge to the same limit.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nB.5 MASK & WEIGHT AVERAGING FOR PREDICTION\n\nFor prediction, we design mask & weight averaging (WMA) to produce the final output model. Specifically, we collect samples of { (cid:98)Z1, (cid:98)U1} during the optimization process, where we use (cid:98)V1 at different epochs to approximate (cid:98)U1. By using weight & mask averging and letting σ tend to zero (Gal & Ghahramani, 2016), the random variable (cid:99)W1, (cid:99)W2, (cid:98)b can be approximated as\n\n(cid:99)W1 ≈\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:98)Z(s)\n\n1 ⊙ (cid:98)U (s)\n\n1 , (cid:99)W2 ≈\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:98)Z(s)\n\n2 ⊙ (cid:98)U (s) 2 ,\n\n(cid:98)b ≈\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:98)u(s).\n\nWMA can be seen as approximating the mean of the posterior based on samples from variational distribution q(W ) using moment matching, which is justified as below:\n\n(i) CigL is connected to the Bayesian approach because of using both deterministic and random masks to explore the weight space. As shown in Equation 3, the design of Z and M results in a hierarchical variational distribution q(W ), where the hierarchy expands the approximation family and leads to a better posterior approximation capability. In Equation 3, updating the mask is equivalent to updating and to bring closer to the posterior 3.\n\n(ii) In a similar idea to weight dropout (Gal & Ghahramani, 2016), WMA can be considered as a Bayesian approximation, which is used to approximate the mean of the posterior by moment matching.\n\n• Weight dropout has been shown to be equivalent to the Bayesian approximation method (Gal & Ghahramani, 2016). After obtaining the final W , dropout approximates (cid:82) f (W Z)p(Z)dZ using f (E(W Z)), where f is the neural network and the mean E(W Z) = (cid:82) W Zp(Z)dZ (Srivastava et al., 2014). This approximation actually approximates the whole posterior by the first moment of the posterior (i.e., the mean).\n\n• For our WMA, since we assume a hierarchy, we collect multiple samples of W and Z. Then, the WMA is used to approximate the first moment of the posterior which is used as an approximation of the posterior itself (Srivastava et al., 2014).\n\n• In addition, if we really want the second moment, it is straightforward to obtain an estimation based on samples using moment matching again, similar to Maddox et al. (2019). We do not estimate the second moment since the sparse training typically wants a single sparse model in the end to reduce both computational and memory costs, and we find that using the posterior mean already significantly improves the calibration of the sparse training.\n\nC APPENDIX: ADDITIONAL EXPERIMENTAL RESULTS\n\nC.1 STRONGER CORRELATION BETWEEN HIDDEN VARIABLES\n\nEmpirically, we find that a stronger correlation between Z and W in sparse training. We use CigL to train sparse Wide-ResNet-22-2 on CIFAR-10 at multiple sparsities (0%, 50%, 80%, 90%). Then, we randomly draw five random masks Zi, i ∈ 1, · · · , 5 from Bernoulli distribution. Using the final sparse weights W , we obtain several new sparse models Zi ⊙ W , i ∈ 1, · · · , 5, and record their test accuracies. We compare the test accuracy of W with the average accuracy of Zi ⊙ W , i ∈ 1, · · · , 5 to see the correlation. The larger decrease in test accuracy after multiplying by Zi implies a stronger correlation between Z and W .\n\nAs shown in Figure 6 (a), both the accuracy of W (red curve) and the average accuracy of Zi ⊙ W , i ∈ 1, · · · , 5 (blue curve) are decrease with increasing sparsity, and we see a more pronounced decrease in the blue curve. Figure 6 (b) further shows the decrease in test accuracy at each sparsity. We observe that the decrease is very small in the dense or sparse model at low sparsity. However, when it shifts to high sparsity such as 90%, we observe a larger decrease, which indicates a stronger correlation between Z and W in sparse training.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n(a) Test accuracy of W and Zi ⊙ W\n\n(b) Test accuracy decrease from W to Zi ⊙ W\n\nFigure 6: (a) Test accuracy of sparse model W and the newly produced sparse model Zi ⊙ W . (b) decrease in test accuracy from sparse model W to newly produced sparse model Zi ⊙ W . At low sparsity, the decrease of the dense or sparse models is small. At high sparsity, the decrease is larger.\n\nC.2 RELIABILITY IN SPARSE TRAINING\n\nTo get a more comprehensive understanding of the reliability issues in sparse training, we also evaluated the ECE values of the sparse models generated by SET (Mocanu et al., 2018). We find that the sparse model produced by SET is also more over-confident than the dense model. As shown in Table 4, the ECE values of dense ResNet-50 are smaller than those of sparse ResNet-50 on both CIFAR-10 and CIFAR-100. This proves the reliability issue of sparse training.\n\nTable 4: ECE value of sparse ResNet-50 on CIFAR-10 and CIFAR-100 produced by SET at different sparsity including 0%, 50%, 80%, 90%, 95%, 99%.\n\nSPARSITY\n\n0%\n\n50%\n\n80%\n\n90%\n\n95%\n\n99%\n\nCIFAR-100 CIFAR-100\n\n0.0381 0.0841\n\n0.0429 0.0931\n\n0.0416 0.1058\n\n0.0459 0.1290\n\n0.0460 0.1282\n\n0.0589 0.0873\n\nC.3 MORE COMPARISON WITH SPARSE TRAINING BASELINE\n\nWe further compare our CigL with a recent Sparse training baseline Sup-tickets (Yin et al., 2022) to show the effectiveness of CigL in reducing ECE values. Table 5 shows the change in ECE after using Sup-tickets or our CigL. We can see that Sup-tickets brings only a limited reduction in ECE, while the reduction of our CigL is much larger than that of Sup-tickets.\n\nTable 5: ECE value changes of Sup-tickets and CigL in ResNet-50 on CIFAR-10 and CIFAR-100 at different sparsity including 80%, 90%, 95%.\n\nCIFAR-10 90%\n\n80%\n\n95%\n\n80%\n\nCIFAR-100 90%\n\n95%\n\nSUP-TICKETS CIGL\n\n-0.0012 -0.0067\n\n-0.0005 -0.0080\n\n-0.0007 -0.0119\n\n-0.0005 -0.0141\n\n-0.0010 -0.0113\n\n-0.0010 -0.0104\n\nC.4 MORE RESULTS ABOUT THE EFFECT OF WEIGHT & MASK AVERAGING\n\nTo demonstrate that weight & mask averaging (WMA) is not effective in reducing ECE alone, we add more results of using only WMA without random masking (CigL w/o RM). Table 6 shows the change in ECE after using CigL w/o RM or our CigL. We find that when only WMA is used,\n\n20\n\n0%50%80%90%Sparsity0.860.870.880.890.90Test accuracyWW X random Z0%50%80%90%Sparsity0.00250.00500.00750.01000.01250.01500.01750.0200Test accuracy decreasePublished as a conference paper at ICLR 2023\n\nthe ECE value cannot be effectively reduced, either increasing or with limited reduction. On the contrary, the reduction of our CigL is much larger than that of CigL w/o RM.\n\nTable 6: ECE value changes of CigL w/o RM and CigL in ResNet-50 (CIFAR-100) and WideResNet-22-2 (CIFAR-10) at different sparsity including 80%, 90%, 95%, 99%.\n\nRESNET-50, CIFAR-100\n\nWIDE-RESNET-22-2, CIFAR-10\n\n80%\n\n90%\n\n95%\n\n99%\n\n80%\n\n90%\n\n95%\n\n99%\n\nCIGL W/O RM 0.0038 -0.0010\n\nCIGL\n\n-0.0098 -0.0152\n\n-0.0144 -0.0344\n\n-0.0046 -0.0269\n\n0.0060 -0.0141\n\n0.0129 -0.0113\n\n0.0149 -0.0104\n\n0.0017 -0.0049\n\nD MORE DISCUSSION\n\nD.1 WEIGHT SPACE EXPLORATION\n\nITOP (Liu et al., 2021) and DST-EE (Huang et al., 2022) study weight space exploration in sparse training and emphasized its importance. Compared to their studies, our work has two main differences that address their limitations.\n\nOn the one hand, our work has a different goal from ITOP and DST-EE with respect to encouraging exploration of the weight space. Specifically, our work aims to better explore the weight space to find more reliable models, while ITOP and DST-EE aims to build models with higher accuracy, ignoring the safety aspects.\n\nOn the other hand, the exploration of weight space has two aspects, namely “which weight is active” & “what value that weight has”. The limitation of ITOP is that, given the mask, the second aspect is not addressed and the optimization of the algorithm remains more challenging than dense training due to the pseudo-local optimization introduced by the sparsity constraints. To meet this challenge, ITOP increases the iterations between mask updates, leading to an increase in training time. For DST-EE, it mainly targets the first aspect. In contrast to their study, our work addresses this limitation, as shown in the following discussion:\n\nThe first aspect of weight space exploration is reflected by the ITOP rate, which is the percentage of all weights that have ever been selected as active weights by the mask. The second aspect of weight space exploration is reflected by the idea of ”reliable exploration” in the ITOP paper. Ideally, a reliable exploration should allow a model to find the good direction and jump out of the bad local optimum. The sparsity constraints introduce some pseudo-local optima, which is difficult to jump out of. Our random mask can randomly cut off some directions and force the model to explore other directions, thus encouraging the model to better explore the weight space and avoid missing the correct direction.\n\nD.2 DOUBLE DESCENT IN RELIABILITY\n\nOne phenomenon we find worth discussing is the double descent in the reliability of sparse training. We discuss it in Section 6, where we divide the sparsity into four stages, i.e., poor model, shallow model, sparse deep model, and dense deep model.\n\nThe four stages are first supported by intuition. In the discussion, we draw analogies between model types such as ”shallow models” and ”poor models” in terms of model accuracy (expressiveness) and size. Consistent with the previous definition of double descent (Nakkiran et al., 2021; Somepalli et al., 2022), we consider sparsity as a measure of model size. Intuitively, as we gradually reduce the model size (increase the sparsity), we will go through four stages.\n\nThe four arguments are also supported by our sparse training experiments on ResNet-50 at CIFAR100. As shown in Figure 1 (c), we can infer the model type by sparsity and accuracy:\n\n• For 99.7% sparsity, the accuracy of the model is 41.7%, which is similar to a shallow model.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n• For 99.9% sparsity, the accuracy of the model is 23.5%, which can be viewed as a poor model.\n\nMore detailed and quantitative support for these four arguments is beyond the main scope of this paper and could be a good direction for future research. One potential direction is the use of effective depth as a measure of stage identification.\n\nD.3 WEIGHT & MASK AVERAGING\n\nWithout the use of WMA, the analysis in Section 4.2 would be a non-hierarchical Bayesian method or a poor approximation to a hierarchical Bayesian approach.\n\n(i) In the absence of WMA, the algorithm can be viewed as a non-hierarchical variational inference. As described in Section 4.3, using the final W for prediction without WMA is equivalent to using weight dropout in RigL. Thus, the analysis in Section 4.2 will be updated in a similar way to Section 3 in Gal & Ghahramani (2016) which shows that weight dropout can be viewed as a non-hierarchical Bayesian approximation.\n\n(ii) Without WMA, the algorithm can also be viewed as a poor approximation to hierarchical variational inference.\n\n• If we continue to interpret the algorithm without WMA using the current analysis structure from\n\nSection 4.1, then how we generate the final posterior approximation will change.\n\n• In this case, although the algorithm is still a Bayesian approximation, we only use the final W to represent the posterior, which does not effectively capture all the information we explore from the weight space and the increased correlation between Z and W .\n\n• Therefore, it turns out to be a bad hierarchical approximation, which limits its power.\n\nD.4 MULTI-MASK SPARSE DNNS\n\nExisting multi-mask methods are not designed for improved weight space exploration. Bibikar et al. (2022) considers sparse training in federated learning and investigates the aggregation of multiple masks in edge devices. Xia et al. (2022) utilizes multiple masks with different granularities to allow greater flexibility in structured pruning and to improve accuracy. Despite the use of multiple masks, existing work (Xia et al., 2022; Bibikar et al., 2022) differ significantly from our work. They still use deterministic masks, which still suffer from the lack of exploration of the weight space, and consider only the accuracy of sparse models. In addition, their setups are federated learning and pruning, which are different from our work.\n\nD.5 SPARSE DNNS: PRUNING & SPARSE TRAINING\n\nAlthough pruning (e.g., Lottery Tickets) and sparse training are related and both produce subnetworks with high accuracy, their goals and discovering journeys are quite different, which leads to significant differences in several important properties, including uncertainty, geometry of the loss surface, generalization ability, and so on.\n\n(i) For the goal, Lottery Tickets mainly aim to reduce the inference cost, while the sparse training also aims to save resources during the training phase.\n\n(ii) Lottery Tickets and sparse training are different in several important properties. As shown in Figure 11 of Chen et al. (2022), the blue and purple bars represent Lottery Tickets and sparse training with a sparsity level of 79%, respectively.\n\n• For uncertainty, sparse training does not improve the confidence calibration compared to dense\n\ntraining, while Lottery Tickets allows for improved confidence calibration.\n\n• For the geometry of the loss surface, sparse training leads to larger trace values and cannot locate\n\nflat local minima. In contrast, Lottery Tickets can still locate flat local minima.\n\n• For generalization ability, sparse training provides higher accuracy and improved robustness com-\n\npared to dense training, while Lottery Tickets provide relatively less improvement.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(iii) The main reason for the different properties is their different discovering journeys.\n\nFor Lottery Tickets:\n\n• It retrains the weights from the initial training phase after each pruning, which significantly in-\n\ncreases the training time but allows more time for the model to explore the weight space.\n\n• It starts from a dense model and has low sparsity in the early stages, which reduces the difficulty\n\nof weight space exploration caused by the sparsity constraints.\n\nFor spare training:\n\n• It maintains a high level of sparsity throughout the training process, which does not extend the\n\ntraining time to enable more exploration of the weight space.\n\n• In addition, maintaining high sparsity can cut off a large portion of the optimization route and\n\nproduce more spurious local minima, thus making training very difficult.\n\n• Chen et al. (2022) shows the differences in the properties of Lottery Tickets and sparse training at the 79% sparsity level. The difficulty of training typically increases with increasing sparsity, implying that the difference is likely to be greater at higher sparsity levels.\n\n23",
    "reference": "# Summary Of The Paper\n\nIn this paper, they proposed a new sparse training method to produce sparse models with improved confidence calibration. A deterministic mask and a random mask are introduced for exploiting the weight magnitude and gradients and exploration.\n\n# Strength And Weaknesses\n\n+The mechanism of random mask and deterministic mask is interesting, and there is also a connection to Gaussian Process and hierarchical variational inference.\n\n+The empirical performance looks good.\n\n-It seems that the weight moving averaging (WMA) process is not integrated into the analysis of section 4.1. On the other hand, the figure. 5 shows that WMA significantly reduces the ECE value, which is much more obvious than the random mask. With this evidence, the analysis of section 4.2 seems questionable since the major confidence calibration comes from WMA instead of the Bayesian formulation.\n\n-The process in Algorithm.1 and the Bayesian approximation introduced in section 4.1 is not well aligned, especially when learning the mask $M$. In Algorithm.1, the mask $M$ is updated using weights and gradients, like RigL in every $\\Delta T$ steps. In section 4.1, the mask $M$ is only related to the weight magnitude. Also, weight $W$ and $M$ are updated iteratively. Why the approximation of section 4.1 still holds when the learning of $M$ is so different? Also, WMA is omitted in section 4.1.\n\n-In Eq.(3), the authors show that $q(M_l | U_l) \\propto \\text{exp} (M_l\\odot |U_l|)$. This formulation seems wired: $M_l$ appeared in both left and right head sizes of $\\propto$. In addition, it's confusing whether $M_l$ is learned or sampled from a distribution. From the context, authors show that $M$ is updated by maximizing $q(M_l | U_l)$. If $M$ is updated and has exact 0 or 1 values, why are you sampling $M$? In addition, if $M$ is sampled, this is still not aligned with Algorithm.1. Algorithm.1 updates $M$ deterministically. \n\n-In the final section, the authors provide four arguments in (a), (b), (c), and (d), and they are not very well supported. For example, in (b), the authors say that \"It gradually becomes equivalent to a shallow model.\" How this conclusion arrives? There are no experiments to show measurements like effective depth.\n\n-What is ADOPT? This is not explained in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing is mostly clear. The idea of combining the deterministic and random masks seems novel. The code is provided for reproducibility.\n\n# Summary Of The Review\n\nIn summary, the idea of combining the deterministic and random masks seems novel. It's a good attempt to connect the proposed method with GP using hierarchical variational inference. However, there are some inconsistencies between the analysis of section 4.1 and Algorithm.1; for example, WMA is ignored, and the learning of $M$ is different. In addition, the confidence calibration seems to come from WMA instead of the Bayesian approximation. In addition, the arguments in section 6 are not well supported.\n\n_____________________________________\n\nThe authors clarify most of my concerns. As a result, I increased my score to 6.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\n3D SEGMENTER: 3D TRANSFORMER BASED SEMANTIC SEGMENTATION VIA 2D PANORAMIC DISTILLATION\n\nZhennan Wu1,2, Yang Li1, Yifei Huang1,3, Lin Gu ∗2,1 , Tatsuya Harada1,2, Hiroyuki Sato1 1 The University of Tokyo 2 RIKEN Center for Advanced Intelligence Project 3 Shanghai AI Laboratory\n\nABSTRACT\n\nRecently, 2D semantic segmentation has witnessed a significant advancement thanks to the huge amount of 2D image datasets available. Therefore, in this work, we propose the first 2D-to-3D knowledge distillation strategy to enhance 3D semantic segmentation model with knowledge embedded in the latent space of powerful 2D models. Specifically, unlike standard knowledge distillation, where teacher and student models take the same data as input, we use 2D panoramas properly aligned with corresponding 3D rooms to train the teacher network and use the learned knowledge from 2D teacher to guide 3D student. To facilitate our research, we create a large-scale, fine-annotated 3D semantic segmentation benchmark, containing voxel-wise semantic labels and aligned panoramas of 5175 scenes. Based on this benchmark, we propose a 3D volumetric semantic segmentation network, which adapts Video Swin Transformer as backbone and introduces a skip connected linear decoder. Achieving a state-of-the-art performance, our 3D Segmenter is computationally efficient and only requires 3.8% of the parameters compared to the prior art. Our code is released on https://github.com/swwzn714/3DSegmenter.\n\n1\n\nINTRODUCTION\n\nSemantic segmentation assigns each 2D pixel(Long et al., 2015) or 3D point(Qi et al., 2017a)/voxel(C ̧ ic ̧ek et al., 2016) to a separate category label representing the corresponding object class. As a fundamental computer vision technique, semantic segmentation has been widely applied to medical image analysis(Ronneberger et al., 2015), autonomous driving(Cordts et al., 2016) and robotics(Ainetter & Fraundorfer, 2021). Most of the existing efforts are invested in 2D settings thanks to great amount of public 2D semantic segmentation datasets(Zhou et al., 2017; Cordts et al., 2016; Nathan Silberman & Fergus, 2012). Nowadays, the wide availability of consumer 3D sensors also largely promotes the need for 3D semantic segmentation (Han et al., 2020; Choy et al., 2019; Thomas et al., 2019; Graham et al., 2018b; Nekrasov et al., 2021).\n\nWe have seen great success in semantic segmentation in 2D images. However, such success does not fully transfer to the 3D domain. One reason is that, given the same scene, processing the 3D data usually requires orders of magnitude more computations than processing a 2D image. E.g. using a volumetric 3D representation, the number of voxels grows O(n3) with the size of the scene. As a result, existing 3D semantic segmentation methods usually need to use smaller receptive fields or shallower networks than 2D models to handle large scenes.\n\nThis motivates us to facilitate 3D semantic segmentation using 2D models. Specifically, we propose a novel 2D-to-3D knowledge distillation method to enhance 3D semantic segmentation by leveraging the knowledge embedded in a 2D semantic segmentation network. Unlike traditional knowledge distillation approaches, where student and teacher models should take the same input, in our case, the 2D teacher model is pre-trained on a large-scale 2D image repository and finetuned by panoramas rendered from 3D scenes. The panorama image is rendered at the center of the 3D scans with\n\n∗Corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n360◦ receptive field, i.e. it contains the global context of the environment. Our 3D student model takes the 3D scene as input and outputs 3D volumetric semantic segmentation. Through differentiable panoramic rendering of 3D semantic segmentation prediction, we could obtain the mapping from the 2D teacher’s prediction to the 3D student’s prediction. Then we transfer the class distributions of the pixel produced from the 2D teacher to the corresponding voxel of the 3D student. To the best of our knowledge, this is the first solution to distill from a pre-trained 2D model to enhance the 3D computer vision task.\n\nTo facilitate this 2D-to-3D knowledge distillation design, we create a large-scale indoor dataset called PanoRooms3D. PanoRooms3D contains 5917 rooms diversely furnished by 3D CAD objects and augmented by randomly sampled floor and wall textures. We manually filter out scenes that contain unlabeled furniture. We prepare for each room a dense 3D volume, and a corresponding 2D panorama that is rendered in the center of the room with 360◦ receptive field, both containing clean semantic labels. Upon finding panorama-to-volume correspondence, the knowledge distillation from 2D to 3D is then enabled.\n\nAs our 3D student backbone, we propose a novel efficient architecture, 3D Segmenter, for semantic segmentation of 3D volumetric data. Inspired by Video Swin Transformer(Liu et al., 2022a), 3D Segmenter employ a pure-transformer structure with an efficient linear decoder. We demonstrate that our 3D Segmenter achieves superior performance and only requires 3.8% of the parameters compared to the prior art that uses 3D convolutional backbones.\n\nOur contributions are three-fold:\n\n• We propose the first 2D-to-3D knowledge distillation method to utilize the data-abundant\n\nand pretrain-ready 2D semantic segmentation to improve 3D semantic segmentation.\n\n• We propose PanoRooms3D, a large-scale 3D volumetric dataset with a clean voxel-wise semantic label and well-aligned corresponding 2D panoramic renderings with pixel-wise semantic labels.\n\n• We propose a novel efficient pure-transformer-based network for the 3D semantic segmentation task. Our 3D Segmenter outperforms prior art that uses 3D convolutional backbone with only 3.8% of the parameters.\n\nWe proved through experiments that our baseline variants have already achieved SoTA performance. The distilled knowledge from 2D further widened our lead. Considering the difficulty of 3D data collection, and the cubic memory consumption of 3D models, our model paves the way to bridge 2D and 3D vision tasks.\n\n2 RELATED WORKS\n\n2.1\n\n2D SEMANTIC SEGMENTATION\n\nThe success of deep convolutional neural networks (Simonyan & Zisserman, 2014; He et al., 2016) for object classification led researchers to exploit the possibility of working out dense prediction problems. Fully Convolutional Networks (FCN)(Long et al., 2015) based encoder-decoder architectures have dominated the research of semantic segmentation since 2015. Later works extend FCN on different facets. Follow-up approaches (Badrinarayanan et al., 2017; Lin et al., 2017; Pohlen et al., 2017; Ronneberger et al., 2015; Zhao et al., 2017) leverage multi-scale feature aggregation. (Fu et al., 2019; Yin et al., 2020; Yu et al., 2020; Yuan et al., 2018; Zhao et al., 2018) apply attention mechanism to model long-range dependencies. Recently, ConNeXt(Liu et al., 2022b) modernizes the CNNs to achieve higher performance than transformers, suggesting the effectiveness of convolutional methods. On the other hand, ViT(Dosovitskiy et al., 2020) successfully introduced Transformer(Vaswani et al., 2017) to computer vision. Later, SETR(Zheng et al., 2021) demonstrates the feasibility of using Transformer-based semantic segmentation. PVT(Pyramid Vision Transformer)(Wang et al., 2021) combines pyramid structures and ViT for dense prediction. SegFormer(Xie et al., 2021) proposes a hierarchical Transformer encoder and a lightweight MLP decoder to fuse multi-level features and predict the semantic segmentation mask. Using vanilla ViT and DeiT(Touvron et al., 2021) as backbone, Segmenter(Strudel et al., 2021) designed a trainable\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nmask decoder to decode a patch-level embedding and interpolate to the original resolution. Combine with UPerNet(Xiao et al., 2018) structure, Swin Transformer (Liu et al., 2021) shows excellent performance on semantic segmentation.\n\n2.2\n\n3D SEMANTIC SEGMENTATION\n\nPointNet and PointNet++ (Qi et al., 2017a;b) are the pioneer works using MLP (Multi-layer Perceptron) to directly process 3D coordinates for classification and semantic segmentation. Later works specially design convolution operations to extract features inside the neighborhood for the point clouds. Kpconv(Thomas et al., 2019) designs both rigid and deformable kernel point convolution operators for 3D point clouds nearing a set of kernel points. Minkowski Engine (Choy et al., 2019) proposes a sparse convolution to process high-dimensional data efficiently. Occuseg(Han et al., 2020) voxelized the input and uses occupancy signal to achieve instance segmentation.\n\nThe aforementioned 3D semantic segmentation works generally use 3D point cloud as representation. Apart from point cloud, volumetric representation is another widely used 3D shape representation. 3D U-Net (C ̧ ic ̧ek et al., 2016) extends convolutional U-Net(Ronneberger et al., 2015) to 3D biomedical data segmentation. SSCNet (Song et al., 2017a) proposed to jointly do semantic segmentation and completion on RGB-D images. Scancomplete (Dai et al., 2018) propose a coarseto-fine structure to complete scene and predict semantics. U-net structure with 3D CNN kernels is the dominant backbone in this research field.\n\n2.3 TRANSFORMERS IN COMPUTER VISION\n\nTransformer(Vaswani et al., 2017) was proposed to model long-range dependence between words in NLP tasks. Vision Transformer (Dosovitskiy et al., 2020) firstly introduce transformer to computer vision research by embedding image into non-overlapping patches, which serve as tokens for selfattention computation. Variants of ViT such as T2T-ViT(Yuan et al., 2021) , TNT(Han et al., 2021), CrossViT (Chen et al., 2021), LocalViT (Li et al., 2021),Msg-TransformerFang et al. (2022), CvT (Wu et al., 2021), and XCiT(Ali et al., 2021) optimized vanilla ViT for further image classification performance gain. Apart from image classification, PVT (Wang et al., 2021), Swin Transformer (Liu et al., 2021), CoaT (Xu et al., 2021), LeViT(Graham et al., 2021) and Twins (Chu et al., 2021) refer to the hierarchical design of FCN(Long et al., 2015), tailor transformer model for dense prediction tasks. Because of the remarkable performance of transformers in modeling long-range dependence, we adopt a pure-transformer architecture in this work.\n\n2.4 KNOWLEDGE DISTILLATION\n\nThe idea of model compression first came out in 2006 where (Buciluˇa et al., 2006) proposed to transfer knowledge from large model to smaller model without much performance drop. (Hinton et al., 2015) systematically summarized existing knowledge distillation ideas. They demonstrated the effectiveness of student-teacher strategy and response-based knowledge distillation. Following Hinton, many works (Ge et al., 2020; Pham et al., 2021; Li et al., 2020; Xie et al., 2020; Mirzadeh et al., 2020; Yang et al., 2019; Zhang et al., 2020; Wu & Gong, 2021; Touvron et al., 2021) further explore the technique of training response-based knowledge distillation models. Since the knowledge conveyed by output layers is limited, many researchers focus on distillation through intermediate layers. Fitnets(Romero et al., 2014) firstly introduce intermediate layer distillation by using hints from the teachers’ hidden layers to guide the training of the student. the idea of feature-based distillation was carried forward by following researches (Zagoruyko & Komodakis, 2016; Tung & Mori, 2019; Guan et al., 2020; Ahn et al., 2019; Heo et al., 2019; Kim et al., 2018). Recently, BP-Net(Hu et al., 2021) utilizes a bidirectional projection module with symmetric 2D and 3D architectures to achieve mutual enhancement between 2D and 3D. Compared to our distillation design, BP-Net relies on 2D-3D paired input in both training and testing process.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Overview of our 3D Segmenter pipeline. On the left-hand side is the encoder: the input blocks are partitioned and embedded into a sequence of tokens before flowing forward through 3D Swin Transformer. On the right-hand side is the Decoder: a linear-layers-only decoder with a skip connection. Receiving a sequence of tokens from the encoder, the decoder maps the class embedding to per-voxel semantic label.\n\n3\n\n3D SEGMENTER\n\n3.1 ENCODER\n\nAn overview of the 3D Segmenter pipeline is shown in Figure 1.\n\nWe adapt Video Swin Transformer(Liu et al., 2022a) as our backbone. The encoder takes a Truncated Signed Distance Function (Curless & Levoy, 1996)(TSDF) block x = [xtsdf , xrgb] ∈ RX×Y ×Z×4 as input , where X × Y × Z specifies the size of the block and 4 channels consist of one TSDF channel and three RGB color channels.\n\nTSDF is an implicit regular 3D shape representation that stores distance to the nearest surface in each voxel. It is proficient in producing high-quality real-time rendering. The encoder split input into small overlappable patches before outputting a sequence of feature embedding.\n\nDifferent from 2D images, when applying transformer models for 3D tasks, the length of the token sequence grows cubically with the size of the input. The shifted window design proposed by Swin Transformer(Liu et al., 2021) is efficient in limiting the sequence length of each Multi-head Self Attention (MSA) computation, which makes it has linear computational complexity to image size. For efficient and scalable 3D computation, we choose Video Swin Transformer (Liu et al., 2022a) as our encoder backbone.\n\nOur model is trained on fixed-sized 3D blocks cropped from 3D scenes. Receiving an input block x = [xtsdf , xrgb] ∈ RX×Y ×Z×4, we regard each 3D patch of spatial size P × P × P as a token. Therefore, each 3D block x is partitioned into X P × Y P tokens. A single 3D convolution layer embeds each token from 4 channels to feature dimension of size denoted by D.\n\nP × Z\n\nTo formulate hierarchical architecture, Video Swin Transformer reduces the number of tokens through patch merging between blocks. They use a 3D patch merging operation that concatenates the C dimensional features of 2 × 2 spatially neighboring patches to 4C. The token size on the temporal dimension is kept the same during merging. This reduces the number of tokens by a multiple of 2 × 2 = 4. To keep the resolution consistent with typical convolutional networks, the 4C dimensional concatenated feature is downsampled to 2C using a linear layer. Following this design, we use a patch merging layer to merge neighboring 2 × 2 × 2 patches into 8C and downsample it to 2C.\n\nOur encoder is comprised of h Swin Transformer blocks. Patch merging is performed between every two blocks. Therefore, the output of encoder is a sequence with N = X tokens. The dimension of each token is D × 2h−1.\n\nP ×2h−1 × Z\n\nP ×2h−1 × Y\n\nP ×2h−1\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3.2 DECODER\n\nThe decoder maps tokens coming from the encoder to voxel-wise prediction ˆysem ∈ RX×Y ×Z×K, where K is the number of categories in this task. In this work, we propose a decoder with only two linear layers.\n\nAccepting token sequence of shape N × (D × 2h−1) from the encoder, a token-wise linear layer is applied to the sequence, getting a N × K class embedding. The class embedding is then rearranged P ×2h−1 × K and trilinearly interpolated to P ×2h−1 × to patch-wise class embedding original resolution to get yf ∈ RX×Y ×Z×K.\n\nP ×2h−1 ×\n\nX\n\nZ\n\nY\n\nWe use a skip connection here to ’remind’ the model of the input x. By adding a small number of parameters, the skip connection effectively enhances our performance. Concretely, we concatenate x and yf and feed them through a linear layer to predict the output ˆysem ∈ RX×Y ×Z×K = Linear(Concatenate(yf , x)) The model is trained using cross entropy loss LCE between ˆysem and ground truth ysem.\n\n4\n\n2D TO 3D DISTILLATION\n\nFigure 2: Overview of our proposed 2D-to-3D panoramic knowledge distillation. The panorama and 3D volume are fed into the 2D Segmentor and 3D Segmentor in parallel, and predict the semantic distributions in 2D and 3D respectively. Then we apply differentiable rendering on the predicted 3D semantic volume to obtain its panoramic projection. Finally, the knowledge is distilled from 2D to 3D projection using a Kullback–Leibler divergence loss.\n\nAs shown in Figure 2, We use pairs of panoramic renderings Ir and 3D rooms r = [rtsdf , rrgb] ∈ RX ′×Y ′×Z′×4 as our input for 2D-to-3D distillation.\n\nThe 2D teacher is pretrained on large scale 2D repository and finetuned on 2D panoramas Ir and 2D semantic ground truth rendered from 3D rooms. The 3D Segmenter, as the student model in distillation, is first trained to converge using fix-sized block data. Then, the knowledge distillation from 2D teacher to 3D student is performed on the scene level.\n\nSuppose we have a input room r = [rtsdf , rrgb] ∈ RX ′×Y ′×Z′×4, we feed it to the student model and get voxel-level semantic segmentation ˆyr ∈ RX ′×Y ′×Z′×K. The 2D panoramic image could be acquired from panoramic camera placed at any position that is not occupied by objects. Here, we place it at the center of the room for sake of simplification. Combining 3D semantic segmentation prediction ˆyr and input geometry rtsdf , a mapping between pixels and voxels is required to project ˆyr to 2D. The mapping is determined by raycasting. For each pixel in the image, we construct a ray\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nfrom the view and march along the ray through rtsdf . Trilinear interpolation is used to determine TSDF values along the ray. The surface voxel is located when zero-crossing is detected. In this way, we establish a mapping between a pixel and a voxel. Through this mapping, 6 squared semantic segmentation maps are rendered from +X, −X, +Y , −Y , +Z, −Z directions with 90◦ field of view. The 6 views are then stitched into a panorama through pixel mapping and interpolation. The total process is differentiable, making it possible for gradient backpropagates from 2D to 3D. We project a panoramic view of ˆyr.\n\nStudentSegM ap = P anoramicP rojecting(rtsdf , ˆyr)\n\n(1)\n\nwhere StudentSegM ap ∈ RH×W ×K, H and W is the height and width of rendered panorama. After forwarding Ir through the teacher model, we receive a T eacherSegM ap ∈ RH×W ×K which contains the teacher model’s learned knowledge. We follow (Hinton et al., 2015) to formulate a Kullback–Leibler divergence loss to measure the difference of distribution between StudentSegM ap and T eacherSegM ap. We have\n\nLDist = τ 2KLDiv(StudentSegM ap/τ, T eacherSegM ap/τ )\n\n(2)\n\nWhere τ refers to distillation temperature. The model is trained using the sum of 2D-to-3D distillation loss LDist and 3D cross-entropy loss LCE.\n\n5 PANOROOMS3D DATASET\n\nFigure 3: Sample of PanoRooms3D: a pair of input scene and its panoramic rendering.\n\nTo facilitate the knowledge distillation from the advanced 2D segmentation models to 3D models, we need a large-scale dataset of 3D scans with their corresponding panorama images. Existing datasets such as ScanNet (Dai et al., 2017) and Matterport3D (Chang et al., 2017) usually require intensive annotation effort, contain noisy labels, and do not cover complete scene geometry, while SUNCG (Song et al., 2017b) does have clean annotation but is limited to narrow field-of-view RGBD images. To this end, we propose a novel large TSDF-based semantic segmentation dataset called PanoRooms3D.\n\nTo achieve a human-like understanding of the 3D environment, we use professional designed room layouts from 3D-FRONT (Fu et al., 2021a), furnished by 3D-FUTURE(Fu et al., 2021b), a largescale 3D CAD shapes with high-resolution informative textures.\n\nTo diversify the generation, we augment each scene with randomly sampled floor/wall textures. To obtain the TSDF representation together with semantic labels, we randomly sample camera poses inside the rooms with a height between 1.4m and 1.8m. This process simulates users holding RGBD cameras to capture indoor scenes like (Chang et al., 2017; Dai et al., 2017; Hua et al., 2016; Armeni et al., 2017). RGB-D plus semantic frames are rendered at all camera poses and later fused into 3D scenes and semantic ground truth using TSDF fusion. A total of 2,509,873 RGB-D images and corresponding pixel-wise semantic segmentation maps are rendered. We set each voxel a 4cm cube. We use 120 NVIDIA Tesla V100 GPU to render for 48 hours. 3D-FRONT includes 99 semantic categories. In practice, considering the semantical consistency between labels, we consolidate similar labels, merging the original 99 classes into 30 hyper classes. Since many scenes contain furniture that is left unlabeled, we manually filter out noisy rooms. 5917 TSDF-based living rooms and bedrooms are created. We split 5325 rooms for training, 297 for validation, and 295 for\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nInput Scene\n\n3D U-net\n\n3DSeg-L\n\nGround Truth\n\nFigure 4: Qualitative comparison between our variant 3DSeg-L and convolutional baseline 3D U-Net(C ̧ ic ̧ek et al., 2016) on our PanoRooms3D dataset(top two rows) and Scannet(Dai et al., 2017)(bottom two rows)\n\ntesting. For the convenience of window partition, we pad the all spatial dimensions of scenes to the multiples of 128.\n\nTo get panoramic images for the training of 2D teachers, we place a camera at the center of each room. The camera renders 6 square images from +X, −X, +Y , −Y , +Z, −Z directions with a field of view of 90◦, which constitute a skybox. The skybox is then converted to an equirectangular panorama through pixel mapping and bilinear interpolation. A sample of PanoRooms3D is visualized in Figure 3.\n\n6 EXPERIMENTS\n\n6.1\n\nIMPLEMENTATION DETAIL\n\nIn our training of 3D Segmenters on our PanoRooms3D dataset, we slice the 5325 rooms for training into 128×128×128 cubes as the training set. The patch size P is set to 4, resulting 128 4 = 32768 tokens at the beginning. The number of Swin Transformer blocks h is set to 2, with h − 1 = 1 patch merging layer in between. The decoder receives N = 32768 (2h−1)3 = 4096 tokens from encoder. The number of class K is 30. We change the number of layers and number of MSA heads within each Swin Transformer block and embedding dimension D to create different sizes of variants. Variants of encoder architecture include:\n\n4 × 128\n\n4 × 128\n\n• 3DSeg-T(iny), D = 48, Swin Layers [2, 2], Num Heads [3, 3], #PARAM 0.38M\n\n• 3DSeg-B(ase), D = 48, Swin Layers [2, 4], Num Heads [3, 3], #PARAM 0.62M\n\n• 3DSeg-M(iddle), D = 64, Swin Layers [3, 6], Num Heads [4, 4], #PARAM 1.55M\n\n• 3DSeg-L(arge), D = 96, Swin Layers [4, 8], Num Heads [6, 6], #PARAM 4.43M\n\nWe train our 3D Segmenter on NVIDIA Ampere A100 80GB, using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 0.001 and batch size of 8. It takes 48 hours for our models to converge.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFor 2D to 3D knowledge distillation, we operate the experiments taking room as a unit. we render the 3D rooms for train/val/test sets to get 2D panoramas and 2D ground truth segmentation for train/val/test sets respectively. The 2D training set is used to finetune the 2D teacher model. The size of rendered panorama is set to (H, W ) = (512, 1024). We manually filter out noisy renderings in 5323 training rooms caused by objects that block the view. We use the 5175 panoramas after filtration paired up with their original 3D rooms to do distillation.\n\nWe choose 2D Segmenter(Strudel et al., 2021) Seg-B-Mask/16 and Seg-L-Mask/16 as the 2D teacher models. Pretrained on ImageNet-21K(Steiner et al., 2021), the models are finetuned on our rendered 2D panoramas training set for 200 epochs.\n\nBecause the scale of rooms varies, we train/validate the distillation with a batch size of 1. We train the distillation for 45000 iterations and validate every 1500 iterations. The distillation temperature τ is set to 20 in all of our experiments.\n\nApart from our PanoRoom3D dataset, we also include the Scannet (Dai et al., 2017) for baseline comparison. We random split the 1513 rooms into train/val/test 1361/77/75 respectively. All rooms are fused with a voxel resolution of 4cm.All spatial dimensions of scenes are padded to the multiples of 128. For training, the rooms are sliced into 128 × 128 × 128 cubes.\n\n6.2 BASELINE COMPARISON\n\n3D U-Net Sparseconv VT-UNet 3DSeg-T 3DSeg-T Dist 3DSeg-B 3DSeg-B Dist 3DSeg-M 3DSeg-M Dist 3DSeg-L 3DSeg-L Dist\n\naAcc↑ 98.98 98.50 98.74 98.83 98.82 98.88 98.87 98.93 98.92 98.95 98.95\n\nIoU↑ Acc↑ Dice↑ 76.88 65.73 58.54 54.52 65.27 47.33 54.75 71.87 47.90 64.34 73.60 56.69 65.89 73.65 58.06 68.28 75.81 60.41 69.69 75.21 61.58 70.55 76.11 62.57 71.20 75.94 63.14 72.32 76.48 64.23 72.84 64.67 76.83\n\nFscore↑ 80.89 72.00 75.58 78.38 78.22 80.05 80.18 80.54 80.41 80.98 81.08\n\nPrecision↑ Recall↑ 76.88 65.27 71.87 73.60 73.65 75.81 75.21 76.11 75.94 76.48 76.83\n\n65.20 63.13 59.46 70.56 72.03 73.62 75.88 76.47 77.57 78.43 78.86\n\n#Param↓ 16.32M 0.64M 20.89M 0.38M 0.38M 0.62M 0.62M 1.55M 1.55M 4.43M 4.43M\n\nTable 1: Quantitative comparison of our proposed 3D Segmenter with FCN baseline 3D UNet(C ̧ ic ̧ek et al., 2016), SparseConvNet(Graham et al., 2018a) and VT-UNet(Peiris et al., 2022) on our PanoRoom3D dataset. All of the metrics are reported in percentage.\n\naAcc↑ 77.73 3D U-Net 72.33 3DSeg-T 3DSeg-B 74.37 3DSeg-M 76.91 77.45 3DSeg-L\n\nIoU↑ Acc↑ Dice↑ 63.64 40.01 33.71 36.04 52.22 28.96 40.30 55.80 32.83 45.33 59.39 37.65 47.42 39.83 61.07\n\nFscore↑ 67.82 59.54 62.34 65.72 66.83\n\nPrecision↑ Recall↑ 63.64 52.22 55.80 59.39 61.07\n\n44.33 44.98 48.27 53.13 54.47\n\n#Param↓ 16.32M 0.38M 0.62M 1.55M 4.44M\n\nTable 2: Quantitative comparison of our proposed 3D Segmenter with FCN baseline 3D UNet(C ̧ ic ̧ek et al., 2016) on Scannet(Dai et al., 2017) dataset. All of the metrics are reported in percentage.\n\nAll of the quantitative metrics are run on a room-level. Metrics evaluated from different rooms are weighted with the corresponding rooms’ volume. Here we compare our four variants and four distilled models with Fully Convolutional Network 3D U-Net(C ̧ ic ̧ek et al., 2016), sparse convolution(Graham et al., 2018a) based model and recent transformer-based model VT-UNet(Peiris et al., 2022) . We Report 7 metrics used in semantic segmentation: overall accuracy(aAcc), mean intersection over union(mIoU), mean accuracy(Acc), Dice, Fscore, Precision, Recall. Among these metrics, mIoU is the most representative one. 3DSeg-T Dist distills knowledge from 2D Segmenter (Strudel et al., 2021) Seg-B-Mask/16. 3DSeg-B Dist, 3DSeg-M Dist and 3D Seg-L Dist distill knowledge from Seg-L-Mask/16. See the quantitative comparison in table 1. 3DSeg-B already outperforms 3D\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nU-Net even with only 3.8% parameters. The distillation schemes achieve a higher mIoU score without adding extra parameters. This implies that our 2D-to-3D panoramic distillation demonstrates the feasibility of training powerful 3D networks with 2D knowledge. Our baseline comparison on Scannet (Dai et al., 2017) is shown in table 2. The result shows that our proposed 3D Segmenter also achieves the best performance on real world data.\n\nWe also conduct a complexity comparison of our four baseline variants and 3D U-Net. We randomly choose 20 scenes to calculate the total inference time. Since some of the scenes are too large to be run on GPU for 3D U-Net, we run our inference time comparison on CPU. The total inference time of our four variants is 43.62s, 44.79s, 61.08s, 95.89s from Tiny to Large, and 254.75s for 3D U-Net.\n\n6.3 ABLATION STUDY\n\nThe effectiveness of 2D-to-3D distillation. Our 3D Segmenter is pretrained on fixed-size blocks and then distilled at room level. To control variables and validate the effectiveness of our proposed distillation strategy, we conduct two experiments by finetuning 3DSeg-T and 3DSeg-B at room level with 3D cross-entropy loss only.\n\n3DSeg-T 3DSeg-T Finetune 3DSeg-T Dist 3DSeg-B 3DSeg-B Finetune 3DSeg-B Dist\n\naAcc↑ 98.83 98.85 98.82 98.88 98.90 98.87\n\nIoU↑ Acc↑ Dice↑ 64.34 73.60 56.69 74.58 64.17 56.62 65.89 58.06 73.65 68.28 75.81 60.41 76.15 68.79 60.87 69.69 61.58 75.21\n\nFscore↑ 78.38 78.72 78.22 80.05 80.09 80.18\n\nPrecision↑ Recall↑ 73.60 74.58 73.65 75.81 76.15 75.21\n\n70.56 69.31 72.03 73.62 73.95 75.88\n\n#Param↓ 0.38M 0.38M 0.38M 0.62M 0.62M 0.62M\n\nTable 3: Quantitative ablation study of distillation effectiveness. All of the metrics are reported in percentage.\n\nWe can see from Table 3 that compared with directly finetune the model on rooms, our panoramic distillation design can steadily improve the mIoU performance.\n\nThe necessity of skip connection. While introducing the decoder of 3D Segmenter, we use a skip connection to ’remind’ our model of the input x. We validate the necessity of the design in this experiment.\n\n3DSeg-T 3DSeg-T w/o skip 3DSeg-B 3DSeg-B w/o skip 3DSeg-M 3DSeg-M w/o skip\n\naAcc↑ 98.83 97.79 98.88 97.86 98.93 97.92\n\nIoU↑ Acc↑ Dice↑ 64.34 73.60 56.69 60.35 68.81 51.35 68.28 75.81 60.41 63.81 69.18 54.44 70.55 76.11 62.57 65.66 70.72 56.20\n\nFscore↑ 78.38 73.0 80.05 74.12 80.54 74.87\n\nPrecision↑ Recall↑ 73.60 68.81 75.81 69.18 76.11 70.72\n\n70.56 66.06 73.62 70.10 76.47 71.23\n\n#Param↓ 0.38M 0.37M 0.62M 0.62M 1.55M 1.55M\n\nTable 4: Quantitative ablation study of the skip connection. All of the metrics are reported in percentage.\n\nWe can see in Table 4 by adding a skip connection before output significantly improves the overall performance by adding a small number of parameters.\n\n7 DISCUSSIONS\n\nIn this work, we propose the first 2D-to-3D knowledge distillation method to utilize the data abundant and pretrain-ready 2D semantic segmentation to improve 3D semantic segmentation. Experiments demonstrate our technique significantly improves the 3D model over the baseline. We have also introduced a PanoRooms3D dataset that contains a large variety of indoor scenes with dense 3D volumes and their corresponding panorama renderings.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work is supported by JST Moonshot R&D Grant Number JPMJMS2011 and JST ACT-X Grant Number JPMJAX190D, Japan and partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100)\n\nREFERENCES\n\nSungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information distillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9163–9171, 2019. 3\n\nStefan Ainetter and Friedrich Fraundorfer. End-to-end trainable deep neural network for robotic grasp detection and semantic segmentation from rgb. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13452–13458. IEEE, 2021. 1\n\nAlaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. Advances in neural information processing systems, 34:20014–20027, 2021. 3\n\nIro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor\n\nscene understanding. arXiv preprint arXiv:1702.01105, 2017. 6\n\nVijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481–2495, 2017. 2\n\nCristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression.\n\nIn Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535–541, 2006. 3\n\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV), 2017. 6\n\nChun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 357–366, 2021. 3\n\nChristopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3075–3084, 2019. 1, 3\n\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34:9355–9366, 2021. 3\n\n ̈Ozg ̈un C ̧ ic ̧ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d unet: learning dense volumetric segmentation from sparse annotation. In International conference on medical image computing and computer-assisted intervention, pp. 424–432. Springer, 2016. 1, 3, 7, 8, 15\n\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban In Proceedings of the IEEE conference on computer vision and pattern scene understanding. recognition, pp. 3213–3223, 2016. 1\n\nBrian Curless and Marc Levoy. A volumetric method for building complex models from range In Proceedings of the 23rd annual conference on Computer graphics and interactive\n\nimages. techniques, pp. 303–312, 1996. 4\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAngela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias In Proc. Computer\n\nNießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. Vision and Pattern Recognition (CVPR), IEEE, 2017. 6, 7, 8, 9\n\nAngela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J ̈urgen Sturm, and Matthias Nießner. Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4578–4587, 2018. 3\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2, 3\n\nJiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msgtransformer: Exchanging local spatial information by manipulating messenger tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12063– 12072, 2022. 3\n\nHuan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10933– 10942, 2021a. 6\n\nHuan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, pp. 1–25, 2021b. 6\n\nJun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3146–3154, 2019. 2\n\nYixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rJlnOhVYPS. 3\n\nBenjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with\n\nsubmanifold sparse convolutional networks. CVPR, 2018a. 8\n\nBenjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9224–9232, 2018b. 1\n\nBenjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv ́e J ́egou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12259–12269, 2021. 3\n\nYushuo Guan, Pengyu Zhao, Bingxuan Wang, Yuanxing Zhang, Cong Yao, Kaigui Bian, and Jian Tang. Differentiable feature aggregation search for knowledge distillation. In European Conference on Computer Vision, pp. 469–484. Springer, 2020. 3\n\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908–15919, 2021. 3\n\nLei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2940–2949, 2020. 1, 3\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 2\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nByeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3779–3787, 2019. 3\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2(7), 2015. 3, 6\n\nWenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, and Tien-Tsin Wong. Bidirectional projection network for cross dimension scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14373–14382, 2021. 3\n\nBinh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and SaiIn 2016 fourth international\n\nKit Yeung. Scenenn: A scene meshes dataset with annotations. conference on 3D vision (3DV), pp. 92–101. Ieee, 2016. 6\n\nJingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser, Matthias Nießner, and Leonidas J Guibas. Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4440–4449, 2019. 15\n\nJangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network compres-\n\nsion via factor transfer. Advances in neural information processing systems, 31, 2018. 3\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014. 7\n\nYandong Li, Di Huang, Danfeng Qin, Liqiang Wang, and Boqing Gong. Improving object detection In European Conference on Computer Vision, pp.\n\nwith selective self-supervised self-training. 589–607. Springer, 2020. 3\n\nYawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality\n\nto vision transformers. arXiv preprint arXiv:2104.05707, 2021. 3\n\nGuosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement netIn Proceedings of the IEEE conference on\n\nworks for high-resolution semantic segmentation. computer vision and pattern recognition, pp. 1925–1934, 2017. 2\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021. 3, 4\n\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3202–3211, 2022a. 2, 4\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976–11986, 2022b. 2\n\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431–3440, 2015. 1, 2, 3\n\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 5191–5198, 2020. 3\n\nPushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support\n\ninference from rgbd images. In ECCV, 2012. 1\n\nAlexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3D: Outof-Context Data Augmentation for 3D Scenes. In International Conference on 3D Vision (3DV), 2021. 1\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nHimashi Peiris, Munawar Hayat, Zhaolin Chen, Gary Egan, and Mehrtash Harandi. A robust volumetric transformer for accurate 3d tumor segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 162–172. Springer, 2022. 8\n\nHieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11557–11568, 2021. 3\n\nTobias Pohlen, Alexander Hermans, Markus Mathias, and Bastian Leibe. Full-resolution residual networks for semantic segmentation in street scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4151–4160, 2017. 2\n\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660, 2017a. 1, 3\n\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017b. 3\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and\n\nYoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 3\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015. 1, 2, 3\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014. 2\n\nShuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition, 2017a. 3\n\nShuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition, 2017b. 6\n\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021. 8\n\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7262–7272, 2021. 2, 8\n\nHugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc ̧ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6411–6420, 2019. 1, 3\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021. 2, 3\n\nFrederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the\n\nIEEE/CVF International Conference on Computer Vision, pp. 1365–1374, 2019. 3\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2, 3\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 568–578, 2021. 2, 3\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nGuile Wu and Shaogang Gong. Peer collaborative learning for online knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10302–10310, 2021. 3\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22–31, 2021. 3\n\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European conference on computer vision (ECCV), pp. 418–434, 2018. 3\n\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077–12090, 2021. 2\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687–10698, 2020. 3\n\nWeijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9981– 9990, 2021. 3\n\nChenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-student optimization in one generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2859–2868, 2019. 3\n\nMinghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In European Conference on Computer Vision, pp. 191–207. Springer, 2020. 2\n\nChangqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12416–12425, 2020. 2\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 558–567, 2021. 3\n\nYuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet:\n\nObject context network for scene parsing. arXiv preprint arXiv:1809.00916, 2018. 2\n\nSergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. 3\n\nYoucai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, and Yichen Wei. Prime-aware adaptive distillation. In European Conference on Computer Vision, pp. 658–674. Springer, 2020. 3\n\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881–2890, 2017. 2\n\nHengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In Proceedings of the European conference on computer vision (ECCV), pp. 267–283, 2018. 2\n\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. 2\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633–641, 2017. 1\n\nA APPENDIX\n\nLimitations. However, a few limitations are yet to be addressed. First, though the panorama images could provide the global context of the environment, the single-camera rendering inevitably suffers from occlusion, a promising direction is to directly leverage the full 2D textures from the surface of the 3D scans, as done in TextureNet (Huang et al., 2019). Second, currently, we do not consider the similarity between different models, we leave this as future work.\n\nInput Scene\n\n3D U-net\n\n3DSeg-L\n\nGround Truth\n\nFigure 5: Qualitative comparison between our variant 3DSeg-L and convolutional baseline 3D UNet(C ̧ ic ̧ek et al., 2016) on our PanoRooms3D dataset.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper presents a novel method to perform knowledge distillation between a parent model that is only trained on 2d data and a child model trained on 3d data. To my knowledge this is the first paper to show that it is possible to distill knowledge between models that accept different inputs. This in itself is a novel and interesting contribution.  The model that they construct with this approach outperforms a baseline model while using only 3.8% as many parameters.\n\n# Strength And Weaknesses\n\nStrengths:\n* Proves that knowledge distillation is possible between two models that accept different inputs\n* Uses this method to produce a high-quality model that performed as-good or better than one baseline model with a fraction of the parameters and runtime.\n* Introduced a new dataset benchmark to encourage further development of this task.\nWeaknesses:\n* Only compares to one older (2016) baseline method when there are several more recent works that include code that could be used.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe quality of this work is very good and the work is very novel. Just the insight that knowledge distillation can be applied to models with different inputs is significant. The authors have agreed to release code and data upon acceptance so hopefully reproducing the results will be trivial, but the writing is clear enough that for a researcher with a background in 3d geometrical reasoning should be able to reproduce independently. \n\nThe main drawback is that there is not enough comparison to state of the art 3d semantic segmentation methods.  I would recommend looking at the Scannet benchmark challenge and at minimum finding some of the top entries with code and adding them to the papers comparisons.\n\n# Summary Of The Review\n\nThis is a novel approach for knowledge distillation between a 2d model and a 3d model for 3d semantic segmentation. The paper presents a new dataset to support this problem and presents nominal results showing that the approach is effective. Unfortunately there is not enough comparison to state of the art methods.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLOTTERY AWARE SPARSITY HUNTING: ENABLING FEDERATED LEARNING ON RESOURCE-LIMITED EDGE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLimited computation and communication capabilities of clients pose significant challenges in federated learning (FL) over resource-limited edge nodes. A potential solution to this problem is to deploy off-the-shelf sparse learning algorithms that train a binary sparse mask on each client with the expectation of training a consistent sparse server mask yielding sparse weight tensors. However, as we investigate in this paper, such naive deployments result in a significant drop in accuracy compared to FL with dense models, especially for clients with limited resource budgets. In particular, our investigations reveal a serious lack of consensus among the trained sparsity masks on clients, which prevents convergence for the server mask and potentially leads to a substantial drop in model performance. Based on such key observations, we propose federated lottery aware sparsity hunting (FLASH), a unified sparse learning framework to make the server win a lottery in terms of yielding a sparse sub-model, able to maintain classification performance under highly resource-limited client settings. Moreover, to support FL on different devices requiring different parameter density, we leverage our findings to present heteroFLASH, where clients can have different target sparsity budgets based on their device resource limits. Experimental evaluations with multiple models on various datasets (both IID and non-IID) show superiority of our models in closing the gap with unpruned baseline while yielding up to ∼10.1% improved accuracy with ∼10.26× fewer communication costs, compared to existing alternatives, at similar hyperparameter settings. Code is released as Supplementary.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) McMahan et al. (2017) is a popular form of distributed training, which has gained significant traction due to its ability to allow multiple clients to learn a shared global model without the requirement to transfer their private data. However, clients’ heterogeneity and resource limitations pose significant challenges for FL deployment over edge nodes, including mobile phones and IoT devices. To resolve these issues, various methods have been proposed over the past few years including efficient learning for heterogeneous collaborative training Lin et al. (2020); Zhu et al. (2021), distillation He et al. (2020), federated dropout techniques Horvath et al. (2021); Caldas et al. (2018b), efficient aggregation for faster convergence and reduced communication Reddi et al. (2020); Li et al. (2020b). However, these methods do not necessarily address the growing concerns of highly computation and communication limited edge.\n\nMeanwhile, reducing the memory, compute, and latency costs for deep neural networks (DNNs) in centralized training for their efficient edge deployment has also become an active area of research. In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and compute cost Qiu et al. (2021); Raihan & Aamodt (2020), while creating a model to meet a target parameter density denoted as d, and is able to yield accuracy close to that of the unpruned baseline.\n\nHowever, the challenges and opportunities of sparse learning in FL is yet to be fully unveiled. Only very recently, few works Bibikar et al. (2021); Huang et al. (2022) have tried to leverage sparse learning in FL primarily to show their efficacy in non-IID settings. Nevertheless, these works primarily used sparsity for non-aggressive model compression,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nlimiting the actual benefits of sparse learning, and required multiple local epochs, that may further increase the training time for stragglers making the overall FL process inefficient Zhang et al. (2021). Moreover, the server-side pruning used in these methods may not necessarily adhere to the layers’ pruning sensitivity1 that often plays a crucial role in sparse model performance Kundu et al. (2021b); Zhang et al. (2018). Another recent work, ZeroFL Qiu et al. (2021), has explored deploying sparse learning in FL settings. However, Qiu et al. (2021) could not leverage any advantage of model sparsity in the clients’ communication cost and had to keep significantly more parameters active compared to a target d to yield good accuracy. Moreover, as shown in Fig. 1(b), for d = 0.05, ZeroFL still suffers from substantial accuracy drop of ∼14% compared to the baseline.\n\nFigure 1: Comparison of (a) accuracy at different communication budget with, ZeroFL Qiu et al. (2021) and FedAvg. (w/ d = 1.0) (b) Accuracy vs. parameter density of each client. Proposed approaches can significantly outperform the existing alternative Qiu et al. (2021) at ultra-low target parameter density (d).\n\nOur Contributions. Our contribution is fourfold. In view of the above limitations, we first identify crucial differences between a centralized and the corresponding FL model, in learning the sparse masks for each layer. In particular, we observe that in FL, the server model fails to yield convergent sparse masks, primarily due to the lack of consensus among clients’ later layers’ masks. In contrast, the centralized model show significantly higher convergence trend in learning sparse masks for all layers. We then experimentally demonstrate the utility of pruning sensitivity and mask convergence in achieving good accuracy setting the platform to close the performance gap in sparse FL.\n\nWe then leverage our findings and present federated lottery aware sparsity hunting (FLASH), a sparse FL methodology addressing the aforementioned limitations in a unified manner. At the core, FLASH leverages a two-stage FL, a robust and low-cost layer sensitivity evaluation stage and a FL training stage. In particular, the disentangling of the layer sensitivity evaluation from sparse weight training allows us to either choose to train a sparse mask or freeze a sensitivity driven pre-defined mask. This can further translate to a proportional communication saving.\n\nTo deal with the heterogeneity in clients’ compute-budget, we further extend our methodologies to hetero-FLASH, where individual clients can support different density based on their resources. Here, to deal with the unique problem of the server selecting different sparse models for clients, we present server-side gradual mask sub-sampling, that identifies sparse masks via a form of layer sensitivity re-calibration, starting for models with highest to that with lowest density support.\n\nWe conduct experiments on MNIST, FEMNIST, and CIFAR-10 with different models for both IID and non-IID client data partitioning. Experimental results show that, compared to the existing alternative Qiu et al. (2021), at iso-hyperparameter settings, FLASH can yield up to ∼8.9% and ∼10.1%, on IID and non-IID data settings, respectively, with reduced communication of up to ∼10.2×.\n\n2 RELATED WORKS\n\nModel Pruning. Over the past few years, a plethora of research has been done to perform efficient model compression via pruning, particularly in centralized training Ma et al. (2021); Frankle & Carbin (2018); Liu et al. (2021); You et al. (2019); He et al. (2018). Pruning essentially identifies and removes the unimportant parameters to yield compute-efficient inference models. More recently, sparse learning Evci et al. (2020); Kundu et al. (2021b); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020); Kundu et al. (2020; 2019), a popular form of model pruning, has gained significant traction as it can yield FLOPs advantage even during training. In particular, it ensures only d% of the model parameters remain non-zero during the training for a target parameter density d (d < 1.0 and sparsity is 100 − d%), potentially enabling training compute and comm. cost if deployed for FL.\n\nDynamic network rewiring (DNR). We leverage DNR Kundu et al. (2021b), to sparsely learn the sparsity mask of each client. In DNR, a model starts with randomly initiated mask following the\n\n1We measure layer importance via the proxy of sensitivity. A layer with higher sensitivity demands higher %\n\nof non-zero weights compared to a less sensitive layer.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ntarget parameter density d. After an epoch, the client evenly prunes the lowest pr% weights from each layer based on absolute magnitude, where pr is prune rate. Note, this pr% pruning happens on top of the sparse model with density d, allowing pr% weights to be regrown. DNR then ranks each layer based on the normalized contribution of the summed non-zero weight magnitudes. Finally, the client regrows total pr% weights in a non-uniform way, allowing more regrowth to the layers having higher rank. This process iteratively repeats over epochs to finally learn the mask.\n\nFederated learning for resource and communication limited edge. To address device heterogeneity, existing works have explored the idea of heterogeneous training Horvath et al. (2021); Diao et al. (2020); Yao et al. (2021) allowing different clients to train on different fractions of full-model based on their compute-budget. On a parallel track, various optimizations are proposed in FL training framework to accelerate convergence, thus requiring fewer communication rounds Han et al. (2020); Gorbunov et al. (2021); Zhang et al. (2013); Li et al. (2019); Reddi et al. (2020); Islamov et al. (2021); Albasyoni et al. (2020).\n\nTo address the issue of client resource limitations, a few research have leveraged pruning in FL Li et al. (2020a); Jiang et al. (2022); Li et al. (2021). In particular, LotteryFL Li et al. (2020a) trained each client to have their personalized mask with which they are able to perform well only on their own data. Moreover, the clients often need to send full model costing bandwidth. PruneFL Jiang et al. (2022) also asks for significant communication costs as it demands participating clients to send all the gradient values to the server while updating the masks.\n\nOnly a few contemporary works Huang et al. (2022); Bibikar et al. (2021); Qiu et al. (2021) tried to leverage the benefits of sparse learning in federated settings. In particular, Huang et al. (2022) relied on a randomly initialized sparse mask, and recommended keeping it frozen throughout the training, yet failed to provide any supporting intuition. FedDST Bibikar et al. (2021), on the other hand, leveraged the idea of RigL Evci et al. (2020) to perform sparse learning of the clients, relied on a large number of local epochs to avoid gradient noise, and focused primarily on only highly non-IID data without targeting ultra-low density d. More importantly, neither of these works investigated the key differences between centralized and FL sparse learning. With similar philosophy as ours, ZeroFL Qiu et al. (2021) first identified a key aspect of sparse learning in FL in terms of all clients’ masks to be within 30% of the total model weights to yield good accuracy at high compression. However, ZeroFL suffered significantly in exploiting a proportional advantage in communication saving as even for low parameter density d, all clients had to download the dense model and send back a 3× denser model. Furthermore, these algorithms sacrifice significant accuracy at ultra-low d.\n\n3 REVISITING SPARSE LEARNING: WHY DOES IT MISS THE MARK IN FL?\n\nTable 1: FL training settings considered in this work.\n\nDataset\n\nModel\n\n#Params.\n\nDatapartioning\n\nRounds Clients Clients/Round Optimizer\n\n(T )\n\n(CN )\n\n(cr , cd)\n\nAggregation type\n\nLocal Batch epoch (E) size\n\nMNIST CIFAR-10 FEMNIST Same as Caldas et al. (2018a) 6.6M Reddi et al. (2020) 1000 3400\n\nMNISTNet ResNet18\n\n262K 11.2M\n\n400 600\n\nLDA\n\n100\n\n10, 10\n\n34, 34\n\nSGD\n\nFedAvg McMahan et al. (2017)\n\n1\n\n32 32 16\n\nNote, centralized training has shown significant benefits with sparse learning in FLOPs reduction during forward operations Evci et al. (2020), and potential training speed-up of up to 3.3× Qiu et al. (2021) while maintaining close to the baseline accuracy, even at d ≤ 0.1. We now use a sparse learning, namely Kundu et al. (2021b), in FL settings (refer to Table 1 for details) on CIFAR-10, where each client separately performs Kundu et al. (2021b) to train a sparse server-side ResNet18 and meet a fixed parameter density d, starting from a random sparse mask. After sending the updates to server, it aggregates them using FedAvg. We term this as naive sparse training (NST). Note, due to lack of knowledge about the pruning sensitivity for each layer, the server fails to sub-sample from the aggregated weights to meet target non-zero parameter density d. Thus, the down-link communication cost is higher as generally the aggregated non-zero parameter density is > d.\n\nObservation 1. At high compression d ≤ 0.1, the collaboratively learned FL model significantly sacrifices performance, while the centralized sparse learning yields close to baseline performance.\n\nAs shown in Fig.2(a), naive deployment of sparse learning significantly sacrifices accuracy in FL. In particular, for d = 0.1, the trained server-side model suffers an accuracy drop of 3.67%. At even lower d = 0.05, this drop significantly increases to 12.03%, hinting at serious limitations of sparse\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: (a-b) Accuracy vs. round plot on deployment of off-the-shelf sparse learning in FL for different d, (c-d) visualization of the Model’s SM in terms of Jaccard distance while training with sparse learning for (c) centralized and (d) FL, respectively.\n\nlearning in FL. However, an off-the-shelf centralized sparse learning can yield model having close to the baseline accuracy, even at d = 0.05.\n\nObservation 2. As the training progresses, the sparse masks in centralized training tend to agree across epochs, showing convergence, while the server mask in FL does lack agreement across rounds.\n\nDefinition 1. Sparse mask mismatch. For a model at round t, we define the sparse mask mismatch (SM) smt as the Jaccard distance that is measured as follows.\n\nsmt = 1 −\n\n((cid:80)L ((cid:80)L\n\nl=1 Mt l=1 Mt\n\nl ∩ Mt−1 l ∪ Mt−1\n\nl\n\nl\n\n)\n\n)\n\n(1)\n\nwhere Mt l represents the sparse mask tensor for layer l at the end of round t. Interestingly, as depicted in Fig. 2(a), the SM for centralized learning tends to zero as the training progresses. In contrast, with the same model, dataset and d values, in FL, the SM remains > 0.4 indicating a substantial distinction in the sparse mask learning between centralized and federated learning.\n\nObservation 3. At low target density, in federated sparse learning, throughout the training rounds, the disagreement on the later layer’s masks remains more severe than the earlier ones.\n\nAs the training progresses, in centralized learning, mask for each layer shows significant convergence trend as measured by SM for the layer (Fig. 3(a)). However, Fig. 3(b) shows in FL, the later layers’ masks differ significantly and continue to disagree over rounds with SM value as high as ∼0.8. This may be attributed to many possible mask choices due to the later layers’ significantly fewer non-zero parameter allocation, compared to the initial layers, driven by their respective pruning sensitivities. For example, layer 1 requires 90% parameters to be present, compared to only 5% for layer 14, with the later costing an SM of ∼0.73.\n\nUse sens- Masks Layer Test change at SM acc% 89.72 –\n– 91.66 layer 9-16 0.8 88.88 layer 1-16 0.5 84.62 layer 1-16 0.8 82.32\n\nTable 2: Performance based on the different levels of mask disagreement in centralized.\n\nTraining type Pre-defined w/ mask frozen Pre-defined w/ mask frozen\n\nitivity N\nY Y\nY Y\n\nw/o mask frozen\n\n– –\n\nTo further investigate the impact of higher SM and layer sensitivity on a model’s accuracy, we performed five different training in centralized as described in Table 2. In particular, for the training in row1 we randomly generate sparse masks with uniform density for all the layers. For all other training, we first randomly create each layer’s mask by following its pruning sensitivity2 and then decide to keep the layer mask frozen for some or all the layers. For training described in rows 3-5, we allow a fraction of the mentioned layers’ masks to differ between consecutive epochs such that they meet the target SM value, creating the situation of non-convergent masks. As Table 2 clearly shows that large SM for the layers can degrade the accuracy by up to 9.34%, we can safely conclude that disagreement of masks across epochs can significantly affect the model’s final performance. Moreover, the model trained via sparse learning with sensitivity-driven pre-defined masks yields better performance than the one trained with uniform density sparse mask.\n\n4 FLASH: METHODOLOGY\n\nTo win a lottery of having a sparse network yielding high accuracy at reduced parameters, we identify two key characteristics of sparse learning, namely, the pruning sensitivity and mask learnability towards convergence. To explicitly adhere to these two important aspects, in FLASH, we present a\n\n2For a sparse model it is evaluated as the ratio # of non-zero layer parameters another pre-trained model of the same architecture and target d for this evaluation.\n\n# layer parameters\n\nDing et al. (2019). We use\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Layer-wise sparse mask mismatch (SM) vs. training epochs (rounds) plot for (a) centralized and (b) FL, respectively. In FL, the layer layers continue to have higher SM contrary to centralized, where every layer tend to reduce the SM as the training matures.\n\ntwo-stage sparse FL method, stage 1: targeting sensitivity analysis to identify good initial sparse mask for each layer, stage 2: targeting training to learn masks and weights. In particular, to evaluate layer sensitivity in stage 1, the server randomly selects a small fraction of clients ([Cd]), each locally sparse learning Kundu et al. (2021b) for few warm-up epochs (Ed) (L4-9 in Algo. 1). Upon collection of layer-wise sensitivity from the clients, for each layer l, the server estimates average density3 ˆdl as i is the density at layer l in ith client. As these averaged layer-wise density values may not necessarily yield to the target density d, for a model with K parameters we follow the following density re-calibration\n\n, where dl\n\ni=1 dl cd\n\n(cid:80)cd\n\ni\n\nc = ˆdl.rf , where rf = dl\n\nd × K\n\n(cid:80)L\n\nl=1\n\nˆdl.kl\n\n(2)\n\nkl is dense model’s parameter size for layer l. For each layer l of the model, the server then creates a binary sparse mask tensor that is randomly initialized, with a fraction of 1s ∝ dl c (L10). In stage 2, the server begins the training rounds starting with a sparse model initialization following the sparse mask computed in stage 1 (L11). Particularly, at each round, the clients perform sparse learning for E epochs (L23-29) with the choice to either train the mask or keep training on the weights with the mask frozen (L26). The later allows the clients to intermittently share masks saving up-link cost.\n\nHowever, in FL settings, the masks often show poor convergence (section 3, Obs. 2). To address this, in stage 2, we present two sparse FL methods depending on the mask learnability being disabled or enabled (L13). The disabled scenario (mf reez = 1) essentially translates to sparse learning with pre-defined layer masks at initialization, allowing to only learn the weights and forcing all the clients to use the same initialized masks. This guarantees no mask divergence issue (smt = 0 for all t). Moreover, as FLASH disentangles the sensitivity evaluation stage from the training, the pre-defined mask in this scenario benefits from the notion of layer sensitivity. We thus aptly name this scenario sensitivity-driven pre-defined sparse training (SPDST). Interestingly, earlier research Bibikar et al. (2021) hinted at poor model performance with pre-defined masks, contrasting ours where we see significantly improved model performance, implying the importance of stage 1 (as will be elaborated in section 5).\n\nIn the enabled mask learning scenario (mf reez = 0), model masks and weights are jointly learned during clients’ local learning, thus termed as joint mask weight sparse training (JMWST). However, as highlighted earlier, clients’ naive sparse mask selection at the beginning of each round costs a considerable accuracy drop (section 3 Obs. 1). JMWST allows the server to select a sparse model for the clients at round t + 1. For clients’ target density d, the aggregated server model (L20) at the end of round t, generally has density dS > d. To enable efficient sampling of sparse model, we leverage the density re-calibration strategy (Eq. 2) by taking the tth round’s clients’ sensitivity into c fraction of parameters for lth consideration. We then perform magnitude pruning to retain the top-dl layer at the server and send the pruned model to clients at round t (L21). Intuitively, such sampling of non-zero weights by the server reduces chances of wasted updates, and allows the layer masks to converge faster due to alignment with the layers’ pruning sensitivity. The clients then perform local sparse learning, yielding another set of sparse models and so on. Note, the aggregation and sampling is simpler in SPDST, as the server model always remains at density d. In terms of yielding convergent masks, we indeed observed a lower SM for JMWST by ∼85% compared to that in NST, evaluated after 300 rounds on CIFAR-10. Algorithm 1 details the FLASH training methods. It is noteworthy that, the clients are only allowed to update mask after an interval of rint, rounds, which for JMWST is set to 1 by default, allowing the server to evaluate masks at the end of every round.\n\nExtension to support heterogeneous parameter density. To support different density budgets for different clients, we now present hetero-FLASH. Let us assume a total of N support densities\n\n3which is same as sensitivity for a layer.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: FLASH Training. Data: Training rounds T , local epochs E, client set [CN ], clients per rounds cr, target density d , sensitivity warm-up epochs Ed,\n\ndensity warm up client count cd, initial value of freeze masks mf reez = 0, training algorithm A and Aggregation type Agr.\n\n1 Minit ← createRandomMask(d) 2 Θinit ← initMaskedWeight(Minit) 3 serverExecute: 4 # Calculate the layer-wise sensitivity in stage 1 5 Randomly sample cd clients [Cd] ⊂ [CN ] 6 for each client c ∈ [Cd] in parallel do 7\n8 9 end\n\nΘc ← clientExecute(Θinit, Ed, 0) # m_f reeze = 0 Sc ← computeSensitivity(Θc)\n\n10 # Initialize a sensitivity-driven mask 11 M0 ← initMask([Sc], d) 12 Θ0 ← initMaskedWeight(M0) 13 mf reez ← freezeMask(A)#set to 1, and 0 for SPDST, and JMWST, respectively 14 # Start Stage 2 15 for each round t ← 1 to T do 16 17\n\nRandomly sample cr clients [Cr] ⊂ [CN ] for each client c ∈ [Cr] in parallel do\n\nΘt\n\nc ← clientExecute(Θt−1, E, mf reez)\n\n18 19\n\n20\n\nend Θt S ← aggrParamUpdateMask([Θt Θt ← subsampleServerModel(Θt\n\n21 22 end 23 clientExecute(Θc, E, mf reez) : 24 Θc0 ← Θc 25 for local epoch i ← 1 to E do 26\n\nΘci ← doSparseLearning(Θci−1 , mf reez) mf reez ← checkUpdateMask()\n\nc], Agr) S , [Θt\n\nc], d, mf reez)\n\n27 28 end 29 return ΘcE\n\ndset = [d1, .., dM ], where di < di+1. Now, for hetero-SPDST, we perform a sensitivity warm-up, to create the masks for the clients’ with the highest density dN . For any other density di, we sample a sparse mask from that with density di+1. Note, while creating the mask from di+1 to di, we follow the layer-wise density re-calibration approach as mentioned earlier. For hetero-JMWST, at the beginning of each round, the server performs magnitude pruning to yield N sub models meeting N different density levels, contrasting to the creation of one model in JMWST. Participating clients of different densities use the corresponding sub models to start their local sparse training. In hetero-FLASH, server performs aggregation by following a form of weighted fed averaging (WFA). In particular, with similar inspiration as Diao et al. (2020), to give equal importance to each parameter update in such heterogeneous settings, WFA averages the values by their number of non-zero occurrences among the participating clients.We have provided the algorithm for hetero-FLASH in the Appendix.\n\n5 EXPERIMENTS\n\nDatasets and Models. We evaluated the performance of FLASH on MNISTLeCun & Cortes (2010), Federated EMNIST (FEMNIST) Caldas et al. (2018a), and CIFAR-10 Krizhevsky et al. (2009) datasets with the CNN models described in McMahan et al. (2017), Caldas et al. (2018a), and ResNet18, respectively. Further model details are provided in the Appendix. For data partitioning of MNIST and CIFAR-10, we use Latent Dirichlet Allocation (LDA)Reddi et al. (2020) with three different α (α = 1000 for IID and α = 1 and 0.1 for non-IID). For FEMNIST, we employ the same setting as in Han et al. (2020), which partitions the data based on the writer into 3400 clients, making it inherently non-IID.\n\nTraining Hyperparameters. We use Clients’ starting learning rate (ηinit) as 0.1 that is exponentially decayed to 0.001 (ηend) at the end of training. Specifically, learning rate for participants at round t is ηt = ηinit(exp( t ))). In all the sparse learning experiments, prune rate is set to 0.254. Summary of the rest of the training hyperparameters can be found in 1. Furthermore, all the experiments were performed with three different seeds. We report the final results as the averaged accuracy with corresponding std deviation in the tables.\n\nT log( ηinit\n\nηend\n\n4Prune rate controls the fraction of non-zero weights participating in the redistribution during sparse learning.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Results with FLASH (SPDST, and JMWST) and its comparison with NST and PDST.\n\nDataset Data Distribution Density\n\nIID (α = 1000)\n\nMNIST non-IID (α = 1.0)\n\nnon-IID (α = 0.1)\n\nIID (α = 1000)\n\nCIFAR-10 non-IID (α = 1.0)\n\nnon-IID (α = 0.1)\n\nFEMNIST\n\nnon-IID\n\n(d) 1.0 0.1 0.05 1.0 0.1 0.05 1.0 0.1 0.05 1.0 0.1 0.05 1.0 0.1 0.05 1.0 0.1 0.05\n\n1.0 0.1 0.05\n\nBaseline Acc % 98.79 ± 0.06 –\n– 98.76 ± 0.06 –\n– 98.45 ± 0.17 –\n– 88.56 ± 0.06 –\n– 87.13 ± 0.18 –\n– 77.64 ± 0.49 –\n–\n\n84.68 ± 0.20 –\n–\n\nNST Acc % –\n\nSPDST Acc % –\n\nPDST Acc % –\n97.57 ± 0.11 97.09 ± 0.18 98.21 ± 0.06 95.19 ± 0.56 94.8 ± 1.04 97.46 ± 0.14 –\n97.36 ± 0.19 96.82 ± 0.25 97.96 ± 0.13 97.3 ± 0.26 95.75 ± 0.31 95.34 ± 0.77 –\n\n– 96.19 ± 0.22 94.41 ± 1.23 97.22 ± 0.43 91.66 ± 1.74 91.06 ± 1.1\n\n95.7 ± 0.37\n\n–\n\n–\n\n–\n\n–\n\n–\n\n– 88 ± 0.28\n\n–\n\n84.89 ± 0.26 86.72 ± 0.09 77.48 ± 0.54 84.38 ± 0.12 86.99 ± 0.14 –\n83.46 ± 0.19 85.07 ± 0.24 86.42 ± 0.49 75.1 ± 0.76 83.33 ± 0.14 85.64 ± 0.58 –\n71.18 ± 1.23 74.82 ± 0.72 76.74 ± 1.46 61.29 ± 2.76 72.32 ± 1.05 75.47 ± 2.31\n\n–\n\n–\n\n–\n\n–\n\n– 76.92 ± 0.42 76.01 ± 1.26 82.70 ± 0.26 63.65 ± 0.86 81.18 ± 0.36\n\n61.9 ± 2.6\n\n–\n\nJMWST(rint = 1) JMWST(rint = 5)\n\nAcc % –\n97.95 ± 0.16 97.24 ± 0.21 –\n97.72 ± 0.12 97.38 ± 0.11 –\n96.53 ± 0.19 95.83 ± 0.84\n\n– 87.62 ± 0.35 86.87 ± 0.08 –\n86.45 ± 0.31 85.34 ± 0.27 –\n74.74 ± 1.07 73.9 ± 1.45\n\n– 83.02 ± 0.21 82.01 ± 0.53\n\nAcc % –\n98.09 ± 0.16 97.37 ± 0.23 –\n98.11 ± 0.12 97.59 ± 0.07 –\n96.7 ± 0.14 95.91 ± 0.64\n\n– 87.86 ± 0.13 87.18 ± 0.09 –\n86.36 ± 0.13 85.9 ± 0.24 –\n75.47 ± 1.18 75.49 ± 0.9\n\n– 83.4 ± 0.26 82.48 ± 0.18\n\nTable 4: Comparison of ZeroFL on various performance metrics with existing alternative sparse federated learning schemes. The italicized values are taken from the original manuscript.\n\nDataset Data Distribution\n\nMethod\n\nDensity\n\nAcc%\n\nDown-link Up-link Savings\n\nSavings\n\nCIFAR-10\n\nIID\n\nnon-IID (α = 1.0)\n\nFEMNIST\n\nnon-IID\n\n1×\n\n0.1 0.1\n\n82 .71 ± 0 .37 88 ± 0.28\n\nZeroFL Qiu et al. (2021) FLASH-SPDST (ours) ZeroFL Qiu et al. (2021) 0.05 78 .22 ± 0 .35 0.05 86.99 ± 0.14 19.5× 19.5× FLASH-SPDST (ours) 81 .04 ± 0 .28 0.1 ZeroFL Qiu et al. (2021) 0.1 86.42 ± 0.49 FLASH-SPDST (ours) ZeroFL Qiu et al. (2021) 0.05 75 .54 ± 1 .15 FLASH-SPDST (ours)\n\n1.6× 9.8× 9.8× 1.9×\n\n1.6× 9.8× 9.8× 1.9×\n\n0.05 85.64 ± 0.58 19.5× 19.5×\n\n1×\n\n1×\n\n1×\n\nZeroFL Qiu et al. (2021) 0.05 77 .16 ± 2 .07 FLASH-SPDST (ours)\n\n17.7× 0.05 81.18 ± 0.36 14.6× 14.6×\n\n1×\n\n5.1 EXPERIMENTAL RESULTS WITH FLASH\n\nTo understand the importance of stage 1 in FLASH methodology, we identify a baseline training with uniform layer sensitivity driven pre-defined sparse training (PDST) in FL. Table 3 details the performance of FLASH at different levels of d, for various choices of sparse learning methods. In particular, as we can see in Table 3 column 5 and 6, the performance of both NST and PDST produced models cost heavy accuracy drop at ultra low parameter density d = 0.05. For example, on CIFAR-10 (α = 0.1), models from NST and PDST sacrifice an accuracy of 16.35% and 5.32%, respectively. However, at comparatively higher density (d = 0.1), both can yield models with a lower accuracy difference from the baseline by around 6.46% and 2.82%. SPDST, on the other hand, can maintain close to the baseline accuracy at even ultra-low density for all data partitions. Interestingly, for majority of the cases, it even outperforms JMWST yielded models. These results clearly highlight the efficacy of both sensitivity driven sparse learning (as SPDST Figure 4: Test accuracy vs. round for > PDST) and early mask convergence (as SPDST ≈ JMWST) different approaches on CIFAR-10. in FL settings. Importantly, for increased rint in JMWST, we observe a consistent improvement in accuracy. The inferior accuracy at rint = 1 can be attributed to the mask divergence caused by frequent noisy gradient dependent update. We thus believe efficient hyperparameter search including rint is essential for sparse FL model’s improved performance, particularly for JMWST. Moreover, JMWST requires additional communication of non-zero weight indices, contrasting SPDST, where clients do not need to send the mask at all, allowing us to yield proportional communication saving as the model density. Fig. 4 shows the acc. vs. round comparison among the two proposed methods on different data distributions.\n\nComparison with ZeroFL. Despite leveraging a form of sparse learning Raihan & Aamodt (2020), ZeroFL required significantly higher up-link/down-link communication cost compared to the target density d. This enables FLASH to gain a significant advantage in communication saving over ZeroFL, particularly for SPDST, as it only asks for the reduced size parameters to be communicated between the server and clients. In particular, we evaluate the communication saving as the ratio of the dense model size and corresponding sparse model size with the tensors represented in compressed sparse\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Performance of hetero-FLASH on various datasets where each client can have a density from the set dset ∈ [0.1, 0.15, 0.2].\n\nDataset Data Distribution Max Client Density Hetero-SPDST Hetero-JMWST Hetero-JMWST\n\n(dset)\n\nAcc %\n\n(rint = 1) Acc % (rint = 5) Acc %\n\nIID (α = 1000)\n\nMNIST non-IID (α = 1.0) non-IID (α = 0.1)\n\nIID (α = 1000)\n\nCIFAR-10 non-IID (α = 1.0) non-IID (α = 0.1)\n\nFEMNIST\n\nnon-IID\n\n0.2\n\n0.2\n\n0.2\n\n98.29 ± 0.05 97.44 ± 0.23 98.29 ± 0.09 97.47 ± 0.22 97.63 ± 0.22 96.11 ± 0.75\n\n97.83 ± 0.10 97.80 ± 0.23 96.25 ± 0.86\n\n86.37 ± 0.2 87.19 ± 0.26 84.67 ± 0.06 86.16 ± 0.04 75.23 ± 1.26 71.3 ± 2.75\n\n87.39 ± 0.15 86.19 ± 0.24 74.34 ± 0.85\n\n82.58 ± 0.24 82.2 ± 0.42\n\n82.5 ± 0.55\n\nrow (CSR) format Tinney & Walker (1967). As depicted in Table 45, FLASH can yield an accuracy improvement of up to 10.1% at a reduced communication cost of up to 10.26× (computed at up-link when both send sparse models).\n\n5.2 EXPERIMENTAL RESULTS WITH HETERO-FLASH\n\nTable 5 shows the performance of hetero-FLASH where the clients can have three possible density budgets as defined by the dset. We assume the maximum capacity clients’ density budget of 0.2. To train on all the density values, we first create three sets, each having 40%, 30%, and 30% of total clients, and corresponds to density 0.2, 0.15, and 0.1, respectively. Now, during every round, we sample 10% from each set with corresponding target density. Similar to the trend in FLASH, hetero-SPDST outperforms the JMWST counter-parts by up to 3.93% evaluated on the three datasets. Also, following similar trend as with homogeneous density clients, with increased mask update interval (rint), the performance of hetero-JMWST gets a significant boost in accuracy of up to 3.04%.\n\n5.3 QUANTITATIVE ANALYSIS\n\nFigure 5: (a) Layer sensitivity evaluated at the end of sensitivity warm-up stage for different client participation size and their local epochs, (b) Comparison of server side model performance with the initialized sparse mask based on different sensitivity evaluated from (a).\n\nDependence of initial sensitivity warm-up of participating clients. To understand the importance of the clients’ participation in the warm-up, we experimented with six different scenarios. In particular, we used two different values of participating clients ([10, 20]) each corresponding to three different local epoch choices ([10, 20, 40]). As shown in Fig. 5(a), the yielded pruning sensitivity follows a similar trend. Moreover, an SPDST training with mask chosen from any of these sensitivity lists finally yield FL models with similar performances (Fig. 5(b)), clearly demonstrating the robustness of our warm-up based sensitivity evaluation stage 1.\n\nComparison with ERK+ initialization. We now compare our SPDST mask initialization, with that of parameter density distribution evaluated via ERK+ Huang et al. (2022); Evci et al. (2020). Notably, contrary to uniform density, ERK+ scheme keeps more weights for the layers having fewer parameters. Note here, we use SPDST, ERK+, or uniform (PDST) as the initial mask for stage 2, and keep the mask frozen throughout the training of stage 2. As shown in Fig. 6(a-b), the mask evaluation stage 1 to initialize mask allows SPDST to consistently provide superior results over the other two. We hypothesize this to the better layer sensitivity evaluation scheme of SPDST, particularly at the earlier layers, allowing it to retain more information at these layers.\n\nImportance of parameters’ weighted fed averaging at the server. Earlier literature Diao et al. (2020) suggested a form of weighted fed averaging, for clients with different model sizes. Inspired by that, we now investigate the necessity of WFA in FLASH. In particular, we performed experiments\n\n5We understand for FEMNIST, ZeroFL reported significantly higher up-link saving, however, to the best of\n\nour understanding it should be similar to their report on other datasets, i.e. ∼1.9×.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: (a)-(b) Performance comparison of sparse models trained with SPDST, uniform (PDST) and ERK+ initialized layer-wise parameter density; (c-d)Performance comparison of fedavg with weighted fedavg for (c) different training algorithms and (d) different dataset partitioning (α). on CIFAR-10 (α = 1.0), both with and without WFA, during server aggregation. As shown in Fig. 6(c), WFA model performs inferior to the fed averaged model in FLASH. On the contrary, hetero-FLASH, enjoys consistent superior performance with WFA 6(c-d). The inferior performance of WFA in FLASH may hint at the fact that if a weight is non-zero only for fewer clients, as compared to other non-zero weights, giving it equal weight as the others nullifies its lower importance, that may be necessary to preserve for mask convergence. On the other hand, having WFA in hetero FLASH is necessary, as a weight’s less frequent non-zero occurrence can be due to fewer number of high-parameter density clients in a round. Further investigations on the utility and use case of such weighted averaging is an interesting future research direction.\n\nTime and communication overhead for stage 1. Mask evaluation stage 1 uses one round with Ed local epochs (for us Ed = 10) per client. A normal FL stage in our settings trains the clients for T rounds, 1 epoch per client/round. Therefore, stage 1 increases the time by a factor of ( Ed T + 1). Usually, Ed << T , making the pre-training time overhead negligible.\n\nThe communication overhead of stage 1 is also negligible compared to that in each round for the stage 2 FL training. Each participant only needs to send L values for an L-layer model. So, cd clients will have a total communication overhead of (L × cd × 32) bits, assuming 32-bit number representation.\n\nFigure 7: performance comparison at different uplink limits for (a) α = 1.0 and (b) α = 0.1.\n\nComputation saving for FLASH. The training FLOPs for a layer l (F l layer) can be partitioned into forward operation FLOPs (F l back_wt) compute FLOPs. With the assumption of no-compute cost associated to the zero-valued weights via zero-gating logic Kundu et al. (2021a), the F l layer for FLASH with parameter density d (d << 1.0) is\n\nback_in) and weight gradient (F l\n\nf wd), backward input (F l\n\nF l\n\nlayer = d × [F ul where sa is d and 1 for SPDST and JMWST, respectively. F ul x represents the corresponding FLOPs associated with an unpruned layer. Thus SPDST provides improved computation benefits along with the communication savings. Further details on FLOPs computation is provided in the Appendix.\n\nback_in] + sa × F ul\n\nf wd + F ul\n\nback_wt\n\n(3)\n\nPerformance at limited communication budget. Fig. 7(a) and (b) show the performance of FL models when the clients are communication limited. In particular, we see both PDST and SPDST can significantly outperform other approaches in yielding a significantly well-trained model at low comm. budget. This can be attributed to their significantly smaller model sizes, helping them to run for higher number of rounds than others, on a limited bandwidth scenario.\n\n6 CONCLUSIONS\n\nThis paper presented federated lottery aware sparsity hunting methodologies to yield sparse server models with low parameter density while costing insignificant accuracy drop compared to the unpruned counterparts. In particular, we demonstrated two efficient sparse learning solutions specifically tailored for FL, enabling better computation and communication benefits over existing sparse learning alternatives. We experimentally demonstrated the superiority of our models in yielding up to ∼10.1% improved accuracy with ∼10.26× fewer communication costs, compared to the existing alternatives Qiu et al. (2021), at similar hyperparameter settings. Future research direction of this work includes the theoretical understanding of our observations, and further empirical demonstrations on newer class of models including transformers.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlyazeed Albasyoni, Mher Safaryan, Laurent Condat, and Peter Richtárik. Optimal gradient com-\n\npression for distributed and federated learning. arXiv preprint arXiv:2010.03246, 2020.\n\nSameer Bibikar, Haris Vikalo, Zhangyang Wang, and Xiaohan Chen. Federated dynamic sparse training: Computing less, communicating less, yet learning better. arXiv preprint arXiv:2112.09824, 2021.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018a.\n\nSebastian Caldas, Jakub Koneˇcny, H Brendan McMahan, and Ameet Talwalkar. Expanding the reach of federated learning by reducing client resource requirements. arXiv preprint arXiv:1812.07210, 2018b.\n\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing\n\nperformance. arXiv preprint arXiv:1907.04840, 2019.\n\nEnmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient\n\nfederated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020.\n\nXiaohan Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, Ji Liu, et al. Global sparse momentum sgd for pruning very deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. arXiv preprint arXiv:1803.03635, 2018.\n\nEduard Gorbunov, Konstantin P Burlachenko, Zhize Li, and Peter Richtárik. Marina: Faster nonconvex distributed learning with compression. In International Conference on Machine Learning, pp. 3788–3798. PMLR, 2021.\n\nPengchao Han, Shiqiang Wang, and Kin K Leung. Adaptive gradient sparsification for efficient federated learning: An online learning approach. In 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS), pp. 300–310. IEEE, 2020.\n\nChaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Federated learning of large cnns at the edge. Advances in Neural Information Processing Systems, 33: 14068–14080, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision (ECCV), pp. 784–800, 2018.\n\nSamuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nicholas Lane. Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout. Advances in Neural Information Processing Systems, 34, 2021.\n\nTiansheng Huang, Shiwei Liu, Li Shen, Fengxiang He, Weiwei Lin, and Dacheng Tao. Achieving personalized federated learning with sparse local models. arXiv preprint arXiv:2201.11380, 2022.\n\nRustem Islamov, Xun Qian, and Peter Richtárik. Distributed second order methods with fast rates and compressed communication. In International Conference on Machine Learning, pp. 4617–4628. PMLR, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, and Leandros Tassiulas. Model pruning enables efficient federated learning on edge devices. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nSouvik Kundu, Saurav Prakash, Haleh Akrami, Peter A Beerel, and Keith M Chugg. psconv: A predefined sparse kernel based convolution for deep cnns. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 100–107. IEEE, 2019.\n\nSouvik Kundu, Mahdi Nazemi, Massoud Pedram, Keith M Chugg, and Peter A Beerel. Pre-defined sparsity for low-complexity convolutional neural networks. IEEE Transactions on Computers, 69 (7):1045–1058, 2020.\n\nSouvik Kundu, Gourav Datta, Massoud Pedram, and Peter A Beerel. Spike-thrift: Towards energyefficient deep spiking neural networks by limiting spiking activity via attention-guided compression. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3953– 3962, 2021a.\n\nSouvik Kundu, Mahdi Nazemi, Peter A Beerel, and Massoud Pedram. Dnr: A tunable robust pruning framework through dynamic network rewiring of dnns. In Proceedings of the 26th Asia and South Pacific Design Automation Conference, pp. 344–350, 2021b.\n\nYann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.\n\nlecun.com/exdb/mnist/.\n\nAng Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets. arXiv preprint arXiv:2008.03371, 2020a.\n\nAng Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, pp. 42–55, 2021.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020b.\n\nXiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of\n\nfedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.\n\nTao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351–2363, 2020.\n\nShiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong Pei, and Mykola Pechenizkiy. Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware. Neural Computing and Applications, 33(7):2589–2604, 2021.\n\nXiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Zhang, Geng Yuan, Sia Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, et al. Non-structured dnn weight pruning–is it beneficial in any platform? IEEE Transactions on Neural Networks and Learning Systems, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018.\n\nEric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan Srinivasan, Dipankar Das, Bharat Kaul, and Tushar Krishna. Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 58–70. IEEE, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXinchi Qiu, Javier Fernandez-Marques, Pedro PB Gusmao, Yan Gao, Titouan Parcollet, and Nicholas Donald Lane. Zerofl: Efficient on-device training for federated learning with local sparsity. In International Conference on Learning Representations, 2021.\n\nMd Aamir Raihan and Tor Aamodt. Sparse weight activation training. Advances in Neural Information\n\nProcessing Systems, 33:15625–15638, 2020.\n\nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.\n\nWilliam F Tinney and John W Walker. Direct solutions of sparse network equations by optimally\n\nordered triangular factorization. Proceedings of the IEEE, 55(11):1801–1809, 1967.\n\nGiyoung Yang and Taewhan Kim. Design and algorithm for clock gating and flip-flop co-optimization. In 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 1–6. IEEE, 2018.\n\nDezhong Yao, Wanning Pan, Yao Wan, Hai Jin, and Lichao Sun. Fedhm: Efficient federated learning for heterogeneous models via low-rank factorization. arXiv preprint arXiv:2111.14655, 2021.\n\nHaoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv preprint arXiv:1909.11957, 2019.\n\nTianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and Yanzhi Wang. A systematic dnn weight pruning framework using alternating direction method of multipliers. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 184–199, 2018.\n\nTuo Zhang, Lei Gao, Chaoyang He, Mi Zhang, Bhaskar Krishnamachari, and Salman Avestimehr. Federated learning for internet of things: Applications, challenges, and opportunities. arXiv preprint arXiv:2111.07494, 2021.\n\nYuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright.\n\nInformationtheoretic lower bounds for distributed statistical estimation with communication conIn C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger straints. (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/ d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf.\n\nAojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv preprint arXiv:2102.04010, 2021.\n\nZhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International Conference on Machine Learning, pp. 12878–12889. PMLR, 2021.\n\nA APPENDIX\n\nA.1 MODEL ARCHITECTURES\n\nTable 6 shows the model architectures used for MNIST and FEMNIST datasets. For CIFAR-10 we used ResNet18 He et al. (2016) with the first CONV layer kernel size as 3 × 3 instead of original 7 × 7.\n\nA.2 HETERO-FLASH ALGORITHM\n\nthe\n\ndetails\n\ntraining\n\nand Algorithm 2 aggrParamUpdateMask and subSampleServerModel are the two functions that play key role in supoorting heterogeniety in sparsity ratios for different clients. The details of these two functions are elaborated in Algortihm 3 and 4, respectively. We plan to open-source our code upon acceptance of the paper.\n\nhetero-FLASH. Note\n\nalgorithm in\n\nthat\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Architecture used for MNIST and FEMNIST datasets\n\nMNIST\n\nFEMNIST\n\nCONV5 × 5(Co = 10) CONV5 × 5(Co = 32)\n\nmax_pool\n\nmax_pool\n\nCONV5 × 5(Co = 20) CONV5 × 5(Co = 64)\n\nmax_pool FC(5120, 50) FC(50, 10)\n\nmax_pool FC(3136, 2048) FC(2028, 62)\n\nAlgorithm 2: Hetero-FLASH Training. Data: Training rounds T , local epochs E, client set [[CN1 ], ..., [CNM ]], clients per rounds cr, target density set dset = [d1, ..., dM ], sensitivity warm-up epochs Ed, density warm up client count cd, initial value of freeze masks mf reez = 0, training algorithm A and aggregation type Agr.\n\n1 Minit ← createRandomMask() 2 Θinit ← initMaskedWeight(Minit) 3 serverExecute: 4 Randomly sample cd clients [Cd] ⊂ [CNM ] 5 for each client c ∈ [Cd] in parallel do 6\n\nΘc ← clientExecute(Θinit, Ed, 0) Sc ← computeSensitivity(Θc)\n\n7 8 end 9 M0 ← initMask([Sc], dset) 10 Θ0 ← initMaskedWeight(M0) 11 mf reez ← freezeMask(A) 12 for each round t ← 1 to T do 13\n\nRandomly sample cr clients [Cr] ⊂ [CN ] for each client c ∈ [Cr] in parallel do\n\nΘt\n\nc ← clientExecute(Θt−1, E, mf reez)\n\n14\n\n15\n\n16\n\n17\n\n18\n\nend Θt\n\nS ← aggrParamUpdateMask ([Θt Θt ← subSampleServerModel (Θt\n\nc], Agr) S, [Θt\n\nc], dset, mf reez)\n\n19 end 20 clientExecute(Θc, E, mf reez) : 21 Θc0 ← Θc 22 for local epoch i ← 1 to E do 23\n\nΘci ← doSparseLearning(Θci−1 , mcf reez ) mf reez ← checkUpdateMask()\n\n24 25 end 26 return ΘcE\n\nAlgorithm 3: aggrParamUpdateMask Data: Round t, aggregation type Agr [fedAvg, weightedFedAvg], clients updates [Θt] =\n\n[Θc1 , ..., Θcr ], client data size [dsc1 , ..., dscr ]\n\n1 if Agr is fedAvg then\n\nΘt\n\nS ←\n\nΣcr\n\n1 ci=1dsci\n\nΣcr\n\nci=1dsci · Θt\n\nci\n\n2\n\n3 else 4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11 12 end\n\n//For hetero-FLASH W t ← initWeightFactor() for each update Θci ∈ [Θt] do\n\nci ← dsci × retrieveMask(Θci )\n\nW t W t ← W t + W t ci\n\nend //safeDivide(a,b): gives zero anywhere the b is queal to zero Θt\n\nci=1[safeDivide(W t\n\nci , W t) · Θt\n\nS ← Σcr\n\nci ]\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 4: subsampleServerModel Data: Current round id t, client set [Cr], aggregated Weight Θt\n\nS of model with L layers, support density set\n\ndset = [d1, ..., dM ] where di < di+1, model layer-wise parameter count [k] = [k1, ..., kL].\n\n1 if size(dset) is 1 then //JMWST subsampling in FLASH 2\n3 M ← initMaskWithZeros()\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 else 12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n[ ˆd1, ..., ˆdL] ← avgLayerWiseDensity([Cr]) rf ← d1×K\n\n(cid:80)L\n\nl=1\n\nˆdl.kl\n\nfor layer l ← 1 to L do\n\nidx ← getSortedWeightIndeices(Θt S, l) nz ← int(rf × ˆdl × kl) //number of non-zeros Ml[idx[: nz]] ← 1\n\nend\n\n//For hetero-FLASH for di ∈ dset do\n\nMi ← initMaskWithZeros()\n\ns ← getCurrentDensity(Θt\n\nend Dt [ ˆd1, ..., ˆdL] ← getLayerWiseDensity(Θt for layer l ← 1 to L do\n\nS)\n\nS)\n\nidx ← getSortedWeightIndeices(Θt for di ∈ dset do rf i ← di nz ← int(rf i × ˆdl × kl) l[idx[: nz]] ← 1 Mi\n\nDt s\n\nS, l)\n\nend\n\nend\n\n25 26 end\n\nA.3 ADDITIONAL COMPARISONS\n\nWe now compare the performance of FLASH with that of yielded via FedSpa Huang et al. (2022), and FedDST Bibikar et al. (2021). For FedSpa, we implemented their proposed algorithm in our settings and kept all the hyperparameters same for an apple-to-apple comparison. For FLASH, we report the best of the accuracy yielded via models trained using SPDST and JMWST. As shown in Table 7, FLASH generated models can outperform that generated via FedSpa with an improved accuracy of up to 2.41%. Similar trend is observed when we compare with FedDST and as Table 8, on MNIST dataset, FLASH can have an accuracy improvement of up to 1.41%.\n\nTable 7: Comparison of FLASH with FedSpa Huang et al. (2022) on CIFAR-10 with ResNet18.\n\nData distribution α = 1000\n\nα = 0.1\n\nMethod Density (d) Best Acc. (%)\n\nδAcc\n\nFedSpa FLASH FedSpa FLASH\n\n0.05 0.05 0.05 0.05\n\n85.63 87.18 73.08 75.49\n\n– +1.55 –\n+2.41\n\nA.4 MORE QUANTITATIVE ANALYSIS\n\n1. Ablation with the mask update interval rounds (rint). As mentioned in the original manuscript, in the case of JMWST, to save communication energy we often can choose not to update the mask every round. We thus performed ablation with an increased frequency of mask update interval round rint from the default value of 1 (similar to Qiu et al. (2021)). Table 9 shows the results with different rint. In particular, as we can see in the table, less frequent update intervals does not degrade the final\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Comparison of FLASH with FedDST Bibikar et al. (2021) on pathologically non-IID MNIST. For this comparison we used same hyperparameter settings and models as that in Bibikar et al. (2021).\n\nMethod Density (d) Communication Best Acc. (%)\n\nδAcc\n\nFedDST FLASH FedDST FLASH\n\n0.2\n\n0.2\n\nCost (GiB) 1.0\n\n2.0\n\n96.10 97.51 97.35 97.69\n\n– +1.41 –\n+0.34\n\nmodel performance. Moreover, a less frequent update can provide additional savings in terms of up-link cost for the models as the masks do not change every round.\n\nTable 9: Ablation with different mask update intervals for JMWST for a target density d = 0.1 on CIFAR-10.\n\nModel\n\nResNet18\n\nData distribution IID (α = 1000) non-IID (α = 1) non-IID (α = 0.1)\n\nMask update interval rounds (rint) rint = 2 87.76 ± 0.07 86.26 ± 0.07 73.73 ± 1.18\n\nrint = 5 87.86 ± 0.13 86.36 ± 0.13 75.47 ± 1.08\n\nrint = 1 87.62 ± 0.35 86.45 ± 0.31 74.74 ± 1.07\n\nrint = 10 87.67 ± 0.09 86.68 ± 0.25 77.14 ± 0.22\n\n2. Impact of Number of participating clients per round. Fig. 8 (a), shows that JMWST and SPDST follow the same pattern at the baseline model (d = 1.0) with FedAvg. In other words, similar to FedAvg, as the cr increases, the performance enhances. Also, for a specific cr, JMWST and SPDST perform better than PDST and NST.\n\n3. Impact of Batch-Normalization layer statistics. Fig 8 (b), shows the performance comparison between batch bormalization (BN) and static batch normalization (static BN, as suggested in Diao et al. (2020)). In particular in our setting, using BN layer statistics always outperform the static BN.\n\nFigure 8: (a) Performance of the final trained model for different participating clients per round, (b) Significance of BN and Static BN in final model performance.\n\n4a. Revisiting sparse mask mismatch for NST with VGG16. Fig. 9 shows the comparison of SM between centralized and FL settings with NST on VGG16, another popular model variant. Similar to our observed trend with ResNet18, we see a significantly high SM for FL settings with a target d − 0.05. This strengthens the generality of our observed limitations across different class of DNN models.\n\nFigure 9: (a)-(b) Sparse mask mismatch (SM) for VGG16 in (a) centralized and (b) FL settings with NST. (c)-(d) Layer-wise SM vs. training epochs (rounds) for VGG16 in (c) centralized and (d) FL settings, respectively, with NST.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n4b. Revisiting sparse mask mismatch for FLASH. As demonstrated in Figs 10, the sparse mask mismatch in the case of JMWST significantly reduces, helping the mask train in a convergent way, significantly faster than that in NST.\n\nFig. 11 shows the layer-wise SM, for centralized trained model (Fig. 11a) and FL trained model with sparsity (Fig. 11b-c). In particular, the SM at later layer can significantly reduce in the case of JMWST as compared to NST, further demonstrating the convergence ability even at the later layers.\n\nFigure 10: Sparse mask mismatch (SM) for (a) centralized sparse learning, (b) NST, and (c) JMWST in federated settings.\n\nFigure 11: Layer-wise sparse mask mismatch (SM) vs. training epochs (rounds) plot for (a) centralized and (b) FL with NST, and (c) FL with JMWST.\n\n4c. Sparse mask mismatch as a function of d. To understand the relation of SM with d, we performed the baseline sparse training (NST) with ResNet18 on CIFAR-10 for three different target densities, 0.05, 0.25, 0.5. As shown in Fig. 12, the SM tends to reduce for higher density. In particular, Fig. 12(d) shows the SM for CONV layer 16 (a later layer), after round 200. The SM reduces by 1.53× for d = 0.5 than that with d = 0.05, strengthening our general observation that SM becomes prominent as the density gets lower.\n\nFigure 12: (a-c) SM for FL settings for three different d of 0.05, 0.25, and 0.5, respectively. (d) Comparison of Jaccard distance values for the 16th CONV layer of ResNet18 after round 200 for different ds.\n\n4d. Sparse mask mismatch as a function of number of clients. To understand the relation of SM with number of total clients, we performed the baseline sparse training (NST) with ResNet18 on CIFAR-10 for 50 and 200 clients, respectively. As shown in Fig. 13, the SM concern persists, irrespective of the number of clients. This strengthens the generality of our observations over total number of clients.\n\n5. Convergence trend of proposed algorithms. Fig. 14 shows the test accuracy vs FL rounds for NST, PDST, SPDST, and JMWST algorithms on CIFAR-10 dataset with non-IID data distribution (α = 1). As shown in the plots, for d = 0.05 and d = 0.1, NST has slower convergence with lower final accuracy. Introducing consensus among the clients for the sparse mask accelerates the convergence and enhances the final performance.\n\n6. Communication saving and FLOPs evaluations.Employing sparse learning in FL helps participating clients reduce both the communication and compute costs (FLOPs) for training. Without\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 13: (a-b) SM for FL settings for (a) 50 and (b) 200 clients. (c-d) Layer-wise SM vs. training rounds for (c) 50 and (d) 200 clients.\n\nFigure 14: Performance of proposed algorithms vs. comm. rounds on CIFAR-10 dataset for (a) d = 0.05 (b) d = 0.1.\n\nthe loss of generality, we now evaluate the convolutional layer training FLOPs for FLASH, and demonstrate the relation of parameter density d with FLOPs and communication saving. Let us assume a layer l has a weight tensor θl ∈ RCo×Ci×h×w, where h and w are the height and width of the convolutional kernel, and Co and Ci represent the number of filters and channels per filter, respectively. It takes an input tensor I ∈ RCi×H×W to produce an output tensor O ∈ RCo×R×S. Let us also assume d to be the density of non-zero in the weight tensor for all the layers. During training, FLOPs associated to each weight tensor update can be partitioned in to three component, namely, forward pass FLOPs (Ff wd), backward input grad FLOPs (Fback_in), and backward weight grad FLOPs (Fback_wt). The weight sparsity helps both Ff wd and Fback_in to reduce proportionally as given below.\n\nFf wd = d × Ci × (h × w) × (R × S) × Co Fback_in = d × Ci × (h × w) × (H × W ) × Co\n\n(4)\n\n(5)\n\nFinally, if the zero weights’ gradients flow is computed for the purpose of mask learning, then Fback_wt can’t leverage the advantage of low parameter density. Thus during mask training of JMWST, as the gradient needs to be dense, Fback_wt is same as that in dense computation. However, for SPDST, zero weights remain as zero, allowing us to safely skip the associated gradient computation. This essentially helps SPDST to extract benefits of sparsity during all the three stages of FLOPs computation. Following Eq. shows the Fback_wt in FLASH.\n\nFback_wt = sa × Ci × (h × w) × (H × W ) × Co\n\nwhere,\n\nsa =\n\n(cid:26)1, d,\n\nfor JMWST for SPDST\n\n(6)\n\n(7)\n\nFor similar density, clients in ZeroFL also enjoys similar benefits in Ff wd and Fback_in. However, Fback_wt can be reduced only via introduction of activation sparsity, a. It is well surveyed in literature that having sparse activation density as low as parameter density may significantly impact model performance. Thus generally, a > d that allows SPDST to enjoy a FLOPs benefit of a d for Fback_wt. These computations can be easily extended to linear layers. Also, we can safely ignore the FLOPs associated to the BN layers due to their negligible contribution to the total FLOPs.\n\n7. Discussion on compute benefits at the edge. To extract FLOPs benefits for irregular pruning in FLASH, we assume that the compute energy for the sparse network can be avoided via the means of clock-gating Yang & Kim (2018) of the zero-valued weights. Moreover, there has been recent development of sparsity-friendly DNN\n\nFigure 15: Test accuracy vs. mask update interval round.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 16: Computation and Communication relation with (a) each other (b, c) with different density levels for SPDST algorithm.\n\naccelerators Qin et al. (2020) that can efficiently reduce the compute cost by a significant margin. Such accelerators can leverage the yielded sparse FL models to deploy at compute constrained edges.\n\n8. FLOPs vs. communication cost for different density budget. To reach a target accuracy value, we now plot the FLOPs to uplink communication cost for different density budget in Fig. 16.\n\n9. Impact of rint. As depicted in Fig. 15, the test accuracy improves with the increase in JMWST mask update interval rint. In particular, for both α = 0.1 and 1.0, the accuracy with increased rint can be up to ∼1.6% and ∼0.98%, respectively. Interestingly, the improvement tend to saturate after certain rint. Thus, we consider important sparse learning hyperparameter search as an interesting future research direction.\n\nA.5 DISCUSSION ON SUPPORT FOR HARDWARE-FRIENDLY SPARSITY PATTERNS\n\nIrregular sparsity often are not well suited for hardware benefits without any dedicated architecture or compiler support. Among the various hardware-friendly sparsity patterns recently proposed N : M sparsity Zhou et al. (2021) has gained significant attention, due its less stricter constraints compared to other structured sparsity patterns. For SPDST, post stage 1 sparse mask selection can be easily extended to support the N : M sparsity. In particular, for a layer l, instead of random assignment of dl × kl non-zero mask locations, we can partition the total non-zero elements in to Gl groups, where each group will contain dl × kl/Gl non-zero elements. Here Gl is evaluated as kl/M , M representing the total element size out of which we need to have a certain fraction as non-zero, and kl represents the total number of weights for that layer. As the masks remain frozen, we are ensured such pattern is maintained throughout the training for each client to extract the benefit. For JMWST, we can adapt this principle in the prune and regrow policy that happens during local training of each client. We have added a section in the appendix detailing on this important discussion.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper tries to improve the sparse network training efficiency in a Federated learning framework in two folds: 1) bridging the gap between the sparse network and its dense counterpart (e.g., FedAvg) and 2) saving the communication cost between clients and the server.   To this end, the proposed method performs a two-stage sparse training, including the mask-sensitive evaluation (for mask initialization) and networking training, and investigates two different mask learning strategies -- 1) fixed masks and 2) jointly training over masks and weights. Experiments on three public datasets were provided in terms of both IID and non-IID settings.\n\n# Strength And Weaknesses\n\nPros:\n- The proposed hetero-FLASH method is interesting and well adapted to the Federated learning setting over diverse edge resources. While the sampling and fusion strategy is straightforward, the proposed solution poses a good baseline for exploring dynamic budgets in a Federated learning framework. \n- It is technically sound to bridge the gap between sparse network training and its dense counterpart by developing a two-stage method to evaluate mask sensitivity in advance. \n- Two mask learning strategies, namely SPDST and JMWST, were designed and developed on three datasets under two different settings. \n\nCons:\n- While several observations were discussed and provided with empirical evidence, it somewhat lacks in-depth or theoretical analysis regarding the reasons behind these observations. Also, all the observations were provided on the same model architecture (ResNet18), which might lead to model bias in the conclusions. \n- The proposed stage 1 lies in the key contribution of this work. Thus, some ablated models like SPDST w/o state 1, and JMWST w/o state 1 are expected to be provided in the experiment. Plus, it is unclear to me how the proposed method encourages mask convergence -- by individually using SPDST or JMWST, or by dynamically switching between these two methods?\n- The experiment results are less convincing due to the lack of baselines (e.g., Huang et al. (2022) and Bibikar et al. (2021)) and ablated models. Although mask convergence has been well discussed, training convergence and parameter analysis on mask initialization are expected. It also remains unclear if the proposed method can be applied in a deeper model, such as ResNet34, ResNet50, and ResNet101\n\n# Clarity, Quality, Novelty And Reproducibility\n\n*Clarity*: The methodology in the paper is somewhat hard to follow due to 1) the missing preliminary knowledge for specifying the Federated learning setting, 2) the lack of an overall framework or formal formulation of the proposed method, and 3) the mixup between algorithm lines and descriptions. \n\n*Novelty*: The proposed method seems novel to me owing to its two-stage sparse mask training method and the practical heterogeneous device budget setting. However, it remains unclear to me what is the key contribution of this work to address the observed limitations. The main technical concern is whether the proposed method can be used in very deep neural networks (like over 100 layers and the recent transformer-based architecture).\n\n# Summary Of The Review\n\nOverall, the paper provides a technically sound method to solve several practical challenges in incorporating sparse network training into a Federated learning framework. However, please refer to my concerns on methodology, experiment, and network depth in the above sections.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBYZANTINE-ROBUST DECENTRALIZED LEARNING VIA CLIPPEDGOSSIP\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus and benefit from collaborative training. To address these issues, we propose a CLIPPEDGOSSIP algorithm for Byzantine-robust consensus and optimization, which is the first to provably converge to a O(δmaxζ 2/γ2) neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of CLIPPEDGOSSIP under a large number of attacks.\n\nINTRODUCTION\n\n1 Distributed training arises as an important topic due to privacy constraints of decentralized data storage (McMahan et al., 2017; Kairouz et al., 2019). As the server-worker paradigm suffers from a single point of failure, there is a growing amount of works on training in the absence of server (Lian et al., 2017; Nedic, 2020; Koloskova et al., 2020b). We are particularly interested in decentralized scenarios where direct communication may be unavailable due to physical constraints. For example, devices in a sensor network can only communicate devices within short physical distances.\n\n“Divide et impera”.\n\nFailures—from malfunctioning or even malicious participants—are ubiquitous in all kinds of distributed computing. A Byzantine adversarial worker can deviate from the prescribed algorithm and send arbitrary messages and is assumed to have the knowledge of the whole system (Lamport et al., 2019). It means Byzantine workers not only collude, but also know the data, algorithm, and models of all regular workers. However, they cannot directly modify the states on regular workers, nor compromise messages sent between two connected regular workers.\n\nDefending Byzantine attacks in a communication-constrained graph is challenging. As secure broadcast protocols are no longer available (Pease et al., 1980; Dolev & Strong, 1983; Hirt & Raykov, 2014), regular workers can only utilize information from their own neighbors who have heterogeneous data distribution or are malicious, making it very difficult to reach global consensus. While there are some works attempt to solve this problem (Su & Vaidya, 2016a; Sundaram & Gharesifard, 2018), their strategies suffer from serious drawbacks: 1) they require regular workers to be very densely connected; 2) they only show asymptotic convergence or no convergence proof; 3) there is no evidence if their algorithms are better than training alone.\n\nIn this work, we study the Byzantine-robustness decentralized training in a constrained topology and address the aforementioned issues. The main contributions of our paper are summarized as follows:\n\n• We identify a novel network robustness criterion, characterized in terms of the spectral gap of the topology (γ) and the number of attackers (δ), for consensus and decentralized training, applying to a much broader spectrum of graphs than (Su & Vaidya, 2016a; Sundaram & Gharesifard, 2018). • We propose CLIPPEDGOSSIP as the defense strategy and provide, for the first time, precise rates of robust convergence to a O(δmaxζ 2/γ2) neighborhood of a stationary point for stochastic objectives under standard assumptions.1 We also empirically demonstrate the advantages of CLIPPEDGOSSIP over previous works.\n\n• Along the way, we also obtain the fastest convergence rates for standard non-robust (Byzantine-free)\n\ndecentralized stochastic non-convex optimization by using local worker momentum.\n\n1In a previous version, we referred to CLIPPEDGOSSIP as self-centered clipping.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK Recently there have been extensive works on Byzantine-resilient distributed learning with a trustworthy server. The statistics-based robust aggregation methods cover a wide spectrum of works including median (Chen et al., 2017; Blanchard et al., 2017; Yin et al., 2018; Mhamdi et al., 2018; Xie et al., 2018; Yin et al., 2019), geometric median (Pillutla et al., 2019), signSGD (Bernstein et al., 2019; Li et al., 2019; yong Sohn et al., 2020), clipping (Karimireddy et al., 2021a;b), and concentration filtering (Alistarh et al., 2018; Allen-Zhu et al., 2020; Data & Diggavi, 2021). Other works explore special settings where the server owns the entire training dataset (Xie et al., 2020a; Regatti et al., 2020; Su & Vaidya, 2016b; Chen et al., 2018; Rajput et al., 2019; Gupta et al., 2021). The state-of-the-art attacks take advantage of the variance of good gradients and accumulate bias over time (Baruch et al., 2019; Xie et al., 2019). A few strategies have been proposed to provably defend against such attacks, including momentum (Karimireddy et al., 2021a; El Mhamdi et al., 2021) and concentration filtering (Allen-Zhu et al., 2021).\n\nDecentralized machine learning has been extensively studied in the past few years (Lian et al., 2017; Koloskova et al., 2020b; Li et al., 2021; Ying et al., 2021b; Lin et al., 2021; Kong et al., 2021; Yuan et al., 2021; Kovalev et al., 2021). The state-of-the-art convergence rate is established in (Koloskova et al., 2020b) is O( σ2 nε2 is optimal. In this paper we improve this nε2 + σ2/3 rate to O( σ2\n\nγε3/2 ) where the leading σ2\n\nγ2/3ε4/3 ) using local momentum.\n\nnε2 + σ√\n\nDecentralized machine learning with certified Byzantine-robustness is less studied. When the communication is unconstrained, there exist secure broadcast protocols that guarantee all regular workers have identical copies of each other’s update (Gorbunov et al., 2021; El-Mhamdi et al., 2021). We are interested in a more challenging scenario where not all workers have direct communication links. In this case, regular workers may behave very differently depending on their neighbors in the topology. One line of work constructs a Public-Key Infrastructure (PKI) so that the message from each worker can be authenticated using digital signatures. However, this is very inefficient requiring quadratic communication (Abraham et al., 2020). Further, it also requires every worker to have a globally unique identifier which is known to every other worker. This assumption is rendered impossible on general communication graphs, motivating our work to explicitly address the graph topology in decentralized training. Sybil attacks are an important orthogonal issue where a single Byzantine node can create innumerable “fake nodes” overwhelming the network (cf. recent overview by Ford (2021)). Truly decentralized solutions to this are challenging and sometimes rely on heavy machinery, e.g. blockchains (Poupko et al., 2021) or Proof-of-Personhood (Borge et al., 2017).\n\nMore related to the approaches we study, Su & Vaidya (2016a); Sundaram & Gharesifard (2018); Yang & Bajwa (2019b;a) use trimmed mean at each worker to aggregate models of its neighbors. This approach only works when all regular workers have an honest majority among their neighbors and are densely connected. Guo et al. (2021) evaluate the incoming models of a good worker with its local samples and only keep those well-perform models for its local update step. However, this method only works for IID data. Peng & Ling (2020) reformulate the original problem by adding TV-regularization and propose a GossipSGD type algorithm which works for strongly convex and non-IID objectives. However, its convergence guarantees are inferior to non-parallel SGD. In this work, we address all of the above issues and are able to provably relate the communication graph (spectral gap) with the fraction of Byzantine workers. Besides, most works do not consider attacks that exploit communication topology, except (Peng & Ling, 2020) who propose zero-sum attack. We defer detailed comparisons and more related works to § F.\n\n3 SETUP\n\n3.1 DECENTRALIZED THREAT MODEL\n\nConsider an undirected graph G = (V, E) where V = {1, . . . , n} denotes the set of workers and E denotes the set of edges. Let Ni ⊂ V be the neighbors of node i and N i := Ni ∪ {i}. In addition, we assume there are no self-loops and the system is synchronous. Let VB ⊂ V be the set of Byzantine workers with b = |VB| and the set of regular (non-Byzantine) workers is VR := V\\VB. Let GR be the subgraph of G induced by the regular nodes VR which means removing all Byzantine nodes and their associated edges. If the reduced graph GR is disconnected, then there exist two regular workers who cannot reliably exchange information. In this setting, training on the combined data of all the good workers is impossible. Hence, we make the following necessary assumption.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(A1) Connectivity. GR is connected.\n\nRemark 1. In contrast, Su & Vaidya (2016a); Sundaram & Gharesifard (2018) impose a much stronger assumption that the subgraph of GR of the regular workers remain connected even after additionally removing any |VB| number of edges. For example, the graph in Fig. 1 with 1 Byzantine worker V1 satisfies (A1) but does not satisfy their assumption as removing an additional edge at A1 or B1 may discard the graph cut.\n\nIn decentralized learning, each regular worker i ∈ VR locally stores a vector {Wij}n j=1 of mixing weights, for how to aggregate model updates received from neighbors. We make the following assumption on the weight vectors.\n\n(A2) Mixing weights. The weight vectors on regular workers satisfy the following properties:\n\n• Each regular worker i ∈ VR stores non-negative {Wij}n • The adjacent weights to each regular worker i ∈ VR sum up to 1, i.e. (cid:80)n • For i, j ∈ VR, Wij = Wji.\n\nj=1 with Wij > 0 iff j ∈ N i; j=1 Wij = 1;\n\nj∈VB\n\nWij to be the total weight of adjacent Byzantine edges around a regular\n\nWe can construct such weights even in the presence of Byzantine workers, using algorithms that only rely on communication with local neighbors, e.g. Metropolis-Hastings (Hastings, 1970). We defer details of the construction to § C.2. Note that the Byzantine workers VB might also obtain such weights, however, they can use arbitrary different weights in reality during the training. We define δi := (cid:80) worker i, and define the maximum Byzantine weight as δmax := maxi∈VR δi. Remark 2. In the decentralized setting, the total fraction of Byzantine nodes |VB|/n is irrelevant. Instead, what matters is the fraction of the edge weights they control which are adjacent to regular nodes (as defined by δi and δmax). This is because a Byzantine worker can send different messages along each edge. Thus, a single Byzantine worker connected to all other workers with large edge weights can have a large influence on all the other workers. Similarly, a potentially very large number of Byzantine workers may overall have very little effect—if the edges they control towards good nodes have little weight. When we have a uniform fully connected graph (such as in the centralized setting), the two notions of bad nodes & edges become equivalent.\n\nTo facilitate our analysis of convergence rate, we define a hypothetical mixing matrix (cid:102)W ∈ R(n−b)×(n−b) for the subgraph GR of regular workers with entry i, j ∈ VR defined as\n\n(cid:102)Wij =\n\n(cid:26)Wij\n\nWii + δi\n\nif i (cid:54)= j if i = j.\n\n(1)\n\nBy the construction of this hypothetical matrix (cid:102)W , the following property directly follows. Lemma 3. Given (A2), then (cid:102)W is symmetric and doubly stochastic, i.e.\n\n(cid:102)Wij = (cid:102)Wji, (cid:80)n\n\ni=1 (cid:102)Wij = 1, (cid:80)n\n\nj=1 (cid:102)Wij = 1.\n\n∀i, j ∈ [n−b]\n\nFurther, the spectral gap of the matrix (cid:102)W is positive. Lemma 4. By (A1) and (A2), there exists γ ∈ (0, 1] such that ∀ x ∈ Rn−b and ̄x = 1(cid:62)x\n\n(cid:107) (cid:102)W x − ̄x(cid:107)2 ≤ (1 − γ)(cid:107)x − ̄x(cid:107)2.\n\nn−b 1 ∈ Rn−b (2)\n\nThe γ( (cid:102)W ) is the spectral gap of the subgraph of regular workers GR. We have γ = 0 if and only if GR is disconnected, and γ = 1 if and only if GR is fully connected.\n\nIn summary, γ measures the connectivity of the regular subgraph GR formed after removing the Byzantine nodes, whereas δi and δmax are a measure of the influence of the Byzantine nodes.\n\n3.2 OPTIMIZATION ASSUMPTIONS\n\nWe study the general distributed optimization problem\n\nminx∈Rd f (x) := 1\n\n|VR|\n\n(cid:80)\n\ni∈VR\n\n(cid:8)fi(x) := Eξi∼Di Fi(x; ξi)(cid:9)\n\n(3)\n\non heterogeneous (non-IID) data, where fi is the local objective on worker i with data distribution Di and independent noise ξi. We assume that the gradients computed over these data distributions satisfy the following standard properties.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(A3) Bounded noise and heterogeneity. Assume that for all i ∈ VR and x ∈ Rd, we have\n\nEξ∼Di(cid:107)∇Fi(x; ξ) − ∇fi(x)(cid:107)2 ≤ σ2,\n\nEj∼VR (cid:107)∇fj(x) − ∇f (x)(cid:107)2 ≤ ζ 2.\n\n(4)\n\n(A4) L-smoothness. For i ∈ VR, fi(x) : Rd → R is differentiable and there exists a constant\n\nsL ≥ 0 such that for each x, y ∈ Rd:\n\n(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107).\n\n(5)\n\nWe denote xt\n\ni ∈ Rd as the state of worker i ∈ VR at time t.\n\n4 ROBUST DECENTRALIZED CONSENSUS\n\nAgreeing on one value (consensus) among regular workers is one of the fundamental questions in distributed computing. Gossip averaging is a common consensus algorithm in the Byzantine-free case (δ = 0). Applying gossip averaging steps iteratively to all nodes formally writes as\n\nxt+1\n\ni\n\n:= (cid:80)n\n\nj=1 Wijxt j,\n\nt = 0, 1, . . .\n\n(GOSSIP)\n\nSuppose each worker i ∈ [n] initially owns a different x0 i and (A1) and (A2) hold true, then each worker’s iterate xt j , for all i ∈ [n], which is also known as average consensus (Boyd et al., 2006). Reaching consensus in the presence of Byzantine workers is more challenging, with a long history of study (LeBlanc et al., 2013; Su & Vaidya, 2016a).\n\ni asymptotically converges to x∞\n\ni = ̄x = 1\n\nj=1 x0\n\n(cid:80)n\n\nn\n\n4.1 THE CLIPPED GOSSIP ALGORITHM\n\nWe introduce a novel decentralized gossip-based aggregator, termed CLIPPEDGOSSIP, for Byzantinerobust consensus. CLIPPEDGOSSIP uses its local reference model as center and clips all received neighbor model weights. Formally, for CLIP(z, τ ) := min(1, τ /(cid:107)z(cid:107)) · z, we define for node i\n\nxt+1\n\ni\n\n:= (cid:80)n\n\nj=1Wij(xt\n\ni +CLIP(xt\n\nj −xt\n\ni, τi)),\n\nt = 0, 1, . . .\n\n(CLIPPEDGOSSIP)\n\nTheorem I. Let ̄xt := 1 If the initial consensus distance is bounded as the output xt+1\n\ni∈VR\n\nxt\n\n|VR|\n\n(cid:80)\n\ni\n\ni be the average iterate over the unknown set of regular nodes. i − ̄xt(cid:107)2 ≤ ρ2, then for all i ∈ VR,\n\nE(cid:107)xt\n\n(cid:80)\n\ni∈VR\n\n1 |VR|\n\nof CLIPPEDGOSSIP with an appropriate choice of clipping radius satisfies\n\n(cid:80)\n\n1 |VR|\n\ni∈VR\n\nE(cid:107)xt+1\n\ni − ̄xt(cid:107)2 ≤ (cid:0)1 − γ + c\n\n√\n\nδmax\n\n(cid:1)2\n\nρ2\n\nand E(cid:107) ̄xt+1 − ̄xt(cid:107)2 ≤ c2δmaxρ2\n\nwhere the expectation is over the random variable {xt\n\ni}i∈VR and c > 0 is a constant.\n\nWe inspect Theorem I on corner cases. If regular workers have already reached consensus before aggregation (ρ = 0), then Theorem I shows that we retain consensus even in the face of Byzantine agents. In this case, we can use a simple majority, which corresponds to setting clipping threshold τi = 0. Further, if there is no Byzantine worker (δmax = 0), then the robust aggregator must improve the consensus distance by a factor of (1 − γ)2 which matches standard gossiping analysis (Boyd et al., 2006). Finally, for the complete graph (γ = 1) CLIPPEDGOSSIP satisfies the centralized notion of (δmax, c2)-robust aggregator in (Karimireddy et al., 2021a, Definition C). Thus, CLIPPEDGOSSIP recovers all past optimal aggregation methods as special cases.\n\nNote that if the topology is poorly connected and there are Byzantine attackers with (γ < c δmax), then Theorem I gives no guarantee that the consensus distance will reduce after aggregation. This is unfortunately not possible to improve upon, as we will show in the following § 4.2—if the connectivity is poor then the effect of Byzantine workers can be significantly amplified.\n\n√\n\n4.2 LOWER BOUNDS DUE TO COMMUNICATION CONSTRAINTS\n\nNot all pairs of workers have direct communication links due to constraints such as physical distances in a sensor network. It is common that a subset of sensors are clustered within a small physical space while only few of them have communication links to the rest of the sensors. Such links form a cut-set of the communication topology and are crucial for information diffusion. On the other hand, attackers can increase consensus errors in the presence of these critical links.\n\nTheorem II. Consider networks satisfying (A1) of n nodes, each holding a number in {0, 1}, and only O(1/n2) of the edges are adjacent to attackers. For any robust consensus algorithm A, there exists a network such that the output of A has an average consensus error of at least Ω(1).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: A dumbbell topology of two cliques A and B of regular workers connected by an edge (graph cut). Byzantine workers (red) may attack the graph at different places.\n\nFigure 2: Accuracies of models trained with robust aggregators over dumbbell topology and CIFAR10 dataset (δ = 0). The models are averaged within clique A, B, or all regular workers separately.\n\nFigure 3: Performance of CLIPPEDGOSSIP and baselines (TM and MEDIAN) under Byzantine attacks with varying γ and δmax. Each point represents the squared average consensus error of the last iterate of an algorithm. MEDIAN and TM have identical performance and CLIPPEDGOSSIP is consistently better. Further, the performance of CLIPPEDGOSSIP is best explained by the magnitude of (δ/γ2) – it is excellent when the ratio is less than a threshold and degrades as it increases.\n\nProof. Consider two cliques A and B with n nodes each connected by an edge to each other and to a Byzantine node V2, c.f. Fig. 1. Suppose that we know all nodes have values in {0, 1}. Let all nodes in A have value 0. Now consider two settings:\n\nWorld 1. All B nodes have value 0. However, Byzantine node V2 pretends to be part of a clique identical to B which it simulates, except that all nodes have value 1. The true consensus average is 0. World 2. All B nodes have value 1. This time the Byzantine node V2 simulates clique B with value 0. The true consensus average here is 0.5.\n\nFrom the perspective of clique A, the two worlds are identical–it seems to be connected to one clique with value 0 and another with value 1. Thus, it must make Ω(1) error at least in one of the worlds. This proves that consensus is impossible in this setting.\n\nWhile arguments above are similar to classical lower bounds in decentralized consensus which show we need δ ≤ 1/3 (Fischer et al., 1986), in our case there is only 1 Byzantine node (out of 2n + 1 regular nodes) which controls only 2 edges i.e. δ = O(1/n2). This impossibility result thus drives home the additional impact through the restricted communication topology. Further, past impossibility results about robust decentralized consensus such as (Sundaram & Gharesifard, 2018; Su & Vaidya, 2016a) use combinatorial concepts such as the number of node-disjoint paths between the good nodes. However, such notions cannot account for the edge weights easily and cannot give finite-time convergence guarantees. Instead, our theory shows that the ratio of δmax/γ2 accurately captures the difficulty of the problem. We next verify this empirically.\n\nIn Fig. 3, we show the final consensus error of three defenses under Byzantine attacks. TM and MEDIAN have a large error even for small δmax and large γ. The consensus error of CLIPPEDGOSSIP increases almost linearly with δmax/γ2. However, this phenomenon is not observed by looking at γ−2 or δmax alone, validating our theoretical analysis in Theorem I. Details are deferred to § D.1.\n\n5\n\nClique AClique BCutDumbbellV2V1A1B1GMMOZITMClippedGos.Agg020406080Accuracy (%)IIDGMMOZITMClippedGos.AggNonIIDGroupglobalclique Aclique B1/20ErrorClippedGossipMedianTM/2Under review as a conference paper at ICLR 2023\n\nAlgorithm 1 Byzantine-Resilient Decentralized Optimization with CLIPPEDGOSSIP\n\nInput: x0 ∈ Rd, α, η, {τ t\n\ni }, m0\n\ni = gi(x0)\n\n1: for t = 0, 1, . . . do 2: 3:\n\nfor i = 1, . . . , n in parallel\n\ni − ηmt+1\n\ni\n\ni = (1 − α)mt\n\ni\n\nmt+1 xt+1/2 = xt Exchange xt+1/2 xt+1 end for\n\ni\n\nwith Ni\n\n4:\n\n5:\n\n6: 7:\n\ni + αgi(xt i) if i ∈ VR else ∗\n\ni = CLIPPEDGOSSIPi(xt+1/2\n\n1\n\n, . . . , xt+1/2\n\nn\n\n; τ t+1\n\ni\n\n)\n\nTable 1: Comparison with prior work of convergence rates for non-convex objectives to a O(δζ 2)- neighborhood of stationary points. We recover comparable or improved rates as special cases.\n\nReference\n\nSetting\n\nConvergence to ε-accuracy\n\nRegular (δ = 0) Decentralized\n\nByzantine-robust Fully-connected (γ = 1) IID (ζ = 0)\n\nKoloskova et al. (2020b)\n\nThis work\n\nGuo et al. (2021) Gorbunov et al. (2021)\n\nGorbunov et al. (2021)\n\nThis work\n\nByzantine-robust Federated Learning\n\nKarimireddy et al. (2021b)\n\nThis work\n\n-\n\nδ = 0\n\n- δ known δ unknown γ = 1, ζ = 0\n\n-\n\nγ = 1\n\nO( σ2 O( σ2\n\nnε2 + ζ nε2 + ζ\n\nγε3/2 + σ γε3/2 + σ2/3\n\nγε3/2 + 1 γε ) γ2/3ε4/3 + 1 γε )\n\n√\n\nO( σ2 O( σ2\n\nO( σ2\n\n(cid:55) nε2 + nδσ2 ε ) † mε + 1 nε2 + n2δσ2 ε ) † mε + 1 nε2 + δσ2 ε2 + 1 ε ) n )+ 1 ε2 (δ+ 1 ε ) ε3/2 + σ2/3 n )+ ζ ε4/3 + 1 ε )\n\nO( σ2 ε2 (δ+ 1\n\nO( σ2\n\n† This method does not generalize to constrained communication topologies.\n\n5 ROBUST DECENTRALIZED OPTIMIZATION\n\nThe general decentralized training algorithm can be formulated as\n\nxt+1/2\n\ni\n\n:=\n\ni − ηgi(xt i)\n\n(cid:26)xt ∗\n\ni ∈ VR i ∈ VB\n\n,\n\nxt+1\n\ni\n\n:= AGGi({xt+1/2\n\nk\n\n: k ∈ N i})\n\nwhere η is the learning rate, gi(x) := ∇F (x, ξi) is a stochastic gradient, and ξt i ∼ Di is the random batch at time t on worker i. The received message xt+1/2 can be arbitrary for Byzantine nodes k ∈ VB. Replacing AGG with plain gossip averaging (GOSSIP) recovers standard gossip SGD (Koloskova et al., 2019). Under the presence of Byzantine workers, which is the main interest of our work, we will show that we can replace AGG with CLIPPEDGOSSIP and use local worker momentum to achieve Byzantine robustness (Karimireddy et al., 2021a). The full procedure is described in Algorithm 1. Theorem III. Suppose Assumptions 1–4 hold and δmax = O(γ2). Then for α := 3ηL, Algorithm 1 reaches\n\nγ2 + ε in iteration complexity\n\nt=0(cid:107)∇f ( ̄xt)(cid:107)2\n\n1 T +1\n\n(cid:80)T\n\nk\n\n2 ≤ δmaxζ2 (cid:16) 1 n\n\n(cid:18) σ2 nε2\n\nO\n\n+δmax\n\n(cid:17)\n\n+\n\nζ γε3/2\n\n+\n\nσ2/3 γ2/3ε4/3\n\n+\n\n(cid:19)\n\n.\n\n1 γε\n\nFurthermore, the consensus distance satisfies the upper bound\n\n1 |VR|\n\n(cid:80)\n\ni∈VR\n\n(cid:107)xT\n\ni − ̄xT (cid:107)2\n\n2 ≤ O(\n\nζ2 γ2(T +1) ).\n\nWe compare our analysis with existing works for non-convex objectives in Table 1.\n\nRegular decentralized training. Even if there are no Byzantine workers (δmax = 0), our convergence rate is slightly faster than that of standard gossip SGD (Koloskova et al., 2020b). The difference is that our third term O( σ2/3 γε3/2 ) for large σ and small ε. This is because we use local momentum which reduces the effect of variance σ. Thus momentum has a double use in this paper in achieving robustness as well as accelerating optimization.\n\nγ2/3ε4/3 ) is faster than their O(\n\nσ√\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nByzantine-robust federated learning. Federated learning uses a fully connected graph (γ = 1). We compare state of the art federated learning method (Karimireddy et al., 2021b) with our rate when γ = 1. Both algorithms converge to a Θ(δζ 2)-neighborhood of a stationary point and share the same leading term. This neighborhood can be circumvented with strong growth condition and overparameterized models (Karimireddy et al., 2021b, Theorem III). We incur additional higher-order terms O( γ2/3ε4/3 ) as a penalty for the generality of our analysis. This shows that the trusted server in federated learning can be removed without significant slowdowns.\n\nγε3/2 + σ2/3\n\nζ\n\nByzantine-robust decentralized SGD with fully connected topology. If we limit our analysis to a special case of a fully connected graph (γ = 1) and IID data (ζ = 0), then our rate has the same leading term as (Gorbunov et al., 2021), which enjoys the scaling of the total number of regular nodes. The δσ2 second term O( n ε ) for small ε because they additionally validate m random updates in each step. However, (Gorbunov et al., 2021) relies on secure protocols which do not easily generalize to constrained communication.\n\nε ) of (Gorbunov et al., 2021) is better than our O( 1\n\nδσ2\n\nm\n\nε\n\nByzantine-robust decentralized SGD with constrained communication. MOZI (Guo et al., 2021) does not provide a theoretical analysis on convergence and TM (Sundaram & Gharesifard, 2018; Su & Vaidya, 2016a; Yang & Bajwa, 2019a) only prove the asymptotic convergence of full gradient under a very strong assumption on connectivity and local honest majority.2 Peng & Ling (2020) don’t prove a rate for non-convex objective; but Gorbunov et al. (2021) which shows convergence of (Peng & Ling, 2020) on strongly convex objectives at a rate inferior to parallel SGD. In contrast, our convergence rate matches the standard stochastic analysis under much weaker assumptions than Sundaram & Gharesifard (2018); Su & Vaidya (2016a); Yang & Bajwa (2019a). Unlike these prior works, our guarantees hold even if some subsets of nodes are surrounded by a majority of Byzantine attackers. This can also be observed in practice, as we show in § D.2.3.\n\nConsensus for Byzantine-robust decentralized optimization. Theorem III gives a non-trivial result that regular workers reach consensus under the CLIPPEDGOSSIP aggregator. In Fig. 2 we demonstrate the consensus behavior of robust aggregators on the CIFAR-10 dataset on a dumbbell topology, without attackers (δ = 0). We compare the accuracies of models averaged within cliques A and B with model averaged over all workers. In the IID setting, the clique-averaged models of GM and TM are over 80% accuracy but the globally-averaged models are less than 30% accuracy. It means clique A and clique B are converging to two different critical points and GM and TM fail to reach consensus within the entire network! In contrast, the globally-averaged model of CLIPPEDGOSSIP is as good as or better than the clique-averaged models, both in the IID and non-IID setting.\n\nFinally, we point out some avenues for further improvement: our results depend on the worst-case δmax. We believe it is possible to replace it with a (weighted) average of the {δi} instead. Also, extending our protocols to time-varying topologies would greatly increase their practicality. Remark 5 (Adaptive choice of clipping radius τ t i ). In § D.5, we give an adaptive rule to choose the clipping radius τ t i for all i ∈ VR and times t, based on the top percentile of close neighbors. This adaptive rule results in a value τ t i slightly smaller than the required theoretical value to preserve Byzantine robustness. In experiments, we found that the performance of optimization is robust to small perturbations of the clipping radius and that the adaptive rule performs well in all cases.\n\n6 EXPERIMENTS\n\nIn this section, we empirically demonstrate successes and failures of decentralized training in the presence of Byzantine workers, and compare the performance of CLIPPEDGOSSIP with existing robust aggregators: 1) geometric median GM (Pillutla et al., 2019); 2) coordinate-wise trimmed mean TM (Yang & Bajwa, 2019a); 3) MOZI (Guo et al., 2020). Coordinate-wise median (Yin et al., 2018) and Krum (Blanchard et al., 2017) usually perform worse than GM so we exclude them in the experiments. All implementations are based on PyTorch (Paszke et al., 2019) and evaluated on different graph topologies, with a distributed MNIST dataset (LeCun & Cortes, 2010). We defer the experiments on CIFAR10 (Krizhevsky et al., 2009) to § D.3. 3\n\nWe defer details of robust aggregators to § A, attacks to § B, topologies and mixing matrix to § C and experiment setups and additional experiments to § D.\n\n2MOZI is renamed to UBAR in the latest version. 3The code is available at this anonymous repository.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Accuracy of the averaged model in clique A for the dumbbell topology. In the plot title “B.” stands for the bucketing (aggregating means of bucketed values) and “R.” stands for adding 1 additional random edge between two cliques. We see that i) CLIPPEDGOSSIP is consistently the best matching ideal averaging performance, ii) performance mildly improves by using bucketing, and iii) significantly improves when adding a single random edge (thereby improving connectivity).\n\n6.1 DECENTRALIZED DEFENSES WITHOUT ATTACKERS\n\nChallenging topologies and data distribution may prevent existing robust aggregators from reaching consensus even when there is no Byzantine worker (δ = 0). In this part, we consider the “dumbbell” topology c.f. Fig. 1. As non-IID data distribution, we split the training dataset by labels such that workers in clique A are training on digits 0 to 4 while workers in clique B are training on digits 5 to 9. This entanglement of topology and data distribution is motivated by realistic geographic constraints such as continents with dense intra-connectivity but sparse inter-connection links e.g. through an undersea cable. In Fig. 4 we compare CLIPPEDGOSSIP with existing robust aggregators GM, TM, MOZI in terms of their accuracies of averaged model in clique A. The ideal communication refers to aggregation with gossip averaging.\n\nExisting robust aggregators impede information diffusion. When cliques A and B have distinct data distribution (non-IID), workers in clique A rely on the graph cut to access the full spectrum of data and attain good performance. However, existing robust aggregators in clique A completely discard information from clique B because: 1) clique B model updates are outliers to clique A due to data heterogeneity; 2) clique B updates are outnumbered by clique A updates — clique A can only observe 1 update from B due to constrained communication. The 2nd plot in Fig. 4 shows that GM, TM, and MOZI only reach 50% accuracy in the non-IID setting, supporting that they impede information diffusion. This is in contrast to the 1st plot where cliques A and B have identical data distribution (IID) and information on clique A alone is enough to attain good performance. However, reaching local models does not imply reaching consensus, c.f. Fig. 2. On the other hand, CLIPPEDGOSSIP is the only robust aggregator that preserves the information diffusion rate as the ideal gossip averaging.\n\nTechniques that improve information diffusion. To address these issues, we locally employ the bucketing technique of (Karimireddy et al., 2021b) for the non-IID case in the 3rd subplot. Plots 4 and 5 demonstrate the impact of one additional edge between the cliques to improve the spectral gap.\n\n• The bucketing technique randomly inputs received vectors into buckets of equal size, averages the vectors in each bucket, and finally feeds the averaged vectors to the aggregator. While bucketing helps TM to overcome 50% accuracy, TM is still behind CLIPPEDGOSSIP. GM only improves by 1% while MOZI remains at almost the same accuracy.\n\n• Adding one more random edge between two cliques improves the spectral gap γ from 0.0154 to 0.0286. CLIPPEDGOSSIP and gossip averaging converge faster as the theory predicts. However, TM, GM, and MOZI are still stuck at 50% for the same heterogeneity reason. • Bucketing and adding a random edge help all aggregators exceed 50% accuracy.\n\n6.2 DECENTRALIZED LEARNING UNDER MORE ATTACKS AND TOPOLOGIES.\n\nIn this section, we compare robust aggregators over more topologies and Byzantine attacks in the nonIID setting. We consider two topologies: randomized small world (γ = 0.084) and torus (γ = 0.131). They are much less restrictive than the dumbbell topology (γ = 0.043) where all existing aggregators fail to reach consensus even δ = 0. For attacks, we implement state of the art federated attacks Inner product manipulation (IPM) (Xie et al., 2019) and A little is enough (ALIE) (Baruch et al., 2019) and label-flipping (LF) and bit-flipping (BF). Details about topologies and the adaptation of FL attacks to the decentralized setup are provided in § C.1 and § B.\n\n8\n\n0250500750Iterations5060708090100Accuracy (%)IIDClippedGossipGMMOZITMIdeal Comm.0250500750IterationsNonIID0250500750IterationsNonIID + B.0250500750IterationsNonIID + R.0250500750IterationsNonIID + B. + R.Under review as a conference paper at ICLR 2023\n\nFigure 5: Robust aggregators on randomized small-world (10 regular nodes) and torus topology (9 regular nodes) under Byzantine attacks (2 attackers). We observe that across all attacks and networks, clipped gossip has excellent performance, with the geometric median (GM) coming second.\n\nThe results in Fig. 5 show that CLIPPEDGOSSIP has consistently superior performance under all topologies and attacks. All robust aggregators are generally performing better on easier topology (large γ). The GM has a very good performance on these two topologies but, as we have demonstrated in the dumbbell topology, GM does not work in more challenging topologies. Therefore, CLIPPEDGOSSIP is recommended for a general constrained topology.\n\n6.3 LOWER BOUND OF OPTIMIZATION We empirically investigate the lower bound of optimization O(δmaxζ 2γ−2) in Theorem III. In this experiment, we fix spectral gap γ, heterogeneity ζ 2 and use different δmax fractions of Byzantine edges in the dumbbell topology. The Byzantine workers are added to V1 in clique A and its mirror node in clique B. We define the following dissensus attack for decentralized optimization\n\nDefinition A (DISSENSUS attack). For i ∈ VR and εi > 0, a dissensus attacker j ∈ Ni ∩ VB sends (6)\n\nxj := xi − εi\n\nk∈Ni∩VR\n\n(cid:80)\n\n(cid:80)\n\n.\n\nWik(xk−xi) Wij\n\nj∈Ni∩VB\n\nThe resulting Figure 6 shows that with increasing δmax the model quality drops significantly. This is in line with our proven robust convergence rate in terms of δmax. Notice that for large δmax, the model averaged over all workers performs even worse than those averaged within cliques. It means the models in two cliques are essentially disconnected and are converging to different local minima or stationary points of a non-convex landscape. See § D.2.2 for details.\n\nFigure 6: Effect of the number of attackers on the accuracy of CLIPPEDGOSSIP under dissensus attack with varying δmax and fixed γ, ζ 2. The solid (resp. dashed) lines denote models averaged over all (resp. clique A or B) regular workers. The right figure shows the performance of the last iterates of curves in the left figure.\n\n7 DISCUSSION The main takeaway from our work is that illconnected communication topologies can vastly magnify the effect of bad actors. As long as the communication topology is reasonably well connected (say γ = 0.35) and the fraction of attackers is mild (say δ = 10%), clipped gossip provably ensures robustness. Under more extreme conditions, however, no algorithm can guarantee robust convergence. Given that decentralized consensus has been proposed as a backbone for digital democracy (Bulteau et al., 2021), and that decentralized learning is touted to be an alternative to current centralized training paradigms, our findings are significant. A simple strategy we recommend (along with using CLIPPEDGOSSIP) is adding random edges to improve the connectivity and robustify the network.\n\n9\n\n5060708090100Accuracy (%)Small-world | BFSmall-world | LFSmall-world | ALIESmall-world | IPM0200400600Iterations5060708090100Torus | BF0200400600IterationsTorus | LF0200400600IterationsTorus | ALIEClippedGossipGMMOZITM0200400600IterationsTorus | IPM05001000Iterations20406080Accuracy (%)max0.00.06250.1250.18750.250.00.10.2max GroupAllClique AClique BUnder review as a conference paper at ICLR 2023\n\nBIBLIOGRAPHY\n\nIttai Abraham, T-H. Hubert Chan, Danny Dolev, Kartik Nayak, Rafael Pass, Ling Ren, and Elaine Shi. Commu-\n\nnication complexity of byzantine agreement, revisited, 2020.\n\nDan Alistarh, Zeyuan Allen-Zhu, and Jerry Li.\n\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 4618–4628, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ a07c2f3b3b907aaf8436a26c6d77f0a2-Abstract.html.\n\nByzantine stochastic gradient descent.\n\nZeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, and Dan Alistarh. Byzantine-resilient non-convex stochastic\n\ngradient descent. arXiv preprint arXiv:2012.14368, 2020.\n\nZeyuan Allen-Zhu, Faeze Ebrahimianghazani, Jerry Li, and Dan Alistarh. Byzantine-resilient non-convex stochastic gradient descent. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= PbEHqvFtcS.\n\nMahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat. Stochastic gradient push for distributed\n\ndeep learning, 2019.\n\nGilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8632–8642, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html.\n\nJeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with majority vote\n\nis communication efficient and fault tolerant, 2019.\n\nPeva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 119–129, 2017. URL https://proceedings.neurips.cc/paper/2017/ hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html.\n\nMaria Borge, Eleftherios Kokoris-Kogias, Philipp Jovanovic, Linus Gasser, Nicolas Gailly, and Bryan Ford. Proof-of-personhood: Redemocratizing permissionless cryptocurrencies. In 2017 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW), pp. 23–26. IEEE, 2017.\n\nStephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE\n\ntransactions on information theory, 52(6):2508–2530, 2006.\n\nLaurent Bulteau, Gal Shahaf, Ehud Shapiro, and Nimrod Talmon. Aggregation over metric spaces: Proposing and voting in elections, budgeting, and legislation. Journal of Artificial Intelligence Research, 70:1413–1439, 2021.\n\nLukas Burkhalter, Hidde Lycklama, Alexander Viand, Nicolas Küchler, and Anwar Hithnawi. Rofl: Attestable\n\nrobustness for secure federated learning, 2021.\n\nLingjiao Chen, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos. DRACO: byzantine-resilient distributed training via redundant gradients. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 902–911. PMLR, 2018. URL http://proceedings.mlr.press/v80/chen18l.html.\n\nYudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):1–25, 2017.\n\nGeorgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Rhicheek Patra, and Mahsa Taziki. Asynchronous byzantine machine learning (the case of SGD). In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1153– 1162. PMLR, 2018. URL http://proceedings.mlr.press/v80/damaskinos18a.html.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDeepesh Data and Suhas Diggavi. Byzantine-resilient sgd in high dimensions on heterogeneous data. In 2021\n\nIEEE International Symposium on Information Theory (ISIT), pp. 2310–2315. IEEE, 2021.\n\nDanny Dolev and H. Raymond Strong. Authenticated algorithms for byzantine agreement. SIAM J. Comput., 12:\n\n656–666, 1983.\n\nEl Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, Lê-Nguyên Hoang, and Sébastien Rouault. Collaborative learning in the jungle (decentralized, byzantine, heterogeneous, asynchronous and nonconvex learning). Advances in Neural Information Processing Systems, 34, 2021.\n\nEl Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Louis Alexandre Rouault. Distributed momentum for byzantine-resilient stochastic gradient descent. In 9th International Conference on Learning Representations (ICLR), number CONF, 2021.\n\nMichael J Fischer, Nancy A Lynch, and Michael Merritt. Easy impossibility proofs for distributed consensus\n\nproblems. Distributed Computing, 1(1):26–39, 1986.\n\nBryan Ford. 10. technologizing democracy or democratizing technology? a layered-architecture perspective on potentials and challenges. In Digital Technology and Democratic Theory, pp. 274–321. University of Chicago Press, 2021.\n\nEduard Gorbunov, Alexander Borzunov, Michael Diskin, and Max Ryabinin. Secure distributed training at scale,\n\n2021.\n\nShangwei Guo, Tianwei Zhang, Xiaofei Xie, Lei Ma, Tao Xiang, and Yang Liu. Towards byzantine-resilient\n\nlearning in decentralized systems. arXiv 2002.08569, 2020.\n\nShangwei Guo, Tianwei Zhang, Han Yu, Xiaofei Xie, Lei Ma, Tao Xiang, and Yang Liu. Byzantine-resilient\n\ndecentralized stochastic gradient descent, 2021.\n\nNirupam Gupta, Thinh T Doan, and Nitin Vaidya. Byzantine fault-tolerance in federated local sgd under\n\n2f-redundancy. arXiv preprint arXiv:2108.11769, 2021.\n\nAric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.\n\nW Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.\n\nLie He, Sai Praneeth Karimireddy, and Martin Jaggi. Secure byzantine-robust machine learning. arXiv\n\n2006.04747, 2020.\n\nMartin Hirt and Pavel Raykov. Multi-valued byzantine broadcast: The t < n case. In ASIACRYPT, 2014.\n\nPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaïd Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. arXiv 1912.04977, 2019.\n\nSai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for byzantine robust optimization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 5311–5319. PMLR, 2021a. URL http://proceedings.mlr.press/v139/karimireddy21a. html.\n\nSai Praneeth Karimireddy, Lie He, and Martin Jaggi. Byzantine-robust learning on heterogeneous datasets via\n\nbucketing, 2021b.\n\nAnastasia Koloskova, Sebastian U. Stich, and Martin Jaggi. Decentralized stochastic optimization and gossip\n\nalgorithms with compressed communication, 2019.\n\nAnastasia Koloskova, Tao Lin, Sebastian U. Stich, and Martin Jaggi. Decentralized deep learning with arbitrary\n\ncommunication compression, 2020a.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAnastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U. Stich. A unified theory of decentralized SGD with changing topology and local updates. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5381–5393. PMLR, 2020b. URL http://proceedings.mlr. press/v119/koloskova20a.html.\n\nAnastasiia Koloskova, Tao Lin, and Sebastian U Stich. An improved analysis of gradient tracking for decentral-\n\nized machine learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nLingjing Kong, Tao Lin, Anastasia Koloskova, Martin Jaggi, and Sebastian U Stich. Consensus control for\n\ndecentralized deep learning. arXiv preprint arXiv:2102.04828, 2021.\n\nDmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, and Sebastian Stich. A linearly convergent algorithm for decentralized optimization: Sending less bits for free! In International Conference on Artificial Intelligence and Statistics, pp. 4087–4095. PMLR, 2021.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nLeslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. In Concurrency: the\n\nWorks of Leslie Lamport, pp. 203–226. 2019.\n\nHeath J LeBlanc, Haotian Zhang, Xenofon Koutsoukos, and Shreyas Sundaram. Resilient asymptotic consensus\n\nin robust networks. IEEE Journal on Selected Areas in Communications, 31(4):766–781, 2013.\n\nYann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.\n\ncom/exdb/mnist/.\n\nLiping Li, Wei Xu, Tianyi Chen, Georgios B. Giannakis, and Qing Ling. RSA: byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 1544–1551. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.33011544. URL https://doi.org/10.1609/aaai.v33i01.33011544.\n\nXiang Li, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Communication-efficient local decentralized sgd\n\nmethods, 2021.\n\nXiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent, 2017.\n\nTao Lin, Sai Praneeth Karimireddy, Sebastian U Stich, and Martin Jaggi. Quasi-global momentum: Accelerating\n\ndecentralized deep learning on heterogeneous data. arXiv preprint arXiv:2102.04761, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. Communicationefficient learning of deep networks from decentralized data. In Aarti Singh and Xiaojin (Jerry) Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, volume 54 of Proceedings of Machine Learning Research, pp. 1273–1282. PMLR, 2017. URL http://proceedings.mlr.press/v54/mcmahan17a.html.\n\nEl Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault. The hidden vulnerability of distributed learning in byzantium. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3518–3527. PMLR, 2018. URL http: //proceedings.mlr.press/v80/mhamdi18a.html.\n\nAngelia Nedic. Distributed gradient methods for convex machine learning problems in networks: Distributed\n\noptimization. IEEE Signal Processing Magazine, 37(3):92–101, 2020.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nMarshall C. Pease, Robert E. Shostak, and Leslie Lamport. Reaching agreement in the presence of faults. J.\n\nACM, 27:228–234, 1980.\n\nJie Peng and Qing Ling. Byzantine-robust decentralized stochastic optimization. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pp. 5935–5939. IEEE, 2020. doi: 10.1109/ICASSP40776.2020.9054377. URL https://doi.org/10. 1109/ICASSP40776.2020.9054377.\n\nKrishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. arXiv\n\npreprint arXiv:1912.13445, 2019.\n\nOuri Poupko, Gal Shahaf, Ehud Shapiro, and Nimrod Talmon. Building a sybil-resilient digital community\n\nutilizing trust-graph connectivity. IEEE/ACM Transactions on Networking, 2021.\n\nShashank Rajput, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos. DETOX: A In Hanna M. Walredundancy-based framework for faster and more robust gradient aggregation. lach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10320–10330, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/415185ea244ea2b2bedeb0449b926802-Abstract.html.\n\nJayanth Regatti, Hao Chen, and Abhishek Gupta. Bygars: Byzantine sgd with arbitrary number of attackers,\n\n2020.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\n\narXiv preprint arXiv:1409.1556, 2014.\n\nLili Su and Nitin Vaidya. Multi-agent optimization in the presence of byzantine adversaries: Fundamental limits.\n\nIn 2016 American Control Conference (ACC), pp. 7183–7188. IEEE, 2016a.\n\nLili Su and Nitin H Vaidya. Robust multi-agent optimization: coping with byzantine agents with input redundancy. In International Symposium on Stabilization, Safety, and Security of Distributed Systems, pp. 368–382. Springer, 2016b.\n\nShreyas Sundaram and Bahman Gharesifard. Distributed optimization under adversarial nodes. IEEE Transac-\n\ntions on Automatic Control, 64(3):1063–1076, 2018.\n\nHanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over decentralized data,\n\n2018.\n\nThijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Practical low-rank communication compression in\n\ndecentralized deep learning. In NeurIPS, 2020.\n\nThijs Vogels, Lie He, Anastasia Koloskova, Tao Lin, Sai Praneeth Karimireddy, Sebastian U. Stich, and Martin\n\nJaggi. Relaysum for decentralized deep learning on heterogeneous data, 2021.\n\nDuncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’networks. nature, 393(6684):\n\n440–442, 1998.\n\nCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized byzantine-tolerant sgd. arXiv 1802.10116,\n\n2018.\n\nCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant SGD by inner product manipulation. In Amir Globerson and Ricardo Silva (eds.), Proceedings of the ThirtyFifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp. 261–270. AUAI Press, 2019. URL http: //proceedings.mlr.press/v115/xie20a.html.\n\nCong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous SGD. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 10495–10503. PMLR, 2020a. URL http: //proceedings.mlr.press/v119/xie20c.html.\n\nCong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In International\n\nConference on Machine Learning, pp. 10495–10503. PMLR, 2020b.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nYi-Rui Yang and Wu-Jun Li. BASGD: buffered asynchronous SGD for byzantine learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 11751–11761. PMLR, 2021. URL http://proceedings.mlr.press/v139/yang21e.html.\n\nZhixiong Yang and Waheed U Bajwa. Bridge: Byzantine-resilient decentralized gradient descent. arXiv\n\n1908.08098, 2019a.\n\nZhixiong Yang and Waheed U Bajwa. Byrdie: Byzantine-resilient distributed coordinate descent for decentralized\n\nlearning. IEEE Transactions on Signal and Information Processing over Networks, 2019b.\n\nDong Yin, Yudong Chen, Kannan Ramchandran, and Peter L. Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 5636–5645. PMLR, 2018. URL http://proceedings.mlr.press/v80/yin18a.html.\n\nDong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Defending against saddle point attack in byzantine-robust distributed learning. In International Conference on Machine Learning, pp. 7074–7084. PMLR, 2019.\n\nBicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin. Exponential graph is provably efficient for decentralized deep training. Advances in Neural Information Processing Systems, 34, 2021a.\n\nBicheng Ying, Kun Yuan, Hanbin Hu, Yiming Chen, and Wotao Yin. Bluefog: Make decentralized algorithms\n\npractical for optimization and deep learning. arXiv preprint arXiv:2111.04287, 2021b.\n\nJy yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon. Election coding for distributed learning:\n\nProtecting signsgd against byzantine attacks, 2020.\n\nKun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin. Decentlam:\n\nDecentralized momentum sgd for large-batch deep training. arXiv preprint arXiv:2104.11981, 2021.\n\nChengcheng Zhao, Jianping He, and Qing-Guo Wang. Resilient distributed optimization algorithm against\n\nadversarial attacks. IEEE Transactions on Automatic Control, 65(10):4308–4315, 2019.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAppendices\n\nCONTENTS OF THE APPENDIX\n\nA Existing robust aggregators\n\nB Byzantine attacks in the decentralized environment\n\nB.1 Existing attacks in federated learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Dissensus attack and other attacks in the decentralized environment\n\nC Topologies and mixing matrices C.1 Constrained topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Constructing mixing matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD Experiments\n\nD.1 Byzantine-robust consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Byzantine-robust decentralized optimization . . . . . . . . . . . . . . . . . . . . . D.2.1 Setup for “Decentralized defenses without attackers” . . . . . . . . . . . . D.2.2 Setup for “Effects of the number of Byzantine workers” . . . . . . . . . . D.2.3 Setup for “Defense without honest majority” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.4 Setup for “More topologies and attacks.” D.3 Experiment: CIFAR-10 task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Experiment for “Weaker topology assumption” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Experiment: choosing clipping radius\n\nE Analysis\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 Definitions, and inequalities E.2 Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nE.3 Proof of the main theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nF Other related works and discussions\n\nA EXISTING ROBUST AGGREGATORS\n\n15\n\n16 16 16\n\n17 17 18\n\n18 18 19 20 21 21 21 22 22 23\n\n24 24 25 33\n\n36\n\nIn this section, we describe existing robust aggregators mentioned in this paper. Regular nodes can replace gossip averaging (GOSSIP) with robust aggregators in the federated learning. Let’s take geometric median and trimmed mean for example.\n\n• Geometric median (GM). Pillutla et al. (2019) implements the geometric median\n\nGM(x1, . . . , xn) := arg min\n\nv\n\n(cid:80)n\n\ni=1(cid:107)v − xi(cid:107)2.\n\n• Coordinate-wise trimmed mean (TM). Yin et al. (2018); Yang & Bajwa (2019a) computes the\n\nk-th coordinate of TM as\n\n[TM(x1, . . . , xn)]k :=\n\n1 (1−2β)n\n\n(cid:80)\n\ni∈Uk\n\n[xi]k\n\nwhere Uk is a subset of [n] obtained by removing the largest and smallest β-fraction of its elements.\n\nThese aggregators don’t take advantage of the trusted local information and treat all models equally.\n\nThe MOZI algorithm (Guo et al., 2021) leverages local information to filter outliers.\n\n• Mozi. Guo et al. (2021) applies two screening steps on worker i ∈ VR\n\nN s\n\ni\n\n:= arg min N ∗⊂Ni |N ∗|=δi|Ni|\n\n(cid:88)\n\nj∈N ∗\n\n(cid:107)xi − xj(cid:107),\n\nN r\n\ni\n\n:=N s\n\ni ∩ {j ∈ [n] : (cid:96)(xj, ξi) ≤ (cid:96)(xi, ξi)}\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ξi ∼ Di is a random sample. If N r they update the model with\n\ni = ∅, then redefine N r\n\ni\n\n:= {arg minj (cid:96)(xj, ξi)}. Then\n\nxt+1\n\ni\n\n:= αxt\n\ni + 1−α |N r i |\n\n(cid:80)\n\nj∈N r i\n\nwhere α ∈ [0, 1] is an hyperparameter.\n\nxt\n\nj − η∇Fi(xt\n\ni; ξt i )\n\nB BYZANTINE ATTACKS IN THE DECENTRALIZED ENVIRONMENT\n\nIn this section, we first describe how to transform attacks from the federated learning to the decentralized environment. Then we introduce the dissensus attack for decentralized environment.\n\nB.1 EXISTING ATTACKS IN FEDERATED LEARNING\n\nA little is enough (ALIE). The attackers estimate the mean μNi and standard deviation σNi of the regular models, and send μNi − zσNi to regular worker i where z is a small constant controlling the strength of the attack (Baruch et al., 2019). The hyperparameter z for ALIE is computed according to (Baruch et al., 2019)\n\n(cid:18)\n\nz = max\n\nz\n\nφ(z) <\n\n(cid:19)\n\nn − b − s n − b\n\n(7)\n\nwhere s = (cid:98) n\n\n2 + 1(cid:99) − b and φ is the cumulative standard normal function.\n\nInner product manipulation attack (IPM). The inner product manipulation attack is proposed in (Xie et al., 2019) which lets all attackers send same corrupted gradient u based on the good gradients\n\nuj = −εAVG({vi : i ∈ VR})\n\n∀ j ∈ VB.\n\nIf ε is small enough, then uj can be detected as good by the defense, circumventing the defense. There are 3 main differences where IPM need to adapt to the decentralized environment:\n\n1. Byzantine workers may not connected to the same good worker.\n\n2. The model vectors are transmitted instead of gradients.\n\n3. The AVG should be replaced by its equivalent gossip form.\n\nThis motivates our dissensus attack in the next section.\n\nB.2 DISSENSUS ATTACK AND OTHER ATTACKS IN THE DECENTRALIZED ENVIRONMENT\n\nIn this section, we introduce a novel dissensus attack inspired by our impossibility construction in Theorem II and the IPM attack described above. The dissensus attack aims to prevent regular worker models from reaching consensus. Roughly speaking, dissensus attackers around worker i send its model weights that are symmetric to the weighted average of regular neighbors around i. Then after gossip averaging step, the consensus distance drops slower or even grows which motivates the name “dissensus”.\n\nWe can parameterize the attack through hyperparameter εi and summarize the attack in Definition A\n\nxj := xi − εi\n\nk∈Ni∩VR\n\n(cid:80)\n\n(cid:80)\n\nWik(xk−xi) Wij\n\nj∈Ni∩VB\n\n.\n\n(8)\n\nThe εi determines the behavior of the attack. By taking smaller εi, Byzantine model weights are closer to the target updates i and difficult to be detected. On the other hand, a larger εi pulls the model away from the consensus.\n\nFigure 7: Example of the DISSENSUS attack. The gray (resp. red) denotes regular (resp. Byzantine) nodes. The blue dots represents the parameters of regular nodes after gossip steps.\n\nNote that this attack requires omniscience since it exploits model information from across the network. If the attackers in addition can choose which node to attack, then they can choose either to spread about the attack across the network or focus on the targeting graph cut, that is min-cut of the graph.\n\n16\n\nx1x2x1x3x2Under review as a conference paper at ICLR 2023\n\nEffect of the dissensus attack. The dissensus attack enjoy the following properties.\n\nProposition IV. (i) For all i ∈ VR, under the dissensus attack with εi = 1, the gossip averaging step (GOSSIP) is equivalent to no communication on i, xt+1 i. Secondly, (ii) If the graph is fully connected, gossip averaging recovers the correct consensus even in the presence of dissensus attack.\n\ni = xt\n\nThe above proposition illustrates two interesting aspects of the attack. Firstly, dissensus works by negating the progress that would be made by gossip. The attack in (Peng & Ling, 2020) also satisfies this property (see Appendix for additional discussion). Secondly, it is a uniquely decentralized attack and has no effect in the centralized setting. Hence, its effect can be used to measure the additional difficulty posed due to the restricted communication topology.\n\nProof. For the first part, by definition (GOSSIP) we know that i = (cid:80)n\n\nj=1 Wijxt\n\ni + (cid:80)\n\nj = xt\n\nxt+1\n\nj∈Ni\n\nWij(xt\n\nj − xt i)\n\nBy setting εi = 1 in the attack (6), the second term 0 and therefore xt+1 i. For part (ii), note that in a fully connected graph the gossip average is the same as standard average. Averaging all the perturbations introduced by the dissensus attack gives\n\ni = xt\n\n−ε (cid:80)\n\ni,j∈VR\n\nWi,j(xt\n\nj − xt\n\ni) = 0 .\n\nAll terms cancel and sum to 0 by symmetry. Thus, in a fully connected graph the dissensus perturbations cancel out and the gossip average returns the correct consensus.\n\nRelation with zero-sum attack and dissensus. Peng & Ling (2020) propose the “zero-sum” attack which achieves similar effects as Proposition IV part (i). This attack is defined for j ∈ VB\n\nxj := −\n\n(cid:80)\n\nk∈Ni∩VR |Ni∩VB|\n\nxk\n\n.\n\nThe key difference between zero-sum attack and our proposed attack is three-fold. First, zero-sum attack ensures (cid:80) i and therefore easy to detect. This attack pull the aggregated model to 0. On the other hand, our attack ensures\n\nxj = 0 which means the Byzantine models have to be far away from xt\n\nj∈Ni\n\n1\n\n(cid:80)\n\nj∈Ni\n\nWij\n\n(cid:80)\n\nj∈Ni\n\nWijxt\n\nj = xt\n\ni\n\nand the Byzantine updates can be very close to xt i and it is more difficult to be detected. Second, our proposed attack considers the gossip averaging which is prevalent in decentralized training (Koloskova et al., 2020b) while the zero-sum attack only targets simple average. Third, our attack has an additional parameter ε controlling the strength of the attack with ε > 1 further compromise the model quality while zero-sum attack is fixed to training alone.\n\nC TOPOLOGIES AND MIXING MATRICES\n\nC.1 CONSTRAINED TOPOLOGIES\n\nTopologies that do not satisfy the robust network assumption in (LeBlanc et al., 2013; Sundaram & Gharesifard, 2018; Su & Vaidya, 2016a). The robust network assumption requires there to be at least b + 1 paths between any two regular workers when there are b Byzantine workers in the network (LeBlanc et al., 2013; Sundaram & Gharesifard, 2018; Su & Vaidya, 2016a). The topology in Figure 8 only has 1 path between regular workers in two cliques while having 2 Byzantine workers in the network. Therefore this topology does not satisfy the robust network assumption. But the graph cut is not adjacent to the Byzantine workers and, intuitively, it would be possible for an ideal robust aggregator to help reach consensus. The experimental results are given in Appendix D.4.\n\n(Randomized) Small-world topology. The small-world topology is a random graph generated with Watts-Strogatz model (Watts & Strogatz, 1998). The topology is created using NetworkX package (Hagberg et al., 2008) with 10 regular workers each connected to 2 nearest neighbors and probability of rewiring each edge as 0.15. Two additional Byzantine workers are linked to 2 random regular workers. There are 12 workers in total.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Example topology that does not satisfy the robust network assumptions in (Sundaram & Gharesifard, 2018; Su & Vaidya, 2016a).\n\nTorus topology. The regular workers form a torus grid T3,3 and two additional Byzantine workers are linked to 2 random regular workers. There are 11 workers in total.\n\nThe mixing matrix for these topologies are constructed with Metropolis-Hastings algorithm introduced in the previous section. The spectral gap for small-world topology and torus topology are 0.084 and 0.131 respectively. In contrast, the dumbbell topology in Figure 16 is more challenging with a spectral gap of 0.043. The data distribution is non-IID.\n\nC.2 CONSTRUCTING MIXING MATRICES\n\nIn this section, we introduce a few possible ways to construct the mixing weight vectors in the presence of Byzantine workers. The constructed weight vectors satisfy (A2) in Section 3.\n\n• Metropolis-Hastings weight (Hastings, 1970). The Metropolis-Hastings algorithm locally constructs the mixing weights by exchanging degree information (di and dj) between two nodes i and j. The mixing weight vector on regular worker i ∈ VR is computed as follows\n\n1 max{di,dj }+1\n\nWij =\n\n \n\n\n\n1 − (cid:80) 0\n\nWil\n\nl∈Ni\n\nj ∈ Ni, j = i, Otherwise.\n\nIf worker j ∈ VB is Byzantine, then the only way for j to maximize its weight Wij to regular worker i is to report a smaller degree dj. However, such Byzantine behavior of node j has limited influence on worker i’s weight Wij because it can not be greater than\n\n1\n\ndi+1 .\n\n• Equal-weight. Let dmax be the maximum degree of nodes in a graph. Such upper bound dmax can be a public information, for example, a bluetooth device can at most connect to dmax other devices due to physical constraints. The Byzantine worker cannot change the value of dmax. Then we use the following naive construction\n\nWij =\n\n \n\n\n\n1 dmax+1 1 − |Ni| 0\n\ndmax+1\n\nj ∈ Ni, j = i, Otherwise.\n\n(9)\n\nNote that these construction schemes are not proved to be the optimal. In this work, we focus on the Byzantine attacks given a topology and associated mixing weights. We leave it as future work to explore the best strategy to construct mixing weights.\n\nD EXPERIMENTS\n\nWe summarize the hardware and software for experiments in Table 2. We list the setups and results of experiments for consensus in Appendix D.1 and optimization in Appendix D.2.\n\nD.1 BYZANTINE-ROBUST CONSENSUS\n\n18\n\nClique AClique BCutA1B1Under review as a conference paper at ICLR 2023\n\nTable 2: Runtime hardwares and softwares.\n\nCPU\n\nModel name # CPU(s) NUMA node(s)\n\nGPU\n\nProduct Name CUDA Version\n\nPyTorch\n\nVersion\n\nIntel (R) Xeon (R) Gold 6132 CPU @ 2.60 GHz 56 2\n\nTesla V100-SXM2-32GB 11.0\n\n1.7.1\n\nTable 3: Default experimental settings for MNIST\n\nDataset Architecture Training objective Evaluation objective\n\nMNIST CONV-CONV-DROPOUT-FC-DROPOUT-FC Negative log likelihood loss Top-1 accuracy\n\nBatch size per worker Momentum Learning rate LR decay LR warmup Weight decay\n\n32 0.9 0.01 No No No\n\nRepetitions Reported metric\n\n1 Mean test accuracy over the last 150 iterations\n\nIn this section, we provide detailed setups for Figure 3. The Figure 9 demonstrates the topology for the experiment. The 4 regular workers are connected with two of them holding value 0 and the others holding 200. Then the average consensus is 100 with initial mean square error equals 10000. Two Byzantine workers are connected to two regular workers in the middle. We can tune the weights of each edge to change the mixing matrix and γ. Then we can decide the weight δ on the Byzantine edge. The γ and δ used in the experiments are\n\n• p\n\n:=\n\n∈ [0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.005, 0.0014, 3.7e − 4, 1e − 4, 1e − 5]\n\n(1\n\n−\n\n−\n\n1\n\nγ)2\n\nFigure 9: The topology for the attacks on consensus. The grey and red nodes denote regular and Byzantine workers respectively.\n\n• δ ∈ [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nwhere non-compatible combination of γ and δ are ignored in the Figure 3. The dissensus attack is applied with ε = 0.05. The hyperparameter β of trimmed mean (TM) is set to the actual number of Byzantine workers around the regular worker. The clipping radius of CLIPPEDGOSSIP is chosen according to (27).\n\nIn Figure 10, we show the iteration-to-error curves for all possible combinations of γ and δ. In addition, we provide a version of TM and MEDIAN which takes the mixing weight into account. As we can see, the naive TM, MEDIAN, and MEDIAN* cannot bring workers closer because of the data distribution we constructed. The TM* is performing better than the other baselines but worse than CLIPPEDGOSSIP especially on the challenging cases where γ is small and δ is large. For CLIPPEDGOSSIP, it matches with our intuition that for a fixed γ the convergences is worse with increasing δ while for a fixed δ the convergence is worse with decreasing γ.\n\nD.2 BYZANTINE-ROBUST DECENTRALIZED OPTIMIZATION\n\nIn this section, we provide detailed hyperparameters and setups for experiments in the main text and then provide additional experiments. For all MNIST tasks, we use the default setup listed in Table 3 unless specifically stated. The default hyperparameters of the robust aggregators: 1) For GM, we\n\n19\n\n00200200Under review as a conference paper at ICLR 2023\n\nFigure 10: The iteration-to-error curves for defenses under dissensus attack. The TM* and MEDIAN* refer to the version of TM and MEDIAN which considers mixing weight.\n\nchoose number of iterations T = 8; 2) The TM drops top and bottom β = δmaxn percent of values in each coordinate; 3) The clipping radius of CLIPPEDGOSSIP is τ = 1; 4) The model averaging hyperparameter of MOZI is α = 1.\n\nD.2.1 SETUP FOR “DECENTRALIZED DEFENSES WITHOUT ATTACKERS”\n\nThe Fig. 4 uses the dumbbell topology in Fig. 1 with 10 regular workers in each clique. There is no Byzantine workers. The experiments run for 900 iterations. MOZI uses α = 0.5 and ρi = 0.99 in\n\n20\n\n0500010000Errorp = 0.060 | max = 0.050p = 0.060 | max = 0.100p = 0.060 | max = 0.200p = 0.060 | max = 0.300p = 0.060 | max = 0.400p = 0.060 | max = 0.5000500010000Errorp = 0.050 | max = 0.050p = 0.050 | max = 0.100p = 0.050 | max = 0.200p = 0.050 | max = 0.300p = 0.050 | max = 0.400p = 0.050 | max = 0.5000500010000Errorp = 0.040 | max = 0.050p = 0.040 | max = 0.100p = 0.040 | max = 0.200p = 0.040 | max = 0.300p = 0.040 | max = 0.400p = 0.040 | max = 0.5000500010000Errorp = 0.030 | max = 0.050p = 0.030 | max = 0.100p = 0.030 | max = 0.200p = 0.030 | max = 0.300p = 0.030 | max = 0.400p = 0.030 | max = 0.5000500010000Errorp = 0.020 | max = 0.050p = 0.020 | max = 0.100p = 0.020 | max = 0.200p = 0.020 | max = 0.300p = 0.020 | max = 0.400p = 0.020 | max = 0.5000500010000Errorp = 0.010 | max = 0.050p = 0.010 | max = 0.100p = 0.010 | max = 0.200p = 0.010 | max = 0.300p = 0.010 | max = 0.400p = 0.010 | max = 0.5000500010000Errorp = 0.005 | max = 0.050p = 0.005 | max = 0.100p = 0.005 | max = 0.200p = 0.005 | max = 0.300p = 0.005 | max = 0.400p = 0.005 | max = 0.5000500010000Errorp = 0.001 | max = 0.050p = 0.001 | max = 0.100p = 0.001 | max = 0.200p = 0.001 | max = 0.300p = 0.001 | max = 0.400p = 0.001 | max = 0.500050100T0500010000Errorp = 0.000 | max = 0.050050100Tp = 0.000 | max = 0.100050100Tp = 0.000 | max = 0.200050100Tp = 0.000 | max = 0.300050100Tp = 0.000 | max = 0.400050100Tp = 0.000 | max = 0.500AggClippedGossipMedianTMMedian*TM*Under review as a conference paper at ICLR 2023\n\nFigure 11: Dumbbell variant where Byzantine workers maybe added to the central worker.\n\nFigure 12: Accuracy of aggregators with or without the honest majority everywhere (H.M.E.) assumption. Regular workers are connected through a ring and have IID data.\n\nthis setting. For bucketing experiment, we choose bucket size of s = 2. It means we randomly put at most two updates into one bucket and average within each bucket and then apply robust aggregators to the averaged updates.\n\nD.2.2 SETUP FOR “EFFECTS OF THE NUMBER OF BYZANTINE WORKERS”\n\nThe Fig. 6 uses a dumbbell topology variant in Fig. 11 . The experiments run for 1500 iterations. In this experiment we choose n − b = 11 and b = 0, 1, 2, 3. We choose the edge weight of Byzantine workers such that the (cid:102)W and p remain the same for all these b. Then we can easily investigate the relation between δmax ∈ [0, b+3 ] and p by varying b. The hyperparameter of dissensus attack is set to εi = 1.5 for all workers and all experiments.\n\nb\n\nD.2.3 SETUP FOR “DEFENSE WITHOUT HONEST MAJORITY”\n\nThe Fig. 12 uses the ring topology of 5 regular workers in Fig. 13. 11 Byzantine workers are added to the ring so that 1 regular worker do no have honest majority. The experiments run for 900 iterations. We use εi = 1.5 for dissensus attacks. We use clipping radius τ = 0.1 for CLIPPEDGOSSIP.\n\nIn the decentralized environment, the common honest majority assumption in the federated learning setup can be strengthen to honest majority everywhere, meaning all regular workers have an honest majority of neighbors (Su & Vaidya, 2016b; Yang & Bajwa, 2019b;a). Considering a ring of 5 regular workers with IID data, and adding 2 Byzantine workers to each node will still satisfy the honest majority assumption everywhere. Now adding one more Byzantine worker to a node will break the assumption.\n\nFigure 12 shows that while TM and GM can sometimes counter the attack under the honest majority assumption, adding one more Byzantine worker always corrupts the entire training. The CLIPPEDGOSSIP defend attacks successfully even beyond the assumption, because they leverage the fact that local updates are trustworthy. This suggest that existing statistics-based aggregators which take no advantage of local information are vulnerable under this realistic decentralized threat model.\n\nFigure 13: Ring topology without honest majority.\n\nD.2.4 SETUP FOR “MORE TOPOLOGIES AND ATTACKS.”\n\nIn Figure 5, we use the small-world and torus topologies described in Appendix C.1. More specifically, we created a randomized small-world topology using NetworkX package (Hagberg et al., 2008) with 10 regular workers each connected to 2 nearest neighbors and probability of rewiring each edge as 0.15. Two additional Byzantine workers are linked to 2 random regular workers. There are 12\n\n21\n\nClique AClique B0500Iterations020406080100Accuracy (%)ATK = ALIEAggClippedGossipGMTM0500IterationsATK = DissensusH.M.E.FalseTrueUnder review as a conference paper at ICLR 2023\n\nTable 4: Default experimental settings for CIFAR-10\n\nDataset Architecture Training objective Evaluation objective\n\nCIFAR-10 VGG-11Simonyan & Zisserman (2014) Cross entropy loss Top-1 accuracy\n\nBatch size per worker Momentum Learning rate LR decay LR warmup Weight decay\n\n64 0.9 0.1 0.1 at epoch 80 and 120 No No\n\nRepetitions Reported metric\n\n14 Mean test accuracy over the last 150 iterations\n\nworkers in total. For the torus topology, we let regular workers form a torus grid T3,3 where all 9 regular workers are connected to 3 other workers. Two additional Byzantine workers are linked to 2 random regular workers. There are 11 workers in total.\n\nThe mixing matrix for these topologies are constructed with Metropolis-Hastings algorithm in Appendix C.2. The spectral gap for small-world topology and torus topology are 0.084 and 0.131 respectively. In contrast, the dumbbell topology in Figure 16 is more challenging with a spectral gap of 0.043. The data distribution is non-IID.\n\nD.3 EXPERIMENT: CIFAR-10 TASK\n\nIn this section, we conduct experiments on CIFAR-10 dataset Krizhevsky et al. (2009). The running environment of this experiment is the same as MNIST experiment Table 2. The default setup for CIFAR-10 experiment is summarized in Table 4.\n\nWe compare performances of 5 aggregators on dumbbell topology with 10 nodes in each clique (no attackers). The results of experiments are shown in Figure 14. In order to investigate if consensus has reached among the workers, we average the worker nodes in 3 different categories ( “Global”, Clique A, and Clique B) and compare their performances on IID and NonIID datasets. The “IID-Global” result show that GM and TM is much worse than CLIPPEDGOSSIP and Gossip, in contrast to the MNIST experiment Figure 4 where they have matching result. This is because the workers with in each clique are converging to different stationary point — “IID-Clique A” and “IID-Clique B” show GM and TM in each clique can reach over 80% accuracy which is close to Gossip. It demonstrates that GM and TM fail to reach consensus even in this Byzantine-free case and therefore vulnerable to attacks.\n\nThe NonIID experiment also support that CLIPPEDGOSSIP perform much better than all other robust aggregators. Notice that CLIPPEDGOSSIP’s “NonIID-Global” performance is better than “NonIID-Clique A” and “NonIID-Clique B” while GM and TM’s result are opposite. This is because CLIPPEDGOSSIP allows effective communication in this topology and therefore clique models are close to each other in the same local minima basin such that their average (global model) is better than both of them. The GM’s and TM’s clique models converge to different local minima, making their averaged model underperform.\n\nD.4 EXPERIMENT FOR “WEAKER TOPOLOGY ASSUMPTION”\n\nAs is mentioned in Remark 1 and Appendix C.1, the topology assumption in this work is weaker than the robust network assumption in Su & Vaidya (2016a); Sundaram & Gharesifard (2018). We use the topology in Figure 8 which consists of 10 regular workers and 2 dissensus attack workers. While this topology does not satisfy the robust network assumption, it intuitively should allow communication between two cliques as no Byzantine workers are attached to the cut. However, both GM and TM will discard the graph cut due to data heterogeneity. This shows that GM and TM impede information diffusion. On the other hand, CLIPPEDGOSSIP is the only robust aggregator which help two cliques\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Train models on dumbbell topology with IID and NonIID datasets. The three figures in each row correspond to the same experiment with “Global”, “Clique A”, “Clique B” denoting the performances of globally averaged model, within-Clique A averaged model, and within-Clique B averaged model.\n\nFigure 15: Compare robust aggregators under dissensus attacks over dumbbell topology Figure 5.\n\nreaching consensus in the NonIID case. The CLIPPEDGOSSIP theoretically applies to more topologies and empirically perform better.\n\nD.5 EXPERIMENT: CHOOSING CLIPPING RADIUS\n\nIn Figure 16 we show the sensitive of tuning clipping radius. We use dumbbell topology with 5 regular workers in each clique and add 1 more Byzantine worker to each clique. The clipping radius is searched over a grid of [0.1, 0.5, 1, 2, 10]. The Byzantine workers are chosen to be Bit-Flipping, Label-Flipping, and ALIE.\n\nWe also give an adaptive clipping strategy for different i ∈ VR and time t. After communication step (cid:13) 2\nat time t, the value of xt+1/2 (cid:13) (cid:13) 2\nfor all j ∈ Ni. We denote the set of indices set S t i as the indices of workers that have the smallest distances to worker i\n\nis available. Therefore we can sort the values of\n\n(cid:13)xt+1/2\n\n− xt+1/2\n\n(cid:13) (cid:13)\n\nj\n\ni\n\ni\n\nS t\n\ni =\n\nS:(cid:80)\n\narg min\n\n(cid:88)\n\nj∈S Wij ≤1−δmax\n\nj∈S\n\n23\n\n(cid:13) (cid:13)\n\n(cid:13)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n.\n\n20406080Accuracy (%)IID-GlobalClippedGos.GMMOZITMGossipIID-Clique AIID-Clique B0.000.250.500.751.001.25Iterations1e620406080Accuracy (%)NonIID-Global0.000.250.500.751.001.25Iterations1e6NonIID-Clique A0.000.250.500.751.001.25Iterations1e6NonIID-Clique B0250500750Iterations5060708090100Accuracy (%)IIDClippedGossipGMMOZITMIdeal Comm.0250500750IterationsNonIID0250500750IterationsNonIID + B.0250500750IterationsNonIID + R.0250500750IterationsNonIID + B. + R.Under review as a conference paper at ICLR 2023\n\nFigure 16: Tuning clipping radius on the dumbbell topology against Byzantine attacks. The y-axis is the averaged test accuracy over all of the regular workers.\n\nThen the adaptive strategy picks clipping radius as follows\n\n(cid:114)\n\nτ t+1 i =\n\n(cid:80)\n\nj∈S t i\n\nWij\n\n(cid:13) (cid:13)\n\n(cid:13)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n.\n\n(10)\n\nNote that this adaptive choice of clipping radius is generally a bit smaller than the theoretical value (27). It guarantees that the Byzantine workers have limited influences at cost of small slow down on the convergence.\n\nAs we can see from Figure 16, the performances of CLIPPEDGOSSIP are similar with different constant choices of τ which shows that the choice of τ is not very sensitive. The adaptive algorithms perform well in all cases. Therefore, the adaptive choice of τ will be recommended in general.\n\nE ANALYSIS\n\nWe restate the core equations in Algorithm 1 at time t on worker i as follows\n\ni = (1 − α)mt\n\ni + αgi(xt i)\n\nmt+1 xt+1/2\n\ni − ηmt+1\n\ni\n\ni\n\n= xt j→i = xt+1/2 zt+1\n\n+ CLIP(xt+1/2\n\nj\n\n− xt+1/2\n\ni\n\n, τ t i )\n\ni n\n(cid:88)\n\nxt+1\n\ni =\n\nWijzt+1\n\nj→i\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\nIn addition, we define the following virtual iterates on the set of good nodes VR\n\nj=1\n\n• xt = 1\n\n|VR| • mt = 1\n\n|VR|\n\n(cid:80)\n\ni∈VR\n\nxt\n\ni the average (over time) of good iterates.\n\n(cid:80)\n\ni∈VR\n\nmt\n\ni the average (over time) of momentum iterates.\n\nIn this proof, we define p := 1 − (1 − γ)2 ∈ (0, 1] for convenience.\n\nIn this section, we show that the convergence behavior of the virtual iterates xt. The structure of this section is as follows:\n\n• In Appendix E.1, we give common quantities, simplified notations and list common equali-\n\nties/inequalities used in the proof.\n\n• In Appendix E.2, we provide all auxiliary lemmas necessary for the proof. Among these\n\nlemmas, Lemma 8 is the key sufficient descent lemma.\n\n• In Appendix E.3, we provide the proof of the main theorem.\n\nE.1 DEFINITIONS, AND INEQUALITIES\n\nNotations for the proof. We use the following variables to simplify the notation\n\n24\n\n0200400600Iterations5060708090100Accuracy (%)Bit-Flipping0200400600IterationsLabel-Flipping0200400600IterationsALIE=0.1=0.5=1=2=10AdaptiveUnder review as a conference paper at ICLR 2023\n\n• Optimization sub-optimality:\n\n• Consensus distance:\n\nrt := f ( ̄xt) − f (cid:63)\n\nΞt :=\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\n(cid:107)xt\n\ni − ̄xt(cid:107)2\n\n2\n\n• The distance between the ideal gradient and actual averaged momentum\n\net+1\n\n1\n\n:= E(cid:107)∇f ( ̄xt) − ̄mt+1(cid:107)2\n\n2\n\n• Similarly, the distance between the ideal gradient and individual momentums\n\n ̃et+1\n\n1\n\n:=\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)∇f ( ̄xt) − mt+1\n\ni\n\n(cid:107)2\n\n2\n\n• Similar, distance between individual ideal gradients and individual momentums which is\n\nweighted by the mixing matrix\n\n ̄et+1\n\n1\n\n:=\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wij(∇fj( ̄xt) − mt+1\n\nj\n\n)(cid:107)2\n\n2\n\n• Similar we have distance between individual ideal gradients and individual momentums\n\net+1\n\nI\n\n:=\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)mt+1\n\ni − ∇fi( ̄xt)(cid:107)2 2,\n\n• Let et+1\n\n2\n\nbe the averaged squared error introduced by clipping and Byzantine workers\n\net+1\n\n2\n\n:=\n\n1 |VR|\n\n(cid:88)\n\nE\n\ni∈VR\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nj∈VR\n\n(cid:88)\n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\n) +\n\n(cid:88)\n\nj∈VB\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13) (cid:13) 2\n\n.\n\nLemma 6 (Common equalities and inequalities). We use the following equalities and inequalities\n\n• The cosine theorem: ∀ x, y ∈ Rd\n\n(cid:104)x, y(cid:105) = −\n\n1 2\n\n(cid:107)x − y(cid:107)2\n\n2 +\n\n1 2\n\n(cid:107)x(cid:107)2\n\n2 +\n\n1 2\n\n(cid:107)y(cid:107)2\n\n2\n\n• Young’s inequality: For ε > 0 and x, y ∈ Rd\n\n(cid:107)x + y(cid:107)2\n\n2 ≤ (1 + ε)(cid:107)x(cid:107)2\n\n2 + (1 + ε−1)(cid:107)y(cid:107)2\n\n2\n\n• If f is convex, then for α ∈ [0, 1] and x, y ∈ Rd\n\nf (αx + (1 − α)y) ≤ αf (x) + (1 − α)f (y)\n\n• Cauchy-Schwarz inequality\n\n(cid:104)x, y(cid:105) ≤ (cid:107)x(cid:107)2(cid:107)y(cid:107)2\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\n• Let {xi : i ∈ [m]} be independent random variables and E xi = 0 and E(cid:107)xi(cid:107)2 = σ2 then\n\nE(cid:107) 1 m\n\n(cid:80)m\n\ni=1 xi(cid:107)2\n\n2 = σ2\n\nm\n\nE.2 LEMMAS\n\nThe following lemma establish the update rule for ̄xt. Lemma 7. Assume Lemma 3. Let ∆t+1 be the error incurred by clipping and VB\n\n∆t+1 :=\n\n1 |VR|\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\nThen the virtual iterate updates\n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\n) +\n\n(cid:88)\n\nj∈VB\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n ̄xt+1 = ̄xt − η ̄mt+1 + ∆t+1.\n\n25\n\n(19)\n\n(20)\n\n(21)\n\n\n\n)\n\n .\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Expand ̄xt+1 with the definition of xt+1\n\ni\n\nin (14) yields \n\n ̄xt+1 =\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nxt+1\n\ni =\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\n\n\ni∈VR\n\nj∈VR\n\nWijzt+1\n\nj→i +\n\n(cid:88)\n\nj∈VB\n\n\n\nWijzt+1\n\nj→i\n\n\n\n=\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\n\n\n\n\n(cid:88)\n\nj∈VR \n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\n\n\nWijxt+1/2\n\nj\n\n\n\n) +\n\n(cid:88)\n\nj∈VR\n\n+\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\n\n\ni∈VR\n\nj∈VB\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n\n\nWijxt+1/2\n\ni\n\n .\n\n) +\n\n(cid:88)\n\nj∈VB\n\nReorganize the terms to form ∆t+1 \n\n ̄xt+1 =\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\n\n\ni∈VR\n\nj∈VR\n\nWijxt+1/2\n\nj\n\n+\n\n(cid:88)\n\nj∈VB\n\nWijxt+1/2\n\ni\n\n  + ∆t+1\n\n=\n\n=\n\n1 |VR|\n\n1 |VR|\n\n(cid:88)\n\nj∈VR\n\n(cid:88)\n\ni∈VR\n\n(1 − δj)xt+1/2\n\nj\n\n+\n\nxt+1/2\n\ni\n\n+ ∆t+1 =\n\n1 |VR|\n\n1 |VR|\n\n= ̄xt\n\ni − η ̄mt+1 + ∆t+1.\n\n(cid:88)\n\ni∈VR\n\n(cid:88)\n\nδixt+1/2\n\ni\n\n+ ∆t+1\n\n(xt\n\ni − ηmt+1\n\ni\n\n) + ∆t+1\n\ni∈VR\n\nNote that the ∆t+1 can be written as the follows\n\n∆t+1 =\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\n xt+1\n\ni −\n\n ̃Wijxt+1/2\n\nj\n\n(cid:88)\n\nj∈VR\n\n  = ̄xt+1 −\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nxt+1/2\n\ni\n\n.\n\nwhere measures the error introduced to ̄xt+1 considering the impact of Byzantine workers and clipping. Therefore when VB = ∅ and τ is sufficiently large, ∆t+1 = 0 and ̄xt+1 converge at the same rate as the centralized SGD with momentum.\n\nRecall that et+1 := E(cid:107)∇f ( ̄xt) − ̄mt+1(cid:107)2 Lemma 8 (Sufficient decrease). Assume (A4) and η ≤ 1\n\n1\n\n2. The key descent lemma is stated as follow\n\nE f ( ̄xt+1) ≤f ( ̄xt) −\n\nη 2\n\n(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 −\n\nη 4\n\nE(cid:107) ̄mt+1 −\n\n∆t+1(cid:107)2\n\n2 + ηet+1\n\n1 +\n\n1 η\n\net+1\n\n2\n\n.\n\n2L , then 1\nη\n\nProof. Use smoothness (A4) and expand it with (21)\n\nf ( ̄xt+1) ≤f ( ̄xt) − (cid:104)∇f ( ̄xt), η ̄mt+1 − ∆t+1(cid:105) +\n\nL 2\n\n(cid:107)η ̄mt+1 − ∆t+1(cid:107)2\n\n2\n\nApply cosine theorem (15) to the inner product η(cid:104)∇f ( ̄xt), ̄mt+1 − 1\n\nη ∆t+1(cid:105) yields\n\nE f ( ̄xt+1) ≤f ( ̄xt) −\n\nη 2\n\n(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 −\n\n(cid:19)\n\n(cid:18) η − Lη2 2\n\nE(cid:107) ̄mt+1 −\n\n1 η\n\n∆t+1(cid:107)2\n\n2\n\n+\n\nη 2\n\nE(cid:107)∇f ( ̄xt) − ̄mt+1 +\n\n1 η\n\n∆t+1(cid:107)2 2.\n\nIf step size η ≤ 1\n\n2L , then − η−Lη2\n\n4 . Applying inequality (16) to the last term\n\nη 2\n\nE(cid:107)∇f ( ̄xt) − ̄mt+1 +\n\n∆t+1(cid:107)2\n\n2 ≤ η E(cid:107)∇f ( ̄xt) − ̄mt+1(cid:107)2\n\n2 +\n\n1 η\n\nE(cid:107)∆t+1(cid:107)2 2.\n\nSince et+1\n\n1\n\n:= E(cid:107)∇f ( ̄xt) − ̄mt+1(cid:107)2\n\n2 and E(cid:107)∆t+1(cid:107)2\n\n2 ≤ et+1\n\n2\n\n, then we have\n\nE f ( ̄xt+1) ≤f ( ̄xt) −\n\nη 2\n\n(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 −\n\nη 4\n\nE(cid:107) ̄mt+1 −\n\n1 η\n\n26\n\n∆t+1(cid:107)2\n\n2 + ηet+1\n\n1 +\n\n1 η\n\net+1\n\n2\n\n.\n\n2 ≤ − η 1\nη\n\nUnder review as a conference paper at ICLR 2023\n\nIn the next lemma, we establish the recursion for the distance between momentums and gradients Lemma 9. Assume (A3) and (A4) and Lemma 3, For any doubly stochastic mixing matrix A ∈ Rn×n\n\net+1 A =\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj∈VR\n\nAij(mt+1\n\nj − ∇fj( ̄xt))(cid:107)2 2,\n\nthen we have the following recursion\n\net+1 A ≤ (1 − α)et\n\nA +\n\nα2σ2 |VR|\n\n(cid:107)A(cid:107)2\n\nF,VR\n\n+ 2αL2Ξt +\n\n2L2η2 α\n\n(cid:107) ̄mt −\n\n1 η\n\n∆t(cid:107)2 2.\n\n(22)\n\nwhere we define (cid:107)A(cid:107)2\n\nF,VR\n\n:= (cid:80)\n\ni∈VR\n\n(cid:80)\n\nj∈VR\n\nA2\n\nij Therefore,\n\n• If Aij = 1\n\n|VR| for all i, j ∈ VR, then et+1\n\nA = et+1\n\n1\n\nand (cid:107)A(cid:107)2\n\nF,VR\n\n= 1.\n\n• If A = (cid:102)W , then et+1\n\nA = ̄et+1\n\n1\n\nand (cid:107)A(cid:107)2\n\nF,VR\n\n= (cid:80)\n\ni∈VR\n\n(cid:80)\n\nj∈VR (cid:102)W 2\n\nij ≤ |VR|.\n\n• If A = I, then (cid:107)A(cid:107)2\n\nF,VR\n\n= |VR|. In addition,\n\n1 ≤ 2et+1 ̃et+1\n\nI + 2ζ 2\n\nwhere A = I.\n\nProof. We can expand et+1\n\nA by expanding mt+1\n\nj\n\net+1\n\nA\n\n(11)=\n\n=\n\n1 |VR|\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR\n\n(cid:88)\n\nj∈VR\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj∈VR\n\nAij((1 − α)mt\n\nj + αgj(xt\n\nj) − ∇fj( ̄xt))(cid:107)2\n\n2\n\nAij((1−α)mt\n\nj +α(gj(xt\n\nj) ± ∇fj(xt\n\nj))−∇fj( ̄xt))(cid:107)2\n\n2\n\nExtract the stochastic term gj(xt\n\nj) − ∇fj(xt\n\nj) inside the norm and use that E gj(xt\n\nj) = ∇fj(xt\n\nj),\n\net+1 A =\n\n1 |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\nAij((1−α)mt\n\nj +α∇fj(xt\n\nj)−∇fj( ̄xt))(cid:107)2\n\n2\n\nj∈VR\n\ni∈VR 1\n|VR|\n\n+\n\n(cid:88)\n\ni∈VR\n\n(cid:88)\n\nE(cid:107)\n\nj∈VR\n\nAijα(gj(xt\n\nj) − ∇fj(xt\n\nj))(cid:107)2\n\n2\n\n≤\n\n1 |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\nAij((1−α)mt\n\nj +α∇fj(xt\n\nj)−∇fj( ̄xt))(cid:107)2\n\n2\n\n+\n\nα2 |VR|\n\n(cid:88)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\nA2\n\nij E(cid:107)gj(xt\n\nj) − ∇fj(xt\n\nj)(cid:107)2 2.\n\nThen we can use (A3) for the last term to get\n\net+1 A =\n\n1 |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\nAij((1−α)mt\n\nj +α∇fj(xt\n\nj)−∇fj( ̄xt))(cid:107)2\n\n2 +\n\nα2σ2 |VR|\n\n(cid:107)A(cid:107)2\n\nF,VR\n\n.\n\nThen we insert ±(1 − α)∇fj( ̄xt−1) inside the first norm and expand using (17)\n\net+1 A ≤\n\n1 − α |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\nAij(mt\n\nj − ∇fj( ̄xt−1))(cid:107)2\n\n2 +\n\nα2σ2 |VR|\n\n(cid:107)A(cid:107)2\n\nF,VR\n\n+\n\nα |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\nAij(∇fj(xt\n\nj) − ∇fj( ̄xt) +\n\n1 − α α\n\n(∇fj( ̄xt−1) − ∇fj( ̄xt))(cid:107)2 2.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nNote that the first term is et\n\nA and by the convexity of (cid:107)·(cid:107) for the last term we have\n\net+1 A ≤(1 − α)et\n\nA +\n\nα2σ2 |VR|\n\n(cid:107)A(cid:107)2\n\nF,VR\n\n+\n\nα |VR|\n\n(cid:88)\n\nj∈VR\n\n(cid:107)∇fj(xt\n\nj) − ∇fj( ̄xt) +\n\n1 − α α\n\n(∇fj( ̄xt−1) − ∇fj( ̄xt))(cid:107)2 2.\n\nThen we can further expand the last term\n\net+1 A ≤(1 − α)et\n\nA +\n\nα2σ2 |VR|\n\n(cid:107)A(cid:107)2\n\nF,VR\n\n+\n\n2α |VR|\n\n(cid:88)\n\nj∈VR\n\n(cid:107)∇fj(xt\n\nj) − ∇fj( ̄xt)(cid:107)2\n\n2 +\n\n2(1 − α)2 α|VR|\n\n(cid:88)\n\nj∈VR\n\n(cid:107)∇fj( ̄xt−1) − ∇fj( ̄xt)(cid:107)2 2.\n\nThen we can apply smoothness (A4) and use (1 − α)2 ≤ 1\n\net+1 A ≤(1 − α)et\n\nA +\n\nα2σ2 |VR|\n\n(cid:107)A(cid:107)2\n\nF,VR\n\n+ 2αL2Ξt +\n\n2L2η2 α\n\n(cid:107) ̄mt −\n\n1 η\n\n∆t(cid:107)2 2.\n\nBesides, consider ̃et+1 1\n(cid:88)\n\n ̃et+1 1 =\n\n1 |VR|\n\ni∈VR\n\nE(cid:107)mt+1\n\ni − ∇f ( ̄xt)(cid:107)2\n\n2 =\n\n1 |VR|\n\n≤2\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)mt+1\n\ni − ∇fi( ̄xt)(cid:107)2\n\n2 + 2\n\n=2et+1\n\nI + 2ζ 2.\n\ni∈VR 1\n|VR|\n\n(cid:88)\n\ni∈VR\n\n(cid:88)\n\nE(cid:107)mt+1\n\ni ± ∇fi( ̄xt) − ∇f ( ̄xt)(cid:107)2\n\n2\n\n(cid:107)∇fi( ̄xt) − ∇f ( ̄xt)(cid:107)2\n\n2\n\nAs we know that (cid:107)∆t+1(cid:107)2 Lemma 10 (Bound on et+1\n\n2\n\n2 ≤ et+1\n\n2\n\n, then we need to finally bound et+1\n\n2\n\n). For δmax := maxi∈VR δi, if\n\nτ t+1 i =\n\n(cid:115) 1 δi\n\n(cid:88)\n\nj∈VR\n\nWij E\n\n(cid:13) (cid:13)\n\n(cid:13)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n,\n\nthen we have\n\nwhere constant c1 = 32.\n\n2 ≤ c1δmax(2η2(et+1 et+1\n\nI + ζ 2) + Ξt).\n\nProof. Use Young’s inequality (16) to bound et+1\n\n2\n\nby two parts\n\net+1 2 =\n\n1 |VR|\n\n(cid:88)\n\nE\n\ni∈VR\n\nj∈VR\n\n(cid:88)\n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\n2 |VR|\n\n(cid:88)\n\ni∈VR\n\n(cid:124)\n\nj∈VR\n\n(cid:123)(cid:122) =:A1\n\n) +\n\n(cid:88)\n\nj∈VB\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13) (cid:13) 2\n\n(cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13) (cid:13) 2\n(cid:125)\n\n+\n\n2 |VR|\n\n(cid:88)\n\nE\n\ni∈VR\n\n(cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nj∈VB\n\n(cid:123)(cid:122) =:A2\n\nE\n\n(cid:88)\n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\n(cid:88)\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n.\n\n(cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13) (cid:13) 2\n(cid:125)\n\nLook at the first term use triangular inequality of (cid:107)·(cid:107) and the definition of τ t+1\n\ni\n\nA1 ≤\n\n2 |VR|\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\nWij E\n\ni∈VR\n\nj∈VR\n\n(cid:13) (cid:13)zt+1 (cid:13)\n\nj→i − xt+1/2\n\nj\n\n 2\n\n\n\n(cid:13) (cid:13) (cid:13)2\n\n≤\n\n2 |VR|\n\n(cid:88)\n\ni∈VR\n\n\n\n\n\n1 τ t+1\n\ni\n\n(cid:88)\n\nj∈VR\n\nWij E\n\n(cid:13) (cid:13)\n\n(cid:13)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n 2\n\n\n\n.\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\nThe second inequality holds true because we can consider two cases of zt+1\n\nj→i for all j ∈ VR\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\n• If (cid:107)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n2 ≤ τ t+1 (cid:107)2\n\ni\n\n, then CLIP has no effect and therefore zt+1\n\nj→i = xt+1/2\n\nj\n\n0 = (cid:107)zt+1\n\nj→i − xt+1/2\n\nj\n\n(cid:107)2 ≤\n\n1 τ t+1\n\ni\n\n(cid:107)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:107)2 2.\n\n• If (cid:107)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n, then zt+1\n\ni\n\n2 > τ t+1 (cid:107)2 (cid:107)zt+1\n\nj→i − xt+1/2\n\nj\n\nj→i sits between xt+1/2 i = (cid:107)xt+1/2 (cid:107)2 + τ t+1\n\nj\n\ni\n\n− xt+1/2\n\nj\n\n(cid:107)2.\n\nand xt+1/2\n\ni\n\nwith\n\nTherefore, using the inequality a − τ ≤ a2\n\nτ for a > 0 we have that\n\n(cid:107)zt+1\n\nj→i − xt+1/2\n\nj\n\n(cid:107)2 = (cid:107)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:107)2 − τ t+1\n\ni ≤\n\n1 τ t+1\n\ni\n\n(cid:107)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:107)2 2.\n\nTherefore we justify the second inequality.\n\nOn the other hand,\n\nA2 ≤\n\n=\n\n2 |VR|\n\n2 |VR|\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\nWij E\n\ni∈VR\n\nj∈VB\n\n(cid:88)\n\ni∈VR\n\ni (τ t+1 δ2\n\ni\n\n)2.\n\n(cid:13) (cid:13)zt+1 (cid:13)\n\nj→i − xt+1/2\n\ni\n\n\n\n2\n\n\n\n≤\n\n(cid:13) (cid:13) (cid:13)2\n\n2 |VR|\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\ni∈VR\n\nj∈VB\n\n 2\n\nWij(τ t+1\n\ni\n\n) \n\nThen minimizing the RHS of et+1\n\n2\n\nτ t+1 i =\n\nby tuning radius for clipping (cid:115) 1 δi\n\n(cid:13)xt+1/2\n\nWij E\n\n(cid:88)\n\n(cid:13) (cid:13)\n\ni\n\n− xt+1/2\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\nj∈VR\n\nThen we come to the following bound\n\net+1 2 ≤\n\n4 |VR|\n\n(cid:88)\n\nδi\n\n(cid:88)\n\nWij E\n\ni∈VR\n\nj∈VR\n\n(cid:13) (cid:13)\n\n(cid:13)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n.\n\nThen we expand the norm as follows = E (cid:13)\n\n− xt+1/2\n\n(cid:13) (cid:13)\n\nE\n\n(cid:13)xt+1/2\n\ni\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n(cid:13)xt\n\ni − ηmt+1\n\ni − xt\n\nj + ηmt+1\n\nj\n\n(cid:13) 2\n(cid:13) 2\n\ni ± ̄xt − xt\n\n= E (cid:13) (cid:13)xt ≤4η2 E(cid:107)mt+1\n\nj + ηmt+1\n\nj ± η∇f ( ̄xt) − ηmt+1 2 + 4η2 E(cid:107)mt+1\n\nj − ∇f ( ̄xt)(cid:107)2\n\n2\n\ni\n\ni − ∇f ( ̄xt)(cid:107)2\n\n(cid:13) 2\n(cid:13) 2\n\n(23)\n\n+ 4(cid:107)xt\n\ni − ̄xt(cid:107)2\n\n2 + 4(cid:107)xt\n\nj − ̄xt(cid:107)2\n\n2\n\nWij = 1 − δi we have\n\nδi(1 − δi) E(cid:107)mt+1\n\ni − ∇f ( ̄xt)(cid:107)2\n\n2 +\n\n16η2 |VR|\n\n(cid:88)\n\n(cid:88)\n\nj∈VR\n\ni∈VR\n\nδiWij E(cid:107)mt+1\n\nj − ∇f ( ̄xt)(cid:107)2\n\n2\n\nUse the fact that (cid:80) 16η2 |VR|\n\net+1 2 ≤\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\n+\n\n16 |VR|\n\n(cid:88)\n\ni∈VR\n\nδi(1 − δi)(cid:107)xt\n\ni − ̄xt(cid:107)2\n\n2 +\n\n16 |VR|\n\n(cid:88)\n\n(cid:88)\n\nj∈VR\n\ni∈VR\n\nδiWij(cid:107)xt\n\nj − ̄xt(cid:107)2\n\n2\n\nUse the fact that δi ≤ δmax and 1 − δi ≤ 1 for all i ∈ VR,\n\n2 ≤ 32δmax(2η2(et+1 et+1\n\nI + ζ 2) + Ξt).\n\nTheorem I(cid:48). Let ̄x := 1 with\n\n|VR|\n\n(cid:80)\n\ni∈VR\n\nIf the initial consensus distance is bounded as output ˆxi of CLIPPEDGOSSIP satisfies\n\nxi be the average iterate over the unknown set of regular nodes (cid:113) 1\n\n(cid:80)\n\n(24)\n\nτi =\n\nδi\n\nWij E (cid:107)xi − xj(cid:107)2 2. (cid:80)\n\ni∈VR\n\nj∈VR 1\n|VR|\n\nE(cid:107)xi − ̄x(cid:107)2 ≤ ρ2, then for all i ∈ VR, the\n\n1 |VR|\n\n(cid:80)\n\ni∈VR\n\nE(cid:107) ˆxi − ̄x(cid:107)2 ≤ (cid:0)1 − γ + c\n\nδmax\n\n(cid:1)2\n\nρ2\n\n√\n\nwhere the expectation is over the random variable {xi}i∈VR and c > 0 is a constant.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We can consider the 1-step consensus problem as 1-step of optimization problem with ρ2 = Ξt 2 in terms of ρ2, p, and and η = 0. Then we look for the upper bound of δmax.\n\ni − ̄xt(cid:107)2\n\nE(cid:107)xt+1\n\n1 |VR|\n\ni∈VR\n\n(cid:80)\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)xt+1\n\ni − ̄xt(cid:107)2\n\n2 =\n\n1 |VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i − ̄xt(cid:107)2\n\n2\n\n=\n\n1 |VR|\n\n(cid:88)\n\n(cid:88)\n\nE(cid:107)(\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wijxt\n\nj − ̄xt) + (\n\nn (cid:88)\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt\n\nj)(cid:107)2 2.\n\nApply (16) with ε > 0 and use the expected improvement Lemma 4\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)xt+1\n\ni − ̄xt(cid:107)2\n\n2\n\n≤\n\n1 + ε |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wijxt\n\nj − ̄xt(cid:107)2\n\n2 +\n\n≤\n\n(1 + ε)(1 − p) |VR|\n\n(cid:88)\n\ni∈VR\n\n≤(1 + ε)(1 − p)Ξt +\n\n(cid:107)xt\n\ni − ̄xt(cid:107)2\n\n2 +\n\n1 + 1 ε\n|VR|\n\n1 + 1 ε\n|VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt\n\nj(cid:107)2\n\n2\n\n(cid:102)Wijxt\n\nj(cid:107)2\n\n2\n\n1 + 1 ε\n|VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt\n\nj(cid:107)2\n\n2\n\nReplace xt\n\nj = xt+1/2\n\nj\n\n+ ηmt+1\n\nj\n\nusing (12), then apply (18) and η = 0\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)xt+1\n\ni − ̄xt(cid:107)2\n\n2 ≤ (1 + ε)(1 − p)Ξt +\n\n1 + 1 ε\n|VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt+1/2\n\nj\n\n(cid:107)2 2.\n\nRecall the definition of et+1\n\n2\n\net+1\n\n2\n\n:=\n\n1 |VR|\n\nn (cid:88)\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt+1/2\n\nj\n\n(cid:107)2 2.\n\nThen use Lemma 9 with the case A = (cid:102)W and apply Lemma 10 with η = 0\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)xt+1\n\ni − ̄xt(cid:107)2\n\n2 ≤ (1 + ε)(1 − p)Ξt + (1 +\n\n1 ε\n\n)et+1\n\n2 ≤ (1 + ε)(1 − p)Ξt + (1 +\n\n1 ε\n\n)32δmaxΞt.\n\nLet’s minimize the right hand side of the above inequality by taking ε such that ε(1 − p) = 32δmax\n\nε\n\nwhich leads to ε =\n\n1−p , then the above inequality becomes\n\n(cid:113) 32δmax\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\nE(cid:107)xt+1\n\ni − ̄xt(cid:107)2\n\n2 ≤ (1 − p + 32δmax + 2(cid:112)32δmax(1 − p))Ξt = ((cid:112)1 − p +\n\n(cid:112)\n\n32δmax)2Ξt.\n\nThe consensus distance to the average consensus is only guaranteed to reduce if 1 which is\n\nδmax <\n\n(1 − (cid:112)1 − p)2.\n\n1 32\n\nFinally, we complete the proof by simplifying the notation to spectral gap γ := 1 −\n\n√\n\n1 − p.\n\n√\n\n1 − p+\n\n√\n\n32δmax <\n\nRecall that\n\net+1\n\n2\n\n:=\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nj∈VR\n\n(cid:88)\n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\nNext we consider the bound on consensus distance Ξt.\n\n30\n\n) +\n\n(cid:88)\n\nj∈VB\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13) (cid:13) 2\n\n.\n\n(25)\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 11 (Bound consensus distance Ξt). Assume Lemma 4, then Ξt has the following iteration\n\nΞt+1 ≤ (1 + ε)(1 − p)Ξt + c2(1 +\n\n(cid:18)\n\n1 ε\n\n)\n\n2 + η2 ̄et+1 et+1\n\n1 + η2ζ 2 + η2(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 + η2 E(cid:107) ̄mt+1 −\n\n∆t+1(cid:107)2\n\n2\n\n(cid:19)\n\n.\n\n1 η\n\nwhere ε > 0 is determined later such that (1 + ε)(1 − p) < 1 and c2 = 5.\n\nProof. Expand the consensus distance at time t + 1\n\nΞt+1 =\n\n1 |VR|\n\n=\n\n1 |VR|\n\n=\n\n1 |VR|\n\n(cid:88)\n\ni∈VR\n\n(cid:88)\n\nE(cid:107)xt+1\n\ni − ̄xt+1(cid:107)2\n\n2 =\n\n1 |VR|\n\nn (cid:88)\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i − ̄xt+1(cid:107)2\n\n2\n\nn (cid:88)\n\nE(cid:107)\n\nWijzt+1\n\nj→i − ̄xt + ̄xt − ̄xt+1(cid:107)2\n\n2\n\ni∈VR\n\nj=1\n\n(cid:88)\n\n(cid:88)\n\nE(cid:107)(\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wijxt\n\nj − ̄xt) + (\n\nn (cid:88)\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt\n\nj) + ̄xt − ̄xt+1(cid:107)2 2.\n\nApply Young’s inequality (16) with coefficient ε, like the proof of Theorem I, and use the expected improvement Lemma 4\n\nΞt+1 ≤\n\n1 + ε |VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wijxt\n\nj − ̄xt(cid:107)2\n\n2\n\n+\n\n1 + ε ε|VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt\n\nj + ̄xt − ̄xt+1(cid:107)2\n\n2\n\n≤\n\n(1 + ε)(1 − p) |VR|\n\n(cid:88)\n\ni∈VR\n\n≤(1 + ε)(1 − p)Ξt +\n\n(cid:107)xt\n\ni − ̄xt(cid:107)2\n\n2 +\n\n1 + ε ε|VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt\n\nj + ̄xt − ̄xt+1(cid:107)2\n\n2\n\n1 + ε ε|VR|\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)(\n\ni∈VR\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:124)\n\n(cid:123)(cid:122) =:T1\n\n(cid:102)Wijxt\n\nj) + ̄xt − ̄xt+1(cid:107)2\n\n2\n\n(cid:125)\n\nReplace xt\n\nj = xt+1/2\n\nj\n\n+ ηmt+1\n\nj\n\nusing (12), then apply (18)\n\nn (cid:88)\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt+1/2\n\nj\n\n−η\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijmt+1\n\nj + ̄xt − ̄xt+1(cid:107)2\n\n2\n\nT1 =\n\n1 + ε ε|VR|\n\n≤5\n\n1 + ε ε\n\n(cid:88)\n\nE(cid:107)\n\ni∈VR \n 1\n\n|VR|\n\n+ η2\n\n|VR|\n\n(cid:88)\n\nE(cid:107)\n\n(cid:88)\n\ni∈VR (cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\nj=1\n\nWijzt+1\n\nj→i −\n\n(cid:88)\n\nj∈VR\n\n(cid:102)Wijxt+1/2\n\nj\n\n(cid:107)2\n\n2\n\n(cid:102)Wij(mt+1\n\nj −∇fj( ̄xt))(cid:107)2\n\n2\n\ni∈VR\n\nj∈VR\n\n+ η2\n\n|VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wij∇fj( ̄xt) − ∇f ( ̄xt)(cid:107)2\n\n2 + η2(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 + E(cid:107) ̄xt − ̄xt+1(cid:107)2\n\n2\n\nRecall the definition of et+1\n\n2\n\n(26)\n\n\n\n .\n\n(cid:88)\n\nE\n\n(cid:88)\n\nWij(zt+1\n\nj→i − xt+1/2\n\nj\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nj∈VR\n\n) +\n\n(cid:88)\n\nj∈VB\n\nWij(zt+1\n\nj→i − xt+1/2\n\ni\n\n)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n\net+1\n\n2\n\n:=\n\n1 |VR|\n\n=\n\n1 |VR|\n\ni∈VR\n\n(cid:88)\n\nn (cid:88)\n\nE(cid:107)\n\nWijzt+1\n\nj→i −\n\ni∈VR\n\nj=1\n\n(cid:102)Wijxt+1/2\n\nj\n\n(cid:107)2\n\n2\n\n(cid:88)\n\nj∈VR\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nThen use Lemma 9 with the case A = (cid:102)W ,\n\nT1 ≤5(1 +\n\n1 ε\n\n)\n\n et+1\n\n2 + η2 ̄et+1\n\n1 + η2\n\n|VR|\n\n(cid:88)\n\n(cid:107)\n\n(cid:88)\n\ni∈VR\n\nj∈VR\n\n(cid:102)Wij∇fj( ̄xt) − ∇f ( ̄xt)(cid:107)2\n\n2 + η2(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 + E(cid:107) ̄xt − ̄xt+1(cid:107)2\n\n2\n\n\n\n .\n\nUse convexity of (cid:107)·(cid:107)2\n\n2 and (A3) we have\n\nT1 ≤5(1 +\n\n1 ε\n\n) (cid:0)et+1\n\n2 + η2 ̄et+1\n\n1 + η2ζ 2 + η2(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 + E(cid:107) ̄xt − ̄xt+1(cid:107)2\n\n2\n\n(cid:1) .\n\nUse (21) for the last term\n\nT1 ≤5(1 +\n\n(cid:18)\n\n1 ε\n\n)\n\n2 + η2 ̄et+1 et+1\n\n1 + η2ζ 2 + η2(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 + η2 E(cid:107) ̄mt+1 −\n\n∆t+1(cid:107)2\n\n2\n\n(cid:19)\n\n.\n\n1 η\n\nFinally, by the definition of ̃et+1\n\n1\n\n, we have\n\nΞt+1 ≤ (1 + ε)(1 − p)Ξt + 5(1 +\n\n(cid:18)\n\n1 ε\n\n)\n\n2 + η2 ̄et+1 et+1\n\n1 + η2ζ 2 + η2(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 + η2 E(cid:107) ̄mt+1 −\n\n∆t+1(cid:107)2\n\n2\n\n(cid:19)\n\n.\n\n1 η\n\nLemma 12 (Tuning stepsize.). Suppose the following holds for any step size η ≤ d:\n\nΨT ≤\n\nr0 η(T + 1)\n\n+ bη + eη2 + f η3 .\n\nThen, there exists a step-size η ≤ d such that\n\nΨT ≤ 2(\n\nbr0 T + 1\n\n1\n\n2 + 2e\n\n1\n\n3 (\n\n)\n\nr0 T + 1\n\n2\n\n3 + 2f\n\n)\n\n1\n\n4 (\n\nr0 T + 1\n\n3\n\n4 +\n\n)\n\ndr0 T + 1\n\n.\n\nProof. Choosing η = min\n\n(cid:26)(cid:16) r0\n\nb(T +1)\n\n(cid:17) 1\n\n2\n\n,\n\n(cid:16) r0\n\ne(T +1)\n\n(cid:17) 1\n\n3\n\n,\n\n(cid:16) r0\n\nf (T +1)\n\n(cid:17) 1\n\n4\n\n, 1 d\n\n(cid:27)\n\n≤ 1\n\nd we have four cases\n\n• η = 1\n\nd and is smaller than\n\n(cid:16) r0\n\nb(T +1)\n\n(cid:17) 1\n\n2\n\n,\n\n(cid:16) r0\n\ne(T +1)\n\n(cid:17) 1\n\n3\n\n,\n\n(cid:16) r0\n\nf (T +1)\n\n(cid:17) 1\n\n4\n\n, then\n\nΨT ≤\n\ndr0 T + 1\n\n+\n\nb d\n\n+\n\ne\n\nd2 +\n\nf\n\nd3 ≤\n\ndr0 T + 1\n\n+\n\n(cid:18) br0\n\n(cid:19) 1\n\n2\n\nT + 1\n\n+ e1/3\n\n(cid:18) r0\n\n(cid:19) 2\n\n3\n\nT + 1\n\n+ f 1/4\n\n(cid:18) r0\n\n(cid:19) 3\n\n4\n\nT + 1\n\n.\n\n• η =\n\n(cid:16) r0\n\nb(T +1)\n\n(cid:17) 1\n\n2\n\n< min{\n\n(cid:16) r0\n\ne(T +1)\n\n(cid:17) 1\n\n3\n\n,\n\n(cid:16) r0\n\nf (T +1)\n\n(cid:17) 1\n\n4\n\n}, then\n\nΨT ≤ 2\n\n(cid:18) br0\n\n(cid:19) 1\n\n2\n\nT + 1\n\n+\n\ner0 b(T + 1)\n\n+f\n\n(cid:19) 3\n\n2\n\n(cid:18)\n\nr0 b(T + 1)\n\n≤ 2\n\n(cid:18) br0\n\n(cid:19) 1\n\n2\n\nbT + 1\n\n+e1/3\n\n(cid:18) r0\n\n(cid:19) 2\n\n3\n\nT + 1\n\n+f 1/4\n\n(cid:18) r0\n\n(cid:19) 3\n\n4\n\nT + 1\n\n.\n\n• η =\n\n(cid:16) r0\n\ne(T +1)\n\n(cid:17) 1\n\n3\n\n< min{\n\n(cid:16) r0\n\nb(T +1)\n\n(cid:17) 1\n\n2\n\n,\n\n(cid:16) r0\n\nf (T +1)\n\n(cid:17) 1\n\n4\n\n}, then\n\nΨT ≤ 2e1/3\n\n(cid:18) r0\n\nT + 1\n\n(cid:19) 2\n\n3\n\n(cid:18)\n\n+b\n\nr0 e(T + 1)\n\n(cid:19) 1\n\n3\n\n+\n\nf r0 e(T + 1)\n\n≤\n\n(cid:18) br0\n\n(cid:19) 1\n\n2\n\nT + 1\n\n+2e1/3\n\n(cid:18) r0\n\n(cid:19) 2\n\n3\n\nT + 1\n\n+f 1/4\n\n(cid:18) r0\n\n(cid:19) 3\n\n4\n\nT + 1\n\n.\n\n• η =\n\n(cid:16) r0\n\nf (T +1)\n\n(cid:17) 1\n\n4\n\n< min{\n\n(cid:16) r0\n\nb(T +1)\n\n(cid:17) 1\n\n2\n\n,\n\n(cid:16) r0\n\ne(T +1)\n\n(cid:17) 1\n\n3\n\n}, then\n\nΨT ≤ 2f 1/4\n\n(cid:18) r0\n\nT + 1\n\n(cid:19) 3\n\n4\n\n(cid:18)\n\n+b\n\nr0 f (T + 1)\n\n(cid:19) 1\n\n4\n\n(cid:18)\n\n+e\n\nr0 f (T + 1)\n\n(cid:19) 1\n\n2\n\n≤\n\n(cid:18) br0\n\n(cid:19) 1\n\n2\n\nT + 1\n\n+e1/3\n\n(cid:18) r0\n\n(cid:19) 2\n\n3\n\nT + 1\n\n+2f 1/4\n\n(cid:18) r0\n\n(cid:19) 3\n\n4\n\nT + 1\n\n.\n\nThen, take the uniform upper bound of the upper bound gives the result.\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nE.3 PROOF OF THE MAIN THEOREM\n\nTheorem III(cid:48). Suppose Assumptions 1–4 hold and δmax = O(γ2). Define the clipping radius as\n\n(cid:114)\n\nτ t+1 i =\n\n1 δi\n\n(cid:80)\n\nj∈VR\n\nWij E\n\n(cid:13) (cid:13)\n\n(cid:13)xt+1/2\n\ni\n\n− xt+1/2\n\nj\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n.\n\n(27)\n\nThen for α := 3ηL, the iterates of Algorithm 1 satisfy\n\n1 T +1\n\n(cid:80)T\n\nt=0(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 ≤ 200c1c2\n\nγ2\n\nδmaxζ 2 + 2( 32 γ2 ζ 2(cid:17)1/3 (cid:16) r0L\n\nT +1\n\n(cid:16) 48c2\n\n+ 2\n\n|VR| + 320c1c2\n\nγ2\n\n(cid:17)2/3\n\n+ 2\n\nδmax)1/2 (cid:16) 3Lσ2r0 γ2 σ2(cid:17)1/4 (cid:16) r0L\n\nT +1\n\nT +1\n\n(cid:16) 144c2\n\n(cid:17)1/2\n\n(cid:17)3/4\n\n+ d0r0 T +1 .\n\nwhere r0 := f (x0) − f (cid:63) and c1 = 32 and c2 = 5. Furthermore, the consensus distance has an upper bound\n\n1 |VR|\n\n(cid:80)\n\ni∈VR\n\n(cid:107)xt\n\ni − ̄xt(cid:107)2\n\n2 = O(\n\nζ2 γ2(T +1) ).\n\nRemark 13. The requirement δmax = O(γ2) suggest that δmax and γ2 are of same order. The exact constant are determined in the proof and can be tighten simply through better constants in equalities like (23), (26). In practice CLIPPEDGOSSIP allow high number of attackers. For example in Figure 15, 1/6 of workers are Byzantine and CLIPPEDGOSSIP still perform well in the non-IID setting.\n\nProof. Denote the terms of average t from 0 to T as follows\n\nC1 :=\n\n1 1 + T\n\nE1 :=\n\n1 1 + T\n\nt=0\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\n(cid:107)∇f ( ̄xt)(cid:107)2\n\n2, C2 :=\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\n(cid:107) ̄mt+1 −\n\n1 η\n\n∆t+1(cid:107)2\n\n2, D1 :=\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\nΞt+1\n\net+1\n\n1\n\n, ̄E1 :=\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\n ̄et+1\n\n1\n\n, EI :=\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\net+1\n\nI\n\n, E2 :=\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\net+1\n\n2\n\nFirst we apply average to Lemma 10\n\nE2 ≤ c2δmax(2η2(EI + ζ 2) + D1).\n\n(28)\n\nThen we rewrite key Lemma 8 as\n\n(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 +\n\n1 2\n\nE(cid:107) ̄mt+1 −\n\n1 η\n\n∆t+1(cid:107)2\n\n2 ≤\n\n2 η\n\n(rt − rt+1) + 2et+1\n\n1 +\n\nand further average over time t\n\nC1 +\n\n1 2\n\nC2 ≤\n\n2r0 η(T + 1)\n\n+ 2E1 +\n\n2\n\nη2 E2\n\nwhere we use −f (xT +1) ≤ −f (cid:63). Combined with (28) gives\n\n2\n\nη2 et+1\n\n2\n\n,\n\nC1 +\n\n1 2\n\nC2 ≤\n\n2r0 η(T + 1)\n\n+ 2E1 + 4c2δmaxEI + 4c2δmaxζ 2 +\n\n2c2δmax\n\nη2 D1\n\n(29)\n\nNow we also average Lemma 9 for et+1\n\n1\n\nover t gives\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\net+1 1 ≤\n\n1 − α 1 + T\n\n≤\n\n1 − α 1 + T\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\nt=0\n\n1 + 2αL2D1 + et\n\nα2σ2 |VR|\n\n+\n\n2L2η2 α\n\n1 1 + T\n\nT (cid:88)\n\nt=0\n\n(cid:107) ̄mt −\n\n1 η\n\n∆t(cid:107)2\n\n2\n\net+1 1 + 2αL2D1 +\n\nα2σ2 |VR|\n\n+\n\n2L2η2 α\n\nC2\n\nwhere we use Ξ0 = e0\n\n1 = 0 and ̄m0 = ∆0 = 0. Then let β1 := 2L2η2\n\nα2\n\nE1 ≤ 2L2D1 +\n\nασ2 |VR|\n\n+ β1C2.\n\n33\n\n(30)\n\nUnder review as a conference paper at ICLR 2023\n\nSimilarly, Lemma 9 for et+1\n\nI\n\nthe only difference is that we don’t have 1\n\nn for σ2\n\nEI ≤ 2L2D1 + ασ2 + β1C2.\n\nSimilarly, let’s call β2 := 1 |VR|\n\n(cid:80)\n\ni∈VR\n\n(cid:80)\n\nj∈VR (cid:102)W 2\n\nij ≤ 1\n\nThe consensus distance Lemma 11 has\n\n ̄E1 ≤ 2L2D1 + β2ασ2 + β1C2.\n\n(31)\n\n(32)\n\nD1 ≤\n\n(1 + ε)(1 − p) 1 + T\n\nT (cid:88)\n\nt=0\n\nΞt + c2(1 + 1\n\nε )E2 + c2(1 + 1\n\nε )η2( ̄Et+1\n\n1 + ζ 2 + C1 + C2)\n\n≤(1 + ε)(1 − p)D1 + c2(1 + 1\n\nε )E2 + c2(1 + 1\n\nε )η2( ̄Et+1\n\n1 + ζ 2 + C1 + C2).\n\nReplace E2 using (28) gives\n\nD1 ≤(1 + ε)(1 − p)D1 + c2(1 + 1 ≤((1 + ε)(1 − p) + c1c2(1 + 1\n\nε )(c1δmax(2η2(Et+1 ε )δmax)D1 + c2(1 + 1\n\nI + ζ 2) + D1)) + c2(1 + 1 ε )η2(2c1δmaxEt+1\n\nI + ̄Et+1\n\nε )η2( ̄Et+1 1 + ζ 2 + C1 + C2) 1 + (1 + 2c1δmax)ζ 2 + C1 + C2).\n\nNow replace ̄E1, EI with (32), (31), then\n\nD1 ≤((1 + ε)(1 − p) + c2(1 + 1\n\nε )(c1δmax(1 + 4L2η2) + 2L2η2))D1\n\n+ c2(1 + 1\n\nε )η2((2c1δmax + β2)ασ2 + (2c1δmax + 1)ζ 2 + ((2c1δmax + 1)β1 + 1)C2 + C1).\n\nBy enforcing η ≤ γ\n\n9L and δmax ≤ γ2\n\n10c1c2\n\nwe have\n\n2c2L2η2 ≤γ2/8 c1c2δmax(1 + 4L2η2) ≤γ2/8\n\n(cid:112)c1c2δmax(1 + 4L2η2) + 2c2L2η2 ≤\n\nγ 2\n\n.\n\nwe can achieve\n\nThen\n\nD1 ≤ ((1 + ε)(1 − p) + (1 + 1\n\n(cid:124)\n\n(cid:123)(cid:122) =:T2\n\nε ) γ2 4 ) (cid:125)\n\nD1\n\n+ c2(1 + 1\n\nε )η2((2c1δmax + β2)ασ2 + (2c1δmax + 1)ζ 2 + ((2c1δmax + 1)β1 + 1)C2 + C1).\n\nLet us minimize the the coefficients of D1 on the right hand side of inequality by having\n\nthat is ε =\n\n(cid:113) γ2\n\n4(1−p) . Then the coefficient becomes\n\nε(1 − p) =\n\n1 ε\n\nγ2 4\n\n,\n\nT2 =(1 + ε)(1 − p) + (1 + 1 ε )\n\nγ 2\n\n)2\n\n=((cid:112)1 − p + γ\n2\n\n=(1 −\n\n)2.\n\nγ2 4\n\nThen we use 1\n\nε =\n\n(cid:113) 4(1−p)\n\nγ2 ≤ 2\n\nγ and 1 + 1\n\nε ≤ 3\n\nγ\n\nD1 ≤ 4c2η2\n\nγ2 ((2c1δmax + β2)ασ2 + (2c1δmax + 1)ζ 2 + ((2c1δmax + 1)β1 + 1)C2 + C1).\n\nThis leads to 2c1δmax ≤ γ2\n\n5c2\n\n≤ 1 and β2 ≤ 1, then we know\n\nD1 ≤\n\n4c2η2 γ2\n\n(2ασ2 + 2ζ 2 + C1 + (1 + 2β1)C2)\n\n(33)\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nFinally, we combine (29), (30), (32)\n\nC1 +\n\n1 2\n\nC2 ≤\n\n≤\n\n≤\n\n2r0 η(T + 1) 2r0 η(T + 1)\n\n+ 2E1 + 4c1δmaxEI + 4c1δmaxζ 2 +\n\n2c1δmax\n\nη2 D1\n\n+(4L2D1 + 2ασ2\n\n|VR| + 2β1C2)+2c1δmax(4L2D1 + 2β2ασ2 + 2β1C2)\n\n+ 4c1δmaxζ 2 +\n\n2c1δmax\n\nη2 D1\n\n+ (4L2 + 8c1δmaxL2 +\n\n2r0 η(T + 1) +4β1C2 + 4c1δmaxζ 2\n\n2c1δmax η2\n\n)D1 + ( 1\n\n|VR| + 2c1δmax)2ασ2\n\nThen we replace D1 with (33)\n\nC1 +\n\n1 2\n\nC2 ≤ 2r0\n\n|VR| + 2c1δmax)2ασ2 +4β1C2 + 4c1δmaxζ 2\n\nη(T +1) + ( 1 + (4L2η2 + 8c1δmaxL2η2 + 2c1δmax) 4c2\n\nγ2 (2ασ2 + 2ζ 2 + C1 + (1 + 2β1)C2)\n\n(34)\n\nTo have a valid bound on C1, there are two constraints on the coefficient of the RHS C1 and C2.\n\n(4L2η2 + 8c1δmaxL2η2 + 2c1δmax) 4c2\n\n(4L2η2 + 8c1δmaxL2η2 + 2c1δmax) 4c2\n\nγ2 (1 + 2β1) + 4β1 ≤\n\n.\n\nWe can strength the first requirement to\n\n(4L2η2 + 8c1δmaxL2η2 + 2c1δmax) 4c2\n\nγ2 ≤\n\n1 4\n\n.\n\n(35)\n\nThen, apply this inequality to the second inequality gives\n\n1 4\n\n+\n\n1 2\n\nβ1 + 4β1 ≤\n\n1 2\n\nwhich requires η ≤ α\n\n3L . Next (35) can be achieved by requiring δmax ≤ γ2\n\n(4 + 8c1δmax)L2η2 + 2c1δmax ≤ 8L2η2 + 2c1δmax ≤\n\nγ2 <1 1\n2\n\n64c1c2 γ2 16c2\n\nwhich requires 8η2L2 ≤ γ2 (35)\n\n32c2\n\n, and we can simplify it to η ≤ γ\n\n40L . Now we can simplify (34) with\n\n3\n\n4 C1 ≤ 2r0\n\n|VR| + 2c1δmax)2ασ2 + 4c1δmaxζ 2\n\nη(T +1) + ( 1 + (4L2η2 + 8c1δmaxL2η2 + 2c1δmax) 4c2\n\nγ2 (2ασ2 + 2ζ 2)\n\nMultiply both sides with 4 η(T +1) + ( 1\n\nC1 ≤ 3r0\n\n3 and relax constant 4 |VR| + 151\n\nγ2 2c1δmax)3ασ2 + 200c1c2\n\nγ2\n\n3 · 2 ≤ 3. Then by taking η ≤ 1\n\n2L we have that\n\nδmaxζ 2 + 48c2\n\nγ2 (ασ2 + ζ 2)L2η2\n\nBy taking α := 3ηL and relax the constants we have\n\nC1 ≤ 3r0\n\nη(T +1) + ( 32\n\n|VR| + 320c1\n\nγ2 δmax)Lσ2η + 48c2\n\nγ2 (ασ2 + ζ 2)L2η2 + 200c1c2\n\nγ2\n\nδmaxζ 2.\n\nMinimize the the right hand side by tuning step size Lemma 12 we have\n\n1 T + 1\n\nT (cid:88)\n\nt=0\n\n(cid:107)∇f ( ̄xt)(cid:107)2\n\n2 ≤ 200c1c2\n\nγ2\n\nδmaxζ 2 + 2\n\n\n\n\n\n( 32 |VR| + 320c1\n\nγ2 δmax)3Lσ2r0 T + 1\n\n+ 2\n\n(cid:16) 48c2\n\nγ2 ζ 2(cid:17) 1\n\n3 (cid:16) r0L\n\nT +1\n\n(cid:17) 2\n\n3\n\n+ 2\n\n(cid:16) 144c2\n\nγ2 σ2(cid:17) 1\n\n4 (cid:16) r0L\n\nT +1\n\n\n\n1 2\n\n\n\n(cid:17) 3\n\n4\n\n+\n\nd0r0 T + 1\n\n2L , γ\n\n9L , γ\n\n40L } = γ\n\n40L and\n\nwhere 1 d0\n\nη = min\n\n:= min{ 1 \n\n\n(cid:32)\n\n2r0\n\n\n\n|VR| + 320c1 ( 9\n\nγ2 δmax)Lσ2(T + 1)\n\n(cid:33)1/2\n\n(cid:18)\n\n,\n\n2r0γ2 48c2ζ 2L2(T + 1)\n\n(cid:19)1/3\n\n(cid:18)\n\n,\n\n2r0γ2 L3σ2(T + 1)\n\n(cid:19)1/4\n\n, 1 d0\n\n \n\n\n\n.\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nBound on the consensus distance D1. Since β1 = 2L2η2\n\nα2 = 2\n\n9 , we can relax (33) to\n\nD1 ≤ 4c2η2 ≤ 4c2η2\n\nγ2 (2ασ2 + 2ζ 2 + 2(1 + 2β1)(C1 + 1 γ2 (2ασ2 + 2ζ 2 + 3(C1 + 1\n\n2 C2)).\n\n2 C2))\n\nFor significantly large T , we know that η = α = O( 2ζ 2 + C1 + 1\n\n1√ ) and find the upper bound of 2ασ2 + 2 C2 with O(ζ 2) where higher order terms of 1/T are dropped. Therefore, the upper\n\nT +1\n\nbound on the consensus distance D1 is O\n\n(cid:16)\n\nζ2 γ2(T +1)\n\n(cid:17)\n\n.\n\nF OTHER RELATED WORKS AND DISCUSSIONS\n\nIn this section, we add more related works and discussions.\n\nByzantine resilient learning with constraints Byzantine-robustness is challenging when the training is combined with other constraints, such as asynchrony (Damaskinos et al., 2018; Xie et al., 2020b; Yang & Li, 2021), data heterogeneity (Karimireddy et al., 2021b; Peng & Ling, 2020; Li et al., 2019; Data & Diggavi, 2021), privacy (He et al., 2020; Burkhalter et al., 2021). These works all assume the existence of a central server which can communicate with all regular workers. In this paper, we consider the decentralized setting and focus on the constraint that not all regular workers can communicate with each other.\n\nMore works on decentralized learning. Many works focus on compression-techniques (Koloskova et al., 2019; 2020a; Vogels et al., 2020), data heterogeneity (Tang et al., 2018; Vogels et al., 2021; Koloskova et al., 2021), and communication topology (Assran et al., 2019; Ying et al., 2021a).\n\nDetailed comparison with one line of work. Among all the works on robust decentralized training, Sundaram et al. Sundaram & Gharesifard (2018) and Su et al. Su & Vaidya (2016a) and their followup works Yang & Bajwa (2019b;a) have the most similar setup with ours. They are all using the trimmed mean as the aggregator assumptions on the graph. We illustrate our advantages over these methods as follows\n\n1. Their methods (TM) make unrealistic assumptions about the graph while our method is much more relaxed. Their main assumption on the graph has 2 parts: 1) each good node should have at least 2b + 1 neighbors where b is the maximum number of Byzantine workers in the whole network; 2) by removing any b edges the good nodes should be connected. This assumption essentially requires the good workers have honest majority everywhere and additionally they have to be well connected. This can be hardly enforced in the decentralized environment. In contrast, our method has a weaker condition relating the spectral gap and δ. Our method also works without a honest majority Figure 12. The second part of their assumption exclude common topologies like Dumbbell.\n\n2. TM fails to reach consensus even in some Byzantine-free graphs (e.g. Dumbbell) while SSClip converges as fast as gossip. For example, TM fails to reach consensus in NonIID setting for MNIST dataset (Figure 4) and even fails in IID setting for CIFAR-10 dataset (Figure 14).\n\n3. We have a clear convergence rate for SGD while they only show asymptotic convergence for GD. In fact, we even improve the state-of-art decentralized SGD analysis (Koloskova et al., 2020b).\n\n4. Our work reveals how the quantitative relation between percentage of Byzantine workers (δ) and\n\ninformation bottleneck (γ) influence the consensus (see Figure 3 and Theorem I).\n\n5. We propose a novel dissensus attacks that utilize topology information.\n\n6. Impossibility results. Sundaram et al. Sundaram & Gharesifard (2018) and Su et al. Su & Vaidya (2016a) give impossibility results in terms of number of nodes while we give a novel results in terms of spectral gap (γ).\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Before aggregation.\n\n(b) Clipping updates.\n\n(c) Gossip averaging.\n\nFigure 17: Diagram of ClippedGossip at time t on worker i. Let purple node be the model of worker i and green nodes be models of worker i’s regular neighbors and red nodes be models of worker i’s Byzantine neighbors. The figure (a), (b), and (c) demonstrate the 3 stages of ClippedGossip. First, in the left figure (a) worker i collects models {xt+1/2 : j ∈ Ni} from its neighbors. Then in the middle figure (b) worker i clips neighbor models to ensure the clipped models are no farther than τ t+1 from node i. Nodes outside the circle (e.g. xt+1/2 j→i) while nodes inside the circle (e.g. xt+1 j(cid:48)→i). In the right figure (c) worker i update its model to xt+1\n\n) remain the same after clipping (e.g. zt+1\n\nusing gossip averaging over clipped models.\n\n) clipped to the circle (e.g. zt+1\n\nj(cid:48)\n\nj\n\nj\n\ni\n\ni\n\nOther related works and discussions. Zhao et al. Zhao et al. (2019) make assumption that some users are trusted and then adopt trimmed mean as robust aggregator. But this assumption is incompatible with our setting where every node only trusts itself. Peng et al. Peng & Ling (2020) propose a “zero-sum” attack which exploits the topology where Byzantine worker j construct\n\nxj := −\n\n(cid:80)\n\nk∈Ni∩VR |Ni∩VB|\n\nxk\n\n.\n\nThey aim to manipulate the good worker i’s model to 0, but it also makes the constructed Byzantine model very far away from the good worker models, making it easy to detect. In contrast, our dissensus attack (6) simply amplifies the existing disagreement amongst the good workers, which keeps the attack much less undetectable. In addition, we take mixing matrix into consideration and use εi to parameterize the attack which makes it more flexible.\n\nClarifications about our method. We make the following clarifications regarding our method:\n\n• Ideally we would like to replace the δmax = maxj δj with an average ̄δ = 1\n\nj δj. However, the requirement that δmax be small may be achieved by the good workers increasing its weight on itself. Note that Byzantine workers cannot alter good workers local behavior.\n\nn\n\n(cid:80)\n\n• Theorem III does not tell us what happens if the percentage of Byzantine workers δ is relatively larger than spectral gap (γ), but it does not necessarily mean that CLIPPEDGOSSIP diverges. Instead, it means reaching global consensus is not possible as Byzantine workers effectively block the information bottleneck. We conjecture that within each connected good component not blocked by the byzantine workers, the good workers still reach component-level consensus by applying the analysis of Theorem III to only this component. We leave such a component-wise analysis for future work.\n\n37\n\nxit+ൗ12xjt+ൗ12τit+1xj′t+ൗ12zj→it+1=+Clip−,τit+1zj→it+1zj′→it+1xit+1=Σi=1nWijzj→it+1zj→it+1zj′→it+1WijWij′xit+1",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a novel algorithm to deal with Byzantine resilient decentralized optimization for data training.\n\n# Strength And Weaknesses\n\n+ The algorithm dose not rely on additional graph connectivity\n+ The performance is better than some existing algorithms\n\n- The algorithm does not guarantee accuracy of the optimization process\n- The idea behind the algorithm is unclear. In particular, the Clipping idea involves a parameter \\tau but its role is not well explained and illustrated. \n- The paper has an adaptive selection process for \\tau_i, but it cannot be computed only rely on local information. In particular, it involves \\delta_max, which seems not an available information.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work idea is original.  In general, the paper is easy to follow, except for the \\tau issue.\n\n# Summary Of The Review\n\nThe paper proposes a novel algorithm to deal with Byzantine resilient decentralized optimization for data training, which has better performance compared with some existing works. However, the idea behind the algorithm is not clearly explained. In particular, the role of the \\tau parameter and the adaptive selection process for \\tau_i is unclear to me.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSKETCHKNITTER: VECTORIZED SKETCH GENERATION WITH DIFFUSION MODELS\n\nQiang Wang1 Haoge Deng1 Yonggang Qi1∗ Da Li2 Yi-Zhe Song2 1Beijing University of Posts and Telecommunications, CN 2SketchX, CVSSP, University of Surrey, UK {wanqqiang, denghaoge, qiyg}@bupt.edu.cn\n\ndali.academic@gmail.com y.song@surrey.ac.uk\n\nABSTRACT\n\nWe show vectorized sketch generation can be identified as a reversal of the stroke deformation process. This relationship was established by means of a diffusion model that learns data distributions over the stroke-point locations and pen states of real human sketches. Given randomly scattered stroke-points, sketch generation becomes a process of deformation-based denoising, where the generator rectifies positions of stroke points at each timestep to converge at a recognizable sketch. A key innovation was to embed recognizability into the reverse time diffusion process. It was observed that the estimated noise during the reversal process is strongly correlated with sketch classification accuracy. An auxiliary recurrent neural network (RNN) was consequently used to quantify recognizability during data sampling. It follows that, based on the recognizability scores, a sampling shortcut function can also be devised that renders better quality sketches with fewer sampling steps. Finally it is shown that the model can be easily extended to a conditional generation framework, where given incomplete and unfaithful sketches, it yields one that is more visually appealing and with higher recognizability.\n\n1\n\nINTRODUCTION\n\nFree-hand human sketches are abstract concepts which can efficiently express ideas. Generative models for sketches have received increasing attentions in recent years. Compared with producing pixelated sketches (Ge et al., 2020; Chen et al., 2001; Liu et al., 2020), modeling sketches with point trajectories is more reasonable and appealing as it more closely resembles drawing process of humans. Sketch-RNN (Ha & Eck, 2018) utilizes a set of discrete stroke points and binary pen states as an approximation of the continuous drawing trajectory. B ́ezierSketch (Das et al., 2020) makes use of parametric representation, which fits the stroke trajectory by B ́ezier curves. Very recently, SketchODE (Das et al., 2021a) applies neural ordinary differential equations to representing stroke trajectory through continuous-time functions. All said approaches however suffer from the inability to model complex vectorized sketches. This is largely attributed to the de-facto RNN backbone that falls short in accommodating large stroke point numbers – rule of thumb is anything beyond 200 points will fail (Pascanu et al., 2013; Das et al., 2021b).\n\nIn this paper, we attempt to change the status quo in how stroke-point trajectories are modeled. Instead of seeing sketch generation as a process of determining where the next stroke-point lies under each recurrent step (as per RNN), we attempt to estimate distributions of all stroke-points holistically at each time instance – as every knitting enthusiast will tell you, it is all about having a global plan, never just about the next thread! 1.\n\nOur key novelty lies with the realization that sketch generation can be conceptualized as the reversal of a stroke deformation process. Through modeling a forward deformation process (i.e., sketch to noise), our diffusion model learns the stroke-point distributions of real human sketches, and thus able to reverse the process to generate novel sketches given noisy input. It follows that given this diffusion setup, the sequential information in sketches can be persevered by simply maintaining the temporal ordering of stroke-points during reverse-time diffusion.\n\n∗Correspondence to: Yonggang Qi (qiyg@bupt.edu.cn). Code to be found at GitHub page 1https://www.ravelry.com/\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: (a) Built on diffusion models, sketch generation is formulated as a stroke deformation rectification problem. Essentially, our model is to reorganize points with fixed adjacency, such that placing them on meaningful locations out of a total mess, dubbed sketch knitting. Note that the order of stroke points is pre-determined and unchanged. (b) Generated sketches with different deformation levels associated with timesteps by our model. Early sampling stage is not efficient that the obtained data change little and remain noisy. (c) Early sampling is inefficient as basically the same estimated noise to each stroke point is used, led sketch remains noisy and unrecognizable. And different noise patterns can be observed at different timesteps, which motivate us to devise recognizability-based skipping (blue△) based on the estimated noise to find shortcut sampling. Red solid curve denotes mean and the shade denotes variance. 1000 generated sketches are used for plotting.\n\nWe further draw importance on the overall quality (recognizability) of the sketches generated. We show that the estimated noise in the sampling stage naturally can reflect the recognizability of the generated sketch at each timestep. It follows that a learnable RNN was devised to explicitly model the relation between estimated noise and recognizability. This is achieved by introducing an pretrained image classifier as supervision signal. Embedding recognizabilty into the sampling process also yields the added benefit of introducing skip steps that allows for more efficient and effective data generation. This is because early stages for generating sequential data is very inefficient using vanilla DDPMs sampling (Ho et al., 2020) as witnessed in Figure 1 (b), resulting in minor improvement of recognizability in a long period of sampling as unveiled in Figure 1 (c).\n\nLast but not least, we demonstrate the model (without retraining) can be readily used to remedy defects in sketches due to unfaithful or incomplete drawing, by incorporating instance-aware guidance into data sampling. Motivated by recent works on guided diffusion models (Dhariwal & Nichol, 2021; Ho & Salimans, 2022), gradients of perceptual similarity (Zhang et al., 2018) between the generated data and the conditional sketch were incorporated during sampling to guide the noise prediction, thereby influencing the obtained sample at each timestep. This was done with the goal of enforcing visual similarity to the conditional, flawed sketch, while also being more appealing and recognizable after reverse-time diffusion.\n\nOur contributions can be summarized as follows: (i) Denoising diffusion models are exploited for sketch generation in vector format. The generative model is to learn distribution over stroke points’ locations, from a deformation-based denoising process which starts from noise. (ii) The quality, i.e., recognizablity, of the generated sketches is quantifiable by leveraging the knowledge of the estimated noises during sampling. This is achieved by devising an auxiliary RNN, which is trained supervised under a pre-trained image classifier, to predict the recognizability of a generated sketch at timestep t from the corresponding estimated noise. (iii) A shortcut sampling path can be discovered through a simple skip strategy based on the learned quality measurement net. This allows faster and more effective generation with little trade off in data quality. (iv) Instance-aware guidance built on perceptual metric is embedded into the reverse-time diffusion. It enables our model to recover distorted or corrupted sketches without retraining.\n\n2 RELATED WORKS\n\nSketch Generation There is a rich literature of research works related to sketch generation. Early works (Guo et al., 2007; Li et al., 2019) leverage edge maps as substitution for sketches. Coupled\n\n2\n\nSequential Samplingxy12n3456Diffusion ModelsxyT stepsData SampleRandom Noise1212AppleStartEndIntermidiateLong but inefficient stepsshorter but efficient(a)(b)(c)Published as a conference paper at ICLR 2023\n\nwith deep learning, much progress has been made recently. Particularly, generative adversarial networks (GANs) have motivated extensive works in sketch synthesis, involving doodle-sketch generation (Ge et al., 2020), image-to-sketch translation (Liu et al., 2020), colored sketch rendering(Rathod et al., 2021), pencil-shading sketch generation (Li et al., 2020b), and face sketch synthesis (Wang et al., 2020). However, those models are all pixel-based generation, which is fundamentally different from how humans sketch objects using pens or brushes. Towards modeling sketches like humans, sketches are preferred to be treated as sequential pen actions, and RNN-based variational autoencoder (VAE) (Ha & Eck, 2018; Zhang et al., 2017; Graves, 2013), reinforcement learning (RL) (Xie et al., 2013; Zheng et al., 2018; Ganin et al., 2018), Transformed-based sketch representation (Ribeiro et al., 2020; Lin et al., 2020), learning parametric B ́ezier curve (Das et al., 2020; 2021b), and neural ODE (Das et al., 2021a) are explored for sketch generation. (Aksan et al., 2020) proposes a relational model built on auto-encoder, which can decompose sketch formed by a single temporal sequence into a group of disordered strokes. Particularly, promising generative results on complex structures have been witnessed. Other notable works include generating stylized line drawing from 3D shapes (Liu et al., 2021), and intent communication through sketching by referential communication game (Mihai & Hare, 2021).\n\nDiffusion Models Recently, approaches with diffusion models have delivered impressive results on several generative tasks, including image generation (Ho et al., 2020; Dhariwal & Nichol, 2021), shape generation (Cai et al., 2020), 3D shape modelling (Luo & Hu, 2021), audio synthesis (Kong et al., 2020), and cross-domain generation (Popov et al., 2021; Nichol et al., 2021). Different from likelihood-based models (variational auto-encoder (VAEs) (Kingma & Welling, 2013), normalizing flow models (Dinh et al., 2014; Papamakarios et al., 2021), energy-based models (EBMs) (LeCun et al., 2006)) and implicit generative models (GANs (Goodfellow et al., 2014)), diffusion models can be categorized into score-based generative modeling (SGM) (Song et al., 2020), which aims to model the gradient of the log probability density function by score matching (Hyv ̈arinen & Dayan, 2005). There are two popular sub-classes in SGMs, i.e., score matching with Langevin dynamics (SMLD) (Song & Ermon, 2019) and denoising diffusion probabilistic models (DDPM) (SohlDickstein et al., 2015; Ho et al., 2020). Despite attractive, rare work with diffusion models targets at handling sketches. The most relevant work to ours is Diff-HW (Luhman & Luhman, 2020) which also applies diffusion models for sequential data generation, i.e., handwriting. However, Diff-HW adopts the vanilla DDPMs and focuses on text-to-sketch translation. On contrast, we offer (model built-in) quality quantifiable diffusion models, improved sampling strategy and gradients guided conditional sampling based on DDIM.\n\n3 DIFFUSION MODELS FOR VECTORIZED SKETCH GENERATION\n\nOur objective is to generate sketch stroke sequences from noise by a novel method built on denoising diffusion implicit models (DDIMs). DDIMs generalize DDPMs by introducing a non-Markovian diffusion process, which yet still has the same forward marginals as DDPMs. Uniquely, we propose a method to find a shortcut sampling trajectory based on recognizablity of a generated sketch. Moreover, we will present how to rectify a flawed sketch by our trained unconditional sketch generation model on-the-fly during the generative process.\n\n3.1 PROBLEM SETUP\n\nWe construct a sketch in a sequence as s0 = {s1, s2, . . . , sN } using the representation from Ha & Eck (2018), i.e. stroke-3. Each point si is represented as a 3-D vector (∆xi, ∆yi, gi), where (∆xi, ∆yi) indicates the offsets at point i during the pen’s moving trajectory, and gi is a binary pen state, denoting whether the pen is touching the paper or not. Our goal is to learn the probability distribution of the offsets {(∆xi, ∆yi)} from the training data by diffusion models. Then, a sketch can be drawn given the estimated (∆xi, ∆yi) for each point and the corresponding pen state pi inferred by a pen-state network. We will describe more details in the following sections.\n\n3.2 SKETCH DIFFUSION IN FORWARD PROCESS\n\nDuring the forward process, the noise will be gradually added to each point offsets of an original sketch, resulting in increased stroke distortion over time. Consider we have N ordered stroke points\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nfor a given sketch s0 = {s1, s2, . . . , sN }, the offset (∆xi, ∆yi) of each point si is supposed to be sampled independently from a distribution q(s0). To diffuse s0 into s1, . . . , sT , the Markov diffusion process introduced in DDPMs is applied here. Formally, the forward process enforces each of the point offsets (∆xi, ∆yi) in s0 to drift along both x and y coordinates by gradually adding noises sampled from Gaussain distributions with pre-defined schedules α1, . . . , αT . Therefore, the Markov chain in forward process is defined by:\n\nq(s1:T |s0) :=\n\nT (cid:89)\n\nt=1\n\nq(st|st−1),\n\nq(st|st−1) := N\n\nst;\n\n(cid:18)\n\n(cid:114) αt αt−1\n\n(cid:18)\n\nst−1,\n\n1 −\n\n(cid:19)\n\n(cid:19)\n\nI\n\n.\n\nαt αt−1\n\n(1)\n\n3.3 DDIM BASED GENERATIVE PROCESS\n\nFollowing (Song et al., 2021), DDIM-based sampling is adopted to generate sketches from noise, i.e., the reverse process, as it normally achieves high data quality with significantly small sampling steps. More importantly, their non-Markovian sampling process supports us in discovering a novel shortcut sampling function being more efficient and effective for our sketch generation. Formally, the generative process is defined as follows:\n\nqσ(st−1|st, s0) = N\n\n(cid:18)√\n\nαt−1s0 +\n\n(cid:113)\n\n1 − αt−1 − σ2\n\nt ·\n\nWith DDIM we can predict s0 through the generative process\n\nst − √\n\n√\n\nαts0\n\n1 − αt\n\n(cid:19)\n\n.\n\n, σ2\n\nt I\n\n(2)\n\n(3)\n\np(t) θ (st−1|st) =\n\n(cid:40)\n\nN (f (1) (s1), σ2 qσ(st−1|st, f (t)\n\nθ\n\nif t=1\n\n1I) θ (st)), otherwise\n\nwhere f (t) mator ε(t) repeating the following equation\n\nθ (st) = (st − αt is a prediction of s0 based on our noise approxiθ . Then we can sample a data sample s0 from a random noise sT = N (0, I) by iteratively\n\nθ (st))/\n\n√\n\n1 − αt · ε(t)\n\n√\n\nst−1 =\n\n√\n\nαt−1\n\n(cid:32)\n\nst −\n\n√\n\n1 − αtε(t) √\nαt\n\nθ (st)\n\n(cid:33)\n\n(cid:113)\n\n+\n\n1 − αt−1 − σ2\n\nt · ε(t)\n\nθ (st).\n\n(4)\n\nMore detailed derivations about DDPMs and how DDIMs generalize DDPMs by defining a nonMarkovian forward process and obtain the corresponding generative process described in Eq. 3 can be found in Appendix A.1 and Appendix A.2.\n\n3.4 NOISE APPROXIMATOR ε(t)\n\nθ\n\nIn our case the noise approximator ε(t) θ (st) in Eq (4) is a trainable network to estimate noise ε(t) ∈ RN ×2 as coordinate offsets for st ∈ RN ×2 at timestep t. An improved U-Net is developed to handle the sequence of the points from a vectorized sketch. Namely, we add a trainable embedding and a decoding layer into U-Net to transform the input st into an embedding et ∈ RN ×128 and convert the penultimate feature embedding back to coordinates. Then the rest design is as per the conventional U-Net for dealing with 2D images. Please refer to Appendix A.3 for details.\n\n3.5 RECOGNIZABILITY BASED SHORTCUT SAMPLING\n\nAs shown in Figure 1 (c), we observe that at the early sampling steps, the recognizability of generated st is consistently low, although the amount of the estimated noise is large if we take full reverse steps of length T used in DDPMs. In contrast, denoising gets much more efficient and effective, leading to a noticeable leap of recognizability at the later steps. We then intuitively suppose that the recognizability of a generated sample ought to be inferred from the pattern of denoising sequence for all stroke points, i.e., ε(t) θ ∈ RN ×2. Therefore, we incorporate an additional trainable network to predict the recognizability rt given the estimated noise:\n\nˆrt = hφ(ε(t)\n\nθ , t).\n\n4\n\n(5)\n\nPublished as a conference paper at ICLR 2023\n\nAnd the prediction ˆrt could be used as a signal to find a shortcut sampling path – we can skip m sampling steps if ˆrt < ζ, which implies the period of ineffective sampling. m is a constant indicating the interval steps to skip, and ζ is a predefined threshold. Therefore the timestep sampling function can be formulated as\n\ntn =\n\n(cid:40)\n\ntc − m, tc − 1,\n\nif hφ(ε(tc) otherwise,\n\nθ\n\n, tc) < ζ,\n\n(6)\n\nwhere tc is the current timestep and tn is the next sampled timestep. Different from the Linear or Quadratic sub-sequence selection proposed in DDIM Song et al. (2021), our sub-sequence is chosen adaptively, which can be more effective and shorter.\n\nIn practice, hφ(·) is implemented as a bi-directional RNN as (Ha & Eck, 2018) with the estimated sequence of noise as input. Then the output latent vector z ∈ Rd is fed into a trainable linear layer to predict ˆrt. And the ground truth rt could be obtained from an extra pre-trained sketch classifier as done in (Song et al., 2018). Namely, we use the maximum probability of the softmax prediction as rt. Once trained, hφ(·) could be accommodatingly used to inspect how recognizable a sketch is during sampling, while no need to render it into an image. Please refer to Appendix A.4 for more analysis and insights about the shortcut sampling.\n\n3.6 PEN STATE ESTIMATION\n\nThe diffusion model is to learn the distribution of the coordinate offsets for sketch points. However, it is also required to predict binary pen state for each of the stroke point. Following (Luhman & Luhman, 2020), the feature vector, i.e., v ∈ Rn×128, from the penultimate layer of the above U-Net for noise approximation is utilized. We feed this feature sequence into another trainable linear layer followed by a sigmoid function to predict each point’s pen status:\n\nwhere ψ are parameters of the trainable linear layer. When ˆgi > 0.5, it indicates the pen is touching the canvas at point i. We do this for each timestep in the generative process.\n\nˆg = sigmoid(fψ(v)),\n\n(7)\n\n3.7 TRAINING OBJECTIVE OF ε(t)\n\nθ\n\nTo train our noise approximator ε(t) θ , we follow Luhman & Luhman (2020) to jointly minimize the denoising loss and pen state loss. Specifically, we train ε(t) following DDIMs Song et al. (2021) to minimize the L2 discrepancy between the estimated noise ε(t) and εt generated in the forward process:\n\nθ\n\nAdditionally, at each timestep t, ε(t)\n\nθ\n\nis also optimized by minimizing the pen state loss\n\nLd(θ) = E||ε(t) − ε(t)\n\nθ (st)||2 2.\n\nLp(θ) =\n\n1 N\n\nN (cid:88)\n\n[−gi log(ˆgi) − (1 − gi) log(1 − ˆgi)],\n\ni=1\n\nwhere ˆg is predicted using Eq. 7. In summary, the total training loss is:\n\nwhere γ is a weight to balance two losses.\n\nL(θ) = Ld(θ) + γLp(θ),\n\n3.8 CONDITIONAL GENERATION TO RECTIFY BAD SKETCHES\n\n(8)\n\n(9)\n\n(10)\n\nThe above has described the details of the unconditional sketch generation. We now further present how to rectify a flawed sketch using our model trained with unconditional generation only. Inspired by the classifier guidance widely used in diffusion models (Dhariwal & Nichol, 2021; Ho & Salimans, 2022), a sketch generation guidance is introduced for the generative process. Different from existing works using gradients of an image classifier as guidance, a perceptual metric is employed to\n\n5\n\nPublished as a conference paper at ICLR 2023\n\ngenerate guidance – the gradients of the log probability of perceptual distance between a generated st and a condition sc, denoted as ∇st log p(st, sc). Then the estimated noise is refined as\n\nˆε(t)(st) = ε(t)\n\nθ (st) + η∇st log p(st, sc),\n\n(11)\n\nwhere η is to control the strength of the guidance. Then the sample ˆst−1 will be generated using Eq 4 with the new noise ˆε(t)(st). Intuitively, we aim to guide the generation of s0 by repeatedly sampling st, which has similar content as sc. We follow prior work (Zhang et al., 2018) based on the L2 distance between two image features to measure their perceptual similarity, as it is effective in transferring image content. Neural line rendering (NLR) (Li et al., 2020a) is used to convert a vector sketch into an image sketch to allow the gradients being propagated to the vectorized sketch. After rasterizing sketch into its image version, a perceptual metric is thus applied as\n\np(st, sc) =\n\nL (cid:88)\n\nl=1\n\n1 HlWl\n\n(cid:88)\n\nh,w\n\n||wl ⊙ (F l\n\nhw\n\n(cid:0)st) − F l\n\nhw(sc)(cid:1) ||2 2,\n\n(12)\n\nhw(·) ∈ RHl×Wl×Cl denotes the feature maps for l-th layer of an ImageNet pre-trained where F l VGG Simonyan & Zisserman (2014), wl ∈ RCl is adopted to scale the feature activations channelwise as per (Zhang et al., 2018). Details about NLR are provided in Appendix A.5.\n\n4 EXPERIMENTS\n\nIn this section, we evaluate our model in two modes, i.e., unconditional and conditional generation, to verify the quality of the generated data and the ability to mend inferior sketches of our model. Please refer to Appendix A.6 for implementation details.\n\n4.1 UNCONDITIONAL GENERATION\n\nDataset. We evaluate our proposed method on QuickDraw Ha & Eck (2018), which contains over 50M sketches in vector format across 345 common categories. A subset2 from QuickDraw is collected for the experiments. Specifically, there are 10 classes chosen adhered to the following principles: (i) Complexity - simple, moderate and complex sketch drawings are all included, e.g., fish, umbrella and lion; (ii) Diversity - objects with diverse sub-category variations are involved, e.g., bus and spider; (iii) Ambiguity - sketches belong different classes share highly similar appearance, e.g., apple and moon. The original data split is adopted, i.e., 70,000 training and 2,500 testing sketches for each class.\n\nCompetitors. Three current RNN-based state-of-the-art methods, i.e., SketchRNN Ha & Eck (2018), SketchPix2seq Chen et al. (2017) and SketchHealer Su et al. (2020), are included. Diff-HW Luhman & Luhman (2020), which is built on DDPMs, is enabled for comparison by tweaking the cross-modal attention layers 3 to be self-attention since only sketches are available in our problem. Additionally, the unconditional mode of SketchODE (Das et al., 2021a) is also used for comparison.\n\nEvaluation metrics. We gauge the quality of the generated data resorting to the evaluation metrics for image generation once the vector sketches are rasterized into images. Specifically, Fr ́echet Inception Distance (FID) Heusel et al. (2017) measures the distance between the generated (image) data and real ones by comparing the mean and variance of image features, which are obtained from Inception-V3 Szegedy et al. (2016) trained on ImageNet Krizhevsky et al. (2012) for image classification. The Geometry Score (GS) Khrulkov & Oseledets (2018) metric compares the geometrical properties of data manifold between the generated and real data. In addition, the improved precision and recall (Kynk ̈a ̈anniemi et al., 2019) are used as complementary evaluation metrics following other image generation works (Nichol & Dhariwal, 2021).\n\nQualitative results. Figure 2(a) shows some examples of reverse-time diffusion process, i.e., from random noise till reach the data sample, the generated sketch at each step exhibits different (reduced) level of distortion. More results of unconditional sketch generation are demonstrated in Figure 2(b).\n\n2Class list: moon, airplane, fish, umbrella, train, spider, shoe, apple, lion, bus. 3Diff-HW is originally proposed for stylized text-to-handwriting generation, requiring text and an image as\n\ncondtions to control the content and style of the generated handwriting, respectively.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: (a) Sketch generated from random noise. (b) More examples of unconditional generation.\n\nTable 1: Quantitative comparison results. Testing categories are orgnaized in three folds according to the complexity, i.e., the average number of stroke points (ASP). Simple: < 40 ASP, Moderate: 40 ∼ 100 ASP and Complex: > 100 ASP. Speed: second per sketch sampling. Simple\n\nModel\n\nComplex\n\nModerate FID↓ GS↓ Prec↑ Rec↑ FID↓ GS↓ Prec↑ Rec↑ FID↓ GS↓ Prec↑ Rec↑ 0.72 13.3 SketchPix2seq 0.63 10.3 SketchHealer 0.72 10.8 SketchRNN 0.64 13.3 Diff-HW 0.58 11.5 SketchODE 0.85 6.9 Ours (full 1000 steps) Ours (r-Shortcut, S=30) 0.81 7.4 0.72 Ours (Linear-DDIMs, S=30) 11.9 0.75 Ours (Quadratic-DDIMs, S=30) 12.3 0.39 Ours (Abs) 0.61 Ours (Point-Shuffle) 0.62 Ours (Stroke-Shuffle)\n\n0.75 18.0 73.3 0.36 0.79 25.9 93.2 0.29 0.77 21.4 97.6 0.35 0.76 18.3 64.4 0.23 0.66 33.5 68.1 0.20 0.42 0.87 9.4 0.39 0.85 10.5 0.78 15.1 0.33 0.34 0.76 15.4 0.48 29.4 98.9 0.10 0.20 0.65 12.4 0.25 0.66 10.3\n\n0.79 16.4 49.7 0.38 0.81 12.9 0.39 0.82 13.0 11.0 0.42 0.81 15.9 23.4 0.37 0.74 18.8 29.6 0.31 0.45 8.4 0.88 0.44 0.87 8.9 0.81 13.3 0.36 0.35 0.79 13.8 0.55 23.4 64.6 0.13 0.31 0.72 11.3 0.34 9.6 0.74\n\n7.0 0.40 5.9 0.45 5.4 0.44 6.8 0.42 9.4 0.48 3.4 0.52 3.9 0.47 6.4 0.38 0.41 6.6 20.7 12.1 0.18 0.35 5.3 9.5 0.36 3.8 8.2\n\n4.7 5.2 8.8 8.7\n\n5.2 6.1 9.6 9.9\n\n7.5 7.4\n\n8.1 7.6\n\n9.8\n\nSpeed↓\n\n0.04 0.03 0.03 0.19 0.03 1.29 0.08 0.08 0.09 0.20 0.18 0.18\n\nQuantitative results. As shown in Table 1, our sketch generator clearly outperforms other competitors, suggesting better quality of the generated data. Particularly, our model is able to maintain relative stable FID, GS, Precision and Recall scores regardless of the complexity of the generated sketches. On the contrary, obvious performance decline is witnessed when constructing sketches with more complicated structures for other baseline methods.\n\nSampling trajectory . We compare different approaches for choosing sampling trajectory, including full reverse steps, linear and quadratic in DDIMs and ours using recognizability-based skip function. Results in Table 1 show that our recognizability-based skipping (r-Shortcut) achieves the best results when performing the same total sampling steps, i.e., S = 30, suggesting the superiority of our sampling strategy. In addition, compressing sampling steps from 1000 to 30 using our method deteriorates the data quality very minor, while 16x faster speed is reached.\n\nEffectiveness of hφ(·). We further conduct experiments to testify if the learned network hφ(·) in Eq.5 can faithfully reflect the recognizability of sketches generated during sampling. Specifically, we compare how well the predicted recognizability can match the probability of assigning the correct class label given by the pre-trained classifier. This experiments are conducted using 10 single class models, thus we can know which class probability in the classifier to be compared. Results in Table 2 show that low and stable error can be achieved along the sampling steps.\n\nImpact of N . To study the impact of point number N , we train our model with different settings. We can see from Table 3 that it is inferior to using too fewer points (N = 24) for modeling relative complex structures, while applying too much points (N = 384) is also sub-optimal. An unique and optimal N is hard to reach as the complexity of sketch structure varies case by case.\n\nAbsolute coordinates works? To gain more insights about the importance of modeling relative coordinates by our model, i.e., (∆x, ∆y), we train a model to learn from sketches represented by stroke points in absolute coordinates, i.e., (x, y), with other settings/components unchanged, denoted as “Ours (Abs)” in Table 1. Significant decrease on performances is observed, verifying the crucial role of training model with relative coordinates.\n\nWhat is learned? We suspect that capturing the implicit drawing structure is the key to success. A simple way to verify the speculation could be destroying the drawing structure. Specifically, we reorganize the original sketch data into a disordered version by randomly shuffling sketch segments\n\n7\n\n(b) (a) Published as a conference paper at ICLR 2023\n\nTable 2: Averaged error (10k samples for each class) of the predicted recognizability using hφ(·).\n\nt\n\n1000 Error 0.0608 0.0619 0.0723 0.0877 0.1006 0.0928 0.0935 0.0876 0.0891 0.0819 0.0908 0.0972 0.1027\n\n100\n\n200\n\n300\n\n800\n\n400\n\n700\n\n600\n\n500\n\n30\n\n80\n\n10\n\n50\n\nTable 3: Impact of points number n (full reverse steps are performed).\n\nSimple\n\nModerate FID GS Prec Rec FID GS Prec Rec FID GS Prec Rec 7.4 4.4 0.32 0.74 14.7 13.5 0.30 0.76 16.3 18.2 0.26 0.69 0.16 N=24 N=192 6.9 3.4 0.33 0.76 9.8 5.3 0.31 0.74 10.3 6.3 0.27 0.71 0.18 N=384 8.5 5.0 0.30 0.72 10.6 8.1 0.28 0.69 11.9 12.6 0.23 0.67 0.22\n\nComplex\n\nSpeed\n\nT=1000\n\n(each sketch segment is formed by connecting two neighbor stroke points). Intuitively, the newly constructed sequential data discards the implicit interdependent relations among stroke points, as the corresponding segments are geometrically far from each other. Then we can obtain a variant model trained with such structure-broken data, denoted by “Ours (Point-Shuffle)” in Table 1. We can see that the performance is clearly harmed compared to the model trained with normal data. Furthermore, we also experiment with a model variant trained with a stroke-level shuffle tactic, which means that the structural cues at stroke-level are preserved as each stroke remain complete, however the stroke orders are changed given the random shuffle applied. Less declines are observed (“Ours (Stroke-Shuffle)” in Table 1), revealing that the middle-level structural cues (i.e., complete strokes) are of great importance to our model. Note full steps are taken for these two model variants.\n\n4.2 CONDITIONAL GENERATION\n\nExperiments on sketch refinement and sketch healing are conducted to verify the effectiveness of the conditional sampling by our model. We show that, given any conditional sketch with defeats due to either stroke distortion or corruption, the generated sketch could be a refined version accordingly.\n\nDataset. We utilize the same data in sec 4.1 for the sketch refinement task. To synthesis sketches with different degrees of distortion, random Gaussian noise e ∼ N (0, I) is added to each location of stroke points. The deformation degrees could be easily controlled by adding noise using different t timesteps. Because a real sketch can be transformed into a random noise after T diffusion steps, then a new sample obtained from the intermediate step t would be considered as x% deformed if t = x%T . The sketch healing task aims to create new sketches which should resemble the given the partial sketches. For fair comparison, we follow the same experimental setups in SketchHealer (Su et al., 2020). 17 categories from QuickDraw and the same data splits (70,000 for training and another 2,500 novel ones for testing per class) are adopted. Sketches are damaged using two different mask ratios: pmask = 10% or 30%, that is, 10% or 30% key stroke points will be randomly removed to form the corrupted versions from a complete sketch.\n\nSketch classifier. A multi-category classifier built on AlexNet is pre-trained on the training set of all 345 QuickDraw categories. Intuitively, better recognizability of sketches is manifested by higher recognition accuracy offered by this pre-trained classifier. Following the common practice in Song et al. (2018); Su et al. (2020), acc@1 and acc@10 are adopted as evaluation metrics. Specifically, acc@K denotes the accuracy of the true positive is ranked within the top K predictions.\n\nSBIR model. Similarly, a sketch-based image retrieval (SBIR) model is pre-trained to verify if the rectified sketch generated by our model could enable better retrieval performances. Specifically, Triplet-SAN Yu et al. (2016), which is constructed by employing Sketch-A-Net Yu et al. (2017) as backbone network, is trained on the QuickDraw Extended dataset Dey et al. (2019) under the supervision of triplet loss. Built on QuickDraw, QuickDraw Extended dataset is the largest SBIR dataset which contains 330,000 sketches paired with 204,000 photos over 110 categories. In our case, the SBIR model is trained by using the training set of the same selected 10 categories in the section 4.1. The remaining testing set of the 10 classes is utilized as samples to be deformed. Evaluation metric acc@1 and acc@10 measure if a target image could be ranked within the top 1/10 retrieval results. The mean average precision (mAP) is adopted for evaluation as well.\n\nResults. As shown in Table 4, we can observe that (i) for all cases involving noisy sketches (i.e., deformation levels from 10% ∼ 50%), performance improvements on recognition and retrieval can be achieved by using sketches after rectification, revealing the obvious enhancement benefits gained from our model; (ii) The recognition and retrieval results are stable regardless of the deformation level tackled, e.g., recognition accuracy acc@1 is kept around 47% ∼ 50% for sketches\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Qualitative comparison on sketch refinement (a)(b) and healing (c)(d). (a) Normal degree of complexity. (b) Complex classes. (c) Corruption pmask = 10%. (d) Corruption pmask = 30%.\n\nTable 4: Recognition and retrieval results before and after (separated by “|”) sketch rectification by our model at different deformation levels (DL). Performance gain in red. “–”: without injecting noise, i.e., real data.\n\nDL\n\nRecognition\n\nRetrieval\n\nacc@1(%)\n\nacc@10(%)\n\nmAP 51.9 | 52.4 (+0.90) 87.7 | 90.2 (+2.50) 0.704 | 0.789 (+0.045) 67.4 | 73.3 (+5.90) 91.3 | 96.2 (+4.90) –\n10% 45.7 | 48.9 (+3.20) 82.3 | 82.4 (+0.10) 0.724 | 0.788 (+0.064) 66.9 | 73.1 (+6.20) 92.1 | 96.8 (+4.70) 20% 33.0 | 47.3 (+14.3) 68.2 | 81.9 (+13.7) 0.607 | 0.772 (+0.165) 55.8 | 72.8 (+17.0) 81.8 | 94.7 (+12.9) 30% 20.6 | 48.2 (+27.6) 51.5 | 81.9 (+30.4) 0.496 | 0.787 (+0.291) 46.9 | 72.8 (+25.9) 68.9 | 95.0 (+26.1) 50% 7.29 | 50.1 (+42.8) 27.1 | 84.3 (+57.2) 0.328 | 0.786 (+0.458) 28.6 | 74.9 (+46.3) 47.8 | 96.3 (+48.5)\n\nacc@10(%)\n\nacc@1(%)\n\nwith deformation 10% ∼ 50%. (iii) Interestingly, recognition and SBIR results could be further significantly increased on the original human-drawn sketches after refinement. Exemplar refined sketches from the original ones are demonstrate in Figure 3 (a), revealing that badly drawn sketches (i.e., missing part, random strokes and line distortions) can be largely rectified by our model, while other competitors either not able to faithfully resemble the conditions or unable to model complex structures. Quantitative healing results are shown in Table 5. We can see that our generator outperforms all the other competitors on sketch recognition in most cases (expect the top 10 result when pmask = 10%), indicating the superiority of our model on recovering incomplete sketches. The advantage of our method is further enlarged when the corruption level increases, i.e., pmask = 30%. Similar situation can be observed for human study results. The healed sketches given by our model are mostly preferred compared against with other baseline methods. Qualitative comparisons against Sketch-RNN, SketchODE and B ́ezierSketch are also provided in Figure 3 (b).\n\nTable 5: Comparison results on sketch healing. Recognition results are obtained by classifying generated healed sketches with a pre-trained multi-category sketch classifier. “Human” denotes human’s preference of choice among the synthetic outputs by different competitors. SketchRNN SketchPix2seq SketchHealer Ours\n\nMetric\n\npmask\n\nRecognition\n\n10%\n\nHuman\n\nRecognition\n\n30%\n\nHuman\n\nTop 1 Top 5 Top 10 N/A Top 1 Top 5 Top 10 N/A\n\n24.41% 46.23% 56.28% 21.11% 3.14% 10.25% 15.91% 6.14%\n\n21.88% 31.92% 36.91% 10.82% 9.51% 16.06% 20.26% 6.98%\n\n56.61% 49.77% 71.63% 69.92% 80.01% 79.79% 34.51% 33.56% 55.88% 41.59% 74.57% 62.76% 80.70% 68.12% 56.12% 30.76%\n\n5 CONCLUSION\n\nWe show for the first time sketch generation can be formulated as a process of deformation-based denoising. The key finding is that increased sketch deformation degrees can be monotonically synthesized by diffusing stroke points with Gaussian noise, and the demanded probabilistic distribution of the stroke points of sketch objects can thus be effectively learned by diffusion inversion. Importantly, the ability of quantifying recognizability of the generated sketch was injected during the sampling. For that, a RNN was developed to predict the recognizability of a sampled sketch based on the estimated noise at each timestep. As a result, more efficient sampling can be enabled by a recognizability-based skip function. Additionally, our model trained for unconditional generation could be readily extended for conditional generation by incorporating a perceptual similarity based gradients into the sampling. Extensive experiments validated the effectiveness of our model. To manage the abstraction level of the generated sketch would be a potential future work.\n\n9\n\nOursSketch-RNNConditionSketchODEBezierSketch(a) (d)(b)(c)Published as a conference paper at ICLR 2023\n\nREFERENCES\n\nEmre Aksan, Thomas Deselaers, Andrea Tagliasacchi, and Otmar Hilliges. Cose: Compositional\n\nstroke embeddings. 2020.\n\nRuojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and\n\nBharath Hariharan. Learning gradient fields for shape generation. In ECCV, 2020.\n\nHong Chen, Ying-Qing Xu, Heung-Yeung Shum, Song-Chun Zhu, and Nan-Ning Zheng. Example-\n\nbased facial sketch generation with non-parametric sampling. In ICCV, 2001.\n\nYajing Chen, Shikui Tu, Yuqi Yi, and Lei Xu. Sketch-pix2seq: a model to generate sketches of\n\nmultiple categories. In arXiv preprint arXiv:1709.04121, 2017.\n\nAyan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, and Yi-Zhe Song. B ́eziersketch: A\n\ngenerative model for scalable vector sketches. In ECCV, 2020.\n\nAyan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, and Yi-Zhe Song. Sketchode: Learning\n\nneural sketch representation in continuous time. In ICLR, 2021a.\n\nAyan Das, Yongxin Yang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. Cloud2curve:\n\nGeneration and vectorization of parametric sketches. In CVPR, 2021b.\n\nSounak Dey, Pau Riba, Anjan Dutta, Josep Llados, and Yi-Zhe Song. Doodle to search: Practical\n\nzero-shot sketch-based image retrieval. In CVPR, 2019.\n\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint\n\narXiv:2105.05233, 2021.\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-\n\nmation. arXiv preprint arXiv:1410.8516, 2014.\n\nYaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Ali Eslami, and Oriol Vinyals. Synthesizing\n\nprograms for images using reinforced adversarial learning. In ICML, 2018.\n\nSongwei Ge, Vedanuj Goswami, Larry Zitnick, and Devi Parikh. Creative sketch generation. In\n\nICLR, 2020.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n\nAlex Graves.\n\nGenerating sequences with recurrent neural networks.\n\narXiv preprint\n\narXiv:1308.0850, 2013.\n\nCheng-en Guo, Song-Chun Zhu, and Ying Nian Wu. Primal sketch: Integrating structure and texture.\n\nCVIU, 2007.\n\nDavid Ha and Douglas Eck. A neural representation of sketch drawings. In ICLR, 2018.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017.\n\nJonathan Ho and Tim Salimans.\n\nClassifier-free diffusion guidance.\n\narXiv preprint\n\narXiv:2207.12598, 2022.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,\n\n2020.\n\nAapo Hyv ̈arinen and Peter Dayan. Estimation of non-normalized statistical models by score match-\n\ning. Journal of Machine Learning Research, 2005.\n\nValentin Khrulkov and Ivan Oseledets. Geometry score: A method for comparing generative adver-\n\nsarial networks. In ICML, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\n\ndiffusion model for audio synthesis. In ICLR, 2020.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. In NeurIPS, 2012.\n\nTuomas Kynk ̈a ̈anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila.\n\nImproved\n\nprecision and recall metric for assessing generative models. In NeurIPS, 2019.\n\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based\n\nlearning. Predicting structured data, 2006.\n\nLei Li, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, and Chiew-Lan Tai. Sketch-r2cnn:\n\nAn rnn-rasterization-cnn architecture for vector sketch recognition. IEEE TVCG, 2020a.\n\nMengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and Deva Ramanan. Photo-sketching: Inferring\n\ncontour drawings from images. In WACV, 2019.\n\nSuChang Li, Kan Li, Ilyes Kacher, Yuichiro Taira, Bungo Yanatori, and Imari Sato. Artpdgan: Creating artistic pencil drawing with key map using generative adversarial networks. In ICCS, 2020b.\n\nHangyu Lin, Yanwei Fu, Xiangyang Xue, and Yu-Gang Jiang. Sketch-bert: Learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt. In CVPR, 2020.\n\nDifan Liu, Matthew Fisher, Aaron Hertzmann, and Evangelos Kalogerakis. Neural strokes: Stylized\n\nline drawing of 3d shapes. In ICCV, 2021.\n\nRuntao Liu, Qian Yu, and Stella X Yu. Unsupervised sketch to photo synthesis. In ECCV, 2020.\n\nTroy Luhman and Eric Luhman. Diffusion models for handwriting generation, 2020.\n\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR,\n\n2021.\n\nDaniela Mihai and Jonathon Hare. Learning to draw: Emergent communication through sketching.\n\nIn NeurIPS, 2021.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\n\nIn ICML, 2021.\n\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 2021.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\n\nnetworks. In ICML, 2013.\n\nVadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts:\n\nA diffusion probabilistic model for text-to-speech. arXiv preprint arXiv:2105.06337, 2021.\n\nHarsh Rathod, Manisimha Varma, Parna Chowdhury, Sameer Saxena, V Manushree, Ankita Ghosh, and Sahil Khose. Xci-sketch: Extraction of color information from images for generation of colored outlines and sketches. arXiv preprint arXiv:2108.11554, 2021.\n\nLeo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and Moacir Ponti.\n\nSketchformer:\n\nTransformer-based representation for sketched structure. In CVPR, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\n\nlearning using nonequilibrium thermodynamics. In ICML, 2015.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR,\n\n2021.\n\nJifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, and Timothy M Hospedales. Learning to sketch\n\nwith shortcut cycle consistency. In CVPR, 2018.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\n\nIn NeurIPS, 2019.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020.\n\nGuoyao Su, Yonggang Qi, Kaiyue Pang, Jie Yang, and Yi-Zhe Song. Sketchhealer: A graph-to-\n\nsequence network for recreating partial human sketches. In BMVC, 2020.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\n\nthe inception architecture for computer vision. In CVPR, 2016.\n\nTianying Wang, Wei Qi Toh, Hao Zhang, Xiuchao Sui, Shaohua Li, Yong Liu, and Wei Jing. Robocodraw: Robotic avatar drawing with gan-based style transfer and time-efficient path optimization. In AAAI, 2020.\n\nNing Xie, Hirotaka Hachiya, and Masashi Sugiyama. Artist agent: A reinforcement learning approach to automatic stroke generation in oriental ink painting. IEICE TRANSACTIONS on Information and Systems, 2013.\n\nQian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, and Chen-Change Loy. Sketch\n\nme that shoe. In CVPR, 2016.\n\nQian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang, and Timothy M Hospedales. Sketch-\n\na-net: A deep neural network that beats humans. IJCV, 2017.\n\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\n\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\n\nXu-Yao Zhang, Fei Yin, Yan-Ming Zhang, Cheng-Lin Liu, and Yoshua Bengio. Drawing and rec-\n\nognizing chinese characters with recurrent neural network. TPAMI, 2017.\n\nNingyuan Zheng, Yifan Jiang, and Dingjiang Huang. Strokenet: A neural painting environment. In\n\nICLR, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 DENOISING DIFFUSION PROBABILISTIC MODELS (DDPM)\n\nTo learn the probability distribution over data x, diffusion models corrupt training data by slowly injecting noise and learn to reverse the corruption, such that the obtained models can gradually transform random noise into sample for data generation.\n\nForward process. Formally, for each training data x0 ∼ qdata(x0), a discrete Markov chain x0, x1, . . . , xT is formed by the forward process (also known as diffusion process). This process is defined as a Markov chain which slowly adds Gaussian noise to the data according to a variance schedule β1, . . . , βT :\n\nq(x1:T |x0) :=\n\nT (cid:89)\n\nt=1\n\nq(xt|xt−1)\n\n(13)\n\nq(xt|xt−1) := N (xt; (cid:112)1 − βtxt−1, βtI) (14) If we know q(xt−1|xt), we could sample data from the data distribution q(x0) by first sampling xT from q(xT ) (isotropic Gaussian) and then sampling from q(xt−1|xt) until we get x0. However, it is difficult to estimate q(xt−1|xt) since it needs entire dataset to do so. Therefore, pθ is proposed to approximate the conditional probabilities q(xt−1|xt) during the backward process.\n\nBackward process. In the backward/reverse process, diffusion models have to denoise the perturbed data (starting at random noise p(xT ) = N (xT ; 0, I)) back to the origin data x0. Mathematically, diffusion models is defined as\n\n(cid:90)\n\npθ(x0) :=\n\npθ(x0:T )dx1:T\n\nin which the joint distribution pθ(x0:T ) defines the reverse process:\n\npθ(x0:T ) := p(xT )\n\nT (cid:89)\n\nt=1\n\npθ(xt−1|xt)\n\nThe training objective is to optimize variational bound on negative log likelihood:\n\npθ(xt−1|xt) := N (xt−1; μθ(xt, t), Σθ(xt, t))\n\nE[− log pθ(x0)] ≤ Eq[− log\n\npθ(x0:T ) q(x1:T |x0)\n\n]\n\n= Eq[− log p(xT ) −\n\n(cid:88)\n\nt≥1\n\nlog\n\npθ(xt−1|xt) q(xt|xt−1)\n\n]\n\nwhich is equivalent to optimize the following variational lower-bound Lvlb:\n\nLvlb := L0 + L1 + · · · + LT −1 + LT\n\nL0 := − log pθ(x0|x1)\n\nLt−1 := DKL(q(xt−1|xt, x0)||pθ(xt−1|xt))\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nLT := DKL(q(xT |x0)||p(xT )) Essentially, the above KL terms compare two Gaussian distributions which can be addressed in The training objective for Eq (17) is to get μθ(xt, t), while not closed form Ho et al. (2020).\n\n(22)\n\n13\n\nPublished as a conference paper at ICLR 2023\n\ninvolve Σθ(xt, t), as it is set to time-dependent constants σ2 t=1I. Furthermore, instead of predicting μθ(xt, t) (forward process posterior mean) by a neural network, Ho et al. (2020) proposed to utilize an approximator εθ(xt, t) to predict noise ε from xt, which is proven to be more effective than optimizing μθ(xt, t). The simplified training objective is:\n\nLsimple(θ) := Et∼[1,T ],x0∼q(x0),ε∼N (0,I)[||ε − εθ(xt, t)||2]\n\n(23)\n\nData sampling. Once trained, we have a neural network to estimate noise ε from sample xt at timestep t, i.e., εθ(xt, t). Then μθ(xt, t) can be derived from εθ(xt, t) by the following equation:\n\nμθ(xt, t) =\n\n1 √\nαt\n\n(xt −\n\n1 − αt √\n1 − ̄αt\n\nεθ(xt, t))\n\n(24)\n\nwhere αt = 1−βt and ̄αt := (cid:81)t to Eq (17) repeatedly until we reach s0.\n\ns=1 αs. To this end, we can sample data from pθ(xt−1|xt) according\n\nA.2 DENOISING DIFFUSION IMPLICIT MODELS (DDIMS)\n\nTo improve the sampling efficiency, authors of DDIM (Song et al., 2021) have proposed a novel non-Markov chain process to reduce the forward and reverse process steps of DDPMs. They found a special property of the forward process of DDPM as\n\nq(xt|x0) = N (xt;\n\nαtx0, (1 − αt)I)\n\n(25)\n\n√\n\nsuch that the training objective of DDPM is able to not directly based on the joint q(x1:T |x0). Then, they derive that\n\nq(xt−1|xt, x0) = N\n\n(cid:18)√\n\nαt−1x0 +\n\n(cid:113)\n\n1 − αt−1 − σ2\n\nt ·\n\nxt − √\n\n(cid:19)\n\n,\n\n, σ2\n\nt I\n\n(26)\n\n√\n\nαtx0\n\n1 − αt\n\nαT x0, (1 − αT )I). And based on Bayes’ rule, the forward\n\nwhere t > 1 and q(xT |x0) = N ( process can be derived as\n\n√\n\nqσ(xt|xt−1, x0) =\n\nqσ(xt−1|xt, x0)qσ(xt|x0) qσ(xt−1|x0)\n\n,\n\n(27)\n\ni.e. the forward process is no longer Markovian as each xt is dependent on xt−1 and x0.\n\nDuring the generative process, they train a noise approximator ε(t) on the probability qσ(xt−1|xt, x0) as follows\n\nθ\n\nto estimate p(t)\n\nθ (xt−1|xt) based\n\n(cid:40)\n\np(t) θ (xt−1|xt) =\n\nθ\n\nN (f (1) (x1), σ2 q(xt−1|xt, f (t)\n\n1I) θ (xt)),\n\nif t=1\n\notherwise,\n\nwhere f (t)\n\nθ (xt) is the prediction of x0 using the noise approximator ε(t)\n\nθ give xt, as\n\nf (t) θ (xt) := (xt −\n\n√\n\n1 − αtε(t)\n\nθ (xt))/\n\n√\n\nαt.\n\nTherefore as they derived their generative training objective is\n\nJσ(εθ) :=\n\nT (cid:88)\n\nt−1\n\nEx0∼q(x0)\n\n(cid:2)||ε(t) θ (\n\n√\n\nαtx0 +\n\n√\n\n1 − αtεt) − εt||2\n\n2\n\n(cid:3)\n\n(28)\n\n(29)\n\n(30)\n\nwhere εt ∼ N (0, I). Once the noise approximator is trained, one can generate a sample xt−1 give a sample from xt as\n\nxt−1 =\n\n√\n\nαt−1\n\n(cid:32)\n\nxt −\n\n√\n\n1 − αtε(t) √\nαt\n\nθ (xt)\n\n(cid:33)\n\n(cid:113)\n\n+\n\n1 − αt−1 − σ2\n\nt · ε(t)\n\nθ (xt)\n\n(31)\n\nAccelerated Generation Process The DDIM authors proposed to use non-Markovian chain for the forward procedure as qσ(xt|x0) can be directly estimated. Therefore, they propose to use subsequence τ from [1, · · · , T ] to speed up the generative process.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Architecture of the U-Net for estimating noise ε(t)\n\nθ and pen state ˆg.\n\nTable 6: Comparison results of using U-Net and Bi-directional RNN as noise approximator.\n\nModel\n\nSimple\n\nModerate FID↓ GS↓ Prec↑ Rec↑ FID↓ GS↓ Prec↑ Rec↑ FID↓ GS↓ Prec↑ Rec↑ 0.85 0.56\n\n0.45 0.39\n\n0.87 0.74\n\n0.88 0.78\n\n0.52 0.42\n\n0.42 0.24\n\n8.4 14.7\n\n4.7 18.2\n\n9.4 22.8\n\n5.2 86.6\n\nComplex\n\n3.4 5.6\n\nSpeed↓\n\n1.29 -\n\n6.9 Ours-UNet Ours-biRNN 12.3\n\nA.3 U-NET ARCHITECTURE\n\nThe architecture of our U-Net is shown in Figure 4. It consists of a stack of convolution blocks and average pooling for downsampling, followed by a stack of upsampling convolutions and convolution blocks, with skip connections for concatenating feature maps in the same resolution. Additionally, a single head attention layer is used to inject the timestep t embedding into the convolution blocks during downsampling following DDIM Song et al. (2021).\n\nBi-directional RNN as noise approximator. Instead of using the conventional convolutional U-Net, a different network architecture, i.e., bi-directional RNN, is explored to estimate the noise. Specifit } where st ∈ RN ×2, cally, given a sketch represented by a sequence of points st = {s1 t ∈ Rd. Then bi-directional RNN is first applied to map each point si all hidden states are concatenated together to construct the overall feature ht ∈ RN ×d for a sketch. Finally, ht is fed into a fully-connected layer to predict the noises, i.e., εt ∈ RN ×2. Similarly, the pen states can be obtained from ht by another FC-layer. The results are shown in Table 6. We can find out that the obtained performances can not surpass the results using U-Net.\n\nt ∈ R2 into a hidden state hi\n\nt , . . . , sN\n\nt , s2\n\nFigure 5: Examples of the process of data sampling (one example in a row). We can see that early steps (T=1000 to about T=300) are often not effective that samples in the red box change slowly. In contrast, later steps get much more efficient to rectify sketches as in the blue box.\n\n15\n\nN=962N=96linearcopydownsample12864961923264conv1D192288321628838416attention38416upsample67232288324806419264288128N=96128N=962N=96concate1N=963T=300T=0T=1000Published as a conference paper at ICLR 2023\n\nFigure 6: The obtained rasterized sketches using neural line rendering (NLR).\n\nA.4 FURTHER INSIGHTS INTO SHORTCUT SAMPLING\n\nShortcut sampling is a cute discovery we made that is specific to sketch data – coordinates in sketch sequence are far less robust to added noise than pixel values of an image. This can be seen from Figure 1(c) and Figure 5 – that early sampling steps are often long and inefficient. Addressing this merely means a speed-up in generation while retaining quality (16x faster, see Table 1).\n\nA.5 NEURAL LINE RENDERING\n\nFollowing Li et al. (2020a), neural line rendering (NLR) is performed to convert a vectorized sketch to its pixelative image. Specifically, given a sketch represented by a sequence of points t , . . . , sN st = {s1 t } at timestep t, a bi-directional Long Short-Term Memory (LSTM) is firstly t ∈ Rd for each point si used to extract per-point features f i t:\n\nt , s2\n\n[hi\n\nt, [hi−1 t] = LSTM(si t; ci t = sigmoid(whi f i\n\nt\n\nt + b)\n\n; ci−1\n\nt\n\n])\n\n(32)\n\nwhere h and c are the hidden states and the optional cell states, w and b are the weights and biases of a fully connected layer. Then the point sequence and the features, i.e., {(si t )}, are fed into the NLR module to produce a d-channel feature map of size H × W × d. And the c-th channel of the feature map I c (the timestep t and the index for point i are omitted for clarity) can be obtained as follows:\n\nt, f i\n\nI c k =\n\n(cid:26)(1 − αk) · f c\n\ni + αk · f c\n\ni+1\n\n0,\n\nif D(Ik, pipi+1) < γ otherwise,\n\n(33)\n\nk is computed by a linear interpolation of f c\n\nwhich means the pixel value I c i+1 (i.e., the c-th feature values of two nearby stroke points pi and pi+1) if Ik is a stroke pixel (the distance from Ik to the line segment pipi+1 is smaller than a threshold, i.e., D(Ik, pipi+1) < γ.) Note that pi and pi+1 are absolute coordinates corresponding to points si and si+1. And αk = ∥pk − pi∥2/∥pi+1 − pi∥2, where pk is the projection point of Ik on line segment pipi+1.\n\ni and f c\n\nTo this end, by rendering the stroke points’ features into pixel values Ik, a vectorized sketch can be transformed into an image sketch. NLR is differentiable due to the linear interpolation based rendering, thus the gradient w.r.t the perceptual similarity given by a 2D-CNN in Eq 12 can be backpropagated to the vectorized sketch. Some examples of rendered sketch images are demonstrated in Figure 6 where d = 3, γ = 1, H and W are both set to 256.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.6\n\nIMPLEMENTATION DETAILS\n\nA single Nvidia 3090 GPU is used for model training. The batch size is set to 512. The point number is selected as 96, which is the average value of stroke points in the QuickDraw dataset. Cosequently, sketches with more than 96 stroke points are excluded during model training. In addition, we will pad the point sequence with zeros to reach N=96 if the actual number of stroke points of any sketch It turns out that N = 96 works well for most cases since about 91% sketches is less than 96. (statistics from all the 345 categories) having stroke points less than 96. The default setting of skipping stride m = 50, and the recognizability threshold ζ = 0.2. Instead of directly using α1, . . . , αT , β1, . . . , βT is adopted to define the mean and variance of Gaussian noise. And a linear noise schedule is leveraged and βt is defined as:\n\nwhere β1 = 10−4 and βT = 0.02 in our case. Then the mean is Adam optimizer (β1 = 0.9 and β2 = 0.98) is used for optimization.\n\nβt = β1 +\n\nt − 1 T − 1\n\n× (βT − β1)\n\n(34)\n\n√\n\n1 − βt and variance is βt in Eq 1.\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper proposed using diffusion model to generate vectorized sketches. A vectorized sketch is represented as a sequence of points, and a *pen state* indicating whether the pen is touching the paper. The authors formulated the problem as generating a fixed number of 3D points using a 1D convolution-based DDPM. To accelerate the sampling process, the paper proposed using an RNN network to predict whether the noisy example is already recognizable -- empirically, more reverse steps are needed when the result starts to become recognizable.\nThe proposed model achieved SOTA performance on sketch generation. The version using RNN-based shortcut sampling required significantly less computation compared to Naive DDPM sampling while retaining the generation quality.\nIn addition, similar to image diffusion models, the proposed model is able to perform tasks such as sketch refinement and healing.\n\n# Strength And Weaknesses\n\n## Strength:\n* This is the first paper that applies DDPM to the task of sketch generation.\n* The proposed method significantly outperforms previous methods.\n* The proposed shortcut sampling scheme significantly reduced sampling time while retaining the quality better than DDIM given the same number of steps.\n* The proposed method has all the versatility a DDPM provides -- it can be applied to conditional generation tasks without retraining.\n\n## Weaknesses\n* The shortcut sampling technique, although worked well empirically, is not backed by theory. It will be more useful if the author can share more theoretical insights & generalize it to other tasks.\n* In section 3.8, the paper mentioned using a pretrained 2D CNN to evaluate similarity between two sketches. It is unclear how the gradient from a 2D CNN is propagated to a vectorized sketch. Is there differentiable rendering?\n* The paper used a fixed number of stroke points for each sketch. This might limit its real-world application.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written, with comprehensive visualization and numerical results. The originality of the paper lies in the application of DDPM to the new task, as well as the task-specific fast sampling algorithm. \n\n[Reproducibility] Some of the domain-specific implementation details are lacking, such as how the fixed number of points per sketch are sampled from the dataset, as well as how the guidance scores in the conditional generation tasks are obtained. The paper also did not include any promise of code release.\n\n# Summary Of The Review\n\nThe paper is well-written and the experiments are comprehensive. Being a domain-specific application of an existing technique (DDPM), the paper has not only thoroughly explored the benefits it provides, but also included some interesting new contributions, such as shortcut sampling. Although it will be better if this finding can be generalized beyond sketch generation.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDEEP PROBABILISTIC TIME SERIES FORECASTING OVER LONG HORIZONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecent advances in neural network architectures for time series have led to significant improvements on deterministic forecasting metrics like mean squared error. We show that for many common benchmark datasets with deterministic evaluation metrics, intrinsic stochasticity is so significant that simply predicting summary statistics of the inputs outperforms many state-of-the-art methods, despite these simple forecasters capturing essentially no information from the noisy signals in the dataset. We demonstrate that using a probabilistic framework and moving away from deterministic evaluation acts as a simple fix for this apparent misalignment between good performance and poor understanding. With simple and scalable approaches for uncertainty representation we can adapt state-of-the-art architectures for point prediction to be excellent probabilistic forecasters, outperforming complex probabilistic methods constructed from deep generative models (DGMs) on popular benchmarks. Finally, we demonstrate that our simple adaptations to point predictors yield reliable probabilistic forecasts on many additional problems of practical significance, namely large and highly stochastic datasets of climatological and economic data.\n\nFigure 1: Example predictions on exchange rate (left), ETTm2 (a sequence of electricity transformer temperature readings, center), and weather (right) for NHiTS (Challu et al., 2022b), Autoformer (Wu et al., 2021), and last value predictions, as well as the historical standard deviation of the change from the last observed value. On the exchange (Lai et al., 2018) and ETTm2 (Zhou et al., 2021) datasets there is minimal structure to be exploited except on very short horizons, and forecasts tend to under-perform simple baselines. On semi-structured datasets like weather, models can capture some overall structure, such as NHiTS accurately predicting the final values in the forecasting window, but are still only on par with naive predictions. From these plots we see why probabilistic evaluation is necessary and point estimates are insufficient.\n\n1\n\nINTRODUCTION\n\nFollowing deep learning’s breakthroughs in sequence modeling for text and audio, significant research effort has sought to achieve comparable success in time series, where the unique challenges of data scarcity and long-range dependencies have created a niche for creative architecture design. In rapid succession, many new models and architectures have demonstrated improved point predictions on benchmarks adopted by the community (Challu et al., 2022b; Salinas et al., 2020; Wu et al., 2021). These methods hold incredible promise for real impact on time series based decision making, especially in economic domains that require highly accurate long-term predictions.\n\nWhile demonstrating steady improvements, however, research on deep learning for point prediction frequently ignores a key but simple fact: the real world is complex and predicting the future accurately from past observations alone is often impossible. In highly structured time series, such as observed\n\n1\n\n0501001502001.41.61.82.0Standardized YExchange050100150200Time1.000.750.500.250.00ETTm2DataAutoformerNHiTSLast ValueEmpirical std.0501001502001.00.80.60.40.2WeatherUnder review as a conference paper at ICLR 2023\n\nTable 1: Multivariate results with varying prediction lengths. Bolded results indicate the best performing model, and italics the second best. In all cases simple statistics of the input data to the model are either the first or second best performing models in terms of both MSE and MAE accuracy. Historical Inertia (HI) (Cui et al., 2021) was also introduced as a trivial baseline but has worse performance than our constants.\n\nModels\n\nMean\n\nLast Value\n\nN-HiTS\n\nAutoformer\n\nInformer\n\nARIMA\n\nHI\n\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n. 96 0.139 0.269 0.081 0.196 0.092 0.211 0.197 0.323 0.847 0.752 0.396 0.371 0.443 0.509 0.524 1.672 1.036\n\n0.296 0.214 0.112 0.251 336 0.384 0.454 0.305 2.298 0.467 0.434 0.517 720 0.938 0.736 0.823 0.681 0.888 0.723 1.447 0.941 2.478 1.310 20.666 0.864 0.955 0.772\n\nn a\nh c\nx E\n\n2 96 0.150 0.272 0.203 0.312 0.176 0.255 0.255 0.339 0.365 0.4536 0.225 0.301 0.158 0.261 m\n0.370 0.386 0.379 0.438 T\nT 0.478 0.445 0.446 0.477 E\n\n336 0.205 0.313 0.270 0.361 0.295 0.346 0.339 0.372 1.363 0.887 720 0.261 0.350 0.335 0.401 0.401 0.426 0.422 0.419 3.379 1.388\n\ne h\n\nr 96 0.216 0.271 0.259 0.254 0.158 0.195 0.266 0.336 0.300 0.384 336 0.313 0.336 0.377 0.338 0.274 0.300 0.359 0.395 0.578 0.523 720 0.380 0.377 0.465 0.394 0.351 0.353 0.419 0.428 1.059 0.741\n\nt a\ne\n\nW\n\n0.217 0.258 0.345 0.339 0.330 0.347 0.529 0.440 0.425 0.405 0.545 0.439\n\ntraffic, with strong periodicity from both daily and weekly cycles, we may be able to forecast point predictions with a high degree of accuracy. However, in stochastic time series which display only modest structure (e.g. periodicity or seasonality), such as precipitation or wind speed patterns, we cannot hope to produce accurate predictions of specific future outcomes using only historic observations.\n\nResearchers working in architecture design for timeseries frequently overlook intrinsic stochasticity in benchmark datasets. In Figure 1 we show how even sophisticated methods struggle to forecast accurately on data with a low signal-to-noise ratio. In Table 1 we take this observation further and show that, shockingly, naive constant predictors outperform two state-of-the-art time series models (Lai et al., 2018; Zhou et al., 2021) on several widely reported MSE evaluations. The numbers shown are taken directly from Challu et al. (2022b), Wu et al. (2021), and Zhou et al. (2021), but include new columns showing the performance of simply predicting either the mean or the last value of the observations. Notably, these datasets span many domains of practical interest (finance, energy, and climatology), and contain varying levels of structure and periodicity.\n\nWe present these surprising shortcomings of state-of-the-art models in order to encourage adoption of more meaningful evaluations. Although these deep learning methods are extremely good at extracting and extrapolating trends and periodic structure far into the future, aspiring to predict a constant value in the best case reflects a misalignment. Meaningful comparisons require a baseline that excels in both highly predictable and stochastic environments.\n\nKeeping in mind the need for probabilistic, rather than deterministic frameworks for forecasting, we instead ask if the strong trend extrapolation performance of point prediction models is being overlooked in the probabilistic time series literature. For datasets that have both noise and structure (e.g. wind), we find that high performing point predictors, such as NHiTS, typically outperform the mean forecasts provided by state of the art probabilistic methods. In Figure 2 we show the cumulative mean squared error for forecasts produced by NHiTS and the mean prediction taken from several popular probabilistic frameworks on common benchmark datasets as well as real world data.1 Motivated by improving probabilistic forecasting while retaining the strength in trend extrapolation found in some deterministic models, we explore adapting point predictors to the probabilistic setting rather than building new models entirely from the ground up.\n\nIn particular, we examine two baselines for constructing high performing probabilistic forecasters, built on quantile regression and heteroscedastic variance models (Section 3). These methods leverage advances in architecture design for time series modeling without succumbing to the pitfalls of point prediction. In Section 4, we provide a detailed comparison of our models with state-of-the-art deep learning methods for probabilistic forecasting, demonstrating that recent advances in architecture design are directly relevant to uncertainty quantification. Finally, in Section 5, we demonstrate\n\n1Models and datasets described in Sections 4 and 5 along with plots of cumulative CRPS scores.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Cumulative mean squared error with shade representing 2 standard deviations over 5 trials for both NHiTS and the mean prediction from several popular probabilistic forecasting models. NHiTS has better mean predictions than the probabilistic approaches in semi-structured settings like precipitation and wind speed modeling, where there are still underlying trends caused by seasonality and daily effects. To build better probabilistic models we aim to leverage the accurate trend capture abilities of models such as NHiTS.\n\nour methods’ exemplary performance on large and noisy datasets for climatology and financial forecasting.\n\nWe provide our code at https://anonymous.4open.science/r/timeformer-87D0.\n\n2 RELATED WORK\n\nInsights from classical methods. Prior to the first applications of machine learning in forecasting (De Gooijer and Hyndman, 2006), time series methods grew out of signal processing. Exponential smoothing (Brown, 1959), ARIMA (Box and Jenkins, 1968), state space models (Kalman, 1960; Shumway and Stoffer, 1982), and the (G)ARCH families of models (Engle, 1982) use simple filters or autoregressive models to extract trend components from noisy data. Many modern forecasting methods move beyond fixed parametric representations in order to better extract complex structure from data, but signal processing fundamentals continue to be guiding principles behind approaches that use filtering to decompose inputs and construct predictions.\n\nDeep decomposition approaches. Many recent deep learning architectures provide generalizations of older seasonality accounting approaches using decompositions. N-BEATS (Oreshkin et al., 2019), for example, uses several layers removing trend and seasonality components from univariate times series, while N-HiTS (Challu et al., 2022b) combines this approach with a multi-scale forecasting setup that uses several different networks to predict at different scales. Similarly, Smyl (2020) uses RNNs with exponential smoothing for seasonality components, ultimately winning the M4 forecasting competition. Exponential smoothing has been a key factor in many recent architectures, such as Autoformer (Wu et al., 2021) and ETSFormer (Woo et al., 2022), which both use smoothing modules as part of transformer-inspired architectures. Wen et al. (2017) use sequence-to-sequence RNN and CNN models in conjunction with quantile regression to produce a multi-horizon quantile forecasting model.\n\nAs we demonstrated above, these methods have a critical but easily resolvable shortcoming: they are constructed and evaluated without uncertainty in mind. In order to investigate the true impact of developments and deep decomposition methods, we draw on recent work in probabilistic forecasting.\n\nProbabilistic forecasting. Salinas et al. (2020) proposed a deep autoregressive forecasting method with simple closed-form likelihood models. Other methods have combined deep autoregressive models with more complex conditional density estimators in an effort to better model the joint distribution over covariates. Salinas et al. (2019), for example, extend the popular non-parametric Gaussian copula process volatility (GCPV) model of Wilson and Ghahramani (2010) for highdimensional forecasting with RNNs. More recently, authors have turned to using normalizing flows (Rasul et al., 2020; de Bézenac et al., 2020) and denoising diffusion models (Rasul et al., 2021; Tashiro et al., 2021) as highly flexible conditional density estimators. In particular, Rasul et al. (2020) and Rasul et al. (2021) condition normalizing flow and diffusion models on the outputs of an RNN to construct a joint model over all timesteps. In contrast, Tang and Matteson (2021) eschew conditioning on a deterministic backbone and uses a fully probabilistic transformer architecture.\n\n3\n\n0200400Horizon051015Cumulative MSEExchange Rate0200400Horizon0.00.51.01.5Traffic0200400Horizon0.000.250.500.751.001e7Precipitation0200400Horizon0500100015002000Wind Speed0100200Horizon0200000400000NASDAQ-100Model TypeNHiTSTimeGradTransMafDeepARMQ-RNNMQ-CNNUnder review as a conference paper at ICLR 2023\n\nFigure 3: In probabilistic forecasting we form a distribution over possible future outcomes that most closely matches the empirical distribution at some future time point. Given a model architecture capable of mean trend extrapolation we can simply fit Gaussian distributions to the residuals, or simultaneously predict many quantiles of the distribution, as in quantile regression. These approaches are inexpensive relative to using a deep generative model (DGM) or the Bayesian posterior to quantifying uncertainty. We find, surprisingly, that residual Gaussians and quantile regression coupled with powerful point predictors often outperform more complex and expensive alternatives in the long horizon setting.\n\nWhile prior work in probabilistic forecasting has focused on high-dimensional density estimation, we show that architecture design should be prioritized instead. Indeed, with extensive experiments, we demonstrate that when given a successful long-term predictor simple uncertainty representation is sufficient.\n\n3 EXTENDING POINT FORECASTING METHODS\n\nPrior work contains many methods for converting sequence models into probabilistic forecasters. We focus our attention on approaches that are simple and computationally efficient (Figure 4): quantile regression (Chernozhukov et al., 2010) and two-stage heteroscedastic Gaussian regression (Nix and Weigend, 1994). While quantile regression provides a more flexible forecast distribution that can asymmetric about the mean prediction, the Gaussian regression approach gives a closed form forecast which has practical use in applications like risk prediction.\n\nQuantile Regression We first consider quantile regression (QR) for representing uncertainty because of its unique combination of simplicity and flexibility. Unlike simple parametric families, QR can learn arbitrary distributions while maintaining efficient sampling. QR learns a conditional order statistic, fτ (x), given a quantile level τ ∈ (0, 1) using the pinball loss (Chernozhukov et al., 2010):\n\nLτ (fτ , y) = (fτ − y)(I{y ≤ fτ (x)} − τ ).\n\n(1)\n\nWith τ = 0.5, this loss reduces to mean absolute error, which yields an estimate of the conditional median. If we take τ = 0.01 instead, the model yields an estimate of the smallest conditional percentile. To estimate a full predictive distribution, we train a series of conditional estimates for quantile levels {τ0, ..., τk} simultaneously (Chung et al., 2021; Rodrigues and Pereira, 2020; Tagasovska and Lopez-Paz, 2019). At test time, we use interpolation to ensure monotonic quantiles (Chernozhukov et al., 2010).\n\nQuantile regression is popular instrument within the probabilistic forecasting community (Gouttes et al., 2021; Kan et al., 2022; Park et al., 2022; Romano et al., 2019), and we do not propose any novel extensions of it. Instead, we show that simple UQ methods can outperform more complex and computationally expensive techniques. When using quantile regression in conjunction with NHiTS we refer to the method as NHiTS-QR.\n\nHeteroscedastic Gaussian Regression Distinct from NHiTS-QR, we propose to train a neural network to output the variance parameter in a Gaussian likelihood, analogous to earlier approaches (Bishop, 1995; Nix and Weigend, 1994; Rodrigues and Pereira, 2020). Here, we assume a het-\n\n4\n\nEmpirical DensityEmpirical CDFHistoryFutureCheap & SimpleLess Cheap & Less SimpleResidual GaussianQuantile RegressionBayesianDGMUnder review as a conference paper at ICLR 2023\n\n(a) Inference time vs Dimension.\n\n(b) Inference Time vs Prediction Horizon.\n\nFigure 4: (a) Time in seconds to generate a single forecast as a function of the output dimensionality of the time series. (b) Time in seconds to generate a single forecast as a function of prediction length. In both cases we see that the modified point predictor models (NHiTS-G and NHiTS-QR) run much faster than either diffusion-based models for all but the longest or highest dimensional forecasts.\n\nFigure 5: Many previous works on probabilistic time series forecasting have focused on short horizons, shown as the vertical dashed line. Here we extend the targets to more than 20 times the original length. By expanding both the forecast horizon and the test set size we able to validate methods on longer horizons as well as a broader distribution of test points.\n\neroscedastic Gaussian observation model, ˆyt ∼ N (f (x1:T )t, g(f (x1:T )t)), where f (·) is a hierarchical time series model as described in Challu et al. (2022a) and g(.) is a simple multi-layer perceptron. We term this model NHiTS-Gaussian (NHiTS-G). To train this model, we use maximum likelihood in a two-step manner, first training the mean model, f (·), using MSE loss, and then fixing that model and training the variance model, g(·), by maximizing the likelihood of the Gaussian observation model. Our approach parallels Model Agnostic Quantile Regression (MAQR) (Rodrigues and Pereira, 2020) and the heteroscedastic MLPs fit in Nix and Weigend (1994) and Bishop (1995), but with inductive biases particularly suited for probabilistic time series. While it is a strong assumption to assume the marginal distribution at each time point is Gaussian, this model serves as a simple baseline, and as we show in Section 4 is capable of outperforming more expensive methods.\n\n4 BENCHMARK EXPERIMENTS\n\nDataset Description We consider multivariate time series datasets provided by the GluonTS time series library and used in several time series papers focused on uncertainty quantification (UQ) (Salinas et al., 2019; 2020; Tang and Matteson, 2021). There are notable shortcomings with this experimental setup, however, as the default formulations use limited test sets and short horizon predictions. For the exchange, solar, electricity, and traffic datasets the prediction horizon is only either 24 or 30 time steps, and the test sets are comprised of no more than seven testing forecast windows. These prediction horizons are less than 10% of the same evaluation sets as commonly used in the non-probabilistic time series literature, e.g. Challu et al. (2022a); Cui et al. (2021).\n\nTo test on these datasets more fully and compare the performance of probabilistic forecasters in more challenging and realistic settings we modify the testing and training splits from leaving approximately 3% of the data as the test set to be 80% train, 10% validation, 10% test in order to assess the performance across a set of long range horizons of 100’s of steps into the future, with thousands of test points. We show examples of the expanded forecast windows in Figure 5, for further information on dataset setup, and additional experimental results see Appendix B.\n\nMetrics For probabilistic forecasts, we compute CRPS (Gneiting and Raftery, 2007; Matheson and Winkler, 1976), summed over the dimensions of the time series, and averaged across the prediction\n\n5\n\n100200300400500Time Series Dimension0.010.02Prediction Time200400600Prediction Horizon0.010.020.03Prediction TimeModel TypeNHiTS-GNHiTS-QRNHiTS-SWAGTimeGradTransMAF0200400600800HorizonValueExchange0200400600800HorizonSolar0200400600800HorizonElectricityInputsTargetsOrignal CutoffUnder review as a conference paper at ICLR 2023\n\ntimes. For a single prediction, the CRPS score is defined against the estimated cumulative distribution function (CDF), ˆF as\n\nCRPS( ˆF , y) =\n\n(cid:90)\n\nR\n\n( ˆF (z) − I(z−y)>0)2dz,\n\nwhere ˆF (z) is estimated as the empirical CDF produced by sampling from our forecasts. Given our forecast contains multivariate time series and is predicted out over multiple time steps, we compare using CRPSsum, first by summing over the dimensions of the time series, then by averaging over time points. The summation gives ˆFsum(t) and yt = (cid:80) i yt,i where yt,i is the dth dimension of time (cid:105) (cid:104) CRPS( ˆFsum(t), yt) series y at time t, and the average yields CRPSsum = Et . CRPSsum is a proper scoring rule and takes into account both the sharpnes and accuracy of the prediction. CRPS can also be computed directly from the pinball loss (Eq. 1) as\n\nCRPS(F, y) = 2\n\n(cid:90) 1\n\n0\n\nLτ (y, F −1(τ ))dτ,\n\nthe integral of the pinball loss over all quantiles (Gneiting and Raftery, 2007; Laio and Tamea, 2007).\n\nBaselines We include several probabilistic time series methods as baselines for comparison. These methods cover a breadth of approaches to time series forecasting including autoregressive models (Salinas et al., 2020), sequence to sequence based quantile regression (MQ-RNN and MQ-CNN) (Wen et al., 2017), and diffusion models TransMAF and TimeGrad (Rasul et al., 2021; 2020). These models represent state of the art and are highly competitive approaches on the widely used GluonTS datasets as seen in Rasul et al. (2021). Despite the strong performance shown by these methods for short horizon forecasting, at longer horizon predictions we see these established baselines suffer in performance, with the strong trend extrapolation provided by using an underlying deterministic model being critical to forecasting success.\n\n4.1 PROBABILISTIC LONG TERM FORECASTING\n\nWhile recent models such as TimeGrad (Rasul et al., 2021), TransMAF (Rasul et al., 2020), and DeepAR (Salinas et al., 2020) have achieved significant improvements in continuous ranked probability scores on benchmark datasets, these methods have primarily been explored in short forecasting settings with limited sized test sets. While the datasets typically contain thousands of observations in time, only the last few hundred are reserved for testing amounting in some cases to a mere 1% of the data, and only in prediction intervals of 24 time steps, see Figure 13 for plots of the standard splits.\n\nA primary strength of decomposition-based methods like Autoformer and NHiTS is their ability to produce long term forecasts, showing success with predictions out to 720 time steps in the future (Challu et al., 2022b; Wu et al., 2021). Our probabilistic forecasters build on these methods to be well suited for producing accurate long horizon mean forecasts, and ultimately strong probabilistic modeling due to the underlying strength of the point predictor.\n\nLong Range Evaluation To more accurately gauge performance over long term forecasts with larger test sets, we change the GluonTS datasets to a 80%/20% train/test split, and predict on a horizons of {96, 192, 336} time steps. We show the results in Figure 6.\n\nWhile in highly stochastic settings with little trend to be extrapolated, such as the Exchange rate dataset, the advantages to using models like NHiTS-G and NHiTS-QR are muted. However as the datasets exhibit more structure, such as the daily effects in electricity consumption and traffic congestion, or the seasonal effects in solar radiation, the improved trend extrapolation given by the underlying point predictor leads to stronger performance. This trend is more significantly pronounced at longer forecast horizons, where we see NHiTS-G and NHiTS-QR achieving state of the art in longer term forecasting for the electricity, traffic, and solar datasets.\n\nBayesian methods In probabilistic time series forecasting we are primarily concerned with representing epistemic uncertainty, or uncertainty about the forecasts our model is producing (rather than aleatoric, or irreducible, uncertainty). If our aim is to accurately represent our model uncertainty without altering the base architecture then Bayesian methods are a natural choice (MacKay,\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Barplot Of CRPSsum scores for varying prediction lengths. As the forecast horizon increases, NHiTS approaches consistently outperform current state of the art methods. Note, we used several hyper-parameters for TransMAF, including those given in publicly available implementation here, but were not able to find stable training procedures for all datasets. Error bars represent ±2 standard deviations over 5 random initializations, the y-axis is truncated for clarity.\n\nFigure 7: Example forecasts produced by NHiTS-G in comparison to samples produced by SWAG and deep ensembles. In all cases the deep ensemble samples produce limited diversity and do not provide the uncertainty needed for probabilistic forecasting. SWAG samples tend to be noisy, and can fail to predict trends even when we are confident of their existence as on traffic.\n\n1992), and recent advances in approximate Bayesian neural networks (BNNs) have given us simple approaches to build models with out of the box probabilistic forecasts (Maddox et al., 2019).\n\nIn Figure 7, we show uncertainties learned by NHiTS-G side by side with two different approximations of model uncertainty, Deep Ensembles (Lakshminarayanan et al., 2017) and SWAG (Maddox et al., 2019) for several datasets (Lai et al., 2018). On the traffic dataset, the base architecture is easily capable of producing accurate mean forecasts, leading to both ensembles and SWAG producing homogeneous forecasts that under-represent our true uncertainty. For the solar and traffic datasets we are able to forecast means that fit the cyclical nature of the data, but cannot predict peak solar radiation or traffic consistently leading to the need for uncertainty in our forecasts. In these cases SWAG overestimates our model uncertainty and fails to confidently predict the mean, even when it is appropriate to do so (such as predicting low solar radiation at night).\n\nWe show the cumulative CRPS of both the adapted point forecasting methods, as well as SWAG and deep ensembles in Figure 8. Despite promising performance on short horizons, as the length of the forecast increases the performance of SWAG and deep ensembles degrades, and both NHiTS based models tend to perform much better at long term predictions. While Bayesian methods might also provide a simple mechanism for wrapping uncertainty around an effective base model, they carry serious computational costs at test time, because we need to do as many forward passes through the model as there are samples, as illustrated in Figure 4.\n\nFigure 8: Cumulative CRPS of the NHiTS based methods with Bayesian baseline adaptations. While on short horizons SWAG and deep ensembles can provide reasonable forecast distributions, at longer horizons we find limited sample diversity or failure to predict trends leading to reduced performance.\n\n7\n\nExch.Elec.Traf.Solar0.00.20.40.6CRPSPrediction Length 96Model TypeNHiTS-GNHiTS-QRTimeGradTransMAFDeepARMQ-RNNMQ-CNNExch.Elec.Traf.SolarPrediction Length 192Exch.Elec.Traf.SolarPrediction Length 336050Horizon1.41.6ValueExchange050Horizon0100200Solar050Horizon0.00.5Traffic050Horizon100150200ElectricityNHiTS-GSWAG SamplesEnsembleTruth0500Horizon012Cumulative CRPS1e7Electricity0500Horizon0500Exchange0500Horizon0.00.51.01e6Solar0500Horizon0500010000TrafficModel TypeNHiTS-GNHiTS-QRNHiTS-SWAGEnsembleUnder review as a conference paper at ICLR 2023\n\n(a) Precipitation CRPS\n\n(b) Precipitation Calibration\n\nFigure 9: Left: CRPS scores for various prediction lengths in precipitation forecasting. Right: Calibration curves for forecasts of length 720. Both NHiTS-G and NHiTS-QR achieve low CRPS, outperforming all competing methods at every time horizon. While NHiTS-G attains lower CRPS than NHiTS-QR, NHiTS-QR produces highly calibrated forecasts. These metrics together suggest that methods like NHiTS-QR have practical relevance in climatological study, where accurate simulation is a key area of interest.\n\n5 NOISY, LONG-HORIZON DATASETS WITH RICH STRUCTURE\n\nClimatology and finance are key areas of interest for probabilistic forecasting, as they are stochastic domains where we cannot make confident predictions over long horizons, but retain exploitable structure like long term trends or cyclical climatological patterns. The ability to estimate distributions of climatological quantities of interest such as precipitation and wind speed has broad impact in ecological impact study, insurance pricing models, architectural and civil engineering, and beyond. Similarly, the probabilistic financial forecasting are central to the applications market pricing, automated trading, asset allocation. To demonstrate the efficacy of adapting deterministic models like NHiTS to probabilistic forecasting we examine predictive performance on two real world climatology datasets and historical prices from the component assets of the NASDAQ-100.\n\nFor these experiments we also consider calibration, as applications such as simulation and risk estimation demand that the probabilistic forecast is a faithful representation of the ground truth distribution at all quantiles. For a time series y and forecast time T , to compute the calibration at p, Cp, we compute the empirical quantile of the forecast qT where ˆP(yT < qT ) = p. Cp is then the observed Iyt,k<qt,k . frequency of yT < qT over all forecasts and time horizons, yielding Cp = 1 If a predictor is calibrated, Cp ≈ p for all p ∈ (0, 1), thus we can assess the overall calibration of a predictor by plotting calibration against percentiles, which should be close to the identity line.\n\n(cid:80)T\n\n(cid:80)\n\nt=1\n\nT K\n\nk\n\nPrecipitation Forecasting For precipitation forecasting we consider the United States Historical Climatology Network (USHCN) long-term daily climate records dataset (Menne et al., 2015). This dataset contains daily recorded precipitation values at approximately 1200 locations over the continental United States with observations ranging back over 100 years. In order to more accurately consider a scenario in which we are forecasting typical precipitation for a given day and location, rather than a single observed event, we preprocess the data by averaging over 5 day rolling windows.\n\nIn Figure 9b we show CRPS performance over varying horizons, as well as calibration for forecasts made over 720 time steps. Both NHiTS-G and NHiTS-QR attain low CRPS at all time horizons, and achieve high calibration relative to competing methods. While diffusion-based TimeGrad provides calibration on par with NHiTS-QR it yields much higher CRPS, particularly at the longer horizons that we are specifically interested in. MQ-CNN provides competitive performance in terms of CRPS for long forecasts, but gives very poor calibration, showing that the central quantiles of the forecast distribution are not well matched to the data.\n\nWind Speed Forecasting For wind speed forecasting we use the United States Climate Reference Network (USCRN) dataset composed of wind speed observations taken at 5 minute intervals at 154 spatial locations in the United States for the full 2021 calendar year (Diamond et al., 2013). Wind speed forecasting provides a particularly challenging problem for forecasting models, as there are many cyclic trends in the data that relate to both seasonal and daily effects, but there are also gust-like effects that produce erratic localized behavior including random periods of either high or absent wind.\n\n8\n\n96192336720Length0.00.20.40.6CRPSModel TypeNHiTS-GNHiTS-QRTimeGradTransMafDeepARMQ-RNNMQ-CNN0.20.40.60.8Percentile0.000.250.500.751.00CalibrationModel TypeNHiTS-GNHiTS-QRTimeGradTransMafDeepARMQ-RNNMQ-CNNUnder review as a conference paper at ICLR 2023\n\n(a) Wind Speed CRPS\n\n(b) Wind Speed Calibration\n\nFigure 10: Left: CRPS scores for various prediction lengths in wind speed forecasting. Right: Calibration curves for forecasts of length 720. For all time horizons NHiTS-G, NHiTS-QR, and TimeGrad achieve low CRPS, with NHiTS-QR and TimeGrad both producing well calibrated forecasts. Wind forecasting presents unique challenges due to the stochastic nature of gust-like effects, and the ability of NHiTS-QR to produce calibrated forecasts indicates that we are accurately capturing both the frequency and magnitude of such effects.\n\n(a) NASDAQ-100 CRPS\n\n(b) NASDAQ-100 Calibration\n\nFigure 11: Left: CRPS scores for prediction lengths of 200. Error bars represent one standard deviation from the mean over 5 trials, and the plot is truncated on the y-axis for clarity. Right: Calibration curves. Both variants of the probabilistic adaptation of NHiTS achieve low CRPS values, however NHiTS-QR in particular achieves near perfect calibration. NHiTS-G underperforms in calibration as may be expected from the non-normality of securities prices.\n\nFigure 10b includes both the CRPS and calibration comparison for the probabilistic NHiTS variant and competing methods. In wind speed forecasting TimeGrad performs approximately as well the adapted deterministic models in terms of both CRPS and calibration, but the remaining methods suffer both from higher CRPS scores and poor calibration.\n\nFinancial Data To demonstrate the performance of the probabilistic forecasting for real world financial data we construct a dataset containing hourly data from the component assets of the NASDAQ-100 for the two years leading up to September 2022. In this setting we forecast 200 time steps into the future, which equates to approximately a month of hourly predictions when accounting for trading hours. We show in Figure 11b that again both NHiTS-G and NHiTS-QR are among the best performing models, with NHiTS-QR achieving nearly perfectly calibrated forecasts. Given the typically assumed log-normality of stock prices it is unsurprising that NHiTS-G is asymmetrically miscalibrated relative to its performance on the climatology datasets.\n\n6 CONCLUSION\n\nTime series has rapidly become a popular target of innovation in neural network architecture design, and we argue that this increased popularity must be accompanied by increased scrutiny of common baselines and evaluation methods. In particular, we demonstrate that probabilistic evaluation is critical for understanding the efficacy of new methods on highly stochastic data. Without probabilistic evaluation, many sophisticated methods can be matched or outperformed with naive predictors. In promoting probabilistic methods and evaluations, we also emphasize the importance of simple baselines for uncertainty quantification. From our experimental results, we conclude that architectural design can have a significant impact on the quality of learned forecasts. Contrary to a perceived consensus in recent work, we find that elaborate uncertainty quantification techniques are less important than architecture design. We hope that our work provides a foundation for future methods in probabilistic forecasting, especially those that target long horizons and noisy datasets.\n\n9\n\n96192336720Length012CRPSModel TypeNHiTS-GNHiTS-QRTimeGradTransMafDeepARMQ-RNNMQ-CNN0.20.40.60.8Percentile0.000.250.500.751.00CalibrationModel TypeNHiTS-GNHiTS-QRTimeGradTransMafDeepARMQ-RNNMQ-CNNModel0.00.10.2CRPSModel TypeNHiTS-GNHiTS-QRTimeGradTransMafDeepARMQ-RNNMQ-CNN0.20.40.60.8Percentile0.000.250.500.751.00CalibrationModel TypeNHiTS-GNHiTS-QRTimeGradTransMafDeepARMQ-RNNMQ-CNNUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nWe do not anticipate that this work should have any negative societal implications. On the contrary, improvements in time series forecasting should have primarily beneficial impacts. For example, improvements in weather forecasting, especially at higher frequencies should provide longer windows for evacuation from severe weather such as hurricanes and tornadoes. Similarly, improvements in time series forecasting should be extremely helpful in disease progression forecasting, enabling doctors and other practitioners to have a better understanding of which patients need increased attention or different treatments (Jarrett et al., 2021). At the same time, over-reliance on machine learning models in quantitative finance settings (like in Section 5) could potentially lead to financial shocks and decreased robustness to risk in the stock market, like was the case for several of the first usages of statistical learning in these settings (Jorion, 2000; MacKenzie and Spears, 2014).\n\nREPRODUCIBILITY STATEMENT\n\nOur experiments are easily reproducible with the scripts provided in our anonymized repo. The GluonTS and Autoformer benchmark datasets can be easily downloaded from each package’s public code repo. The USHCN precipitation and wind speed datasets can be easily downloaded from public logs and are used without data cleaning. The hourly NASDAQ-100 dataset can also be easily reproduced using public logs and is used without data cleaning. The hyperparameters for our models are included in the appendix.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner Türkmen, and Yuyang Wang. GluonTS: Probabilistic and Neural Time Series Modeling in Python. Journal of Machine Learning Research, 21(116):1–6, 2020. URL http://jmlr.org/papers/v21/19-820.html.\n\nChristopher M Bishop. Neural networks for pattern recognition. Oxford university press, 1995.\n\nGeorge EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. Journal of\n\nthe Royal Statistical Society. Series C (Applied Statistics), 17(2):91–109, 1968.\n\nRobert Goodell Brown. Statistical forecasting for inventory control. McGraw/Hill, 1959.\n\nCristian Challu, Peihong Jiang, Ying Nian Wu, and Laurent Callot. Deep generative model with hierarchical latent factors for time series anomaly detection. arXiv preprint arXiv:2202.07586, 2022a.\n\nCristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. arXiv preprint arXiv:2201.12886, 2022b.\n\nVictor Chernozhukov, Iván Fernández-Val, and Alfred Galichon. Quantile and probability curves\n\nwithout crossing. Econometrica, 78(3):1093–1125, 2010.\n\nYoungseog Chung, Willie Neiswanger, Ian Char, and Jeff Schneider. Beyond pinball loss: Quantile methods for calibrated uncertainty quantification. Advances in Neural Information Processing Systems, 34, 2021.\n\nYue Cui, Jiandong Xie, and Kai Zheng. Historical inertia: A neglected but powerful baseline for long sequence time-series forecasting. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2965–2969, 2021.\n\nEmmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael BohlkeSchneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski. Normalizing kalman filters for multivariate time series analysis. Advances in Neural Information Processing Systems, 33:2995–3007, 2020.\n\nJan G De Gooijer and Rob J Hyndman. 25 years of time series forecasting. International journal of\n\nforecasting, 22(3):443–473, 2006.\n\nHoward J Diamond, Thomas R Karl, Michael A Palecki, C Bruce Baker, Jesse E Bell, Ronald D Leeper, David R Easterling, Jay H Lawrimore, Tilden P Meyers, Michael R Helfert, et al. Us climate reference network after one decade of operations: Status and assessment. Bulletin of the American Meteorological Society, 94(4):485–498, 2013.\n\nRobert F Engle. Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation. Econometrica: Journal of the econometric society, pages 987–1007, 1982.\n\nTilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.\n\nJournal of the American statistical Association, 102(477):359–378, 2007.\n\nAdèle Gouttes, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi. Probabilistic time series forecasting with implicit quantile networks. arXiv preprint arXiv:2107.03743, 2021.\n\nDaniel Jarrett, Jinsung Yoon, Ioana Bica, Zhaozhi Qian, Ari Ercole, and Mihaela van der Schaar. Clairvoyance: A pipeline toolkit for medical time series. In International Conference on Learning Representations, 2021.\n\nPhilippe Jorion. Risk management lessons from long-term capital management. European financial\n\nmanagement, 6(3):277–300, 2000.\n\nRudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKelvin Kan, François-Xavier Aubet, Tim Januschowski, Youngsuk Park, Konstantinos Benidis, Lars Ruthotto, and Jan Gasthaus. Multivariate quantile function forecaster. arXiv preprint arXiv:2202.11316, 2022.\n\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 95–104, 2018.\n\nFrancesco Laio and Stefania Tamea. Verification tools for probabilistic forecasts of continuous\n\nhydrological variables. Hydrology and Earth System Sciences, 11(4):1267–1277, 2007.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.\n\nDavid JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa-\n\ntion, 4(3):448–472, 1992.\n\nDonald MacKenzie and Taylor Spears. ‘the formula that killed wall street’: The gaussian copula and\n\nmodelling practices in investment banking. Social Studies of Science, 44(3):393–417, 2014.\n\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems, 32, 2019.\n\nJames E Matheson and Robert L Winkler. Scoring rules for continuous probability distributions.\n\nManagement science, 22(10):1087–1096, 1976.\n\nMJ Menne, CN Williams Jr, and RS Vose. United states historical climatology network daily temperature, precipitation, and snow data. Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, Oak Ridge, Tennessee, 2015.\n\nDavid A Nix and Andreas S Weigend. Estimating the mean and variance of the target probability distribution. In Proceedings of 1994 ieee international conference on neural networks (ICNN’94), volume 1, pages 55–60. IEEE, 1994.\n\nBoris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.\n\nYoungsuk Park, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang. Learning quantile functions without quantile crossing for distribution-free time series forecasting. In International Conference on Artificial Intelligence and Statistics, pages 8127–8150. PMLR, 2022.\n\nKashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf. Multivariate probabilistic time series forecasting via conditioned normalizing flows. arXiv preprint arXiv:2002.06103, 2020.\n\nKashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pages 8857–8868. PMLR, 2021.\n\nFilipe Rodrigues and Francisco C Pereira. Beyond expectation: deep joint mean and quantile regression for spatiotemporal problems. IEEE Transactions on Neural Networks and Learning Systems, 31(12):5377–5389, 2020.\n\nYaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. Advances\n\nin neural information processing systems, 32, 2019.\n\nDavid Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. Highdimensional multivariate forecasting with low-rank gaussian copula processes. Advances in neural information processing systems, 32, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3): 1181–1191, 2020.\n\nRobert H Shumway and David S Stoffer. An approach to time series smoothing and forecasting using\n\nthe em algorithm. Journal of time series analysis, 3(4):253–264, 1982.\n\nSlawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\n\nseries forecasting. International Journal of Forecasting, 36(1):75–85, 2020.\n\nNatasa Tagasovska and David Lopez-Paz. Single-model uncertainties for deep learning. Advances in\n\nNeural Information Processing Systems, 32, 2019.\n\nBinh Tang and David S Matteson. Probabilistic transformer for time series analysis. Advances in\n\nNeural Information Processing Systems, 34:23592–23608, 2021.\n\nYusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34, 2021.\n\nRuofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon\n\nquantile recurrent forecaster. arXiv preprint arXiv:1711.11053, 2017.\n\nAndrew G Wilson and Zoubin Ghahramani. Copula processes. Advances in Neural Information\n\nProcessing Systems, 23, 2010.\n\nGerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Exponential\n\nsmoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.\n\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34, 2021.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of AAAI, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\nA Limitations\n\nB Experimental Details\n\nB.1 Compute .\n\n.\n\n.\n\n.\n\nB.2 GluonTS Splits\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nB.3 Data Pre-processing .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nB.4 NHiTS Hyperparameters\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nB.5 Univariate Experiment Details .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nC Further Experiments\n\nC.1 Comparison with Conformal Quantile Regression .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n14\n\n14\n\n14\n\n14\n\n15\n\n15\n\n15\n\n15\n\n15\n\nA LIMITATIONS\n\nWe see that our work, as a primarily empirical study, has several limitations:\n\n• Our empirical work focuses primarily on well-used benchmark datasets that could be quite distinct from other time series forecasting problems used in the literature. Thus, approaches that do not perform well here might perform better on other tasks.\n\n• Our specific architecture is limited to forecasting only up to a pre-specified number of steps\n\ninto the future.\n\n• Our training and evaluation procedure is quite distinct from earlier efforts for forecasting using online learning, preventing easy direct comparison with methods like ARIMA and Gaussian processes.\n\n• If the underlying time series is extremely non-stationary and the input time series is quite\n\ndifferent from any previously observed data, we may still be unable to extrapolate.\n\nB EXPERIMENTAL DETAILS\n\nB.1 COMPUTE\n\nExperiments were carried out across several computing platforms. All models were trained on a single GPU. Figures 1, 6, 7, 4 were generated using a single NVIDIA Titan RTX. The remaining figures were generating from experiments using a single GPU on a larger cluster with a mix of NVIDIA V100s, RTX 8000s, and A100s. Depending on hyperparameter setting and dataset, models take approximately 10-30 minutes to train.\n\nB.2 GLUONTS SPLITS\n\nIn Figure 13, we display the training and testing splits for the six GluonTS datasets, with the dividing line shown in orange. Only on Wikipedia and Taxi are the training and testing splits especially large given the quantities of the data. Furthermore, the responses on Wikipedia, solar, and electricity may not be well described as having Gaussian observation noise, as they roughly reflect counts or drop to zero at times (solar at night).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Training and test splits for GluonTS datasets. From top left down are exchange, solar, electricity, traffic, taxi, and wikipedia.\n\nHyperparameter\n\nhidden dimension # hidden layers {θ1, θ2, θ3} {φ1, φ2, φ3} learning rate weight decay quantiles (NHiTS-QR)\n\nValues\n\n{256, 512} {2, 3} {{4, 2, 1}, {3, 2, 1}, {6, 2, 1}, {8, 2, 1}, {8, 4, 1}} {{4, 2, 1}, {3, 2, 1}, {6, 2, 1}, {8, 2, 1}, {8, 4, 1}} {10−3, 10−4} 2 × 10−6 {0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99}\n\nTable 2: Hyperparameters for NHiTS on short probabilistic evaluations\n\nB.3 DATA PRE-PROCESSING\n\nPrior to training, all data were standardized using the StandardScaler function from Scikit Learn. Except where specified for the datasets taken from the GluonTS library from Alexandrov et al. (2020), we use the same data splits as Wu et al. (2021). For the long horizon experiments in Section 4.1, we use the first 10% of the data as the validation set, the last 20% as the test set, and the remaining data as the training set.\n\nB.4 NHITS HYPERPARAMETERS\n\nTable 2 shows the hyperparameter space we searched for the GluonTS results in Section 4.1.\n\nB.5 UNIVARIATE EXPERIMENT DETAILS\n\nFor the Autoformer and Transformer models, we used the public code Autoformer repo. We adapted the data loaders to work with GluonTS (Salinas et al., 2019) datasets (which have a history of 192 and prediction horizon of 24), and we searched over the common model hyperparameters from the repo, for both the Transformer and Autoformer architecture. For the univariate models, we reduced the models’ capacity (through embedding dimension) from 512 to 256 to avoid overfitting. We converted multivariate tim eseries to univariate time series by sampling along the channels per batch and then flattening channels into the batch dimension. Models were trained for 20 epochs or until early stopping. We trained the NHiTS models with the hyperparameter space shown in Table 2.\n\nC FURTHER EXPERIMENTS\n\nC.1 COMPARISON WITH CONFORMAL QUANTILE REGRESSION\n\nConformalized quantile regression (CQR) (Romano et al., 2019) is a method for recalibrating a quantile regression forecaster using conformal sets constructed from a validation (calibration) dataset. In Figure 13, we show a comparison of our two probabilistic forecasting methods with CQR.\n\n15\n\n1992199620002004200820120.60.81.0Exchange2006-012006-022006-032006-042006-052006-062006-072006-082006-092006-100100200300Solar2014-022014-032014-042014-052014-062014-072014-082014-090200Electricity2012-012012-042012-072012-102013-012013-042013-072013-102014-01050000100000Wikipedia2008-012008-022008-032008-042008-052008-060.00.10.20.3Traffic2015-01-012015-01-052015-01-092015-01-132015-01-172015-01-212015-01-252015-01-292015-02-0101020TaxiEnd of TrainUnder review as a conference paper at ICLR 2023\n\nFigure 13: Comparing NHiTS-G and NHiTS-QR (ours) with conformal quantile regression (CQR), a conformal prediction method that adjusts the uncertainty estimates using a calibration/validation dataset. On the majority of the datasets conformal recalibration offers few benefits over our methods. In the datasets in the second row, CQR leads to significantly worse performance because constructing a compelling validation set is challenging.\n\n16",
    "reference": "# Summary Of The Paper\n\nThis paper provides a new view of the probabilistic forecasting problem. The authors have shown some drawbacks of some current state-of-the-art probabilistic forecasting methods and propose that a simple model coupled with quantile loss or heteroscedastic Gaussian regression can often outperform more complex and expensive alternatives, particularly in the long horizon setting. The authors support their claim on multiple time series datasets from different domains and with distinct properties. They have also studied how the prediction horizon and time series dimension affect the inference time. Finally, they reach the conclusion that the architecture design of forecasting models should be prioritized over uncertainty quantification techniques.\n\n# Strength And Weaknesses\n\nStrength:\nThe authors have conducted extensive empirical evaluations and conducted a comprehensive background review. The most inspiring part is the discussion on the impact of intrinsic stochasticity and the structure of data on forecasting models. Based on this property, the authors have divided the analysis into time series from different domains. This can serve as an interesting reference for the characteristics of time series datasets and provide insights on which model to choose.\n\nWeakness:\nThis paper is driven by pure empirical studies without theoretical insights or methodological innovation. The employed forecasting models and loss functions are all well-studied. The major work done by the authors is to combine them together and make a comparison on different datasets. Moreover, the authors don't provide a clear explanation of why the NHiTS is chosen as the base model and what is the intuition of its better performance than other complex methods. Lastly, I am also confused about how the authors reached the conclusion that architecture design should be prioritized over uncertainty quantification since the whole paper is comparing CRPS results across different methods. The authors apparently need a transition on this.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is ambiguous on how its conclusion is reached. There is also not too much technical novelty. The authors have provided code to reproduce their experiments.\n\n# Summary Of The Review\n\nThis paper is an empirical study of probabilistic forecasting models, while it lacks solid technical novelty and theoretical insights. There is not much innovation in experiments either. I think this paper will benefit from another round of revision.\n\n**Update after rebuttal**\n\nThe authors' responses have addressed some of my concerns, but I believe they are still not sufficient to significantly change my view of this paper. I would encourage authors to incorporate answers to the abovementioned questions in the next revision and discuss the underlying theoretical insights on why the methodology is performed.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nHIERARCHIES OF REWARD MACHINES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nReward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode landmarks of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.\n\n1\n\nINTRODUCTION\n\nMore than a decade ago, Dietterich et al. (2008) argued for the need to “learn at multiple time scales simultaneously, and with a rich structure of events and durations”. Finite-state machines (FSMs) are a simple yet powerful formalism for abstractly representing temporal tasks in a structured manner. One of the most prominent types of FSMs used in reinforcement learning (RL; Sutton & Barto, 2018) are reward machines (RMs; Toro Icarte et al., 2018; 2022), where each edge is labeled with (i) a formula over a set of high-level events that capture a task’s subgoal, and (ii) a reward for satisfying the formula. Hence, RMs fulfill the need for structuring events and durations. Hierarchical reinforcement learning (HRL; Barto & Mahadevan, 2003) frameworks, such as options (Sutton et al., 1999), have been applied over RMs to learn policies at two levels of abstraction: (i) select a formula (i.e., subgoal) from a given RM state to complete the overall task, and (ii) select an action to satisfy the chosen formula (Toro Icarte et al., 2018; Furelos-Blanco et al., 2021). Thus, RMs also allow learning at multiple scales simultaneously. The subtask decomposition powered by HRL eases the handling of long-horizon and sparse reward tasks. Besides, RMs can act as an external memory in partially observable tasks by keeping track of the subgoals achieved so far and those to be achieved.\n\nIn this work, we make the following contributions:\n\n1. Enhance the abstraction power of RMs by defining hierarchies of RMs (HRMs), where constituent RMs can call other RMs (Section 3). We prove that any HRM can be transformed into an equivalent flat HRM that behaves exactly like the original RMs. We show that under certain conditions, the equivalent flat HRM can have exponentially more states and edges.\n\n2. Propose an HRL algorithm to exploit HRMs by treating each call as a subtask (Section 4). Learning policies in HRMs further fulfills the desiderata posed by Dietterich et al. since (i) there is an arbitrary number of time scales to learn across (not only two), and (ii) there is a richer range of increasingly abstract events and durations. Besides, hierarchies enable modularity and, hence, the reusability of the RMs and policies. Empirically, we show that leveraging a handcrafted HRM enables faster convergence than an equivalent flat HRM.\n\n3. Introduce a curriculum-based method for learning HRMs from traces given a set of hierarchically composable tasks (Section 5). In line with the theory (Contribution 1), our experiments reveal that decomposing an RM into several is crucial to make its learning feasible (i.e., the flat HRM cannot be efficiently learned from scratch) since (i) the constituent RMs are simpler (i.e., they have fewer states and edges), and (ii) previously learned RMs can be used to efficiently explore the environment in the search for traces in more complex tasks.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND\n\nGiven a finite set X\nempty) sequences of elements from\n\n, we use ∆(\n\nX\n\n) to denote the probability simplex over\n\n,\n\n, and\n\n+ to denote non-empty sequences. We also use\n\nX\n\nX\n\n∗ to denote (possibly and\n\n⊥\n\nX\n\nX\n\nS × A\n\n, a set of actions\n\nto denote the truth values false and true, respectively. 1[A] is the indicator function of event A.\n\nS )+ representing high-level events, a labeling function l :\n\n⊤ We represent RL tasks as episodic labeled Markov decision processes (MDPs, Xu et al., 2020), each consisting of a set of states ), a [0, 1), a finite set of propositions reward function r : ( 2P mapping states to proposition P\nsubsets called labels, and a termination function τ : ( . Hence the transition function p is Markovian, but the reward function r and termination function τ are not. Given a history ht = , a label trace (or trace, for short) λt = (2P )+ assigns labels to all states in ht. We assume (λt, st) captures all relevant l(s0), . . . , l(st) ⟨\ninformation about ht; thus, the reward and transition information can be written r(ht, at, st+1) = r(ht+1) = r(λt+1, st+1) and τ (ht) = τ (λt, st), respectively. We aim to find a policy π : (2P )+\n\nA R, a discount factor γ\n\n, a transition function p :\n\ns0, a0, . . . , st ⟨\n\n, × S → {⊥\n\n× , a mapping from traces-states to actions, that maximizes the expected cumulative discounted\n\n( S × A\n\nS × A →\n\n⊤} × {⊥\n\n× S →\n\nS → )∗\n\nS × A\n\n× S\n\n⟩ ∈\n\n⟩ ∈\n\n⊤}\n\n∆(\n\n)∗\n\n∈\n\nS\n\n,\n\nreward (or return) Rt = Eπ[(cid:80)n S → A\n\nk=t γk−tr(λk+1, sk+1)], where n is the last episode’s step.\n\n∈\n\n(2P )+, and the agent observes a tuple st =\n\nst, sT ⟨\nt ) = τ (λt, st) is the termination information, with sT\n\nAt time t, the trace is λt ∈ S t , sG is the state and (sT indicating whether or not the history (λt, st) is terminal or a goal, respectively. If the history is non-terminal, the agent runs action at st, at). The l(st+1), receives reward rt+1 = r(λt+1, st+1), agent then extends the trace as λt+1 = λt and observes a new tuple st+1. A trace λt is a goal trace if (sT t , sG ), a dead-end ,\ntrace if (sT . We assume that the reward is t ) = ( r(λt+1, st+1) = 1[τ (λt+1, st+1) = (\n\n, and the environment transitions to state st+1\n\n)], i.e. 1 for goal histories and 0 otherwise.\n\n), and an incomplete trace if sT\n\nt , sG t ⟩ t and sG\n\n, where st\n\nt ) = (\n\nt , sG\n\nt =\n\n∈ A\n\np(\n\n⊤\n\n∼\n\n⊕\n\n⊤\n\n⊥\n\n⊤\n\n⊥\n\n·|\n\n,\n\n,\n\nt\n\n, φ, r, u0,\n\n,\n\nU\n\nP\n\nis a finite set of states;\n\nis a finite set of propositions; φ :\n\nA (simple) reward machine (RM; Toro Icarte et al., 2018; 2022) is a tuple ,\n⟩ U\nDNFP is a state where transition function such that φ(u, u′) denotes the disjunctive normal form (DNF) formula over to R is a reward transition function such that r(u, u′) be satisfied to transition from u to u′; r : is the reward for transitioning from u to u′; u0 is a set of accepting ∈ U states denoting the task’s goal achievement; and is a set of rejecting states denoting the U\nunfeasibility of achieving the goal. Ideally, RM states should capture traces, such that (i) pairs (u, s) of an RM state and an MDP state are sufficient to predict the future, and (ii) the reward r(u, u′) matches the underlying MDP’s reward. The state transition function is deterministic, i.e. at most is one formula from each state is satisfied. To verify if a formula is satisfied by a label b). a\nused as truth assignment where propositions in\n\nare true, and false otherwise (e.g.,\n\nis an initial state; R\n\nU × U →\n\nU ×U →\n\n⊆ U\n\n⊆ U\n\n⟨U\n\nP\n\nP\n\nU\n\nU\n\nA\n\n,\n\nA,\n\nR\n\nL ⊆ P = a } |\n\nL ∧ ¬\n\n{\n\nOptions (Sutton et al., 1999) address temporal abstraction in RL. Given an episodic labeled MDP, an option is a tuple ω = is ⟨I [0, 1] is the option’s termination condition. An option is available the option’s policy, and βω : with probability βω(s). in s\n\nω, selects actions according to πω, and terminates in s\n\nis the option’s initiation set, πω :\n\n, where ⟩\n\nω, πω, βω\n\nS → A\n\nS →\n\n⊆ S\n\nif s\n\nI\n\nω\n\nL\n\n⊤\n\n⊤\n\n∈ S\n\n∈ I\n\n∈ S\n\n3 FORMALIZATION OF HIERARCHIES OF REWARD MACHINES\n\nWe here introduce our formalism for hierarchically composing reward machines, and propose the CRAFTWORLD domain (cf. Figure 1a) to illustrate it. ) can move forward or rotate 90◦, staying put if it moves towards a wall. Locations are labeled with propositions . The agent observes propositions that it steps on (e.g., from P\nin the top-left corner). Table 1 lists tasks that consist of observing a sequence of propositions, { } where the reward is 1 if the sequence is observed and 0 otherwise. These tasks are based on those by Andreas et al. (2017) and Toro Icarte et al. (2018), but they can be defined in terms of each other.\n\nIn this domain, the agent (\n\n=\n\n{\n\n}\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nReward machines (RMs) are the building blocks of our formalism. To constitute a hierarchy of RMs, we need to endow RMs with the ability to call each other. We redefine the state transition is a set of RMs. The expression φ(u, u′, M ) function as φ : denotes the disjunctive normal form (DNF) formula over that must be satisfied to transition from . We refer to the formulas φ(u, u′, M ) as contexts u\n\nby calling RM M\n\nU × U × M →\n\nDNFP , where\n\nto u′\n\nM\n\nP\n\n∈ U\n\n∈ U\n\n∈ M\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nu0 0\n\nM1\n\n| ¬\n\nM2\n\n| ⊤\n\nu1 0\n\nu2 0\n\nM2\n\n| ⊤\n\nM1\n\n| ⊤\n\nu3 0\n\nM⊤\n\n|\n\nuA 0\n\nM0 (root)\n\n(b)\n\nu0 1\n\nM⊤\n\n|\n\nu1 1\n\nM⊤\n\n|\n\nuA 1\n\nM1\n\nu0 2\n\nM⊤\n\n|\n\nu1 2\n\nM⊤\n\n|\n\nuA 2\n\nM2\n\nu0\n\nu3\n\n∧ ¬\n\nu1\n\nu4\n\nu2\n\nu6\n\nu5\n\nuA\n\n(c)\n\n(a)\n\nFigure 1: A CRAFTWORLD grid (a), an HRM for BOOK (b), and an equivalent flat HRM (c). An edge from state u to u′ of an RM Mi is of the form Mj φi(u, u′, Mj), double circled states are accepting states, and loop transitions are omitted. Calls to the leaf RM M⊤ are omitted in (c).\n\n|\n\nTable 1: List of CRAFTWORLD tasks. Descriptions “x ; y” express sequential order (observe/do x then y), and descriptions “x & y” express that x and y can be observed/done in any order.\n\nTask\n\nBATTER BUCKET COMPASS LEATHER PAPER\n\nh\n\n1 1\n1 1\n1\n\nDescription\n\nTask\n\nh\n\nDescription\n\nTask\n\nh\n\nDescription\n\n( & ) ;\n\n;\n\n( & ) ;\n\n; ;\n\nQUILL SUGAR BOOK MAP MILKBUCKET\n\n;\n\n( & ) ;\n\n1 1\n2 2\n2 BUCKET ;\n\n(PAPER & LEATHER) ; (PAPER & COMPASS) ;\n\nBOOKQUILL MILKB.SUGAR CAKE\n\n3 BOOK & QUILL 3 MILKBUCKET & SUGAR 4 BATTER ; MILKB.SUGAR ;\n\nsince they represent conditions under which calls are made. As we will see later, contexts help preserve determinism and must be satisfied to start a call (a necessary but not sufficient condition). The hierarchies we consider contain an RM M⊤ called the leaf RM, which solely consists of an accepting state (i.e.,\n\n), and immediately returns control to the RM that calls it.\n\nu0\n\nA\n\n⊤ =\n\nU\n\n⊤ =\n\nU\n\n{\n\n⊤}\n\nDefinition 1. A hierarchy of reward machines (HRM) is a tuple H =\n\nM0, . . . , Mm−1\n\nM⊤\n\nis a set of m RMs and the leaf RM M⊤, Mr\n\n⟨M\n\n, Mr,\n\n, where\n\n= M\nis the root\n\nP⟩ M⊤\n\n{ RM, and\n\n} ∪ {\n\n}\n\nis a finite set of propositions used by all constituent RMs.\n\n∈ M \\ {\n\n}\n\nP\n\nWe make the following assumptions: (i) HRMs do not have circular dependencies (i.e., an RM cannot be called back from itself, including recursion), (ii) rejecting states are global (i.e., cause the root task to fail), (iii) accepting and rejecting states do not have transitions to other states, and (iv) the reward function of the root corresponds to the reward obtained in the underlying MDP. Given assumption (i), each RM Mi has a height hi, which corresponds to the maximum number of nested , then hi = 0; otherwise, hi = 1 + maxj hj, where calls needed to reach the leaf. Formally, if i = j ranges over all RMs called by Mi (i.e., there exists (u, v) ).\n\ni such that φi(u, v, Mj)\n\n⊤\n\n=\n\ni ∈ U\n\n× U\n\n⊥\n\nFigure 1b shows BOOK’s HRM, whose root has height 2. The PAPER and LEATHER RMs, which have height 1 and consist of observing a two-proposition sequence, can be run in any order followed by observing\n\nin the call to M1 preserves determinism, as detailed later.\n\n. The context\n\nIn the following paragraphs, we describe how an HRM processes a label trace. To indicate where the agent is in an HRM, we define the notion of hierarchy states.\n\n∈ M\n\n, Mr,\n\nis an RM, u\n\ni is a state, Φ\n\nDefinition 2. Given an HRM H = Mi Definition 3. Given an HRM H = each denoting a call where u the calling RM Mi are the disjuncts of φi(u, v, Mj) satisfied by a label; and Φ\n\n⟨ ∈ U after reaching an accepting state of the called RM Mj\n\nDNFP is an accumulated context, and Γ is a call stack. u, v, Mi, Mj, φ, Φ ,\n⟩ i is the next state in DNFP\n\ni is the state from which the call is made; v\n\nDNFP is the accumulated context.\n\n, a call stack Γ contains tuples\n\n, a hierarchy state is a tuple\n\nMi, u, Φ, Γ ⟩\n\n∈ , Mr,\n\n, where\n\n∈ M\n\n∈ M\n\n⟨M\n\n⟨M\n\n∈ U\n\n∈ U\n\n; φ\n\nP⟩\n\nP⟩\n\n∈\n\n⟨\n\n∈\n\nCall stacks determine where to resume the execution. Each RM appears in the stack at most once to since, by assumption, HRMs have no circular dependencies. We use Γ denote a stack recursively defined by a stack Γ and a top element , where the accumulated context Φ is the condition under which a call from a state u is made. The initial hierarchy state of an HRM H = : we are in the initial state of the is ⊤\nroot, there is no accumulated context, and the stack is empty.\n\nu, v, Mi, Mj, φ, Φ ⟨\n\nu, v, Mi, Mj, φ, Φ\n\nMr, u0 r,\n\n, [] ⟩\n\n, Mr,\n\n⟨M\n\n⊕ ⟨\n\nP⟩\n\n⟩\n\n⟩\n\n⟨\n\n3\n\n¬\n\n̸ Under review as a conference paper at ICLR 2023\n\nAt the beginning of this section, we mention that satisfying the context of a call is a necessary but not sufficient condition to start the call. We now introduce a sufficient condition, called exit condition. Definition 4. Given an HRM H = tion ξi,u,Φ\n\nDNFP is the formula that must be satisfied to leave that hierarchy state. Formally,\n\n, the exit condi- ⟩\n\nMi, u, Φ, Γ ⟨\n\nand a hierarchy state\n\n, Mr,\n\n⟨M\n\nP⟩\n\n∈\n\nξi,u,Φ =\n\nΦ (cid:87)\n\n \n\n\n\nφ=φi(u,v,Mj ), φ̸=⊥,v∈Ui,Mj ∈M\n\nξj,u0\n\nj ,DNF(Φ∧φ)\n\n,\n\nif i = otherwise,\n\n⊤\n\nφ) is Φ\n\nφ in DNF. The formula is Φ if Mi = M⊤ since it always returns control where DNF(Φ once called. Otherwise, the formula is recursively defined as the disjunction of the exit conditions from the initial state of the called RM. For instance, the exit condition for the initial hierarchy state in Figure 1b is (\n\n∧\n\n∧\n\n)\n\n.\n\n¬ ∧\n\n∨\n\nWe now have everything needed to define the hierarchical transition function δH , which maps a hierarchy state\n\ninto another given a label\n\n. There are three cases:\n\nMi, u, Φ, Γ\n\n⟨\n\n⟩\n\nL\n\n1. If u is an accepting state of Mi and the stack Γ is non-empty, pop the top element of Γ and return control to the previous RM, recursively applying δH in case several accepting states Mj, u′, are reached simultaneously. Formally, the next hierarchy state is δH ( ) if ⟨\nu denotes a label that cannot satisfy any formula, and\n\ndenotes something unimportant for the case.\n\n> 0, where Γ = Γ′\n\n, u′, Mj, Mi,\n\n⊕ ⟨·\n\n, ·⟩\n\n∈ U\n\nΓ |\n\n, Γ′\n\ni ,\n\n, ⟩\n\n, ·\n\n⊥\n\n⊤\n\n⊥\n\nA\n\n|\n\n2. If\n\nL\n\nsatisfies the context of a call and the exit condition from the initial state of the called RM, push the call onto the stack and recursively apply δH until M⊤ is reached. Formally, the next hierarchy state is δH ( j ,Φ′, L | where φ = φi(u, u′, Mj)( ) denotes the disjuncts of a DNF formula φ\n\n⊕ ⟨ ) and Φ′ = DNF(Φ\n\nu, u′, Mi, Mj, φ, Φ\n\nMj, u0 ⟨\n\nDNFP satisfied by\n\nφ). Here, φ(\n\nj , Φ′, Γ\n\n= ξj,u0\n\n) if\n\n⟩⟩\n\nL\n\nL\n\nL\n\n∧\n\n,\n\n.\n\n3. If none of the previous conditions holds, the hierarchy state remains unchanged.\n\n·\n\n∈\n\nL\n\n⟩\n\n⟩\n\n¬\n\n⊤\n\ninstead of\n\nu, v, Mi ⟨\n\nsuch that either (i) v = v′ and i\n\n, then M1 and M2 could be both started if\n\nThe state transition functions φ of the RMs must be such that δH is deterministic, i.e. a label cannot simultaneously satisfy the contexts and exit conditions associated with two triplets and = v′. Contexts help enforce determinism u, v′, Mj = j, or (ii) v ⟨\nby making formulas mutually exclusive. For instance, if the call to M1 from the initial state of M0 in Figure 1b had context }\nwas observed, thus making the HRM non-deterministic. Finally, we introduce hierarchy traversals, which determine how a label trace is processed by an HRM using δH . Definition 5. Given a label trace λ = vn+1 ⟩\n(ii) δH (vi, ⊤\n(i.e., an accepting state of the root is reached). Analogously, H rejects λ if vn+1 = u\nExample 1. The HRM in Figure 1b accepts label trace λ = ,\n{ }⟩ { } M1, u1 M0, u1 1, 0, ,\nthe traversal is H(λ) = ⟨\n⟩ ⟨\nM2, u1 M0, u1 , [] 2, , [] 0, ⊤\n⟨ ⟩\n⟨ by-step application of the hierarchical transition function δH is shown in Appendix A.\n\n, a hierarchy traversal H(λ) = v0, v1, . . . , ⟨\n⟩ Mr, u0 , [] r, , and ⊤\n⟩ ⟨\nA and u , [] r\n∈ U ⟩\n, Mk, u, and ·⟩ ·\n\n⟨L is a unique sequence of hierarchy states such that (i) v0 = i) = vi+1 for i = 0, . . . , n. An HRM H accepts λ if vn+1 =\n\n1] (i.e., a rejecting state in the HRM is reached). ,\n\n, ,\n⟨{ } { } 0, u1 u0 0, M0, M1, ⟨\nM0, u3 , [] 0, ⟩\n⟨\n\nM0, u0 0, , [] ⟩\n⟨⟨ 0, u3 u1 0, M0, M2, , [ ⟨\n\nsince ,\n, [] ⊤\n⟩ . The step-\n\n, { } {} ,\n] ¬\n⊤⟩ M0, uA 0 ,\n\nL k for any k\n\n, [ ⊤\n, ]\n⟩ ⊤⟩\n\nMr, u,\n\n0, . . . ,\n\n[0, m\n\n∈ U\n\n⟩⟩\n\n⊤\n\n⊤\n\n−\n\n⊤\n\n⊤\n\n⊤\n\nL\n\n∈\n\n{\n\nR\n\n⟨\n\n⟨\n\n⟨\n\nn\n\n,\n\n,\n\n,\n\n,\n\n,\n\nThe behavior of an HRM H can be reproduced by an equivalent flat HRM ̄H; that is, (i) the root of ̄H has height 1 and, (ii) ̄H accepts a trace iff H accepts it, rejects a trace iff H rejects it, and neither accepts nor rejects a trace iff H does not accept it nor reject it. Flat HRMs thus capture the original RM definition. Figure 1c shows a flat HRM for the BOOK task. We formally define equivalence and prove the equivalence theorem below by construction in Appendix B.1. Theorem 1. Given an HRM H, there exists an equivalent flat HRM ̄H.\n\nGiven the construction used in Theorem 1, we show that the number of states and edges of the resulting flat HRM can be exponential in the height of the root (see Theorem 2). We prove this result in Appendix B.2 through an instance of a general HRM parametrization where the constituent RMs are highly reused, hence illustrating the convenience of HRMs to succinctly compose existing knowledge. In line with the theory, learning a non-flat HRM can take a few seconds, whereas learning an equivalent flat HRM is often unfeasible (see Section 6).\n\n4\n\n̸ ̸\nUnder review as a conference paper at ICLR 2023\n\nTheorem 2. Let H = of states and edges in an equivalent flat HRM ̄H can be exponential in hr.\n\nbe an HRM and let hr be the height of its root Mr. The number\n\n, Mr,\n\n⟨M\n\nP⟩\n\n4 POLICY LEARNING IN HIERARCHIES OF REWARD MACHINES\n\nIn what follows, we explain how to exploit the temporal structure of an HRM H = P⟩ using two types of options. We describe (i) how to learn the policies of these options, (ii) when these options terminate, and (iii) an option selection algorithm that ensures the currently running options and the current hierarchy state are aligned. We discuss implementation details in Appendix C.\n\n, Mr,\n\n⟨M\n\n, a state u\n\nTypes. Given an RM Mi non-false disjunct φ of each transition φi(u, v, Mj), where v (i) a formula option if j = attempts to reach a label that satisfies φ reach an accepting state of the called RM Mj under context φ\n\ni,u,Φ is derived for each . An option is either (i.e., M⊤ is called), or (ii) a call option otherwise. A formula option Φ through primitive actions, whereas a call option aims to\n\nΦ by invoking other options.\n\ni and a context Φ, an option ωj,φ\n\ni and Mj\n\n∈ M\n\n∈ M\n\n∈ U\n\n∈ U\n\n⊤\n\n∧\n\nPolicies. Policies are ε-greedy during training, and greedy during evaluation. A formula option’s policy is derived from a Q-function qφ∧Φ(s, a; θφ∧Φ) approximated by a deep Q-network (DQN; Mnih et al., 2015) with parameters θφ∧Φ, which outputs the Q-value of each action given an MDP state. We store all options’ experiences (st, a, st+1) in a single replay buffer , thus performing intra-option learning (Sutton et al., 1998). The Q-learning update uses the following loss function:\n\nD\n\n∧\n\nE(st,a,st+1)∼D\n\n(cid:20)(cid:16)\n\nrφ∧Φ(st+1) + γ max\n\na′\n\nqφ∧Φ(st+1, a′; θ−\n\nφ∧Φ)\n\n−\n\nqφ∧Φ(st, a; θφ∧Φ)\n\n(cid:17)2(cid:21)\n\n,\n\n(1)\n\nwhere rφ∧Φ(st+1) = 1[l(st+1) the term qφ∧Φ(st+1, a′; θ− and sG\n\n); and θ−\n\nt+1 =\n\n= φ |\nφ∧Φ) is 0 when φ\n\n∧\n\n∧\n\nΦ], i.e. the reward is 1 if φ\n\nΦ is satisfied and 0 otherwise;\n\n∧ Φ is satisfied or a dead-end is reached (i.e., sT\n\nt+1 =\n\n⊤\n\nφ∧Φ are the parameters of a fixed target network.\n\nA call option’s policy is induced by a Q-function qi(s, u, Φ, ; θi) associated with the called RM Mi and approximated by a DQN with parameters θi that outputs the Q-value of each call in the RM given an MDP state, an RM state and a context. We store experiences (st, ωj,φ i,u,Φ, st+k) in a replay buffer\n\ni associated with Mi, and perform SMDP Q-learning using the following loss:\n\nMj, φ ⟩\n\n⟨\n\nE\n\n(st,ωj,φ\n\ni,u,Φ,st+k)∼Di\n\n(cid:34)(cid:18)\n\nr + γk max j′,φ′\n\nqi(st+k, u′, Φ′,\n\nMj′, φ′ ⟨\n\n; θ− i ) ⟩\n\n−\n\nqi(st, u, Φ,\n\nMj, φ ⟩\n⟨\n\n; θi)\n\n(cid:19)2(cid:35)\n\n,\n\n⊥\n\nD\n\nφi(u′,\n\nif the hierarchy state changes; thus, Φ′ =\n\nwhere k is the number of steps between st and st+k; r is the sum of discounted rewards during this time; u′ and Φ′ are the RM state and context after running the option; Mj′ and φ′ correspond to an outgoing transition from u′, i.e. φ′ i are the parameters of a fixed target network. The term qi(st+k, . . .) is 0 if u′ is accepting or rejecting. Following the definition of δH , Φ′ is = u, and Φ′ = Φ otherwise. Following our assumption on the MDP reward, we define reward transition functions as ri(u, u′) = 1[u / ∈\ni ]. Learning a call option’s policy and lower-level option policies simultaneously can U\nbe unstable due to non-stationarity (Levy et al., 2019), e.g. the same lower-level option may only sometimes achieve its goal. To relax this problem, experiences are added to the buffer only when options achieve their goal (i.e., call options assume low-level options to terminate successfully). Due to the hierarchical structure, the policies will be recursively optimal (Dietterich, 2000) at best.\n\n, Mj′); and θ− ·\n\n⊤ u′\n\ni ∧\n\nif u′\n\n∈ U\n\n⊤\n\n∈\n\nA\n\nA\n\nTermination. An option terminates in two cases. First, if the episode ends in a goal state or in a dead-end state. Second, if the hierarchy state changes and either successfully completes the option or interrupts the option. Concretely, a formula option ωj,φ i,u,Φ is only applicable in a hierarchy state u, Mi, u, Φ, Γ . We ⟩\n⟨ ⟨\ncan thus analyze the hierarchy state to see if an option is still executing or should terminate.\n\n, while a call option ωj,φ ⟩\n\ni,u,Φ always corresponds to a stack item\n\n, Mi, Mj, φ, Φ ·\n\nAlgorithm. An option stack ΩH stores the currently executing options. Initially, ΩH is empty. At each step, ΩH is filled by repeatedly choosing options starting from the current hierarchy state using call option policies until a formula option is selected. Since HRMs have, by assumption, no circular dependencies, a formula option will eventually be chosen. After an action is selected using the\n\n5\n\n̸ Under review as a conference paper at ICLR 2023\n\nformula option’s policy and applied, the DQNs associated with formula options are updated. The new hierarchy state is then used to determine which options in ΩH have terminated. Experiences for the terminated options that achieved their goal are pushed into the corresponding buffers, and the DQNs associated with the call options are updated. Finally, ΩH is updated to match the call stack of the new hierarchy state (if needed) by mapping each call stack item into an option, and adding it to ΩH if it is not already there. By aligning the option stack with the call stack, we can update DQNs for options that ended up being run in hindsight and which would have been otherwise ignored.\n\n5 LEARNING HIERARCHIES OF REWARD MACHINES FROM TRACES\n\nIn the previous section, we explained how a given HRM can be exploited using options; however, engineering an HRM is impractical. We here describe LHRM, a method that interleaves policy learning with HRM learning from interaction. We consider a multi-task setting. Given T tasks and I instances (e.g., grids) of an environment, the agent learns (i) an HRM for each task using traces from several instances for better accuracy, and (ii) general policies to reach the goal in each task-instance pair. Namely, the agent interacts with T [1, I]. The learning proceeds from simpler to harder tasks such that HRMs for the latter build on the former.\n\nI MDPs Mij, where i\n\n[1, T ] and j\n\n×\n\n∈\n\n∈\n\nand actions\n\nIn what follows, we detail the components of LHRM. We assume that (i) all MDPs share propositions and labeling function , while those defined on a given instance share states l; (ii) to stabilize policy learning, dead-end traces must be common across tasks;1 (iii) the root’s height of a task’s HRM (or task level, for brevity) is known (see Table 1 for CRAFTWORLD); and (iv) without loss of generality, each RM has a single accepting state and a single rejecting state.\n\nA\n\nP\n\nS\n\n∈\n\n[1, T ] and j\n\nCurriculum Learning (Bengio et al., 2009). LHRM learns the tasks’ HRMs from lower to higher levels akin to Pierrot et al. (2019). Before starting an episode, LHRM selects an MDP Mij, where [1, I]. The probability of selecting an MDP Mij is determined by an estimate of i\nits average undiscounted return Rij such that lower returns are mapped into higher probabilities (see details in Appendix D). Initially, only level 1 MDPs can be chosen. When the minimum average return across MDPs up to the current level surpasses a given threshold, the current level increases by 1, hence ensuring the learned HRMs and their associated policies are reusable in higher level tasks.\n\n∈\n\nΛD\n\nLearning an HRM. The learning of an HRM is analogous to learning a flat RM (Toro Icarte et al., 2019; Xu et al., 2020; Furelos-Blanco et al., 2021; Hasanbeig et al., 2021). The objective is to learn the state transition function φr of the root Mr with height hr given (i) a set of states r, (ii) a set of ΛI , (iii) a set of propositions label traces Λ = ΛG with lower heights P\nthan hr, (v) a set of callable RMs ), and (vi) the maximum number (by default, M\nof disjuncts κ in the DNF formulas labeling the edges. The learned state transition function φr is accepts all goal traces ΛG, rejects all deadsuch that the resulting HRM H = , Mr, end traces ΛD, and neither accepts or rejects incomplete traces ΛI . The transition functions can be represented as sets of logic rules, which are learned using the ILASP (Law et al., 2015) inductive logic programming system (see Appendix E for details on the ILASP encoding).\n\n, (iv) a set of RMs C =\n\n⟨M ∪ {\n\n⊆ M\n\nMr\n\nM\n\nM\n\nM\n\nP⟩\n\n∪\n\n∪\n\nU\n\n}\n\nC\n\n∈\n\nInterleaving Algorithm. LHRM interleaves the induction of HRMs with policy learning akin to Furelos-Blanco et al. (2021). Initially, the HRM’s root of each task i [1, T ] consists of 3 states (the initial, accepting, and rejecting states) and neither accepts nor rejects anything. A new HRM is learned when an episode’s label trace is not correctly recognized by the current HRM (i.e., if a goal trace is not accepted, a dead-end trace is not rejected, or an incomplete trace is accepted or rejected). The number of states in r increases by 1 when an HRM that covers the examples cannot be learned, hence guaranteeing that the root has the smallest possible number of states (i.e., it is minimal) for a specific value of κ. When an HRM for task i is learned, the returns Rij in the curriculum are set to 0 for all j [1, I]. Analogously to some RM learning methods (Toro Icarte et al., 2019; Xu et al., 2020; Hasanbeig et al., 2021), the first HRM for a task is learned using a set of traces; in our case, the ρs shortest traces from a set of ρ goal traces are used (empirically, short traces speed up learning). Finally, LHRM leverages learned options to explore the environment during the collection of the ρ goal traces, speeding up the process when labels are sparse. Specifically, options from lower height RMs are sequentially selected uniformly at random, and their greedy policy is run until termination.\n\n∈\n\nU\n\n1The term qφ∧Φ(st+1, . . .) in Equation 1 is 0 if (sT\n\nt+1, sG\n\nt+1) = (⊤, ⊥). Since experiences (st, a, st+1) are\n\nshared through the replay buffer, evaluating the condition differently can produce instabilities.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n6 EXPERIMENTAL RESULTS\n\nWe evaluate the policy and HRM learning components of our approach using two domains described below. We report the average performances across 5 runs, each consisting of a different set of 10 random instances. Learning curves show the average undiscounted return obtained by the greedy policy every 100 episodes across instances. For other metrics (e.g., learning times), we present the average and the standard error, with the latter in brackets. In HRM learning experiments, we set a 2-hour timeout to learn the HRMs. See Appendix F for experimental details and extended results.\n\n×\n\nDomains. We consider four grid types for the CRAFTWORLD domain introduced in Section 3: an 7 grid with a lava location (OPL), a 13 open plan 7 7 grid (OP, Figure 1a), an open plan 7 13 four rooms grid (FR; Sutton et al., 1999), and a 13 13 four rooms grid with a lava location per room (FRL). The lava proposition must always be avoided. WATERWORLD (Karpathy, 2015; Sidor, 2016; Toro Icarte et al., 2018) consists of a 2D box containing 12 balls of 6 different colors (2 per color) each moving at a constant speed in a fixed direction. The agent ball can change its velocity in any cardinal direction. The propositions are the balls’ colors. Labels consist of the color of the balls the agent overlaps with and, unlike CRAFTWORLD, they may contain multiple propositions. The tasks consist in observing color sequences. We consider two settings: without dead-ends (WOD) and with dead-ends (WD). In WD, the agent must avoid 2 balls of an extra color.\n\nr, g, b, c, m, y\n\n× ×\n\n=\n\n×\n\nP\n\n{\n\n}\n\nPolicy Learning in Handcrafted HRMs. We compare the performance of policy learning in handcrafted non-flat HRMs against in flat equivalents. Remember that an equivalent flat HRM always exists for any HRM (see Theorem 1). For fairness, the flat HRMs are minimal. Figure 2 shows the learning curves for some CRAFTWORLD tasks in the FRL setting. The convergence rate is similar in the simplest task (MILKBUCKET), but higher for non-flat HRMs in the hardest ones. As both approaches use the same set of formula option policies, differences arise from the lack of modularity in flat HRMs. Call options, which are not present in flat HRMs, constitute independent modules that help reduce reward sparsity. MILKBUCKET involves less high-level steps than BOOKQUILL and CAKE, thus reward is less sparse and non-flat HRMs are not as beneficial. The effectiveness of non-flat HRMs is also limited when (i) the task’s goal is reachable regardless of the chosen options (e.g., if the are no edges to rejecting states, like in OP and FR), and (ii) the reward is not too sparse, like in OPL (the grid is small) or WATERWORLD (the balls can easily get near the agent).\n\nFigure 2: Learning curves for three CRAFTWORLD (FRL) tasks using handcrafted HRMs.\n\nLearning of Non-Flat HRMs. Figure 3 shows the LHRM learning curves for CRAFTWORLD (FRL) and WATERWORLD (WD). These settings are the most challenging due to the inclusion of dead-ends since (i) they hinder the observation of goal examples in level 1 tasks using random walks, (ii) the RMs must include rejecting states, (iii) formula options must avoid dead-ends, and (iv) call options must avoid invoking options leading to rejecting states. In line with the curriculum method, LHRM does not start learning a task of a given level until tasks in previous levels are mastered. The convergence for high-level tasks is often fast due to the reuse of lower level HRMs and policies.\n\nThe average time (in seconds) exclusively spent on learning all HRMs is 1009.8 (122.3) for OP, 1622.6 (328.7) for OPL, 1031.6 (150.3) for FR, 1476.8 (175.3) for FRL, 35.4 (2.0) for WOD, and 67.0 (6.2) for WD. Including dead-ends (OPL, FRL, WD) incurs longer executions since (i) there is one more proposition, (ii) there are edges to the rejecting state(s), and (iii) there are dead-end traces to cover. We observe that the complexity of learning an HRM does not necessarily correspond with the task complexity (e.g., the times for OP and FRL are similar). Learning in WATERWORLD is faster than in CRAFTWORLD since the RMs have fewer states and there are fewer callable RMs.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nRG [r ; g] BC [b ; c] MY [m ; y] ————————– RG&BC [RG & BC] BC&MY [BC & MY] RG&MY [RG & MY] RGB [RG ; b] CMY [c ; MY] ————————– RGB&CMY [RGB & CMY]\n\nFigure 3: LHRM learning curves for CRAFTWORLD (FRL) and WATERWORLD (WD). The legend in WATERWORLD separates tasks by level, and the subtask order (in brackets) follows that introduced in Table 1. The dotted vertical lines correspond to episodes in which an HRM is learned.\n\nBy restricting the callable RMs to those required by the HRM (e.g., using just PAPER and LEATHER RMs to learn BOOK’s HRM), there are fewer ways to label the edges of the induced root. Learning is 5-7× faster using 20% fewer calls to the learner (i.e., fewer examples) in CRAFTWORLD, and 1.5× faster in WATERWORLD; hence, HRM learning becomes less scalable as the number of tasks and levels grows. This is an instance of the utility problem (Minton, 1988). Refining the callable RMs set ‘a priori’ to speed up HRM learning is a direction for future work.\n\nWe evaluate the performance of exploration with options using the number of episodes needed to collect the ρ goal traces for a given task since the activation of its level. Intuitively, the agent will rarely move far from a region of the state space using primitive actions only, thus taking more time to collect the traces; in contrast, options enable the agent to explore the state space more efficiently. In the FRL setting of CRAFTWORLD, we observe that using primitive actions requires 128.1× more episodes than options in MILKBUCKET, the only level 2 task for which ρ traces are collected (although in just 2/5 runs). Likewise, primitive actions take 20.8× and 7.7× more episodes in OPL and WD respectively. In OP and WOD options are not beneficial since episodes are relatively long (1,000 steps), there are no dead-ends and it is easy to observe the different propositions.\n\nFinally, we observe that using a single goal trace to learn the first HRMs (ρ = ρs = 1) incurs timeouts across all CRAFTWORLD settings, thus showing the value of using many short traces instead.\n\nLearning Flat HRMs. Learning a flat HRM is often less scalable than learning a non-flat equivalent since (i) previously learned HRMs cannot be reused, and (ii) the flat HRM usually has more states and edges (as shown in Theorem 2, the growth can be exponential). We compare the performance of learning (from interaction) a non-flat HRM using LHRM with that of an equivalent flat HRM using LHRM, DeepSynth (Hasanbeig et al., 2021), LRM (Toro Icarte et al., 2019) and JIRP (Xu et al., 2020). Akin to LHRM, JIRP induces RMs with explicit accepting states, while DeepSynth and LRM do not. We use OP and WOD instances for CRAFTWORLD and WATERWORLD respectively.\n\nThe non-flat HRM for MILKBUCKET is learned in 1.5 (0.2) seconds, whereas the flat HRMs take longer to learn: 3.2 (0.6) w/LHRM, 325.6 (29.7) w/DeepSynth, 347.5 (64.5) w/LRM and 17.1 (5.5) w/JIRP. LHRM and JIRP learn minimal RMs, hence producing the same RM consisting of 4 states and 3 edges. DeepSynth and LRM do not learn a minimal RM but one that is good at predicting the next possible label given the current one. In domains like ours where propositions can be observed anytime (i.e., without temporal dependencies between them), these methods tend to ‘overfit’ the input traces and produce large outputs that barely reflect the task’s structure, e.g. DeepSynth learns RMs with 13.4 (0.4) states and 93.2 (1.7) edges. In contrast, methods learning minimal machines exclusively from observable traces may suffer from overgeneralization (Angluin, 1980) in other domains (e.g., with temporally-dependent propositions). In more complex tasks such as BOOK, LHRM learns the non-flat HRM (see Figure 1b) in 191.2 (36.4) seconds, whereas methods learning the flat HRM (see Figure 1c) usually time out or, in the case of DeepSynth, learn bigger representations.\n\nThe performance of DeepSynth, LRM and JIRP is poor in WATERWORLD since they all learn RMs whose edges are labeled with proposition sets instead of formulas, unlike LHRM; thus, the RMs may require exponentially more edges, motivating the use of formulas for abstraction. For instance, the flat HRM for RG requires 64 edges instead of 2, and only LHRM and JIRP can learn it on time. All flat HRM learners time out in RG&BC, whereas the non-flat HRM is learned in 4.5 (0.3) seconds.\n\n8\n\n0.010.020.030.0Numberofepisodes×1040.00.20.40.60.81.0Averagereturn0.010.020.030.0Numberofepisodes×1040.00.20.40.60.81.0AveragereturnUnder review as a conference paper at ICLR 2023\n\n7 RELATED WORK\n\nRMs and Composability. Our RMs differ from the original ones (Toro Icarte et al., 2018; 2022) in that (i) an RM can call other RMs, (ii) there are explicit accepting and rejecting states (Xu et al., 2020; Furelos-Blanco et al., 2021), and (iii) transitions are labeled with propositional logic formulas instead of proposition sets (Furelos-Blanco et al., 2021). Recent works derive RMs (and similar FSMs) from formal language specifications (Camacho et al., 2019; Araki et al., 2021) and expert demonstrations (Camacho et al., 2021), or learn them from experience using discrete optimization (Toro Icarte et al., 2019), SAT solving (Xu et al., 2020), active learning (Gaon & Brafman, 2020; Xu et al., 2021), state-merging (Xu et al., 2019; Gaon & Brafman, 2020), program synthesis (Hasanbeig et al., 2021) or inductive logic programming (Furelos-Blanco et al., 2021). Prior ways of composing RMs include (i) merging the state and reward transition functions (De Giacomo et al., 2020), and (ii) encoding a multi-agent task using an RM, decomposing it into one RM per agent and executing them in parallel (Neary et al., 2021). Task composability has also been modeled using subtask sequences called sketches (Andreas et al., 2017), context-free grammars defining a subset of English (Chevalier-Boisvert et al., 2019), formal languages (Jothimurugan et al., 2019; Le ́on et al., 2020; Wang et al., 2020) and logic-based algebras (Nangue Tasse et al., 2020).\n\nHierarchical RL. Our method for exploiting HRMs resembles a hierarchy of DQNs (Kulkarni et al., 2016). Akin to option discovery methods, LHRM induces a set of options from experience. LHRM requires a set of propositions and tasks, which bound the number of discoverable options; similarly, some of these methods impose an explicit bound (Bacon et al., 2017; Machado et al., 2017). LHRM requires each task to be solved at least once before learning an HRM (and, hence, options), just like other methods (McGovern & Barto, 2001; Stolle & Precup, 2002). The problem of discovering options for exploration has been considered before (Bellemare et al., 2016; Machado et al., 2017; Jinnai et al., 2019; Dabney et al., 2021). While our options are not explicitly discovered for exploration, we leverage them to find goal traces in new tasks. Levy et al. (2019) learn policies from multiple hierarchical levels in parallel by training each level as if the lower levels were optimal; likewise, we train call option policies from experiences where invoked options achieve their goal.\n\nHRMs are close to hierarchical abstract machines (HAMs; Parr & Russell, 1997) since both are hierarchies of FSMs; however, there are two core differences. First, HAMs do not have reward transition functions. Second, (H)RMs decouple the traversal from the policies, i.e. independently of the agent’s choices, the (H)RM is followed; thus, an agent exploiting an (H)RM must be able to interrupt its choices (see Section 4). While HAMs do not support interruption, Programmable HAMs (Andre & Russell, 2000) extend HAMs to support it along with other program-like features. Despite the resemblance, there are few works on learning HAMs (Leonetti et al., 2012) and there are many on learning RMs, hence showing (H)RMs are more amenable (yet expressive) to learning.\n\nCurriculum Learning. Pierrot et al. (2019) learn hierarchies of neural programs given the level of each program, akin to our RMs’ height; likewise, Andreas et al. (2017) prioritize tasks consisting of fewer high-level steps. The ‘online’ method by Matiisen et al. (2020) also keeps an estimate of each task’s average return, but it is not applied in an HRL scenario. Wang et al. (2020) learn increasingly complex temporal logic formulas leveraging previously learned formulas using a set of templates.\n\n8 CONCLUSIONS\n\nWe have here proposed (1) HRMs, a formalism that composes RMs in a hierarchy by enabling them to call each other, (2) an HRL method that exploits the structure of an HRM, and (3) a curriculumbased method for learning a collection of HRMs from traces. Non-flat HRMs have significant advantages over their flat equivalents. Theoretically, we have proved that the flat equivalent of a given HRM can have exponentially more states and edges. Empirically, we have shown that (i) our HRL method converges faster given a non-flat HRM instead of a flat equivalent one, and (ii) in line with the theory, learning an HRM is feasible in cases where its flat equivalent is not.\n\nLHRM assumes that the proposition set is known, dead-end indicators are shared across tasks, there is a fixed set of tasks and the height for each HRM is provided. Relaxing these assumptions by forming the propositions from raw data, conditioning policies to dead-ends, and letting the agent propose its own composable tasks are promising directions for future work. Other interesting extensions include non-episodic settings and methods for learning globally optimal policies over HRMs.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY\n\nTo make our work more understandable and reproducible, we provide pseudo-code, proofs and examples throughout the paper. We here outline the content covered in the appendices that help with reproducibility:\n\nAppendix A All the intermediate steps for the hierarchy traversal example in Section 3.\n\nAppendix B Proofs for Theorems 1 and 2.\n\nAppendix C The implementation details (including pseudo-code) for the policy learning algorithm outlined in Section 4. We include running examples of the algorithm to aid understanding.\n\nAppendix D The implementation details for the curriculum learning mechanism introduced in Sec-\n\ntion 5.\n\nAppendix E How the root of an HRM is learned using the ILASP inductive logic programming\n\nsystem, including proofs of correctness.\n\nAppendix F Details on the experiments described in Section 6, such as computational resources, implementation of the domains, training and evaluation details (e.g., hyperparameters) and extended results (e.g., tables on which the results in the main paper are based).\n\nAppendix G Handcrafted HRMs for the tasks used in the experiments.\n\nWe plan to release the code if the paper is accepted.\n\nREFERENCES\n\nDavid Andre and Stuart J. Russell. Programmable Reinforcement Learning Agents. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 1019– 1025, 2000.\n\nJacob Andreas, Dan Klein, and Sergey Levine. Modular Multitask Reinforcement Learning with Policy Sketches. In Proceedings of the International Conference on Machine Learning (ICML), pp. 166–175, 2017.\n\nDana Angluin. Inductive Inference of Formal Languages from Positive Data. Inf. Control., 45(2):\n\n117–135, 1980.\n\nBrandon Araki, Xiao Li, Kiran Vodrahalli, Jonathan A. DeCastro, Micah J. Fry, and Daniela Rus. The Logical Options Framework. In Proceedings of the International Conference on Machine Learning (ICML), pp. 307–317, 2021.\n\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The Option-Critic Architecture. In Proceedings of\n\nthe AAAI Conference on Artificial Intelligence (AAAI), pp. 1726–1734, 2017.\n\nAndrew G. Barto and Sridhar Mahadevan. Recent Advances in Hierarchical Reinforcement Learn-\n\ning. Discrete Event Dynamic Systems, 13(4):341–379, 2003.\n\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R ́emi Munos. Unifying Count-Based Exploration and Intrinsic Motivation. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 1471–1479, 2016.\n\nYoshua Bengio, J ́erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 41–48, 2009.\n\nAlberto Camacho, Rodrigo Toro Icarte, Toryn Q. Klassen, Richard Anthony Valenzano, and Sheila A. McIlraith. LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 6065–6073, 2019.\n\nAlberto Camacho, Jacob Varley, Andy Zeng, Deepali Jain, Atil Iscen, and Dmitry Kalashnikov. Reward Machines for Vision-Based Robotic Manipulation. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pp. 14284–14290, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic Gridworld Environment\n\nfor OpenAI Gym. https://github.com/maximecb/gym-minigrid, 2018.\n\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\n\nWill Dabney, Georg Ostrovski, and Andr ́e Barreto. Temporally-Extended ε-Greedy Exploration. In\n\nProceedings of the International Conference on Learning Representations (ICLR), 2021.\n\nGiuseppe De Giacomo, Marco Favorito, Luca Iocchi, Fabio Patrizi, and Alessandro Ronca. Temporal Logic Monitoring Rewards via Transducers. In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning (KR), pp. 860–870, 2020.\n\nThomas G. Dietterich. Hierarchical Reinforcement Learning with the MAXQ Value Function De-\n\ncomposition. J. Artif. Intell. Res., 13:227–303, 2000.\n\nThomas G. Dietterich, Pedro M. Domingos, Lise Getoor, Stephen Muggleton, and Prasad Tadepalli.\n\nStructured machine learning: the next ten years. Mach. Learn., 73(1):3–23, 2008.\n\nThomas Eiter and Georg Gottlob. On the Computational Cost of Disjunctive Logic Programming:\n\nPropositional Case. Ann. Math. Artif. Intell., 15(3-4):289–323, 1995.\n\nDaniel Furelos-Blanco, Mark Law, Anders Jonsson, Krysia Broda, and Alessandra Russo. Induction and Exploitation of Subgoal Automata for Reinforcement Learning. J. Artif. Intell. Res., 70:1031– 1116, 2021.\n\nMaor Gaon and Ronen I. Brafman. Reinforcement Learning with Non-Markovian Rewards. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 3980–3987, 2020.\n\nIn\n\nMichael Gelfond and Yulia Kahl. Knowledge Representation, Reasoning, and the Design of Intelli-\n\ngent Agents: The Answer-Set Programming Approach. Cambridge University Press, 2014.\n\nMichael Gelfond and Vladimir Lifschitz. The Stable Model Semantics for Logic Programming. In Proceedings of the International Conference and Symposium on Logic Programming, pp. 1070– 1080, 1988.\n\nMohammadhosein Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel Kroening. DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 7647–7656, 2021.\n\nGeoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural Networks for Machine Learning -\n\nLecture 6a: Overview of Mini-Batch Gradient Descent. https://www.cs.toronto.edu/ ̃tijmen/csc321/slides/lecture_slides_ lec6.pdf, 2012.\n\nMaximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin, and Katja Hofmann. Generalization in Reinforcement Learning with Selective Noise Injection In Proceedings of the Annual Conference on Neural Information and Information Bottleneck. Processing Systems (NeurIPS), pp. 13956–13968, 2019.\n\nMinqi Jiang, Edward Grefenstette, and Tim Rockt ̈aschel. Prioritized Level Replay. In Proceedings\n\nof the International Conference on Machine Learning (ICML), pp. 4940–4950, 2021.\n\nYuu Jinnai, Jee Won Park, David Abel, and George Dimitri Konidaris. Discovering Options for Exploration by Minimizing Cover Time. In Proceedings of the International Conference on Machine Learning (ICML), pp. 3130–3139, 2019.\n\nKishor Jothimurugan, Rajeev Alur, and Osbert Bastani. A Composable Specification Language for Reinforcement Learning Tasks. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 13021–13030, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAndrej Karpathy. REINFORCEjs: WaterWorld demo.\n\nhttp://cs.stanford.edu/people/karpathy/reinforcejs/waterworld. html, 2015.\n\nTejas D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical Deep In ProReinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. ceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 3675–3683, 2016.\n\nMark Law. Inductive Learning of Answer Set Programs. PhD thesis, Imperial College London, UK,\n\n2018.\n\nMark Law, Alessandra Russo, and Krysia Broda. The ILASP System for Learning Answer Set\n\nPrograms, 2015. URL https://www.doc.ic.ac.uk/ ̃ml1909/ILASP.\n\nMark Law, Alessandra Russo, and Krysia Broda. Iterative Learning of Answer Set Programs from\n\nContext Dependent Examples. Theory Pract. Log. Program., 16(5-6):834–848, 2016.\n\nMark Law, Alessandra Russo, and Krysia Broda. The Meta-program Injection Feature in ILASP. Technical report, Imperial College London, June 2018. URL https://www.doc.ic.ac. uk/ ̃ml1909/ILASP/inject.pdf.\n\nBorja G. Le ́on, Murray Shanahan, and Francesco Belardinelli. Systematic Generalisation through\n\nTask Temporal Logic and Deep Reinforcement Learning. CoRR, abs/2006.08767, 2020.\n\nMatteo Leonetti, Luca Iocchi, and Fabio Patrizi. Automatic Generation and Learning of Finite-State Controllers. In Proceedings of the International Conference on Artificial Intelligence: Methodology, Systems, Applications (AIMSA), pp. 135–144, 2012.\n\nAndrew Levy, George Dimitri Konidaris, Robert Platt Jr., and Kate Saenko. Learning Multi-Level Hierarchies with Hindsight. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\n\nMarlos C. Machado, Marc G. Bellemare, and Michael H. Bowling. A Laplacian Framework for Option Discovery in Reinforcement Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 2295–2304, 2017.\n\nTambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-Student Curriculum\n\nLearning. IEEE Trans. Neural Networks Learn. Syst., 31(9):3732–3740, 2020.\n\nAmy McGovern and Andrew G. Barto. Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density. In Proceedings of the International Conference on Machine Learning (ICML), pp. 361–368, 2001.\n\nSteven Minton. Quantitative Results Concerning the Utility of Explanation-Based Learning. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 564–569, 1988.\n\nIn\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n\nGeraud Nangue Tasse, Steven D. James, and Benjamin Rosman. A Boolean Task Algebra for Reinforcement Learning. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 9497–9507, 2020.\n\nCyrus Neary, Zhe Xu, Bo Wu, and Ufuk Topcu. Reward Machines for Cooperative Multi-Agent Reinforcement Learning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 934–942, 2021.\n\nRonald Parr and Stuart J. Russell. Reinforcement Learning with Hierarchies of Machines. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 1043–1049, 1997.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nThomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de Freitas. Learning Compositional Neural Programs with In Proceedings of the Advances in Neural Information Recursive Tree Search and Planning. Processing Systems (NeurIPS) Conference, pp. 14646–14656, 2019.\n\nSzymon Sidor. Reinforcement Learning with Natural Language Signals. Master’s thesis, Mas-\n\nsachusetts Institute of Technology, 2016.\n\nMichael Sipser. Introduction to the Theory of Computation. PWS Publishing Company, 1997. ISBN\n\n978-0-534-94728-6.\n\nMartin Stolle and Doina Precup. Learning Options in Reinforcement Learning.\n\nIn Proceedings of the International Symposium on Abstraction, Reformulation and Approximation (SARA), pp. 212–223, 2002.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,\n\n2018.\n\nRichard S. Sutton, Doina Precup, and Satinder P. Singh. Intra-Option Learning about Temporally Abstract Actions. In Proceedings of the International Conference on Machine Learning (ICML), pp. 556–564, 1998.\n\nRichard S. Sutton, Doina Precup, and Satinder P. Singh. Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. Artif. Intell., 112(1-2):181–211, 1999.\n\nRodrigo Toro Icarte, Toryn Q. Klassen, Richard Anthony Valenzano, and Sheila A. McIlraith. Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement In Proceedings of the International Conference on Machine Learning (ICML), pp. Learning. 2112–2121, 2018.\n\nRodrigo Toro Icarte, Ethan Waldie, Toryn Q. Klassen, Richard Anthony Valenzano, Margarita P. Castro, and Sheila A. McIlraith. Learning Reward Machines for Partially Observable Reinforcement Learning. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) Conference, pp. 15497–15508, 2019.\n\nRodrigo Toro Icarte, Toryn Q. Klassen, Richard Valenzano, and Sheila A. McIlraith. Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning. J. Artif. Intell. Res., 73:173–208, 2022.\n\nHado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double Q-Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 2094– 2100, 2016.\n\nGuan Wang, Carl Trimbach, Jun Ki Lee, Mark K. Ho, and Michael L. Littman. Teaching a Robot Tasks of Arbitrary Complexity via Human Feedback. In Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 649–657, 2020.\n\nZhe Xu, Ivan Gavran, Yousef Ahmad, Rupak Majumdar, Daniel Neider, Ufuk Topcu, and Bo Wu. Joint Inference of Reward Machines and Policies for Reinforcement Learning. CoRR, abs/1909.05912, 2019.\n\nZhe Xu, Ivan Gavran, Yousef Ahmad, Rupak Majumdar, Daniel Neider, Ufuk Topcu, and Bo Wu. Joint Inference of Reward Machines and Policies for Reinforcement Learning. In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS), pp. 590–598, 2020.\n\nZhe Xu, Bo Wu, Aditya Ojha, Daniel Neider, and Ufuk Topcu. Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. In Proceedings of the International Cross-Domain Conference for Machine Learning and Knowledge Extraction (CD-MAKE), pp. 115–135, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA HIERARCHY TRAVERSAL EXAMPLE\n\nThe HRM in Figure 1b accepts trace λ = H(λ) =\n\n, where:\n\nv0, v1, v2, v3, v4, v5, v6 ⟨\nM0, u0 v0 = 0, ⟨\nv1 = δH (v0,\n\n,\n\n⟩\n\n, ⟨{ }\n\n, { }\n\n,\n\n, { }\n\n, { }\n\n{}\n\n{ }⟩\n\n, whose traversal is\n\n1, u1\n\n1, M1, M⊤,\n\n,\n\n] ¬ ⟩\n\n, ⟩\n\n)\n\n{ }\n\n, , [] ⟩\n, [ ⟨\n\n¬ ∧ u0 , [ ⟨\n⊤ 0, u1 u0 ⟨\n\n) { } 0, u1 u0 , [ 0, u1 0, M0, M1,\n\n0, M0, M1, 0, u1 u0 ⟨\n0, M0, M1,\n\n¬ ,\n\n¬\n\n,\n\n] ⟩\n⊤⟩\n\n¬ 0, M0, M1,\n\n,\n\n,\n\n¬ ]\n⊤⟩ ⟩\n, ⟩\n\n] ⊤⟩\n\n,\n\n,\n\n) { } u0 ,\n⊤⟩ ⟨\n)\n\n⊥\n\n0, M0, M1, ¬\n0, M0, M1, 0, M0, M1,\n\n¬\n\n¬\n\n,\n\n,\n\n] ⟩\n⊤⟩ ,\n, ⊤⟩ ]\n⟩ ⊤⟩\n\n) { } 1, uA u1 ⟨\n, )\n⊥\n\n,\n\n1 , M1, M⊤,\n\n⊤\n\n⊤ ,\n\n⊤ ,\n\n⊤\n\n⊤\n\n⊤\n\n⊤\n\n=\n\n⊤\n\n=\n\n=\n\n, [] ⟩\n) { } M0, u0 = δH ( 0, ⟨\nM1, u0 = δH ( 1, ⟨\n¬ M⊤, u0 = δH ( ⊤, ⟨\nM1, u1 1, = δH ( ⟨\nM1, u1 , [ 1, ⟨\nv2 = δH (v1, )\n{ } M1, u1 = δH ( 1, ⟨\nM⊤, u0 = δH ( ⊤, ⟨\nM1, uA = δH ( 1 , ⟨\nM0, u1 0, = δH ( ⟨\nM0, u1 , [] 0, ⟩\n⟨ v3 = δH (v2,\n\n⊤ )\n{} M0, u1 0, = δH ( ⟨\nM0, u1 , [] 0, ⟩\n⟨ v4 = δH (v3, )\n{ } M0, u1 = δH ( 0, ⟨\nM2, u0 = δH ( 2, ⟨\nM⊤, u0 = δH ( ⊤, ⟨\nM2, u1 2, = δH ( ⟨\nM2, u1 , [ 2, ⟨\nv5 = δH (v4, )\n{ } M2, u1 = δH ( 2, ⟨\nM⊤, u0 = δH ( ⊤, ⟨\nM2, uA = δH ( 2 , ⟨\nM0, u3 0, = δH ( ⟨\nM0, u3 , [] 0, ⊤\n⟨ ⟩\nv6 = δH (v5, )\n{ } M0, u3 0, M⊤, u0 ⊤, M0, uA 0 , , [] ⟩\n\n= δH ( ⟨\n= δH ( ⟨\n= δH ( ⟨\nM0, uA 0 , ⟨\n\n⊤\n\n⊤\n\n=\n\n=\n\n=\n\n⊤\n\n⊤\n\n⊤\n\n⊤ ,\n\n⊤ .\n\n0, u1 u0 , [ ⟨\n0, u1 u0 , [ ⟨\n0, u1 u0 ⟨\n, , [] )\n⟩\n\n⊥\n\n, [\n\n, , [] ⟩\n\n)\n\n{}\n\n0, u3 u1 , [ ⟨\nu1 0, u3 , [ ⟨\n0, u3 u1 ⟨\n, , [] )\n⟩\n\n⊥\n\n, [\n\n, , [] )\n{ } ⟩\n0, uA u3 , [ ⟨\n, , [] )\n⟩\n\n⊥\n\n,\n\n⊤\n\n)\n\n⊤\n\n⊤\n\n, , [] { } ⟩\nu1 0, u3 , [ ⟨\n0, u3 u1 , [ ⟨\n0, u3 u1 , [ ⊤\n⟨ 0, u3 u1 0, M0, M2, ⟨\n\n0, M0, M2, 0, M0, M2, 0, M0, M2, ,\n⊤\n\n⊤ ,\n\n⊤\n\n] ⊤⟩\n\n] ⊤⟩ ⟩\n, ,\n⊤⟩ ]\n⊤⟩ ⟩\n, ⟩\n\n,\n\n) { } 2, u1 u0 ⟨\n, )\n\n⊥\n\n2, M2, M⊤,\n\n,\n\n⊤\n\n0, M0, M2, 0, M0, M2, 0, M0, M2,\n\n⊤ ,\n\n⊤\n\n,\n\n] ⟩\n⊤⟩ ,\n, ⊤⟩ ]\n⟩ ⊤⟩\n\n) { } 2, uA u1 ⟨\n, )\n⊥\n\n2 , M2, M⊤,\n\n0 , M0, M⊤,\n\n,\n\n,\n\n] ⟩\n⊤⟩\n\n) { }\n\n,\n\n,\n\n] ⟩\n⊤⟩\n\n) { }\n\n,\n\n,\n\n] ⟩\n⊤⟩\n\n) { }\n\n,\n\n,\n\n] ⟩\n⊤⟩\n\n) { }\n\nExample 1 shows the traversal but omits the intermediate applications of the hierarchical transition function δH .\n\nB EQUIVALENCE TO FLAT HIERARCHIES OF REWARD MACHINES\n\nIn this section, we prove the theorems introduced in Section 3 regarding the equivalence of an arbitrary HRM to a flat HRM.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB.1 PROOF OF THEOREM 1\n\nWe formally show that any HRM can be transformed into an equivalent one consisting of a single non-leaf RM. The latter HRM type is called flat since there is a single hierarchy level.\n\nDefinition 6. Given an HRM H = is 1.\n\n, Mr,\n\n⟨M\n\nP⟩\n\n, a constituent RM Mi\n\nis flat if its height hi\n\n∈ M\n\nDefinition 7. An HRM H =\n\n, Mr,\n\nP⟩\n\n⟨M\n\nis flat if the root RM Mr is flat.\n\nWe now define what it means for two HRMs to be equivalent. This definition is based on that used in automaton theory (Sipser, 1997).\n\nDefinition 8. Given a set of propositions and H ′ = ′, M ′ r, (i) both HRMs accept λ, (ii) both HRMs reject λ, or (iii) neither of the HRMs accepts or rejects λ.\n\nP⟩ are equivalent if for any label trace λ one of the following conditions holds:\n\nand a labeling function l, two HRMs H =\n\n, Mr,\n\n⟨M\n\n⟨M\n\nP⟩\n\nP\n\nWe now have all the required definitions to prove Theorem 1, which is restated below. Theorem 1. Given an HRM H, there exists an equivalent flat HRM ̄H.\n\nTo prove the theorem, we introduce an algorithm for flattening any HRM. Without loss of generality, we work on the case of an HRM with two hierarchy levels; that is, an HRM consisting of a root RM that calls flat RMs. Note that an HRM with an arbitrary number of levels can be flattened by considering the RMs in two levels at a time. We start flattening RMs in the second level (i.e., with height 2), which use RMs in the first level (by definition, these are already flat), and once the second level RMs are flat, we repeat the process with the levels above until the root is reached. This process is applicable since, by assumption, the hierarchies do not have cyclic dependencies nor recursion. For simplicity, we use the MDP reward assumption made in Section 2, i.e. the reward transition function of any RM Mi is ri(u, u′) = 1[u / i ] like in Section 4. However, the proof ∈ U below could be adapted to arbitrary definitions of ri(u, u′).\n\ni ∧\n\n∈ U\n\nu′\n\nA\n\nA\n\nPreliminary Transformation Algorithm. Before proving Theorem 1, we introduce an intermediate step that transforms a flat HRM into an equivalent one that takes contexts with which it may be called into account. Remember that a call to an RM is associated with a context. In the case of two-level HRMs such as the ones we are considering in this flattening process, the context and the exit condition from the called flat RM must be satisfied. Crucially, the context must only be satisfied at the time of the call; that is, it only lasts for a single transition. Therefore, if we revisit the initial state of the called RM by taking an edge to it, the context should not be checked anymore.\n\nTo make the need for this transformation clearer, we use the HRM illustrated in Figure 4a. The flattening algorithm described later embeds the called RM into the caller one; crucially, the context of the call is taken into account by putting it in conjunction with the outgoing edges from the initial state of the called RM.2 Figure 4b is a flat HRM obtained using the flattening algorithm; however, it does not behave like the HRM in Figure 4a. Following the definition of the hierarchical transition function δH , the context of a call only lasts for a single transition in the called RM in Figure 4a (i.e., a c is only checked when M1 is started), but the context is kept permanently in Figure 4b, which is problematic if we go back to the initial state at some point. We later come back to this example after presenting the transformation algorithm.\n\n∧ ¬\n\nTo deal with the situation above, we need to transform an RM to ensure that contexts are only checked once from the initial state. We describe this transformation as follows. Given a flat HRM , φr, rr, u0 , we construct a new HRM H ′ = r, r, H = P\n⟨U r, u0 r, r′ , φ′ r,\n\nwith root Mr = r =\n\nr ⟩ such that:\n\nwith root M ′\n\n⟨M ′, M ′ r,\n\n, Mr,\n\nr ,\n\nr ,\n\nP⟩\n\nr,\n\nU\n\nR\n\nA\n\nA\n\n′\n\nˆu0 •\nr} • The state transition function φ′\n\n, where ˆu0\n\n∪ {\n\nU\n\nU\n\nr\n\n⟨U\n\nU\n\nU\n\nP r plays the role of the initial state after the first transition is taken. r is built by copying φr and applying the following changes: r, M⊤) =\n\nr(v, u0\n\n′\n\n1. Remove the edges to the actual initial state from any state v\n\nr: φ′ . Note that since the RM is flat, the only callable RM is the leaf M⊤.\n\n∈ U\n\n⟨M\n\nP⟩ ′\nr =\n\nU R\nr ⟩\n\n⊥\n\n2. Add edges to the dummy initial state ˆu0\n\nr from all states v\n\nactual initial state: φ′\n\nr(v, ˆu0\n\nr, M⊤) = φr(v, u0\n\nr, M⊤).\n\n′\n\nr that had an edge to the\n\n∈ U\n\n2We refer the reader to the ‘Flattening Algorithm’ description introduced later for specific details.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nu0 0\n\nu0 1\n\nM1\n\nc\n\n| ¬\n\nM⊤\n\na\n\n|\n\nM⊤\n\nb\n\n|\n\nuA 0\n\nM0\n\nc\n\nb\n\n∧ ¬\n\nuA 1\n\nu1 1\n\nM⊤\n\n| M1\n\nu0 0\n\nM⊤ | a ∧ ¬c M⊤\n\nb\n\n|\n\nu1 1\n\nM⊤\n\nc\n\n|\n\nb\n\n∧ ¬\n\nuA 0\n\n(a) Original HRM with root M0.\n\n(b) Flattened HRM without transforming M1.\n\nu0 1\n\nM⊤\n\na\n\n|\n\nu1 1\n\nM⊤\n\nc\n\n|\n\nb\n\n∧ ¬\n\nuA 1\n\nˆu0\n\n1\n\nM⊤\n\nM⊤\n\nb\n\na\n\n|\n\n|\n\nˆu0\n\n1\n\nM⊤\n\nM⊤\n\nb\n\n|\n\n|\n\nu0 0\n\nM⊤ | a ∧ ¬c\n\nu1 1\n\nM⊤\n\nc\n\n|\n\nb\n\n∧ ¬\n\nuA 0\n\na\n\n(c) Transformed M1 from (a).\n\n(d) Flattened HRM after transforming M1.\n\nFigure 4: Example to justify the need for the preliminary transformation algorithm.\n\n3. Add edges from the dummy initial state ˆu0 r points to: φ′ • The reward transition function r′\n\nr(ˆu0 r(u, u′) = 1[u /\n\ninitial state u0\n\nr, v, M⊤) = φ′\n\nr to all those states v r, v, M⊤).\n\nr(u0\n\n′\n\nr that the actual\n\n∈ U\n\nA\n\n∈ U\n\nr ∧\n\nu′\n\nA\n\nr ] is defined as stated at the\n\n∈ U\n\nbeginning of the section.\n\nThe HRM H ′ is such that M\nwhere the RMs have initial states with incoming edges.\n\nr, M⊤\n\n′ =\n\nM ′\n\n{\n\n}\n\n. Note that this transformation is only required in HRMs\n\nr, making it unreachable. Even though edges from ˆu0\n\nr does not have incoming edges, step 1 does not remove any edges going to u0\n\nWe now prove that this transformation is correct; that is, the HRMs are equivalent. There are two cases depending on whether the initial state has incoming edges or not. First, if the initial state u0 r, and step 2 does not add any edges going to ˆu0 r to other states may be added, it is irrelevant since it is unreachable. Therefore, we can safely say that in this case, the transformed HRM is equivalent to the original one. Second, if the initial state has incoming edges, we prove equivalence by examining the traversals H(λ) and H ′(λ) for the original HRM given a generic label trace λ = H = . By construction, both H(λ) and H ′(λ) will be identical until reaching a state w with ⟨L ⟩\nr in the case of H ′. More r in the case of H and the dummy initial state ˆu0 an outgoing transition to u0 specifically, upon reaching w and satisfying an outgoing formula to the aforementioned states, the traversals are:\n\nand the transformed one H ′ =\n\n, Mr, n\n\n⟨M L\n\n′, M ′ r,\n\n0, . . . ,\n\n⟨M\n\nP⟩\n\nP⟩\n\nH(λ) = H ′(λ) =\n\nMr, u0 r, r, u0 M ′ r,\n\n⟨⟨\n\n⟨⟨\n\n, . . . , , [] ⟩\n, . . . , , [] ⟩\n\n⊤\n\n⊤\n\nMr, w, M ′\n\nr, w,\n\n⟨\n\n⟨\n\n, []\n\n, []\n\n,\n\n.\n\n⟩⟩\n\n⟩⟩\n\n⊤\n\n⊤\n\nBy construction, state w is in both HRMs, and both of the aforementioned transitions from this state are associated with the same formula, i.e. φr(w, u0 r, M⊤). Therefore, if one of them is satisfied, the other will be too, and the traversals will become:\n\nr, M⊤) = φ′\n\nr(w, ˆu0\n\nH(λ) = H ′(λ) =\n\n⟨⟨\n\nMr, u0 r, r, u0 M ′ r,\n\n⊤\n\n, . . . , , [] ⟩\n, . . . , , [] ⟩\n\nMr, w, ⟨\nM ′ ⟨\n\nr, w,\n\n⊤\n\n, , [] ⟩\n, , [] ⟩\n\nMr, u0 r, ⟨\nr, ˆu0 M ′ r, ⟨\n\n⊤\n\n, []\n\n, []\n\n, ⟩⟩ .\n⟩⟩\n\n⟨⟨\n\n⊤\n\n⊤ r until a transition to a state w′ is satisfied. By construction, w′ is in both HRMs r(ˆu0, w′, M⊤). The hierarchy traversals\n\nr, w′, M⊤) = φ′\n\n⊤\n\nr and ˆu0\n\nWe stay in u0 and the same formula is satisfied, i.e., φr(u0 then become:\n\nH(λ) = H ′(λ) =\n\nMr, u0 r, r, u0 M ′ r,\n\n, . . . ,\n\n, . . . ,\n\n, [] ⟩\n, [] ⟩\n\n⊤\n\n⊤\n\nMr, w, ⟨\nM ′ ⟨\n\nr, w,\n\n,\n\n,\n\n, [] ⟩\n, [] ⟩\n\n⊤\n\n⊤\n\nMr, u0 r, ⟨\nr, ˆu0 M ′ r, ⟨\n\n, [] ⟩\n, [] ⟩\n\n⊤\n\n⊤\n\n, . . . ,\n\n, . . . ,\n\nMr, u0 r, ⟨\nr, ˆu0 M ′ r, ⟨\n\n, , [] ⟩\n, , [] ⟩\n\n⊤\n\n⊤\n\nMr, w′, ⟨\nr, w′, M ′ ⟨\n\n⊤\n\n⊤\n\n⟨⟨\n\n⟨⟨\n\n16\n\n, []\n\n, []\n\n, ⟩⟩ .\n⟩⟩\n\nUnder review as a conference paper at ICLR 2023\n\nFrom here both traversals will be the same until transitions to u0 r are respectively satisfied again (if any) in H and H ′. Clearly, the only change in H(λ) with respect to H ′(λ) (except for the different roots) is that the hierarchy states of the form in the latter appear as Mr, u0 r, ⟨\n\nin the former. We now check if the equivalence conditions from Definition 8 hold:\n\nr and ˆu0\n\nM ′ ⟨\n\nr, ˆu0 r,\n\n, [] ⟩\n\n, [] ⟩\n\n⊤\n\n⊤\n\n• If H(λ) ends with state u0\n\nr following the reasoning above. By construction, neither of these states is accepting or rejecting; therefore, neither of these HRMs accepts or rejects λ.\n\nr, H ′(λ) ends with state ˆu0\n\n• If H(λ) ends with state w, H ′(λ) will also end with this state following the reasoning above. Therefore, if w is an accepting state, both HRMs accept λ; if w is a rejecting state, both HRMs reject λ; and if w is not an accepting or rejecting state, neither of the HRMs accepts or rejects λ.\n\nSince all equivalence conditions are satisfied for any trace λ, H and H ′ are equivalent.\n\nFigure 4c exemplifies the output of the transformation algorithm given M1 in Figure 4a as input, whereas Figure 4d is the output of the flattening algorithm discussed next, which properly handles the context unlike the HRM in Figure 4b.\n\nFlattening Algorithm. We describe the algorithm for flattening an HRM. As previously stated, we assume without loss of generality that the HRM to be flattened consists of two hierarchy levels (i.e., the root calls flat RMs). We also assume that the flat RMs have the form produced by the previously presented transformation algorithm.\n\nGiven an HRM H = ̄Mr = ̄\nr, U\n\n, ̄φr, ̄rr, ̄u0\n\n, Mr, r , ̄ A\nU\n\nr, ̄ U\n\n⟨M\n\nP⟩ R\nr ⟩\n\nP\n\n⟨ 1. Copy the sets of states and initial state from Mr (i.e., ̄ U\n\nR\n\nwith root Mr = using the following steps:\n\n⟨U\n\nr,\n\nP\n\n, φr, rr, u0 r,\n\nA\n\nr ,\n\nR\n\nr ⟩\n\nU\n\nU\n\n, we build a flat RM\n\nr =\n\nr, ̄u0\n\nr = u0\n\nr, ̄ U\n\nA r =\n\nA\n\nr , ̄ U\n\nR r =\n\nU\n\nU\n\nr ).\n\nU\n\n2. Loop through the non-false entries of the transition function φr and decide what to such that\n\ncopy. That is, for each triplet (u, u′, Mj) where u, u′ φr(u, u′, Mj)\n\nr and Mj\n\n∈ M\n\n∈ U\n\n=\n\n(a) If Mj = M⊤ (i.e., the called RM is the leaf), we copy the transition: ̄φr(u, u′, M⊤) =\n\n: ⊥\n\nφr(u, u′, M⊤).\n\n(b) If Mj\n\n= M⊤, we embed the transition function of Mj =\n\nj,\n\n⟨U\n\nP\n\n, φj, rj, u0 j ,\n\nA\n\nj ,\n\nR\n\nj ⟩\n\nU\n\nU\n\ninto ̄Mr. Remember that Mj is flat. To do so, we run the following steps: i. Update the set of states by adding all non-initial and non-accepting states from Mj. Similarly, the set of rejecting states is also updated by adding all rejecting states of the called RM. The initial and accepting states from Mj are unimportant: their roles are played by u and u′ respectively. In contrast, the rejecting states are important since, by assumption, they are global. Note that the added states v are renamed to vu,u′,j in order to take into account the edge being embedded: if the same state v was reused for another edge, then we would not be able to distinguish them.\n\nr = ̄ ̄\nU U\nr = ̄ ̄\nR U\nU\n\nr\n\nR\n\n∪ r ∪\n\n(cid:8)vu,u′,j (cid:8)vu,u′,j\n\n|\n\nv\n\nv\n\n|\n\n(cid:0)\n\nj\n\n∈\n\nU R\nj ∈ U\n\n(cid:0)\n\n\\ (cid:9) .\n\nu0\n\n{\n\nj } ∪ U\n\n(cid:1)(cid:1)(cid:9) ,\n\nA j\n\nii. Embed the transition function φj of Mj into ̄φr. Since Mj is flat, we can make copies of the transitions straightaway: the only important thing is to check whether these transitions involve initial or accepting states which, as stated before, are going to be replaced by u and u′ accordingly. Given a triplet (v, w, M⊤) such that we update ̄φr as follows:3 v, w\n\nj and for which φj(v, w, M⊤) = φ and φ\n\n=\n\n∈ U\n\n⊥\n\n3We do not to cover the case where v is an accepting state since, by assumption, there are no outgoing transitions from it. In the case of rejecting states, we keep all of them as explained in the previous case and, therefore, there are no substitutions to be made. We also do not cover the case where w = u0 j since the input flat machines never have edges to their initial states, but to the dummy initial state.\n\n17\n\n̸ ̸\n̸ Under review as a conference paper at ICLR 2023\n\nA\n\n∈ U\n\nj and w /\n\nA. If v = u0\n\nj , then ̄φr(u, wu,u′,j, M⊤) = DNF(φ\n\nφr(u, u′, Mj)). The initial state of Mj has been substituted by u, we use the clone of w associated with the call (wu,u′,j), and append the context of the call to Mj to the formula φ. B. If v = u0\n\nφr(u, u′, Mj)). Like j , then ̄φr(u, u′, M⊤) = DNF(φ the previous case but performing two substitutions: u replaces v and u′ replaces w. The context is appended since it is a transition from the initial state of Mj.\n\nj and w\n\n∈ U\n\n∧\n\n∧\n\nA\n\nA\n\n∈ U\n\n= u0\n\nC. If v\n\nj and w\n\nj , then ̄φr(vu,u′,j, u′, M⊤) = φ. We substitute the accepting state w by u′, and use the clone of v associated with the call (vu,u′,j). This time the call’s context is not added since v is not the initial state of Mj. D. If none of the previous cases holds, there are no substitutions to be made nor contexts to be taken into account. Hence, ̄φr(vu,u′,j, wu,u′,j, M⊤) = φ. We just use the clones of v and w corresponding to the call (vu,u′,j and wu,u′,j).\n\n3. We apply the transformation algorithm we described before, and form a new flat HRM\n\n ̄H =\n\n ̄Mr, M⊤\n\n, ̄Mr, }\n\nwith the flattened (and transformed) root ̄Mr.\n\n⟨{ ̄\n ̄ The reward transition function r′ r ] is defined as stated at the beginning r ∧ U\nU of the section. Note that u might not necessarily be a state of the non-flat root, but derived from an RM with lower height.\n\nP⟩ r(u, u′) = 1[u / ∈\n\nu′\n\n∈\n\nA\n\nA\n\nWe now have everything to prove the previous theorem. Without loss of generality and for simplicity, we assume that the transformation algorithm has not been applied over the flattened root (we have already shown that the transformation produces an equivalent flat machine). Theorem 1. Given an HRM H, there exists an equivalent flat HRM ̄H.\n\nProof. Let us assume that an HRM ̄H = ̄\nM ⟨\nis a flat HRM that results from applying the flattening algorithm on an HRM H = where Mr = 0, . . . , n\n\n, r ⟩ ,\nP⟩ . For these HRMs to be equivalent, any label trace λ = ⟨U must satisfy one of the conditions in Definition 8. To prove the equivalence, we\n\nU ⟨L examine the hierarchy traversals H(λ) and ̄H(λ) given a generic label trace λ.\n\nr , ̄ r, ̄ U\nU , Mr,\n\n, where ̄Mr =\n\n, φr, rr, u0 r,\n\n, ̄φr, ̄rr, ̄u0\n\n, ̄Mr,\n\n ̄ r, U\n\n⟨M\n\nr ⟩\n\nr ,\n\nP⟩\n\nr,\n\nP\n\nP\n\nL\n\nU\n\nR\n\nR\n\nA\n\nA\n\n⟩\n\n⟨\n\n∈ U\n\nr be a state in the root Mr of H and let φr(u, u′, M⊤) be a satisfied transition from that Let u state. By construction, u is also in the root ̄Mr of the flat hierarchy ̄H, and ̄Mr has an identical transition ̄φr(u, u′, M⊤), which must also be satisfied. If the hierarchy states are and for H and ̄H respectively, then the next hierarchy states upon application of δH will ̄Mr, u, ⟨\n, [] , [] be . Therefore, both HRMs behave equivalently when calls to the ⟩\n⟩ leaf RM are made.\n\n, [] ⟩\nMr, u′, ⊤\n⟨\n\n ̄Mr, u′, ⟨\n\nMr, u,\n\n, [] ⟩\n\nand\n\n⊤\n\n⊤\n\n⊤\n\n⟨\n\nWe now examine what occurs when a non-leaf RM is called in H. Let φr(u, u′, Mj) be a satisfied transition in Mr, and let φj(u0 j , w, M⊤) be a satisfied transition from Mj’s initial state. By construction, ̄Mr contains a transition whose associated formula is the conjunction of the previous two, i.e. φr(u, u′, Mj) j , w, M⊤). Now, the hierarchy traversals will be different depending on w:\n\nφj(u0\n\n∧\n\n• If w /\n\nA j\n∈ U\n\n(i.e., w is not an accepting state of Mj), by construction, ̄Mr contains the transition ̄φr(u, wu,u′,j, M⊤) = φr(u, u′, Mj) j , w, M⊤). If the current hierarchy for H and ̄H, then the next hi- , [] states are (the equivalent) ⟩\nu, u′, Mr, Mj, φr(u, u′, Mj), erarchy states upon application of δH are ]\n⟩ ⊤⟩ . These hierarchy states are equivalent since wu,u′,j is a clone of w , [] and ⟩\nthat saves all the call information (i.e., a call to machine Mj for transitioning from u to u′).\n\n ̄Mr, wu,u′,j, ⟨\n\nMj, w, ⟨\n\nMr, u, ⟨\n\n∧ ̄Mr, u,\n\nφj(u0\n\n, [] ⟩\n\n⊤ , [ ⟨\n\nand\n\n⊤\n\n⊤\n\n⊤\n\n⟨\n\n• If w\n\nA\n\n∈ U\n\nj (i.e., w is an accepting state of Mj), by construction, ̄Mr contains the transition j , w, M⊤). If the current hierarchy states are (the for H and ̄H, then the next hierarchy states , [] ⟩\n. These hierarchy states are and\n\n ̄φr(u, u′, M⊤) = φr(u, u′, Mj) Mr, u, equivalent) ⟨\nMr, u′, upon application of δH are clearly equivalent since the machine states are exactly the same.\n\n ̄Mr, u′, ⟨\n\n∧ ̄Mr, u,\n\n⊤ , [] ⟩\n\nφj(u0\n\n, [] ⟩\n\n, [] ⟩\n\nand\n\n⊤\n\n⊤\n\n⊤\n\n⟨\n\n⟨\n\nWe now check the case in which we are inside a called RM. Let φr(u, u′, Mj) be the transition that caused H to start running Mj, and let φj(v, w, M⊤) be a satisfied transition within Mj such that\n\n18\n\n̸ Under review as a conference paper at ICLR 2023\n\nj . By construction, ̄Mr contains a transition associated with the same formula φj(v, w, M⊤).\n\n= u0\n\nv The hierarchy traversals vary depending on w:\n\n• If w /\n\n⊤\n\n⊤\n\n, [ ⟨\n\nA j\n∈ U\n\n ̄Mr, vu,u′,j, ⟨\n\n(i.e., w is not an accepting state of Mj), by construction, ̄Mr contains the transition ̄φr(vu,u′,j, wu,u′,j, M⊤) = φj(v, w, M⊤). For the transition to be taken in H, , whereas in ̄H Mj, v, the hierarchy state must be ]\n⟨ ⟩\n⊤⟩ . These hierarchy states are clearly equivalent: vu,u′,j is a , [] it will be ⟩\nclone of v that saves all information related to the call being made (the called machine, and the starting and resulting states in the transition). Upon application of δH , the hierarchy states will remain equivalent: and ̄Mr, wu,u′,j,\n\n, [ ]\n⟩ ⊤⟩ ⟨\n(again wu,u′,j saves all the call information, just like the stack).\n\nu, u′, Mr, Mj, φr(u, u′, Mj),\n\nu, u′, Mr, Mj, φr(u, u′, Mj),\n\n⊤ (i.e., w is an accepting state of Mj), by construction, ̄Mr contains the transition ̄φr(vu,u′,j, u′, M⊤) = φj(v, w, M⊤). This case corresponds to that where control is returned to the calling RM. Like in the previous case, for the transition to be taken in H, the hierarchy state must be Mj, v, , whereas in ]\n⊤⟩ ⟨\n⟩ ̄H it will be ̄Mr, vu,u′,j, Mr, u′, , [] , [] . The resulting hierarchy states then become ⟩\n⟨ ⟩\n⟨ ̄Mr, u′, respectively, which are clearly equivalent (the state is exactly the same and ⊤\n⟨ and both come from equivalent hierarchy states).\n\nu, u′, Mr, Mj, φr(u, u′, Mj), ⟨\n\nMj, w, ⟨\n\nA j\n∈ U\n\n, [] ⟩\n\n, [] ⟩\n\n⊤\n\n⊤\n\n⊤\n\n⊤\n\n, [\n\n⟨ • If w\n\nWe have shown both HRMs have equivalent traversals for any given trace, implying that both will accept, reject, or not accept nor reject a trace. Therefore, the HRMs are equivalent.\n\nFigure 5a shows the result of applying the flattening algorithm on the BOOK HRM shown in Figure 1b. Note that the resulting HRM can be compressed: there are two states having an edge with the same label to a specific state. Indeed, the presented algorithm might not produce the smallest possible flat equivalent. Figure 5b shows the resulting compressed HRM, which is like Figure 1c but naming the states following the algorithm for clarity. Estimating how much a flat HRM (or any HRM) can be compressed and designing an algorithm to perform such compression are left as future work.\n\nu0 0\n\nu1\n\n1:0,1\n\n∧ ¬\n\nu1\n\n2:0,2\n\nu1 0\n\nu2 0\n\nu1\n\n2:1,3\n\nu1\n\n1:2,3\n\nu0 0\n\n∧ ¬\n\nu1\n\n1:0,1\n\nu1\n\n2:1,3\n\nu1\n\n2:0,2\n\nu1 0\n\nu3 0\n\nu2 0\n\nu3 0\n\nuA 0\n\nuA 0\n\n(a) Without compression.\n\n(b) With compression.\n\nFigure 5: Results of flattening the HRM in Figure 1b. The notation ui j:x,y denotes the i-th state of RM j in the call between states x and y in the parent RM. Note that x and y appear only if that state comes from a called RM. The blue states and edges in (a) can be compressed as shown in (b).\n\nB.2 PROOF OF THEOREM 2\n\nWe prove Theorem 2 by first characterizing an HRM H using a set of abstract parameters. Then, we describe how the number of states and edges in an HRM and its corresponding flat equivalent are computed, and use these quantities to give an example for which the theorem holds. The parameters are the following:\n\n19\n\n̸ Under review as a conference paper at ICLR 2023\n\n• The height of the root hr. • The number of RMs with height i, N (i).\n\n• The number of states in RMs with height i, U (i).\n\n• The number of edges from each state in RMs with height i, E(i).\n\nWe assume that (i) RMs with height i only call RMs with height i 1; (ii) all RMs have a single accepting state and no rejecting states; (iii) all RMs except for the root are called; and (iv) the HRM is well-formed (i.e., it behaves deterministically and there are no cyclic dependencies). Note that N (hr) = 1 since there is a single root. Assumption (i) can be made since for the root to have height hr we need it to call at least one RM with height hr 1. Considering that all called RMs are have the same height simplifies the analysis since we can characterize the RMs at each height independently. Assumption (ii) is safe to make since a single accepting state is enough, and helps simplify the counting since only some RMs could have rejecting states. Assumption (iii) ensures that the flat HRM will comprise all RMs in the original HRM. This is also a fair assumption: if a given RM is not called by any RM in the hierarchy, we could remove it beforehand.\n\n−\n\n−\n\nThe number of states\n\nH\n\n|\n\n|\n\nin the HRM H is obtained by summing the number of states of each RM:\n\n=\n\nH |\n\n|\n\nhr(cid:88)\n\ni=1\n\nN (i)U (i).\n\nin the flat HRM ̄H is given by the number of states in the flattened root\n\nThe number of states RM\n\n ̄H\n\n|\n\n|\n\n ̄H\n\n= ̄U (hr),\n\n|\n\n|\n\nwhere ̄U (i) is the number of states in the flattened representation of an RM with height i, which is recursively defined as:\n\n ̄U (i) =\n\n(cid:26)U (i)\n\nU (i) + (cid:0) ̄U (i−1)\n\n2(cid:1) (cid:0)U (i)\n\n1(cid:1) E(i)\n\n−\n\n−\n\nif i = 1, if i > 1.\n\nThat is, the number of states in a flattened RM with height i has all states that the non-flat HRM had. 1 non-accepting states in the non-flat RM, there are E(i) edges, In addition, for each of the U (i) 1 whose number of states is ̄U (i−1). These edges are each of which calls an RM with height i replaced by the called RM except for the initial and accepting states, whose role is now played by 2). This construction process corresponds to the states involved in the substituted edge (hence the the one used to prove Theorem 1.\n\n−\n\n−\n\n−\n\nThe total of number of edges in an HRM is given by:\n\nhr(cid:88)\n\ni=1\n\nN (i)(U (i)\n\n1)E(i),\n\n−\n\nwhere (U (i) accepting state is discarded), so N (i)(U (i) RMs with height i.\n\n−\n\n1)E(i) is the total number of edges in an RM with height i (the\n\n1 is because the 1)E(i) determines how many edges there are across\n\n−\n\n−\n\nThe total number of edges in the flat HRM is given by the total number of edges in the flattened root RM, ̄E(hr), where ̄E(i) is the total number of edges in the flattened representation of an RM with height i, which is recursively defined as follows:\n\n ̄E(i) =\n\n(cid:26)(U (i) (U (i)\n\n− −\n\n1)E(i) 1)E(i) ̄E(i−1)\n\nif i = 1, if i > 1.\n\n1)E(i) edges in an RM with height i is replaced by ̄E(i−1) edges given\n\nThat is, each of the (U (i) by an RM with height i\n\n− 1 (if any).\n\n−\n\nThe key intuition is that an HRM with root height hr > 1 is beneficial representation-wise if the number of calls across RMs with height i is higher than the number of RMs with height i 1; in\n\n−\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nother words, RMs with lower heights are being reused. Numerically, the total number of edges/calls in an RM with height i is (U (i) 1)E(i) and, therefore, the total number of calls across RMs with height i is (U (i) 1)E(i)N (i). If this quantity is higher than N (i−1), then RMs with lower heights are reused, and therefore having RMs with different heights is beneficial. Theorem 2. Let H = of states and edges in an equivalent flat HRM ̄H can be exponential in hr.\n\nbe an HRM and let hr be the height of its root Mr. The number\n\n, Mr,\n\n⟨M\n\nP⟩\n\n−\n\n−\n\nProof. By example. Let H = be an HRM whose root Mr has height hr and is parameterized by N (i) = 1, U (i) = 3, E(i) = 1 for i = 1, . . . , hr. Figure 6 shows an instance of this hierarchy. Let us write the number of states in the flat RMs of each level:\n\n, Mr,\n\n⟨M\n\nP⟩\n\n ̄U (1) = U (1) = 3, (cid:16)\n\n ̄U (2) = U (2) +\n\n ̄U (1)\n\n ̄U (3) = U (3) +\n\n(cid:16)\n\n ̄U (2)\n\n−\n\n−\n\n(cid:17) (cid:16)\n\n(cid:17) (cid:16)\n\n2\n\n2\n\nU (2)\n\nU (3)\n\n(cid:17)\n\n(cid:17)\n\n1\n\n1\n\n−\n\n−\n\nE(2) = 3 + (3\n\nE(3) = 3 + (5\n\n2) (3\n\n2) (3\n\n−\n\n−\n\n−\n\n−\n\n1) 1 = 5,\n\n1) 1 = 9,\n\n...\n\n ̄U (i) = 2 ̄U (i−1)\n\n1 = 2i + 1.\n\n−\n\n= ̄U (hr) = 2hr + 1, showing that the number ̄H Hence, the number of states in the flat HRM is of states in the flat HRM grows exponentially with the height of the root. In contrast, the number of states in the HRM grows linearly with the height of the root, 3 = 3hr.\n\ni=1 N (i)U (i) = (cid:80)hr\n\n= (cid:80)hr\n\ni=1 1\n\nH |\n\n|\n\n·\n\n|\n\n|\n\nu0 i\n\nMi−1\n\n| ⊤\n\nu1 i\n\nMi−1\n\n| ⊤\n\nuA i\n\nu0 1\n\nM⊤\n\na\n\n|\n\nu1 1\n\nM⊤\n\nb\n\n|\n\nuA 1\n\nMi, 1 < i\n\nhr\n\n≤\n\nM1\n\nFigure 6: Example of an HRM whose root has height hr used in the proof of Theorem 2.\n\nIn the case of the total number of edges, we again write some iterations to derive a general expression:\n\n ̄E(1) = (U (1) ̄E(2) = (U (2) ̄E(3) = (U (3) ...\n\n−\n\n−\n\n−\n\n1)1 = 2,\n\n1)E(1) = (3 1)E(2) ̄E(1) = (3 1)E(3) ̄E(2) = (3\n\n−\n\n1)\n\n1)\n\n1\n\n1\n\n·\n\n·\n\n·\n\n·\n\n−\n\n−\n\n2 = 4,\n\n4 = 8,\n\n ̄E(i) = 2 ̄E(i−1) = 2i.\n\nTherefore, the total number of edges in the flat HRM is ̄E(hr) = 2hr . In contrast, the total number of edges in the HRM grows linearly: (cid:80)hr\n\n1)E(i) = (cid:80)hr\n\n1)1 = 2hr.\n\ni=1 N (i)(U (i)\n\ni=1 1(3\n\n−\n\n−\n\nFinally, we emphasize that the resulting flat HRM cannot be compressed, unlike the HRM in Figure 5: each state has at most one incoming edge, so there are not multiple paths that can be merged. We have thus shown that there are HRMs whose equivalent flat HRM has a number of states and edges that grows exponentially with the height of the root.\n\nUsing the aforementioned intuition, we observe that the hierarchical structure is actually expected to be useful: the number of calls across RMs with height i is (U (i) 1)1 = 2, which is greater than the number of RMs with height i\n\n− There are cases where having a multi-level hierarchy (i.e., with hr > 1) is not beneficial. For instance, given an HRM whose root has height hr and parameterized by N (i) = 1, U (i) = 2 and E(i) = 1, the number of states in the equivalent flat HRM is constant (2), whereas in the HRM itself it grows linearly with hr. The same occurs with the number of edges. By checking the previously introduced intuition, we observe that (U (i) > N (i−1) = 1, which ·\nverifies that having non-reused RMs with multiple heights is not useful.\n\n1)E(i)N (i) = (2\n\n1)E(i) = (3\n\n1 (only 1).\n\n1 = 1\n\n1)\n\n−\n\n−\n\n−\n\n−\n\n1\n\n·\n\n21\n\n̸ Under review as a conference paper at ICLR 2023\n\nC POLICY LEARNING IMPLEMENTATION DETAILS\n\nIn this appendix, we describe some implementation details that were omitted in Section 4 for simplicity. First, we start by describing some methods used in policy learning. Second, we explain the option selection algorithm step-by-step and provide examples to ease its understanding.\n\nC.1 POLICIES\n\nDeep Q-networks (DQNs; Mnih et al., 2015). We use Double DQNs (van Hasselt et al., 2016) for both formula and call options. The DQNs associated with formula options simply take an MDP state and output a Q-value for each action. In contrast, the DQNs associated with call options also take an RM state and a context, which are encoded as follows:\n\n• The RM state is encoded using a one-hot vector. The size of the vector is given by the\n\nnumber of states in the RM.\n\n• The context, which is either\n\nor a DNF formula with a single disjunct/conjunction, is . Each vector position encoded using a vector whose size is the number of propositions whose value depends on how p appears in the context: corresponds to a proposition p (i) +1 if p appears positively, (ii) -1 if p appears negatively, or (iii) 0 if p does not appear. , the vector solely consists of zeros. Note that if the context is\n\n∈ P\n\n|P|\n\n⊤\n\n⊤\n\nThese DQNs output a value for each possible call in the RM; however, some of these values must be masked if the corresponding calls are not available from the RM state-context used as input. For instance, the DQN for M0 in Figure 1b outputs a value for M2, , and M⊤, , only the values for the first two calls are ⟨\nrelevant. Just like unavailable calls, we also mask unsatisfiable calls (i.e., calls whose context cannot be satisfied in conjunction with the accumulated context used as input).\n\n. If the RM state was u0 ⟩\n\n0 and the context was\n\nM1, ⟨\n\nM1, ⟨\n\n, ⊤⟩\n\n¬ ⟩\n\n⊤⟩\n\n⊤\n\n⟨\n\n,\n\nTo speed up learning, a subset of the Q-functions associated with formula options is updated after each step. Updating all the Q-functions after each step is costly and we observed that similar performance could be obtained with this strategy. To determine the subset, we keep an update counter cφ for each Q-function qφ, and a global counter c (i.e., the total number of times Q-functions have been updated). The probability of updating qφ is:\n\npφ =\n\nsφ φ′ sφ′\n\n(cid:80)\n\n, where sφ = c\n\ncφ\n\n1.\n\n−\n\n−\n\nA subset of Q-functions is chosen using this probability distribution without replacement.\n\nExploration. During training, the formula and call option policies are ε-greedy. In the case of formula options, akin to Q-functions, each option ωj,φ i,u,Φ performs exploration with an exploration factor εφ∧Φ, which linearly decreases with the number of steps performed using the policy induced by qφ∧Φ. Likewise, Kulkarni et al. (2016) keep an exploration factor for each subgoal, but vary it depending on the option’s success rather than on the number of performed steps. In the case of call options, each RM state-context pair is associated with its own exploration factor, which linearly decreases as options started from that pair terminate.\n\nThe Formula Tree. As explained in Section 4, each formula option’s policy is induced by a Qfunction associated with a formula. In domains where certain proposition sets cannot occur, it is unnecessary to consider formulas that cover some of these sets. For instance, in a domain where two propositions a and b cannot be simultaneously observed (i.e., it is impossible to observe ), a could instead be represented by the more abstract formulas a formulas such as a or b; therefore, a a and b could be both associated with a Q-function qb. By reducing the number of Q-functions, the learning naturally becomes more efficient.\n\nb and a could be both associated with a Q-function qa, whereas b\n\nb or b\n\n∧ ¬\n\n∧ ¬\n\n∧ ¬\n\n∧ ¬\n\na, b\n\n{\n\n}\n\nWe represent relationships between formulas using a formula tree which, as the name suggests, , a formula tree arranges a set of formulas in a tree structure. Formally, given a set of propositions is a tuple is the root , where (2P )∗ is a set of labels. All the nodes of the tree and it is associated with the formula 22P denote the set of in the tree except for the root are associated with conjunctions. Let ν(X)\n\nP is a set of nodes, each associated with a formula; Fr\n\n, Fr, L ⟩\n\n; and L\n\n∈ F\n\n⟨F\n\n⊤\n\n⊆\n\nF\n\n⊆\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n⊤\n\na\n\na\n\nb\n\n∧ ¬\n\na\n\nc\n\n∧ ¬\n\na\n\nb\n\n∧ ¬\n\nc\n\n∧ ¬\n\n⊤\n\na\n\na\n\nc\n\n∧ ¬\n\na\n\nb\n\n∧ ¬\n\na\n\nb\n\n∧ ¬\n\nc\n\n∧ ¬\n\na\n\nb\n\nc\n\nd\n\n∧ ¬\n\n∧ ¬\n\n∧ ¬\n\n(a) Tree for L = {{a}, {b}, {c}, {d}}.\n\na\n\nb\n\nc\n\nd\n\n∧ ¬ (b) Tree for L = {{a}, {b}, {c}, {d}, {a, c}}.\n\n∧ ¬\n\n∧ ¬\n\nFigure 7: Examples of formula trees for different sets of literals. Note that the node a (a) could also be a child of a\n\nc (the parent depends on the insertion order).\n\n∧ ¬\n\nb\n\n∧ ¬\n\n∧ ¬\n\nc in\n\nliterals of a formula X, e.g. if X = a Y if (1) X = = X and\n\n. A formula X subsumes a formula , or (2.i) ν(X) = Y , or = Y . Case (2) indicates that Y is a special case of X (it adds literals but it is satisfied L ̸| by exactly the same labels). The tree is organized such that the formula at a given node subsumes all its descendants. The set of Q-functions is determined by the children of the root.\n\nb, then ν(X) = ¬\nν(Y ) and (2.ii) for all labels\n\nL, either\n\n= X and\n\n} L ∈\n\n⊤ L ̸|\n\n∧ ¬\n\nL |\n\nL |\n\na,\n\n⊆\n\n{\n\nb\n\nDuring the agent-environment interaction, the formula tree is updated if (i) a new formula appears in the learned HRMs, or (ii) a new label is observed. Algorithm 1 contains the pseudo-code for updating the tree in these two cases. When a new formula is added (line 1), we create a node for the formula (line 2) and add it to the tree. The insertion place is determined by exploring the tree topdown from the root Fr (lines 3-19). First, we check whether a child of the current node subsumes the new node (line 7). If such a node exists, then we go down this path (lines 8-9); otherwise, the new node is going to be a child of the current node (lines 16-17). In the latter case, in addition, all those children nodes from the current node that are subsumed by the new node need to become children of the new node (lines 11-15). The other core case in which the tree may need an update occurs when a new label is observed (lines 20-25) since we need to make sure that parenting relationships comply with the set of labels L. First, we find nodes inconsistent with the new label: a parenting relationship is broken (line 39) when the formula of the parent non-root node is satisfied by the label but the formula of the child node is not (or vice versa). Once the inconsistent nodes are found, we remove their current parenting relationship (lines 45-46) and reinsert them in the tree (line 47). Figure 7 shows two simple examples of formula trees, where the Q-functions are qa in (a), and qa and qa∧¬c in (b).\n\nC.2 OPTION SELECTION ALGORITHM\n\nAlgorithm 2 shows how options are selected, updated and interrupted during an episode. Lines 1-3 correspond to the algorithm’s initialization. The initial state is that of the environment, while the initial hierarchy state is formed by the root RM Mr, its initial state u0 r, an empty context (i.e., ), and an empty call stack. The option stack ΩH contains the options we are currently Φ = running, where options at the front are the shallowest ones (e.g., the first option in the list is taken in the root RM). The steps taken during an episode are shown in lines 4-14, which are grouped as follows:\n\n⊤\n\n1. The agent fills the option stack ΩH by selecting options in the HRM from the current hierarchy state until a formula option is chosen (lines 15-25). The context is propagated and augmented through the HRM (i.e., the context of the calls is conjuncted with the propagat- (true), and ing context and converted into DNF form). Note that the context is initially not that of the hierarchy state. It is possible that no new options are selected if the formula option chosen in a previous step has not terminated yet.\n\n⊤\n\n2. The agent chooses an action according to the last option in the option stack (line 6), which will always be a formula option whose policy maps states into actions. The action is ap-\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Formula tree operations\n\n⊤\n\nFr\n\n← ← ⊥\n\ncurrent node added node while added node =\n\n1: procedure ADDFORMULA(f ) ADDNODE(CREATENODE(f )) 2: 3: procedure ADDNODE(new node) 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:\n\nsubsumed children new node.children for child\n\nchild node if child node\n\ncurrent node.children child.parent\n\ncurrent node\n\n= nil then\n\n← ←\n\nelse\n\ndo\n\n←\n\n←\n\n⊥\n\n∈\n\nchild node\n\n←\n\ncurrent node.children new node.parent added node\n\n←\n\n16: 17: 18: 19: F ← F ∪ { 20: procedure ONLABEL( L\n21: 22: 23: 24:\n\nL inconsistent nodes for child\n\n∪ {L}\n\n←\n\n∈\n\n← ⊤ new node }\n) L\n\nFr.children do\n\n← {}\n\nInput: A formula tree\n\nwith the formula\n\n, Fr, L , where ⟩\n\n⟨F ), and L is a set of labels.\n\nF\n\nis a set of nodes, Fr\n\nis the root node (associated\n\n∈ F\n\nFINDSUBSUMINGCHILD(current node, new node)\n\n▷ Keep exploring down this path\n\n▷ Insert the node\n\nGETSUBSUMEDCHILDREN(current node, new node) new node.children\n\nsubsumed children\n\nsubsumed children do\n\n∪\n\n← new node\n\ncurrent node.children\n\nchild\n\n}\n\n\\ {\n\ncurrent node.children\n\n←\n\ncurrent node\n\nnew node\n\n}\n\n∪ {\n\nL\n\nFINDINCONSISTENTNODES(child,\n\n, inconsistent nodes)\n\n∈\n\nfor child\n\nreturn child\n\ncurrent node.children do\n\nREINSERTINCONSISTENTNODES(inconsistent nodes)\n\nif child.formula subsumes new node.formula then\n\n25: 26: procedure FINDSUBSUMINGCHILD(current node, new node) 27: 28: 29: 30: 31: procedure GETSUBSUMEDCHILDREN(current node, new node) 32: 33: 34: 35:\n\nif new node.formula subsumes child.formula then\n\nsubsumed children for child\n\ncurrent node.children do\n\nsubsumed children\n\nreturn nil\n\n← {}\n\n∈\n\nsubsumed children return subsumed children\n\n←\n\nL\n\nL |\n\nfor child if\n\ninconsistent nodes\n\ncurrent node.children do\n\n∈ = current node.formula\n\n36: 37: procedure FINDINCONSISTENTNODES(current node, 38: 39: 40: 41: 42: 43: procedure REINSERTINCONSISTENTNODES(inconsistent nodes) 44: 45: 46: 47:\n\nnode.parent.children node.parent nil ADDNODE(node)\n\nFINDINCONSISTENTNODES(child,\n\n= child.formula then\n\ninconsistent nodes do\n\nnode.parent.children\n\ninconsistent nodes\n\nfor node\n\n⊕ L |\n\nchild\n\nnode\n\nelse\n\n∪ {\n\n\\ {\n\n←\n\n←\n\n←\n\nL\n\n∈\n\n}\n\n}\n\nnew node\n\n}\n\n∪ {\n\n, inconsistent nodes)\n\n, inconsistent nodes)\n\n24\n\n̸ Under review as a conference paper at ICLR 2023\n\nplied, and the agent gets the next state (line 7). The next hierarchy state is obtained by applying the hierarchical transition function δH using label l(st+1) (line 8). The Q-functions associated with formula options’ policies are updated after this step (line 9).\n\n3. The option stack ΩH is updated by removing those options that have terminated (lines 10, 26-45). The terminated options are saved in a different list Ωβ to update the Q-functions of the RMs where they were initiated later on (line 11). The termination of the options is performed as described in Section 4. All options terminate if a terminal state is reached (lines 27-28). Otherwise, we check options in ΩH from deeper to shallower levels. The first checked option is always a formula option, which terminates if the hierarchy state has changed (line 40). In contrast, a call option terminates if it does not appear in the stack (lines 33, 46-51).4 When an option is found to terminate, it is added to Ωβ and removed from ΩH (lines 35-36, 41-42). If a non-terminating option is found (lines 37, 43), we stop checking for termination (no higher level options can have terminated in this case).\n\n4. If at least one option has terminated (line 12), the option stack is updated such that it contains all options appearing in the call stack (lines 13, 52-70). Options are derived for the full stack if ΩH is empty (lines 53, 54), or for the part of the stack not appearing in ΩH (lines 56-59). The new derived options (lines 61-70) from the call stack are assumed to start in the same state as the last terminated option (i.e., the shallowest terminated option, line 63) and to have been run for the same number of steps too. Crucially, the contexts should be propagated accordingly, starting from the context of the last terminated option (line 69). As a result of the definition of the hierarchical transition function δH , the contexts in the stack may be DNF formulas with more than one disjunct. In contrast, the contexts associated with options are either or DNFs with a single disjunct (remember that an option is formed for each disjunct). For instance, this occurs if the context is a }\nis observed: since both disjuncts are satisfied, the context shown in the call stack will be b. In the simplest case, the derived option (which as said before the full disjunction a is associated with a DNF with a single disjunct or ) can include one of these disjuncts chosen uniformly at random (line 67). Alternatively, we could memorize all the derived options and perform identical updates for both later on once terminated.\n\nb and\n\na, b\n\n⊤\n\n⊤\n\n∨\n\n∨\n\n{\n\nC.3 EXAMPLES\n\n1,1,⊤, ω⊤,\n\nWe briefly describe some examples of how policy learning is performed in the HRM of Figure 1b. We first enumerate the options in the hierarchy. The formula options are ω⊤, 1,0,⊤, 2,1,⊤, and ω⊤, ω⊤, to satisfy 0,3,⊤. The first option should lead the agent to observe the label { } . The Q-functions associated with this set of options are q ∧¬ , q , q , q and q . Note 2,1,⊤ are both associated with q . Conversely, the call options are ω1,¬ 0,0,⊤, ω2,⊤ 0,0,⊤, 1,0,¬ and ω⊤,\n\n∧ ¬ that ω⊤, ω2,⊤ sequentially achieve theirs. The associated Q-functions are q0, q1 and q2. Note that ω2,⊤ ω2,⊤\n\n0,2,⊤, where the first one achieves its local goal if formula options ω⊤,\n\n0,1,⊤, and ω1,⊤\n\n1,1,⊤ and ω⊤,\n\n0,1,⊤ are both associated with q2.\n\n1,0,¬ , ω⊤,\n\n2,0,⊤, ω⊤,\n\n0,0,⊤ and\n\n1,1,⊤\n\nWe now describe a few steps of the aforementioned option selection algorithm in two scenarios. First, we consider the scenario where all chosen options are run to completion (i.e., until their local goals are achieved):\n\nM0, u0 0, ⟨\n\n1. The initial hierarchy state is\n\nand the option stack ΩH is empty. We select 0 in M0 using a policy induced by q0. 0,0,⊤. Let us assume that the former is is chosen, which can 1,0,¬ . Since this option is a formula option (the call is made to M⊤), we do not\n\n, [] ⟩\noptions to fill ΩH . The first option is chosen from u0 At this state, the available options are ω1,¬ 0,0,⊤ and ω2,⊤ chosen. Then an option from the initial state of M1 under context only be ω⊤, select any more options and the option stack is ΩH =\n\nω1,¬\n\n⊤\n\n¬\n\n0,0,⊤, ω⊤,\n\n⟨\n\n. 1,0,¬ ⟩\n\n4We denote by φ1 ⊆ φ2, where φ1, φ2 ∈ DNFP , the fact that all the disjuncts of φ1 appear in φ2. This\n\ncontainment relationship also holds if both formulas are ⊤. For instance, (a ∧ ¬c) ⊆ (a ∧ ¬c) ∨ d.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Episode execution using an HRM (continues in p. 27)\n\nand an environment ENV =\n\n,\n\n⟨S\n\nP⟩\n\nA\n\n, p, r, γ,\n\n, l, τ\n\n.\n\n⟩\n\nP ▷ Initial MDP tuple ▷ Initial hierarchy state ▷ Initial option stack\n\n▷ Expand the option stack , ΩH ) ⟩\n▷ Choose a according to the last option in ΩH\n\n▷ Apply transition function\n\n,\n\nMj, u′, Φ′, Γ′ ⟨\n\n) ⟩\n\nMi, u, Φ, Γ ⟩\n\n[]\n\n, Mr,\n\nInput: An HRM H = ENV.INIT()\n\n⟨M Mr, u0 r,\n\n⊤\n\n←\n\n←\n\n⟩ ← ⟨\n\n, [] ⟩\n\nMi, u, Φ, Γ\n\nFILLOPTIONSTACK(st,\n\n← SELECTACTION(st, ΩH )\n\n1: s0 2: ⟨\n3: ΩH 4: for each step t = 0, . . . , do 5: 6: 7: 8: 9: 10: 11: 12: 13:\n\nΩH a\n← st+1 ←\nMj, u′, Φ′, Γ′ , l(st+1)) Mi, u, Φ, Γ ⟨\n⟩ ⟨\nUPDATEFORMULAQFUNCTIONS(st, a, st+1) Ωβ, ΩH UPDATECALLQFUNCTIONS(Ωβ, st+1, l(st+1)) if\n\nENV.APPLYACTION(a) δH (\n\nALIGNOPTIONSTACK(ΩH , Γ′, Ωβ)\n\nTERMINATEOPTIONS(ΩH , s,\n\nMi, u, Φ, Γ ⟨\n\nΩβ |\n| ΩH\n\n> 0 then\n\n⟩ ←\n\n←\n\n⟨\n\n←\n\n⟨\n\n⟩ ← ⟨\n\nMi, u, Φ, Γ\n\nMj, u′, Φ′, Γ′ 14: 15: procedure FILLOPTIONSTACK(s, 16: 17: 18: 19:\n\nΩ′ Φ\nMj while the last option in Ω′\n\nMi; v\n\nΩH\n\n←\n\nu\n\n⟩ Mi, u, ⟨\n\n, Γ ·\n\n, ΩH ) ⟩\n\n20: 21: 22: 23:\n\n24:\n\n29: 30:\n\n31: 32:\n\n33: 34: 35:\n\n36: 37: 38:\n\n39: 40:\n\n41:\n\n42: 43: 44:\n\nH ← ← ⊤ ←\nωx,φ if x\n\nj,v,Φ ← =\n⊤ Mj Φ\n\nΩ′ H ← return Ω′\n\nMx; v DNF(Φ\n\n←\n\nu0 x\nφ)\n\n← ∧\nωx,φ j,v,Φ\n\n←\n\nΩH\n\n⊕\n\n▷ The context is initially true ▷ The state-automaton pair in which an option is selected\n\nH is not a formula option do\n\nSELECTOPTION(s, Mj, v, Φ) then\n\n▷ Select an option (e.g., with ε-greedy) ▷ If the option is a call option ▷ Next option is chosen on the called RM’s initial state ▷ Update the context ▷ Update the option stack (concatenate new option)\n\nH\n\n25: 26: procedure TERMINATEOPTIONS(ΩH , s, 27: 28:\n\nif sT =\n\nthen\n\n⊤\n\nΩH\n\nH ← > 0 do\n\nreturn ΩH , [] []; Ω′ Ωβ ←\nΩ′ while H | |\nωx,φ k,v,Ψ ← if x =\n⊤ in stack, if\n\nlast option in Ω′ then\n\nH\n\n¬\n\n←\n\nin stack then Ωβ Ω′\n\nΩβ Ω′\n\n← H ←\n\n⊕ H ⊖\n\nk,v,Ψ\n\nωx,φ ωx,φ\n\nk,v,Ψ\n\nelse\n\nMi, u, Φ, Γ ⟨\n\n, ⟩\n\nMj, u′, Φ′, Γ′ ⟨\n\n⟩\n\n)\n\n▷ All options terminate ▷ Initialize structures ▷ While the option stack is not empty\n\n▷ If the option is a call option\n\n▷ Update the list of terminated options\n\n▷ Remove the last option from the option stack\n\nOPTIONINSTACK(ωx,φ\n\nk,v,Ψ, Γ′)\n\nbreak\n\n▷ Stop unrolling\n\nelse\n\nif\n\nMi, u, Φ, Γ ⟨\nΩβ Ωβ Ω′ Ω′\n\n← H ←\n\n⟩ ̸ ⊕\nH ⊖\n\nMj, u′, Φ′, Γ′ =\n⟨ ωx,φ k,v,Ψ ωx,φ\n\nk,v,Ψ\n\nelse\n\nthen\n\n⟩\n\n▷ If the hierarchy state has changed. . . ▷ Update the list of terminated options\n\n▷ Remove the last option from the option stack\n\nbreak\n\n▷ Stop unrolling\n\nH\n\nreturn Ωβ, Ω′\n\n45: 46: procedure OPTIONINSTACK(ωx,φ 47: 48: 49: 50:\n\nfor l = 0 . . . |\nuf , ⟨\n· if uf = v\n\n| − , Mi, Mj, φ′, Φ′\n\ni = k , l\n\n⟩ ← j = x\n\n1 do\n\n∧\n\n∧\n\nΓ\n\nΓl\n\n∧\n\nk,v,Φ, Γ)\n\n51:\n\nreturn\n\nreturn 1\n\n,\n\n⊥\n\n−\n\n⊤\n\nφ\n\n⊆\n\nΦ\n\nφ′ Φ′ then ▷ The call option is in the call stack ▷ Return whether it appears in the stack and the index\n\n⊆\n\n∧\n\n26\n\n̸ ̸\nUnder review as a conference paper at ICLR 2023\n\nif\n\n= 0 then\n\n52: procedure ALIGNOPTIONSTACK(ΩH , Γ, Ωβ) 53: 54: 55: 56:\n\nΩH |\nreturn ALIGNOPTIONSTACKHELPER(ΩH , Γ, Ωβ, 0)\n\nlast option in ΩH\n\nωx,φ\n\nelse\n\n|\n\nk,v,Φ ←\n\nin stack, stack index if in stack then\n\nOPTIONINSTACK(ωx,φ\n\nk,v,Φ, Γ)\n\n←\n\nreturn ALIGNOPTIONSTACKHELPER(ΩH , Γ, Ωβ, stack index)\n\nreturn ΩH\n\n60: 61: procedure ALIGNOPTIONSTACKHELPER(ΩH , Γ, Ωβ, stack index) 62: 63:\n\nlast option in Ωβ\n\nΩH\n\nΩ′ ω·,· Φ′ for l = stack index . . .\n\nH ← ·,·,Φ ← Φ\n←\n\n, Mi, Mj, φ,\n\nuf , ·\n⟨ φsel ←\nΩ′ H ← Φ′ ←\nreturn Ω′\n\nH\n\n1 do\n\nΓ |\n·⟩ ←\n\n| −\n\nΓl\n\nSelect disjunct from φ (e.g., randomly) Ω′\n\nH ⊕ DNF(Φ′\n\nωj,φsel i,uf ,Φ′ φ)\n\n∧\n\n57: 58: 59:\n\n64: 65: 66: 67:\n\n68:\n\n69:\n\n70:\n\n▷ Shallowest terminated option ▷ Context initialized from last option\n\n▷ Append new option to the option stack\n\n2. The agent selects options according to the formula option in ΩH , ω⊤,\n\n1,0,¬ , whose policy is induced by q ∧¬ . Let us assume that the policy tells the agent to turn right. Since the label at this location is empty, the hierarchy state remains the same; therefore, no options terminate, and the option stack does not change.\n\n⊤\n\nu0 , [ ⟨\n\n0, M0, M1,\n\nM1, u1 1, ⟨\n\n3. Let us assume that the agent moves forward twice, thus observing 0, u1\n\n. The hierarchy state then becomes (see Appendix A for a step-by-step ¬\napplication of the hierarchical transition function). We check which options in ΩH have terminated starting from the last chosen one. The formula option ω⊤, 1,0,¬ terminates because the hierarchy state has changed. In contrast, the call option ω1,¬ 0,0,⊤ does not terminate that can be mapped into it since there is an item in the call stack, (meaning that the option is running).\n\n0, M0, M1,\n\n0, u1\n\n{ }\n\n⊤⟩\n\n⊤⟩\n\n] ⟩\n\nu0\n\n¬\n\n⟨\n\n,\n\n,\n\n4. An experience (s, ω⊤,\n\n1,0,¬ , s′) is formed for the terminated option, where s and s′ are the observed tuples on initiation and termination respectively. This tuple is added to the replay buffer associated with the RM where the option appears, 1, since it achieved its goal (i.e., a label that satisfied\n\nwas observed).\n\nD\n\n∧ ¬\n\n5. We align ΩH with the new stack. In this case, ΩH remains unchanged since its only option\n\ncan be mapped into an item of the new stack.\n\n6. We start a new step. Since the option stack does not contain a formula option, we select new options from the current hierarchy state according to a policy induced by q1. In this case, there is a single eligible option: ω⊤,\n\n1,1,⊤.\n\nIn the second scenario, we observe what occurs when the HRM traversal differs from the options chosen by the agent:\n\n1. The initial step is like the one in the previous scenario, but we assume ω2,⊤\n\n0,0,⊤ is selected instead. Then, since this is a call option, an option from the initial state of M2 under context 2,0,⊤. The option stack thus becomes ΩH = ω2,⊤\n\nis chosen, which can only be ω⊤,\n\n⊤\n\n.\n\n0,0,⊤, ω⊤,\n\n2,0,⊤⟩\n\n⟨\n\n2. Let us assume that by taking actions according to ω⊤, the previous scenario, the hierarchy state becomes We check which options in ΩH have terminated. The formula option ω⊤, since the hierarchy state has changed, and the call option ω2,⊤\n\n2,0,⊤ we end up observing u0 M1, u1 , [ 1, ⟨\n⟨\n\n. Like in { } .\n] ⟩\n¬ 2,0,⊤ terminates 0,0,⊤ also terminates since it\n\n0, M0, M1,\n\n0, u1\n\n⊤⟩\n\n⊤\n\n,\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\ncannot be mapped into an item of the call stack. Note that these options should intuitively finish since the HRM is being traversed through a path different from that chosen by the agent.\n\n3. The replay buffers are not updated for these options since they have not achieved their local\n\ngoals.\n\n4. We align ΩH with the new stack. The only item of the stack\n\nbe mapped into option ω1,¬ that it has run for the same number of steps as the last terminated option ω2,⊤\n\ncan 0,0,⊤. We assume that this option starts on the same tuple s and\n\n0, M0, M1,\n\n⊤⟩\n\n¬\n\n,\n\n0, u1\n\nu0 ⟨\n\n0,0,⊤.\n\nD CURRICULUM LEARNING IMPLEMENTATION\n\n−\n\nβ)r, where β\n\nWe here describe the details of the curriculum learning method described in Section 5. When an episode is completed for Mij, Rij is updated using the episode’s undiscounted return r as Rij ←\nβRij + (1 Rij is computed from the return and used to determine the probability of selecting tasks and instances. Note that this scoring function, also used in the curriculum method by Andreas et al. (2017), assumes that the undiscounted return ranges between 0 and 1 (see Section 2). The probability of choosing task i is maxj cij/ (cid:80) k maxl ckl; that is, the task for which an instance is performing very poorly has a higher probability. Having selected task i, the probability of choosing instance j is cij/ (cid:80) k cik, i.e. instances where performance is worse have a higher probability of being chosen.\n\n[0, 1] is a hyperparameter. A score cij = 1\n\n−\n\n∈\n\nE LEARNING AN HRM FROM TRACES WITH ILASP\n\nWe formalize the task of learning an HRM using ILASP (Law et al., 2015), an inductive logic programming system that learns answer set programs (ASP) from examples. We address the reader to Gelfond & Kahl (2014) for an introduction to ASP, and to Law (2018) for ILASP. Our formalization is close to that by Furelos-Blanco et al. (2021) for flat finite-state machines. Without loss of generality, as stated in Section 5, we assume that each RM has exactly one accepting and one rejecting state. We first describe how HRMs are represented in ASP, and then explain the encoding of the HRM learning task in ILASP.\n\nE.1 REPRESENTATION OF AN HRM IN ANSWER SET PROGRAMMING\n\nIn this section, we explain how HRMs are represented using Answer Set Programming (ASP). First, we describe how traces are represented. Then, we present how HRMs themselves are represented and also introduce the general rules that describe the behavior of these hierarchies. Finally, we prove the correctness of the representation. We use A(X) to denote the ASP representation of X (e.g., a trace).\n\nDefinition 9 (ASP representation of a label trace). Given a label trace λ = denotes the set of ASP facts that describe it:\n\n0, . . . ,\n\nn\n\nL\n\n, M (λ) ⟩\n\n⟨L\n\nA(λ) = { {\n{\n\nlabel(p, t). step(t). 0\n. last(n).\n\n| }\n\n0 t\n\n| ≤\n\n≤ ≤\n\nn, p\n\nt n\n\n≤ } ∪\n\nt ∈ L\n\n} ∪\n\nThe label(p, t) fact indicates that proposition p step of the trace, and last(n) indicates that the trace ends in step n.\n\n∈ P\n\nis observed in step t, step(t) states that t is a\n\nExample 2. The set of ASP facts for the label trace λ = label( , 2)., step(0), step(1)., step(2)., last(2).\n\n, ⟨{ } .\n}\n\n, {}\n\n{ }⟩\n\nis A(λ) =\n\nlabel( , 0).,\n\n{\n\nDefinition 10 (ASP representation of an HRM). Given an HRM H = (cid:83)\n\nA(Mi), where:\n\nMi∈M\\{M⊤}\n\n, Mr,\n\nP⟩\n\n⟨M\n\n, A(H) =\n\nA(Mi) = AU (Mi)\n\nAU (Mi) =\n\nstate(u, Mi).\n\n{\n\n∪\n\nAφ(Mi), u\n\ni ∈ U\n\n,\n\n}\n\n|\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nAφ (Mi) =\n\n\n\n \n\ncall(u, u′, x + e, Mi, Mj). ̄φ(u, u′, x + e, Mi, T) : - not label(p1, T), step(T). ...\n\n ̄φ(u, u′, x + e, Mi, T) : - not label(pn, T), step(T). x = (cid:80)j−1 ̄φ(u, u′, x + e, Mi, T) : - label(pn+1, T), step(T). ...\n\nMj ∈ M, u, u′ ∈ Ui,\n\nφi(u, u′, Mj) ̸= ⊥,\n\nk=0 |φi(u, u′, Mk)|,\n\ne ∈ [1, |φi(u, u′, Mj)|],\n\n.\n\nφe ∈ φi(u, u′, Mj),\n\n ̄φ(u, u′, x + e, Mi, T) : - label(pm, T), step(T).\n\nφe = p1 ∧ · · · ∧ pn\n\n∧ ¬pn+1 ∧ · · · ∧ ¬pm\n\n\n\n \n\nNote that each non-leaf RM Mi in the hierarchy is associated with its own set of rules A(Mi), which are described as follows:\n\n• Facts state(u, Mi) indicate that u is a state of RM Mi. • Facts call(u, u′, e, Mi, Mj) indicate that edge e between states u and u′ in RM Mi is\n\nlabeled with a call to RM Mj.\n\n• Normal rules whose head is of the form ̄φ(u, u′, e, Mi, T) indicate that the transition from state u to u′ with edge e in RM Mi does not hold at step T. The body of these rules consists of a single label(p, T) literal and a step(T) atom indicating that T is a step. Commonly, variables are represented using upper case letters in ASP, which is the case of steps T here.\n\nThere are some important things to take into account regarding the encoding:\n\n• There is no leaf RM M⊤. We later introduce the ASP rules to emulate it. • The edge identifiers e between a given pair of states (u, u′) range from 1 to the total number of conjunctions/disjuncts between them. Note that in Aφ we assume that the leaf RM has an index, just like the other RMs in the HRM. The index could be n since the rest are numbered from 0 to n\n\n1.\n\n− Example 3. The following rules represent the HRM in Figure 1b:\n\n \n\n0, M0). 0, 1, M0, M2).\n\n0, u2\n\n0, M0). state(u2\n\nstate(u0 call(u0 call(u3  ̄φ(u3 0, uA (cid:26)state(u0 ̄φ(u0 1, u1 (cid:26)state(u0 ̄φ(u0 2, u1\n\n0, 1, M0, M1). call(u0 0 , 1, M0, M⊤).\n\n0, M0). state(u1 0, u1 0, uA 0 , 1, M0, T) : - not label( , T), step(T). 1, M1). state(u1 1 , M1). 1, 1, M1, T) : - not label( , T), step(T). 2, M2). state(u1 2, 1, M2, T) : - not label( , T), step(T).\n\n1, M1). state(uA\n\n2, M2). state(uA\n\n2 , M2).\n\nstate(u3 call(u1 0, u1 ̄φ(u0\n\n0, M0). state(uA 0, u3 0, 1, M0, T) : - label( , T), step(T).\n\n0, 1, M0, M2). call(u2\n\n0 , M0).\n\n0, u3\n\n0, 1, M0, M1).\n\n1, uA\n\ncall(u0 ̄φ(u1 call(u0 ̄φ(u1\n\n1, 1, M1, M⊤). call(u1\n\n1, u1 1 , 1, M1, T) : - not label( , T), step(T). 2, u1 2 , 1, M2, T) : - not label( , T), step(T).\n\n2, 1, M2, M⊤). call(u1\n\n2, uA\n\n1, uA\n\n2, uA\n\n2 , 1, M2, M⊤).\n\n1 , 1, M1, M⊤).\n\n \n\n∪\n\n (cid:27)\n\n∪\n\n(cid:27)\n\n.\n\nGeneral Rules. The following sets of rules, whose union is denoted by i, represent how an HRM functions (e.g., how transitions are taken or the acceptance/rejection criteria). For simplicity, all initial, accepting and rejecting states are denoted by u0, uA and uR respectively.\n\ni=0R\n\nR\n\n=\n\n5 ∪\n\nThe rule below is the inversion of the negation of the state transition function ̄φ. Note that the predicate for φ includes the called RM M2 as an argument.\n\n0 =\n\nR\n\n{\n\nφ(X, Y, E, M, M2, T) : - not ̄φ(X, Y, E, M, T), call(X, Y, E, M, M2), step(T).\n\n.\n\nR\n\n1 introduces the pre sat(X, M, T) predicate, which encodes the exit condition preThe rule set sented in Section 3 and indicates whether a call from state X of RM M can be started at time T. The first rule corresponds to the base case and indicates that if the leaf M⊤ is called then the condition is satisfied if the associated formula is satisfied. The second rule applies to calls to non-leaf RMs, where we need to satisfy the context of the call (like in the base case), and also check whether a call from the initial state of the potentially called RM can be started.\n\n(cid:26)pre sat(X, M, T) : - φ(X, , , M, M⊤, T).\n\n1 =\n\nR\n\npre sat(X, M, T) : - φ(X, , , M, M2, T), pre sat(u0, M2, T), M2!=M⊤.\n\n}\n\n(cid:27)\n\n.\n\n2 introduces the reachable(X, M, TO, T2) predicate, which indicates that state X of The rule set RM M is reached between steps T0 and T2. The latter step can also be seen as the step we are\n\nR\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\ncurrently at. The first fact indicates that the initial state of the root RM is reached from step 0 to step 0. The second rule indicates that the initial state of a non-root RM is reached from step T to step T (i.e., it is reached anytime). The third rule represents the loop transition in the initial state of the root Mr: we stay there if no call can be started at T (i.e., we are not moving in the HRM). The fourth rule is analogous to the third but for the accepting state of the root instead of the initial state. Remember this is the only accepting state in the HRM that does not return control to the calling RM. The fifth rule is also similar to the previous ones: it applies to states reached after TO that are non-accepting, which excludes looping in initial states of non-root RMs at the time of starting them (i.e., loops are permitted in the initial state of a non-root RM if we can reach it afterwards by going back to it). The last rule indicates that Y is reached at step T2 in RM M started at T0 if there is an outgoing transition from the current state X to Y at time T that holds between T and T2, and state X has been reached between T0 and T. We will later see how δ is defined.\n\n\n\n \n\n2 =\n\nR\n\nreachable(u0, Mr, 0, 0). reachable(u0, M, T, T) : - state(u0, M), M!=Mr, step(T). reachable(X, M, T0, T+1) : - reachable(X, M, T0, T), not pre sat(X, M, T),\n\nreachable(X, M, T0, T+1) : - reachable(X, M, T0, T), not pre sat(X, M, T),\n\n.\n\nreachable(X, M, T0, T+1) : - reachable(X, M, T0, T), not pre sat(X, M, T),\n\nstep(T), X=u0, M=Mr.\n\nstep(T), X=uA, M=Mr.\n\nstep(T), TO<T, X!=uA.\n\nreachable(Y, M, T0, T2) : - reachable(X, M, T0, T), δ(X, Y, M, T, T2).\n\n\n\n \n\nR\n\n3 introduces two predicates. The predicate satisfied(M, T0, TE) indicates that RM The rule set M is satisfied if its accepting state uA is reached between steps T0 and TE. Likewise, the predicate failed(M, T0, TE) indicates that RM M fails if its rejecting state uR is reached between steps T0 and TE. These two descriptions correspond to the first and third rules. The second rule applies to the leaf RM M⊤, which always returns control immediately; thus, it is always satisfied between any two consecutive steps.\n\n \n\n\n\n3 =\n\nR\n\nsatisfied(M, T0, TE) : - reachable(uA, M, T0, TE). satisfied(M⊤, T, T+1) : - step(T). failed(M, T0, TE) : - reachable(uR, M, T0, TE).\n\n \n\n\n\nR\n\n4, encodes multi-step transitions within an RM. The predicate δ(X, Y, M, T, T2) The following set, expresses that the transition from state X to state Y in RM M is satisfied between steps T and T2. The first rule indicates that this occurs if the context labeling a call to an RM M2 is satisfied and that RM is also satisfied (i.e., its accepting state is reached) between these two steps. In contrast, the second rule is used for the case in which the rejecting state of the called RM is reached between those steps. In the latter case, we transition to the local rejecting state uR of M (i.e., the state we would have transitioned to does not matter). This follows from the assumption that rejecting states are global rejectors (see Section 3). The idea of this rule is that rejection is propagated bottom-up in the HRM.\n\n(cid:26)δ(X, Y, M, T, T2) : - φ(X, Y, , M, M2, T), satisfied(M2, T, T2). δ(X,uR, M, T, T2) : - φ(X, , , M, M2, T), failed(M2, T, T2).\n\n(cid:27)\n\n.\n\n4 =\n\nR\n\nR\n\nThe last set, 5, encodes the accepting/rejecting criteria. Remember that the last(T) predicate indicates that T is the last step of a trace. Therefore, the trace is accepted if the root RM is satisfied from the initial step 0 to step T + 1 (the step after the last step of the trace, once the final label has been processed). In contrast, the trace is rejected if a rejecting state in the hierarchy is reached between these two same steps.\n\n(cid:26)accept : - last(T), satisfied(Mr, 0, T+1). reject : - last(T), failed(Mr, 0, T+1).\n\n(cid:27)\n\n5 =\n\nR\n\nUnlike the formalism introduced in Section 3, this encoding does not use stacks, which would be costly to do. Here we know the trace to be processed and, therefore, the RMs can be evaluated bottom-up; that is, we start evaluating the lowest level RMs first on different subtraces, and the result of this evaluation is used in higher level RMs.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nWe now prove the correctness of the ASP encoding. To do so, we first introduce what means for an HRM to be valid with respect to a trace, as well as a definition and a theorem due to Gelfond & Lifschitz (1988) that will help us derive the proof. Definition 11. Given a label trace λ∗, where λ∗ if H accepts λ∗ and dead-end trace), or H does not accept nor reject λ∗ and\n\n, an HRM H is valid with respect to = D (i.e., λ∗ is a\n\n= G (i.e., λ∗ is a goal trace), or H rejects λ∗ and\n\n= I (i.e., λ∗ is an incomplete trace).\n\nG, D, I\n\n∗ ∈ {\n\n∗\n\n∗\n\n}\n\nDefinition 12. An ASP program P is stratified when there is a partition\n\n∗\n\nP = P0\n\nP1\n\n∪\n\n∪ · · · ∪\n\nPn\n\n(Pi and Pj disjoint for all i\n\n= j)\n\nsuch that, (1) for every predicate p, the definition of p (all clauses with p in the head) is contained in one of the partitions Pi and, (2) for each 1 n, if a predicate occurs positively in a clause of Pi then its definition is contained within (cid:83) j≤i Pj, and if a predicate occurs negatively in a clause of Pi then its definition is contained within (cid:83) j<i Pj. Theorem 3. If an ASP program P is stratified, then it has a unique answer set.\n\n≤\n\n≤\n\ni\n\nProposition 1 (Correctness of the ASP encoding). Given a finite label trace λ∗, where\n\n, and an HRM H =\n\n, Mr, A(λ∗) has a unique answer set AS and (1) accept AS if and only if\n\n⟨M\n\nP⟩\n\n= D.\n\n∈\n\n∗ ∈ that is valid with respect to λ∗, the program P = = G, and\n\nAS if and only if\n\n∗\n\nG, D, I {\nA(H) (2) reject\n\n} ∪ R ∪ ∈\n\n∗\n\nProof. First, we prove that the program P = A(H) i, has a unique answer set. By Theorem 3, if P is stratified then it has a unique answer set. We show there is a way of partitioning P following the constraints in Definition 12. A possible partition is P = P0 5. The unique answer set AS = AS0 AS3, where ASi corresponds to partition Pi, is shown in Figure 8. For simplicity, λ∗[t] denotes the t-th label in trace λ∗, λ∗[t :] denotes the subtrace starting from the t-th label onwards, and Mi(λ∗) denotes the hierarchy traversal using RM Mi as the root.\n\nP3, where P0 = A(λ∗), P1 = A(H), P2 = AS1\n\nA(λ∗), where\n\n1, P3 =\n\ni=0 R\n\n∪ R ∪\n\nAS2\n\n∪R\n\n∪R\n\n∪R\n\n∪R\n\nP2\n\nP1\n\nR\n\nR\n\nR\n\n∪\n\n∪\n\n∪\n\n∪\n\n∪\n\n∪\n\n3\n\n0\n\n2\n\n4\n\n= (cid:83)5\n\n= G We now prove that accept ∗\nthen, since the hierarchy is valid with respect to λ∗ (see Definition 11), the hierarchy traversal H(λ∗) finishes in the accepting state uA of the root; that is, H(λ∗)[n + 1] = . This holds if and only if accept\n\n= G (i.e., the trace achieves the goal). If\n\nAS if and only if\n\nMr, uA r ,\n\nAS.\n\n, ·\n\n·⟩\n\n∈\n\n∗\n\n⟨\n\nThe proof showing that reject is similar to the previous one. If hierarchy traversal H(λ∗) finishes in a rejecting state uR; that is, H(λ∗)[n + 1] = where Mk\n\nAS if and only if = D (i.e., the trace reaches a dead-end) = D then, since the hierarchy is valid with respect to λ∗, the ,\n·⟩\n\n. This holds if and only if reject\n\nMk, uR, ⟨\n\nAS.\n\n∈ ∗\n\n∗\n\n,\n\n·\n\n∈ M\n\n∈\n\n∈\n\nE.2 REPRESENTATION OF THE HRM LEARNING TASK IN ILASP\n\nWe here formalize the learning of an HRM and its mapping to a general ILASP learning task. We start by defining the HRM learning task introduced in Section 5.\n\nC, u0, uA, uR, Λ, κ Definition 13. An HRM learning task is a tuple TH = ,\n, where ⟩\nM r is the index of the root RM in the HRM; is a set of states of the root RM always containing an initial state u0, an accepting state uA, and a rejecting state uR; is a set of ΛI ΛD propositions; is a set of label traces; and κ is the maximum number of conjunctions/disjuncts in each formula. An is a solution of TH if and only if it is valid with respect to all the HRM H = traces in Λ.\n\nP is a set of callable RMs; Λ = ΛG\n\nr, P\nu0, uA, uR\n\nis a set of RMs;\n\n} , Mr,\n\n⟨M ∪ {\n\nM ⊇ {\n\nU ⊇ {\n\n⊆ M\n\nM⊤\n\nMr\n\nM\n\nM\n\nP⟩\n\n∪\n\n∪\n\nU\n\n}\n\n}\n\n⟨\n\nC\n\n,\n\n,\n\nWe make some assumptions about the sets of RMs be in M\nof propositions\n\nM (or a subset of it).\n\n, (ii) all RMs in\n\nare deterministic, and (iii) all RMs in\n\nM\n\n: (i) all RMs reachable from RMs in\n\nC must are defined over the same set\n\nM\n\nM\n\nFor completeness, we provide the definition of an ILASP task introduced by Law et al. (2016). The first definition corresponds to the form of the examples taken by ILASP, while the second corresponds to the ILASP tasks themselves.\n\nP\n\n31\n\n̸ Under review as a conference paper at ICLR 2023\n\nAS0 = {label(p, t). | 0 ≤ t ≤ n, p ∈ Lt} ∪ {step(t). | 0 ≤ t ≤ n} ∪ {last(n).} ,\n\n{state(u, Mi). | Mi ∈ M \\ {M⊤}, u ∈ Ui} ∪ (cid:26)call(u, u′, x + e, Mi, Mj). Mi ∈ M \\ {M⊤}, Mj ∈ M, u, u′ ∈ Ui, φi(u, u′, Mj) ̸= ⊥,\n\nAS1 =\n\n \n\n ̄φ(u, u′, x + e, Mi, t).\n\n\n\nx = (cid:80)j−1\n\nk=0 |φi(u, u′, Mk)|, e ∈ [1, |φi(u, u′, Mj)|]\n\n0 ≤ t ≤ n, Mi ∈ M \\ {M⊤}, Mj ∈ M, u, u′ ∈ Ui, φi(u, u′, Mj) ̸= ⊥, x = (cid:80)j−1 k=0 |φi(u, u′, Mk)|, e ∈ [1, |φi(u, u′, Mj)|], λ∗[t] ̸|= φi(u, u′, Mj)[e]\n\n(cid:27)\n\n∪ ,\n\n \n\nφ(u, u′, x + e, Mi, t).\n\nAS2 =\n\n (cid:8)pre sat(u, Mi, t).\n\n0 ≤ t ≤ n, Mi ∈ M \\ {M⊤}, Mj ∈ M, u, u′ ∈ Ui, φi(u, u′, Mj) ̸= ⊥, x = (cid:80)j−1 k=0 |φi(u, u′, Mk)|, e ∈ [1, |φi(u, u′, Mj)|], λ∗[t] |= φi(u, u′, Mj)[e] 0 ≤ t ≤ n, Mi ∈ M \\ {M⊤}, u ∈ Ui, λ∗[t] |= ξi,u,⊤\n\n(cid:9)\n\n \n\n\n\n \n\n\n\n∪\n\n,\n\n(cid:8)reachable(u0, Mr, 0, 0).(cid:9) ∪ (cid:8)reachable(u0, Mi, t, t). | 0 ≤ t ≤ n, Mi ∈ M \\ {M⊤, Mr}, u0 ∈ Ui (cid:26)reachable(u, Mr, t1, t2).\n\n(cid:27)\n\n(cid:9) ∪\n\nreachable(u, Mi, t1, t2).\n\n \n\n0 ≤ t1 < t2 ≤ n + 1, u ∈ Ur, H(λ∗[t1 :])[t2 − t1] = ⟨Mr, u, ·, ·⟩ 0 ≤ t1 < t2 ≤ n + 1, Mi ∈ M \\ {Mr, M⊤}, u ∈ Ui, λ∗[t1] |= ξi,u0,⊤, Mi(λ∗[t1 :])[t2 − t1] = ⟨Mi, u, ·, ·⟩, Mi(λ∗[t1 :])[t2 − t1 − 1] ̸= ⟨Mi, uA, ·, ·⟩\n\n∪\n\n \n\n∪\n\n  (cid:8)satisfied(Mr, t1, t2) | 0 ≤ t1 < t2 ≤ n + 1, H(λ∗[t1 :])[t2 − t1] = ⟨Mr, uA, ·, ·⟩(cid:9) \n\n\nsatisfied(Mi, t1, t2).\n\n \n\n0 ≤ t1 < t2 ≤ n + 1, Mi ∈ M \\ {Mr, M⊤}, λ∗[t1] |= ξi,u0,⊤, Mi(λ∗[t1 :])[t2 − t1] = ⟨Mi, uA, ·, ·⟩, Mi(λ∗[t1 :])[t2 − t1 − 1] ̸= ⟨Mi, uA, ·, ·⟩\n\n {satisfied(M⊤, t, t + 1) | 0 ≤ t ≤ n} ∪ (cid:8)failed(Mr, t1, t2) | 0 ≤ t1 < t2 ≤ n + 1, H(λ∗[t1 :])[t2 − t1] = ⟨·, uR, ·, ·⟩(cid:9) ∪ \n\n\nfailed(Mi, t1, t2).\n\n \n\n\n\n∪\n\nAS3 =\n\n (cid:26)δ(u, u′, Mi, t, t + 1).\n\nδ(u, u′, Mi, t1, t2).\n\n \n\n \n\n\n0 ≤ t1 < t2 ≤ n + 1, Mi ∈ M \\ {Mr, M⊤}, λ∗[t1] |= ξi,u0,⊤, Mi(λ∗[t1 :])[t2 − t1] = ⟨·, uR, ·, ·⟩ 0 ≤ t ≤ n, Mi ∈ M \\ {M⊤}, u, u′ ∈ Ui, λ∗[t1] |= φi(u, u′, M⊤)\n\n(cid:27)\n\n∪\n\n∪\n\n\n\n0 ≤ t1 < t2 ≤ n + 1, Mi ∈ M \\ {M⊤}, u, u′ ∈ Ui, ∃Mj ∈ M \\ {M⊤} s.t. φ = φi(u, u′, Mj), λ∗[t1] |= ξj,u0,φ, Mj(λ∗[t1 :])[t2 − t1] = ⟨Mj, uA, ·, ·⟩, Mj(λ∗[t1 :])[t2 − t1 − 1] ̸= ⟨Mj, uA, ·, ·⟩\n\n.\n\n \n\n∪\n\n \n\n\nδ(u, uR, Mi, t1, t2). Mi ∈ M \\ {M⊤}, u ∈ Ui, 0 ≤ t1 < t2 ≤ n + 1,\n\n∃Mj ∈ M \\ {M⊤} s.t. φ = φi(u, u′, Mj), λ∗[t1] |= ξj,u0,φ, Mj(λ∗[t1 :])[t2 − t1] = ⟨Mk, uR, ·, ·⟩, Mk ∈ M\n\n\n\n∪\n\n (cid:8)accept | H(λ∗)[n + 1] = ⟨Mr, uA, ·, ·⟩(cid:9) ∪ (cid:8)reject | H(λ∗)[n + 1] = ⟨Mk, uR, ·, ·⟩, Mk ∈ M \\ {M⊤}(cid:9)\n\nFigure 8: Answer sets for each of the partitions in the program P = A(H) is an HRM,\n\nis the set of general rules and λ∗ is a label trace.\n\nR\n\nA(λ∗), where H\n\n∪ R ∪\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\n⟩\n\neinc, eexc ⟨\n\n, ⟩\nis a pair of sets of atoms, called a partial interpretation, and ectx is an ASP if and only if there is an\n\nDefinition 14. A context-dependent partial interpretation (CDPI) is a pair where program called a context. A program P accepts a CDPI answer set AS of P Definition 15. An ILASP task is a tuple T = is the ASP background B\nknowledge, which describes a set of known concepts before learning; M is the set of ASP rules + and − are sets of CDPIs called, respectively, the positive and allowed in the hypotheses; and E\nE +, M is an inductive solution of T if and only if (i) negative examples. A hypothesis H ⊆ S −, accepts e, and (ii)\n\nAS and eexc ,\n\neinc, eexc AS = −\n\n, ectx ⟩\n. ∅\nwhere\n\nectx such that einc\n\ndoes not accept e.\n\n∩ +,\n\n∈ E\n\nM ,\n\n⟨B\n\n⟨E\n\n⟨⟨\n\n⟨⟨\n\n⟩⟩\n\n⊆\n\n∪\n\nS\n\nS\n\n∀\n\nE\n\ne\n\ne\n\n⟩\n\n, ectx ⟩\n\neinc, eexc\n\n∀\n\n∈ E\n\nB ∪ H Given an HRM learning task TH , we map it into an ILASP learning task A(TH ) = and use the ILASP system (Law et al., 2015) to find an inductive solution Aφ(H) the examples. Note that we do not use negative examples ( A(TH ) below.\n\nB ∪ H\n\n− =\n\nE\n\n∅\n\n,\n\nM ,\n\n+,\n\n∅⟩⟩ ⟨E S\n⟨B M that covers ⊆ S ). We define the components of\n\nBackground Knowledge. The background knowledge describe the behavior of the HRM. The set ∈ U of the root RM with index r we aim to induce, whereas i) contains the Mi∈M\\{M⊤} ASP representations of all RMs. Finally, is the set of general rules introduced in Appendix E.1 that defines how HRMs process label traces. Importantly, the index of the root r in these rules must correspond to the one used in TH .\n\n∪ B U consists of state(u, Mr) facts for each state u\n\nB B\nM = (cid:83)\n\nis a set of rules that\n\n∪ R\n\nA(\n\nM\n\nR\n\n=\n\nM\n\nB\n\nB\n\nU\n\nHypothesis Space. The hypothesis space sition from a non-terminal state u [1, κ]. Formally, it is defined as i\n\n∈ U \\ {\n\nM contains all ed and ̄φ rules that characterize a tranusing edge\n\nto a different state u′\n\nS uA, uR\n\nu\n\n}\n\n∈ U \\ {\n\n}\n\n∈\n\n \n\n\n\ncall(u, u′, i, M ). ̄φ(u, u′, i, M, T) : - label(p, T), step(T). ̄φ(u, u′, i, M, T) : - not label(p, T), step(T).\n\nu′\n\nM =\n\nS\n\nu\n\n(cid:8)uA, uR(cid:9) , u\n\n[1, κ] ,\n\n∈ U \\ ∈ U \\ { M\n\n, i }\nC, p ∈ M\n\n∈ ∈ P\n\n \n\n\n\n.\n\nExample Sets. Given a set of traces Λ = ΛG\n\nΛI , the set of positive examples is defined as\n\n+ =\n\nE\n\n{⟨\n\ne∗, A(λ)\n\n⟩ | ∗ ∈ {\n\n, λ\n\n}\n\nΛ∗\n\n, }\n\n∈\n\nΛD\n\n∪\n\n∪ G, D, I\n\nwhere\n\n• eG = • eD = • eI =\n\naccept\n\nreject\n\n}\n\n{\n\n,\n\n,\n\nreject\n\naccept\n\n, and\n\n}\n\n{ accept, reject\n\n}⟩\n\n,\n\n}⟩\n\n}⟩\n\n⟨{\n\n⟨{\n\n, ⟨{}\n\n{\n\nare the partial interpretations for goal, dead-end and incomplete traces. The accept and reject atoms express whether a trace is accepted or rejected by the HRM; hence, goal traces must only be accepted, dead-end traces must only be rejected, and incomplete traces cannot be accepted or rejected. Note that the context of each example is the set of ASP facts A(λ) that represents the corresponding trace (see Definition 9).\n\nCorrectness of the Learning Task. The following theorem captures the correctness of the HRM learning task. Theorem 4. Given an HRM learning task TH = H = A(TH ) =\n\nC, u0, uA, uR, Λ, κ , an HRM ,\n⟩ M\nis a solution of TH if and only if Aφ(Mr) is an inductive solution of\n\n, Mr, +,\n\n⟨M ∪ { ,\n\nr, ⟨\n\nM\n\nP\n\nU\n\n,\n\n,\n\nMr }\nM , ⟨E\n\nS\n\n⟨B\n\nP⟩ .\n∅⟩⟩\n\nProof. Assume H is a solution of TH .\n\nH is valid with respect to all traces in Λ (i.e., H accepts all traces in ΛG, rejects all traces in\n\n⇐⇒ ΛD and does not accept nor reject any trace in ΛI ).\n\nBy Proposition 1, for each trace λ∗\n\n⇐⇒ unique answer set AS and (1) accept only if\n\n= D.\n\n∈\n\nΛ∗ where AS if and only if\n\n∈\n\n, A(H) G, D, I ∪ R ∪ = G, and (2) reject\n\n}\n\nA(λ∗) has a AS if and\n\n∈\n\n∗ ∈ { ∗\n\n∗\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFor each example e\n\n∈ E\n\nR ∪\n\n+,\n\n+,\n\nA(H) accepts e. Aφ(Mr) accepts e (the two programs are identical).\n\nFor each example e ∈ E Aφ(Mr) is an inductive solution of A(TH ).\n\nB ∪\n\n⇐⇒\n\n⇐⇒\n\n⇐⇒\n\nConstraints. We introduce several constraints encoding structural properties of the HRMs we want to learn. Some of these constraints are expressed in terms of facts pos(u, u′, e, m, p) and neg(u, u′, e, m, p), which indicate that proposition p appears positively (resp. negatively) in edge e from state u to state u′ in RM Mm. These facts are derived from ̄φ rules in A(H) and injected in the ILASP tasks using meta-program injection (Law et al., 2018).\n\n∈ P\n\nThe following set of constraints ensures that the learned root RM is deterministic using the saturation technique (Eiter & Gottlob, 1995). The idea is to check determinism top-down by selecting two edges from a given state in the root, each associated with a set of literals. Initially, the set of literals is formed by those in the formula labeling the edges. If a selected edge calls a non-leaf RM, we select an edge from the initial state of the called RM, augment the set of literals with the associated formula, and repeat the process until a call to the leaf RM is reached. We then check if the literal sets are mutually exclusive. If there is a pair of edges from the root that are not mutually exclusive, the solution is discarded. The set of rules is shown below. The first rule states that we keep two saturation IDs, one for each of the edges we select next and for which mutual exclusivity is checked. The second rule chooses a state X in the root, whereas the third rule selects two edges from this state and assigns a saturation ID to each of them. The fourth rule indicates that if one of the edges we have selected so far calls a non-leaf RM, we select one of the edges from the initial state of the called RM and create a new edge with the same saturation ID. The fifth and sixth rules take the propositions for each set of edges (one per saturation ID). The next three rules indicate that if the edges are mutually exclusive (i.e., a proposition appears positively in one set and negatively in the other) or they are the same, then the answer set is saturated. The saturation itself is encoded in the following three rules: an answer set is saturated by adding every possible ed mtx and root point atoms to the answer set. Due to the minimality of answer sets in disjunctive answer set programming, this “maximal” interpretation can only be an answer set if there is no smaller answer set. This will be the case if and only if every choice of edges satisfies the condition (i.e. every choice of ed mtx and root point atoms results in saturation). The constraint encoded in the final rule then discards answer sets in which saturation did not occur, meaning that the remaining solutions must satisfy the condition.\n\n\n\n \n\nsat id(1; 2). root point(X, M) : call(X, , , M, ), M=Mr. ed mtx((X, Y, E, M, M2), SatID) : call(X, Y, E, M, M2) : - root point(X, M), sat id(SatID). ed mtx((u0, Y2, E2, M2, M3), SatID) : call(u0, Y2, E2, M2, M3) : - ed mtx(( , , , , M2), SatID),\n\nM2!=M⊤.\n\npos prop(P, ID) : - ed mtx((X, Y, E, M, ), ID), pos(X, Y, E, M, P). neg prop(P, ID) : - ed mtx((X, Y, E, M, ), ID), neg(X, Y, E, M, P). saturate : - pos prop(P, 1), neg prop(P, 2). saturate : - pos prop(P, 2), neg prop(P, 1). saturate : - ed mtx((X, Y, , M, M2), 1), ed mtx((X, Y, , M, M2), 2), root point(X, M). root point(X, M) : - call(X, , , M, ), saturate, M=Mr. ed mtx((X, Y, E, M, M2), SatID) : - call(X, Y, E, M, M2), M=Mr, sat id(SatID), saturate. ed mtx((u0, Y, E, M, M2), SatID) : - call(u0, Y, E, M, M2), sat id(SatID), saturate. : - not saturate.\n\n\n\n \n\nOther required constraints to learn sensible HRMs are shown below. The first rule prevents an edge from being labeled with calls to two different RMs. The second rule prevents edges from being labeled with the same literal both positively and negatively.\n\n(cid:26): - call(X, Y, E, M, M2), call(X, Y, E, M, M3), M2!=M3.\n\n(cid:27)\n\n: - pos(X, Y, E, M, P), neg(X, Y, E, M, P).\n\nThe following constraints are used to speed up the learning of an HRM. First, we extend the symmetry breaking method by Furelos-Blanco et al. (2021), originally proposed for flat RMs, to our hierarchical setting. The main advantage of this method is that it accelerates learning without restricting the family of learnable HRMs. Other constraints analogous to those in previous work (FurelosBlanco et al., 2021) that speed up the learning process further are enumerated below. For simplicity,\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nsome of these constraints use the auxiliary rule below to define the ed(X, Y, E, M) predicate, which is equivalent to the call(X, Y, E, M, M2) predicate but omitting the called RM:\n\nThe constraints are the following:\n\ned(X, Y, E, M) : - call(X, Y, E, M, ).\n\n• Rule out inductive solutions where an edge calling the leaf M⊤ is labeled by a formula formed only by negated propositions. The rule below enforces a proposition to occur positively whenever a proposition appears negatively in an edge calling M⊤.\n\n: - neg(X, Y, E, M, ), not pos(X, Y, E, M, ), call(X, Y, E, M,M⊤).\n\n• Rule out any inductive solution where an edge from X to Y with index E is not labeled by a positive or a negative literal. This rule only applies to calls to the leaf M⊤, thus avoiding unconditional transitions.\n\n: - not pos(X, Y, E, M, ), not neg(X, Y, E, M, ), call(X, Y, E, M, M⊤).\n\n• Rule out inductive solutions containing states different from the accepting and rejecting\n\nstates without outgoing edges. In general, these states are not interesting.\n\n(cid:26)has outgoing edges(X, M) : - ed(X, , , M).\n\n(cid:27)\n\n: - state(X, M), not has outgoing edges(X, M), X!=uA, X!=uR.\n\n• Rule out inductive solutions containing cycles; that is, solutions where two states can be reached from each other. The path(X, Y, M) predicate indicates there is a directed path (i.e., a sequence of directed edges) from X to Y in RM M. The first rule states that there is a path from X to Y if there is an edge from X to Y. The second rule indicates that there is a path from X to Y if there is an edge from X to an intermediate state Z from which there is a path to Y. Finally, the third rule discards the solutions where X and Y can be reached from each other through directed edges.\n\n(cid:40)path(X, Y, M) : - ed(X, Y, , M).\n\n(cid:41)\n\npath(X, Y, M) : - ed(X, Z, , M), path(Z, Y, M). : - path(X, Y, M), path(Y, X, M).\n\nF EXPERIMENTAL DETAILS\n\nIn this section, we describe the details of the experiments introduced in Section 6. We discuss how the domains are implemented, the hyperparameters used to run the algorithms, and provide all specific results through tables and plots. All experiments ran on 3.40GHz Intel® CoreTM i7-6700 processors.\n\nF.1 DOMAINS\n\nThe CRAFTWORLD domain is based on MiniGrid (Chevalier-Boisvert et al., 2018), thus inheriting many of its features. At each step, the agent observes a W 3 tensor, where W and H are the width and height of the grid. The three channels contain the object IDs, the color IDs, and object state IDs (including the orientation of the agent) respectively. Each of the objects we define (except for the lava , which already existed in MiniGrid) has its own object and color IDs. Before providing the agent with the state, the content of all matrices is scaled between -1 and 1. Note that even though the agent gets a full view of the grid, it is still unaware of the completion degree of a task. Other works have previously used the full view of the grid (Igl et al., 2019; Jiang et al., 2021).\n\nH\n\n×\n\n×\n\nThe grids are randomly generated. In all settings (OP, OPL, FR, FRL), the agent and the objects are randomly assigned an unoccupied position. In the case of FR and FRL, no object occupies a position between rooms nor its adjoining positions. There is a single object per object type (i.e., proposition) in OP and OPL, whereas there can be one or two per type in FR and FRL. Finally, there is a single lava location in OPL, which is randomly assigned (like the rest of the propositions), whereas in FRL there are four fixed lava locations placed in the intersections between doors as shown in Figure 9.\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: An instance of the CRAFTWORLD grid in the FRL setting.\n\nFigure 10: An instance of the WATERWORLD in the WOD setting (Toro Icarte et al., 2018).\n\nThe WATERWORLD domain (cf. Figure 10) has a continuous state space. The states are vectors containing the absolute position and velocity of the agent, and the relative positions and velocities of the other balls. The agent does not know the color of each ball. In all settings (WOD and WD), a WATERWORLD instance is created by assigning a random position and direction to each ball. Like in CRAFTWORLD, the agent does not know the degree of completion of a task.\n\nF.2 HYPERPARAMETERS\n\nTable 2 lists some of the hyperparameters used in the experiments. The rest of the hyperparameters and other details (e.g., architectures, evaluation of other methods) are discussed in the following paragraphs.\n\nArchitectures. The DQNs for CRAFTWORLD consist of a 3-layer convolutional neural network 2 and use a stride of 1. In the FR (CNN) with 16, 32 and 32 filters respectively. All kernels are 2 ×\nand FRL settings, there is a max pooling layer with kernel size 2 2 after the first convolutional layer. This part of the architecture is based on that by Igl et al. (2019) and Jiang et al. (2021), who also work on MiniGrid using the full view of the grid. In DQNs associated with formulas, the CNN’s output is fed to a 3-layer multilayer perceptron (MLP) where the hidden layer has 256 rectifier units and the output layer has a single output for each action. In the case of DQNs for RMs, the output of the CNN is extended with the encoding of the RM state and the context (as discussed in Appendix C) before being fed to a 3-layer MLP where the hidden layer has 256 rectifier units and the output layer has a single output for each call in the RM.\n\n×\n\nThe architecture for WATERWORLD is a simple modification of the one introduced by Toro Icarte et al. (2018). The formula DQNs consist of a 5-layer MLP, where each of the 3 hidden layers has 512 rectifier units. The DQN for the RMs share the same architecture and, like in CRAFTWORLD, the state from the environment is extended with the state and context encodings.\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: List of hyperparameters and their values.\n\nParameter\n\nGeneral\n\nEpisodes Maximum episode length Num. of instances I\n\nDQNs\n\nLearning rate α Learning rate (SMDP) α Optimizer Discount γ Discount (SMDP) γ Updated formula Q-functions per step Replay memory size Replay start size Target network update frequency Replay memory size (SMDP) Replay start size (SMDP) Target network update frequency (SMDP) Minibatch size\n\nExploration\n\nInitial exploration Final exploration Annealing steps Annealing steps (SMDP)\n\nHRM learning\n\nCurriculum weight β Curriculum threshold ILASP time budget Num. collected goal traces ρ (height 1) Num. collected goal traces ρ (height ≥\nNum. goal traces ρs to learn first HRM\n\n2)\n\nCRAFTWORLD\n\nWATERWORLD\n\n300,000 1,000 10\n\n300,000 1,000 10\n\n1 1\n\n5 5\n\n× ×\n\n× ×\n\n10−4 10−4\n\n10−5 10−3 RMSprop (Hinton et al., 2012) RMSprop (Hinton et al., 2012) 0.9 0.99 4\n500,000 100,000 1,500 10,000 1,000 500 32\n\n0.9 0.99 4\n500,000 100,000 1,500 10,000 1,000 500 32\n\n1.0 0.1 2,000,000 10,000\n\n0.99 0.85 2 hours 25 150 10\n\n1.0 0.1 5,000,000 10,000\n\n0.99 0.75 2 hours 25 150 10\n\nCompression. Akin to some methods for learning RMs (Furelos-Blanco et al., 2021; Toro Icarte et al., 2019), we compress label traces by merging consecutive equal labels into a single one. For instance,\n\nbecomes\n\n,\n\n,\n\n,\n\n,\n\n.\n\n, ⟨{}\n\n{ }\n\n{ }\n\n, {}\n\n{}\n\n, { }\n\n{ }⟩\n\n, ⟨{}\n\n, { }\n\n{}\n\n, { }\n\n{ }⟩\n\nCurriculum. The average undiscounted returns (see Section 5 and Appendix D) are updated for each task-instance pair every 100 training episodes using the undiscounted return obtained by the greedy policies in a single evaluation episode.\n\nFlat HRM Learning Methods. We briefly describe the methods used to learn flat HRMs. Each run consists of 150,000 episodes, and the set of instances is exactly the same across methods. The core difference with respect to learning non-flat HRMs is that there is a single task for which the HRM is learned. Our method, LHRM, is therefore not able to reuse previously learned HRMs for other tasks; however, it still uses the same hyperparameters. In the case of DeepSynth (Hasanbeig et al., 2021), LRM (Toro Icarte et al., 2019) and JIRP (Xu et al., 2020), we exclusively evaluate their RM learning components using traces collected through random walks. For a fair comparison against LHRM (both in the non-flat and flat learning cases), we (i) compress the traces using the aforementioned methodology, and (ii) use the OP and WOD settings of CRAFTWORLD and WATERWORLD respectively, where observing goal traces is very easy for simple tasks such as MILKBUCKET. In these approaches, a different instance is selected at each episode following a cyclic order (i.e., 1, 2,. . . , I 1, I, 1, 2, . . . ). The set of propositions in these approaches includes a proposition covering the case where no other propositions are observed (if needed). In the case of LRM, one of the parameters is the maximum number of RM states, which we set to that of the minimal RM. Finally, we modify DeepSynth to avoid periodically calling the learner (i.e., only call it when a counterexample\n\n−\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\ntrace is observed): this is not done in other approaches and usually causes timeouts unnecessarily (the same RM is repeatedly learned).5\n\nILASP. We use ILASP2 (Law et al., 2015) to learn the HRMs.6 For efficiency, the default calls to the underlying ASP solver are modified to be made with the flag ---opt-mode=ignore, meaning that non-minimal solutions might be obtained (i.e., solutions involving more rules than needed), so the learned root might contain some unnecessary edges. In practice, the solutions produced by ILASP rarely contain such edges and, if they do, these edges eventually disappear by observing an appropriate counterexample. We hypothesize that using this flag helps since no optimization is made every time ILASP is called. The design of the ILASP tasks is discussed in Appendix E.2. We highlight that this notion of minimality is not related to that of a minimal RM (i.e., an RM with the fewest number of states) described in Section 5.\n\nF.3 EXTENDED RESULTS\n\nWe organize the tables and figures following the structure in Section 6.\n\nPolicy Learning. Figure 11 shows the plots omitted in the main paper (the remaining CRAFTWORLD setting, FRL, is shown in Figure 2). The number of plotted episodes varies across domains and tasks for clarity. Figure 12 shows the learning curves for a task called TENPAPERS, which consists of performing PAPER a total of 10 times.7 The plot shows that exploiting the non-flat HRM leads to much faster convergence than the equivalent flat one. The difference arises from the fact that the non-flat HRM can reuse the policies for the called PAPER RM, whereas the flat one cannot.\n\nLearning Non-Flat HRMs. We present tables containing the results for the HRM learning component of LHRM. The content of the columns is the following left-to-right: (1) task name; (2) number of runs in which at least one goal trace was observed; (3) number of runs in which at least one HRM was learned; (4) time spent to learn the HRMs; (5) number of calls to ILASP made to learn the HRMs; (6) number of states of the final HRM; (7) number of edges of the final HRM; (8) number of episodes between the learning of the first HRM and the activation of the task’s level; (9) number of example traces of a given type (G = goal, D = dead-end, I = incomplete); and (10) length of the example traces of a given type. In addition, the bottom of the tables contains the number of completed runs (i.e., the number of runs that have not timed out), the total time spent on learning the HRMs, and the total number of calls made to ILASP. In the case of CRAFTWORLD, Table 3 shows the results for the default case (all lower level RMs are callable and options are used for exploration), Table 4 shows the results when the set of callable RMs contains only those actually needed, and Table 5 shows the results using primitive actions for exploration instead of options. Analogous results are shown for WATERWORLD in Tables 6, 7 and 8. The discussion of these results can be found in Section 6.\n\nLearning Flat HRMs. Table 9 shows the results of learning a non-flat HRM using LHRM, and the results of learning a flat HRM using several approaches (LHRM, DeepSynth, LRM and JIRP). An extended discussion of these results can be found in Section 6.\n\nG EXAMPLES OF HIERARCHIES OF REWARD MACHINES\n\nFigures 14 and 15 show the minimal root RMs for the CRAFTWORLD and WATERWORLD tasks, respectively. In the case of CRAFTWORLD, since two or more propositions can never occur simultaneously, the mutual exclusivity between formulas could be enforced differently. These RMs correspond to the settings without dead-ends; thus, they do not include rejecting states.\n\n5The code of these methods is publicly available: DeepSynth (https://github.com/grockious/ deepsynth, MIT license), LRM (https://bitbucket.org/RToroIcarte/lrm, no license), and JIRP (https://github.com/corazza/stochastic-reward-machines, no license). The first two links can be found in the papers, whereas the last one was provided by one of the authors through personal communication.\n\n6ILASP is free to use for research and education (see https://www.ilasp.com/terms). 7Disclaimer: At the time of writing this revised version (November 10), this experiment consisting of 5 runs has not finished. However, we are confident the observations we have made still will hold when it is completed.\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\n(a) CRAFTWORLD – OP\n\n(b) CRAFTWORLD – OPL\n\n(c) CRAFTWORLD – FR\n\n(d) WATERWORLD – WOD\n\nFigure 11: Learning curves comparing the performance of handcrafted non-flat and flat HRMs.\n\n(e) WATERWORLD – WD\n\n39\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Results of LHRM in CRAFTWORLD for the default case.\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(a) OP\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n11.1 (1.7) 0.9 (0.0) 135.4 (73.3) 0.9 (0.0) 0.8 (0.1) 18.0 (3.5) 0.8 (0.1) 191.2 (36.4) 549.4 (149.5) 1.5 (0.2) 17.9 (1.4) 7.3 (1.2) 74.5 (25.7)\n\n17.8 (1.9) 3.6 (0.2) 18.6 (1.6) 3.8 (0.2) 3.4 (0.2) 19.8 (1.2) 3.2 (0.2) 22.8 (2.6) 33.4 (3.2) 4.6 (0.4) 19.6 (1.1) 12.4 (1.2) 26.4 (3.7)\n\n5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 5.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5.2 (0.2) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 5.8 (0.2) 5.6 (0.2) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.2 (0.2)\n\n5 1009.8 (122.3) 189.4 (4.1)\n\n(b) OPL\n\n102)\n\nG\n\nI\n\n( ×\n\n1.8 (0.1) 1.7 (0.1) 1.8 (0.2) 1.8 (0.1) 1.6 (0.1) 2.1 (0.1) 1.7 (0.2) 6.0 (0.2) 6.0 (0.2) 6.8 (0.5) 3.8 (0.1) 3.8 (0.1) 2.1 (0.1)\n\n12.2 (0.7) 10.0 (0.0) 11.8 (0.6) 10.0 (0.0) 10.0 (0.0) 13.2 (0.4) 10.0 (0.0) 11.4 (0.7) 12.2 (0.6) 10.0 (0.0) 10.0 (0.0) 10.2 (0.2) 10.2 (0.2)\n\n11.6 (1.4) 1.6 (0.2) 12.8 (1.4) 1.8 (0.2) 1.4 (0.2) 12.6 (1.1) 1.2 (0.2) 17.4 (2.2) 27.2 (2.9) 2.6 (0.4) 16.6 (1.1) 9.2 (1.2) 23.2 (3.6)\n\nExample Length\n\nG\n\nI\n\n26.5 (2.1) 19.4 (1.1) 28.7 (1.9) 16.7 (1.7) 19.8 (2.0) 29.6 (2.5) 17.7 (1.6) 20.5 (1.8) 29.5 (3.2) 11.6 (0.7) 27.2 (1.3) 16.9 (0.8) 38.4 (0.9)\n\n24.2 (3.2) 19.3 (5.7) 20.3 (2.8) 17.9 (4.4) 40.6 (27.0) 24.4 (3.2) 17.5 (3.2) 24.8 (1.5) 28.7 (1.7) 15.3 (4.3) 20.8 (1.4) 14.3 (1.7) 22.7 (1.6)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n13.7 (2.9) 1.8 (0.2) 13.1 (1.7) 1.9 (0.2) 2.0 (0.2) 11.3 (1.2) 1.7 (0.1) 427.8 (201.6) 647.9 (110.7) 2.1 (0.2) 18.7 (2.3) 7.7 (0.7) 472.9 (216.6)\n\n23.0 (3.0) 7.2 (0.6) 22.0 (1.7) 7.0 (0.5) 7.6 (0.6) 22.0 (1.2) 6.4 (0.4) 32.6 (4.2) 38.6 (3.6) 5.4 (0.4) 16.6 (1.3) 12.2 (0.9) 36.0 (6.0)\n\n6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 5.0 (0.0)\n\n9.2 (0.2) 4.0 (0.0) 9.2 (0.2) 4.0 (0.0) 4.0 (0.0) 9.2 (0.2) 4.0 (0.0) 6.6 (0.2) 6.4 (0.2) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.6 (0.2)\n\n5 1622.6 (328.7) 236.6 (9.3)\n\n102)\n\n( ×\n12.0 (1.0) 8.0 (0.5) 10.4 (1.4) 6.9 (0.5) 7.7 (1.1) 12.8 (1.5) 6.5 (0.7) 5.6 (0.2) 5.6 (0.2) 7.6 (0.5) 3.7 (0.2) 3.8 (0.2) 2.1 (0.0)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n11.4 (0.4) 10.2 (0.2) 11.0 (0.6) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 12.0 (0.3) 11.2 (0.4) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0)\n\n7.0 (1.2) 2.2 (0.2) 6.8 (1.0) 2.4 (0.2) 3.0 (0.3) 6.4 (0.7) 2.4 (0.2) 3.6 (0.7) 3.8 (0.9) 1.4 (0.4) 0.4 (0.2) 0.2 (0.2) 1.6 (0.4)\n\n10.6 (1.6) 2.8 (0.4) 10.2 (1.0) 2.6 (0.4) 2.6 (0.4) 11.0 (0.9) 2.0 (0.3) 23.0 (3.4) 29.6 (3.5) 2.0 (0.0) 13.2 (1.4) 9.0 (0.9) 31.4 (5.7)\n\n20.4 (1.1) 10.2 (0.5) 17.2 (1.6) 11.1 (0.9) 10.1 (0.9) 15.3 (1.3) 9.6 (0.6) 21.6 (1.5) 23.1 (1.0) 11.1 (0.5) 29.0 (1.1) 16.0 (0.9) 39.5 (1.2)\n\n18.7 (1.6) 13.4 (1.9) 20.9 (1.9) 16.9 (5.6) 18.9 (3.3) 13.5 (1.0) 15.3 (3.6) 25.9 (3.4) 27.8 (4.6) 26.3 (6.5) 6.2 (5.5) 1.6 (1.6) 41.5 (8.6)\n\n12.1 (1.7) 6.8 (1.7) 14.3 (0.8) 8.9 (3.3) 5.6 (0.8) 12.1 (1.4) 16.6 (9.2) 23.7 (1.3) 26.1 (0.4) 15.2 (5.8) 27.8 (1.4) 16.3 (1.3) 26.9 (0.8)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(c) FR\n\n(\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n12.3 (1.7) 1.2 (0.1) 14.1 (1.6) 1.1 (0.1) 1.2 (0.0) 8.9 (0.9) 1.1 (0.1) 220.2 (83.3) 628.3 (85.4) 1.9 (0.2) 12.9 (2.2) 7.2 (0.6) 121.1 (41.1)\n\n17.6 (1.3) 3.8 (0.2) 20.2 (1.7) 3.6 (0.2) 4.0 (0.0) 16.0 (0.8) 3.8 (0.2) 25.2 (3.4) 37.8 (3.7) 5.0 (0.3) 15.6 (1.7) 12.0 (0.7) 34.0 (4.8)\n\n5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 5.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5.4 (0.2) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 5.6 (0.2) 5.6 (0.2) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.0 (0.0)\n\n5 1031.6 (150.3) 198.6 (11.3)\n\n(d) FRL\n\n102)\n\n×\n\n9.2 (1.2) 6.7 (0.9) 9.8 (0.7) 4.5 (0.7) 4.9 (0.9) 9.4 (1.7) 5.2 (0.3) 6.1 (0.2) 5.8 (0.1) 9.8 (0.7) 3.9 (0.1) 3.9 (0.2) 2.2 (0.0)\n\nG\n\nI\n\n11.6 (0.4) 10.0 (0.0) 11.6 (0.6) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 10.2 (0.2) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0)\n\n12.0 (1.2) 1.8 (0.2) 14.6 (1.2) 1.6 (0.2) 2.0 (0.0) 11.4 (0.6) 1.8 (0.2) 21.0 (3.4) 33.8 (3.7) 3.0 (0.3) 12.6 (1.7) 9.0 (0.7) 31.0 (4.8)\n\nExample Length\n\nG\n\nI\n\n30.3 (2.3) 16.6 (2.5) 26.5 (0.8) 13.4 (1.3) 12.4 (1.1) 25.4 (0.3) 15.3 (1.7) 21.9 (1.0) 26.4 (1.0) 13.2 (0.7) 29.0 (1.5) 18.9 (0.9) 42.2 (1.7)\n\n27.8 (2.0) 28.5 (4.1) 26.5 (2.1) 16.7 (3.6) 10.9 (2.5) 25.5 (2.7) 21.0 (10.1) 18.4 (0.7) 21.4 (0.7) 12.8 (3.2) 13.3 (0.8) 10.1 (1.0) 16.2 (1.1)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n11.3 (1.4) 2.3 (0.2) 13.0 (1.9) 2.5 (0.3) 2.2 (0.1) 11.6 (1.1) 2.7 (0.2) 301.7 (98.1) 754.1 (158.2) 2.8 (0.1) 19.8 (2.9) 8.8 (0.9) 344.0 (87.7)\n\n23.4 (2.5) 7.0 (0.3) 24.6 (2.2) 7.8 (0.7) 7.0 (0.3) 23.8 (1.5) 8.4 (0.7) 36.4 (1.9) 44.6 (2.6) 6.6 (0.2) 19.6 (1.6) 12.6 (1.0) 46.2 (4.9)\n\n6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 5.0 (0.0)\n\n9.2 (0.2) 4.0 (0.0) 9.4 (0.2) 4.0 (0.0) 4.0 (0.0) 9.6 (0.2) 4.0 (0.0) 6.8 (0.2) 7.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.8 (0.2)\n\n5 1476.8 (175.3) 268.0 (6.5)\n\n102)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n( ×\n\n468.4 (121.9) 129.5 (69.4) 550.8 (156.4) 89.0 (18.0) 82.7 (18.8) 458.9 (61.0) 103.5 (39.5) 5.3 (0.1) 5.5 (0.2) 6.9 (0.4) 4.3 (0.1) 4.0 (0.1) 2.8 (0.1)\n\n10.4 (0.2) 10.2 (0.2) 10.4 (0.2) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 10.2 (0.2) 10.2 (0.2) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0)\n\n7.6 (0.9) 2.8 (0.2) 7.8 (1.0) 3.2 (0.4) 3.0 (0.0) 8.0 (0.9) 3.6 (0.4) 5.0 (0.7) 5.2 (0.4) 2.0 (0.0) 0.8 (0.4) 1.2 (0.5) 2.8 (0.7)\n\n11.4 (1.9) 2.0 (0.3) 12.4 (1.2) 2.6 (0.4) 2.0 (0.3) 11.2 (1.2) 2.8 (0.5) 27.2 (1.9) 35.2 (2.3) 2.6 (0.2) 15.8 (1.2) 8.4 (0.7) 40.4 (4.5)\n\n11.9 (0.6) 7.8 (0.5) 12.5 (1.6) 7.3 (0.4) 6.9 (0.7) 11.9 (0.6) 8.2 (0.7) 21.7 (1.1) 25.6 (0.5) 12.5 (0.8) 28.4 (1.1) 19.3 (1.3) 44.5 (2.3)\n\n10.1 (1.3) 9.9 (1.7) 9.4 (1.0) 9.3 (1.7) 10.2 (1.8) 13.1 (2.7) 10.1 (1.9) 18.8 (2.2) 20.4 (2.9) 13.1 (3.7) 2.7 (1.3) 3.7 (2.0) 21.8 (2.2)\n\n9.9 (0.4) 6.4 (2.1) 8.4 (0.5) 3.7 (0.4) 4.7 (2.7) 9.2 (0.8) 5.0 (1.1) 16.1 (0.6) 18.7 (0.6) 7.4 (2.2) 13.5 (0.9) 10.7 (2.0) 17.3 (1.0)\n\n40\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Results of LHRM in CRAFTWORLD with a restricted set of callable RMs.\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(a) OP\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n11.2 (1.6) 0.9 (0.0) 15.5 (4.2) 0.9 (0.0) 0.9 (0.0) 18.2 (3.5) 0.8 (0.0) 45.8 (4.5) 64.1 (10.6) 1.2 (0.1) 4.5 (0.8) 3.5 (0.5) 9.1 (0.9)\n\n17.8 (1.9) 3.6 (0.2) 18.6 (1.6) 3.8 (0.2) 3.4 (0.2) 19.8 (1.2) 3.2 (0.2) 19.6 (0.9) 22.0 (2.6) 4.4 (0.4) 10.2 (1.4) 9.6 (1.3) 17.0 (0.9)\n\n5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 5.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5.2 (0.2) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 5.6 (0.2) 5.2 (0.2) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.2 (0.2)\n\n5 176.6 (13.1) 153.0 (3.6)\n\n(b) OPL\n\n102)\n\nG\n\nI\n\n( ×\n\n1.8 (0.1) 1.7 (0.1) 1.8 (0.2) 1.8 (0.1) 1.6 (0.1) 2.1 (0.1) 1.7 (0.2) 6.0 (0.2) 6.1 (0.2) 6.8 (0.3) 3.9 (0.1) 3.9 (0.1) 2.1 (0.1)\n\n12.2 (0.7) 10.0 (0.0) 11.8 (0.6) 10.0 (0.0) 10.0 (0.0) 13.2 (0.4) 10.0 (0.0) 11.2 (1.0) 10.8 (0.4) 10.0 (0.0) 10.0 (0.0) 10.2 (0.2) 10.0 (0.0)\n\n11.6 (1.4) 1.6 (0.2) 12.8 (1.4) 1.8 (0.2) 1.4 (0.2) 12.6 (1.1) 1.2 (0.2) 14.4 (0.9) 17.2 (2.7) 2.4 (0.4) 7.2 (1.4) 6.4 (1.2) 14.0 (0.9)\n\nExample Length\n\nG\n\nI\n\n26.5 (2.1) 19.4 (1.1) 28.7 (1.9) 16.7 (1.7) 19.8 (2.0) 29.6 (2.5) 17.7 (1.6) 21.6 (1.8) 22.5 (1.6) 12.1 (0.7) 26.1 (0.8) 17.4 (0.5) 37.5 (1.9)\n\n24.2 (3.2) 19.3 (5.7) 20.3 (2.8) 17.9 (4.4) 40.6 (27.0) 24.4 (3.2) 17.5 (3.2) 21.0 (1.7) 23.0 (1.2) 15.3 (1.6) 22.4 (0.9) 12.5 (0.8) 18.0 (1.9)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n13.9 (3.0) 1.8 (0.1) 13.2 (1.7) 1.9 (0.1) 2.0 (0.2) 11.5 (1.3) 1.6 (0.1) 69.0 (20.5) 76.5 (6.0) 1.7 (0.2) 5.3 (0.9) 4.0 (0.9) 16.2 (0.4)\n\n23.0 (3.0) 7.2 (0.6) 22.0 (1.7) 7.0 (0.5) 7.6 (0.6) 22.0 (1.2) 6.4 (0.4) 21.8 (2.2) 24.2 (1.3) 6.0 (0.6) 10.8 (1.4) 9.8 (1.7) 20.8 (0.2)\n\n6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 5.0 (0.0)\n\n9.2 (0.2) 4.0 (0.0) 9.2 (0.2) 4.0 (0.0) 4.0 (0.0) 9.2 (0.2) 4.0 (0.0) 6.2 (0.2) 6.4 (0.2) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5 218.6 (21.1) 188.6 (5.4)\n\n102)\n\n( ×\n12.0 (1.0) 8.0 (0.5) 10.4 (1.4) 6.9 (0.5) 7.7 (1.1) 12.8 (1.5) 6.5 (0.7) 5.5 (0.1) 5.7 (0.3) 7.5 (0.7) 3.7 (0.1) 3.8 (0.1) 2.1 (0.1)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n11.4 (0.4) 10.2 (0.2) 11.0 (0.6) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 10.4 (0.2) 11.6 (0.8) 10.2 (0.2) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0)\n\n7.0 (1.2) 2.2 (0.2) 6.8 (1.0) 2.4 (0.2) 3.0 (0.3) 6.4 (0.7) 2.4 (0.2) 5.2 (0.9) 4.0 (0.3) 1.4 (0.2) 1.0 (0.5) 1.6 (0.7) 3.2 (0.2)\n\n10.6 (1.6) 2.8 (0.4) 10.2 (1.0) 2.6 (0.4) 2.6 (0.4) 11.0 (0.9) 2.0 (0.3) 12.2 (1.9) 14.6 (1.0) 2.4 (0.2) 6.8 (0.9) 5.2 (1.2) 14.6 (0.2)\n\n20.4 (1.1) 10.2 (0.5) 17.2 (1.6) 11.1 (0.9) 10.1 (0.9) 15.3 (1.3) 9.6 (0.6) 20.4 (1.3) 24.8 (3.0) 11.7 (0.7) 27.7 (1.0) 18.4 (0.7) 38.1 (0.9)\n\n18.7 (1.6) 13.4 (1.9) 20.9 (1.9) 16.9 (5.6) 18.9 (3.3) 13.5 (1.0) 15.3 (3.6) 21.2 (2.0) 21.4 (2.1) 25.4 (6.4) 11.2 (5.4) 8.3 (2.9) 22.5 (3.3)\n\n12.1 (1.7) 6.8 (1.7) 14.3 (0.8) 8.9 (3.3) 5.6 (0.8) 12.1 (1.4) 16.6 (9.2) 20.8 (1.7) 25.7 (0.8) 14.2 (3.4) 21.1 (1.7) 15.6 (1.7) 25.8 (1.7)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(c) FR\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n12.6 (1.8) 1.2 (0.1) 14.1 (1.5) 1.1 (0.1) 1.2 (0.1) 9.3 (0.8) 1.4 (0.2) 43.8 (13.0) 85.2 (13.4) 1.4 (0.1) 6.3 (0.9) 4.8 (0.6) 12.5 (1.8)\n\n17.6 (1.3) 3.8 (0.2) 20.2 (1.7) 3.6 (0.2) 4.0 (0.0) 16.0 (0.8) 3.8 (0.2) 20.0 (1.9) 22.2 (2.5) 4.4 (0.2) 13.2 (1.7) 11.8 (1.3) 20.8 (2.5)\n\n5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 5.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5.4 (0.2) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 5.4 (0.2) 5.2 (0.2) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.0 (0.0)\n\n5 194.9 (17.6) 161.4 (7.0)\n\n(d) FRL\n\n102)\n\nG\n\nI\n\n( ×\n\n9.2 (1.2) 6.7 (0.9) 9.8 (0.7) 4.5 (0.7) 4.9 (0.9) 9.4 (1.7) 5.2 (0.3) 6.0 (0.1) 5.9 (0.1) 10.2 (0.9) 3.8 (0.1) 3.8 (0.1) 2.3 (0.1)\n\n11.6 (0.4) 10.0 (0.0) 11.6 (0.6) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 10.0 (0.0) 10.2 (0.2) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 10.0 (0.0)\n\n12.0 (1.2) 1.8 (0.2) 14.6 (1.2) 1.6 (0.2) 2.0 (0.0) 11.4 (0.6) 1.8 (0.2) 16.0 (1.9) 18.0 (2.6) 2.4 (0.2) 10.2 (1.7) 8.8 (1.3) 17.8 (2.5)\n\nExample Length\n\nG\n\nI\n\n30.3 (2.3) 16.6 (2.5) 26.5 (0.8) 13.4 (1.3) 12.4 (1.1) 25.4 (0.3) 15.3 (1.7) 21.9 (1.0) 26.5 (0.9) 13.0 (0.8) 30.6 (2.0) 19.8 (0.7) 44.2 (2.6)\n\n27.8 (2.0) 28.5 (4.1) 26.5 (2.1) 16.7 (3.6) 10.9 (2.5) 25.5 (2.7) 21.0 (10.1) 14.7 (1.4) 18.2 (1.2) 12.2 (2.8) 11.9 (1.2) 8.6 (1.0) 13.1 (1.2)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n11.2 (1.4) 2.4 (0.1) 13.1 (1.9) 2.5 (0.4) 2.1 (0.1) 11.6 (1.2) 2.6 (0.2) 62.2 (13.2) 131.3 (28.0) 2.7 (0.7) 6.8 (0.6) 5.4 (0.6) 16.3 (1.2)\n\n23.4 (2.5) 7.0 (0.3) 24.6 (2.2) 7.8 (0.7) 7.0 (0.3) 23.8 (1.5) 8.4 (0.7) 27.4 (2.2) 34.0 (3.0) 6.6 (0.6) 12.6 (0.7) 12.2 (1.0) 21.2 (1.0)\n\n6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 5.0 (0.0)\n\n9.2 (0.2) 4.0 (0.0) 9.4 (0.2) 4.0 (0.0) 4.0 (0.0) 9.6 (0.2) 4.0 (0.0) 6.6 (0.2) 6.6 (0.2) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5 270.1 (34.6) 216.0 (5.1)\n\n102)\n\n(\n\n×\n\n468.4 (121.9) 129.5 (69.4) 550.8 (156.4) 89.0 (18.0) 82.7 (18.8) 458.9 (61.0) 103.5 (39.5) 5.3 (0.1) 5.5 (0.2) 6.8 (0.3) 4.4 (0.2) 4.0 (0.1) 2.8 (0.0)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n10.4 (0.2) 10.2 (0.2) 10.4 (0.2) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 10.2 (0.2) 10.2 (0.2) 10.0 (0.0) 10.0 (0.0) 10.2 (0.2) 10.0 (0.0)\n\n7.6 (0.9) 2.8 (0.2) 7.8 (1.0) 3.2 (0.4) 3.0 (0.0) 8.0 (0.9) 3.6 (0.4) 5.6 (0.6) 6.8 (0.7) 2.2 (0.2) 1.6 (0.5) 1.0 (0.4) 2.6 (0.2)\n\n11.4 (1.9) 2.0 (0.3) 12.4 (1.2) 2.6 (0.4) 2.0 (0.3) 11.2 (1.2) 2.8 (0.5) 17.6 (1.7) 23.0 (2.4) 2.4 (0.4) 8.0 (0.3) 8.0 (0.4) 15.6 (1.2)\n\n11.9 (0.6) 7.8 (0.5) 12.5 (1.6) 7.3 (0.4) 6.9 (0.7) 11.9 (0.6) 8.2 (0.7) 23.0 (1.0) 26.2 (0.7) 12.0 (0.8) 32.3 (2.3) 20.4 (1.7) 47.7 (1.9)\n\n10.1 (1.3) 9.9 (1.7) 9.4 (1.0) 9.3 (1.7) 10.2 (1.8) 13.1 (2.7) 10.1 (1.9) 16.5 (2.0) 16.9 (1.6) 9.4 (1.0) 4.9 (1.5) 2.9 (1.4) 15.0 (0.7)\n\n9.9 (0.4) 6.4 (2.1) 8.4 (0.5) 3.7 (0.4) 4.7 (2.7) 9.2 (0.8) 5.0 (1.1) 13.4 (1.0) 14.5 (0.5) 9.9 (2.3) 11.6 (1.1) 10.3 (1.2) 16.0 (0.9)\n\n41\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Learning curves comparing the performance of handcrafted non-flat and flat HRMs in the TENPAPERS task (FRL setting).\n\nTable 5: Results of LHRM in CRAFTWORLD without exploration using options.\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(a) OP\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5\n\n11.2 (1.7) 0.9 (0.0) 15.6 (4.1) 0.9 (0.1) 0.9 (0.1) 18.3 (3.6) 0.9 (0.0) 529.0 (164.2) 1924.2 (443.5) 1.6 (0.2) 42.7 (10.1) 8.1 (0.8) 198.3 (47.5)\n\n17.8 (1.9) 3.6 (0.2) 18.6 (1.6) 3.8 (0.2) 3.4 (0.2) 19.8 (1.2) 3.2 (0.2) 21.2 (1.4) 28.0 (3.8) 4.4 (0.4) 24.6 (3.9) 11.8 (1.0) 43.0 (5.3)\n\n5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 3.0 (0.0) 5.0 (0.0) 3.0 (0.0) 5.0 (0.0) 5.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5.2 (0.2) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 2.0 (0.0) 5.2 (0.2) 2.0 (0.0) 5.8 (0.2) 5.4 (0.2) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.8 (0.2)\n\n5 2752.8 (503.2) 203.2 (11.8)\n\n(b) OPL\n\n102)\n\nG\n\nI\n\n( ×\n\n1.8 (0.1) 1.7 (0.1) 1.8 (0.2) 1.8 (0.1) 1.6 (0.1) 2.1 (0.1) 1.7 (0.2) 6.8 (0.2) 7.8 (0.4) 6.1 (0.3) 6.8 (0.2) 4.9 (0.1) 5.5 (0.2)\n\n12.2 (0.7) 10.0 (0.0) 11.8 (0.6) 10.0 (0.0) 10.0 (0.0) 13.2 (0.4) 10.0 (0.0) 10.2 (0.2) 10.4 (0.2) 10.0 (0.0) 10.0 (0.0) 10.2 (0.2) 10.0 (0.0)\n\n11.6 (1.4) 1.6 (0.2) 12.8 (1.4) 1.8 (0.2) 1.4 (0.2) 12.6 (1.1) 1.2 (0.2) 17.0 (1.5) 23.6 (3.7) 2.4 (0.4) 21.6 (3.9) 8.6 (1.2) 40.0 (5.3)\n\nExample Length\n\nG\n\nI\n\n26.5 (2.1) 19.4 (1.1) 28.7 (1.9) 16.7 (1.7) 19.8 (2.0) 29.6 (2.5) 17.7 (1.6) 33.0 (2.6) 40.1 (1.0) 16.0 (1.0) 55.8 (2.7) 31.1 (0.7) 65.0 (0.9)\n\n24.2 (3.2) 19.3 (5.7) 20.3 (2.8) 17.9 (4.4) 40.6 (27.0) 24.4 (3.2) 17.5 (3.2) 23.7 (1.3) 29.4 (1.3) 14.2 (1.3) 21.2 (1.1) 13.1 (0.8) 22.0 (0.9)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n4\n\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n1\n\n14.1 (3.2) 1.8 (0.1) 13.5 (1.8) 1.8 (0.1) 2.0 (0.2) 11.8 (1.3) 1.6 (0.1) 224.8 (71.6) 339.9 (33.6) 3.5 (0.3) 19.0 (2.2) 11.4 (2.1) 277.4 (0.0)\n\n23.0 (3.0) 7.2 (0.6) 22.0 (1.7) 7.0 (0.5) 7.6 (0.6) 22.0 (1.2) 6.4 (0.4) 27.0 (1.9) 33.0 (2.8) 8.2 (0.6) 15.4 (1.5) 14.4 (1.7) 33.0 (0.0)\n\n6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 5.0 (0.0)\n\n9.2 (0.2) 4.0 (0.0) 9.2 (0.2) 4.0 (0.0) 4.0 (0.0) 9.2 (0.2) 4.0 (0.0) 6.4 (0.2) 6.4 (0.2) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0)\n\n5 701.0 (111.2) 199.8 (6.9)\n\n(\n\n102)\n\n× 12.0 (1.0) 8.0 (0.5) 10.4 (1.4) 6.9 (0.5) 7.7 (1.1) 12.8 (1.5) 6.5 (0.7) 139.7 (21.8) 204.8 (27.1) 47.6 (3.7) 383.4 (83.7) 87.4 (8.9) 264.1 (0.0)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n11.4 (0.4) 10.2 (0.2) 11.0 (0.6) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 11.6 (0.4) 10.6 (0.2) 10.2 (0.2) 10.0 (0.0) 10.4 (0.2) 10.0 (0.0)\n\n7.0 (1.2) 2.2 (0.2) 6.8 (1.0) 2.4 (0.2) 3.0 (0.3) 6.4 (0.7) 2.4 (0.2) 3.2 (0.4) 2.8 (0.5) 2.6 (0.4) 1.0 (0.3) 1.0 (0.4) 2.0 (0.0)\n\n10.6 (1.6) 2.8 (0.4) 10.2 (1.0) 2.6 (0.4) 2.6 (0.4) 11.0 (0.9) 2.0 (0.3) 18.2 (1.4) 25.6 (2.5) 3.4 (0.4) 11.4 (1.3) 10.0 (1.3) 28.0 (0.0)\n\n20.4 (1.1) 10.2 (0.5) 17.2 (1.6) 11.1 (0.9) 10.1 (0.9) 15.3 (1.3) 9.6 (0.6) 22.0 (1.6) 25.4 (0.8) 10.3 (0.7) 38.2 (1.6) 19.7 (1.2) 46.7 (0.0)\n\n18.7 (1.6) 13.4 (1.9) 20.9 (1.9) 16.9 (5.6) 18.9 (3.3) 13.5 (1.0) 15.3 (3.6) 24.7 (6.5) 21.8 (3.1) 16.2 (1.7) 14.1 (4.9) 8.7 (4.9) 36.0 (0.0)\n\n12.1 (1.7) 6.8 (1.7) 14.3 (0.8) 8.9 (3.3) 5.6 (0.8) 12.1 (1.4) 16.6 (9.2) 23.5 (1.2) 25.2 (1.1) 14.2 (1.8) 23.8 (1.0) 17.6 (1.2) 22.9 (0.0)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\n(c) FRL\n\n102)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\nBATTER BUCKET COMPASS LEATHER PAPER QUILL SUGAR BOOK MAP MILKBUCKET BOOKQUILL MILKB.SUGAR CAKE\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 5\n5 5\n5 5\n5 5\n3 5\n0 0\n0\n\n5 5\n5 5\n5 5\n5 0\n0 2\n0 0\n0\n\n11.1 (1.4) 2.2 (0.1) 12.9 (1.9) 2.8 (0.4) 2.1 (0.1) 11.6 (1.1) 2.6 (0.3) 0.0 (0.0) 0.0 (0.0) 4.7 (0.5) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n23.4 (2.5) 7.0 (0.3) 24.6 (2.2) 7.8 (0.7) 7.0 (0.3) 23.8 (1.5) 8.4 (0.7) 0.0 (0.0) 0.0 (0.0) 11.0 (1.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n6.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 4.0 (0.0) 6.0 (0.0) 4.0 (0.0) 0.0 (0.0) 0.0 (0.0) 4.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n9.2 (0.2) 4.0 (0.0) 9.4 (0.2) 4.0 (0.0) 4.0 (0.0) 9.6 (0.2) 4.0 (0.0) 0.0 (0.0) 0.0 (0.0) 3.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n5 47.1 (0.9) 106.4 (2.9)\n\n10.4 (0.2) 10.2 (0.2) 10.4 (0.2) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 10.0 (0.0) 0.0 (0.0) 0.0 (0.0) 10.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n7.6 (0.9) 2.8 (0.2) 7.8 (1.0) 3.2 (0.4) 3.0 (0.0) 8.0 (0.9) 3.6 (0.4) 0.0 (0.0) 0.0 (0.0) 2.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n11.4 (1.9) 2.0 (0.3) 12.4 (1.2) 2.6 (0.4) 2.0 (0.3) 11.2 (1.2) 2.8 (0.5) 0.0 (0.0) 0.0 (0.0) 7.0 (1.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n11.9 (0.6) 7.8 (0.5) 12.5 (1.6) 7.3 (0.4) 6.9 (0.7) 11.9 (0.6) 8.2 (0.7) 0.0 (0.0) 0.0 (0.0) 8.2 (0.4) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n10.1 (1.3) 9.9 (1.7) 9.4 (1.0) 9.3 (1.7) 10.2 (1.8) 13.1 (2.7) 10.1 (1.9) 0.0 (0.0) 0.0 (0.0) 10.2 (1.7) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n9.9 (0.4) 6.4 (2.1) 8.4 (0.5) 3.7 (0.4) 4.7 (2.7) 9.2 (0.8) 5.0 (1.1) 0.0 (0.0) 0.0 (0.0) 8.8 (0.2) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n( ×\n\n468.4 (121.9) 129.5 (69.4) 550.8 (156.4) 89.0 (18.0) 82.7 (18.8) 458.9 (61.0) 103.5 (39.5) 0.0 (0.0) 0.0 (0.0) 885.6 (142.3) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0)\n\n42\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Results of LHRM in WATERWORLD for the default case.\n\n(a) WOD\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nRG BC MY RG&BC BC&MY RG&MY RGB CMY RGB&CMY\n\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5\n\n0.9 (0.0) 0.9 (0.1) 0.9 (0.0) 4.5 (0.3) 5.8 (1.0) 4.7 (0.5) 1.2 (0.1) 1.4 (0.2) 15.1 (1.7)\n\n4.0 (0.0) 3.8 (0.2) 3.6 (0.2) 13.4 (0.4) 15.6 (2.1) 13.2 (1.0) 4.8 (0.5) 5.4 (0.7) 21.6 (1.7)\n\n3.0 (0.0) 3.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.0 (0.0) 3.0 (0.0) 4.0 (0.0)\n\n2.0 (0.0) 2.0 (0.0) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 2.0 (0.0) 2.0 (0.0) 4.0 (0.0)\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 35.4 (2.0) 85.4 (3.1)\n\n(b) WD\n\n102)\n\nG\n\nI\n\n( ×\n\n0.9 (0.1) 0.8 (0.1) 0.7 (0.0) 8.8 (0.3) 8.1 (0.2) 8.5 (0.2) 8.6 (0.2) 8.8 (0.5) 2.3 (0.0)\n\n10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 11.8 (0.6) 12.8 (1.3) 10.8 (0.2) 10.0 (0.0) 10.0 (0.0) 11.0 (0.4)\n\n2.0 (0.0) 1.8 (0.2) 1.6 (0.2) 8.6 (0.7) 9.8 (1.5) 9.4 (0.9) 2.8 (0.5) 3.4 (0.7) 17.6 (1.7)\n\nExample Length\n\nG\n\nI\n\n11.2 (1.0) 10.8 (0.8) 8.7 (0.8) 12.2 (0.9) 13.2 (1.7) 12.2 (0.7) 7.8 (0.2) 8.0 (0.3) 17.3 (0.4)\n\n5.8 (1.1) 11.9 (3.4) 6.6 (1.9) 14.8 (1.2) 17.1 (1.6) 18.6 (1.2) 7.0 (1.4) 10.2 (1.3) 22.6 (1.6)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nRG BC MY RG&BC BC&MY RG&MY RGB CMY RGB&CMY\n\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5\n\n1.9 (0.2) 1.8 (0.3) 1.4 (0.1) 11.7 (2.5) 9.5 (1.5) 5.4 (0.5) 2.5 (0.3) 3.6 (0.4) 29.0 (4.1)\n\n7.8 (1.1) 7.2 (1.0) 5.8 (0.4) 24.0 (3.8) 20.8 (2.4) 14.0 (1.3) 8.2 (0.7) 11.4 (1.2) 31.4 (2.8)\n\n4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.8 (0.2) 4.8 (0.2) 4.2 (0.2) 4.0 (0.0) 4.0 (0.0) 4.4 (0.2)\n\n4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.8 (0.2) 4.8 (0.2) 4.2 (0.2) 3.0 (0.0) 3.0 (0.0) 4.4 (0.2)\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 67.0 (6.2) 130.6 (6.0)\n\n102)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n( ×\n\n3.0 (0.3) 2.7 (0.3) 2.9 (0.3) 12.0 (0.4) 11.5 (0.4) 11.8 (0.6) 11.9 (0.6) 10.8 (0.3) 4.9 (0.3)\n\n10.0 (0.0) 10.2 (0.2) 10.0 (0.0) 13.0 (0.7) 11.2 (0.6) 10.6 (0.2) 10.2 (0.2) 10.0 (0.0) 11.2 (0.4)\n\n3.0 (0.3) 2.4 (0.2) 2.4 (0.2) 6.2 (1.6) 5.2 (0.7) 3.2 (0.7) 3.0 (0.3) 4.2 (0.7) 5.4 (1.6)\n\n2.8 (0.9) 2.6 (0.7) 1.4 (0.2) 11.8 (1.9) 11.4 (1.6) 7.2 (1.1) 3.0 (0.5) 5.2 (0.6) 21.8 (1.3)\n\n7.0 (0.7) 8.4 (0.5) 6.9 (0.4) 11.7 (0.8) 10.6 (0.9) 9.8 (0.3) 7.9 (0.4) 7.9 (0.2) 16.6 (0.8)\n\n10.3 (1.6) 6.9 (1.5) 5.8 (1.5) 6.9 (1.5) 8.8 (1.4) 6.2 (1.9) 14.0 (1.8) 8.8 (1.6) 7.4 (0.9)\n\n4.2 (0.8) 6.3 (1.7) 4.6 (1.6) 11.8 (0.8) 13.2 (0.9) 11.6 (0.7) 10.6 (2.0) 10.5 (1.1) 17.2 (0.6)\n\nTable 7: Results of LHRM in WATERWORLD with a restricted set of callable RMs.\n\nTask\n\n# G # L Time (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(a) WOD\n\nRG BC MY RG&BC BC&MY RG&MY RGB CMY RGB&CMY\n\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5\n\n0.9 (0.0) 0.9 (0.1) 0.9 (0.0) 5.3 (0.4) 3.9 (0.1) 4.6 (0.3) 1.2 (0.1) 1.6 (0.2) 5.7 (0.8)\n\n4.0 (0.0) 3.8 (0.2) 3.6 (0.2) 15.2 (0.9) 12.4 (0.2) 13.8 (0.9) 4.8 (0.7) 6.2 (0.7) 15.0 (1.6)\n\n3.0 (0.0) 3.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.0 (0.0) 3.0 (0.0) 4.0 (0.0)\n\n2.0 (0.0) 2.0 (0.0) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 2.0 (0.0) 2.0 (0.0) 4.0 (0.0)\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 24.9 (0.9) 78.8 (2.7)\n\n(b) WD\n\n102)\n\nG\n\nI\n\n( ×\n\n0.9 (0.1) 0.8 (0.1) 0.7 (0.0) 8.6 (0.3) 8.3 (0.4) 8.5 (0.2) 8.7 (0.2) 8.6 (0.5) 2.6 (0.1)\n\n10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 12.4 (0.2) 11.8 (0.7) 10.2 (0.2) 10.2 (0.2) 10.0 (0.0) 10.4 (0.4)\n\n2.0 (0.0) 1.8 (0.2) 1.6 (0.2) 9.8 (0.8) 7.6 (0.7) 10.6 (0.9) 2.6 (0.7) 4.2 (0.7) 11.6 (1.6)\n\nExample Length\n\nG\n\nI\n\n11.2 (1.0) 10.8 (0.8) 8.7 (0.8) 14.7 (1.3) 11.2 (0.8) 10.7 (0.5) 8.3 (0.5) 8.0 (0.3) 17.0 (1.1)\n\n5.8 (1.1) 11.9 (3.4) 6.6 (1.9) 16.0 (0.8) 13.2 (1.0) 15.8 (1.6) 16.2 (3.8) 10.8 (1.2) 15.9 (1.3)\n\nTask\n\n# G # L Time (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nRG BC MY RG&BC BC&MY RG&MY RGB CMY RGB&CMY\n\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5\n\n1.9 (0.3) 1.8 (0.3) 1.4 (0.1) 6.9 (0.7) 9.3 (1.8) 7.8 (1.1) 2.1 (0.1) 2.3 (0.2) 9.6 (1.5)\n\n7.8 (1.1) 7.2 (1.0) 5.8 (0.4) 17.6 (1.5) 21.4 (2.9) 18.8 (1.9) 7.6 (0.2) 8.2 (0.8) 20.6 (2.6)\n\n4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.6 (0.2) 4.8 (0.2) 4.8 (0.2) 4.0 (0.0) 4.0 (0.0) 5.0 (0.0)\n\n4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.6 (0.2) 4.8 (0.2) 4.8 (0.2) 3.0 (0.0) 3.0 (0.0) 5.0 (0.0)\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 42.9 (3.7) 115.0 (7.5)\n\n102)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n10.0 (0.0) 10.2 (0.2) 10.0 (0.0) 10.8 (0.2) 12.2 (1.0) 11.0 (0.3) 10.0 (0.0) 10.0 (0.0) 10.2 (0.2)\n\n3.0 (0.3) 2.4 (0.2) 2.4 (0.2) 4.6 (0.5) 5.8 (1.1) 4.8 (0.4) 2.6 (0.2) 2.2 (0.4) 6.4 (0.9)\n\n2.8 (0.9) 2.6 (0.7) 1.4 (0.2) 9.2 (1.0) 10.4 (1.8) 10.0 (2.0) 3.0 (0.0) 4.0 (0.5) 11.0 (1.8)\n\n7.0 (0.7) 8.4 (0.5) 6.9 (0.4) 10.4 (0.5) 11.4 (0.7) 9.8 (0.2) 7.6 (0.5) 7.8 (0.2) 15.0 (0.5)\n\n10.3 (1.6) 6.9 (1.5) 5.8 (1.5) 9.4 (1.8) 6.9 (1.3) 8.6 (0.8) 11.8 (1.7) 9.9 (1.6) 12.3 (0.8)\n\n4.2 (0.8) 6.3 (1.7) 4.6 (1.6) 12.6 (0.7) 12.1 (0.7) 13.0 (0.8) 10.7 (1.7) 8.9 (0.5) 14.2 (1.2)\n\n( ×\n\n3.0 (0.3) 2.7 (0.3) 2.9 (0.3) 12.0 (0.4) 11.7 (0.5) 11.8 (0.5) 11.9 (0.5) 10.7 (0.2) 5.0 (0.6)\n\n43\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Results of LHRM in WATERWORLD without exploration using options.\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\n(a) WOD\n\nRG BC MY RG&BC BC&MY RG&MY RGB CMY RGB&CMY\n\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5\n\n0.9 (0.0) 0.9 (0.1) 0.9 (0.0) 4.2 (0.4) 4.3 (0.3) 4.6 (0.3) 1.2 (0.2) 1.4 (0.1) 16.1 (1.1)\n\n4.0 (0.0) 3.8 (0.2) 3.6 (0.2) 12.2 (0.9) 11.8 (0.7) 12.6 (0.7) 4.6 (0.7) 5.0 (0.5) 19.8 (1.1)\n\n3.0 (0.0) 3.0 (0.0) 3.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 3.0 (0.0) 3.0 (0.0) 4.0 (0.0)\n\n2.0 (0.0) 2.0 (0.0) 2.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 2.0 (0.0) 2.0 (0.0) 4.0 (0.0)\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 34.4 (1.4) 77.4 (2.0)\n\n(b) WD\n\n102)\n\nG\n\nI\n\n( ×\n\n0.9 (0.1) 0.8 (0.1) 0.7 (0.0) 9.5 (0.3) 9.8 (0.1) 9.5 (0.1) 9.0 (0.1) 8.8 (0.2) 4.1 (0.1)\n\n10.0 (0.0) 10.0 (0.0) 10.0 (0.0) 10.6 (0.2) 11.6 (0.2) 11.2 (0.4) 10.0 (0.0) 10.0 (0.0) 11.2 (0.6)\n\n2.0 (0.0) 1.8 (0.2) 1.6 (0.2) 8.6 (1.1) 7.2 (0.6) 8.4 (0.7) 2.6 (0.7) 3.0 (0.5) 15.6 (1.3)\n\nExample Length\n\nG\n\nI\n\n11.2 (1.0) 10.8 (0.8) 8.7 (0.8) 13.8 (0.2) 15.3 (0.9) 14.2 (0.9) 9.4 (0.4) 8.8 (0.2) 26.0 (1.2)\n\n5.8 (1.1) 11.9 (3.4) 6.6 (1.9) 15.3 (1.6) 16.7 (1.7) 14.8 (0.8) 8.7 (1.7) 10.6 (1.5) 21.6 (0.9)\n\nTask\n\n# G # L\n\nTime (s.)\n\nCalls\n\nStates\n\nEdges\n\nEp. First HRM\n\n# Examples\n\nExample Length\n\nRG BC MY RG&BC BC&MY RG&MY RGB CMY RGB&CMY\n\n5 5\n5 5\n5 5\n5 5\n5\n\n5 5\n5 5\n5 5\n5 5\n5\n\n1.9 (0.3) 1.8 (0.2) 1.4 (0.1) 8.1 (1.4) 6.2 (0.5) 8.6 (1.8) 2.3 (0.1) 4.4 (0.6) 32.1 (5.0)\n\n7.8 (1.1) 7.2 (1.0) 5.8 (0.4) 18.2 (2.2) 15.6 (0.7) 19.2 (2.7) 7.6 (0.2) 13.2 (1.5) 31.4 (3.3)\n\n4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.6 (0.2) 4.6 (0.2) 4.4 (0.2) 4.0 (0.0) 3.8 (0.2) 4.4 (0.2)\n\n4.0 (0.0) 4.0 (0.0) 4.0 (0.0) 4.6 (0.2) 4.6 (0.2) 4.4 (0.2) 3.0 (0.0) 3.0 (0.0) 4.4 (0.2)\n\nCompleted Runs Total Time (s.) Total Calls\n\n5 66.7 (6.6) 126.0 (6.3)\n\n102)\n\nG\n\nD\n\nI\n\nG\n\nD\n\nI\n\n( ×\n\n3.0 (0.3) 2.7 (0.3) 2.9 (0.3) 97.4 (4.2) 91.5 (5.8) 90.3 (5.3) 65.3 (1.6) 59.2 (2.9) 125.7 (9.9)\n\n10.0 (0.0) 10.2 (0.2) 10.0 (0.0) 10.8 (0.4) 10.6 (0.2) 11.2 (0.8) 10.2 (0.2) 10.2 (0.2) 11.4 (0.5)\n\n3.0 (0.3) 2.4 (0.2) 2.4 (0.2) 5.2 (0.6) 4.6 (0.6) 5.6 (0.9) 2.6 (0.4) 3.8 (0.6) 5.8 (1.2)\n\n2.8 (0.9) 2.6 (0.7) 1.4 (0.2) 9.2 (1.4) 7.4 (0.7) 9.4 (1.3) 2.8 (0.4) 7.2 (1.0) 21.2 (2.2)\n\n7.0 (0.7) 8.4 (0.5) 6.9 (0.4) 10.5 (0.5) 9.7 (0.2) 10.4 (0.7) 7.6 (0.3) 6.9 (0.5) 17.1 (0.6)\n\n10.3 (1.6) 6.9 (1.5) 5.8 (1.5) 10.3 (1.7) 7.0 (1.2) 7.7 (0.7) 11.1 (3.0) 8.2 (1.0) 8.4 (1.6)\n\n4.2 (0.8) 6.3 (1.7) 4.6 (1.6) 13.7 (1.5) 11.3 (1.0) 13.3 (0.9) 8.8 (1.7) 7.6 (0.8) 17.8 (1.0)\n\nFigure 13: Overview of LHRM.\n\nH ILLUSTRATION OF LHRM\n\nFigure 13 illustrates the main components of our algorithm for learning HRMs, LHRM, described in Section 5. Given a set of tasks with known levels and a set of instances, we select a (task, instance) pair at the beginning of an episode. The HRM corresponding to the selected task is taken from the bank of HRMs. At each step, the agent performs an action at+1 in the (task, instance) environment, and observes a tuple st and a label t. The label is used to (i) know the next hierarchy L\nstate ut+1 in the HRM and the reward rt+1, and (ii) update the label trace . If the trace is a counterexample (i.e., the tuple st and the new hierarchy state ut+1 are inconsistent with each other),8 we add it to our set of counterexample traces for the current task and learn a new HRM using the ILASP system. The learned HRM is then updated in the bank of HRMs so that future tasks can reuse it. Note that the description omits some aspects for simplicity, such as how are the (task, instance) pairs selected, the exploration using options from lower level HRMs, or the accumulation of traces before learning the first HRM for a given task.\n\n0, . . . ,\n\n⟨L\n\nL\n\n⟩\n\nt\n\n8For example if (sT\n\nt , sG\n\nt ) = (⊤, ⊤) and the state in ut+1 is not the accepting state of the root RM.\n\n44\n\nTasks1Batter4CakeInstancesTask - InstanceAgentHRMstaskTask HRMCounterexample?ILASPTraceTracesYesUpdated task's HRMCallable RMsUnder review as a conference paper at ICLR 2023\n\ne h\n\nt\n\n, t\n\nu o\n\ng n\nm\n\ni\n\ni t\n\nt\n\nu o\nh\n\nt i\n\nw\n\ns n\nu r\n\nd e\nt e\nl\n\np m\no c\n\nf o\n\nr e\nb m\nu n\n\ne h\n\nt\n\n:\n\ni\n\ng n\nw o\n\nl l\n\no f\n\ne h\n\nt\n\ne r\na\n\ns n\nm u\n\nl\n\no c\n\ne h\nT\n\n. s\nd o\nh\n\nt e\n\nm\n\nt\n\nn e\nr e\nf f\ni\n\nd\n\ng n\n\ni s\nu\n\ns\n\nM R\nH\n\nt a\nfl\n\nd n\na\n\nt a\nfl\n\n- n\no n\n\ng n\n\ni\n\nn r\na e\nl\n\nf o\n\ns t\nl\n\nu s\ne R\n\n:\n\n9\n\ne l\n\nb a\nT\n\n.\n\nM R\ne h\n\nt\n\nf o\n\ns e\ng d\ne\n\nd n\na\n\ns e\nt a\nt s\n\nf o\n\nr e\nb m\nu n\n\ne h\n\nt\n\nd n\na\n\n, s\n\nM R\n\nr o\n\ns\n\nM R\nH e\nh\n\nt\n\nn r\na e\nl\n\no\n\nt\n\nd e\nd e\ne n\n\ne\n\nm\n\ni t\n\nf o\n\nt\n\nn u\no m\n\na\n\nP R\n\nI J\n\nM R\nL\n\nh\n\nt\n\nn y\nS p\ne e\nD\n\n) t\na l\n\nF\n\n(\n\nM R\nH L\n\n) t\na l\n\nF\n\n- n\no N\n\n(\n\nM R\nH L\n\nk s\na T\n\ns e\ng d\nE\n\ns e\nt a\nt\n\nS\n\n) .\ns (\n\ne\n\nm T\n\ni\n\nC\n\ns e\ng d\nE\n\ns e\nt a\nt\n\nS\n\n) .\ns (\n\ne\n\nm T\n\ni\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n3\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n4\n\n) 5\n\n.\n\n5 (\n\n1\n\n.\n\n7 1\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n) 1\n\n.\n\n9 (\n\n4\n\n.\n\n2 8\n\n) 2\n\n.\n\n0 (\n\n8\n\n.\n\n3\n\n) 9\n\n.\n\n7 (\n\n3\n\n.\n\n2 3\n\n5\n\n0\n\n0\n\n0\n\n5\n\n0\n\n0\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n) 0\n\n.\n\n1 (\n\n) 0\n\n.\n\n2 (\n\n0\n\n.\n\n4 1\n\n2\n\n.\n\n1 3\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n4\n\n0\n\n.\n\n8\n\n) 5\n\n.\n\n4 6\n(\n\n5\n\n.\n\n7 4\n3\n\n) 2\n\n.\n\n2 5\n5 (\n\n0\n\n.\n\n1 6\n2 2\n\nC\n\n5\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n) 7\n\n.\n\n1 (\n\n2\n\n.\n\n3 9\n\n) 3\n\n.\n\n2 (\n\n8\n\n.\n\n2 9\n\n) 4\n\n.\n\n9 1\n(\n\n0\n\n.\n\n9 1\n1\n\n) 6\n\n.\n\n1 1\n(\n\n2\n\n.\n\n0 1\n1\n\n) 4\n\n.\n\n0 (\n\n) 1\n\n.\n\n3 (\n\n) 5\n\n.\n\n0 (\n\n) 5\n\n.\n\n2 (\n\n4\n\n.\n\n3 1\n\n6\n\n.\n\n6 1\n\n8\n\n.\n\n2 1\n\n2\n\n.\n\n7 1\n\n) 7\n\n.\n\n9 2\n(\n\n) 7\n\n.\n\n1 3\n(\n\n) 6\n\n.\n\n2 5\n(\n\n) 4\n\n.\n\n6 3\n(\n\n6\n\n.\n\n5 2\n3\n\n9\n\n.\n\n8 8\n2\n\n6\n\n.\n\n8 0\n3\n\n6\n\n.\n\n0 9\n2\n\ns e\ng d\nE\n\ns e\nt a\nt\n\nS\n\n) .\ns (\n\ne\n\nm T\n\ni\n\nC\n\n5\n\n5\n\n5\n\n4\n\n0\n\n0\n\n0\n\ns e\ng d\nE\n\ns e\nt a\nt\n\nS\n\n) .\ns (\n\ne\n\nm T\n\ni\n\nC\n\ns e\ng d\nE\n\ns e\nt a\nt\n\nS\n\n) .\ns (\n\ne\n\nm T\n\ni\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n) 2\n\n.\n\n0 (\n\n6\n\n.\n\n3\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n4\n\n) 6\n\n.\n\n0 (\n\n2\n\n.\n\n3\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n2\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n3\n\n) 0\n\n.\n\n0 (\n\n9\n\n.\n\n0\n\n5\n\n0\n\n0\n\n0\n\n5\n\n0\n\n0\n\n) 0\n\n.\n\n0 (\n\n) 2\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 2\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n2\n\n8\n\n.\n\n5\n\n0\n\n.\n\n4\n\n2\n\n.\n\n3\n\n0\n\n.\n\n2\n\n0\n\n.\n\n4\n\n0\n\n.\n\n4\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n) 0\n\n.\n\n0 (\n\n0\n\n.\n\n3\n\n0\n\n.\n\n5\n\n0\n\n.\n\n4\n\n0\n\n.\n\n4\n\n0\n\n.\n\n3\n\n0\n\n.\n\n4\n\n0\n\n.\n\n4\n\n) 2\n\n.\n\n0 (\n\n5\n\n.\n\n1\n\n) 4\n\n.\n\n1 (\n\n9\n\n.\n\n7 1\n\n) 7\n\n.\n\n5 2\n(\n\n5\n\n.\n\n4 7\n\n) 4\n\n.\n\n6 3\n(\n\n2\n\n.\n\n1 9\n1\n\n) 0\n\n.\n\n0 (\n\n) 3\n\n.\n\n0 (\n\n) 7\n\n.\n\n1 (\n\n9\n\n.\n\n0\n\n5\n\n.\n\n4\n\n1\n\n.\n\n5 1\n\nC\n\n5\n\n5\n\n5\n\n5\n\n5\n\n5\n\n5\n\nT E\nK C\nU B\nK L\n\nI\n\nM\n\nL L\n\nI\n\nU Q\nK O\nO B\n\nK O\nO B\n\nY M\nC &\nB G\nR\n\nC B\n& G\nR\n\nE K\nA C\n\nG R\n\n45\n\nUnder review as a conference paper at ICLR 2023\n\nu0 0\n\nM⊤\n\n|\n\nu1 0\n\nM⊤\n\n|\n\nuA 0\n\nu0 1\n\nM⊤\n\n|\n\nu1 1\n\nM⊤\n\n|\n\nuA 1\n\nu0 2\n\nM⊤\n\n| ∧ ¬\n\nu2 2\n\nM⊤\n\n|\n\nu1 2\n\nM⊤\n\nM⊤\n\n|\n\n|\n\nu3 2\n\nM⊤\n\n|\n\nuA 2\n\nu0 3\n\nM⊤\n\n|\n\nu1 3\n\nM⊤\n\n|\n\nuA 3\n\n(a) M0 – BUCKET\n\n(b) M1 – SUGAR\n\n(c) M2 – BATTER\n\n(d) M3 – PAPER\n\nu0 4\n\nM⊤\n\n|\n\n∧ ¬\n\nu2 4\n\nM⊤\n\n|\n\nu1 4\n\nM⊤\n\n|\n\nM⊤\n\n|\n\nu3 4\n\nM⊤\n\n|\n\nuA 4\n\nu0 5\n\nM⊤\n\n|\n\nu1 5\n\nM⊤\n\n|\n\nuA 5\n\nu0 6\n\nM⊤\n\n| ∧ ¬\n\nu2 6\n\nM⊤\n\n|\n\nu1 6\n\nM⊤\n\nM⊤\n\n|\n\n|\n\nu3 6\n\nM⊤\n\n|\n\nuA 6\n\nu0 7\n\nM0\n\n| ⊤\n\nu1 7\n\nM⊤\n\n|\n\nuA 7\n\n(e) M4 – COMPASS\n\n(f) M5 – LEATHER\n\n(g) M6 – QUILL\n\n(h) M7 – MILKBUCKET\n\nu0 8\n\nM3\n\n| ⊤\n\nM4\n\n| ¬\n\nu0 9\n\nM3\n\n| ⊤\n\nM5\n\n| ¬\n\nu0 10\n\nM1\n\n| ⊤\n\nM7\n\n| ¬\n\nu1 8\n\nu2 8\n\nu1 9\n\nu2 9\n\nu1 10\n\nu2 10\n\nM4\n\n| ⊤\n\nM3\n\n| ⊤\n\nM5\n\n| ⊤\n\nM3\n\n| ⊤\n\nu3 8\n\nM⊤\n\n|\n\nuA 8\n\nu3 9\n\nM⊤\n\n|\n\nuA 9\n\nM7\n\n| ⊤\n\nM1\n\n| ⊤\n\nuA 10\n\n(i) M8 – MAP\n\n(j) M9 – BOOK\n\n(k) M10 – MILKB.SUGAR\n\nu0 11\n\nM9\n\nM6\n\n| ⊤\n\nu1 11\n\n| ¬ ∧ ¬\n\nu2 11\n\nM9\n\n| ⊤\n\nM6\n\n| ⊤\n\nuA 11\n\nu0 12\n\nM2\n\n| ⊤\n\nu1 12\n\nM10\n\n| ⊤\n\nu2 12\n\nM⊤\n\n|\n\nuA 12\n\n(l) M11 – BOOKQUILL\n\n(m) M12 – CAKE\n\nFigure 14: Root reward machines for each of the CRAFTWORLD tasks.\n\n46\n\nUnder review as a conference paper at ICLR 2023\n\nu0 0\n\nM⊤\n\nr\n\n|\n\nu1 0\n\nM⊤\n\ng\n\n|\n\nuA 0\n\nu0 1\n\nM⊤\n\nb\n\n|\n\nu1 1\n\nM⊤\n\nc\n\n|\n\nuA 1\n\nu0 2\n\nM⊤\n\nm\n\n|\n\nu1 2\n\nM⊤\n\ny\n\n|\n\nuA 2\n\nu0 3\n\nM0\n\nb\n\n| ¬\n\nM1\n\nr\n\n| ¬\n\nu1 3\n\nu2 3\n\nM1\n\n| ⊤\n\nM0\n\n| ⊤\n\nuA 3\n\n(a) M0 – RG\n\n(b) M1 – BC\n\n(c) M2 – MY\n\n(d) M3 – RG&BC\n\nu0 4\n\nu0 5\n\nM1\n\nm\n\n| ¬\n\nM2\n\nb\n\n| ¬\n\nM0\n\nm\n\n| ¬\n\nM2\n\nr\n\n| ¬\n\nu1 4\n\nu2 4\n\nu1 5\n\nu2 5\n\nM2\n\n| ⊤\n\nM1\n\n| ⊤\n\nM2\n\n| ⊤\n\nM0\n\n| ⊤\n\nuA 4\n\nuA 5\n\nu0 6\n\nM0\n\n| ⊤\n\nu1 6\n\nM⊤\n\ng\n\n|\n\nuA 6\n\nu0 7\n\nM⊤\n\nc\n\n|\n\nu1 7\n\nM2\n\n| ⊤\n\nuA 7\n\n(e) M4 – BC&MY\n\n(f) M5 – RG&MY\n\n(g) M6 – RGB\n\n(h) M7 – CMY\n\nu0 8\n\nM6\n\nc\n\n| ¬\n\nM7\n\nr\n\n| ¬\n\nu1 8\n\nu2 8\n\nM7\n\n| ⊤\n\nM6\n\n| ⊤\n\nuA 8\n\n(i) M8 – RGB&CMY\n\nFigure 15: Root reward machines for each of the WATERWORLD tasks.\n\n47",
    "reference": "# Summary Of The Paper\n\nThe paper extends the Reward Machines (RM) formalism to hierarchical RMs (HRMs) by allowing RMs to call other RMs as sub-programs. The paper demonstrates that this allows for more efficient learning of RMs on long-horizon tasks in a number of toy grid-world environments.\n\n# Strength And Weaknesses\n\n# Strengths\n\n- incorporating hierarchy into RMs seems like an important step towards making them more applicable to long-horizon tasks\n\n- the submission rigorously defines all aspects of the problem setup\n\n- the toy experiments demonstrate that the introduced hierarchical RMs learn faster than regular (flat) RMs if a RM hierarchy is pre-defined, they also demonstrate learning of long-horizon tasks with HRMs (not possible with RMs)\n\n# Weaknesses\n\n(A) **hard to follow writing**: The writing of the paper is very hard to follow. I attribute this in large part to the very heavy usage of formalism and introduction of terms, which make smooth reading challenging. It seems that the paper could benefit a lot from replacing / augmenting some of the formal definitions with their plain-english equivalents. On other occasions crucial terms are not properly defined. Eg in the background section 2, the terms “propositions” and “labels” are not properly introduced, no intuition / example is given for what they could refer to. This makes it very hard to understand the paper for readers (like myself) that aren’t already familiar with the RM literature.\n\n(B) **assumptions not clearly mentioned**: compared to other papers that use (deep) RL to learn options & policies the submission seemingly makes a number of assumptions (given subtask hierarchies or at least a priori knowledge about depth of the hierarchy, symbolic observations, access to training tasks for *non-hierarchical* RMs, …). However, these assumptions are only step-by-step introduced throughout the text and can be easily missed. It would be good to add a “problem formulation” section or similar that explicitly lists the required assumptions for learning hierarchies of HRMs and how they compare to prior works, so that readers can better understand in what use cases they can consider HRMs.\n\n(C) **no comparisons to other option learning approaches**: the only comparison in the experimental section is to flat RMs — this is a meaningful comparison since the proposed approach is a hierarchical extension of flat RMs. However, since the paper proposes an approach for learning hierarchical options, it should compare to prior works on option learning with deep RL too, if applicable. E.g. the work of Sungryull Sohn could be applicable, since it also addresses hierarchical RL with discrete observational symbols (eg Sohn et al., NeurIPS 2018).\n\n(D) **only tested in toy environments**: the submission tests the proposed approach only in simple 2D grid-world like environments with low-dimensional observation and action spaces. This makes it hard to judge how well the method would scale to more realistic settings, eg. learning robot control in 3D environments.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is rigorous but hard to read. It provides a clear extension of the prior RM work, but significance with respect to other option learning papers is not clearly established.\n\n# Summary Of The Review\n\nOverall, I found the submission quite hard to read. I was not previously familiar with RMs and I found the heavy use of formalism and special terms throughout the submission hampered the reading flow. It also makes it hard for me to properly judge the novelty of the work: my perception is that the submission adds novelty over the previous RM work, but it also seems to make multiple assumptions that can reduce its impact outside the currently rather niche RM topic. The experimental evaluation does not help judge this either, since it is limited to toy environments and does not compare to non-RM works.\n\nI am leaning towards not accepting the submission, but am willing to change my mind if the authors can clearly state their assumptions and how they relate to other option learning papers (ie is there good use cases for their approach that others cannot do, can you compare to any non-RM approach?). Additionally, I am curious to know what other reviewers opinions are, especially for reviewers with a better overview of the RM literature.\n\n=============\nPost-Rebuttal: I updated my score to a weak accept based on the discussion with the other reviewers -- see my reply below.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nESCHER: ESCHEWING IMPORTANCE SAMPLING IN GAMES BY COMPUTING A HISTORY VALUE FUNCTION TO ESTIMATE REGRET\n\nStephen McAleer Carnegie Mellon University smcaleer@cs.cmu.edu\n\nGabriele Farina Carnegie Mellon University gfarina@cs.cmu.edu\n\nMarc Lanctot DeepMind lanctot@deepmind.com\n\nTuomas Sandholm Carnegie Mellon University Strategy Robot, Inc. Optimized Markets, Inc. Strategic Machine, Inc. sandholm@cs.cmu.edu\n\nABSTRACT\n\nRecent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art—DREAM and neural fictitious self play (NFSP)—on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.\n\n1\n\nINTRODUCTION\n\nA core challenge in computational game theory is the problem of learning strategies that approximate Nash equilibrium in very large imperfect-information games such as Starcraft (Vinyals et al., 2019), dark chess (Zhang & Sandholm, 2021), and Stratego (McAleer et al., 2020; Perolat et al., 2022). Due to the size of these games, tabular game-solving algorithms such as counterfactual regret minimization (CFR) are unable to produce such equilibrium strategies. To sidestep the issue, in the past stochastic methods such as Monte-Carlo CFR (MCCFR) have been proposed. These methods use computationally inexpensive unbiased estimators of the regret (i.e., utility gradient) of each player, trading off speed for convergence guarantees that hold with high probability rather than in the worst case. Several unbiased estimation techniques of utility gradients are known. Some, such as external sampling, produce low-variance gradient estimates that are dense, and therefore are prohibitive in the settings mentioned above. Others, such as outcome sampling, produce high-variance estimates that are sparse and can be computed given only the realization of play, and are therefore more appropriate for massive games.\n\nHowever, even outcome-sampling MCCFR is inapplicable in practice. First, since it is a tabular method, it can only update regret on information sets that it has seen during training. In very large games, only a small fraction of all information sets will be seen during training. Therefore,\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ngeneralization (via neural networks) is necessary. Second, to achieve unbiasedness of the utility gradient estimates, outcome-sampling MCCFR uses importance sampling (specifically, it divides the utility of each terminal state by a reach probability, which is often tiny), leading to estimates with extremely large magnitudes and high variance. This drawback is especially problematic when MCCFR is implemented using function approximation, as the high variance of the updates can cause instability of the neural network training.\n\nDeep CFR (Brown et al., 2019) addresses the first shortcoming above by training a neural network to estimate the regrets cumulated by outcome-sampling MCCFR, but is vulnerable to the second shortcoming, causing the neural network training procedure to be unstable. DREAM (Steinberger et al., 2020) improves on Deep CFR by partially addressing the second shortcoming by using a history-based value function as a baseline (Schmid et al., 2019). This baseline greatly reduces the variance in the updates and is shown to have better performance than simply regressing on the MCCFR updates. However, DREAM still uses importance sampling to remain unbiased. So, while DREAM was shown to work in small artificial poker variants, it is still vulnerable to the high variance of the estimated counterfactual regret and indeed we demonstrate that in games with long horizons and/or large action spaces, this importance sampling term causes DREAM to fail.\n\nIn this paper, we introduce Eschewing importance Sampling by Computing a History value function to Estimate Regret (ESCHER), a method that is unbiased, low variance, and does not use importance sampling. ESCHER is different from DREAM in two important ways, both of which we show are critical to achieving good performance. First, instead of using a history-dependent value function as a baseline, ESCHER uses one directly as an estimator of the counterfactual value. Second, ESCHER does not multiply estimated counterfactual values by an importance-weighted reach term. To remove the need to weight by the reach to the current information state, ESCHER samples actions from a fixed sampling policy that does not change from one iteration to the next. Since this distribution is static, our fixed sampling policy simply weights certain information sets more than others. When the fixed sampling policy is close to the balanced policy (i.e., one where each leaf is reached with equal probability), these weighting terms minimally affect overall convergence of ESCHER with high probability.\n\nWe find that ESCHER has orders of magnitude lower variance of its estimated regret. In experiments with a deep learning version of ESCHER on the large games of phantom tic tac toe, dark hex, and dark chess, we find that ESCHER outperforms NFSP and DREAM, and that the performance difference increases to be dramatic as the size of the game increases. Finally, we show through ablations that both differences between ESCHER and DREAM (removing the bootstrapped baseline and removing importance sampling) are necessary in order to get low variance and good performance on large games.\n\n2 BACKGROUND\n\n=\n\n∈ N\n\n∈ W\n\n1, . . . , N\n\nat each step. In an N -player game,\n\nWe consider extensive-form games with perfect recall (Osborne & Rubinstein, 1994; Hansen et al., 2004; Kovaˇr ́ık et al., 2022). An extensive-form game progresses through a sequence of player actions, and has a world state w N is A\ni denotes the set of legal actions for player the space of joint actions for the players. i\ndenotes a joint action. At each ∆W determines world state, after the players choose a joint action, a transition function the probability distribution of the next world state w′. Upon transition from world state w to w′ via i(w′). In each world state w, player i receives joint action a, player i makes an observation oi = a utility ui(w). The game ends when the players reach a terminal world state. In this paper, we consider games that are guaranteed to end in a finite number of actions and that have zero utility at non-terminal world states.\n\nat world state w and a = (a1, . . . , aN )\n\nA1 × · · · × A\n\n(w, a)\n\n⊆ A\n\ni(w)\n\n∈ A\n\nA\n\nO\n\n=\n\n∈\n\nT\n\n{\n\n}\n\nA history is a sequence of actions and world states, denoted h = (w0, a0, w1, a1, . . . , wt), where w0 is the known initial world state of the game. i(h) are, respectively, the utility and set of legal actions for player i in the last world state of a history h. An information set for player i, denoted by si, is a sequence of that player’s observations and actions up until that time si(h) = (a0 i. The set of histories that correspond to an information set si is denoted , and it is\n\ni). Define the set of all information sets for player i to be\n\nh : si(h) = si\n\ni , . . . , ot\n\ni(h) and\n\n(si) =\n\ni , a1\n\ni , o1\n\nA\n\nU\n\nI\n\nH\n\n{\n\n}\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nassumed that they all share the same set of legal actions A\ndrop the subscript i for an information set s when the player is implied.\n\ni(si(h)) =\n\nA\n\ni(h). For simplicity we often\n\nA player’s strategy πi is a function mapping from an information set to a probability distribution over actions. A strategy profile π is a tuple (π1, . . . , πN ). All players other than i are denoted i, and their strategies are jointly denoted π−i. A strategy for a history h is denoted πi(h) = πi(si(h)) and π(h) is the corresponding strategy profile. When a strategy πi is learned through reinforcement learning (RL), we refer to the learned strategy as a policy.\n\n−\n\nThe expected value (EV) vπ i (h) for player i is the expected sum of future utilities for player i in history h, when all players play strategy profile π. The EV for an information set si is denoted vπ i (si) and the EV for the entire game is denoted vi(π). A two-player zero-sum game has v1(π) + v2(π) = 0 for all strategy profiles π. The EV for an action in an information set is denoted vπ i (si, ai). A Nash equilibrium (NE) is a strategy profile such that, if all players played their NE strategy, no player could achieve higher EV by deviating from it. Formally, π∗ is a NE if vi(π∗) = maxπi vi(πi, π∗ −i) for each player i. The exploitability e(π) of a strategy profile π is defined as e(π) = (cid:80) i, π−i). A best response (BR) strategy BRi(π−i) for player i to a strategy π−i is a strategy that maximally exploits π−i: BRi(π−i) i (π−i) for player i to a strategy π−i is a strategy that is at most ε worse for player i than the best response: vi(BRε ε. An ε-Nash equilibrium (ε-NE) is a strategy profile π in which, for each player i, πi is an ε-BR to π−i.\n\narg maxπi vi(πi, π−i). An ε-best response (ε-BR) strategy BRε\n\nvi(BRi(π−i), π−i)\n\ni (π−i), π−i)\n\ni∈N maxπ′\n\ni vi(π′\n\n−\n\n≥\n\n∈\n\n2.1 COUNTERFACTUAL REGRET MINIMIZATION (CFR)\n\nIn this section we review the counterfactual regret minimization (CFR) framework. All superhuman poker AIs have used advanced variants of the framework as part of their architectures (Bowling et al., 2015; Brown & Sandholm, 2018; 2019). CFR is also the basis of several reinforcement learning algorithms described in Section B. We will leverage and extend the CFR framework in the rest of the paper. We will start by reviewing the framework. Define ηπ(h) = (cid:81) a∈h π(a) to be the reach weight of joint policy π to reach history h, and z is a terminal history. Define ηπ(h, z) = ηπ(z) ηπ(h) to be the reach weight of joint policy π to reach terminal history z from history h. Define Z to be the set of all terminal histories. Define Z(s) Z to be the set of terminal histories z that can be reached from information state s and define z[s] to be the unique history h\n\ns that is a subset of z. Define\n\n⊆\n\n∈\n\nvi(π, h) =\n\n(cid:88)\n\nz⊐h\n\nηπ(h, z)ui(z)\n\n(1)\n\nto be the expected value under π for player i having reached h. Note that this value function takes as input the full-information history h and not an information set. Define\n\nvc i (π, s) =\n\n(cid:88)\n\nz∈Z(s)\n\n−i(z[s])ηπ(z[s], z)ui(z) = ηπ\n\n(cid:88)\n\nh∈s\n\nηπ\n\n−i(h)vi(π, h)\n\n(2)\n\ni (π, s, a) = vc\n\nto be the counterfactual value for player i at state s under the joint strategy π. Define the strategy πs→a to be a modified version of π where a is played at information set s, and the counterfactual state-action value qc i (πs→a, s). Similarly, define the history-action value qi(π, h, a) = vi(πh→a, h). (s), one can define a local counterfactual regret For any state s, strategy π, and action a for not switching to playing a at s as rc(π, s, a) = qc i (π, s). Counterfactual regret minimization (CFR) (Zinkevich et al., 2008a) is a strategy iteration algorithm that produces a sequence . Each policy πt+1(s) is derived directly from a collection of cumulative , πT of policies: t=1 rc(πt, s, a), for all a regrets RT (s) using regret-matching (Hart & Mas-Colell, 2000). Total regret for player i in the entire game is defined as RT vi(πt\n\n−i). In two-player zero-sum games, the average policy ̄πT (s) =\n\nπ1, π2, · · · i (s, a) = (cid:80)T\n\n− converges\n\ni = maxπ′\n\nt=1 vi(π′\n\ni (π, s, a)\n\nt=1 ηπt (cid:80)T\n\ni , πt\n\ni, πt\n\n∈ A\n\n∈ A\n\n−i)\n\n(cid:80)T\n\nvc\n\n(cid:80)T\n\n−\n\n{\n\n}\n\ni\n\ni (s)πt(s) i (s)\n\nt=1 ηπt\n\nto an approximate Nash equilibrium at a rate of e( ̄πT )\n\nO(1/√T ).\n\n≤\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n2.2 MONTE CARLO COUNTERFACTUAL REGRET MINIMIZATION (MCCFR)\n\nIn the standard CFR algorithm, the quantities required to produce new policies in Equations 1 and 2 require full traversals of the game to compute exactly. Monte Carlo CFR (Lanctot et al., 2009) is a stochastic version of CFR which instead estimates these quantities. In particular, MCCFR uses a sampling approach which specifies a distribution over blocks Zj of terminal histories such that , the set of terminal histories. Upon sampling a block j, a certain sampled counterfactual ∪\nvalue ˆvc(π, s j) (defined in detail later in this section) is computed for all prefix histories that occur in Zj. Then, estimated regrets are accumulated and new policies derived as in CFR. The main result is that E[ˆvc(π, s j)] = vc(π, s), so MCCFR is an unbiased approximation of CFR, and inherits its convergence properties albeit under a probabilistic guarantee.\n\njZj =\n\nZ\n\n|\n\n|\n\nBlocks are sampled via sampling policy ̃π which is commonly a function of the players’ joint policy π. Two sampling variants were defined in the original MCCFR paper: outcome sampling (OS-MCCFR) and external sampling (ES-MCCFR). External sampling samples only the opponent (and chance’s) choices; hence, it requires a forward model of the game to recursively traverse over all of the subtrees under the player’s actions. Outcome sampling is the most extreme sampling variant where blocks consist of a single terminal history: it is the only model-free variant of MCCFR compliant with the standard reinforcement learning loop where the agent learns entirely from experience with the environment. The OS-MCCFR counterfactual value estimator when the opponent samples from their current policy as is commonly done is given as follows:\n\nˆvi(π, s |\n\nz) =\n\nηπ−i(z[s])ηπ(z[s], z)ui(z) η ̃π(z)\n\n=\n\n1 η ̃πi(z[s])\n\nηπi(z[s], z) η ̃πi(z[s], z)\n\nui(z)\n\n(3)\n\nThe importance sampling term that is used to satisfy the unbiasedness of the values can have a significant detrimental effect on the convergence rate Gibson et al. (2012). Variance reduction techniques provide some empirical benefit Schmid et al. (2019); Davis et al. (2019), but have not been evaluated on games with long trajectories where the importance corrections have their largest impact.\n\n2.3 DEEP COUNTERFACTUAL REGRET MINIMIZATION\n\nDeep CFR (Brown et al., 2019; Steinberger, 2019; Li et al., 2019) is a method that uses neural networks to scale MCCFR to large games. Deep CFR performs external sampling MCCFR and trains a regret network Ri(s, a ψ) on a replay buffer of information sets and estimated cumulative regrets. |\nThe regret network is trained to approximate the cumulative regrets seen so far at that information state. The estimated counterfactual regrets are computed the same as in MCCFR, namely using outcome sampling or external sampling.\n\n2.4 DREAM\n\nDREAM (Steinberger et al., 2020) builds on Deep CFR and approximates OS-MCCFR with deep neural networks. Like Deep CFR, it trains a regret network Ri(s, a ψ) on a replay buffer of information |\nsets and estimated cumulative regrets. Additionally, in order to limit the high variance of OS-MCCFR, DREAM uses a learned history value function qi(π, h, a θ) and uses it as a baseline (Schmid et al., 2019). While the baseline helps remove variance in the estimation of future utility, in order to remain unbiased DREAM must use importance sampling as in OS-MCCFR. We show empirically that variance of the DREAM estimator of the counterfactual value, although lower than OS-MCCFR, will often be quite high, even in small games and with an oracle history value function. This high variance estimator might make neural network training very difficult. In contrast, ESCHER has no importance sampling term and instead directly uses the learned history value function qi(π, h, a θ) to estimate regrets.\n\n|\n\n|\n\n3 ESCHER\n\nIn this section we define our proposed algorithm, ESCHER, where we use a learned history value function vi(π, h θ) to estimate regret. This history value function is a neural network trained on |\non-policy rollouts to predict future reward. In our theoretical analysis we model the learned history\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nvalue function to be equal to the exact history value vi(π, h) plus an additional error. In our theoretical analysis we show that ESCHER is sound and converges to a Nash equilibrium with high probability.\n\nAs shown in Equation 3, the OS-MCCFR estimator can be seen as containing two separate terms. The first 1/η ̃πi(z[s]) term ensures that each information set is updated equally often in expectation. The second ηπi(z[s], z)ui(z)/η ̃πi(z[s], z) term is an unbiased estimator of the history value vi(π, z[s]). In DREAM, the second term gets updated by a bootstrapped baseline to reduce variance. But since the baseline is not perfect in practice, as we show in our ablations, this term still induces high variance, which prevents the regret network from learning effectively. The main idea behind ESCHER is to remove the first reach weighting term by ensuring that the sampling distribution for the update player remains fixed across iterations, and to replace the second term with a history value function vi(π, z[s]\n\nθ). |\n\nOur method is built on Deep CFR. In particular, like Deep CFR, we traverse the game tree and add this experience into replay buffers. The first replay buffer stores information states and instantaneous regret estimates is used to train a regret network Rψ(s, a) that is trained to estimate the cumulative regret at a given information set. This replay buffer is filled with information sets from trajectories gathered by following the sampling distribution in Equation 5. The regret network Ri(s, a ψ) is |\nreinitialized and trained from scratch on the entire replay buffer to estimate the average estimated counterfactual regret at every information set via regression. Since regret matching is scale invariant, the policy computed from average counterfactual regret is equivalent to the policy computed from total counterfactual regret. Similar to Deep CFR, each player’s current policy πi is given by performing ψ). regret matching (described in Equation 7) on the output of the current regret network Ri(s, a |\n\nThe second replay buffer stores histories and terminal utilities and is used to train the value network vθ to estimate the expected utility for both players when both players are at that history and play from their current policies. The value network vθ is reinitialized after every iteration, and is trained by predicting future reward from on-policy trajectories. In particular, to avoid the issue that the regret-matching policy does not generally put strictly positive mass on all actions, the on-policy sampling is performed by sampling from a weighted mixture of 0.99 ×\nthe uniformly-random policy. Lastly, the third replay buffer stores information states and actions taken by the policy π and uses that data to train an average policy network ̄πφ that approximates the average policy across all iterations. It is this average policy that has no regret and converges to an approximate Nash equilibrium in self play.\n\nthe current policy and 0.01\n\n×\n\nUnlike Deep CFR and DREAM, which use the terminal utility and sampling probabilities from the current trajectory to estimate the value, in ESCHER the instantaneous regret estimates are estimated using the current history value function vi(π, h θ) alone. Since we only update regret on information |\nstates visited during the trajectory, our estimator is zero on all other information sets. Formally, we define our estimator for the counterfactual regret as follows:\n\nˆri(π, s, a\n\nz) =\n\n|\n\n(cid:26)vi(π, z[s]a θ) |\n\n0\n\nvi(π, z[s] |\n\nθ)\n\n−\n\nZ(s)\n\nif z ∈\notherwise\n\n(4)\n\nESCHER samples from the opponent’s current strategy when it is their turn but samples from a fixed strategy that roughly visits every information set equally likely when it is the update player’s turn. As a result, the expected value of the history value is equal to the counterfactual value scaled by a term that weights certain information sets up or down based on the fixed sampling policy. Formally, define the fixed sampling policy bi(s, a) to be any policy that remains constant across iterations and puts positive probability on every action. This fixed sampling policy can be one of many distributions such as one that samples uniformly over available actions at every information set. In games with an equal number of actions at every information set, the uniform policy will sample all information sets at each level of the tree equally likely. An interesting open research direction is finding good fixed sampling policy. In this paper, our fixed sampling policy uniformly samples over actions, which is somewhat similar to the robust sampling technique introduced in Li et al. (2019). When updating player i, we construct a joint fixed sampling policy ̃πi(s, a) to be\n\n ̃πi(s, a) =\n\n(cid:26)bi(s, a)\n\nif it’s the update player i’s turn\n\nπ−i(s, a) otherwise\n\n(5)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: ESCHER 1 Initialize history value function q 2 Initialize policy πi for both players 3 for t = 1, ..., T do\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\nRefill value replay buffer with on-policy data from π Retrain history value function on data from value replay buffer Reinitialize regret networks R0, R1 for update player i\n\n0, 1\n\ndo\n\nfor P trajectories do\n\n∈ {\n\n}\n\nGet trajectory τ using sampling distribution (Equation 5) for each state s\n\nτ do for each action a do\n\n∈\n\nEstimate immediate cf-regret ˆr(π, s, a\n\nθ) z) = vi(π, z[s]a |\n| Add (s, ˆr(π, s)) to cumulative regret buffer Add (s, a′) to average policy buffer where a′ is action taken at state s\n\n−\n\nθ) vi(π, z[s] |\n\nTrain regret network Ri on cumulative regret buffer\n\n16 Train average policy network ̄πφ on average policy buffer 17 return average policy network ̄πφ\n\nWe use a fixed sampling policy because it allows us to remove any importance sampling in our estimator. Unlike Deep CFR and DREAM which must divide by the current player’s reach probability to remain unbiased, our method does not use importance sampling but total average regret is still guaranteed to converge to zero. We describe our algorithm in Algorithm 1. Highlighted in blue are the differences between our algorithm and Deep CFR. Namely, we train a history value function and use it to estimate counterfactual regret.\n\n3.1 THEORETICAL RESULTS\n\nIn the following, assume that our learned value function vi(π, h θ) is equal to the exact value function |\nvi(π, h). In the appendix we analyze the case where the learned value function is inaccurate. If we sample from ̃πi when updating player i, then the expected value of our counterfactual regret estimator is:\n\nEz∼ ̃πi[ˆri(π, s, a\n\nz)] =\n\n|\n\n=\n\n=\n\n=\n\n(cid:88)\n\nz∈Z\n\n(cid:88)\n\nz∈Z(s) (cid:88)\n\n(cid:88)\n\nz⊐h η ̃πi\n\nh∈s (cid:88)\n\nh∈s\n\nη ̃πi\n\n(z)[ˆri(π, s, a\n\nz)]\n\n|\n\nη ̃πi\n\n(z)[vi(π, z[s]a)\n\nvi(π, z[s])]\n\n−\n\nη ̃πi\n\n(z)[vi(π, z[s]a)\n\nvi(π, z[s])]\n\n−\n\n(h)[vi(π, ha)\n\nvi(π, h)]\n\n−\n\n= η ̃πi\n\ni (s)\n\n(cid:88)\n\nηπ\n\n−i(h)[vi(π, ha)\n\nvi(π, h)]\n\n−\n\n= w(s)[vc\n\nh∈s i (π, s, a)\n\n−\n\ni (π, s)] = w(s)rc(π, s, a) vc\n\n(6)\n\n∈\n\ns, ηb\n\ni (h) = ηb\n\ni (h′) = ηb\n\nWhere for h, h′ i (s) =: w(s) is the reach probability for reaching that infostate for player i via the fixed sampling distribution. Unlike the estimator in Deep CFR and DREAM, our estimator has no importance sampling terms, and as a result has much lower variance. When all information sets are visited by the sampling distribution with equal probability, then ESCHER is perfectly unbiased. The correctness of our method is established by the next theorem, whose proof can be found in Appendix A. As shown in the proof, the regret of our method is bounded with high probability by a term that is inversely proportional to the minimum over information sets s of w(s). Therefore, our theory suggests that the balanced sampling distribution is the optimal sampling distribution, but in practice other sampling distributions might perform better. In our experiments we approximate the balanced distribution with uniform sampling over actions.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nGame\n\nESCHER (Ours)\n\nAblation 1\n\nAblation 2\n\nDREAM\n\nPhantom Tic-Tac-Toe (2.6 ± 0.1)×10−1\n\n(4.1 ± 0.7)×101\n\n(1.4 ± 0.4)×107\n\n(4.6 ± 1.0)×107\n\nDark Hex 4\n\nDark Hex 5\n\nGame\n\nLeduc\n\nBattleship\n\nLiar’s Dice\n\n(1.8 ± 0.1)×10−1\n\n(1.3 ± 0.9)×102\n\n(3.1 ± 1.7)×108\n\n(2.8 ± 2.0)×108\n\n(1.3 ± 0.1)×10−1\n\n(3.3 ± 1.6)×102\n\n(2.0 ± 0.6)×105\n\n(5.3 ± 3.9)×108\n\nESCHER (Ours)\n\nAblation 2\n\nDREAM\n\nOS-MCCFR\n\n(5.3 ± 0.0)×100\n\n(3.3 ± 0.7)×102\n\n(2.8 ± 0.0)×102\n\n(2.2 ± 0.0)×103\n\n(1.4 ± 0.0)×100\n\n(7.1 ± 0.3)×102\n\n(1.2 ± 0.0)×103\n\n(2.4 ± 0.0)×103\n\n(9.0 ± 0.1)×10−1\n\n(7.8 ± 0.9)×101\n\n(4.0 ± 0.8)×102\n\n(1.2 ± 0.1)×103\n\nTable 1: These results track the average variance of the regret estimator of each algorithm over all iterations. The top table shows results of deep algorithms on large games. Ablation 1 is ESCHER but with a bootstrapped baseline, and ablation 2 is ESCHER but with reach weighting. The bottom table shows tabular versions of the algorithms with oracle value functions on small games. Because ESCHER does not use importance sampling, the variance of its estimator is orders of magnitude smaller than baselines. Colors scale with the ratio of the minimum value in each row, according to\n\nthe logarithmic color scale\n\n.\n\nAlgorithm\n\nESCHER (Ours) Ablation 1 Ablation 2\n\nDREAM / VR-MCCFR Deep CFR / OS-MCCFR\n\nHistory value Boostrapped\n\nfunction ✓\n✓ ✓\n\n✓ ✗\n\nbaseline ✗\n✓ ✗\n\n✓ ✗\n\nImportance sampling ✗\n✗ ✓\n\n✓ ✓\n\nTable 2: ESCHER is different from DREAM in two important ways. First, ESCHER does not use importance sampling (Lanctot et al., 2009; Brown et al., 2019; Steinberger et al., 2020). Second, ESCHER does not estimate counterfactual values for sampled actions via a bootstrapped baseline (Schmid et al., 2019; Steinberger et al., 2020). Our ablations show that both improvements are necessary.\n\n∈\n\n(0, 1), with probability at least 1\n\nTheorem 1. Assume a fixed sampling policy that puts positive probability on every action. For any p, the regret accumulated by each agent learning using the p\ntabular algorithm ESCHER (Algorithm 2) is upper bounded by O(√T poly log(1/p)), where the O( ·\n\n) notation hides game-dependent and sampling-policy-dependent constants.\n\n−\n\n·\n\nIn the appendix we extend the analysis to the case of approximate history value function, and give a bound with an explicit dependence on the magnitude of the approximation error.\n\n4 RESULTS\n\nWe compare the variance of the counterfactual value estimates from ESCHER, DREAM, and ablations in Table 1. Variance is computed over the set of all counterfactual regret values estimated in a single iteration. The results in the table are the average of each iteration’s variance over the first five iterations of training. The top table shows results of deep algorithms on large games. A summary of the different ablations is given in Table 2. From this experiment we can see that because ESCHER does not use importance sampling, the variance of its estimator is orders of magnitude smaller than baselines. Also, we see that even ablation 1, which does not use importance sampling, has high variance. This is because when the history value function is not exact, the bootstrapping method recursively divides by the sampling probability on sampled actions. We see that this higher variance\n\n7\n\n110100≥10001Published as a conference paper at ICLR 2023\n\nFigure 1: ESCHER is competitive with NFSP and DREAM in Phantom Tic-Tac-Toe. But as the size of the game increases, ESCHER performs increasingly better than both DREAM and NFSP. In Dark Chess, ESCHER beats DREAM and NFSP in over 90% of the matches.\n\nindeed leads to worse performance for DREAM and these ablations in Figure 2. Therefore, both improvements of ESCHER over DREAM are necessary.\n\n3 board while dark hex 4 is played on a size 4\n\nWe compare our method to DREAM and NFSP, the most popular baselines that are also open-source, on the games of Phantom Tic Tac Toe (TTT), Dark Hex, and Dark Chess. Dark chess is a popular game among humans under the name Fog of War Chess on the website chess.com, and has emerged as a benchmark task (Zhang & Sandholm, 2021). All of these games are similar in that they are both imperfect information versions of perfect-information games played on square boards. Phantom TTT 4 board and dark hex 5 is played is played on a 3 on a size 5 5 board. Because these games are large, we are not able to compare exact exploitability so instead we compare performance through head-to-head evaluation. Results are shown in Figure 1, where the x axis tracks the number of information sets visited during training. We see that our method is competitive with DREAM and NFSP on Phantom TTT. On the larger game of Dark Hex 5, ESCHER beats DREAM and NFSP head to head and also scores higher against a random opponent. Moving to the largest game of Dark Chess, we see that ESCHER beats DREAM and NFSP head to head over 90% of the time and also is able to beat a random opponent while DREAM and NFSP are no better than random.\n\n×\n\n×\n\n×\n\n5 DISCUSSION: LIMITATIONS AND FUTURE RESEARCH\n\nOur method has a number of ways it can be improved. First, it requires two separate updates in one iteration. Perhaps the method could be more efficient if only on-policy data were used. Second, our method, like Deep CFR, DREAM, and NFSP, trains neural networks on large replay buffers of past experience. Unlike RL algorithms like DQN (Mnih et al., 2015), these replay buffers must record all data ever seen in order to learn an average. This can be a problem when the amount of data required is much larger than the replay buffer memory. Third, we do not use various implementation details that help performance in Deep CFR such as weighting by the sum of the reach probabilities over all iterations. Finally, our method uses separate data to train the value function. Our method could be made much more efficient by also using the data generated for training the policy to also train the value function.\n\n8\n\n246Informationsetstouched×1060%25%50%75%100%Win%PhantomTic-Tac-ToeESCHER(Ours)DREAMNFSP123Informationsetstouched×1060%25%50%75%100%DarkHex5ESCHER(Ours)DREAMNFSP0.51.0Informationsetstouched×1070%25%50%75%100%DarkChessESCHER(Ours)DREAMNFSPExperiment:Playingagainstanopponentthatplaysuniformlyatrandom246Informationsetstouched×1060%25%50%75%100%Win%PhantomTic-Tac-ToeESCHERvsNFSPESCHERvsDREAM123Informationsetstouched×1060%25%50%75%100%DarkHex5ESCHERvsNFSPESCHERvsDREAM0.51.0Informationsetstouched×1070%25%50%75%100%DarkChessESCHERvsNFSPESCHERvsDREAMExperiment:Playingagainstanotherbaselinehead-to-headPublished as a conference paper at ICLR 2023\n\nFigure 2: Ablation study on ESCHER. As summarized in Table 2, “Ablation 1” is ESCHER but with a bootstrapped history-value baseline, while “Ablation 2” is ESCHER but with reach weighting. Since ESCHER with a bootstrapped history-value baseline and reach weighting is equivalent to DREAM, these results show that both changes to DREAM are necessary for ESCHER to work in large games.\n\nOne direction of future research is finding optimal sampling distributions. In our method we use the uniform distribution over actions as our fixed sampling distribution, but this can be far from optimal. In principle any distribution that remains fixed will guarantee the method to converge with high probability. One possible direction would be to try to estimate the theoretically optimal balanced distribution. Other, less principled, methods such as using the average policy might work well in practice as well (Burch et al., 2012). Another direction is in connecting this work with the reinforcement learning literature. Similar to reinforcement learning, we learn a Q value and a policy, and there are many techniques from reinforcement learning that are promising to try in this setting. For example, although we learned the value function simply through Monte-Carlo rollouts, one could use bootstrapping-based methods such as TD-λ (Sutton, 1988) and expected SARSA (Rummery & Niranjan, 1994). The policy might be able to be learned via some sort of policy gradient, similar to QPG (Srinivasan et al., 2018), NeuRD (Hennes et al., 2020), and F-FoReL (Perolat et al., 2021).\n\nREFERENCES\n\nBowling, M., Burch, N., Johanson, M., and Tammelin, O. Heads-up limit hold’em poker is solved.\n\nScience, 347(6218):145–149, 2015.\n\nBrafman, R. I. and Tennenholtz, M. R-max-a general polynomial time algorithm for near-optimal\n\nreinforcement learning. Journal of Machine Learning Research, 3(Oct):213–231, 2002.\n\nBrown, N. and Sandholm, T. Libratus: The superhuman AI for no-limit poker.\n\nIn IJCAI, pp.\n\n5226–5228, 2017a.\n\nBrown, N. and Sandholm, T. Safe and nested subgame solving for imperfect-information games.\n\nAdvances in neural information processing systems, 30, 2017b.\n\nBrown, N. and Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top\n\nprofessionals. Science, 359(6374):418–424, 2018.\n\n9\n\n12Informationsetstouched×1060%25%50%75%100%Win%PhantomTic-Tac-ToeESCHER(Ours)Ablation1Ablation2123Informationsetstouched×1060%25%50%75%100%DarkHex4ESCHER(Ours)Ablation1Ablation2123Informationsetstouched×1060%25%50%75%100%DarkHex5ESCHER(Ours)Ablation1Ablation2Ablationexperiment:Playingagainstanopponentthatplaysuniformlyatrandom12Informationsetstouched×1060%25%50%75%100%Win%PhantomTic-Tac-ToeESCHERvsAblation1ESCHERvsAblation2123Informationsetstouched×1060%25%50%75%100%DarkHex4ESCHERvsAblation1ESCHERvsAblation2123Informationsetstouched×1060%25%50%75%100%DarkHex5ESCHERvsAblation1ESCHERvsAblation2Ablationexperiment:Playingagainstanotherablationhead-to-headPublished as a conference paper at ICLR 2023\n\nBrown, N. and Sandholm, T. Superhuman AI for multiplayer poker. Science, 365(6456):885–890,\n\n2019.\n\nBrown, N., Sandholm, T., and Amos, B. Depth-limited solving for imperfect-information games.\n\nAdvances in neural information processing systems, 31, 2018.\n\nBrown, N., Lerer, A., Gross, S., and Sandholm, T. Deep counterfactual regret minimization. In\n\nInternational Conference on Machine Learning, pp. 793–802, 2019.\n\nBrown, N., Bakhtin, A., Lerer, A., and Gong, Q. Combining deep reinforcement learning and search for imperfect-information games. Advances in Neural Information Processing Systems, 33: 17057–17069, 2020.\n\nBurch, N., Lanctot, M., Szafron, D., and Gibson, R. Efficient monte carlo counterfactual regret minimization in games with many player actions. Advances in neural information processing systems, 25, 2012.\n\nBurch, N., Johanson, M., and Bowling, M. Solving imperfect information games using decomposition.\n\nIn Twenty-eighth AAAI conference on artificial intelligence, 2014.\n\nDaskalakis, C., Foster, D. J., and Golowich, N. Independent policy gradient methods for competitive reinforcement learning. Advances in neural information processing systems, 33:5527–5540, 2020.\n\nDavis, T., Schmid, M., and Bowling, M. Low-variance and zero-variance baselines for extensive-form\n\ngames. CoRR, abs/1907.09633, 2019. URL http://arxiv.org/abs/1907.09633.\n\nDing, D., Wei, C.-Y., Zhang, K., and Jovanovi ́c, M. R. Independent policy gradient for large-scale markov potential games: Sharper rates, function approximation, and game-agnostic convergence. arXiv preprint arXiv:2202.04129, 2022.\n\nFarina, G., Kroer, C., and Sandholm, T. Online convex optimization for sequential decision processes\n\nand extensive-form games. In AAAI Conference on Artificial Intelligence, 2019a.\n\nFarina, G., Kroer, C., and Sandholm, T. Regret circuits: Composability of regret minimizers. In\n\nInternational Conference on Machine Learning, 2019b.\n\nFarina, G., Kroer, C., and Sandholm, T. Stochastic regret minimization in extensive-form games. In\n\nInternational Conference on Machine Learning, 2020.\n\nFarina, G., Kroer, C., and Sandholm, T. Faster game solving via predictive blackwell approachability:\n\nConnecting regret matching and mirror descent. 2021.\n\nFeng, X., Slumbers, O., Yang, Y., Wan, Z., Liu, B., McAleer, S., Wen, Y., and Wang, J. Discovering multi-agent auto-curricula in two-player zero-sum games. Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nFox, R., Mcaleer, S. M., Overman, W., and Panageas, I. Independent natural policy gradient always converges in markov potential games. In International Conference on Artificial Intelligence and Statistics, pp. 4414–4425. PMLR, 2022.\n\nFu, H., Liu, W., Wu, S., Wang, Y., Yang, T., Li, K., Xing, J., Li, B., Ma, B., Fu, Q., and Wei, Y. Actor-critic policy optimization in a large-scale imperfect-information game. In Proceedings of the Tenth International Conference on Learning Representations (ICLR), 2022.\n\nGibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowling, M. Generalized sampling and variance in counterfactual regret minimization. In Proceedings of the Twenty-Sixth Conference on Artificial Intelligence (AAAI-12)., pp. 1355–1361, 2012.\n\nGordon, G. No-regret algorithms for online convex programs. 2007.\n\nGray, J., Lerer, A., Bakhtin, A., and Brown, N. Human-level performance in no-press diplomacy via\n\nequilibrium search. In International Conference on Learning Representations, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nGruslys, A., Lanctot, M., Munos, R., Timbers, F., Schmid, M., Perolat, J., Morrill, D., Zambaldi, V., Lespiau, J.-B., Schultz, J., et al. The advantage regret-matching actor-critic. arXiv preprint arXiv:2008.12234, 2020.\n\nHansen, E. A., Bernstein, D. S., and Zilberstein, S. Dynamic programming for partially observable\n\nstochastic games. Conference on Artificial Intelligence (AAAI), 2004.\n\nHart, S. and Mas-Colell, A. A simple adaptive procedure leading to correlated equilibrium. Econo-\n\nmetrica, 68(5):1127–1150, 2000.\n\nHeinrich, J. and Silver, D. Deep reinforcement learning from self-play in imperfect-information\n\ngames. arXiv preprint arXiv:1603.01121, 2016.\n\nHennes, D., Morrill, D., Omidshafiei, S., Munos, R., Perolat, J., Lanctot, M., Gruslys, A., Lespiau, J.-B., Parmas, P., Du ́e ̃nez-Guzm ́an, E., et al. Neural replicator dynamics: Multiagent learning via hedging policy gradients. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, pp. 492–501, 2020.\n\nJin, C., Liu, Q., Wang, Y., and Yu, T. V-learning–a simple, efficient, decentralized algorithm for\n\nmultiagent rl. arXiv preprint arXiv:2110.14555, 2021.\n\nKovaˇr ́ık, V., Schmid, M., Burch, N., Bowling, M., and Lis`y, V. Rethinking formal models of partially\n\nobservable multiagent decision making. Artificial Intelligence, 303:103645, 2022.\n\nLanctot, M., Waugh, K., Zinkevich, M., and Bowling, M. Monte carlo sampling for regret minimization in extensive games. In Advances in neural information processing systems, pp. 1078–1086, 2009.\n\nLanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P ́erolat, J., Silver, D., and Graepel, T. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n\nLanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S., P ́erolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S., et al. Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019.\n\nLanier, J., McAleer, S., Baldi, P., and Fox, R. Feasible adversarial robust reinforcement learning for\n\nunderspecified environments. arXiv preprint arXiv:2207.09597, 2022.\n\nLeonardos, S., Overman, W., Panageas, I., and Piliouras, G. Global convergence of multi-agent policy\n\ngradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021.\n\nLi, H., Hu, K., Zhang, S., Qi, Y., and Song, L. Double neural counterfactual regret minimization. In\n\nInternational Conference on Learning Representations, 2019.\n\nLi, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T.-Y., and Hon, H.-W. Suphx: Mastering mahjong with deep reinforcement learning. arXiv preprint arXiv:2003.13590, 2020.\n\nLiang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gonzalez, J., Jordan, M., and Stoica, I. Rllib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pp. 3053–3062, 2018.\n\nLiu, W., Li, B., and Togelius, J. Model-free neural counterfactual regret minimization with bootstrap\n\nlearning. IEEE Transactions on Games, 2022.\n\nMcAleer, S., Lanier, J., Fox, R., and Baldi, P. Pipeline PSRO: A scalable approach for finding In Advances in Neural Information Processing\n\napproximate Nash equilibria in large games. Systems, 2020.\n\nMcAleer, S., Lanier, J., Baldi, P., and Fox, R. XDO: A double oracle algorithm for extensive-form\n\ngames. Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nMcAleer, S., Wang, K., Lanctot, M., Lanier, J., Baldi, P., and Fox, R. Anytime optimal psro for\n\ntwo-player zero-sum games. arXiv preprint arXiv:2201.07700, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMguni, D. H., Wu, Y., Du, Y., Yang, Y., Wang, Z., Li, M., Wen, Y., Jennings, J., and Wang, J. Learning in nonzero-sum stochastic games with potentials. In International Conference on Machine Learning, pp. 7688–7699. PMLR, 2021.\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n\nMoravcik, M., Schmid, M., Ha, K., Hladik, M., and Gaukrodger, S. Refining subgames in large imperfect information games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.\n\nMoravˇc ́ık, M., Schmid, M., Burch, N., Lis`y, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017.\n\nMorimoto, J. and Doya, K. Robust reinforcement learning. Neural computation, 17(2):335–359,\n\n2005.\n\nMoritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, In 18), pp.\n\nW., Jordan, M. I., et al. Ray: A distributed framework for emerging 13th USENIX }\n561–577, 2018.\n\nSymposium on Operating Systems Design and Implementation (\n\napplications.\n\nOSDI\n\nAI\n\n{\n\n{\n\n}\n\n{\n\n}\n\nMuller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., Hennes, D., Marris, L., Lanctot, M., Hughes, E., et al. A generalized training approach for multiagent learning. In International Conference on Learning Representations, 2019.\n\nOsborne, M. J. and Rubinstein, A. A Course in Game Theory. MIT Press, 1994.\n\nPerolat, J., Piot, B., and Pietquin, O. Actor-critic fictitious play in simultaneous move multistage games. In International Conference on Artificial Intelligence and Statistics, pp. 919–928. PMLR, 2018.\n\nPerolat, J., Munos, R., Lespiau, J.-B., Omidshafiei, S., Rowland, M., Ortega, P., Burch, N., Anthony, T., Balduzzi, D., De Vylder, B., et al. From Poincar ́e recurrence to convergence in imperfect In International Conference on information games: Finding equilibrium via regularization. Machine Learning, pp. 8525–8535. PMLR, 2021.\n\nPerolat, J., de Vylder, B., Hennes, D., Tarassov, E., Strub, F., de Boer, V., Muller, P., Connor, J. T., Burch, N., Anthony, T., et al. Mastering the game of stratego with model-free multiagent reinforcement learning. arXiv preprint arXiv:2206.15378, 2022.\n\nPinto, L., Davidson, J., Sukthankar, R., and Gupta, A. Robust adversarial reinforcement learning. In\n\nInternational Conference on Machine Learning, pp. 2817–2826. PMLR, 2017.\n\nRummery, G. A. and Niranjan, M. On-line Q-learning using connectionist systems, volume 37.\n\nCiteseer, 1994.\n\nSchmid, M., Burch, N., Lanctot, M., Moravcik, M., Kadlec, R., and Bowling, M. Variance reduction in monte carlo counterfactual regret minimization (VR-MCCFR) for extensive form games using baselines. In Proceedings of the The Thirty-Third AAAI Conference on Artificial Intelligence, 2019.\n\nSchmid, M., Moravcik, M., Burch, N., Kadlec, R., Davidson, J., Waugh, K., Bard, N., Timbers, F.,\n\nLanctot, M., Holland, Z., et al. Player of games. arXiv preprint arXiv:2112.03178, 2021.\n\nSerrino, J., Kleiman-Weiner, M., Parkes, D. C., and Tenenbaum, J. Finding friend and foe in\n\nmulti-agent games. Advances in Neural Information Processing Systems, 32, 2019.\n\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. nature, 550 (7676):354–359, 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nSrinivasan, S., Lanctot, M., Zambaldi, V., P ́erolat, J., Tuyls, K., Munos, R., and Bowling, M. Actorcritic policy optimization in partially observable multiagent environments. Advances in neural information processing systems, 31, 2018.\n\nSteinberger, E. Single deep counterfactual regret minimization. arXiv preprint arXiv:1901.07621,\n\n2019.\n\nSteinberger, E., Lerer, A., and Brown, N. DREAM: Deep regret minimization with advantage\n\nbaselines and model-free learning. arXiv preprint arXiv:2006.10410, 2020.\n\nSutton, R. S. Learning to predict by the methods of temporal differences. Machine learning, 3(1):\n\n9–44, 1988.\n\nTessler, C., Efroni, Y., and Mannor, S. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pp. 6215–6224. PMLR, 2019.\n\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nWei, C.-Y., Hong, Y.-T., and Lu, C.-J. Online reinforcement learning in stochastic games. Advances\n\nin Neural Information Processing Systems, 30, 2017.\n\nWurman, P. R., Barrett, S., Kawamoto, K., MacGlashan, J., Subramanian, K., Walsh, T. J., Capobianco, R., Devlic, A., Eckert, F., Fuchs, F., et al. Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):223–228, 2022.\n\nXie, Q., Chen, Y., Wang, Z., and Yang, Z. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In Conference on learning theory, pp. 3674–3682. PMLR, 2020.\n\nZha, D., Xie, J., Ma, W., Zhang, S., Lian, X., Hu, X., and Liu, J. Douzero: Mastering doudizhu with self-play deep reinforcement learning. In International Conference on Machine Learning, pp. 12333–12344. PMLR, 2021.\n\nZhang, B. and Sandholm, T. Subgame solving without common knowledge. Advances in Neural\n\nInformation Processing Systems, 34, 2021.\n\nZhang, R., Ren, Z., and Li, N. Gradient play in stochastic games: stationary points, convergence, and\n\nsample complexity. arXiv preprint arXiv:2106.00198, 2021.\n\nZinkevich, M., Johanson, M., Bowling, M., and Piccione, C. Regret minimization in games with incomplete information. In Advances in Neural Information Processing Systems (NeurIPS), 2008a.\n\nZinkevich, M., Johanson, M., Bowling, M., and Piccione, C. Regret minimization in games with incomplete information. In Advances in Neural Information Processing Systems (NeurIPS), 2008b.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 KNOWN RESULTS ABOUT REGRET MATCHING (RM)\n\nIn this section, we recall the definition and the regret bound of the Regret Matching (RM) regret minimization algorithm for probability simplices. In the interest of keeping the paper as self-contained as possible, we propose a proof for the result. For a deeper treatment of RM, we invite the reader to consult the works of Hart & Mas-Colell (2000); Gordon (2007); Farina et al. (2021).\n\nWe start by recalling the definition of RM. Let A be a set of discrete actions and ∆(A) the simplex of probability distributions over A. Regret Matching operates by keeping a tally Rt a of the regret accumulated up to each time t compared to always selecting each action a, and picks the next distribution according to\n\nxt(a) :=\n\n \n\n\n\n0, Rt\n\nmax a′∈A max\n\n{\n\na} 0, Rt a′\n\n{\n\n(cid:80)\n\n1 |A|\n\nif (cid:80)\n\na′∈A max\n\n0, Rt a′\n\n{\n\n}\n\n> 0\n\n}\n\notherwise.\n\n(7)\n\nUpon observing the utility vector gt correspondingly as\n\n∈ a + gt(a)\n\nRt+1\n\na\n\n:= Rt\n\nRA, the tally of the regret for each action is updated\n\n(cid:88)\n\n−\n\na′∈A\n\ngt(a′)xt(a′).\n\n(8)\n\na = 0 for all a\n\n(at time t = 0, R0 Proposition 1. Let M > 0, and let gt be an arbitrary (possibly adversarially picked) sequence of utility vectors with\n\nA received by RM. The regret\n\nM for all a\n\ngt(a)\n\nA).\n\n∈\n\n|\n\n| ≤\n\nRT := max\n\na∈A\n\nRT\n\na = max a∈A\n\n∈ (cid:40) T\n\n(cid:88)\n\nt=1\n\ngt(a)\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\na′∈A\n\n(cid:41)\n\ngt(a′)xt(a′)\n\naccumulated up to any time T by RM satisfies\n\nRT\n\n≤\n\n(cid:112)\n\nM\n\nA |\n\nT . |\n\nProof. The proof hinges on the following observation. Fix any time t.\n\n• If (cid:80)\n\na′∈A max\n\n0, Rt a′\n\n{\n\n} ≤\n\n0, then Rt a′ (cid:32)\n\n≤\n\n0 for all a′\n\n∈\n\nA, and so trivially\n\n(cid:33)\n\n(cid:88)\n\na∈A\n\nmax\n\n{\n\n0, Rt\n\na}\n\ngt(a)\n\n(cid:88)\n\n−\n\na′∈A\n\ngt(a′)xt(a′)\n\n= 0.\n\n• On the other hand, if (cid:80)\n\na′∈A max\n\n0, Rt a′\n\n{\n\n}\n\n> 0 then\n\nxt(a) =\n\n0, Rt\n\nmax a′∈A max\n\n{\n\na} 0, Rt a′\n\n(cid:80)\n\n{\n\n}\n\nand so\n\n(cid:88)\n\na∈A\n\nmax\n\n0, Rt\n\na}\n\n{\n\n(cid:32)\n\ngt(a)\n\n(cid:88)\n\n−\n\na′∈A\n\n(cid:33)\n\ngt(a′)xt(a′)\n\n=\n\n(cid:32)\n\n(cid:88)\n\na∈A\n\n= 0.\n\nmax\n\n{\n\n0, Rt\n\ngt(a)\n\na}\n\n(cid:33)\n\n(cid:32)\n\n(cid:88)\n\n−\n\na∈A\n\n(cid:33)\n\nmax\n\n{\n\n0, Rt\n\na}\n\n(cid:88)\n\na′∈A\n\ngt(a′)xt(a′)\n\nSo, in either case, at all times t,\n\n(cid:88)\n\na∈A\n\nmax\n\n0, Rt\n\na}\n\n{\n\n(cid:32)\n\ngt(a)\n\n(cid:88)\n\n−\n\na′∈A\n\n(cid:33)\n\ngt(a′)xt(a′)\n\n= 0.\n\n(9)\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nNow, observe that for all t\n\n(cid:88)\n\na∈A\n\nmax\n\n0, Rt+1\n\na }\n\n{\n\n2\n\n≥ (cid:88)\n\n≤\n\n=\n\n=\n\n=\n\na∈A\n\n(cid:88)\n\na∈A\n\n(cid:88)\n\na∈A\n\n(cid:88)\n\na∈A\n\n2 (Rt+1\n\na −\n\nmin\n\n{\n\n0, Rt\n\n)2\n\na}\n\n(cid:32)\n\nRt\n\na −\n\nmin\n\n0, Rt\n\n{\n\na}\n\n+ gt(a)\n\n(cid:88)\n\n−\n\na′∈A\n\n(cid:33)2\n\ngt(a′)xt(a′)\n\n(from (8))\n\n(cid:32)\n\nmax\n\n0, Rt\n\na}\n\n{\n\n+ gt(a)\n\n(cid:88)\n\n−\n\na′∈A\n\n(cid:33)2\n\ngt(a′)xt(a′)\n\nmax\n\n0, Rt\n\n{\n\na}\n\n+ 2\n\n(cid:88)\n\na∈A\n\n(cid:32)\n\n2 +\n\ngt(a)\n\n(cid:88)\n\na′∈A\n\n−\n\n(cid:32)\n\nmax\n\n{\n\n0, Rt\n\na}\n\ngt(a)\n\n(cid:33)2\n\ngt(a′)xt(a′)\n\n(cid:33)\n\ngt(a′)xt(a′)\n\n(cid:88)\n\n−\n\na′∈A\n\n=\n\n≤\n\n(cid:88)\n\na∈A (cid:88)\n\na∈A\n\nmax\n\nmax\n\n{\n\n{\n\n0, Rt\n\na}\n\n0, Rt\n\na}\n\n(cid:33)2\n\ngt(a′)xt(a′)\n\n(cid:88)\n\n−\n\na′∈A\n\n(from (9))\n\n(cid:32)\n\n2 +\n\ngt(a)\n\n2 +\n\nA\n\nM 2, |\n\n|\n\nwhere the last inequality follows from the fact that by assumption each utility has absolute value at most M , that is, A, the previous recursive inequality leads to\n\nM . Using the fact that R0\n\na = 0 for all a\n\ngt(a)\n\n| ≤\n\n∈\n\n|\n\n(cid:88)\n\na∈A\n\nmax\n\n{\n\n0, RT\n\n2\n\na }\n\nT\n\nA |\n\n|\n\n≤\n\nM 2\n\nT\n\n∀\n\n1, 2, . . .\n\n∈ {\n\n. }\n\nFrom the previous inequality we finally conclude that (cid:115)(cid:88)\n\nRT = max\n\nmax\n\n0, RT\n\na∈A\n\n{\n\na } ≤\n\na∈A\n\nmax\n\n0, RT\n\n2\n\n{\n\na }\n\n(cid:112)\n\nM\n\nT ,\n\nA |\n\n|\n\n≤\n\nas we wanted to show.\n\nA.2 ANALYSIS OF ESCHER\n\nWe start by recalling a central theorem connecting regret to counterfactual regret (see, e.g., Zinkevich et al. (2008b); Farina et al. (2019b;a)). Proposition 2. Fix any player i, and let\n\nRT\n\ns := max ˆa∈As\n\nT (cid:88)\n\nt=1\n\ni (πt, s, ˆa) = max rc\n\nˆa∈As\n\nT (cid:88)\n\nt=1\n\ni (πt, s, ˆa) qc\n\ni (πt, s) vc\n\n−\n\nbe the counterfactual regret accumulated up to time T by the regret minimizer local at each information set s. Then, the regret\n\nRT\n\ni\n\n:= max\n\nˆπi\n\nT (cid:88)\n\nt=1\n\nvi(ˆπi, πt\n\n−i)\n\n−\n\nvi(πt\n\ni , πt\n\n−i)\n\naccumulated by the policies πt on the overall game tree satisfies\n\nRT\n\ni ≤\n\n(cid:88)\n\ns\n\nmax\n\n{\n\nRT\n\ns , 0\n\n}\n\n.\n\nWe can now use a modification of the argument by Farina et al. (2020) to bound the degradation of regret due to the use of an estimator of the counterfactual regrets. However, our analysis requires some modifications compared to that of Farina et al. (2020), in that ESCHER introduces estimation at the level of counterfactuals, while the latter paper introduces estimation at the level of the game utilities.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: Tabular ESCHER with Oracle Value Function 1 for t = 1, ..., T do\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nfor update player i\n\n0, 1\n\ndo\n\n∈ {\n\n}\n\nSample trajectory τ using sampling distribution ̃πi (Equation 5) for each state s\n\nτ do for each action a do\n\n∈\n\nEstimate immediate regret vector ˆr(π, s, a Update total estimated regret of action a at infostate s: ˆR(s, a) = ˆR(s, a) + ˆr(π, s, a Update πi(s, a) via regret matching (Equation 7) on total estimated regret\n\nz) = qi(π, z[s], a)\n\nvi(π, z[s])\n\nz) |\n\n−\n\n|\n\n9 return average policy ̄π\n\n∈\n\n(0, 1), with probability at least 1\n\nTheorem 1. Assume a fixed sampling policy that puts positive probability on every action. For any p, the regret accumulated by each agent learning using the p\ntabular algorithm ESCHER (Algorithm 2) is upper bounded by O(√T poly log(1/p)), where the O( ·\n\n) notation hides game-dependent and sampling-policy-dependent constants.\n\n−\n\n·\n\nProof. As shown in Section 3, for any information set s the counterfactual regret estimators ˆri(π, s, a\n\nh) are unbiased up to a time-independent multiplicative factor; specifically,\n\n|\n\nEh∼ ̃πi[ˆri(π, s, a\n\nh)] = w(s)ri(π, s, a)\n\n|\n\nfor all actions a available to player i at world states in s. Hence, for each a the martingale difference sequences\n\nAs we can construct\n\n∈\n\nt is bounded, with\n\nClearly, X a Hence, from the Azuma-Hoeffding inequality, we obtain that the regret RT policies produced by ESCHER with respect to the correct counterfactuals satisfies, for all p\n\nupper bounded by (twice) the range D of payoffs of player i. s accumulated by the local (0, 1)\n\nX t\n\na := w(s)ri(πt, s, a) X a t | |\n\n−\n\nˆri(πt, s, a\n\nh).\n\n|\n\n∈\n\nP\n\n(cid:34) T\n\n(cid:88)\n\nt=1\n\nX t\n\na ≤\n\n(cid:114)\n\n2D\n\n2T log\n\n(cid:35)\n\n1 p\n\np,\n\n1\n\n−\n\n≥\n\nUsing a union bound on the actions, we can therefore write\n\n(cid:34)\n\nP\n\nmax a\n\nT (cid:88)\n\nt=1\n\nX t\n\na ≤\n\n(cid:115)\n\n2D\n\n2T log |\n\n(cid:35)\n\nAs p\n\n|\n\np,\n\n1\n\n−\n\n≥\n\nThe left-hand side in the probability can be expanded as follows:\n\nmax a\n\nT (cid:88)\n\nt=1\n\nX t\n\na = max\n\na\n\nmax a\n\n≥\n\n(cid:40)\n\nw(s)\n\n(cid:40)\n\nw(s)\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nri(πt, s, a)\n\nT (cid:88)\n\nt=1\n\n−\n\n(cid:41)\n\nˆri(πt, s, a\n\nh)\n\n|\n\n(cid:41)\n\nri(πt, s, a)\n\nmax a\n\n−\n\n(cid:40) T\n\n(cid:88)\n\nt=1\n\n(cid:41)\n\nˆri(πt, s, a\n\nh)\n\n|\n\nw(s)RT\n\ns −\n\n≥\n\nAs |\n\nT , |\n\nt=1 (cid:112)\n\nD\n\nwhere the last inequality follows from the fact that the regret cumulated by regret matching (which is run on the regret estimates ˆri) is upper bounded by D T (see Proposition 1). Hence, we can write\n\nAs |\n\n(cid:112)\n\n|\n\n(cid:34)\n\nw(s)RT\n\ns −\n\nD\n\n(cid:112) |\n\nAs\n\nT |\n\n≤\n\n(cid:115)\n\n2D\n\n2T log |\n\n(cid:35)\n\nAs p\n\n|\n\np\n\n1\n\n−\n\n≥\n\n(cid:34)\n\nRT\n\ns ≤\n\nD w(s)\n\n(cid:112) |\n\nAs\n\nT + |\n\n2D w(s)\n\n(cid:115)\n\n2T log |\n\n(cid:35)\n\nAs p\n\n|\n\np,\n\n1\n\n−\n\n≥\n\nP\n\nP\n\n⇐⇒\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nwhere in the second step we used the hypothesis that w(s) > 0 for all s. Since the right-hand size inside of the probability is non-negative, we can further write\n\n(cid:34)\n\nP\n\nmax\n\n{\n\nRT\n\ns , 0\n\n} ≤\n\n(cid:112)\n\nD w(s)\n\nAs |\n\nT + |\n\n2D w(s)\n\n(cid:115)\n\n2T log |\n\nAs p\n\n(cid:35)\n\n|\n\np,\n\n1\n\n−\n\n≥\n\nvalid for every information set s.\n\nNow, using the known analysis of CFR (Proposition 2), we obtain that the regret accumulated by the ESCHER iterates satisfies\n\nRT\n\ni ≤\n\n(cid:88)\n\ns\n\nmax\n\n{\n\n0, RT\n\n.\n\ns }\n\nHence, using a union bound over all information sets s\n\ni of player i, we find that\n\n∈ S (cid:115)\n\n(cid:34)\n\n(cid:34)\n\nP\n\nP\n\nRT\n\ni ≤\n\nRT\n\ni ≤\n\n⇐⇒\n\n(cid:32)\n\n(cid:88)\n\ns\n\n(cid:112)\n\nD\n\nAs |\nw(s)\n\n|\n\n+\n\n2D w(s)\n\n2 log |\n\n(cid:33)\n\n(cid:35)\n\n|\n\n√T\n\nAs\n\ni ||S p\n\np\n\n1\n\n−\n\n≥\n\n(cid:32)\n\n2D\n\ni |S mins w(s)\n\n|\n\n(cid:115)\n\nAs\n\n2 |\n\n|\n\nlog |\n\nAs\n\ni ||S p\n\n(cid:33)\n\n(cid:35)\n\n|\n\n√T\n\np,\n\n1\n\n−\n\n≥\n\nfor all p ∈\nstatements.\n\n(0, 1). Absorbing game-dependent and sampling-policy-dependent constants yields the\n\nWe remark that when the exploration policy is chosen to be the exploration-balanced strategy (Farina et al., 2020), then the minimum reach probability term 1/ mins w(s) is upper bounded by the number of terminal states in the game, a polynomial number in the game tree. When the exploration policy is set to be the uniform strategy, 1/ mins w(s) is a bounded parameter that depends on the game tree structure. In some artificial games, such as the centipede game, such a parameter is exponential in the game tree size. In most games with a reasonably balanced structure, such a parameter is polynomial in the game tree size.\n\nA.3\n\nINCORPORATING FUNCTION APPROXIMATION ERRORS\n\nWe now adapt the correctness analysis above to keep into account errors in the approximation of the history value function by the deep neural network. We can model that error explicitly by modifying (4) to incorporate a history-action-dependent error δ(h, a) as follows:\n\nˆri(π, s, a\n\nz) =\n\n|\n\n(cid:26)vi(π, z[s]a)\n\n0\n\n−\n\nvi(π, z[s]) + δ(z[s], a)\n\nZ(s)\n\nif z ∈\notherwise.\n\nRepeating the analysis of Section 3, we have\n\nEz∼ ̃πi[ˆri(π, s, a\n\nz)] = w(s)rc(π, s, a) + w(s) |\n\nηπ\n\n−i(h)δ(h, a).\n\n(cid:88)\n\nh∈s\n\nPropagating the error term throughout the analysis, assuming that each error term is at most ε > 0 in magnitude, we obtain that for each infostate s and p\n\n(0, 1),\n\n(cid:34)\n\nP\n\nRT\n\ns ≤\n\n(D + ε) w(s)\n\n(cid:112) |\n\nAs\n\nT + ε + |\n\n2(D + ε) w(s)\n\n(cid:115)\n\n2T log |\n\nAs p\n\n(cid:35)\n\n|\n\np,\n\n1\n\n−\n\n≥\n\n∈\n\nAgain using the union bound across all infostates of player i, we obtain\n\n(cid:34)\n\n(cid:32)\n\nP\n\nRT\n\ni ≤\n\n2(D + ε)\n\ni |S mins w(s)\n\n|\n\n(cid:115)\n\nAs\n\n2 |\n\n|\n\nlog |\n\nAs\n\ni ||S p\n\n(cid:33)\n\n|\n\n√T +\n\n(cid:35)\n\ni\n\nε |\n\n|S\n\np,\n\n1\n\n−\n\n≥\n\nshowing that errors in the function approximation cause an additive regret overhead linear in ε.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nB RELATED WORK\n\nSuperhuman performance in two-player games usually involves two components: the first focuses on finding a model-free blueprint strategy, which is the setting we focus on in this paper. The second component improves this blueprint online via model-based subgame solving and search (Burch et al., 2014; Moravcik et al., 2016; Brown et al., 2018; 2020; Brown & Sandholm, 2017b; Schmid et al., 2021). This combination of blueprint strategies with subgame solving has led to state-of the art performance in Go (Silver et al., 2017), Poker (Brown & Sandholm, 2017a; 2018; Moravˇc ́ık et al., 2017), Diplomacy (Gray et al., 2020), and The Resistance: Avalon (Serrino et al., 2019). Methods that only use a blueprint have achieved state-of-the-art performance on Starcraft (Vinyals et al., 2019), Gran Turismo (Wurman et al., 2022), DouDizhu (Zha et al., 2021), Mahjohng (Li et al., 2020), and Stratego (McAleer et al., 2020; Perolat et al., 2022). Because ESCHER is a method for finding a blueprint, it can be combined with subgame solving and is complementary to these approaches. In the rest of this section we focus on other model-free methods for finding blueprints.\n\nDeep CFR (Brown et al., 2019; Steinberger, 2019) is a general method that trains a neural network on a buffer of counterfactual values. However, Deep CFR uses external sampling, which may be impractical for games with a large branching factor, such as Stratego and Barrage Stratego. DREAM (Steinberger et al., 2020) and ARMAC (Gruslys et al., 2020) are model-free regret-based deep learning approaches. ReCFR (Liu et al., 2022) propose a bootstrap method for estimating cumulative regrets with neural networks that could potentially be combined with our method.\n\nNeural Fictitious Self-Play (NFSP) (Heinrich & Silver, 2016) approximates fictitious play by progressively training a best response against an average of all past opponent policies using reinforcement learning. The average policy converges to an approximate Nash equilibrium in two-player zero-sum games.\n\nPolicy Space Response Oracles (PSRO) (Lanctot et al., 2017; Muller et al., 2019; Feng et al., 2021; McAleer et al., 2022) are another promising method for approximately solving very large games. PSRO maintains a population of reinforcement learning policies and iteratively trains a best response to a mixture of the opponent’s population. PSRO is a fundamentally different method than the previously described methods in that in certain games it can be much faster but in other games it can take exponentially long in the worst case. Neural Extensive Form Double Oracle (NXDO) (McAleer et al., 2021) combines PSRO with extensive-form game solvers, and could potentially be combined with our method.\n\nThere is an emerging literature connecting reinforcement learning to game theory. QPG (Srinivasan et al., 2018) shows that state-conditioned Q-values are related to counterfactual values by a reach weighted term summed over all histories in an infostate and proposes an actor-critic algorithm that empirically converges to a NE when the learning rate is annealed. NeuRD (Hennes et al., 2020), and F-FoReL (Perolat et al., 2021) approximate replicator dynamics and follow the regularized leader, respectively, with policy gradients. Actor Critic Hedge (ACH) (Fu et al., 2022) is similar to NeuRD but uses an information set based value function. All of these policy-gradient methods do not have theory proving that they converge with high probability in extensive form games when sampling trajectories from the policy. In practice, they often perform worse than NFSP and DREAM on small games but remain promising approaches for scaling to large games (Perolat et al., 2022). Robust reinforcement learning (Morimoto & Doya, 2005; Pinto et al., 2017; Tessler et al., 2019; Lanier et al., 2022), seeks to train an RL policy to be robust against an adversarial environment. In future work we will look to apply ESCHER to this setting.\n\nMarkov games (or stochastic games) are extensive-form games where the world state information is shared among all players at each timestep, but players take simultaneous actions. Recent literature has shown that reinforcement learning algorithms converge to Nash equilibrium in two-player zero-sum Markov games (Brafman & Tennenholtz, 2002; Wei et al., 2017; Perolat et al., 2018; Xie et al., 2020; Daskalakis et al., 2020; Jin et al., 2021) and in multi-player general-sum Markov potential games (Leonardos et al., 2021; Mguni et al., 2021; Fox et al., 2022; Zhang et al., 2021; Ding et al., 2022).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nC ADDITIONAL EXPERIMENTAL RESULTS\n\nC.1 DESCRIPTION OF GAME INSTANCES\n\nWe use Openspiel (Lanctot et al., 2019) for all our games. Below we list the parameters used to define each game in Openspiel.\n\nLeduc (leduc poker) Parameters: {\"players\": 2} Battleship (battleship) Parameters: {\"board_width\": 2, \"board_height\": 2,\n\n\"ship_sizes\": \"[2]\", \"ship_values\": \"[2]\", \"num_shots\": 3, \"allow_repeated_shots\": False}\n\nLiar’s Dice (liars dice) Parameters: None Phantom Tic Tac Toe (phantom ttt) Parameters: None Dark Hex 4 (dark hex) Parameters: {\"board_size\": 4} Dark Hex 5 (dark hex) Parameters: {\"board_size\": 5} Dark Chess (dark chess) Parameters: None\n\nC.2 TABULAR EXPERIMENTS\n\nWe compare a tabular version of ESCHER with oracle value functions to a tabular version of DREAM with oracle value functions and with OS-MCCFR. We run experiments on Leduc poker, Battleship, and Liar’s dice, and use the implementations from OpenSpiel (Lanctot et al., 2019). We see in Figure 3 on the top row that ESCHER remains competitive with DREAM and OS-MCCFR on these games. On the bottom row we plot the average variance of the regret estimators over all information sets visited over an iteration window for each of these algorithms. While DREAM does improve upon OS-MCCFR, it still has orders of magnitude higher variance than ESCHER. Although this does not matter much in tabular experiments, we conjecture that high regret estimator variance makes neural network training unstable without prohibitively large buffer sizes.\n\n(a) Leduc Exploitability\n\n(b) Battleship Exploitability\n\n(c) Liar’s Dice Exploitability\n\n(d) Leduc Variance\n\n(e) Battleship Variance\n\n(f) Liar’s Dice Variance\n\nFigure 3: The tabular version of ESCHER with an oracle value function is competitive with the tabular version of DREAM with an oracle value function and with OS-MCCFR in terms of exploitability (top row). The regret estimator in ESCHER has orders of magnitude lower variance than those of DREAM and OS-MCCFR (bottom row).\n\nC.3 ADDITIONAL ABLATIONS\n\nIn this section we describe two sets of experiments. In the first set of experiments we ablate the exploration term for the sampling policy. The algorithm presented in the main paper corresponds to\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nexploration equal to 1, i.e. the sampling policy always plays uniform. Alternatively, we could sample from only the current policy, which we call exploration of 0. Lastly, we present results where we sample from a mixture of 0.1 times the uniform policy and 0.9 times the current policy, which we call exploration of 0.1. In the second set of experiments, we compare training the value function from scratch every iteration against not re-initializing the value function.\n\nAs shown in the top row of Figure 4, these preliminary results suggest that there is not much difference in which sampling distribution we use. However, this is likely due the the games we evaluate on. We suspect that in games such as video games, sampling from a uniform distribution will perform worse than from the current policy, because a uniform distribution will spend most of its time on bad actions. However, little is known theoretically about this on-policy setting, and it is an interesting direction for future research. On the bottom row we see that there again isn’t much difference between re-initializing the value function and keeping the same value function, but keeping the same value function performs slightly better. We suspect that using best practices from learning on-policy value functions in common single-agent actor-critic algorithms will improve performance.\n\n(a) Phantom TTT Exploration\n\n(b) Dark Hex 5 Exploration\n\n(c) Dark Chess Exploration\n\n(d) Phantom TTT Reinitialize Value Function\n\n(e) Dark Hex 5 Reinitialize Value Function\n\n(f) Dark Chess Reinitialize Value Function\n\nFigure 4: Sampling from a uniform strategy vs. sampling from the current policy or a mixture of the two does not seem to make a large difference against a random opponent (top row). Not re-initializing the value function seems to perform slightly better than training from scratch every iteration (bottom row).\n\nD HYPERPARAMETERS FOR DEEP EXPERIMENTS\n\nFor all deep experiments we first did a hyperparameter search that starts with good hyperparameters for flop hold ’em poker. We report the final hyperparameters used for the deep experiments. As described in the Dark Chess section, DREAM experiments on dark chess used 500 batches for advantage and average training to not run out of memory. All experiments shown include two seeds, with error bars that correspond to standard error of the mean.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nD.1 ESCHER\n\nParameter\n\nn regret network traversals n history value network traversals batch size regret network batch size history value network train steps regret network train steps history value network train steps average policy network\n\nValue\n\n1,000 1,000 2048 2048 5,000 5,000 10,000\n\nTable 3: ESCHER and Ablations Hyperparameters for Phantom TTT, Dark Hex 4, Dark Hex 5\n\nParameter\n\nn regret network traversals n history value network traversals batch size regret network batch size history value network train steps regret network train steps history value network train steps average policy network\n\nValue\n\n1,000 1,000 2048 2048 500 500 10,000\n\nTable 4: ESCHER and Ablations Hyperparameters for Variance Experiments\n\nWhen computing the value function, we random noise to the current policy to induce coverage over all information sets. To do this we added 0.01 times a uniform distribution to the current policy and renormalized.\n\nD.2 DREAM\n\nWe use the codebase from the original DREAM paper (Steinberger et al., 2020) with a wrapper to integrate with Openspiel (Lanctot et al., 2019) and rllib (Liang et al., 2018). When otherwise specified, we use default parameters from the DREAM codebase.\n\nParameter\n\nn batches adv training n traversals per iter n batches per iter baseline periodic restart max n las sync simultaneously mini batch size adv max buffer size adv mini batch size avrg max buffer size avrg batch size baseline n batches avrg training\n\nValue\n\n4,000 1,000 1,000 10 12 10,000 2,000,000 10,000 2,000,000 2048 4000\n\nTable 5: DREAM Hyperparameters for Phantom TTT, Dark Hex 4, Dark Hex 5\n\nD.3 NFSP\n\nWe use our own implementation of NFSP that uses RLLib’s (Liang et al., 2018) DQN implementation and outperforms the original paper’s results on Leduc poker.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nParameter\n\ncircular buffer size total rollout experience gathered each iter learning rate batch size TD-error loss type target network update frequency RL learner params anticipatory param avg policy reservoir buffer size avg policy learning starts after avg policy learning rate avg policy batch size avg policy optimizer\n\nValue\n\n2e5 1024 steps 0.01 4096 MSE every 10,000 steps DDQN 0.1 2e6 16,000 steps 0.1 4096 SGD\n\nTable 6: NFSP Hyperparameters for Phantom TTT, Dark Hex 4, Dark Hex 5\n\nD.4 DARK CHESS\n\nHyperparameters are the same as in other deep experiments (described above), except DREAM experiments on dark chess used 500 batches for advantage and average training to not run out of memory. For these experiments only the current observation was passed in to the network for each method. As a result, we cannot expect these algorithms to learn a strong strategy on dark chess, but it is still a fair comparison. In future work we plan on doing more engineering to include more information to the networks.\n\nE CODE\n\nWe have open sourced our code here: https://github.com/Sandholm-Lab/ESCHER. Our code is built on top of the OpenSpiel (Lanctot et al., 2019) and Ray (Moritz et al., 2018) frameworks, both of which are open source and available under the Apache-2.0 license.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper proposes to learn a history-dependent value function and sample actions from a fixed sampling policy to replace the importance sampling and reduce variance.\n\nOverall, this paper is well-written and easy to follow, the reviewer is not very professional in the field of CFR, but still catches most of the content.\n\n*** The reviewer has reviewed this paper in neurips2022, since most of the concerns are addressed by the reply at that time, the reviewer would suggest acceptance at this time. \n\nThere are still some minor questions as follows.\n\n# Strength And Weaknesses\n\nStrength:\n\n(1) This paper is well-written and easy to follow, the background part and related work are very detailed.\n\n(2) The authors give proof of the upper bound of ESCHER assuming a fixed sampling policy.\n\n(3) The additional Table 2 in this version clearly shows the differences between ESCHER and previous works \n\nWeaknesses:\n\n(1) ESCHER proposes a two-stage iteration, which may not be a good practice (also discussed in the Limitation part). The gap between the outer fixed sampling policy and the inner regret learning should be addressed experimentally or theoretically.\n\n(2) In the neurips2022 rebuttal, the authors also agreed that \"Yes, retraining a value function every iteration is slow, but that's not the main point of the paper\", therefore, it is expected to estimate the additional overheads for training the value function. And some of the results should be presented in another way. For example, the x-axis in Figure 3 is the number of iterations, which may guide the reader that ESCHER converges much faster than DREAM and OS-MCCFR, but the effort within an iteration should be noted.\n\n(3) The correlation between the high variance of the regret estimator and the low performance/winrate of the algorithm should be addressed. Since the algorithm may have a larger variance due to its unique settings and hyperparameters but can perform well in practice (e.g., proximal policy optimization), the authors are encouraged to give more evidence why ESCHER is better.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nsound\n\n# Summary Of The Review\n\nsee above\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nBITRATE-CONSTRAINED DRO: BEYOND WORST CASE ROBUSTNESS TO UNKNOWN GROUP SHIFTS\n\nAmrith Setlur1 Don Dennis1 Benjamin Eysenbach1 Aditi Raghunathan1 Chelsea Finn2 Virginia Smith1 1 Carnegie Mellon University\n\n2 Stanford University\n\nSergey Levine3\n\n3 UC Berkeley\n\nABSTRACT\n\nTraining machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need not spend valuable model capacity achieving high accuracy on contrived groups of examples. Based on this, we consider the two-player game formulation of DRO where the adversary’s capacity is bitrate-constrained. Our resulting practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations and that of CVaR DRO on long-tailed distributions. Our theoretical analysis reveals that in some settings BR-DRO objective can provably yield statistically efficient and less conservative solutions than unconstrained CVaR DRO.\n\n1\n\nINTRODUCTION\n\nMachine learning models may perform poorly when tested on distributions that differ from the training distribution. A common form of distribution shift is group shift, where the source and target differ only in the marginal distribution over finite groups or sub-populations, with no change in group conditionals (Oren et al., 2019; Duchi et al., 2019) (e.g., when the groups are defined by spurious correlations and the target distribution upsamples the group where the correlation is absent Sagawa et al. (2019)).\n\nPrior works consider various approaches to address group shift. One solution is to ensure robustness to worst case shifts using distributionally robust optimization (DRO) (Bagnell, 2005; Ben-Tal et al., 2013; Duchi et al., 2016), which considers a two-player game where a learner minimizes risk on distributions chosen by an adversary from a predefined uncertainty set. As the adversary is only constrained to propose distributions that lie within an f-divergence based uncertainty set, DRO often yields overly conservative (pessimistic) solutions (Hu et al., 2018) and can suffer from statistical challenges (Duchi et al., 2019). This is mainly because DRO upweights high loss points that may not form a meaningful group in the real world, and may even be contrived if the high loss points simply correspond to randomly mislabeled examples in the training set. Methods like Group DRO (Sagawa et al., 2019) avoid overly pessimistic solutions by assuming knowledge of group membership for each training example. However, these group-based methods provide no guarantees on shifts that deviate from the predefined groups (e.g., when there is a new group), and are not applicable to problems that lack group knowledge. In this work, we therefore ask: Can we train non-pessimistic robust models without access to group information on training samples?\n\nWe address this question by considering a more nuanced assumption on the structure of the underlying groups. We assume that, conditioned on the label, group boundaries are realized by high-level features that depend on a small set of underlying factors (e.g., background color, brightness). This leads to simpler group\n\n⇤Correspondence can be sent to asetlur@cs.cmu.edu.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTrue group classifier\n\nTrue label classifier\n\nP(min) = P(noise) = 0.05, d = 10, n = 500, core /spu = 10\n\nJTT (Liu et al.), CVaR DRO (Levy et al.) minority\n\nmajority\n\nBitrate-Constrained DRO (ours) \n\nminority\n\nmajority\n\ny t\ni r\no n\nm\n\ni\n\ny t\ni r\no\n\nj\n\na\n\nm\n\nLandbird in land background\n\nLandbird in water background\n\nWaterbird in land background\n\nWaterbird in water background\n\nmajority\n\nminority\n\nmajority\n\nminority\n\nFigure 1: Bitrate-Constrained DRO: A method that assumes group shifts along low-bitrate features, and restricts the adversary appropriately so that the solution found is less pessimistic and more robust to unknown group shifts. Our method is also robust to training noise. (Left) In Waterbirds (Wah et al., 2011), the spurious feature background is a large margin simple feature that separates the majority and minority points in each class. (Right) Prior works (Levy et al., 2020; Liu et al., 2021) that upweight arbitrary points with high losses force the model to memorize noisy mislabeled points while our method is robust to noise and only upweights the true minority group without any knowledge of its identity (see Section 6.2).\n\nfunctions with large margin and simple decision boundaries between groups (Figure 1 (left)). Invoking the principle of minimum description length (Gr ̈unwald, 2007), restricting our adversary to functions that satisfy this assumption corresponds to a bitrate constraint. In DRO, the adversary upweights points with higher losses under the current learner, which in practice often correspond to examples that belong to a rare group, contain complex patterns, or are mislabeled (Carlini et al., 2019; Toneva et al., 2018). Restricting the adversary’s capacity prevents it from upweighting individual hard or mislabeled examples (as they cannot be identified with simple features), and biases it towards identifying erroneous data points misclassified by simple features. This also complements the failure mode of neural networks trained with stochastic gradient descent (SGD) that rely on simple spurious features which correctly classify points in the majority group but may fail on minority groups (Blodgett et al., 2016).\n\nThe main contribution of this paper is Bitrate-Constrained DRO (BR-DRO), a supervised learning procedure that provides robustness to distribution shifts along groups realized by simple functions. Despite not using group information on training examples, we demonstrate that BR-DRO can match the performance of methods requiring them. We also find that BR-DRO is more successful in identifying true minority training points, compared to unconstrained DRO. This indicates that not optimizing for performance on contrived worst-case shifts can reduce the pessimism inherent in DRO. It further validates: (i) our assumption on the simple nature of group shift; and (ii) that our bitrate constraint meaningfully structures the uncertainty set to be robust to such shifts. As a consequence of the constraint, we also find that BR-DRO is robust to random noise in the training data (Song et al., 2022), since it cannot form “groups” entirely based on randomly mislabeled points with low bitrate features. This is in contrast with existing methods that use the learner’s training error to up-weight arbitrary sets of difficult training points (e.g., Liu et al., 2021; Levy et al., 2020), which we show are highly susceptible to label noise (see Figure 1 (right)). Finally, we theoretically analyze our approach—characterizing how the degree of constraint on the adversary can effect worst risk estimation and excess risk (pessimism) bounds, as well as convergence rates for specific online solvers.\n\n2 RELATED WORK\n\nPrior works in robust ML (e.g., Li et al., 2018; Lipton et al., 2018; Goodfellow et al., 2014) address various forms of adversarial or structured shifts. We specifically review prior work on robustness to group shifts. While those based on DRO optimize for worst-case shifts in an explicit uncertainty set, the robust set is implicit for some others, with most using some form of importance weighting.\n\nDistributionally robust optimization (DRO). DRO methods generally optimize for worst-case performance on joint (x,y) distributions that lie in an f-divergence ball (uncertainty set) around the training distribution (Ben-Tal et al., 2013; Rahimian & Mehrotra, 2019; Bertsimas et al., 2018; Blanchet & Murthy, 2019; Miyato et al., 2018; Duchi et al., 2016; Duchi & Namkoong, 2021). Hu et al. (2018) highlights that the conservative nature of DRO may lead to degenerate solutions when the unrestricted adversary uniformly upweights all misclassified points. Sagawa et al. (2019) proposes to address this by limiting the adversary to shifts that only differ in marginals over predefined groups. However, in addition to it being difficult to obtain this information, Kearns et al. (2018) raise “gerrymandering” concerns with notions of robustness that fix a small number of groups apriori. While they propose a solution that looks at exponentially many subgroups defined over protected attributes, our method does not assume access to such attributes and\n\n2\n\nPublished as a conference paper at ICLR 2023\n\naims to be fair on them as long as they are realized by simple functions. Finally, Zhai et al. (2021) avoid conservative solutions by solving the DRO objective over randomized predictors learned through boosting. We consider deterministic and over-parameterized learners and instead constrain the adversary’s class.\n\nConstraining the DRO uncertainty set. In the marginal DRO setting, Duchi et al. (2019) limit the adversary via easier-to-control reproducing kernel hilbert spaces (RKHS) or bounded H ̈older continuous functions (Liu & Ziebart, 2014; Wen et al., 2014). While this reduces the statistical error in worst risk estimation, the size of the uncertainty set (scales with the data) remains too large to avoid cases where an adversary can reweight mislabeled and hard examples from the majority set (Carlini et al., 2019). In contrast, we restrict the adversary even for large datasets where the estimation error would be low, as this would reduce excess risk when we only care about robustness to rare sub-populations defined by simple functions. Additionally, while their analysis and method prefers the adversary’s objective to have a strong dual, we show empirical results on real-world datasets and generalization bounds where the adversary’s objective is not necessarily convex.\n\nRobustness to group shifts without demographics. Recent works (Sohoni et al., 2020; Creager et al., 2021; Bao & Barzilay, 2022) that aim to achieve group robustness without access to group labels employ various heuristics where the robust set is implicit while others require data from multiple domains (Arjovsky et al., 2019; Yao et al., 2022) or ability to query test samples (Lee et al., 2022). Liu et al. (2021) use training losses for a heavily regularized model trained with empirical risk minimization (ERM) to directly identify minority data points with higher losses and re-train on the dataset that up-weights the identified set. Nam et al. (2020) take a similar approach. Other methods (Idrissi et al., 2022) propose simple baselines that subsample the majority class in the absence of group demographics and the majority group in its presence. Hashimoto et al. (2018) find DRO over a 2-divergence ball can reduce the otherwise increasing disparity of per-group risks in a dynamical system. Since it does not use features to upweight points (like BR-DRO) it is vulnerable to label noise. Same can be said about some other works (e.g., Liu et al. (2021); Nam et al. (2020)).\n\nImportance weighting in deep learning. Finally, numerous works (Duchi et al., 2016; Levy et al., 2020; Lipton et al., 2018; Oren et al., 2019) enforce robustness by re-weighting losses on individual data points. Recent investigations (Soudry et al., 2018; Byrd & Lipton, 2019; Lu et al., 2022) reveal that such objectives have little impact on the learned solution in interpolation regimes. One way to avoid this pitfall is to train with heavily regularized models (Sagawa et al., 2019; 2020) and employ early stopping. Another way is to subsample certain points, as opposed to up-weighting (Idrissi et al., 2022). In this work, we use both techniques while training our objective and the baselines, ensuring that the regularized class is robust to shifts under misspecification (Wen et al., 2014).\n\n3 PRELIMINARIES\n\nWe introduce the notation we use in the rest of the paper and describe the DRO problem. In the following section, we will formalize our assumptions on the nature of the shift before introducing our optimization objective and algorithm.\n\nd and labels\n\nR\n\nX⇢\n\n, the given source P and unknown true target Q0 are Notation. With covariates ,⌃) and have densities p and q0 respectively (w.r.t. base measures over the measurable space ( L2(P ), and the adversary’s action measure μ). The learner’s choice is a hypothesis h: QP, := in standard DRO is a target distribution Q in set . Here, Df is the f-divergence between Q and P for a convex function f 1 with f(1) = 0. An equivalent action space for the adversary is the set of re-weighting functions:\n\nin class Q : Q\n\nH⇢ ⌧\n\nP,Df (Q\n\nX7!Y\n\nX⇥Y\n\nP )\n\n\n\nY\n\n\n\n||\n\n}\n\n{\n\nWP, =\n\n{\n\nw :\n\nX⇥Y7!\n\nR: w is measurable underP,EP [w]=1,EP f(w)\n\n(1)\n\n\n\n\n\n}\n\nFor a convex loss function l : l(h(x),y), and use l0 a re-weighting function w\n\nY⇥Y7!\n\n1 to denote the loss function (h(x) 2WP,, the risk of a learner h is:\n\nR+, we denote l(h) as the function over (x,y) that evaluates 2Q P,, or\n\n= y). Given either distribution Q\n\nR(h,Q)=EQ [l(h)] R(h,w)=E(x,y)\n\nP [l(h(x),y)\n\nw(x,y)]=\n\n⇠\n\n·\n\nl(h),w h\n\niP\n\n). If the adversary is stochastic it picks a mixed action  Note the overload of notation for R(h, ·\nWP,. Whenever it is clear, we drop P,. which is the set of all distributions over\n\n(2)\n\n(\n\nWP,),\n\n2\n\nUnconstrained DRO (Ben-Tal et al., 2013). This is a min-max optimization problem understood as a two-player game, where the learner chooses a hypothesis, to minimize risk on the worst distribution that\n\n1For e.g., KL(Q\n\n||\n\nP ) can be derived with f(x)=xlogx and for Total Variation f(x)=\n\nx |\n\n/2. 1\n|\n\n3\n\n6 Published as a conference paper at ICLR 2023\n\nWP,).\n\nR(h,w)\n\nthe adversary can choose from its set. Formally, this is given by Equation 3. The first equivalence is clear from the definitions and for the second since R(h,Q) is linear in Q, the supremum over ( Dirac delta over the best weighting in adversary can only pick certain actions from (\n\nWP,) is a WP,. In the next section, we will see how a bitrate-constrained\n\ninf h\n2H\n\nsup 2QP,\n\nQ\n\nR(h,Q)\n\n⌘\n\ninf h\n2H\n\nsup 2WP,\n\nw\n\n⌘\n\ninf h\n2H\n\nsup (\n\nWP,)\n\n2\n\n[R(h,w)]\n\nEw\n\n⇠\n\n(3)\n\nGroup Shift. While the DRO framework in Section 3 is broad and addresses any unstructured shift, we focus on the specific case of group shift. First, for a given pair of measures P,Q we define what GP,Q (Definition 3.1). Intuitively, it is a set of sub-populations along we mean by the group structure which the distribution shifts, defined in a way that makes them uniquely identifiable. For e.g., in the Waterbirds dataset (Figure 1), there are four groups given by combinations of (label, background). Corollary 3.2 follows immediately from the definition of shift assumption (Sagawa et al., 2019) can be formally re-stated as Assumption 3.3.\n\nGP,Q. Using this definition, the standard group\n\nDefinition 3.1 (group structure finite set of disjoint groups Gk)=q(x,y p(x,y\n\nGP,Q). For Q k=1 s.t. Q( Gk}\n\nK\n\n⌧ [\n\n8 Gk)>0a.e. in μ. If such a structure exists then\n\n{\n\nP the group structure K\nk=1Gk)=1 and\n\nGP,Q= k (i) Gk 2 GP,Q is well defined. (P,Q) is unique if it is well defined.\n\nGk} k=1 is the smallest {\n⌃, Q(Gk) > 0 and (ii)\n\nCorollary 3.2 (uniqueness of Assumption 3.3 (standard group shift). There exists a well-defined group structure differs from P only in terms of marginal probabilities over all G\n\nP,Q, the structure\n\nGP,Q).\n\nG\n\n8\n\nGP,Q0 s.t. target Q0\n\n|\n\n|\n\nK\n\n2GP,Q0.\n\n4 BITRATE-CONSTRAINED DRO\n\nWe begin with a note on the expressivity of the adversary in Unconstrained DRO and formally introduce the assumption we make on the nature of shift. Then, we build intuition for why unconstrained adversaries fail but restricted ones do better under our assumption. Finally, we state our main objective and discuss a specific instance of it.\n\nHow expressive is unconstrained adversary? Note that the set WP, includes all measurable functions (under P ) such that the re-weighted distribution is bounded in f-divergence (by ). While prior works (Shafieezadeh Abadeh et al., 2015; Duchi et al., 2016) shrink  to construct confidence intervals, ⌃, but does not this only controls the total mass that can be moved between measurable sets G1,G2 2 restrict the choice of G1 and G2 itself. As noted by Hu et al. (2018), such an adversary is highly expressive, and optimizing for the worst case only leads to the solution of empirical risk minimization (ERM) under l0 1 loss. Thus, we can conclude that DRO recovers degenerate solutions because the worst target in WP, lies far from the subspace of naturally occurring targets. Since it is hard to precisely characterize natural targets we make a nuanced assumption: the target Q0 only upsamples those rare subpopulations that are misclassified by simple features. We state this formally in Assumption 4.2 after we define the bitrate-constrained function class\n\n() in Definition 4.1.\n\n{\n\nW\n\n(\n\n()=\n\nW ),KL(\n\n() is bitrate-constrained if there exists a data independent prior ⇡,\n\nDefinition 4.1. A function class E[]:  s.t. Assumption 4.2 (simple group shift). Target Q0 satisfies Assumption 3.3 (group shift) w.r.t. source P . Additionally, For some prior ⇡ and a small ⇤, the re-weighting function q0/p lies in a bitrate-constrained G) = wG a.e.. (⇤). In other words, for every group G class We refer to such a G as a simple group that is realized in\n\nwG 2W\n\n(P,Q0),\n\n(⇤) s.t.\n\n9 (⇤).\n\n((x,y)\n\n2G\n\n. }\n\nW\n\nW\n\n⇡)\n\n\n\n2\n\n2\n\n||\n\nW\n\nW\n\n||\n\n⇡)) increases the description length of the encoding \n\nUnder the principle of minimum description length (Gr ̈unwald, 2007) any deviation from the prior (i.e., KL( () as being bitrate-constrained in the sense that it contains functions (means of distributions) that can be described with a limited number of bits given the prior ⇡. See Appendix A.3 for an example of a bitrate-constrained class of functions. Next we present arguments for why identifiability of simple (satisfy Assumption 4.2) minority groups can be critical for robustness.\n\n), thus we refer to\n\n(\n\nW\n\nW\n\n2\n\nNeural networks can perform poorly on simple minorities. For a fixed target Q0, let’s say there P (Gmaj). By Assumption 4.2, both (P,Q0) such that P (Gmin) exists two groups: Gmin and Gmaj 2G Gmin and Gmaj are simple (realized in (⇤)), and are thus separated by some simple feature. The learner’s class is usually a class of overparameterized neural networks. When trained with stochastic gradient descent (SGD), these are biased towards learning simple features that classify a majority of the\n\n⌧\n\nW\n\nH\n\n4\n\nPublished as a conference paper at ICLR 2023\n\ndata (Shah et al., 2020; Soudry et al., 2018). Thus, if the simple feature separating Gmin and Gmaj itself correlates with the label y on Gmaj, then neural networks would fit on this feature. This is precisely the case in the Waterbirds example, where the groups are defined by whether the simple feature background correlates with the label (Figure 1). Thus our assumption on the nature of shift complements the nature of neural networks perform poorly on simple minorities.\n\n(P,Q0). Any method that aims The bitrate constraint helps identify simple unfair minorities in to be robust on Q0 must up-weight data points from Gmin but without knowing its identity. Since the unconstrained adversary upsamples any group of data points with high loss and low probability, it cannot (⇤) and a rare group of distinguish between a rare group that is realized by simple functions in examples that share no feature in common or may even be mislabeled. On the other hand, the group of (⇤). Thus, a bitrate constraint mislabeled examples cannot be separated from the rest by functions in adversary can only identify simple groups and upsamples those that incur high losses – possibly due to the simplicity bias of neural networks.\n\nW\n\nW\n\nG\n\n(P,Q0) is not realized in bitrate constrained class\n\nBR-DRO objective. According to Assumption 4.2, there cannot exist a target Q0 such that minority (⇤). Thus, by constraining our adversary Gmin 2G () (for some  that is user defined), we can possibly evade issues emerging from optimizing to a class for performance on mislabeled or hard examples, even if they were rare. This gives us the objective in Equation 4 where the equalities hold from the linearity of\n\nand Definition 4.1.\n\nW\n\nW\n\n, h· ·i l(h),E[w] h\n\niP = inf\n\nh\n\nw\n\n2H\n\n2W\n\n()\n\nsup\n\nR(h,w)\n\n(4)\n\ninf h\n2H\n\nsup (\n\n 2\nKL(\n\n)\n\nW ⇡)\n\n\n\n||\n\nEw\n\nR(h,w) = inf 2H\n\nh\n\n⇠\n\nsup (\n\n 2\nKL(\n\n)\n\nW ⇡)\n\n\n\n||\n\n⇥h and adversary ✓w 2\n\n⇥w as neural networks2. BR-DRO in practice. We parameterize the learner ✓h 2 In practice, we implement the adversary either as a one hidden layer variational information bottleneck (VIB) (Alemi et al., 2016), where the Kullback-Leibler (KL) constraint on the latent variable z (output of VIB’s hidden layer) directly constrains the bitrate; or as an l2 norm constrained linear layer. The objective =0) in Equation 5 below. See Appendix A.2 for the VIB (l2) version is obtained by setting vib 6 for details. Note that the objective in Equation 5 is no longer convex-concave and can have multiple local equilibria or stationary points (Mangoubi & Vishnoi, 2021). The adversary’s objective also does not have a strong dual that can be solved through conic programs—a standard practice in DRO literature (Namkoong & Duchi, 2016). Thus, we provide an algorithm where both learner and adversary optimize BR-DRO iteratively through stochastic gradient ascent/descent (Algorithm 1 in Appendix A.1).\n\n=0 (l2 6\n\nLadv(✓w;✓h,vib,l2,⌘)\n\n(5)\n\nmin ✓h2 Ladv(✓w;✓h,vib,l2,⌘)=\n\n⇥hh\n\nl(✓h),✓⇤wiP s.t. ✓⇤w =argmax\n\n✓w2\n\n⇥w\n\nl(✓h) h\n\n⌘,✓wiP \n\nvib EP KL(p(z\n\nx;✓w)\n\n(0,Id))\n\n|\n\n||N\n\nl2k\n\n✓wk\n\n2 2\n\nTraining. For each example, the adversary takes as input: (i) the last layer output of the current learner’s feature network; and (ii) the input label. The adversary then outputs a weight (in [0,1]). The idea of applying the adversary directly on the learner’s features (instead of the original input) is based on recent literature (Rosenfeld et al., 2022; Kirichenko et al., 2022) that suggests re-training the prediction head is sufficient for robustness to shifts. The adversary tries to maximize weights on examples with value ⌘ (hyperparameter) and minimize on others. For the learner, in addition to the example it takes as input \nthe adversary assigned weight for that example from the previous round and uses it to reweigh its loss in a minibatch. Both players are updated in a round (Algorithm 1).\n\n5 THEORETICAL ANALYSIS\n\nThe main objective of our analysis of BR-DRO is to show how adding a bitrate constraint on the adversary can: (i) give us tighter statistical estimates of the worst risk; and (ii) control the pessimism (excess risk) of the learned solution. First, we provide worst risk generalization guarantees using the PAC-Bayes framework (Catoni, 2007), along with a result for kernel adversary. Then, we provide convergence rates () For and pessimism guarantees for the solution found by our online solver for a specific instance of both these, we analyze the constrained form of the conditional value at risk (CVaR) DRO objective (Levy et al., 2020) below.\n\nW\n\nBitrate-Constrained CVaR DRO. When the uncertainty set butions Q that have bounded likelihood i.e.,\n\nis defined by the set of all distri1/↵0, we recover the original CVaR DRO\n\nq/p\n\nQ\n\nk\n\nk1 \n\n2We use ✓h,✓w and l(✓h) to denote w(✓w;(x,y)),h(✓h;x) and l(h(✓h;x),y) respectively.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nobjective (Duchi & Namkoong, 2021). The bitrate-constrained version of CVaR DRO is given in Equation 6 (see Appendix C for derivation). Note that, slightly different from Section 3, we define as [0,1], since the other convex restrictions in Equation 1 are the set of all measurable functions w: handled by dual variable ⌘. As in Section 4, using Definition 4.1. In Equation 6, then we recover the variational if we replace the bitrate-constrained class form of unconstrained CVaR DRO in Duchi et al. (2016).\n\nX⇥Y 7! () is derived from W\nW () with the unrestricted\n\nW\n\nW\n\nW\n\n⇤cvar()= inf\n\nL\n\nh\n\n,⌘\n\n2R\n\nw\n\n2H\n\nsup\n\n()\n\n2W\n\nl(h) R(h,⌘,w) where, R(h,⌘,w)=(1/↵0) h\n\n⌘,w\n\niP +⌘\n\n(6)\n\nD,ˆ⌘\n\nP n, Worst risk estimation bounds for BR-DRO. Since we are only given a finite sampled dataset we solve the objective in Equation 6 using the empirical distribution ˆPn. We denote the plug-in estimates as ˆh D. This incurs an estimation error for the true worst risk. But when we restrict our adversary ,), for a fixed learner h we reduce the worst-case risk estimation error which scales with the to ( ⇡) of the solution (deviation from prior ⇡). Expanding this argument to every learner in bitrate KL( D. Theorem 5.1 states\n\n, with high probability we also reduce the estimation error for the worst risk of ˆh\n\nD⇠\n\nW\n\n· ||\n\nH this generalization guarantee more precisely. Theorem 5.1 (worst-case risk generalization). With probability bitrate-constrained ↵0-CVaR risk for ˆh\n\n D can be upper bounded by the following oracle inequality:\n\nD⇠\n\n1\n\n over\n\nP n, the worst\n\nw\n\nsup\n\nR(ˆh\n\nD,ˆ⌘\n\nM ↵0 s✓ ) is [0,M]-bounded, L-Lipschitz and ,\nwhen l( ·\n·\n\nD,w) <\n\n⇤cvar()+\n\n⇠ L\n\n2W\n\n()\n\nH\n\n+log\n\n1 \n\n◆\n\n+(d+1)log\n\n✓\n\nL2n \n\n◆\n\n✓\n\n+logn\n\n/(2n\n\n1),\n\nis parameterized by convex set ⇥\n\n◆\n\nd.\n\nR\n\n⇢\n\n(\n\nC\n\np\n\n(+\n\n( H\n\n))/n) (where\n\nInformally, Theorem 5.1 tells us that bitrate-constraint  gracefully controls the estimation error ) is a complexity measure) if we know that Assumption 4.2 is satisfied.\n\n( O\nH C\nOp(1/pn), the estimate may itself be conWhile this only tells us that our estimator is consistent with ⇤cvar() may be very high. For example, if the adversary can cleanly verging to a degenerate predictor, i.e., separate mislabeled points even after the bitrate constraint, then presumably these noisy points with high losses would be the ones mainly contributing to the worst risk, and up-weighting these points would result in a learner that has memorized noise. Thus, it becomes equally important for us to analyze the excess risk (or the pessimism) for the learned solution. Since this is hard to study for any arbitrary bitrate-constrained class\n\n(), we shall do so for the specific class of reproducing kernel Hilbert space (RKHS) functions.\n\nL\n\n() in Definition 4.1 Special case of bounded RKHS. Let us assume there exists a prior ⇧ such that is given by an RKHS induced by Mercer kernel k : the eigenvalues of the kernel operator decay polynomially, i.e., μj < D by doing kernel ⇠\n1) smooth functions f then we can control: (i) the f\nridge regression over norm bounded ( \nk pessimism of the learned solution; and (ii) the generalization error (Theorem 5.2). Formally, we refer to pessimism for estimates ˆh\n\nW 2/ (< 2). Then, if we solve for ˆh D,ˆ⌘\n\nD as excess risk defined as:\n\nX⇥X7!\n\nD,ˆ⌘\n\nR, s.t.\n\n()\n\nkW\n\nj\n\nB\n\nexcess risk:= sup\n\n()|\n\nw\n\n2W\n\ninf h,⌘\n\nR(h,⌘,w)\n\nR(ˆh\n\nD,ˆ⌘\n\n. D,w) |\n\n(7)\n\nTheorem 5.2 (bounded RKHS). For l, all sufficiently bitrate-constrained\n\nH\n\n() i.e., \n\n(1/n)\n\nW log(1/)+(d+1)log(nB\n\nL/2)\n\nin Theorem 5.1, and for\n\n0, w.h.p. 1 and the excess risk is\n\n\n\n() described above\n\n0 s.t. for  worst risk generalization error is\n\nW\n\n9\n\n(B) for ˆh\n\nD,ˆ⌘\n\nD above.\n\nO Thus, in the setting described above we have shown how bitrate-constraints given indirectly by ,R can control both the pessimism and statistical estimation errors. Here, we directly analyzed the estimates ˆh D,ˆ⌘ D but did not describe the specific algorithm used to solve the objective in Equation 6 with ˆPn. Now, we look at an iterative online algorithm to solve the same objective and see how bitrate-constraints can also influence convergence rates in this setting.\n\nO\n\nConvergence and excess risk analysis for an online solver. In the following, we provide an algorithm to solve the objective in Equation 6 and analyze how bitrate-constraint impacts the solver and the solution. For convex losses, the min-max objective in Equation 6 has a unique solution and this matches the unique Nash equilibrium for the generic online algorithm (game) we describe (Lemma 5.3). The algorithm is as follows: Consider a two-player zero-sum game where the learner uses a no-regret strategy to first play h R\nR(h,⌘,w). Then, the adversary plays follow the regularized leader (FTRL) strategy to minimize Ew\n\n2H\n\n,⌘\n\n2\n\n⇠\n\n6\n\nW\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nAvg\n\nWG\n\nAvg\n\nWG\n\nWaterbirds\n\nCelebA\n\nCivilComments\n\nAvg\n\nWG\n\nERM LfF (Nam et al., 2020) RWY (Idrissi et al., 2022) JTT (Liu et al., 2021) CVaR DRO (Levy et al., 2020)\n\nBR-DRO (VIB) (ours) BR-DRO (l2) (ours)\n\n97.1 (0.1) 90.7 (0.2) 93.7 (0.3) 93.2 (0.2) 96.3 (0.2)\n\n94.1 (0.2) 93.8 (0.2)\n\n71.0 (0.4) 77.6 (0.5) 85.8 (0.5) 86.6 (0.4) 75.5 (0.4)\n\n86.3 (0.3) 86.4 (0.3)\n\n95.4 (0.2) 85.3 (0.2) 84.9 (0.2) 87.6 (0.2) 82.2 (0.3)\n\n86.7 (0.2) 87.7 (0.3)\n\n46.9 (1.0) 77.4 (0.7) 80.4 (0.3) 81.3 (0.5) 64.7 (0.6)\n\n80.9 (0.4) 80.4 (0.6)\n\n92.3 (0.2) 92.4 (0.1) 91.7 (0.2) 90.8 (0.3) 92.3 (0.2)\n\n90.5 (0.2) 91.0 (0.3)\n\n57.2 (0.9) 58.9 (1.1) 67.7 (0.7) 69.4 (0.8) 60.2 (0.8)\n\n68.7 (0.9) 68.9 (0.7)\n\nGroup DRO Sagawa et al. (2019)\n\n93.2 (0.3)\n\n91.1 (0.3)\n\n92.3 (0.3)\n\n88.4 (0.6)\n\n88.5 (0.3)\n\n70.0 (0.5)\n\nTable 1: BR-DRO recovers worst group performance gap between CVaR DRO and Group DRO: On Waterbirds, CelebA and CivilComments we report test average (Avg) and test worst group (WG) accuracies for BR-DRO ) we report the standard error of the mean accuracy across five runs. and baselines. In ( ·\n\n2\n\nW\n\nW\n\n(\n\nto pick distribution  ()) to maximize the same. Our goal is to analyze the bitrate-constraint ’s effect on the above algorithm’s convergence rate and the pessimistic nature of the solution found. For (). If we assume there exists a prior ⇧ this, we need to first characterize the bitrate-constraint class () is Vapnik-Chervenokis (VC) class of dimension O(), then in Theorem 5.4, we see that such that logn/T ) steps. Clearly, the the iterates of our algorithm converge to the equilibrium (solution) in degree of bitrate constraint can significantly impact the convergence rate for a generic solver that solves the constrained DRO objective. Theorem 5.4 also bounds the excess risk (Equation 7) on ˆPn. Lemma 5.3 (Nash equilibrium). For strictly convex l(h), l(h) [0,M], the objective in Equation 6 has a unique solution which is also the Nash equilibrium of the game above when played over compact sets\n\np\n\nW\n\nO\n\n2\n\n(\n\n[0,M], (\n\n,). We denote this equilibrium as h⇤D(),⌘⇤D(),⇤D().\n\nW H⇥ Theorem 5.4. At time step t, if the learner plays (ht,⌘ t) with no-regret and the adversary plays t with FTRL strategy that uses a negative entropy regularizer on  then average iterates ( ̄hT , ̄⌘T , ̄T ) = (1/T ) t=1(ht,⌘ t, t) converge to the equilibrium (h⇤D(),⌘ ⇤D(), ⇤D()) at rate\n\nT\n\n(\n\nO\n\nlogn/T ). Further the excess risk defined above is\n\nP\n\np\n\n6 EXPERIMENTS\n\n((M/↵0)\n\n1\n\nO\n\n1 n\n\n).\n\nOur experiments aim to evaluate the performance of BR-DRO and compare it with ERM and group shift robustness methods that do not require group annotations for training examples. We conduct empirical analyses along the following axes: (i) worst group performance on datasets that exhibit known spurious correlations; (ii) robustness to random label noise in the training data; (iii) average performance on hybrid covariate shift datasets with unspecified groups; and (iv) accuracy in identifying minority groups. See Appendix B for additional experiments and details3.\n\nBaselines. Since our objective is to be robust to group shifts without group annotations on training examples, we explore baselines that either optimize for the worst minority group (CVaR DRO (Levy et al., 2020)) or use training losses to identify specific minority points (LfF (Nam et al., 2020), JTT (Liu et al., 2021)). Group DRO (Sagawa et al., 2019) is treated as an oracle. We also compare with the simple re-weighting baseline (RWY) proposed by Idrissi et al. (2022).\n\nImplementation details. We train using Resnet-50 (He et al., 2016) for all methods and datasets except CivilComments, where we use BERT (Wolf et al., 2019). For our VIB adversary, we use a 1-hidden layer neural network encoder and decoder (one for each label). As mentioned in Section 4, the adversary takes as input the learner model’s features and the true label to generate weights. All implementation and design choices for baselines were adopted directly from Liu et al. (2021); Idrissi et al. (2022). We provide model selection methodology and other details in Appendix B.\n\nDatasets. For experiments in the known groups and label noise settings we use: (i) Waterbirds (Wah et al., 2011) (background is spurious), CelebA (Liu et al., 2015) (binary gender is spuriously correlated with label “blond”); and CivilComments (WILDS) (Borkan et al., 2019) where the task is to predict “toxic” texts and there are 16 predefined groups Koh et al. (2021). We use FMoW and Camelyon17 (Koh et al., 2021) to test methods on datasets that do not have explicit group shifts. In FMoW the task is to predict land use from satellite images where the training/test set comprises of data before/after 2013. Test involves both subpopulation shifts over regions (e.g., Africa, Asia) and domain generalization over time (year). Camelyon17 presents a domain generalization problem where the task is to detect tumor in tissue slides from different sets of hospitals in train and test sets.\n\n3The code used in our experiments can be found at https://github.com/ars22/bitrate_DRO.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nBitrate-Constrained DRO \n\nJTT\n\nP(min) = P(noise) = 0.05, d = 10, n = 500, core /spu = 10\n\nminority\n\nmajority\n\nmajority\n\nminority\n\n(a)\n\n(b)\n\nFigure 2: (Left) Visualization (2d) of noisy synthetic data and learned predictors: We plot the decision boundaries (projected onto core and spurious features) learned by JTT and BR-DRO when the adversary is restricted to a sparse predictor. While our method recovers the core feature the baselines memorize the minority points. (Right) BR-DRO is robust to random label noise in training data: Across varying levels of noise fraction in training data we compare performance of BR-DRO with ERM and methods (JTT, CVaR DRO) that naively up weight high loss points.\n\n6.1\n\nIS BR-DRO ROBUST TO GROUP SHIFTS WITHOUT TRAINING DATA GROUP ANNOTATIONS?\n\nTable 1 compares the average and worst group accuracy for BR-DRO with ERM and four group shift robustness baselines: JTT, LtF, SUBY, and CVaR DRO. First, we see that unconstrained CVaR DRO underperforms other heuristic algorithms. This matches the observation made by Liu et al. (2021). Next, we see that adding bitrate constraints on the adversary via a KL term or l2 penalty significantly improves the performance of BR-DRO (VIB) or BR-DRO (l2), which now matches the best performing baseline (JTT). Thus, we see the less conservative nature of BR-DRO allows it to recover a large portion of the performance gap between Group DRO and CVaR DRO. Indirectly, this partially validates our Assumption 4.2, which states that the minority group is identified by a low bitrate adversary class. In Section 6.3 we discuss exactly what fraction of the minority group is identified, and the role played by the strength of bitrate-constraint.\n\n6.2 BR-DRO IS MORE ROBUST TO RANDOM LABEL NOISE\n\nSeveral methods for group robustness (e.g., CVaR DRO, JTT) are based on the idea of up weighting points with high training losses. The goal is to obtain a learner with matching performance on every (small) fraction of points in the dataset. However, when training data has mislabeled examples, such an approach will likely yield degenerate solutions. This is because the adversary directly upweights any example where the learner has high loss, including datapoints with incorrect labels. Hence, even if the learner’s prediction matches the (unknown) true label, this formulation would force the learner to memorize incorrect labelings at the expense of learning the true underlying function. On the other hand, if the adversary is sufficiently bitrate constrained, it cannot upweight the arbitrary set of randomly mislabeled points, as this would require it to memorize those points. Our Assumption 4.2 also dictates that the distribution shift would not upsample such high bitrate noisy examples. Thus, our constraint on the adversary ensures BR-DRO is robust to label noise in the training data and our assumption on the target distribution retains its robustness to test time distribution shifts.\n\nIn Figure 2b we highlight this failure mode of unconstrained up-weighting methods in contrast to BR-DRO. We first induce random label noise (Carlini et al., 2019) of varying degrees into the Waterbirds and CelebA training sets. Then we run each method and compare worst group performance. In the absence of noise we see that the performance of JTT is comparable with BR-DRO, if not slightly better (Table 1). Thus, both BR-DRO and JTT perform reasonably well in identifying and upsampling the simple minority group in the absence of noise. In its presence, BR-DRO significantly outperforms JTT and other approaches on both Waterbirds and CelebA, as it only upsamples the minority examples misclassified by simple features, ignoring the noisy examples for the reasons above. To further verify our claims, we set up a noisily labeled synthetic dataset (see Appendix B for details). In Figure 2a we plot training samples as well as the solutions learned by BR-DRO and and JTT on synthetic data. In Figure 1(right) we also plot exactly which points are upweighted by BR-DRO and JTT. Using both figures, we note that JTT mainly upweights the noisy points (in red) and memorizes them using xnoise. Without any weights on minority, it memorizes them as well and learns component along spurious feature. On the contrary, when we restrict the adversary with BR-DRO to be sparse (l1 penalty), it only upweights minority samples, since no sparse predictor can separate noisy points in the data. Thus, the learner can no longer memorize the upweighted minority and we recover the robust predictor along core feature.\n\n6.3 WHAT FRACTION OF MINORITY IS RECOVERED BY BR-DRO?\n\nWe claim that our less pessimistic objective can more accurately recover (upsample) the true minority group if indeed the minority group is simple (see Assumption 4.2 for our definition of simple). In this section, we aim to verify this claim. If we treat examples in the top 10% (chosen for post hoc analysis)\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: By considering the fraction of points upweighted by our adversary (top 10%) as the positive class we analyze the precision and recall of this class with respect to the minority group. and do the same for JTT, random baseline and CVaR DRO. BR-DRO achieves highest precision and matches recall with JTT asymptotically. We also find that increasing bitrate constraint vib helps improving precision/recall.\n\nfraction of examples as our predicted minorities, we can check precision and recall of this decision on the Waterbirds and CelebA datasets. Figure 3 plots these metrics at each training epoch for BR-DRO (with varying vib), JTT and CVaR DRO. Precision of the random baseline tells us the true fraction of minority examples in the data. First we note that BR-DRO consistently performs much better on this metric than unconstrained CVaR DRO. In fact, as we reduce strength of vib we recover precision/recall close to the latter. This controlled experiment shows that the bitrate constraint is helpful (and very much needed) in practice to identify rare simple groups. In Figure 3 we observe that asymptotically, the precision of BR-DRO is better than JTT on both datasets, while the recall is similar. Since importance weighting has little impact in later stages with exponential tail losses (Soudry et al., 2018; Byrd & Lipton, 2019), other losses (e.g., polytail Wang et al. (2021)) may further improve the performance of BR-DRO as it gets better at identifying the minority classes when trained longer.\n\n6.4 HOW DOES BR-DRO PERFORM ON MORE GENERAL COVARIATE SHIFTS?\n\nIn Table 2 we report the average test accuracies for BR-DRO and baselines on the hybrid dataset FMoW and domain generalization dataset Camelyon17. Given its hybrid nature, on FMoW we also report worst region accuracy. First, we note that on these datasets group shift robustness baselines do not do better than ERM. Some are either too pessimistic (e.g., CVaR DRO), or require heavy assumptions (e.g., Group DRO) to be robust to domain generalization. This is also noted by Gulrajani & Lopez-Paz (2020). Next, we see that BR-DRO (l2 version) does better than other group shift baselines on both both worst region and average datasets and matches ERM performance on Camelyon17. One explanation could be that even though these datasets test models on new domains, there maybe some latent groups defining these domains that are simple and form a part of latent subpopulation Investigating this claim further is a shift. promising line of future work.\n\nJTT Liu et al. (2021) 52.1 (0.1) 31.8 (0.2) LfF Nam et al. (2020) 49.6 (0.2) 31.0 (0.3) RWY Idrissi et al. (2022) 50.8 (0.1) 30.9 (0.2) Group DRO Sagawa et al. (2019) 51.9 (0.2) 30.4 (0.3) CVaR DRO Levy et al. (2020) 51.5 (0.1) 31.0 (0.3)\n\nTable 2: Average (Avg) and worst region (W-Reg for FMoW) test accuracies on Camelyon17 and FMoW.\n\nBR-DRO (VIB) (ours) 52.0 (0.2) 31.8 (0.2) BR-DRO (l2) (ours) 53.1 (0.1) 32.3 (0.2)\n\n66.3 (1.3) 65.8 (1.2) 69.9 (1.3) 68.5 (0.9) 66.8 (1.3)\n\nERM 53.3 (0.1) 32.4 (0.3)\n\n70.4 (1.5) 71.2 (1.0)\n\nCamelyon17 Avg\n\n70.6 (1.6)\n\nMethod\n\nFMoW\n\nW-Reg\n\nAvg\n\n7 CONCLUSION\n\nIn this paper, we proposed a method for making machine learning models more robust. While prior methods optimize robustness on a per-example or per-group basis, our work focuses on features. In doing so, we avoid requiring group annotations on training samples, but also avoid the excessively conservative solutions that might arise from CVaR DRO with fully unconstrained adversaries. Our results show that our method avoids learning spurious features, is robust to noise in the training labels, and does better on other forms of covariate shifts compared to prior approaches. Our theoretical analysis also highlights other provable benefits in some settings like reduced estimation error, lower excess risk and faster convergence rates for certain solvers.\n\nLimitations. While our method lifts the main limitation of Group DRO (access to training group annotations), it does so at the cost of increased complexity. Further, to tune hyperparameters, like prior work we assume access to a some group annotations on validation set but also get decent performance (on some datasets) with only a balanced validation set (see Appendix B). Adapting group shift methods to more generic settings remains an important and open problem.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nAcknowledgement. The authors would like to thank Tian Li, Saurabh Garg at Carnegie Mellon University, and Yoonho Lee at Stanford University for helpful feedback and discussion.\n\nREFERENCES\n\nJacob Abernethy, Kevin A Lai, Kfir Y Levy, and Jun-Kun Wang. Faster rates for convex-concave games.\n\nIn Conference On Learning Theory, pp. 1595–1625. PMLR, 2018.\n\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\n\nbottleneck. arXiv preprint arXiv:1612.00410, 2016.\n\nMartin Arjovsky, L ́eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv\n\npreprint arXiv:1907.02893, 2019.\n\nJ Andrew Bagnell. Robust supervised learning. In AAAI, pp. 714–719, 2005.\n\nYujia Bao and Regina Barzilay. Learning to split for automatic bias detection. arXiv preprint\n\narXiv:2204.13749, 2022.\n\nPeter L Bartlett, Sanjeev R Kulkarni, and S Eli Posner. Covering numbers for real-valued function classes.\n\nIEEE transactions on information theory, 43(5):1721–1724, 1997.\n\nAharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2): 341–357, 2013.\n\nDimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathematical\n\nProgramming, 167(2):235–292, 2018.\n\nJose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathe-\n\nmatics of Operations Research, 44(2):565–600, 2019.\n\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social media: A\n\ncase study of african-american english. arXiv preprint arXiv:1608.08868, 2016.\n\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion proceedings of the 2019 world wide web conference, pp. 491–500, 2019.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university\n\npress, 2004.\n\nJonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In\n\nInternational Conference on Machine Learning, pp. 872–881. PMLR, 2019.\n\nNicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot. Distribution density, tails, and outliers in machine\n\nlearning: Metrics and applications. arXiv preprint arXiv:1910.13427, 2019.\n\nOlivier Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning. arXiv\n\npreprint arXiv:0712.0248, 2007.\n\nElliot Creager, J ̈orn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In\n\nInternational Conference on Machine Learning, pp. 2189–2200. PMLR, 2021.\n\nJohn Duchi, Peter Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized\n\nempirical likelihood approach. arXiv preprint arXiv:1610.03425, 2016.\n\nJohn C. Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378 – 1406, 2021. doi: 10.1214/20-AOS2004. URL https://doi.org/10.1214/20-AOS2004.\n\nJohn C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses against\n\nmixture covariate shifts. Under review, 2, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.\n\narXiv preprint arXiv:1412.6572, 2014.\n\nPeter D Gr ̈unwald. The minimum description length principle. MIT press, 2007.\n\nIshaan Gulrajani and David Lopez-Paz.\n\nIn search of lost domain generalization. arXiv preprint\n\narXiv:2007.01434, 2020.\n\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pp. 1929–1938. PMLR, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nWeihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pp. 2029–2037. PMLR, 2018.\n\nBadr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In Conference on Causal Learning and Reasoning, pp. 336–351. PMLR, 2022.\n\nMichael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In International Conference on Machine Learning, pp. 2564–2572. PMLR, 2018.\n\nPolina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for\n\nrobustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.\n\nYoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from underspecified\n\ndata. arXiv preprint arXiv:2202.03418, 2022.\n\nDaniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally\n\nrobust optimization. Advances in Neural Information Processing Systems, 33:8847–8860, 2020.\n\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nZachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black\n\nbox predictors. In International conference on machine learning, pp. 3122–3130. PMLR, 2018.\n\nAnqi Liu and Brian Ziebart. Robust classification under sample selection bias. Advances in neural\n\ninformation processing systems, 27, 2014.\n\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR, 2021.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\n\nProceedings of the IEEE international conference on computer vision, pp. 3730–3738, 2015.\n\nYiping Lu, Wenlong Ji, Zachary Izzo, and Lexing Ying. Importance tempering: Group robustness for\n\noverparameterized models. arXiv preprint arXiv:2209.08745, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nOren Mangoubi and Nisheeth K Vishnoi. Greedy adversarial equilibrium: an efficient alternative to nonconvex-nonconcave min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pp. 896–909, 2021.\n\nDavid A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on\n\nComputational learning theory, pp. 230–234, 1998.\n\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018.\n\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems, 33:20673–20684, 2020.\n\nHongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimiza-\n\ntion with f-divergences. Advances in neural information processing systems, 29, 2016.\n\nYonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust language\n\nmodeling. arXiv preprint arXiv:1909.02060, 2019.\n\nNicholas G Polson and Vadim Sokolov. Bayesian regularization: From tikhonov to horseshoe. Wiley\n\nInterdisciplinary Reviews: Computational Statistics, 11(4):e1463, 2019.\n\nHamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv preprint\n\narXiv:1908.05659, 2019.\n\nR Tyrrell Rockafellar. Convex analysis, volume 18. Princeton university press, 1970.\n\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm may already learn features sufficient for out-of-distribution generalization. arXiv preprint arXiv:2202.06856, 2022.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In International Conference on Machine Learning, pp. 8346–8356. PMLR, 2020.\n\nSeonguk Seo, Joon-Young Lee, and Bohyung Han. Unsupervised learning of debiased representations with pseudo-attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16742–16751, 2022.\n\nSoroosh Shafieezadeh Abadeh, Peyman M Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust\n\nlogistic regression. Advances in Neural Information Processing Systems, 28, 2015.\n\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573–9585, 2020.\n\nNimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R ́e. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural Information Processing Systems, 33:19339–19352, 2020.\n\nHwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee\n\ninformation theory workshop (itw), pp. 1–5. IEEE, 2015.\n\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. None, 2011.\n\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge\n\nUniversity Press, 2019.\n\nKe Alexander Wang, Niladri S Chatterji, Saminul Haque, and Tatsunori Hashimoto.\n\nIs importance\n\nweighting incompatible with interpolating classifiers? arXiv preprint arXiv:2112.12986, 2021.\n\nJunfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions: Relating covariate shift to model misspecification. In International Conference on Machine Learning, pp. 631–639. PMLR, 2014.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nHuaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. arXiv preprint arXiv:2201.00299, 2022.\n\nRuntian Zhai, Chen Dan, Arun Suggala, J Zico Kolter, and Pradeep Ravikumar. Boosted cvar classification.\n\nAdvances in Neural Information Processing Systems, 34:21860–21871, 2021.\n\nYuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression. In\n\nConference on learning theory, pp. 592–617. PMLR, 2013.\n\n13",
    "reference": "# Summary Of The Paper\n\nThis paper proposes bit-rate constrained group DRO, where the assumption is that group identities can be modeled by a simple function class. This allows the framework to improve the overall utility of the learned model by relying less on arbitrarily chosen mis-labeled points (in a DRO framework), but ensure that only once that are also well modeled by the bit-rate constrained function are upweighted. Using the motivation, an objective is proposed to come up with a local minima where both the adversary and the hypothesis model is a neural network. Theoretical analysis provides risk bounds as well as analysis on convergence for online solver is provided. Experimental results demonstrate some benefit of the approach on real world data, including domain shift benchmarks.\n\n# Strength And Weaknesses\n\nStrengths:\n1. The paper is largely well written and motivated. \n2. The problem setup is well motivated and being robust to unknown but well modeled group shift is an interesting approach to address lack of robustness of ML models to spurious correlations (if modeled as potential groups).\n\nWeaknesses:\nAlthough the paper is well written, there are many clarifications necessary that could improve the paper:\n1. I am guessing $z$ in Equation 5 corresponds to latent group identities? It seems to be introduced in a rush without explaining the objective function at all, leaving it to the reader to decipher details.\n\n2. Unclear motivation of why adversary needs to be a deep neural network and then add l-2 regularization to the parametrization. I would've liked to see a simpler set up where an exact solution is available, and then a heuristic proposed when adversary is also a complex model. It is also not clear how this ties to the assumption that the group identity function is actually a simple function.\n\n3. The latent group identification via KL regularization is interesting, but then I am a little confused what happens if the group identities somehow overlap with class labels themselves, guessing no real robust learning can happen then?\n\n4. Comparison of Theorem 5.1 to vanilla group DRO would be more informative.\n\n5. Same overall concern: What is the applicability of theorem 5.4 to the heuristic assumption of adversary being neural network with regularization added in the objective function.\n\n6. Empirical evaluation could also use more insights:\n   6.1 Are celebA results worse compared to other datasets for domain-specific reasons? If yes, why is the worst-group test accuracy lower for Bit-rate DRO always?\n   6.2 If the benefit of BR-DRO is more visible only with label noise, how practically useful is BR-DRO?\n   6.3 What are the groups used by group DRO vs BR-DRO and how is that affected by the choice of regularization, beyond the fact that higher regularization will constrain the function to be less complex.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity:The clarity of the paper can be improved relatively easily, in my opinion, so I will wait for the rebuttal/response of authors.\n\nQuality:The paper is high quality\n\nReproducibility: I have not thoroughly checked the code, but authors have provided detailed code to reproduce all results, included datasets wherever necessary.\n\n# Summary Of The Review\n\nOverall, I am a little unsure about some modeling choices and practical utility of the paper. But I will look forward to the response and update my review accordingly.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROTEIN STRUCTURE GENERATION VIA FOLDING DIFFUSION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe ability to computationally generate novel yet physically foldable protein structures could lead to new biological discoveries and new treatments targeting yet incurable diseases. Despite recent advances in protein structure prediction, directly generating diverse, novel protein structures from neural networks remains difficult. In this work, we present a new diffusion-based generative model that designs protein backbone structures via a procedure that mirrors the native folding process. We describe protein backbone structure as a series of consecutive angles capturing the relative orientation of the constituent amino acid residues, and generate new structures by denoising from a random, unfolded state towards a stable folded structure. Not only does this mirror how proteins biologically twist into energetically favorable conformations, the inherent shift and rotational invariance of this representation crucially alleviates the need for complex equivariant networks. We train a denoising diffusion probabilistic model with a simple transformer backbone and demonstrate that our resulting model unconditionally generates highly realistic protein structures with complexity and structural patterns akin to those of naturally-occurring proteins. As a useful resource, we release the first open-source codebase and trained models for protein structure diffusion.\n\n1\n\nINTRODUCTION\n\nProteins are critical for life, playing a role in almost every biological process, from relaying signals across neurons (Zhou et al., 2017) to recognizing microscopic invaders and subsequently activating the immune response (Mariuzza et al., 1987), from producing energy for cells (Bonora et al., 2012) to transporting molecules along cellular highways (Dominguez & Holmes, 2011). Misbehaving proteins, on the other hand, cause some of the most challenging ailments in human healthcare, including Alzheimer’s disease, Parkinson’s disease, Huntington’s disease, and cystic fibrosis (Chaudhuri & Paul, 2006). Due to their ability to perform complex functions with high specificity, proteins have been extensively studied as a therapeutic medium (Leader et al., 2008; Kamionka, 2011; Dimitrov, 2012) and constitute a rapidly growing segment of approved therapies (H Tobin et al., 2014). Thus, the ability to computationally generate novel yet physically foldable protein structures could open the door to discovering novel ways to harness cellular pathways and eventually lead to new treatments targeting yet incurable diseases.\n\nMany works have tackled the problem of computationally generating new protein structures, but have generally run into challenges with creating diverse yet realistic folds. Traditional approaches typically apply heuristics to assemble fragments of experimentally profiled proteins into structures (Schenkelberg & Bystroff, 2016; Holm & Sander, 1991). This approach is limited by the boundaries of expert knowledge and available data. More recently, deep generative models have been proposed. However, due to the incredibly complex structure of proteins, these commonly do not directly generate protein structures, but rather constraints (such as pairwise distance between residues) that are heavily post-processed to obtain structures (Anand et al., 2019; Lee & Kim, 2022). Not only does this add complexity to the design pipeline, but noise in these predicted constraints can also be compounded during post-processing, resulting in unrealistic structures – that is, if the constraints are at all satisfiable to begin with. Other generative models rely on complex equivariant network architectures or loss functions to learn to generate a 3D point cloud that describes a protein structure (Anand & Achim, 2022; Trippe et al., 2022; Luo et al., 2022; Eguchi et al., 2022). Such equivariant architectures can ensure that the probability density from which the protein structures are sampled is\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ninvariant under translation and rotation. However, translation- and rotation-equivariant architectures are often also symmetric under reflection, leading to violations of fundamental structural properties of proteins like chirality (Trippe et al., 2022). Intuitively, this point cloud formulation is also quite detached from how proteins biologically fold – by twisting to adopt energetically favorable configurations ( ˇSali et al., 1994; Englander et al., 2007).\n\nInspired by the in vivo protein folding process, we introduce a generative model that acts on the inter-residue angles in protein backbones instead of on Cartesian atom coordinates (Figure 1). This treats each residue as an independent reference frame, thus shifting the equivariance requirements from the neural network to the coordinate system itself. A similar angular representation has been used in some protein structure prediction works (Gao et al., 2017; AlQuraishi, 2019; Chowdhury et al., 2022). For generation, we use a denoising diffusion probabilistic model (diffusion model, for brevity) (Ho et al., 2020; Sohl-Dickstein et al., 2015) with a vanilla transformer parameterization without any equivariance constraints. Diffusion models train a neural network to start from noise and iteratively “denoise” it to generate data samples. Such models have been highly successful in a wide range of data modalities from images (Saharia et al., 2022; Rombach et al., 2022) to audio (Rouard & Hadjeres, 2021; Kong et al., 2021), and are easier to train with better modal coverage than methods like generative adversarial networks (GANs) (Dhariwal & Nichol, 2021; Nichol & Dhariwal, 2021). We present a suite of validations to quantitatively demonstrate that unconditional sampling from our model directly generates realistic protein backbones – from recapitulating the natural distribution of protein inter-residue angles, to producing overall structures with appropriate arrangements of multiple structural building block motifs. We show that our generated backbones are diverse and designable, and are thus biologically plausible protein structures. Our work demonstrates the power of biologically-inspired problem formulations and represents an important step towards accelerating the development of new proteins and protein-based therapies.\n\n2 RELATED WORK\n\n2.1 GENERATING NEW PROTEIN STRUCTURES\n\nMany generative deep learning architectures have been applied to the task of generating novel protein structures. Anand et al. (2019) train a GAN to sample pairwise distance matrices that describe protein backbone arrangements. However, these pairwise distance matrices must be corrected, refined, and converted into realizable backbones via two independent post-processing steps, the Alternating Direction Method of Multipliers (ADMM) and Rosetta. Crucially, inconsistencies in these predicted constraints can render them unsatisfiable or lead to significant errors when reconstructing the final protein structure. Sabban & Markovsky (2020) use a long short-term memory (LSTM) GAN to generate (φ, ψ) dihedral angles. However, their network only generates α helices and relies on downstream post-processing to filter, refine, and fold structures, partly due to the fact that these two dihedrals do not sufficiently specify backbone structure. Eguchi et al. (2022) propose a variational auto-encoder with equivariant losses to generate protein backbones in 3D space. However, their work only targets immunoglobulin proteins and also requires refinement through Rosetta. Nondeep learning methods have also been explored: Schenkelberg & Bystroff (2016) apply heuristics to ensembles of similar sequences to perturb known protein structures, while Holm & Sander (1991) use a database search to find and assemble existing protein fragments that might fit a new scaffold structure. These approaches’ reliance on known proteins and hand-engineered heuristics limit them to relatively small deviations from naturally-occurring proteins.\n\n2.1.1 DIFFUSION MODELS FOR PROTEIN STRUCTURE GENERATION\n\nSeveral recent works have proposed extending diffusion models towards generating protein structures. These predominantly perform diffusion on the 3D Cartesian coordinates of the residues themselves. For example, Trippe et al. (2022) use an E(3)-equivariant graph neural network to model the coordinates of protein residues. Anand & Achim (2022) adopt a hybrid approach where they train an equivariant transformer with invariant point attention (Jumper et al., 2021); this model generates the 3D coordinates of Cα atoms, the amino acid sequence, and the angles defining the orientation of side chains. Another recent work by Luo et al. (2022) performs diffusion for generating antibody fragments’ structure and sequence by modeling 3D coordinates using an equivariant neural network. Note that these prior works all use some form of equivariance to translation, rotation,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: We perform diffusion on six angles as illustrated in the schematic in the bottom center (also defined in Table 1). Three of these are dihedral torsion angles (orange), and three are bond angles (green). We start with an experimentally observed backbone described by angles x0 and iteratively add Gaussian noise via the forward noising process q until the angles are indistinguishable from a wrapped Gaussian at xT . We use these examples to learn the “reverse” denoising process pξ.\n\nand/or reflection due to their formulation of diffusion on Cartesian coordinates. Another method, ProteinSGM (Lee & Kim, 2022), implements a score-based diffusion model (Song et al., 2020) that generates image-like square matrices describing pairwise angles and distances between all residues in an amino acid chain. However, this set of values is highly over-constrained, and must be used as a set of input constraints for Rosetta’s folding algorithm (Yang et al., 2020), which in turn produces the final folded output. This is a similar approach to Anand et al. (2019), and is likewise subject to the aforementioned concerns regarding complexity, satisfiability, and cleanliness of predicted constraints. Our work instead uses a minimal set of angles required to specify a protein backbone, and thus directly generates structures without relying on additional methods for refinement. Unfortunately, none of these prior works have publicly-available code, model weights, or generated examples at the time of this writing. Thus, our ability to perform direct comparisons is limited.\n\n2.2 DIFFUSION MODELS FOR SMALL MOLECULES\n\nA related line of work focuses on creating and modeling small molecules, typically in the context of drug design, using similar generative approaches. These small molecules average 44 atoms in size (Jing et al., 2022). Compared to proteins, which average several hundred residues and thousands of atoms (Tiessen et al., 2012), the relatively small size of small molecules makes them easier to model. The E(3) Equivariant Diffusion Model (Hoogeboom et al., 2022) uses an equivariant transformer to design small molecules by diffusing their coordinates in Euclidean space. Other works have explored torsional diffusion, i.e., modelling the angles that specify a small molecule, to sample from the space of energetically favorable molecular conformations (Jing et al., 2022). This work still requires an SE(3)-equivariant model as the input to their model is a 3D point cloud. In contrast, our problem formulation allows us to work entirely in terms of relative angles.\n\n3 METHOD\n\n3.1 SIMPLIFIED FRAMING OF PROTEIN BACKBONES USING INTERNAL ANGLES\n\nProteins are variable-length chains of amino acid residues. There are 20 canonical amino acids, all of which share the same three-atom N − Cα − C backbone, but have varying side chains attached to the Cα atom (typically denoted Ri, see illustration in Figure 1). These residues assemble to form polymer chains typically hundreds of residues long (Tiessen et al., 2012). These chains of amino acids fold into 3D structures, taking on a shape that largely determines the protein’s functions. These folded structures can be described on four levels: primary structure, which simply captures the linear sequence of amino acids; secondary structure, which describes the local arrangement of amino acids and includes structural motifs like α-helices and β-sheets; tertiary structure, which describes the full\n\n3\n\n. . .. . .NCαCOR1N(cid:31)(cid:30)(cid:29)Cα(cid:28)(cid:27)(cid:28)(cid:26)(cid:28)(cid:25)...R2Corresponding structureCorresponding structureUnder review as a conference paper at ICLR 2023\n\nTable 1: Internal angles used to specify protein backbone structure. Some of these involve multiple residues, indicated via i index subscripts. These are illustrated in Figure 1.\n\nAngle Description\n\nψ ω\nφ θ1 θ2 θ3\n\nDihedral torsion about Ni − Cαi − Ci − Ni+1 Dihedral torsion about Cαi − Ci − Ni+1 − Cαi+1 Dihedral torsion about Ci − Ni+1 − Cαi+1 − Ci+1 Bond angle about Ni − Cαi − Ci Bond angle about Cαi − Ci − Ni+1 Bond angle about Ci − Ni+1 − Cαi+1\n\nspatial arrangement of all residues; and quaternary structure, which describes how multiple different amino acid chains come together to form larger complexes (Sun et al., 2004).\n\nWe propose a simplified framing of protein backbones that follows the biological intuition of protein folding while removing the need for complex equivariant networks. Rather than viewing a protein backbone of length N amino acids as a cloud of 3D coordinates (i.e., x ∈ RN ×3 if modeling only Cα atoms, or x ∈ R3N ×3 for a full backbone) as prior works have done, we view it as a sequence of six internal, consecutive angles x ∈ [−π, π)(N −1)×6. That is, each vector of six angles describes the relative position of all backbone atoms in the next residue given the position of the current residue. These six angles are defined precisely in Table 1 and illustrated in Figure 1. These internal angles can be easily computed using trigonometry, and converted back to 3D Cartesian coordinates by iteratively adding atoms to the protein backbone as described in Parsons et al. (2005), fixing bond distances to average lengths (Appendix A.1, Figure S1). Despite building the structure from a fixed point, this formulation does not appear to overly accumulate errors (Appendix A.2, Figures S2, S3).\n\nThis internal angle formulation has several key advantages. Most importantly, since each residue forms its own independent reference frame, there is no need to use an equivariant neural network. No matter how the protein is rotated or shifted, the angles specifying the next residue given the current residue never changes. This allows us to use a simple transformer as the backbone architecture; in fact, we demonstrate that our model fails when substituting our shift- and rotation-invariant internal angle representation with Cartesian coordinates, keeping all other design choices identical (Appendix C.1, Figure S4). This internal angle formulation also closely mimics how proteins actually fold by twisting into more energetically stable conformations.\n\n3.2 DENOISING DIFFUSION PROBABILISTIC MODELS\n\nDenoising diffusion probabilistic models (or diffusion models, for short) leverage a Markov process q(xt | xt−1) to corrupt a data sample x0 over T discrete timesteps until it is indistinguishable from noise at xT . A diffusion model pξ(xt−1 | xt) parameterized by ξ is trained to reverse this forward noising process, “denoising” pure noise towards samples that appear drawn from the native data distribution (Sohl-Dickstein et al., 2015). Diffusion models were first shown to achieve good generative performance by Ho et al. (2020); we adapt this framework for generating protein backbones, introducing necessary modifications to work with periodic angular values.\n\nWe modify the standard Markov forward noising process that adds noise at each discrete timestep t to sample from a wrapped normal instead of a standard normal (Jing et al., 2022):\n\nq(xt | xt−1) = Nwrapped(xt; (cid:112)1 − βtxt−1, βtI) ∝\n\n∞ (cid:88)\n\nexp\n\nk=−∞\n\n(cid:18) −∥xt −\n\n√\n\n1 − βtxt−1 + 2πk∥2\n\n(cid:19)\n\n2β2 t\n\nwhere βt ∈ (0, 1)T Dhariwal, 2021) with T = 1000 timesteps:\n\nt=1 are set by a variance schedule. We use the cosine variance schedule (Nichol &\n\n(cid:18)\n\nβt = clip\n\n1 −\n\n ̄αt ̄αt−1\n\n(cid:19)\n\n, 0.999\n\n ̄αt =\n\nf (t) f (0)\n\nf (t) = cos\n\n(cid:18) t/T + s 1 + s\n\n·\n\nπ 2\n\n(cid:19)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nwhere s = 8 × 10−3 is a small constant for numerical stability. We train our model for pξ(xt−1|xt) with the simplified loss proposed by Ho et al. (2020), using a neural network nnξ(xt, t) that predicts the noise ε ∼ N (0, I) present at a given timestep (rather than the denoised mean values themselves). To handle the periodic nature of angular values, we introduce a function to “wrap” values within the range [−π, π): w(x) = ((x + π) mod 2π) − π. We use w to wrap a smooth L1 loss (Girshick, 2015) Lw, which behaves like L1 loss when error is high, and like an L2 loss when error is low; we set the transition between these two regimes at βL = 0.1π. While this loss is not as well-motivated as torsional losses proposed by Jing et al. (2022), we find that it achieves strong empirical results.\n\n(cid:0)w (cid:0)√\n\n ̄αtx0 +\n\n√\n\n1 − ̄αtε(cid:1) , t(cid:1)(cid:1)\n\ndw = w (cid:0)ε − nnξ (cid:40)\n\nLw =\n\nw βL\n\n0.5 d2 |dw| − 0.5βL otherwise\n\nif |dw| < βL\n\nDuring training, timesteps are sampled uniformly t ∼ U (0, T ). We normalize all angles in the training set to be zero mean by subtracting their element-wise angular mean μ; validation and test sets are shifted by this same offset.\n\nFigure 1 illustrates this overall training process, including our previously described internal angle framing. The internal angles describing the folded chain x0 are corrupted until they become indistinguishable from random angles, which results in a disordered mass of residues at xT ; we sample points along this diffusion process to train our model nnξ. Once trained, the reverse process of sampling from pξ also requires modifications to account for the periodic nature of angles, as described\n\nin Algorithm 1. The variance of this reverse process is given by σt =\n\nAlgorithm 1 Sampling from pξ with FoldingDiff\n\n(cid:113) 1− ̄αt−1 1− ̄αt\n\n· βt.\n\n1: xT ∼ w (N (0, I)) 2: for t = T, . . . , 1 do 3:\n\n4:\n\nxt−1 = w\n\nz = N (0, I) if t > 1 else z = 0\n\n(cid:16)\n\n(cid:16) 1√\n\nαt\n\nxt − 1−αt√ 1− ̄αt\n\nnnξ(xt, t)\n\n▷ Sample from a wrapped Gaussian\n\n(cid:17)\n\n+ σtz\n\n▷ Wrap sampled values about [−π, π)\n\n(cid:17)\n\n5: end for 6: return w(x0 + μ)\n\n▷ Un-shift generated values by original mean shift\n\nThis sampling process can be intuitively described as refining internal angles from an unfolded state towards a folded state. As this is akin to how proteins fold in vivo, we name our method FoldingDiff.\n\n3.3 MODELING AND DATASET\n\nFor our reverse (denoising) model pξ(xt, t), we adopt a vanilla bidirectional transformer architecture (Vaswani et al., 2017) with relative positional embeddings (Shaw et al., 2018). Our six-dimensional input is linearly upscaled to the model’s embedding dimension (d = 384). To incorporate the timestep t, we generate random Fourier feature embeddings (Tancik et al., 2020) as done in Song et al. (2020) and add these embeddings to each upscaled input. To convert the transformer’s final per-position representations to our six outputs, we apply a regression head consisting of a densely connected layer, followed by GELU activation (Hendrycks & Gimpel, 2016), layer normalization, and finally a fully connected layer outputting our six values. We train this network with the AdamW optimizer (Loshchilov & Hutter, 2019) over 10,000 epochs, with a learning rate that linearly scales from 0 to 5 × 10−5 over 1,000 epochs, and back to 0 over the final 9,000 epochs. Validation loss appears to plateau after ≈ 1,400 epochs; additional training does not improve validation loss, but appears to lead to a poorer diversity of generated structures. We thus take a model checkpoint at 1,488 epochs for all subsequent analyses.\n\nWe train our model on the CATH dataset, which provides a “de-duplicated” set of protein structural folds spanning a wide range of functions where no two chains share more than 40% sequence identity over 60% overlap (Sillitoe et al., 2015). We exclude any chains with fewer than 40 residues. Chains longer than 128 residues are randomly cropped to a 128-residue window at each epoch. A random\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n80/10/10 training/validation/test split yields 24,316 training backbones, 3,039 validation backbones, and 3,040 test backbones.\n\n4 EXPERIMENTS\n\n4.1 GENERATING PROTEIN INTERNAL ANGLES\n\nAfter training our model, we check that it is able to recapitulate the correct marginal distributions of dihedral and bond angles in proteins. We unconditionally generate 10 backbone chains each for every length l ∈ [50, 128), generating a total of 780 backbones as was done in Trippe et al. (2022). We plot the distributions of all six angles, aggregated across these 780 structures, and compare each distribution to that of experimental test set structures less than 128 residues in length (Figures 2, S9). We observe that, across all angles, the generated distribution almost exactly recapitulates the test distribution. This is true both for angles that are nearly Gaussian with low variance (ω, θ1, θ2, θ3) as well as for angles with highly complex, high-variance distributions (φ, ψ). Angles that wrap about the −π/π boundary (ω) are correctly handled as well. Compared to similar plots generated from other protein diffusion methods (e.g., Figure 1 in Anand & Achim (2022), reproduced with permission in Figure S10), we qualitatively observe that our method produces a much tighter distribution that more closely matches the natural distribution of bond angles.\n\nFigure 2: Comparison of the distributions of angular values in held-out test set and in generated samples. Top row shows dihedral angles (torsional angles involving 4 atoms), and bottom row shows bond angles (involving 3 atoms). KL divergence is calculated between DKL(sampled||test). Figure S9 shows the cumulative distribution function (CDF) corresponding to these histograms.\n\nHowever, looking at individual distributions of angles alone does not capture the fact that these angles are not independently distributed, but rather exhibit significant correlations. A Ramachandran plot, which shows the frequency of co-occurrence between the dihedrals (φ, ψ), is commonly used to illustrate these correlations between angles (Ramachandran & Sasisekharan, 1968). Figure 3 shows the Ramachandran plot for (experimental) test set chains with fewer than 128 residues, as well as that for our 780 generated structures. The Ramachandran plot for natural structures (Figure 3a) contains three major concentrated regions corresponding to right-handed α helices, left-handed α helices, and β sheets. All three of these regions are recapitulated in our generated structures (Figure 3b). In other words, FoldingDiff is able to generate all three major secondary structure elements in protein backbones. Furthermore, we see that our model correctly learns that right-handed α helices are much more common than left-handed α helices (Cintas, 2002). Prior works that use equivariant networks, such as Trippe et al. (2022), cannot differentiate between these two types of helices due to network equivariance to reflection. This concretely demonstrates that our internal angle formulation leads to improved handling of chirality (i.e., the asymmetric nature of proteins) in generated backbones.\n\n6\n\n32101230.00.51.01.52.0Normalized frequency distribution, KL=0.0412TestSampled32101230.00.20.40.60.81.01.2 distribution, KL=0.04803210123012345 distribution, KL=0.03180.00.51.01.52.02.5Angle (rad)0246810Normalized frequency1 distribution, KL=0.02590.00.51.01.52.02.53.0Angle (rad)024681012142 distribution, KL=0.06300.00.51.01.52.02.53.0Angle (rad)024681012143 distribution, KL=0.0312Under review as a conference paper at ICLR 2023\n\n(a) Ramachandran plot, test set\n\n(b) Ramachandran plot, generated backbones\n\nFigure 3: Ramachandran plots comparing the (φ, ψ) dihedral angles for test set (3a) and generated protein backbones (3b). Each major region of this plot indicates a different secondary structure element, as indicated in panel 3a. All three main structural elements are recapitulated in our generated backbones, along with some less common angle combinations. Lines are artifacts of null values, and appear shifted due to zero centering/uncentering.\n\n4.2 ANALYZING GENERATED STRUCTURES\n\nWe have shown that our model generates realistic distributions of angles and that our generated joint distributions capture secondary structure elements. We now demonstrate that the overall structures specified by these angles are biologically reasonable. Recall that natural protein structures contain multiple secondary structure elements. We use P-SEA (Labesse et al., 1997) to count the number of secondary structure elements in each test-set backbone of fewer than 128 residues, and generate a 2D histogram describing the frequency of α/β co-occurrence counts in Figure 4a. Figure 4b repeats this analysis for our generated structures, which frequently contain multiple secondary structure elements just as naturally-occurring proteins do. FoldingDiff thus appears to generate rich structural information, with consistent performance across multiple generation replicates (Figure S11). This is a nontrivial task – an autoregressive transformer, for example, collapses to a failure mode of endlessly generating α helices (Appendix C.2, Figures S5, S6).\n\n(a) Secondary structure co-occurrence, test\n\n(b) Secondary structure co-occurrence, generated\n\nFigure 4: 2D histograms describing co-occurrence of secondary structures in test backbones (4a) and generated backbones (4b). Axes indicate the number of secondary structure present in a chain; color indicates the frequency of a specific combination of secondary structure elements. Our generated structures mirror real structures with multiple α helices, multiple β sheets, and a mixture of both. See Figure S11 for additional generation replicates.\n\n7\n\n3210123 (radians)3210123 (radians) helix, LH helix, RH sheet3210123 (radians)3210123 (radians)0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09Frequency0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09FrequencyUnder review as a conference paper at ICLR 2023\n\nBeyond demonstrating that FoldingDiff’s generated backbones contain reasonable structural motifs, it is also important to show that they are designable – meaning that we can find a sequence of amino acids that can fold into our designed backbone structure. After all, a novel protein structure is not useful if we cannot physically realize it. Previous works evaluate this in silico by predicting possible amino acids that fold into a generated backbone and checking whether the predicted structure for these sequences matches the original backbone (Trippe et al. 2022, Lee & Kim 2022, Appendix B). Following this general procedure, for a generated structure s, we use the ProteinMPNN inverse folding model (Dauparas et al., 2022) to generate 8 different amino acid sequences, as it yields improved performance compared to ESM-IF1 (Hsu et al. 2022, Appendix C.3, Tables S1, S2). We then use OmegaFold (Wu et al., 2022) to predict the 3D structures ˆs1, . . . , ˆs8 corresponding to each of these sequences. We use TMalign (Zhang & Skolnick, 2005), which evaluates structural similarity between backbones, to score each of these 8 structures against the original structure s. The maximum score maxi∈[1,8] TMalign(s, ˆsi) is the self-consistency TM (scTM) score. A scTM score of ≥ 0.5 is considered to be in the same fold, and thus is “designable.”\n\n(a) Backbone designability by length\n\n(b) Designability compared to training set similarity\n\nFigure 5: Of our 780 generated backbones, ranging in length from 50-128 residues, 177 are designable (scTM ≥ 0.5) using ProteinMPNN and OmegaFold. Shorter structures of 70 amino acids or fewer tend to have higher scTM scores than longer structures (5a). Generated backbones that are more similar to training examples (greater maximum training TM score) tend to have better designability (5b). The three structures indicated by arrows are illustrated in Figure S12.\n\nWith this procedure, we find that 177 of our 780 structures, or 22.7%, are designable with an scTM score ≥ 0.5 (Figure 5a) without any refinement or relaxation. This designability is highly consistent across different generation runs (Table S1), and is also consistent when substituting AlphaFold2 without MSAs (Jumper et al., 2021) in place of OmegaFold (163/780 designable with AlphaFold2). Trippe et al. (2022) use an identical scTM pipeline using ProteinMPNN and AlphaFold2, and report a significantly lower proportion of designable structures (92/780 designable, p ≪ 10−5, Chi-square test). Compared to this prior work, FoldingDiff improves upon designability of both short sequences (up to 70 residues, 76/210 designable compared to 36/210, p = 1 × 10−5, Chi-square test) and long sequences (beyond 70 residues, 87/570 designable compared to 56/570, p = 5.6 × 10−3, Chi-square test). While ProteinSGM (ESM-IF1 for inverse folding, AlphaFold2 for fold prediction) reports an even higher designability proportion of 50.5%, this value is not directly comparable, as ProteinSGM generates constraints that are subsequently folded using Rosetta; therefore, their designability does not directly reflect their generative process. The authors themselves note that Rosetta “post-processing” significantly improves the viability of their structures. To further contextualize our scTM scores, we evaluate a naive method that samples from the empirical distribution of dihedral angles. This baseline produces no designable structures whatsoever (Appendix C.3, Figures S7, S8). Conversely, we evaluate experimental structures to establish an upper bound for designability; 87% of natural structures have an scTM ≥ 0.5 (Appendix C.3, Figure S7).\n\nWe additionally evaluate the similarity of each generated backbone to any training backbone by taking the maximum TM score across the entire training set. The maximum training TM-score is significantly correlated with scTM score (Spearman’s r = 0.78, p = 7.9 × 10−165, Figure 5b), indi-\n\n8\n\n0.20.30.40.50.60.70.80.9Self-consistency TM score (scTM)020406080100120CountscTM scores, 780 generated protein backboneslengthshort (70 aa)long (>70 aa)Poor designability, low training similarityMedium designability& similarity to trainingHighly designable,high training similarityUnder review as a conference paper at ICLR 2023\n\ncating that structures more similar to the training set tend to be more designable. However, this does not suggest that we are merely memorizing the training set; doing so would result in a distribution of training TM scores near 1.0, which is not what we observe. ProteinSGM reports a distribution of training set TMscores much closer to 1.0; this suggests a greater degree of memorization and may indicate that their high designability ratio is partially driven by memorization.\n\nSelected examples of our generated backbones and corresponding OmegaFold predictions of various lengths are visualized using PyMOL (Schr ̈odinger, LLC, 2015) in Figure 6. Interestingly, we find that of our 177 designable backbones, only 16 contain β sheets as annotated by P-SEA. Conversely, of our 603 backbones with scTM < 0.5, 533 contain β sheets. This suggests that generated structures with β sheets may be less designable (p ≪ 1.0 × 10−5, Chi-square test). We additionally cluster our designable backbones and observe a large diversity of structures (Figure S14) comparable to that of natural structures (Figure S15). This suggests that our model is not simply generating small variants on a handful of core structures, which prior works appear to do (Figure S16).\n\nFigure 6: Selected generated protein backbones of varying length that are approximately designable (scTM ≈ 0.5). Top row shows our directly generated backbones; bottom row shows OmegaFold predicted structure for residues inferred by ProteinMPNN to produce our generated backbone. Structures contain both α helices (coils, columns 1-4) and β sheets (ribbons, columns 1 and 2), and each appears meaningfully different from its most similar training example (Figure S13).\n\n5 CONCLUSION\n\nIn this work, we present a novel parameterization of protein backbone structures that allows for simplified generative modeling. By considering each residue to be its own reference frame, we describe a protein using the resulting relative internal angle representation. We show that a vanilla transformer can then be used to build a diffusion model that generates high-quality, biologically plausible, diverse protein structures. These generated backbones respect protein chirality and exhibit high designability.\n\nWhile we demonstrate promising results with our model, there are several limitations to our work. Though formulating a protein as a series of angles enables use of simpler models without equivariance mechanisms, this framing allows for errors early in the chain to significantly alter the overall generated structure – a sort of “lever arm effect.” Additionally, some generated structures exhibit collisions where the generated structure crosses through itself. Future work could explore methods to avoid these pitfalls using geometrically-informed architectures such as those used in Wu et al. (2022). Our generated structures are still of relatively short lengths compared to natural proteins which typically have several hundred residues; future work could extend towards longer structures, potentially incorporating additional losses or inputs that help “checkpoint” the structure and reduce accumulation of error. We also do not handle multi-chain complexes or ligand interactions, and are only able to generate static structures that do not capture the dynamic nature of proteins. Future work could incorporate amino acid sequence generation in parallel with structure generation, along with guided generation using functional or domain annotations. In summary, our work provides an important step in using biologically-inspired problem formulations for generative protein design.\n\n9\n\n59 residues, scTM = 0.5961 residues, scTM = 0.51107 residues, scTM = 0.53111 residues, scTM = 0.55Generated (ours)OmegaFoldUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMohammed AlQuraishi. End-to-end differentiable learning of protein structure. Cell systems, 8(4):\n\n292–301, 2019.\n\nNamrata Anand and Tudor Achim. Protein structure and sequence generation with equivariant de-\n\nnoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019, 2022.\n\nNamrata Anand, Raphael Eguchi, and Po-Ssu Huang. Fully differentiable full-atom protein back-\n\nbone generation. In DGS@ICLR, 2019.\n\nMassimo Bonora, Simone Patergnani, Alessandro Rimessi, Elena De Marchi, Jan M Suski, Angela Bononi, Carlotta Giorgi, Saverio Marchi, Sonia Missiroli, Federica Poletti, et al. ATP synthesis and storage. Purinergic Signalling, 8(3):343–357, 2012.\n\nTapan K Chaudhuri and Subhankar Paul. Protein-misfolding diseases and chaperone-based thera-\n\npeutic approaches. The FEBS Journal, 273(7):1331–1349, 2006.\n\nRatul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant Kharkar, Koushik Roy, Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang, George M Church, et al. Singlesequence protein structure prediction using a language model and deep learning. Nature Biotechnology, pp. 1–7, 2022.\n\nPedro Cintas. Chirality of living systems: a helping hand from crystals and oligopeptides. Ange-\n\nwandte Chemie International Edition, 41(7):1139–1145, 2002.\n\nJustas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning– based protein sequence design using proteinmpnn. Science, 378(6615):49–56, 2022.\n\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Ad-\n\nvances in Neural Information Processing Systems, 34:8780–8794, 2021.\n\nDimiter S Dimitrov. Therapeutic proteins. Therapeutic Proteins, pp. 1–26, 2012.\n\nRoberto Dominguez and Kenneth C Holmes. Actin structure and function. Annual Review of Bio-\n\nphysics, 40:169, 2011.\n\nRaphael R Eguchi, Christian A Choe, and Po-Ssu Huang. Ig-vae: Generative modeling of protein structure by direct 3d coordinate generation. PLoS computational biology, 18(6):e1010271, 2022.\n\nS Walter Englander, Leland Mayne, and Mallela MG Krishna. Protein folding and misfolding:\n\nmechanism and principles. Quarterly Reviews of Biophysics, 40(4):1–41, 2007.\n\nYujuan Gao, Sheng Wang, Minghua Deng, and Jinbo Xu. Real-value and confidence prediction of protein backbone dihedral angles through a hybrid method of clustering and deep learning. arXiv preprint arXiv:1712.07244, 2017.\n\nRoss Girshick. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer\n\nVision, pp. 1440–1448, 2015.\n\nPeter H Tobin, David H Richards, Randolph A Callender, and Corey J Wilson. Protein engineering:\n\na new frontier for biological therapeutics. Current Drug Metabolism, 15(7):743–756, 2014.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs).\n\narXiv preprint\n\narXiv:1606.08415, 2016.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nLiisa Holm and Chris Sander. Database algorithm for generating protein backbone and side-chain co-ordinates from a Cα trace: application to model building and detection of co-ordinate errors. Journal of Molecular Biology, 218(1):183–194, 1991.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEmiel Hoogeboom, Vıctor Garcia Satorras, Cl ́ement Vignac, and Max Welling. Equivariant difIn International Conference on Machine Learning, pp.\n\nfusion for molecule generation in 3D. 8867–8887. PMLR, 2022.\n\nChloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In International Conference on Machine Learning, pp. 8946–8970. PMLR, 2022.\n\nBowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffu-\n\nsion for molecular conformer generation. arXiv preprint arXiv:2206.01729, 2022.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021.\n\nMariusz Kamionka. Engineering of therapeutic proteins production in Escherichia coli. Current\n\nPharmaceutical Biotechnology, 12(2):268–274, 2011.\n\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.\n\nGilles Labesse, N Colloc’h, Jo ̈el Pothier, and J-P Mornon. P-SEA: a new efficient assignment of\n\nsecondary structure from Cα trace of proteins. Bioinformatics, 13(3):291–295, 1997.\n\nBenjamin Leader, Quentin J Baca, and David E Golan. Protein therapeutics: a summary and phar-\n\nmacological classification. Nature reviews Drug discovery, 7(1):21–39, 2008.\n\nJin Sub Lee and Philip M. Kim. ProteinSGM: Score-based generative modeling for de novo protein\n\ndesign. bioRxiv, 2022. doi: 10.1101/2022.07.13.499967.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\n\nence on Learning Representations, 2019.\n\nShitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. bioRxiv, 2022. doi: 10.1101/2022.07.10.499510.\n\nRA Mariuzza, SEV Phillips, and RJ Poljak. The structural basis of antigen-antibody recognition.\n\nAnnual Review of Biophysics and Biophysical Chemistry, 16(1):139–159, 1987.\n\nMilot Mirdita, Konstantin Sch ̈utze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. Colabfold: making protein folding accessible to all. Nature Methods, pp. 1–4, 2022.\n\nAlexander Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In\n\nInternational Conference on Machine Learning, pp. 8162–8171. PMLR, 2021.\n\nJerod Parsons, J Bradley Holmes, J Maurice Rojas, Jerry Tsai, and Charlie EM Strauss. Practical conversion from torsion space to cartesian space for in silico protein synthesis. Journal of Computational Chemistry, 26(10):1063–1068, 2005.\n\nZhao Qin, Andrea Fabre, and Markus J Buehler. Structure and mechanism of maximum stability of isolated alpha-helical protein domains at a critical length scale. The European Physical Journal E, 36(5):1–12, 2013.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nGN Ramachandran and V Sasisekharan. Conformation of polypeptides and proteins. Advances in\n\nProtein Chemistry, 23:283–437, 1968.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSimon Rouard and Ga ̈etan Hadjeres. CRASH: Raw audio score-based generative modeling for\n\ncontrollable high-resolution drum sound synthesis. arXiv preprint arXiv:2106.07431, 2021.\n\nSari Sabban and Mikhail Markovsky. RamaNet: Computational de novo helical protein backbone design using a long short-term memory generative neural network. bioRxiv, pp. 671552, 2020.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nAndrej ˇSali, Eugene Shakhnovich, and Martin Karplus. How does a protein fold. Nature, 369(6477):\n\n248–251, 1994.\n\nChristian D Schenkelberg and Christopher Bystroff. Protein backbone ensemble generation explores the local structural space of unseen natural homologs. Bioinformatics, 32(10):1454–1461, 2016.\n\nSchr ̈odinger, LLC. The PyMOL molecular graphics system, version 1.8. November 2015.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\n\ntions. arXiv preprint arXiv:1803.02155, 2018.\n\nIan Sillitoe, Tony E Lewis, Alison Cuff, Sayoni Das, Paul Ashford, Natalie L Dawson, Nicholas Furnham, Roman A Laskowski, David Lee, Jonathan G Lees, et al. CATH: comprehensive structural and functional annotations for genome sequences. Nucleic Acids Research, 43(D1): D376–D381, 2015.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\n\nPeter D Sun, Christine E Foster, and Jeffrey C Boyington. Overview of protein structural and\n\nfunctional folds. Current Protocols in Protein Science, 35(1):17–1, 2004.\n\nMatthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33:7537–7547, 2020.\n\nMartha M Teeter. Water structure of a hydrophobic protein at atomic resolution: Pentagon rings of water molecules in crystals of crambin. Proceedings of the National Academy of Sciences, 81 (19):6014–6018, 1984.\n\nAxel Tiessen, Paulino P ́erez-Rodr ́ıguez, and Luis Jos ́e Delaye-Arredondo. Mathematical modeling and comparison of protein size distribution in different plant, animal, fungal and microbial species reveals a negative correlation between protein size and protein number, thus providing insight into the evolution of proteomes. BMC Research Notes, 5(1):1–23, 2012.\n\nBrian L Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3D for the motifscaffolding problem. arXiv preprint arXiv:2206.04119, 2022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.\n\nRuidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, and Jian Peng. High-resolution de novo structure prediction from primary sequence. bioRxiv, 2022. doi: 10.1101/2022.07.21.500999.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nJianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David Baker. Improved protein structure prediction using predicted interresidue orientations. Proceedings of the National Academy of Sciences, 117(3):1496–1503, 2020.\n\nYang Zhang and Jeffrey Skolnick. TM-align: a protein structure alignment algorithm based on the\n\nTM-score. Nucleic Acids Research, 33(7):2302–2309, 2005.\n\nQiangjun Zhou, Peng Zhou, Austin L Wang, Dick Wu, Minglei Zhao, Thomas C S ̈udhof, and Axel T Brunger. The primed snare–complexin–synaptotagmin complex for neuronal exocytosis. Nature, 548(7668):420–425, 2017.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA INTERNAL ANGLE FORMULATION OF PROTEIN BACKBONES\n\nA.1 CHOICE OF ANGLES FOR REPRESENTATION\n\nA protein backbone structure can be fully specified by a total of 9 values per residue: 3 bond distances, 3 bond angles, and 3 dihedral torsional angles. The three bond angles and dihedrals are described in Table 1, and the three bond distances correspond to Ni → Cαi, Cαi → Ci, and Ci → Ni+1 where i denotes residue index. These 9 values enable a protein backbone to be losslessly converted from Cartesian to internal angle representation, and vice versa. To determine which subset of values to use to formulate proteins in our model, we take a set of experimentally profiled proteins and translate their coordinates from Cartesian to internal angles and distances and back, measuring the TM score between the initial and reconstructed structures. When excluding an angle or distance, we fix all corresponding values to the mean. The reconstruction TM scores of various combinations of values is illustrated in Figure S1. Of these 9 values, the three bond distances are the least important for reliably reconstructing a structure from Cartesian coordinates to the inter-residue representation and back; they can usually be replaced with constant average values without much impact on the recovered structure. In comparison, removing even two bond angles with relatively little variance (θ2, θ3) results in a large loss in reconstruction TM score (third bar). Removing all bond angles and retaining only dihedrals (φ, ψ, ω) results in only about half of proteins being able to be reconstructed (last bar). Thus, we model the three dihedrals and the three bond angles (second bar in Figure S1); this simplifies our prediction problem to use only periodic angular values (instead of a mixture of angular and real values) without a substantial loss in the accuracy of described structures. Future work might include additional modeling of these real-valued bond distances.\n\nFigure S1: Various combinations of angles and distances and their ability to faithfully reconstruct protein backbones. A TM-score of 0.5 (dashed grey line) indicates the minimum similarity for two structures to be considered to have the same general shape. Error bars represent standard deviation in reconstruction TM scores. Using all bond angles, dihedral angles, and bond distances perfectly reconstructs Cartesian coordinates from internal angles (first column). The second column corresponds to the formulation used in the main text, where we model the 3 dihedrals and 3 bond angles, but keep the 3 bond distances fixed to average values. Other columns fix even more values to their respective means and result in reconstruction TM scores that are too low to be reliably useful.\n\n14\n\ndihedrals, angles, and distancesdihedrals and anglesdihedrals, 1, and distancesdihedrals, 1dihedrals, distancesdihedrals only0.00.20.40.60.81.0TM-score of reconstructionReconstruction accuracy, combinations of angles/distancesUnder review as a conference paper at ICLR 2023\n\nOne detail when converting between a N -residue set of Cartesian coordinates to a set of N −1 angles between consecutive residues is that the latter representation does not capture the first residue’s information (as there is no prior residue to orient against). To solve this, we use a fixed set of coordinates to “seed” all generation of Cartesian coordinates, using the N − 1 specified angles to build out from this fixed point. For all generations, this fixed point is extracted from the coordinates of the N − Cα − C atoms in the first residue on the N-terminus of the PDB structure 1CRN (Teeter, 1984). Doing so does not result in any meaningful reconstruction error in natural structures.\n\nA.2 EFFECT OF LENGTH ON STRUCTURE RECONSTRUCTION\n\nOne of the primary concerns of using an angle-based formulation is that small errors might propagate across many residues to culminate in a large difference in overall structure. To try and quantify the effect of this, we evaluate the “lossiness” of the representation itself, and the ability of the model to learn long-range angle dependencies.\n\nWe start by evaluating the accuracy of our representation itself over different structure lengths. To do this, we sample 5000 structures of varying length from the CATH dataset. For each, we compare the original 3D coordinate representation xc and the 3D coordinates obtained after converting the structure to angles and back to coordinates ˆxc using the TM score algorithm, i.e., TMscore(xc, ˆxc). We find that longer structures exhibit greater TM score divergences when converted through our representation (Figure S2). However, even at our maximum considered structure length of 128 residues, structures still retain a reconstruction TMscore ≈ 0.9, which is well above the accepted threshold of 0.5 denoting the same fold. This indicates that while our representation itself is slightly “lossy”, the losses do not change the overall fold. Even when considering longer structures up to 512 residues in length, the reconstructed structures still share a TMscore similarity much greater than 0.5 (graph not shown), which suggests that our method could scale up to larger structures.\n\nFigure S2: Faithfulness of reconstruction (via TMscore, y-axis) when using the 3 dihedrals and 3 angles described in Table 1 and keeping bond distances fixed to average values, evaluated across 5000 structures of varying length (x-axis). We observe a significant negative correlation between length and reconstruction TM score (Spearman’s correlation r = −0.18, p = 7.2 × 10−39).\n\nNext, we evaluate our model’s ability to successfully reconstruct sequences of varying length. For each structure in our held-out test set (n = 3040), we add t = 750 timesteps of noise to that structure’s angles (recall that we use a total of T = 1000 timesteps during training). This adds a significant amount of noise to corrupt the structure, while retaining a hint of the target true structure as a weak “guide” to what the model should ultimately denoise towards. We then apply our trained\n\n15\n\n406080100120Structure length0.750.800.850.900.951.00TM-score of reconstructionStructure reconstruction with dihedrals and anglesUnder review as a conference paper at ICLR 2023\n\nmodel to these mostly-noised examples, running them for the requisite 750 iterations to fully denoise and reconstruct the angles. Afterwards, we assess reconstruction accuracy by taking the TMscore between the structure specified by the reconstructed angles, and the true structure specified by the ground truth angles. Comparing structures specified by reconstructed and true angles isolates the effects of model error, irrespective of any minor lossiness induced by the representation itself, which we investigated previously (Figure S2). Figure S3 illustrates the relationship between test set structure length and this reconstruction TM score. We find no significant correlation between length and reconstruction TM score (Spearman’s correlation r = −0.0024, p = 0.89). FoldingDiff accurately reconstructs structures with a TM score of greater than 0.95 for 98% of test set, and achieves an average test reconstruction TM score of 0.988.\n\nFigure S3: Test set structure reconstruction accuracy after injecting 750 timesteps of noise into angles. This is evaluated by taking the TM score between the structured specified by the reconstructed angles, and the structure specified by the ground truth angles. The x-axis denotes length of the test set structure, and each bar denotes the distribution of TM scores within that length.\n\nAll in all, these results indicate that (1) our representation itself is indeed lossier with longer structures, but not to a degree that would significantly impact the overall structures, and (2) our model is capable of learning robust relationships between angles regardless of structure length.\n\nB ADDITIONAL NOTES ON SELF-CONSISTENCY TM SCORE\n\nOur scTM evaluation pipeline is similar to previous evaluations done by Trippe et al. (2022) and Lee & Kim (2022), with the primary difference that we use OmegaFold (Wu et al., 2022) instead of AlphaFold (Jumper et al., 2021). OmegaFold is designed without reliance on multiple sequence alignments (MSAs), and performs similarly to AlphaFold while generalizing better to orphan proteins that may not have such evolutionary neighbors (Wu et al., 2022). Furthermore, given that prior works use AlphaFold without MSA information in their evaluation pipelines, OmegaFold appears to be a more appropriate method for scTM evaluation.\n\nOmegaFold is run using default parameters (and release1 weights). We also run AlphaFold without MSA input for benchmarking against Trippe et al. (2022). We provide a single sequence reformatted to mimic a “MSA” to the colabfold tool (Mirdita et al., 2022) with 15 recycling iterations. While the full AlphFold model runs 5 models and picks the best prediction, we use a singular model (model1) to reduce runtime.\n\nTrippe et al. (2022) use ProteinMPNN (Dauparas et al., 2022) for inverse folding and generate 8 candidate sequences per structure, whereas Lee & Kim (2022) use ESM-IF1 (Hsu et al., 2022)\n\n16\n\n406080100120Structure length0.840.860.880.900.920.940.960.981.00TM score(reconstructed coords, truth)Test structure (n=3040) reconstruction, add 750/1000 noise timestepsUnder review as a conference paper at ICLR 2023\n\nand generate 10 candidate sequences for each structure. We performed self-consistency TM score evaluation for both these methods, generating 8 candidate sequences using author-recommended temperature values (T = 1.0 for ESM-IF1, T = 0.1 for ProteinMPNN). We use OmegaFold to fold all amino acid sequences for this comparison. We found that ProteinMPNN in Cα mode (i.e., alpha-carbon mode) consistently yields much stronger scTM values (Tables S1, S2); we thus adopt ProteinMPNN for our primary results. While generating more candidate sequences leads to a higher scTM score (as there are more chances to encounter a successfully folded sequence), we conservatively choose to run 8 samples to be directly comparable to Trippe et al. (2022). We also use the same generation strategy as Trippe et al. (2022), generating 10 structures for each structure length l ∈ [50, 128) – thus the only difference in our scTM analyses is the generated structures themselves.\n\nC ABLATIONS AND BASELINES\n\nC.1 SUBSTITUTING INTERNAL ANGLE FORMULATION FOR CARTESIAN COORDINATES\n\nWe perform an “ablation” of our internal angle representation by replacing our framing of proteins as a series of inter-residue internal angles with a simple Cartesian representation of Cα coordinates x ∈ RN ×3. Notably, this Cartesian representation is no longer rotation or shift invariant. We train a denoising diffusion model with this Cartesian representation, using the same variance schedule, transformer backbone architecture, and loss function, but sampling from a standard Gaussian and with all usages of our wrapping function w removed. This represents the same modelling approach as our main diffusion model, with only our internal angle formulation removed.\n\nTo evaluate the quality of this Cartesian-based diffusion model’s generated structures, we calculate the pairwise distances between all Cα atoms in its generated structures and compare these with distance matrices calculated for real proteins and for our internal angle diffusion model’s generations. For a real protein, this produces a pattern that reveals close proximity between pairwise residues where the protein is folded inwards to produce a compact, coherent structure (Figure S4a). However, similarly visualizing the Cα pairwise distances in the Cartesian model’s generated structures yields no significant proximity or patterns between any residues (Figure S4b). This suggests that the ablated Cartesian model cannot learn to generate meaningful structure, and instead generates a nondescript point cloud. Our internal angle model, on the other hand, produces a visualization that is very similar to that of real proteins (Figure S4c). Simply put, our model’s performance drastically degrades when we change only how inputs are represented. This demonstrates the importance and effectiveness of our internal angle formulation.\n\nC.2 AUTOREGRESSIVE BASELINE FOR ANGLE GENERATION\n\nAs a baseline method for our generative diffusion model, we also implemented an autoregressive (AR) transformer fAR that predicts the next set of six angles in a backbone structure (i.e., the same angles used by FoldingDiff, described in Figure 1 and Table 1) given all prior angles.\n\nArchitecturally, this model consists of the same transformer backbone as used in FoldingDiff combined with the same regression head converting per-token embeddings to angle outputs, though it is trained using absolute positional embeddings rather than relative embeddings as this improved validation loss. The total length of the sequence is encoded using random Fourier feature embeddings, similarly to how time was encoded in FoldingDiff, and this embedding is similarly added to each position in the sequence of angles. The model is trained to predict the i-th set of six angles given all prior angles, using masking to hide the i-th angle and onwards. We use the same wrapped smooth L1 loss as our main FoldingDiff model to handle the fact that these angle predictions exist in the range [−π, π); specifically: Lw(x(i), fAR(x(0,...,i−1))) where superscripts indicate positional indexing. This approach is conceptually similar to causal language modeling (Radford et al., 2019), with the important difference that the inputs and outputs are continuous values, rather than (probabilities over) discrete tokens.\n\nThis model is trained using the same data set and data splits as our main FoldingDiff model with the same preprocessing and normalization. We train fAR using the AdamW optimizer with weight decay set to 0.01. We use a batch size of 256 over 10,000 epochs, linearly scaling the learning rate from 0 to 5 × 10−5 over the first 1,000 epochs, and back to 0 over the remaining 9,000 epochs.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cα pairwise distances, real structure\n\n(b) Cα pairwise distances, Cartesian model\n\n(c) Cα pairwise distances, FoldingDiff (ours)\n\nFigure S4: Pairwise distances between all Cα atoms in various protein backbone structures, all of similar length. All panels use the same color scale. S4a illustrates a set of distances for a real protein structure; note the visual patterns that correspond to various secondary structures and potential contacts and interactions between residues. S4b shows these distances for a structure generated by an ablated model that replaces our proposed internal angle representation with Cartesian coordinates, which results in no coherent structural generation. For comparison, our FoldingDiff model produces structures that compactly fold to create many potential contacts, just as real proteins do (S4c).\n\nTo generate structures from fAR, we “seed” the autoregressive model with 4 sets of 6 angles taken from the corresponding first 4 angle sets in a randomly-chosen naturally occurring protein structure. This serves as a random, but biologically realistic, “prompt” for the model to begin generation. We then supply a fixed length l and repeatedly run fAR to obtain the next i-th set of angles, appending each prediction to the existing i − 1 values in order to predict the i + 1 set of angles. We repeat this until we reach our desired structure length.\n\nWe use the above procedure to generate 10 structures for each structure length l ∈ [50, 128) each with a different set of seed angles. Examples of generated structures of varying lengths are illustrated in Figure S5. All structures generated by this autoregressive approach consist of one singular α helix. This is confirmed by running P-SEA on the generated structures (Figure S6a). Besides being an obvious sign of modal collapse, these extremely long α helices are also not commonly observed in nature (Qin et al., 2013).\n\nSuch behavior where autoregressive models generate a single looped pattern endlessly has been observed in language models as well, where it is often circumvented using a temperature parameter to inject randomness. In our continuous regime, we try to do something similar by adding small amounts of noise to each angle during generation, but are unable to meaningfully deviate from the aforementioned modal collapse to endless α helices.\n\n18\n\n04812162024283236404448525660646872768084889296100104108Residue index05101520253035404550556065707580859095100105Residue index0102030405060CC distance, Å05101520253035404550556065707580859095100105110115120125Residue index05101520253035404550556065707580859095100105110115120125Residue index0102030405060CC distance, Å05101520253035404550556065707580859095100105110115120125Residue index05101520253035404550556065707580859095100105110115120125Residue index0102030405060CC distance, ÅUnder review as a conference paper at ICLR 2023\n\nFigure S5: Structures generated using an autoregressive (AR) baseline approach trained on the same angle-based formulation we propose. This approach predicts the next set of angles given all prior angles, and can be thus used to iteratively generate structures in an autoregressive fashion (see Appendix C.2 for additional details). However, the structures generated this way are all straight α helices, regardless of initial “prompt” angles (see Figure S6a). This complete lack of diversity and meaningful complexity indicates that while this AR model can produce technically “correct” structures, it cannot be used for generative modeling to any meaningful capacity. Figure 6 analogously illustrates FoldingDiff’s generations, which are structurally much more diverse.\n\n(a) AR baseline, secondary structure co-occurrence\n\n(b) AR baseline, scTM designability\n\nFigure S6: Secondary structure elements (a) and designability (b) for structures generated by the autoregressive baseline described in Appendix C.2. We observe that using P-SEA to annotate these generated structures detects exclusively singular α helices, and no β sheets (S6a). This quantifies the observations in Figure S5 that the AR model has “collapsed” into repeatedly generating these coils. We find that these helices exhibit greater designability via scTM scores (computed using ProteinMPNN and OmegaFold) (S6b), with 693 of the 780 structures having scTM ≥ 0.5, though this increase is not meaningful due to the utter lack of diversity in generated sequences.\n\nWe evaluate the AR model’s generated structures’ scTM designability using ProteinMPNN (CAonly mode) and OmegaFold. We find that 693 of the 780 generated structures are designable, leading to an overall designability ratio of 0.89. (Figure S6b). Importantly however, this gain in designability is meaningless, as singular endless coils are biologically neither useful nor novel.\n\nC.3 BASELINES CONTEXTUALIZING SCTM SCORES\n\nTo contextualize FoldingDiff’s scTM scores (Figure 5a), we implement a naive angle generation baseline. We take our test dataset, and concatenate all examples into a matrix of ˆx ∈ [−π, π) ˆN ×6, where ˆN denotes the total number of angle sets in our test dataset, aggregating across all individual chains. To generate a backbone structure of length l, we simply sample l indices from U (0, ˆN ). This creates a chain that perfectly matches the natural distribution of protein internal angles, while also perfectly reproducing the pairwise correlations, i.e., of dihedrals in a Ramachandran plot, but\n\n19\n\n0123456789Number of helices0123456789Number of sheets0.00.20.40.60.81.0Frequency0.350.400.450.500.550.600.65Self-consistency TM score (scTM)020406080CountscTM scores, 780 generated protein backboneslengthshort (70 aa)long (>70 aa)Under review as a conference paper at ICLR 2023\n\ncritically loses the correct ordering of these angles. We randomly generate 780 such structures (10 samples for each integer value of l ∈ [50, 128)). This is the same distribution of lengths as the generated set in our main analysis. For each of these, we perform scTM evaluation with ProteinMPNN (CA-only mode) and OmegaFold. The distribution of scTM scores for these randomly-sampled structures compared to that of FoldingDiff’s generated backbones is shown in Figure S7. We observe that this random protein generation method produces significantly poorer scTM scores than FoldingDiff (p = 1.6 × 10−121, Mann-Whitney test). In fact, not a single structure generated this way is designable. This suggests that our model is not simply learning the overall distribution of angles, but is learning orderings and arrangements of angles that comprise folded protein structures.\n\nFigure S7: Distribution of scTM scores for our generated structures (orange), compared to scTM scores for structures created by randomly shuffling naturally-occurring internal angles (blue). The randomly sampled angles result in no designable structures, despite perfectly capturing the overall distribution and pairwise relations between angles. This suggests our method correctly learns the spatial ordering of angles that folds a valid structure. We additionally take a set of 780 experimental structures and pass them through our scTM pipeline to evaluate the fragility of this pipeline itself. We find that 87% of natural proteins (green) are designable; this forms a “soft” upper bound for what would be achievable by sampling from the true data distribution.\n\nWe take the structures specified by these randomly sampled angles and analyze them for secondary structures using P-SEA, as we did for Figure 4. The resulting 2D histogram is illustrated in Figure S8, and represents a completely different distribution of secondary structures compared to natural structures, or that of FoldingDiff’s generations. This further suggests that this random angle baseline cannot generate natural-appearing structures. It is clear that scTM scores and secondary structure analyses cannot be satisfied using this trivial solution.\n\n20\n\n0.20.40.60.81.0Self-consistency TM (scTM) scores024681012Normalized frequencyscTM scores, naive baseline vs. generated structuresRandomly sampled anglesGeneratedTest set (natural) structuresUnder review as a conference paper at ICLR 2023\n\nFigure S8: Secondary structures present in structures generated by randomly shuffling naturallyoccurring angles, as described in Appendix C.3. This random baseline produces a few hits to β sheets by chance, but notably lacks the α helices present in both natural structures and FoldingDiff’s generations (Figures 4, S11).\n\n21\n\n0123456789Number of helices0123456789Number of sheets0.00.20.40.60.81.0FrequencyUnder review as a conference paper at ICLR 2023\n\nD ADDITIONAL SUPPLEMENTARY FIGURES AND TABLES\n\nFigure S9: Comparison of the cumulative distribution functions (CDF) of angular values in test set and in generated samples. Top row shows dihedral angles (torsional angles involving 4 atoms), and bottom row shows bond angles (involving 3 atoms). Figure 2 shows the histogram distributions corresponding to these CDFs.\n\nFigure S10: Figure 1B from Anand & Achim (2022), reproduced with permission for ease of reference. Illustrated Cαi − Ci − Ni+1 bond angle (third plot from the left) corresponds to θ2 in our formulation. Sampled angles in the work of Anand & Achim (2022) exhibit a much larger spread than the natural distribution of angles, whereas our work matches much more tightly (Figures 2, S9).\n\n22\n\n32101230.00.20.40.60.81.0Cumulative frequency CDFTestSampled32101230.00.20.40.60.81.0 CDF32101230.00.20.40.60.81.0 CDF0.00.51.01.52.02.5Angle (rad)0.00.20.40.60.81.0Cumulative frequency1 CDF0.00.51.01.52.02.53.0Angle (rad)0.00.20.40.60.81.02 CDF0.00.51.01.52.02.53.0Angle (rad)0.00.20.40.60.81.03 CDFUnder review as a conference paper at ICLR 2023\n\nTable S1: Self-consistency TM scores (using ProteinMPNN and OmegaFold) across replicates. Each run generates 10 different structures for each length in l ∈ [50, 128), resulting in 780 total generated structures, and is started from a different random seed, using the same pre-trained model as in the primary text and Appendix C.3. Self-consistency TM scores are computed using ProteinMPNN and OmegaFold, as described in the primary text. Short structures are defined as having 70 residues or fewer (n = 210); long structures are defined as having more than 70 residues (n = 570). Values from our main text are reproduced in the last row for ease of reference. Each generation run produces a consistently high number of designable structures, and also contains a realistic mixture of secondary structure elements (Figure S11).\n\nSeed 1\n2 3\n4 5\n7344 (main text)\n\nDesignable Designable, short (n = 210) Designable, long (n = 570)\n\n173 154 185 182 187 177\n\n72 75 90 86 76 80\n\n101 79 95 96 111 97\n\nTable S2: Self-consistency TM scores calculated using ESM-IF1 to perform inverse folding, rather than ProteinMPNN. These analyze the same FoldingDiff generations as Table S1, and are likewise folded with OmegaFold. ESM-IF1 consistently results in much lower scTM scores.\n\nSeed 1\n2 3\n4 5\n7344 (main text)\n\nDesignable Designable, short (n = 210) Designable, long (n = 570)\n\n117 105 122 115 126 111\n\n61 64 76 72 67 57\n\n56 41 46 43 59 54\n\n(a) Generations, seed 1\n\n(b) Generations, seed 2\n\n(c) Generations, seed 3\n\n(d) Generations, seed 4\n\n(e) Generations, seed 5\n\nFigure S11: For each of the 5 replicates shown in Table S1, we use P-SEA to annotate secondary structures. Each run’s generations contain a mixture of α helices (x-axis) and β sheets (y-axis) that is comparable to natural structures (Figure 4a). FoldingDiff generates reasonable, complex structures consistently.\n\n23\n\n0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09Frequency0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09Frequency0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09Frequency0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09Frequency0123456789Number of helices0123456789Number of sheets0.000.010.020.030.040.050.060.070.080.09FrequencyUnder review as a conference paper at ICLR 2023\n\nFigure S12: Generated structures representing the full range of designability (scTM) and training similarity scores. The top row indicates the original generated structure, the middle row shows the training structure with the highest TM score, and the bottom row indicates the structure predicted by OmegaFold based on residues predicted to produce our generated structure by ProteinMPNN. The first column shows a structure with high designability and high training similarity. The second column shows a structure with designability and training similarity close to 0.5. The third column shows a generated structure that is very different from any training chain, but is also not designable.\n\nFigure S13: Structures from Figure 6 illustrated with the training example with the highest TM score (most similar). Figure rows are arranged as in Figure S12. Our generated structures are visually quite different compared to the best training set match – in almost every example, our generated structure contains a completely different arrangement of secondary structure elements than the closest training structure, indicating that they may be more distinct than TM scores alone might suggest.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure S14: Clustering of our n = 177 “designable” generated backbones with scTM scores ≥ 0.5. We use the average distance metric to perform hierarchical clustering on the pairwise distance matrix d(x, y) = 1−TMscore(x, y). Dark values corresponding to 0 (or conversely, a TM score of 1) indicate (nearly) identical structures. While there are some loosely related groups of structures, we do not observe clearly delineated groups. This indicates that the designable backbones we generate are diverse and represent a wide range of potential structures. For comparison to naturally-occurring structures and prior works, see Figures S15 and S16.\n\n25\n\n0.00.20.40.60.81.0d(x,y)=1TMscore(x,y)Under review as a conference paper at ICLR 2023\n\nFigure S15: To provide additional context for the degree of diversity that is reasonable to expect from our generated sequences, we similarly perform clustering on a set of 177 randomly chosen naturally-occurring CATH structures between 50 and 128 residues in length. We find that natural sequences tend to cluster much more similarly to our designable sequences (Figure S14) compared to that of prior works (Figure S16).\n\n26\n\n0.00.20.40.60.81.0d(x,y)=1TMscore(x,y)Under review as a conference paper at ICLR 2023\n\nFigure S16: Figure from Trippe et al. (2022), reproduced with permission for ease of reference. These authors similarly cluster unconditionally generated backbones with scTM ≥ 0.5 using 1 − TMscore(x, y) as a distance metric. Compared to our identical evaluation, illustrated in Figure S14, we notice that this clustering has a few dark blocks of nearly 0 distance, or a TM score of nearly 1. This suggests that among these designable backbones, many are actually minor variants of a core structure; in actuality, though this work claims to produce 92 designable structures, there seem to be fewer unique structures in this set due to many being near-duplicates.\n\n27",
    "reference": "# Summary Of The Paper\n\nThis paper presents a new diffusion-based generative model that designs protein backbone structures via a procedure that mirrors the native folding process. By considering each residue to be its own reference frame, it describes protein backbone structure as a series of consecutive angles capturing the relative orientation of the constituent amino acid residues instead of Cartesian atom coordinates. A vanilla transformer is used to build a diffusion model to generate new protein structures by denoising from a random, unfolded state towards a stable folded structure. Some experiments indicate resulting model generates lifelike protein, better-respecting protein chirality.\n\n# Strength And Weaknesses\n\n1. This paper proposes a simplified framing of protein backbones . Unlike viewing a protein backbone of amino acids as a cloud of 3D coordinates, authors view it as a sequence of six internal, consecutive angles. The independence of reference frame of each residue leads to no need to use an equivariant neural network. No matter how the protein is rotated or shifted, the angle of the next residue given the current residue never changes.\n2. As mentioned before, no requirements of the use of equivariant networks leads to the possibility of using simple transformer as the backbone architecture, thus the model directly generates structures without relying on additional methods for refinement. Generated backbones exhibit greater designability compared to prior works that use equivariance assumptions.\n3. It presents a suite of validations to quantitatively demonstrate that unconditional sampling from proposed model directly generates realistic protein backbones – from recapitulating the natural distribution of protein inter-residue angles, to producing overall structures with appropriate arrangements of multiple structural building block motifs.\n4. Abundant numerical experiments are supplied to demonstrate that generated backbones contain reasonable structural motifs and they are designable.\n\n**Questions:**\n1. The effectiveness of the proposed model has been shown via solid experiments. The potential advantages can be known from an explanation of the paper, but there seems to be less compared with other models in terms of performance and prediction results, could you please provide more valid evidence?\n2. Although formulating a protein as a series of angles enables to use simpler models without equivariance mechanisms, this framing allows accumulated errors to significantly alter the overall structure of a generated structure. Could you further analyze the impact of this cumulative error compared to other noise errors based on Cartesian coordinate systems on the prediction results? Does additional refinement similar to the original approach help improve reliability?\n3. The generated structures are still simple compared to natural proteins which typically have several hundred residues and it is of static structures. Could you please further explain the possibility and operability of the proposed model in terms of further extensions?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall, this paper is clear and helpful work for computationally generating novel yet physically foldable protein structures. To my best knowledge, describing protein backbone structure as a series of consecutive angles instead of Cartesian coordinates is a new and promising idea.\nOtherwise, authors release the first open-source codebase and trained models for protein structure diffusion which is beneficial for comparison and exchange of other studies.\n\n# Summary Of The Review\n\nThis paper presents a novel parameterization of protein backbone structures that allows for simplified generative modeling. It trains a denoising diffusion probabilistic model with a simple transformer back-bone and demonstrates resulting model unconditionally generates real-life protein structures.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nACTIVE IMAGE INDEXING\n\nPierre Fernandez1,2,⋆ 1Meta AI, FAIR 2Centre Inria de l’Universit ́e de Rennes ⋆Correspondence: pfz@meta.com.\n\n, Matthijs Douze1, Herv ́e J ́egou1, Teddy Furon2,\n\nABSTRACT\n\nImage copy detection and retrieval from large databases leverage two components. First, a neural network maps an image to a vector representation, that is relatively robust to various transformations of the image. Second, an efficient but approximate similarity search algorithm trades scalability (size and speed) against quality of the search, thereby introducing a source of error. This paper improves the robustness of image copy detection with active indexing, that optimizes the interplay of these two components. We reduce the quantization loss of a given image representation by making imperceptible changes to the image before its release. The loss is back-propagated through the deep neural network back to the image, under perceptual constraints. These modifications make the image more retrievable. Our experiments show that the retrieval and copy detection of activated images is significantly improved. For instance, activation improves by +40% the Recall1@1 on various image transformations, and for several popular indexing structures based on product quantization and locality sensitivity hashing.\n\n1\n\nINTRODUCTION\n\nThe traceability of images on a media sharing platform is a challenge: they are widely used, easily edited and disseminated both inside and outside the platform. In this paper, we tackle the corresponding task of Image Copy Detection (ICD), i.e. finding whether an image already exists in the database; and if so, give back its identifier. ICD methods power reverse search engines, photography service providers checking copyrights, or media platforms moderating and tracking down malicious content (e.g. Microsoft’s PhotoDNA (2009) or Apple’s NeuralHash (2021)). Image identification systems have to be robust to identify images that are edited (cropping, colorimetric change, JPEG compression . . . ) after their release (Douze et al., 2021; Wang et al., 2022).\n\nThe common approach for content-based image retrieval reduces images to high-dimensional vectors, referred to as representations. Early representations used for retrieval were hand-crafted features such as color histograms (Swain & Ballard, 1991), GIST (Oliva & Torralba, 2001), or Fisher\n\nFigure 1: Overview of the method and latent space representation. We start from an original image Io that can be edited t(·) in various ways: its feature extraction f (t(Io)) spawns the shaded region in the embedding space. The edited versions should be recoverable by nearest neighbor search on quantized representations. In the regular (non-active) case, f (Io) is quantized by the index as . When the image is edited, t(Io) switches cells and the closest neighbor returned by the index is the wrong one . In active indexing: Io is modified in an imperceptible way to generate I ⋆ such that f (I ⋆) is further away from the boundary. When edited copies f (t(I ⋆)) are queried, retrieval errors are significantly reduced.\n\n1\n\nIndexOutside worldImage editingOriginal imagePublicationI*t(I*)IoIndexingSearch resultsActivated imageActivationLatent spaceApproximate Searchf(Io)f(t(I*))f(I*)f(t(Io))Published as a conference paper at ICLR 2023\n\nVectors (Perronnin et al., 2010). As of now, a large body of work on self-supervised learning focuses on producing discriminative representations with deep neural networks, which has inspired recent ICD systems. In fact, all submissions to the NeurIPS2021 Image Similarity challenge (Papakipos et al., 2022) exploit neural networks. They are trained to provide invariance to potential image transformations, akin to data augmentation in self-supervised learning.\n\nScalability is another key requirement of image similarity search: searching must be fast on largescale databases, which exhaustive vector comparisons cannot do. In practice, ICD engines leverage approximate neighbor search algorithms, that trade search accuracy against scalability. Approximate similarity search algorithms speed up the search by not computing the exact distance between all representations in the dataset (Johnson et al., 2019; Guo et al., 2020). First they lower the number of scored items by partitioning the representation space, and evaluate the distances of only a few subsets. Second, they reduce the computational cost of similarity evaluation with quantization or binarization. These mechanisms make indexing methods subject to the curse of dimensionality. In particular, in high-dimensional spaces, vector representations lie close to boundaries of the partition (B ̈ohm et al., 2001). Since edited versions of an original image have noisy vector representations, they sometimes fall into different subsets or are not quantized the same way by the index. All in all, it makes approximate similarity search very sensitive to perturbations of the edited image representations, which causes images to evade detection.\n\nIn this paper, we introduce a method that improves similarity search on large databases, provided that the platform or photo provider can modify the images before their release (see Fig. 1). We put the popular saying “attack is the best form of defense” into practice by applying image perturbations and drawing inspiration from adversarial attacks. Indeed, representations produced with neural networks are subject to adversarial examples (Szegedy et al., 2013): small perturbations of the input image can lead to very different vector representations, making it possible to create adversarial queries that fool image retrieval systems (Liu et al., 2019; Tolias et al., 2019; Dolhansky & Ferrer, 2020). In contrast, we modify an image to make it more indexing friendly. With minimal changes in the image domain, the image representation is pushed towards the center of the indexing partition, rising the odds that edited versions will remain in the same subset. This property is obtained by minimizing an indexation loss by gradient descent back to the image pixels, like for adversarial examples. For indexing structures based on product quantization (Jegou et al., 2010), this strategy amounts to pushing the representation closer to its quantized codeword, in which case the indexation loss is simply measured by the reconstruction error. Since the image quality is an important constraint here, the perturbation is shaped by perceptual filters to remain invisible to the human eye.\n\nOur contributions are:\n\n• a new approach to improve ICD and retrieval, when images can be changed before release;\n\n• an adversarial image optimization scheme that adds minimal perceptual perturbations to images\n\nin order to reduce reconstruction errors, and improve vector representation for indexing;\n\n• experimental evidence that the method significantly improves index performance.\n\n2 PRELIMINARIES: REPRESENTATION LEARNING AND INDEXING\n\nthe exposure focuses on image representations from SSCD netFor the sake of simplicity, works (Pizzi et al., 2022) and the indexing technique IVF-PQ (Jegou et al., 2010), since both are typically used for ICD. Extensions to other methods can be found in Sec. 5.4.\n\n2.1 DEEP DESCRIPTOR LEARNING\n\nMetric embedding learning aims to learn a mapping f : Rc×h×w → Rd, such that measuring the similarity between images I and I ′ amounts to computing the distance ∥f (I) − f (I ′)∥. In recent works, f is typically a neural network trained with self-supervision on raw data to learn metrically meaningful representations. Methods include contrastive learning (Chen et al., 2020), selfdistillation (Grill et al., 2020; Caron et al., 2021), or masking random patches of images (He et al., 2022; Assran et al., 2022). In particular, SSCD (Pizzi et al., 2022) is a training method specialized for ICD. It employs the contrastive self-supervised method SimCLR (Chen et al., 2020) and entropy regularization (Sablayrolles et al., 2019) to improve the distribution of the representations.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2.2\n\nINDEXING\n\nGiven a dataset X = {xi}n images and a query vector xq, we consider the indexing task that addresses the problem: x∗ := argmin\n\ni=1 ⊂ Rd of d-dimensional vector representations extracted from n\n\n∥x − xq∥.\n\n(1)\n\nx∈X\n\nThis exact nearest neighbor search is not tractable over large-scale databases. Approximate search algorithms lower the amount of scored items thanks to space partitioning and/or accelerate the computations of distances thanks to quantization and pre-computation.\n\nSpace partitioning and cell-probe algorithms. As a first approximation, nearest neighbors are sought only within a fraction of X : at indexing time, X is partitioned into X = (cid:83)b i=1 Xi. At search time, an algorithm Q : Rd → {1, .., b}k′ determines a subset of k′ buckets in which to search, such that k′ = |Q(xq)| ≪ b, yielding the approximation:\n\nargmin x∈X\n\n∥x − xq∥ ≈ argmin\n\nx∈(cid:83)\n\ni∈Q(xq ) Xi\n\n∥x − xq∥.\n\n(2)\n\nA well known partition is the KD-tree (Bentley, 1975) that divides the space along predetermined directions. Subsequently, locality sensitive hashing (LSH) (Indyk & Motwani, 1998; Gionis et al., 1999) and derivative (Datar et al., 2004; Paulev ́e et al., 2010) employ various hash functions for bucket assignment, which implicitly partitions the space.\n\nWe focus on the popular clustering and Inverted Files methods (Sivic & Zisserman, 2003), herein i=1 ⊂ Rd of k centroids (also called “visual denoted by IVF. They employ a codebook C = {ci}k words” in a local descriptor context), for instance learned with k-means over a training set of representations. Then, Q associates x to its nearest centroid qc(x) such that the induced partition is the set of the k Vorono ̈ı cells. When indexing x, the IVF stores x in the bucket associated with ci = qc(x). When querying xq, IVF searches only the k′ buckets associated to centroids ci nearest to xq.\n\nEfficient metric computation and product quantization. Another approximation comes from compressed-domain distance estimation. Vector Quantization (VQ) maps a representation x ∈ Rd to a codeword qf (x) ∈ C = {Ci}K i=1. The function qf is often referred to a quantizer and Ci as a reproduction value. The vector x is then stored as an integer in {1, .., K} corresponding to qf (x). The distance between x and query xq is approximated by ∥qf (x) − xq∥, which is an “asymmetric” distance computation (ADC) because the query is not compressed. This leads to:\n\nargmin x∈X\n\n∥x − xq∥ ≈ argmin\n\nx∈X\n\n∥qf (x) − xq∥.\n\n(3)\n\nBinary quantizers (a.k.a. sketches, Charikar (2002) lead to efficient computations but inaccurate distance estimates (Weiss et al., 2008). Product Quantization (PQ) (Jegou et al., 2010) or In PQ, a vector x ∈ Rd is split into m derivatives Ge et al. (2013) offer better estimates. subvectors in Rd/m: x = (u1, . . . , um). The product quantizer then quantizes the subvectors: qf : x (cid:55)→ (q1(u1), . . . , qm(um)). If each subquantizer qj has Ks reproduction values, the resulting quantizer qf has a high K = (Ks)m. The squared distance estimate is decomposed as:\n\n∥qf (x) − xq∥2 =\n\nm (cid:88)\n\n∥qj(uj) − uj\n\nq∥2.\n\n(4)\n\nThis is efficient since x is stored by the index as qf (x) which has m log2 Ks bits, and since summands can be precomputed without requiring decompression at search time.\n\nj=1\n\n3 ACTIVE INDEXING\n\nActive indexing takes as input an image Io, adds the image representation to the index and outputs an activated image I ⋆ with better traceability properties for the index. It makes the feature representation produced by the neural network more compliant with the indexing structure. The activated image is the one that is disseminated on the platform, therefore the alteration must not degrade the perceived quality of the image.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nImages are activated by an optimization on their pixels. The general optimization problem reads:\n\nI ⋆ := argmin I∈C(Io)\n\nL (I; Io) ,\n\n(5)\n\nwhere L is an indexation loss dependent on the indexing structure, C(Io) is the set of images perceptually close to Io. Algorithm 1 and Figure 1 provide an overview of active indexing.\n\n3.1\n\nIMAGE OPTIMIZATION DEDICATED TO IVF-PQ (“ACTIVATION”)\n\nThe indexing structure IVF-PQ involves a coarse quantizer qc built with k-means clustering for space partitioning, and a fine product quantizer qf on the residual vectors, such that a vector x ∈ Rd is approximated by q(x) = qc(x) + qf (x − qc(x)).\n\nWe solve the optimization problem (5) by iterative gradient descent, back-propagating through the neural network back to the image. The method is classically used in adversarial example generation (Szegedy et al., 2013; Carlini & Wagner, 2017) and watermarking (Vukoti ́c et al., 2020; Fernandez et al., 2022).\n\nAlgorithm 1 Active indexing for IVF-PQ\n\nInput: Io: original image; f : feature extractor; Add xo = f (Io) to Index, get q(xo); Initialize δ0 = 0(c×h×w); for t = 0, ..., N − 1 do\n\nIt ← Io + α . HJND(Io) ⊙ tanh(δt) xt ← f (It) L ← Lf (xt, q(xo)) + λLi(δt) δt+1 ← δt + η × Adam(L)\n\nend for Output: I ⋆ = IN activated image\n\nGiven an original image Io, the loss is an aggregation of the following objectives:\n\nLf (x, q(xo)) = ∥x − q(xo)∥2 Li(I, Io) = ∥I − Io∥2.\n\nwith xo = f (Io), x = f (I)\n\n(6)\n\n(7)\n\nLi is a regularization on the image distortion. Lf is the indexation loss that operates on the representation space. Lf is the Euclidean distance between x and the target q(xo) and its goal is to push the image feature towards q(xo). With IVF-PQ as index, the representation of the activated image gets closer to the quantized version of the original representation, but also closer to the coarse centroid. Finally, the losses are combined as L(I; Io) = Lf (x, q(xo)) + λLi(I, Io).\n\n3.2 PERCEPTUAL ATTENUATION\n\nIt is common to optimize a perturbation δ added to the image, rather than the image itself. The adversarial example literature often considers perceptual constraints in the form of an lp-norm bound applied on δ (Madry et al. (2018) use ∥δ∥∞ < ε = 8/255). Although a smaller ε makes the perturbation less visible, this constraint is not optimal for the human visual system (HVS), e.g. perturbations are more noticeable on flat than on textured areas of the image (see App. A.2).\n\nWe employ a handcrafted perceptual attenuation model based on a Just Noticeable Difference (JND) map (Wu et al., 2017), that adjusts the perturbation intensity according to luminance and contrast masking. Given an image I, the JND map HJND(I) ∈ Rc×h×w models the minimum difference perceivable by the HVS at each pixel and additionally rescales the perturbation channel-wise since the human eye is more sensible to red and green than blue color shift (see App. A for details).\n\nThe relation that links the image I sent to f , δ being optimized and the original Io, reads:\n\nI = Io + α . HJND(Io) ⊙ tanh(δ),\n\n(8)\n\nwith α a global scaling parameter that controls the strength of the perturbation and ⊙ the pointwise multiplication. Coupled with the regularization Li (6), it enforces that the activated image is perceptually similar, i.e. I ⋆ ∈ C(Io) as required in (5).\n\n3.3\n\nIMPACT ON THE INDEXING PERFORMANCE\n\nFigure 1 illustrates that the representation of the activated image gets closer to the reproduction value q(f (Io)), and farther away from the Vorono ̈ı boundary. This is expected to make image similarity search more robust because (1) it decreases the probability that x = f (t(Io)) “falls” outside the bucket; and (2) it lowers the distance between x and q(x), improving the PQ distance estimate.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nBesides, by design, the representation stored by the index is invariant to the activation. Formally stated, consider two images I, J, and one activated version J ⋆ together with their representations x, y, y⋆. When querying x = f (I), the distance estimate is ∥q(y⋆) − x∥ = ∥q(y) − x∥, so the index is oblivious to the change J → J ⋆. This means that the structure can index passive and activated images at the same time. Retrieval of activated images is more accurate but the performance on passive images does not change. This compatibility property makes it possible to select only a subset of images to activate, but also to activate already-indexed images at any time.\n\n4 ANALYSES\n\nWe provide insights on the method for IVF-PQ, considering the effects of quantization and space partitioning. For an image I whose representation is x = f (I) ∈ Rd, ˆx denotes the representation of a transformed version: ˆx = f (t(I)) ∈ Rd, and x⋆ the representation of the activated image I ⋆. For details on the images and the implementation used in the experimental validations, see Sec. 5.1.\n\n4.1 PRODUCT QUANTIZATION: IMPACT ON DISTANCE ESTIMATE\n\nWe start by analyzing the distance estimate considered by the index:\n\n∥ˆx − q(x)∥2 = ∥x − q(x)∥2 + ∥ˆx − x∥2 + 2(x − q(x))⊤(ˆx − x).\n\n(9)\n\nThe activation aims to reduce the first term, i.e. the quantization error ∥x − q(x)∥2, which in turn reduces ∥ˆx − q(x)∥2. Figure 3 shows in blue the empirical distributions of ∥x − q(x)∥2 (passive) and ∥x⋆ − q(x)∥2 (activated). As expected the latter has a lower mean, but also a stronger variance. The variation of the following factors may explain this: i) the strength of the perturbation (due to the HVS modeled by HJND in (8)), ii) the sensitivity of the feature extractor ∥∇xf (x)∥ (some features are easier to push than others), iii) the shapes and sizes of the Vorono ̈ı cells of PQ.\n\nThe second term of (9) models the impact of the image transformation in the feature space. Comparing the orange and blue distributions in Fig. 3, we see that it has a positive mean, but the shift is bigger for activated images. We can assume that the third term has null expectation for two reasons: i) the noise ˆx − x is independent of q(x) and centered around 0, ii) in the high definition regime, quantification noise x − q(x) is independent of x and centered on 0. Thus, this term only increases the variance. Since x⋆ − q(x) has smaller norm, this increase is smaller for activated images.\n\nAll in all, ∥ˆx⋆ − q(x)∥2 has a lower mean but a stronger variance than its passive counterpart ∥ˆx − q(x)∥2. Nevertheless, the decrease of the mean is so large that it compensates the larger variance. The orange distribution in active indexing is further away from the green distribution for negative pairs, i.e. the distance between an indexed vector q(x) and an independent query y.\n\nFigure 2: Precision-Recall curve for ICD with 50k queries and 1M reference images (more details for the experimental setup in Sec. 5.1). pivf is the probability of failure of the IVF (Sec. 4.2).\n\nf\n\nFigure 3: Distance estimates histograms (sec. 4.1). With active indexing, ∥x − q(x)∥2 is reduced (←), inducing a shift (←) in the distribution of ∥ˆx − q(x)∥2, where t(I) a hue-shifted version of I. y is a random query.\n\n5\n\n0.00.10.20.30.40.5Recall0.00.20.40.60.81.0PrecisionPassive (AP=0.130 )Active (AP=0.308 )1pivff=0.1331pivff=0.323Published as a conference paper at ICLR 2023\n\n4.2 SPACE PARTITIONING: IMPACT ON THE IVF PROBABILITY OF FAILURE\n\nWe denote by pf := P(qc(x) ̸= qc(ˆx)) the probability that ˆx is assigned to a wrong bucket by IVF assignment qc. In the single-probe search (k′ = 1), the recall (probability that a pair is detected when it is a true match, for a given threshold τ on the distance) is upper-bounded by 1 − pf :\n\nRτ = P ({qc(ˆx) = qc(x)} ∩ {∥ˆx − q(x)∥ < τ }) ≤ P ({qc(ˆx) = qc(x)}) = 1 − pf .\n\n(10)\n\nIn other terms, even with a high threshold τ → ∞ (and low precision), the detection misses representations that ought to be matched, with probability pf . It explains the sharp drop at recall R = 0.13 in Fig. 2. This is why it is crucial to decrease pf . The effect of active indexing is to reduce ∥ˆx − qc(x)∥ therefore reducing pf and increasing the upper-bound for R: the drop shifts towards R = 0.32.\n\nThis explanation suggests that pushing x towards qc(x) decreases even more efficiently pf . This makes the IVF more robust to transformation but this may jeopardize the PQ search because features of activated images are packed altogether. In a way, our strategy, which pushes x towards q(x), dispatches the improvement over the IVF and the PQ search.\n\n5 EXPERIMENTAL RESULTS\n\n5.1 EXPERIMENTAL SETUP\n\nDataset. We use DISC21 (Douze et al., 2021) a dataset dedicated to ICD. It includes 1M reference images and 50k query images, 10k of which are true copies from reference images. A disjoint 1Mimage set with same distribution as the reference images is given for training. Images resolutions range from 200×200 to 1024×1024 pixels (most of the images are around 1024×768 pixels).\n\nThe queries used in our experiments are not the queries in DISC21, since we need to control the image transformations in our experiments, and most transformations of DISC21 were done manually so they are not reproducible. Our queries are transformations of images after active indexing. These transformations range from simple attacks like rotation to more realistic social network transformations which created the original DISC21 queries (see App. B.1).\n\nMetrics. For retrieval, our main metric is Recall 1@1 (R@1 for simplicity), which corresponds to the proportion of positive queries where the top-1 retrieved result is the reference image.\n\nFor copy detection, we use the same metric as the NeurIPS Image Similarity Challenge (Douze et al., 2021). We retrieve the k = 10 most similar database features for every query; and we declare a pair is a match if the distance is lower than a threshold τ . To evaluate detection efficiency, we use the 10k matching queries above-mentioned together with 40k negative queries (i.e. not included in the database). We use precision and recall, as well as the area under the precision-recall curve, which is equivalent to the micro average precision (μAP). While R@1 only measures ranking quality of the index, μAP takes into account the confidence of a match.\n\nAs for image quality metric, we use the Peak Signal-to-Noise Ratio (PSNR) which is defined as (cid:0)2552/MSE(I, I ′)2(cid:1), as well as SSIM (Wang et al., 2004) and the norm ∥I − I ′∥∞. 10 log10\n\nImplementation details. The evaluation procedure is: (1) we train an index on the 1M training images, (2) index the 1M reference images, (3) activate (or not) 10k images from this reference set. (4) At search time, we use the index to get closest neighbors (and their distances) of transformed versions from a query set made of the 10k images.\n\nUnless stated otherwise, we use a IVF4096,PQ8x8 index (IVF quantizer with 4096 centroids, and PQ with 8 subquantizers of 28 centroids), and use only one probe on IVF search for shortlist selection (k′ = 1). Compared to a realistic setting, we voluntarily use an indexing method that severely degrades learned representations to showcase and analyze the effect of the active indexing. For feature extraction, we use an SSCD model with a ResNet50 trunk (He et al., 2016). It takes image resized to 288×288 and generates normalized representations in R512. Optimization (5) is done with the Adam optimizer (Kingma & Ba, 2015), the learning rate is set to 1, the number of iterations to N = 10 and the regularization to λ = 1. In (8), the distortion scaling is set to α = 3 (leading to an average PNSR around 43 dB). In this setup, activating 128 images takes around 6s (≈ 40ms/image) with a 32GB GPU. It can be sped-up at the cost of some accuracy (see App. C.2).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of the index performance between activated and passive images. The search is done on a 1M image set and R@1 is averaged over 10k query images submitted to different transformations before search. Random: randomly apply 1 to 4 transformations. Avg.: average on the transformations presented in the table (details in App. B.2). No index: exhaustive brute-force nearest neighbor search. IVF-PQ: IVF4096,PQ8X8 index with k′=1 (16 for IVF-PQ16). IVF-PQ†: IVF512,PQ32X8 with k′ = 32.\n\nSearch(ms)\n\nBytes/vector\n\nActivated\n\nIdentity\n\nContr.0.5\n\nContr.2.0\n\nBright.0.5\n\nBright.2.0\n\nHue0.2\n\nBlur2.0\n\n50 JPEG\n\nRot.25\n\nRot.90\n\n0.5\n\nResi.0.5\n\nMeme\n\nRandom\n\nAvg.\n\nCrop\n\nNo index\n\n252\n\n2048\n\n✗\n\n1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.90 0.99\n\nIVF-PQ\n\n0.38\n\nIVF-PQ16 0.42\n\n8\n\n8\n\n✗ 1.00 0.73 0.39 0.73 0.28 0.62 0.48 0.72 0.07 0.14 0.14 0.72 0.14 0.13 0.45 ✔ 1.00 1.00 0.96 1.00 0.92 1.00 0.96 0.99 0.10 0.50 0.29 1.00 0.43 0.32 0.75\n\n✗ 1.00 1.00 0.90 1.00 0.78 0.99 0.95 0.99 0.35 0.57 0.57 1.00 0.56 0.39 0.79 ✔ 1.00 1.00 1.00 1.00 0.98 1.00 1.00 1.00 0.43 0.88 0.75 1.00 0.84 0.50 0.88\n\nIVF-PQ†\n\n1.9\n\n32\n\n✗ 1.00 1.00 0.99 1.00 0.95 1.00 0.99 1.00 0.72 0.87 0.88 1.00 0.87 0.61 0.92 ✔ 1.00 1.00 0.99 1.00 0.98 1.00 1.00 1.00 0.75 0.92 0.91 1.00 0.92 0.63 0.94\n\n5.2 ACTIVE VS. PASSIVE\n\nThis section compares retrieval performance of active and passive indexing. We evaluate R@1 when different transformations are applied to the 10k reference images before search. The “Passive” lines of Tab. 1 show how the IVF-PQ degrades the recall. This is expected, but the IVF-PQ also accelerates search 500× and the index is 256× more compact, which is necessary for large-scale applications. Edited images are retrieved more often when they were activated for the index: increase of up to +60 R@1 for strong brightness and contrast changes, close to results of the brute-force search. We also notice that the performance of the active IVF-PQk′=1 is approximately the same as the one of the passive IVF-PQk′=16, meaning that the search can be made more efficient at equal performance. For the IVF-PQ† that does less approximation in the search (but is slower and takes more memory), retrieval on activated images is also improved, though to a lesser extent.\n\nAs for copy detection, Figure 2 gives the precision-recall curves obtained for a sliding value of τ , and corresponding μAP. Again, we observe a significant increase (×2) in μAP with active indexing. Note that the detection performance is much weaker than the brute-force search even in the active case because of the strong approximation made by space partitioning (more details in Sec. 4.2).\n\nExample of activated images are given in Fig. 5 (more in App. E), while the qualitative image metrics are as follows: PSNR= 43.8 ± 2.2 dB, SSIM= 0.98 ± 0.01, and ∥I − I ′∥∞ = 14.5 ± 1.2. These results are computed on 10k images, the ± indicates the standard deviation.\n\n5.3\n\nIMAGE QUALITY TRADE-OFF\n\nFor a fixed index and neural extractor, the performance of active indexing mainly depends on the scaling α that controls the activated image quality. In Fig. 4, we repeat the previous experiment for different values of α and plot the μAP against the average PSNR. As expected, lower PSNR implies better μAP. For instance, at PSNR 30 dB, the μAP is augmented threefold compared to the passive case. Indeed, for strong perturbations the objective function of (6) can be further lowered, reducing even more the gap between representations and their quantized counterparts.\n\n5.4 GENERALIZATION\n\nGeneralization to other neural feature extractors.\n\nWe first reproduce the experiment of Sec. 5.1 with different extractors, that cover distinct training methods and architectures. Among them, we evaluate a ResNext101 (Xie et al., 2017) trained with SSCD (Pizzi et al., 2022), a larger network than the ResNet50 used in our main experiments ; the winner of the descriptor track of the NeurIPS ISC, LYAKAAP-dt1 (Yokoo, 2021), that uses an EfficientNetv2 architecture (Tan & Le, 2021) ; networks from DINO (Caron et al., 2021), either based on ResNet50 or ViT (Dosovitskiy et al., 2021), like the ViT-S model (Touvron et al., 2021).\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: R@1 for different transformations before search. We use our method to activate images for indexing with IVF-PQ, with different neural networks used as feature extractors.\n\nArchitecture\n\nName\n\nResNet50\n\nActivated\n\nIdentity\n\nContr.0.5\n\nContr.2.0\n\nBright.0.5\n\nBright.2.0\n\nHue0.2\n\nBlur2.0\n\n50 JPEG\n\nRot.25\n\nRot.90\n\n0.5\n\nResi.0.5\n\nMeme\n\nRandom\n\nAvg.\n\nCrop\n\nSSCD\n\n✗ 1.00 0.73 0.39 0.73 0.28 0.62 0.48 0.72 0.07 0.14 0.14 0.72 0.14 0.13 0.45 ✔ 1.00 1.00 0.96 1.00 0.92 1.00 0.96 0.99 0.10 0.50 0.29 1.00 0.43 0.32 0.75 ✗\n\n1.00 0.88 0.68 0.88 0.57 0.84 0.46 0.79 0.46 0.63 0.53 0.80 0.48 0.28 0.66 ResNext101 ✔ 1.00 1.00 0.96 1.00 0.90 0.99 0.77 0.97 0.53 0.85 0.64 1.00 0.74 0.37 0.84\n\nResNet50\n\nDINO\n\nViT-s\n\nISC-dt1 EffNetv2\n\n✗ 1.00 0.66 0.65 0.65 0.52 0.71 0.52 0.82 0.07 0.20 0.51 0.84 0.62 0.18 0.57 ✔ 1.00 0.99 0.88 0.99 0.75 0.93 0.72 0.94 0.08 0.25 0.57 0.99 0.82 0.23 0.72 ✗\n1.00 0.89 0.71 0.86 0.64 0.75 0.74 0.90 0.14 0.18 0.57 0.88 0.61 0.25 0.65 ✔ 1.00 0.99 0.94 0.99 0.92 0.98 0.89 0.99 0.15 0.28 0.63 0.99 0.77 0.32 0.77\n\n✗ 1.00 0.25 0.08 0.16 0.01 0.51 0.54 0.84 0.18 0.16 0.23 0.79 0.16 0.18 0.36 ✔ 1.00 0.57 0.16 0.33 0.01 0.88 0.79 0.97 0.20 0.24 0.29 0.97 0.26 0.26 0.49\n\nTable 2 presents the R@1 obtained on 10k activated images when applying different transformations before search. The R@1 is better for activated images for all transformations and all neural networks. The average improvement on all transformations ranges from +12% for DINO ViT-s to +30% for SSCD ResNet50.\n\nGeneralization to other indexes.\n\nThe method easily generalizes to other types of indexing structures, the only difference being in the indexation loss Lf (6). We present some of them below:\n\n• PQ and OPQ.\n\nIn PQ (Jegou et al., 2010), a vector x ∈ Rd is approximated by qf (x). Lf reads ∥x − qf (xo)∥. In OPQ (Ge et al., 2013), vectors are rotated by matrix R before codeword assignment, such that RR⊤ = I. Lf becomes ∥x − R⊤qf (Rxo)∥.\n\n• IVF. Here, we only do space partitioning. Employing Lf = ∥x − qc(xo)∥ (“pushing towards the cluster centroid”) decreases the odds of x falling in the wrong cell (see Sec. 4.2). In this case, an issue can be that similar representations are all pushed together to a same centroid, which makes them less discriminate. Empirically, we found that this does not happen because perceptual constraint in the image domain prevents features from getting too close.\n\n• LSH.\n\nLocality Sensitive Hashing maps x ∈ Rd to a binary hash b(x) ∈ RL. It is commonly j x). j x, allows to push x along the LSH directions and\n\ndone with projections against a set of vectors, which give for j ∈ [1, .., L], bj(x) = sign(w⊤ The objective Lf = −1/L (cid:80) to improve the robustness of the hash.\n\nj sign(b(xo)) · w⊤\n\nTable 3 presents the R@1 and μAP obtained on the 50k query set. Again, results are always better in the active scenario. We remark that active indexing has more impact on space partitioning tech-\n\nFigure 4: PSNR trade-off. As the PSNR decreases, the μAP (orange) gets better, because the distance (blue) between activated representations x and q(x) decreases.\n\nFigure 5: Activated images. Left: reference from DISC (R000643.jpg and R000761.jpg), middle: activated image, right: pixel-wise difference.\n\n8\n\n30405060PSNR (dB)0.40.50.60.70.80.91.0||xq(x)||20.10.20.30.40.50.60.7APActive - ||xq(x)||2PassiveActive - APPassivePublished as a conference paper at ICLR 2023\n\nTable 3: R@1 averaged on transformations presented in Tab. 1 and μAP for different indexing structures\n\nIndex\n\nSearch time\n\nR@1 avg.\n\nPassive Activated\n\nμAP Passive Activated\n\nIVF 1024\n\n0.32 ms PCA64, LSH 0.99 ms 5.71 ms\n\nOPQ 8x8\n\n0.66 0.78 0.92\n\n0.89 0.86 0.94\n\n0.16 0.25 0.48\n\n0.43 0.39 0.55\n\nniques: the improvement for IVF is higher than with PQ and the LSH binary sketches. As to be expected, the impact is smaller when the indexing method is more accurate.\n\n6 RELATED WORK\n\nImage watermarking hides a message in a host image, such that it can be reliably decoded even if the host image is edited. Early methods directly embed the watermark signal in the spatial or transform domain like DCT or DWT (Cox et al., 2007). Recently, deep-learning based methods jointly train an encoder and a decoder to learn how to watermark images (Zhu et al., 2018; Ahmadi et al., 2020; Zhang et al., 2020).\n\nWatermarking is an alternative technology for ICD. Our method bridges indexing and watermarking, where the image is modified before publication. Regarding retrieval performance, active indexing is more robust than watermarking. Indeed, the embedded signal reinforces the structure naturally present in the original image, whereas watermarking has to hide a large secret keyed signal independent of the original feature. App. D provides a more thorough discussion and experimental results comparing indexing and watermarking.\n\nActive fingerprint is more related to our work. As far as we know, this concept was invented by Voloshynovskiy et al. (2012). They consider that the image I ∈ RN is mapped to x ∈ RN by an invertible transform W such that W W ⊤. The binary fingerprint is obtained by taking the sign of the projections of x against a set of vectors b1, ., bL ∈ RN (`a la LSH). Then, they change x to strengthen the amplitude of these projections so that their signs become more robust to noise. They recover I ⋆ with W ⊤. This scheme is applied to image patches in (Kostadinov et al., 2016) where the performance is measured as a bit error rate after JPEG compression. Our paper adapts this idea from fingerprinting to indexing, with modern deep learning representations and state-of-the-art indexing techniques. The range of transformations is also much broader and includes geometric transforms.\n\n7 CONCLUSION & DISCUSSION\n\nWe introduce a way to improve ICD in large-scale settings, when images can be changed before release. It leverages an optimization scheme, similar to adversarial examples, that modifies images so that (1) their representations are better suited for indexing, (2) the perturbation is invisible to the human eye. We provide grounded analyses on the method and show that it significantly improves retrieval performance of activated images, on a number of neural extractors and indexing structures.\n\nActivating images takes time (in the order of 10 ms/image) but one advantage is that the database may contain both active and passive images: active indexing does not spoil the performance of passive indexing and vice-versa. This is good for legacy compliance and also opens the door to flexible digital asset management strategies (actively indexing images of particular importance).\n\nThe main limitation of the method is that images need to be activated before release. In the case of existing databases where images have already been released, images could still be activated for future releases (meaning that there would be 2 versions of the image online, a passive one that can be retrieved as long as it is not transformed too strongly, and an activated one with better copy detection properties). Another one is that it is not agnostic to the indexing structure and extractor that are used by the similarity search. Finally, an adversary could still break the indexing system in several ways. In a black-box setting, adversarial purification (Shi et al., 2021) could get rid of the perturbation that activated the image. In a semi-white-box setting (knowledge of the feature extractor), targeted mismatch attacks against passive indexing like Tolias et al. (2019) may also degrade the retrieval.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nTeddy Furon thanks ANR-AID for funding Chaire SAIDA ANR-20-CHIA-0011-01.\n\nETHICS STATEMENT\n\nSocietal impact statement. Content tracing is a double-edged sword. On the one hand, it allows media platforms to more accurately track malicious content (pornographic, terrorist, violent images, e.g. Apple’s NeuralHash and Microsoft’s PhotoDNA) and to protect copyright (e.g. Youtube’s Content ID). On the other hand it can be used as a means of societal and political censorship, to restrict free speech of specific communities. However, we still believe that research needs to be advanced to improve global moderation in the internet. We also believe that advantages that a better copy detection could bring are more numerous than its drawbacks.\n\nEnvironmental impact statement. We roughly estimated that the total GPU-days used for running all our experiments to 200, or ≈ 5000 GPU-hours. Experiments were conducted using a private infrastructure and we estimate total emissions to be in the order of a ton CO2eq. Estimations were conducted using the MachineLearning Impact calculator presented in Lacoste et al. (2019). We do not consider in this approximation: memory storage, CPU-hours, production cost of GPUs/ CPUs, etc. as well as the environmental cost of training the neural networks used as feature extractors. Although the cost of the experiments and the method is high, it could possibly allow a reduction of the computations needed in large data-centers thanks to improved performance of indexing structures.\n\nREPRODUCIBILITY STATEMENT\n\nThe implementation is available at github.com/facebookresearch/active indexing. Models used for feature extraction (SSCD, DINO, ISC-dt1) can be downloaded in their respective repositories. It builds upon the open-source Pytorch (Paszke et al., 2019) and FAISS (Johnson et al., 2019) libraries.\n\nThe main dataset used in the experiments (DISC21) can be freely downloaded on its webpage https://ai.facebook.com/datasets/disc21-dataset/. Dataset processing is described in App. B.1.\n\nREFERENCES\n\nMahdi Ahmadi, Alireza Norouzi, Nader Karimi, Shadrokh Samavi, and Ali Emami. Redmark: Framework for residual diffusion watermarking based on deep networks. Expert Systems with Applications, 2020.\n\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. arXiv preprint arXiv:2204.07141, 2022.\n\nAnish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial\n\nexamples. In ICML. PMLR, 2018.\n\nJon Louis Bentley. Multidimensional binary search trees used for associative searching. Communi-\n\ncations of the ACM, 18(9):509–517, 1975.\n\nChristian B ̈ohm, Stefan Berchtold, and Daniel A Keim. Searching in high-dimensional spaces: Index structures for improving the performance of multimedia databases. ACM Computing Surveys (CSUR), 33(3):322–373, 2001.\n\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017\n\nIEEE Symposium on Security and Privacy (SP). IEEE, 2017.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and In ICCV. IEEE,\n\nArmand Joulin. Emerging properties in self-supervised vision transformers. 2021.\n\nMoses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of\n\nthe thiry-fourth annual ACM symposium on Theory of computing, pp. 380–388, 2002.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML. PMLR, 2020.\n\nChun-Hsien Chou and Yun-Chin Li. A perceptually tuned subband image coder based on the meaIEEE Transactions on circuits and systems for video\n\nsure of just-noticeable-distortion profile. technology, 5(6):467–476, 1995.\n\nIngemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. Digital watermark-\n\ning and steganography. Morgan kaufmann, 2007.\n\nMayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253–262, 2004.\n\nBrian Dolhansky and Cristian Canton Ferrer. Adversarial collision attacks on image hashing func-\n\ntions. arXiv preprint arXiv:2011.09473, 2020.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nMatthijs Douze, Giorgos Tolias, Ed Pizzi, Zo ̈e Papakipos, Lowik Chanussot, Filip Radenovic, Tomas Jenicek, Maxim Maximov, Laura Leal-Taix ́e, Ismail Elezi, et al. The 2021 image similarity dataset and challenge. arXiv preprint arXiv:2106.09672, 2021.\n\nPierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Herv ́e J ́egou, and Matthijs Douze. Watermarking images in self-supervised latent spaces. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022.\n\nTiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate\n\nnearest neighbor search. In CVPR. IEEE, 2013.\n\nAristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via\n\nhashing. In Vldb, volume 99, pp. 518–529, 1999.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020.\n\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML. PMLR, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR. IEEE, 2016.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. In CVPR, 2022.\n\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pp. 604–613, 1998.\n\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\n\nsearch. Transactions on Pattern Analysis and Machine Intelligence, 33(1):117–128, 2010.\n\nQiuping Jiang, Zhentao Liu, Shiqi Wang, Feng Shao, and Weisi Lin. Towards top-down just noticeable difference estimation of natural images. IEEE Transactions on Image Processing, 2022.\n\nJeff Johnson, Matthijs Douze, and Herv ́e J ́egou. Billion-scale similarity search with gpus. IEEE\n\nTransactions on Big Data, 2019.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nDimche Kostadinov, Slava Voloshynovskiy, Maurits Diephuis, and Taras Holotyak. Local active\n\ncontent fingerprinting: Optimal solution under linear modulation. In ICIP, 2016.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nLester E Krueger. Reconciling fechner and stevens: Toward a unified psychophysical law. Behav-\n\nioral and Brain Sciences, 12(2):251–267, 1989.\n\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the\n\ncarbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.\n\nCassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against\n\nunseen threat models. In ICLR, 2021.\n\nZhuoran Liu, Zhengyu Zhao, and Martha Larson. Who’s afraid of adversarial queries? the imIn Proceedings of the 2019 on\n\npact of image modifications on content-based image retrieval. International Conference on Multimedia Retrieval, pp. 306–314, 2019.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\n\nTowards deep learning models resistant to adversarial attacks. In ICLR, 2018.\n\nS ́ebastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Inter-\n\nnational Conference on Multimedia. ACM, 2010.\n\nNeuralHash. Apple. https://www.apple.com/child-safety/pdf, 2021.\n\nAude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation of the\n\nspatial envelope. International journal of computer vision, 42(3):145–175, 2001.\n\nZoe Papakipos and Joanna Bitton. Augly: Data augmentations for robustness. arXiv preprint\n\narXiv:2201.06494, 2022.\n\nZo ̈e Papakipos, Giorgos Tolias, Tomas Jenicek, Ed Pizzi, Shuhei Yokoo, Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, Sanjay Addicam, et al. Results and findings of the 2021 image similarity challenge. In NeurIPS 2021 Competitions and Demonstrations Track, pp. 1–12. PMLR, 2022.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), NeurIPS. Curran Associates, Inc., 2019.\n\nLo ̈ıc Paulev ́e, Herv ́e J ́egou, and Laurent Amsaleg. Locality sensitive hashing: A comparison of hash\n\nfunction types and querying mechanisms. Pattern recognition letters, 31(11), 2010.\n\nFlorent Perronnin, Yan Liu, Jorge S ́anchez, and Herv ́e Poirier. Large-scale image retrieval with\n\ncompressed fisher vectors. In CVPR, pp. 3384–3391. IEEE, 2010.\n\nPhotoDNA. Microsoft. https://www.microsoft.com/en-us/photodna, 2009.\n\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-\n\nsupervised descriptor for image copy detection. In CVPR. IEEE, 2022.\n\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv ́e J ́egou. Spreading vectors for\n\nsimilarity search. ICML, 2019.\n\nChanghao Shi, Chester Holtz, and Gal Mishne. Online adversarial purification based on self-\n\nsupervised learning. In ICLR, 2021.\n\nRichard Shin and Dawn Song. Jpeg-resistant adversarial images. In NeurIPS Workshop on Machine\n\nLearning and Computer Security, 2017.\n\nJosef Sivic and Andrew Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pp. 1470–1470. IEEE Computer Society, 2003.\n\nMichael J Swain and Dana H Ballard. Color indexing. International journal of computer vision, 7\n\n(1):11–32, 1991.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\n\nand Rob Fergus. Intriguing properties of neural networks. In ICLR, 2013.\n\nMingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In ICML. PMLR,\n\n2021.\n\nGiorgos Tolias, Filip Radenovic, and Ondrej Chum. Targeted mismatch adversarial attack: Query\n\nwith a flower to retrieve the tower. In ICCV. IEEE, 2019.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, 2021.\n\nSviatoslav Voloshynovskiy, Farzad Farhadzadeh, Oleksiy Koval, and Taras Holotyak. Active content fingerprinting: a marriage of digital watermarking and content fingerprinting. In International Workshop on Information Forensics and Security (WIFS), pp. 175–180. IEEE, 2012.\n\nVedran Vukoti ́c, Vivien Chappelier, and Teddy Furon. Are classification deep neural networks good\n\nfor blind image watermarking? Entropy, 2020.\n\nWenhao Wang, Yifan Sun, and Yi Yang. A benchmark and asymmetrical-similarity learning for\n\npractical image copy detection. arXiv preprint arXiv:2205.12358, 2022.\n\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\n\nfrom error visibility to structural similarity. IEEE Transactions on image processing, 2004.\n\nAndrew B Watson. Dct quantization matrices visually optimized for individual images. In Human\n\nvision, visual processing, and digital display IV, volume 1913, pp. 202–216. SPIE, 1993.\n\nYair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. NeurIPS, 21, 2008.\n\nJinjian Wu, Leida Li, Weisheng Dong, Guangming Shi, Weisi Lin, and C-C Jay Kuo. Enhanced just noticeable difference model for images with pattern complexity. IEEE Transactions on Image Processing, 2017.\n\nSaining Xie, Ross Girshick, Piotr Doll ́ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-\n\nformations for deep neural networks. In CVPR. IEEE, 2017.\n\nXK Yang, WS Ling, ZK Lu, Ee Ping Ong, and SS Yao. Just noticeable distortion model and its applications in video coding. Signal processing: Image communication, 20(7):662–680, 2005.\n\nShuhei Yokoo. Contrastive learning with large memory bank and negative embedding subtraction\n\nfor accurate copy detection. arXiv preprint arXiv:2112.04323, 2021.\n\nChaoning Zhang, Philipp Benz, Adil Karjauv, Geng Sun, and In So Kweon. Udh: Universal deep\n\nhiding for steganography, watermarking, and light field messaging. NeurIPS, 2020.\n\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\n\neffectiveness of deep features as a perceptual metric. In CVPR. IEEE, 2018.\n\nXiaohui Zhang, Weisi Lin, and Ping Xue. Just-noticeable difference estimation with pixels in im-\n\nages. Journal of Visual Communication and Image Representation, 19(1):30–41, 2008.\n\nJiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei. Hidden: Hiding data with deep networks.\n\nIn ECCV, 2018.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nSupplementary material - Active Image Indexing\n\nA DETAILS ON THE PERCEPTUAL ATTENUATION MODEL\n\nA.1\n\nJUST NOTICEABLE DIFFERENCE MAP\n\nThe maximum change that the human visual system (HVS) cannot perceive is sometimes referred to as the just noticeable difference (JND) Krueger (1989). It is used in many applications, such as image/video watemarking, compression, quality assessment (JND is also used in audio).\n\nJND models in pixel domain directly calculate the JND at each pixel location (i.e. how much pixel difference is perceivable by the HVS). The JND map that we use is based on the work of Chou & Li (1995). We use this model for its simplicity, its efficiency and its good qualitative results. More complex HVS models could also be used if even higher imperceptibility is needed (Watson (1993); Yang et al. (2005); Zhang et al. (2008); Jiang et al. (2022) to cite a few). The JND map takes into account two characteristics of the HVS, namely the luminance adaptation (LA) and the contrast masking (CM) phenomena. We follow the same notations as Wu et al. (2017).\n\nThe CM map MC is a function of the image gradient magnitude Cl (the Sobel filter of the image):\n\nMC(x) = 0.115 ×\n\nα · Cl(x)2.4 Cl(x)2 + β2\n\n, with Cl =\n\n(cid:113)\n\n∇xI(x)2 + ∇yI(x)2,\n\n(11)\n\nwhere x is the pixel location, I(x) the image intensity, α = 16, and β = 26. It is an increasing function of Cl, meaning that the stronger the gradient is at x, the more the image is masking a local perturbation, and the higher the noticeable pixel difference is.\n\nLA takes into account the fact that the HVS presents different sensitivity to background luminance (e.g. it is less sensible in dark backgrounds). It is modeled as:\n\nLA(x) =\n\n(cid:33)\n\n \n\n\n\n(cid:32)\n\n(cid:114)\n\n17 ×\n\n1 −\n\nB(x) 127 3 × (B(x) − 127) 128\n\n+ 3\n\nif B(x) < 127\n\nif B(x) ≥ 127,\n\n(12)\n\nwhere B(x) is the background luminance, which is calculated as the mean luminance value of a local patch centered on x.\n\nFinally, both effects are combined with a nonlinear additivity model:\n\nHJND = LA + MC − C · min{LA, MC},\n\n(13)\n\nwhere C is set to 0.3 and determines the overlapping effect. For color images, the final RGB heatmap is HJND = [αRH, αGH, αBH], where (αR, αG, αB) are inversely proportional to the mixing coefficients for the luminance: (αR, αG, αB) = 0.072/(0.299, 0.587, 0.114).\n\nFigure 6: A reference image I from DISC21 (R002815.jpg), and the associated perceptual heatmap HJND(I).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.2 COMPARISON WITH l∞ CONSTRAINT EMBEDDING\n\nFigure 7 shows the same image activated using either the l∞ constraint (commonly used in the adversarial attack literature) or our perceptual constraint based on the JND model explained above. Even with very small ε (4 over 255 in the example bellow), the perturbation is visible especially in the flat regions of the images, such as the sea or sky.\n\nLaidlaw et al. (2021) also show that the l∞ is not a good perceptual constraint. They use the LPIPS loss (Zhang et al., 2018) as a surrogate for the HVS to develop more imperceptible adversarial attacks. Although a similar approach could be used here, we found that at this small level of image distortion the LPIPS did not capture CM and LA as well as the handcrafted perceptual models present in the compression and watermarking literature.\n\n(a) l∞ = 4, PSNR = 36.4 dB, SSIM = 0.91\n\n(b) l∞ = 23, PSNR = 34.4 dB, SSIM = 0.94\n\nFigure 7: Activated images, either with (a) the l∞ ≤ 4 constraint or with (b) our perceptual model (best viewed on screen). We give the corresponding measures between the original and the protected image, as well as the pixel-wise difference. The perturbation on the right is much less perceptible thanks to the perceptual model, even though its l∞ distance with the original image is much higher.\n\nB EXPERIMENTS DETAILS\n\nThis section describes the details omitted in experimental sections.\n\nB.1 DATASET\n\nThe dataset DISC 2021 was designed for the Image Similarity Challenge (Douze et al., 2021) and can be downloaded in the dataset webpage: https://ai.facebook.com/datasets/disc21-dataset/.\n\nWe want to test performance on edited versions of activated images but in DISC query set transformations are already applied to images. Therefore the query set cannot be used as it is.\n\nWe create a first test set “Ref10k” by selecting the 10k images from the reference set that were originally used to generate the queries (the “dev queries” from the downloadable version). We also re-create a query set “Query50k”. To be as close as possible, we use the same images that were used for generating queries in DISC. Edited images are generated using the AugLy library (Papakipos & Bitton, 2022), following the guidelines given in the “Automatic Transformations” section of the DISC paper. Therefore, the main difference between the query set used in our experiments and the original one is that ours do not have manual augmentations.\n\nB.2 TRANSFORMATIONS SEEN AT TEST TIME\n\nThey cover both spatial transformations (crops, rotation, etc.), pixel-value transformations (contrast, hue, jpeg, etc.) and “everyday life” transformations with the AugLy augmentations. All transformations are illustrated in Fig. 4. The parameters for all transformations are the ones of the torchvision library (Marcel & Rodriguez, 2010), except for the crop and resize that represent area ratios. For the Gaussian blur transformation we use alternatively σ, the scaling factor in the exponential, or the kernel size kb (in torchvision kb = (σ − 0.35)/0.15). The “Random” transformation is the one used to\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Illustration of all transformations evaluated in Tab. 1.\n\nIdentity\n\nContrast 0.5\n\nContrast 2.0\n\nBrightness 0.5\n\nBrightness 2.0\n\nHue 0.2\n\nBlur 2.0\n\nJPEG 50\n\nRotation 25\n\nRotation 90\n\nCrop 0.5\n\nResize 0.5\n\nMeme\n\nRandom\n\ndevelop the 50k query set. A series of simple 1-4 AugLy transformations are picked at random, with skewed probability for a higher number. Among the possible transformations, there are pixel-level, geometric ones, as well as embedding the image as a screenshot of a social network GUI.\n\nC MORE EXPERIMENTAL RESULTS\n\nC.1 DETAILED METRICS ON DIFFERENT IMAGE TRANSFORMATIONS\n\nOn Fig. 8, we evaluate the average R@1 over the 10k images from the reference dataset. The experimental setup is the same as for Tab. 1 but a higher number of transformation parameters\n\nFigure 8: Average R@1 comparison between active and passive indexing with IVF-PQ.\n\n16\n\n500500.00.20.40.60.81.0Rotation0.500.250.000.250.500.00.20.40.60.81.0Hue0.20.40.60.81.00.00.20.40.60.81.0Resize0.51.01.50.00.20.40.60.81.0Brightness204060801000.00.20.40.60.81.0Jpeg0.20.40.60.81.00.00.20.40.60.81.0Center crop1020300.00.20.40.60.81.0Blur0.51.01.50.00.20.40.60.81.0ContrastPassiveActivePublished as a conference paper at ICLR 2023\n\nare evaluated. As expected, the higher the strength of the transformation, the lower the retrieval performance is. The decrease in performance is significantly reduced with activated images.\n\nC.2 ADDITIONAL ABLATIONS\n\nSpeeding-up the optimization. In our experiments, the optimization is done using 10 iterations of gradient descent, which takes approximately 40ms/image. If the indexation time is important (often, this is not the case and only the search time is), it can be reduced at the cost of some accuracy.\n\nWe activated 10k reference images, with the same IVF-PQ indexed presented in Sec. 5.2 with only one step of gradient descent with a higher learning rate. Activation times are computed on average. The R@1 results in Tab. 5 indicate that the speed-up in the image optimization has a small cost in retrieval accuracy. Specifically, it reduces the R@1 for unedited images. The reason is that the learning rate is too high: it can cause the representation to be pushed too far and to leave the indexing cell. This is why a higher number number of steps and a lower learning rate are used in practice. If activation time is a bottleneck, it can however be useful to use less optimization steps.\n\nTable 5: R@1 for different transformations applied before search, with either 1 step at learning rate 10, or 10 steps at learning rate 1. Results are averaged on 10k images.\n\nActivation\n\nContr.0.5\n\nContr.2.0\n\nBright.0.5\n\nBright.2.0\n\nIdentity\n\nHue0.2\n\nBlur2.0\n\n50 JPEG\n\nRot.25\n\nRot.90\n\nCrop0.5\n\nResi.0.5\n\nMeme\n\nRandom\n\nAvg.\n\nPassive 1.00 0.73 0.39 0.73 Adam,lr=1 - 10 steps 39.8 ms/img 1.00 1.00 0.96 1.00 0.99 0.99 0.92 0.99 Adam,lr=10 - 1 step\n\n4.3 ms/img\n\n-\n\n0.28 0.62 0.48 0.72 0.07 0.14 0.14 0.72 0.14 0.13 0.45 0.92 1.00 0.96 0.99 0.10 0.50 0.29 1.00 0.43 0.32 0.75 0.84 0.99 0.95 0.99 0.10 0.39 0.25 0.99 0.36 0.27 0.72\n\nData augmentation at indexing time and EoT. Expectation over Transformations (Athalye et al., 2018) was originally designed to create adversarial attacks robust to a set of image transformations. We follow a similar approach to improve robustness of the marked image against a set of augmentations T . At each optimization step, we randomly sample A augmentations {ti}A i=1 in T and consider the average loss: Lf = (cid:80)A i=1 L(I, ti; Io)/A. In our experiments, T encompasses rotations, Gaussian blurs, color jitters and a differentiable approximation of the JPEG compression Shin & Song (2017). A is set to 8 and we always take the un-augmented image in the chosen set of augmentations.\n\nWe activated 10k reference images, with the same IVF-PQ as Sec. 5.2 with or without using EoT. Table 6 shows the average R@1 performance over the images submitted to different transformations before search. EoT brings a small improvement, specifically on transformations where base performance is low (e.g. rotation or crops here). However, it comes at a higher computational cost since each gradient descent iteration needs A passes through the network, and since fewer images can be jointly activated due to GPU memory limitations (we need to store and back-propagate through A transformations for every image). If the time needed to index or activate an image is not a bottleneck, using EoT can therefore be useful. Otherwise, it is not worth the computational cost.\n\nTable 6: R@1 for different transformations applied before search, with or without EoT when activating the images. Results are averaged on 10k images.\n\nActivation\n\nContr.0.5\n\nContr.2.0\n\nBright.0.5\n\nBright.2.0\n\nIdentity\n\nHue0.2\n\nBlur2.0\n\n50 JPEG\n\nRot.25\n\nRot.90\n\nCrop\n\n0.5\n\nResi.0.5\n\nMeme\n\nRandom\n\nAvg.\n\nWithout EOT 40 ms With EOT\n\n1.00 1.00 0.96 1.00 870 ms 1.00 1.00 0.95 1.00\n\n0.92 1.00 0.96 0.99 0.10 0.50 0.29 1.00 0.43 0.32 0.75 0.92 1.00 0.95 0.99 0.14 0.64 0.33 1.00 0.45 0.33 0.76\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nD ACTIVE INDEXING VS. WATERMARKING\n\nDiscussion. Watermarking and active indexing both modify images for tracing and authentication, however there are significant differences between them. Watermarking embeds arbitrary information into the image. The information can be a message, a copyright, a user ID, etc. In contrast, active indexing modifies it to improve the efficiency of the search engine. Watermarking also focuses on the control over the False Positive Rate of copyright detection, i.e. a bound on the probability that a random image has the same message as the watermarked one (up to a certain distance).\n\nAlthough watermarking considers different settings than indexing methods, it could also be leveraged to facilitate the re-identification of near-duplicate images. In this supplemental section, we consider it to address a use-case similar to the one we address in this paper with our active indexing approach. In this scenario, the watermark encoder embeds binary identifiers into database images. The decoded identifier is then directly mapped to the image (as the index of a list of images).\n\nExperimental setup. marking techniques based on deep learning.\n\nIn the rest of the section, we compare active indexing against recent water-\n\n• For indexing, we use the same setting as in Sec. 5.1 (IVF-PQ index with 1M reference images).\n\nWhen searching for an image, we look up the closest neighbor with the help of the index.\n\n• For watermarking, we encode 20-bit messages into images, which allows to represent 210 ≈ 106 images (the number of reference images). When searching for an image, we use the watermark decoder to get back an identifier and the corresponding image in the database.\n\nLike before, we use R@1 as evaluation metric. For indexing, it corresponds to the accuracy of the top-1 search result. For watermarking, the R@1 also corresponds to the word accuracy of the decoding, that is the proportion of images where the message is perfectly decoded. Indeed, with 20-bit encoding almost all messages have an associated image in the reference set, so an error on a single bit causes a mis-identification (there is no error correction1).\n\nWe use two state-of-the-art watermarking methods based on deep learning: SSL Watermarking (Fernandez et al., 2022), which also uses an adversarial-like optimization to embed messages, and HiDDeN (Zhu et al., 2018), which encodes and decodes messages thanks to Conv-BN-ReLU networks. The only difference with the original methods is that their perturbation δ is modulated by the handcrafted perceptual attenuation model presented in App. A. This approximately gives the same image quality, thereby allowing for a direct comparison between active indexing and watermarking.\n\nResults. Tab. 7 compares the R@1 when different transformations are applied before search or decoding. Our active indexing method is overall the best by a large margin. For some transformations, watermarking methods are not as effective as passive indexing, yet for some others, like crops for HiDDeN, the watermarks are more robust.\n\nTable 7: R@1 for different transformations applied before search, when using either watermarking or active indexing. Results are averaged on 1k images. Best result is in bold and second best in italic.\n\nContr.0.5\n\nContr.2.0\n\nBright.0.5\n\nBright.2.0\n\nIdentity\n\nHue0.2\n\nBlur2.0\n\n50 JPEG\n\nRot.25\n\nRot.90\n\nCrop\n\n0.5\n\nResi.0.5\n\nMeme\n\nRandom\n\nAvg.\n\nPassive indexing Active indexing (ours)\n\n1.00 0.73 0.39 0.73 1.00 1.00 0.96 1.00\n\n0.28 0.62 0.48 0.72 0.07 0.14 0.14 0.72 0.14 0.13 0.45 0.92 1.00 0.96 0.99 0.10 0.50 0.29 1.00 0.43 0.32 0.75\n\nSSL Watermarking (Fernandez et al., 2022) HiDDeN2 (Zhu et al., 2018)\n\n1.00 0.98 0.53 0.98\n\n0.63 0.85 0.13 0.00 0.00 0.15 0.11 0.00 0.46 0.07 0.42\n\n0.94 0.87 0.36 0.85\n\n0.55 0.00 0.81 0.00 0.00 0.00 0.92 0.44 0.77 0.16 0.48\n\n1In order to provide error correction capabilities, one needs longer messages. This makes it more difficult to insert bits: in our experiments, with 64 bits we observe a drastic increase of the watermarking bit error rate. 2Our implementation. As reported in other papers from the literature, results of the original paper are hard to reproduce. Therefore to make it work better, our model is trained on higher resolution images (224×224), with a payload of 20-bits, instead of 30 bits embedded into 128×128. Afterwards, the same network is used on images of arbitrary resolutions, to predict the image distortion which is later rescaled as in Eq. (8). In this setting the watermark can not always be inserted (6% failure).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nE MORE QUALITATIVE RESULTS\n\nFigure 10 gives more examples of activated images from the DISC dataset, using the same parameters as in Sec. 5.2. The perturbation is very hard to notice (if not invisible), even in flat areas of the images because the perceptual model focuses on textures. We also see that the perturbation forms a regular pattern. This is due to the image (bilinear) resize that happens before feature extraction.\n\nFigure 9 gives example of an image activated at several values of perturbation strength α of Eq. (8) (for instance, for α = 20 the image has PSNR 27dB and for α = 1 the image has PSNR 49dB). The higher the α, the more visible the perturbation induced by the activation is. Nevertheless, even with low PSNR values (< 35dB), it is hard to notice if an image is activated or not.\n\nFigure 9: Example of one activated image at different levels of α.\n\nFigure 10: Example of activated images for α = 3.0. (Left) original images, (Middle) activated images, (Right) pixel-wise difference.\n\n19\n\nα=20.0, PSNR=27dBα=15.0, PSNR=29dBα=11.3, PSNR=31dBα=8.43, PSNR=34dBα=6.32, PSNR=36dBα=4.74, PSNR=39dBα=3.55, PSNR=41dBα=2.66, PSNR=43dBα=2.00, PSNR=46dBα=1.50, PSNR=49dBPublished as a conference paper at ICLR 2023\n\nFigure 10: Example of activated images for α = 3.0. (Left) original images, (Middle) activated images, (Right) pixel-wise difference. Images are R000005.jpg, R000045.jpg, R000076.jpg, R000172.jpg and R000396.jpg.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper addresses the problem of image copy detection through image retrieval and proposes a method that optimizes jointly both representation learning and approximate similarity search. Large databases rely on indexes for approximate and efficient search, but the challenge comes when copies are significantly modified and then its representation switches indexing partitions and the closest neighbour cannot be returned. The authors address this problem by modifying the image in a way that its representation is pushed towards the center of the indexing partition, giving more \"room\" for edited images to still fall in the same partition as the original copy. This is done by minimizing and indextation loss back to the image pixels, assuming that the image can be modified before its release.\n\n# Strength And Weaknesses\n\nPros:\n- The paper is very well written and very well motivated by pointing out the challenges that indexing faces regarding copy detection and proposing a solution for it.\n- I really like the main idea proposed in this paper of imperceptibly modifying the image in order to push its represnation to the center of the indexing partition to make it more indexing friendly.\n- The paper contains a thorough experimental analaysis that justifies the authors claims and shows that their method significantly improves retrieval performance on a number of neural extractors and indexing structures\n\nCons:\n- The fact that the image has to be modified before its release is a big limitation since it cannot be applied to already existing databases where images have already been released. However, it can still be applied to new images released in the same database, benefiting all future releases.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very well written and easy to follow and the idea of active indexing is novel. The authors promised to release the implementation, so the experiments should be reproducible.\n\n# Summary Of The Review\n\nThe paper is well written, the idea is both interesting and novel, and the experimental analysis is extensive and thorough. The method has a few limitations (not only the adversial attacks discussed by the authors but the fact that images need to be activated before its release) but they do not have a large impact on my positive opinion towards the paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTASK-CUSTOMIZED MASKED AUTOENCODER VIA MIXTURE OF CLUSTER-CONDITIONAL EXPERTS\n\nZhili Liu1,2, Kai Chen1, Jianhua Han2, Lanqing Hong2, Hang Xu2, Zhenguo Li2, James T. Kwok 1 1 Department of Computer Science and Engineering, Hong Kong University of Science and Technology 2 Huawei Noah’s Ark Lab {zhili.liu, kai.chen}@connect.ust.hk, {hanjianhua4, honglanqing, xu.hang, li.zhenguo}@huawei.com jamesk@cse.ust.hk\n\nABSTRACT\n\nMasked Autoencoder (MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE’s scalability. To address this issue, we propose a novel MAEbased pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pretrained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation.\n\n1\n\nINTRODUCTION\n\nSelf-supervised learning (SSL), which learns effective transferable representations without human annotations, has become a prevailing model pre-training paradigm (He et al., 2020; Chen et al., 2021a; Bao et al., 2022). Currently, the most prevalent SSL method is the Masked Autoencoder (MAE) (He et al., 2022), which constructs supervision signals from raw image data by masking random input patches and then reconstructing the missing pixels. This simple strategy has proved efficient in the training of large-scale models. For example, ViT (Dosovitskiy et al., 2021) shows impressive performance on popular benchmarks such as the ImageNet 1 (Deng et al., 2009). However, does MAE really scale well for various downstream tasks (Deng et al., 2009; Lin et al., 2014; Zhou et al., 2019; Han et al., 2021; Li et al., 2022a)?\n\nPreliminary studies (in Section 3.1) show that the MAE indeed suffers from negative transfer (Liu et al., 2022) when transferring to downstream tasks with very different semantics. Figure 1(a) shows that on 9 of 11 downstream tasks, an MAE pre-trained on the full ImageNet data is outperformed by the one that is pre-trained on only the semantically relevant data subsets. Hence, using pre-training data that are semantically irrelevant can hurt transfer performance.\n\nThe above observation motivates the need for task-customized pre-training. A promising model for this is the Mixture of Experts (MoE) (Shazeer et al., 2017; Riquelme et al., 2021), which uses a multi-expert architecture to provide customized models for different input tokens. However, unlike supervised pre-training, self-supervised learning lacks semantic labels, and thus the experts differ more on low-level information than semantics, referring to Figure 1(b). Experiments in Section 4.2 show that a naive adoption of MoE to the MAE has inferior performance. Since various downstream tasks contain different semantics, semantic-related experts may be preferred.\n\n1We refer to ImageNet-1K as ImageNet if not specified in this paper.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) Negative transfer phenomenon on MAE.\n\n(b) Problem with TokenMoE.\n\nFigure 1: (a) Transfer performance of MAEs pre-trained on Split-A (blue), Split-B (red) and full ImageNet data (white). Only two of the eleven downstream tasks benefit from using the full ImageNet data for pre-training (more details in Section 3.1). (b) TokenMoE uses pixel RGB values as reconstruction targets. Thus, tokens with similar pixel values tend to be routed to the same expert, leading to two types of mistakes: (i) same semantics but routed to different experts, (ii) different semantics but routed to the same expert.\n\nIn this paper, we propose the Mixture of Cluster-conditional Expert (MoCE), a novel paradigm to achieve task-customized self-supervised pre-training by data clustering and explicitly training each expert with images of similar semantics. The MoCE procedure has three stages. First, we cluster the whole dataset by using a pre-trained, dense MAE model. We then construct the MoCE with a multi-expert structure. Each expert is trained using clusters selected by routing tokens based on cluster embedding (instead of token embedding). To stabilize training and enhance confidence of the gate results, a regularization loss is proposed. Finally, with the arrival of a downstream task, we propose a search procedure to select the closest cluster. Empirically, the proposed MoCE shows superior performance over MAE on a collection of 11 downstream tasks. Besides, one can use only a MoCE sub-model on deployment, thus saving inference time and model capacity.\n\nTo summarize, our main contributions are:\n\n1. We systematically analyze the negative transfer phenomenon of MAE, and show that naively adopting the MoE to MAE cannot improve transfer performance of downstream tasks.\n\n2. We propose the MoCE, which trains each expert with semantics-aware clusters so that\n\nsimilar clusters can be routed to the same expert.\n\n3. We demonstrate effectiveness of the proposed MoCE on a collection of 11 downstream tasks, and achieve up to 2.45% performance improvement in Top-1 accuracy. State-ofthe-art self-supervised results are also achieved on the detection and segmentation tasks. To the best of our knowledge, this is the first work that achieves state-of-the-art transfer performance by training vision MoE models with ImageNet under the SSL setting.\n\n2 RELATED WORK\n\nSelf-supervised Learning. Previous works mainly focus on the design of pretext tasks with image transformations (Doersch et al., 2015; Gidaris et al., 2018), inpainting (Pathak et al., 2016), colorization (Zhang et al., 2016), contrastive learning (Chen et al., 2020; He et al., 2020; Grill et al., 2020; Caron et al., 2020; Radford et al., 2021b; Yao et al., 2022b), and for specific downstream tasks (Wang et al., 2020; Xie et al., 2020; 2021a; Chen et al., 2021a; Yao et al., 2022a). Motivated by the design of BERT (Devlin et al., 2018), masked image modeling (MIM) is recently proposed to learn by reconstructing masked images. BEiT (Bao et al., 2022) is the pioneering work that predicts visual tokens generated by a pre-trained tokenizor (Radford et al., 2021a). SimMIM (Xie et al., 2021c) simplifies the framework by directly utilizing the pixel RGB values as reconstruction targets.\n\n2\n\n$LUFUDIW&DUV681'7')ORZHUV)RRG3HWV&&&DOWHFK92&7UDQVIHUDFFXUDF\\0$(6SOLW$0$(6SOLW%0$(IXOOVHW#1, expert 1#1#2Same semantics, Different experts#2, expert 3#3, expert 1#1#3Different semantics, Same expertsPublished as a conference paper at ICLR 2023\n\nMAE (He et al., 2022) proposes an asymmetric encoder-decoder architecture for better training efficiency. MixedAE (Chen et al., 2023) further explores image mixing for object-aware pre-training. In this paper, we will focus on the MAE due to its effectiveness and efficiency.\n\nWhile self-supervised learning methods have achieved improved transfer performance, most of them only provide a unified representation to various downstream tasks. This may suffer from negative transfer as demonstrated in Section 3.1. The work most relevant to ours is SDR (Liu et al., 2022), which trains 256 subnets with 256 disjoint ImageNet subsets simultaneously. However, this paper differs from SDR in three ways: (i) the mapping from subsets to subnets in SDR is randomly selected and fixed during pre-training, while MoCE achieves self-adaptive mapping with cluster-conditional gates; (ii) Progressive training is required in SDR, while MoCE enjoys one-time end-to-end training; (iii) During the transfer process, SDR uses brute force to select the best sub-model, while MoCE reuses the clustering module to achieve more efficient selection.\n\nMixture of Experts. The mixture of experts (MoE) has a long history (Jacobs et al., 1991; Jordan & Jacobs, 1994; Shazeer et al., 2017). Recently, it is considered as an effective tool for model scaleup in natural language processing (Lepikhin et al., 2020; Fedus et al., 2021; Yang et al., 2021; Lewis et al., 2021). With the growing interest of the Vision Transformer (Dosovitskiy et al., 2021; Liu et al., 2021; Wang et al., 2021; Xie et al., 2021b), MoE for vision (Riquelme et al., 2021; Wu et al., 2022) is also explored recently. However, there is still no self-supervised MoE model that can be trained on medium-sized datasets such as the ImageNet-1k.\n\nKudugunta et al. (2021); Ma et al. (2018) regard the MoE as a multi-task learning model, and use it for multi-language translation and recommendation systems, respectively. In this paper, we show that for self-supervised learning on images, an additional clustering component is crucial in the learning of a highly performant MoE model. Moreover, while the downstream tasks should follow the pre-training task in (Kudugunta et al., 2021; Ma et al., 2018), the MoCE can be used with any downstream task due to its unsupervised pre-training. Puigcerver et al. (2020) shares a similar setting with us, but their model is pre-trained in a supervised learning manner. Moreover, their mapping between experts and data is pre-defined and fixed during training, while that for the MoCE is learned dynamically and achieves better performance.\n\nMulti-Task Learning aims to learn a model that is appropriate for multiple tasks. Hard-parameter sharing, which uses a shared backbone with multi-heads for the different tasks, has been shown to be effective on time series, language and graph data (Liu et al., 2019; Hu et al., 2019; McDermott et al., 2021). Gao et al. (2021) claims that the network design may further benefit from the use of task relationships, and trains masks for different tasks. However, they require the task information be available during model training, which is not possible for downstream tasks in SSL pre-training.\n\n3 PROPOSED METHOD\n\nIn this section, we first empirically demonstrate the negative transfer phenomenon in MAE (Section 3.1). We then discuss the limitations of adopting TokenMoE (Riquelme et al., 2021) with MAE (Section 3.2), and propose the Mixture of Cluster-conditional Experts (MoCE), a novel paradigm achieving customized pre-training for various downstream tasks (Section 3.3).\n\n3.1 NEGATIVE TRANSFER IN MASKED AUTOENCODER\n\nIn this section, we evaluate the transfer performance of MAE models pre-trained with data of different semantics on various downstream tasks. As in (Huh et al., 2016; Liu et al., 2022), we first split the ImageNet data into two disjoint subsets, Split-A and Split-B, based on the labels’ semantic dissimilarities in the WordNet tree (Miller, 1998). Split-A mainly contains inanimate objects (such as cars and airplanes), while Split-B primarily involves organisms (such as plants and animals). We then pre-train MAEs on Split-A, Split-B and the full ImageNet without data annotation, and evaluate the three resulting models on 11 downstream tasks. See more implementation details in Section 4.1.\n\nAs shown in Figure 1(a), the MAE pre-trained with Split-A performs best on Aircraft, Cars, SUN397 and DTD, while the MAE pre-trained with Split-B performs best on Flowers, Food, Pets, Cifar10 and Cifar-100. Only two of the eleven tasks (Caltech and VOC) benefit from using the full\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Model design comparison between (a) TokenMoE (Riquelme et al., 2021) and (b) MoCE. Both methods utilize the multi-expert architecture with the main difference about the input of the gating network. MoCE adopts the corresponding cluster embedding of the current token as in Eqn. 4, instead of the token embedding in Eqn. 3.2. Therefore, each expert can be trained by semantically similar images to alleviate the negative transfer phenomenon.\n\nImageNet data. This suggests that for tasks whose semantics are close to inanimate objects, adding pre-training data from Split-B is not useful, and vice versa for tasks whose semantics are close to organisms. To conclude, the introduction of semantically irrelevant pre-training data may impede transfer performance for downstream tasks. This negative transfer phenomenon motivates us to develop an efficient and automatic paradigm for task-customized pre-training.\n\n3.2 EXPLORING TOKENMOE TO MASKED AUTOENCODER\n\nOverview of TokenMoE. TokenMoE (Riquelme et al., 2021) is a successful customized supervised pre-training model built upon the ViT (Dosovitskiy et al., 2021), which mainly consists of transformer blocks with alternating multi-head self-attention (MSA) and multi-layer perceptron (MLP). Specifically, the TokenMoE converts several transformer blocks to Mixture of Expert (MoE) blocks by expanding the MLP layer N times, each of them is considered as an expert (denoted as Ei(·), i = 1, 2, . . . , N ). Conditional computation on the N experts is controlled by a gate, which is a linear layer whose input is the token embedding x, and the output is the top-K probabilities on the experts: G(x) = T opK(σ(Wgx + ε)), where K is the number of experts to be activated, Wg is the gate parameter, σ is the softmax function, and ε ∼ N (0, 1 N ). T opK(·) returns the K largest entries of σ(Wgx + ε) unchanged but set the others to zero. Thus, each token is routed to its corresponding experts. The final output is represented as\n\ny =\n\nN (cid:88)\n\ni=1\n\n[G(x)]iEi(x).\n\n(1)\n\nAs in (Riquelme et al., 2021), importance loss and load loss are also used to enforce a balanced use of the experts. Unless otherwise specified, we set K = 1 and N = 8 in all our experiments.\n\nLimitation of TokenMoE. As will be shown in the experimental results (Table 3), naively adopting TokenMoE to the MAE cannot improve performance, even with intense hyper-parameter tuning and data augmentations (e.g., Repeat Augment (Hoffer et al., 2020) and RandAugment (Cubuk et al., 2020) with larger magnitude). Figure 3(a) shows the routing heatmaps of the pre-trained TokenMoE model. As can be seen, the routing process has little correlation with the ImageNet labels. Moreover, expert 3 is selected most of the time (91.8% of the classes). This degenerates the multi-expert network into a single-expert network. As demonstrated in Figure 1(b), we speculate that this is due to the use of low-level pixel values (instead of semantic class labels in the original TokenMoE) as reconstruction targets. This is also observed in Li et al. (2022b).\n\n3.3 MIXTURE OF CLUSTER-CONDITIONAL EXPERTS\n\nTo address the limitations of TokenMoE, we propose the Mixture of Cluster-conditional Experts (MoCE), which trains each expert in a semantic-aware manner. The procedure consists of data clustering, architecture and gate design, and deployment.\n\n4\n\n(a) TokenMoE(b) MoCEExpert AExpert CGatingNetworkInputPatches / TokensInputGatingNetworkClusteringClusterInfomationExpert AExpert CExpert BClustersExpert BPublished as a conference paper at ICLR 2023\n\nData Clustering. To train each expert semantically, a clustering procedure is first performed to simulate the label partitioning in Section 3.1. With a pre-trained MAE model, we collect all the image features fi’s (normalized to unit length ∥fi∥ = 1), and represent the feature matrix as F = [f1, f2, . . . , fn] ∈ Rd×n, where n is the number of images and d is the dimension of the feature. The learnable cluster centroids are represented as C = [c1, c2, . . . , cm] ∈ Rd×m, (with ∥ci∥ = 1), where m is the desired number of clusters. The assignment of feature to clusters is computed as A = F T C. Following Asano et al. (2019), let Q ∈ Rm×n be the posterior distribution of clustering, whose objective is\n\nmax Q\n\nT r(QT A) + εH(Q)\n\ns.t. Q1n =\n\n1 m\n\n1m, QT 1m =\n\n1 n\n\n1n,\n\n(2)\n\nwhere 1m is the m-dimensional vector of all ones, H is the entropy function, and the constraints force the clustering results to be balanced. Q and C are optimized iteratively. For a given C, Q is solved by the Sinkhorn-Knopp algorithm (Cuturi, 2013); while for a given Q, C is obtained by minimizing the cross entropy between Q and A with SGD. We take the final C and Q as the cluster centroids and clustering assignments, respectively. The implementation details are in Appendix A.1.\n\nArchitecture. The whole network is trained on the full ImageNet data, with each expert trained by images from selected clusters decided by the MoCE gates’ routing results. As on average each data cluster has only a fraction of 1/K of the original sample size, the training time of each expert is also K times shorter than the other parameters with dense modeling (e.g., MSA parameters (Riquelme et al., 2021)), we further adopt a distillation loss Ldistill, which is defined as the l2 distance between the features generated by the whole network and each expert. This loss function can be formulated as\n\nmin θ\n\nm (cid:88)\n\ni=1\n\nLM AE(Di; θi) + Ldistill,\n\n(3)\n\nwhere Di is the ith cluster, θi is the parameter used for training Di, and LM AE(Di; θi) is the reconstruction loss for masked image modeling. θi consists of several experts in different layers, as explained in the following.\n\nGate Design. As in the TokenMoE, we replace several MLP layers in the ViT with layers equipped with MoCE gates. In TokenMoE, routings of the tokens to experts are considered separately. In MoCE, we route tokens from images of the same cluster to the same expert. The MoCE gate output can thus be written as\n\nG(x) = T opK(σ(Wg · C[x] + ε)),\n\n(4)\n\nwhere Wg is the gate parameter, and C[x] is the embedding of the cluster that x belongs to. Empirically, we find that the confidence of G(x) (the max entry) is low and consequently, the mapping between clusters and experts varies a lot during pre-training. Inspired by the importance and load losses (Riquelme et al., 2021), we add the following loss Limbalance to enhance the confidence of the gates. Since it makes G(x) shaper, we call it imbalance loss.\n\nLimbalance = −\n\nn (cid:88)\n\ni=1\n\n(cid:18) std(G(x)i) mean(G(x)i)\n\n(cid:19)2\n\n,\n\n(5)\n\nFor practical implementation, the loss is calculated over the samples in a batch. The imbalance loss penalizes on the negative variance of the gate confidence.\n\nDeployment. On deployment, customized experts are selected from MoCE, and fine-tuned for each downstream task. As shown in Section 3.1, we prefer to use the experts that is pre-trained from data whose semantics is closest to that of the downstream task. This can be obtained by reusing the data clustering module. Specifically, we feed images for the downstream task through the pre-trained MAE model and collect all the image features as Ftask. The assignment of downstream images to the clusters is then computed as Atask = F T taskC. We select the largest cluster with assigned downstream images, and use the corresponding experts (a sub-model of the whole MoCE model) for deployment. In the case when only one expert is activated at each MoCE layer (K = 1), a regular ViT model is needed for downstream fine-tuning, which is much more efficient than MoE.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Transfer accuracy (%) of self-supervised learning models on 11 downstream tasks.\n\nAircraft Caltech Cars C10 C100 DTD Flowers Food Pets SUN VOC\n\nAvg.\n\nResNet-50\n\nBYOL DeepCluster-v2\n\n82.39 78.75\n\n90.12 87.33 96.28 82.15 74.57 90.51 86.33 96.48 82.28 75.43\n\n95.96 96.16\n\n82.13 88.52 64.41 83.97 83.68 90.33 66.68 81.37\n\n84.35 84.36\n\nVision Transformer\n\nSupervised DINO MoCo v3 BEiT MAE\n\nMAE* MoCE (Ours)\n\n76.55 66.50 76.29 53.16 72.38\n\n72.71 78.73\n\n89.98 86.19 96.79 83.96 75.09 91.65 76.37 98.12 86.69 75.73 91.64 85.18 97.99 86.98 72.64 79.02 68.11 94.34 73.54 68.04 90.47 83.51 95.69 68.40 75.48\n\n91.24 84.47 96.15 77.33 75.05 90.61 88.56 97.79 84.68 74.04\n\n93.94 96.40 95.33 91.33 96.10\n\n96.25 96.94\n\n85.17 92.54 64.54 87.22 93.77 93.97 59.33 86.62 83.94 92.35 65.54 84.21 79.59 84.02 56.13 65.65 79.98 92.35 62.43 84.79\n\n84.72 84.10 84.74 73.90 81.96\n\n80.49 92.78 62.46 85.02 86.24 93.07 65.05 85.26 85.54+2.45\n\n83.09\n\nTable 2: Transfer accuracy (%) on detection and segmentation.\n\nMethod\n\nSupervised DINO MoCo v3 BEiT MAE\n\nMoCE\n\nADE20K mIoU\n\n46.9 46.9 46.8 45.6 48.1\n\n48.3\n\nCOCO\n\nAPbb APbb\n\n50 APbb\n\n75 APmk APmk\n\n50\n\n48.8 49.5 47.2 40.8 50.6\n\n51.1\n\n68.7 69.1 66.9 59.4 69.4\n\n69.8\n\n52.7 53.6 50.8 44.1 55.0\n\n55.4\n\n42.5 42.9 41.1 36.0 43.8\n\n44.2\n\n65.9 66.0 63.6 56.8 66.6\n\n67.0\n\nAPmk\n\n75\n\n45.5 46.3 44.1 38.2 47.5\n\n48.1\n\n4 EXPERIMENTS\n\nIn this section, we first introduce the setup of pre-training and fine-tuning stage of MoCE in Sec. 4.1. Then we demonstrate the effectiveness of MoCE by evaluating the pre-trained models on a collection of 11 downstream tasks with detailed analysis of our MoCE superior to vanilla MAE and TokenMoE in Sec. 4.2. Finally we take ablation studies on the key components of MoCE in Sec. 4.3.\n\n4.1 SETUP\n\nFor all experiments, we replace two MLP layers with MoCE layers in the original ViT-B (Dosovitskiy et al., 2021). Following Wu et al. (2022), layers with the greatest gradient magnitude are selected (which are the last two MLP layers in our experiments). Unless otherwise specified, the number of experts is 8 and the number of clusters is 256. Our model utilizes the officially released 1600-epoch pre-trained MAE model2 and continues to train for an extra 200 epochs. Each expert is initialized by the corresponding dense model with a small weight perturbation. The training procedure mainly follows that of MAE, except that we multiply the base learning rate by 0.1. All regularization loss weight is set to 0.01 by default.\n\nTo ensure a fair comparison with the vision transformer on downstream classification tasks, we mainly follow the hyper-parameter settings in (Dosovitskiy et al., 2021; Riquelme et al., 2021) and the benchmark settings in (Ericsson et al., 2021). The proposed model is compared with various self-supervised models, including DINO (Caron et al., 2021), MoCo v3 (Chen et al., 2021b), BEiT (Bao et al., 2022), and the highly-performant ResNet-50 models of BYOL (Grill et al., 2020) and DeepCluster-v2 (Caron et al., 2018). We also compare with the supervised pre-trained model DeiT (Touvron et al., 2021). To make a fair comparison of training time, we continue to train a 1600-epoch pre-trained MAE for 200 epochs with total ImageNet as our baseline, and is denoted as MAE* in Table 1. For detection and segmentation tasks, following Bao et al. (2022), we perform experiments on ADE20K (Zhou et al., 2019) and COCO (Lin et al., 2014). We utilize the officially released checkpoints for all baselines. Details are in Appendix A.5.\n\n2https://github.com/facebookresearch/mae\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Transfer accuracy of MAE, TokenMoE, SDR and MoCE. SDR(ViT) is our reimplementation of SDR under ViT. We observe that TokenMoE cannot outperform vanilla MAE, while SDR(ViT) achieves better performance, which is further outperformed by MoCE.\n\nAircraft Caltech Cars C10 C100 DTD Flowers Food Pets SUN VOC Avg.\n\nMAE* TokenMoE\n\nSDR SDR(ViT) MoCE\n\n72.71 70.51\n\n75.77 76.57 78.73\n\n91.24 84.47 96.15 77.33 75.05 89.70 81.40 95.18 76.44 73.67\n\n96.25 80.49 92.78 62.46 85.02 83.09 95.09 77.45 90.71 61.12 80.15 81.04\n\n89.73 86.65 95.31 83.60 73.62 90.04 86.95 96.92 81.42 73.09 90.61 88.56 97.79 84.68 74.04\n\n95.53 84.77 91.25 64.64 83.51 84.03 96.14 82.90 92.65 64.40 85.37 84.22 96.94 86.24 93.07 65.05 85.26 85.54\n\n4.2 RESULTS\n\nTransfer Results. The classification transfer performance of various self-supervised models are shown in Table 1. As can be seen, MoCE achieves a 2.45% improvement over MAE* and reaches the state-of-the-art averaged accuracy, demonstrating the effectiveness of the task-customized pretraining paradigm. On fine-grained datasets such as Aircraft, Cars and Food, MoCE outperforms the baseline model by a large margin. This is because these fine-grained tasks are similar to only a subset of the pre-training dataset. Hence, MoCE can alleviate negative transfer by using the model that is trained by the cluster most similar to the particular downstream task. On the other hand, MoCE shows only limited improvement on tasks such as Caltech, Cifar-10 and VOC. These tasks are more general and contain images covering the various semantics in the pre-training dataset, and thus negative transfer does not exist.\n\nTable 2 shows the transfer performance on the detection and segmentation tasks. As can be seen, MoCE outperforms MAE and the other baselines (including the supervised one), and achieves stateof-the-art results.\n\nComparison between MoCE, TokenMoE, MAE and SDR. In this experiment, we compare MoCE with the following models: (i) MAE, (ii) TokenMoE, (iii) SDR (Liu et al., 2022), a taskcustomized model that aims at alleviating negative transfer, and (iv) SDR(ViT), which re-implements SDR with the ViT architecture. Table 3 shows the transfer accuracy on 11 downstream tasks. As can be seen, TokenMoE performs even worse than MAE, suggesting that naively adopting MoE to MAE is not desirable. Both MoCE and SDR(ViT) outperform MAE, demonstrating the effectiveness of task-customized methods for alleviating negative transfer. MoCE further outperforms SDR(ViT), indicating the importance of self-adaptive routing.\n\nFigure 3(d) shows the peak signal-to-noise ratio (PSNR) (Sara et al., 2019), which reflects the generation quality of these autoencoder models. MoCE exhibits improvement over TokenMoE and MAE on most datasets. We also provide the comparisons in the case of a fair parameter count, large architectures, and training from scratch in the Appendix A.2, A.3 and A.4, respectively.\n\nAnalysis on experts. Figure 3(a) and Figure 3(c) show the routing heatmaps for TokenMoE and MoCE, respectively. As can be seen, routing of the TokenMoE experts has little correlation with semantics. On the other hand, each MoCE expert is trained by several classes, showing a more balanced assignment of images to experts. This verifies that the improvement of MoCE is due to more effective learning of the experts. Moreover, notice that the importance loss and load balance loss (Riquelme et al., 2021) are applied and indeed work as “expected” because they are only applied with respect to patch tokens instead of semantic classes. On the other hand, MoCE can balance the experts both at the token level and semantic level.\n\nFigure 3(b) shows example pre-training samples for 3 random MoCE experts. Note that expert 1 is mostly trained by images containing clothes, experts 2 is pre-trained mostly by bird images, while expert 3 is pre-trained mostly by dog images.\n\nNext, we show that each expert is trained by samples with similar semantics. Following (Mikolov et al., 2013), we select the label set used by each expert, and then compute the l2 distances between CLIP embeddings (Radford et al., 2021a) of labels used by the same expert and by different experts. The average distance between labels used by the same expert is 0.84, while that between labels\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Routing heatmap for TokenMoE experts.\n\n(b) Examples pre-training samples for expert 1 (top), expert 2 (middle), and expert 3 (bottom).\n\n(c) Routing heatmap for MoCE experts.\n\n(d) Relative PSNR improvement over MAE.\n\n(a),(c): Routing heatmaps for experts in TokenMoE and MoCE. The x-axis is the expert Figure 3: ID, and the y-axis is the ImageNet semantic label ID. Darker green means a higher proportion of tokens belonging to the corresponding class are allocated to the expert. The label is sorted differently in each figure to make it readable. (b): Example samples from the pre-training dataset of 3 MoCE experts. (d): Relative PSNR improvement of TokenMoE and MoCE over MAE for each downstream task.\n\nused by different experts is 0.92, indicating that the MoCE gate automatically aggregates labels with similar semantics to each expert, thus benefiting downstream transfer.\n\nTraining and testing efficiency. Table 4 compares the efficiencies of MAE, TokenMoE and MoCE during training and testing. As can be seen, all of them have similar FLOPs. However, TokenMoE needs to use the whole model during both training and testing, while MoCE only needs to use a single expert and thus halves the required number of parameters when testing. In addition, the training and testing speeds are improved by respectively 18% and 37%, which is attributed to the reduction of token shuffle operations as tokens in the same image do not need to be split and are dispatched to the same expert, significantly reducing the communication overhead.\n\n4.3 ABLATION\n\nSearch method. When a downstream task arrives, it is expensive to fine-tune all experts to choose the best one. To find the task-customized expert (K = 1), we compare the method proposed in Section 3.3 with (i) early stop, (ii) KNN (Liu et al., 2022), (iii) LogME (You et al., 2021). The experiment is performed on the task with the most samples (Food), the task with the least samples (Flowers), and the one with a medium number of samples (Aircraft). For comparison, we additionally show the performance of the best and worst experts based on an exhaustive search. As can be seen in Table 5, MoCE performs stably among different sizes of the dataset, and the search cost is negligible as we only need to infer the downstream task once and feed it to the clustering module. This illustrates another advantage of combining clustering and pre-training in a single paradigm.\n\nMoCE Architectures. In this experiment, we study the different architecture hyper-parameters of MoCE in three aspects. First, we vary the number of experts in each MoCE layer. As can be seen from Table 6, using more experts leads to consistent improvement on the accuracy.\n\nNext, we vary the location of the MoCE layers. As mentioned in Section 4.1, we select the MoCE layers based on the gradient magnitudes. In the experiments, MoCE selects the 11th and 12th MLP layers. On the other hand, TokenMoE chooses the last 2 even-numbered (i.e., 10th and 12th) MLP\n\n8\n\nAircraftCaltechCarsCifar10Cifar100DTDFlowersFoodPetsSUN397voc200721012Relative PSNR improvementMAEtokenMoEMoCEPublished as a conference paper at ICLR 2023\n\nTable 4: Efficiency during training (top) and testing (bottom).\n\nTable 5: The search cost (in GPU hours) for different expert search algorithms.\n\nMAE TokenMoE MoCE\n\nAircraft Flowers Food GPU hours\n\nParams (M) 111.91 FLOPs (G) Speed↑\n\n9.80 1.41x\n\n# Params (M) 85.88 16.88 1.37x\n\nFLOPs (G) Speed↑\n\n178.03 9.81 1x\n\n152.00 16.88 1x\n\n178.03 9.81 1.18x\n\n85.88 16.88 1.37x\n\nBest Worst\n\n79.92 69.84\n\n97.96 86.24 94.97 81.51\n\nEarly stop 77.00 71.40 KNN LogME 73.84 78.73 MoCE\n\n96.83 85.33 95.10 83.32 96.54 85.11 96.94 86.24\n\n288 288\n\n144 5\n5 1\n\nTable 6: Accuracies with different numbers of experts in a MoCE layer. (default setting used is in bold).\n\nTable 7: Accuracies with different numbers and locations of the MoCE layers (default setting used is in bold).\n\nTable 8: Accuracies with different numbers of clusters (default setting used is in bold).\n\n# experts Acc (%)\n\n# MoCE layers Acc (%)\n\n1 2\n4 8\n\n83.09 83.01 84.22 85.54\n\n1 2 (10th & 12th) 2 (11th & 12th) 4\n\n83.09 83.08 85.54 85.59\n\n# clusters Acc (%)\n\n16 64 256 512\n\n82.00 84.02 85.54 85.33\n\nlayers. Furthermore, we exhibit the performance with only 1 and 4 MoCE layers, which are also selected based on the gradient magnitudes. As shown in Table 7, we notice that it is essential to choose the right layer to be MoCE layer. Adding more MoCE layers shows little improvement.\n\nWe also train MoCE with different numbers of clusters. As shown in Table 8, the accuracy increases up to 256 clusters, and then begins to drop. We hypothesize that with a moderate number of clusters, MoCE can produce a variety of task-customized models. With even more clusters, the number of experts become the bottleneck and performance starts to saturate.\n\n5 CONCLUSION\n\nIn this work, we first show that the negative transfer phenomenon exists in the prevailing selfsupervised learning method MAE through extensive experiments. It will impede the scalability of MAE as more pre-training data may instead degenerate the downstream performance. In order to tackle the problem, we introduce Mixture of Expert to MAE as the multi-experts design can equip MAE with different ability that aids transfer. However, different from supervised pre-training, TokenMoE suffers from the fact that the gate shows no correlation to the semantics and the transfer ability is not improved. Based on this, we propose MoCE to explicitly train each expert with different clusters through the MoCE gate design and several losses to stabilize the training process. A search algorithm for selecting the best model for transfer is also proposed based on the clustering priors. Extensive experiments show that MoCE trains each expert with meaningful semantics and achieves state-of-the-art transfer performance on a collection of 11 downstream tasks and both detection and segmentation tasks. It is the first work that successfully trains a self-supervised learning MoE model on ImageNet only. We hope such a design will motivate more research on the self-supervised MoE models.\n\nACKNOWLEDGMENTS\n\nWe gratefully acknowledge the support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous\n\nclustering and representation learning. Preprint arXiv:1911.05371, 2019.\n\nYutong Bai, Zeyu Wang, Junfei Xiao, Chen Wei, Huiyu Wang, Alan Yuille, Yuyin Zhou, and Cihang Xie. Masked autoencoders enable efficient knowledge distillers. Preprint arXiv:2208.122561, 2022.\n\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.\n\nIn ICLR, 2022.\n\nZhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: delving into high quality object detection.\n\nIn CVPR, 2018.\n\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-\n\npervised learning of visual features. In ECCV, pp. 132–149, 2018.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 33:9912– 9924, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pp. 9650– 9660, 2021.\n\nKai Chen, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving. In ICCV, pp. 7546–7554, 2021a.\n\nKai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Mixed autoencoder\n\nfor self-supervised visual representation learning. In CVPR, 2023.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, 2020.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\n\ntransformers. In ICCV, pp. 9640–9649, 2021b.\n\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training\n\ntext encoders as discriminators rather than generators. In ICLR, 2020.\n\nEkin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In NeurIPS, volume 33, pp. 18613–18624. Curran Associates, Inc., 2020.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS, 26,\n\n2013.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n\nbidirectional transformers for language understanding. Preprint arXiv:1810.04805, 2018.\n\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\n\ncontext prediction. In ICCV, pp. 1422–1430, 2015.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nLinus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models\n\ntransfer? In CVPR, pp. 5414–5423, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\n\nmodels with simple and efficient sparsity, 2021.\n\nDehong Gao, Wenjing Yang, Huiling Zhou, Yi Wei, Yi Hu, and Hao Wang. Network clustering for\n\nmulti-task learning. Preprint arXiv:2101.09018, 2021.\n\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\n\npredicting image rotations. In ICLR, 2018.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.\n\nJianhua Han, Xiwen Liang, Hang Xu, Kai Chen, Lanqing Hong, Jiageng Mao, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Xiaodan Liang, and Chunjing Xu. Soda10m: A large-scale 2d self/semisupervised object detection dataset for autonomous driving. Preprint arXiv:2106.11118, 2021.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, pp. 9729–9738, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. In CVPR, pp. 16000–16009, 2022.\n\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment\n\nyour batch: Improving generalization through instance repetition. In CVPR, June 2020.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. Preprint arXiv:1905.122651, 2019.\n\nMinyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learn-\n\ning? Preprint arXiv:1608.08614, 2016.\n\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\n\nlocal experts. Neural computation, 3(1):79–87, 1991.\n\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\n\nNeural computation, 6(2):181–214, 1994.\n\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. Preprint arXiv:2110.03742, 2021.\n\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. Preprint arXiv:2006.16668, 2020.\n\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\n\nSimplifying training of large, sparse models. In ICML, pp. 6265–6274. PMLR, 2021.\n\nKaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, et al. Coda: A real-world road corner case dataset for object detection in autonomous driving. Preprint arXiv:2203.07724, 2022a.\n\nXiaotong Li, Yixiao Ge, Kun Yi, Zixuan Hu, Ying Shan, and Ling-Yu Duan. mc-beit: Multi-choice\n\ndiscretization for image bert pre-training. Preprint arXiv:2203.15371, 2022b.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\n\nfor natural language understanding. Preprint arXiv:1901.11504, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp. 10012– 10022, 2021.\n\nZhili Liu, Jianhua Han, Kai Chen, Lanqing Hong, Hang Xu, Chunjing Xu, and Zhenguo Li. Task-\n\ncustomized self-supervised pre-training with scalable dynamic routing. AAAI, 55:65, 2022.\n\nIlya Loshchilov and Frank Hutter.\n\nDecoupled weight decay regularization.\n\nPreprint\n\narXiv:1711.05101, 2017.\n\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1930–1939, 2018.\n\nMatthew McDermott, Bret Nestor, Evan Kim, Wancong Zhang, Anna Goldenberg, Peter Szolovits, and Marzyeh Ghassemi. A comprehensive ehr timeseries pre-training benchmark. In Proceedings of the Conference on Health, Inference, and Learning, pp. 257–278, 2021.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\n\ntations in vector space. Preprint arXiv:1301.3781, 2013.\n\nGeorge A Miller. WordNet: An electronic lexical database. MIT press, 1998.\n\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\n\nencoders: Feature learning by inpainting. In CVPR, 2016.\n\nJoan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, Cedric Renggli, Andr ́e Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models. In ICLR, 2020.\n\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, and J. Clark. Learning transferable visual models from natural language supervision. In ICML, 2021a.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pp. 8748–8763. PMLR, 2021b.\n\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ́e Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. NeurIPS, 34:8583–8595, 2021.\n\nUmme Sara, Morium Akter, and Mohammad Shorif Uddin. Image quality assessment through fsim, ssim, mse and psnr—a comparative study. Journal of Computer and Communications, 7(3):8–18, 2019.\n\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In ICML, volume 139, pp. 10347–10357, July 2021.\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, pp. 568–578, 2021.\n\nXinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning\n\nfor self-supervised visual pre-training. Preprint arXiv:2011.09157, 2020.\n\nLemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu Yuan. Residual\n\nmixture of experts. Preprint arXiv:2204.09636, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nEnze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Zhenguo Li, and Ping Luo. Detco:\n\nUnsupervised contrastive learning for object detection. Preprint arXiv:2102.04803, 2021a.\n\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021b.\n\nZhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. Preprint arXiv:2011.10043, 2020.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. Preprint arXiv:2111.09886, 2021c.\n\nAn Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, et al. M6-t: Exploring sparse expert models and beyond. Preprint arXiv:2105.15082, 2021.\n\nLewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for openworld detection. In NeurIPS, 2022a.\n\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In ICLR, 2022b.\n\nKaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment of\n\npre-trained models for transfer learning. In ICML, pp. 12133–12143. PMLR, 2021.\n\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\n\nSemantic understanding of scenes through the ade20k dataset. In IJCV, 2019.\n\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.\n\nibot:\n\nImage bert pre-training with online tokenizer. Preprint arXiv:2111.07832, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 DETAILS FOR CLUSTERING.\n\nFor data clustering, the features are computed by inferring the pre-train MAE, and the matrix Q and C are solved by the Sinkhorn-Knopp algorithm and SGD optimizer iteratively. For the SinkhornKnopp algorithm, we set the iteration number as 3. The learning rate of SGD is set to 0.1, the momentum is 0.9 and weight decay is set to 0.9 for the sparse assignment of cluster results. We train 10 epochs in total and it costs 3 minutes and 20 seconds on average for a single GPU.\n\nA.2 COMPARISON UNDER FAIR PARAMETER COUNTS.\n\nThe setting used in our work focuses on a fair comparison of FLOPs, referring to Table 4 in the main paper. Since TokenMoE and MoCE always activate only one expert throughout the whole pre-training and fine-tuning procedure, the FLOPs value is maintained close to MAE. Apart from this criterion, we further provide the comparison on equal parameter counts. As shown in Table 9, we train MAE under the same parameter count as the whole model of MoCE, and MoCE still outperforms MAE consistently.\n\nTable 9: Comparison of MAE and MoCE under equal parameter counts. We train MAE with a larger model that shares the same parameter count as the whole model of MoCE.\n\n# Params Aircraft Caltech Cars C10 C100 DTD Flowers Food Pets SUN VOC Avg.\n\nMAE 178.03 MoCE 178.03\n\n74.43 78.73\n\n90.30 85.50 96.90 83.80 74.84 90.61 88.56 97.79 84.68 74.04\n\n96.30 96.94\n\n81.86 92.97 62.98 85.51 84.13 86.24 93.07 65.05 85.26 85.54\n\nA.3 MOCE FOR LARGER ARCHITECTURE.\n\nHere we provide the analysis on the larger architecture(ViT-L, 2.57× larger than ViT-B) to explore the scalability of MoCE. We first demonstrate that negative transfer still exists for larger architecture by training MAE with total ImageNet, Split-A and Split-B following the same setting in Sec. 3.1. As shown in the first three rows of Table. 10, a similar phenomenon is observed that MAE-L trained by Split-A performs better in Aircraft, Cars, DTD and SUN while Split-B in Flowers, Food, and Pets. On the other hand, MoCE-L can still alleviate the problem and therefore transfers better. We believe that the negative transfer phenomenon mainly exists when a common pre-trained model is used for various downstream tasks, due to the inevitable semantic gaps between the pre-training and downstream datasets, rather than the architecture.\n\nTable 10: Comparison of MAE and MoCE on ViT-L. We also train MAE with 2 subsets of ImageNet, namely Split-A and Split-B, following the same setting mentioned in Sec. 3.1. This table shows that negative transfer still exists on larger architectures, while MoCE can alleviate this problem and achieve better transfer results.\n\nAircraft Caltech Cars C10 C100 DTD Flowers Food Pets SUN VOC\n\nAvg.\n\nMAE-L (full set) MAE-L (Split-A) MAE-L (Split-B) MoCE-L\n\n74.30 79.70 73.42 87.04\n\n93.97 88.60 97.85 82.47 77.61 91.59 89.33 96.97 80.38 78.67 90.80 86.00 96.18 78.73 77.34 94.86 90.72 98.29 87.49 76.65\n\n96.67 95.44 96.75 97.38\n\n81.22 93.97 67.99 88.30 82.97 92.49 68.73 82.41 83.63 94.92 66.06 85.85 88.21 95.89 69.49 89.13 88.65+2.93\n\n85.72 85.33 84.52\n\nA.4 PERFORMANCE OF MOCE WITHOUT PRE-TRAINING.\n\nWe provide results of MoCE trained from scratch for 200 epochs and 1600 epochs in Table 11. In this experiment, for clustering, we first pre-train MAE for 50 epochs and perform clustering. We then train MoCE from scratch for 200 epochs and 1600 epochs based on the clustering results. Although it is a common practice to utilize pre-trained dense models as initialization to accelerate pre-training (Wu et al., 2022; Bai et al., 2022), MoCE still outperforms MAE consistently in various downstream tasks when trained from scratch.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: Comparison of MAE and MoCE both training from scratch for 200 epochs (first two rows) and 1600 epochs (last two rows).\n\nAircraft Caltech Cars C10 C100 DTD Flowers Food Pets SUN VOC Avg.\n\n64.73 MAE MoCE 71.16\n\n85.91 77.10 92.92 72.50 73.30 90.55 82.46 96.06 76.56 74.57\n\nMAE 72.38 MoCE 78.75\n\n90.47 83.51 95.69 68.40 75.48 91.64 87.04 97.15 83.12 73.62\n\n93.11 95.70\n\n96.10 96.08\n\n73.14 88.70 57.84 73.27 77.50 79.67 92.58 62.20 84.25 82.34\n\n79.98 92.35 62.43 84.79 81.96 83.84 93.06 65.49 85.81 85.05\n\nA.5 EVALUATION DETAILS FOR DOWNSTREAM TASKS.\n\nClassification. We mainly follow the settings of Ericsson et al. (2021).to make a fair comparison. Specifically, all models are trained by SGD with a momentum of 0.9. Weight decay is set to be 0 and the learning rate is searched among [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1]. Each model is fine-tuned for 2500 steps with cosine learning rate decay, a batch size of 64, and 224×224 resolution. We fine-tune each model 3 times and report the average performance. We find such a setting generates a stable result.\n\nSemantic segmentation. We evaluate MoCE on the semantic segmentation task that aims to predict the class for each pixel in the image. We report the metric of mean Intersection of Union (mIoU) averaged over all semantic categories in ADE20K (Zhou et al., 2019). We choose the best expert by applying ADE20K images to our clustering module and selecting the cluster that contains the most images. We use Adam (Loshchilov & Hutter, 2017) as the optimizer. The learning rate is set to 1e-3 with layer-wise learning rate decay (Clark et al., 2020) to be 0.65. We conduct fine-tuning for 160K steps. The batch size is 16. The detailed hyper-parameters can refer to Bao et al. (2022).\n\nDetection and Instance segmentation are also evaluated on COCO (Lin et al., 2014). We follow the same deployment method as the one used in the semantic segmentation task to choose the best expert. Following iBOT (Zhou et al., 2021) we adopt the Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017) and the multi-scale training. The shorter side is randomly resized between 480 and 800 while the longer one is no longer than 1333. The batch size is 16, and the initial learning rate is 1e-4. The layer-wise learning rate decay ratio (Clark et al., 2020) is set to 0.75. We train the model for 12 epochs and decrease the learning rate by 10x at epoch 9 and 11.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper initiates an interesting exploration of MAE with mixture of experts (MoE). The method is quite well-motivated, with interesting and fairness-in-mind designs, and a good amount of experiments devoted to it. The final results are reported using a suite of 11 downstream classification tasks, typically used to evaluate self-supervised learning methods. The proposed method is shown to significantly improve the overall accuracy, while maintaining efficiency.\n\n# Strength And Weaknesses\n\nStrengths:\n- This is one of the first works that I am aware to study MAE and MoE jointly. The exploration on this direction is interesting and of significance.\n- The paper motivates the approach by pointing to an empirical result of MAE suffering from the negative transfer phenomenon, and the final approach (guided by the motivation) is able to significantly outperform the MAE baseline. Overall it looks like a healthy research cycle to me.\n\nWeaknesses:\n- One major concern I am having with MoE-kind approach is about larger models. While used in a sparse way, MoE is still having a lot of parameters, and if directly comparing against MAE of the same backbone, it does not sound too fair to me. So I would like to see two things: 1) with a larger MAE model that roughly has the same number of parameters as a model with MoE, what's the comparison? and 2) Whether the improvements of the current model can transfer to larger models. Related: in table 3, I am not able to find the column for MAE. All I find are columns for TokenMoE and MoCE, while the caption says MAE is under comparison.\n- Related to the above, I am not sure the \"negative transfer\" phenomenon still exists with larger and larger MAE models? The hypothesis here is that maybe with larger models, it can capture both the man-made objects and the natural ones from ImageNet?\n- Maybe it is demanding too much computation overhead, but I noticed that the method in the paper is built from pre-trained MAE, and not training from scratch.  So I am wondering what would MoCE be like when pre-trained from scratch?\n- For the distillation loss, I am not sure why it still maintains an efficiency advantage over TokenMoE because the \"whole\" network is used as a \"teacher\", and presumably it can be costly to forward through all the parameters.\n- (minor as I don't know where to put it in the review) Why does MoCo v3 achieve so good results on Table 1? Given that MoCo v3 is better than MAE, isn't it more reasonable to start MoE explorations from MoCo v3 given the results listed in the table? In the same table, I am also curious to see what the supervised ViT will achieve on these downstream datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Overall the paper is a good read, and it is reasonably clear. One thing not clear to me: for data clustering, what features do it use? I am thinking the features are important, and if they are computed online, their FLOPs should also be counted? One could compute it offline, but it is still a computation budget over there if we want to compare overall pre-training cost fairly. Another (potentially minor) thing: for PSNR comparison in Fig 4, is it evaluating the reconstruction quality of the pre-training (auto-encoding) task? Or is it something else?\n\nQuality: I think this is a useful exploration marrying MAE and MoE. The paper has good illustrations, nice storyline, and sufficient amount of experiments. One pity is that the MAE w/ MoE model is not pre-trained from scratch, and this may cause some noise in the signals, but the presented work is done with high-quality.\n\nNovelty: While the techniques in the paper (how to do clustering, how to do gating in MoE) do appear to be existing ones, I think the work has directional novelty in exploring MAE w/ MoEs.\n\nReproducibility: If there is no difficult constraints, I hope the code of the paper is released to facilitate reproducing the results.\n\n# Summary Of The Review\n\nI would be on the acceptance side for the paper given the pros and cons listed above. I hope the authors can address the clarification questions I have, and try to tackle the empirical study (e.g. with larger models) as best as they can.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPARTAFFORD: PART-LEVEL AFFORDANCE DISCOVERY\n\nAnonymous authors Paper under double-blind review\n\nTrained with per-shape sparse annotations (affordance label set)\n\nInfer part decomposition and corresponding part affordances\n\ne p\na h\ns\n\nsittable framework\n\nsittable backrest framework\n\nsittable backrest armrest framework\n\nbackrest\n\nsittable\n\nframework\n\nInput: 3D shape\n\nOutput: Discovered 3D parts w/ their affordances\n\nl\n\ne b\na\n\nl\n\nFigure 1: The proposed PartAfford: discover 3D object part affordances by learning contrast in affordance compositions. During training (left), given weak annotations (per-shape affordance label set), a learning framework is devised to ground affordance (e.g., backrest) to 3D part (e.g., sofa back) through learning crosscategory, affordance-related shapes (e.g., chair, sofa) with various affordance compositions. At test time (right), the learned model decomposes the 3D object into parts and infers the part-level affordances.\n\nABSTRACT\n\nUnderstanding what objects could furnish for humansÐlearning object affordanceÐ is the crux of bridging perception and action. In the vision community, prior work has primarily focused on learning object affordance with dense (e.g., at a per-pixel level) supervision. In stark contrast, we humans learn the object affordance without dense labels. As such, the fundamental question to devise a computational model is: What is the natural way to learn the object affordance from geometry with humanlike weak supervision? In this work, we present the new task of part-level affordance discovery (PartAfford): Given only the affordance labels for each object, the machine is tasked to (i) decompose 3D shapes into parts and (ii) discover how each part of the object corresponds to a certain affordance category. We propose a novel learning framework that discovers part-level representations by leveraging only the affordance set supervision and geometric primitive regularization without dense supervision. To learn and evaluate PartAfford, we construct a part-level, crosscategory 3D object affordance dataset, annotated with 24 affordance categories shared among > 25, 000 objects. We demonstrate through extensive experiments that our method enables both the abstraction of 3D objects and part-level affordance discovery, with generalizability to difficult and cross-category examples. Further ablations reveal the contribution of each component.\n\n1\n\nINTRODUCTION\n\nThe human vision system could swiftly locate the functional part upon using an object for specific tasks (Land et al., 1999). Such a critical capability in object interaction requires fine-grained object affordance understanding. Affordance, coined and originally theorized by Gibson (Gibson & Carmichael, 1966; Gibson, 1979), characterizes how humans interact with human-made objects and environments. As such, affordance understanding of objects and scenes has a significant influence on bridging visual perception and holistic scene understanding (Huang et al., 2018b;a; Chen et al., 2019) with actionable information (Soatto, 2013; Han et al., 2022).\n\nObject affordances have two main characteristics. First, object affordances are not defined in terms of conventional categorical labels in computer vision; instead, they are defined by the associated actions\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nfor various tasks and are naturally cross-category. For example, both chair and sofa can be sat on, which indicates they share the sittable affordance. Similarly, desktop and bookshelf share the support affordance. Second, object affordances are intrinsically part-based. We could easily associate sittable affordance with the seats of chairs and sofas, and support with the boards of desktop and bookshelf. As such, the ability to learn part-based, cross-category affordance is essential to demonstrate the general object affordance understanding.\n\nIn passive affordance learning, prior literature follows the supervised learning paradigm, in which dense affordance annotation on the objects is fed as supervised signals (Deng et al., 2021). However, this line of thought depends heavily on the quality of dense annotation, which significantly deviates from how we humans learn to understand affordance. Humanlike supervision would be: ayou can sit on this chair and rest your arm,o ayou can open the lid and hold water with the cup.o In this paper, we try to answer: How to distinguish each object part while recognizing corresponding affordances with such weak and natural supervisions?\n\nTo tackle this problem, we present PartAfford, a new task of part-level affordance discovery, which learns the object affordance with the natural supervision of the affordance set. As shown in Fig. 1, by providing only the set of affordance labels for each object, the algorithm is tasked to decompose the 3D shapes into parts and discover how each part corresponds to a certain affordance category, which is challenging and under-explored in the area of generalizable part-level object understanding and affordance learning.\n\nTo address this, we propose a novel method that discovers part-level representations with selfsupervised 3D reconstruction, affordance set supervision, and primitive regularization. The proposed approach consists of two main components. The first component is an encoder with slot attention for unsupervised clustering and abstraction. Specifically, we encode the 3D object into visual features and abstract the low-level features into a set of slot variables (Locatello et al., 2020). The second component is a decoder built upon the learned slot features. It has three output branches that jointly reconstruct the 3D parts and object, predict the affordance labels, and regularize the learned part-level shapes with cuboidal primitives. Our method does not rely on dense supervision but instead learns from the weak set supervision. It discovers the part-level affordance by learning the correspondence between affordance labels and abstracted 3D object parts.\n\nLearning and evaluating PartAfford demands collections of 3D objects and their affordance labels for object parts. Prior work on visual affordance learning (Hassanin et al., 2021) either focuses on 2D objects and scenes or lacks part-based annotation (Deng et al., 2021). Hence, we construct a part-level, cross-category 3D object affordance dataset annotated with 24 affordance categories shared among over 25, 000 3D objects. The 3D objects are collected from PartNet dataset (Mo et al., 2019b) and the PartNet-Mobility dataset (Xiang et al., 2020). The 24 part affordance categories are defined in terms of adjectives (e.g., asittableo) or nouns (e.g., aarmresto); they describe how object parts could afford human daily actions and activities. We annotate the part-level object affordances by manually mapping the fine-grained object part defined in PartNet to the part affordances defined in this work.\n\nBy experimenting on this newly constructed PartAfford dataset, we empirically demonstrate that our method jointly enables the abstraction of 3D objects and part-level affordance discovery. Our model also shows strong generalizability on hard and cross-category objects. Further experiments and ablations analyze each component’s contribution and point out future directions.\n\nIn summary, our work makes four main contributions:\n\n• We present a new PartAfford task for part-level affordance discovery. Compared to the prior densely-\n\nsupervised learning paradigm, PartAfford learns the visual object affordance more naturally.\n\n• We propose a novel learning framework for tackling PartAfford, which jointly abstracts 3D objects into part-level representations and discovers affordances by learning the affordance correspondence. • We build the benchmark for learning and evaluating PartAfford by curating a dataset consisting of\n\n3D objects and annotating part-level affordances.\n\n• We empirically demonstrate the efficacy and generalization capability of the proposed method and analyze each component’s significance via a suite of ablation studies. Code and data will be released for research purposes.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nAffordance Learning Affordance learning is a multidisciplinary research field of vision, cognition, and robotics. In general, aaffordanceo is first perceived from images (Gupta et al., 2011; Kjellström et al., 2011; Zhu et al., 2015; Myers et al., 2015; Roy & Todorovic, 2016) or videos (Xie et al., 2013; Zhu et al., 2016; Fang et al., 2018; Nagarajan et al., 2020), followed by cognitive reasoning (Zhu et al., 2015; 2020), and finally serves for task and motion planning in robotics (Nagarajan & Grauman, 2020; Mo et al., 2022). Prior work tackles affordance at various scales and representations. Although affordance has been studied at the scene level (Zhao & Zhu, 2013; Gupta et al., 2015; Roy & Todorovic, 2016), object level (Nguyen et al., 2017; Mo et al., 2021; Gadre et al., 2021), and associated with generated human poses (Zhu et al., 2015; Wang et al., 2017b), few attempts study affordance as a 3D shape analysis task (Yu et al., 2015; Liang et al., 2016; Zhu et al., 2016; Wang et al., 2017a) since it would normally require large-scale, high-quality 3D data. A recent work (Deng et al., 2021) benchmarks several affordance estimation tasks on PartNet (Mo et al., 2019b) with dense affordance heatmap supervisions, annotated by densely selecting keypoints without considering affordance compositionality. Prior work also tackles affordance learning through interaction-based methods, either from human demonstration videos (Kjellström et al., 2011; Nagarajan et al., 2019; 2020) or simulation-based active learning (Wang et al., 2022; Mo et al., 2021). The first usually infers high-level and coarse 2D affordance, and the second is often restricted to basic manipulations in specific domains. In comparison, PartAfford studies affordance in a weakly supervised manner, such that the affordance discovery will be guided by affordance set matching and geometry abstraction. The new affordance dataset we construct provides fine-grained, part-level 3D affordance annotations, tailored for the weak supervision setting and affordance compositionality study.\n\nObject-centric Learning Object discovery has been studied in an iterative end-to-end fashion (Greff et al., 2017; Van Steenkiste et al., 2018; Burgess et al., 2019; Engelcke et al., 2019; Greff et al., 2019; Du et al., 2021). Recently, Locatello et al. (2020) presents the slot attention module, an efficient and generic framework for object-centric representation extraction. It is capable of modeling compositional nature in synthetic scenes with multiple simple geometry shapes (Kabra et al., 2019). Subsequently, Stelzner et al. (2021) and Yu et al. (2021) apply slot attention on unsupervised 3Daware scene decomposition, integrating NeRF (Mildenhall et al., 2020) as object representations. They demonstrate that slot-based bottleneck could perform reasonably on synthetic multi-view RGB datasets with a textureless background. In the weakly-supervised regime, methods have been proposed for 3D semantic segmentation with scene-level labels (Wei et al., 2020; Ren et al., 2021). However, these methods rely on additional input (e.g., color, normal) and abstract object-level features in different data domains. Our work takes one step further to tackle the challenge of part-level affordance discovery of 3D objects; part discovery is more complex than object discovery, primarily due to the ambiguity in the object part segmentation without applying additional constraints. Fortunately, for man-made objects, affordances are attached to objects at the part level. This observation implies the possibility of combining part discovery and affordance learning with minimal supervision. In this work, we integrate part discovery with affordance estimation, hoping that affordance information would help discover object parts sharing similar affordances.\n\nUnsupervised Geometric Primitive Modeling Whereas supervised geometric primitive abstraction methods (Mo et al., 2019a; Yang et al., 2020) require dense hierarchical annotations, unsupervised frameworks using cuboid-based (Tulsiani et al., 2017; Sun et al., 2019), superquadrics-based (Paschalidou et al., 2019; 2020), or other genus-zero-shape (Deng et al., 2020; Paschalidou et al., 2021) primitives discover structural information naturally embedded in the geometry. Recently, Yang & Chen (2021) unsupervisedly learn the cuboid-based shape abstraction with shape co-segmentation. Yet, it relies heavily on the ground-truth point normals for accurate abstraction and lacks semantic representation for object understanding. In our affordance discovery framework, we leverage the cuboidal regularization to refine the reconstructed affordance part, which distinguishes densely connected 3D parts by providing geometric prior, thus improving the affordance part discovery.\n\n3 TASK DEFINITION\n\nWe formulate the new task PartAfford as discovering the part-level object affordance with the affordance set supervision. We define K = 24 common affordance categories S = {sk}K k=1, such as asittableo and aopenable,o for object understanding. Input is given as a collection of N objects\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Abstraction Encoder\n\nT-iteration 3D Slot Attention (Slots Compete for Parts)\n\n3D Attention Mask Visualization (Slots Attend to Parts)\n\n3 D\nC N\nN E\nn c\no d\ne\n\nr\n\nInput shape\n\n3D feat + pos_embed\n\nm\n\nr\n\no N\n\nr\n\ne y\na L\n\nP L\nM\n\nP L\nM\n\nkey\n\nquery\n\nSoftMax\n\nvalue\n\nGRU\n\nM L\nP\n\nL a\ny e\n\nr\n\nN o\n\nr\n\nm\n\nslots!\"#\n\nslots!\n\nSlots\n\nt = 1\n\nt = 2\n\nt = 3\n\n(b) 3D Part Reconstruction Decoder\n\n(c) Affordance Prediction Decoder\n\n(d) Cuboidal Primitive Regularization Decoder\n\nr\n\ne d\no c\ne D\nN N\nC D\n3\n\nSlots\n\nRecon. Parts\n\nRecon. 3D Object\n\nPart-Cuboid Consistency\n\nSet Matching\n\nsittable\n\nbackrest\n\narmrest\n\narmrest\n\nbackrest\n\nsittable\n\nframework\n\nframework\n\nP L\nM\n\nP L\nM\n\nrotation\n\nscale\n\nSlots\n\nPred. Affordances\n\nGT Affordances\n\nSlots\n\nPred. Cuboids\n\nRecon. Parts\n\nFigure 2: Illustration of the proposed method for PartAfford. Our model contains two main components: abstraction encoder and affordance decoder. (a) Abstraction encoder takes a 3D object as input, extracts features with 3D convolutional neural networks, and abstracts it into several slots. Affordance decoder with three branches jointly (b) reconstructs the 3D parts, (c) predicts affordance labels, and (d) regularizes cuboidal primitives. Each predicted cuboid in (d) wraps around the corresponding predicted object part in (b) tightly.\n\ni=1 and their corresponding affordance set labels {Ai}N\n\n{oi}N i ∈ S, and Ji represents the number of distinct assigned affordances for each object i. PartAfford requires an algorithm to decompose each object into parts and discover the affordance corresponding to each object part. Fig. 1 illustrates the PartAfford task.\n\ni=1, where Ai = {aj\n\nj=1. aj\n\ni }Ji\n\n4 METHOD\n\nWe propose a novel framework for affordance discovery from 3D objects. It integrates unsupervised part discovery with affordance set prediction and geometric primitive abstraction; see Fig. 2. Given a 3D shape represented by voxel grids V of resolution 323, our method first encodes the 3D shape into visual features and abstracts it into M slots; each slot represents an abstracted high-level feature for downstream tasks. Next, we utilize a decoder with three branches to jointly (i) decode the features into parts, (ii) predict the affordance label, and (iii) regularize the parts with cuboidal primitives. Below, we describe in detail how each module is constructed and the loss design.\n\n4.1 ABSTRACTION ENCODER\n\nThe encoder takes a 3D shape as input and abstracts part-centric latent codes in an unsupervised manner. It consists of a feature extraction module and a 3D slot attention module; see Fig. 2a.\n\nFeature Extraction The feature embedding backbone encodes the input voxels and generates a D = 64 dimensional feature for each voxel. Following (Mescheder et al., 2019), voxels are encoded by five layers of 3D convolutional neural networks. The embedded feature is then augmented with absolute positional embedding (Locatello et al., 2020).\n\n3D Slot Attention The 3D slot attention architecture, adapted from Locatello et al. (2020), serves as the part-centric representational bottleneck between the 3D feature embedding network and the downstream decoders. The encoded feature of a 3D shape is fed into an iterative attention module, where M randomly initialized slots are updated for T = 3 iterations through a GRU (Cho et al., 2014). During each iteration, the attention coefficients are calculated by applying softmax normalization over the slots on the dot-product similarity between queries (i.e., linearly-mapped 3D slot features) and keys (i.e., linearly-mapped input features). The attention coefficients are then applied as weights for aggregating the values (i.e., linearly-mapped input feature) and updating the slots.\n\nSince the inputs-to-slots attention assignment is normalized over the 3D slots, those slots compete to attend to a clustering of the input 3D shape. Such clusterings are similar to human-defined parts on common objects. 3D slot attention masks naturally segment the object through iterations. An example of the learned 3D attention masks is shown in Fig. 2a.\n\n4\n\n \n \nUnder review as a conference paper at ICLR 2023\n\n4.2 AFFORDANCE DECODER\n\nShown in Fig. 2b-d, the affordance decoder takes part-centric slot features as input, followed by three branches for 3D part reconstruction, affordance prediction, and primitive regularization. The decoder parameter is shared across slots.\n\n3D Part Reconstruction We design a 3-layer 3D transposed convolutional decoder followed by a single MLP layer to reconstruct voxel values ˆV m and a voxel mask for each slot. The mask is normalized across slots with softmax, which generates a normalized mask ˆΛm ∈ R32×32×32. It is then used to compute the weighted sum of voxel values across slots and combine the reconstructed M\nm=1 ˆΛm ˆV m. The 3D part reconstruction branch is parts { ˆV m}M self-supervised by the reconstruction loss between original voxels V and reconstructed voxels ˆV; we use the binary cross-entropy (BCE): Lrecon = BCE(V, ˆV).\n\nm=1 into a full 3D shape ˆV : ˆV =\n\nP\n\nAffordance Prediction We predict a one-hot affordance label for each slot with a two-layer MLP with sharing weights across slots for classification.\n\nThe affordance prediction branch is weakly-supervised as we do not provide affordance labels for each voxel. Instead, only the affordance label set for the entire object is used as the supervision signal. The model is tasked to learn the alignment between the abstracted parts and the affordance labels from set supervision.\n\nAs defined in Sec. 3, the ground-truth set of affordance labels for an input 3D object is denoted as A. We denote ˆA as the set of slot affordance predictions. ˆAσ is a permutation of elements in ˆA, where σ ∈ G and G represent all M ! possible permutations. Lmatch is the pairwise matching cost between two sets, which can be calculated by mean square error (MSE) or cross-entropy:\n\nLpred = min σ∈G\n\nLmatch(A, ˆAσ).\n\n(1)\n\nDue to the order-invariant nature of slot modules, we apply the Hungarian matching algorithm (Kuhn, 1955), with Huber loss as the pair-wise matching cost, to calculate the set-based (Carion et al., 2020) affordance prediction loss in Eq. (1).\n\nCuboidal Primitive Regularization As a generalized soft k-means algorithm, the slot attention mechanism heavily relies on visual cues, such as the clustering of pixel colors on the image. As such, it cannot perform precisely in a crowded scene with overlapping objects even on a toy image dataset (Locatello et al., 2020). In the 3D voxel regime, segmenting objects into parts is challenging since every voxel is connected to neighboring voxels without distinguishable visual appearances.\n\nTherefore, we introduce the cuboidal primitive regularization module, providing a geometric prior for segmentation: Human-made objects usually have geometric regularity, and cuboid is a concise structural representation for abstraction.\n\nFrom each slot embedding, the cuboid abstraction module predicts a cuboid parametrized by two vectors (Yang & Chen, 2021): a scale vector s ∈ R3 and a quaternion vector r ∈ R4 for 3D rotation. Of note, we calculate the cuboid center from the weighted mean of voxel positions in the slot.\n\ni\n\nfrom each voxel pm i\n\nTo evaluate how the predicted cuboid fits the reconstructed part in the m-th slot, we first compute the Euclidean distance dm to its closest cuboid face. Next, we calculate the i ∈ ˆV m is the reconstructed voxel value weighted sum distance for all voxels, where the weight vm within [0, 1]. Additionally, we designed a binary surface mask f (i) that masks out internal voxels in the loss. We also regularize the cuboid loss by adding a cuboid scale penalty term, i.e., L1 norm for the scale vector. Thus, the loss encourages a cuboid to wrap around a solid object part tightly. The regularization loss for all the slots is defined as:\n\nLcuboid =\n\nm \" X\n\nλscale∥sm∥1 +\n\ni X\n\nf (i)vm\n\ni dm\n\ni\n\n.\n\n#\n\n(2)\n\nTotal Loss Taking together, the total training loss is the sum of 3D reconstruction loss, affordance prediction loss, and primitive regularization loss:\n\nLtotal = λreconLrecon + λpredLpred + λcuboidLcuboid,\n\n(3)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Our affordance description for “openable”\n\n(b) An example door from PartNet\n\nOpenable: • Definition. Parts that could be moved via a hinge-like mechanism on an articulated object. • Details. Usually, a handle is found attached to an openable part ... (more in supp.) • Multi-label. Openable is given priority over potential co-existing affordances like containment. • e.g., door/surface_board,\n\ndishwasher/door/door_frame.\n\nAffordance Annotation\n\nsurface_board\n\nfixed_part\n\nopenable\n\ndoor\n\nlever\n\nhandle\n\nmovable_part\n\ngeometry physics\n\n(c) A dishwasher augmented from SAPIEN\n\nopenable\n\ncontainment\n\nhanging\n\ncontainment liquidcontainment\n\nopenable\n\nhandle\n\nillumination\n\nframework\n\nstep\n\nlyable\n\nheadrest\n\narmrest\n\nsittable\n\nrollable\n\npourable\n\nsupport\n\nbackrest\n\ntwistable\n\ncutting\n\nlever\n\ndisplay\n\npressable\n\npinchable\n\nwrapgrasp\n\naudible\n\nopenable\n\ncontainment\n\n(d) 24 cross-category, fine-grained affordance labels\n\nFigure 3: Part affordance dataset. (a) Description for the aopenable\" affordance to construct the mapping. (b) Given the part hierarchy of a door from PartNet (Mo et al., 2019b), we annotate its affordance labels by manual mapping and inspection. (c) Given a dishwasher from PartNet-Mobility (SAPIEN) (Xiang et al., 2020) and its kinematics, we rotate the door frame to include 3D objects with different articulation states. (d) Some exemplar 3D object models with color-coded affordance visualization.\n\nwhere λrecon, λcuboid, and λpred are balancing coefficients.\n\nOf note, with the current architecture design, for the first time, we demonstrate the capability of part-level affordance discovery from set labels. The exploration of more complex and practical modules is left for future work.\n\n5 PART AFFORDANCE DATASET\n\nTo benchmark PartAfford and facilitate the research in affordance understanding, we construct a part-level 3D object affordance dataset. We focus on 24 cross-category, fine-grained affordance labels as shown in Fig. 3. The dataset is annotated with over 25, 000 3D CAD models from the PartNet dataset (Mo et al., 2019b) and 625 articulated objects among 9 categories from the PartNet-Mobility dataset in SAPIEN (Xiang et al., 2020). Below, we describe how to define part affordances and generate affordance annotation. See appendix for more details.\n\nPart affordances in our dataset are defined in terms of adjectives (e.g., sittable) or nouns (e.g., armrest), which describe how object parts could afford human daily actions and activities. We adopt certain common affordance categories from a comprehensive survey of visual affordance (Hassanin et al., 2021), e.g., containment, sittable, support, openable, rollable, display, and wrapgrasp. However, they are coarse-grained±either at the object level or scene level. For example, aopenableo only indicates whether an object can be opened, not on which object part can afford the object to be opened.\n\nTo pursue a fine-grained understanding of object affordance, we manually construct a one-to-multiple mapping from 479 kinds of object part labels defined at the finest granularity in Mo et al. (2019b) to 24 potential affordance labels, given the detailed affordance description. We provide expert-defined descriptions for 24 affordances to guarantee the quality and consistency of the mapping construction. An example is shown in Fig. 3a.\n\nGiven the part hierarchy of a 3D object, we can get the corresponding affordance annotation by mapping. We also perform a manual inspection to correct the affordance labels, especially for some fine-grained parts, according to their specific geometry and physics properties. For instance, different door handles will be mapped to different affordance labels (twistable, lever, etc.) according to how they should be operated (Fig. 3b).\n\nThe PartNet dataset does not contain articulation information, making affordances such as openable not geometrically distinguishable. Therefore, we generate a set of shapes with openable affordance from the PartNet-Mobility dataset by capturing 3D shapes with various opening angles (Fig. 3c).\n\nAs can be seen from Fig. 3d, each affordance type±due to its cross-category nature±may be found on various object part instances. For example, openable is usually afforded by rotatable doors for unobstructed access. Under such criteria, the door frame of a dishwasher and the surface board of a door are both mapped to openable. Please refer to the appendix for a full list of all affordance categories, descriptions, and mapping examples.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nopenable\n\nframework\n\nhandle\n\nsupport\n\nsittable\n\narmrest \n\nbackrest\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(f)\n\n(g)\n\n(h)\n\nInput\n\nSlot 4 Figure 4: Qualitative results on three curated subsets: aopenableo (a-b), asupporto (c-e), and asittableo (f-h).\n\nRecon.\n\nRecon.\n\nSlot 3\n\nSlot 3\n\nSlot 2\n\nSlot 4\n\nSlot 1\n\nSlot 1\n\nSlot 2\n\nInput\n\nGT\n\nGT\n\n6 EXPERIMENTS\n\nIn this section, we design and conduct comprehensive experiments to evaluate the proposed method. Fig. 4 visualizes our main results. We present both quantitative and qualitative comparisons of baseline models and other variants. We also evaluate the model generalization on novel shapes, analyze failure cases, and propose potential improvement directions. Please refer to the appendix for additional experimental settings, results, and analyses.\n\n6.1 EXPERIMENTAL SETTINGS\n\nBenchmarks To benchmark PartAfford, we curate different subsets of samples from our constructed dataset. Specifically, we study the subsets related to the most representative affordance categories asittable,o asupport,o and aopenableo separately, where the subsets are created by collecting all crosscategory objects that have the corresponding affordance label in our dataset. They contain 7 kinds of affordances from 8 object categories, covering 17, 842/25K ≈ 71% instances. For each subset, we learn to distinguish all the affordance labels appear in the 3D objects. Note that although a part can have multiple affordances as mentioned in Sec. 5, we only keep the most prioritized affordance for each part to ease the ambiguities in learning.\n\nEvaluation Metrics similarity), 3D reconstruction, and affordance prediction.\n\nIn PartAfford, we evaluate the performances of part discovery (clustering\n\n• Part Discovery: We use the Intersection over Union (IoU) to evaluate the part similarity. Specifically, we employ Hungarian matching to find the best matches between the reconstructed parts and ground truth parts using voxel IoU as the matching score. Then we compute the mean IoU (mIoU) by averaging the IoU between best matches.\n\n• 3D Reconstruction: We evaluate the shape reconstruction quality by Mean Squared Error (MSE). • Affordance Prediction: Following Locatello et al. (2020), we use Average Precision (AP) to evaluate the prediction accuracy. A correct prediction means an exact match of the affordance label set.\n\nBaselines and Ablations Since we are the first to propose and formulate PartAfford, we have no previous work to make direct comparisons. Therefore, we compare our method with two designed baseline models and three variants:\n\n• Slot MLP: a simple MLP-based baseline where we replace Slot Attention with an MLP that maps from the learned feature maps (resized and flattened) to the (now ordered) slot representation. • IODINE: a baseline where we replace the Slot Attention with an object-centric learning method\n\nIODINE (Greff et al., 2019) to abstract and cluster the encoded feature.\n\n• Ours w/o Afford & Cuboid: our model variant that only keeps the 3D part reconstruction branch. • Ours w/o Afford: our model variant without the affordance prediction branch. • Ours w/o Cuboid: our model variant that discards the cuboidal primitive regularization branch. • Ours Full: our full model with all branches.\n\n6.2\n\nIMPLEMENTATION DETAILS\n\nLearning Strategy To stabilize the training, we split the training into two stages. In the first stage, we train the decoder only with 3D part reconstruction and affordance prediction branches. In the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nrecon. only/null\n\nframework\n\nbackrest\n\nsittable\n\narmrest\n\nsupport\n\nP L\nM\n\nP L\nM\n\nt\n\nl\n\no S\n\n) l\nl\n\nu\n\nf (\n\ns r\n\nu O\n\nInput\n\nGT\n\nRecon.\n\nt\n\nl\n\no S\n\n) l\nl\n\nu\n\nf (\n\ns r\n\nu O\n\nSlot 2\n\nSlot 1 (a) Comparisons between the Slot MLP and our model.\n\nSlot 3\n\nSlot 4\n\nRecon.\n\nInput\n\nGT\n\nSlot 1\n\nSlot 2\n\nSlot 3\n\nSlot 4\n\n(b) Comparisons between the models without and with affordance prediction branch. The affordance prediction facilitates the abstraction of 3D shape (rows 1-2) and elimination of spare slots (rows 3-4).\n\n(c) Comparisons between the models without and with cuboidal primitive regularization branch. The cuboidal primitive regularization helps to guide and improve the segmentation.\n\nFigure 5: Qualitative comparison results.\n\nsecond stage, the cuboidal primitive regularization branch is incorporated into joint training with a lower learning rate. Hyperparameter We set learning rate as 4 × 10−4 for the first stage, 2 × 10−4 for the second stage, and apply Adam optimizer (Kingma & Ba, 2014) for optimization. It takes 5 + 9 hours on 4 RTX A6000 GPUs for two-stage full-model training of asittableo-related objects. For slot attention, we empirically set the number of GRU iterations T = 3. We set the number of slots to the maximal number of affordance labels that appear in each training subset. For example, we learn the asittableo with 4 slots, asupporto with 2 slots, and aopenableo with 3 slots. Appendix C.1 further discusses choices of the number of slots. For the joint loss weight, we set λrecon = 1.0, λpred = 0.5, λcuboid = 0.1.\n\n6.3 RESULTS AND ANALYSIS\n\nTabs. 1 and 2 tabulate the quantitative performances of all the models under different settings. Fig. 4 qualitatively shows the capability of our method and Fig. 5 compares different models. Below, we summarize some key findings:\n\n1. The proposed method achieves the best overall performance on the PartAfford task, especially in the part discovery (mean IoU) where it outperforms the baselines by a large margin. This demonstrates the outstanding abstraction capability of our approach given the weak supervision. From Fig. 4, we can see our method can discover the detailed part-level representation with their aligned affordances for the 3D objects.\n\n2. The most challenging part of PartAfford lies in the part discovery; it is also where our model differentiates from other baselines. For example, Tab. 1 shows that Slot MLP achieves the best affordance prediction performance (AP) but fails in the part discovery (mean IoU) and 3D reconstruction (MSE). As also shown in Fig. 5a, the Slot MLP cannot segment the object input to parts due to the lack of abstraction capability.\n\n3. Affordance prediction branch significantly escalates the part discovery performance since it helps to learn part-affordance correspondence from the affordance composition, which provides contrasts to distinguish different parts among the training objects. Our qualitative results also show that the affordance prediction facilitates the abstraction of 3D shape (e.g., rows 1-2 of Fig. 5b) and elimination of spare slots (e.g., rows 3-4 of Fig. 5b);\n\n4. Cuboidal primitive regularization branch also boosts the part discovery, especially when affordance prediction is unavailable. This demonstrates that geometric priors play a crucial role in\n\n8\n\nInputRecon.Slot 1Slot 4Slot 3Slot 2GTw/o Aff.w/ Aff.w/o Aff.w/ Aff.InputRecon.Slot 1Slot 4Slot 3Slot 2w/o Cub.w/ Cub.w/o Cub.w/ Cub.Under review as a conference paper at ICLR 2023\n\nFigure 6: Generalization results.\n\nFigure 7: Failure cases.\n\nsegmentation when data are not diverse enough. From Fig. 5c, we can see the cuboidal primitive regularization helps to segment better primitives and avoid scattered voxels.\n\n6.4 MODEL GENERALIZATION\n\nTable 1: Quantitative results on “sittable.” We evaluate the mean IoU (mIoU), mean squared error (MSE), and average precision (AP) on included objects.\n\nWith the cross-category nature of affordance, we qualitatively test how the learned model can be generalized to novel objects and unseen categories. We conduct model generalization experiments by testing hard examples or objects from other categories. Examples from Fig. 6 demonstrate the learned model could be generalized to objects with diverse shapes. We show the results of testing the learned model on a novel object shape (bean bag) (a) from Fu et al. (2021) and unseen categories (b-d). For example, (b) shows the result of learning with asupporto and testing on an aopenableo object (i.e., a microwave). Despite the imperfect reconstructions , partly due to the reconstruction bottleneck’s impact on disentanglement quality (Engelcke et al., 2020), the learned model can identify the functional parts given novel objects.\n\nModel Slot MLP IODINE Ours w/o Afford & Cuboid Ours w/o Afford Ours w/o Cuboid Ours (full)\n\n0.0150 0.0102 0.0112 0.0100 0.0102 0.0097\n\nmIoU (%) ↑ MSE ↓ AP (%)↑\n\n94.5 92.5 N/A N/A 92.7 92.9\n\n21.5 49.2 31.5 39.4 55.3 57.3\n\n6.5 FAILURE CASES\n\nWe show some failure cases of our method in Fig. 7. For asittableo and asupport,o the failures are commonly caused by (i) the difficulties in reconstructing the fine-grained details of 3D objects with novel shapes; (ii) certain parts that violate the cuboid assumption, and thus hurt other components.\n\nTable 2: Quantitative results on “support” and “openable.”\n\nFor objects in aopenableo category, our model cannot discover and reconstruct ahandle,o as shown in Fig. 4. This is because the objects with related affordances come from various object categories with diverse shapes, making it challenging for the model to capture such complex mixtures of distributions and reconstruct finegrained 3D shapes, especially tiny parts . This points out future directions to better understand object parts (e.g., segment, reconstruct), and potentially an interactive learning framework to learn beyond geometry and appearance.\n\nModel Slot MLP Ours w/o Afford Ours w/o Cuboid Ours (full) Slot MLP Ours w/o Afford Ours w/o Cuboid Ours (full)\n\n36.8 34.8 51.3 52.7 21.0 19.9 46.7 47.6\n\nopenable\n\nsupport\n\n0.0099 0.0092 0.0087 0.0085 0.0130 0.0104 0.0097 0.0093\n\n91.6 N/A 95.2 95.1 70.8 N/A 55.8 60.4\n\nmIoU (%) ↑ MSE ↓ AP (%) ↑\n\n7 CONCLUSION\n\nWe present PartAfford, a new task in visual affordance research that aims at discovering part-level affordances from 3D shapes. We propose a novel learning framework that discovers part-level affordances by leveraging only the affordance set supervision and geometric primitive regularization. With comprehensive experiments and analyses, we point out potential directions for incorporating visual appearance to facilitate better shape abstraction and combining it with an active learning approach for efficient affordance learning.\n\n9\n\nInputRecon.Slot 1Slot 4Slot 3Slot 2sittablebackrestframeworkarmrestsupporta. Novel Sittableb. Support Openablec. Support Sittabled. Sittable SupportInputRecon.Slot 1Slot 4Slot 3Slot 2GTsittablebackrestframeworkarmrestsupportUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019. 3\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision (ECCV), 2020. 5\n\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 20\n\nYixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, and Song-Chun Zhu. Holistic++ scene understanding: Single-view 3d holistic scene parsing and human pose estimation with humanobject interaction and physical commonsense. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1\n\nKyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. 4\n\nBoyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, and Andrea In Conference on Computer Vision\n\nTagliasacchi. Cvxnet: Learnable convex decomposition. and Pattern Recognition (CVPR), 2020. 3\n\nShengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and Kui Jia. 3d affordancenet: A benchmark for visual object affordance understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 3, 19\n\nYilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and Igor Mordatch. Unsupervised learning of compositional energy concepts. Advances in Neural Information Processing Systems (NeurIPS), 2021. 3\n\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052, 2019. 3\n\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Reconstruction bottlenecks in object-centric\n\ngenerative models. arXiv preprint arXiv:2007.06245, 2020. 9\n\nKuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, and Joseph J Lim. Demo2vec: Reasoning object affordances from online videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3\n\nHuan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision (IJCV), 129 (12):3313±3337, 2021. 9\n\nSamir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction strategies for articulated object part discovery. In International Conference on Computer Vision (ICCV), pp. 15752±15761, 2021. 3\n\nCaelan Reed Garrett, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 30, pp. 440±448, 2020. 19\n\nCaelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, and Tomás Lozano-Pérez. Integrated task and motion planning. Annual review of control, robotics, and autonomous systems, 4:265±293, 2021. 19\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJames Jerome Gibson. The ecological approach to visual perception. Houghton, Mifflin and Company,\n\n1979. 1\n\nJames Jerome Gibson and Leonard Carmichael. The senses considered as perceptual systems,\n\nvolume 2. Houghton Mifflin Boston, 1966. 1\n\nKlaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximization.\n\narXiv preprint arXiv:1708.03498, 2017. 3\n\nKlaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning (ICML), 2019. 3, 7\n\nAbhinav Gupta, Scott Satkin, Alexei A Efros, and Martial Hebert. From 3d scene geometry to human\n\nworkspace. In Conference on Computer Vision and Pattern Recognition (CVPR), 2011. 3\n\nSaurabh Gupta, Pablo Arbeláez, Ross Girshick, and Jitendra Malik. Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation. International Journal of Computer Vision (IJCV), 112(2):133±149, 2015. 3\n\nDenis Hadjivelichkov, Sicelukwanda Zwane, Marc Deisenroth, Lourdes Agapito, and Dimitrios Kanoulas. One-shot transfer of affordance regions? affcorrs! arXiv preprint arXiv:2209.07147, 2022. 19\n\nMuzhi Han, Zeyu Zhang, Ziyuan Jiao, Xu Xie, Yixin Zhu, Song-Chun Zhu, and Hangxin Liu. Scene reconstruction with functional objects for robot autonomy. International Journal of Computer Vision (IJCV), pp. 1±22, 2022. 1\n\nMohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J Black. Populating 3d scenes by learning human-scene interaction. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 20\n\nMohammed Hassanin, Salman Khan, and Murat Tahtali. Visual affordance and function understand-\n\ning: A survey. ACM Computing Surveys (CSUR), 54(3):1±35, 2021. 2, 6\n\nSiyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Cooperative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2018a. 1\n\nSiyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and Song-Chun Zhu. Holistic 3d scene parsing and reconstruction from a single rgb image. In European Conference on Computer Vision (ECCV), 2018b. 1\n\nRishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm Reynolds, and Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multiobject-datasets/, 2019. 3\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014. 8\n\nHedvig Kjellström, Javier Romero, and Danica KragiÂc. Visual object-action recognition: Inferring object affordances from human demonstration. Computer Vision and Image Understanding, 115 (1):81±90, 2011. 3\n\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\n\nquarterly, 2(1-2):83±97, 1955. 5\n\nMichael Land, Neil Mennie, and Jennifer Rusted. The roles of vision and eye movements in the\n\ncontrol of activities of daily living. Perception, 28(11):1311±1328, 1999. 1\n\nWei Liang, Yibiao Zhao, Yixin Zhu, and Song-Chun Zhu. What is where: Inferring containment relations from videos. In International Joint Conference on Artificial Intelligence (IJCAI), 2016. 3\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems (NeurIPS), 2020. 2, 3, 4, 5, 7, 18, 21\n\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 4\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. 3\n\nKaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy J Mitra, and Leonidas J Guibas. Structurenet: hierarchical graph networks for 3d shape generation. ACM Transactions on Graphics (TOG), 38(6):1±19, 2019a. 3\n\nKaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019b. 2, 3, 6, 20, 24\n\nKaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: From pixels to actions for articulated 3d objects. In International Conference on Computer Vision (ICCV), 2021. 3\n\nKaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. O2o-afford: Annotation-free large-scale object-object affordance learning. In Conference on Robot Learning, pp. 1666±1677. PMLR, 2022. 3\n\nAustin Myers, Ching L Teo, Cornelia Fermüller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In International Conference on Robotics and Automation (ICRA), pp. 1374±1381. IEEE, 2015. 3\n\nTushar Nagarajan and Kristen Grauman. Learning affordance landscapes for interaction exploration in 3d environments. Advances in Neural Information Processing Systems (NeurIPS), 2020. 3\n\nTushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In International Conference on Computer Vision (ICCV), 2019. 3\n\nTushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, and Kristen Grauman. Ego-topo: EnviIn Conference on Computer Vision and Pattern\n\nronment affordances from egocentric video. Recognition (CVPR), 2020. 3\n\nAnh Nguyen, Dimitrios Kanoulas, Darwin G Caldwell, and Nikos G Tsagarakis. Object-based affordances detection with convolutional neural networks and dense conditional random fields. In International Conference on Intelligent Robots and Systems (IROS), 2017. 3\n\nDespoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning 3d shape parsing beyond cuboids. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3\n\nDespoina Paschalidou, Luc Van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of 3d objects from a single rgb image. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nDespoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, and Sanja Fidler. Neural parts: Learning expressive 3d shape abstractions with invertible neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3\n\nZhongzheng Ren, Ishan Misra, Alexander G Schwing, and Rohit Girdhar. 3d spatial recognition without spatially labeled 3d. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13204±13213, 2021. 3\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAnirban Roy and Sinisa Todorovic. A multi-scale cnn for affordance segmentation in rgb images. In\n\nEuropean Conference on Computer Vision (ECCV), 2016. 3\n\nStefano Soatto. Actionable information in vision. In Machine Learning for Computer Vision, pp.\n\n17±48. Springer, 2013. 1\n\nRobyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of\n\ngeneral knowledge. In AAAI Conference on Artificial Intelligence (AAAI), 2017. 21\n\nKarl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via\n\nunsupervised volume segmentation. arXiv preprint arXiv:2104.01148, 2021. 3\n\nJulian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 15, 20\n\nChun-Yu Sun, Qian-Fang Zou, Xin Tong, and Yang Liu. Learning adaptive hierarchical cuboid abstractions of 3d shape collections. ACM Transactions on Graphics (TOG), 38(6):1±13, 2019. 3\n\nMarc Toussaint, Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum. Differentiable physics and stable modes for tool-use and manipulation planning-extended abtract. In IJCAI, pp. 6231±6235, 2019. 19\n\nShubham Tulsiani, Hao Su, Leonidas J Guibas, Alexei A Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3\n\nSjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353, 2018. 3\n\nHanqing Wang, Wei Liang, and Lap-Fai Yu. Transferring objects: Joint inference of container and\n\nhuman pose. In International Conference on Computer Vision (ICCV), 2017a. 3\n\nXiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge watching: Scaling affordance learning from sitcoms. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017b. 3\n\nYian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas J Guibas, and Hao Dong. Adaafford: Learning to adapt manipulation affordance for 3d articulated objects via few-shot interactions. In European Conference on Computer Vision (ECCV), pp. 90±107. Springer, 2022. 3\n\nJiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, and Lihua Xie. Multi-path region mining for weakly supervised 3d semantic segmentation on point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4384±4393, 2020. 3\n\nFanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. SAPIEN: A simulated part-based interactive environment. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2, 6, 24\n\nDan Xie, Sinisa Todorovic, and Song-Chun Zhu. Inferring\" dark matter\" and\" dark energy\" from\n\nvideos. In International Conference on Computer Vision (ICCV), 2013. 3\n\nJie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas J Guibas, and Lin Gao. Dsg-net: Disentangled structured mesh net for controllable generation of fine geometry. arXiv preprint arXiv:2008.05440, 2020. 3\n\nKaizhi Yang and Xuejin Chen. Unsupervised learning for cuboid shape abstraction via joint segmen-\n\ntation from point clouds. ACM Transactions on Graphics (TOG), 2021. 3, 5\n\nHong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance fields.\n\narXiv preprint arXiv:2107.07905, 2021. 3\n\nLap-Fai Yu, Noah Duncan, and Sai-Kit Yeung. Fill and transfer: A simple physics-based approach for containability reasoning. In International Conference on Computer Vision (ICCV), 2015. 3\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nYibiao Zhao and Song-Chun Zhu. Scene parsing by integrating function, geometry and appearance\n\nmodels. In Conference on Computer Vision and Pattern Recognition (CVPR), 2013. 3\n\nYixin Zhu, Yibiao Zhao, and Song-Chun Zhu. Understanding tools: Task-oriented object modeling, learning and recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 3\n\nYixin Zhu, Chenfanfu Jiang, Yibiao Zhao, Demetri Terzopoulos, and Song-Chun Zhu. Inferring forces and learning human utilities from videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 3\n\nYixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, et al. Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. Engineering, 6(3):310±345, 2020. 3\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nsittable\n\nframework\n\nbackrest\n\narmrest\n\nReal-world Mesh\n\nInput\n\nReconstructed\n\nSlot 1\n\nSlot 2\n\nSlot 3\n\nSlot 4\n\nFigure 8: Qualitative results on real-world objects (chairs and sofas) from Replica (Straub et al., 2019).\n\nsupport\n\nframework\n\nReal-world Mesh\n\nInput\n\nReconstructed\n\nSlot 1\n\nSlot 2\n\nFigure 9: Qualitative results on real-world objects (tables and cabinets) from Replica (Straub et al., 2019).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\ndisplay\n\nwrapgrasp\n\npressable\n\nInput\n\nGround Truth\n\nReconstructed\n\nSlot 1\n\nSlot 2\n\nSlot 3\n\nFigure 10: Qualitative results on objects (laptops and displays) in our adisplay\" subset.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\ncutting\n\nhandle\n\nInput\n\nGround Truth\n\nReconstructed\n\nSlot 1\n\nSlot 2\n\nFigure 11: Qualitative results on objects (knives and scissors) in our acutting\" subset.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Number of object categories covered by each affordance category.\n\n(b) Number of object instances covered by each affordance category.\n\nFigure 12: Statistics of our part affordance dataset.\n\nA EXPERIMENTS ON MORE AFFORDANCES\n\nTable 3: Quantitative results on the “display” subset and the “cutting” subset.\n\nWe conduct further experiments and report performance on more affordance categories on the long tail of the dataset distribution. More specifically, we include the subsets adisplayo and acuttingo. The adisplayo set includes part-level affordances {display, wrapgrasp, pressable} from 1,441 object instances, most of which are laptops and displays. The acuttingo set includes affordances {cutting, handle} from 641 object instances, most of which are knives and scissors on the long tail of the dataset distribution. Quantitative results are shown in Tab. 3 and qualitative results are shown in Fig. 10 and Fig. 11. Our method still shows a significant advantage over the baseline methods on two core metrics: mIoU for part affordance discovery and MSE for 3D reconstruction.\n\nModel Slot MLP Ours w/o Afford & Cuboid Ours w/o Cuboid Ours (full) Slot MLP Ours w/o Afford & Cuboid Ours w/o Cuboid Ours (full)\n\n0.0163 0.0146 0.0140 0.0135 0.0030 0.0028 0.0027 0.0026\n\n97.2 N/A 80.6 82.0 97.7 N/A 96.9 97.7\n\n17.2 14.8 47.0 47.6 21.6 27.8 28.1 29.8\n\nmIoU (%) ↑ MSE ↓ AP (%) ↑\n\ndisplay\n\ncutting\n\nB ATTENTION VISUALIZATION\n\nFollowing the settings in Locatello et al. (2020), we train the models using T = 3 attention iterations in the slot attention module. In Fig. 13, we observe that each slot gradually attends to the correct part of the object as the number of attention iterations increases. The attention mask at t = 3 depicts the silhouette of the reconstructed shape and parts.\n\nC FURTHER ANALYSIS & DISCUSSION\n\nC.1 NUMBER OF SLOTS\n\nWe set the number of slots as the maximal number of affordance labels that appear in one subset, which is different from previous object-centric learning algorithm (Locatello et al., 2020), where the number of slots could be arbitrary. This is because when we increase the number of slots, we also increase the ambiguities of the affordance composition and set matching at the same time. It prevents the model from learning accurate correspondence between affordance labels and parts. As shown in Fig. 14, the model learns the asittableo in a anullo slot.\n\nC.2 SELECTION OF STUDIED AFFORDANCES\n\nAlthough we annotate objects with 24 affordance categories, we benchmark PartAfford only on three subsets with 7 kinds of affordances in this work. The main reason is that we find it much more challenging to discover part-level affordance for some other affordance categories. For example, arollableo and acuttingo usually connect to tiny object parts that are challenging to be segmented from objects, ailluminationo and adisplayo cannot be distinguished from the objects without a deeper understanding of the visual appearance and reflections, apressableo and apinchableo require richer interactions to be discovered. In summary, it is either especially challenging to segment or requires more than geometric information (e.g., active interactions) to discover the other affordance categories.\n\n18\n\n02500500075001000012500150001750020000hangingsteptwistablelyableheadrestaudiblecuttingpinchablerollablepressableliquidcontainmentwrapgrasppourableleverilluminationdisplaycontainmentopenablearmresthandlesittablebackrestsupportframeworkUnder review as a conference paper at ICLR 2023\n\nFigure 13: Example affordance discovery model trained with T = 3 attention iterations. Attention is visualized in various colors and point radius. The point is colored according to the predicted affordance label of the slot which attends the most to the point. Point radius positively depends on the maximal attention value across the slots. We use trilinear upsampling to rescale the attention mask to the input resolution (32 × 32 × 32).\n\nFigure 14: Model performance when the number of slots increases. The model learns the asittableo part in a anullo slot.\n\nWe believe that future solutions to our task could involve those interaction cues to take the best of both worlds, further improving the performance and extending the scope. Our solution could also provide an intuitive prior for active learning methods, which could boost learning efficiency compared to learning from scratch. We hope to further explore the learning of these affordances in the future.\n\nC.3 AMBIGUITIES IN AFFORDANCE LEARNING\n\nAffordance is naturally ambiguous since the functions of object parts are rich. This work provides a well-defined benchmark to study how to learn affordance from accurate affordance definition and sparse set supervision. In another work, (Deng et al., 2021) models the multiple affordances with a mixture of distributions. Our dataset has supported learning multiple affordances per part; we annotate a single part with multiple affordances (see details in Sec. 5). Yet, as the first step towards affordance discovery, our current method focuses on learning the most important affordance. We believe our current learning framework could also be extended to learn multiple affordances by switching the one-hot affordance label to multi-hot labels. We also suggest further fine-tuning the multi-hot affordance prediction based on the one-hot version, since the affordance prediction of single-hot labels appears significant to guide the slot competition from our ablation study.\n\nC.4 POTENTIAL IMPACTS FOR VISION & ROBOTICS COMMUNITY\n\nBy learning to discover the part-level affordance, our model could facilitate the understanding of human-object interaction and object manipulation. As shown in Fig. 15, the learned affordance could be applied to synthesize potential human actions and interactions with various 3D objects. Additionally, our work could provide priors for robotic cross-category object manipulation and task planning (Toussaint et al., 2019; Garrett et al., 2020; 2021). For example, Hadjivelichkov et al. (2022) demonstrates a single reference image of an object with annotated affordance regions can help the robot to use the affordance skill in the real-world setting, where our method can provide generalizable affordance information on the 3D shapes without the need for manual annotation.\n\n19\n\n Input GT Recon. Attn. t=1 Attn. t=2 Attn. t=3 sittableframeworkbackrestarmrestnullsittablebackrestframeworkInputRecon.Slot 1Slot 4Slot 3Slot 2GTSlot 5Slot 6Under review as a conference paper at ICLR 2023\n\nOrig. Scaled\n\nOrig. Scaled\n\nOrig. Scaled\n\nsittable 72.44 74.95\n\nbackrest 80.51 81.36\n\nrollable 53.33 58.43\n\nsupport 82.92 83.68\n\narmrest 73.19 73.63\n\npourable 68.15 73.89\n\nframe. 74.74 77.87\n\npress. 78.70 83.57\n\ntwist. 40.24 53.97\n\ncontain. 54.01 75.01\n\nhandle 38.09 49.31\n\nlever 60.34 64.24\n\nliquid. 20.34 69.42\n\nillum. 24.57 24.59\n\npinch. 62.07 66.84\n\nopenable 58.62 66.26\n\nwrapgrasp 63.07 71.48\n\naudible 56.67 46.11\n\ndisplay 87.26 90.03\n\nlyable 46.24 45.32\n\ncutting 83.29 79.67\n\nheadrest 34.73 32.76\n\nTable 4: Supervised affordance segmentation results (category mIoU %). aOrig.\" refers to the original PartNet dataset with 3D shapes scaled to a unit bounding sphere. aScaled\" refers to rescaling 3D shapes to real-world dimensions. aavg\" refers to shape average Intersection-over-Union (IoU).\n\nFigure 15: 3D Human synthesis conditioned on inferred part affordance using (Hassan et al., 2021).\n\nD SUPERVISED AFFORDANCE ESTIMATION\n\nApart from the PartAfford task, we also benchmark the state-of-the-art 3D auto-encoder for the supervised affordance segmentation task in a cross-category fashion on our proposed dataset. Evaluation is reported on the Intersection-over-Union (IoU) metric following Mo et al. (2019b), as shown in Table 4.\n\nNote that the baseline is trained and tested on all object and affordance categories. In contrast, semantic segmentation in PartNet (Chang et al., 2015; Mo et al., 2019b) is trained on each object category separately. Results show that the algorithm could still achieve high performance, which demonstrates that our affordance annotation is reasonable and consistent.\n\nPartNet normalizes every object shape into a unit bounding sphere. This is not realistic since objects in different categories may have very different dimensions. For example, a mug is much smaller than a bed. If normalized in the same way, cross-category training performance may be hurt to some extent. Thus, we calculate the average real-world 3D dimensions of each object category from metadata of ShapeNet (Chang et al., 2015) and scale the point cloud in our part affordance dataset according to its object category.\n\nE MODEL GENERALIZATION TO UNSEEN OBJECTS\n\nTo further demonstrate the generalizability of our methods qualitatively, we additionally provide examples for real scanned and more diverse objects from the Replica (Straub et al., 2019) dataset, results shown in Fig. 8 and Fig. 9. Although the scan quality is not perfect, our learned model generalizes well and can reconstruct and identify the functional parts given novel objects.\n\nF DETAILS ON EXPERIMENTS\n\nBenchmarks Below, we describe the statistics of objects and the related affordances for these subsets; we discuss detailed reasons about why we choose these three subsets in Appendix C.2.\n\n• aSittableo: We collect all object instances that have affordance asittableo; most of them are chairs and sofas. Their part-level affordances belong to the set {sittable, backrest, armrest, f ramework}. We split the training, validation, and test set in the ratio of 7 : 1 : 2. In total, we have 5, 093 instances for training and 1, 457 for test.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n• aSupporto: We collect objects with affordance asupport\", mainly from categories table and cabinet. Their affordances belong to {support, f ramework}. There are 7, 974 instances for training and 2, 279 instances for test.\n\n• aOpenableo: This subset contains objects from frige, dishwasher, washing machine, and microwave. Their affordances belong to {openable, f ramework, handle}. There are 807 instances for training and 232 instances for test.\n\nData Augmentation To enrich affordance compositions, we augment the training data by randomly removing certain object parts with corresponding affordance labels.\n\nG MORE MODULE ABLATION\n\nG.1 SOFT K-MEANS\n\nSlot attention, as a generalized soft k-means algorithm, could be reduced to the soft k-means algorithm according to Locatello et al. (2020). It turns out that the reduced model can achieve 23.0% Mean IoU on \"sittable\" objects, which is slightly better than the Slot MLP baseline but significantly worse than our full model using slot attention (57.3%). It reconstructs the whole shape with a quality (MSE: 0.0096) on par with the full model (MSE: 0.0097). Its affordance set prediction accuracy is the lowest among all models (AP: 0.87). Overall, the soft k-means algorithm cannot effectively segment the shape or discover affordance parts.\n\nH ADDITIONAL EXPERIMENTAL RESULTS\n\nWe show additional qualitative results for our full model’s affordance discovery results on asittable\" (Fig. 16), asupport\" (Fig. 17), aopenable\" (Fig. 18) objects, respectively.\n\nI PART AFFORDANCE DATASET\n\nHere we report more statistics and details on how we define and annotate the affordance labels in our dataset. As shown in Fig. 12, most of the affordance categories are annotated on more than 2 kinds of objects. Our curated subsets cover the most common affordance categories (top 7), as well as some rare affordance categories (e.g., acuttingo) on the long tail.\n\nI.1 PRINCIPLES FOR AFFORDANCE ANNOTATION\n\nTo keep annotations consistent across object categories, we design a guideline for affordance annotation. Below are some general principles to annotate a leaf part of an object instance with affordances.\n\nMultiple affordances. A part can afford multiple kinds of human actions. For example, the seat of chair could afford sittable for resting of human body or support if one wants to place some books on it. We refer to ConceptNet (Speer et al., 2017), a giant knowledge database, for annotating common usage of object parts.\n\nPrioritized fine-grained affordances. When there are multiple affordances labels for a part, we give the more fine-grained affordance label higher priority. For the chair seat in the example above, sittable is prioritized compared with support as it is a fine-grained support affordance for body resting.\n\nArticulation-related affordances. The PartNet dataset does not contain articulation information, which makes affordances such as openable not geometrically distinguishable. Thus, we also generate a set of shapes with openable affordance from the PartNet-Mobility dataset by capturing 3D shapes with various opening angles. More geometric variation helps models to learn articulation-related affordances.\n\nI.2 AFFORDANCE DESCRIPTIONS\n\nOur description for each affordance contains a brief definition, some supplemental clarification and priority statements if needed, and some example leaf nodes in the part hierarchy of various reasonable objects (full path from root to leaf).\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nsittable: Indicates whether the object can be used for sitting. Anything sittable of course affords support, and the requirement for a supporting object to be sittable is that it must be both comfortable and safe for human seating. For example, a table is not sittable despite affording support because it is not comfortable. Sittable is given priority over potential co-existing affordances like support.\n\nE.g., chair/chair_seat/seat_surface.\n\nsupport: A trait of objects which can safely keep other objects on top of themselves. Common characteristics of support-affording objects are that they are flat and can support multiple objects at once. A key distinction between support-affording objects and non-supporting objects is that support-affording objects will remain stable when other objects are placed on them. For example, a table is a supporting object because it is just as stable with objects on it as it is without. However, a stack of plates is not supporting because they become more unstable as you add more plates.\n\nE.g., table/regular_table/tabletop,\n\nstorage_furniture/cabinet/shelf, bed/bed_unit/bed_sleep_area.\n\nopenable: Parts which may be moved with a hinge-like mechanism on an articulated object. Openable objects do not need to afford handles, but usually handles can be found attached to the openable part. Openable parts are distinct from other moving parts in that they need to swing to some degree to be moved. Openable is given priority over potential co-existing affordances like containment.\n\nE.g., door/door_body/surface_board,\n\ndish_washer/body/door/door_frame, microwave/body/door.\n\nbackrest: Objects which are reasonably designed for providing support to a person’s back. By reasonably designed, we mean either specifically (as in the back of a chair) or can afford back support if a person wanted to sit upright (like a headboard).\n\nE.g., chair/chair_back/back_support.\n\narmrest: Objects which are specifically designed to support an arm. For example, a table can support a variety of things and is thus not an armrest. A chair’s arm is the perfect size for a human arm, so it must be an armrest.\n\nE.g., chair/chair_arm.\n\nhandle: An object extension which affords the ability to open an attached ‘openable’ part. Handles are mostly grabbed with hand-wrapping, so it’s important to only afford ‘handle’ to parts which are specifically involved in an opening mechanism, like a door handle.\n\nE.g., storage_furniture/cabinet/cabinet_door/handle,\n\ntable/regular_table/table_base/drawer_base/ cabinet_door/handle, mug/handle, dish_washer/body/door/handle, bag/bag_handle.\n\nframework: Any object segment which either: a) helps to define the shape of the object as a whole or b) is an unaffording extension of the object or connector for other segments. For example, a hinge affords framework because it is integral to connects the door to the door frame. Overall, this affordance is the most general of all, so it should only be used when it clearly applies to either case of the definition. For example, most handles do not afford framework because it is both a small part of an object’s shape and already affords handle. It has the least priority among potential co-existing affordances.\n\nE.g., chair/chair_base.\n\ncontainment: An affordance of object which can store physical items. The size of the physical items does not matter, so long as they are not too small. For example, anything that can only contain objects smaller than say a marble do not afford containment. Also, items that afford containment must afford security to the items they contain, such that they will not fall out.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nE.g., table/regular_table/table_base/drawer_base/drawer,\n\nstorage_furniture/cabinet/drawer, mug/container, trash_can/container, refrigerator/body, bowl/container, bag/bag_body, bottle/normal_bottle/body.\n\nliquidcontainment: A more specific version of the containment affordance. Objects that afford liquid-containment must be able to safely contain liquid. Examples of these are bottles, bath tubs, etc.\n\nE.g., mug/body,\n\nbottle/normal_bottle/body.\n\ndisplay: Something which visualizes information for a useful purpose. Examples of these would be monitor screens or a clock surface.\n\nE.g., display/display_screen/screen, laptop/screen_side/screen, clock/table_clock/clock_body/surface.\n\ncutting: The quality of being able to slice through other objects. Certain things that can cut are not considered to have cutting affordance if it was used against its intended purpose, like smashing a glass vase. Cutting is only afforded to objects which are specifically designed for cutting, like a blade-edge.\n\nE.g., cutting_instrument/knife/blade_side, scissors/blade_handle_set/blade.\n\npressable: A mechanical feature of objects which either have buttons or can interact with a finger. Good examples of these are keyboard keys.\n\nE.g., keyboard/key.\n\nhanging: A part which can be hung on another object. These parts almost always only serve the purpose of hanging the rest of the entire object. An example of this would be a shoulder strap for a handbag.\n\nE.g., bag/shoulder_strap.\n\nwrapgrasp: The ‘wrap-grasp’ trait is afforded by parts which are explicitly meant to be grabbed in a hand-wrapping motion. Just because a hand can wrap around an object part does not mean it affords wrap-grasp. It must be useful to grip the part in this way. An example of this would be a ladder rung, which a person is meant to wrap their hand around to climb the ladder.\n\nE.g., cup,\n\nbed/ladder/rung.\n\nillumination: The affordance of light emission. This only applies to object parts which are meant to light up a broad area. For example, a monitor screen does not afford illumination despite emitting light because it is not supposed to be used to light up the area around it.\n\nE.g., lamp/table_or_floor_lamp/lamp_unit\n\n/lamp_head/light_bulb.\n\nlyable: Indicates that a human can comfortably rest his/her entire body on the object. These objects are usually flat with a soft surface. Lyable is given priority over potential co-existing affordances such as sittable and support.\n\nE.g., bed/bed_unit/bed_sleep_area/mattress.\n\nheadrest: An extension of an object which is oriented so a human head can rest comfortably on it. Examples of these are chair headrests or bedframe headrests.\n\nE.g., bed/bed_unit/bed_sleep_area_pillow, bed/bed_unit/bed_frame/headboard.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nstep: A part which affords the human foot climbing or resting functionality. For example, a ladder rung is a step because it affords climbing with both hands and feet. A foot pedestal is also a step because it can be stood on or feet can be rested on it.\n\nE.g., bed/ladder/rung.\n\npourable: Meant for parts which liquid can flow out of. Things that are pourable may also be dependent on a mechanism for controlling flow, like a bottle cap or a knob.\n\nE.g., bottle/normal_bottle/mouth,\n\nbottle/jug/body.\n\ntwistable: These objects can either be detached or provide special functionality by twisting them in a clockwise or counterclockwise motion. Examples include bottle caps and knobs.\n\nE.g., bottle/normal_bottle/lid,\n\nbottle/normal_bottle/mouth.\n\nrollable: A part which can roll to move around. Exceptions to this affordance are objects which roll but stay fixed in place, like a rocking chair.\n\nE.g., wheel.\n\nlever: Any handle which can rotate up to a point. For example, knobs rotate but are not levers because they do not provide handles. Levers must be treated differently from twistable objects or handles because if they are twisted too much they will break.\n\nE.g., lever.\n\npinchable: An object which is small enough such that it can be manipulated by pinching with two or more fingers. Things that are pinchable must not be heavy, and they usually fit inside the palm of a hand.\n\nE.g., earbud.\n\naudible: Anything which emits sound. This does not include sound emitted indirectly, such as a door creaking when opened, which makes sound as a side-effect.\n\nE.g., headphone/padding.\n\nI.3 LICENSE\n\nOur dataset is annotated based on PartNet (v0) (Mo et al., 2019b) and PartNet-Mobility (v2.0) (Xiang et al., 2020), both of which are licensed under the terms of the MIT License.\n\nJ CODE AND DATA\n\nCode, data, and instructions needed to reproduce the main experimental results are available in the supplementary materials.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 16: Additional qualitative results on objects in our asittable\" subset.\n\n25\n\nsittableframeworkbackrestarmrest Input GT Recon. Slot 1 Slot 2 Slot 3 Slot 4 Under review as a conference paper at ICLR 2023\n\nFigure 17: Additional qualitative results on objects in our \"support\" subset.\n\n26\n\nsupportframework Input GT Recon. Slot 1 Slot 2 Under review as a conference paper at ICLR 2023\n\nFigure 18: Additional qualitative results on objects in our aopenable\" subset.\n\n27\n\nopenableframeworkhandle Input GT Recon. Slot 1 Slot 2 Slot 3",
    "reference": "# Summary Of The Paper\n\nThis paper proposes the new task of part-level affordance discovery: it is a joint task of decomposing 3D shapes into their parts and predicting how each part corresponds to affordances. A learning framework has been proposed for this task. It learns to segment 3D shapes into parts from weak shape-level labels. It also associates affordances with predicted parts. The framework is powered by 3D position-embedded features and 3D slot attention. To facilitate this new task, a novel dataset that features part-level cross-category 3D object affordances is constructed. Extensive evaluation and ablation studies have been performed on the dataset.\n\n# Strength And Weaknesses\n\n### Strength\n- The new task, PartAfford, is interesting and I believe will promote new research in this direction.\n- The new dataset is also a valuable contribution to the community. I especially like the openable affordance shapes (Figure 3c).\n- The proposed method to discover 3D parts and associate affordance in a weakly supervised setting is interesting. \n- I like Section 6.4 which evaluates the generalization power of the proposed method on unseen objects.\n\n### Weakness\n- Although the 3D part affordance dataset contains 24 affordance labels, the experiment section only considers three: 'sittable', 'support', and 'openable'. I understand that they are very useful functions for everyday objects but I wish the method is evaluated on more categories for a comprehensive understanding of the task.\n\n- I know the paper proposes a new task. But I think it could compare its individual components to other methods mentioned in the related work. For example, (unsupervised) 3D part discovery work could be compared. Also, even a fully supervised dense affordance learning scheme can be compared as a reference too because it is common to have such a method in the evaluation as the performance upper bound reference.\n\n- Section 6.1 mentions that the current setting only allows one affordance per part. I think quite frequently an object part can afford multiple functions. It is not clear to me if the method is able to handle multiple affordance per part. \n\n- Comparing to Table 1, Table 2 doesn't have results of Slot MLP and IODINE. A complete table will be better.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n### Clarity\nThe paper is straightforward and easy to follow. Some editing suggestions:\n- Figure 1 left: I think **sparse** is confusing here. How about \"weak\"? \n- Figure 2 (d): the illustration of overlapping cuboids and parts is very confusing. It looks like that is another reconstruction while it is an intuitive illustration of the loss.\n\n### Quality\nThe quality of the work is good.\n\n### Novelty\nThe 3D part affordance dataset is novel. The proposed workflow contains components from prior work such as the 3D feature and position embedding and 3D slot attention mechanisms but the combination of everything to solve this new task is novel.\n\n### Reproducibility\nThere seems to be a sufficient amount of detail in the paper for reproducibility. The author will release the code and data.\n\n# Summary Of The Review\n\nOverall, I like this paper as it defines an interesting yet challenging task, i.e., jointly finding 3D parts and their affordance, and provides a viable solution. The new 3D part affordance dataset is also a nice contribution; it will promote more interesting research in this direction. There are a few places the paper could be improved as mentioned in the \"Weakness\" above. But I think the paper will be a good contribution to ICLR 2023 in its current form.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSELF-SUPERVISED OFF-POLICY RANKING VIA CROWD LAYER\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOff-policy evaluation (OPE) aims to estimate the online performance of target policies given dataset collected by some behavioral policies. OPE is crucial in many applications where online policy evaluation is expensive. However, existing OPE methods are far from reliable. Fortunately, in many real-world scenarios, we care only about the ranking of the evaluating policies, rather than their exact online performance. Existing works on off-policy ranking (OPR) adopt a supervised training paradigm, which assumes that there are plenty of deployed policies and the labels of their performance are available. However, this assumption does not apply to most OPE scenarios because collecting such training data might be highly expensive. In this paper, we propose a novel OPR framework called SOCCER, where the existing OPE methods are modeled as workers in a crowdsourcing system. SOCCER can be trained in a self-supervised way as it does not require any ground-truth labels of policies. Moreover, in order to capture the relative discrepancies between policies, we propose a novel transformer-based architecture to learn effective pairwise policy representations. Experimental results show that SOCCER achieves significantly high accuracy in a variety of OPR tasks. Surprisingly, SOCCER even performs better than baselines trained in a supervised way using additional labeled data, which further demonstrates the superiority of SOCCER in OPR tasks.\n\n1\n\nINTRODUCTION\n\nOff-policy evaluation (OPE) aims to estimate online performance of given policies using only historical data collected by some other behavior policies. It is crucial to deploying reinforcement learning (RL) to real-world applications, such as trading, advertising, autonomous vehicles and drug trials, where online policy evaluation might be highly expensive. OPE also becomes increasingly important in causal inference and model selection for offline RL (Oberst & Sontag, 2019; Nie et al., 2021).\n\nMost existing works on OPE focus on estimating the online performance of target policies and can be categorized into three classes: Inverse Propensity Scoring (IPS) based methods, Direct Methods (DM) and Hybrid Methods (HM). Unfortunately, existing OPE methods are far from reliable in real applications. Standard IPS based estimators such as importance sampling suffer from high variance due to the product of importance weights (Hanna et al., 2019). DM requires extra estimators of environmental dynamics or value functions, which are hard to learn when the observation data is high-dimensional or insufficient. HM such as doubly robust estimators combine IPS and DM (Jiang & Li, 2016), yet it often comes with additional hyperparameters that need to be carefully chosen.\n\nFortunately, in many real-world scenarios, we do not need to estimate the exact online performance of target policies. Instead, we only care about which policy would perform the best when deployed online. This inspires us to develop a policy ranker that focuses on predicting the ranking of target policies regarding to their online performance. A recent work proposes a policy ranking model called SOPR-T (Jin et al., 2022), which is trained in a supervised paradigm under the assumption that there are plenty of extra deployed policies whose performance can be used as supervision signals. However, this assumption is impracticable in many real-world OPE tasks since collecting online performance of policies can be extremely expensive. In addition, SOPR-T directly maps the data of state-action pairs to a score, yielding a low-efficient policy representation scheme which fails to capture the relative discrepancies between policies.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn this paper, we propose a novel Self-supervised Off-poliCy ranking model based on Crowd layER (SOCCER) to address the above challenges. The novelty of SOCCER is two-fold. First, we employ a crowdsourcing paradigm to solve the OPR problem, where the workers come from a diverse pool of existing OPE methods, who provide labels of whether a policy would perform better than another one. Note that these labels are constructed by comparing the estimated accumulated rewards of the target policies, thus our model can be trained in a self-supervised way. Moreover, we propose a novel Policy Comparison Transformer (PCT) architecture to learn efficient policy representations. Instead of directly mapping the state-action pairs to a policy embedding (as is done in SOPR-T), PCT learns pairwise representation of two policies capturing difference of them at the same set of states. With the help of PCT, our policy ranking model generalizes well in the policy space. Experimental results show that SOCCER not only achieves significant higher ranking performance than existing OPE methods, but also outperforms baselines trained using additional ground-truth labels.\n\n2 RELATED WORKS\n\nOff-policy evaluation/ranking. The goal of OPE is to precisely predict the online performance of target policies given trajectory data collected by some other behavior policies. Standard importance sampling approach suffers from exponential variance with respect to the time horizon (Li et al., 2015; Jiang & Li, 2016). Recent works such as Fitted-Q evaluation (Hoang et al., 2019) and marginalized importance sampling (Liu et al., 2018) achieve polynomial variance, yet they rely on additional function approximators. Direct methods avoid the large variance by learning the dynamic model or Q-function, which could be biased especially when the data is insufficient. Some works study the hyperparameter-free policy selection problem, yet their method only applies to Q-learning based policies (Zhang & Jiang, 2021). A recent work directly studies the OPR problem, where it collects online performance of a large set of policies and uses these labeled data to train a policy ranker (Jin et al., 2022). However, collecting such data might be extremely expensive in many applications.\n\nLearning from crowds. Crowdsourcing systems enable machine learners to collect labels of large datasets from crowds. One big issue with crowdsourcing systems is that the labels provided by crowds are often noisy (S. & Zhang, 2019). To tackle this challenge, various probabilistic generative methods are proposed for statistical inference (Yuchen et al., 2016; Tian & Zhu, 2015). Another line of works use discriminative models that find the most likely label for each instance (Jing et al., 2014; 2015). A recently work called Crowd Layer (CL) first describes an algorithm for jointly learning the target model and the reliability of workers (Filipe & Pereira, 2018). CL proposes a simple yet efficient crowd layer that can train deep neural networks end-to-end directly from the noisy labels. In our work, we treat existing OPE methods as workers and adopt CL to process multiple noisy labels, because CL is naturally compatible with our model.\n\nPolicy representation. Compact but informative representations of policies not only benefit the policy learning process (Tang et al., 2022), but also help with the policy transfer among different tasks (Isac et al., 2019; G. et al., 2017). A straightforward idea is to represent a policy by its network parameters, yet this leads to a very sparse representation space. Network Fingerprint (Harb et al., 2020) proposes a differentiable representation that uses the concatenation of the vectors of actions outputted by the policy network on a set of probing states. Some recent works try to encode policy parameters as well as state-action pair data into a low-dimensional embedding space (Tang et al., 2022; Jin et al., 2022). However, existing works focus on single policy representations, which fail to capture the relative discrepancies between policies.\n\n3 PROBLEM STATEMENT\n\nMarkov decision process. We consider the underlying environment as a Markov decision process (MDP) and define an MDP as a finite–horizon tuple M = (S, A, T , P, R, γ). Here, S is the state space, and A is the action space. T is the length of time horizon. P and R are the transition function and the reward function, respectively. P(st+1|st, at) represents the probability of transitioning from state st to state st+1 ∈ S when the agent takes action at ∈ A under state st ∈ S and R(st, at) represents the immediate reward the agent receives. The expected return of a policy π can be computed by EP [(cid:80)T\n\nt=1[γtR(st, π(st)]], where γ ∈ (0, 1] is the discount factor.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nOff-policy ranking. The goal of OPE is to estimate the expected return of a policy π without deploying it online, given an offline dataset D = {τi}N i=1, where τi = (si,0, ai,0, ri,0, · · · , si,T , ai,T , ri,T ) are trajectories generated by some behavior policies. OPE is usually used for model selection: We are required to select the most promising policy from a candidate set of available policies before actual deployment. Take recommender systems as example, we can easily obtain a set of candidate policies by adjusting the training data or the hyperparameters of the model. However, we often need to select very few policies from the candidates for online test, since a bad policy would harm the user experience. Therefore, we care more about the ranking of the candidate policies, instead of their exact expected reward. We formally define the off-policy ranking problem as follows.\n\nDefinition 3.1 (Off-Policy Ranking, OPR). Given a set of trajectory data D = {τi}N by some behavior policies and a set of target policies Π = {πj}M ranking of the target policies that aligns with their online expected accumulated rewards.\n\ni=1 generated j=1, an OPR algorithm outputs a\n\nIntuitively, OPR should be easier than OPE, since the solution of OPE also implies the solution of OPR. However, OPR faces some unique challenges. First, since the policy space might be extremely large, we need efficient policy representations that capture their relative differences so that the policy ranker could generalize across the policy space. Second, we lack the ground-truth ranking of the policies in the training set, thus the direct supervised learning approaches do not apply. We will elaborate on how we address these challenges in Section 4.\n\n4 APPROACH\n\nIn this section, we elaborate on how the OPR problem can be addressed under our SOCCER framework. SOCCER takes the offline trajectory data D and two target policies πi, πj as inputs and outputs the probability of whether πi would perform better than πj. We begin by introducing how to learn effective pairwise policy representations under a novel transformer-based architecture. Then, we will introduce how to train our model using the labels provided by existing OPE methods.\n\n4.1 LEARNING PAIRWISE POLICY REPRESENTATIONS\n\nPairwise Policy Representation. A policy is generally considered as a conditional distribution over actions given current state. Therefore, a policy can be naturally represented by a set of stateaction pairs where the actions are sampled from the policy. However, such a straightforward policy representation could be inefficient since the number of state-action pairs can be extremely large. Previous works address this issue by extracting high-level features from the state-action pairs using deep neural networks (Jin et al., 2022). Although these representations reflect the features of single policies, they fail to capture the discrepancies of different policies at some crucial states.\n\nTo this end, we aim to learn pairwise policy representations by comparing two policies’ decisions at the same set of states. Formally, given a set of states {s1, ..., sK} and two policies πi, πj, we can construct the following sequence of state-action pairs by taking actions at these states:\n\nτi,j =< (s1, ai\n\n1), (s1, aj\n\n1), · · · , (sK, ai\n\nK), (sK, aj\n\nK) >,\n\n(1)\n\nwhere ai ∼ πi(·|s), and aj ∼ πj(·|s). We denote by χi,j = g(τi,j) ∈ Rn the pairwise policy representation where g is a function that maps τi,j to an n-dimensional representation space. Since our goal is to predict whether πi performs better than πj, therefore a pairwise policy representation should indicate the order of the two policies. We regard χi,j and χj,i as different representations and will show how to learn them using transformers. In addition, since the datasets are often very large, computing the policy representations using all the states can be extremely slow. In practice, we use a sampled set of states to compute the approximated representations during training, and take the averaged output results of multiple samples as the final representation during inference.\n\nPolicy Comparison Transformer (PCT). Transformers are proved to be effective for learning dependencies between different positions in sequences. Prior works has employed transformers to extract features from trajectory sequences (Lili et al., 2021; Michael et al., 2021). However, existing transformer architectures fail to capture the differences of two policies’ decisions. In our work, we propose the PCT architecture to learn pairwise policy representations. Unlike previous works where\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Construction of input tokens.\n\nthe positional encodings indicate the positions of state-action pairs in a trajectory, PCT uses positional encoding to distinguish the relative order of two policies. In this way, the learned pairwise policy representation χi,j can be directly used to predict whether πi performs better than πj.\n\nFigure 1 shows the construction of input tokens. We first sample K states from D and then use a linear encoder f to map the K state-action pairs into 2K tokens: k = f (sk, ai\n\nk = f (sk, aj\n\nk = 1, ..., K\n\nk), xj\n\nk),\n\n(2)\n\nxi\n\nwhere i and j represent the indexes of two policies. In order to represent the relative order of πi and πj, we introduce two one-hot positional encodings eα = [1, 0] and eβ = [0, 1], where eα indicates the policy ranked higher and eβ indicates the policy ranked lower. We also use an aggregation token e0, which is a learnable vector for aggregating the information from the other 2K tokens (Zhu et al., 2021). The final inputs that indicate πi ranked higher than πj can be represented as:\n\nzi>j = [e0, xi\n\n1 + eα, xj\n\n1 + eβ, · · · xi\n\nK + eα, xj\n\nK + eβ]\n\n(3)\n\nThis construction of inputs has two advantages. First, the two policies share the same set of states, thus their discrepancies are naturally represented by the different actions taken at these states. Second, we can easily get a mirrored representation zj>i by simply exchange the positional encoding eα and eβ used in zj>i .\n\nWe adopt a widely used transformer architecture as our encoder (Dosovitskiy et al., 2021). It contains L alternating layers of multi-head self-attention (MSA) and multi-layer perception (MLP) blocks. Layernorm (LN) and residual connections are applied to the outputs of each block. For brevity, we re-write the inputs in Equation 3 as z(0). And the computations at each block can be represented as:\n\nˆz0 = MSA(LN(z(l−1))) + z(l−1) zl = MLP(LN(ˆz(l−1))) + ˆzl−1\n\nl = 1, · · · , L\n\nl = 1, · · · , L\n\n(4)\n\nχi,j = LN(z0\n\nL).\n\nThe final pairwise policy representation χi,j is the corresponding outputs of the aggregation token e0 taken from z(L). Note that χi,j changes to χj,i when we exchange the positional encodings eα and eβ, but they are permutation invariant to the order of inputted state-action pairs.\n\n4.2 A CROWDSOURCING APPROACH TO OPR\n\nIn this section, we will introduce how to train the PCT in two cases regarding to the existence of ground-truth ranking labels of policies. First, we show that the policy ranking problem can be reduced to a binary classification problem since our pairwise policy representations can be directly used to predict the ranking of two policies. Second, we will introduce a novel crowdsourcing system where multiple OPE methods are modeled as workers. We will also show how to train the PCT leveraging the inaccurate labels provided by the workers.\n\ni, Ri)}T\n\nReducing OPR to binary classification. We first consider the case when there is a training set Π′ = {(π′ i as well as their real expected accumulated rewards Ri. In this case, we can directly construct binary labels by comparing the performance of the two policies. We use an indicator 1Ri>Rj to represent the label of a pair of policies (πi, πj). The PCT can be trained by minimizing the following binary cross entropy loss:\n\ni=1 consisting of T deployed policies π′\n\nLsup = − E\n\nπi,πj ∼Π′\n\n[(1Ri>Rj ) · log(ˆyi,j) + (1Ri≤Rj ) · log(1 − ˆyi,j)],\n\n(5)\n\n4\n\n Trajectory Dataset . . . Sample . . . . . . Make decisions by πm and πn Project (s,a) to tokens Sampled States Pairwise Decision Sequence Input Tokenss1s2sKs1s1s2s2a2sKsKia2jaKiaKja1ia1jx1ix1jx2ix2jxKixKjUnder review as a conference paper at ICLR 2023\n\nFigure 2: The framework of SOCCER.\n\nwhere ˆyi,j = sigmoid(φ(χi,j)) represents the predicted probability that πi performs better than πj. φ is a function that projects χi,j to a real number. The final ranking of test policies is based on their scores computed by:\n\nscorei =\n\n1 N\n\n(cid:88)\n\nj̸=i\n\nˆyi,j, i = 1, ..., N,\n\n(6)\n\nwhich can be interpreted as the expected probability that πi performs better than other test policies Rodrigo et al. (2019).\n\ni, Ri)}T\n\nLearning from crowds by Crowd Layer. Supervised training is efficient when the dataset Π′ = {(π′ i=1 contains enough policies. Unfortunately, collecting such training data can be extremely expensive in many real applications. Meanwhile, we note that although existing OPE methods are not robust enough, they actually provide candidate solutions to the OPR problem. To this end, we aim to borrow ideas from crowdsourcing domain as an alternative way to approach the OPR problem. Specifically, suppose that there exists a set of OPE algorithms estimating the policy performance, we can treat these algorithms as crowd workers for generating inaccurate labels and make use of these labels to train our models. The intuition is that the inaccurate labels generated by OPE annotators are implicitly conditioned on the ground-truth performance of policies. If we can take advantage of these labels and learn their relationships with the ground-truth labels, our prediction ˆyi,j would be more close to the ground-truth labels.\n\nIn the framework of SOCCER, we adopt Crowd Layer (CL, (Filipe & Pereira, 2018)) as our backbone for learning from crowd labels. CL is able to automatically distinguish the good from the unreliable annotators and capture their individual biases in many other domains, such as image annotation (Guan et al., 2018; Li et al., 2022) and music genre classification (Rodrigues et al., 2013). In addition, CL is naturally compatible with deep learning approaches since it simply adds a crowd layer to the deep neural networks and can be trained in an end-to-end way. As shown in Figure 2, we add CL to the top of our predicted probability ˆyi,j. During training, CL adjusts the gradients coming from these noisy labels according to its reliability by scaling them and adjusting their bias. The adjusted gradients are then backpropagated to PCT according to the chain rule.\n\n5\n\n Policy Comparison Transfrormer Encoder Linear Projection. . . x1ix1jx2ix2jxKixKje0eαeβeαeβeαeβ Pairwise Policy Representation χi,j Aggregation Token Policy Positional Embeddings Input Tokens yi,j Trajectory Dataset { πi , πj } OPE Worker Set Noisy Binary Labels Estimated Noisy Binary Labels L C LCrowd Layer Probability of whether perfoms better than πi πj Under review as a conference paper at ICLR 2023\n\nFormally, assume that there are R annotators of OPE methods. For each annotator r, its estimation about the expected return of πm is denoted as Rr m. The goal of CL is to train a mapping function ˆyr i,j = ζ r(ˆyi,j) to predict the noisy binary label generated by annotator r: yr . The overall objective can be written as:\n\ni,j = 1Rr\n\ni >Rr\n\nj\n\nLCL = − E\n\nr=1,··· ,R πi,πj ∼Π′\n\n[yr\n\ni,j · log(ˆyr\n\ni,j) + (1 − yr\n\ni,j) · log(1 − ˆyr\n\ni,j).]\n\n(7)\n\nThe complete training procedures of SOCCER is summarized in Algorithm 1. In practice, to reduce the computational cost brought by CL, we set ζ r as a linear projection followed by a sigmoid function. Therefore, the number of additional parameters only grows linearly with the number of annotators. Note that the CL is only available during training since we still use ˆyi,j to generate the predicted ranking of test policies.\n\n# The number of trainning epochs # The number of sampled states\n\nAlgorithm 1: SOCCER Training Data: Offline Trajectory set D, Policy set Π, OPE worker set W. Result: Policy comparison transformer g . N ← n; K ← k; Initialize the transformer g; Initialize the aggregation token e0; Initialize the policy positional embeddings eα and eβ; Initialize the token encoder f ; Initialize the linear projection function φ; Initialize the crowd layer ξ; while N ̸= 0 do\n\n# Initialize parameters\n\nSample 2 policies: πi, πj ∼ Π; Sample K states: s1, s2, · · · , sK ∼ D; k ∼ πi(·|sk), an Take actions by both policies: ai Construct a pairwise policy decision sequence:\n\nτi,j =< (s1, ai\n\n1), (s1, aj\n\n1), · · · , (sK, ai\n\nK), (sK, aj\n\nj ∼ πj(·|sk);\n\nK) >; k = f (sk, ai\n\nk), xj\n\nk = f (sk, aj\n\nk);\n\nEncode state-action pairs into input tokens: xi Add policy positional embeddings to tokens: K + eα, xj\n\n1 + eβ, · · · xi\n\n1 + eα, xj\n\nz0 = [e0, xi\n\nK + eβ] Get the pairwise policy representation: χi,j = f (z0); Get the probability of whether πi performs better than πj: ˆyi,j = o(χi,j); for each OPE worker r ∈ W do Generate the noisy labels: yr Get the estimated noisy labels by crowd layer ξr: ˆyr Compute the loss according to Equation (7); Backpropagate gradients to all parameters for minimizing the loss;\n\ni,j = ξr(ˆyi,j);\n\ni,j = 1Rr\n\ni >Rr\n\n;\n\nj\n\nend N ← N − 1;\n\nend\n\n5 EXPERIMENTS\n\nIn this section, we compare SOCCER with widely-used OPE methods on various tasks. We also present ablation studies with respect to PCT and CL, which are the main components of SOCCER.\n\n5.1 EXPERIMENTAL SETTINGS\n\nTrajectory Set. We evaluate SOCCER and all baseline OPE methods on D4RL dataset consisting of various trajectory sets (Fu et al., 2020). These sets of trajectory data are generated by different behavioral policies in different simulated environments. Overall, we adopt trajectory sets collected\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Comparing SOCCER with other OPE baselines.\n\nfrom 2 environments of Mujoco games: Hopper-v2 and Walker2d-v2. Besides, there are 3 different types of trajectory sets for each environment: expert, full-replay and medium. The difference between them is that the behavioral policies collecting these 3 types of trajectories show different performance in the simulated environment. And these behavioral policies are trained by the Soft Actor-Critic (SAC) algorithm online (Haarnoja et al., 2018).\n\nPolicy Set. To evaluate the abilities of all methods to correctly rank a set of policies in an offline way. We use the policy set released by Jin et al. (2022) as the candidate set of policies. For each trajectory set mentioned above, there are 2 types of policy sets ( referred as Set I and Set II) in which the expected return of policies are evenly spaced in the performance range of them. As mentioned in Jin et al. (2022), Set I and Set II aim to simulate two kinds of OPE cases. Set I consists of policies trained by offline RL algorithms (CQL (Kumar et al., 2020), BEAR (Kumar et al., 2019), CRR (Wang et al., 2020)). Since these algorithms have different network architectures and the generated policies are snapshots of models that stop training at different epochs, the policies contained in Set I show diverse behavioral performance. This is aligned with practical cases where the sources of policies are diverse and unknown. On the other hand, set II contains policies trained by SAC, which is the same as the algorithm of training the behavioral policies. Therefore, Set II corresponds to the practical OPE cases, such as production development and update. The updated policies share many common properties with the policies generating the trajectory data. In the experiments of ablations, we also compare some models that use extra training policy sets for providing supervised labels. These training sets are also released by (Jin et al., 2022), and policies in them are trained by SAC online.\n\nBaselines 1. We compare SOCCER with six state-of-the-art baselines. i) Fitted Q-Evaluation (FQE (Hoang et al., 2019)). It is a value-based OPE method, which learns a neural network to approximate the Q-function of the evaluated policy by temporal difference learning on the trajectory set. ii) Model-based estimation (MB (Paduraru, 2013)). It learns the dynamics model of environment, and estimates the expected return of evaluated policies by computing their average returns of MonteCarlo rollouts in the model environment. iii) Weighted importance sampling (IW (Mahmood et al., 2014)). It leverages weighted importance sampling to correct the weight of the reward, regarding the collected trajectory data distribution to the data distribution of the evaluated policy. iv) DualDICE (Nachum et al., 2019). It also aims to achieve distribution correction yet without directly using importance sampling. It learns an estimation of the state-action stationary distribution for achieving distribution correction. v) Doubly Robust (DR (Jiang & Li, 2016)). It utilizes an unbiased estimator that leverages an estimated environment model to decrease the variance of the unbiased estimates\n\n1We\n\nleverage\n\na\n\npopular\n\nimplementation\n\nof OPE algorithms:\n\ngoogle-research/google-research/tree/master/policy_eval. baselines used in our paper\n\nhttps://github.com/ It contains the first 5\n\n7\n\nRank CorrelationRegret@3HalfCheetah-v2Walker2d-v2expertfull-replaymediumexpertfull-replaymediumUnder review as a conference paper at ICLR 2023\n\nFigure 4: Comparing SOCCER with other ablations.\n\nproduced by importance sampling techniques. vi) SOPR-T 2 (Jin et al., 2022). This transformerbased model learns to achieve OPR tasks by learning a score function to score policies according to their generated state-action pairs. However, it relies on a strong assumption that there are extra deployed policies as well as these policies’ true performance. Therefore, we only compare it when there are extra supervised information.\n\nEvaluation Metrics. We evaluate all models according two widely-used metrics. i) Spearman’s rank correlation. It is the Pearson correlation between the ground truth rank sequence and the evaluated rank sequence of the evaluated policies. ii) Normalized Regret@k. It is the normalized difference between the actual value of the best policy in the policy set, and the actual value of the best policy in the estimated top-k set. Mathematically, it can be computed by\n\nregret@k =\n\nVmax − Vtopk Vmax − Vmin\n\n,\n\nwhere Vmax and Vmin is the expected return of the best and the worse policies, respectively, in the entire set, while Vtopk is the estimated top k policies.\n\n5.2 HYPERPARAMETERS.\n\nIn the implementation of our model, we control the scale of the learning objective function in our models by controlling the optimization procedure. It is conducted using Adam with a learning rate of 10−2, and with no momentum or weight decay. PCT is set as a 8-layer and 8-head transformer, and the dimensions of representations and all tokens are 128. The aggregation token e0 and the policy positional embeddings eα and eβ are initialized as random vectors. The token encoder f is a 3-layer MLP followed by a ReLU() activation function, and its feedforward dimension is 128. The linear projection φ is a single-layer MLP. During training, we set the number of training epochs as 100, and use the model snapshots at the final epoch to achieve OPR tasks in experiments. For each epoch, we sample 1280 pairwise policy decision sequences for training, and each sequence contains 256 sampled states. Besides, in each epoch, we sample 5 models from the baseline models as the OPE worker to generate noisy labels. In the inference stage, we sample 32 pairwise sequences for each policy pairs to compute the average score of each policy. All experiments in this paper are carried out with 3 different seeds, and they are 1, 2, 3, respectively.\n\n2We use the official implementation of SOPR-T: https://github.com/SOPR-T/SOPR-T\n\n8\n\nRank CorrelationRegret@3HalfCheetah-v2Walker2d-v2expertfull-replaymediumexpertfull-replaymediumUnder review as a conference paper at ICLR 2023\n\n5.3 EXPERIMENTAL RESULTS\n\nComparison with Other OPE Baselines. We first compare SOCCER against 5 baselines across 2 environments. For each environment, there are 3 different trajectory sets in which each set contains 2 distinct policy sets. So there are 12 policy sets required to be correctly ranked by all models. We show the rank correlation and regret@k values of the estimated rank sequences generated by each model in Figure 3.\n\nOverall, we can find SOCCER shows superior performance compared with other baselines. Furthermore, among all policy sets, SOCCER shows the highest rank correlations in 10 sets and the lowest regret@k in 8 sets, indicating that it can achieve robust and effective performance in diverse sets of policies. By contrast, the results of other baselines shows high variance across different policy sets. On the other hand, SOCCER shows great performance on learning noisy labels generated by other OPE workers. Note that in these experiments, all OPE workers which generate noisy labels for SOCCER are directly sampled from the trained models of baselines. We can find that SOCCER could still perform well despite of the low quality of OPE workers. For example, looking at the results in Set I of the expert trajectory set in HalfCheetah-v2, all OPE baselines show poor performance (the average rank correlation of them is about −0.27), indicating that labels generated by them are very noisy. However, SOCCER shows a high performance (about 0.62) while its supervised information is from such low-quality labels. Intuitively, we believe that there are two main reasons causing this phenomenon. First, the effective representation capability of our proposed policy comparison transformer could help to reduce the biases of noisy annotators. Second, the crowd layer could automatically distinguishes the good from the unreliable OPE annotators and captures their individual biases. In conclusion, SOCCER shows highly effective and robust OPR results across diverse kinds of policies. Besides, it can largely reduce the biases induced by OPE annotators and thus gets better performance than these OPE workers.\n\nAblations. To figure out the importance of each component in our framework. We perform several ablations on the same policy sets as the experiments mentioned above. All results are illustrated in Figure 4. Specifically, we compare SOCCER with 3 different models. The first one referred to SOCCER without CL is the model using our PCT architecture but discarding the crowd layer. Since it has no module to learn from crowds, so we make train it by providing extra sets of deployed policies released by (Jin et al., 2022). The second one is SOPR-T (Jin et al., 2022). It is also a transformer-based model to learn policy representations. The difference between it and SOCCER is that it learns an individual policy representation, while SOCCER learns pairwise policy representations which aim to capture subtle differences between any two policies. Furthermore, SOPR-T also cannot learn from crowds, so we train it in the same supervised way. As shown in Figure 4, SOCCER without CL shows better results than SOPR-T on 10 policy sets, and SOCCER performs better than SOPR-T on 8 policy sets. This indicates that in the OPR tasks, our proposed pairwise policy representation which aims to capture relations between intra-policy decisions has stronger representation ability than the pointwise policy representation, which regards each policy as an individual point, without considering the subtle decision differences between policies. On the other hand, SOCCER and SOPR-T with CL both show comparable performance with SOCCER without CL and SOPR-T, respectively, while SOCCER and SOPR-T with CL cannot get any supervised information from extra deployed policies. This indicates that regarding results estimated by other OPE methods as noisy labels and using a crowd layer to learn from such labels is an effective way when there is no sufficient extra policies providing supervised labels.\n\n6 CONCLUSIONS\n\nThis paper proposes SOCCER, a novel self-supervised learning framework for addressing OPR problems. SOCCER is the first framework that combines crowdsourcing with OPR problems. In order to capture the relative discrepancies between policies, SOCCER employs a policy comparison transformer with a novel positional encoding to learn effective pairwise policy representations. Experimental results show that SOCCER not only outperforms baselines on various tasks but also generalizes well in diverse policy sets.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n\nRodrigues Filipe and Francisco Pereira. Deep learning from crowds. In Proceedings of the AAAI\n\nConference on Artificial Intelligence, pp. 1611–1618, 2018.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\n\ndata-driven reinforcement learning. arXiv, 2020.\n\nBellemare Marc G., Will Dabney, and R ́emi Munos. A distributional perspective on reinforcement\n\nlearning. In International Conference on Machine Learning, pp. 449–458, 2017.\n\nMelody Guan, Varun Gulshan, Andrew Dai, and Geoffrey Hinton. Who said what: Modeling inIn Proceedings of the AAAI Conference on Artificial\n\ndividual labelers improves classification. Intelligence, pp. 3109–3118, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pp. 1861–1870, 2018.\n\nJosiah Hanna, Niekum Scott, and Peter Stone.\n\nImportance sampling policy evaluation with an estimated behavior policy. In International Conference on Machine Learning, pp. 2605–2613, 2019.\n\nJean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks. arXiv,\n\n2020.\n\nLe Hoang, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Inter-\n\nnational Conference on Machine Learning, pp. 3703–3712, 2019.\n\nArnekvist Isac, Danica Kragic, and Johannes A. Stork. VPE: Variational policy embedding for\n\ntransfer reinforcement learning. In ICRA, pp. 36–42, 2019.\n\nNan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In\n\nInternational Conference on Machine Learning, pp. 652–661, 2016.\n\nYue Jin, Yue Zhang, Tao Qin, Xudong Zhang, Jian Yuan, Houqiang Li, and Tie-Yan Liu. Supervised off-policy ranking. In International Conference on Machine Learning, pp. 10323–10339, 2022.\n\nZhang Jing, Xindong Wu, and Victor S. Sheng. Imbalanced multiple noisy labeling. Transactions\n\non Knowledge and Data Engineering, 27(2):489–503, 2014.\n\nZhang Jing, Victor S. Sheng, Jian Wu, and Xindong Wu. Multi-class ground truth inference in crowdsourcing with clustering. Transactions on Knowledge and Data Engineering, 28(4):1080– 1085, 2015.\n\nAviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy Q-Learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, pp. 11761–11771, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-Learning for offline reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1179–1191, 2020.\n\nJunbing Li, Changqing Zhang, Joey Tianyi Zhou, Huazhu Fu, Shuyin Xia, and Qinghua Hu. Deeplift: Deep label-specific feature learning for image annotation. IEEE Transactions on Cybernetics, 52(8):7732–7741, 2022.\n\nLihong Li, R ́emi Munos, and Csaba Szepesv ́ari. Toward minimax off-policy value estimation. In\n\nArtificial Intelligence and Statistics, pp. 608–616, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nChen Lili, Lu Kevin, Rajeswaran Aravind, Lee Kimin, Grover Aditya, Laskin Michael, Abbeel Pieter, Srinivas Aravind, and Mordatch Igor. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, pp. 15084–15097, 2021.\n\nQiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: InfiniteIn Advances in Neural Information Processing Systems, pp.\n\nhorizon off-policy estimation. 5361–5371, 2018.\n\nA. Rupam Mahmood, Hado P van Hasselt, and Richard S Sutton. Weighted importance sampling for off-policy learning with linear function approximation. In Advances in Neural Information Processing Systems, 2014.\n\nJanner Michael, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. In Advances in Neural Information Processing Systems, pp. 1273–1286, 2021.\n\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections. In Advances in Neural Information Processing Systems, 2019.\n\nXinkun Nie, Emma Brunskill, and Stefan Wager. Learning when-to-treat policies. Journal of the\n\nAmerican Statistical Association, 116(533):392–409, 2021.\n\nMichael Oberst and David Sontag. Counterfactual off-policy evaluation with gumbel-max structural\n\ncausal models. In International Conference on Machine Learning, pp. 4881–4890, 2019.\n\nCosmin Paduraru. Off-policy evaluation in Markov decision processes. PhD thesis, McGill Univer-\n\nsity Libraries, 2013.\n\nNogueira Rodrigo, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with\n\nbert. arXiv preprint arXiv:1910.14424, 2019.\n\nFilipe Rodrigues, Francisco Pereira, and Bernardete Ribeiro. Learning from multiple annotators: Distinguishing good from random labelers. Pattern Recognition Letters, 34(12):1428–1436, 2013. ISSN 0167-8655.\n\nSheng Victor S. and Jing Zhang. Machine learning with crowdsourcing: A brief summary of the past research and future directions. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 9837–9843, 2019.\n\nHongyao Tang, Zhaopeng Meng, Jianye Hao, Chen Chen, Daniel Graves, Dong Li, Changmin Yu, Hangyu Mao, Wulong Liu, Yaodong Yang, Wenyuan Tao, and Li Wang. What about inputting policy in value function: Policy representation and policy-extended value function approximator. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8441–8449, 2022.\n\nTian Tian and Jun Zhu. Max-margin majority voting for learning from crowds.\n\nIn Advances in\n\nNeural Information Processing Systems, pp. 1621–1629, 2015.\n\nZiyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression. In Advances in Neural Information Processing Systems, pp. 7768– 7778, 2020.\n\nZhang Yuchen, Xi Chen, Dengyong Zhou, and Michael I. Jordan. Spectral methods meet em: A\n\nprovably optimal algorithm for crowdsourcing. JMLR, 17(1):3537–3580, 2016.\n\nSiyuan Zhang and Nan Jiang. Towards hyperparameter-free policy selection for offline reinforce-\n\nment learning. In Advances in Neural Information Processing Systems, 2021.\n\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. In Advances in Neural Information Processing Systems, pp. 17723–17736, 2021.\n\n11",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new method for ranking of offline RL policies with off-policy evaluation (OPE). The ranking is produced with a model that 1) learns a pairwise policy representation with a transformer architecture, 2) uses a crowd layer to aggregate OPE scores of other methods. In the experimental results the authors show that their method is able to outperform the other baselines. The ablation studies show the importance of various components of the proposed method.\n\n# Strength And Weaknesses\n\nStrengths:\n\n- The paper is well written and easy to follow. The details of the method and the experiments are clearly explained. The figures are informative and well explained.\n- The idea of using pairwise policy representation sounds interesting and novel.\n- The experimental results where the proposed method outperformed the other baseline is very encouraging.\n- The experimental results studies the problem with different settings.\n\nWeaknesses:\n\n- To my mind, the experimental results are lacking an adequate baseline that is comparable with the proposed method. A baseline would be comparable if it also uses other existing OPE methods to aggregate the results. For example, I can imagine several easy baselines in this case: 1) take the average OPE scores of all methods and produce a ranking out of them, 2) use a majority voting scheme to aggregate the rankings, 3) there are many rank aggregation methods that could be considered, for example, [1]. \n- As the proposed method aggregates the ranks from the existing policies, the computational cost for it is much higher than for any other method and it includes the costs of all other methods. This should also be discussed. \n- Another limitation in the current experiments is that as far as I understand the method needs to be trained for every new set of policies and the environment from scratch. Then, it is tested on its own training set (no validation or test set). Do I understand the setting correctly? Would the policy representations generalize across different sets of policies? Suppose a new policy is added to the set of policies, can the previous results be re-used? To me, the method would be useful in practice if it can show signs of such generalization.\n- I do not understand why transformers are the best architecture in the given policy representation design. As the states (equation 1) are chosen as just a set (not ordered), what is the advantage of using a transformer which is known to be the best suited for sequential data? Did the authors consider other architectures (possibly simpler, e.g., MLP) here?\n\nOther comments:\n- Several times the authors mention that in practice finding the best policy is the main objective. In that case, it would be more logical to consider off-policy policy selection (OPS) problem formulation and as the quality metric measure the regret @1. How would the method perform in that case? \n- I still do not understand the role and training of the \"aggregation token\" very well, maybe this could be explained further.\n- In the first part of section 4.2 the authors say that they \"show how the policy ranking can be reduced to binary classification\". I think this is a common way to approach the ranking problem (but the text sounds now like this is one of the contributions). Some work could be references here, for example, [2].\n\n[1] Fast and Accurate Inference of Plackett–Luce Models. Lucas Maystre, Matthias Grossglauser. NIPS 2015.\n\n[2] Preference Learning with Gaussian Processes. Wei Chu, Zoubin Ghahramani. ICML 2005.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: good.\n\nQuality: good.\n\nNovelty: the paper combines several existing components and aggregates the results of the existing methods, but the idea of the aggregation is reasonably novel. Also, using pairwise instead of direct policy representation sounds novel to me.\n\nReproducibility: the paper provides sufficient details on the methodology as the space permits. Are the authors planning to open source the code?\n\n# Summary Of The Review\n\nI am leaning toward rejecting this paper mainly because I find the experiments lacking comparable baselines that would benefit from aggregating the results of the existing methods in the same way as the proposed method. Also, I would also like to see some generalization of the method or policy representation to the unseen policies that would make the method scalable to real world problems.\n\n---\nUpdated my score after rebuttal in the light of new empirical results.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBRAIN SIGNAL GENERATION AND DATA AUGMENTATION WITH A SINGLE-STEP DIFFUSION PROBABILISTIC MODEL\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nBrain-computer interfaces based on deep learning rely on large amounts of highquality data. Finding publicly available brain signal datasets that meet all requirements is a challenge. However, brain signals synthesized with generative models may provide a solution to this problem. Our work builds on diffusion probabilistic models (DPMs) and aims to generate brain signals that have the properties needed to develop further classification models based on deep learning. We show that our DPM can generate high-quality event-related potentials (ERPs) and motor imagery (MI) signals. Furthermore, with the progressive distillation of the model, subject-specific data can be produced in a one-step reverse process. We augment publicly available datasets and demonstrate the impact of the generated signals on a deep learning classification model. DPMs are versatile models, and this work shows that brain signal processing is one of many other tasks in which these models can be useful.\n\n1\n\nINTRODUCTION\n\nElectroencephalography (EEG) is undoubtedly one of the most popular brain mapping technologies, which is widely used in research and clinical diagnosis (de Aguiar Neto & Rosa (2019), van Mierlo et al. (2020), Wang et al. (2020)). EEG records the neural activity of the brain in a non-invasive manner. (Biasiucci et al. (2019)) EEG is less complex and cheaper than other brain imaging technologies. EEG has one of the best temporal resolutions. However, the spatial resolution of the technology is quite poor due to its heavy dependence on the number of electrodes used for signal recording and non-invasiveness. (Craik et al. (2019b))\n\nBrain-computer interfaces (BCIs) connect the brain and external processing devices, making it possible to perform tasks using only brain signals. BCIs can help in everyday life for people with limited movement and communication abilities (Pandarinath et al. (2017)). BCIs are also applied in many other fields from healthcare (Gal ́an et al. (2008), Vilela & Hochberg (2020)) to entertainment (Finke et al. (2009)). BCIs are often based on EEG due to the ability of the technology to measure signals with only a couple of milliseconds difference and its relatively low cost and more comfort. The measurements are then processed by a decoder unit in the BCI that turns the recorded temporal and frequency patterns into actions. (Lotte et al. (2018))\n\nIn recent years, deep learning (DL) algorithms have become more and more commonly used in EEG signal processing (Roy et al. (2019), Craik et al. (2019a), Kotowski et al. (2020)). DL models can decode brain signals with high accuracy. However, developing DL models requires a large amount of high-quality data. The size and quality of publicly available data sets are limited, also often insufficient and imbalanced. Recording a new data set can be highly resource-consuming and requires professionals to check the measurements. Another option to augment data sets is data synthesis. (Lashgari et al. (2020))\n\nScore-based models (Tashiro et al. (2021), Song et al. (2021)), diffusion probabilistic models (DPMs) (Ho et al. (2020), Luo & Hu (2021)) and generative adversarial networks (GANs) (Liu et al. (2021), Chan et al. (2021)) hold the state-of-the-art in deep-learning-based generative modelling. The recent advances show the performance and effectiveness DPMs over GANs in both image (Dhariwal & Nichol (2021)) and audio generation (Kong et al. (2021)). There are a handful\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nof works for brain signal generation with GANs (Xu et al. (2022), Hartmann et al. (2018), Fahimi et al. (2019), Panwar et al. (2020)). To the best of our knowledge, there are no works examining the capabilities of DPMs or score-based models in multi-channel EEG signal generation tasks.\n\nin Section 2 we present the background on the DPM The structure of this work is as follows: framework that we used in this paper, followed by a brief description of the progressive distillation process in Section 3. Our EEGWave architecture is presented in Section 4. The description of the experiments with the used datasets and procedures are given in Section 5. Finally, we conclude our work and thoughts in Section 6.\n\n2 CONTINUOUS-TIME DIFFUSION MODELS\n\nThe distribution of the training data set is given as p(x). Let x ∈ RE×L, where E is the number of electrodes (or EEG channels) and L is the length of the recorded sequence. In a continuous-time diffusion framework (Kingma et al. (2021)), in the forward and reverse diffusion processes, there are latent variables that are denoted by zt. For every time step, where t ∈ [0, 1], the latent variables have the same shape as the training data samples (zt ∈ RE×L).\n\nThe forward diffusion process, which is a Gaussian process in continuous time can be given as:\n\nq(zt|x) = N (zt; αtx, σ2\n\nt I)\n\n(1)\n\n,where αt and σ2 log signal-to-noise ratio is given as: λt = log (α2 t → 1. For any 0 ≤ s ≤ t ≤ 1, the following Gaussian conditional distribution can be given:\n\nt are smooth, differentiable, positive scalar-valued functions. With αt and σ2\n\nt the t ), which decreases strictly monotonically as\n\nt /σ2\n\nq(zt|zs) = N (zt;\n\nαt αs\n\nzs, ̃σ2\n\nt|sI),\n\n ̃σ2\n\nt|s = (1 − eλt−λs )σ2\n\nt\n\nThe reverse process is based on the a posteriori distribution:\n\nq(zs|zt, x) = N (zs; ̃μs|t(zt, x), ̃σ2\n\ns|tI)\n\n, where s ≤ t and\n\n ̃μs|t(zt, x) = eλt−λs\n\nαs αt\n\nzt + (1 − eλt−λs )αsx,\n\n ̃σ2\n\ns|t = (1 − eλt−λs)σ2\n\ns\n\n(2)\n\n(3)\n\n(4)\n\nIn this framework, x in the reverse process is predicted by the neural network ̃xθ(zt, λt) with the parameter set θ.\n\nData inference is done by sampling a latent white noise variable z1 at t = 1, setting a noise controlling γ factor and iteratively applying the following, until t = 0 (Salimans & Ho (2022)):\n\nzs = ̃μs|t(zt, ̃xθ(zt, λt)) +\n\n(cid:113)\n\n( ̃σ2\n\ns|t)1−γ( ̃σ2\n\nt|s)γε,\n\nε ∼ N (0, I)\n\n(5)\n\nDuring training, the model is aimed to maximize the variational lower bound (ELBO) on the loglikelihood of the data. However, with a re-parameterization, the weighted ELBO can be given as the weighted mean squared error objective:\n\nL(θ) = Eε,t\n\n(cid:104) ω(λt)∥x − ̃xθ(zt, λt)∥2\n\n2\n\n(cid:105)\n\nmin θ\n\n(6)\n\n, where ω(λt) weighting is choosable, however it is ω(λt) = max( α2 & Ho (2022)\n\nt σ2 t\n\n, 1) in our approach. Salimans\n\n3 PROGRESSIVE DISTILLATION\n\nDiffusion models need many iterations during sampling to synthesize data, making them significantly slower than GANs. Recent studies (Luhman & Luhman (2021), Kong & Ping (2021)) presented multiple ways to fasten the inference from which we applied progressive distillation (Salimans & Ho (2022)) as we found this approach the most efficient one.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nProgressive distillation is based on a teacher-student setup, where the student model approximates the teacher with halved sampling steps. At first, a teacher model is trained as a continuous-time diffusion model. Then a number of finite discrete sampling steps is given for the teacher and the student, denote it as Tt and Ts = Tt/2. The distillation process is iterative, where each iteration starts with weight initialization of the student model with the teacher’s parameters. The target ̃x is then calculated from the latent variable zt by sampling in 2 DDIM steps using the teacher model to get the predictions at time step (t − 0.5)/Ts and the (t − 1.)/Ts. The student has to denoise the same zt in 1 reverse DDIM step to get the approximate result for ̃x. When the student converges, Ts is halved, the student becomes the teacher, and the process is repeated.\n\nFigure 1: EEGWave architecture.\n\n4 ARCHITECTURE\n\nOur architecture aims to synthesize multi-channel EEG signals as many works on brain signal processing show the benefits of processing signals recorded on multiple electrodes. Therefore, our model is ̃xθ : RE×L × R → RE×L that builds on non-causal bi-directional dilated convolutions.\n\nMany works on multi-channel brain signal processing handle the measurements as 2-dimensional samples, similarly to images, to make use of spatio-temporal features. In GANs and VAEs, this approach means that EEG signals are synthesized through up-sampling interpolations or transposed convolutions. Data synthesis in this approach with time-series signals can be ill-posed due to the heavy influence of these operations’ temporal and spectral artifacts that occur in the generated data.\n\nOur approach builds on the work of Kong et al. (2021). We omit up- and down-sampling layers to avoid temporal and spectral artifacts in the synthesized data. We build on the residual layers of DiffWave. Although EEG epochs are not as long as audio samples, we keep bi-directional dilated convolutions with smaller dilations as they can help maintain global context and consistency through the epochs. We realize the architecture with 2-dimensional convolutions. Our input convolution layer maps the multi-channel data into a single-channel by applying (E, 1) sized filters, where E is the number of EEG channels. We use kernels with size (1, K) in the dilated convolution layers as we process 1-dimensional data in these blocks. The output point-wise convolution layer produces the 1-dimensional signals with the number of EEG channels (N × E × 1 × L). Then this data is reshaped to match the dimensions of the input (N × 1 × E × L). (We implemented the architecture in PyTorch.)\n\nλt is embedded through two global and one residual-local linear layers and added to the residual features. The class and subject conditions are handled the same way, the only difference being that they are not encoded in any way. The architecture is shown in Figure 1.\n\nIn the experiments given in this work, we use a rather deep than wide neural network with 32 residual layers and 64 channels in each convolution layer. λt, one-hot encoded class, and subject conditions are embedded into 512 dimensions in the global linear layers. We use a kernel size of (1, 3) in the dilated convolutions and a dilation cycle of [1, 2, ..., 64] because empirically, we found it to be\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nbeneficial to use a dilation cycle that with there are kernels that cover the whole data sample. λt is embedded through two global and one residual-local linear layers and added to the residual features. The class and subject conditions are handled the same way, the only difference being that they are not encoded in any way.\n\n5 EXPERIMENTS\n\n5.1 DATASETS\n\nVEPESS dataset: the set contains visually evoked potentials from 18 subjects recorded by the authors of Robbins et al. (2018). The measurements were done by following the oddball paradigm. Subjects were presented with sequences of images that consisted of target and non-target types. Based on the type of the shown image, the subjects had to push the corresponding buttons. Data were recorded with an EEG device with 64 + 6 electrodes in the 10-20 standard configuration and sampled at 512 Hz. In this work, we use raw measurements. Signals were band-pass filtered from 1 - 40 Hz with a zero-phase filter, and epochs were extracted between [0, 1] seconds from the onset of the target/non-target image. The epochs were normalized by channel-wise mean subtraction and deviation division into the range [−1., 1.].\n\nBCI Competition IV Dataset IIa (BCIC4D2a): the set contains motor imagery signals from 9 participants. The subjects were asked to imagine the movement of their left and right hand, also their feet and tongue, for a couple of seconds after the instruction cue were presented on their screen. Data was recorded from 22 EEG and 3 EOG channels following the 10-20 standard system. The measurements were sampled at 250 Hz and band-pass filtered from 0.5 - 100 Hz. Furthermore, a notch filter at 50 Hz was applied to eliminate the line noise. We further band-pass filtered the signals between 4 - 38 Hz with a zero-phase filter and down-sampled them to 128 Hz. Following the work of , we extracted epochs from the recordings between [0.5, 4] seconds from the onset of the cue and normalized them by channel-wise mean subtraction and deviation division. We excluded samples marked as rejected due to artifacts by the publishers of the set.\n\n5.2 CLASS-CONDITIONAL SIGNAL GENERATION\n\nThe current experiment aims to generate EEG data of good quality from samples from a simple Gaussian noise distribution. We condition the generation process based on classes in the data sets. We use the VEPESS and the BCIC4D2a data sets to examine whether the model can capture the features of the sets that contain different types of signals. For both data sets, EEGWave has to model characteristics in both the time and frequency domain, but in a slightly different way. The signals from the VEPESS set are characterized mainly by their amplitude deviation and the corresponding latency in the time domain. On the other hand, the essential features of the samples from the BCIC4D2a set are in the frequency domain, specifically in the theta, alpha, and beta bands. Although the effect of progressive distillation on the quality of the generated data is examined in the following subsection, we also present the results from the distilled models, which generated the signals in a single step.\n\nWe chose only the RWGAN (Panwar et al. (2020)) as a baseline model because other deep learning models in the literature were either incompletely documented, designed only for single-channel EEG generation, or did not work based on the given information in the published work. We trained RWGAN exactly as the publishers did in their work.\n\nWe give both qualitative and quantitative results. The Inception Score (IS) (Salimans et al. (2016)), Frechet Inception Distance (FID) (Heusel et al. (2017)), spatial FID (Nash et al. (2021)), Precision, Recall (Kynk ̈a ̈anniemi et al. (2019)) scores are computed based on EEGNet (Lawhern et al. (2018)), which was trained on the data sets separately the same way as the authors of EEGNet did. We also use Sliced Wasserstein Distance (SWD) (Wu et al. (2019)) and Gaussian Mixture Model (GMM) differences (Panwar et al. (2020)) to measure the differences between real and generated distributions directly. We also found that qualitative results are a good way to have a greater understanding and a deeper interpretation of results.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: The quality of the generated EEG signals on the VEPESS set was measured indirectly (with the feature maps of EEGNet) and directly (through distribution measures).\n\nOrigin\n\nIS ↑\n\nFID ↓\n\nsFID ↓\n\nPrec ↑ Rec ↑\n\nSWD ↓\n\ndGMM ↓\n\nTrain set Test set RWGAN EEGWave EEGWave 1x\n\n1.2522 1.2360 1.1761 1.2284 1.2146\n\n0 0.0315 2.2708 0.1479 1.4212\n\n0 1.3264 3.0530 1.5266 5.6885\n\n1.0 0.9199 0.8865 0.9238 0.8063\n\n1.0 0.9369 0.9087 0.8740 0.8284\n\n0 0.9851 1.1769 1.0915 1.7785\n\n0 49.8316 119.0032 81.1503 141.4728\n\nFigure 2: Comparison of averaged real and generated samples by different models on the VEPESS set. The averaging was done across the epochs, and the results are plotted on all 64 channels.\n\nVEPESS results: first, we present the results on the VEPESS set. The quantitative metrics are given in Table 1. We also give scores measured on the test set to understand the generated signals’ results better. Figure 2 presents the averaged target class samples from the original and the generated sets. Based on the given results, it can be said that EEGWave captured the ERPs’ main features in the VEPESS set. RWGAN also generated good signals, although these samples are heavily contaminated by frequency artifacts, which, in our hypothesis, are mostly the results of the up-sampling layers (also mentioned frequently in the literature). The singlestep EEGWave has slightly poorer scores in the table, but the generated signals seem to have more fidelity than the ones from the RWGAN.\n\nBCIC4D2a results: for the BCIC4D2a set, the quantitative results are given in Table 2. Figure 3 presents the power spectral densities distributed over the scalp in the theta, alpha, and beta frequency bands for the left-hand class. Generated signals from the EEGWave 1024 step, EEGWave 1 step, and RW-\n\n5\n\nFigure 3: Real and generated signal PSD top plots from the BCIC4D2a set.\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: The quality of the generated EEG signals on the BCIC4D2a set was measured indirectly (with the feature maps of EEGNet) and directly (through distribution measures).\n\nOrigin\n\nIS ↑\n\nFID ↓\n\nsFID ↓\n\nPrec ↑ Rec ↑\n\nSWD ↓\n\ndGMM ↓\n\nTrain set Test set RWGAN EEGWave EEGWave 1x\n\n1.3998 1.3594 1.2949 1.1881 1.2136\n\n0 0.1798 7.2315 3.1708 3.3100\n\n0 80.4392 142.8659 51.9159 42.8244\n\n1.0 0.9040 0.4740 0.9668 0.9282\n\n1.0 0.9172 0.4123 0.5724 0.5245\n\n0 0.4781 1.6983 1.3566 1.0874\n\n0 2.4963 751.2371 597.6406 430.7368\n\nGAN models are compared to the real signals from the dataset. The results show that the single-step EEGWave model could perform slightly better than RWGAN in this experiment. The top plots show that all models were able to learn the main frequency features. The two EEGWave models outperformed the GAN. The figures corresponding to the rest of the classes are given in Appendix A.1.\n\n5.3 SUBJECT SPECIFICITY\n\nBrain signals vary not just between classes but subjects. In many cases, it could be beneficial to generate data for only specific subjects, e.g., imbalanced sets, fine-tuning. We use the VEPESS set to show that subject-specific features can be learned and reproduced by our model. Subject information is one-hot encoded and injected into the network similarly to the signal class labels. The model is trained the same way as in the class-conditional case. During inference, the signal generation process is conditioned on the class labels and the subjects. We then compare the generated signals from each subject to the real signals from all subjects to examine whether the model could learn each subject’s features. The generated signals are visualized in top plots. Furthermore, we measure the SWD and sFID metrics between the subject-specific distributions. For the sFID calculation, the same EEGNet model is used as in the previous experiment.\n\nResults: the metrics in Figure 4 imply that the distributions of the real and generated signal corresponding to the same subjects are closer to each other than to other subject data. Figure 5 visually supports this implication. The averaged ERP epochs of each subject are visually easily distinguishable from each other. Although we only present a few examples here, we include the rest of the top plots in Appendix A.2.\n\nFigure 4: Two types of distance metrics were measured between the distributions of the real and generated subjects from the VEPESS set to examine the ability of the model to learn subject-specific features. (The lower, the better.)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Comparison of the averaged real and generated ERP epochs from the VEPESS set for Subjects 1 and 15.\n\n5.4 AUGMENTATION\n\nIn the augmentation task, we try to improve the performance of EEGNet on the BCI4D2a dataset. A generated dataset is created with the single-step EEGWave model with the same amount of signals as the original set. The original dataset is split into train, validation, and test subsets with 0.7%, 0.15%, 0.15% ratios. The subsets are created to contain the same number of signals from each subject. We try to improve the accuracy of the EEGNet model with two approaches:\n\n1. we double the size of the training subset by mixing the same amount of generated signals\n\nas the number in the original subset (6)\n\n2. pre-train EEGNet on the generated data and then re-train this initialized model with the real\n\nsignals (7)\n\nIn both cases, we only stop the training if over-fitting is detected via monitoring the validation loss. We use Adam optimizer with a learning rate of 1e − 3.\n\nFigure 6: The effect of training EEGNet with the real-generated mixed training subset. The left confusion matrix shows the test results without augmentation, while the right one shows the training results with the mixed-set augmentation.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: The effect of pre-training EEGNet with the generated signals. The left confusion matrix shows the test results without pre-training, while the right one shows the results with pre-training.\n\nResults: although the mixed training gave slightly better accuracy on the validation subset during training, it did not improve the accuracy of the test set. On the contrary, with the pre-training approach, the model converged faster and achieved better accuracy on the test. These observations imply that although EEGWave was able to learn the main characteristics of the real signals, it could not produce signals that are diverse enough to regularize EEGNet.\n\n5.5 DISTILLATION\n\nBy progressive distillation, we aim to attain a DPM model that can synthesize EEG signals in a single step. We distill EEGWave on the VEPESS and BCIC4D2a data sets to examine the effect of the process. The distillation is started at 1024 steps, and we continue it until the single-step generation is reached. The number of training iterations in steps 2 and 1 is doubled, following the work of . The distilled models with different inference steps are saved and compared to the initial model generating signals through the same number of steps as the distilled ones. We evaluate the generated signals at these steps and present results in Figures 8 and 9.\n\nFigure 8: Effect of the distillation of EEGWave trained on the VEPESS data set. F-Score is calculated from the Precision and Recall scores with β = 2.\n\nResults: while at a higher number of steps (> 8), distillation did not result in a better-performing model. At steps ≤ 8, the metrics show much better models than the ones without distillation. In the case of the VEPESS set, the scores achieved with the single-step distilled model are much closer to the scores of the initial model than without distillation. This shows that distillation can be a good option for achieving a DPM that can effectively balance the trade-off between fast sampling and high-quality samples. Interestingly, in the case of the BCIC4D2a set, the scores of the distilled mod-\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Effect of the distillation of EEGWave trained on the BCIC4D2a data set. F-Score is calculated from the Precision and Recall scores with β = 2.\n\nels show better performance than those of the initial model. This implies that the distilled models are more likely to generate EEG signals that are copies of the ones from the train set, resulting in better metrics due to higher fidelity but a narrower learned distribution.\n\n6 CONCLUSION\n\nOur work shows a novel way of generating brain signals that can be useful in augmentation tasks. Although this work aimed to examine as many aspects of brain signal generation as possible, there is much yet to explore. We believe the current work shows that DPM-based brain signal generation is a very feasible task and can be used to create datasets that help improve deep learning models in classification tasks.\n\nThis work is mainly limited in that the quality and diversity of EEG signals can not be measured the same way as in the case of images. The metrics commonly used in image synthesis tasks often give contrary results in brain signal generation.\n\nERP and MI signals were generated conditioned on class labels. The performance of our single-step DPM was close to 1024-step DPM and the RWGAN. We also showed that EEGWave could learn subject-specific features. In the augmentation task, the generated signals were the most useful for the pre-training of EEGNet, before re-training on the original set. The distillation results show that progressive distillation is an excellent approach to obtaining a DPM with a low number of inference steps that can generate signals of good quality. The diversity of the generated signals is still an open question, as well as the metrics that can measure the realness of the generated data. We hope that DPM-based signal generation will be much more explored in the future, and we are eager to see the development of this field.\n\nREFERENCES\n\nAndrea Biasiucci, Benedetta Franceschiello, and Micah M. Murray. Electroencephalography. Current Biology, 29(3):R80–R85, 2019. ISSN 0960-9822. doi: https://doi.org/10.1016/j.cub. 2018.11.052. URL https://www.sciencedirect.com/science/article/pii/ S0960982218315513.\n\nEric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. Pi-gan: Periodic In Proceedings of the implicit generative adversarial networks for 3d-aware image synthesis. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5799–5809, June 2021.\n\nAlexander Craik, Yongtian He, and Jose L Contreras-Vidal. Deep learning for electroenJournal of Neural Engineering, 16(3):\n\ncephalogram (EEG) classification tasks: a review.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n031001, apr 2019a. doi: 10.1088/1741-2552/ab0ab5. URL https://doi.org/10.1088/ 1741-2552/ab0ab5.\n\nAlexander Craik, Yongtian He, and Jose L Contreras-Vidal. Deep learning for electroenJournal of Neural Engineering, 16(3): cephalogram (EEG) classification tasks: a review. 031001, apr 2019b. doi: 10.1088/1741-2552/ab0ab5. URL https://doi.org/10.1088/ 1741-2552/ab0ab5.\n\nFernando Soares de Aguiar Neto and Jo ̃ao Lu ́ıs Garcia Rosa. Depression biomarkers using ISSN doi: https://doi.org/10.1016/j.neubiorev.2019.07.021. URL https://www.\n\nnon-invasive eeg: A review. Neuroscience Biobehavioral Reviews, 105:83–93, 2019. 0149-7634. sciencedirect.com/science/article/pii/S0149763419303823.\n\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\n\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 8780–8794. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf.\n\nFatemeh Fahimi, Zhuo Zhang, Wooi Boon Goh, Kai Keng Ang, and Cuntai Guan. Towards eeg In 2019 IEEE EMBS International Conference on\n\ngeneration using gans for bci applications. Biomedical Health Informatics (BHI), pp. 1–4, May 2019. doi: 10.1109/BHI.2019.8834503.\n\nAndrea Finke, Alexander Lenhardt, and Helge Ritter. The mindgame: A p300-based brain–computer interface game. Neural Networks, 22(9):1329–1333, 2009. ISSN 0893-6080. doi: https://doi.org/ 10.1016/j.neunet.2009.07.003. URL https://www.sciencedirect.com/science/ article/pii/S0893608009001579. Brain-Machine Interface.\n\nF. Gal ́an, M. Nuttin, E. Lew, P.W. Ferrez, G. Vanacker, J. Philips, and J. del R. Mill ́an. A brainactuated wheelchair: Asynchronous and non-invasive brain–computer interfaces for continuous control of robots. Clinical Neurophysiology, 119(9):2159–2169, 2008. ISSN 1388-2457. doi: https://doi.org/10.1016/j.clinph.2008.06.001. URL https://www.sciencedirect.com/ science/article/pii/S1388245708005750.\n\nKay Gregor Hartmann, Robin Tibor Schirrmeister, and Tonio Ball. Eeg-gan: Generative adversarial networks for electroencephalograhic (eeg) brain signals, 2018. URL https://arxiv.org/ abs/1806.01875.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 8a1d694707eb0fefe65871369074926d-Paper.pdf.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\n\nDenoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6840–6851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.\n\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=2LdBqxc1Yv.\n\nZhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021. URL https://openreview.net/forum?id=agj4cdOfrAP.\n\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=a-xFK8Ymz5J.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKrzysztof Kotowski, Katarzyna Stapor, and Jeremi Ochab. Deep Learning Methods in Electroencephalography, pp. 191–212. Springer International Publishing, Cham, 2020. ISBN 978-3030-49724-8. doi: 10.1007/978-3-030-49724-8 8. URL https://doi.org/10.1007/ 978-3-030-49724-8_8.\n\nTuomas Kynk ̈a ̈anniemi, Tero Karras, Samuli Laine,\n\nImproved precision and recall metric for assessing generative models. lach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett Advances Inc., 0234c510bc6d908b28c70ff313743079-Paper.pdf.\n\nand Timo Aila. In H. Wal- (eds.), Information Processing Systems, volume 32. Curran Associates, URL https://proceedings.neurips.cc/paper/2019/file/\n\nJaakko Lehtinen,\n\nin Neural\n\n2019.\n\nElnaz Lashgari, Dehua Liang, and Uri Maoz. Data augmentation for deep-learning-based electroencephalography. Journal of Neuroscience Methods, 346:108885, 2020. ISSN 0165-0270. doi: https://doi.org/10.1016/j.jneumeth.2020.108885. URL https://www.sciencedirect. com/science/article/pii/S0165027020303083.\n\nVernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P Hung, and Brent J Lance. EEGNet: a compact convolutional neural network for EEG-based brain–computer interfaces. Journal of Neural Engineering, 15(5):056013, jul 2018. doi: 10.1088/1741-2552/ aace8c. URL https://doi.org/10.1088/1741-2552/aace8c.\n\nRui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, and Hongsheng Li. Divco: Diverse conIn Proceedings of the ditional image synthesis via contrastive generative adversarial network. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16377–16386, June 2021.\n\nF Lotte, L Bougrain, A Cichocki, M Clerc, M Congedo, A Rakotomamonjy, and F Yger. A review of classification algorithms for EEG-based brain–computer interfaces: a 10 year update. Journal of Neural Engineering, 15(3):031005, apr 2018. doi: 10.1088/1741-2552/aab2f2. URL https: //doi.org/10.1088/1741-2552/aab2f2.\n\nEric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved\n\nsampling speed, 2021. URL https://arxiv.org/abs/2101.02388.\n\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2837–2845, June 2021.\n\nCharlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with\n\nsparse representations, 2021. URL https://arxiv.org/abs/2103.03841.\n\nChethan Pandarinath, Paul Nuyujukian, Christine H Blabe, Brittany L Sorice, Jad Saab, Francis R Willett, Leigh R Hochberg, Krishna V Shenoy, and Jaimie M Henderson. High performance communication by people with paralysis using an intracortical brain-computer interface. eLife, 6: e18554, feb 2017. ISSN 2050-084X. doi: 10.7554/eLife.18554. URL https://doi.org/ 10.7554/eLife.18554.\n\nSharaj Panwar, Paul Rad, Tzyy-Ping Jung, and Yufei Huang. Modeling eeg data distribution with a wasserstein generative adversarial network to predict rsvp events. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 28(8):1720–1730, Aug 2020. ISSN 1558-0210. doi: 10.1109/TNSRE.2020.3006180.\n\nKay Robbins, Kyung min Su, and W. David Hairston. An 18-subject eeg data collection using a visual-oddball task, designed for benchmarking algorithms and headset performance comparisons. Data in Brief, 16:227–230, 2018. ISSN 2352-3409. doi: https://doi.org/10.1016/j.dib. 2017.11.032. URL https://www.sciencedirect.com/science/article/pii/ S2352340917306285.\n\nYannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Jocelyn Faubert. Deep learning-based electroencephalography analysis: a systematic review. Journal of Neural Engineering, 16(5):051001, aug 2019. doi: 10.1088/1741-2552/ab260c. URL https://doi.org/10.1088/1741-2552/ab260c.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=TIdIXIpzhoI.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/ file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf.\n\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1415–1428. Curran Associates, Inc., 2021. URL https://proceedings.neurips. cc/paper/2021/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf.\n\nYusuke Tashiro,\n\nJiaming Song, Yang Song, and Stefano Ermon.\n\nCsdi: Conditional score-based diffusion models for probabilistic time series imputation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 24804–24816. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ cfe8504bda37b575c70ee1a8276f3486-Paper.pdf.\n\nPieter van Mierlo, Bernd J. Vorderw ̈ulbecke, Willeke Staljanssens, Margitta Seeck, and Serge Vulli ́emoz. Ictal eeg source localization in focal epilepsy: Review and future perspectives. Clinical Neurophysiology, 131(11):2600–2616, 2020. ISSN 1388-2457. doi: https://doi.org/10.1016/ j.clinph.2020.08.001. URL https://www.sciencedirect.com/science/article/ pii/S1388245720304417.\n\nMarco Vilela and Leigh R. Hochberg. Chapter 8 - applications of brain-computer interfaces In Nick F. Ramsey and Jos ́e del R. Mill ́an to the control of robotic and prosthetic arms. (eds.), Brain-Computer Interfaces, volume 168 of Handbook of Clinical Neurology, pp. 87– 99. Elsevier, 2020. doi: https://doi.org/10.1016/B978-0-444-63934-9.00008-1. URL https: //www.sciencedirect.com/science/article/pii/B9780444639349000081.\n\nQing Wang, Lin Meng, Jun Pang, Xiaodong Zhu, and Dong Ming. Characterization of eeg data revealing relationships with cognitive and motor symptoms in parkinson’s disease: A systematic review. Frontiers in Aging Neuroscience, 12, 2020. ISSN 1663-4365. doi: 10.3389/fnagi. 2020.587396. URL https://www.frontiersin.org/articles/10.3389/fnagi. 2020.587396.\n\nJiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and Luc Van\n\nGool. Sliced wasserstein generative models, 2019.\n\nMeng Xu, Yuanfang Chen, Yijun Wang, Dan Wang, Zehua Liu, and Lijian Zhang. Bwgan-gp: An eeg data generation method for class imbalance problem in rsvp tasks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:251–263, 2022. ISSN 1558-0210. doi: 10.1109/TNSRE.2022.3145515.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 GENERATED MOTOR IMAGERY SIGNALS\n\nFigure 10: PSD top plots of the generated signals from the BCIC4D2a dataset. The top, middle, and bottom figures visualize the right hand, feet, and tongue classes.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 GENERATED SUBJECT-SPECIFIC ERP SIGNALS\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n17",
    "reference": "# Summary Of The Paper\n\nThe authors proposed the use of diffusion probabilistic models (DPMs) to generate EEG data in order to increase the size of datasets for training brain-computer interface classifiers. The qualitative and quantitative characteristics of the generated dataset were compared to the RWGAN model. The DPM model turned out to be comparable with RWGAN. The dataset of evoked potentials and the task of separating imaginary movements were taken as initial datasets. The authors note that DPM is able to generate EEG recordings that preserve the individual characteristics of subjects. The possibility of increasing the performance of EEGNet on the BCI4D2a dataset (classification of imaginary movements) was considered by expanding the dataset with generated data and by pretraining the dataset on these data. The second approach showed better results.\n\n# Strength And Weaknesses\n\n# Strengths\n\n-   The authors have shown that diffusion probabilistic models (DPMs) are promising for generating brain data.\n-   The problem to be solved is sufficiently substantiated in the introduction.\n-   Two different datasets with different tasks are considered\n\n# Weaknesses\n\n-   It is not clear how the data was divided into train and test sets in section 5.2\n-   a more detailed analysis of the specificity of subjects is missing (Section 5.3):\n\n    1.  it would be good to supplement with a comparison\n\n    of the specificity of subjects with other methods,\n\n    1.  It is interesting to see the class of problems where such a property of the method is an advantage, and in which it is a limitation. Does this mean that this method cannot generate new subject data?\n-   In section 5.4, the conclusion that pretraining on the generated data gives a gain is not obvious from the form of the matrix. Perhaps a general statistic would help.\n\nDDIM - no decryption of the abbreviation One-hot - typo The number of training iterations in steps 2 and 1 is doubled, following the work of . - link missing\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClear, but not terribly original demonstration that diffusion models have a potential for EEG signal generation.\n\n# Summary Of The Review\n\nA paper applying diffusion models to the task of EEG data generation. Although the idea is interesting and the results are potentially promising, as mostly an application, the paper lacks in rigorous evaluation and empirical demonstration.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nFINDE: NEURAL DIFFERENTIAL EQUATIONS FOR FINDING AND PRESERVING INVARIANT QUANTITIES\n\nTakashi Matsubara Osaka University Toyonaka, Osaka, 560–8531 Japan matsubara@sys.es.osaka-u.ac.jp\n\nTakaharu Yaguchi Kobe University Kobe, Hyogo, 657–8501 Japan yaguchi@pearl.kobe-u.ac.jp\n\nABSTRACT\n\nMany real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the projection method and the discrete gradient method, FINDE finds and preserves first integrals from data, even in the absence of prior knowledge about underlying structures. Experimental results demonstrate that FINDE can predict future states of target systems much longer and find various quantities consistent with well-known first integrals in a unified manner.\n\n1\n\nINTRODUCTION\n\nModeling and predicting real-world systems are fundamental aspects of understanding the world in natural science and improving computer simulations in industry. Target systems include chemical dynamics for discovering new drugs (Raff et al., 2012), climate dynamics for climate change prediction and weather forecasting (Rasp et al., 2020; Trigo & Palutikof, 1999), and physical dynamics of vehicles and robots for optimal control (Nelles, 2001). In addition to image processing and natural language processing (Devlin et al., 2018; He et al., 2016), neural networks have been actively studied for modeling dynamical systems (Nelles, 2001). Their history dates back to at least the 1990s (see Chen et al. (1990); Clouse et al. (1997); Levin & Narendra (1995); Narendra & Parthasarathy (1990); Sj ̈oberg et al. (1994); Wang & Lin (1998) for examples). Recently, two notable but distinct families have been proposed. Physics-informed neural networks (PINNs) directly solve partial differential equations (PDEs) given as symbolic equations (Raissi et al., 2019). Neural ordinary differential equations (NODEs) learn ordinary differential equations (ODEs) from observed data and solve them using numerical integrators (Chen et al., 2018). Our focus this time is on NODEs.\n\nMost real-world systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time (Hairer et al., 2006). First integrals arise from intrinsic geometric structures of systems and are sometimes more important than superficial dynamics in understanding systems (see Appendix A for details). Many previous studies have extended NODEs by incorporating prior knowledge about first integrals and attempted to accurately learn a target system. Greydanus et al. (2019) proposed the Hamiltonian neural network (HNN), which employs a neural network to approximate Hamilton’s equation, thereby conserving the system energy called the Hamiltonian. Finzi et al. (2020a) proposed neural network architectures that conserve linear and angular momenta by utilizing the graph structure. Finzi et al. (2020b) also extended an HNN to a system with holonomic constraints, which led to first integrals such as a pendulum length.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of Related Studies on Preservation of First Integrals.\n\nEnergy Monentum\n\nMass\n\nConstraint\n\nLearninginvariants\n\nExactconservation\n\nNODE (Chen et al., 2018) HNN (Greydanus et al., 2019) LieConv (Finzi et al., 2020a) DGNet (Matsubara et al., 2020) CHNN (Finzi et al., 2020b) NPM (Yang et al., 2020)\n\nContinuous FINDE (proposed) Discrete FINDE (proposed)\n\n✓ ✓\n✓ ✓\n\n✓ ✓\n\n✓\n\n✓ ✓\n\n✓\n\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n\n✓\n\n✓ ✓\n\n✓\n\n✓\n\n✓\n\nMatsubara et al. (2020) proposed a model that preserves the total mass of a discretized PDE. These studies have demonstrated that the more prior knowledge a neural network has about first integrals, the more accurate their dynamics prediction. See Table 1 for comparisons.\n\nPrevious studies have mainly attempted to preserve known first integrals for better computer simulations. However, in situations where a neural network learns a target system, it is naturally expected that first integrals associated with the target system are unknown, and it is not clear which of the above methods are available. Therefore, this study proposes first integral-preserving neural differential equation (FINDE) to find and preserve unknown first integrals from data in a unified manner. FINDE has two versions for continuous and discrete time; these have the following advantages.\n\nFinding First Integrals Many studies have designed architectures or operations of neural networks to model continuous-time dynamics with known types of first integrals. However, the underlying geometric structures of a target system are generally unknown in practice. In contrast, FINDE finds various types of first integrals from data in a unified manner and preserves them in predictions. For example, from an energy-dissipating system, FINDE can find first integrals other than energy. FINDE can find not only known first integrals, but also unknown ones. Hence, FINDE can lead to scientific discoveries.\n\nCombination with Known First Integrals FINDE can be combined with previously proposed neural networks designed to preserve known first integrals, such as HNNs. In addition, when some first integrals are known in advance, they can also be incorporated into FINDE to avoid rediscovery. Therefore, FINDE is available in various situations.\n\nExact Preservation of First Integrals The first integral associated with a continuous-time system is destroyed after the dynamics is temporally discretized for computer simulations. By leveraging the discrete gradient, the discrete-time version of FINDE preserves first integrals exactly (up to rounding errors) in discrete time and further improves the prediction performance.\n\n2 BACKGROUND AND RELATED WORK\n\nFirst Integrals Let us consider a time-invariant differential system d dt u = f (u) on an N - dimensional manifold M, where u denotes the system state and f : M → TuM represents a vector field on M. For simplicity, we suppose the manifold M to be a Euclidean space RN . Definition 1 (first integral). A quantity V : M → R is referred to as a first integral of a system dt u = f (u) if it remains constant along with any solution u(t), i.e., d\n\ndt V (u) = 0.\n\nd\n\nIf a differential system d solution u(t) given an initial value u0 stays at the (N − K)-dimensional submanifold\n\ndt u = f (u) has K functionally independent first integrals V1, . . . , VK, the\n\nM′ = {u ∈ M : V1(u) = V1(u0), . . . , VK(u) = VK(u0)}. (1) The tangent space TuM′ ⊂ TuM of the submanifold M′ ⊂ M at a point u is the orthogonal complement to the space spanned by the gradients ∇Vk(u) of the first integrals Vk for k = 1, . . . , K; TuM′ = {w ∈ TuM : ∇Vk(u)⊤w = 0 for k = 1, . . . , K}. (2) Conversely, if the time-derivative f at point u is on the tangent space TuM′ for certain functions Vk’s, the quantities Vk’s are first integrals of the system d dt Vk(u) = ∇Vk(u)⊤ d\n\ndt u = f (u); it holds that d\n\ndt u = ∇Vk(u)⊤f (u) = 0.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nOne of the most well-known first integrals is the Hamiltonian H, which represents the system energy of a Hamiltonian system. Noether’s theorem states that a continuous symmetry of a system leads to a conservation law (and hence a first integral) (Hairer et al., 2006). A Hamiltonian system is symmetric to translation in time, and the corresponding first integral is the Hamiltonian. Symmetries to translation and rotation in space lead to the conservation of linear and angular momenta. However, not all first integrals are related to symmetries. A pendulum can be expressed in Cartesian coordinates, and then the rod length constrains the mass position. This type of constraint is called a holonomic constraint and leads to first integrals. Models of disease spreads and chemical reactions have the total mass (population) as the first integral. Also for a system described by a PDE, the total mass is sometimes a first integral (Furihata & Matsuo, 2010). See Appendix A for the classes of dynamics, their geometric structures, and related studies to find or preserve first integrals.\n\nFirst Integrals in Numerical Analysis For computer simulations, differential systems are discretized in time and solved by numerical integration, causing numerical errors (which is composed of temporal discretization errors and rounding errors). Moreover, the geometric structures of the system are often destroyed, and the corresponding first integrals are no longer preserved. A common remedy is a symplectic integrator, which preserves the symplectic structure and accurately integrates Hamiltonian systems (Hairer et al., 2006). However, the Ge–Marsden theorem states that a symplectic integrator only approximately conserves the Hamiltonian (Zhong & Marsden, 1988). Hence, many numerical schemes have also been investigated to preserve first integrals exactly, while these schemes cannot preserve the symplectic structure. Some examples are shown below.\n\nLet the superscript s ∈ {0, 1, . . . , S} denote the state us or time ts at the s-th time step, and ∆ts = ts+1 − ts denote a time-step size. A projection method uses a numerical integrator to predict the next state ̃us+1 from the current state us and then projects the state ̃us+1 onto the submanifold M′ (Gear, 1986; Hairer et al., 2006, Section IV.4). The projected state us+1 preserves the first integrals Vk. In particular, the projected state us+1 is obtained by solving the optimization problem (3)\n\n∥us+1 − ̃us+1∥ subject to Vk(us+1) − Vk(us) = 0 for k = 1, . . . , K.\n\narg min us+1\n\nThe local coordinate method defines a coordinate system on the neighborhood of the current state us and integrates a differential equation on it (Potra & Yen, 1991; Hairer et al., 2006, Section IV.5). The discrete gradient method defines a discrete analogue to a differential system and integrates it in discrete time, thereby preserving the Hamiltonian exactly (up to rounding errors) in discrete time (Furihata & Matsuo, 2010; Gonzalez, 1996; Hong et al., 2011).\n\nNeural Networks to Preserve First Integrals NODE defines the right-hand side f of a differential system d dt u = f (u) using a neural network in the most general way with no associated first integrals (Chen et al., 2018). NODE is a universal approximator to ODEs and can approximate any ODE with arbitrary accuracy if there is an infinite amount of training data (Teshima et al., 2020). In practice, the amount of training data is limited, and prior knowledge about the target system is helpful for learning (see Sannai et al. (2021) for the case with convolutional neural networks (CNNs)). HNN (Greydanus et al., 2019) assumes the target system to be a Hamiltonian system in the canonical form, thereby guaranteeing various properties of Hamiltonian systems by definition, including the conservation of energy and preservation of the symplectic structure in continuous time (Hairer et al., 2006). Some studies have employed a symplectic integrator for HNN to preserve the energy and symplectic structure with smaller numerical errors (Chen et al., 2020). LieConv and EMLPHNN employ neural network architectures with translational and rotational symmetries to preserve momenta (Finzi et al., 2020a; 2021). CHNN incorporates a known holonomic constraint in the dynamics (Finzi et al., 2020b). Deep conservation extracts latent dynamics of a PDE system and preserves a quantity of interest by forcing its flux to be zero (Lee & Carlberg, 2021). HNN++ also guarantees the conservation of mass in PDE systems by using a coefficient matrix derived from differential operators (Matsubara et al., 2020). These methods preserve known types of first integrals and suffer from temporal discretization errors. In contrast, FINDE learns any types of first integrals from data and preserves them even after temporal discretization.\n\nThe neural projection method (NPM) learns fixed holonomic constraints using the projection (and inequality constraints) (Yang et al., 2020). DGNet employed discrete gradient methods to guarantee the energy conservation in Hamiltonian systems (and the energy dissipation in friction systems) (Matsubara et al., 2020). While these methods preserve the aforementioned first integrals exactly in discrete time, their formulations are not available for other first integrals.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nSeveral studies have proposed neural networks to learn Lyapunov functions, which are expected to be non-increasing over time, in contrast to first integrals (Manek & Kolter, 2019; Takeishi & Kawahara, 2020). If the state moves in the direction of increasing the function, it is projected onto or moved inside the contour line of the Lyapunov function. This concept is similar to that of the continuous-time version of FINDE but focuses on a single non-increasing quantity in continuous time; FINDE preserves multiple quantities in both continuous and discrete time.\n\n3 FIRST INTEGRAL-PRESERVING NEURAL DIFFERENTIAL EQUATION\n\nWe suppose that a target system has at least K unknown functionally independent first integrals. When a neural network learns the dynamics of the target system, it is not guaranteed to learn these first integrals. We suppose that a certain neural network ˆf for modeling the target dynamics is given, and in addition to this model ˆf , we introduce a neural network that outputs a K-dimensional vector V (u) = (V1(u) V2(u) . . . VK(u))⊤. Each element is expected to learn one of the first integrals as Vk : RN → R for k = 1, . . . , K. Then, the submanifold M′ is defined as in Eq. (1).\n\n3.1 CONTINUOUS FINDE: TIME-DERIVATIVE PROJECTION METHOD\n\nWe propose a time-derivative projection method called continuous FINDE (cFINDE). The cFINDE projects the time-derivative onto the tangent space TuM′. Roughly speaking, the cFINDE projects the dynamics on the space of the directions in which the first integrals do not change. In this way, the method can learn dynamics while preserving first integrals V , thereby finding unknown first integrals from data.\n\nWe refer to the neural network that defines the time-derivative ˆf : RN → RN as the base model. Applying the method of Lagrange multipliers to the projection method in Eq. (3), and taking the limit as the time-step size approaches zero, we have\n\nd\n\ndt u = f (u), f (u) = ˆf (u) − M (u)⊤λ(u), d\n\n(4) ∂u and λ ∈ RN is the Lagrange multiplier (see Appendix B.1 for detailed derivation).\n\ndt V (u) = 0,\n\nwhere M = ∂V We transform the second equation to obtain\n\n0 = d\n\ndt V (u(t)) = ∂V\n\n(5) from which we obtain the Lagrange multiplier λ(u) = (M (u)M (u)⊤)−1M (u) ˆf (u). By eliminating λ(u), we define the cFINDE as\n\ndt u = M (u)f (u) = M (u)( ˆf (u) − M (u)⊤λ(u)),\n\n∂u\n\nd\n\nd\n\ndt u = f (u) = (I − Y (u)) ˆf (u) for Y (u) = M (u)⊤(M (u)M (u)⊤)−1M (u).\n\n(6)\n\nTheorem 1 (continuous-time first integral preservation). The cFINDE d first integrals Vk for k = 1, . . . , K in continuous time, that is, d\n\ndt Vk = 0.\n\ndt u = f (u) preserves all\n\nSee Appendix B.1 for proof. The base model ˆf can be a NODE, an HNN, or any other model depending on the available prior knowledge. Additionally, if a first integral is already known, it can be directly used as one of the first integrals Vk instead of being found by the neural network. Note that even though the base model ˆf is an HNN, due to the projection, the cFINDE f is no longer a Hamiltonian system in the strict sense.\n\nCompared to the base model ˆf , the cFINDE requires the additional computation of the neural network V , several matrix multiplications, and an inverse operation. The inverse operation has a computational cost of O(K 3), which is not costly if the number K of first integrals is small. Many previous models also need the inverse operation to satisfy the constraints and geometric structures, such as Lagrangian neural network (LNN) (Cranmer et al., 2020), neural symplectic form (Chen et al., 2021), and CHNN (Finzi et al., 2020b).\n\n3.2 DISCRETE FINDE: DISCRETE-TIME DERIVATIVE PROJECTION METHOD\n\nThe cFINDE is still an ODE and hence needs to be solved using a numerical integrator, which causes the temporal discretization errors in the first integrals. In order to eliminate these errors, it is necessary to constrain the destination (i.e., finite difference) rather than the direction (i.e., time-derivative).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFor this purpose, we propose discrete FINDE (dFINDE) by employing discrete gradients to define discrete tangent spaces, which are needed to constraint the state variables on the submanifold M′.\n\nA discrete gradient ∇V is a discrete analogue to a gradient ∇V (Furihata & Matsuo, 2010; Gonzalez, 1996; Hong et al., 2011). Recall that a gradient ∇V of a function V : RN → R can be regarded as a function RN → RN that satisfies the chain rule d dt u. Analogously, a discrete gradient ∇ is defined as follows: Definition 2 (discrete gradient). A discrete gradient ∇V of a function V : RN → R is a function RN × RN → RN that satisfies V (v) − V (u) = ∇V (v, u)⊤(v − u) and ∇V (u, u) = ∇V (u).\n\ndt V (u) = ∇V (u)⊤ d\n\nThe first condition is a discrete analogue to the chain rule when replacing the time-derivatives d dt V and d dt u with finite differences (V (v) − V (u)) and (v − u), respectively, and the second condition ensures consistency with the ordinary gradient ∇V . A discrete gradient ∇V is not uniquely determined and has been obtained manually. Recently, the automatic discrete differentiation algorithm (ADDA) has been proposed by Matsubara et al. (2020), which obtains a discrete gradient of a neural network in a manner similar to the automatic differentiation algorithm (Abadi et al., 2016; Paszke et al., 2017). The discrete gradient is defined in discrete time; hence, the prediction using the discrete gradient is free from temporal discretization errors. See Appendix B.2 and the references Furihata & Matsuo (2010); Matsubara et al. (2020) for more details.\n\nFollowing Christiansen et al. (2011); Dahlby et al. (2011), we introduce a discrete analogue to the tangent space TuM′ called the discrete tangent space T(v,u)M′. In particular, for a pair of points (v, u) ∈ M′, the discrete tangent space is defined as\n\nT(v,u)M′ = {w ∈ RN : ∇Vk(v, u)⊤w = 0 for k = 1, . . . , K}. (7) If the finite difference (us+1 − us) between the predicted and current states is on the discrete tangent space T(us+1,us)M′, the first integrals Vk are preserved because Vk(us+1) − Vk(us) = ∇Vk(us+1, us)⊤(us+1 − us) = 0. Note that similar concepts defined in different ways are also referred to as discrete tangent spaces (Cuell & Patrick, 2009; Dehmamy et al., 2021).\n\nWe suppose that a neural network (e.g., NODE) ˆf defines an ODE and a numerical integrator predicts the next state ̃us+1 from a given state us. We call this process a discrete-time base model ˆψ, which satisfies ̃us+1−us\n\n∆ts = ˆψ(us; ∆ts). Subsequently, we consider the model\n\nus+1−us\n\n∆ts = ψ(us+1, us; ∆ts),\n\nψ(us+1, us; ∆ts) = ˆψ(us; ∆ts) − M (us+1, us)⊤λ(us+1, us), V (us+1) − V (us) = 0,\n\n(8)\n\nwhere M (us+1, us) = (∇V1(us+1, us) . . . ∇VK(us+1, us))⊤. As shown in Appendix B.1, this formulation is also derived from the projection method in Eq. (3). Using the chain rule of the discrete gradient,\n\n0 = V (us+1)−V (us)\n\n∆ts\n\n= M (us+1, us) us+1−us\n\n∆ts = M (us+1, us)ψ(us+1, us; ∆ts),\n\n(9)\n\nSubstituting this into Eq. (8) and eliminating the Lagrange multiplier λ, we define the dFINDE as\n\nus+1−us\n\n∆ts = ψ(us+1, us; ∆ts) = (I −Y (us+1, us)) ˆψ(us; ∆ts) for Y = M\n\n⊤\n\n(M M\n\n⊤\n\n)−1M , (10)\n\nwhere we have abbreviated M (us+1, us) and Y (us+1, us) to M and Y , respectively. Theorem 2 (discrete-time first integral preservation). The dFINDE us+1−us preserves all first integrals Vk for k = 1, . . . , K in discrete time, that is, Vk(us+1) − Vk(us) = 0.\n\n= ψ(us+1, us; ∆ts)\n\n∆ts\n\nSee Appendix B.1 for proof. Intuitively, dFINDE projects the finite difference (discrete-time derivative) ˆψ onto the discrete tangent space T(us+1,us)M′ after the numerical integration for each step, whereas cFINDE projects the time-derivative ˆf onto the tangent space TuM′ at every substep inside a numerical integrator. In the discrete-time base model ˆψ, the ODE ˆf can be defined by any model, such as NODE or HNN, and the numerical integrator can be implemented by any method, such as the Runge–Kutta method or the leapfrog integrator. The projection method in Eq. (3), the method in Eq. (8), and the dFINDE in Eq (10) are implicit methods and hence relatively computationally expensive. However, only the dFINDE can be trained non-iteratively by standard backpropagation algorithms. As explained in Appendix B.3, this is because the next state us+1 is given during training and the ADDA can explicitly obtain the discrete gradient and its computational graph.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Datasets, Dynamics, and First Integrals.\n\nDataset\n\nDynamics (Structure)\n\nN Energy Momentum Mass Constraint\n\nTwo-body problem Discretized KdV equation Double pendulum FitzHugh–Nagumo model\n\nCanonical Hamiltonian Non-canonical Hamiltonian Poisson Dirac\n\n8 50 8\n4\n\n✓ ✓\n✓\n\n✓\n\n✓\n\n✓ ✓\n\nFirst Integrals\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETTINGS\n\nTarget Systems We evaluated FINDE and base models using datasets associated with first integrals; these are summarized in Table 2. A gravitational two-body problem (2-body) on a 2dimensional configuration space is a typical Hamiltonian system in the canonical form. In addition to the total energy, the system has first integrals related to symmetries in space, namely, the linear and angular momenta. The Korteweg–De Vries (KdV) equation is a PDE model of shallow water waves. This equation is a Hamiltonian system in a non-canonical form and has the Hamiltonian, total mass, and many other quantities as first integrals. We discretized the KdV equation in space, obtaining a fifty-dimensional state u. A double pendulum (2-pend) is a Hamiltonian system in polar coordinates. However, we transformed it to Cartesian coordinates; hence, it became a Poisson system. The lengths of the two rods work as holonomic constraints and lead to four first integrals in addition to the Hamiltonian. The FitzHugh–Nagumo model is a biological neuron model as an electric circuit, which exhibits a rapid and transient change of voltage called a spike. As an electric circuit, the currents through and voltages applied to the inductor and capacitor can be regarded as system states, which are constrained by the circuit topology and Kirchhoff’s current and voltage laws. Then, this system has a state of four elements and two first integrals. Because the resistor dissipates the energy, the system is not a Poisson system, but a Dirac structure can be found (van der Schaft & Jeltsema, 2014). We generated a time-series set of each dataset with different initial conditions (hence, different values of first integrals). See Appendix C for more details.\n\nImplementation We implemented the proposed FINDE and evaluated it under the following settings. We implemented all codes by modifying the officially released codes of HNN (Greydanus et al., 2019) 1 and DGNet (Matsubara et al., 2020)2. We used Python v. 3.8.12 with packages scipy v. 1.7.3, pytorch v. 1.10.2, torchdiffeq v. 0.1.1, functorch v. 1.10 preview, and gplearn v. 0.4.2. We used the Dormand–Prince method (dopri5) (Dormand & Prince, 1986) as the numerical integrator, except in Section 4.2. All experiments were performed on a single NVIDIA A100.\n\nFollowing HNN (Greydanus et al., 2019) and DGNet (Matsubara et al., 2020), we used fullyconnected neural networks with two hidden layers. The input was the state u, and the output represented the first integrals V for FINDE, time-derivative ˆf for NODE, or the Hamiltonian H for HNN. Each hidden layer had 200 units and preceded a hyperbolic tangent activation function. Each weight matrix was initialized as an orthogonal matrix. For the KdV dataset, we used a 1-dimensional CNN, wherein the kernel size of each layer was 3. The double pendulum is a second–order system, implying that the time-derivative d dt q of the position q is known to be the velocity v. Hence, we treated only the acceleration d dt v as the output to learn in the 2-pend dataset. This assumption slightly improved the absolute performances but did not change the relative trends.\n\nGT and the future state us+1\n\nAs the loss function for the cFINDE, we used the mean squared error (MSE) between the ground truth future state us+1 GT normalized by the time-step size ∆ts; we named this the 1-step error. For the dFINDE, we used the MSE between the left- and right-hand sides of Eq. (10) because the ground truth states us GT are available during the training phase. The base model and FINDE were jointly trained using the Adam optimizer (Kingma & Ba, 2015) with the parameters (β1, β2) = (0.9, 0.999) and a batch size of 200.\n\npred. predicted from the current step us\n\nGT and us+1\n\n1https://github.com/greydanus/hamiltonian-nn 2https://github.com/tksmatsubara/discrete-autograd\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nThe learning rate was initialized to 10−3 and decayed to zero with cosine annealing (Loshchilov & Hutter, 2017). See Appendix B.3 and the enclosed source code for details about implementations.\n\nEvaluation Metric We used the 1-step error as an evaluation metric, which is identical to the loss function for the cFINDE, and displayed it in the scale ×10−9. The lower this indicator, the better, as indicated by ↓. The MSEs of the state or system energy over a long period are misleading indicators, as suggested in prior studies (Botev et al., 2021; Jin et al., 2020b; Vlachas et al., 2020). For example, a periodic orbit that is correctly learned except for a slight difference in angular velocity would have the same MSE as an orbit that never moves from its initial position. Instead, we used the valid prediction time (VPT) (Botev et al., 2021; Jin et al., 2020b; Vlachas et al., 2020). VPT denotes the time point s divided by the length S of the time-series at which the MSE of the predicted state us first exceeds a given threshold θ in an initial value problem, that is,\n\npred.\n\nV P T (upred.; uGT) = 1\n\nS max{sf |MSE(us\n\npred., us\n\nGT) < θ for all s ≤ sf , 0 ≤ sf ≤ S}.\n\n(11)\n\nThe higher this indicator, the better, as indicated by ↑. To obtain VPTs, we normalized each element of state to have the zero mean and unit variance in the training data and set θ to 0.01. For systems with “spiking” behaviors, a small error in phase may be regarded as a significant error in the state; for the FitzHugh–Nagumo model, we obtained the VPTs by allowing for a delay and advance of up to 5 steps.\n\n4.2 DEMONSTRATION OF FIRST INTEGRAL PRESERVATION\n\nBefore learning first integrals from data, we demonstrate that dFINDE can preserve first integrals without temporal discretization errors. We used a mass-spring system, which had the state u = (q v)⊤, dynamics d dt v = −q, and system energy E(q, v) = 1 2 (q2 + v2). Using an initial value of (1.0 0.0)⊤ and a time-step size of ∆t = 0.2, we solved the initial value problem of the true ODE using the leapfrog integrator with or without FINDE, with the true system energy E as the first integral V . Notably, no neural networks nor training were involved.\n\ndt q = v and d\n\nFigure 1: Integration of a known mass-spring system by (top) the leapfrog integrator. States predicted by comparison methods. (bottom) Energy calculated from the states predicted.\n\nFigure 1 shows the results, along with the analytical solution. The states predicted by comparison methods overlap and are apparently identical. However, the energy obtained by the leapfrog integrator fluctuates and the same is true for cFINDE. This is because the leapfrog integrator and cFINDE suffer from temporal discretization errors in first integrals. In contrast, dFINDE preserves the energy accurately, the same as the analytical solution. This is because dFINDE projects the state (q v)⊤ onto the discrete tangent space T(v,u)M′ at every step. Although a smaller time-step size reduces temporal discretization errors, this result demonstrates the advantage of dFINDE. See Appendix D.1 for the case with the Dormand-Prince integrator.\n\n4.3 FINDING NON-HAMILTONIAN FIRST INTEGRALS OF HAMILTONIAN SYSTEMS\n\nWe evaluated cFINDE and dFINDE on learning from the 2-body dataset. We used HNN as the base model ˆf . We found that cFINDE and dFINDE obtained better performances if it did not treat the Hamiltonian H of the HNN as one of the first integrals Vk. The medians and standard deviations of five trials are summarized in the leftmost column of Table 3. The cFINDE achieved better VPTs than the original HNN with K = 1 to 2, and its performance was suddenly degraded with K = 3. The dFINDE showed a similar trend with slightly better performances; there is a trade-off between performance and computational cost. The HNN with either cFINDE or dFINDE found two first integrals in addition to the Hamiltonian H of the HNN. Even though a two-body problem is a Hamiltonian system that an HNN can learn, the prior knowledge that there exist first integrals other than the Hamiltonian H can be a clue that enables better learning. Despite their better long-term prediction performance, the HNN with either cFINDE or dFINDE yielded 1-step errors worse than the HNN, indicating that the 1-step error is misleading as an evaluation criterion.\n\nThese example results are depicted in Fig. 2. In the absence of FINDE, the mass positions (x1, y1) and (x2, y2) became inaccurate in a short time and the center-of-gravity position (xc, yc) =\n\n7\n\n−1.00.01.0stateqv050steps0.4950.500energyanalyticalcFINDEleapfrogdFINDEPublished as a conference paper at ICLR 2023\n\nTable 3: Results of cFINDE and dFINDE.\n\n2-body + HNN\n\nKdV\n\n2-pend\n\nFitzHugh–Nagumo\n\nModel\n\nK 1-step↓\n\nVPT↑\n\n1-step↓\n\nVPT↑\n\n1-step↓\n\nVPT↑\n\n1-step↓\n\nVPT↑\n\nbase model – 5.17 ±0.570 0.362 ±0.026 5.59 ±0.300 0.339 ±0.038 0.82 ±0.020 0.110 ±0.035 73.66 ±12.59 0.236 ±0.053\n\n1 7.10 ±1.250 0.374 ±0.036 6.24 ±0.440 0.371 ±0.088 0.75 ±0.040 0.156 ±0.042 54.18 ±8.120 0.127 ±0.148 2 7.78 ±1.390 0.450 ±0.052 2.59 ±0.110 0.608 ±0.085 0.73 ±0.050 0.198 ±0.088 37.03 ±3.810 0.437 ±0.084\n\n+ cFINDE 3 > 103 4 > 103 5 > 103 6 > 103\n\n> 106 0.147 ±0.146∗ 3.19 ±0.370 0.730 ±0.091 0.69 ±0.030 0.411 ±0.093 —\n0.101 ±0.005 3.65 ±0.300 0.641 ±0.071 0.77 ±0.070 0.395 ±0.083 0.080 ±0.014 4.68 ±0.430 0.601 ±0.069 0.80 ±0.070 0.585 ±0.097 —\n0.070 ±0.019 7.79 ±0.510 0.425 ±0.067 12.53 ±0.000 0.005 ±0.000∗ —\n\n0.007 ±0.007∗\n\n1 7.01 ±1.060 0.379 ±0.040 11.61 ±6.600 0.288 ±0.083 0.75 ±0.100 0.152 ±0.017 47.07 ±8.030 0.117 ±0.122 2 7.03 ±1.000 0.475 ±0.022 2.70 ±0.260 0.598 ±0.059 0.74 ±0.050 0.271 ±0.111 33.24 ±3.400 0.455 ±0.032 + dFINDE 3 54.78 ±36.39 0.309 ±0.024 3.78 ±0.270 0.636 ±0.024 0.69 ±0.050 0.447 ±0.081 319.70 ±91.11 0.049 ±0.007 0.102 ±0.015 3.48 ±0.320 0.780 ±0.059 0.71 ±0.030 0.454 ±0.060 0.086 ±0.011∗ 5.26 ±0.150 0.718 ±0.038 0.86 ±0.090 0.591 ±0.087 0.059 ±0.017 9.60 ±3.610 0.573 ±0.121 58.88 ±22.98 0.037 ±0.039\n\n4 > 103 5 > 103 6 > 103\n\n— —\n—\n\nNotes: Standard deviation follows the ± symbol; underlined results are better than those of the base models; bold font indicates best results; ∗ denotes trials that failed in training because of underflow of time-step size.\n\nFigure 2: Example results of 2-body dataset. (left) Ground truth. (middle) HNN. (right) HNN with cFINDE.\n\nFigure 3: Mean absolute errors for results of 2-body dataset. (left) HNN. (right) HNN with cFINDE.\n\n2\n\n2\n\n, y1+y2\n\n( x1+x2 ) deviated rapidly. The HNN with cFINDE accurately predicted the state for a longer period. Even after errors in the mass positions became non-negligible, errors in the center-of-gravity position were still small. Figure 3 shows the absolute errors averaged over all trials, which demonstrate how the trend changes with cFINDE. In both the x- and y-directions, the HNN without FINDE produced errors in the center-of-gravity position xc (or yc), and those in the mass positions x1, x2 (or y1, y2) at a similar level. In contrast, with the cFINDE, errors in the center-of-gravity position were much smaller than those in the mass positions, implying that errors in one mass position canceled out errors in the other. We performed a symbolic regression of first integrals V found by the neural network. For K = 2, the found first integrals V were identical to the linear momenta in the x- and y-directions up to affine transformation in most cases. See Appendix D.2 for detailed results. Therefore, we conclude that FINDE not only had better prediction accuracy but also found and preserved linear momenta (which are related to symmetries in space) more accurately despite not having prior knowledge about symmetries.\n\n4.4 FINDING FIRST INTEGRALS OF UNKNOWN SYSTEMS\n\nIt is often unclear whether a target system is a Hamiltonian system or not, but one can expect that it has several first integrals. We evaluated cFINDE and dFINDE using NODE as the base model and display the results in Table 3.\n\nFor the KdV dataset, the NODE with either cFINDE or dFINDE obtained improved VPTs for a wide range of K. Figure 4 shows an example result. The prediction states were apparently similar. In the absence of FINDE, the NODE increased all of its errors in proportion to time. With cFINDE, the error in total mass increased at the point where the two solitons collided, but then returned to the original level. Although the calculation is slightly inaccurate, the cFINDE learned to preserve the total mass. The error in energy continued to increase for K = 2, but remained within a small range for K = 3. These results suggest that the first or second quantity learned by the cFINDE was total\n\n8\n\ngroundtruthHNN+cFINDEtrue(xc,yc)(xc,yc)(x1,y1)(x2,y2)0104HNN020104+cFINDE0.00.5x1,x2y1,y2xcycPublished as a conference paper at ICLR 2023\n\nFigure 4: Example results of KdV dataset. (top) Predicted states. Red belts denote moving solitons. (bottom) Mean absolute errors in states u, total mass (cid:80)N\n\nk=1 uk, and energy, from left to right.\n\nFigure 5: Example results of 2-pend dataset for 2,000 steps. (left) Ground (middle) NODE. (right) NODE truth. with cFINDE for K = 5.\n\nFigure 6: Example results of FitzHugh–Nagumo dataset. Each panel shows one of four states.\n\nmass, the third quantity was system energy, and the remaining quantity may correspond to one of the many first integrals of the KdV equation.\n\nFor the 2-pend dataset, the NODE with either cFINDE or dFINDE obtained improved VPTs with K = 1 to 5. In addition to the system energy, the double pendulum has two holonomic constraints on the position, which lead to two additional constraints involving the velocity (see Appendix C for details). Thus, it is reasonable that the NODE with either cFINDE or dFINDE obtained the best VPT for K = 5 first integrals and completely failed for K > 5 first integrals. As exemplified in Fig. 5, the NODE without FINDE did not preserve the lengths of rods, making the states deviate gradually. See Appendix D.3 for the case when actual constraints are known. For the FitzHugh–Nagumo dataset, the NODE with either cFINDE or dFINDE obtained improved VPTs for K = 2. As exemplified in Fig. 6, the ground truth state converged to a periodic orbit, and only the NODE with cFINDE for K = 2 reproduced similar dynamics. Without FINDE, the state did not remain in a limited region. For K = 1, the state converged to a wrong equilibrium; the sole quantity V1 may have attempted and failed to learn both first integrals. We conclude that both cFINDE and dFINDE found all first integrals of the 2-pend and FitzHugh–Nagumo datasets; K = 5 and K = 2, respectively.\n\n5 CONCLUSION\n\nThis study proposed first integral-preserving neural differential equation (FINDE), which can find and preserve any type of first integrals from data in a unified manner. FINDE projects the time evolution onto the submanifold defined using the (discrete) gradients of first integrals represented by a neural network. We experimentally demonstrated that FINDE found and preserved first integrals that come from the energy and mass conservation laws, symmetries in space, and constraints, thereby predicting the dynamics for far longer. FINDE is available even for an energy-dissipating system. When FINDE obtains the best prediction accuracy with K = K ′, it suggests that the target system has at least K ′ first integrals. Hence, FINDE has the potential to make scientific discoveries by revealing geometric structures of dynamical systems. See Appendix D.4 for more discussions on K.\n\nThe numerical error tolerance 10−9 was negligible compared to the 1-step errors (which were 10−5 to 10−4 in absolute error). However, the dFINDE tended to obtain much better VPTs than the cFINDE. This result suggests that a method leading to smaller numerical errors produces a model with smaller modeling errors, as observed in previous works (Chen et al., 2020; Matsubara et al., 2020). These results may form a new frontier for integrating numerical and modeling errors.\n\n9\n\nGroundTruth050NODE050+cFINDEK=2050+cFINDEK=3050TruthNODEK=2K=30104errorinstate020104errorintotalmass0.00.50104errorinenergy0510−10010GroundTruthNODE+cFINDE−505VCTruthNODEK=1K=2−202VL02000−101IC02000−505ILPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nSee Section 4.1 for experimental settings. More detailed descriptions can be found in Appendix B.3 for training procedure and Appendix C for datasets. The authors have enclosed the source code for generating the datasets and running the experiments as supplementary material.\n\nACKNOWLEDGEMENT\n\nThis study was partially supported by JST CREST (JPMJCR1914), JST PRESTO (JPMJPR21C7), and JSPS KAKENHI (19K20344, 20K11693).\n\nREFERENCES\n\nMart ́ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man ́e, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi ́egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems. USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2016.\n\nFerran Alet, Dylan Doblar, Allan Zhou, Joshua Tenenbaum, Kenji Kawaguchi, and Chelsea Finn. Noether Networks: Meta-Learning Useful Conserved Quantities. Advances in Neural Information Processing Systems (NeurIPS), (NeurIPS):1–20, 2021.\n\nDavid G. T. Barrett and Benoit Dherin. Implicit Gradient Regularization. In International Confer-\n\nence on Learning Representations (ICLR), 2021.\n\nAleksandar Botev, Andrew Jaegle, Peter Wirnsberger, Daniel Hennes, and Irina Higgins. Which priors matter? Benchmarking models for learning latent dynamics. In Advances in Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021.\n\nYuan Cao, Zhiying Fang, Yue Wu, Ding Xuan Zhou, and Quanquan Gu. Towards Understanding the Spectral Bias of Deep Learning. International Joint Conference on Artificial Intelligence (IJCAI), pp. 2205–2211, 2021.\n\nElena Celledoni, Andrea Leone, Davide Murari, and Brynjulf Owren. Learning Hamiltonians of\n\nConstrained Mechanical Systems. arXiv, pp. 1–18, 2022.\n\nS. Chen, S. A. Billings, and P. M. Grant. Non-linear system identification using neural networks.\n\nInternational Journal of Control, 51(6):1191–1214, 1990.\n\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud, Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Differential Equations. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1–19, 2018.\n\nYuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Neural Symplectic Form : Learning Hamiltonian Equations on General Coordinate Systems. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nZhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L ́eon Bottou. Symplectic Recurrent Neural Networks. In International Conference on Learning Representations (ICLR), pp. 1–23, 2020.\n\nSnorre H. Christiansen, Hans Z. Munthe-Kaas, and Brynjulf Owren. Topics in structure-preserving\n\ndiscretization. Acta Numerica, 20:1–119, 2011.\n\nD S Clouse, C L Giles, B G Horne, and G W Cottrell. Time-delay neural networks: Representation and induction of finite-state machines. IEEE Transactions on Neural Networks, 8(5):1065–70, January 1997.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKevin L. Course, Trefor W. Evans, and Prasanth B. Nair. Weak form generalized Hamiltonian learning. In Advances in Neural Information Processing Systems (NeurIPS), number NeurIPS, 2020.\n\nMiles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. Lagrangian Neural Networks. In ICLR Deep Differential Equations Workshop, pp. 1–9, March 2020.\n\nCharles Cuell and George W. Patrick. Geometric discrete analogues of tangent bundles and con-\n\nstrained Lagrangian systems. Journal of Geometry and Physics, 59(7):976–997, 2009.\n\nMorten Dahlby, Brynjulf Owren, and Takaharu Yaguchi. Preserving multiple first integrals by dis-\n\ncrete gradients. Journal of Physics A: Mathematical and Theoretical, 44(30), 2011.\n\nNima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu. Automatic Symmetry Discovery with Lie Algebra Convolutional Network. In Advances in Neural Information Processing Systems (NeurIPS), number 2018, pp. 1–30, 2021.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\n\nBidirectional Transformers for Language Understanding. arXiv, pp. 1–15, October 2018.\n\nJ. R. Dormand and P. J. Prince. A reconsideration of some embedded Runge-Kutta formulae. Journal\n\nof Computational and Applied Mathematics, 15(2):203–211, 1986.\n\nMarc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data. In International Conference on Machine Learning (ICML), pp. 3146–3157, 2020a.\n\nMarc Finzi, Ke Alexander Wang, and Andrew Gordon Wilson. Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints. In Advances in Neural Information Processing Systems (NeurIPS), 2020b.\n\nMarc Finzi, Max Welling, and Andrew Gordon Wilson. A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups. In International Conference on Machine Learning (ICML), 2021.\n\nK. Fukunaga and D.R. Olsen. An Algorithm for Finding Intrinsic Dimensionality of Data. IEEE\n\nTransactions on Computers, C-20(2):176–183, February 1971.\n\nDaisuke Furihata. A stable and conservative finite difference scheme for the Cahn-Hilliard equation.\n\nNumerische Mathematik, 87(4):675–699, February 2001.\n\nDaisuke Furihata and Takayasu Matsuo. Discrete Variational Derivative Method: A StructurePreserving Numerical Method for Partial Differential Equations. Chapman and Hall/CRC, December 2010.\n\nC. W. Gear. Maintaining Solution Invariants in the Numerical Solution of ODE s. SIAM Journal on\n\nScientific and Statistical Computing, 7(3):734–743, 1986.\n\nO. Gonzalez. Time integration and discrete Hamiltonian systems. Journal of Nonlinear Science, 6\n\n(5):449–467, September 1996.\n\nSam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian Neural Networks. In Advances\n\nin Neural Information Processing Systems (NeurIPS), pp. 1–16, 2019.\n\nErnst Hairer, Christian Lubich, and Gerhard Wanner. Geometric Numerical Integration: StructureSpringer-Verlag,\n\nPreserving Algorithms for Ordinary Differential Equations, volume 31. Berlin/Heidelberg, 2006.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1–9, December 2016.\n\nPhilipp Holl, Nils Thuerey, and Vladlen Koltun. Learning to Control PDEs with Differentiable\n\nPhysics. In International Conference on Learning Representations (ICLR), 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJialin Hong, Shuxing Zhai, and Jingjing Zhang. Discrete Gradient Approach to Stochastic Differential Equations with a Conserved Quantity. SIAM Journal on Numerical Analysis, 49(5): 2017–2038, January 2011.\n\nEugene M.\n\nand http://scholarpedia.org/article/FitzHugh-Nagumo model, 2006.\n\nIzhikevich\n\nFitzHugh.\n\nRichard\n\nFitzHugh-Nagumo model.\n\nPengzhan Jin, Zhen Zhang, Ioannis G. Kevrekidis, and George Em Karniadakis. Learning Poisson systems and trajectories of autonomous systems via Poisson neural networks. pp. 1–12, 2020a.\n\nPengzhan Jin, Aiqing Zhu, George Em Karniadakis, and Yifa Tang. Symplectic networks: Intrinsic structure-preserving networks for identifying Hamiltonian systems. Neural Networks, 132:166– 179, 2020b.\n\nMuhammad Firmansyah Kasim and Yi Heng Lim. Constants of motion network, August 2022.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International\n\nConference on Learning Representations (ICLR), pp. 1–15, December 2015.\n\nKookjin Lee and Kevin Carlberg. Deep Conservation: A latent-dynamics model for exact satisfaction of physical conservation laws. In AAAI Conference on Artificial Intelligence (AAAI), 2021.\n\nAsriel U. Levin and Kumpati S. Narendra. Recursive identification using feedforward neural net-\n\nworks. International Journal of Control, 61(3):533–547, 1995.\n\nZiming Liu and Max Tegmark. Machine Learning Conservation Laws from Trajectories. Physical\n\nReview Letters, 126(18):180604, May 2021.\n\nZichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-Net: Learning PDEs from Data. In\n\nInternational Conference on Machine Learning (ICML), pp. 3208–3216. PMLR, July 2018.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In Inter-\n\nnational Conference on Learning Representations (ICLR), pp. 1–16, 2017.\n\nGaurav Manek and J. Zico Kolter. Learning Stable Deep Dynamics Models. In Advances in Neural\n\nInformation Processing Systems (NeurIPS), pp. 1–9, 2019.\n\nTakashi Matsubara, Ai Ishikawa, and Takaharu Yaguchi. Deep Energy-Based Modeling of Discrete-\n\nTime Physics. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nRobert M. Miura, Clifford S. Gardner, and Martin D. Kruskal. Korteweg-de Vries equation and generalizations. II. Existence of conservation laws and constants of motion. Journal of Mathematical Physics, 9(8):1204–1209, 1968.\n\nKumpati S. Narendra and Kannan Parthasarathy. Identification and Control of Dynamical Systems\n\nUsing Neural Networks. IEEE Transactions on Neural Networks, 1(1):4–27, 1990.\n\nOliver Nelles. Nonlinear System Identification. Springer Berlin Heidelberg, Berlin, Heidelberg,\n\n2001.\n\nAdam Paszke, Gregory Chanan, Zeming Lin, Sam Gross, Edward Yang, Luca Antiga, and Zachary In Autodiff Workshop on Advances in Neural\n\nDevito. Automatic differentiation in PyTorch. Information Processing Systems, pp. 1–4, 2017.\n\nFlorian A. Potra and Jeng Yen.\n\nImplicit numerical integration for euler-lagrange equations via\n\ntangent space parametrization. Mechanics of Structures and Machines, 19(1):77–98, 1991.\n\nLionel Raff, Ranga Komanduri, Martin Hagan, and Satish Bukkapatnam. Neural Networks in Chem-\n\nical Reaction Dynamics. 2012.\n\nM. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nStephan Rasp, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. WeatherBench: A Benchmark Data Set for Data-Driven Weather Forecasting. Journal of Advances in Modeling Earth Systems, 12(11), 2020.\n\nAkiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano.\n\nGroup Invariant / Equivariant Deep Networks via Quotient Feature Spaces. Uncertainty in Artificial Intelligence (UAI), October 2021.\n\nImproved Generalization Bounds of In Conference on\n\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of GANs for semantic face editing. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9240–9249, 2020.\n\nJ. Sj ̈oberg, H. Hjalmarsson, and L. Ljung. Neural Networks in System Identification. IFAC Pro-\n\nceedings Volumes, 27(8):359–382, 1994.\n\nYifan Sun, Linan Zhang, and Hayden Schaeffer. NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data. In Mathematical and Scientific Machine Learning Conference, pp. 352–372. PMLR, August 2020.\n\nNaoya Takeishi and Yoshinobu Kawahara. Learning dynamics models with stable invariant sets. In\n\nAAAI Conference on Artificial Intelligence (AAAI), 2020.\n\nTakeshi Teshima, Koichi Tojo, Masahiro Ikeda, Isao Ishikawa, and Kenta Oono. Universal Approximation Property of Neural Ordinary Differential Equations. In NeurIPS Workshop on Differential Geometry Meets Deep Learning (DiffGeo4DL), 2020.\n\nRicardo M. Trigo and Jean P. Palutikof. Simulation of daily temperatures for climate change scenarios over Portugal: A neural network model approach. Climate Research, 13(1):45–59, 1999.\n\nArjan van der Schaft and Dimitri Jeltsema. Port-Hamiltonian Systems Theory: An Introductory\n\nOverview. Foundations and Trends® in Systems and Control, 1(2):173–378, 2014.\n\nP. R. Vlachas, J. Pathak, B. R. Hunt, T. P. Sapsis, M. Girvan, E. Ott, and P. Koumoutsakos. Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting of complex spatiotemporal dynamics. Neural Networks, 126:191–217, 2020.\n\nYi Jen Wang and Chin Teng Lin. Runge-Kutta neural network for identification of dynamical sys-\n\ntems in high accuracy. IEEE Transactions on Neural Networks, 9(2):294–307, 1998.\n\nShuqi Yang, Xingzhe He, and Bo Zhu. Learning Physical Constraints with Neural Projections.\n\nAdvances in Neural Information Processing Systems, pp. 1–15, 2020.\n\nGe Zhong and Jerrold E Marsden. Lie-Poisson Hamilton-Jacobi Theory and Lie-Poisson Integrators.\n\nPhysics Letters A, 133(3):3–8, November 1988.\n\nYaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control. In International Conference on Learning Representations (ICLR), pp. 1–17, 2020a.\n\nYaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Dissipative SymODEN: Encoding Hamiltonian Dynamics with Dissipation and Control into Deep Learning. arXiv, pp. 1–6, 2020b.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA HAMILTONIAN SYSTEM, ITS GENERALIZATION, AND FIRST INTEGRALS\n\nPreliminary In this section, we briefly introduce potential target systems and related works. Methods proposed by related works use specific prior knowledge about target systems, such as constraints. In contrast, our proposed FINDE assumes a situation where neural networks learn systems with unknown properties. See, for example, Hairer et al. (2006); van der Schaft & Jeltsema (2014) for more details about geometric mechanics.\n\nOn an N -dimensional manifold M, an ODE is defined using a vector field f : M → TuM, which maps a point u on the manifold M to a tangent vector f (u) on the tangent space TuM. The NODE defines an ODE in this way (Chen et al., 2018). Given a scalar-valued function H : M → R on the manifold M, its differential dH : M → T ∗ u M is a cotangent vector field (a.k.a. a differential 1-form), which maps a point u on the manifold M to a cotangent vector dH(u) on the cotangent space T ∗\n\nu M.\n\nHamiltoanian System A Hamiltonian system is defined using a non-degenerate closed differential 2-form ω called symplectic form, which is a skew-symmetric bilinear map ωu : TuM × TuM → R at point u. A symplectic form assigned to a manifold is called the symplectic structure. The coordinate-free form of Hamilton’s equation is d dt u = XH (u), ωu(XH (u), w) = ⟨dH(u), w⟩ for any w ∈ TuM, where XH is the Hamiltonian vector field. The symplectic form ω gives rise to a bundle map ω♭ dt u = u)−1(dH(u)). The right-hand side is locally equivalent to the product of a coefficient XH (u) = (ω♭ matrix S and the gradient ∇H of the Hamiltonian H. Then, Hamilton’s equation is obtained as d\ndt u = S∇H(u). Hamiltonian systems are often expressed in the canonical form, in other words, they are defined on Darboux coordinates, on which the state u is the paired generalized position q (cid:1) for 2n = N and generalized momentum p. The corresponding coefficient matrix is S = (cid:0) 0 and the n-dimensional identity matrix In. The HNN was developed to model Hamiltonian systems in the canonical forms (Greydanus et al., 2019).\n\nu M, with which Hamilton’s equation is rewritten as d\n\nu : TuM → T ∗\n\nIn −In 0\n\nAn Euler–Lagrange equation with a hyperregular Lagrangian and a Lotka–Volterra equation are also Hamiltonian systems; however, their coordinate systems are not Darboux coordinates. A neural symplectic form (NSF) handles this class of equations (Chen et al., 2021). The KdV equation is also a Hamiltonian system not on Darboux coordinates. For Hamiltonian PDE systems, HNN++ was proposed (Matsubara et al., 2020). According to Darboux’s theorem, any Hamiltonian system on an even–dimensional manifold can be transformed into the canonical form.\n\nNoether’s theorem states that a continuous symmetry of a system leads to a conservation law. A Hamiltonian system is symmetric (invariant) to translation in time and conserves the Hamiltonian H. A two-body problem is symmetric to translation and rotation in space and conserves linear and angular momenta. These quantities are first integrals. LieConv and EMLP-HNN had such symmetries implemented in their architectures (Finzi et al., 2020a; 2021). A pendulum is not symmetric to translation and rotation in space and does not conserve linear and angular momenta, but does exchange them with the base to which it is fixed.\n\nPoisson System A Poisson system is named after a Poisson bracket {·, ·}, but it is convenient to refer to it as a degenerate Hamiltonian system. A Poisson bracket is defined using a Poisson 2-vector u M → R at point u. The Poisson B, which is a skew-symmetric bilinear map Bu : T ∗ 2-vector B gives rise to a bundle map B♯ u M → TuM and defines Hamilton’s equation as d\ndt u = B♯(dH(u)). The Darboux–Lie theorem states that any Poisson system can be transformed into the canonical form d for 2k < N . The last\n\ndt u = S∇H(u) by using a matrix S =\n\nu M × T ∗\n\nu : T ∗\n\n(cid:17)\n\n(cid:16) 0 Ik 0 −Ik 0 0 0 0\n\n0\n\nN − 2k elements remain unchanged and correspond to the first integrals. In this sense, a Poisson system is a degenerate Hamiltonian system. A Poisson 2-vector assigned to a manifold is called a Poisson structure. Several models of the dynamics of disease spreading and chemical reactions are Poisson systems, and total population and molecular mass are typical first integrals.\n\nA Poisson neural network (PNN) learns to transform a given Poisson system into a canonical form (Jin et al., 2020a).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nConstrained Hamiltonian System A constraint C(q) = 0 on the position q is called a holonomic constraint. Holonomic constraint appear, for example, when the arm’s length restricts the position of a robot’s hand. Differentiating a holonomic constraint C(q) = 0 yields a constraint involving the velocity G(q, v) = ∂C ∂q v = 0, which is simply called a velocity constraint. Hence, each holonomic constraint leads to two first integrals C and G. A Hamiltonian system with holonomic constraints is also a Poisson system; in particular, it is a constrained Hamiltonian system.\n\nA CHNN incorporates the known holonomic constraints C(q) and corresponding velocity constraints G(q, v) of a Hamiltonian system in the canonical form (Finzi et al., 2020b). The original study suggested that CHNN may learn holonomic constraints from data, but this has not been tested. For modeling a constrained Hamiltonian system, it is sufficient to incorporate only velocity constraints G(q, v) because a holonomic constraint C(q) is implicitly satisfied if the corresponding velocity constraint G(q, v) is satisfied. Celledoni et al. (2022) used such formulation, and extended HNN and CHNN to systems on non-Euclidean spaces. A neural projection method learns fixed holonomic constraints, as well as inequality constraints, which are outside the scope of this study (Yang et al., 2020). This method updates the state by solving an optimization problem similar to Eq. (3) iteratively using the gradient descent method at every training step. Subsequently, it applies the backpropagation algorithm to all the optimization iterations. Thus, it has high computational and memory costs.\n\nThese studies mainly focused on physically-induced holonomic constraints and may not work for other first integrals, as shown in Appendices D.3 and D.5. However, the purpose of FINDE is to find and preserve general first integrals, including energy and mass not limited by constraints.\n\nDirac Structure A Dirac structure is named after a Dirac bracket, a generalization of the Poisson bracket (van der Schaft & Jeltsema, 2014), and can be found in various systems. For a rolling disk, the direction in which the disk can move forward without slipping is limited by the disk’s orientation. This constraint is called a non-holonomic constraint. In an electric circuit, when elements are connected in series, the current flow through each element is always the same. This constraint is called Kirchhoff’s current law. One can find Dirac structures in these systems. The dissipative SymODEN was proposed to model a port-Hamiltonian system in the canonical form (Zhong et al., 2020b), which is a special case of the Dirac structure. To the best of our knowledge, a neural network model for a general Dirac structure has not yet been proposed. FINDE is the first neural network method to learn Dirac structures better than NODE can, even though it is not specialized for Dirac structures.\n\nPDE with Mass Conservation The total mass of a PDE system is sometimes preserved (Furihata & Matsuo, 2010). The KdV equation is a Hamiltonian system that describes shallow water waves, in which the energy and total mass are preserved. The Cahn–Hilliard equation is a model of phase separation of copolymer melts, in which the total mass is preserved, but the energy is dissipated. In general, a quantity in an area is preserved if its flux entering minus its flux leaving is zero. Deep conservation extracts latent dynamics of a PDE system and preserves a quantity of interest by forcing its flux to be zero (Lee & Carlberg, 2021). HNN++ also ensures mass conservation by designing a coefficient matrix that determines local interaction (Matsubara et al., 2020).\n\nGeneral First Intergals A concurrent study, “Constants-of-motion network,” introduced the penalty loss function so that NODEs learn to preserve first integrals (Kasim & Lim, 2022); however, unlike other related methods, this method does not guarantee preservation. A Noether network was proposed to model videos that do not always capture physical phenomena (Alet et al., 2021). A subset of the latent variable is assumed to represent image features that do not change during a video, such as the appearance of objects. For prediction, these features are forced not to change. The Noether network is potentially useful for learning physical phenomena from videos, but is more similar to semantic manipulation of latent variables (Shen et al., 2020).\n\nSome studies have investigated methods that do not predict dynamics but specialize in finding first integrals (Fukunaga & Olsen, 1971; Liu & Tegmark, 2021). These methods can be used to help FINDE determine the hyperparameter K. They commonly estimate the number (N − K) of dimensions of the tangent space TuM′ of the submanifold M ′ at point u using its neighbors. For example, AI Poincar ́e proposed by Liu & Tegmark (2021) assumes that all data points share the submanifold M′ and uses an autoencoder to reconstruct the tangent space TuM′. Hence, it can only process a\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nsingle long time series with fixed first integrals. In contrast, our proposed FINDE can leverage a dataset of multiple time series with different values of the first integrals.\n\nB DETAILS OF METHODS\n\nB.1 DERIVATION OF FINDE\n\nContinuous FINDE (cFINDE) Let us denote a current state and ˆf denote a vector field. After a time interval ∆t, the state transitions to ˆus+1. A typical projection method projects the state ̃us+1 onto a submanifold M′ and obtains a state us+1, which preserves the first integrals V = (V1 . . . VK)⊤. This procedure is defined as an optimization problem in Eq. (3);\n\narg min us+1\n\n∥us+1 − ̃us+1∥ subject to Vk(us+1) − Vk(us) = 0 for k = 1, . . . , K.\n\n(A1)\n\nOne can solve the problem using the method of Lagrange multipliers. A Lagrangian function is\n\nF (us+1, λ) = 1\n\n2 ||us+1 − ̃us+1||2\n\n2 + (V (us+1) − V (us))⊤λ′,\n\nwhere λ′ is the Lagrange multiplier. The stationary point satisfies\n\n∂F\n\n∂us+1 = us+1 − ̃us+1 + (cid:0) ∂V\n\n∂us+1\n\n(cid:1)⊤\n\nλ′ = 0,\n\nSubsequently, a projection method can be redefined as\n\n∂F\n\n∂λ′ = V (us+1) − V (us) = 0.\n\nus+1 = ̃us+1 − (cid:0) ∂V\n\n∂us+1\n\n(cid:1)⊤\n\nλ′,\n\nV (us+1) − V (us) = 0.\n\nWe transform Eq. (A4) into\n\nus+1−us ∆t\n\n= ̃us+1−us\n\n∆t − (cid:0) ∂V\n\n∂us+1\n\n(cid:1)⊤\n\nλ,\n\nwhere λ = λ′/∆t. Taking the limit as ∆t → +0, we obtain Eq. (4);\n\nV (us+1)−V (us) ∆t\n\n= 0,\n\nf (us) = ˆf (us) − (cid:0) ∂V\n\n∂us\n\n(cid:1)⊤\n\nλ,\n\nd\n\ndt V (us) = 0.\n\n(A2)\n\n(A3)\n\n(A4)\n\n(A5)\n\n(A6)\n\nThe second equation ensures that a state transition following the new vector field f preserves the first integrals V . By eliminating the Lagrange multiplier λ(u), we define the cFINDE as in Eq. (6), that is,\n\nd\n\ndt u = f (u) = (I − Y (u)) ˆf (u) for Y (u) = M (u)⊤(M (u)M (u)⊤)−1M (u),\n\n(A7)\n\nwhere M = ∂V version of a projection method. The preservation of first integrals can be proved as follows.\n\n∂u . Because of the above derivation, the cFINDE can be considered a continuous-time\n\nProof of Theorem 1.\n\nd\n\ndt V (u) = ∂V\n\n∂u\n\nd\n\ndt u\n\n= M (u)f (u) = M (u)(I − M (u)⊤(M (u)M (u)⊤)−1M (u)) ˆf (u) = (M (u) − (M (u)M (u)⊤)(M (u)M (u)⊤)−1M (u)) ˆf (u) = (M (u) − M (u)) ˆf (u) = 0.\n\nHence, it holds that d preserves all first integrals Vk in continuous time.\n\ndt Vk(u) = 0 for k = 1, . . . , K, indicating that the cFINDE d\n\ndt u = f (u)\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nDiscrete FINDE (dFINDE) For dFINDE, we take the discrete gradient of the Lagrangian equation in Eq. (A2) and obtain the discrete version of the necessary conditions for the stationary point;\n\n∇(us+1,us)F = us+1 − ̃us+1 + M (us+1, us)λ′ = 0,\n\n∂F\n\n∂λ′ = V (us+1) − V (us) = 0.\n\n(A8)\n\nM (us+1, us) corresponds to the Jacobian ∂V ˆψ(us; ∆ts) and the dFINDE us+1−us the first equation by ∆t, we obtain Eq. (8);\n\n∆ts\n\n∂u . By substituting the base model ̃us+1−us =\n= ψ(us+1, us; ∆ts) into the above equation and dividing\n\n∆ts\n\nus+1−us\n\n∆ts = ψ(us+1, us; ∆ts),\n\nψ(us+1, us; ∆ts) = ˆψ(us; ∆ts) − M (us+1, us)⊤λ(us+1, us),\n\n(A9)\n\nV (us+1) − V (us) = 0,\n\nwhere λ = λ′/∆ts. By eliminating the Lagrange multiplier λ, we define the dFINDE as in Eq. (10), that is,\n\nus+1−us\n\n∆ts = ψ(us+1, us; ∆ts) = (I − Y (us+1, us)) ˆψ(us; ∆ts) for Y = M\n\n⊤\n\n(M M\n\n⊤\n\n)−1M .\n\n(A10)\n\nThe preservation of first integrals can be proved as follows.\n\nProof of Theorem 2.\n\nV (us+1) − V (us) = M (us+1, us)(us+1 − us)\n\n= M (us+1, us)ψ(us+1, us; ∆ts)∆ts\n\n= M (I − M\n\n⊤\n\n(M M\n\n⊤\n\n)−1M ) ˆψ(us+1, us; ∆ts)∆ts\n\n⊤\n\n⊤\n\n)−1M ) ˆψ(us+1, us; ∆ts)∆ts\n\n)(M M\n\n= (M − (M M = (M − M ) ˆψ(us+1, us; ∆ts)∆ts = 0.\n\nHence, it holds that Vk(us+1) = Vk(us) for k = 1, . . . , K, indicating that the dFINDE us+1−us ψ(us+1, us; ∆ts) preserves all first integrals Vk in discrete time.\n\n∆ts =\n\nB.2 DISCRETE GRADIENT\n\nA discrete gradient is a discrete analogue to a gradient (Furihata & Matsuo, 2010; Gonzalez, 1996; Hong et al., 2011). Discrete gradients that satisfy Definition 2 are not unique, and many variations have been proposed. For a neural network, Matsubara et al. (2020) proposed the automatic discrete differentiation algorithm (ADDA). We briefly introduce the algorithm in the case of finitedimensional Euclidean spaces. The differential dg of a function g : RN → RM is a linear operator dgu : RN → RM at point u and satisfies\n\nlim ||h||RN →0\n\n||g(u + h) − g(u) + dgu(h)||RM ||h||RN\n\n= 0.\n\n(A11)\n\nThe differential dg acting on a vector w is equivalent to the product of a vector w with the Jacobian Jg(u) of the function g at point u: dgu(w) = Jg(u)w. Similarly, according to the chain rule, the differential d(h ◦ g) of a composition h ◦ g of functions g, h is equivalent to the multiplication with a series Jh(g(u))Jg(u) of Jacobians. Therefore, the automatic differentiation algorithm obtains the differential of a neural network. The differential dg of a function g : RN → R is a horizontal vector, and the gradient ∇g of the function g is a vertical vector dual to the differential. Therefore, the gradient ∇g is obtained by transposing the differential dg. The ADDA replaces each Jacobian with its discrete analogue. For linear layers, such as fully-connected and convolution layers, the discrete Jacobian is identical to the ordinary Jacobian. For element-wise nonlinear layers, such as activation functions, a diagonal matrix composed of the slopes between two inputs can act as the discrete Jacobian. A discrete gradient obtained by the above steps satisfies Definition 2.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nB.3 PREDICTION AND TRAINING PROCEDURES\n\nFor ODEs modeled by neural networks, various training and prediction strategies have been proposed to date (Chen et al., 2018; 2020; Course et al., 2020; Matsubara et al., 2020; Zhong et al., 2020a); FINDE can adopt any of these. In our experiments, we used the following simple strategies.\n\nIn the case of the cFINDE and base models, taking a state us grator solves the ODE d expressed as\n\ndt u = f (u) and predicts the next state us+1\n\nGT from the dataset, a numerical intepred.. This process can be informally\n\nus+1\n\npred. ≃ us\n\nGT +\n\n(cid:90) ts+∆ts\n\nts\n\nf (u(τ ))dτ for u(ts).\n\n(A12)\n\nWe solved this integration using torchdiffeq.odeint. The prediction accuracy can be evaluated using the difference between the predicted state us+1 GT taken from the dataset. We normalized the difference by the time-step size ∆ts and defined the 1-step error L1-step as\n\npred. and ground truth us+1\n\nL1-step(us+1\n\npred.; us+1\n\nGT , us\n\nGT, ∆ts) =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nus+1\n\nGT − us ∆ts\n\nGT\n\n−\n\nus+1\n\npred. − us ∆ts\n\nGT\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\n.\n\n(A13)\n\nThe cFINDE and base models were trained to minimize the 1-step error L1-step.\n\nIn the case of the dFINDE, the next state us+1 in particular,\n\npred. is predicted by solving Eq. (10) as an implicit scheme;\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\narg min us+1\n\npred.\n\nus+1\n\npred. − us ∆ts\n\nGT\n\n− (I − Y (us+1\n\npred., us\n\nGT)) ˆψ(us\n\nGT; ∆ts)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\n(A14)\n\nTherefore, prediction by the dFINDE is implicit. For evaluation, we solved this scheme using scipy. optimize.fsolve and obtained the 1-step error in Eq. (A13). However, during the training phase, the ground truth us+1 GT of the next state is known. Hence, we substituted this into Eq. (10), and then used the difference between the left- and right-hand sides of the dFINDE as the loss function:\n\nLdFINDE(us+1\n\nGT , us\n\nGT, ∆ts) =\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nus+1\n\nGT − us ∆ts\n\nGT\n\n− (I − Y (us+1\n\nGT , us\n\nGT)) ˆψ(us\n\nGT; ∆ts)\n\n2 (cid:13) (cid:13) (cid:13) (cid:13) 2\n\n.\n\n(A15)\n\nThe discrete Jacobian M (and hence Y ) can be obtained explicitly, and an explicit numerical integrator can be used for the base model ˆψ. Hence, the process to obtain the value of the loss function is explicit, and the dFINDE can be trained in an explicit way, whereas the prediction is still implicit.\n\nSome previous studies have proposed alternative strategies. For example, a loss function can be defined as the sum of the errors at multiple time points during a long-term prediction. The cFINDE can naturally adopt such a training strategy, and the dFINDE can adopt it after a minor modification. While it is helpful to pursue absolute performance, it requires additional hyperparameters, such as the length of prediction time, and additional effort to adjust them. We used the 1-step error in the present study for simplicity and fair comparisons.\n\nThe function V (u) learning a first integral may become a constant function during training; subsequently, its Jacobian matrix vanishes ( ∂V (u) ∂u ≡ 0). In this case, our algorithm returns a division-byzero error because it requires the inverse of the matrix ∂V (u) for the projection. We have not taken any special measures to prevent such errors, but no errors occurred in any experiments with proper settings. The division-by-zero errors have occurred only when FINDE assumes an unreasonable number of first integrals (e.g., K = 6 for the double pendulum, which has five first integrals). FINDE works correctly even when the functions f (u) and V (u) learn the same first integrals; we verified such a case in Section 4.2, where both functions are known.\n\n∂V (u) ∂u\n\n∂u\n\n⊤\n\nFINDE learns first integrals point-by-point, and the found first integral is not always consistent over the domain. The same can be said about the energy function of HNN, and this type of problem is an open problem for neural network models of dynamical systems.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nC DETAILS OF DATASETS\n\nTo generate each dataset, we used scipy package and the Dormand–Prince method (dopri5) with the default relative tolerance of 10−9, unless otherwise stated. Experiments on the KdV dataset were performed with double precision, and all other experiments were performed with single precision.\n\nHamiltonian System in Canonical Form: Two-Body Problem A gravitational two-body problem on a 2-dimensional configuration space has a state u composed of the 4-dimensional position q = (x1 y1 x2 y2)⊤ and 4-dimensional velocity v = (vx1 vy1 vx2 vy2)⊤. This is a secondorder ODE, indicating that d dt q = v. The momentum px1 of x1 equals m1vx1. The timederivative d dt v of the velocity v is called the acceleration. The acceleration of x1 is given by dt vx1 = −Gm1m2 ((x1−x2)2+(y1−y2)2)3/2 , where G, m1, and m2 denote the constant of gravity and masses of two bodies, respectively. The same process applies for the remaining positions.\n\nx1−x2\n\nd\n\nThe total energy of the two-body problem is given by\n\nH =\n\n1 2\n\n(m1(v2\n\nx1 + v2\n\ny1) + m2(v2\n\nx2 + v2\n\ny2)) −\n\nGm1m2 (cid:112)(x1 − x2)2 + (y1 − y2)2\n\n.\n\n(A16)\n\nThe first and second terms denote the kinetic and potential energies, respectively. The two-body problem is a Hamiltonian system, and the dynamics mentioned above can be rewritten as Hamilton’s equation. The Hamiltonian H is one of the first integrals; the two-body problem has other first integrals, such as the linear momenta in the x- and y-directions\n\npx =\n\nm1vx1 + m2vx2 m1 + m2\n\n, py =\n\nm1vy1 + m2vy2 m1 + m2\n\n,\n\n(A17)\n\nand angular momentum (Hairer et al., 2006). We set G, m1, and m2 to 1.0. The initial distance r1 = (cid:112)x2 was set to r1 ∼ U(0.5, 1.0), and the initial angle θ1 = tan−1( y1\n\n1 + y2\n\n1 of a mass m1 from the origin ) was set to θ1 ∼ U(0, 2π). The\n\nx1\n\n(cid:113)\n\nv2 x1\n\n+ v2\n\ny1 was set to 1\n\n2r2 εv, where εv ∼ N (1, 0.05). The initial angle of the initial speed |v1| = velocity was set to θ ± 0.5π + εθπ, where εθ ∼ N (0, 0.05). The initial condition of the other mass m2 was set to the opposite of the mass m1. Subsequently, the two masses trace elliptical orbits, and when εv = εθ = 0, they trace exactly circular orbits. In addition, we added a perturbation following N (0, 0.01) to the velocities of both masses, which corresponds to the center-of-gravity velocity.\n\nWe set the time-step size ∆t to 0.01 and generated 1,000 time-series of S = 500 steps for training and 10 time-series of S = 10, 000 steps for evaluation. We trained each model for 100,000 iterations.\n\nHamiltonian System in Non-Canonical Form: KdV equation The KdV equation is a model of shallow water waves and is known to have soliton solutions (Furihata, 2001). The dynamics is given by\n\nut = −αuux + βuxxx, where x denotes the spatial position and the subscripts denote partial derivatives; for example, ut = ∂u ∂t . The Hamiltonian is given by\n\n(A18)\n\nH(u) =\n\n(cid:90)\n\n−\n\n1 6\n\nαu3 −\n\n1 2\n\nβu2\n\nx dx.\n\n(A19)\n\ndt u = S∇H, the partial differential operator ∂ As Hamilton’s equation d ∂x acts as the coefficient matrix S. This system is Liouville integrable and has infinitely many first integrals, including the Hamiltonian H, total mass I1 = (cid:82) udx, and T2 = (cid:82) u2dx (Miura et al., 1968). Other first integrals are defined using higher-order partial derivatives.\n\nFor PDEs, PINNs are known to provide solutions when symbolic equations and boundary conditions are given (Raissi et al., 2019). We, in contrast, consider learning spatially discretized PDEs as ODEs from observed data and solving them using numerical integrators, in the same context as NODEs and HNNs; this topic has also been studied extensively (Long et al., 2018; Matsubara et al., 2020; Sun et al., 2020; Holl et al., 2020). Following the experiments in a previous study (Matsubara et al.,\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n2020), we discretized the KdV equation in space; it no longer has infinitely many first integrals. We set α = −6, β = 1, spatial size to 10 space units, and space mesh size to 0.2; the system state u had 50 elements. We generated two solitons as the initial condition; each was expressed as α κ2sech2(κ(x − d)), where the size κ followed U(0.5, 2) and the initial position d of one soliton − 12 was set to be at least 2.0 from that of the other.\n\nWe set the time-step size ∆t to 0.001 and generated 1,000 time-series of S = 500 steps for training and 10 time-series of S = 10, 000 steps for evaluation, using the discrete gradient method to ensure energy conservation (Furihata, 2001). We trained each model for 30,000 iterations.\n\nDue to the spatial discretization, the KdV dataset contains spatial truncation errors. When the neural network learns this dataset, no spatial truncation errors are additionally introduced. An evaluation using the analytical solution as a dataset or datasets created with different spatial resolutions is included in future work.\n\nPoisson System: Double Pendulum A double pendulum (2-pend) is depicted in Fig. A1. In polar coordinates, this is a Hamiltonian system. The state is composed of the angles (θ1, θ2) of the two rods and their angular velocities (ω1, ω2). This is also a second-order ODE, indicating that d\ndt θ1 = ω1 and d Let l1, l2 denote the lengths of the two rods, m1, m2 denote the masses of the two weights, and g denote the gravitational acceleration. The acceleration is given by\n\ndt θ2 = ω2.\n\nd dt\n\nd dt\n\nω1 =\n\nω2 =\n\nm2g sin θ2 cos ∆ − (l1ω2\n\n1 cos ∆ + l2ω2 l1(m1 + m2 sin2 ∆)\n\n2)m2 sin ∆ − (m1 + m2)g sin θ1\n\n,\n\n(m1 + m2)(l1ω2\n\n1 sin ∆ − g sin θ2 + g sin θ1 cos ∆) + m2l2ω2\n\n2 sin ∆ cos ∆\n\nl2(m1 + m2 sin2 ∆)\n\n(A20)\n\n,\n\nwhere ∆ = θ1 − θ2. In 2-dimensional Cartesian coordinates, the state is composed of the positions (x1, y1, x2, y2) of the two masses and the corresponding velocities (vx1, vy1, vx2, vy2). The position is transformed by x1 = l1 sin θ1, y1 = l1 cos θ1, x2 = x1 + l2 sin θ2, and y2 = y1 + l2 cos θ2, and the velocity is transformed accordingly. The total energy H is given by\n\nH =\n\n1 2\n\n(m1(v2 x1\n\n+ v2 y1\n\n) + m2(v2 x2\n\n+ v2 y2\n\n)) + g(m1y1 + m2y2).\n\n(A21)\n\nThe first and second terms denote the kinetic and potential energies, respectively. The double pendulum is no longer a Hamiltonian system in Cartesian coordinates. Because the lengths of the two rods are constant, the double pendulum has two constraints on the position: l2 1 and l2 2 = (x2 − x1)2 + (y2 − y1)2. These constraints are holonomic constraints, and they lead to constraints involving the velocity, namely 0 = x1vx1 + y1vy1 and 0 = (x2 − x1)(vx2 − vx1) + (y2 − y1)(vy2 − vy1 ). When the constraints involving the velocity are satisfied, the holonomic constraints are implicitly satisfied. Therefore, the number of first integrals is five; however, three first integrals are sufficient to determine the dynamics. The dynamics is degenerate and classified as a constrained Hamiltonian system, or a Poisson system in a more general case.\n\n1 = x2\n\n1 + y2\n\nFigure A1: Diagram of the double pendulum.\n\nWe set the masses of the two weights to m1 = m2 = 1.0 and the gravitational acceleration g to 9.8. We set the lengths l1, l2 of the two rods to follow U(0.9, 1.1), the initial angles θ1, θ2 to follow U(−0.5, 0.5), and the initial angular velocities ̇θ1, ̇θ2 to follow U(−0.1, 0.1).\n\nWe set the time-step size ∆t to 0.1 and generated 1,000 time-series of S = 500 steps for training and 10 time-series of S = 5, 000 steps for evaluation. We trained each model for 100,000 iterations.\n\nDirac Structure: FitzHugh–Nagumo Model R. FitzHugh proposed a model of the electrical dynamics of a biological neuron, and J. Nagumo created an equivalent electric circuit. This model is called the FitzHugh–Nagumo model (Izhikevich & FitzHugh, 2006) and is a modified version of the van der Pol oscillator; the state oscillates when the magnitude of the external current source I is within an appropriate range. The circuit comprises a resistor R, inductor L, capacitor C, tunnel diode D, and voltage source E connected as shown in Fig. A2. The whole circuit is connected to\n\n20\n\nm2θ2θ1l1l2m1xy⇓gPublished as a conference paper at ICLR 2023\n\nan external current source I. Let IR denote the current through the resistor R, and VR denote the applied voltage. Ohm’s law and other properties of the elements give VR = IRR, C d dt VC = IC, L d dt IL = VL, and ID = D(VD), where we treat D as a nonlinear function. Kirchhoff’s current law (KCL) obtains IC + ID + IR = I and IR = IL, and Kirchhoff’s voltage law (KVL) obtains VC = VD = VR + VL + E. We denote W = IR and V = VC, and set L = 1/0.08, R = 0.8, C = 1.0, VE = −0.7, and D(V ) = V 3/3 − V . Subsequently, we obtain the FitzHugh–Nagumo model of the original parameters as\n\nd dt d\ndt\n\nV = V − V 3/3 − W + I,\n\n(A22)\n\nW = 0.08(V + 0.7 − 0.8W ).\n\nDue to the resistor R, the FitzHugh–Nagumo model is not an energy-conserving system.\n\nConsider a situation where the current through and the voltage applied to stateful elements (capacitors and inductors) are measurable, but the connections between the elements are unknown. We treated IC, IL, VC, VL as the system state u. Because the state is in 4dimensional space and the dynamics is intrinsically 2-dimensional, there exist two first integrals; for example, but not limited to, I = IC + D(VC) + IL and E = VC − ILR − VL. This type of electric circuit is an example of a Dirac structure because the state variables are constrained by the circuit topology and Kirchhoff’s current and voltage laws (van der Schaft & Jeltsema, 2014). From the viewpoint of generalized Hamiltonian systems, (IL, VC) corresponds to the position, and (VL, IC) corresponds to the momentum. The electric circuit can be described as a port-Hamiltonian system in a non-canonical form. Because of the non-canonical form, the FitzHugh–Nagumo model is outside the scope of CHNN and dissipative SymODEN (Finzi et al., 2020b; Zhong et al., 2020b).\n\nFigure A2: Circuit diagram of FitzHugh–Nagumo model &\nFitzHugh, 2006).\n\n(Izhikevich\n\nWe set the external current source I to follow U(0.7, 1.1), set the initial values of V and W to follow U(−1.5, 1.5) and U(0.0, 2.0), and transformed them to the state.\n\nWe set the time-step size ∆t to 0.1 and generated 1,000 time-series of S = 500 steps for training and 10 time-series of S = 2, 000 steps for evaluation. We trained each model for 30,000 iterations.\n\nD ADDITIONAL RESULTS AND DISCUSSION\n\nD.1 DEMONSTRATION OF FIRST INTEGRAL PRESERVATION\n\nIn Fig. 1, we examined a mass-spring system and FINDE using the leapfrog integrator. We also examined the case with the Dormand– Prince integrator (dopri5), as shown in Fig. A3. We increased the number of steps to 105, and displayed the MSEs of the state instead of the state itself. First, we focus on the energy. Even using the Dormand–Prince integrator, a fourth-order method, the energy is slightly decreased. The cFINDE with the Dormand–Prince integrator shows the same tendency. This phenomenon is due to temporal discretization errors and is called energy drift. The dFINDE with the Dormand–Prince integrator significantly suppresses the error in energy. The remaining error is caused by rounding errors.\n\nWhen the focus is on the MSEs of the state, the trend is different: the dFINDE with the Dormand–Prince integrator suffers from the most significant errors in state. Although the dFINDE is designed to eliminate temporal discretization errors in energy, it does not necessarily reduce those in state. In contrast, the Dormand–Prince integrator is designed to suppress temporal discretization errors in state.\n\nFigure A3: Integration of a known mass-spring system by Dormand–Prince integrator. (top) Mean squared errors in states predicted by comparison methods. (bottom) Energy calculated from the states predicted.\n\n21\n\nCRLDIVE010−6stateMSE0105steps0.4990.500energyanalyticalcFINDEdopri5dFINDEPublished as a conference paper at ICLR 2023\n\nTherefore, there is no guarantee that the dFINDE improves the prediction performance when defined using errors in state. Conversely, the experimental results in Table 3 demonstrate that the dFINDE is superior to the base model and cFINDE in VPT. This is because dFINDE reduces the modeling errors rather. For the mass-spring system, the governing equation is already known as an ODE and is discretized by the dFINDE, leading to temporal discretization errors. However, when dFINDE learns dynamics from data, the training data points are already sampled in discrete time, and the dFINDE predicts future states in discrete time. Therefore, no temporal discretization error occurs, and we obtain only the advantages of exactly preserving the first integral.\n\nThis type of paradox has been repeatedly discovered in previous studies. For example, the leapfrog integrator and discrete gradient method are second-order methods. However, they are superior to the Dormand–Prince integrator when combined with neural networks and learning dynamics from data (Matsubara et al., 2020). For better learning (i.e., smaller modeling errors), the preservation of specific properties of target systems is more important than the order of accuracy.\n\nD.2 SYMBOLIC REGRESSION OF FOUND FIRST INTEGRALS\n\nUsing gplearn (based on genetic programming), we performed a symbolic regression of the first integrals V found by the neural network. We prepared addition, subtraction, multiplication, and division as candidate operations, used Pearson’s correlation coefficient as the evaluation criterion, set the early stopping threshold to 0.9, and set the population size to 10,000. We set the other hyperparameters to their default values, e.g., the maximum number of generations was 20.\n\nWe summarize the regression results of the HNN with cFINDE for K = 2 trained using the twobody dataset in Table A1. Note that Pearson’s correlation coefficient is invariant to biases and scale factors. FINDE is also invariant because it only uses the directions of the gradients of first integrals. Hence, we removed biases and scale factors from the regression results. When the focus is on the symbolic regression of the training data, V1, V1, V2, and V2 for trials 0, 1, 2, and 3 are identical to the linear momentum in the x-direction up to scale factors; recall that we set m1 = m2 = 1.0 and see Eq. (A17). V2, V2, V1, and V1 for trials 0, 1, 2, and 3 are also identical to the linear momentum in the y-direction. V1 and V2 for trial 4 are weighted sums of the linear momenta in the x- and y-directions; in particular, they can be regarded as the linear momenta in the (1, −1)- and (1, 1)- directions, respectively.\n\nWhen the quantities V1(u) and V2(u) are first integrals, any function of only V1(u), V2(u), and arbitrary constants is a first integral functionally dependent on V1(u) and V2(u). Thus, it is in principle impossible to re-discover a first integral as a well-known symbolic expression, and a failure in symbolic regression is not a problem in any way. Previous studies introduced certain constraints (such as “gauge fixing”) for symbolic regression (Liu & Tegmark, 2021); a combination of such method may improve the results. However, recent studies on neural networks have revealed that typical initialization and training procedures tend to learn simple functions (Barrett & Dherin, 2021; Cao et al., 2021). Additionally, the symbolic regression limited the depth of the computation graph, biasing the results toward simple functions; hence, the found first integrals were identical to the well-known forms and were separated in the x- and y-directions in most cases.\n\nThe same is true for the symbolic regression of the test data, except for V1 for trial 0, which had a small perturbation α. Because of the limited extrapolation ability, neural networks cannot always accurately represent functions outside the training data range. Once first integrals are found by FINDE and identified as equations by symbolic regression, one can use the equations instead of neural networks, ensuring the preservation of first integrals in the entire domain. From these results, we can conclude that cFINDE identified the linear momenta.\n\nThe state of the KdV dataset has 50 elements, which is too large to apply a symbolic regression. For the 2-pend and FitzHugh–Nagumo datasets, we did not find consistent equations of first integrals. For example, the symbolic regression identified a quantity x2 1 − y1 as a first integral in the 2-pend dataset, which is not directly related to well-known first integrals. When the angle θ1 of the upper rod is small, y1 takes a value close to −1, and the quantity x2 1 − y1 is close to x2 1, which is a well-known first integral, namely the square l2 1 of the upper rod length l1. It is difficult to determine whether this inaccuracy is because of the training of FINDE or symbolic regression. There may still be room for improvement in the training of FINDE or symbolic regression.\n\n1 + y2\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTable A1: Symbolic Regression of First Integrals Found in Two-Body Problem\n\nTraining Data\n\nTest Data\n\nTrial\n\nV1\n\nV2\n\nV1\n\nV2\n\n0 1\n2 3\n4\n\nvx1 +vx2 vx1 +vx2 vy1 +vy2 vy1 +vy2 vx1 +vx2 − vy1 − vy2\n\nvy1 +vy2 vy1 +vy2 vx1 +vx2 vx1 +vx2 vx1 +vx2 +vy1 +vy2\n\nvx1 +vx2 +α vx1 +vx2 vy1 +vy2 vy1 +vy2 vx1 +vx2 − vy1 − vy2\n\nvy1 +vy2 vy1 +vy2 vx1 +vx2 vx1 +vx2 vx1 +vx2 +vy1 +vy2\n\nWe removed biases and scale factors. α = 0.003(y1 + y2)(vx2 + x1 + y1(vx2 + y1 + y2) + 1.402).\n\nTable A2: Results with Known Holonomic Constraints.\n\n2-pend\n\n2-body\n\nModel\n\n1-step↓\n\nVPT↑\n\n1-step↓\n\nVPT↑\n\nNODE HNN (Greydanus et al., 2019) CHNN (Finzi et al., 2020b)\n\n0.82 ±0.020 6220.26 ±91.57 0.07 ±0.000\n\n0.110 ±0.035 0.002 ±0.000 0.928 ±0.036\n\n144.21 ±12.65 5.17 ±0.570\n\n0.134 ±0.014 0.362 ±0.026\n\n(not working)\n\nNODE+cFINDE HNN+cFINDE\n\n0.71 ±0.040 236.51 ±7.150\n\n0.461 ±0.071 0.020 ±0.002\n\n163.64 ±9.790 8.32 ±0.430\n\n0.147 ±0.024 0.476 ±0.040\n\nD.3 COMPARISON WITH MODEL OF KNOWN HOLONOMIC CONSTRAINTS\n\nThe double pendulum (2-pend) is classified as a constrained Hamiltonian system. CHNN was proposed for cases when holonomic constraints are known (Finzi et al., 2020b). We evaluated comparison methods under the assumption that the holonomic constraints were known. We summarized the results in Table A2. The HNN, without constraints, completely failed to learn the dynamics. This is unsurprising because the dynamics of the double pendulum is outside the scope of the HNN. The two known holonomic constraints lead to two constraints involving the velocity; the CHNN took into account all four known constraints and worked remarkably. The HNN with cFINDE was given all four known constraints as the first integrals, but did not work properly. The original purpose of projection methods is to eliminate temporal discretization errors of first integrals but not to change the class to which the dynamics belong. Therefore, when a target system is not a subject of the base model, the base model with FINDE does not work. The NODE learns an ODE in a general way, and thus constrained Hamiltonian systems are included in its subjects. Given all four known constraints, the NODE with cFINDE worked better but never surpassed the CHNN.\n\nHowever, the CHNN works only for Hamiltonian systems in the canonical form with holonomic constraints. We also evaluated comparison methods using the 2-body dataset under the assumption that the linear momenta were known as first integrals. The CHNN attempted to obtain the inverse of a singular matrix and could not learn the dynamics. In contrast, the cFINDE improved the performances of both NODE and HNN.\n\nExisting methods (e.g., HNN and CHNN) assume geometric structures (e.g., Hamiltonian structure) described in Appendix A in order to guarantee conservation laws. When multiple structures are assumed at the same time, they must be integrated using appropriate prior knowledge. If it is possible, it would achieve extremely high performance. Otherwise, the geometric structures would conflict with each other and would not produce an appropriate model. This is the reason why CHNN failed to learn the 2-body dataset and HNN+FINDE failed to learn the 2-pend dataset. In contrast, NODE+FINDE does not assume any geometric structure and assumes first integrals in the most general way, being available to any situation. Hence, FINDE can assume one or more first integrals without changing anything.\n\nIf When the detailed properties of target systems are known, one can choose the best models. the chosen model is inappropriate, the training procedure totally fails. FINDE provides a better alternative when prior knowledge is limited. Moreover, a constrained Hamiltonian system can have first integrals other than holonomic constraints and the Hamiltonian. In this case, the CHNN with FINDE is potentially the best choice.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nTable A3: Results of NODE with cFINDE on Training Set of 2-Pend Dataset.\n\nModel\n\nNODE\n\n+ cFINDE\n\n2-pend\n\nK\n\n1-step↓\n\nVPT↑\n\n–\n\n1 2\n3 4\n5 6\n\n0.76 ±0.02\n\n0.966 ±0.007\n\n0.72 ±0.06 0.69 ±0.08 0.63 ±0.02 0.67 ±0.05 0.65 ±0.02 9.93 ±0.00\n\n0.974 ±0.004 0.981 ±0.014 0.994 ±0.002 0.990 ±0.005 0.998 ±0.000 0.126 ±0.000\n\nD.4 REASON FOR HIGH PERFORMANCE AND HOW TO DETERMINE NUMBER OF FIRST\n\nINTEGRALS\n\nThe theoretical explanation for the high performance of neural networks (e.g., HNN) that assume first integrals for physical phenomena is an open question. Sannai et al. (2021) has theoretically shown that neural networks (e.g., CNNs and GNNs) with symmetry have faster learning convergence, and we consider this approach can be applied to the above question. At least for cFINDE and dFinde, we have an intuitive but not rigorous explanation; assuming one more first integral (i.e., increasing K by 1) reduces the number of degrees of freedom in the dynamics by 1, narrows the hypothesis space, accelerates learning convergence, and suppresses generalization errors.\n\nAs shown in Table 3, the performance of cFINDE and dFINDE is sensitive to the assumed number K of first integrals. Because K is a hyperparameter, it is basically a subject to be adjusted through evaluations on a validation set. With inappropriately large K, both cFINDE and dFINDE dropped their performance significantly. See the results of the 2-pend and FitzHugh–Nagumo datasets for K = 6 and K = 3, respectively.\n\nHowever, the performance drop can be found even with the training set. Table A3 summarizes the prediction performance on the training set of the 2-pend dataset. As was the case with the test set, the performance significantly dropped at K = 6. This is because NODE with cFINDE for K = 6 assumes the submanifold M′ to be 2-dimensional. The submanifold M′ is in fact 3-dimensional, so NODE with cFINDE for K = 6 is incapable of learning the dynamics and performs poorly even on the training set. Hence, the training set is enough to avoid a fatally inappropriate K.\n\nAlternatively, K can be determined by using other methods (e.g., Fukunaga & Olsen (1971); Liu & Tegmark (2021)). Although these methods have some drawbacks introduced in Appendix A, they may be complementary to FINDE.\n\nD.5 COMPARISON WITH MODIFIED NEURAL PROJECTION METHOD\n\nThe neural projection method (NPM) also employs a projection method (Yang et al., 2020). Using a manner similar to Newton’s method, it enforces the constraint C(u) = 0 by the projection of the state u under the assumption that the quantity C(u) is always zero. This assumption holds for some cases (e.g., holonomic constraints in a fixed environment), but not for most first integrals, whose values depend on initial conditions.\n\nFor example, the linear momentum in the x-direction of the two-body problem is the first integral expressed as V (u) = m1vx1(t) + m2vx2(t). This quantity V is constant within a trial (i.e., V (u(t)) = V (u(0))) and varies between trials depending on the initial speed vx1(0) and vx2(0). The total energy, the total mass, and many other first integrals depend on the initial condition in the same manner; hence, they are outside the scope of the NPM. In contrast, by imposing the constraint on the gradient ∇V = 0 or discrete gradient ∇V = 0, our proposed FINDE keeps the quantity V constant and can handle any first integrals.\n\nFor comparison, we replaced the constraint C(u) = 0 with C(us+1, us) = V (us+1) − V (us) = 0 and adopted the NPM to first integrals varying from trial to trial. We evaluated the modified NPM using the 2-pend dataset. Because the modified NPM is a discrete-time projection method,\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable A4: Comparison with Neural Projection Method (NPM)\n\nK\n\ndFINDE (proposed)\n\nmodified NPM\n\n1-step↓\n\nVPT↑\n\n1-step↓\n\nVPT↑\n\nsuccessful\n\n1 2\n3 4\n5 6\n\n0.75 ±0.10 0.74 ±0.05 0.69 ±0.05 0.71 ±0.03 0.86 ±0.09 58.88 ±22.98\n\n0.152 ±0.017 0.271 ±0.111 0.447 ±0.081 0.454 ±0.060 0.591 ±0.087 0.037 ±0.039\n\n0.73 ±0.08 —\n(0.69 ±0.00) (0.72 ±0.03) 0.85 ±0.11 (1.29 ±0.20)\n\n0.150 ±0.014 —\n(0.138 ±0.000) (0.383 ±0.023) 0.364 ±0.134 (0.103 ±0.016)\n\n5/5 0/5 1/5 3/5 5/5 3/5\n\nwe compared it with the discrete-time version of the proposed FINDE (dFINDE). The results are summarized in Table A4.\n\nThe dFINDE successfully learned the dynamics in all trials, but the modified NPM failed to learn the dynamics in half the trials (see the rightmost column for the numbers of successful trials out of 5). The modified NPM often encountered of the underflow of the time-step size or a division by the zero gradient of the first integral. Even when the learning was successful, the performance of the NPM was inferior to that of the dFINDE. The modified NPM solved the optimization problem in Eq. (3) at every step, but it sometimes diverged or failed to converge, especially in the early phase of learning. The NPM was successful for fixed environments but might be unsuited for general first integrals varying from trial to trial. However, the dFINDE does not require solving an optimization problem during training, making the learning process robust against randomness such as initialization.\n\n25",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a neural network based approach to preserve and discover first integrals in the underlying target systems. In this paper, authors propose two instances of the approach, namely cFINDE and dFINDE, which works in continuous-time and discrete-time respectively. The proposed FINDE method is a great exploration in the area of preserving and/or discovering invariant quantities when learning dynamical systems with deep learning methods, and it is a generalization of existing deep learning based methods. As I see, the FINDE method makes a solid contribution.\n\n# Strength And Weaknesses\n\nThe main strengths of this paper are \n\n$\\mathbf{1}.$ on the proposal of enhancing the neural network based approach to learn dynamical systems by proposing an additional structure/equation to preserve and/or discover invariant quantities. \n\n$\\mathbf{2}.$ This paper is very well organized, written and illustrated with graphic results. The background knowledge is adequate for readers (relevant in deep learning fields) to understand the rationale and the intuition driving the proposal, and the paper is very readable.  \n\nThere are a few weakness, which could make the paper better but should not overshadow the strength\n\n$\\mathbf{1}.$ The numerical results can be explained and presented better. For example, the switch between dFINDE and cFINDE could be explained better as they often make a bit confusion in the text; the plots can be improved with better presentation: for instance, Figure 3 - it can be shown as four subplots, each of which can compare HNN vs HNN+cFINDE; the plots could confuse audience a bit: for instance, it is hard to see the ground truth. \n\n$\\mathbf{2}.$ It is very interesting to see the big difference induced by using different values of $K$. As discovering the invariant quantities is one of the cores of this approach and also as author addressed in the conclusion, conducting more comprehensive and in-depth numeric investigation on the role of $K$, with more detailed discussion, along with more analysis in theory, would benefit a lot in helping audience understand and the field leverage the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n$\\cdot$ This paper is very well clearly written and presented, with a great deal of details in introducing the background, literature review, intuitive explanation, theoretical analysis and numerical study. \n\n$\\cdot$ The novelty of this paper is obvious as it proposed novel approach to generalize and enhance the existing literature to capture and discover first integral of the neural-network based dynamic system learning methods. The implementation is straightforward.\n\n$\\cdot$ This paper has detailed explanation of theoretical analysis, implementations, and experiments. And, it also listed most (if not all) of the used libraries that author used in the implementation.\n\n# Summary Of The Review\n\nAs stated in the above comment sections, this paper has a solid contribution with a novel approach. It is a good paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nThere is no ethics concerns."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nPINK NOISE IS ALL YOU NEED: COLORED NOISE EXPLORATION IN DEEP REINFORCEMENT LEARNING\n\nJakob Hollenstein2,1\n\nOnno Eberhard1 1Max Planck Institute for Intelligent Systems, Tübingen, Germany 3Max Planck ETH Center for Learning Systems {firstname.lastname}@tuebingen.mpg.de\n\nCristina Pinneri3,1\n\nGeorg Martius1 2Universität Innsbruck\n\nABSTRACT\n\nIn off-policy deep reinforcement learning with continuous action spaces, exploration is often implemented by injecting action noise into the action selection process. Popular algorithms based on stochastic policies, such as SAC or MPO, inject white noise by sampling actions from uncorrelated Gaussian distributions. In many tasks, however, white noise does not provide sufficient exploration, and temporally correlated noise is used instead. A common choice is Ornstein-Uhlenbeck (OU) noise, which is closely related to Brownian motion (red noise). Both red noise and white noise belong to the broad family of colored noise. In this work, we perform a comprehensive experimental evaluation on MPO and SAC to explore the effectiveness of other colors of noise as action noise. We find that pink noise, which is halfway between white and red noise, significantly outperforms white noise, OU noise, and other alternatives on a wide range of environments. Thus, we recommend it as the default choice for action noise in continuous control.\n\n1\n\nINTRODUCTION\n\nExploration is vitally important in reinforcement learning (RL) to find unknown high reward regions in the state space. This is especially challenging in continuous control settings, such as robotics, because it is often necessary to coordinate behavior over many steps to reach a sufficiently different state. The simplest exploration method is to use action noise, which adds small random perturbations to the policy’s actions. In off-policy algorithms, where the exploratory behavioral policy does not need to match the target policy, action noise may be drawn from any random process. If the policy is deterministic, as in DDPG (Lillicrap et al., 2016) and TD3 (Fujimoto et al., 2018), action noise is typically white noise (drawn from temporally uncorrelated Gaussian distributions) or OrnsteinUhlenbeck (OU) noise, and is added to the policy’s actions. In algorithms where the policy is stochastic, such as SAC (Haarnoja et al., 2018) or MPO (Abdolmaleki et al., 2018), the action sampling itself introduces randomness. As the sampling noise is typically uncorrelated over time, these algorithms effectively employ a scale-modulated version of additive white noise, where the noise scale varies for different states.\n\nFigure 1: Trajectories of pure noise agents on a bounded integrator environment (Sec. 6). White action noise (left) does not reach far in this environment, and it would not be able to collect a sparse reward at the it explores locally. edges: OU noise (right) only explores globally and gets stuck at the edges. Pink noise (center) provides a balance of local and global exploration, and covers the state space more uniformly than the other two.\n\n1\n\nWhitenoisePinknoiseOUnoisePublished as a conference paper at ICLR 2023\n\nIn many cases, white noise exploration is not sufficient to reach relevant states. Both MPO and SAC have severe problems with certain simple tasks like MountainCar because of inadequate exploration. As in TD3 or DDPG, the off-policy nature of these algorithms makes it possible to replace the white noise process, which is implicitly used for action sampling, by a different random process. The effectiveness of temporal correlation in the action selection has been noted before (e.g. Osband et al., 2016) and is illustrated in Fig. 1, where the exploration behavior of white noise (uncorrelated) is compared to that of noises with intermediate (pink noise) and strong (OU noise) temporal correlation on a simple integrator environment (more on this in Sec. 6). Using highly correlated noise, such as OU noise, can yield sufficient exploration to deal with these hard cases, but it also introduces a different problem: strongly off-policy trajectories. Too much exploration is not beneficial for learning a good policy, as the on-policy state-visitation distribution must be covered during training to make statistical learning possible. Thus, a typical approach is to use white noise by default, and alternatives like OU noise only when necessary. In this work, our goal is to find a better strategy, by considering noises with intermediate temporal correlation, in the hope that these work well both on environments where white noise is enough, and on those which require increased exploration.\n\nTo this end, we investigate the effectiveness of colored noise as action noise in deep RL. Colored noise is a general family of temporally correlated noise processes with a parameter β to control the correlation strength. It generalizes white noise (β = 0) and Brownian motion (red noise, β = 2), which is closely related to OU noise. We find that average performance across a broad range of environments can be increased significantly by using colored action noise with intermediate temporal correlation (0 < β < 2). In particular, we find pink noise (β = 1) to be an excellent default choice. Interestingly, pink noise has also been observed in the movement of humans: the slight swaying of still-standing subjects, as well as the temporal deviations of musicians from the beat, have both been measured to exhibit temporal correlations in accord with pink noise (Duarte & Zatsiorsky, 2001; Hennig et al., 2011).\n\nOur work contributes a comprehensive experimental evaluation of various action noise types on MPO and SAC. We find that pink noise has not only the best average performance across our selection of environments, but that in 80% of cases it is not outperformed by any other noise type. We also find that pink noise performs on par with an oracle that tunes the noise type to an environment, while white and OU noise perform at 50% and 25% between the worst noise type selection and the oracle, respectively. To investigate whether there are even better noise strategies, we test a color-schedule that goes from globally exploring red noise to locally exploring white noise over the course of training, as well as a bandit method to automatically tune the noise color to maximize rollout returns. Both methods, though they significantly improve average performance when compared to white and OU noise, are nevertheless significantly outperformed by pink noise. In addition to the results of our experiments, we attempt to explain why pink noise works so well as a default choice, by constructing environments with simplified dynamics and analyzing the different behaviors of pink, white and OU noise. Our recommendation is to switch from the current default of white noise to pink noise.\n\n2 BACKGROUND & RELATED WORK\n\nReinforcement learning (RL) has achieved impressive results, particularly in the discrete control setting, such as achieving human-level performance in Atari games with DQN (Mnih et al., 2015) or mastering the game of Go (Silver et al., 2016) by using deep networks as function approximators. In this paper, we are concerned with the continuous control setting, which is especially appropriate in robotics. In continuous action spaces, it is typically intractable to choose actions by optimizing a value function over the action space. This makes many deep RL methods designed for discrete control, such as DQN, not applicable. Instead, researchers have developed policy search methods (e.g. Williams, 1992; Silver et al., 2014), which directly parameterize a policy. These methods can be divided into on-policy algorithms, such as TRPO (Schulman et al., 2015) and PPO (Schulman et al., 2017), and off-policy algorithms such as DDPG, TD3, SAC and MPO.\n\nAll these algorithms have to address the problem of exploration, which is fundamental to RL: in order to improve policy performance, agents need to explore new behaviors while still learning to act optimally. One idea to address exploration is to add a novelty bonus to the reward (e.g. Thrun, 1992). In deep RL, this can be done by applying a bonus based on sample density (Tang et al., 2017) or prediction error (Burda et al., 2019). Another method to encourage exploration is to take inspiration from bandit methods like Thompson sampling (e.g. Russo et al., 2018), and act optimistically with\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nrespect to the uncertainty in the Q-function (Osband et al., 2016). The simplest strategy, however, is to randomly perturb either the policy parameters (Plappert et al., 2018; Mania et al., 2018), or the actions themselves. This can be done by randomly sampling a function for each episode that deterministically alters the action selection (Raffin & Stulp, 2020), by learning correlations between action dimensions and state space dimensions to induce increasing excitation in the environment (Schumacher et al., 2022), or by learning an action prior from task-agnostic data (Bagatella et al., 2022).\n\nIn this work, we consider the simplest and most common form of exploration in continuous control: action noise. Action noise can be either explicitly added to the policy, or implicitly, by randomly sampling actions from a stochastic policy. The most common form of action noise is white noise, which typically comes from sampling from independent Gaussian distributions at every time step. Apart from white action noise, Lillicrap et al. (2016) successfully used temporally correlated OrnsteinUhlenbeck noise, and Pinneri et al. (2020) achieved improvements in model predictive control by utilizing colored noise. Inspired by this success, in this work we investigate the effectiveness of colored action noise in the context of model-free RL, specifically on MPO and SAC.\n\n3 METHOD\n\nIn this paper, we investigate exploration using action noise. In algorithms like DDPG and TD3, where the learned policy μ is deterministic, action noise is simply added to the policy:\n\nat = μ(st) + σεt,\n\n(1)\n\nwhere ε1:T = (ε1, . . . , εT ) is sampled from a random process, and σ is a scale parameter. If εt is sampled independently at every time step, e.g. from a Gaussian distribution, then ε1:T is called white noise (WN). This is the prevailing choice of action noise, though it is also common to use time-correlated Ornstein-Uhlenbeck noise (ε1:T ∼ OUT ) (Uhlenbeck & Ornstein, 1930).\n\nAlgorithms which parameterize a stochastic policy, such as SAC and MPO, also use action noise. In continuous action spaces, the most common policy distribution is a diagonal Gaussian, represented by the functions μ(st) and σ(st): at ∼ N (μ(st), diag(σ(st))2). This can equivalently be written as\n\nat = μ(st) + σ(st) (cid:12) εt,\n\n(2)\n\nwhere εt ∼ N (0, I). In this case, the action noise ε1:T is again Gaussian white noise, which is scale-modulated by the function σ.\n\nWhite noise is not correlated over time (cov[εt, εt(cid:48)] = 0). In some environments, this leads to very slow exploration, which in turn leads to inadequate state space coverage, leaving high reward regions undiscovered. Thus, it is often beneficial to use action noise with temporal correlation (cov[εt, εt(cid:48)] > 0), like Ornstein-Uhlenbeck (OU) noise. OU noise was recommended as the default choice for DDPG, and has been shown to lead to a significant increase in state space coverage (Hollenstein et al., 2022). OU noise is defined by the stochastic differential equation (SDE)\n\n ̇εt = −θεt + σηt,\n\n(3)\n\nwhere ηt is a white noise process. If θ = 0, then this equation defines integrated white noise, also called Brownian motion. Brownian motion is temporally correlated, but cannot be used as action noise if generated in this way, because its variance increases unboundedly over time, violating the action space limits. This problem is addressed by setting θ > 0 (a typical choice is θ = 0.15), which bounds the variance. More details about OU noise and Brownian motion can be found in Sec. A.\n\nA broad family of temporally correlated noises is given by colored noise, which generalizes both white noise and Brownian motion (in this context called red noise).\n\nDefinition 1 (Colored noise). A stochastic process is called colored noise with color parameter β, if signals ε(t) drawn from it have the property |ˆε(f )|2 ∝ f −β, where ˆε(f ) = F[ε(t)](f ) denotes the Fourier transform of ε(t) (f is the frequency) and |ˆε(f )|2 is called the power spectral density (PSD).\n\nThe color parameter β controls the amount of temporal correlation in the signal. The PSDs of colored noise with different β are shown in Fig. A.2. If β = 0, then the signal is uncorrelated, and the PSD is flat, meaning that all frequencies are equally represented. This noise is called white noise in analogy to light, where a signal with equal power on all visible frequencies is perceived as white.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The environments we use: Pendulum, CartPole (balance + swingup tasks), Ball-In-Cup, MountainCar, Hopper, Walker, Reacher, Cheetah, Door. See Sec. C for more details. Images partly taken from Tassa et al. (2018) with permission.\n\nRed noise (β = 2) is named so, because it has more weight on lower frequencies, which in light corresponds to the red part of the spectrum. Gaussian colored noise with constant variance can be efficiently generated, and the complete noise signal for an episode can be sampled at once, to be used as action noise according to Equations (1) and (2). If generated like this, which we denote by ε1:T ∼ CNT (β) (more details in Sec. A), white noise is identical to independently sampling from a Gaussian distribution at every time step. Red noise (CNT (2)) is very similar to OU noise with the default setting θ = 0.15, as both are essentially Brownian motion with bounded variance (see Fig. A.2).1 By setting 0 < β < 2, colored noise allows us to search for a better default action noise type with intermediate temporal correlation between white and red noise. One special case is pink noise, which is defined by β = 1.\n\n4\n\nIS PINK NOISE ALL YOU NEED?\n\nFujimoto et al. (2018) found that the type of action noise (white or OU) in general does not influence the performance of TD3. In contrast to this, Hollenstein et al. (2022) found that the noise type does have an influence, but that the impact of this choice, as well as which noise type is preferable, depends entirely on the environment. We start by confirming these latter results2, and compare white and OU action noise with a selection of colored action noises (β ∈ [0, 2]), in terms of the achieved performance. In all of our experiments we use MPO and SAC, relying on the implementations by Pardo (2020) and Raffin et al. (2021), respectively. We found that both algorithms significantly outperform TD3 across tasks, and thus only briefly discuss TD3 in Sec. B.1. Since the optimality of an action noise type depends on the environment, we perform experiments on a diverse set of 10 different tasks taken from the DeepMind Control Suite (Tassa et al., 2018), OpenAI Gym (Brockman et al., 2016), and the Adroit hand suite (Rajeswaran et al., 2018). These environments are shown in Fig. 2 and are described in more detail in Sec. C. We report results on some additional tasks in Sec. G.\n\nTo evaluate the performance of a training procedure (which always lasts 106 environment interactions), we run 5 evaluation rollouts every 104 interactions. We then report the performance as the mean return of all these evaluation rollouts. Since this performance is related to the area under the learning curve, it is a measure combining both the final policy performance, and the sample efficiency of an algorithm. More detailed results, including learning curves and an analysis of the final policy performance, can be found in Sections B.2 and H.\n\n4.1 DOES THE NOISE TYPE MATTER?\n\nTo assess the importance of the choice of action noise, we evaluate the performances achieved by SAC and MPO when using white noise, OU noise and colored noise as action noise (where\n\n1Other settings of θ for OU noise, which are less similar to red noise, are discussed in Sec. A. In the main\n\ntext we only consider the default setting of θ = 0.15.\n\n2We also confirm the former results (see Sec. B.1), but find that colored noise (especially pink noise)\n\noutperforms both white and OU noise on TD3.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nEnvironment\n\nBest noise\n\np\n\nPink?\n\nPendulum Cartpole (b.) Cartpole (s.) Ball-In-Cup MountainCar Hopper Walker Reacher Cheetah Door\n\n2.0 1.0 (Pink) 1.0 (Pink) 0.75 2.0 1.0 (Pink) 0.5 White noise 0.75 0.75\n\n0.01\n\n(cid:55) — (cid:51) — (cid:51) (cid:51) (cid:51) — (cid:51) (cid:51) (cid:55) (cid:51) (cid:51)\n\n0.88 0.59\n\n0.36 0.02 0.62 0.65\n\nFigure 3: Bootstrap distributions for the expected average performance of MPO and SAC using different action noise types (details in Sec. B.2). Highlighted are white noise (WN), pink noise (β = 1), and Ornstein-Uhlenbeck noise (OU).\n\nTable 1: A Welch t-test reveals that the performance difference between pink noise and the best noise is only significant in two out of ten environments. The rightmost column answers whether pink noise performs equally well as the best noise type.\n\nβ ∈ {0.1, 0.2, 0.35, 0.5, 0.75, 1, 1.5, 2}), on the benchmark environments shown in Fig. 2. We repeat all learning runs with 20 different seeds, resulting in a total of 20 × 10 × 2 × 10 = 4000 experiments3, each one reporting a single scalar performance. To control for the influence of the algorithm and environment on the performance of a particular noise type, we group all results by algorithm and task, and normalize them to zero mean and unit variance. We then calculate a noise type’s average performance: the normalized performance of all runs using this noise type, averaged across algorithms and environments. In Fig. 3, bootstrap distributions for the expected average performances are shown, generated using the 20 random seeds available per task and algorithm (more details in Sec. B.2). It can be seen that the noise type indeed matters for performance. A clear preference for pink noise (β = 1) becomes visible, which considerably outperforms white noise and OU noise across tasks. In Sec. B.2, this performance difference can be seen on the corresponding learning curves, where we compare white, OU and pink noise.\n\nAchieving the best average performance across environments is not the same as being the best performing option on each individual environment. This begs the question of when pink noise (the best general option) also performs as good as an environment’s best noise type. We perform a Welch t-test for each environment to check whether the expected difference between the performances of pink noise and the task-specific best noise4 is significant. The results are listed in Table 1. Although pink noise only achieves the highest mean across seeds in three of the ten tasks, the statistical analysis reveals that the difference between pink noise and the best noise type is only significant in two out of ten cases. In other words, in the tested environments, pink noise performs on par with the best choice of noise type in 80% of cases! What about the two environments, Pendulum and Reacher, where pink noise is outperformed by other noise types? On Pendulum, pink noise, on average, achieves 83% of the performance of red noise (β = 2). In contrast, white noise only achieves 39% of the performance of red noise (OU performs similarly to red noise). On Reacher, pink noise achieves 99% of white noise’s performance, while OU noise achieves only 76%. So, even on the few environments where pink noise is outperformed significantly, it is clearly preferable as a default over white noise and OU noise. These results indicate that pink noise seems to be all you need.\n\n4.2\n\nIS PINK NOISE A GOOD DEFAULT?\n\nThe best performance on a given environment is always achieved with the task-specific best action noise type. It would be nice to always use this best noise type, but it is often unpractical to run a large hyperparameter search to find it, especially when including many possible colors β. It is common therefore, to stick to a “default” choice, which is typically white noise. In the previous section, we saw that pink noise is a better default choice than white noise, but it is still unclear whether this is\n\n3seeds × tasks × algorithms × noise types (WN + OU + 8 colors) 4To compute the best noise type, we normalize out the contribution of the algorithm (similarly to the average\n\nperformance) and take the mean performance over random seeds on each environment.\n\n5\n\nWN0.10.20.350.50.751.01.52.0OUβ−1.0−0.50.00.5AveragePerformancePublished as a conference paper at ICLR 2023\n\nFigure 4: Bootstrap distributions for the expected average performances of all methods we discuss in this paper. Highlighted are again white noise and OU noise (the popular options), as well as pink noise (our suggestion). While OU noise and white noise only achieve about 25% and 50% of the possible performance gain of an oracle method, pink noise performs equally to the oracle! Pink noise is also not outperformed by a colorscheduling method (Sec. 5.1), nor by a bandit algorithm (Sec. 5.2).\n\nenough, or if a hyperparameter search might be needed for good performance. In this section, we will analyze how much performance is lost by sticking to a default value of white noise, pink noise, or OU noise, compared to using the task-specific best noise type.\n\nWe choose the task-specific best noise type via an “oracle” method, which can be thought of as a very extensive grid search: an environment’s best noise type is selected by looking at the results of all noise types, and choosing the best performing option across 10 seeds. By also doing an “anti-oracle” experiment, which selects the worst noise type on each environment (i.e. the most unlucky pick of noise types possible), we can define a new “performance gain” measure, which might be easier to interpret than the average performance in the previous section.5 The performance gain of a noise type specifies where its average performance falls between the anti-oracle’s performance (0%) and the oracle’s performance (100%). Figure 4 presents the performance gains of using white noise, Ornstein-Uhlenbeck noise and pink noise as a default for all environments.\n\nBy always sticking to Ornstein-Uhlenbeck noise, only about 25% of the highest possible performance gain is achieved, and the resulting performance would be closer to using the anti-oracle. By using white noise instead, we already achieve a performance gain of over 50%. However, picking pink noise does not appear to sacrifice any performance compared to the oracle!6 The gain achieved by switching from white noise as the default to pink noise is both considerable and significant, and we recommend switching to pink noise as the default action noise.\n\n5 ALL THE COLORS OF THE RAINBOW\n\nWe have found that pink noise is the best default action noise over a broad range of environments. There are still some environments, however, where pink noise is outperformed by other noise types, specifically by white and red noise on the Reacher and Pendulum tasks, respectively. This indicates that there may not exist a single noise type which performs best on all environments. However, this consideration is only valid if the noise is kept constant over the course of training. If we instead try a different approach, and choose the noise type separately for each rollout, we may find a strategy that is outperformed nowhere. In this section we discuss two such non-constant methods, which differ in the way a rollout’s noise type is selected: a color-schedule going from β = 2 to β = 0, and a bandit approach with the intention of finding the optimal color for an environment.\n\n5.1\n\nIS COLOR-SCHEDULING BETTER THAN PINK NOISE?\n\nTo find a more effective exploration method than pink noise, it is helpful to understand why learning in the Pendulum and Reacher environments works better with other noise types. The Pendulum environment is underactuated and requires a gradual build-up of momentum by slowly swinging back and forth. Strongly correlated action noise, such as red noise, makes this behavior much more likely. The Reacher task, on the other hand, has neither a particularly large state space, nor does it exhibit\n\n5The (anti-)oracle is evaluated on the 10 seeds not used for noise type selection to avoid sampling bias. We\n\nrepeat this by selecting (evaluating) once on the first (latter) 10 seeds, and once on the latter (first) 10 seeds.\n\n6It looks as if pink noise is even exceeding the oracle’s performance. This difference is not statistically\n\nsignificant and is due to the oracle only having access to the 10 seeds not used for evaluation.\n\n6\n\n0%20%40%60%80%100%PerformanceGainAnti-OracleOrnstein-UhlenbeckWhitenoiseSchedule(Sec.5.1)Random(Sec.5.2)Bandit(Sec.5.2)PinknoiseOracle−1.0−0.50.00.5AveragePerformancePublished as a conference paper at ICLR 2023\n\nEnvironment\n\nP > S\n\nP < S\n\n<0.01\n\n0.15\n\nPendulum Cartpole (b.) Cartpole (s.) Ball-In-Cup MountainCar Hopper Walker Reacher Cheetah Door\n\n<0.01 0.01 <0.01\n\n0.05 <0.01 <0.01 0.05 0.15\n\nPink ≥ Schedule? (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n\nP > B P < B Pink ≥ Bandit?\n\n0.73\n\n0.28 0.50\n\n0.10 <0.01 0.47 0.29 0.15\n\n0.64 0.27\n\n(cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n\nTable 2: How does pink noise compare to a schedule and a bandit method? For each environment, we perform a Welch t-test to test for inequality of the performances of pink noise vs. the schedule/bandit method. The p-values are arranged to show which performance is higher. Pink noise performs significantly better than the schedule on most environments. Compared to the bandit algorithm, pink noise performs better overall, but on most environments the difference is not significant. In the “Pink ≥ ...?” columns, ((cid:51)) means that pink noise does not perform significantly worse than the alternative.\n\nunder-actuation, such that white noise is well suited to explore the space. Here, temporally correlated noise will only lead to off-policy trajectory data, thereby inhibiting learning. In general, strongly correlated noise leads to more global exploration, while uncorrelated noise explores more locally. We return to these ideas in Sec. 6, where we analyze the effects of high and low correlation on two simple environments.\n\nOur method should work well on both of these environments, and on environments which require a mix of local and global exploration. Thus, we are looking for a strategy which balances local and global exploration. A simple idea to do this is a color-schedule: start with highly correlated red noise (β = 2) and then slowly decrease β to white noise (β = 0) over the course of training. The rationale behind this strategy is that high-reward regions can be quickly discovered at the beginning of training when β is large, while the trajectories get more on-policy over time, helping with environments like Reacher. Indeed, a similar approach that schedules the action noise scale, has been shown to work quite well (Hollenstein et al., 2022).\n\nWe implement a β-schedule, which linearly goes from β = 2 to β = 0, on MPO and SAC and repeat the experiment with 20 random seeds on all environments. Bootstrap distributions for the expected average performance across environments are shown in Fig. 4 (denoted by Schedule). The results indicate that the schedule is generally better than OU and white noise, but does not outperform pink noise. Indeed, pink noise is significantly better, as the confidence intervals do not overlap. If we take a more detailed look at the individual environments (Table 2), we see that thanks to the additional highly correlated noise, the schedule does outperform pink noise on the Pendulum environment, as expected. However, in all other environments pink noise either significantly outperforms the schedule or they perform on par, so our recommendation to use pink noise as a default remains.\n\n5.2\n\nIS BANDIT COLOR SELECTION BETTER THAN PINK NOISE?\n\nThe results in the previous section indicate that, while changing the noise type over the training process can improve performance, simply moving from globally exploring red noise to more locally exploring white noise does not outperform pink noise. Instead of trying to find a different schedule to fit all environments, in this section we consider an adaptive approach. By using a bandit algorithm to select the action noise color for each rollout on the basis of past rollout returns, it might be possible to find not only the general best noise for a given environment, but even to automatically adapt the noise to different stages of training. The bandit algorithm we use is based on Thompson sampling, the details are explained in Sec. D.\n\nWe use the rollout return itself as the bandit reward signal. The reasoning for this is that in environments where strong exploration is necessary (such as Pendulum and MountainCar), high return will only be achieved by strongly correlated actions. On the other hand, if environments do not require correlated actions, or a capable policy has been learned, the highest return should be achieved by the action noise which least disturbs the policy, i.e. noise with a low correlation.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nAs an additional baseline, we also perform an experiment where a color (β) is randomly selected for each rollout.7 For both methods we use the same list of β values as in Sec. 4.1 (incl. β = 0), and repeat the experiments with 20 random seeds. The results on MPO and SAC are shown in Fig. 4 (marked with Bandit and Random) and Table 2. It can be seen that the bandit method is again outperformed by pink noise. Indeed, a bootstrapping test yields a highly significant difference in expected average performance across environments (p = 0.005). Looking at the results on the individual tasks (Table 2), it seems like the bandit method does outperform pink noise on the two problematic environments (Pendulum and Reacher), however, this difference is not significant. A detailed comparison of the bandit and its random baseline can be found in Table D.1, which shows that neither of the two methods significantly outperforms the other on any environment. This indicates that, while there may be merit in changing the noise type over the training process, the rollout return appears to contain too little information to effectively guide the noise type selection. Thus, our recommendation to use pink noise as a default remains unchanged.\n\n6 HOW DO ACTION NOISE AND ENVIRONMENT DYNAMICS INTERACT?\n\nWhy is pink noise such a good default noise type? In Sec. 5.1, we briefly discussed the concepts of local and global exploration, and hypothesized that the best exploration behavior provides a balance of the two, such that high reward regions will be found, while trajectories are not too off-policy. To analyze how different noise types behave, we will look at a simplified bounded integrator environment: a velocity-controlled 2D particle moving in a box (more details in Sec. F.2). If we control this particle purely by noise, we can analyze the exploration behavior in isolation of a policy. As a first test, we run 20 episodes of 1000 steps in an environment of size 250 × 250 with white noise, pink noise, and OU noise (all with unit variance, x- and y-velocity controlled independently). The resulting trajectories are shown in Fig. 1. It can be seen that pink noise provides the best combination of local and global exploration: it reaches the edges (unlike white noise), but does not get stuck there (unlike OU noise).\n\nA good mix of local and global exploration gives rise to a more uniform state space coverage, as can be seen in Fig. 1. Thus, how well a noise type explores depends highly on the size of the environment: if the environment was much smaller, white noise would be enough to cover the space and pink noise trajectories would look similar to the OU trajectories shown here. On the other hand, if the environment were bigger, then pink noise would not reach the edges and OU noise would explore better. The uniformity of the state space coverage is measured by the entropy of the state-visitation distribution. We estimate the entropy induced by a noise type using a histogram density approximation: we partition the state space into a number of boxes (50 × 50 = 2500 boxes), sample 104 trajectories, and count the number of sampled points in each box.\n\nFigure 5 shows the entropy achieved by white noise, OU noise and pink noise as a function of the environment size. The sizes are chosen to reflect the complete sensible range for episode lengths of 1000 steps, each with unit variance: from very small (50 × 50) to very large (2000 × 2000). Pink noise is not “special” in the sense that it performs best on all environments, as we already saw in the previous sections. However, it performs best on “medium scales”, as determined by the episode length, and does not suffer from severe degradation in performance over the whole spectrum of sensible environments. If we do not know where on this spectrum a given environment lies, then pink noise is clearly a better default choice than white noise or OU noise!\n\nBesides integrating actions, another common aspect of environment dynamics is oscillation. Oscillation dynamics are dominant in the Pendulum and MountainCar environments8, but also relevant in other domains, like Ball-In-Cup, Cartpole, and Walker. To model these dynamics, we construct a second environment: a simple harmonic oscillator. This system is a frictionless 1-dimensional physical setup, in which a mass m is attached to an ideal spring of stiffness k. The state space consists of the mass’s position and velocity, and the action describes a force that is applied to the mass. The goal is to maximize the energy in the oscillator system (which is equivalent to maximizing the amplitude), similar to the MountainCar and Pendulum tasks, where this is necessary to collect the sparse reward.\n\nThe oscillator environment is parameterized by the resonant frequency f of the system, which is fixed by setting the stiffness k = 4π2 and the mass m = 1/f 2 (more details in Sec. F.1). Figure 5 shows\n\n7This method would be roughly equivalent to the bandit method if we provided no bandit reward signal. 8See Sec. E for a simple method exploiting this property to solve MountainCar.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Pink noise strikes a favorable middle ground between white noise and Ornstein-Uhlenbeck noise on a wide range of environments. On both a bounded integrator environment parameterized by its size (left), and on a simple harmonic oscillator environment parameterized by its resonant frequency (right), it is much more general in terms of the range of parameters which yield good results, and performs well on the complete range of reasonable parameterizations. We argue that this quality is what makes it a good default.\n\nthe average energy in the oscillator system (over 1000 episodes of 1000 steps each) as a function of the resonant frequency f , which we vary from very low (f = 1 1000 , episode length = 1 period) to very high (f = 1 2 , Nyquist frequency), when driven by white noise, pink noise, and OU noise. The energy is measured relative to the average energy achieved by a sinusoidal excitation at the resonant frequency, denoted harmf . Even though this is a completely different setup to the bounded integrator, and we are using a very different performance metric, the two plots look remarkably similar. Again, this shows the power of pink noise as a default action noise: if we do not know the resonant frequency of the given environment, pink noise is the best choice.\n\nThese two environments (bounded integrator and oscillator) are rather simplistic. However, the dynamics of many real systems undoubtedly contain parts which resemble oscillations (when a spring or pendulum is present), single or double integration (when velocities/steps or forces/torques are translated into positions) or contact dynamics (such as the box in the bounded integrator). If an environment’s dynamics are very complex, i.e. they contain many such individual parts, then the ideal action noise should score highly on each of these “sub-tasks”. However, if all these individual parts have different parameters (like the environment size or resonant frequency above), it stands to reason that the best single action noise would be the one which is general enough to play well with all parameterizations, i.e. pink noise. On the flip side, the average performance in Fig. 3 over all environments may be interpreted as the performance over a very complicated environment, with the sub-tasks being the “actual” environments. This might explain why we see this curve: all sub-tasks have very different parameters, and require different action noises (as seen in Table 1), but pink noise is general enough to work well on all sub-tasks, and thus easily outperforms noise types like white noise or OU noise, which are only good on very specific environments (see Fig. 5).\n\n7 CONCLUSION\n\nIn this work we performed a comprehensive experimental evaluation of colored noise as action noise in deep reinforcement learning for continuous control. We compared a variety of colored noises with the standard choices of white noise and Ornstein-Uhlenbeck noise, and found that pink noise outperformed all other noise types when averaged across a selection of standard benchmarks. Pink noise is only significantly outperformed by other noise types on two out of ten environments, and overall performs equally well to an oracle selection of the noise type. Additionally, we compared pink noise to more sophisticated methods that change the noise type over the course of training: a color-schedule, a bandit method, and a random selection scheme. No method outperforms pink noise, and our recommendation is to use pink noise as the default action noise. Finally, we studied the behaviors of pure noise agents on two simplified environments: a bounded integrator and a harmonic oscillator. The results showed that pink noise is much more general with respect to the environment parameterization than white noise and OU noise, which sheds some light on why it performs so well as the default choice.\n\n9\n\n104105106Area2468Entropy[nats]BoundedIntegratorEnvironmentWNPinkOU10−310−210−1Resonantfrequencyf10−210−1Energy[harmf]OscillatorEnvironmentPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe want to thank Marco Bagatella, Sebastian Blaes, and Pierre Schumacher for helpful feedback on earlier revisions of this text, and the Max Planck ETH Center for Learning Systems for supporting Cristina Pinneri. Georg Martius is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645. We acknowledge the support from the German Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B).\n\nREPRODUCIBILITY STATEMENT\n\nWe submitted our code as supplementary material, and provide a polished version of it online (https: //github.com/martius-lab/pink-noise-rl). In an effort to ensure reproducibility, we took particular care in applying random seeds to all randomized parts of the algorithms, including the environments, algorithm initialization and training, as well as the action noise signals. In Sec. C, we provide details on the exact algorithms and environments we use. The hyperparameters for our own methods are included in the code submission. Additionally, our main experiments were all repeated with 20 different random seeds to eliminate statistical flukes.\n\nREFERENCES\n\nAbbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Rémi Munos, Nicolas Heess, and Martin A. Riedmiller. Maximum a posteriori policy optimisation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id=S1ANxQW0b. 1\n\nMarco Bagatella, Sammy Joe Christen, and Otmar Hilliges. SFP: State-free priors for exploration in off-policy reinforcement learning. Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id=qYNfwFCX9a. 3\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/ abs/1606.01540. 4, 20\n\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network In 7th International Conference on Learning Representations, ICLR 2019, New distillation. Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. net/forum?id=H1lJJnR5Ym. 2\n\nJames W. Cooley and John W. Tukey. An algorithm for the machine calculation of complex fourier\n\nseries. Mathematics of Computation, 19:297–301, 1965. 13\n\nMarcos Duarte and Vladimir M. Zatsiorsky. Long-range correlations in human standing. Physics Letters A, 283(1):124–128, 2001. ISSN 0375-9601. URL https://www.sciencedirect. com/science/article/pii/S0375960101001888. 2\n\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582–1591. PMLR, 2018. URL http://proceedings. mlr.press/v80/fujimoto18a.html. 1, 4, 18\n\nAurélien Garivier and Eric Moulines. On upper-confidence bound policies for non-stationary bandit problems. arXiv: 0805.3415, 2008. URL https://arxiv.org/abs/0805.3415. 20\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1856–1865. PMLR, 2018. URL http://proceedings.mlr.press/v80/haarnoja18b.html. 1\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nHolger Hennig, Ragnar Fleischmann, Anneke Fredebohm, York Hagmayer, Jan Nagler, Annette Witt, Fabian J. Theis, and Theo Geisel. The nature and perception of fluctuations in human musical rhythms. PLOS ONE, 6(10):1–7, 10 2011. URL https://doi.org/10.1371/journal. pone.0026457. 2\n\nJakob Hollenstein, Sayantan Auddy, Matteo Saveriano, Erwan Renaudo, and Justus Piater. Action noise in off-policy deep reinforcement learning: Impact on exploration and performance. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview. net/forum?id=NljBlZ6hmG. Survey Certification. 3, 4, 7\n\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509. 02971. 1, 3, 15\n\nHoria Mania, Aurelia Guy, and Benjamin Recht.\n\nSimple random search of static linInear policies is competitive for Information Proformation Processing Systems 31: cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 1805–1814, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html. 3\n\nAnnual Conference on Neural\n\nIn Advances in Neural\n\nreinforcement\n\nlearning.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nat., 518(7540):529–533, 2015. URL https://doi.org/10.1038/nature14236. 2\n\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 4026–4034, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 8d8818c8e140c64c743113f563cf750f-Abstract.html. 2, 3\n\nFabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmarking. CoRR, abs/2011.07537, 2020. URL https://arxiv.org/abs/2011.07537. 4, 15, 20\n\nCristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal Rolínek, and Georg Martius. Sample-efficient cross-entropy method for real-time planning. In 4th Conference on Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event / Cambridge, MA, USA, volume 155 of Proceedings of Machine Learning Research, pp. 1049–1065. PMLR, 2020. URL https://proceedings.mlr.press/v155/pinneri21a.html. 3\n\nMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https: //openreview.net/forum?id=ByBAl2eAZ. 3\n\nAntonin Raffin and Freek Stulp. Generalized state-dependent exploration for deep reinforcement learning in robotics. CoRR, abs/2005.05719, 2020. URL https://arxiv.org/abs/2005. 05719. 3\n\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. J. Mach. Learn. Res., 22:268:1–268:8, 2021. URL http://jmlr.org/papers/v22/20-1364.html. 4, 15, 20\n\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforceIn Robotics: Science and Systems XIV, Carnegie Mellon ment learning and demonstrations.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nUniversity, Pittsburgh, Pennsylvania, USA, June 26-30, 2018, 2018. URL http://www. roboticsproceedings.org/rss14/p49.html. 4, 20\n\nDaniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on thompson sampling. Found. Trends Mach. Learn., 11(1):1–96, 2018. URL https://doi.org/ 10.1561/2200000070. 2\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 1889–1897. JMLR.org, 2015. URL http://proceedings.mlr.press/ v37/schulman15.html. 2\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. 2\n\nPierre Schumacher, Daniel Häufle, Dieter Büchler, Syn Schmitt, and Georg Martius. Dep-rl: Embodied exploration for reinforcement learning in overactuated and musculoskeletal systems. arXiv: 2206.00484, 2022. URL https://arxiv.org/abs/2206.00484. 3\n\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceedings, pp. 387–395. JMLR.org, 2014. URL http://proceedings. mlr.press/v32/silver14.html. 2\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484–489, 2016. URL https://doi.org/10.1038/nature16961. 2\n\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep In Advances in Neural Information Processing Systems 30: Annual reinforcement learning. Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 2753–2762, 2017. URL https://proceedings.neurips.cc/paper/ 2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html. 2\n\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018. URL http://arxiv.org/abs/ 1801.00690. 4, 20\n\nSebastian Thrun. Efficient exploration in reinforcement learning. Technical Report CMU-CS-92-102,\n\nCarnegie Mellon University, Pittsburgh, PA, January 1992. 2\n\nJens Timmer and Michel Koenig. On generating power law noise. Astronomy and Astrophysics, 300:\n\n707, 1995. 13\n\nG. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion. Phys. Rev., 36:823–841,\n\nSep 1930. URL https://link.aps.org/doi/10.1103/PhysRev.36.823. 3\n\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8:229–256, 1992. URL https://doi.org/10.1007/ BF00992696. 2\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nSupplementary Material\n\nPINK NOISE IS ALL YOU NEED\n\nA COLORED NOISE AND ORNSTEIN-UHLENBECK NOISE\n\nColored noise has an interesting property that was not mentioned in the main text: integrating a colored noise signal with parameter β again yields a colored noise signal, only with parameter β + 2. This stems from the property of the Fourier transform that an integration in the time domain corresponds to a multiplication with (i2πf )−1 in the frequency domain. Let v(t) be the original colored noise signal with |ˆv(f )|2 ∝ f −β. Then the PSD of x(t) = (cid:82) t\n\n0 v(τ ) dτ is\n\n|ˆx(f )|2 =\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nF\n\n(cid:20)(cid:90) t\n\n0\n\nv(τ ) dτ\n\n(cid:21)\n\n2\n\n(cid:12) (cid:12) (f ) (cid:12) (cid:12)\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 i2πf\n\n2\n\n(cid:12) (cid:12) ˆv(f ) (cid:12) (cid:12)\n\n∝ f −2|ˆv(f )|2 ∝ f −(β+2).\n\n(4)\n\nFrom this, and the definition of white noise as colored noise with β = 0, it follows that Brownian motion (integrated white noise) is also colored noise with parameter β = 2. In Fig. A.1, sampled signals of most of the noise types we use in this paper are shown, and in Fig. A.2, we plot the power spectral densities of some of these.\n\nFigure A.1: Sampled signals from various action noise processes with noise scale σ = 1. The exception is OU noise, whose noise scale is adjusted such that var[εt] = 1 (see discussion below).\n\nWe generate colored noise using the procedure described by Timmer & Koenig (1995), based on the Fast Fourier Transform (FFT) (Cooley & Tukey, 1965). This method is very efficient, as it only requires sampling a Gaussian signal in the frequency space (where the PSD is shaped), and then transforming it to the time domain via the FFT. In particular, this procedure is faster than sampling an Ornstein-Uhlenbeck signal (using the most common procedure, which we describe below). We use the colorednoise Python package (https://github.com/ felixpatzelt/colorednoise) to sample colored noise signals, and always sample signals of the complete episode length (which we denote by ε1:T ∼ CNT (β)). The Python implementation contained a bug, which among other things made it so the generated “white noise” was correlated, and our fix of this bug is included as of version 2.1.0 of the package. Colored noise sampled according to this procedure is stationary and Gaussian: the signals are marginally identical to standard Gaussian distributions, i.e. p(εt) = N (εt | 0, 1). The only difference to white noise (independent Gaussian samples at every time step) is that they are temporally correlated: p(εt, εt(cid:48)) (cid:54)= p(εt)p(εt(cid:48)). This is shown empirically on the example of pink noise in Fig. A.3.\n\n13\n\n−202εtWhitenoise(β=0)β=0.5Pinknoise(β=1)02505007501000t−202εtβ=1.502505007501000tRednosie(β=2)02505007501000tOUnoise(θ=0.15)Published as a conference paper at ICLR 2023\n\nAppendix\n\nFigure A.2: Left: The power law trends can be seen in the PSDs of sampled colored noise signals. Right: Brownian motion, here generated by integrating white noise sampled from N (0, 1), is compared to two related stationary noises: Ornstein-Uhlenbeck noise (θ = 0.15), and red noise. The similarity between OU and red noise is visible. All signals are of length T = 1000.\n\nA.1 ORNSTEIN-UHLENBECK NOISE GENERATION AND VARIANCE CORRECTION\n\nAlso included in Fig. A.3 is Ornstein-Uhlenbeck (OU) noise. It can be seen that OU noise starts out as non-stationary but quickly converges to the same marginal distribution p(εt) = N (εt | 0, 1) as the other noise types. Important to note is that all these noise types are suitable for use as action noise only because they are (or quickly become) stationary, and hence their variance does not grow without bounds (contrary to that of Brownian motion). The property that all noise types have the same marginal distribution shows that our results are only due to a change in the temporal correlation of the action noise, not in the scale or shape of the distribution, as this is the same as of regular Gaussian white noise. To make sure that OU noise converges to a standard Gaussian marginal distribution we cannot use a noise scale of σ = 1, but have to correct it. Ornstein-Uhlenbeck noise can be defined by the stochastic differential equation\n\ndxt = −θxt dt + σ dwt ,\n\n(5)\n\nFigure A.3: The colored noise we use as action noise has the same marginal distribution as independent Gaussian samples. We sampled 3 × 105 action noise signals of length T = 1000 from each of the following random processes: independent Gaussian samples (white noise, left), pink noise (center), Ornstein-Uhlenbeck noise (right). At every time step t we show a histogram density estimate over action noise values εt. This shows that our results are only due to the increased temporal correlation of the action noise signals, as the marginal distributions remain unchanged from white noise.\n\n14\n\n10−210−1100f10−2100102104PowerSpectralDensityColoredNoiseWhitenoise(β=0)Pinknoise(β=1)Rednoise(β=2)Truef−β10−210−1100fBrownianMotion,OUNoise,RedNoiseIntegratedWNOUnoise(θ=0.15)Rednoise(β=2)Truef−202505007501000t−3−2−10123εtWhitenoise(εt∼N(0,1))02505007501000tPinknoise(ε1:T∼CNT(1))02505007501000tOUnoise(ε1:T∼OUT)00.20.40.60.8DensityPublished as a conference paper at ICLR 2023\n\nAppendix\n\nwhere wt is a Wiener process (integrated white noise with the property that w(t)−w(t(cid:48)) ∼ N (0, t−t(cid:48)) for any 0 ≤ t(cid:48) < t). This definition of Ornstein-Uhlenbeck noise is equivalent to the Langevin equation (3) in the main text, but is nicer to work with, as the white noise process ηt is ill-defined as the derivative of the Wiener process. We sample OU noise signals by discretizing the equation above:\n\n(6) where ε ∼ N (0, 1). Denoting xt := x[t∆t] (with x−1 = 0) and εt ∼ N (0, 1) for all t ∈ N0, it can be seen that\n\nx[t + ∆t] = x[t] − θx[t]∆t + σ\n\n∆tε,\n\n√\n\nx0 = σ\n\n∆tε0\n\nx1 = x0 − θx0∆t + σ\n\n∆tε1\n\n√\n\n√\n\n∆t(1 − θ∆t)ε0 + σ ∆t(1 − θ∆t)2ε0 + σ\n\n∆tε1 √\n\n∆t(1 − θ∆t)ε1 + σ\n\n√\n\n∆tε2\n\n√\n\n√\n\n√\n\n= σ\n\nx2 = σ ... xt = σ\n\n√\n\n∆t\n\nt (cid:88)\n\nτ =0\n\n(1 − θ∆t)t−τ ετ .\n\nThus, as a sum of zero-mean Gaussian distributions, the marginal distribution is:\n\n√\n\n(cid:32)\n\n∆tN\n\n0,\n\np(xt) = σ\n\n(cid:33)\n\n((1 − θ∆t)t−τ )2\n\nt (cid:88)\n\nτ =0\n\n(cid:32)\n\n= N\n\n0, σ2∆t\n\nt (cid:88)\n\n(cid:33)\n\n(1 − θ∆t)2τ\n\n.\n\nτ =0 The variance of this distribution is a geometric series which converges as t → ∞ if (1 − θ∆t)2 < 1, which holds if 0 < θ∆t < 2. It is interesting to note that if θ∆t = 1, then Eq. (6) yields white noise, as it reduces to xt = σ ∆tε. On the other hand, if θ∆t = 0, the equation describes integrated white noise (Brownian motion), which is known to have unbounded variance. If 1 < θ∆t < 2, then the signal exhibits negative temporal correlation, which follows from Eq. (6). If the geometric series converges, then the limiting variance is given by\n\n√\n\nσ2∆t 1 − (1 − θ∆t)2 .\n\nWe can thus ensure a standard Gaussian marginal distribution (in the limit) by setting the noise scale to a “corrected” value of\n\n(cid:114)\n\nσ =\n\n1 − (1 − θ∆t)2 ∆t\n\n,\n\n(7)\n\nwhich is how we set the OU noise scale throughout the paper to make the comparison with white and colored noise fair. In Fig. A.3, it can be seen that this limiting marginal distribution is reached fairly quickly. In Sec. B, we also report Ornstein-Uhlenbeck results with the more common choice of σ = 1, which we find to generally perform slightly worse (cf. Fig. B.1).\n\nIf the variance is corrected, then θ∆t is the only parameter of OU noise, such that e.g. (θ = 0.3, ∆t = 1) is equivalent to (θ = 30, ∆t = 0.01). This immediately follows by plugging Eq. (7) into Eq. (6), yielding\n\nxt+1 = (1 − θ∆t)xt + (cid:112)1 − (1 − θ∆t)2εt, which only contains the product θ∆t as a parameter. In this paper we thus set ∆t = 0.01 without loss of generality. In the main text we also only consider OU noise as a replacement for strongly correlated Brownian motion and always set θ = 0.15, as this is the most common default setting used in practice.9 However, as noted in the discussion above, Ornstein-Uhlenbeck noise can also exhibit intermediate temporal correlation between white noise and Brownian motion, by setting 0 < θ < 100 (i.e. 0 < θ∆t < 1). This raises the question of whether there is a certain parameterization of OU noise which is as general as pink noise.\n\n9We chose these values for ∆t and θ because these are the default choices the RL libraries we consider (Raffin et al., 2021; Pardo, 2020). Lillicrap et al. (2016) also recommend θ = 0.15. If the variance is not corrected (we report these experiments in Sec. B), then the choice of ∆t does make a difference.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nFigure A.4: Left: Power spectral densities of OU noise. OU noise interpolates between white noise and Brownian motion by changing the cutoff frequency of a low-pass filter which filters white noise. Center: Entropy achieved by OU noise of different θ on the bounded integrator environment. No θ achieves a higher worst-case entropy than pink noise. Right: Energy achieved by OU noise of different θ on the harmonic oscillator environment. No θ achieves a worst-case energy that comes close to the one of pink noise.\n\nA.2 GENERALITY OF ORNSTEIN-UHLENBECK NOISE\n\nThe way in which OU noise interpolates between white noise and Brownian motion by choosing θ ∈ (0, 100) is very different to colored noise with β ∈ (0, 2). We have shown (e.g. in Fig. A.2) that colored noise with intermediate temporal correlation has a power-law power spectral density with intermediate exponent (or slope in the log-log plot). On the other hand, Ornstein-Uhlenbeck noise can be interpreted as a “leaky integration” of white noise, i.e. white noise passed through a low-pass filter. How “leaky” this integrator is, is controlled by the parameter θ: if θ = 0 then the integrator is ideal, resulting in integrated white noise (Brownian motion with diverging variance). If θ = 100 (with ∆t = 0.01), then the integrator is “completely leaky” (an all-pass filter) and the white noise passes through without being integrated. In terms of the power spectral density this change in θ corresponds to shifting the cutoff frequency of the low-pass filter. This is shown on the left in Fig. A.4 for θ ∈ {0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100}.\n\nFor an action noise type to be general (cf. Sec. 6), we want it to work well on all environments. In the power spectral density plots, it can already be seen that pink noise distributes power over the frequencies much more “generally” than Ornstein-Uhlenbeck noise of any θ: At any given frequency f , pink noise exhibits higher power than most values of θ, and all values of θ have lower power than pink noise at most frequencies. Why this makes pink noise a more general action noise can be made more concrete by revisiting the bounded integrator and harmonic oscillator environments introduced in Sec. 6. The generality of a noise measures how robust it is to the choice or parameterization of the environment: The most general noise type is the one which performs best on the most adversarial environment parameterization. Thus, the most general θ for an environment parameterized by a parameter α solves the following optimization problem:\n\nmax θ\n\nmin α\n\nperf(α, θ),\n\nwhere the performance metric perf(α, θ) should be normalized appropriately such that the maximum performance attainable for different values of α is identical. This can be ensured by simply dividing by the performance attained by the best θ for each value of α:\n\nmax θ\n\nmin α,θ(cid:48) (cid:124)\n\nperf(α, θ) perf(α, θ(cid:48)) (cid:125)\n\n(cid:123)(cid:122) generality(θ)\n\n.\n\n(8)\n\nThis gives the worst-case performance of the most general noise in terms of the best possible performance achievable by changing the noise type on this worst-case environment. By replacing the expression perf(α, θ) by perf(α, pink) and removing the maximization over θ, we can also calculate the generality of pink noise.\n\n16\n\n10−310−1f10−310−1101103PowerSpectralDensityPink104105106Area2468BoundedIntegrator(H/nats)Pink10−310−210−1Resonantfrequencyf10−310−210−1Oscillator(E/harmf)Pink10−210−1100101102θPublished as a conference paper at ICLR 2023\n\nAppendix\n\nAs discussed in Sec. 6, the performance of a noise type on the bounded integrator and oscillator environments is given by the achieved entropy and energy, respectively. This is shown for all values θ (as well as for pink noise) in Fig. A.4, where the parameterization parameter α is the environment size for the bounded integrator and the resonant frequency for the harmonic oscillator. It can already be seen that on both environments, for each choice of θ there exists a parameter α where the performance of θ is worse than the worst-case performance of pink noise. This can be quantified by calculating the generality of each θ and pink noise on these environments according to Eq. (8). On the bounded integrator, the maximum generality of OU noise is 77%, and on the oscillator environment the maximum generality is 9.1%. On both environments, the maximum is attained by θ = 3. Pink noise achieves generalities of 79% and 22% on the bounded integrator and oscillator environments, respectively. This gives further evidence that pink noise is a good default.\n\nB ADDITIONAL RESULTS\n\nB.1 TD3\n\nIn addition to MPO and SAC, we also performed all experiments from the main text on TD3. MPO and SAC parameterize a stochastic policy, meaning they learn the action noise scale as a function σ(s) of the state. TD3, on the other hand, uses a deterministic policy, and the action noise is added independently of the state. Usually, the noise scale σ is kept fixed over the course of training, and this how we handle it in our experiments as well. However, σ is an important hyperparameter, and there is no single value that works well on all environments. Thus, we repeat our experiments with all of the values σ ∈ {0.05, 0.1, 0.3, 0.5, 1}, and 10 different random seeds.\n\nIn Fig. B.1, the results of the TD3 experiments with constant noise type are shown in the form of bootstrap distributions for the expected average performance, and compared to the same experiments on MPO and SAC, as well as to a Fig. 3-like plot where the influence of the agent has been normalized out. As we have an additional hyperparameter (σ), we first average the TD3 performance over all σ values, before computing the average performance across tasks. The beneficial effect of pink noise\n\nFigure B.1: All three algorithms (MPO, SAC, TD3) show a clear preference for pink action noise as measured by the average performance over the environments of Fig. 2. The results of the OU experiments with the uncorrected noise scale of σ = 1 are marked with a dotted median.\n\n17\n\n−1.5−1.0−0.50.00.51.0AveragePerformanceMPO+SAC+TD3MPOWN0.10.20.350.50.751.01.52.0OUβ−1.5−1.0−0.50.00.51.0AveragePerformanceSACWN0.10.20.350.50.751.01.52.0OUβTD3Published as a conference paper at ICLR 2023\n\nAppendix\n\nFigure B.2: Average performances across environments are combined from all β values (incl. WN). It can be seen that TD3 is consistently outperformed by both MPO and SAC. A closer look at the mean performance over all β values on each individual environment reveals that TD3 is outperformed on all environments by both MPO and SAC.\n\ncan be clearly seen on TD3 as well. In this figure we also show the results of Ornstein-Uhlenbeck noise with a noise scale of σ = 1 rather than the corrected noise scale of Eq. (7). Incidentally, these results also confirm Fujimoto et al. (2018)’s finding that, on TD3, white noise and OU noise (with θ = 0.15) perform similarly.\n\nThe reason why we did not include TD3 into the analysis of the main text, is that we found TD3 to be consistently outperformed by both MPO and SAC. In Fig. B.2, the average performances across environments are combined from all β values (incl. white noise), and shown for MPO, SAC and TD3. It can be seen that TD3 generally performs much worse than MPO and SAC. Looking at the mean performance over all β values on each individual environment, TD3 is outperformed on all environments by both MPO and SAC. We thus decided to exclude TD3 from our main analysis.\n\nB.2 MPO & SAC\n\nIn the majority of this work, we measure performance in terms of the mean evaluation return over a training process. We use this method, because it implicitly measures both the final policy performance, and the sample efficiency (how quickly does the algorithm reach high performance). Most of the data we present is additionally normalized, which is necessary to aggregate performances over different environments, and thus it is often not very clear how exactly to interpret the results (other than recognizing statistical significance). In this section, we want to present some of our results in more familiar terms, namely learning curves and final policy performance.\n\nTo validate the approach of using the (mean) performance instead of the performance of the final policy, we have reproduced the results in Fig. 3 using the final policy performance (mean evaluation return in the last 5% of the training process), shown in Fig. B.3. In Fig. B.4, we show learning curves of white noise, pink noise, and OU noise on all environments for MPO and SAC. Both visualizations confirm our takeaway that pink noise is a better default action noise than white noise or OU noise. More detailed results can be found in Sec. H.\n\nThe bootstrap distributions for the expected average performance (such as in Figures 3, 4, B.1, and B.3) are constructed by randomly choosing one seed for each environment, yielding one scalar\n\nFigure B.3: The average final performance is like the average performance (see Sec. 4), but only uses the evaluation returns of the last 5% of training, thereby measuring the quality of the final learned policy. This figure shows the same analysis on MPO and SAC as Fig. 3, and demonstrates that pink noise is preferable also in terms of final policy performance.\n\n18\n\n−4−2024AveragePerformanceMPOSACTD3WN0.10.20.350.50.751.01.52.0OUβ−0.8−0.6−0.4−0.20.00.20.4AverageFinalPerformancePublished as a conference paper at ICLR 2023\n\nAppendix\n\nFigure B.4: Learning curves (median and interquartile range of evaluation returns) of the two baseline action noise types white noise (WN) and Ornstein-Uhlenbeck (OU) noise, as well as our suggestion of pink noise. It can be seen that pink noise, while not being better than both on all environments, is the best default choice. It is never outperformed by both white noise and OU noise, and routinely outperforms white noise (e.g. MountainCar), OU noise (e.g. Door), or both (e.g. Hopper).\n\n(normalized) performance per environment, assuming all other variables like algorithm and noise type are fixed. Averaging these normalized performances (the reason that performances are normalized on each environment is so that this averaging is reasonable) gives an estimate for the average performance across environments of the given variables (e.g. noise type and algorithm). As there are S different random seeds (typically S = 20), we can repeat this procedure S times (with resampling) and take the mean of all S average performance estimates, giving us an estimate for the expected average performance of the given variables. Doing this N times (we use N = 105), the N estimates for the expected average performance can be collected into a bootstrap distribution, as shown in these figures.\n\n19\n\n0200400600800ReturnMPOSAC02505007501000MPOSACWNOUOU(σ=1)Pink0200400600800ReturnMPOSAC02505007501000MPOSAC−50050100ReturnMPOSAC050100150MPOSAC0200400600ReturnMPOSAC02505007501000MPOSAC0.00.51.0Interactions×1060200400600ReturnMPO0.00.51.0Interactions×106SAC0.00.51.0Interactions×1060100020003000MPO0.00.51.0Interactions×106SACPendulumCartpole(b.)Cartpole(s.)Ball-In-CupMountainCarHopperWalkerReacherCheetahDoorPublished as a conference paper at ICLR 2023\n\nAppendix\n\nC ENVIRONMENTS & ALGORITHMS\n\nWe evaluate our method on 10 different tasks (see Fig. 2). Most of these are from the DeepMind Control Suite (DMC, Tassa et al., 2018), but we also use OpenAI Gym (Brockman et al., 2016) and the Adroit hand suite (Rajeswaran et al., 2018). The respective sources and exact IDs of all environments are compiled in Table C.1. See Sec. G for results on additional tasks.\n\nEnvironment\n\nSource\n\nID\n\nDMC Pendulum DMC Cartpole (b.) DMC Cartpole (s.) Ball-In-Cup DMC MountainCar Gym DMC Hopper DMC Walker DMC Reacher DMC Cheetah Adroit Door\n\npendulum (swingup) cartpole (balance_sparse) cartpole (swingup_sparse) ball_in_cup (catch) MountainCarContinuous-v0 hopper (hop) walker (run) reacher (hard) cheetah (run) door-v0\n\nTable C.1: Environments used in this work (see also Fig. 2).\n\nFor our experiments, we relied on the TD3 and SAC implementations in Stable-Baselines3 (Raffin et al., 2021), as well as the MPO implementation in the Tonic RL library (Pardo, 2020). We only used the default hyperparameters of these algorithms, as provided by the libraries. Our own code for using colored noise with these libraries is made available online at https://github.com/ martius-lab/pink-noise-rl.\n\nD BANDIT METHOD DETAILS\n\nTo use a bandit algorithm to select the action noise color β for a rollout, it is necessary to define the bandit reward, which should score a rollout in terms of the β that was chosen. In our case, we use the rollout return (sum of rewards) as the score, as explained in Sec. 5.2. Additionally, we have to select a list of colors (“bandit arms”) to search over: B = (β1, β2, . . . , βK) (with βk ∈ [0, 2], ∀k in our case). If we assume that the bandit rewards (= rollout scores) are Gaussian distributed with a known standard deviation σ, we can use Bayesian inference to estimate the means (μ ∈ RK) of the reward distributions. A simple bandit algorithm we can use in this context is Thompson sampling, shown in Algorithm D.1 (SK the random variables are shown in the Bayesian network in Figure D.1a.\n\n{aj, rj}i\n\nend\n\nj=1, σ\n\nAlgorithm D.1: Thompson Sampling Input: Arms B = (β1, . . . , βK), Reward distributions std σ\n\nInitialize m ∈ RK, Σ ∈ SK for i ∈ N do\n\n+\n\nSample q ∼ N (m, Σ) ai ← arg maxk∈{1,...,K} qk τi ← Run rollout with βai ri ← score of rollout τi Do Bayesian update of m, Σ using\n\n+ denotes the set of positive semi-definite K × K matrices). The relationships between\n\nThere is a second strong assumption in the Thompson sampling algorithm shown in Algorithm D.1 (similarly for other algorithms like UCB): it assumes that the reward distributions are stationary, i.e. that they don’t change over time. This is not the case in the context of reinforcement learning: if the rollout score ri is defined as the return, then, if the reinforcement learning algorithm works, it should naturally be the case that the policy improves over time, and thus, on average, ri > rj for i (cid:29) j. This setting of non-stationary bandit distributions can be addressed by using a sliding-window approach (e.g. Garivier & Moulines, 2008): instead of updating the belief parameters m, Σ with respect to the whole history of observations, only keep a window of the last N rollouts.\n\nThere remains one other problem: how do we choose the prior parameters m and Σ and the variance σ2 of the reward distributions? For Σ, the easiest solution is to assume independent arms, i.e. make\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nμ\n\nμai\n\nσ\n\nN\n\nri\n\nμ\n\nμai\n\nN\n\nm, Σ\n\nai\n\n(a)\n\ni ∈ N\n\nN\n\nm, Σ\n\nai\n\nσ\n\nN\n\n ̃ri\n\nri\n\nb\n\nc\n\ni = 1, . . . , N\n\n(b)\n\nFigure D.1: (a) A Bayesian bandit with Gaussian reward distributions. The rewards from arm k are sampled from N (μk, σ). Thompson sampling (Alg. D.1) can infer μ while trading off exploration and exploitation. (b) By introducing the constants b and c, the algorithm can be made scale invariant by performing Thompson sampling with respect to the normalized reward ̃ri = (ri − b)/c.\n\nΣ diagonal. This is not necessarily the most efficient solution, as one can imagine that two similar β values will also perform similarly in their rollouts.10 For m, the non-stationarity becomes a problem: again assuming we use the rollout return as a score, these scores will probably be much lower at the beginning of training than at the end. Additionally, we might not even know the scale of returns in a task. To account for this, it would be necessary to make the prior variances Σkk very large/uninformed. Similarly, σ needs to be large, to account for the unknown scale of the bandit reward spread. However, this would mean that many more samples (rollouts) are necessary to tighten the belief distributions. This is a problem, especially because we only have a small set of N rollouts when using the sliding-window method.\n\nThe ideal would be a bandit method which is invariant with respect to affine transformations of the rewards, in the sense that it would make no difference if all rewards r were transformed to be br + c for some constants b > 0 and c ∈ R for all arms. In Fig. D.1b, this situation is shown in a Bayesian network. Here, the generative process is almost the same as before (see Fig. D.1a), except that the reward ̃ri is scaled and translated by ri = b ̃ri + c before observation. If, as shown, the constants b and c are independent of the chosen arm and stay constant within the window, it is possible to optimize them via maximum marginal likelihood, given the window of past observations of ri.\n\nThe bandit inference task is to infer the distributional means μ = (μ1, . . . , μk) from the actions (color indices) a = (ai)N i=1. We set the prior means of the belief distributions to 0 (m = 0), because we want the normalized reward distributions to be centered around 0. For now, we don’t fix Σ, but let it be any positive semi-definite K × K matrix. The generative model for r is defined via the following prior and likelihood function:\n\ni=1 and rewards (rollout scores) r = (ri)N\n\np(μ | Σ) = N (μ | 0, Σ)\n\np(r | μ, a, b, c, σ) =\n\nN (ri | bμai + c, (bσ)2)\n\n(cid:89)\n\ni\n\nThese lead us to the following evidence/marginal likelihood function:\n\np(r | a, b, c, σ, Σ) =\n\n(cid:89)\n\np(ri | ai, b, c, σ, Σ)\n\ni (cid:89)\n\n(cid:90)\n\ni (cid:89)\n\n(cid:90)\n\np(ri | μ, ai, b, c, σ) p(μ | Σ) dμ\n\nN (ri | be(cid:62) ai\n\nμ + c, (bσ)2) N (μ | 0, Σ) dμ\n\ni (cid:89)\n\ni\n\nN (ri | be(cid:62) ai\n\n0 + c, (bσ)2 + be(cid:62) ai\n\nΣbeai)\n\n=\n\n=\n\n=\n\n(9)\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n10We also tried a different approach by using a modified RBF kernel matrix to account for covariance between\n\nthe arms, but the results were essentially the same as with independent arms.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\n(cid:89)\n\n=\n\ni\n\nN (ri | c, b2(σ2 + Σaiai)),\n\n(15)\n\nwhere we used canonical basis vectors to represent μai = e(cid:62) to work with the log-evidence:\n\nai\n\nμ. For maximization, it is convenient\n\nlog p(r | a, b, c, σ, Σ) = log\n\n(cid:89)\n\nN (ri | c, b2(σ2 + Σaiai ))\n\ni\n\n−\n\n(cid:88)\n\n=\n\ni =: L(b, c)\n\n1 2\n\nlog(cid:0)2πb2(σ2 + Σaiai )(cid:1) −\n\n(c − ri)2 2b2(σ2 + Σaiai)\n\nWe can now maximize the evidence by setting the partial derivatives to 0:\n\n∂cL(b, c) ∝\n\n∂bL(b, c) =\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(c − ri) = 0\n\n−1 b\n\n+\n\n(c − ri)2 b3(σ2 + Σaiai)\n\n= 0\n\nSolving these equations gives us\n\nc =\n\nb2 =\n\n1 N\n\n1 N\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\nri\n\n(c − ri)2 σ2 + Σaiai\n\n.\n\nUsing these values, we can “reconstruct” the unscaled/normalized reward\n\n ̃ri =\n\nri − c b\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\nand perform Thompson sampling with respect to ̃ri. This normalized Thompson sampling algorithm, including the sliding window modification, is presented in Algorithm D.2.\n\nAlgorithm D.2: Normalized TS Input: Arms B = (β1, . . . , βK),\n\nWindow size N\n\nInitialize\n\nfor l ∈ N do\n\nm ← 0 ∈ RK, Σ ∈ SK\n\n+ , σ ← 1\n\ni ← l mod N M ← min{l, N } Sample q ∼ N (m, Σ) ai ← arg maxk∈{1,...,K} qk τi ← Run rollout with βai ri ← score of rollout τi c ← 1 M\n(cid:114)\n\n(cid:80)M\n\nj=1 rj (cid:80)M\n\nb ←\n\n(c−rj )2 σ2+Σaj aj\n\n1 M\n\nj=1\n\n ̃ri ← ri−c Do Bayesian update of m, Σ using\n\nb\n\n{aj, ̃rj}M\n\nj=1, σ\n\nend\n\nNext, we want to show that this method is indeed invariant to affine transformations of the bandit reward.\n\nProposition 1. The posterior distribution over μ in the normalized bandit algorithm (Alg. D.2) is identical for the observations r = (r1, . . . , rN ) and r(cid:48) = b(cid:48)r + c(cid:48), for all b(cid:48) > 0 and c(cid:48) ∈ R. In other words, the algorithm is invariant to a scaling and translation of the rewards.\n\nProof. In this setting, the observed rewards ri are normalized to\n\n ̃ri =\n\nri − c(r) b(r)\n\nwith\n\nc(r) =\n\nb(r) =\n\n1 N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\nri\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(c(r) − ri)2 σ2 + Σaiai\n\n.\n\n(24)\n\n(25)\n\n(26)\n\nTo prove the invariance of the algorithm, we will simply show that this normalized reward is the same for both sets of observations, i.e. that ̃r = ̃r(cid:48). Then, clearly, the posteriors p(μ | ̃r) and p(μ | ̃r(cid:48)) will also be the same. Expanding ̃r(cid:48), we get:\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\n ̃r(cid:48) =\n\nr(cid:48) − c(r(cid:48)) b(r(cid:48))\n\n=\n\n=\n\n=\n\n=\n\nb(cid:48)r + c(cid:48) − c(b(cid:48)r + c(cid:48)) b(b(cid:48)r + c(cid:48))\n\nb(cid:48)r + c(cid:48) − 1 N\n\n(cid:80)N\n\ni=1(b(cid:48)ri + c(cid:48))\n\nN (cid:88)\n\n( 1 N\n\n(cid:80)N\n\nj=1(b(cid:48)rj + c(cid:48)) − (b(cid:48)ri + c(cid:48)))2\n\ni=1\n\nσ2 + Σaiai\n\nb(cid:48)r + c(cid:48) − b(cid:48) 1 N\n\n(cid:80)N\n\ni=1 ri − c(cid:48)\n\nN (cid:88)\n\n(b(cid:48) 1 N\n\n(cid:80)N\n\nj=1 rj + c(cid:48) − b(cid:48)ri − c(cid:48))2\n\nσ2 + Σaiai\n\ni=1 b(cid:48)(r − c(r))\n\nN (cid:88)\n\ni=1\n\nb(cid:48)2(c(r) − ri)2 σ2 + Σaiai\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n1 N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n1 N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n1 N\n\n=\n\nr − c(r) b(r)\n\n= ̃r\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n(33)\n\nThus, we can conclude that the reward normalization indeed guarantees invariance to affine reward transformations in algorithms such as Thompson sampling.\n\nWith this reward normalization, the prior parameters m (of m = m1) and s (of Σ = s2I) become redundant. We have already set m = 0, and we now also set the prior variances Σkk to 1. This encourages the algorithm to keep the normalized mean estimates μk approximately N (0, 1)-distributed. The “likelihood” parameter σ remains to be tuned, but it is now not necessary to account for the large uncertainty in the reward scale, as σ is only concerned with the normalized reward. In our experiments we always set σ = 1.\n\nD.1 BANDIT VS. RANDOM\n\np\n\nEnvironment\n\nAlthough we found the normalized bandit algorithm (Alg. D.2) to work well on simple non-stationary tasks, in the RL setting (for choosing β) the performance was just as that of a random β selection for every rollout. In Table D.1, we list the results of a Welch t-test, testing for inequality of the performance distributions achieved by the bandit algorithm and random β selection on every environment. It can be seen that the two methods are statistically indistinguishable. This shows that the bandit method does not work as intended, as “random arm selection” should be an easy baseline to outperform. The reason for this is probably due to the rollout return not being informative enough as a bandit reward signal.\n\nBandit (cid:54)= Random (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55)\n\nPendulum Cartpole (b.) Cartpole (s.) Ball-In-Cup MountainCar Hopper Walker Reacher Cheetah Door\n\n0.98 0.09 0.67 0.87 0.54 0.09 0.15 0.70 0.20 0.59\n\nTable D.1: Bandit vs. Random (Welch t-test)\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nE SOLVING MOUNTAINCAR BY FFT\n\nMountainCar is a very simple environment. Although its dynamics are almost those of a harmonic oscillator, there is a difference to the oscillator environment from Sec. 6: MountainCar’s oscillation dynamics are non-linear. At the bottom of MountainCar’s valley (see Fig. 2), the small-angle approximation of a non-linear oscillator may be used, but for the motion to go up to the top, the behavior is different from simple harmonic motion. Nevertheless, we can use this insight to develop a very simple open-loop control algorithm to solve this environment, by running one rollout without applying any action (just letting the mountain make the car go back and forth a bit), then analyzing the resulting trajectory and inferring the hill’s (small-angle) resonant frequency (via the Fast Fourier Transform algorithm). Finally, we can control the car by simply swinging it back and forth at the resonant frequency. This algorithm, which works very well on this task, is shown below.\n\nimport gym import numpy as np from scipy.fft import rfft\n\n# Initialize environment env = gym.make('MountainCarContinuous-v0') T = env._max_episode_steps\n\n# Run a single rollout with no force. Save x-coordinate to `x`. obs = env.reset() x = [obs[0]] for t in range(T):\n\nobs, *_ = env.step([0]) x.append(obs[0])\n\n# Find resonant frequency = highest peak of FFT (excluding DC) f = (np.argmax(abs(rfft(x))[1:]) + 1) / (T + 1)\n\n# Action plan (harmonic excitation) a = np.sin(2*np.pi*f * np.arange(T))\n\n# Test on 1000 rollouts N = 1000 solved = 0 for i in range(N): env.reset() for t in range(T):\n\n_, r, _, _ = env.step([a[t]]) if r > 0:\n\nsolved += 1 break\n\nprint(f\"Solved: {solved/N * 100:.0f}%.\") # prints \"Solved: 100%.\"\n\nF TOY ENVIRONMENT DETAILS\n\nF.1 OSCILLATOR ENVIRONMENT\n\nThe oscillator environment of Sec. 6, which we make available online as a gym environment (https://github.com/onnoeberhard/oscillator-gym), models the 1-dimensional motion of a particle of mass m, attached to the origin by an ideal spring of stiffness k, damped with friction coefficient b, and driven by a force (the action) F . This motion is described by the ordinary differential equation\n\nm ̈x = F − b ̇x − kx,\n\n(34)\n\nwhere x is the particle’s position. In our experiments we set the friction coefficient b to zero, i.e. the system is undamped. This setup is then called a simple harmonic oscillator. The energy of the\n\n24\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\noscillator is the sum of kinetic and potential energy:\n\nThe resonant frequency is:\n\nE =\n\n1 2\n\nm ̇x2 +\n\n1 2\n\nkx2.\n\nf =\n\n(cid:114)\n\n1 2π\n\nk m\n\n.\n\n(35)\n\n(36)\n\nAs we want to configure the oscillator to have a given resonant frequency f , we need to find m and k accordingly. To get a unique solution, we impose a second constraint: the energy at x = 1 and ̇x = 0 should be E = 2π2. If we now solve the two equations (35) and (36) for m and k, imposing the constraint on E, we get the solution\n\nk = 4π2\n\nm =\n\n1 f 2\n\n(37)\n\n(38)\n\nto set the resonant frequency. In Fig. F.1, a few pure-noise trajectories (akin to Fig. 1) are shown on the oscillator environment.\n\nFigure F.1: Trajectories on the oscillator environment. For each of the 3 resonance frequencies f ∈ {0.002, 0.02, 0.2}, we sample 5 action noise signals of length 10 f of white noise, pink noise and OU noise. We can see what was already shown in Fig. 5: pink noise is much less sensitive to the parameterization than white noise and OU noise, and always manages to excite the oscillator up to a certain amplitude. White noise and OU noise only work well in the high- and low-frequency regime, respectively.\n\n25\n\n−2.50.02.5−0.04−0.020.000.020.04f=2×10−3 ̇xWhitenoise−2.50.02.5Pinknoise−2.50.02.5OUnoise05−0.6−0.4−0.20.00.20.4f=2×10−2 ̇x0505−10010x−15−10−5051015f=2×10−1 ̇x−10010x−10010xPublished as a conference paper at ICLR 2023\n\nAppendix\n\nF.2 BOUNDED INTEGRATOR ENVIRONMENT\n\nThe bounded integrator environment of Sec. 6 has very simple dynamics:\n\nst+1 = clip(st + at, −c, c),\n\n(39)\n\nwhere s0 = 0 and c is the parameter determining the size of the environment. Thus, the “area” in Fig. 5 is given by c2. In Fig. 1, this parameter is fixed at c = 250, and in Fig. F.2 these trajectories (center row) are compared to trajectories on a smaller (top row) and a larger environment (bottom row), in a similar spirit to Fig. F.1.\n\nFigure F.2: Trajectories on the bounded integrator environment. For each of the 3 environment sizes c ∈ {25, 250, 1000}, we sample 20 action noise signals of length 1000 steps of white noise, pink noise, and OU noise. We can see what was already shown in Fig. 5: pink noise is less sensitive to the parameterization than white noise (which is too slow to explore the medium and large environments) and OU noise (which, on the medium and small environments, gets stuck at the edges and fails to explore the interior).\n\nG ADDITIONAL ENVIRONMENTS\n\nIn addition to the environments described in Sec. C, we also ran experiments on several tasks from the “MuJoCo-Maze” suite (https://github.com/kngwyu/mujoco-maze). The results are shown in Fig. G.1 in the form of learning curves and the average performance (cf. Sec. B.2) of each noise type over all six environments. We tested white noise, pink noise, and (variance-corrected, cf. Sec. A.1) Ornstein-Uhlenbeck noise, and trained MPO and SAC on all environments for 106 interactions using 20 seeds. Pink noise again outperforms white noise and Ornstein-Uhlenbeck noise as a default choice across environments. These experiments were conducted to verify our method on\n\n26\n\nc=25WhitenoisePinknoiseOUnoisec=250c=1000Published as a conference paper at ICLR 2023\n\nAppendix\n\nFigure G.1: Performances of white noise, pink noise, and OU noise on several “MuJoCo-Maze” tasks (all with sparse rewards). It can be seen that pink noise is the best default choice of the three, as it has the highest average performance (Sec. B.2).\n\na different set of problems and are thus not included in the main analysis. The results provide further evidence in support of our main takeaway: pink noise makes a very good default action noise.\n\nH DETAILED RESULTS\n\nEnvironment Agent WN\n\nOU Pink WN\n\nOU Pink Oracle Anti Gain\n\nFinal Performance\n\nMean Performance\n\nPendulum\n\nMPO SAC Cartpole (b.) MPO SAC Cartpole (s.) MPO SAC Ball-In-Cup MPO SAC MountainCar MPO SAC MPO SAC MPO SAC MPO SAC MPO SAC MPO SAC\n\nCheetah\n\nReacher\n\nHopper\n\nWalker\n\nDoor\n\n311 224 999 960 703 377 974 976 13 0\n25 89 530 593 956 955 666 631 2586 2192\n\n702 350 1000 908 784 608 973 975 56 90 62 94 377 506 856 914 612 577 2492 1535\n\n574 446 1000 958 788 730 978 979 92 94 108 119 448 602 966 940 678 640 2909 2195\n\n247 158 928 939 535 226 926 930 13 0\n14 43 384 437 864 776 481 469 1830 1332\n\n651 283 940 890 499 459 909 901 52 89 34 53 284 363 600 653 440 439 1376 546\n\n558 294 967 941 666 532 948 933 91 93 69 77 363 471 871 745 543 483 2207 1183\n\n670 361 967 950 666 533 948 941 92 93 69 80 390 472 888 776 543 502 2207 1332\n\n239 158 928 890 489 159 909 901 13 0\n14 43 284 363 581 653 440 439 1376 546\n\n430 202 39 59 177 374 39 39 78 93 54 36 106 108 306 122 103 63 830 785\n\nTable H.1: Comparison of final policy performance (see Sec. B.2) and mean performance over the training process (Sec. 4) on all environments. Results are averaged across seeds, and shown for white noise (WN), Ornstein-Uhlenbeck noise (OU), and pink noise (Pink) as action noise on MPO and SAC. Additionally, the Oracle and Anti-Oracle (“Anti”) performances are shown. The gain between these (rightmost column) represents the difference achievable by changing the noise type, and is the basis for the “performance gain” measure used in Sec. 4.2.\n\n27\n\n01ReturnMPOWNOUPinkSACMPOSAC020ReturnMPOSACMPOSAC01t×106020ReturnMPO01t×106SAC01t×106MPO01t×106SACPoint,U-MazePoint,4-RoomsSwimmer,U-MazeSwimmer,4-RoomsAnt,U-MazeAnt,4-RoomsWNPinkOU−1.0−0.8−0.6−0.4−0.20.00.20.40.6Avg.performance",
    "reference": "# Summary Of The Paper\n\nThe work presents a thorough investigation of the type of used action noise in reinforcement learning.\n\nThe typically used noise types are either Gaussian noise or Ornstein-Uhlenbeck noise (OU), which is temporally correlated.\nThese two noise types belong to a family of colored noise, where Gaussian noise is white noise and OU noise is red noise.\nThe family of noise has a frequency spectrum proportional to $f^{-\\beta}$, where $\\beta=0$ corresponds to white noise (a flat frequency spectrum) and $\\beta = 2$ corresponds to OU noise (the frequency power spectrum decays). In the work they consider other noise types with $0 < \\beta < 2$. In particular, they determined that $\\beta=1$ (pink noise) performs well as a default setting.\nThe noise can be generated using an FFT based algorithm prior to starting the episode, stored into a vector and then selected from the vector (this is a sensible procedure and is often also used for other noise types).\n\nThey performed experiments across 10 different tasks taken from DMControl Suite, OpenAI Gym and Adroit hand Suite. They tested different noise types for the SAC, MPO and TD3 algorithms. Experiments were run with 20 seeds. \nTo aggregate the performance across tasks, they normalized the performance in each task to have 0 mean and variance 1.\nPink noise had the best average performance across all tasks. Moreover, pink noise achieved the highest score in 3/10 tasks and the difference to the best was statistically insignificant in 5/10 of the remaining tasks. Meaning that pink noise was comparable to the best in 8/10 tasks. The difference in performance for pink noise in the remaining two tasks was also low, and the work included much discussion and further analysis of these results.\n\nIn addition the work tested scheduling the noise color schedule, as well as selecting the noise type using a bandit algorithm. Neither of these was significantly better than simply using a fixed $\\beta=1$ pink noise.\n\nSection 6 included an intuitive explanation of why pink noise might be better than other noise types, e.g., Figure 4 showed that in a random walk, Gaussian noise does not disperse much from the center, OU noise quickly leaves the center and gets stuck in the edges of the environment, while pink disperses from the center while traveling through a greater range of the intermediate distances.\n\nThe analysis indicated that pink noise is a better default setting for the noise compared to the previous Gaussian and OU noises.\n\n# Strength And Weaknesses\n\n**Strengths:**\n- The experimental work was solid and thorough doing a proper statistical analysis of the results. I believe the results are convincing.\n- Noise is added into most RL algorithms, so this work is applicable to a wide range of researches.\n- The paper was well written and included interesting explanations.\n\n**Weaknesses:**\n- If I have to name a weakness, I would say that ultimately the type of noise is an unalluring component of the RL system, and the specific type does not seem to make or break the performance of the algorithm (e.g., in Figure B.4 the other noise types also perform mostly fine). Even though pink noise statistically performs better, the improvement is not a conceptual advance in terms of performance.\n- I didn’t like the title and I would recommend changing it (this doesn't affect the review).\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n\nExceptionally good clarity. My only comment is that it may be good to say that $f$ is the frequency in Definition 1.\n\n**Quality**\n\nThe quality of the experimental analysis was very high.\n\n**Novelty**\n\nThe novelty is incremental.\n\n**Reproducibility**\n\nI believe the work is reproducible as sufficient details were provided. They also said that they will release the code.\n\n# Summary Of The Review\n\nThis is a well written paper on a topic that is relevant to many RL researches. The experimental work was thorough, and the discussion was interesting. Pink noise may become a default noise setting for future RL algorithms. I think this paper is a clear accept.\n\n**Update**\nI thank the authors for their response. I believe my assessment remains adequate, and I am keeping the score.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFLEXROUND: LEARNABLE ROUNDING BY ELEMENTWISE DIVISION FOR POST-TRAINING QUANTIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nPost-training Quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantizationaware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. We notice that, however, such new rounding schemes are established on element-wise addition. In this work, we propose a simple yet effective new rounding mechanism for post-training weight quantization, coined FlexRound, via element-wise division to learn not only a common quantization grid size but also a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit the importance of a pre-trained weight when updating its corresponding scale, and thus, flexibly quantize a pre-trained weight depending on its own importance. We empirically validate the efficacy of FlexRound on a wide range of models and tasks. To the best of our knowledge, our work is the first to carry out comprehensive experiments on not only image classification and natural language understanding but natural language generation in the per-tensor uniform PTQ setting. Our code will be open-sourced soon.\n\n1\n\nINTRODUCTION\n\nRecent years have witnessed the unprecedented success of deep neural networks in a wide variety of domains including computer vision, natural language processing, automatic speech recognition, and so on. Although state-of-the-art deep neural networks surpass human-level performance, these neural networks cannot help requiring more and more computation cost and memory usage as networks become deeper and wider. In order to reduce the model size and accelerate inference operations, many researchers have attempted diverse compression techniques such as network quantization (Courbariaux et al., 2016) and network pruning (Han et al., 2016). In this paper, we concentrate on network quantization due to the advantage that INT4 or INT8 quantization allows us to accelerate quantized neural networks using off-the-shelf accelerators such as the NVIDIA A100 Tensor Core GPU (Wu et al., 2020) or ARM Cortex MCUs (Kim et al., 2021).\n\nNetwork quantization techniques can be generally divided into two categories: quantization-aware training (QAT) and post-training quantization (PTQ). When quantizing neural networks via QAT (Jung et al., 2019; Jain et al., 2019; Zhao et al., 2020; Esser et al., 2020; Lee et al., 2021), the performance gap between a full-precision neural network and its quantized counterpart can be marginal. Yet, QAT requires end-to-end retraining or fine-tuning on a full training dataset, which often causes an enormous amount of time and resources to obtain a quantized neural network with competitive performance. Furthermore, a whole training dataset may not be available due to data privacy issues or demands to utilize legacy models. Such drawbacks of QAT are the reasons why\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nresearchers recently pay more attention to PTQ (Zhao et al., 2019; Wang et al., 2020; Nahshan et al., 2021) that needs neither a full training dataset nor end-to-end learning at all.\n\nPTQ had been initially performed via rounding-to-nearest scheme by minimizing the quantization error in the parameter space. Unfortunately, this approach suffers from severe performance degradation. Since it is reported that the loss degradation resulting from quantization can be approximated as the second-order error in Taylor Expansion by viewing quantized weights as perturbed weights, Nagel et al. (2020) and Li et al. (2021) substantiate that reconstructing each output of layer or block is equivalent to minimizing the approximation of loss degradation resulting from quantization under some assumptions. Accordingly, recent works (Nagel et al., 2020; Li et al., 2021; Hubara et al., 2021; Wei et al., 2022) have suggested to reconstruct each output of layer or block by devising and learning a new weight-rounding scheme, deviating from rounding-to-nearest, as an effort to preserve the performance of a full-precision model. However, all those new rounding schemes designed in existing studies either round or quantize pre-trained weights adaptively via element-wise addition.\n\nChanging the perspective of a new rounding policy from element-wise addition to element-wise division, we propose a simple yet effective post-training weight quantization method called FlexRound, which flexibly quantizes pre-trained weights by learning how much each pre-trained weight should be divided by. Interestingly, thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound can inherently leverage pre-trained weights when updating an individual scale for every pre-trained weight. Specifically, we corroborate that a relatively wider range of discrete values needs to be explored when quantizing pre-trained weights of large magnitude. The rationale behind such an approach is that the magnitude of weight can be considered as its importance. Given that it is crucial to retain the knowledge of important weights even after quantization so as to maintain the performance of a pre-trained model, the constraints associated with quantizing weights of large absolute value should be relaxed compared to those of small absolute value (i.e., those important weights can be quantized to one of not only its two nearest discrete values but also discrete ones far from it). Accordingly, FlexRound quantizes pre-trained weights flexibly depending on each their own importance, thereby leading to better performance.\n\nOur contributions are threefold:\n\n• We propose FlexRound as a new rounding scheme for post-training weight quantization based on the principle of element-wise division to enable learning separate scales for all pre-trained weights as well as a common quantization grid size across a group (e.g., a channel or a layer).\n\n• We demonstrate that such a new rounding scheme via element-wise division takes into consideration the importance of pre-trained weights when updating their corresponding scales so that FlexRound can quantize pre-trained weights of large magnitude (i.e., important pre-trained weights) more flexibly.\n\n• To the best of our knowledge, we are the first to conduct extensive experiments in the form of per-tensor uniform PTQ reconstruction on natural language generation as well as image classification and natural language understanding. We verify the effectiveness of FlexRound using numerous models such as ResNet, MobileNetV2, BERT, GPT-Neo, and OPT.\n\n2 RELATED WORK\n\nRecently, many researchers have attempted to quantize a wide range of models for various tasks such as vision and language understanding/generation without any (re)training. OCS (Zhao et al., 2019) replicates channels entailing outliers, and then, halves outliers of those channels. Unfortunately, even though OCS explicitly addresses outliers, it still suffers from severe accuracy degradation when both weights and activations are quantized into low-bit. As an alternative solution, Wang et al. (2020) proposed Bit-Split that splits an integer into several bits and optimizes them separately. Although Wang et al. (2020) showed that the performance of Bit-Split is close to that of a full-precision model in the low-bit setting, Bit-Split may not be effective for certain architectures including MobileNetV2.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTo overcome the limitations discussed above, Nagel et al. (2020) and Hubara et al. (2021) minimize the mean squared error (in a layer-by-layer fashion) between the full-precision layer’s output and its quantized layer’s output by inventing and learning a new weight-rounding mechanism dubbed as AdaRound and AdaQuant, respectively. As such a layer-wise reconstruction error minimization opens the door to 4-bit PTQ regime, Li et al. (2021) proposed block-wise reconstruction, titled BRECQ, to consider cross-layer dependency along with the possibility of fully quantizing MobileNetV2 into 4-bit. In addition to block-wise reconstruction, Wei et al. (2022) proposed QDrop that drops the quantization of activations at random during reconstruction to induce activation quantization to be synchronized with weight quantization. Both BRECQ and QDrop, however, are based on AdaRound, which cannot learn a quantization grid size while quantizing weights allows for rounding either up or down only at most. AdaQuant quantizes weights adaptively. AdaQuant, however, does not consider the magnitude of weights for quantization that turns out to be important as we discuss later.\n\nAs another line of post-training quantization (PTQ) research, some PTQ techniques are specialized in quantizing language models such as BERT and GPT-like models. Bondarenko et al. (2021) first applied PTQ to BERT by introducing per-embedding-group activation quantization scheme to deal with highly dynamic activation ranges. Bai et al. (2021) studied the PTQ reconstruction in parallel for BERT. Yao et al. (2022) proposed ZeroQuant that quantizes BERT and GPT-3 in group-wise weight quantization manner driven by token-wise activation quantization via layer-by-layer knowledge distillation. Dettmers et al. (2022) quantizes large language models like OPT with vector-wise weight quantization and mixed-precision decomposition with FP16 activation. All those methods do not consider per-tensor weight quantization which can enable integer matrix-to-matrix multiplication API/function calls (Migacz, 2017).\n\nMost of the aforementioned PTQ studies are targeted to either vision models or language models only, but not to both. Most experimental results in the above PTQ works are conducted via channelwise/group-wise/vector-wise weight quantization at the expense of reduced parallelism. To the best of our knowledge, our work is the first to carry out extensive experiments on diverse tasks ranging from image classification to natural language generation assuming a per-tensor uniform PTQ setting.\n\n3 METHODOLOGY\n\nIn this section, we first present the notations used in the paper, describe the concept and design of FlexRound for per-tensor uniform post-training quantization (PTQ) reconstruction, and then, scrutinize how FlexRound can leverage the importance of a pre-trained weight.\n\n3.1 PRELIMINARIES\n\nNotations. A scalar, a vector, and a matrix (or a tensor) are expressed as a non-bold letter, a small bold letter and a capital bold letter (e.g. s, s and S) respectively. (cid:99)W indicates the quantized counterpart of W . The input to a convolutional or fully-connected layer is denoted as X if all previous layers are intact or as (cid:102)X if all previous layers are quantized. The (i, j) element of a matrix W is represented as W(i,j). We let ⊙ and / indicate element-wise product and element-wise division, respectively, similar to the broadcasting process in Python Numpy. ⌊ · ⌉ and ⌊·⌋ express the rounding function and the floor function. || · ||F represents the Frobenius norm.\n\n(cid:109)\n\n(cid:106) W s1\n\nvia rounding-to-nearest and to minimize ∥W − (cid:99)W ∥2\n\nPTQ Background. The conventional uniform PTQ approach is to quantize pre-trained weights W to be (cid:99)W = s1 F with respect to the quantization grid size s1, but the minimization of quantization error in the parameter space is not equivalent to that of the final task loss. On the grounds that Li et al. (2021) proves that the loss degradation resulting from quantization can be approximated as the quadratic form of the network output and its Hessian matrix, several existing studies have strove to minimize ∥W X − (cid:99)W (cid:102)X∥2 F\nlayer-by-layer or block-by-block with respect to continuous variables V with only a small amount\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) A new rounding scheme via element-wise division. Both s1 and S are updated toward minimizing the reconstruction error, L.\n\n(b) Rounding functions with learned parameters s1 and S as shown in (a).\n\nFigure 1: Illustration of FlexRound in the per-tensor uniform PTQ reconstruction. As seen in (b), FlexRound flexibly quantizes pre-trained weights by observing W(2,4) < W(3,2) but (cid:99)W(2,4) > (cid:99)W(3,2).\n\nof data, where (cid:99)W is either s1(⌊ W\n\ns1\n\n(cid:109)\n\n(cid:106) W +V s1\n\ns1 founded on element-wise addition.\n\n⌋ + h(V )) with a certain function h(·) (Nagel et al., 2020) or\n\n] (Hubara et al., 2021). However, all these aforementioned rounding mechanisms are\n\n3.2 FLEXROUND\n\nUnlike prior works based on element-wise addition, we exploit element-wise division for quantizing pre-trained weights. We can formulate our proposed weight-rounding scheme via element-wise division as follows:\n\n(cid:99)W = s1\n\n(cid:109) ,\n\n(cid:106) W S\n\n(1)\n\nwhere the shape of S is equal to that of W while all entries of S as well as the quantization grid size s1 are positive and learnable. Similarly to preceding studies, both s1 and S are updated as an attempt to minimize ∥W X − (cid:99)W (cid:102)X∥2\n\nF .\n\nEq. 1 implies that the basic formula of FlexRound supports per-tensor uniform PTQ. Notice that although FlexRound can adopt a per-channel weight quantization scheme simply by replacing a scalar s1 with a vector s1, since we show later that per-tensor uniform PTQ (using FlexRound) is enough to provide the accuracy of a full-precision model, we set a single quantization grid size s1 for each layer (Per-tensor quantization schemes might enable integer matrix-to-matrix multiplication API/function calls that can facilitate efficient inference of quantized models. (Migacz, 2017)). From now on, thus, we study only the per-tensor uniform PTQ reconstruction. The overall procedure of FlexRound is described in Figure 1.\n\n>0\n\nNow let us discuss how to design S. Let W ∈ RCout×Cin in the case of a fully-connected layer and W ∈ RCout×Cin×H×W in the case of a convolutional layer. We first start formulating S as S = s1 ⊙ S2 where S2 ∈ RCout×Cin in the case of a fully-connected layer and S2 ∈ RCout×Cin×H×W in the case of a convolutional layer while all elements of S2 are learnable. Then, motivated by a wide acknowledgement that the statistics of output channels can vary greatly (Nagel et al., 2019; Lou et al., 2020), we account for the variation of output channel’s statistics by complementing S with an additional learnable tensor s3, where s3 ∈ RCout×1 in the case of a fully-connected layer and s3 ∈ RCout×1×1×1 in the case of a convolutional layer. For a convolutional layer, S is additionally complemented by another learnable tensor s4, where s4 ∈ R1×Cin×1×1 . Consequently, S is formulated as s1 ⊙ S2 ⊙ s3 for a fully-connected layer as displayed in Figure 2 and s1 ⊙ S2 ⊙ s3 ⊙ s4 for a convolutional layer.\n\nFigure 2: Formation of S for a linear layer.\n\n>0\n\n>0\n\n>0\n\n>0\n\n4\n\n=s!⊙⊙S2Ss#Under review as a conference paper at ICLR 2023\n\nAccordingly, quantization process for FlexRound can be expressed as\n\n(cid:99)W =\n\n \n\ns1\n\n\n\ns1\n\n(cid:106) W\n\n(cid:109)\n\n(cid:106)\n\ns1⊙S2⊙s3\n\nW s1⊙S2⊙s3⊙s4\n\n(cid:109)\n\nif W is a fully-connected layer\n\nif W is a convolutional layer\n\n(2)\n\nwhere all entries of S2, s3, and s4 are initialized to be ones in order to enable learning S2, s3, and s4 . s1, S2, s3, and s4 are updated to minimize ∥W X − (cid:99)W (cid:102)X∥2 from rounding-to-nearest, s1 subject to the constraint that all elements of s1, S2, s3, and s4 are positive.\n\n(cid:106) W s1\n\n(cid:109)\n\nF\n\nSince s1, S2, s3, and s4 are all learnable and FlexRound does not need any explicit regularization terms, no additional hyper-parameter is necessary, and thus, FlexRound would be convenient for practitioners. Moreover, as all entries of s1, S2, s3, and s4 are positive and FlexRound is based on element-wise division, FlexRound encourages (cid:99)W to employ the same sign as W . Hence, FlexRound prevents extreme changes of weights through quantization process unlike some element-wise addition rounding scheme such as AdaQuant (Hubara et al., 2021).\n\n4 EXPERIMENTS\n\nIn this section, we present experimental results for benchmark datasets and network models in computer vision and natural language processing tasks. We first empirically confirm that additional tensors s3 and s4 introduced in Section 3.2 implement distinct contributions in the per-tensor uniform post-training quantization (PTQ) setting. Then, we compare the performance of FlexRound with that of some state-of-the-art PTQ approaches in the following cases: image classification on the ImageNet (Russakovsky et al., 2015) dataset with the ResNet (He et al., 2016) and MobileNetV2 (Sandler et al., 2018) architectures (Section 4.3), natural language understanding (NLU) on the GLUE (Wang et al., 2018) benchmark with the BERT (Devlin et al., 2018) and GPT-Neo (Black et al., 2021) architectures (Section 4.4), and natural language generation (NLG) on WikiText2 (Merity et al., 2016) and Penn Treebank (PTB) (Marcus et al., 1993) with the GPT-Neo and OPT (Zhang et al., 2022) architectures (Section 4.4). For brevity, we let “B + X” and “Q + X” indicate that a certain rounding scheme ‘X’ is performed in the experimental setup described in BRECQ (Li et al., 2021) or QDrop (Wei et al., 2022), respectively (an experimental setup includes the definition of a block unit for reconstruction error minimization or how much the probability of dropping the quantization of activations is). As introduced in BRECQ and QDrop, we also utilize the LSQ technique (Esser et al., 2020) when updating an activation step size for activation quantization. Throughout our comprehensive experiments, we verify that FlexRound can achieve competitive performance with a full-precision model for the above tasks even in the per-tensor uniform PTQ reconstruction, which has not been introduced previously. All experimental results in this section are conducted by our own implementation based on open-source codes.\n\n4.1 LEVERAGING THE IMPORTANCE OF A PRE-TRAINED WEIGHT\n\nAs we discussed previously, either element-wise addition or element-wise division is effective to produce a better rounding scheme than a rounding to the nearest scheme. In order to investigate the difference between element-wise addition and element-wise division, it would be instructive to analyze the gradient of the reconstruction error L = ∥W X − (cid:99)W (cid:102)X∥2 F with respect to S′ (where S′ is S2 ⊙ s3 for a fully-connected layer and S2 ⊙ s3 ⊙ s4 for a convolutional layer). Through analysis, unlike element-wise addition, we show that element-wise division enables ∂L ∂S′ to leverage the importance of pre-trained weights W , as follows1:\n\nUsing the straight-through estimator (Bengio et al., 2013), for every i and j,\n\nproportional to\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nW(i,j)\n\n∂L ∂(cid:99)W(i,j)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n, which implies that S′\n\n(i,j) is (partially) affected by W(i,j). As a result,\n\n(cid:12) (cid:12) (cid:12)\n\n∂L\n\n∂S′\n\n(i,j)\n\n(cid:12) (cid:12) (cid:12) is directly\n\n1For simplicity, we take into account the case of a fully-connected layer.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) MobileNetV2\n\n(b) ResNet-18\n\nFigure 3: Weight updates through FlexRound of the first convolutional layer in the first block of (a) MobileNetV2 and (b) ResNet-18, after quantizing pre-trained weights into 4-bit (by FlexRound) while activations are kept in full-precision.\n\nW (i,j) =\n\n(cid:106) W(i,j) s1⊙S′\n\n(i,j)\n\n(cid:109)\n\ncan also be updated and influenced by W(i,j) as well. In other words, as the\n\nmagnitude of a pre-trained weight W(i,j) is larger, the chance of W (i,j) receiving a larger update becomes higher during the PTQ reconstruction. In light of the fact that the magnitude of a weight can be regarded as a metric to measure importance during compressing a neural network (Han et al., 2015; Zhu & Gupta, 2017), if the goal is to enhance model accuracy after quantization, it would be reasonable to have less important (that is, smaller magnitude) weights rounded either up or down only while allowing more important (i.e., exhibiting larger magnitude) weights to be quantized to one of the two closest quantization grids or more.\n\nFigure 3 presents the amount of weight updates through FlexRound for MobileNetV2 and ResNet-18. On the left side and the center side of Figure 3, histograms describe the change of W (i,j) grouped for small pre-trained weights (|W | < 1, left) and large pre-trained weights (|W | > 1, center). On the right side, scatter plots show the amount of grid shifts from the grids obtainable by the rounding-tonearest (RTN) scheme. We note that MobileNetV2 and ResNet-18 are quantized distinctively due to FlexRound. For example, in the case of MobileNetV2 as illustrated in Figure 3(a), the change of W (i,j) attained by minimizing L is more aggressive (i.e., rounding can be deviated by more than one-step up or one-step down) when the absolute value of W(i,j) is larger than one, which means that FlexRound more flexibly quantizes pre-trained weights of large magnitude as illustrated in red dotted squares in Figure 3(a). The amount of aggressively rounded weights in the first convolutional layer of the first block of MobileNetV2 is around 12.8% of the total. For ResNet-18, however, there are no pre-trained weights whose magnitudes are larger than one. Thus, most pre-trained weights are rounded either up or down as shown in Figure 3(b) (e.g., only about 1.5% weights are rounded aggressively in the first convolutional layer of the first block of ResNet-18). Different rounding results by FlexRound, AdaRound, and AdaQuant are visually compared in Appendix A.\n\n4.2 ABLATION STUDY\n\nTo justify the introduction of s3 and s4 on FlexRound in the per-tensor uniform PTQ setting, we investigate the impact of s3 and s4 on the performance of FlexRound using the ImageNet dataset with pre-trained weights quantized into 2-bit (activations are not quantized). As shown in the last two rows in Table 1, the presence of s3 and s4 enhances the accuracy for all models. Interestingly, FlexRound outperforms both AdaQuant and AdaRound even without s3 and s4, which would support\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Top-1/Top-5 accuracy (%) on ImageNet by ResNet-18, ResNet-50, and MobileNetV2 with only weights quantized into 2-bit. “B + X” denotes the implementation of X in the setting of BRECQ. We employ pre-trained models available from the official PyTorch repository.\n\nMethod\n\nB + AdaQuant B + AdaRound B + FlexRound without s3, s4 B + FlexRound with s3, s4\n\nResNet-18 1.13/4.10 63.01/85.20 63.19/85.08 63.73/85.41\n\nResNet-50 0.12/0.60 68.31/88.98 70.00/89.82 70.57/90.07\n\nMobileNetV2 0.10/0.50 33.10/60.58 34.75/62.51 38.09/64.90\n\nTable 2: Top-1/Top-5 accuracy (%) for ResNet-18, ResNet-50, and MobileNetV2 on ImageNet when only weights are quantized. “B + X” expresses the implementation of X in the BRECQ’s setting. We employ pre-trained models available from the BRECQ github repository\n\nMethod Full-precision B + AdaQuant B + AdaRound B + FlexRound (Ours) B + AdaQuant B + AdaRound B + FlexRound (Ours) B + AdaQuant B + AdaRound B + FlexRound (Ours)\n\n# Bits (W./A.) 32/32 4/32 4/32 4/32 3/32 3/32 3/32 2/32 2/32 2/32\n\nResNet-18 71.00/89.97 67.50/87.75 70.18/89.38 70.28/89.44 57.09/80.82 68.79/88.62 68.65/88.54 0.23/0.92 61.99/84.81 62.57/84.84\n\nResNet-50 76.63/93.04 72.79/90.77 75.86/92.62 75.95/92.68 52.13/75.22 74.31/91.81 74.38/91.81 0.10/0.50 48.47/77.09 63.67/85.72\n\nMobileNetV2 72.62/90.67 15.17/32.89 69.46/88.85 70.82/89.67 0.20/0.79 62.51/84.52 66.87/87.56 0.10/0.50 39.57/66.18 46.04/72.48\n\nour claim that a new rounding scheme, shifted from element-wise addition to element-wise division, is the key to improving quantization quality significantly.\n\n4.3 RESNET-18, RESNET-50, AND MOBILENETV2 ON IMAGENET\n\nIn this subsection, we quantize ResNet-18, ResNet-50, and MobileNetV2 in the low-bit PTQ reconstruction with 1024 randomly sampled images. Linear symmetric per-tensor quantization format is assumed to quantize weights and/or activations. For FlexRound, the output of each layer or block is reconstructed during 5k iterations while all learnable parameters (i.e., s1, S2, s3, and s4) are updated by using one learning rate (e.g., 4e-4 for the ResNet models quantized by 3-bit or 4-bit, or 1e-3 for the ResNet models quantized by 2-bit and MobileNetv2). The first and last layers are quantized into 8-bit and the batch normalization layer is folded into convolution, as done in Li et al. (2021). Our experiments are performed based on full-precision pre-trained models available from the BRECQ (Li et al., 2021) github repository2, and we report the median over five random trials.\n\nAssuming the quantization of weights only, we compare FlexRound with AdaRound and AdaQuant that utilize the principle of element-wise addition to decide rounding operations. Table 2 shows that FlexRound consistently outperforms those two addition-based rounding policies. Note that the performance of AdaQuant is inferior to that of AdaRound in Table 2. Correspondingly, FlexRound would be compared to AdaRound only to save space hereafter. Table 3 provides model accuacy when AdaRound and FlexRound (to quantize both weights and activations) are associated with the settings of BRECQ or QDrop. In Table 3, it should be noted that FlexRound is particularly successful for MobileNetV2 incorporating weights of large magnitude, for the reason that we explained in Section 4.1. It is also interesting to see that even when both weights and activations of the ResNet\n\n2https://github.com/yhhhli/BRECQ\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Top-1/Top-5 accuracy (%) for ResNet-18, ResNet-50, and MobileNetV2 on ImageNet when both weights and activations are quantized. “B + X” and “Q + Y” represent the implementation of X in the BRECQ’s setting and that of Y in the QDrop’s setting, respectively. We employ pre-trained models available from the BRECQ github repository.\n\nMethod\n\nFull-precision B + AdaRound B + FlexRound (Ours) Q + AdaRound Q + FlexRound (Ours) B + AdaRound B + FlexRound (Ours) Q + AdaRound Q + FlexRound (Ours)\n\n# Bits (W./A.) 32/32 4/4 4/4 4/4 4/4 3/3 3/3 3/3 3/3\n\nResNet-18 71.00/89.97 69.18/88.85 69.32/88.83 69.20/88.96 69.26/88.81 64.83/86.12 64.99/85.93 65.71/86.96 65.43/86.60\n\nResNet-50 76.63/93.04 74.44/91.80 74.56/91.87 74.90/92.15 75.08/92.20 67.01/87.28 68.29/87.89 70.49/89.93 70.74/89.78\n\nMobileNetV2 72.62/90.67 61.05/83.30 63.74/85.01 65.42/86.23 66.66/87.21 3.74/11.54 25.43/48.28 39.86/66.00 51.49/76.90\n\nTable 4: Performance of BERTBase, BERTLarge, on the GLUE benchmark. For evaluation metrics, matched and mismatched accuracies are reported for MNLI, F1 score and accuracy are reported for QQP, Mathews correlation is reported for CoLA, Pearson and Spearman correlations are reported for STS-B, and accuracy is reported for the others. “Q + X” indicates the implementation of X in the QDrop’s setting.\n\nDataset\n\nMethod Full-precision MNLI Q+AdaRound\n\nQQP\n\nQNLI\n\nQ+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours) Full-precision STS-B Q+AdaRound\n\nCoLA\n\nSST-2\n\nQ+FlexRound (Ours) Full-precision MRPC Q+AdaRound\n\nQ+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours)\n\nRTE\n\nBERTBASE 84.49/85.20 83.69/84.61 84.53/84.98 88.06/91.08 87.65/90.58 87.81/90.83 91.25 91.16 91.16 93.00 92.66 92.43 58.55 56.79 57.53 88.52/88.20 88.00/87.53 88.29/87.91 85.05 81.62 84.07 64.62 63.54 64.62\n\nBERTLARGE 86.05/85.98 85.75/85.86 85.93/85.99 88.66/91.59 87.48/90.62 88.38/91.31 92.13 92.24 92.04 92.78 93.00 93.58 63.57 54.30 60.57 88.98/88.89 86.87/86.69 88.82/88.76 85.54 82.35 84.31 71.19 66.79 68.95\n\nGPT-Neo125M GPT-Neo1.3B 85.12/86.04 79.11/79.63 84.90/85.82 72.67/74.11 85.56/86.14 72.94/74.24 88.26/91.28 85.20/88.99 87.98/91.04 72.97/79.35 88.27/91.18 73.75/80.65 91.36 85.15 91.40 80.87 91.54 80.52 93.35 89.91 92.55 84.75 93.12 83.03 57.42 37.83 58.93 20.15 59.30 21.59 88.94/88.90 79.87/80.12 88.97/88.77 68.55/68.25 88.82/88.58 67.65/68.34 85.05 80.15 84.80 75.25 85.05 75.49 76.17 64.98 75.09 62.82 76.17 62.82\n\nGPT-Neo2.7B 86.36/87.02 86.33/86.75 86.41/86.89 88.62/91.50 88.38/91.27 88.60/91.39 92.46 92.04 92.50 94.50 93.81 94.04 58.88 57.14 57.37 89.75/89.82 89.03/88.91 89.06/88.69 87.99 85.78 86.76 80.87 80.51 81.23\n\nmodels are quantized into 4-bit under the per-tensor uniform PTQ setting, the performance degradation (compared to a full-precision pre-trained model) is negligible (less than 1.5%) in Table 3.\n\n4.4 LANGUAGE MODELS\n\nAll language models we consider in this paper are based on the structure of Transformers (Vaswani et al., 2017). To quantize Transformers into 8-bit, we apply linear asymmetric per-tensor quantization scheme for both weights and activations, while reconstruction (for PTQ) is considered for each\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Performance of GPT-Neo125M, GPT-Neo1.3B, GPT-Neo2.7B, OPT125M, OPT1.3B and OPT2.7B on the WikiText2 and PTB datasets. The perplexity (PPL) is employed as a performance metric. The lower PPL, the better. “Q + X” means the implementation of X in the QDrop’s setting.\n\nDataset\n\nMethod\n\nGPT-Neo125M GPT-Neo1.3B GPT-Neo2.7B OPT125M OPT1.3B OPT2.7B\n\nFull-precision WikiText2 Q+AdaRound\n\nPTB\n\nQ+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours)\n\n31.54 35.60 33.44 64.63 70.16 66.62\n\n15.40 15.75 15.68 31.51 31.97 31.74\n\n13.35 13.95 13.80 27.22 28.24 27.68\n\n56.08 226.48 66.07 129.90 220.01 145.45\n\n29.76 40.40 40.01 76.06 103.15 101.81\n\n26.13 47.48 40.38 68.81 120.37 106.88\n\nTransformer layer that includes attention sublayers and feedforward sublayers. All weights are quantized into 8-bit except the last randomly initialized layer. As for activation quantization, onthe-fly (static) quantization is conducted before every fully-connected layer except the inputs of the softmax layer and the normalization layer that remain to be of full-precision as in Zafrir et al. (2019) and Zhang et al. (2020).\n\nBERT and GPT-Neo on GLUE We evaluate the natural language understanding (NLU) performance of FlexRound using various models including BERTBase, BERTLarge, GPT-Neo125M, GPT-Neo1.3B and GPT-Neo2.7B on the GLUE benchmark. The learning rate applied to all learnable parameters (s1, S2, and s3) is selected to be 2e-4 for BERT and to be 3e-4 for GPT-Neo. Reconstruction process is performed by using 1024 random samples for 20K iterations. For all experiments, the batch size is 64 and maximum sequence length of all experiments is 128. We utilize pre-trained language models (PLMs) and datasets available from the HuggingFace (Wolf et al., 2020) repository3. Further experimental details are referred to Appendix G. In Table 4, we report the performance of ‘Q + AdaRound’ and ‘Q + FlexRound’ that are potentially promising as shown in Table 3. We can notice that ‘Q + FlexRound’ yields better NLU scores than ‘Q + AdaRound’ for most NLU tasks. In particular, for the MNLI and QQP datasets, ‘Q + FlexRound’ can achieve comparable or even superior performance to a full-precision model in the per-tensor uniform PTQ setting except GPT-Neo125M.\n\nGPT-Neo and OPT on WikiText2 and PTB We test the natural language generation (NLG) performance of FlexRound on the WikiText2 and PTB datasets. PLMs (for NLG) are quantized by FlexRound (in a per-tensor quantization manner) while a small amount of data of downstream tasks are used for reconstruction and evaluation. Specifically, PLMs include GPT-Neo125M, GPT-Neo1.3B, GPT-Neo2.7B, OPT125M, OPT1.3B and OPT2.7B, while 256 downstream task data samples are chosen at random for reconstruction. More details on the experimental setup are provided in Appendix I. Table 5 presents the results of GPT-Neo and OPT on NLG tasks and it is clear that ‘Q + FlexRound’ is superior to ‘Q + AdaRound’ for all models and NLG tasks. Note that for GPT-Neo, ‘Q + FlexRound’ can achieve the similar performance of a full-precision PLM even in the per-tensor uniform PTQ setting, while some previous attempts rely on group-wise or vector-wise quantization (Yao et al., 2022; Dettmers et al., 2022).\n\n5 CONCLUSION\n\nWe propose a new rounding scheme, named FlexRound, for post-training quantization under the the principle of element-wise division, to enable learning both a common quantization grid size and an individual scale for each pre-trained weight. We validate that FlexRound can flexibly quantizes pre-trained weights by exploiting their magnitude as a metric to measure importance. Consequently, FlexRound can achieve comparable performance to a full-precision model even in the per-tensor uniform PTQ setting. As a future work, we plan to quantize large language models beyond 6.7B parameters in the per-tensor uniform PTQ setting.\n\n3https://github.com/huggingface/transformers\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHaoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael R Lyu. Towards efficient post-training quantization of pre-trained language models. arXiv preprint arXiv:2109.15082, 2021.\n\nYoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through\n\nstochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcomIn Proceedings of the 2021 Confering the challenges of efficient transformer quantization. ence on Empirical Methods in Natural Language Processing, pp. 7947–7969. Association for Computational Linguistics, November 2021. doi: 10.18653/v1/2021.emnlp-main.627. URL https://aclanthology.org/2021.emnlp-main.627.\n\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.\n\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\n\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nSteven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkgO66VKDS.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015.\n\nSong Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. URL https://arxiv.org/pdf/1510.00149.pdf.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4466–4475. PMLR, 2021. URL https://proceedings.mlr.press/v139/hubara21a.html.\n\nSambhav R Jain, Albert Gural, Michael Wu, and Chris H Dick. Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks. arXiv preprint arXiv:1903.08066, 2019.\n\nSangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4350–4359, 2019.\n\nSumin Kim, Gunju Park, and Youngmin Yi. Performance evaluation of int8 quantized inference on\n\nmobile gpus. IEEE Access, 9:164245–164255, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJung Hyun Lee, Jihun Yun, Sung Ju Hwang, and Eunho Yang. Cluster-promoting quantization with bit-drop for minimizing network quantization loss. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5350–5359. IEEE Computer Society, 2021. URL https: //doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00532.\n\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=POWv6hDd9XH.\n\nQian Lou, Feng Guo, Minje Kim, Lantao Liu, and Lei Jiang. Autoq: Automated kernel-wise neural network quantization. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygfnn4twS.\n\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. URL https://www.aclweb.org/anthology/J93-2004.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\n\nmodels, 2016.\n\nSzymon Migacz. 8-bit inference with tensorrt. In GPU technology conference, volume 2, pp. 5,\n\n2017.\n\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1325–1334, 2019.\n\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7197–7206. PMLR, 2020. URL https://proceedings.mlr.press/v119/nagel20a. html.\n\nYury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and Avi Mendelson. Loss aware post-training quantization. Machine Learning, 110(11):3245–3262, 2021.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions\n\nfor Machine Comprehension of Text. arXiv e-prints, art. arXiv:1606.05250, 2016.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510–4520, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nPeisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In International Conference on Machine Learning, pp. 9847–9856. PMLR, 2020.\n\nXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. QDrop: Randomly dropping quantization for extremely low-bit post-training quantization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=ySQH0oDyp7.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art In Proceedings of the 2020 Conference on Empirical Methods natural language processing. in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.\n\nHao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602, 2020.\n\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36–39. IEEE, 2019.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert:\n\nDistillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.\n\nRitchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pp. 7543–7552. PMLR, 2019.\n\nXiandong Zhao, Ying Wang, Xuyi Cai, Cheng Liu, and Lei Zhang. Linear symmetric quantization of neural networks for low-precision integer hardware. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1lBj2VFPS.\n\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model\n\ncompression. arXiv preprint arXiv:1710.01878, 2017.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA COMPARISON OF FLEXROUND TO ADAROUND AND ADAQUANT\n\nFigure 4 shows that the comparison of FlexRound to AdaRound and AdaQaunt. As seen in Figure 4(a), FlexRound can quantize pre-trained weights more flexibly than AdaRound and AdaQuant. As weights of large magnitude are not quantized aggressively in the middle of Figure 4(a) compared to the right of Figure 4(a), AdaQuant quantizes weights of large importance marginally, which seems to make it difficult for AdaQuant to quantize MobileNetV2 into 4-bit.\n\n(a) MobileNetV2\n\n(b) ResNet-18\n\nFigure 4: Scatter plot of the amount of grid shifts from rounding-to-nearest gird in the first layer of the first block in MobileNetV2 and ResNet-18 when only weights are quantized into 4-bit.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB DERIVATION OF SECTION 4.1\n\nLet L = ∥W X − (cid:99)W (cid:102)X∥2 convolutional layer. In the case of a fully-connected layer,\n\nF and S′ be S2 ⊙ s3 for a fully-connected layer and S2 ⊙ s3 ⊙ s4 for a\n\n∂L\n\n∂S′\n\n(i,j)\n\n=\n\n=\n\n∂L\n\n∂(cid:99)W(i,j) (cid:16)\n\n∂(cid:99)W(i,j) ∂S′\n\n(i,j)\n\n∂ ∂S′\n\n(i,j)\n\ns1\n\n(cid:106) W(i,j) s1S′\n\n(i,j) (cid:16)(cid:106) W(i,j) s1S′\n\n(i,j)\n\n(cid:109)(cid:17) ∂L\n\n∂(cid:99)W(i,j)\n\n(cid:109)(cid:17) ∂L\n\n∂(cid:99)W(i,j)\n\n= s1\n\n= s1\n\n= s1\n\n∂ ∂S′\n\n(i,j)\n\n∂ ∂S′\n\n(i,j) W(i,j) s1\n\n= W(i,j)\n\n(cid:16)\n\n−\n\n∂ ∂S′\n\n(i,j) 1\nS′2\n\n(i,j)\n\n(cid:17) ∂L\n\n∂(cid:99)W(i,j)\n\n(cid:16) 1 S′\n\n(i,j) (cid:17) ∂L\n\n∂(cid:99)W(i,j)\n\n(cid:16) W(i,j) s1S′\n\n(i,j)\n\n(cid:17) ∂L\n\n∂(cid:99)W(i,j)\n\n(∵ Straight-Through Estimator)\n\n= −\n\nW(i,j) S′2\n\n(i,j)\n\n∂L\n\n∂(cid:99)W(i,j)\n\nThe derivation in the case of a convolutional layer can be done by just replacing (cid:99)W(i,j) with (cid:99)W(i,j,k,l) and S′\n\n(i,j) with S′\n\n(i,j,k,l).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC RESNET-18, RESNET-50, AND MOBILENETV2 ON IMAGENET WITH PRE-TRAINED MODELS FROM THE OFFICIAL PYTORCH REPOSITORY\n\nTable 6: Top-1/Top-5 accuracy (%) for ResNet-18, ResNet-50, and MobileNetV2 on ImageNet when only weights are quantized. “B + X” expresses the implementation of X in the BRECQ’s setting. We employ pre-trained models available from the official PyTorch repository.\n\nMethod\n\nFull-precision B + AdaQuant B + AdaRound B + FlexRound (Ours) B + AdaQuant B + AdaRound B + FlexRound (Ours) B + AdaQuant B + AdaRound B + FlexRound (Ours)\n\n# Bits (W./A.) 32/32 4/32 4/32 4/32 3/32 3/32 3/32 2/32 2/32 2/32\n\nResNet-18 69.76/89.08 67.55/87.73 69.15/88.70 69.21/88.76 60.75/83.41 67.98/88.17 68.02/88.03 1.13/4.10 63.01/85.20 63.73/85.41\n\nResNet-50 76.15/92.87 74.09/91.77 75.51/92.73 75.59/92.63 66.19/87.08 74.51/92.20 74.61/92.11 0.12/0.60 68.31/88.98 70.57/90.07\n\nMobileNetV2 71.88/90.29 0.48/0.53 67.76/88.12 69.56/89.02 0.10/0.52 60.18/83.52 64.85/86.38 0.10/0.50 33.10/60.58 38.09/64.90\n\nTable 7: Top-1/Top-5 accuracy (%) for ResNet-18, ResNet-50, and MobileNetV2 on ImageNet when both weights and activations are quantized. “B + X” and “Q + Y” represent the implementation of X in the BRECQ’s setting and that of Y in the QDrop’s setting, respectively. We employ pre-trained models available from the official PyTorch repository.\n\nMethod\n\nFull-precision B + AdaRound B + FlexRound (Ours) Q + AdaRound Q + FlexRound (Ours) B + AdaRound B + FlexRound (Ours) Q + AdaRound Q + FlexRound (Ours)\n\n# Bits (W./A.) 32/32 4/4 4/4 4/4 4/4 3/3 3/3 3/3 3/3\n\nResNet-18 69.76/89.08 68.32/88.13 68.34/88.19 68.19/88.18 68.23/88.22 64.44/85.73 64.61/85.85 65.33/86.60 65.28/86.49\n\nResNet-50 76.15/92.87 74.28/92.02 74.42/92.04 74.68/92.02 74.83/92.11 68.80/88.79 69.62/89.19 71.80/90.72 71.84/90.48\n\nMobileNetV2 71.88/90.29 28.46/52.60 55.25/78.61 56.68/80.95 61.56/84.18 2.11/7.24 8.80/21.79 32.41/59.27 41.51/68.02\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nD IMPORTANCE OF JOINTLY LEARNING THE QUANTIZATION GRID SIZE s1\n\nWITH ROUNDING\n\nTable 8: Top-1/Top-5 accuracy (%) on ImageNet by ResNet-18, ResNet-50, and MobileNetV2 with only weights quantized into 4-bit. “B + X” denotes the implementation of X in the setting of BRECQ. We employ pre-trained models available from the official PyTorch repository.\n\nMethod\n\nB + AdaQuant B + AdaRound B + FlexRound with s1 fixed B + FlexRound (Ours)\n\nResNet-18 67.55/87.73 69.15/88.70 69.11/88.64 69.21/88.76\n\nResNet-50 74.09/91.77 75.51/92.73 75.52/92.64 75.59/92.63\n\nMobileNetV2 0.48/0.53 67.76/88.12 68.19/88.45 69.56/89.02\n\nTo demonstrate the importance of jointly learning s1 with the rounding, we did an additional study with s1 fixed. When fixing s1, for ResNet models the performance of FlexRound is almost comparable to that of AdaRound, while for MobileNetV2 FlexRound is somewhat superior to AdaRound. When jointly learning s1 with the rounding, however, FlexRound outperforms AdaRound for all models. It is therefore critical to learn s1 jointly with the rounding.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nE ABLATION STUDY ON SAMPLE SIZE\n\nFigure 5: Ablation study on sample size when quantizing MobileNetV2 into 4-bit. Only weights are quantized, with activations kept in full-precision. We employ pre-trained models available from the official PyTorch repository.\n\nNo matter how much data is used, B+FlexRound always outperforms B+AdaRound. When the sample size decreases from 64 to 32, the accuracy of B+FlexRound declines by almost one percent. Correspondingly, a sample size of 32 would be a breakthrough point.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nF COMBINING ELEMENT-WISE ADDITION AND ELEMENT-WISE DIVISION\n\nTable 9: Top-1/Top-5 accuracy (%) for ResNet-18, ResNet-50, and MobileNetV2 on ImageNet when only weights are quantized. “B + X” expresses the implementation of X in the BRECQ’s setting. We employ pre-trained models available from the official PyTorch repository.\n\nMethod\n\nFull-precision B + AdaQuant B + AdaQuant + FlexRound B + FlexRound (Ours) B + AdaQuant B + AdaQuant + FlexRound B + FlexRound (Ours) B + AdaQuant B + AdaQuant + FlexRound B + FlexRound (Ours)\n\n# Bits (W./A.) 32/32 4/32 4/32 4/32 3/32 3/32 3/32 2/32 2/32 2/32\n\nResNet-18 69.76/89.08 67.55/87.73 68.75/88.45 69.21/88.76 60.75/83.41 67.36/87.71 68.02/88.03 1.13/4.10 62.23/84.77 63.73/85.41\n\nResNet-50 76.15/92.87 74.09/91.77 75.14/92.45 75.59/92.63 66.19/87.08 74.05/91.87 74.61/92.11 0.12/0.60 69.39/89.35 70.57/90.07\n\nMobileNetV2 71.88/90.29 0.48/0.53 68.36/88.49 69.56/89.02 0.10/0.52 61.64/84.28 64.85/86.38 0.10/0.50 34.11/61.64 38.09/64.90\n\nTo identify whether there comes any benefit from both addition and division, we combine AdaQuant with FlexRound. AdaQuant + FlexRound is superior to AdaQuant but inferior to FlexRound. This might be due to the naive combination of AdaQuant with FlexRound. Considering both addition and division would be an interesting future work.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nG BERT AND GPT-NEO ON GLUE\n\nThe experimental setting of ‘Q + AdaRound’ follows Wei et al. (2022). To investigate the natural language understanding performance of FlexRound from BERT4 to GPT-Neo5, we directly fine-tune pre-trained models on the GLUE6 dataset. For BERT, we use uncased models. Hyper-parameter selection for fine-tuning a pre-trained model is given in Table 10. We use ADAM optimizer as default for all methods and models. In the QDrop’s setting, the probability of dropping activation quantization is set to 0.5. We utilize the Huggingface repository7 for the evaluation method without any modification.\n\nTable 10: Hyper-parameter selection for fine-tuning BERTBase, BERTLarge, GPT-Neo125M, GPT-Neo1.3B, and GPT-Neo2.7B on the GLUE benchmark.\n\nConfiguration\n\nBERTBase BERTLarge GPT-Neo125M GPT-Neo1.3B GPT-Neo2.7B\n\nLearning Rate Batch Size Epoch Max Sequence Length Weight Decay\n\n2e-5 32\n\n2e-5 32\n\n2e-5 32 3\n128 0.01\n\n2e-5 32\n\n1e-5 16\n\n4https://huggingface.co/bert-base-uncased 5https://huggingface.co/EleutherAI/gpt-neo-1.3B 6https://huggingface.co/datasets/glue 7https://github.com/huggingface/transformers/tree/main/examples/\n\npytorch/text-classification\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nH BERT ON SQUAD\n\nTable 11 additionally shows the performace of FlexRound on the SQuADv1(Rajpurkar et al., 2016)8 dataset for the BERT models. For experimental details, Both BERTBase and BERTLarge are uncased models. For ‘Q + FlexRound’, the learning rate is set to 1e-4 for both models. For both ‘Q + AdaRound’ and ‘Q + FlexRound’, the batch size and the number of iterations for reconstruction are 64 and 20k respectively. We use ADAM optimizer as default for all methods and models. The other experimental setting of ‘Q + AdaRound’ follows Wei et al. (2022). Table 12 shows the hyperparameter selection for fine-tuning. Both BERTBase and BERTLarge are using the same configuration. The other setting for fine-tuning and the evaluation method are the same as HuggingFace repository9.\n\nTable 11: F1 score for BERTBase and BERTLarge on SQuADv1 dataset when both weights and activations are quantized into 8-bit. “Q + X” represent the implementation of X in the QDrop’s setting.\n\nMethod\n\n# Bits (W./A.) BERTBase BERTLarge\n\nFull-precision Q + AdaRound Q + FlexRound (Ours)\n\n32/32 8/8 8/8\n\n87.05 86.90 87.25\n\n89.31 88.89 89.25\n\nTable 12: Hyper-parameter selection for fine-tuning BERTBase and BERTLarge on SQuADv1 dataset.\n\nLearning rate Batch size Epoch Maximum sequence length Document stride\n\n1e-4\n\n32\n\n4\n\n384\n\n128\n\n8https://huggingface.co/datasets/squad 9https://github.com/huggingface/transformers/tree/main/examples/\n\npytorch/question-answering\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nI GPT-NEO AND OPT ON WIKITEXT2 AND PTB\n\nTo evaluate FlexRound for natural language generation tasks, we utilize the WikiText2 10 and PTB 11 datasets. Table 13 reports the learning rate, the batch size, and the number of iterations for ‘Q + FlexRound’. The experimental setting of ‘Q + AdaRound’ follows Wei et al. (2022) except the number of iterations; we employ 15k iterations for GPT-Neo and 20k iterations for OPT12. The batch size for ‘Q + AdaRound’ is same as that for ‘Q + FlexRound’. We use ADAM optimizer as default for all methods and models. The probability of dropping activation quantization is set to 0.5 in the QDrop’s setting. We use the Huggingface repository13 for the evaluation method without any modification.\n\nTable 13: Hyper-parameter selection for ‘Q + FlexRound’ in Table 5.\n\nDataset\n\nConfiguration GPT-Neo125M GPT-Neo1.3B GPT-Neo2.7B OPT125M OPT1.3B OPT2.7B\n\nWikiText2 Learning rate\n\nPTB\n\nBatch size Iteration Learning rate Batch size Iteration\n\n2e-3 32 15k 4e-3 32 15k\n\n6e-4 16 15k 4e-3 16 15k\n\n4e-4 8\n15k 2e-3 8\n15k\n\n1e-3 32 5k 8e-4 32 5k\n\n9e-5 16 5k 1e-3 16 5k\n\n8e-5 8\n5k 5e-3 8\n5k\n\n10https://huggingface.co/datasets/wikitext 11https://huggingface.co/datasets/ptb_text_only 12https://huggingface.co/facebook/opt-1.3b 13https://github.com/huggingface/transformers/tree/main/examples/\n\npytorch/language-modeling\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nJ FINETUNED GPT-NEO AND OPT ON WIKITEXT2 AND PTB\n\nAs for the evaluation of quantized pre-trained language models, the performance (i.e., accuracy) of quantized OPT (by Q+AdaRound or Q+FlexRound) is not close to that of full-precision OPT, while GPT-Neo can be quantized without noticeable accuracy degradation. To investigate whether such an observation is also valid for finetuned OPT or not, we conduct additional experiments on finetuned OPT and GPT-Neo with Wikitext2 and PTB dataset. As shown in the table 14, quantized model’s performance of finetuned OPT turns out to be close to full-precision performance. Considering that the model was finetuned with each downstream dataset, We utilize smaller dataset and lesser iteration for reconstruction. We use 128 samples for calibrations set and the iteration is fixed to 500 for all experiments. Learning rate and batch size for the experiments are shown in Table 15. Other settings are the same as Appendix I.\n\nTable 14: Performance of GPT-Neo125M, GPT-Neo1.3B, GPT-Neo2.7B, OPT125M, OPT1.3B and OPT2.7B Finetuned on the WikiText2 and PTB datasets. The perplexity (PPL) is employed as a performance metric. The lower PPL, the better. “Q + X” means the implementation of X in the QDrop’s setting.\n\nDataset\n\nMethod\n\nGPT-Neo125M GPT-Neo1.3B GPT-Neo2.7B OPT125M OPT1.3B OPT2.7B\n\nFull-precision WikiText2 Q+AdaRound\n\nPTB\n\nQ+FlexRound (Ours) Full-precision Q+AdaRound Q+FlexRound (Ours)\n\n21.96 30.52 24.30 24.20 31.40 26.03\n\n12.09 12.47 12.37 16.09 16.63 16.32\n\n10.78 14.09 12.43 14.70 19.80 16.87\n\n19.85 27.96 21.43 16.50 20.28 17.68\n\n11.52 12.66 12.02 11.62 13.00 12.22\n\n10.27 10.97 10.63 10.80 12.02 11.29\n\nTable 15: Hyper-parameter selection for ‘Q + FlexRound’ in Table 14. Sample size is 128 and iteration is 500.\n\nDataset\n\nConfiguration GPT-Neo125M GPT-Neo1.3B GPT-Neo2.7B OPT125M OPT1.3B OPT2.7B\n\nWikiText2 Learning rate\n\nPTB\n\nBatch size Learning rate Batch size\n\n5e-3 32 5e-3 32\n\n4e-4 16 7e-3 16\n\n4e-3 8\n7e-3 8\n\n3e-5 32 5e-5 32\n\n7e-6 16 3e-5 16\n\n1e-5 8\n8e-6 8\n\n22",
    "reference": "# Summary Of The Paper\n\nThe paper introduces a new post-training quantization algorithm called FlexRound. Unlike prior algorithms such as AdaRound, the learnable rounding factors are division parameters instead of an addition parameters. It has the advantage that it can learn the integer scale and rounding jointly, and also take the weights ‘importance’ into account according to the authors. The proposed approach is well evaluated on CV and NLP tasks and shows good empirical performance compared to their baselines.\n\n# Strength And Weaknesses\n\nStrength:\n* The paper is well written and easy to follow. The visualizations are nice and helpful and the work is put well in context of existing literature.\n* FlexRound allows learning the scale and ‘rounding’ at the same time which can potentially be an advantage over SOTA current methods (AdaRound/BRECQ).\n* The proposed method is extensively evaluated on image classification and NLP tasks and shows good empirical performance (except some missing comparisons, more on that later).\n* Has ablation study on empirical choices such as introducing the factors s3 and s4 which should be in theory not needed. \n\nWeaknesses:\n* A proper comparison to literature and prior work is missing. While table 2 should in theory be comparable to the papers, their numbers do not match. To my understanding,’ B+AdaRound’ should be exactly what BRECQ is, however, the stated numbers are significantly below the results in the original BRECQ paper. Where does the difference come from? Also the 4-bit MobileNet v2 results are below the AdaRound paper while actually BRECQ+AdaRound improves over vanilla AdaRound (and the original AdaRound paper uses per-tensor quantization and the first/last layer are in 4 bits).\n* Claim 3 states that they are the first that do extensive per-tensor study on image classification and NLP. This is not fully true. The white paper of Nagel et al. 2021 (which the authors also cite) has in its PTQ chapter (table 6) a similarly extensive study which also includes per-tensor quantization. On the per-tensor vs per-channel point later, the original AdaRound paper is also with per-tensor quantization (not sure about BRECQ).\n* Claim 2 says they demonstrate that element-wise division includes the importance of the pre-trained weight. This is a fairly strong claim and IMO they only show this partly. They show that the gradient is proportional to the magnitude of the weights but then the link to importance is a bit soft/vague.\n* The authors argue that addition schemes may change the sign of a weight. However, for the most compared schemes, AdaRound and BRECQ, this can not happened based on how it is defined. Only in the case of AdaQuant, which they show performs poorly, this could theoretically happen.\n* Arguing that FlexRound has no extra hyper-parameters (compared to BRECQ/AdaRound) is only somewhat a benefit as the AdaRound paper keeps al hyper-parameter constant inter experimentation and is therefore de-facto also hyper-parameter free.\n\nQuestion: \n* It seems s1 (the general scaling factor) is learned jointly with the ‘rounding’. Could this be also a reason why FelxRound is empirically better than AdaRound/BRECQ? Due to their formulation, AdaRound/BRECQ can not learn the scales jointly with the rounding which is a clear drawback. An additional ablation (in table 1) with a fixed s1 could potentially give some interesting insights into this.\n* Did the author explore combining the additive approach (AdaRound/BRECQ) with the devision based approach? Given that empirically they need 3 new learnable scaling factors (s2, s3, s4,  cf table 1), it might be interesting to see if there comes benefits from both and additive and multiplicative term.\n\nEditorial:\n* Would suggest to use $\\lfloor \\cdot \\rceil$ for rounding such as in most prior literature (BRECQ, AdaRound etc).\n* Section 3.2: “per-tensor quantization schemes facilitate higher parallelism for implementation compared to per-channel quantization schemes (Nagel et al., 2021)”. This seems a not widely known/acknowledged statement and I could not find such a statement in the referred work. Could the authors please point me to the sections where this is discussed or other resources?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written and easy to follow. It has some ablation studies on empirical choices (like s3/s4) which I appreciated. The proposed method is fairly simple and a closely related to AdaQuant and AdaRound, though somewhat novel as the additional factor is a division instead of an addition as in prior work. The paper seems that it should be reproducible but there are some questions wrt the comparisons to prior work (AdaRound and BRECQ, see above).\n\n# Summary Of The Review\n\nThe paper proposes a simple adaptation to AdaRound/BRECQ/AdaQuant which has some novelty as the learned parameter is relative instead of additive. In general the paper has a good empirical evaluation except a few questions/inconsistencies with respect to prior work. A few of the papers claims are a bit strong or can be misleading and should be adapted accordingly. Overall the paper is borderline.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTRANSFORMER-PATCHER: ONE MISTAKE WORTH ONE NEURON\n\nZeyu Huang1,2, Yikang Shen4, Xiaofeng Zhang1,2, Jie Zhou5, Wenge Rong1,3, Zhang Xiong1,3 1State Key Laboratory of Software Development Environment, Beihang University, China 2Sino-French Engineer School, Beihang University, China 3School of Computer Science and Engineering, Beihang University, China 4Mila, University of Montreal, Canada, 5WeChat AI, Tencent Inc, China {zeroy.huang,yikang.shn}@gmail.com,withtomzhou@tencent.com {xiaofeng z,w.rong,xiongz}@buaa.edu.cn\n\nABSTRACT\n\nLarge Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time. For a model deployed in an industrial environment, fixing these mistakes quickly and robustly is vital to improve user experiences. Previous works formalize such problems as Model Editing (ME) and mostly focus on fixing one mistake. However, the one-mistake-fixing scenario is not an accurate abstraction of the real-world challenge. In the deployment of AI services, there are ever-emerging mistakes, and the same mistake may recur if not corrected in time. Thus a preferable solution is to rectify the mistakes as soon as they appear nonstop. Therefore, we extend the existing ME into Sequential Model Editing (SME) to help develop more practical editing methods. Our study shows that most current ME methods could yield unsatisfying results in this scenario. We then introduce Transformer-Patcher, a novel model editor that can shift the behavior of transformer-based models by simply adding and training a few neurons in the last Feed-Forward Network layer. Experimental results on both classification and generation tasks show that Transformer-Patcher can successively correct up to thousands of errors (Reliability) and generalize to their equivalent inputs (Generality) while retaining the model’s accuracy on irrelevant inputs (Locality). Our method outperforms previous fine-tuning and HyperNetwork-based methods and achieves state-of-the-art performance for Sequential Model Editing (SME). The code is available at https://github.com/ZeroYuHuang/Transform er-Patcher.\n\n1\n\nINTRODUCTION\n\nTransformer-based models, particularly large Pretrained Language Models (PLMs) (Devlin et al., 2019; Brown et al., 2020) have become the backbone model of modern Natural Language Processing (NLP) and have enabled promising results in various downstream tasks (Lv et al., 2019; Budzianowski & Vulic, 2019; Ramnath et al., 2020). However, PLMs still produce undesirable outputs occasionally (Zhao et al., 2019; Basta et al., 2021). The cost of such mistakes is non-negligible. For example, a mistaken automatic translation result could get a person arrested (Hern, 2018). One of the most usual expedients was using a manual cache (e.g., lookup table) to overrule these problematic predictions (Sinitsin et al., 2020). Though convenient and straightforward, it lacks robustness and generality because it could be disabled by the slightest change in the input, such as paraphrasing in natural language. On the other hand, one can also re-train the model on the original dataset supplemented with problematic examples. While superior in performance, it is computationally and temporally expensive to re-train large PLMs with billions or even trillions of parameters.\n\nPrevious research formalized such problems as Model Editing (ME) and proposed various methods to intervene model’s behavior on a specific example while preventing the model from forgetting other examples. Some straightly finetune the model on the example and used a constraint loss to maintain the model’s overall performance (Zhu et al., 2020; Sotoudeh & Thakur, 2021). Some edit\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthe model through a HyperNetwork, which regards the model and the false predicted example as inputs and produced a weight update for the model’s parameters (Cao et al., 2021; Sinitsin et al., 2020; Mitchell et al., 2022a). Despite their impressive progress, they mostly focus on one-step editing (fixing one mistake), which is not applicable to practical situations. Because models deployed for real-world applications are expected to face different errors ceaselessly. And the same error may pop up repeatedly and bother different users. In addition, as illustrated in Figure 1, once a wrong answer appears in an online question-answering (QA) model, leaving it unfixed and waiting for future corrections could mislead more people. Therefore, an ideal model editor should provide continuous and promptly fixing of newly emerged mistakes in an effective and efficient manner.\n\nFigure 1: Once an error occurs in a QA model online, it could bother many users contacting the model if not fixed in time. Instant correction is a superior choice to improve the user experience, motivating us to propose a Sequential Model Editing problem.\n\nThus we extend the ME task into the sequential setting and formalize it as Sequential Model Editing (SME) task, which requires a model editor to fix a series of mistakes as soon as they appear. The desiderata of a qualified sequential model editor are three properties (Section 3). For each editing, the post-edit model should be of 1) Reliability: make the desirable output given the input; 2) Generality: generalize over other equivalent inputs; 3) Locality: retain its accuracy over irrelevant inputs. We then propose a standard SME experiment pipeline that is compatible with different tasks and five evaluation metrics to evaluate the three properties. Experiments show that most existing model editors could fail to generalize to the sequential editing scenario. Fine-tuning-based methods are vulnerable to forgetting previous edits. HyperNetwork-based editors are strongly coupled with the initial model that they are trained with, thus failing to edit the model after several steps (Section 5).\n\nTo handle SME, we introduce Transformer-Patcher. Unlike previous methods, Transformer-Patcher retains all original parameters to prevent harming the model’s overall performance. It only adds a handful of trainable neurons (patches) to the last Feed-Forward Network (FFN) layer to revise the model’s behavior on the problematic input and achieve a low editing cost. Furthermore, we train the patch to only respond to specific inputs with the proposed activation loss and memory loss. Experimental results on fact-checking (classification) and question answering (auto-regressive generation) indicated that Transformer-Patcher could rectify a series of mistakes (up to thousands) while almost perfectly retaining the model’s overall performance.\n\nThe main contributions of this work are twofold: 1) We formally propose a sequential model editing task, as well as its standard experiment pipeline and evaluation metrics. 2) We introduce Transformer-Patcher, a simple yet effective model editor to revise transformer-based PLMs, achieving state-of-the-art SME performance.\n\n2 RELATED WORKS\n\nFeed-forward Network Both the Transformer encoder and decoder contain the Feed-Forward Network (FFN). Recent works (Geva et al., 2021; Dai et al., 2022) analogously observed that FFN operates as key-value neural memories (Sukhbaatar et al., 2015). They regarded the input of FFN as a query, the first layer as keys, and the second as values. Thus the intermediate hidden dimension of FFN can be interpreted as the number of memories in the layer, and the intermediate hidden state is a vector containing activation values for each memory. Therefore, the final output of FFN can be viewed as the weighted sum of values activated.\n\nModel editors Existing model editors are mainly separated into two types: fine-tuning-based and HyperNetwork-based. Fine-tuning-based editors usually straightly tune the model with an extra loss to eschew over-fitting to edit examples. For instance, Zhu et al. (2020) proposed an extra loss\n\n2\n\n2022/11/11 19:03图片4.svgfile:///C:/Users/黄泽宇/Desktop/图片4.svg1/1Instant correcon improves user experienceThe slower the fixing, the more users are botheredA QA model onlineWrongAnswer occurs!Instant CorreconUserCorreconUser botheredErrorPublished as a conference paper at ICLR 2023\n\nto reduce the distance between pre-edit and post-edit parameters. Mitchell et al. (2022a); Meng et al. (2022) equipped fine-tuning with KL-divergence to restrict the post-edit model’s output space. For another, HyperNetwork-based editors require additional training phrases. Sinitsin et al. (2020) proposed a Meta Learning-based (Finn et al., 2017) approach named Editable Training to learn editable parameters for model modification. Cao et al. (2021) proposed KnowledgeEditor (KE) trained with constrained optimization to produce weight updates. Mitchell et al. (2022a) proposed MEND that learns to transform the gradient obtained by standard fine-tuning to edit large language models (Raffel et al., 2020). In addition, some works only focus on specific tasks, such as masked language modeling (Dai et al., 2022) and autoregressive language modeling (Meng et al., 2022; Geva et al., 2022). They require special input other than edit examples to conduct model editing.\n\nContinual Learning The proposed SME task could be regarded as an emergent variant of Continual Learning (CL) (Mundt et al., 2020). And dynamically expandable networks are employed for CL as well (Rusu et al., 2016; Li & Hoiem, 2018). But there are some differences in the setting. In CL, usually, the model is continually trained using different datasets and tasks. But SME deals with only one example at once and all examples are from the same task. The difference in setting renders SME an unexplored area with new challenges that may not be properly addressed by general CL methods. For example, KL divergence loss and L2 normalization are usual methods to address the catastrophic forgetting in CL (De Lange et al., 2022), but previous works (Cao et al., 2021; Mitchell et al., 2022a) and our experiments show that they can hardly maintain models accuracy on irrelevant inputs in ME task. And methods that add task-specific parameters for CL usually need extra training (Yoon et al., 2018; Wortsman et al., 2020; de Masson d’Autume et al., 2019), thus falling short of SME’s application efficiency requirement.\n\n3 SEQUENTIAL MODEL EDITING PROBLEM\n\nFigure 2: The process of sequential model editing task. Given the t-th mistake (xt, yxt), the editor takes the model ft−1 and (xt, yxt) as input, and outputs the revised model ft\n\n.\n\nFollowing Mitchell et al. (2022a), a model f ∈ F can be defined as a function f : X (cid:55)→ Y that maps an input x to its prediction f (x). Then, given a model f and an edit example pair (xe, yxe) that f (xe) ̸= yxe , a model editor ME is to output a post-edit model f ′. ME : F × X × Y\n\n(cid:55)→ F\n\n(f, xe, yxe) → f ′ = ME(f, xe, yxe )\n\nGiven a data stream {(x1, yx1), · · · , (xs, yxs)} and an initial model f0, a model editor ME needs to conduct edits successively when the model makes undesirable output, as shown in Figure 2.\n\nft =\n\n \n\n\n\nf0 ft−1 ME(ft−1, xt, yxt)\n\nif t = 0, elif ft−1(xt) = yxt, else.\n\nAnd after every edit in SME the post-edit model f ′ should satisfy the following three properties:\n\nProperty 1 Reliability: the post-edit model should output the desired prediction:\n\nf ′(xe) = yxe\n\n(1)\n\n(2)\n\nProperty 2 Generality: given an edit example xe, Exe = {xj|yxj = yxe } is defined as the set of its equivalent inputs (e.g. rephrased sentences). Then the post-edit model f ′ should satisfy:\n\n∀xj ∈ Exe , f ′(xj) = yxe\n\n(3)\n\n3\n\nexampe (x1,yx1)thatf0x1≠yx1Model f0Model EditorModel f1Model Editor............A series of mistakesexampe(x2,yx2)thatf1x2≠yx2Post-edit ModelfTPublished as a conference paper at ICLR 2023\n\nProperty 3 Locality: the edit should be implemented locally and precisely, which means the postedit model should remain accurate on the irrelevant examples set Ixe = X\\Exe:\n\n∀xj ∈ Ixe , f ′(xj) = yxj\n\nIn particular, an edit should not disrupt the results of past edits in SME setting, which means:\n\nft(xk) = yxk , for k where fk−1(xk) ̸= yxk\n\n(4)\n\n(5)\n\n4 TRANSFORMER-PATCHER\n\nFigure 3: Transformer-patcher enables efficient correction for classification and generation tasks, it rectifies the model’s behavior by adding and training several extra neurons in the last FFN layer.\n\nFirst, we call one misclassification or one wrongly generated token one mistake in the rest of the paper. Aiming at the SME task for transformer-based models, we propose Transformer-Patcher shown in Figure 3. It freezes all original parameters and adds one neuron (patch) to the last FFN layer for one mistake. And we train the patch to take effect only when encountering its corresponding mistake. For classification, we add only one patch to rectify the model. For auto-regressive generation, we count how many tokens are wrongly generated under the teacher-forcing setting and add one patch for each of them. This section describes how to add and train one patch. Multiple patch editing follows exactly the same principle and is formally described in Appendix A.\n\n4.1 WHAT IS A PATCH?\n\nAs mentioned in Section 2, FFN operates as key-value neuron memories. Its forward computation is a process that retrieves values from matrix V by matching keys in matrix K and the input query q. For a standard FFN, given a query q ∈ Rd, its output F F N (q) is:\n\na = Act(q · K + bk) F F N (q) = a · V + bv\n\n(6)\n\n(7)\n\nwhere Act(·) is a non-linear activation function (e.g., Relu or Gelu), a is the vector of activation values, bk, and bv are two bias vectors. A patch is an extra neuron (an extra key-value pair) added to the last FFN layer. After patching, the new output F F Np(q) is:\n\n[a ap] = Act(q · [K kp] + [bk\n\nbp])\n\nF F Np(q) = [a ap] ·\n\n(cid:21)\n\n(cid:20)V vp\n\n+ bv\n\n(8)\n\n(9)\n\nwhere kp ∈ Rd is the patch key, vp ∈ Rd is the patch value, bp is a scalar named patch bias, ap = Act(q·kp+bp) represents the activation value of the patch. With the substitution of equations 6 and 7, equation 9 can be reformulated as:\n\nF F Np(q) = F F N (q) + ap · vp\n\n(10)\n\n4.2 TRAINING A PATCH FOR EDITING\n\nAn ideal edit requires reliability, generality, and locality proposed in Section 3. For reliability, a patch needs to be activated according to equation 10. Let qe represent the input query of the mistake,\n\n4\n\nClassification:Elizabeth Truss is the UK Prime MinisterAutoregressive Generation:Who is the UK Prime Minister?True (×)False (√)PatcherElizabethTruss(×)RishiSunak(√) PatcherPatcherTransformer Layer ×(N-1)Input exampleAttention LayerLayer NFrozenOriginal NeuronsA Patch(a neuron)Published as a conference paper at ICLR 2023\n\nthe patch key kp and patch bias bp should satisfy:\n\nap = Act(qe · kp + bp) ̸= 0\n\nWhen Act is ReLU or GeLU, the above condition can be approximated as follows:\n\nqe · kp + bp > 0\n\nTo meet the constraint 12, we propose a activation loss la to maximize the activation value:\n\nla = exp(−qe · kp − bp))\n\n(11)\n\n(12)\n\n(13)\n\nOnce a patch is activated, according to equation 10, it adds a bias term ap · vp to the output of the last layer. Because we are editing the last layer of the model, the output of the model can be adjusted to any result without worrying that other components of the model would cancel the editing effect. To obtain the target output, we leverage the task’s original loss function and rename it as edit loss le. Formally, for an edit example (xe, ye), the patched model’s output is pe, le is defined as:\n\nle = L(ye, pe)\n\n(14)\n\nwhere L(·) is a function of label ye and model output pe and depends on the specific task.\n\nFor locality, the model’s behavior should not be shifted on irrelevant examples, thus the patch should not be activated by any irrelevant examples. When using ReLU or GeLU, it can be approximated as that all queries from irrelevant examples qi should have a patch activation value less than or equal to a threshold β, i.e., the maximum of them is less than or equal to β:\n\n∀i ∈ Ixe, qi · kp + bp ≤ β → max\n\n(qi · kp + bp) ≤ β\n\ni\n\n(15)\n\nThus we propose the memory loss lm to enforce the constraint 15. To imitate the distribution of queries from irrelevant examples, we randomly retain some queries from previously seen examples as memories. Each query is a d-dimensional vector and we can stack them as a matrix M ∈ Rdm×d, where dm is the number of queries saved. Our proposed memory loss lm is the sum of two terms. The first term lm1 is introduced to make the patch inactivated to all queries in M :\n\nlm1 = S(M · kp + bp − β; k)\n\nwhere S(·; k) is a function that receives a vector v and outputs a scalar\n\nS(v; k) = Avg[TopK(exp(v); k)]\n\n(16)\n\n(17)\n\nIt first employs element-wise exponential function to v and then selects k largest elements to compute their average as the output. Although constraint 15 is about the maximum, we employ TopK here for more efficient optimization. In case that lm1 can not absolutely ensure the constraint 15, we propose lm2 to distance the activation value of qe and qi. That is, the activation value of the mistaken example is larger than that of the irrelevant examples by a certain margin γ.\n\nlm2 = S((M − qe) · kp + bp − γ; k)\n\nTo sum up, the loss lp for training a patch is defined as a weighted sum of the above losses:\n\nlp = le + ala + mlm = le + ala + m(lm1 + lm2)\n\n(18)\n\n(19)\n\nwhere a, m are hyper-parameters. β is selected as -3 for GeLU and 0 for ReLu, since GeLU(- 3)≈0.004 is small enough and ReLU(0)=0. γ is selected as 3 for GeLU and 0 for ReLU.\n\n5 EXPERIMENTS\n\n5.1 EXPERIMENTAL SETTINGS AND EVALUATION METRICS\n\nWe proposed an experimental pipeline for SME used for standard datasets with training set Dtrain, validation set Dval, and test set Dtest. There are two differences between our setting and the previous Model Editing setting. First, we employ multi-step editing rather than one-step. Second, previous works usually generate counterfactual edit examples (e.g., replacing the answer to a question with a random one), while we employ authentic examples where the model makes mistakes. We first split\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nthe original Dtrain into an edit set Dedit and a new training set D′ train. To evaluate generality, backtranslation could be utilized to generate the equivalent set Exe for edit example xe ∈ Dedit following previous works (Cao et al., 2021). To evaluate locality, a subset Dtr randomly sampled from D′ train is used to see how the post-edit model performs on its training data. Our SME pipeline starts with an train and validated using Dval, the model is sequentially edited while initial model f0 trained on D′ encountering mistakes in Dedit. After the tth edit example (xt e, yt e), we obtain a post-edit model ft. Supposing that there are T total edits and I represents the indicator function, our proposed metrics are calculated as follows:\n\n1) Success Rate (SR): to evaluate the reliability, we test if the post-edit model outputs the desired prediction. Thus, SR is:\n\nSR =\n\n1 T\n\nT (cid:88)\n\nt=0\n\nI(ft(xt\n\ne) = yt e)\n\n(20)\n\n2) Generalization Rate (GR): to evaluate the generality, we test the post-edit model ft on the equivalent set Ext } of the edit example xt\n\ne , thus GR is:\n\ne,1 · · · , xt\n\n= {xt\n\ne,Nt\n\ne\n\nGR =\n\n1 T Nt\n\nT (cid:88)\n\nNt(cid:88)\n\nt=0\n\ni=1\n\nI(ft(xt\n\ne,i) = yt e)\n\n(21)\n\n3) Edit Retain Rate (ER): to evaluate locality and reliability, we evaluate how many past edits are retained by the final model fT . In a real application, a reliable model editor should keep the fixed bugs from recurring again, thus SR alone cannot evaluate reliability, and we define ER by testing the final model on all its past edit examples Epe:\n\nER =\n\n1 T\n\nT (cid:88)\n\nt=0\n\nI(fT (xt\n\ne) = yt\n\ne)/T\n\n(22)\n\n4) Training Retain Rate (TrainR): to evaluate locality, we compare the performance of the final model of fT and the initial model f0 on subsampled test Dtr. Thus, the TrainR is defined as:\n\nT rainR =\n\n(cid:80)\n\n(cid:80)\n\n(x,y)∈Dtr\n\n(x,y)∈Dtr\n\nI(fT (x) = y)\n\nI(f0(x) = y)\n\n(23)\n\n5) Test Retain Rate (TestR): to evaluate locality, we see if the post-edit model still retains the generalization ability over unseen data. Then the TestR is defined as:\n\nT estR =\n\n(cid:80)\n\n(cid:80)\n\n(x,y)∈Dtest\n\n(x,y)∈Dtest\n\nI(fT (x) = y)\n\nI(f0(x) = y)\n\n(24)\n\nDatasets and Baselines Both classification and auto-regressive generation tasks are selected for evaluation. Following Cao et al. (2021) and Mitchell et al. (2022a), we employ Fact-Checking (FC) for classification and closed-book Question Answering (QA) for generation. For FC, we apply a BERT base model (Devlin et al., 2019) and the FEVER dataset (Thorne et al., 2018). For QA, we apply a BART base model (Lewis et al., 2020) and the Zero-Shot Relation Extraction (zsRE) dataset (Levy et al., 2017). We directly use the equivalent set released by Cao et al. (2021). We use the same data split as Cao et al. (2021). Both FC and QA are evaluated using accuracy. Our baselines include (1) Fine-Tuning-based editors: The FT directly fine-tunes the model on the edit example. Following Mitchell et al. (2022a), FT+KL is selected as a baseline. It fine-tunes the model with an extra KL divergence loss lkl. Following Sinitsin et al. (2020) and Zhu et al. (2020), we report fine-tuning-based baselines by fine-tuning all parameters (FT(all) and FT(all)+KL) or the last layer (FT(last) and FT(last)+KL). (2) Two HyperNetwork-based editors: KE (Cao et al., 2021) and MEND (Mitchell et al., 2022a). (3) SERA: a variant of the latest SOTA memory-based model editor SERAC (Mitchell et al., 2022b). Other details of our baselines are reported in Appendix B.\n\nExperiment Details Initial models for two tasks are obtained following the same training settings as Cao et al. (2021). For FC, the accuracy of the initial model attains 94.1% on Dtr, 76.9% on Dtest. For QA, the accuracy of the initial model attains 56.6% on Dtr, 23.1% on Dtest. To reduce the experimental uncertainty, we randomly split the edit set into n = 20 folders to run SME 20\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: The Success Rate (SR), Generalization Rate (GR), Edit Retain Rate (ER), Training Retain Rate (TrainR), Test Retain Rate (TestR) of Transformer-Patcher (T-Patcher) and the baselines on FEVER and zsRE dataset. * denotes that the SR of the T-patcher on QA is 0.9987. † means the method requires extra training phases and training data.\n\nFEVER Fact-Checking BERT-base (110M)\n\nzsRE Question-Answering BART-base (139M)\n\nSR\n\n1.00 1.00 1.00 1.00 0.04 0.14 1.00\n\n1.00\n\nGR\n\n0.61 0.74 0.53 0.71 0.03 0.12 0.89\n\n0.82\n\nER\n\n0.59 0.83 0.45 0.49 0.06 0.28 1.00\n\n1.00\n\nTrainR TestR SR\n\n0.893 0.968 0.968 0.998 0.349 0.486 0.904\n\n0.999\n\n0.946 0.994 0.998 1.011 0.652 0.650 0.916\n\n1.00 1.00 1.00 1.00 0.41 0.09 1.00\n\n1.000\n\n1.00*\n\nGR\n\n0.58 0.68 0.57 0.68 0.37 0.08 0.90\n\n0.82\n\nER\n\n0.30 0.43 0.28 0.39 0.00 0.00 0.98\n\n0.99\n\nTrainR TestR\n\n0.914 0.865 0.923 0.889 0.000 0.000 0.906\n\n0.997\n\n0.924 0.910 0.933 0.925 0.000 0.000 0.901\n\n0.996\n\nEditor\n\nFT(last) FT(all) FT(last)+KL FT(all)+KL MEND† KE† SERA†\n\nT-Patcher\n\nTable 2: The experimental results when utilizing all data in Dedit as a single run of SME on QA task. The results of the FC task are presented in Table 7 in Appendix C. E represents how many edits have been conducted. N represents how many mistakes have been made by the initial model f0 on the entire edit set Dedit.\n\nEditor\n\nSR\n\nFT(all)+KL 1.00 SERA 1.00 T-Patcher 0.99\n\nGR\n\n0.69 0.90 0.81\n\nER TrainR TestR\n\nE\n\nN\n\n0.14 0.97 0.97\n\n0.936 0.728 0.912\n\n0.974 0.694 0.948\n\n2821 3558 2308\n\n2766 2766 2766\n\ntimes and report the averaged performance as the final result. The initial model f0 makes about 63 mistakes in an FC folder and about 139 in a QA folder on average. For methods requiring memories (fine-tuning with KL and ours), 40,000 memory examples are sampled from D′ train \\ Dtr are employed for both tasks and are updated as editing proceed. The hyperparameters a and m are selected as 1 and 10 respectively for both tasks to make the extra losses and the original task loss in the same order of magnitude. Other details can be found in Appendix B.\n\n5.2 EXPERIMENTAL RESULTS\n\nMain results The experiment results are shown in Table 1. Our method achieves strong performance in all five metrics across two tasks. It could make a series of model corrections (SR≈1) while nearly retaining every past edit (ER≈1) and almost perfectly keeping the model’s overall performance (TrainR≈1, TestR≈1). The fine-tuning-based editors could partly preserve the model’s behavior and achieve high SR, but it is vulnerable to forgetting previous edits (low ER). Two HyperNetwork-based editors fail in the SME setting. They have trouble retaining models’ overall performance (low ER, TrainR, TestR) and conducting a series of edits (low SR and GR). SERA achieves the highest GR, while can only partially preserve the model’s overall performance (TestR, TrainR≈0.9) compared to T-Patcher. Apart from being effective, our method is efficient enough as well. Using a V100, one edit costs only 7.1s for FC and 18.9s for QA. We could further improve the efficiency to 4.7s and 12.4s by decreasing the number of memory examples to 10,000.\n\nScale up to thousands of edits Table 1 shows that Transformer-Patcher achieves good performance for about 60 edits on FC and 140 edits on QA, thus we wonder if it could handle more edits. So we utilize all data in Dedit as a single data stream to run SME. As shown in Table 2, Transformer-Patcher could effectively correct up to thousands of mistakes and retain the model’s overall performance simultaneously compared with the other two strong baselines. It’s interesting to notice that the number of edits E of Transformer-Patcher is less than the number of actual mistakes N made by the initial model. In other words, our method can fix some potential mistakes in the initial model before the error actually happens. On the contrary, the fine-tuning-based method fixes more mistakes than the original model, which means it created more errors during the editing\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nprocess. It seems contradictory that our method attains fewer E and lower TestR, this may due to the distribution shift between Dedit and Dtest. See more explanation in Appendix C. Furthermore, the post-edit model only gets 1.4% larger for FC and 4.5% larger for QA. We believe this cost is acceptable for automatically correcting the model’s mistakes from time to time during deployment. In practice, we suggest using the transformer-patcher to provide a timely response for each mistake online, and after accumulating certain quantities of mistakes, we could fine-tune the original model on all accumulated mistakes, so that the patches can be removed. In this way, we could achieve a good balance between model size and editing effectiveness.\n\n5.3 ANALYSES\n\n(a) Success Rate on FC\n\n(b) Success Rate on QA\n\nFigure 4: Variation of success rate (SR) with the number of edits. Different methods have different edit times, we plot until they converge.\n\nThe collapse of MEND and KE We discuss here why MEND and KE fail in the SME. Figure 4 presents how SR varies with the number of edits on both FC and QA. Figure 4 shows that MEND and KE are effective in the first few steps, but shortly after they are no longer able to produce valid edits. However, in their original paper (Cao et al., 2021; Mitchell et al., 2022a), they both reported that they achieved high SR when dealing with one-step editing. We find this phenomenon reasonable since both HyperNetwork-based editors are trained with the initial model f0 and thus strongly coupled with the original parameters. As the editing proceeds, the model becomes more different from the initial one, resulting in their failure. We tried to retrain HyperNets after every edit using the post-edit model, but the cost for re-training is unacceptable as it costs hours to train a HyperNet model editor.\n\nTable 3: The ablation results for two alternatives of memory loss.\n\nPatch\n\nw/o lm KL w/o lm2 T-Patcher\n\nFEVER Fact-Checking\n\nzsRE Question-Answering\n\nSR\n\n0.99 1.00 0.95 1.00\n\nGR\n\n0.94 0.76 0.82 0.82\n\nER TrainR TestR\n\n0.61 0.99 0.95 1.00\n\n0.737 0.996 0.994 0.999\n\n0.844 0.998 0.992 1.000\n\nSR\n\n0.99 0.94 0.95 1.00\n\nGR\n\n0.94 0.69 0.82 0.82\n\nER TrainR TestR\n\n0.21 0.49 0.94 0.99\n\n0.069 0.481 0.991 0.997\n\n0.154 0.710 0.984 0.996\n\nMemory loss To validate the effectiveness of our proposed memory loss, we apply several alternative patches: (1) T-Patcher w/o lm, (2) KL Patch, where lm is replaced with the KL divergence loss, (3) T-Patcher w/o lm2. The ablation results in Table 3 show that memory loss is critical. Simply adding patches without memory loss hurts the model’s overall performance severely. The KL divergence partially alleviates this problem (higher TrainR, TestR, and ER) but is still unsatisfying on the more complex QA task, which is similar to the Fintuning with KL results in Table 1. By comparing w/o lm2 and T-Patcher, we observe that the main contribution of our proposed memory loss comes from lm1, while adding lm2 still improves the method’s performance. Furthermore, to investigate whether our added patches do solely respond to the specific error we visualize the activation values of different patches on their corresponding mistakes in Figure 5 for the QA task. The X-axis represents the mistake (8.2 represents the second mistake of the 8th edit example) and the Y-axis represents the patch. Figure 5a shows that the patch can be activated by multiple irrelevant queries without the constraint of memory loss, leading to low ER, TrainR, and TestR. Figure 5b is a lot darker, indicating that the KL loss tends to deactivate patches to bridge the distribution gap\n\n8\n\n03060901201500.00.20.40.60.81.0T­PatcherMENDKE03162931241550.00.20.40.60.81.0MENDKET­PatcherPublished as a conference paper at ICLR 2023\n\n(a) T-Patcher w/o lm\n\n(b) KL Patch\n\n(c) T-Patcher\n\nFigure 5: The activation values of three different patches on their corresponding mistakes.\n\n(a) Patched layer position\n\n(b) Memory size\n\nFigure 6: The ablation studies about patched layer position and the memory size .\n\nbefore patching and after patching. And figure 5c presents a clear diagonal line, which means each patch takes charge of its corresponding mistake. Further analysis of the activation value of different patches is presented in Appendix C.\n\nPatched layer position To validate the benefits of patching the last layer, we focus on the QA task and patch each decoder layer separately. The ablation results are illustrated in Figure 6a. First, patching the bottom layer (layer 0 and 1) can not make effect edits. This may be because patching the bottom layer severely influences every token in the input sequence, making the patch’s optimization more difficult. While the patches added to the last layer only influence correspondent mistaken tokens, Then, compared to the other metrics, what the patching position influenced most is GR, which increases from 0.74 of layer 2 to 0.81 of layer 5, proving that patching the top layers may improve the generality. This phenomenon is aligned with previous studies (Jawahar et al., 2019) which found that high-level semantic features are encoded at the top layers and superficial information is encoded in lower layers. Besides, patching the last layer could ameliorate the editing efficiency as well. Because computation results of previous layers could be cached and reused while editing.\n\nMemory size In order to verify the robustness of our method, we conduct experiments using different memory sizes (from 5,000 to 40,000) on the QA task. As is shown in Figure 6b, our method is not very sensitive to the size of the memory set. Reducing memory examples only causes slight drops in SR, ER, TrainR, and TestR, and a slight increase in GR.\n\n6 CONCLUSION\n\nIn this work, we proposed the Sequential Model Editing task, as well as its experiment pipeline and evaluation metrics. We then introduce Transformer-Patcher, a practical method for sequentially editing transformer-based language models. Experiments on both classification and autoregressive generation tasks demonstrate its ability to edit the model up to a thousand times continuously. This method could have a positive social impact by fixing serious mistakes in large PLMs, including generating biased predictions and hate speech, benefiting a broad spectrum of audiences.\n\n9\n\n1234.14.25.15.25.35.46.16.26.36.46.56.66.77.17.27.38.18.17.37.27.16.76.66.56.46.36.26.15.45.35.25.14.24.1321024681012141.11.22.12.234567.17.27.37.48.18.28.39.19.21011121211109.29.18.38.28.17.47.37.27.165432.22.11.21.102468123456.16.26.36.478.18.2910.110.211.111.211.31213.113.11211.311.211.110.210.198.28.176.46.36.26.15432102468100.50.60.70.80.91SRGRERTrainRTestR50001000020000400000.740.760.790.810.50.60.70.80.91Layer 0Layer 1Layer 2Layer 3Layer 4Layer 5GRSRERTrainRTestR0.50.60.70.80.91SRGRERTrainRTestR50001000020000400000.740.760.790.810.50.60.70.80.91Layer 0Layer 1Layer 2Layer 3Layer 4Layer 5GRSRERTrainRTestRPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nChristine Basta, Marta R. Costa-juss`a, and Noe Casas. Extensive study on the underlying gender bias in contextualized word embeddings. Neural Computing and Applications, 33(8):3371–3384, 2021.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 2020 Annual Conference on Neural Information Processing Systems, 2020.\n\nPawel Budzianowski and Ivan Vulic. Hello, it’s GPT-2 - how can I help you? towards the use In Proceedings of the 3rd\n\nof pretrained language models for task-oriented dialogue systems. Workshop on Neural Generation and Translation, pp. 15–22, 2019.\n\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6491–6506, 2021.\n\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons In Proceedings of the 60th Annual Meeting of the Association for\n\nin pretrained transformers. Computational Linguistics, pp. 8493–8502, 2022.\n\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022.\n\nCyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic memory in lifelong language learning. In Proceedings of the 2019 Annual Conference on Neural Information Processing Systems, pp. 13122–13131, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186, 2019.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 1126–1135, 2017.\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5484–5495, 2021.\n\nMor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. LM-Debugger: An interactive tool for inspection and intervention in transformer-based language models. CoRR, abs/2204.12130, 2022.\n\nAlex Hern.\n\nFacebook translates “good morning” into “attack them”, 2018.\n\nURL\n\nhttps://www.theguardian.com/technology/2017/oct/24/facebook-p alestine-israel-translates-good-morning-attack-them-arrest.\n\nGanesh Jawahar, Benoˆıt Sagot, and Djam ́e Seddah. What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, pp. 3651–3657, 2019.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings\n\nof the 3rd International Conference on Learning Representations, 2015.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction In Proceedings of the 21st Conference on Computational Natural\n\nvia reading comprehension. Language Learning, pp. 333–342, 2017.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020.\n\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis\n\nand Machine Intelligence, 40(12):2935–2947, 2018.\n\nZhengwei Lv, Duoxing Liu, Haifeng Sun, Xiao Liang, Tao Lei, Zhizhong Shi, Feng Zhu, and Lei Yang. AUTOHOME-ORCA at semeval-2019 task 8: Application of BERT for fact-checking in community forums. In Proceedings of the 13th International Workshop on Semantic Evaluation, pp. 870–876, 2019.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\n\nknowledge in GPT. CoRR, abs/2202.05262, 2022.\n\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast In Proceedings of the 10th International Conference on Learning\n\nmodel editing at scale. Representations, 2022a.\n\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memorybased model editing at scale. In Proceedings of the 2022 International Conference on Machine Learning, pp. 15817–15831, 2022b.\n\nMartin Mundt, Yong Won Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning. CoRR, abs/2009.01797, 2020.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:140:1–140:67, 2020.\n\nSahana Ramnath, Preksha Nema, Deep Sahni, and Mitesh M. Khapra. Towards interpreting BERT In Proceedings of the 2020 Conference on Empirical\n\nfor reading comprehension based QA. Methods in Natural Language Processing, pp. 3236–3242, 2020.\n\nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.\n\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry V. Pyrkin, Sergei Popov, and Artem Babenko. In Proceedings of the 8th International Conference on Learning\n\nEditable neural networks. Representations, 2020.\n\nMatthew Sotoudeh and Aditya V. Thakur. Provable repair of deep neural networks. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pp. 588–603, 2021.\n\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In Proceedings of the 2015 Annual Conference on Neural Information Processing Systems, pp. 2440–2448, 2015.\n\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: A largescale dataset for fact extraction and verification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 809–819, 2018.\n\nMitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, and Ali Farhadi. Supermasks in superposition. In Proceedings of 2020 Annual Conference on Neural Information Processing Systems, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamIn Proceedings of the 6th International Conference on Learning\n\nically expandable networks. Representations, 2018.\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. Gender bias in contextualized word embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 629–634, 2019.\n\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix X. Yu, and\n\nSanjiv Kumar. Modifying memories in transformer models. CoRR, abs/2012.00363, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA MULTIPLE NEURON PATCHING\n\nIn auto-regressive generation tasks, the model may make multiple mistakes in one example. Since FFN is a position-wise network, every mistake in the output can be ascribed to one query to the last FFN layer. Therefore, for an example where the model makes n mistakes, each mistake can be ascribed to a query qi e to the last FFN layer, and we add n patches to handle each of them. Specifically, given an input query q, the new output F F Np(q) of a FFN with n patches is: [a ap] = Act(q · [K Kp] + [bk\n\nbp])\n\n(25)\n\nF F Np(q) = [a ap] ·\n\n(cid:21)\n\n(cid:20) V Vp\n\n+ bv\n\n(26)\n\nwhere Kp ∈ Rd×n is the patch key, vp ∈ Rn×d is the patch value, bp ∈ Rn is the patch bias, ap = Act(q · kp + bp) is a vector containing activation values of patches. With the substitution of equations 6 and 7, equation 9 can be reformulated as:\n\nF F Np(q) =\n\n(cid:26)F F N (q)\n\nF F N (q) + ap · vp\n\nif ap = ⃗0 else\n\n(27)\n\nDuring calculating the activation loss for multiple patches, we just constraint the patch ki p to be actie, let qe ∈ Rn×d represent the matrix containing n corresponding vated by its corresponding query qi queries, then we can obtain A ∈ Rn which is defined as a vector containing activation values of each patch on its corresponding query:\n\nAi = qi\n\ne · ki\n\np + bi\n\np\n\n(28)\n\nIt can also be formulated as follows:\n\nA = diag(qe · kp) + bp (29) where diag is a function to select the diagonal elements from a matrix. Then the activation loss for n patches can be calculated as follows:\n\nla = S(−A; ka)\n\n(30)\n\nwhere S is the function defined in Equation 17, ka is a hyper-parameter.\n\nMemory loss lm for multiple patches remains the sum of two terms lm1 and lm2, where lm1 is identical as Equation 16. As for lm2, we restrict that for i-th patch ki p, all its activation value to a query in M should be smaller than that to its corresponding query qi\n\ne, thus lm2 becomes:\n\nlm2 = S(M · kp + bp − A − γ; k)\n\n(31)\n\nFor initialization, every patch ki activation value is 1.\n\np is initialized as its normalized related query qi\n\ne |qi\n\ne|2 so that the initial\n\nB EXPERIMENTAL DETAILS\n\nData splits We utilize the same data split of training and testing following Cao et al. (2021). For closed-book fact-checking, the binary FEVER dataset originally has 104,966 training instances and 10,444 validation instances. In order to adapt it to the SME task, we keep the original validation set intact and employ it as Dtest, and split the original training data into three subsets: a new training set D′ train, a new validation set Dval and an edit set Dedit in the ratio of 0.8 : 0.1 : 0.1. As a result, we get 10,496 instances for the edit set. Since the Bert-based classifier attains 88.3% on the edit set, the ideal edit sequence length is 10496*88.3%/20=63 on average.\n\nFor closed-book question answering, we employ the zsRE dataset released by Cao et al. (2021), which originally has 244,173 examples for training and 27,644 examples for validation. We first filter out examples with only one answer and then employ the same data split process as FEVER in the ratio of 0.9 : 0.075 : 0.025. Finally, we get 5,317 edit data and 15,982 for validation, and 24,051 for testing. Since the Bart-based model attains 47.9% on the edit set, the ideal edit sequence length is 5317*47.9%/20=139 on average. For both datasets, we randomly sampled a subset from D′ train with the size of 10,000 as Dtr, and the edit set Dedit is split into n = 20 folders to run SME n = 20 times independently. For the model editor requiring memories (fine-tuning with KL and Transformer-Patcher), we randomly sampled a subset from D′ train \\ Dtr with the size of 40000 and update it as the editing proceeds.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nInitial models training Initial models are trained following Cao et al. (2021). For the FactChecking task, we fine-tune a BERT base model with an additional linear layer that maps the hidden state of the BOS (beginning of a sentence) token to the probability of the positive label. We maximize the model likelihood and the final model attains an accuracy of 76.9% on Dtest, 94.1% on Dtr and 88.3% on Dedit. For the QA task, we fine-tune a BART base model by maximizing the model likelihood regularized with dropout and label smoothing. The final model attains an accuracy (exact match between model prediction and ground truth) of 23.1% on Dtest , 56.6% on Dtr and 47.9% on Dedit. And these results are comparable with results that the model trained and released by Cao et al. (2021) has achieved.\n\nTransformer-Patcher training details For FC, we add one patch for every edit example. For QA, we employ the teacher forcing setting and count how many target tokens are not assigned to the highest likelihood as the mistake number. For one edit example, we add up to 5 patches.FC and QA task share almost the same hyper-parameters. We repeat one edit example 8 times and feed them to Transformer-Patcher as a batch for training. The initial learning rate is set as 0.01. Adam optimizer (Kingma & Ba, 2015) is applied for both tasks. Every patch is initialized with the normalized corresponding query qe |qe|2 . Such a method makes each patch activated with an initial activate value 1. The patch value vp ∈ Rn×d is parameterized as element-wise production of two matrices: v′ p is initialized with the random number between 0 and 1, and elements in np is initialized with an integer 5 to make the patch value dominant over existing values V .The parameter ka mentioned in equation 30 is set as 5, and parameter k for memory loss is set as 1000. All hyper-parameters are chosen by running a few examples on the validation set.\n\np ∈ Rn×d and np ∈ Rn×d, v′\n\nBaseline implementation details For KE, we directly utilize the released trained HyperNetwork for conducting SME experiments (Cao et al., 2021).\n\nFor MEND, there is no HyperNetwork released and we re-implement the released code with hyperparameters set as Mitchell et al. (2022a). We employ fine-tuning-based methods following Mitchell et al. (2022a) and Cao et al. (2021).\n\nFor all fine-tuning-based baselines, we set the learning rate as 1e-5 and utilize Adam’s optimizer to fine-tune the model until the mistaken example is corrected. For the computation of KL loss for fine-tuning +KL-constraints baselines, we randomly sample a batch of examples in a memory set with the size of 512.\n\nFor SERAC (Mitchell et al., 2022b), we implement one variant of it: SERA. The SERAC maintains a cache of all edit examples. Given an input, it first employs a scope classifier to estimate if the input is relevant to (falls in the scope of) any cached edit examples. If so, it then employs a counterfactual model (needs to have the identical output space as the original model) to produce the output relying on the most relevant cached example. Otherwise, it returns the output of the original model. In our proposed SME experiment setting, the in-scope examples have the same label as the edit example, thus the function of the counterfactual model is to reproduce the answer of the relevant example. During the implementation of QA, we choose the Bart-base as the counterfactual model, but we find is not trivial for the Bart model to reproduce the answer (the original paper use T5 for generation tasks), thus it is more practical to directly return the label of the cached edit example. We refer to this direct-return method as SERA and include it as our baseline for both Fact-Checking and Question-Answering tasks. All other implementation details about SERA are the same as the original paper (Mitchell et al., 2022b).\n\nEnvironment details For all methods, we run SME experiment n=20 times on n different edit folders simultaneously using 8 NVIDIA Tesla V100 GPUs. And it cost around 1 hour for running Trnasformer-Patcher on FEVER and around 3 hours on zsRE.\n\nC EXTRA EXPERIMENT RESULTS\n\nVariation of locality with the number of edits The metric ER, TestR, and TrainR reflect the locality of the final model, but how models behave in the middle is still unclear to us. Thus we choose KE, MEND, FT(all)+KL, and Transformer-Patcher and investigate how their locality varies with the number of edits on the QA task. The results are shown in Figure 7. As editing continues,\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Mean and deviation of absolute patches activation values on three different kinds of examples\n\nPatch\n\nFEVER Fact-Checking\n\nzsRE Question-Answering\n\nEdit\n\nPast-edit\n\nRandom\n\nEdit\n\nPast-edit\n\nRandom\n\nw/o lm KL T-Patcher\n\n34.3±9.3 9.15±2.7 10.25±2.3\n\n15.7±8.1 0.01±0.16 0.00±0.0\n\n0.5±3.0 0.05±0.2 0.05±0.1\n\n11.32±7.3 1.12±1.87 6.78±2.58\n\n1.23±1.64 0.03±0.06 0.00±0.00\n\n0.14±0.3 0.12±0.1 0.10±0.1\n\nTable 5: The standard deviation of Edit Retain Rate (ER), Training Retain Rate (TrainR), Test Retain Rate (TestR) of Transformer-Patcher (T-Patcher) and fine-tuning based baselines on FEVER and zsRE dataset.\n\nEditor\n\nFEVER Fact-Checking BERT-base (110M)\n\nzsRE Question-Answering BART-base (139M)\n\nER\n\nTrainR\n\nTestR ER\n\nTrainR\n\nTestR\n\n0.05589 FT(last) FT(all) 0.07008 FT(last)+KL 0.05929 0.06248 FT(all)+KL\n\nT-Patcher w/o lm KL\n\n0.00000 0.10332 0.00078\n\n0.06242 0.03368 0.02516 0.00677\n\n0.00045 0.21872 0.00536\n\n0.03322 0.02178 0.01635 0.01116\n\n0.00048 0.14569 0.00248\n\n0.03981 0.05168 0.03173 0.06433\n\n0.00916 0.23259 0.07124\n\n0.00920 0.02322 0.01293 0.01659\n\n0.00101 0.05063 0.08237\n\n0.01860 0.01781 0.01697 0.01953\n\n0.00115 0.15795 0.02469\n\nmore and more damage has been done to the model by other baselines, except Transformer-Patcher.\n\n(a) Edit Retain Rate\n\n(b) Test Retain Rate\n\n(c) Train Retain Rate\n\nFigure 7: Variation of ER, TestR, and TrainR with the number of edits on QA task.\n\nStandard deviation of experiment results Since some values in Table 1 and Table 3 are very close, we report the standard deviation in Table 5. Note that the SR and the GR are calculated using all different folders at the same time, the standard deviation is therefore 0. According to Table 5, Transformer-Patcher achieves the smallest deviation on ER, TrainR, and TestR.\n\nStatistics of activation values of different patches In order to study the activation situation of patches on different examples. we present the mean and deviation of absolute patches activation values on three different mistakes: 1) Edit: the mistake for which the patch is added; 2) Past-edit: mistakes from previous edit examples; 3) Random: mistake of examples randomly sampled from Dtest. As BERT and BART utilize GeLU, both positive and negative activation values could activate the patch. We employ absolute value to measure to what extent the patch is activated. The results are shown in Table 4. First, the T-Patcher w/o lm attains the highest value for Edit queries, indicating the effectiveness of our activation loss. Then our memory loss can effectively push the activation values of Past-edit and Random queries to 0, thus disabling the patch on irrelevant examples. The\n\n15\n\n02958871161450.00.20.40.60.81.0KEFT(all)+KLMENDT­Patcher02958871161450.00.20.40.60.81.0KEFT(all)+KLMENDT­Patcher02958871161450.00.20.40.60.81.0KEFT(all)+KLMENDT­PatcherPublished as a conference paper at ICLR 2023\n\nTable 6: The Success Rate (SR), Generalization Rate (GR), Edit Retain Rate (ER), Training Retain Rate (TrainR), Test Retain Rate (TestR) of Transformer-Patcher (T-Patcher) with a fixed memory set.\n\nFEVER Fact-Checking BERT-base (110M)\n\nzsRE Question-Answering BART-base (139M)\n\nEditor\n\nSR\n\nGR\n\nER\n\nTrainR TestR SR\n\nGR\n\nER\n\nTrainR TestR\n\nT-Patcher\n\n1.00\n\n0.82\n\n0.999\n\n1.000\n\n1.000\n\n1.00\n\n0.82\n\n0.97\n\n0.999\n\n0.997\n\nTable 7: The experimental results when utilizing all data in Dedit as a single run of SME. E represents how many edits have been conducted. N represents how many mistakes have been made by the initial model f0 on the entire edit set Dedit.\n\nTaks\n\nSR\n\nFEVER 1.00 zsRE 0.99\n\nFEVER 1.00 zsRE 1.00\n\nFEVER 1.00 zsRE 1.00\n\nGR\n\n0.82 0.81\n\n0.54 0.69\n\n0.89 0.90\n\nER TrainR TestR\n\nE\n\nN\n\nEditor\n\n1.00 0.97\n\n0.16 0.14\n\n1.00 0.97\n\n0.999 0.912\n\n0.998 0.936\n\n0.717 0.728\n\n1.000 0.948\n\n1.002 0.974\n\n0.709 0.694\n\n998 2308\n\n1250 2821\n\n1588 3558\n\n1231 2766\n\n1231 2766\n\n1231 2766\n\nT-Patcher\n\nFT(all)+KL\n\nSERA\n\nKL Patch has the lowest activation value of Edit query on both tasks, which explains the lower SR of QA in Table 3.\n\nEditing results of Transformer-Patcher with fixed memory set The experimental results 1 are obtained using a memory set that is updated with the editing proceeds. Thus in Table 6 we present the editing results of Transformer-Patcher using a fixed memory set. We only observe a slight decline in ER and a slight rise in TrainR. The results further show the robustness of our method. Besides, we have to highlight that our method allows us to save more previous edits as memory and leverage more memories in the training process. Because we do not need to save original raw data but only corresponding input queries (several constant vectors that do not require gradients). On the contrary, KL requires feeding a mini-batch of raw data into the pre-edit model and post-edit model separately, thus the GPU memory becomes a restriction of the number of memories utilized in one batch. But Transformer-Patcher could apply hundreds of thousands of memory vectors in one batch and cost minimal GPU memory and computation resources.\n\nContradictory of lower E and lower TestR in Table 2 It seems inconsistent that TransformerPatcher has achieved fewer E and lower TestR than FT(all)+KL method. Because one would expect the model to reduce future errors and behave better on the test set by fixing errors. The phenomenon may be because of the data distribution gap between the edit set and the test set. Thus the improvement of “reducing future errors” can not directly lead to higher TestR. For FEVER, the accuracy of the initial model attains 88.3% on the edit set and 76.9% on the test set. For zsRE, the accuracy of the initial model attains 47.9% on the edit set and 23.1% on the test set. A distinct gap between the edit set and test set is observed. Thus we should comprehensively consider all metrics to evaluate methods. Another reasonable explanation is that our modified model may slightly overfit the edit example. But fitting more to edit examples may be a desired feature in actual applications because we expect the model to be closer to the real data met during deployment.\n\nACKNOWLEDGMENTS\n\nThis work was partially supported by the State Key Laboratory of Software Development Environment of China under Grant SKLSDE-2023ZX-16.\n\n16",
    "reference": "# Summary Of The Paper\n\nThe authors introduce the task of Sequential Model Editing (SME) where a model needs to adapt to and fix its mistakes from a stream of incoming input-output examples of a certain machine learning task. The setting extends Model Editing (ME), where people focus on editing the model to fix a single occurrence of an observed mistake. The authors define the desired properties of SME, namely reliability, generality, and locality. The authors then propose Transformer-Patcher for SME, which adds a single neuron to the last fully-connected layer of a Transformer; along with specially designed losses and training schemes for Transformer-Patcher. The authors measure 5 metrics that cover all the three desired aspects. Experiments show that 1) HyperNetwork methods fail MSE, 2) the proposed Transformer-Patcher clearly outperforms the fine-tuning and HyperNetwork baselines, often by a big margin, 3) Transformer-Patcher is effective up to thousands of edits. The authors further provide analysis on the metrics progression of the tested methods, and ablation study of the losses.\n\n# Strength And Weaknesses\n\n### Strengths\n\n1. The proposed task of Sequential Model Editing (SME) is novel, interesting and of practical value. With the increasing adoption of large language models, training, and even fine-tuning, can become computationally expensive or inefficient. We've seen rising interest in Model Editing where people try to find efficient ways to fix undesired model behavior. However, existing works focus on single mistakes from an input-output example. In real ML applications, a model needs to take user requests constantly, any downtime or unfixed mistakes could harm user experience. The SME setting manifests the need for a model-editing method that is timely (quick to apply), efficient (cost/computation effective) and effective (fixes the mistake) even after a series of accumulating edits (not just a single edit).\n2. The SME desiderata (Sec 3), namely reliability, generality, and locality, are clearly stated and formulated. It gives a clear base for what the authors are looking for.\n3. The design of Transformer-Patcher (Sec 4) is simple and of intuitive sense.\n4. The design of the objective losses (Sec 4.2) are clearly explained and are of intuitive sense. In Sec 5.3, the ablation study suggests the usefulness of the losses. I do think the exact formula for l_{m1} (Eq (17), see Q1) seems a bit overcomplicated, but this is relatively minor.\n5. The experimental support is strong that SME is an important and challenging setting and that Transformer-Patcher clearly outperforms competitive baselines. This strength can be further break down to\n   1. The 5 metrics (Sec 5.1) are carefully chosen to reflect different aspects namely reliability, generality, and locality, and are clearly described.\n   2. Competitive baselines are chosen (Sec 5.1), including several variants of fine-tuning and recently proposed HyperNetworks KE (Cao et al., 2021) and MEND (Mitchell et al., 2022a).\n   2. Results (Table 1 and 2) show almost unanimous improvements over baselines across all metrics and both tasks, often by a big margin.\n   3. Main results (Table 1) and metric progression over number of edits (Figure 4 and 5) show that HyperNetworks completely fail SME, despite being successful in single step ME.\n   4. Ablation study (Table 3, Figure 6) shows that their proposed losses are effective in preventing regression across edits.\n7. The paper is clearly written and easy to follow. Some details and phrasing can be made more clear, but they do not harm the main strengths.\n8. Experimental details are provided in the appendix. Code is provided in the supplementary material. Both are good for reproducibility,\n\n### Weaknesses\n\nNothing major. See my Questions and LPs. I think clarifying them can further improve the paper's strength and help me gain more confidence in recommending the work.\n\n### Questions for the authors\n1. P5, Eq (17), \"S(v; k) = Avg[TopK(exp(v); k)]\". Why do we need TopK? If we care about maximum (\"Although constraint 15 is about the maximum...\"), why not simply take the maximum, or use a surrogate like softmax?\n2. Table 1 and 2. Why are TrainR and TextR generally < 1? If we are fixing errors (SR↑, GR↑) and not forgetting previous fixes (ER↑), then we should see the final models be more accurate than the initial models (TrainR>1, TestR↑>1)? The authors argue at one point (P8, L6) that there can be distribution shit between the train and test set. However, we are also seeing TrainR < 1 for all methods and datasets. Are we missing something here?\n\n### Localized points (LPs)\n1. P3, Eq (4), Locality, \"$\\forall x_j \\in I_{x_e}, f′(x_j) = y_{x_j}$\". $\\forall x_j \\in I_{x_e}, f′(x_j) = f(y_{x_j})$ makes sense as \"locality\" to me. The former means that the edited model $f'$ should give correct predictions to irrelevant examples, which is the learning task itself. The latter means that the edited model $f'$ should preserve the same prediction as the unedited model $f$ to irrelevant examples, which is more aligned with \"locality\" -- we do not want to change the model behavior at irrelevant examples, because we believe the model is mostly correct but cannot be sure without testing it against the ground-truth data.\n2. P7, L3 after the figures, \"The ideal edit length for each run is about 63 for\nFC and 139 for zsRE.\" What is \"edit length\" and what does \"ideal'' mean here? Is edit length the size of the D_edit or the number of edits the proposed method performed? Is it a hyperparameter that the authors decided based on experimental observations? Does \"ideal\" mean that the proposed method performs worse after 63/139 mistakes or edits? It seems like this claim belongs to results, not the settings.\n2. P7, Experiment Details. It's worth mentioning the size of the test datasets. How many examples are for the test for each task? What is the total number of time steps T for each of the 20-fold runs?\n3. P7, Table 1, \"* denotes that SR of T-patcher on QA is not 1 but very close to 1.\" Does it mean other SRs in the column are actually exact 1? I suppose that the authors mean that the T-patcher SR is >0.995 so it rounds to 1.00 if we only keep two decimal points, but I cannot be sure. If that is indeed the case, I suggest improving the phrasing, e.g. sampling write out the exact number like \"* T-patcher SR on QA = 0.9965\".\n4. P7, bottom, \"Table 1 shows that Transformer-Patcher achieves good performance for about one hundred edits, ...\" Exactly *how many* edits? And again, does \"edit\" here mean the size of the test set D_edit, or the number of times the proposed method applied a patch?\n\n### Typography/Minor points\n1. Abstract, \"... either fail to make a sequence of edits or to remember previous edits\" -> \"... either fail to make a sequence of edits or fail to remember previous edits\".\n2. P2, top, \"Therefore, an ideal model editor should conduct ...\" Perhaps a better phrasing can be \"Therefore, an ideal model editor should enable _countinuous_ fixing of newly emerged mistakes in a both effective and efficient manner.\"\n3. P2, middle, \"To handle SME.We introduce ...\" -> \"To handle SME, we introduce ...\"\n4. P3, Figure 2. Unify the notations of $y_1$ or $y_{x_1}$.\n5. P3, Sec 3. I suggest use $y_e$, $y_j$, $y_1$, etc instead of $y_{x_e}$,  $y_{x_j}$, $y_{x_1}$, etc to reduce clutter.\n6. P3, Property 1-3. Start with either capitalized or uncapitalized letters.\n7. P3, Eq (5), \"$f_t(x_k) = y_{x_k} , \\{k | f_{k-1}...$\" -> \"$f_t(x_k) = y_{x_k}$ , for $k$ where $f_{k-1} ...$\".\n8. P4, Figure 3. Maybe revise the UK Prime Minister example...\n9. P6, Eq (21). Is it $T \\sum N_t$ or $T N_t$? It makes sense to me if you first average over N_t samples at time t and then average the average over T time steps.\n10. P6, Eq (23), \"we compare the performance the final model of f_T and the initial model f_0 on sub-training test D_tr.\" -> \"we compare the performance of the final model of f_T and the initial model f_0 on the subsampled training set D_tr\".\n11. P6, Eq (20-22). I recommend moving the denominators before the \\sum's. It would look more intuitive to me.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarity: Great. The paper is clearly written and easy to follow. Some details and phrasing can be made more clear, but they do not harm any main points.\n- Quality: Great. Clearly designed and explained task, goal, method, metrics, and results. The task is interesting and of practical value. The goal and method makes sense and are clearly described. The method is simple yet effective. The metrics are carefully chosen to reflect different aspects of the desiderata. The experimental support is strong and definitive that MSE poses a new challenge and the proposed Transformer-Patcher works much better than previous methods.\n- Novelty: Great. Sequential Model Editing is a novel, interesting and practical setting that poses a different challenge than the previous studies on Model Editing. This is one step further towards the realistic scenario of the online updating of ML models. The results shown previous models which are good at single edits are not necessarily good at continuous editing. The model design is simple yet effective. The loss design shows the authors' consideration for the task.\n- Reproducibility: Good. Experiment details described in the paper and the appendix. Code is provided.\n\n# Summary Of The Review\n\nThis is a strong work that extends an existing task (Model Editing) to a more realistic and challenging setting (Sequential Model Editing), where previous successful methods fail. The authors clearly state the desiderata, method, and metrics, along with the intuitions behind them. Experimental results show strong support that the proposed Transformer-Patcher is effective by a clear margin. Further analysis confirms the effectiveness of the design. Model Editing is of interest as the language models grow ever large and are increasingly adopted in real applications. I think this is a work that would interest audiences from the application of large language models, model editing, model adaptation, continuous learning, adversarials and robustness, model security and more.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nACCELERATING HAMILTONIAN MONTE CARLO VIA CHEBYSHEV INTEGRATION TIME\n\nJun-Kun Wang and Andre Wibisono Department of Computer Science, Yale University {jun-kun.wang,andre.wibisono}@yale.edu\n\nABSTRACT\n\nHamiltonian Monte Carlo (HMC) is a popular method in sampling. While there are quite a few works of studying this method on various aspects, an interesting question is how to choose its integration time to achieve acceleration. In this work, we consider accelerating the process of sampling from a distribution π(x) ∝ exp(−f (x)) via HMC via time-varying integration time. When the potential f is L-smooth and m-strongly convex, i.e. for sampling from a log-smooth and strongly log-concave target distribution π, it is known that under a constant integration time, the number of iterations that ideal HMC takes to get an (cid:15) Wasserstein-2 distance to the target π is O(κ log 1 m is the condition number. We propose a scheme of time-varying integration time based on the roots of Chebyshev polynomials. We show that in the case of quadratic potential f , i.e. when the target π is a Gaussian distribution, ideal HMC with this choice of integration time only takes O( (cid:15) ) number of iterations to reach Wasserstein-2 distance less than (cid:15); this improvement on the dependence on condition number is akin to acceleration in optimization. The design and analysis of HMC with the proposed integration time is built on the tools of Chebyshev polynomials. Experiments find the advantage of adopting our scheme of time-varying integration time even for sampling from distributions with smooth strongly convex potentials that are not quadratic.\n\n(cid:15) ), where κ := L\n\nκ log 1\n\n√\n\n1\n\nINTRODUCTION\n\nMarkov chain Monte Carlo (MCMC) algorithms are fundamental techniques for sampling from probability distributions, which is a task that naturally arises in statistics (Duane et al., 1987; Girolami & Calderhead, 2011), optimization (Flaxman et al., 2005; Duchi et al., 2012; Jin et al., 2017), machine learning and others (Wenzel et al., 2020; Salakhutdinov & Mnih, 2008; Koller & Friedman, 2009; Welling & Teh, 2011). Among all the MCMC algorithms, the most popular ones perhaps are Langevin methods (Li et al., 2022; Dalalyan, 2017; Durmus et al., 2019; Vempala & Wibisono, 2019; Lee et al., 2021b; Chewi et al., 2020) and Hamiltonian Monte Carlo (HMC) (Neal, 2012; Betancourt, 2017; Hoffman & Gelman, 2014; Levy et al., 2018). For the former, recently there have been a sequence of works leveraging some techniques in optimization to design Langevin methods, which include borrowing the idea of momentum methods like Nesterov acceleration (Nesterov, 2013) to design fast methods, e.g., (Ma et al., 2021; Dalalyan & Riou-Durand, 2020). Specifically, Ma et al. (2021) show that for sampling from distributions satisfying the log-Sobolev inequality, under-damped Langevin improves the iteration complexity of over-damped Langevin from O( d (cid:15) ), where d is the dimension and (cid:15) is the error in KL divergence, though whether their result has an optimal dependency on the condition number is not clear. On the other hand, compared to Langevin methods, the connection between HMCs and techniques in optimization seems rather loose. Moreover, to our knowledge, little is known about how to accelerate HMCs with a provable acceleration guarantee for converging to a target distribution. Specifically, Chen & Vempala (2019) show that for sampling from strongly log-concave distributions, the iteration complexity of ideal HMC is O(κ log 1 (cid:15) ), and Vishnoi (2021) shows the same rate of ideal HMC when the potential is strongly convex quadratic in a nice tutorial. In contrast, there are a few methods that exhibit acceleration when minimizing strongly convex quadratic functions in optimization. For example, while Heavy Ball (Polyak, 1964) does not have an accelerated linear rate globally for minimizing general smooth strongly convex functions, it does show acceleration when minimizing strongly convex quadratic functions (Wang et al., 2020;\n\n(cid:15) ) to O(\n\n(cid:113) d\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: IDEAL HMC\n\n1: Require: an initial point x0 ∈ Rd, number of iterations K, and a scheme of integration time {η(K) 2: for k = 1 to K do 3: 4:\n\nSample velocity ξ ∼ N (0, Id). Set (xk, vk) = HMC\n\n(xk−1, ξ).\n\nk }.\n\n5: end for\n\nη\n\n(K) k\n\n2021; 2022). This observation makes us wonder whether one can get an accelerated linear rate of ideal HMC for sampling, i.e., O(\n\n(cid:15) ), akin to acceleration in optimization.\n\nκ log 1\n\n√\n\nWe answer this question affirmatively, at least in the Gaussian case. We propose a time-varying integration time for HMC, and we show that ideal HMC with this time-varying integration time exhibits acceleration when the potential is a strongly convex quadratic (i.e. the target π is a Gaussian), compared to what is established in Chen & Vempala (2019) and Vishnoi (2021) for using a constant integration time. Our proposed time-varying integration time at each iteration of HMC depends on the total number of iterations K, the current iteration index k, the strong convexity constant m, and the smoothness constant L of the potential; therefore, the integration time at each iteration is simple to compute and is set before executing HMC. Our proposed integration time is based on the roots of Chebysev polynomials, which we will describe in details in the next section. In optimization, Chebyshev polynomials have been used to help design accelerated algorithms for minimizing strongly convex quadratic functions, i.e., Chebyshev iteration (see e.g., Section 2.3 in d’Aspremont et al. (2021)). Our result of accelerating HMC via using the proposed Chebyshev integration time can be viewed as the sampling counterpart of acceleration from optimization. Interestingly, for minimizing strongly convex quadratic functions, acceleration of vanilla gradient descent can be achieved via a scheme of step sizes that is based on a Chebyshev polynomial, see e.g., Agarwal et al. (2021), and our work is inspired by a nice blog article by Pedregosa (2021). Hence, our acceleration result of HMC can also be viewed as a counterpart in this sense. In addition to our theoretical findings, we conduct experiments of sampling from a Gaussian as well as sampling from distributions whose potentials are not quadratics, which include sampling from a mixture of two Gaussians, Bayesian logistic regression, and sampling from a hard distribution that was proposed in Lee et al. (2021a) for establishing some lower-bound results of certain Metropolized sampling methods. Experimental results show that our proposed time-varying integration time also leads to a better performance compared to using the constant integration time of Chen & Vempala (2019) and Vishnoi (2021) for sampling from the distributions whose potential functions are not quadratic. We conjecture that our proposed time-varying integration time also helps accelerate HMC for sampling from log-smooth and strongly log-concave distributions, and we leave the analysis of such cases for future work.\n\n2 PRELIMINARIES\n\n2.1 HAMILTONIAN MONTE CARLO (HMC)\n\nSuppose we want to sample from a target probability distribution ν(x) ∝ exp(−f (x)) on Rd, where f : Rd → R is a continuous function which we refer to as the potential. Denote x ∈ Rd the position and v ∈ Rd the velocity of a particle. In this paper, we consider the standard Hamiltonian of the particle (Chen & Vempala, 2019; Neal, 2012), which is defined as H(x, v) := f (x) + 1\n\n(1)\n\n2 (cid:107)v(cid:107)2,\n\nwhile we refer the readers to Girolami & Calderhead (2011); Hirt et al. (2021); Brofos & Lederman (2021) and the references therein for other notions of the Hamiltonian. The Hamiltonian flow generated by H is the flow of the particle which evolves according to the following differential equations:\n\ndx dt\n\n=\n\n∂H ∂v\n\nand\n\n= −\n\ndv dt\n\n∂H ∂x\n\n.\n\nFor the standard Hamiltonian defined in (1), the Hamiltonian flow becomes\n\ndx dt\n\n= v\n\nand\n\ndv dt\n\n2\n\n= −∇f (x).\n\n(2)\n\nPublished as a conference paper at ICLR 2023\n\nWe will write (xt, vt) = HMCt(x0, v0) as the position x and the velocity v of the Hamiltonian flow after integration time t starting from (x0, v0). There are many important properties of the Hamiltonian flow including that the Hamiltonian is conserved along the flow, the vector field associated with the flow is divergence free, and the Hamiltonian dynamic is time reversible, see e.g., Section 3 in Vishnoi (2021).\n\nThe Ideal HMC algorithm (see Algorithm 1) proceeds as follows: in each iteration k, sample an initial velocity from the normal distribution, and then flow following the Hamiltonian flow with a pre-specified integration time ηk. It is well-known that ideal HMC preserves the target density π(x) ∝ exp(−f (x)); see e.g., Theorem 5.1 in Vishnoi (2021). Furthermore, in each iteration, HMC brings the density of the iterates xk ∼ ρk closer to the target π. However, the Hamiltonian flow HMCt(x0, v0) is in general difficult to simulate exactly, except for some special potentials. In practice, the Verlet integrator is commonly used to approximate the flow and a Metropolis-Hastings filter is applied to correct the induced bias arises from the use of the integrator (Tripuraneni et al., 2017; Brofos & Lederman, 2021; Hoffman et al., 2021; Lee et al., 2021a; Chen et al., 2020). In recent years, there have been some progress on showing some rigorous theoretical guarantees of HMCs for converging to a target distribution, e.g., Chen et al. (2020); Durmus et al. (2017); Bou-Rabee & Eberle (2021); Mangoubi & Smith (2019; 2021); Mangoubi & Vishnoi (2018). There are also other variants of HMCs proposed in the literature, e.g., Riou-Durand & Vogrinc (2022); Bou-Rabee & Sanz-Serna (2017); Zou & Gu (2021); Steeg & Galstyan (2021); Hoffman & Gelman (2014); Tripuraneni et al. (2017); Chen et al. (2014), to name just a few.\n\nRecall that the 2-Wasserstein distance between probability distributions ν1 and ν2 is\n\nW2(ν1, ν2) :=\n\ninf x,y∈Γ(ν1,ν2)\n\nE (cid:2)(cid:107)x − y(cid:107)2(cid:3)1/2\n\nwhere Γ(ν1, ν2) represents the set of all couplings of ν1 and ν2.\n\n2.2 ANALYSIS OF HMC IN QUADRATIC CASE WITH CONSTANT INTEGRATION TIME\n\nIn the following, we replicate the analysis of ideal HMC with a constant integration time for quadratic potentials (Vishnoi, 2021), which provides the necessary ingredients for introducing our method in the next section. Specifically, we consider the following quadratic potential:\n\nf (x) := (cid:80)d\n\nj=1 λjx2\n\nj , where 0 < m ≤ λj ≤ L,\n\n(3)\n\nwhich means the target density is the Gaussian distribution π = N (0, Λ−1), where Λ the diagonal matrix whose jth diagonal entry is λj. We note for a general Gaussian target N (μ, Σ) for some μ ∈ Rd and Σ (cid:31) 0, we can shift and rotate the coordinates to make μ = 0 and Σ a diagonal matrix, and our analysis below applies. So without loss of generality, we may assume the quadratic potential is separable, as in (3).\n\nIn this quadratic case, the Hamiltonian flow (2) becomes a linear system of differential equations, and we have an exact solution given by sinusoidal functions, which are\n\nxt[j] = cos\n\n(cid:16)(cid:112)2λjt\n\n(cid:17)\n\nx0[j] +\n\n1 (cid:112)2λj\n\n(cid:16)(cid:112)2λjt\n\n(cid:17)\n\nsin\n\nv0[j],\n\nvt[j] = −(cid:112)2λj sin\n\n(cid:16)(cid:112)2λjt\n\n(cid:17)\n\nx0[j] + cos\n\n(cid:16)(cid:112)2λjt\n\n(cid:17)\n\nv0[j].\n\n(4)\n\nIn particular, we recall the following result on the deviation between two co-evolving particles with the same initial velocity. Lemma 1. (Vishnoi, 2021) Let x0, y0 ∈ Rd. Consider the following coupling: (xt, vt) = HMCt(x0, ξ) and (yt, ut) = HMCt(y0, ξ) for some ξ ∈ Rd. Then for all t ≥ 0 and for all j ∈ [d], it holds that\n\nxt[j] − yt[j] = cos\n\n× (x0[j] − y0[j]).\n\n(cid:16)(cid:112)2λjt\n\n(cid:17)\n\nUsing Lemma 1, we can derive the convergence rate of ideal HMC for the quadratic potential as follows.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nLemma 2. (Vishnoi, 2021) Let π ∝ exp(−f ) = N (0, Λ−1) be the target distribution, where f (x) is defined on (3). Let ρK be the distribution of xK generated by Algorithm 1 at the final iteration K. Then for any ρ0 and any K ≥ 1, we have\n\nW2(ρK, π) ≤ maxj∈[d]\n\n(cid:12) (cid:12) (cid:12)ΠK\n\nk=1cos\n\n(cid:16)(cid:112)2λjη(K)\n\nk\n\n(cid:17)(cid:12) (cid:12) (cid:12) W2(ρ0, π).\n\nWe replicate the proof of Lemma 1 and Lemma 2 in Appendix B for the reader’s convenience.\n\nVishnoi (2021) shows that by choosing\n\n(Constant integration time)\n\nη(K) k =\n\n1\n\n√\n\nπ 2\n\n,\n\n2L (cid:1) for all the iterations k ∈ [K] and dimensions j ∈ [d].\n\n(5)\n\none has that cos\n\n(cid:16)(cid:112)2λjη(K)\n\nk\n\n(cid:17)\n\n≤ 1 − Θ (cid:0) m\n\nL\n\nHence, by Lemma 2, the distance satisfies\n\nW2(ρK, π) = O\n\n(cid:18)(cid:16)\n\n1 − Θ\n\n(cid:16) m L\n\n(cid:17)(cid:17)K(cid:19)\n\nW2(ρ0, π)\n\n(cid:1) of HMC using a constant integration time η(K)\n\nafter K iterations of ideal HMC with the constant integration time. On the other hand, for general smooth strongly convex potentials f (·), Chen & Vempala (2019) show the same convergence rate 1 − Θ (cid:0) m , where c > 0 is a universal constant. Therefore, under the constant integration time, HMC needs O(κ log 1 (cid:15) ) iterations to reach error W2(ρK, π) ≤ (cid:15), where κ = L m is condition number. Furthermore, they also show that the relaxation time of ideal HMC with a constant integration time is Ω(κ) for the Gaussian case.\n\nk = c√\n\nL\n\nL\n\n2.3 CHEBYSHEV POLYNOMIALS\n\nWe denote ΦK(·) the degree-K Chebyshev polynomial of the first kind, which is defined by:\n\nΦK(x) =\n\n \n\n\n\ncos(K arccos(x)) cosh(K arccosh(x)) (−1)Kcosh(K arccosh(x))\n\nif x ∈ [−1, 1], if x > 1, if x < 1.\n\n(6)\n\nOur proposed integration time is built on a scaled-and-shifted Chebyshev polynomial, defined as:\n\n ̄ΦK(λ) :=\n\nΦK(h(λ)) ΦK(h(0))\n\n,\n\n(7)\n\nwhere h(·) is the mapping h(λ) := L+m−2λ L−m . Observe that the mapping h(·) maps all λ ∈ [m, L] into the interval [−1, 1]. The roots of the degree-K scaled-and-shifted Chebyshev polynomial ̄ΦK(λ) are\n\n(Chebyshev roots)\n\nr(K)\n\nk\n\n:=\n\nL + m 2\n\n−\n\nL − m 2\n\ncos\n\n(cid:18) (k − 1 K\n\n2 )π\n\n(cid:19)\n\n,\n\n(8)\n\nwhere k = 1, 2, . . . , K, i.e., ̄ΦK(r(K) scaled-and-shifted Chebyshev polynomial ̄ΦK. Lemma 3. (e.g., Section 2.3 in d’Aspremont et al. (2021)) For any positive integer K, we have\n\n) = 0. We now recall the following key result regarding the\n\nk\n\nmaxλ∈[m,L]\n\n(cid:12) (cid:12) ̄ΦK(λ)(cid:12)\n\n(cid:12) ≤ 2\n\n(cid:16)\n\n1 − 2\n\n√ m√ √\nL+\n\nm\n\n(cid:17)K\n\n= O\n\n(cid:16)(cid:0)1 − Θ (cid:0)(cid:112) m\n\nL\n\n(cid:1)(cid:1)K(cid:17)\n\n.\n\n(9)\n\nThe proof of Lemma 3 is in Appendix B.\n\n3 CHEBYSHEV INTEGRATION TIME\n\nWe are now ready to introduce our scheme of time-varying integration time. Let K be the pre-specified total number of iterations of HMC. Our proposed method will first permute the array [1, 2, . . . , K] before executing HMC for K iterations. Denote σ(k) the kth element of the array [1, 2, . . . , K] after an arbitrary permutation σ. Then, we propose to set the integration time of HMC at iteration k, i.e., set η(K)\n\n, as follows:\n\nk\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Left: Set K = 400, m = 1 and L = 100.\n\nshev integration time (10)) on the subfigure represents maxλ∈{m,m+0.1,...,L} (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nv.s. k, while the blue dash line (Constant\n\ns=1cos\n\n(cid:114) λ\n\n(K) σ(s)\n\nΠk\n\n(cid:32)\n\nπ 2\n\nr\n\nmaxλ∈{m,m+0.1,...,L}\n\ns=1cos\n\nv.s. k. Since the cosine product con-\n\ntrols the convergence rate of the W2 distance by Lemma 2, this confirms the acceleration via using the proposed scheme of Chebyshev integration over the constant integration time (Chen & Vempala, 2019; Vishnoi, 2021).\n\n(cid:16)√\n\n2λη(K)\n\ns\n\n(cid:17)(cid:12) (cid:12) (cid:12) =\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nΠk\n\ns=1cos\n\n(cid:18)\n\n(cid:113) λ\n\nL\n\nπ 2\n\n(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Πk (cid:12)\n\nThe green solid line (Cheby- (cid:16)√ (cid:17)(cid:12) (cid:12) (cid:12) =\n\n2λη(K)\n\n(cid:12) (cid:12)Πk (cid:12)\n\ns=1cos\n\ns\n\nintegration time (5)) represents\n\nRight: ψ(x) =\n\n√\n\ncos( π\n\n2 1−x\n\nx)\n\nv.s. x.\n\n(Chebyshev integration time)\n\nη(K) k =\n\nπ 2\n\n(cid:113)\n\n1\n\n2r(K)\n\nσ(k)\n\n.\n\n(10)\n\nWe note the usage of the permutation σ is not needed in our analysis below; however, it seems to help improve performance in practice. Specifically, though the guarantees of HMC at the final iteration K provided in Theorem 1 and Lemma 4 below is the same regardless of the permutation, the progress of HMC varies under different permutations of the integration time, which is why we recommend an arbitrary permutation of the integration time in practice.\n\nOur main result is the following improved convergence rate of HMC under the Chebyshev integration time, for quadratic potentials. Theorem 1. Denote the target distribution π ∝ exp(−f (x)) = N (0, Λ−1), where f (x) is defined on (3), and denote the condition number κ := L m . Let ρK be the distribution of xK generated by Algorithm 1 at the final iteration K. Then, we have\n\nW2(ρK, π) ≤ 2\n\n(cid:18)\n\n1 − 2\n\n√\n\n√\n\nm\n\n√\n\n(cid:19)K\n\nm\n\n(cid:32)(cid:18)\n\nW2(ρ0, π) = O\n\n1 − Θ\n\n(cid:19)(cid:19)K(cid:33)\n\n(cid:18) 1 √\nκ\n\nW2(ρ0, π).\n\nL +\n\nConsequently, the total number of iterations K such that the Wasserstein-2 distance satisfies\n\nW2(ρK, π) ≤ (cid:15) is O (cid:0)√\n\nκ log 1 (cid:15)\n\n(cid:1).\n\nTheorem 1 shows an accelerated linear rate 1 − Θ using Chebyshev integration time, and hence improves the previous result of 1 − Θ (cid:0) 1 (cid:1) as discussed above. The proof of Theorem 1 relies on the following lemma, which upper-bounds the cosine products that appear in the bound of the W2 distance in Lemma 2 by the scaled-and-shifted Chebyshev polynomial ̄ΦK(λ) on (7).\n\nκ\n\nκ\n\n(cid:16) 1√\n\n(cid:17)\n\nLemma 4. Denote |P Cos\n\nK (λ)| :=\n\nany positive integer K,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nΠK\n\nk=1cos\n\n(cid:32)\n\nπ 2\n\n(cid:114) λ r(K)\n\nσ(k)\n\n(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n. Suppose λ ∈ [m, L]. Then, we have for\n\n|P Cos\n\nK (λ)| ≤ (cid:12)\n\n(cid:12) ̄ΦK(λ)(cid:12) (cid:12) .\n\n(11)\n\nThe proof of Lemma 4 is available in Appendix C. Figure 1 compares the cosine product (cid:17)(cid:12) (cid:12) maxλ∈[m,L] (cid:12) in Lemma 2 of using the proposed integration time and that\n\n2λη(K)\n\n(cid:12) (cid:12) (cid:12)Πk\n\ns=1cos\n\n(cid:16)√\n\ns\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: HMC WITH CHEBYSHEV INTEGRATION TIME\n\nk\n\n:= L+m\n\n1: Given: a potential f (·), where π(x) ∝ exp(−f (x)) and f (·) is L-smooth and m-strongly convex. 2: Require: number of iterations K and the step size of the leapfrog steps θ. 3: Define r(K) 4: Arbitrarily permute the array [1, 2, . . . , K]. Denote σ(k) the kth element of the array after permutation. 5: for k = 1, 2, . . . , K do 6: 7:\n\nSample velocity ξk ∼ N (0, Id). Set integration time η(K)\n\n, for k = 1, . . . , K.\n\n2 − L−m\n\n(cid:16) (k− 1 K\n\n2 cos\n\n2 )π\n\n(cid:17)\n\n1\n\n.\n\nk ← π\n\n2\n\n(cid:114)\n\n2r\n\n(K) σ(k)\n\n(K) k\n\nθ (cid:99).\n\n8: 9:\n\n10: 11: 12:\n\nSet the number of leapfrog steps Sk ← (cid:98) η ( ̄x0, ̄v0) ← (xk−1, ξk) % Leapfrog steps for s = 0, 2, . . . , Sk − 1 do = ̄vs − θ 2 ∇f ( ̄xs);\n\n2\n\n ̄vs+ 1 end for % Metropolis filter\n\n13:\n\nCompute the acceptance ratio αk = min\n\n(cid:16)\n\n1,\n\nexp(−H( ̄xSk\n\n, ̄vSk exp(−H( ̄x0, ̄v0))\n\n))\n\n(cid:17)\n\n.\n\n ̄xs+1 = ̄xs + θ ̄vs+ 1\n\n;\n\n ̄vs+1 = ̄vs+ 1\n\n2\n\n− θ\n\n2 ∇f ( ̄xs+1);\n\n2\n\nDraw ζ ∼ Uniform[0, 1]. If ζ < αk then xk ← ̄xSk\n\nxk ← xk−1.\n\n14: 15: 16: 17: 18: 19: end for\n\nElse\n\nof using the constant integration time, which illustrates acceleration via the proposed Chebyshev integration time.\n\nWe now provide the proof of Theorem 1.\n\nProof. (of Theorem 1) From Lemma 2, we have\n\nW2(ρK, π) ≤ maxj∈[d]\n\n(cid:12) (cid:12) (cid:12)ΠK\n\nk=1cos\n\n(cid:16)(cid:112)2λjη(K)\n\nk\n\n(cid:17)(cid:12) (cid:12) (cid:12) · W2(ρ0, π).\n\n(12)\n\nWe can upper-bound the cosine product of any j ∈ [d] as,\n\n(cid:12) (cid:12) (cid:12)ΠK\n\nk=1cos\n\n(cid:16)(cid:112)2λjη(K)\n\nk\n\n(cid:17)(cid:12) (cid:12) (cid:12)\n\n(a) =\n\nΠK\n\nk=1cos\n\n(cid:32)\n\n(cid:114) λj r(K)\n\nσ(k)\n\nπ 2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(b)\n\n≤ (cid:12)\n\n(cid:12) ̄ΦK(λj)(cid:12) (cid:12)\n\n(cid:16)\n\n(c) ≤ 2\n\n1 − 2\n\n√ m√ √\nL+\n\nm\n\n(cid:17)K\n\n,\n\n(13) where (a) is due to the use of Chebyshev integration time (10), (b) is by Lemma 4, and (c) is by Lemma 3. Combining (12) and (13) leads to the result.\n\nHMC with Chebyshev Integration Time for General Distributions To sample from general strongly log-concave distributions, we propose Algorithm 2, which adopts the Verlet integrator (a.k.a. the leapfrog integrator) to simulate the Hamiltonian flow HMCη(·, ξ) and uses Metropolis filter to correct the bias. It is noted that the number of leapfrog steps Sk in each iteration k is equal to the integration time η(K) divided by the step size θ used in the leapfrog steps. More precisely, we have Sk = (cid:98) η(K)\n\nθ (cid:99) in iteration k of HMC.\n\nk\n\nk\n\n4 EXPERIMENTS\n\nWe now evaluate HMC with the proposed Chebyshev integration time (Algorithm 2) and HMC with the constant integration time (Algorithm 2 with line 7 replaced by the constant integration time (5)) in several tasks. For all the tasks in the experiments, the total number of iterations of HMCs is set to be K = 10, 000, and hence we collect K = 10, 000 samples along the trajectory. For the step size θ in the leapfrog steps, we let θ ∈ {0.001, 0.005, 0.01, 0.05}. To evaluate the methods, we\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Ideal HMC with K = 10, 000 iterations for sampling from a Gaussian N (μ, Σ), where μ =\n\n(cid:21)\n\n(cid:20)0 0\n\nand\n\nΣ =\n\n(cid:20)1 0\n\n(cid:21)\n\n0 100\n\n. Here, Cheby. (W/) is ideal HMC with a arbitrary permutation of the Chebyshev integration\n\ntime, while Cheby. (W/O) is ideal HMC without a permutation; and Const. refers to using the constant integration time (5).\n\nMethod\n\nCheby. (W/) Cheby. (W/O) Const.\n\nMean ESS\n\nMin ESS\n\n10399.00811 ± 347.25021 10197.09964 ± 276.94894 7692.00382 ± 207.19628\n\n7172.50338 ± 257.21244 7043.55293 ± 284.78037 5533.26519 ± 213.31943\n\ncompute effective sample size (ESS), which is a common performance metric of HMCs (Girolami & Calderhead, 2011; Brofos & Lederman, 2021; Hirt et al., 2021; Riou-Durand & Vogrinc, 2022; Hoffman et al., 2021; Hoffman & Gelman, 2014; Steeg & Galstyan, 2021), by using the toolkit ArViz (Kumar et al., 2019). The ESS of a sequence of N dependent samples is computed based on the autocorrelations within the sequence at different lags: ESS := N/(1 + 2 (cid:80) k γ(k)), where γ(k) is an estimate of the autocorrelation at lag k. We consider 4 metrics, which are (1) Mean ESS: the average of ESS of all variables. That is, ESS is computed for each variable/dimension, and Mean ESS is the average of them. (2) Min ESS: the lowest value of ESS among the ESSs of all variables; (3) Mean ESS/Sec.: Mean ESS normalized by the CPU time in seconds; (4) Min ESS/Sec.: Minimum ESS normalized by the CPU time in seconds. In the following tables, we denote “Cheby.” as our proposed method, and “Const.” as HMC with the the constant integration time (Vishnoi, 2021; Chen & Vempala, 2019). Each of the configurations is repeated 10 times, and we report the average and the standard deviation of the results. We also report the acceptance rate of the Metropolis filter (Acc. Prob) on the tables. Our implementation of the experiments is done by modifying a publicly available code of HMCs by Brofos & Lederman (2021). Code for our experiments can be found in the supplementary.\n\n4.1\n\nIDEAL HMC FLOW FOR SAMPLING FROM A GUSSIAN WITH A DIAGONAL COVARIANCE\n\nBefore evaluating the empirical performance of Algorithm 2 in the following subsections, here we discuss and compare the use of a arbitrary permutation of the Chebyshev integration time and that without permutation (as well as that of using a constant integration time). We simulate ideal HMC\n\n(cid:21) (cid:20)0 0\n\n(cid:20)1 0\n\n(cid:21)\n\n0 100\n\nfor sampling from a Gaussian N (μ, Σ), where μ =\n\nand Σ =\n\n. It is noted that ideal\n\nHMC flow for this case has a closed-form solution as (4) shows. The result are reported on Table 1.\n\nFrom the table, the use of a Chebyshev integration time allows to obtain a larger ESS than that from using a constant integration time, and a arbitrary permutation helps get a better result. An explanation is that the ESS is a quantity that is computed along the trajectory of a chain, and therefore a permutation of the integration time could make a difference. We remark that the observation here (a arbitrary permutation of time generates a larger ESS) does not contradict to Theorem 1, since Theorem 1 is about the guarantee in W2 distance at the last iteration K.\n\n4.2 SAMPLING FROM A GAUSSIAN\n\nWe sample N (μ, Σ), where μ =\n\n(cid:20) 1 0.5 m is approximately 0.01 and the smoothness constant L is approximately 1. Table 2 shows the results. HMC with Chebyshev integration time consistently outperforms that of using the constant integration time in terms of all the metrics: Mean ESS, Min ESS, Mean ESS/Sec, and Min ESS/Sec.\n\n. Therefore, the strong convexity constant\n\n(cid:21) (cid:20)0 1\n\nand Σ =\n\n0.5 100\n\n(cid:21)\n\nWe also plot two quantities throughout the iterations of HMCs on Figure 2. Specifically, Sub-figure (a) on Figure 2 plots the size of the difference between the targeted covariance Σ and an estimated covariance ˆΣk at each iteration k of HMC, where ˆΣk is the sample covariance of 10, 000 samples collected from a number of 10, 000 HMC chains at their kth iteration. Sub-figure (b) plots a discrete TV distance that is computed as follows. We use a built-in function of Numpy to sample 10, 000 samples from the target distribution, while we also have 10, 000 samples collected from a number\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Sampling from a Gaussian distribution. We report 4 metrics regarding ESS (the higher the better), please see the main text for their definitions.\n\nStep Size Method\n\nMean ESS\n\nMin ESS\n\nMean ESS/Sec.\n\nMin. ESS/Sec.\n\nAcc. Prob\n\n0.001 0.001 0.005 0.005 0.01 0.01 0.05 0.05 0.1 0.1\n\nCheby. Const. Cheby. Const. Cheby. Const. Cheby. Const. Cheby. Const.\n\n5187.28 ± 261.13 1912.76 ± 72.10 5146.71 ± 257.65 1926.71 ± 136.53 5127.90 ± 211.46 1832.87 ± 77.47 5133.67 ± 195.07 1849.15 ± 92.75 4948.46 ± 144.03 1852.79 ± 132.95\n\n307.09 ± 21.92 39.87 ± 13.77 304.126 ± 19.09 32.83 ± 9.57 279.59 ± 38.09 35.71 ± 11.74 316.87 ± 36.27 34.98 ± 14.70 281.66 ± 44.79 38.17 ± 16.35\n\n20.28 ± 1.74 15.87 ± 0.89 97.84 ± 9.23 80.31 ± 4.39 184.26 ± 20.99 147.53 ± 12.59 871.72 ± 88.73 615.73 ± 30.16 1492.96 ± 166.21 1035.54 ± 82.34\n\n1.20 ± 0.11 0.33 ± 0.11 5.79 ± 0.68 1.37 ± 0.39 10.01 ± 1.52 2.85 ± 0.95 53.54 ± 6.22 11.70 ± 5.07 84.39 ± 13.04 21.44 ± 9.51\n\n1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00\n\n(a) (cid:107)Σ − ˆΣk(cid:107)F v.s. iteration k\n\n(b) discrete TV(ˆπ, ˆρk) v.s. iteration k\n\nFigure 2: Sampling from a Gaussian distribution. Both lines correspond to HMCs with the same step size h = 0.05 used in the leapfrog steps (but with different schemes of the integration time). Please see the main text for the precise definitions of the quantities and the details of how we compute them.\n\nof 10, 000 HMC chains at each iteration k. Using these two sets of samples, we construct two histograms with 30 number of bins for each dimension, we denote them as ˆπ and ˆρk. The discrete TV(ˆπ, ˆρk) at iteration k is 0.5 times the sum of the absolute value of the difference between the number of counts of all the pairs of the bins divided by 10, 000, which serves as a surrogate of the Wasserstein-2 distance between the true target π and ρk from HMC, since computing or estimating the true Wasserstein distance is challenging.\n\n4.3 SAMPLING FROM A MIXTURE OF TWO GAUSSIANS\n\nFor a vector a ∈ Rd and a positive definite matrix Σ ∈ Rd×d, we consider sampling from a mixture of two Gaussians N (a, Σ) and N (−a, Σ) with equal weights. Denote b := Σ−1a and Λ := Σ−1. The potential is f (x) = 1 Λ − log(1 + exp(−2x(cid:62)b)), and its gradient is 2 (cid:107)x − a(cid:107)2 ∇f (x) = Λx − b + 2b(1 + exp(−2x(cid:62)b))−1. For each dimension i ∈ [d], we set a[i] = 2d and set the covariance Σ = diag1≤i≤d( i d ). The potential is strongly convex if a(cid:62)Σ−1a < 1, see e.g., Riou-Durand & Vogrinc (2022). We set d = 10 in the experiment, and simply use the smallest and the largest eigenvalue of Λ to approximate the strong convexity constant m and the smoothness constant L of the potential, which are ˆm = 1 and ˆL = 10 in this case. Table 3 shows that the proposed method generates a larger effective sample size than the baseline.\n\n√\n\ni\n\n4.4 BAYESIAN LOGISTIC REGRESSION\n\nWe also consider Bayesian logistic regression to evaluate the methods. Given an observation (zi, yi), where zi ∈ Rd and yi ∈ {0, 1}, the likelihood function is modeled as p(yi|zi, w) = i w) . Moreover, the prior on the model parameter w is assumed to follow a Gaussian distribution, p(w) = N (0, α−1Id), where α > 0 is a parameter. The goal is to sample w ∈ Rd from the posterior, p(w|{zi, yi}n i=1) = p(w)Πn i=1p(yi|zi, w), where n is the number of data points in a dataset. The potential function f (w) can be written as\n\n1+exp(−yiz(cid:62)\n\n1\n\nf (w) = (cid:80)n\n\ni=1 fi(w), where fi(w) = log (cid:0)1 + exp(−yiw(cid:62)zi)(cid:1) + α (cid:107)w(cid:107)2 2n .\n\n(14)\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Sampling from a mixture of two Gaussians\n\nStep Size Method\n\nMean ESS\n\nMin ESS\n\nMean ESS/Sec.\n\nMin. ESS/Sec.\n\nAcc. Prob\n\n0.001 0.001 0.005 0.005 0.01 0.01 0.05 0.05 0.1 0.1\n\nCheby. Const. Cheby. Const. Cheby. Const. Cheby. Const. Cheby. Const.\n\n2439.86 ± 71.83 845.44 ± 31.42 2399.50 ± 100.12 876.61 ± 25.62 2341.35 ± 89.99 860.61 ± 20.39 2214.19 ± 87.27 853.40 ± 41.05 2064.42 ± 67.44 632.70 ± 22.78\n\n815.20 ± 83.82 261.14 ± 34.34 784.06 ± 82.07 277.72 ± 30.62 794.27 ± 48.75 235.33 ± 33.73 748.66 ± 46.18 265.70 ± 37.41 657.45 ± 60.44 182.88 ± 37.10\n\n22.68 ± 0.93 12.90 ± 0.52 105.97 ± 8.78 63.80 ± 4.67 194.81 ± 23.51 110.62 ± 14.09 761.59 ± 68.88 376.54 ± 67.83 1162.67 ± 84.19 450.53 ± 93.60\n\n7.57 ± 0.81 3.98 ± 0.53 34.58 ± 4.12 20.22 ± 2.62 66.30 ± 9.89 30.40 ± 6.34 256.51 ± 13.76 116.45 ± 22.23 370.07 ± 41.11 132.58 ± 43.91\n\n0.89 ± 0.00 0.91 ± 0.00 0.89 ± 0.00 0.91 ± 0.00 0.88 ± 0.00 0.91 ± 0.00 0.89 ± 0.00 0.91 ± 0.00 0.90 ± 0.00 0.92 ± 0.00\n\nWe set α = 1 in the experiments. We consider three datasets: Heart, Breast Cancer, and Diabetes binary classification datasets, which are all publicly available online. To approximate the strong convexity constant m and the smoothness constant L of the potential f (w), we compute the smallest eigenvalue and the largest eigenvalue of the Hessian ∇2f (w) at the maximizer of the posterior, and we use them as estimates of m and L respectively. We apply Newton’s method to approximately find the maximizer of the posterior. The experimental results are reported on Table 4 in Appendix E.1 due to the space limit, which show that our method consistently outperforms the baseline.\n\n4.5 SAMPLING FROM A hard DISTRIBUTION\n\nWe also consider sampling from a step-size-dependent distribution π(x) ∝ exp(−fh(x)), where the potential fh(·) is κ-smooth and 1-strongly convex. The distribution is considered in Lee et al. (2021a) for showing a lower bound regarding certain Metropolized sampling methods using a constant integration time and a constant step size h of the leapfrog integrator. More concretely, the potential is\n\nfh(x) := (cid:80)d\n\ni=1 f (h)\n\ni\n\n(xi), where f (h)\n\ni\n\n(xi) =\n\n(cid:40) 1\n\ni = 1\n\n2 x2 i , i − κh 3 x2\n\nκ\n\n3 cos\n\n(cid:16) xi√\n\nh\n\n(cid:17)\n\n,\n\n2 ≤ i ≤ d.\n\n(15)\n\nIn the experiment, we set κ = 50 and d = 10. The results are reported on Table 5 in Appendix E.2. The scheme of the Chebyshev integration time is still better than the constant integration time for this task.\n\n5 DISCUSSION AND OUTLOOK\n\nThe Chebyshev integration time shows promising empirical results for sampling from a various of strongly log-concave distributions. On the other hand, the theoretical guarantee of acceleration that we provide in this work is only for strongly convex quadratic potentials. Therefore, a direction left open by our work is establishing some provable acceleration guarantees for general strongly log-concave distributions. However, unlike quadratic potentials, the output (position, velocity) of a HMC flow does not have a closed-form solution in general, which makes the analysis much more challenging. A starting point might be improving the analysis of Chen & Vempala (2019), where a contraction bound of two HMC chains under a small integration time η = O( 1√ ) is shown. Since (cid:17)\n\nthe scheme of the Chebyshev integration time requires a large integration time η = Θ\n\nat some\n\n(cid:16) 1√\n\nL\n\nm\n\niterations of HMC, a natural question is whether a variant of the result of Chen & Vempala (2019)\n\ncan be extended to a large integration time η = Θ\n\n. We state as an open question: can ideal\n\n(cid:16) 1√\n\n(cid:17)\n\nm\n\nHMC with a scheme of time-varying integration time achieve an accelerated rate O( general smooth strongly log-concave distributions?\n\n√\n\nκ log 1\n\n(cid:15) ) for\n\nThe topic of accelerating HMC with provable guarantees is underexplored, and we hope our work can facilitate the progress in this field. After the preprint of this work was available on arXiv, Jiang (2022) proposes a randomized integration time with partial velocity refreshment and provably shows that ideal HMC with the proposed machinery has the accelerated rate for sampling from a Gaussian distribution. Exploring any connections between the scheme of Jiang (2022) and ours can be an interesting direction.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe thank the reviewers for constructive feedback, which helps improve the presentation of this paper.\n\nREFERENCES\n\nNaman Agarwal, Surbhi Goel, and Cyril Zhang. Acceleration via fractal learning rate schedules.\n\nICML, 2021.\n\nMichael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv:1701.02434,\n\n2017.\n\nNawaf Bou-Rabee and Andreas Eberle. Mixing time guarantees for unadjusted Hamiltonian Monte\n\nCarlo. arXiv:2105.00887, 2021.\n\nNawaf Bou-Rabee and Jesus Maria Sanz-Serna. Randomized Hamiltonian Monte Carlo. Annals of\n\nApplied Probability, 2017.\n\nJames A. Brofos and Roy R. Lederman. Evaluating the implicit midpoint integrator for Riemannian\n\nmanifold Hamiltonian Monte Carlo. ICML, 2021.\n\nTianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo.\n\nICML, 2014.\n\nYuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast mixing of Metropolized\n\nHamiltonian Monte Carlo: Benefits of multi-step gradients. JMLR, 2020.\n\nZongchen Chen and Santosh S Vempala. Optimal convergence rate of Hamiltonian Monte Carlo for strongly logconcave distributions. International Conference on Randomization and Computation (RANDOM), 2019.\n\nSinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin J. Stromme.\n\nExponential ergodicity of mirror-langevin diffusions. NeurIPS, 2020.\n\nArnak S. Dalalyan. Theoretical guarantees for approximate sampling from a smooth and log-concave\n\ndensity. Journal of the Royal Statistical Society: Series B, 2017.\n\nArnak S. Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using kinetic\n\nLangevin diffusions. Bernoulli, 2020.\n\nAlexandre d’Aspremont, Damien Scieur, and Adrien Taylor. Acceleration methods. Foundations and\n\nTrends in Optimization, 2021.\n\nSimon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics\n\nLetters B, 1987.\n\nJohn C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic\n\noptimization. SIAM Journal on Optimization, 2012.\n\nAlain Durmus, Eric Moulines, and Eero Saksman. On the convergence of Hamiltonian Monte Carlo.\n\narXiv:1705.00166, 2017.\n\nAlain Durmus, Szymon Majewski, and Bła ̇zej Miasojedow. Analysis of Langevin Monte Carlo via\n\nconvex optimization. JMLR, 2019.\n\nAbraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization\n\nin the bandit setting: gradient descent without a gradient. SODA, 2005.\n\nMark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo\n\nmethods. Journal of the Royal Statistical Society, 2011.\n\nMarcel Hirt, Michalis K. Titsias, and Petros Dellaportas. Entropy-based adaptive Hamiltonian Monte\n\nCarlo. NeurIPS, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMatthew D. Hoffman and Andrew Gelman. The No-U-Turn sampler: Adaptively setting path lengths\n\nin Hamiltonian Monte Carlo. JMLR, 2014.\n\nMatthew D. Hoffman, Alexey Radul, and Pavel Sountsov. An adaptive-MCMC scheme for setting\n\ntrajectory lengths in Hamiltonian Monte Carlo. AISTATS, 2021.\n\nQijia Jiang. On the dissipation of ideal hamiltonian monte carlo sampler. arXiv:2209.07438, 2022.\n\nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape\n\nsaddle points efficiently. ICML, 2017.\n\nDaphne Koller and Nir Friedman. Probabilistic graphical models: Principles and techniques. MIT\n\nPress, 2009.\n\nRavin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo Martin. Arviz a unified library for exploratory analysis of bayesian models in python. The Journal of Open Source Software, 2019.\n\nYin Tat Lee, Ruoqi Shen, and Kevin Tian. Lower bounds on Metropolized sampling methods for\n\nwell-conditioned distributions. NeurIPS, 2021a.\n\nYin Tat Lee, Ruoqi Shen, and Kevin Tian. Structured logconcave sampling with a restricted gaussian\n\noracle. COLT, 2021b.\n\nDaniel Levy, Matthew D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte\n\nCarlo with neural networks. ICLR, 2018.\n\nRuilin Li, Hongyuan Zha, and Molei Tao. Sqrt(d) dimension dependence of Langevin Monte Carlo.\n\nICLR, 2022.\n\nYi-An Ma, Niladri S. Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L. Bartlett, and Michael I.\n\nJordan. Is there an analog of Nesterov acceleration for MCMC? Bernoulli, 2021.\n\nOren Mangoubi and Aaron Smith. Mixing of Hamiltonian Monte Carlo on strongly logconcave\n\ndistributions 2: Numerical integrators. AISTATS, 2019.\n\nOren Mangoubi and Aaron Smith. Mixing of Hamiltonian Monte Carlo on strongly logconcave\n\ndistributions 1: Continuous dynamics. Annals of Applied Probability, 2021.\n\nOren Mangoubi and Nisheeth K. Vishnoi. Dimensionally tight bounds for second-order Hamiltonian\n\nMonte Carlo. NeurIPS, 2018.\n\nRadford M. Neal. MCMC using Hamiltonian dynamics. arXiv:1206.1901, 2012.\n\nYurii Nesterov. Introductory lectures on convex optimization: a basic course. Springer, 2013.\n\nFabian Pedregosa. Acceleration without momentum, 2021. URL http://fa.bianp.net/\n\nblog/2021/no-momentum/.\n\nB.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-\n\ntional Mathematics and Mathematical Physics, 1964.\n\nLionel Riou-Durand and Jure Vogrinc. Metropolis Adjusted Langevin trajectories: a robust alternative\n\nto Hamiltonian Monte Carlo. arXiv:2202.13230, 2022.\n\nRuslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using Markov\n\nchain Monte Carlo. ICML, 2008.\n\nGreg Ver Steeg and Aram Galstyan. Hamiltonian dynamics with non-newtonian momentum for rapid\n\nsampling. NeurIPS, 2021.\n\nNilesh Tripuraneni, Mark Rowland, Zoubin Ghahramani, and Richard Turner. Magnetic Hamiltonian\n\nMonte Carlo. ICML, 2017.\n\nSantosh S. Vempala and Andre Wibisono. Rapid convergence of the Unadjusted Langevin Algorithm:\n\nIsoperimetry suffices. NeurIPS, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nNisheeth K. Vishnoi. An introduction to Hamiltonian Monte Carlo method for sampling.\n\narXiv:2108.12107, 2021.\n\nJun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. Escaping saddle points faster with stochastic\n\nmomentum. ICLR, 2020.\n\nJun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. A modular analysis of provable acceleration via Polyak’s momentum: Training a wide ReLU network and a deep linear network. ICML, 2021.\n\nJun-Kun Wang, Chi-Heng Lin, Andre Wibisono, and Bin Hu. Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out. ICML, 2022.\n\nMax Welling and Yee Whye Teh. Bayesian learning via Stochastic Gradient Langevin dynamics.\n\nICML, 2011.\n\nFlorian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the Bayes posterior in deep neural networks really. ICML, 2020.\n\nDifan Zou and Quanquan Gu. On the convergence of Hamiltonian Monte Carlo with stochastic\n\ngradients. ICML, 2021.\n\nA A CONNECTION BETWEEN OPTIMIZATION AND SAMPLING\n\nTo provide an intuition of why the technique of Chebyshev polynomials can help accelerate HMC for the case of the strongly convex quadratic potentials, we would like to describe the work of gradient descent with the Chebyshev step sizes Agarwal et al. (2021) in more detail, because we are going to draw a connection between optimization and sampling to showcase the intuition. Agarwal et al. (2021) provably show that gradient descent with a scheme of step sizes based on the Chebyshev Polynomials has an accelerated rate for minimizing strongly convex quadratic functions compared to GD with a constant step size, and their experiments show some promising results for minimizing smooth strongly convex functions beyond quadratics via the proposed scheme of step sizes. More precisely, 2 w(cid:62)Aw, where A ∈ Rd×d is a positive definite matrix which has eigenvalues define f (w) = 1 L := λ1 ≥ λ2 ≥ · · · ≥ λd =: m. Agarwal et al. (2021) consider applying gradient descent\n\nwk+1 = wk − ηk∇f (wk)\n\nto minimize f (·), where ηk is the step size of gradient descent at iteration k. Let w be the unique global minimizer of f (·). It is easy to show that the dynamic of the distance evolves as\n\nwk+1 − w∗ = (Id − ηkA)(Id − ηk−1A) · · · (Id − η1A)(w1 − w∗).\n\nHence, the size of the distance to w∗ at iteration K + 1 is bounded by\n\n(cid:107)wK+1 − w∗(cid:107) ≤ max j∈[d]\n\n|\n\nK (cid:89)\n\n(1 − ηkλj)|(cid:107)w1 − w∗(cid:107).\n\nk=1\n\nThis shows that the convergence rate of GD is governed by maxj∈[d] | (cid:81)K k=1(1 − ηkλj)|. By setting ηk as the inverse of the Chebyshev root r(K) σ(k) (see (8) for the definition), the polynomial (cid:81)K k=1(1 − ηkλ) is actually the K-degree scale-and-shifted polynomial, (cid:18) i.e., (cid:81)K = ̄Φk(λ) (see (7) for the definition). It is well-known\n\nor any permuted root r(K)\n\nk=1(1 − ηkλ) = (cid:81)K\n\nk=1\n\n(cid:19)\n\nk\n\n1 − λ r(K)\n\nσ(k)\n\nin the literature of optimization and numerical linear algebra that the K-degree scale-and-shifted polynomial satisfies\n\n(cid:12) ̄ΦK(λ)(cid:12) (cid:12)\n\n(cid:12) ≤ 2\n\n(cid:18)\n\n1 − 2\n\n√\n\nmax λ∈[m,L]\n\n√\n\nm\n\n√\n\n(cid:19)K\n\n(cid:32)(cid:18)\n\n= O\n\n1 − Θ\n\n(cid:18)(cid:114) m L\n\n(cid:19)(cid:19)K(cid:33)\n\n,\n\nm\n\nL +\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nwhich is restated in Lemma 3 and its proof is replicated in Appendix B of our paper for the reader’s convenience. Applying this result, one gets a simple proof of the accelerated linear rate of GD with the proposed scheme of step sizes for minimizing quadratic functions. A nice blog article by Pedregosa (2021) explains this in detail.\n\nNow we are ready to highlight its connection with HMC. In Lemma 1 of the paper, we restate a known result in HMC literature, where its proof is also replicated in Appendix B for the reader’s convenience. The lemma indicates that the convergence rate of HMC is governed by maxj∈[d] | (cid:81)K )|. By way of comparison to that of GD for minimizing quadratic functions, i.e., maxj∈[d] | (cid:81)K k=1(1 − ηkλj)|, it appears that they share some similarity, which made us wonder if we could bound the former by the latter. We show in Lemma 4 that cos( π x) ≤ 1 − x, which holds for all x ≥ 0, and consequently,\n\nk=1 cos((cid:112)2λjη(K)\n\n√\n\nk\n\n2\n\n|P Cos\n\nK (λ)| :=\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:89)\n\nk=1\n\n\n\ncos\n\n\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nπ 2\n\nλ r(K)\n\nσ(k)\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nK (cid:89)\n\nk=1\n\n\n\n1 −\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\nλ r(K)\n\nσ(k)\n\n= (cid:12)\n\n(cid:12) ̄ΦK(λ)(cid:12) (cid:12) ,\n\nThe key lemma above implies that if we set the integration time as η(K)\n\nk = π\n\n2\n\n(cid:113)\n\n1 2r(K)\n\nσ(k)\n\n, then we get\n\nacceleration of HMC.\n\nB PROOFS OF LEMMAS IN SECTION 2\n\nWe restate the lemmas for the reader’s convenience.\n\n(Vishnoi, 2021) Let x0, y0 ∈ Rd. Consider the following coupling: (xt, vt) = Lemma 1. HMCt(x0, ξ) and (yt, ut) = HMCt(y0, ξ) for some ξ ∈ Rd. Then for all t ≥ 0 and for all j ∈ [d], it holds that\n\nxt[j] − yt[j] = cos\n\n(cid:16)(cid:112)2λjt\n\n(cid:17)\n\n× (x0[j] − y0[j]).\n\nProof. Given (xt, vt) := HMCt(x0, ξ) and (yt, ut) := HMCt(y0, ξ), we have dvt −∇f (xt) + ∇f (yt) = 2Λ(yt − xt). Therefore, we have d2(xt[j]−yt[j]) for all j ∈ [d]. Because of the initial condition dx0[j] implies that xt[j] − yt[j] = cos (cid:0)(cid:112)2λjt(cid:1) × (x0[j] − y0[j]).\n\ndt = = −2λj(xt[j] − yt[j]), dt = ξ[j], the differential equation\n\ndt = dy0[j]\n\ndt − dut\n\ndt2\n\nIt is noted that the result also follows directly from the explicit solution (4).\n\nLemma 2. (Vishnoi, 2021) Let π ∝ exp(−f ) = N (0, Λ−1) be the target distribution, where f (x) is defined on (3). Let ρK be the distribution of xK generated by Algorithm 1 at the final iteration K. Then for any ρ0 and any K ≥ 1, we have\n\nW2(ρK, π) ≤ maxj∈[d]\n\n(cid:12) (cid:12) (cid:12)ΠK\n\nk=1cos\n\n(cid:16)(cid:112)2λjη(K)\n\nk\n\n(cid:17)(cid:12) (cid:12) (cid:12) W2(ρ0, π).\n\nProof. Starting from x0 ∼ ρ0, draw an initial point y0 ∼ π such that (x0, y0) has the optimal W2-coupling between ρ0 and π. Consider the following coupling at each iteration k: (xk, vk) = (yk−1, ξk) where ξk ∼ N (0, I) is an independent HMCη(K) Gaussian. We collect {xk}K k=1 from Algorithm 1. We know each yk ∼ π, since π is a\n\n(xk−1, ξk) and (yk, uk) = HMCη(K)\n\nk=1 and {yk}K\n\nk\n\nk\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nstationary distribution of the HMC Markov chain. Then by Lemma 1 we have\n\nW 2\n\n2 (ρK, π) ≤ E[(cid:107)xK − yK(cid:107)2] = E[(cid:80)\n\nj∈[d](xK[j] − yK[j])2]\n\n= E[(cid:80) (cid:18)\n\n≤\n\nmaxj∈[d]\n\n(cid:18)\n\n=\n\nmaxj∈[d]\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\nΠK\n\nk=1cos\n\nj∈[d]\n\n(cid:17)\n\n(cid:16)(cid:112)2λjη(K) (cid:16)(cid:112)2λjη(K)\n\nk\n\nk\n\n(cid:17)2\n\n× (x0[j] − y0[j]) (cid:17)(cid:17)2(cid:19)\n\nE[(cid:80)\n\nj∈[d](x0[j] − y0[j])2]\n\n]\n\n(16)\n\nΠK\n\nk=1cos\n\nΠK\n\nk=1cos\n\n(cid:16)(cid:112)2λjη(K)\n\nk\n\n(cid:17)(cid:17)2(cid:19)\n\nW 2\n\n2 (ρ0, π),\n\nTaking the square root on both sides leads to the result.\n\nLemma 3. (e.g., Section 2.3 in d’Aspremont et al. (2021)) For any positive integer K, we have\n\nmaxλ∈[m,L]\n\n(cid:12) ̄ΦK(λ)(cid:12) (cid:12)\n\n(cid:12) ≤ 2\n\n(cid:16)\n\n1 − 2\n\n√ m√ √\nL+\n\nm\n\n(cid:17)K\n\n= O\n\n(cid:16)(cid:0)1 − Θ (cid:0)(cid:112) m\n\nL\n\n(cid:1)(cid:1)K(cid:17)\n\n.\n\n(17)\n\nProof. Observe that the numerator of ̄ΦK(λ) = ΦK (h(λ)) ΦK (h(0)) satisfies |ΦK(h(λ))| ≤ 1, since h(λ) ∈ [−1, 1] for λ ∈ [m, L] and that the Chebyshev polynomial satisfies |ΦK(·)| ≤ 1 when its argument is in [−1, 1] by the definition. It remains to bound the denominator, which is\n\nΦK(h(0)) = cosh\n\nK arccosh\n\n(cid:16)\n\n(cid:16) L+m\n\n(cid:17)(cid:17)\n\nL−m\n\n. Since\n\narccosh\n\n(cid:16) L+m\n\n(cid:17)\n\nL−m\n\n(cid:32)\n\n= log\n\nL+m\n\nL−m +\n\n(cid:114)(cid:16) L+m\n\n(cid:17)2\n\nL−m\n\n(cid:33)\n\n− 1\n\n= log(θ), where θ :=\n\n√ √\n\n√ √\n\nm m\n\n,\n\nL+ L−\n\nwe have\n\nΦK(h(0)) = cosh\n\n(cid:16)\n\nK arccosh\n\n(cid:16) L+m\n\n(cid:17)(cid:17)\n\nL−m\n\n= exp(K log(θ))+exp(−K log(θ))\n\n2\n\n= θK +θ−K\n\n2\n\n≥ θK 2 .\n\nCombing the above inequalities, we obtain the desired result:\n\nmax λ∈[m,L]\n\n(cid:12) ̄ΦK(λ)(cid:12) (cid:12)\n\n(cid:12) = max\n\nλ∈[m,L]\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nΦK(h(λ)) ΦK(h(0))\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n2\n\nθK = 2\n\n(cid:18)\n\n1 − 2\n\n√\n\n(cid:19)K\n\n√\n\nm\n\n√\n\nL +\n\nm (cid:18)(cid:114) m L\n\n(cid:32)(cid:18)\n\n= O\n\n1 − Θ\n\n(cid:19)(cid:19)K(cid:33)\n\n.\n\nC PROOF OF LEMMA 4\n\nLemma 4. Denote |P Cos\n\nK (λ)| :=\n\nany positive integer K,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nΠK\n\nk=1cos\n\n(cid:32)\n\nπ 2\n\n(cid:114) λ r(K)\n\nσ(k)\n\n(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n. Suppose λ ∈ [m, L]. Then, we have for\n\n|P Cos\n\nK (λ)| ≤ (cid:12)\n\n(cid:12) ̄ΦK(λ)(cid:12) (cid:12) .\n\n(18)\n\nProof. We use the fact that the K-degree scaled-and-shifted Chebyshev Polynomial can be written as,\n\n,\n\n(19)\n\n(cid:18)\n\n(cid:19)\n\n1 − λ r(K)\n\nσ(k)\n\n ̄ΦK(λ) = ΠK\n\nk=1\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nfor any permutation σ(·), since {r(K) to\n\nσ(k)} are its roots and ̄ΦK(0) = 1. So inequality (18) is equivalent\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nΠK\n\nk=1cos\n\n(cid:32)\n\nπ 2\n\n(cid:114) λ r(K)\n\nσ(k)\n\n(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nΠK\n\nk=1\n\n(cid:18)\n\n1 − λ r(K)\n\nσ(k)\n\n(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\n(20)\n\nTo show (20), let us analyze the mapping ψ(x) := 4 by continuity, and show that maxx:x≥0 |ψ(x)| ≤ 1, as (20) would be immediate. We have ψ(cid:48)(x) = − π √\n4\n\nfor x ≥ 0, x (cid:54)= 1, with ψ(1) = π\n\nx) + cos( π 2\n\n1−x sin( π\n\n2 1−x\n\nx)\n\n√\n\n√\n\nx\n\n2\n\n1\n\n1\n\ncos( π\n\n√\n\nx)\n\n(1−x)2 . Hence, ψ(cid:48)(x) = 0 when x) = 4\n\n√\n\n√\n\nx\n\nπ(1−x) .\n\ntan( π 2\n\n(21)\n\n√\n\nˆx)\n\ncos( π\n\nDenote an extreme point of ψ(x) as ˆx, which satisfies (21). Then, using (21), we have |ψ(ˆx)| = (cid:12) (cid:12) (cid:12) (cid:12) √\n\n16ˆx+π2(1−ˆx)2 . The denominator (cid:112)16ˆx + π2(1 − ˆx)2 has the smallest value at ˆx = 0, which\n\n(cid:12) (cid:12) (cid:12) (cid:12) −π(1−ˆx)\n\n, where we used cos( π\n\n16ˆx+π2(1−ˆx)2 or\n\n16ˆx+π2(1−ˆx)2\n\nˆx) =\n\n2 1−ˆx\n\nπ(1−ˆx)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n√\n\n√\n\n√\n\n=\n\nπ\n\n2\n\nmeans that the largest value of |ψ(x)| happens at x = 0, which is 1. The proof is now completed.\n\nD A COMPARISON OF THE TOTAL INTEGRATION TIME (JIANG, 2022)\n\nSince the Chebyshev integration time are set to be some large values at some steps of HMC, it is natural to ask if the number of steps to get an (cid:15) 2-Wasserstein distance is a fair metric. In this section, we consider the total integration time (cid:80)K to get an (cid:15) distance as another metric for the convergence. It is noted that the comparison between HMC with our integration time and HMC with the best constant integration time has been conducted by Jiang (2022), and our previous version did not have such a comparison. Below, we reproduce the comparision of Jiang (2022).\n\nk=1 η(K)\n\nk\n\nRecall the number of iterations to get an (cid:15) 2-Wasserstein distance to the target distribution is (cid:1)(cid:1) of HMC with the Chebyshev integration time (Theorem 1 in the paper). The\n\nK = O (cid:0)√\n\nκ log (cid:0) 1 average of the integration time is\n\n(cid:15)\n\n1 K\n\nK (cid:88)\n\nk=1\n\nη(K) k =\n\n1 K\n\nK (cid:88)\n\nk=1\n\nπ √\n\n2\n\n2\n\n1\n\n(cid:113)\n\nr(K)\n\nσ(k)\n\n=\n\n1 K\n\nK (cid:88)\n\nk=1\n\nπ √\n\n2\n\n1\n\n(cid:113)\n\n2\n\nr(K)\n\nk\n\n,\n\nwhere we recall that a permutation σ(·) does not affect the average.\n\nThen, if K is even, we can rewrite the averaged integration time as\n\n1 K\n\nK (cid:88)\n\nk=1\n\nη(K) k =\n\n1 K\n\nπ √\n\n2\n\n2\n\n\n\n\n\nK/2 (cid:88)\n\nk=1\n\n1\n\n(cid:113)\n\nr(K)\n\nk\n\n\n\n .\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK+1−k\n\nOtherwise, K is odd, and we can rewrite the averaged integration time as\n\n1 K\n\nK (cid:88)\n\nk=1\n\nη(K) k =\n\n1 K\n\nπ √\n\n2\n\n2\n\n\n\n\n\n1\n\n(cid:113)\n\nr(K)\n\n(K+1)/2\n\n(K−1)/2 (cid:88)\n\n+\n\nk=1\n\n\n\n\n\n(cid:113)\n\n1\n\nr(K)\n\nk\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK+1−k\n\n\n\n\n\n\n\n .\n\nWe will show\n\n1\n\n(cid:113)\n\nr(K)\n\nk\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK+1−k\n\n≤\n\n(cid:113)\n\n1\n\nr(K)\n\n(cid:98)K/2(cid:99)\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK−(cid:98)K/2(cid:99)+1\n\n,\n\nfor any k = {1, 2, . . . , (cid:98) K time as\n\n2 (cid:99)} soon. Given this, we can further upper-bound the averaged integration\n\n1 K\n\nK (cid:88)\n\nk=1\n\nη(K) k ≤\n\n\n\n ,\n\n1\n\n(cid:113)\n\nr(K)\n\nK−(cid:98)K/2(cid:99)+1\n\n\n\n\n\nπ √\n\n4\n\n2\n\n1\n\n(cid:113)\n\nr(K)\n\n(cid:98)K/2(cid:99)\n\n+\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nwhen K is even; when K is odd, we can upper-bound the averaged integration time as\n\n1 K\n\nK (cid:88)\n\nk=1\n\nη(K) k ≤\n\n1 K\n\nπ √\n\n2\n\n2\n\n\n\n\n\n1\n\n(cid:113)\n\nr(K)\n\n(K+1)/2\n\n+\n\nK − 1 2\n\n\n\n\n\n(cid:113)\n\n1\n\nr(K)\n\n(cid:98)K/2(cid:99)\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK−(cid:98)K/2(cid:99)+1\n\n\n\n\n\n\n\n .\n\nUsing the definition of the Chebyshev root, we have\n\nr(K) (cid:98)K/2(cid:99) =\n\nL + m 2\n\n−\n\nL − m 2\n\ncos\n\n(cid:32) (cid:0)(cid:98) K\n\n2 (cid:99) − 1 K\n\n2\n\n(cid:33)\n\n(cid:1) π\n\n≈\n\nL + m 2\n\n,\n\nwhere the approximation is because\n\n0. Similarly, we can approximate\n\n((cid:98) K\n\n2 )π\n\n2 (cid:99)− 1 K\n\n≈ π\n\n2 when K is large, and hence cos\n\n(cid:18) ((cid:98) K\n\n2 )π\n\n2 (cid:99)− 1 K\n\n(cid:19)\n\n≈\n\nr(K) K−(cid:98)K/2(cid:99)+1 =\n\nL + m 2\n\n−\n\nL − m 2\n\ncos\n\n(cid:32) (cid:0)K − (cid:98)K/2(cid:99) + 1 − 1\n\n2\n\n(cid:33)\n\n(cid:1) π\n\nK\n\n≈\n\nL + m 2\n\n(K−(cid:98)K/2(cid:99)+1− 1\n\n2 )π\n\nas odd and large for the same reason.\n\nK\n\n≈ π\n\n2 when K is large. Also, we can approximate r(K)\n\n(K+1)/2 ≈ L+m\n\n2 when K is\n\nCombining the above, the total integration time of HMC with the Chebyshev scheme can be approximated as\n\nnumber of iterations × average integration time\n\n√\n\n=\n\nκ log\n\n(cid:19)\n\n(cid:18) 1 (cid:15)\n\n×\n\n1 K\n\nK (cid:88)\n\nk=1\n\nη(K) k ≈\n\n√\n\nκ log\n\n(cid:19)\n\n(cid:18) 1 (cid:15)\n\n×\n\nπ 2\n\n√\n\n1\n\nL + m\n\n.\n\nWhen κ := L\n\nm is large, the total integration time becomes\n\n√\n\nκ log\n\n(cid:19)\n\n(cid:18) 1 (cid:15)\n\n×\n\nπ 2\n\n√\n\n1\n\nL + m\n\n= Θ\n\n(cid:18) 1 √\n\nm\n\nlog\n\n(cid:19)(cid:19)\n\n.\n\n(cid:18) 1 (cid:15)\n\n(22)\n\nNow let us switch to analyzing HMC with the best constant integration time η = Θ (5), Vishnoi (2021)), which has the non-accelerated rate. Specifically, it needs K = O (cid:0)κ log (cid:0) 1 iterations to converge to the target distribution. Hence, the total integration time of HMC with the best constant integration time is\n\n(see e.g., (cid:1)(cid:1)\n\nL\n\n(cid:15)\n\n(cid:16) 1√\n\n(cid:17)\n\nnumber of iterations×average integration time = κ log\n\n(cid:19)\n\n(cid:18) 1 (cid:15)\n\n×Θ\n\n(cid:19)\n\n(cid:18) 1 √\nL\n\n= Θ\n\n(cid:32) √\n\nL m\n\n(cid:19)(cid:33)\n\n.\n\n(cid:18) 1 (cid:15)\n\nlog\n\n(23) By way of comparison ((22) vs. (23)), we see that the total integration time of HMC with the proposed κ, compared with HMC with the best scheme of Chebyshev integration time reduces by a factor constant integration time.\n\n√\n\nThe remaining thing to show is the inequality\n\n1\n\n(cid:113)\n\nr(K)\n\nk\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK+1−k\n\n≤\n\n(cid:113)\n\n1\n\nr(K)\n\n(cid:98)K/2(cid:99)\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK+1−(cid:98)K/2(cid:99)\n\n,\n\n(24)\n\nfor any k = {1, 2, . . . , (cid:98) K\n\n2 (cid:99)}.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nWe have\n\n1\n\n(cid:113)\n\nr(K)\n\nk\n\n+\n\n(cid:113)\n\n1\n\nr(K)\n\nK+1−k\n\n√\n\n=\n\n2 ×\n\n√\n\n=\n\n2 ×\n\n\n\n \n \n\n\n\n\n \n \n\n\n(cid:115)\n\n(cid:115)\n\n1\n\nL + m − (L − m)cos\n\n+\n\n(cid:115)\n\n(cid:18) (k− 1\n\n2 )π\n\n(cid:19)\n\nK\n\n1\n\nL + m − (L − m)cos\n\n(cid:18) (K−k+ 1\n\n2 )π\n\n(cid:19)\n\nK\n\n1\n\nL + m − (L − m)cos\n\n+\n\n(cid:115)\n\n(cid:18) (k− 1\n\n2 )π\n\n(cid:19)\n\nK\n\n1\n\nL + m + (L − m)cos\n\n(cid:18) (k− 1\n\n2 )π\n\n(cid:19)\n\nK\n\n\n\n \n \n\n\n.\n\n\n\n \n \n\n\n(25)\n\nand\n\n\n\n \n\n\n(cid:33)\n\n1\n\n(cid:118) (cid:117) (cid:117) (cid:116)L+m+(L−m)cos\n\n(cid:32) (k− 1\n\n2 )π\n\nK\n\nNow let us define H(k) :=\n\n\n\n \n\n\n1\n\n(cid:118) (cid:117) (cid:117) (cid:116)L+m−(L−m)cos\n\n(cid:32) (k− 1\n\n2 )π\n\n(cid:33)\n\nK\n\n+\n\ntreat k as a continuous variable.\n\nThe derivative of H(k) is\n\nH (cid:48)(k) =\n\nπ 2K\n\n(L − m)sin\n\n(cid:32) (cid:0)k − 1 2\nK\n\n(cid:33)\n\n(cid:1) π\n\n×\n\n\n\n \n \n\n(cid:18)\n\n1\n\nL + m − (L − m)cos\n\n(cid:18) (k− 1\n\n2 )π\n\n(cid:19)(cid:19)3/2\n\nK\n\n−\n\n(cid:18)\n\n1\n\nL + m + (L − m)cos\n\n(cid:18) (k− 1\n\n2 )π\n\n(cid:19)(cid:19)3/2\n\nK\n\n\n\n \n \n\n> 0.\n\n(26)\n\nThat is, H (cid:48)(k) is an increasing function of k when 1 ≤ k ≤ (cid:98) K (24). Now we have completed the analysis.\n\n2 (cid:99), which implies that the inequality\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nE EXPERIMENTS\n\nE.1 BAYESIAN LOGISTIC REGRESSION\n\nTable 4: Bayesian logistic regression\n\nHEART dataset ( ˆm = 2.59, ˆL = 92.43) Step Size Method\n\nMean ESS\n\nMin ESS\n\nMean ESS/Sec.\n\nMin. ESS/Sec.\n\nAcc. Prob\n\n0.001 0.001 0.005 0.005 0.01 0.01 0.05 0.05\n\nCheby. Const. Cheby. Const. Cheby. Const. Cheby. Const.\n\n1693.71 ± 63.53 312.18 ± 12.65 1664.87 ± 43.72 329.48 ± 13.15 1648.25 ± 47.50 307.52 ± 8.77 1424.21 ± 54.03 242.44 ± 14.61\n\n520.43 ± 62.24 80.97 ± 15.97 481.76 ± 49.00 75.78 ± 17.30 508.69 ± 49.81 82.85 ± 13.88 439.88 ± 56.25 56.42 ± 17.68\n\n18.54 ± 2.88 6.57 ± 0.42 82.90 ± 16.51 31.87 ± 2.73 157.09 ± 26.70 53.89 ± 6.37 458.56 ± 51.33 103.36 ± 12.64\n\n5.69 ± 1.12 1.69 ± 0.28 24.08 ± 5.72 7.40 ± 2.06 48.45 ± 9.64 14.62 ± 3.28 140.51 ± 16.58 23.90 ± 7.40\n\n1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.98 ± 0.00 0.98 ± 0.00\n\nBREAST CANCER dataset ( ˆm = 1.81, ˆL = 69.28) Step Size Method\n\nMean ESS\n\nMin ESS\n\n0.001 0.001 0.005 0.005 0.01 0.01 0.05 0.05\n\nCheby. Const. Cheby. Const. Cheby. Const. Cheby. Const.\n\n1037.98 ± 34.46 174.73 ± 13.91 1010.49 ± 24.15 173.17 ± 11.40 1038.10 ± 31.48 162.64 ± 9.43 886.24 ± 38.92 99.48 ± 10.10\n\n575.72 ± 41.14 78.24 ± 23.28 571.03 ± 36.64 79.76 ± 13.49 565.54 ± 50.51 58.79 ± 16.02 499.54 ± 43.99 44.70 ± 13.23\n\nMean ESS/Sec.\n\nMin. ESS/Sec.\n\nAcc. Prob\n\n9.40 ± 0.31 2.59 ± 0.29 43.09 ± 1.14 11.88 ± 1.39 82.82 ± 3.51 18.92 ± 2.59 240.08 ± 12.55 33.25 ± 6.50\n\n5.21 ± 0.31 2.59 ± 0.29 24.35 ± 1.70 11.88 ± 1.39 45.14 ± 4.44 18.92 ± 2.59 135.28 ± 12.04 33.25 ± 6.50\n\n1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.98 ± 0.00 0.98 ± 0.00\n\nDIABETES dataset ( ˆm = 4.96, ˆL = 270.20) Step Size Method\n\nMean ESS\n\nMin ESS\n\nMean ESS/Sec.\n\nMin. ESS/Sec.\n\nAcc. Prob\n\n0.001 0.001 0.005 0.005 0.01 0.01 0.05 0.05\n\nCheby. Const. Cheby. Const. Cheby. Const. Cheby. Const.\n\n726.08 ± 33.92 100.50 ± 9.32 731.46 ± 33.04 100.16 ± 11.83 687.74 ± 29.31 83.04 ± 9.36 546.80 ± 37.40 57.11 ± 9.52\n\n424.59 ± 58.77 41.84 ± 19.33 395.82 ± 47.98 44.62 ± 20.81 399.44 ± 45.01 36.39 ± 12.43 330.09 ± 34.31 23.44 ± 9.57\n\n11.64 ± 0.85 3.6 ± 0.31 54.92 ± 5.26 14.71 ± 2.52 93.10 ± 6.78 20.87 ± 3.31 206.07 ± 17.76 27.23 ± 5.18\n\n6.83 ± 1.16 1.50 ± 0.68 29.61 ± 3.75 6.67 ± 3.37 53.90 ± 5.38 9.09 ± 3.25 125.07 ± 18.87 11.02 ± 4.34\n\n0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.99 ± 0.00 0.98 ± 0.00 0.98 ± 0.00 0.96 ± 0.00 0.96 ± 0.00\n\nE.2 SAMPLING FROM A hard DISTRIBUTION\n\nTable 5: Sampling from a distribution π(x) ∝ exp(−fh(x)) whose potential fh(·) is defined on (15).\n\nStep Size Method\n\nMean ESS\n\nMin ESS\n\nMean ESS/Sec.\n\nMin. ESS/Sec.\n\nAcc. Prob\n\nsampling from π(x) ∝ exp(−f0.001(x))\n\n0.001 0.001\n\nCheby. Const.\n\n6222.21 ± 88.90 2098.18 ± 46.56\n\n453.03 ± 30.35 63.53 ± 15.00\n\n114.74 ± 7.59 82.31 ± 5.29\n\n8.36 ± 0.83 2.50 ± 0.63\n\n1.00 ± 0.00 1.00 ± 0.00\n\nsampling from π(x) ∝ exp(−f0.005(x))\n\n0.005 0.005\n\nCheby. Const.\n\n6271.43 ± 117.71 2125.36 ± 21.87\n\n429.42 ± 34.52 67.42 ± 16.51\n\n545.76 ± 26.10 361.14 ± 5.65\n\n37.28 ± 2.29 11.44 ± 2.76\n\n0.99 ± 0.00 0.99 ± 0.00\n\nsampling from π(x) ∝ exp(−f0.01(x))\n\n0.01 0.01\n\nCheby. Const.\n\n6523.21 ± 95.65 2125.04 ± 31.83\n\n459.48 ± 38.83 69.66 ± 20.75\n\n1070.77 ± 68.78 528.35 ± 80.17\n\n75.61 ± 9.79 17.19 ± 6.34\n\n0.99 ± 0.00 0.99 ± 0.00\n\nsampling from π(x) ∝ exp(−f0.05(x))\n\n0.05 0.05\n\nCheby. Const.\n\n6457.21 ± 110.05 2796.41 ± 56.89\n\n375.97 ± 30.64 62.33 ± 13.26\n\n3319.51 ± 134.92 1893.99 ± 57.23\n\n193.06 ± 14.49 42.22 ± 9.05\n\n0.97 ± 0.00 0.97 ± 0.00\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper considers the possibility of accelerating Hamiltonian Monte Carlo (HMC) methods for sampling from distributions $\\pi$. For a $L$-smooth and $m$-convex $f$, the complexity of sampling from $\\pi \\;\\alpha\\; e^{-f}$ (via an \"ideal method\" that does not discretize time) is of the order $\\kappa \\log(1/\\delta)$, where $\\delta$ is the desired precision and $\\kappa={L/m}$ is the condition number. The present paper shows that one can lower the complexity of the ideal method by a factor of $\\sqrt{\\kappa}$ when $f$ is quadratic (and so $\\pi$ is Gaussian). This requires knowledge of $L$ and $m$, which one uses to choose special step sizes defined via Chebyshev polynomials. Experiments suggest that the same choice of weights works beyond the case of quadratic $f$; in fact the method seems to \"beat the competition\" even for nonconvex likelihoods.\n\n# Strength And Weaknesses\n\n*Strengths*\n\n- This seems to be the first acceleration result for HMC.  \n- The use of Chebyshev polynomials is somewhat interesting.\n\n*Weaknesses*\n\n- Unfortunately, the theory does not cover any cases of practical interest.\n- The experimental success metrics are all defined in terms of effective sample size (ESS). This is a useful proxy of MCMC quality, but it does not prove convergence, nor does it quantify this convergence in harder problems. It would be nice to see other metrics as in: can one approximate the integrals of some \"interesting/hard functions\" from the MC trajectories of different methods?\n- In line with the previous remark, it'd be nice to look at some *hard* distributions coming from real-life Bayesian statistics. What do the credible regions look like? Does one reobtain known results at lower cost?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is quite clear. I did not ceck the experiments carefully, but the code is well-documented and \"looks reproducible\". The approach also seems novel in this context.\n\nI do not think the paper is of great interest, given that the theory is severely limited, and the experiments do not provide a clear indication of practical aspects of this method. One issue I would like to see explored is this: if the likelihood is complicated, maybe it's hard to compute $\\kappa$. What does one do in practice? Is the method sensitive to the upper bound $L$ and the lower bound $m$?\n\n# Summary Of The Review\n\nThe paper has a nice idea, but, at it is present form, its interest is too limited from both theoretical and practical perspectives. \n\nUPDATE on Nov 30th\n\nI have increased my \"novelty\" and overall scores. I thank the authors for their responses. To comment on a few points. \n\na) I agree ESS is a standard surrogate for convergence. It would be nice, however, to see a problem with eg. a multimodal likelihood and show (empirically) that accelerated HMC converges faster than the standard method. \n\nb) I understand that the above point is (as the authors say) disjoint from their goals, as it'd go beyong the logconcave setting. However, given the lack of theoretical results for general logconcave f, I think the paper would greatly benefit from a more extensive empirical evaluation of the accelerated method in more practical settings. \n\nc) I thank the authors for the clarifiction regarding sensibility to the smoothness ans strong convexity parameters.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDENOISING MASKED AUTOENCODERS HELP ROBUST CLASSIFICATION\n\nQuanlin Wu1, Hang Ye1, Yuntian Gu1, Huishuai Zhang2, Liwei Wang3∗, Di He3∗ 1Peking University 2Microsoft Research Asia 3National Key Lab of General AI, School of Artificial Intelligence, Peking University {quanlin, yehang, dihe}@pku.edu.cn, guyuntian@stu.pku.edu.cn,\n\nhuzhang@microsoft.com, wanglw@cis.pku.edu.cn\n\nABSTRACT\n\nIn this paper, we propose a new self-supervised method, which is called Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers of images. In DMAE, we corrupt each image by adding Gaussian noises to each pixel value and randomly masking several patches. A Transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. In this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to Gaussian additive noises. We show that the pre-trained encoder can naturally be used as the base classifier in Gaussian smoothed models, where we can analytically compute the certified radius for any data point. Although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. We show that the DMAE ViT-Base model, which just uses 1/10 parameters of the model developed in recent work (Carlini et al., 2022), achieves competitive or better certified accuracy in various settings. The DMAE ViT-Large model significantly surpasses all previous results, establishing a new state-of-the-art on ImageNet dataset. We further demonstrate that the pre-trained model has good transferability to the CIFAR10 dataset, suggesting its wide adaptability. Models and code are available at https://github.com/quanlin-wu/dmae.\n\n1\n\nINTRODUCTION\n\nDeep neural networks have demonstrated remarkable performance in many real applications (He et al., 2016; Devlin et al., 2019; Silver et al., 2016). However, at the same time, several works observed that the learned models are vulnerable to adversarial attacks (Szegedy et al., 2013; Biggio et al., 2013). Taking image classification as an example, given an image x that is correctly classified to label y by a neural network, an adversary can find a small perturbation such that the perturbed image, though visually indistinguishable from the original one, is predicted into a wrong class with high confidence by the model. Such a problem raises significant challenges in practical scenarios.\n\nGiven such a critical issue, researchers seek to learn classifiers that can provably resist adversarial attacks, which is usually referred to as certified defense. One of the seminal approaches in this direction is the Gaussian smoothed model. A Gaussian smoothed model g is defined as g(x) = Eηf (x + η), in which η ∼ N (0, σ2I) and f is an arbitrary classifier, e.g., neural network. Intuitively, the smoothed classifier g can be viewed as an ensemble of the predictions of f that takes noise-corrupted images x + η as inputs. Cohen et al. (2019) derived how to analytically compute the certified radius of the smoothed classifier g, and follow-up works improved the training methods of the Gaussian smoothed model with labeled data (Salman et al., 2019; Zhai et al., 2021; Jeong & Shin, 2020; Horv ́ath et al., 2022; Jeong et al., 2021).\n\nRecently, Salman et al. (2020); Carlini et al. (2022) took the first step to train Gaussian smoothed classifiers with the help of self-supervised learning. Both approaches use a compositional model architecture for f and decompose the prediction process into two stages. In the first stage, a denoising\n\n∗Correspondence to: Di He <dihe@pku.edu.cn> and Liwei Wang <wanglw@pku.edu.cn>.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\n#Params Extra data\n\n0.5\n\nCertified Accuracy(%) at l2 radius r 3.0\n\n2.0\n\n1.5\n\n1.0\n\nRS (Cohen et al., 2019) SmoothAdv (Salman et al., 2019) Consistency (Jeong & Shin, 2020) MACER (Zhai et al., 2021) Boosting (Horv ́ath et al., 2022) SmoothMix (Jeong et al., 2021) Diffusion+BEiT (Carlini et al., 2022)\n\nOurs (DMAE ViT-B) Ours (DMAE ViT-L)\n\n26M 26M 26M 26M 78M 26M †857M\n\n87M 304M\n\n(cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33)\n\n(cid:37) (cid:37)\n\n49.0 54.0 50.0 57.0 57.0 50.0 71.1∗\n\n37.0 43.0 44.0 43.0 44.6 43.0 54.3\n\n29.0 37.0 34.0 31.0 38.4 38.0 38.1\n\n19.0 27.0 24.0 25.0 28.6 26.0 29.5\n\n12.0 20.0 17.0 14.0 20.2 17.0 13.1\n\n69.6 73.6∗∗\n\n57.9∗ 64.6∗∗\n\n47.8∗ 53.7∗∗\n\n35.4∗ 41.5∗∗\n\n22.5∗ 27.5∗∗\n\nTable 1: Certified accuracy (top-1) of different models on ImageNet. Following Carlini et al. (2022), for each noise level σ, we select the best certified accuracy from the original papers. ∗∗ denotes the best result, and ∗ denotes the second best at each l2 radius. †Carlini et al. (2022) uses a diffusion model with 552M parameters and a BEiT-Large model with 305M parameters. It can be seen that our DMAE ViT-B/ViT-L models achieve the best performance in most of the settings.\n\nmodel is used to purify the noise-corrupted inputs. Then in the second stage, a classifier is applied to predict the label from the denoised image. Since the first-stage denoising model and the secondstage classification model can be learned or benefited from standard self-supervised approaches, the smoothed classifier g can achieve better performance than previous works. For example, Carlini et al. (2022) achieved 71.1% certified accuracy at l2 radius r = 0.5 and 54.3% at r = 1.0 on ImageNet by applying a pre-trained denoising diffusion model in the first stage (Nichol & Dhariwal, 2021) and a pre-trained BEiT (Bao et al., 2021) in the second stage. Despite its impressive performance, such a two-stage process requires much more parameters and separated training.\n\nDifferent from Salman et al. (2020); Carlini et al. (2022) that use two models trained for separated purposes, we believe that a single compact network (i.e., vision Transformer) has enough expressive power to learn robust feature representation with proper supervision. Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE). Given an unlabeled image, we corrupt the image by adding Gaussian noise to each pixel value and randomly masking several patches. The goal of the task is to train a model to reconstruct the clean image from the corrupted one. Similar to MAE, DMAE also intends to reconstruct the masked information; hence, it can capture relevant features of the image for downstream tasks. Furthermore, DMAE takes noisy patches as inputs and outputs denoised ones, making the learned features robust with respect to additive noises. We expect that the semantics and robustness of the representation can be learned simultaneously, enabling efficient utilization of the model parameters.\n\nAlthough the proposed DMAE method is simple, it yields significant performance improvement on downstream tasks. We pre-train DMAE ViT-Base and DMAE ViT-Large, use the encoder to initialize the Gaussian smoothed classifier, and fine-tune the parameters on ImageNet. We show that the DMAE ViT-Base model with 87M parameters, one-tenth as many as the model used in Carlini et al. (2022), achieves competitive or better certified accuracy in various settings. Furthermore, the DMAE ViT-Large model (304M) significantly surpasses the state-of-the-art results in all tasks, demonstrating a single-stage model is enough to learn robust representations with proper selfsupervised tasks. We also demonstrate that the pre-trained model has good transferability to other datasets. We empirically show that decent improvement can be obtained when applying it to the CIFAR-10 dataset. Model checkpoints will be released in the future.\n\n2 RELATED WORK\n\nSzegedy et al. (2013); Biggio et al. (2013) observed that standardly trained neural networks are vulnerable to adversarial attacks. Since then, many works have investigated how to improve the robustness of the trained model. One of the most successful methods is adversarial training, which adds adversarial examples to the training set to make the learned model robust against such attacks (Madry et al., 2018; Zhang et al., 2019). However, as the generation process of adversarial examples is pre-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ndefined during training, the learned models may be defeated by stronger attacks (Athalye et al., 2018). Therefore, it is important to develop methods that can learn models with certified robustness guarantees. Previous works provide certified guarantees by bounding the certified radius layer by layer using convex relaxation methods (Wong & Kolter, 2018; Wong et al., 2018; Weng et al., 2018; Balunovic & Vechev, 2020; Zhang et al., 2021; 2022a;b). However, such algorithms are usually computationally expensive, provide loose bounds, or have scaling issues in deep and large models.\n\nRandomized smoothing. Randomized smoothing is a scalable approach to obtaining certified robustness guarantees for any neural network. The key idea of randomized smoothing is to add Gaussian noise in the input and to transform any model into a Gaussian smoothed classifier. As the Lipschitz constant of the smoothed classifier is bounded with respect to the l2 norm, we can analytically compute a certified guarantee on small l2 perturbations (Cohen et al., 2019). Follow-up works proposed different training strategies to maximize the certified radius, including ensemble approaches (Horv ́ath et al., 2022), model calibrations (Jeong et al., 2021), adversarial training for smoothed models (Salman et al., 2019) and refined training objectives (Jeong & Shin, 2020; Zhai et al., 2021). Yang et al. (2020); Blum et al. (2020); Kumar et al. (2020) extended the method to general lp perturbations by using different shapes of noises.\n\nSelf-supervised pre-training in vision. Learning the representation of images from unlabeled data is an increasingly popular direction in computer vision. Mainstream approaches can be roughly categorized into two classes. One class is the contrastive learning approach which maximizes agreement between differently augmented views of an image via a contrastive loss (Chen et al., 2020; He et al., 2020). The other class is the generative learning approach, which randomly masks patches in an image and learns to generate the original one (Bao et al., 2021; He et al., 2022). Several works utilized self-supervised pre-training to improve image denoising (Joshua Batson, 2019; Yaochen Xie, 2020), and recently there have been attempts to use pre-trained denoisers to achieve certified robustness. The most relevant works are Salman et al. (2020); Carlini et al. (2022). Both works first leverage a pre-trained denoiser to purify the input, and then use a standard classifier to make predictions. We discuss these two works and ours in depth in Sec. 3.\n\n3 METHOD\n\n3.1 NOTATIONS AND BASICS\n\nDenote x ∈ Rd as the input and y ∈ Y = {1, . . . , C} as the corresponding label. Denote g : Rd → Y as a classifier mapping x to y. For any x, assume that an adversary can perturb x by adding an adversarial noise. The goal of the defense methods is to guarantee that the prediction g(x) doesn’t change when the perturbation is small. Randomized smoothing (Li et al., 2018; Cohen et al., 2019) is a technique that provides provable defenses by constructing a smoothed classifier g of the form:\n\ng(x) = arg max c∈Y\n\nPη[f (x + η) = c], where η ∼ N (0, σ2Id).\n\n(1)\n\nThe function f is called the base classifier, which is usually parameterized by neural networks, and η is Gaussian noise with noise level σ. Intuitively, g(x) can be considered as an ensemble classifier which returns the majority vote of f when its input is sampled from a Gaussian distribution N (x, σ2Id) centered at x. Cohen et al. (2019) theoretically provided the following certified robustness guarantee for the Gaussian smoothed classifier g.\n\nTheorem 1 (Cohen et al., 2019) Given f and g defined as above, assume that g classifies x correctly, i.e., Pη[f (x + η) = y] ≥ maxy′̸=y Pη[f (x + η) = y′]. Then for any x′ satisfying ||x′ − x||2 ≤ R, we always have g(x) = g(x′), where\n\nR =\n\nσ 2\n\n[Φ−1(Pη[f (x + η) = y]) − Φ−1(max\n\ny′̸=y\n\nPη[f (x + η) = y′])].\n\n(2)\n\nΦ is the cumulative distribution function of the standard Gaussian distribution.\n\nThe denoise-then-predict network structure. Even without knowing the label, one can still evaluate the robustness of a model by checking whether it can give consistent predictions when the input is perturbed. Therefore, unlabeled data can naturally be used to improve the model’s robustness (Alayrac et al., 2019; Carmon et al., 2019; Najafi et al., 2019; Zhai et al., 2019). Recently,\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of our DMAE pre-training. We first corrupt the image by adding Gaussian noise to each pixel value, and then randomly masking several patches. The encoder and decoder are trained to reconstruct the clean image from the corrupted one.\n\nSalman et al. (2020); Carlini et al. (2022) took steps to train Gaussian smoothed classifiers with the help of unlabeled data. Both of them use a denoise-then-predict pipeline. In detail, the base classifier f consists of three components: θdenoiser, θencoder and θoutput. Given any input x, the classification process of f is defined as below.\n\nˆx = Denoise(x + η; θdenoiser) h = Encode( ˆx; θencoder) y = Predict(h; θoutput)\n\n(3)\n\n(4)\n\n(5)\n\nAs f takes noisy image as input (see Eq.1), a denoiser with parameter θdenoiser is first used to purify x+η to cleaned image ˆx. After that, ˆx is further encoded into contextual representation h by θencoder and the prediction can be obtained from the output head θoutput. Note that θdenoiser and θencoder can be pre-trained by self-supervised approaches. For example, one can use denoising auto-encoder (Vincent et al., 2008; 2010) or denoising diffusion model (Ho et al., 2020; Nichol & Dhariwal, 2021) to pre-train θdenoiser, and leverage contrastive learning (Chen et al., 2020; He et al., 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder. Especially, Carlini et al. (2022) achieved state-of-the-art performance on ImageNet by applying a pre-trained denoising diffusion model as the denoiser and a pre-trained BEiT (Bao et al., 2021) as the encoder.\n\n3.2 DENOISING MASKED AUTOENCODERS In the denoise-then-predict network structure above, if the denoiser is perfect, h will be robust to the Gaussian additive noise η. Then the robust accuracy of g can be as high as the standard accuracy of models trained on clean images. However, the denoiser requires a huge number of parameters to obtain acceptable results (Nichol & Dhariwal, 2021), limiting the practical usage of the compositional method in real applications.\n\nNote that our goal is to learn representation h that captures rich semantics for classification and resists Gaussian additive noise. Using an explicit purification step before encoding is sufficient to achieve it but may not be a necessity. Instead of using multiple training stages for different purposes, we aim to adopt a single-stage approach to learn robust h through self-supervised learning directly. In particular, we extend the standard masked autoencoder with an additional denoising task, which we call the Denoising Masked AutoEncoder (DMAE). The DMAE works as follows: an image x is first divided into regular non-overlapping patches. Denote Mask(x) as the operation that randomly masks patches with a pre-defined masking ratio. As shown in Fig. 1, we aim to train an autoencoder that takes Mask(x + η) as input and reconstructs the original image:\n\nx → x + η → Mask(x + η) Encoder\n\n−−−−→ h Decoder\n\n−−−−→ ˆx.\n\nLike MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE. Both encoder and decoder use stacked Transformer layers. The encoder takes noisy unmasked patches with positional encoding as inputs and generates the representation h. Then the decoder takes the representation on all patches as inputs (h for unmasked patches and a masked token embedding for\n\n4\n\nCleanImagexMaskedNoisyImageMask(x+η)DMAEEncoderGaussianNoiseηNoisyImagex+ηReconstructedImagex̂+Reconstruc/on LossDMAEDecoderPublished as a conference paper at ICLR 2023\n\nmasked patches) and reconstructs the original image. Pixel-level mean square error is used as the loss function. Slightly different from MAE, the loss is calculated on all patches as the model can also learn purification on the unmasked positions. During pre-training, the encoder and decoder are jointly optimized from scratch, and the decoder will be removed while learning downstream tasks.\n\nIn order to reconstruct the original image, the encoder and the decoder have to learn semantics from the unmasked patches and remove the noise simultaneously. To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).\n\nRobust fine-tuning for downstream classification tasks. As the encoder of DMAE already learns robust features, we can simplify the classification process of the base classifer as\n\nh = Encode(x + η; θencoder) y = Predict(h; θoutput)\n\n(6)\n\n(7)\n\nTo avoid any confusion, we explicitly parameterize the base classifier as f (x; θencoder, θoutput) = Predict(Encode(x; θencoder); θoutput), and denote F (x; θencoder, θoutput) as the output of the last softmax layer of f , i.e., the probability distribution over classes. We aim to maximize the certified accuracy of the corresponding smoothed classifier g by optimizing θencoder and θoutput, where θencoder is initialized by the pre-trained DMAE model. To achieve the best performance, we use the consistency regularization training method developed in Jeong & Shin (2020) to learn θencoder and θoutput. The loss is defined as below.\n\nL(x, y; θencoder, θoutput) = Eη[CrossEntropy(F (x + η; θencoder, θoutput), y)]\n\n+ λ · Eη[DKL( ˆF (x; θencoder, θoutput)∥F (x + η; θencoder, θoutput))] + μ · H( ˆF (x; θencoder, θoutput))\n\n(8)\n\nwhere ˆF (x; θencoder, θoutput) := Eη∼N (0,σ2Id)[F (x + η; θencoder, θoutput)] is the average prediction distribution of the base classifier, and λ, μ > 0 are hyperparameters. DKL(·||·) and H(·) denote the Kullback–Leibler (KL) divergence and the entropy, respectively. The loss function contains three terms. Intuitively, the first term aims to maximize the accuracy of the base classifier with perturbed input. The second term attempts to regularize F (x+η; θencoder, θoutput) to be consistent with different η. The last term prevents the prediction from low confidence scores. All expectations are estimated by Monte Carlo sampling.\n\n4 EXPERIMENTS\n\nIn this section, we empirically evaluate our proposed DMAE on ImageNet and CIFAR-10 datasets. We also study the influence of different hyperparameters and training strategies on the final model performance. All experiments are repeated ten times with different seeds. Average performance is reported, and details can be found in the appendix.\n\n4.1 PRE-TRAINING SETUP\n\nFollowing He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images. All images are resized to a fixed resolution of 224 × 224. We utilize two vision Transformer variants as the encoder, the Base model (ViT-B) and the Large model (ViTL) with 16 × 16 input patch size (Kolesnikov et al., 2021). The ViT-B encoder consists of 12 Transfomer blocks with embedding dimension 768, while the ViT-L encoder consists of 16 blocks with embedding dimension 1024. For both settings, the decoder uses 8 Transformer blocks with embedding dimension 512 and a linear projection whose number of output channels equals the number of pixel values in a patch. All the Transformer blocks have 16 attention heads. The ViTB/ViT-L encoder have roughly 87M and 304M parameters, respectively.\n\nFor the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022). The noise level σ is set to 0.25. Random resizing and cropping are used as data augmentation to avoid overfitting. The ViT-B and ViT-L models are pre-trained for 1100 and 1600 epochs, where the batch size is set to 4096. We use the AdamW optimizer with β1, β2 = 0.9, 0.95, and adjust the learning rate to 1.5 × 10−4. The weight decay factor is set to 0.05. After pre-training, we\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Visualization. For each group, the leftmost column shows the original image. The following two correspond to the image with Gaussian perturbation (noise level σ = 0.25) and the masked noisy image. The last column illustrates the reconstructed image by our DMAE ViT-L model.\n\nalso visualize the model performance of DMAE ViT-L in Fig. 2. From the figure, we can see that the trained model can recover the masked patches and purify the noisy unmasked patches, which demonstrates its capability of accomplishing both tasks simultaneously.\n\n4.2 FINE-TUNING FOR IMAGENET CLASSIFICATION\n\nSetup. In the fine-tuning stage, we add a linear prediction head on top of the encoder for classification. The ViT-B model is fine-tuned for 100 epochs, while the ViT-L is fine-tuned for 50 epochs. Both settings use AdamW with β1, β2 = 0.9, 0.999. The weight decay factor is set to 0.05. We set the base learning rate to 5 × 10−4 for ViT-B and 1 × 10−3 for ViT-L. Following Bao et al. (2021), we use layer-wise learning rate decay (Kevin Clark & Manning, 2020) with an exponential rate of 0.65 for ViT-B and 0.75 for ViT-L. We apply standard augmentation for training ViT models, including RandAug (Ekin D Cubuk & Le, 2020), label smoothing (Szegedy et al., 2016), mixup (Hongyi Zhang & Lopez-Paz, 2018) and cutmix (Yun et al., 2019). Following most previous works, we conduct experiments with different noise levels σ ∈ {0.25, 0.5, 1.0}. For the consistency regularization loss terms, we set the hyperparameters λ = 2.0 and μ = 0.5 for σ ∈ {0.25, 0.5}, and set λ = 2.0 and μ = 0.1 for σ = 1.0.\n\nEvaluation. Following previous works, we report the percentage of samples that can be certified to be robust (a.k.a certified accuracy) at radius r with pre-defined values. For a fair comparison, we use the official implementation1 of CERTIFY to calculate the certified radius for any data point2, with n = 10, 000, n0 = 100 and α = 0.001. The result is averaged over 1,000 images uniformly selected from ImageNet validation set, following Carlini et al. (2022).\n\nResults. We list the detailed results of our model and representative baseline methods in Table 2. We also provide a summarized result that contains the best performance of different methods at each radius r in Table 1. It can be seen from Table 2 that our DMAE ViT-B model significantly surpasses all baselines in all settings except Carlini et al. (2022). This clearly demonstrates the strength of selfsupervised learning. Compared with Carlini et al. (2022), our model achieves better results when r ≥ 1.0 and is slightly worse when r is small. We would like to point out that the DMAE ViT-B model only uses 10% parameters compared to Carlini et al. (2022), which suggests our single-stage pre-training method is more parameter-efficient than the denoise-then-predict approach. Although the diffusion model used in Carlini et al. (2022) can be applied with different noise levels, the huge number of parameters and long inference time make it more difficult to deploy.\n\nOur DMAE ViT-L model achieves the best performance over all prior works in all settings and boosts the certified accuracy by a significant margin when σ and r are large. For example, at r = 1.5, it achieves 53.7% accuracy which is 15.3% better than Boosting (Horv ́ath et al., 2022), and it surpasses Diffusion (Carlini et al., 2022) by 12.0% at r = 2.0. This observation is different from the one reported in Carlini et al. (2022), where the authors found that the diffusion model coupled with an off-the-shelf BEiT only yields better performance with smaller σ and r.\n\n1https://github.com/locuslab/smoothing 2One may notice that randomized smoothing methods require a significant number of samples (e.g., 105) for evaluation. Here the samples are used to calculate the certified radius accurately. A much smaller number of samples are enough to make predictions in practice.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nσ\n\nMethod\n\nRS (Cohen et al., 2019) SmoothAdv (Salman et al., 2019) Consistency (Jeong & Shin, 2020) MACER (Zhai et al., 2021) Boosting (Horv ́ath et al., 2022) SmoothMix (Jeong et al., 2021) Diffusion+BEiT (Carlini et al., 2022)\n\n0.25\n\nOurs (DMAE ViT-B) Ours (DMAE ViT-L)\n\nRS (Cohen et al., 2019) SmoothAdv (Salman et al., 2019) Consistency (Jeong & Shin, 2020) MACER (Zhai et al., 2021) Boosting (Horv ́ath et al., 2022) SmoothMix (Jeong et al., 2021) Diffusion+BEiT (Carlini et al., 2022)\n\nOurs (DMAE ViT-B) Ours (DMAE ViT-L)\n\nRS (Cohen et al., 2019) SmoothAdv (Salman et al., 2019) Consistency (Jeong & Shin, 2020) MACER (Zhai et al., 2021) Boosting (Horv ́ath et al., 2022) SmoothMix (Jeong et al., 2021) Diffusion+BEiT (Carlini et al., 2022)\n\nOurs (DMAE ViT-B) Ours (DMAE ViT-L)\n\n0.5\n\n1.0\n\n0.0\n\n67.0 63.0 -\n68.0 65.6 -\n82.8∗∗\n\n78.1 81.7∗\n\n57.0 54.0 55.0 64.0 57.0 55.0 77.1\n\n73.0 77.6\n\n44.0 40.0 41.0 48.0 44.6 40.0 60.0\n\n58.0 65.7\n\n3.0\n\n0 0\n\n0 0\n\n0\n\n0 0\n\n0 0\n0 0\n0 0\n0\n\n0 0\n\nCertified Accuracy(%) at l2 radius r 0.5\n\n2.0\n\n1.5\n\n1.0\n\n0 0\n\n0 0\n\n0\n\n0 0\n\n0 0\n0 0\n0 0\n0\n\n0 0\n\n49.0 54.0\n\n57.0 57.0\n\n71.1\n\n69.6 73.6∗∗\n\n46.0 49.0 50.0 53.0 52.0 50.0 67.8\n\n64.8 72.4∗\n\n38.0 37.0 37.0 43.0 40.2 37.0 50.0\n\n53.3 59.0\n\n0 0\n\n0 0\n\n0\n\n0 0\n\n37.0 43.0 44.0 43.0 44.6 43.0 54.3 57.9∗ 64.6∗∗\n\n33.0 34.0 32.0 36.0 37.2 34.0 42.0\n\n47.1 53.0\n\n0 0\n\n0 0\n\n0\n\n0 0\n\n29.0 37.0 34.0 31.0 38.4 38.0 38.1\n\n47.8 53.7∗∗\n\n26.0 30.0 28.0 30.0 34.0 30.0 35.5\n\n41.7 47.9∗\n\n19.0 27.0 24.0 25.0 28.6 26.0 29.5 35.4∗ 41.5∗∗\n\n12.0 20.0 17.0 14.0 20.2 20.0 13.1 22.5∗ 27.5∗∗\n\nTable 2: Certified accuracy (top-1) of different models on ImageNet with different noise levels. ∗∗ denotes the best result, and ∗ denotes the second best at each radius r.\n\n4.3 FINE-TUNING FOR CIFAR-10 CLASSIFICATION\n\nSetup. We show the DMAE models can benefit not only ImageNet but also the CIFAR-10 classification tasks, suggesting the nice transferability of our pre-trained models. We use the DMAE ViT-B checkpoint as a showcase. As the sizes of the images in ImageNet and CIFAR-10 are different, we pre-process the images CIFAR-10 to 224 × 224 to match the pre-trained model. Note that the data distributions of ImageNet and CIFAR-10 are far different. To address this significant distributional shift, we continue pre-training the DMAE model on the CIFAR-10 dataset. We set the continued pre-training stage to 50 epochs, the base learning rate to 5 × 10−5, and the batch size to 512. Most of the fine-tuning details is the same as that on ImageNet in Sec. 4.2, except that we use a smaller batch size of 256, apply only the random horizontal flipping as data augmentation, and reduce the number of the fine-tuning epochs to 50.\n\nResult. The evaluation protocol is the same as that on ImageNet in Sec. 4.2. We draw n = 100, 000 noise samples and report results averaged over the entire CIFAR-10 test set. The results are presented in Table 3. Without continued pre-training, our DMAE ViT-B model still yields comparable performance with Carlini et al. (2022), and the model outperforms it when continued pre-training is applied. It is worth noting that the number of parameters of Carlini et al. (2022) is larger, and the diffusion model is trained on CIFAR datasets. In comparison, our model only uses a smaller amount of parameters, and the pre-trained checkpoint is directly borrowed from Sec. 4.1. Our model performance is significantly better than the original consistent regularization method (Jeong & Shin, 2020), demonstrating the transferability of the pre-training model. Specifically, our method outperforms the original consistent regularization by 12.0% at r = 0.25, and by 9.0% at r = 0.5. We believe our pre-trained checkpoint can also improve other baseline methods to achieve better results.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nParams Extra data\n\nCertified Accuracy(%) at l2 radius r 0.25\n\n0.75\n\n0.5\n\n1.0\n\nRS (Cohen et al., 2019) SmoothAdv (Salman et al., 2019) Consistency (Jeong & Shin, 2020) MACER (Zhai et al., 2021) Boosting (Horv ́ath et al., 2022) SmoothMix (Jeong et al., 2021) Diffusion (Carlini et al., 2022)\n\nOurs (DMAE ViT-B) +continued pre-training\n\n1.7M 1.7M 1.7M 1.7M 17M 1.7M †137M\n\n87M 87M\n\n(cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33)\n\n(cid:33) (cid:33)\n\n61.0 74.8 68.8 71.0 70.6 67.9 79.3∗\n\n43.0 60.8 58.1 59.0 60.4 57.9 65.5∗\n\n32.0 47.0 48.5 46.0 52.4∗∗ 47.7 48.7\n\n23.0 37.8 37.8 38.0∗ 38.8∗∗ 37.2 35.5\n\n79.2 80.8∗∗\n\n64.6 67.1∗∗\n\n47.3 49.7∗\n\n36.1 37.7\n\nTable 3: Certified accuracy (top-1) of different models on CIFAR-10. Each entry lists the certified accuracy of best Gaussian noise level σ from the original papers. ∗∗ denotes the best result and ∗ denotes the second best at each l2 radius. †(Carlini et al., 2022) uses a 50M-parameter diffusion model and a 87M-parameter ViT-B model.\n\nCertified Accuracy(%) at l2 radius r\n\nσ\n\nMethod\n\n0.0\n\n0.5\n\n0.25\n\n0.5\n\n1.0\n\nMAE DMAE\n\nMAE DMAE\n\nMAE DMAE\n\n32.5 58.7(+26.2)\n\n13.3 26.4(+13.1)\n\n3.0 7.6(+4.6)\n\n13.3 45.3(+32.0)\n\n6.9 16.7(+9.8)\n\n2.2 5.4(+3.2)\n\n1.0\n\n0 0\n\n1.5\n\n0 0\n\n2.6 10.6(+8.0)\n\n1.2 3.5(+2.3)\n\n0.1 5.1(+5.0)\n\n0.5 2.4(+1.9)\n\n2.0\n\n3.0\n\n0 0\n\n0 0\n\n0 0\n\n0 0\n\n0.4 1.6(+1.2)\n\n0 0.4(+0.4)\n\nTable 4: DMAE v.s. MAE by linear probing on ImageNet. Our proposed DMAE is significantly better than MAE on the ImageNet classification task, indicating that the proposed pre-training method is effective and learns more robust features. Numbers in (.) is the gap between the two methods in the same setting.\n\n4.4 DISCUSSION\n\nIn this section, we discuss several design choices in our methods. Whether DMAE learns more robust features than MAE. Compared with MAE, we additionally use a denoising objective in pre-training to learn robust features. Therefore, we need to examine the quality of the representation learned by DMAE and MAE to investigate whether the proposed objective helps. For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet. Linear probing is a popular scheme to compare the representation learned by different models, where we freeze the parameters of the pre-trained encoders and use a linear layer with batch normalization to make predictions. For both DMAE and MAE, we train the linear layer for 90 epochs with a base learning rate of 0.1. The weight decay factor is set to 0.0. As overfitting seldom occurs in linear probing, we only apply random resizing and cropping as data augmentation and use a large batch size of 16,384.\n\nAs shown in Table 4, our DMAE outperforms MAE by a large margin in linear probing. For example, with Gaussian noise magnitude σ = 0.25, DMAE can achieve 45.3% certified accuracy at r = 0.5, 32.0 points higher than that of MAE. Note that even our models were pre-trained with a small magnitude of Gaussian noise (σ = 0.25), they still yield much better results than that of MAE under large Gaussian noise (σ = 0.5, 1.0). This clearly indicates that our method learns much more robust features compared with MAE. Effects of the pre-training steps. Many previous works observe that longer pre-training steps usually helps the model perform better on downstream tasks. To investigate whether this phenomenon\n\n8\n\n3.0\n\n0 0\n\n0 0\n\n3.0\n\n0 0\n\n0 0\n\nPublished as a conference paper at ICLR 2023\n\nσ\n\nEpochs\n\n0.0\n\n0.5\n\nCertified Accuracy(%) at l2 radius r 2.0\n\n1.5\n\n1.0\n\n0.25\n\n0.5\n\n1.0\n\n700 1100\n\n700 1100\n\n700 1100\n\n78.8 78.1(−0.7)\n\n72.0 73.0(+1.0)\n\n56.4 58.0(+1.6)\n\n68.8 69.6(+0.8)\n\n64.4 64.8(+0.4)\n\n50.7 53.3(+2.6)\n\n0 0\n\n0 0\n\n55.5 57.9(+2.4)\n\n46.2 47.1(+0.9)\n\n45.3 47.8(+2.5)\n\n40.4 41.7(+1.3)\n\n0 0\n\n0 0\n\n34.9 35.4(+0.5)\n\n23.3 22.5(−0.8)\n\nTable 5: Effects of the pre-training steps. From the table, we can see that the 1100-epoch model consistently outperforms the 700-epoch model in almost all settings, demonstrating that longer pretraining leads to better downstream task performance. Numbers in (.) is the gap between the two methods in the same setting.\n\nσ\n\n0.25\n\n0.5\n\n1.0\n\nMethod\n\n0.0\n\n0.5\n\nCertified Accuracy(%) at l2 radius r 2.0\n\n1.0\n\n1.5\n\nDMAE-L+RS DMAE-L+CR 81.7(−0.1)\n\n81.8\n\nDMAE-L+RS DMAE-L+CR 77.6(+1.2)\n\n76.4\n\nDMAE-L+RS DMAE-L+CR 65.7(+2.5)\n\n63.2\n\n72.6 73.6(+1.0)\n\n69.5 72.4(+2.9)\n\n57.0 59.0(+2.0)\n\n0 0\n\n0 0\n\n59.3 64.6(+5.3)\n\n49.8 53.0(+3.2)\n\n45.8 53.7(+7.9)\n\n42.9 47.9(+5.0)\n\n0 0\n\n0 0\n\n35.9 41.5(+5.6)\n\n21.8 27.5(+5.7)\n\nTable 6: DMAE with different fine-tuning methods. From the table, we can see that our pretrained model is compatible with different fine-tuning methods. Numbers in (.) is the gap between the two methods in the same setting.\n\nhappens in our setting, we also conduct experiments to study the downstream performance of model checkpoints at different pre-training steps. In particular, we compare the DMAE ViT-B model (1100 epochs) trained in Sec. 4.1 with an early checkpoint (700 epochs). Both models are fine-tuned under the same configuration. All results on ImageNet are presented in Table 5. It shows that the 1100-epoch model consistently outperforms its 700-epoch counterpart in almost all settings. Other fine-tuning methods. In the main experiment, we use Consistency Regularization (CR) in the fine-tuning stage, and one may be interested in how much the pre-trained model can improve with other methods. To study this, we fine-tune our pre-trained DMAE ViT-L model with the RS algorithm (Cohen et al., 2019), where the only loss used in training is the standard cross-entropy classification loss in Eq.7. For this experiment, we use the same configuration as in Sec. 4.2. The results are provided in Table 6. First, we can see that the regularization loss consistently leads to better certified accuracy. In particular, it yields up to 3-5% improvement at a larger l2 radius (r ≥ 1.0). Second, it can also be seen that the RS model fine-tuned on DMAE ViT-L significantly surpasses lots of baselines on ImageNet. This suggests that our pre-trained DMAE ViT-L model may be combined with other training methods in the literature to improve their performance.\n\n5 CONCLUSION\n\nThis paper proposes a new self-supervised method, Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers of images. DMAE corrupts each image by adding Gaussian noises to each pixel value and randomly masking several patches. A vision Transformer is then trained to reconstruct the original image from the corrupted one. The pre-trained encoder of DMAE can naturally be used as the base classifier in Gaussian smoothed models to achieve certified robustness. Extensive experiments show that the pre-trained model is parameter-efficient, achieves state-of-the-art performance, and has nice transferability. We believe that the pre-trained model has great potential in many aspects. We plan to apply the pre-trained model to more tasks, including image segmentation and detection, and investigate the interpretability of the models in the future.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nThis work is partially supported by the National Key R&D Program of China (2022ZD0160304). The work is supported by National Science Foundation of China (NSFC62276005), The Major Key Project of PCL (PCL2021A12), Exploratory Research Project of Zhejiang Lab (No. 2022RC0AN02), and Project 2020BD006 supported by PKUBaidu Fund. We thank all the anonymous reviewers for the very careful and detailed reviews as well as the valuable suggestions. Their help has further enhanced our work.\n\nREFERENCES\n\nJean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and Pushmeet Kohli. Are labels required for improving adversarial robustness? Advances in Neural Information Processing Systems, 32, 2019.\n\nAnish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL http://arxiv.org/abs/1802.00420.\n\nMislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap.\n\nIn International Conference on Learning Representations, 2020.\n\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.\n\nIn International Conference on Learning Representations, 2021.\n\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi ́c, Pavel Laskov, GiorIn Joint gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. European conference on machine learning and knowledge discovery in databases, pp. 387–402. Springer, 2013.\n\nAvrim Blum, Travis Dick, Naren Manoj, and Hongyang Zhang. Random smoothing might be unable to certify l∞ robustness for high-dimensional images. arXiv preprint arXiv:2002.03517, 2020.\n\nNicholas Carlini, Florian Tramer, J Zico Kolter, et al. (certified!!) adversarial robustness for free!\n\narXiv preprint arXiv:2206.10550, 2022.\n\nYair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. Advances in Neural Information Processing Systems, 32, 2019.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. North American Chapter of the Association for Computational Linguistics, 2019.\n\nJonathon Shlens Ekin D Cubuk, Barret Zoph and Quoc V Le. Randaugment:practical automated\n\ndata augmentation with a reduced search space. CVPR workshops, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nYann N Dauphin Hongyi Zhang, Moustapha Cisse and David Lopez-Paz. Mixup: Beyond empirical\n\nrisk minimization. ICLR, 2018.\n\nMikl ́os Z Horv ́ath, Mark Niklas M ̈uller, Marc Fischer, and Martin Vechev. Boosting randomized\n\nsmoothing with variance reduced classifiers. ICLR, 2022.\n\nJongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed classifiers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nJongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Do-Guk Kim, and Jinwoo Shin. Smoothmix: Training confidence-calibrated smoothed classifiers for certified robustness. Advances in Neural Information Processing Systems, 34:30153–30168, 2021.\n\nLoic Royer Joshua Batson. Noise2self: Blind denoising by self-supervision networks with l-inf-dist\n\nneurons. International conference on machine learning, 2019.\n\nQuoc V Le Kevin Clark, Minh-Thang Luong and Christopher D Manning. Electra: Pre-training text\n\nencoders as discriminators rather than generators. ICLR, 2020.\n\nAlexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas Unterthiner, and Xiaohua Zhai. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\n\nAounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on\n\nrandomized smoothing for certifiable robustness. arXiv preprint arXiv:2002.03239, 2020.\n\nBai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and\n\ncertifiable robustness. 2018.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\n\nTowards deep learning models resistant to adversarial attacks. ICLR, 2018.\n\nAmir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial perturbations in learning from incomplete data. Advances in Neural Information Processing Systems, 32, 2019.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\n\nIn International Conference on Machine Learning, pp. 8162–8171. PMLR, 2021.\n\nHadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. Advances in Neural Information Processing Systems, 32, 2019.\n\nHadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A provable defense for pretrained classifiers. Advances in Neural Information Processing Systems, 33:21945–21957, 2020.\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-\n\nthinking the inception architecture for computer vision. CVPR, 2016.\n\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103, 2008.\n\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L ́eon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research, 11(12), 2010.\n\nLily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5276–5285, Stockholmsm ̈assan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\n\nEric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR, 2018.\n\nEric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 8400–8409. Curran Associates, Inc., 2018.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\n\nGreg Yang, Tony Duan, Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized\n\nsmoothing of all shapes and sizes. ICML, 2020.\n\nShuiwang Ji Yaochen Xie, Zhengyang Wang. Noise2same: Optimizing a self-supervised bound for\n\nimage denoising. Advances in Neural Information Processing Systems, 2020.\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In International Conference on Computer Vision (ICCV), 2019.\n\nRuntian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, and Liwei Wang. Adversarially robust generalization just requires more unlabeled data. arXiv preprint arXiv:1906.00555, 2019.\n\nRuntian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius. ICLR, 2021.\n\nBohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying l-infinity robustness using neural networks with l-inf-dist neurons. International conference on machine learning, 2021.\n\nBohang Zhang, Du Jiang, Di He, and Liwei Wang. Boosting the certified robustness of l-infinity\n\ndistance nets. ICLR, 2022a.\n\nBohang Zhang, Du Jiang, Di He, and Liwei Wang. Rethinking lipschitz neural networks and certified robustness: A boolean function perspective. In Advances in Neural Information Processing Systems, 2022b.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. In Kamalika Chaudhuri Theoretically principled trade-off between robustness and accuracy. and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7472–7482, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/ v97/zhang19p.html.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nWe present the full settings of pre-training and fine-tuning in Table 7 and Table 8, respectively.\n\nconfig\n\nvalue\n\nAdamW 1.5e-4 0.05\n\noptimizer base learning rate weight decay optimizer momentum β1, β2= 0.9, 0.95 batch size learning rate schedule warmup epochs augmentation gaussian noise\n\n4096 cosine decay 40 RandomResizedCrop σ = 0.25\n\nTable 7: Robust pre-train setting.\n\nconfig\n\nvalue\n\noptimizer base learning rate weight decay optimizer momentum layer-wise lr decay batch size learning rate schedule warmup epochs training epochs augmentation label smoothing mixup cutmix drop path gaussian noise\n\nconsistency regularization\n\nAdamW 5e-4 (B), 1e-3 (L) 0.05 β1, β2 = 0.9, 0.999 0.65 (B), 0.75 (L) 1024 cosine decay 5\n100 (B), 50 (L) RandAug (9, 0.5) 0.1 0.8 1.0 0.1 (B), 0.2 (L) σ ∈ {0.25, 0.5, 1.0} λ = 2, η = 0.5 (σ ∈ {0.25, 0.5}) λ = 2, η = 0.1 (σ = 1.0)\n\nTable 8: Fine-tuning setting.\n\nB EVALUATION PROTOCOL\n\nIn this section, we describe the details of how to estimate the approximate certified radius and certified accuracy.\n\nRecall that in Theorem 1, the certified radius R for datapoint x can be expressed as,\n\nR =\n\nσ 2\n\n[Φ−1(Pη[f (x + η) = y]) − Φ−1(max\n\ny′̸=y\n\nPη[f (x + η) = y′])].\n\nFor computational convenience, one can obtain a bit loose, but simpler bound of certified radius by using a upper bound 1 − Pη[f (x + η) = y] ≥ maxy′̸=y Pη[f (x + η) = y′],\n\nR ≥\n\nσ 2\n\n[Φ−1(Pη[f (x+η) = y])−Φ−1(1−Pη[f (x+η) = y])] = σ·Φ−1(Pη[f (x+η) = y]) =: R.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nσ\n\nMethod\n\n0.0\n\nCertified Accuracy(%) at l2 radius r 0.5\n\n1.0\n\n1.5\n\n2.0\n\n3.0\n\n0.5\n\nMAE-B+RS DMAE-B+RS\n\n72.2 72.6(+0.4)\n\nMAE-B+CR DMAE-B+CR 73.0(+1.0)\n\n72.0\n\n61.8 63.0(+1.2)\n\n64.1 64.8(+0.7)\n\n51.5 53.2(+1.7)\n\n55.9 57.9(+2.0)\n\n37.7 39.7(+2.0)\n\n46.2 47.8(+1.6)\n\n0 0\n\n0 0\n\n0 0\n\n0 0\n\nTable 9: DMAE v.s. MAE by Fine-tuning on ImageNet. Our proposed DMAE is consistently better than MAE on the ImageNet classification task, indicating that the proposed pre-training method is effective and learns more robust features. Numbers in (.) is the gap between the two methods in the same setting.\n\nFinetune σ\n\nEvaluate σ\n\nCertified Accuracy(%) at l2 radius r 0.0\n\n0.75\n\n0.25\n\n1.0\n\n0.5\n\n0.25\n\n0.5\n\n[0, 0.75]\n\n0.25 0.5\n\n0.25 0.5\n\n0.25 0.5\n\n90.4 44.5\n\n72.2 74.5\n\n90.2 75.6\n\n81.2 35.4\n\n65.2 66.5\n\n80.9 67.4\n\n65.3 26.8\n\n54.1 57.3\n\n66.1 58.5\n\n44.2 18.8\n\n41.6 46.3\n\n45.2 45.7\n\n0 11.3\n\n0 34.7\n\n0 35.5\n\nTable 10: Certified accuracy of DMAE for different evaluation perturbations on CIFAR-10. [0, 0.75] means training our model with σ uniformly selectly from [0, 0.75]. The results show one model is sufficient to tackle all evaluation settings.\n\nWe use the official implementation3 of CERTIFY to calculate the certified radius for any data point. For one data point x, the smoothed classifier samples n0 = 100 points from the noisy Gaussian distribution N (x, σ2Id) and then votes the predicted class, while we draw n = 10, 000 samples (for ImageNet) to estimate the lower bound of Pη[f (x + η) = y] and certify the robustness. Following previous works, we report the percentage of samples that can be certified to be robust (a.k.a certified accuracy) at radius r with pre-defined values.\n\nC SUPPLEMENTARY EXPERIMENTS\n\nIn this section, we report the results of several supplementary experiments.\n\nMore comparison with MAE. For a fair comparison, we compare our DMAE ViT-B with MAE ViT-B in the fine-tuning setting on ImageNet, in addition to the linear probing. The checkpoints are fine-tuned by the RS and CR method described in 4.4. In Table 9, we can see that DMAE outperforms MAE on all radii, which indicates the effectiveness of the denoising pre-training task.\n\nFine-tuning with various levels of noise. In the previous experiments, all the models are fine-tuned and evaluated under a specific level of Gaussian noise. One may wonder whether a single model can perform well under various levels of noise. To investigate this, we’ve conducted a tiny experiment on the CIFAR-10 dataset (we draw fewer noise samples n = 10, 000 and report results averaged over 1000 images) and reported the results in Table 10. Specifically, we sample the noise scale σ from a uniform distribution over an interval (σ ∈ [0, 0.75]) during fine-tuning, resulting in a robust classifier under different magnitudes of adversarial perturbations. And we compute the certified radius with this single model. The evaluation results show that it even outperforms the original model trained with a fixed noise scale when r = 1.0, which suggests that we can indeed use one DMAE across different settings without retraining.\n\n3https://github.com/locuslab/smoothing\n\n14",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors study the provably/certifiable robust classification problem. Specifically, the authors propose a self-supervised framework called Denoising Masked Auto-encoders to learn robust representations (or to pre-train encoders) by reconstructing images from noisy and masked inputs. The authors further fine-tune the model with the consistency regularization technique to achieve optimal robust classification for the model.\n\n# Strength And Weaknesses\n\n### Strengths\n\n(+) Results on existing tasks are good, showing the effectiveness of the proposed method\n\n(+) The work is well-motivated and the paper is easy to follow.\n\n(+) In addition to the certified accuracy evaluation, the authors include empirical studies to justify that DMAE learns more robust features than MAE.\n\n### Weaknesses\n\n(-) (major) The significance of the work may be limited and it is not well-positioned against existing works (i.e., MAE and Randomized Smoothing). The authors formulate their problem as to study the “robust vision learner” and propose the representation learning (with pre-training & fine-tuning) paradigm to learn robust encoder. The pre-trained encoder should be general-purposed and be evaluated in multiple different downstream tasks, e.g., the robust segmentation on noisy inputs, following the MAE work. However, the current evaluation is only performed on the image classification problem with only 2 datasets. This could limit the significance of the method as a “vision learner“.\n\n(-) (major) The proposed framework that adds noise to the training image and performs the masked reconstruction is straightforward given the desire of learning noise/attack robust encoders. Technically, it seems to be incremental to the MAE work and adapts MAE to the certified robust problem setting. Other techniques used in the paper such as consistency regularization are also from existing work. This may limit the novelty of this work.\n\n(-) (major) The proposed method is not sufficiently justified. Even focusing on the robust classification problem, the experiments may still not be sufficient to show the generalizability of the proposed approach. Current experiments only consider the Gaussian additive noise in both training and evaluation. The certified radius evaluation can cover those cases by selecting a large enough radius, but cannot fully justify or compare the model behavior under certain noise/attack types. In real scenarios, the noise/attack might be more diverse and complicated, e.g., Poisson, impulse, combined, or even dedicated to attacking certain models. It would be better to include more evaluation on some of those cases.\n\n(-) (minor) Does the robust fine-tuning (with regularizations) also be applied to baseline methods so that the comparison is fair? If not, it would be better to make comparisons consistently with/without the robust fine-tuning in order to show that the improvement comes from the DMAE pre-training framework.\n\n(-) (minor) To be more self-contained, it is better to include a better formulation of the certified robust classification problem and the evaluation protocol with the certified radius.\n\n(-) (minor) I also suggest the author discuss the self-supervised (blind-spot) image denoising paper [1, 2] since they are relevant and the frameworks are actually similar. The only difference is that the denoising approaches take the final images as outcomes and this work takes the representation or pre-trained encoder as outcomes. In particular, when considering the masked&noise reconstruction framework and the robust fine-tuning term (KL divergence between two outputs), the framework is very close in its looking to the objective in [2]. It would be better to discuss any connections and highlight differences between those works.\n\n[1] Batson et al. Noise2Self: Blind Denoising by Self-Supervision. ICML 2019.\n\n[2] Xie et al. Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising. NeurIPS 2020.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity and Reproducibility: The authors study an interesting and well-motivated problem. The paper is well-written and easy to follow. Sufficient implementation details are provided for reproduction.\n\nQuality: The experiments done in the work indicate the effectiveness of DMAE to some degree. However, additional evaluations on more tasks and datasets may make the method more convincing.\n\nNovelty: The key idea is very straightforward given the problem setting and the proposed approach seems to be an adaptation of MAE in a stricter setting. Hence the novelty is limited.\n\n# Summary Of The Review\n\nThe paper studies an interesting problem and is overall easy to follow. Experiments done in the work can indicate the effectiveness of DMAE to some degree. However, there are some concerns about the novelty, significance, and insufficient evaluation of the work. I believe the paper would be a good one if the authors can show its capability in more tasks with noisy input (that would be a lot of empirical contribution), but the current form of this paper may not be good enough.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLDMIC: LEARNING-BASED DISTRIBUTED MULTIVIEW IMAGE CODING\n\nXinjie Zhang, Jiawei Shao, Jun Zhang The Hong Kong University of Science and Technology, Hong Kong, China {xinjie.zhang, jiawei.shao}@connect.ust.hk, eejzhang@ust.hk\n\nABSTRACT\n\nMulti-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the crossattention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relationships between images. Experimental results show that LDMIC significantly outperforms both traditional and learning-based MIC methods while enjoying fast encoding speed. Code is released at https://github.com/Xinjie-Q/LDMIC.\n\n1\n\nINTRODUCTION\n\nMulti-view image coding (MIC) aims to jointly compress a set of correlated images captured from different viewpoints, which is promising to achieve high coding efficiency for the whole image set by exploiting inter-image correlation. It plays an important role in many applications, such as autonomous driving (Yin et al., 2020), virtual reality (Fehn, 2004), and robot navigation (SanchezRodriguez & Aceves-Lopez, 2018). As shown in Figure 1(a), existing multi-view coding standards, e.g., H.264-based MVC (Vetro et al., 2011) and H.265-based MV-HEVC (Tech et al., 2015), adopt a joint coding architecture to compress different views. Specifically, they follow the predictive compression procedure of video standards, in which a selected base view is compressed by single image coding. When compressing the dependent view, both the disparity estimation and compensation are employed at the encoder to generate the predicted image. Then the disparity information as well as residual errors between the input and predicted image are compressed and passed to the decoder. In this way, the inner relationship between different views decreases in sequel. These methods depend on hand-crafted modules, which prevents the whole compression system from enjoying the benefits of end-to-end optimization.\n\nInspired by the great success of learning-based single image compression (Ball ́e et al., 2017; 2018; Minnen et al., 2018; Cheng et al., 2020), several recent works have investigated the application of deep learning techniques to stereo image coding, a special case of MIC. In particular, Liu et al. (2019), Deng et al. (2021) and W ̈odlinger et al. (2022), mimicking traditional MIC techniques, adopt a unidirectional coding mechanism and explicitly utilize the disparity compensation prediction in the pixel/feature space to reduce the inter-view redundancy. Meanwhile, Lei et al. (2022) introduces a bi-directional coding framework, called as BCSIC, to jointly compress left and right images simultaneously for exploring the content dependency between the stereo pair. These rudimentary studies demonstrate the potentials of deep neural networks (DNNs) in saving significant bit-rate for MIC.\n\nHowever, there are several significant shortcomings hampering the deployment and application scope of existing MIC methods. Firstly, both the traditional and learning-based approaches demand inter-view prediction at the encoder, i.e., joint encoding, which requires the cameras to communi-\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 1: Overview of different multi-view image coding architectures, including (a) a joint encoding architecture and (b) the proposed symmetric distributed coding architecture.\n\ncate with each other or to transmit the data to an intermediate common receiver, thereby consuming a tremendous amount of communication resources and increasing the deployment cost (Gehrig & Dragotti, 2007). This is undesirable in applications relevant to wireless multimedia sensor networks (Akyildiz et al., 2007). An alternative is to deploy special sensors like stereo cameras as the encoder devices to acquire the data, but these devices are generally more expensive than monocular sensors and suffer from limited field of view (FoV) due to the constraints of distance and position between built-in sensors (Li, 2008). Secondly, most of the prevailing schemes, except BCSIC, are developed based on disparity correlations defined by the epipolar geometric constraint (Scharstein & Szeliski, 2002), which usually requires to know the internal and external parameters of the camera in advance, such as camera locations, orientations, and camera matrices. Whereas, it is difficult for a distributed camera system without communication to access the prior knowledge of cameras (Devarajan et al., 2008). For example, the specific location information of cameras in autonomous driving is usually not expected to be perceived by other vehicles or infrastructure in order to avoid leaking the location and trajectory of individuals (Xiong et al., 2020). Finally, as shown in Table 1 and Figure 4, compared with state-of-the-art (SOTA) learning-based single image codecs (Minnen et al., 2018; Cheng et al., 2020), existing DNN-based MIC methods are not competitive in terms of rate-distortion (RD) performance, which is potentially caused by inefficient inter-view prediction networks.\n\nTo address the above challenges, we resort to innovations in the image coding architecture. Particularly, our inspiration comes from the Slepian-Wolf (SW) theorem (Slepian & Wolf, 1973; Wolf, 1973) on distributed source coding (DSC) 1. The SW theorem illustrates that separate encoding and joint decoding of two or more correlated sources can theoretically achieve the same compression rate as a joint encoding-decoding scheme under lossless compression. It has been extended to the lossy case by Berger (1978) and Tung (1978), which provides the inner and outer bounds of the achievable rate region. Based on these information-theoretic results on DSC, we develop a learning-based distributed multi-view image coding (LDMIC) framework. Specifically, to avoid collaboration between different cameras, as shown in Figure 1(b), each view image is mapped to the corresponding quantized latent representation by an individual encoder, while a joint decoder is used to reconstruct the whole image set, which can successfully avoid the communication among cameras or the usage of special sensors. This architectural innovation is theoretically supported by the DSC theory. Instead of disparity-based correlations, we design a joint context transfer (JCT) module based on the cross-attention mechanism agnostic to geometry priors to exploit the global content dependencies between different views at the decoder, making our approach applicable to arbitrary multi-camera systems with overlapping FoV. Finally, since the separate encoding and joint decoding scheme is implemented by DNNs, the end-to-end RD optimization strategy is leveraged to implicitly help the encoder to learn to remove the partial inter-view redundancy, thus improving the compression performance of the overall system. In summary, our main contributions are as follows:\n\n• To the best of our knowledge, this is the first work to develop a novel deep learning-based view-symmetric framework for multi-view image coding. It decouples the inter-view operations at the encoder, which is highly desirable for distributed camera systems.\n\n• We present a joint context transfer module at the decoder to explicitly capture inter-view correlations for generating more informative representations. We also propose an end-toend encoder-decoder training strategy to implicitly make the latent representations more compact.\n\n1More details about the theorem and proposition of distributed source coding are provided in Appendix 6.4.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n• Extensive experimental results show that our proposed framework is the first distributed codec achieving comparable coding performance to the SOTA joint encoding-decoding schemes, implying the effectiveness of the inter-view cross-attention mechanism compared with the conventional disparity-based prediction. Moreover, our proposed framework outperforms the asymmetric-based coding framework NDIC (Mital et al., 2022b), which demonstrates the advantage of the view-symmetric design over the asymmetric one.\n\n2 RELATED WORKS\n\nSingle Image Coding. In the past decades, various standard image codecs have been developed, including JPEG (Wallace, 1992), JPEG2000 (Skodras et al., 2001), BPG (Bellard, 2014), and VVC intra (Bross et al., 2021). They generally apply three key ideas to reduce redundancy: (i) transform coding, e.g., discrete cosine transform, to decrease the spatial correlation, (ii) quantization of transform coefficients to filter the irrelevancy related to the human visual system, and (iii) entropy coding to lessen the statistical correlation of the coded symbols. Unfortunately, these components are separately optimized, making it hard to achieve optimal coding efficiency.\n\nRecently, end-to-end image compression has engaged increasing interests, which is built upon the transform coding paradigm with nonlinear transform and powerful entropy models for higher compression efficiency. Nonlinear transform is used to produce compact representations, such as generalized divisive normalization (GDN) (Ball ́e et al., 2015), the self-attention block (Cheng et al., 2020), wavelet-like invertible transform (Ma et al., 2020) and stacks of residual bottleneck blocks (He et al., 2022). To approximate the distribution of latent representations, many advanced entropy models have been proposed. For example, Ball ́e et al. (2017; 2018) put forward the factorized and hyper prior entropy models for the first time. Then the auto-regressive context model (Minnen et al., 2018) is combined into the hyper prior to effectively reduce the spatial redundancy of images at the expense of high decoding latency. In order to improve the decoding speed, Minnen & Singh (2020) and He et al. (2021) investigate the channel-wise and spatial-wise context versions, respectively. These existing works are considered as important building blocks for our scheme.\n\nMulti-view Image Coding. Conventional MIC standards (Vetro et al., 2011; Tech et al., 2015) are derived from key frame compression methods designed for multi-view video codecs. Since these methods are still in the development stage and only support YUV420 format, they are uncompetitive against single image codecs that allow the YUV444 or RGB format. Meanwhile, existing learningbased MIC approaches (Liu et al., 2019; Deng et al., 2021; W ̈odlinger et al., 2022; Lei et al., 2022) mainly focus on stereo images, and it is difficult to effectively extend them to the general multi-view scenario. Moreover, they can only handle a fixed number of views. In contrast, our framework exerts average pooling to merge the information between multiple views, making it insensitive to the number of viewpoints.\n\nDistributed Source Coding. There have been some works developing multi-view compression methods based on DSC. They are typically built on the setting of coding with side information (Zhu et al., 2003; Thirumalai et al., 2007; Chen et al., 2008; Wang et al., 2012), where one view is selected as a reference and compressed independently. For other views, the joint decoder uses the reference as side information to capture the inter-view correlations to reduce the coding rate. Recent learning-based distributed multi-view image compression concentrates on this asymmetric paradigm (Ayzik & Avidan, 2020; Whang et al., 2021; Wang et al., 2022; Mital et al., 2022a;b). Nevertheless, this architecture suffers from high transmission cost for the primary sensor, since it requires a hierarchical relationship between different cameras, leading to the unbalanced coding rates among them (Tosic & Frossard, 2009).\n\nDifferent from the above works, we consider a more practical symmetric coding pattern illustrated in Figure 1(b), where all cameras are treated as equal status. While traditional symmetric coding schemes (Thirumalai et al., 2008; Gehrig & Dragotti, 2009) utilize disparity-based estimation at the decoder to reduce the transmission cost, we get rid of the disparity compensation prediction and adopt the cross-attention mechanism (Vaswani et al., 2017) to capture the global relevance between different views, which effectively improves the compression performance and broadens the application scope. As far as our knowledge, our study is the first in applying DNNs into symmetric distributed coding and achieving the RD performance comparable to joint encoding-decoding schemes.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The proposed LDMIC framework with an auto-regressive entropy model, where ˆyK\\{k} and hK\\{k} represent the set of all the view features except for the k-th view feature ˆyk and hk, respectively. Convolution/deconvolution parameters are formatted as (the number of output channels, kernel size, stride). Q denotes quantization. AE and AD represent arithmetic encoder and decoder, respectively.\n\n3 PROPOSED METHOD\n\n3.1 THE OVERALL ARCHITECTURE OF LDMIC\n\nFigure 2 depicts the network architecture of the proposed method. Let K = {1, · · · , K} denote the image index set. Given a group of multi-view images xK = {x1, x2, · · · , xK}, each image xk is independently mapped to the corresponding representation yk by the encoder Ek with shared network parameters. Then yk is quantized to ˆyk. After receiving all the quantized representations ˆyK, the joint decoder JD exploits the inter-view correlations among ˆyK to reconstruct the whole image set ˆxK. The compression procedure is described as\n\nyk = Ek(xk, φ), ∀k ∈ K, ˆyk = Q(yk), ∀k ∈ K, ˆxK = JD(ˆyK; θ),\n\n(1)\n\nwhere φ and θ are optimized parameters of the encoder and decoder. Since the quantizer Q is not differentiable, we apply the mixed quantization approach proposed in Minnen & Singh (2020) during training. Specifically, the latent representation yk with an additive uniform noise is taken as the input to the entropy model for estimating the bitrate, while the rounded representation with a straight-through-estimation (STE) gradient flows to the joint decoder for reconstruction.\n\nTo apply entropy coding to reduce the statistical correlation of the quantized representation ˆyk, each element ˆyk,i is modelled as a univariate Gaussian random variable with its mean μk,i and standard deviation σk,i by introducing a side information ˆzk,i, where i denotes the position of each element in a vector-valued signal. The probability distribution p ˆyk|ˆzk of ˆyk is expressed as follows:\n\np ˆyk|ˆzk (ˆyk|ˆzk) ∼ N (μk, σ2 Meanwhile, a context model is also combined with the entropy model for effectively reducing the spatial redundancy of latent ˆyk. The selection of the context model depends on the specific needs of different applications. We choose an auto-regressive model (Minnen et al., 2018) and a checkerboard model (He et al., 2021) for better coding efficiency and faster coding speed, respectively.\n\nk).\n\n(2)\n\n3.2\n\nJOINT CONTEXT TRANSFER MODULE\n\nDue to the overlap between the cameras’ FoV, there exist significant inter-view correlations in the feature space, which inspires us to propose a joint context transfer (JCT) module to exploit this property for generating more informative representations. As shown in Figure 3, the proposed JCT module receives multi-view features fK as inputs, learns an inter-view context for each view feature, and refines the input features based on the corresponding inter-view contexts. Note that there are K parallel paths in the JCT module. Each path shares the same network parameters and follows a three-step process described below to obtain the refined representations f ∗ K.\n\nFeature extraction. We firstly utilize two residual blocks to extract the representative feature f k\nfrom the k-th view fk. Each residual block, as depicted in Figure 3, is composed of two consecutive convolution layers with Leaky ReLU activation functions.\n\n′\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Illustration of the k-th path in the proposed joint context transfer module, where f\n\n′\n\nK\\{k}\n\ndenotes the set of all the view representations except for the current view representation f\n\n′\n\nk.\n\nMulti-view fusion. All the representations f aggregated to a preliminary context ̃f number of the input features:\n\n′\n\nK from the feature extraction module except f k are k via a simple average pooling over the dimension of the\n\n′\n\n′\n\n′\n\n ̃f\n\nk =\n\n1 K − 1\n\n(cid:88)\n\ni∈K\\{k}\n\n′\n\nf\n\ni ,\n\n(3)\n\nwhere K\\{k} = {1, · · · , k − 1, k + 1, · · · , K}. By this aggregation operation, we achieve fusion between any number of view features. In addition, it is observed that more complex pooling approaches can be developed to further improve the performance.\n\n′\n\n′\n\nk and ̃f\n\nAfter getting the aggregated context, we apply a multi-head cross-attention module to exploit the dependency between f k. Since the original attention module incurs high memory and computational cost under a large spatial dimension of input, we adopt the resource-efficient attention in Shen et al. (2021). Specifically, we use a 1 × 1 convolution layer and a reshape operation to k) ∈ Rn×h×d1, key transform f Kk = Conv( ̃f k) ∈ Rn×h×d2 , where n = H × W and h denotes the number of heads. The notations d, d1 and d2 are the channel dimensions of input, key (query) and value in a head, respectively. Then the multi-head cross-attention module is applied as:\n\nk ∈ RH×W ×d, i.e., query Qk = Conv(f\n\nk) ∈ Rn×h×d1 and value Vk = Conv( ̃f\n\nk ∈ RH×W ×d and ̃f\n\n′\n\n′\n\n′\n\n′\n\n′\n\nAk,i = σrow(Qk,i)(σcol(Kk,i)TVk,i), ∀i = 1, · · · , h\n\n′\n\nf\n\nK\\{k}→k = Conv(Ak,1 ⊕ · · · ⊕ Ak,h),\n\n(4)\n\nwhere σrow (σcol) denotes applying the softmax function along each row (column) of the matrix, K\\{k}→k relevant to the k-th view feature is and ⊕ is the channel-wise concatenation. The context f extracted and will be injected into the current feature in the next step.\n\n′\n\nRefinement. Based on the learned inter-view context f more informative feature f ∗ k :\n\n′\n\nK\\{k}→k, the input feature fk is refined to a\n\nf ∗\n\nk = fk + F (f\n\nK\\{k}→k ⊕ f\n\nk),\n\n′\n\n′\n\n(5)\n\nwhere F (·) consists of two consecutive residual blocks. As shown in Figure 2, the JCT module is placed before the first and third deconvolution layers to connect the different-view decoding stream for feature aggregation and transformation.\n\n3.3 TRAINING\n\nThe target of LDMIC is to optimize the trade-off between the number of encoded bits and the reconstruction quality. Therefore, a training loss composed of two metrics is used:\n\nL = λD + R = λ\n\nK (cid:88)\n\nk=1\n\nd(xk, ˆxk) +\n\nK (cid:88)\n\nk=1\n\n(cid:0)R(ˆyk) + R(ˆzk)(cid:1)\n\n(6)\n\nwhere d(xk, ˆxk) is the distortion between xk and ˆxk under a given metric, such as mean squared error (MSE) R(ˆyk) and R(ˆzk) represent the estimated compression rates of the latent representation ˆyk and the corresponding hyper representation ˆzk, respectively. λ is a hyper parameter that controls the trade-off between the bit rate cost R and distortion D.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(f)\n\nFigure 4: Rate-distortion curves of our proposed methods compared against various competitive baselines.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets. To compare with the recently developed learning-based stereo image compression methods, two common stereo image datasets, i.e., Instereo2K (Bao et al., 2020) and Cityscapes (Cordts et al., 2016), are chosen to evaluate the coding efficiency of the proposed framework. Apart from testing stereo image datasets related to 3D scenes, we also select a pedestrian surveillance dataset, i.e., WildTrack (Chavdarova et al., 2018), acquired by seven random placed cameras with overlapping FoV, which is to demonstrate the potentials of our proposed framework in distributed camera systems without epipolar geometry relationship between images. More details about datasets are provided in Appendix 6.5.\n\nBenchmarks. The competing baselines can be split into three categories: (1) Separate model independently compresses each image, whose typical SOTA representatives are BPG (Bellard, 2014), VVC-intra (Bross et al., 2021), Minnen et al. (2018) and Cheng et al. (2020). For BPG and VVCintra, we disable chroma subsampling. (2) Joint model has access to a set of multi-view images and explicitly utilizes the inter-view redundancy to achieve a high compression ratio. According to performance comparisons in W ̈odlinger et al. (2022), conventional video standards can be applied in the MIC, where each set of multi-view images is compressed as a multi-frame video sequence by using both HEVC (Sullivan et al., 2012) and VVC (Bross et al., 2021) with lowdelay P configuration as well as YUV444 input format. We also test MV-HEVC (Tech et al., 2015) with the multi-view intra mode. Apart from that, we report the results of several recent DNN-based stereo image codecs on the InStereo2K and Cityscapes datasets, including DSIC (Liu et al., 2019), two variants of HESIC (Deng et al., 2021), BCSIC (Lei et al., 2022), and SASIC (W ̈odlinger et al., 2022). (3) Distributed model only uses the joint decoder to implicitly reduce the inter-view dependency. We compare our method with NDIC based on asymmetric DSC (Mital et al., 2022b) to demonstrate the superiority of symmetric DSC. More details on baseline settings are given in Appendix 6.5.\n\nMetrics. The distortion between the reconstructed and original images is measured by peak signalto-noise ratio (PSNR) and multi-scale structural similarity index (MS-SSIM) (Wang et al., 2003). Besides assessing RD curves, we compute the Bjøntegaard Delta bitrate (BDBR) (Bjøntegaard, 2001) results to represent the average bitrate savings at the same distortion level.\n\nImplementation Details. We train our models with five different λ values, where λ = 256, 512, 1024, 2048, 4096 (8, 16, 32, 64, 128) under MSE (MS-SSIM). For MSE optimized models, they are trained from scratch for 400 epochs on InStereo2K/Cityscapes and 700 epochs on\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of BDBR cost relative to BPG on different datasets, with the best results in Bold and second-best ones in underlined.\n\nCategories\n\nMethods\n\nInStereo2K\n\nCityscapes\n\nWildTrack (C1, C4)\n\nPSNR\n\nMS-SSIM\n\nPSNR\n\nMS-SSIM\n\nPSNR\n\nMS-SSIM\n\nSeparate\n\nJoint\n\nMinnen2018 Cheng2020 VVC-intra\n\nVVC HEVC MV-HEVC HESIC HESIC+ DSIC BCSIC SASIC\n\n-7.44% -19.71% -3.23%\n\n-30.54% -15.54% 2.83% 0.47% -15.06% 107.88% 23.80% -19.83%\n\n-34.37% -41.95% -17.38%\n\n-33.80% -14.70% -4.75% -39.55% -43.56% -40.04% -56.11% -23.04%\n\n-21.58% -27.86% -4.14%\n\n-46.74% -49.63% -22.12%\n\n-52.85% -46.35% -24.48% -23.40% -0.17% -18.57% -45.14% -7.92% -51.33% -21.70% -38.26% -1.88% -\n- -30.10% -20.39%\n\n-10.40% -19.23% -10.76%\n\n-4.18% 39.04% 33.88% -\n- -\n- -\n\n-47.73% -52.54% -24.84%\n\n-9.31% 23.27% 9.35% -\n- -\n- -\n\nDistributed\n\nNDIC Proposed-fast Proposed\n\n7.36% -34.38% 13.98% -28.30% -49.89% -29.68% -41.69% -59.20% -40.14%\n\n-51.08% 3.94% -38.07% -55.08% -26.69% -53.61% -62.12% -31.21% -67.77%\n\nTable 2: Complexity of learning-based image codecs evaluated on a pair of stereo images with the resolution as 832×1024 in the InStereo2K dataset, where the encoding latency of DSC-based schemes is determined by the maximum time for independent encoding of each image.\n\nMethods\n\nEncoder\n\nDecoder\n\nFLOPs\n\nParams\n\nTime\n\nFLOPs\n\nParams\n\nTime\n\nDSIC HESIC HESIC+ SASIC NDIC Proposed-fast Proposed\n\n2415.29G 285.3G 205.71G 531.42G 163.93G×2 194.15G×2 187.39G×2\n\n79.26M 32.08M 17.02M 3.58M 7.25M×2 11.24M×2 11.24M×2\n\n25.97s 3.23s 16.79s 10.66s 3.19s 2.37s 9.44s\n\n3378.65G 75.78M 26.45s 1197.22G 29.55M 16.15s 1122.87G 15.28M 49.96s 2532.87G 4.48M 34.45s 1245.89G 25.04M 9.93s 1851.96G 15.24M 11.48s 1838.42G 15.24M 47.87s\n\nWildTrack by using Adam optimizer (Kingma & Ba, 2014), in which the batch size is taken as 8. The learning rate is initially set as 10−4 and decreased by a factor of 2 every 100 epochs until it reaches 400 epochs. As for MS-SSIM optimized models, we fine-tune the MSE optimized networks for 300 (400) epochs with the initial learning as 5 × 10−5 on stereo (multi-camera) image dataset. During training, each image is randomly flipped and cropped to the size of 256 × 256 for data augmentation. The whole framework is implemented by CompressAI (B ́egaint et al., 2020) and trained on a machine with NVIDIA RTX 3090 GPU.\n\n4.2 EXPERIMENTAL RESULTS\n\nCoding performance. Figure 4 presents the RD curves of all compared methods and Table 1 gives the corresponding BDBR results of each codec relative to BPG. For InStereo2K and Cityscapes, the proposed method outperforms most of these compression baselines in both PSNR and MS-SSIM, which implies that relying only on joint decoding can effectively reduce the inter-view redundancy between different views. For example, when compared with Cheng2020 (SASIC), our method and the fast variant reduce around 21.98% (21.86%) and 9.97% (9.85%) bits in terms of PSNR, respectively. Since stereo images contain plenty of homogeneous regions suited for traditional coding, VVC achieves up to 30.54% and 52.85% compression efficiency when measured by PSNR, but notice that it requires joint encoding. On the InStereo2K dataset, VVC underperforms our method by a margin with about 0.44dB coding gains in PSNR due to a larger variation in image content. In addition, our proposed framework attains better reconstruction quality measured by MS-SSIM at the same bitrate when compared with VVC.\n\nAs seen from Figure 4(c) and 4(f), we select the images acquired by two cameras, C1 and C4, on the WildTrack dataset to evaluate the compression performance of different methods without\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Ablation study. Joint Enc-Dec and Sep Enc-Dec denote inserting and removing the JCT module at the encoder and decoder, respectively. Concatenation, SAM and BiCTM represent different inter-view operations to replace the proposed JCT module at the decoder. W/O Joint Training is to fix the pretrained encoder including the entropy model and only train the joint decoder.\n\nFigure 6: Visual examples from the InStereo2K dataset, where we assemble all channels of the latent representation Q(yk − μk) to display the feature map.\n\nusing additional information from other cameras. It is observed that the traditional video codecs perform worse than the corresponding intra-frame ones due to lots of heterogeneous overlapping regions, which makes it difficult for standard video codecs to effectively capture the inter-view redundancy by using compensation-based predictions. However, our proposed framework relies on the cross-attention mechanism to exploit the correlations of different views from the perspective of global receptive fields, thereby providing up to 31.21% and 67.77% bitrate saving in PSNR and MSSSIM, respectively. The remarkable results demonstrate that the proposed LDMIC framework is a promising solution to meet the compression needs of distributed camera systems. The RD curves on the multi-camera case have similar trends with that on the two-camera one, which are provided in Appendix 6.1.\n\nMoreover, compared with asymmetric DSC-based NDIC, the proposed method saves 55.67%, 47.5% and 35.15% bits in PSNR on three datasets (InStereo2K, Cityscapes, WildTrack). For the proposed-fast variant with the checkerboard entropy model, the improvements are also adequate, i.e., 43.66%, 35.66% and 30.63%. This set of results indicate that the usage of bi-directional information based on symmetric DSC can better exploit the inter-view correlations to bring higher coding gains. Additionally, our methods have better compression efficiency in MS-SSIM than in PSNR, which is partly caused by exploiting the inter-view correlations in the feature space rather than pixel space at the decoder. Thus, the network tends to focus on structure information instead of pixel information.\n\nComputational complexity. Table 2 shows the computational complexity of seven image codecs running on an Intel Xeon Gold 6230R processor with base frequency 2.10GHz and a single CPU core, including the number of FLOPs, the model parameters and the coding latency. Different from the joint models, our methods designed on DSC decouples the inter-view operations at the encoder, which allows image-level parallel processing. Therefore, the proposed-fast variant enjoys about 1.36 ∼ 10.95 and 1.41 ∼ 4.35 times encoding and decoding speedup against the learned joint schemes (i.e., DSIC, HESIC, HESIC+, SASIC). Even if the auto-regressive entropy model is used, the encoding of our method is still faster than that of both DSIC and SASIC based on hyper\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Bitrate savings for two-view images with cameras C1 and C2 as the number of viewpoints increases on the WildTrack dataset, where the case of K = 2 is set as the anchor.\n\nNumber of cameras K = 2 K = 3 K = 4 K = 5 K = 6 K = 7\n\nBitrate saving (%)\n\n0\n\n0.0053\n\n0.0801\n\n1.0919\n\n1.4161\n\n1.5004\n\nprior. Moreover, our proposed fast variant with better coding efficiency achieves similar coding time with another DSC-based method NDIC, which demonstrates the superiority of symmetric DSC in coding speed and compression efficiency. For more details on comparison between our methods and traditional codecs, please refer to Appendix 6.2.\n\n4.3 ABLATION STUDY\n\nInter-view Fusion. To verify the contribution of the JCT module for fusing inter-view information, a set of ablation experiments are conducted on the InStereo2K dataset with RD curves shown in Figure 5. Specifically, we allow (forbid) both the encoder and the decoder to access the interview context, which provides an upper (lower) bound on the performance of the proposed method and is denoted by Joint (Sep) Enc-Dec. In this case, the PSNR with (without) the JCT module at the encoder (decoder) improves (drops) by about 0.16dB (0.73dB) at the same bpp level. We further report the compression results when the JCT module is directly replaced by other inter-view fusion operations such as concatenation in Mital et al. (2022b), stereo attention module (SAM) in W ̈odlinger et al. (2022) and bi-directional contextual transform module (Bi-CTM) in Lei et al. (2022). These operations lead to an increase of the bitrate by 32.73%, 27.99%, 10.11% compared with our method. The experimental results indicate that our proposed JCT module have powerful capability in capturing inter-view correlations and generating more informative representations.\n\nJoint Training Strategy. In this paper, we exploit the benefit of joint training to implicitly help the encoder to learn removing the partial inter-view redundancy. Thus, the latent representation is expected to be more compact. To investigate its effect, we perform a experiment by only training the joint decoder with the fixed pre-trained encoder and entropy model. As shown in Figure 5, our approach outperforms the W/O Joint Training method by 0.225 dB. In Figure 6, we provide further visual comparisons. It is noted that the latent feature maps with joint training strategy contain more elements with low magnitudes, which requires much fewer bits for encoding.\n\nNumber of views. Table 3 shows the impact of different numbers of views on coding efficiency. We compare the bitrate of cameras C1 and C2 when incorporating different numbers of views during decoding. The bitrate saving increases gradually as more information is received from different cameras. Due to only using a simple average pooling to merge multi-view information to the interview context, we get a marginal coding gains when incorporating more views. It is possible to further improve the compression gains of our framework by using more complex aggregation approaches.\n\n5 DISCUSSION\n\nIn this paper, we presented a novel end-to-end distributed multi-view image coding framework nicknamed LDMIC. Our proposal inherits the advantages of traditional distributed compression in image-level parallelization, which is desirable for distributed camera systems. Meanwhile, leveraging the insensitivity of the cross-attention mechanism to epiploar geometric relations, we develop a joint context transfer module to account for global correlations between images from different viewpoints. Experimental results demonstrate the competence of LDMIC in achieving higher coding gains than existing learning-based joint and separate encoding-decoding schemes. Moreover, compared with learned joint models, the LDMIC fast variant enjoys a much lower coding complexity with on-par compression performance. To the best of our knowledge, this is the first successful attempt of the distributed coding architecture to fight against the performance of the joint coding paradigm under the lossy compression case.\n\nBased on the proposed framework, there are two clear directions to be explored in the future. On one hand, as mentioned in Section 4.3, it is interesting to investigate how to more effectively incorporate different view information to generate a better inter-view context. One the other hand, it is worth exploring how to extend the framework to multi-view video compression.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was supported by the NSFC/RGC Collaborative Research Scheme (Project No. CRS HKUST603/22).\n\nREFERENCES\n\nIan F Akyildiz, Tommaso Melodia, and Kaushik R Chowdhury. A survey on wireless multimedia\n\nsensor networks. Computer networks, 51(4):921–960, 2007.\n\nSharon Ayzik and Shai Avidan. Deep image compression using decoder side information. In Euro-\n\npean Conference on Computer Vision, pp. 699–714. Springer, 2020.\n\nJohannes Ball ́e, Valero Laparra, and Eero P Simoncelli. Density modeling of images using a gener-\n\nalized normalization transformation. arXiv preprint arXiv:1511.06281, 2015.\n\nJohannes Ball ́e, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.\n\nIn International Conference on Learning Representations, 2017.\n\nJohannes Ball ́e, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In International Conference on Learning Representations, 2018.\n\nWei Bao, Wei Wang, Yuhua Xu, Yulan Guo, Siyu Hong, and Xiaohu Zhang. Instereo2k: A large real dataset for stereo matching in indoor scenes. Science China Information Sciences, 63(11): 1–11, 2020.\n\nJean B ́egaint, Fabien Racap ́e, Simon Feltman, and Akshay Pushparaja. Compressai: a pyarXiv preprint\n\ntorch library and evaluation platform for end-to-end compression research. arXiv:2011.03029, 2020.\n\nFabrice Bellard. Bpg image format. https://bellard.org/bpg/, 2014.\n\nToby Berger. Multiterminal source coding. The Information Theory Approach to Communications\n\n(CISM Courses and Lectures), 229:171–231, 1978.\n\nGisle Bjøntegaard. Calculation of average psnr differences between rd-curves. ITU-T VCEG-M33,\n\nApril, 2001, 2001.\n\nBenjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan, and Jens-Rainer Ohm. Overview of the versatile video coding (vvc) standard and its applications. IEEE Transactions on Circuits and Systems for Video Technology, 31(10):3736–3764, 2021.\n\nTatjana Chavdarova, Pierre Baqu ́e, St ́ephane Bouquet, Andrii Maksai, Cijo Jose, Timur Bagautdinov, Louis Lettry, Pascal Fua, Luc Van Gool, and Franc ̧ois Fleuret. Wildtrack: A multi-camera hd dataset for dense unscripted pedestrian detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5030–5039, 2018.\n\nDavid Chen, David Varodayan, Markus Flierl, and Bernd Girod. Distributed stereo image coding with improved disparity and noise estimation. In 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1137–1140. IEEE, 2008.\n\nZhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7939–7948, 2020.\n\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban In Proceedings of the IEEE conference on computer vision and pattern scene understanding. recognition, pp. 3213–3223, 2016.\n\nXin Deng, Wenzhe Yang, Ren Yang, Mai Xu, Enpeng Liu, Qianhan Feng, and Radu Timofte. Deep homography for efficient stereo image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1492–1501, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDhanya Devarajan, Zhaolin Cheng, and Richard J Radke. Calibrating distributed camera networks.\n\nProceedings of the IEEE, 96(10):1625–1639, 2008.\n\nChristoph Fehn. Depth-image-based rendering (dibr), compression, and transmission for a new In Stereoscopic displays and virtual reality systems XI, volume 5291, pp.\n\napproach on 3d-tv. 93–104. SPIE, 2004.\n\nNicolas Gehrig and Pier Luigi Dragotti. Distributed compression of multi-view images using a In 2007 IEEE International Conference on Image Processing,\n\ngeometrical coding approach. volume 6, pp. VI–421. IEEE, 2007.\n\nNicolas Gehrig and Pier Luigi Dragotti. Geometry-driven distributed compression of the plenoptic function: Performance bounds and constructive algorithms. IEEE transactions on image processing, 18(3):457–470, 2009.\n\nDailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context model for efficient learned image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14771–14780, 2021.\n\nDailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5718–5727, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nJianjun Lei, Xiangrui Liu, Bo Peng, Dengchao Jin, Wanqing Li, and Jingxiao Gu. Deep stereo image compression via bi-directional coding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19669–19678, 2022.\n\nShigang Li. Binocular spherical stereo. IEEE Transactions on intelligent transportation systems, 9\n\n(4):589–600, 2008.\n\nJerry Liu, Shenlong Wang, and Raquel Urtasun. Dsic: Deep stereo image compression. In Proceed-\n\nings of the IEEE/CVF International Conference on Computer Vision, pp. 3136–3145, 2019.\n\nHaichuan Ma, Dong Liu, Ning Yan, Houqiang Li, and Feng Wu. End-to-end optimized versatile IEEE Transactions on Pattern Analysis and\n\nimage compression with wavelet-like transform. Machine Intelligence, 2020.\n\nDavid Minnen and Saurabh Singh. Channel-wise autoregressive entropy models for learned image compression. In 2020 IEEE International Conference on Image Processing (ICIP), pp. 3339– 3343. IEEE, 2020.\n\nDavid Minnen, Johannes Ball ́e, and George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. Advances in neural information processing systems, 31, 2018.\n\nNitish Mital, Ezgi Ozyilkan, Ali Garjani, and Deniz Gunduz. Neural distributed image compression\n\nwith cross-attention feature alignment. arXiv preprint arXiv:2207.08489, 2022a.\n\nNitish Mital, Ezgi ̈Ozyılkan, Ali Garjani, and Deniz G ̈und ̈uz. Neural distributed image compression using common information. In 2022 Data Compression Conference (DCC), pp. 182–191. IEEE, 2022b.\n\nJose-Pablo Sanchez-Rodriguez and Alejandro Aceves-Lopez. A survey on stereo vision-based au-\n\ntonomous navigation for multi-rotor muavs. Robotica, 36(8):1225–1243, 2018.\n\nDaniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo\n\ncorrespondence algorithms. International journal of computer vision, 47(1):7–42, 2002.\n\nSD Servetto. Multiterminal source coding with two encoders–i: a computable outer bound, 2006.\n\nIEEE Transactions on Information Theory, 2006.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531–3539, 2021.\n\nAthanassios Skodras, Charilaos Christopoulos, and Touradj Ebrahimi. The jpeg 2000 still image\n\ncompression standard. IEEE Signal processing magazine, 18(5):36–58, 2001.\n\nDavid Slepian and Jack Wolf. Noiseless coding of correlated information sources. IEEE Transac-\n\ntions on information Theory, 19(4):471–480, 1973.\n\nGary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high IEEE Transactions on circuits and systems for video\n\nefficiency video coding (hevc) standard. technology, 22(12):1649–1668, 2012.\n\nGerhard Tech, Ying Chen, Karsten M ̈uller, Jens-Rainer Ohm, Anthony Vetro, and Ye-Kui Wang. Overview of the multiview and 3d extensions of high efficiency video coding. IEEE Transactions on Circuits and Systems for Video Technology, 26(1):35–49, 2015.\n\nVijayaraghavan Thirumalai, Ivana Tosic, and Pascal Frossard. Distributed coding of multiresolution omnidirectional images. In 2007 IEEE international conference on image processing, volume 2, pp. II–345. IEEE, 2007.\n\nVijayaraghavan Thirumalai, Ivana Tosic, and Pascal Frossard. Symmetric distributed coding of stereo omnidirectional images. Signal Processing: Image Communication, 23(5):379–390, 2008.\n\nIvana Tosic and Pascal Frossard. Distributed multi-view image coding with learned dictionaries.\n\nTechnical report, 2009.\n\nSui-Yin Tung. Multiterminal source coding. Ph. D. dissertation, School of Electrical Engineering,\n\nCornell University, 1978.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAnthony Vetro, Thomas Wiegand, and Gary J Sullivan. Overview of the stereo and multiview video coding extensions of the h. 264/mpeg-4 avc standard. Proceedings of the IEEE, 99(4):626–642, 2011.\n\nGregory K Wallace. The jpeg still picture compression standard. IEEE transactions on consumer\n\nelectronics, 38(1):xviii–xxxiv, 1992.\n\nJin Wang, Yunhui Shi, Yinsen Xing, Nam Ling, and Baocai Yin. Deep correlated image set compression based on distributed source coding and multi-scale fusion. In 2022 Data Compression Conference (DCC), pp. 192–201. IEEE, 2022.\n\nShuang Wang, Lijuan Cui, Samuel Cheng, Lina Stankovic, and Vladimir Stankovic. Onboard lowcomplexity compression of solar stereo images. IEEE transactions on image processing, 21(6): 3114–3118, 2012.\n\nZhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, volume 2, pp. 1398–1402. Ieee, 2003.\n\nJay Whang, Anish Acharya, Hyeji Kim, and Alexandros G Dimakis. Neural distributed source\n\ncoding. arXiv preprint arXiv:2106.02797, 2021.\n\nMatthias W ̈odlinger, Jan Kotera, Jan Xu, and Robert Sablatnig. Sasic: Stereo image compression with latent shifts and stereo attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 661–670, 2022.\n\nJack Wolf. Data reduction for multiple correlated sources. In Colloquium on Microwave Communi-\n\ncation, pp. 287–295, 1973.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nZuobin Xiong, Zhipeng Cai, Qilong Han, Arwa Alrawais, and Wei Li. Adgan: Protect your location privacy in camera data of auto-driving vehicles. IEEE Transactions on Industrial Informatics, 17 (9):6200–6210, 2020.\n\nHuan Yin, Yue Wang, Li Tang, Xiaqing Ding, Shoudong Huang, and Rong Xiong. 3d lidar map IEEE Transactions on\n\ncompression for efficient localization on resource constrained vehicles. Intelligent Transportation Systems, 22(2):837–852, 2020.\n\nXiaoqing Zhu, Anne Aaron, and Bernd Girod. Distributed compression for large camera arrays. In\n\nIEEE Workshop on Statistical Signal Processing, 2003, pp. 30–33. IEEE, 2003.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n6 APPENDIX\n\n6.1 RD CURVES ON MULTI-CAMERA WILDTRACK DATASET\n\n(a)\n\n(b)\n\nFigure 7: Comparison of compression efficiency on WildTrack dataset with seven views.\n\n6.2 CODING COMPLEXITY\n\nFigure 8 reports the coding latency of different codecs on an Intel Xeon Gold 6230R processor with a single CPU core. For the proposed methods, we also evaluate the inference latency on a workstation with an NVIDIA RTX 3090 GPU. On the CPU platform, the proposed methods achieve tremendous encoding speedup improvement against VVC, which benefits from parallel processing all images in the DSC architecture. Because of the auto-regressive model and computation resources constraint, the decoder has a large latency. The proposed framework targets for applications related to distributed camera systems, such as video surveillance and multi-view image acquisition. These applications require a low-power encoder, while the receiver has powerful computation resources to support decoding procedure. As depicted in Figure 8(b), the proposed-fast variant on the GPU platform consumes less decoding time and outperforms HEVC with 14.14% bitrate saving measured by PSNR. When compared with VVC, the fast variant with only 0.86% increase in bits reduces about 50% decoding time. The results demonstrate that the decoding latency of proposed methods with GPU support can meet the basic needs.\n\n(a)\n\n(b)\n\nFigure 8: Encoding and decoding time of proposed methods and traditional codecs on InStereo2K dataset.\n\n6.3 VISUALIZATIONS\n\nIn Figure 9, we present several examples to vividly compare the quantitative results among Cheng2020, VVC, NDIC and the proposed method. It is observed that our proposed method effectively restores the image details and maintains higher reconstruction quality while consuming the\n\n14\n\nPublished as a conference paper at ICLR 2023\n\n(a) Ground truth\n\n(b) Cheng2020\n\n(c) VVC\n\n(d) NDIC\n\n(e) Proposed method\n\nFigure 9: A subjective comparisons on the InStereo2K, Cityscapes and WildTrack datasets, where the best results are outlined in red color.\n\nlower bits on the InStereo2K and WildTrack datasets. Similar as the results in Figure 4(b), VVC achieves the best coding gain on the Cityscapes dataset.\n\n6.4 FOUNDATIONS OF SYMMETRIC DISTRIBUTED SOURCE CODING\n\nThe formal statements of Slepian-Wolf theorem (Slepian & Wolf, 1973; Wolf, 1973) and Berger–Tung proposition (Berger, 1978; Tung, 1978; Servetto, 2006) are as follows.\n\nTheorem 1 (Slepian-Wolf) Let X1 and X2 be two statistically dependent i.i.d. discrete sources. The achievable rate region of independently encoding X1 and X2 with joint decoding under lossless compression is specified by:\n\nR1 ≥ H(X1|X2), R2 ≥ H(X2|X1), R1 + R2 ≥ H(X1, X2),\n\nwhere R1 and R2 are the rates for representing X1 and X2, respectively.\n\nProposition 1 (Berger–Tung Bound) Let U1 and U2 be auxiliary variables such that there exist decoding functions ˆX1 = f1(U1, U2) and ˆX2 = f2(U1, U2). Given the distortion constraints E[d(Xj, ˆXj)] ≤ Dj, j = 1, 2, the rates (R1, R2) follows the rate region R1 ≥ I(X1, X2; U1|U2), R2 ≥ I(X1, X2; U2|U1), R1 + R2 ≥ I(X1, X2; U1, U2), for some joint distribution p(x1, x2, u1, u2).\n\n· Inner Bound: when p(x1, x2, u1, u2) satisfies a Markov chain U1 − X1 − X2 − U2, all rates (R1, R2) are achievable.\n\n· Outer Bound: when p(x1, x2, u1, u2) satisfies two Markov chain U1 − X1 − X2 and X1 − X2 − U2, those rate points outside the union composed of the set of rates defined for each such p(x1, x2, u1, u2) are not available.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nThe Slepian-Wolf theorem and Berger-Tung bound proposition investigate the lossless and lossy compression of two correlated sources with separate encoders and a joint decoder, respectively. Although until now the compression limit of symmetric coding in the lossy case is still open, these theoretical results indicate that it is possible to compress two statistically dependent signals in a distributed way while approaching the compression performance of joint encoding and decoding.\n\n6.5 EXPERIMENTAL DETAILS\n\nDataset. We take two public stereo image datasets, InStereo2K (Bao et al., 2020) and Cityscapes (Cordts et al., 2016), and a multi-camera dataset, WildTrack (Chavdarova et al., 2018), for evaluation. The InStereo2K dataset involves 2060 image pairs for close views and indoor scenes, where 2010 and 50 pairs are selected as the training and testing data, respectively. The Cityscapes dataset is comprised of 5000 image pairs for far views and outdoor scenes, which is categorized into 2975 training, 500 validation and 1525 testing pairs. For the WildTrack dataset, we use FFMPEG to extract the images from seven HD 1080 videos at one frame per second. We choose the first 2000 images and the remaining 51 images in each view for training and testing. During evaluation, we minimally crop each image on the InStereo2K dataset so that both height and width are multiples of 64. As for the Cityscapes dataset, we follow the same cropping operations in W ̈odlinger et al. (2022) to remove rectification artefacts and ego-vehicle, where 64, 256 and 128 pixels from the top, bottom, and sides in each image are cut off.\n\nTraditional baseline codecs. We use the evaluation script from CompressAI 2 to obtain the results of conventional codecs. Specifically, instead of using the default x265 encoder in BPG, we adopt the slower but efficient JCTVC encoder option to achieve the higher compression performance. For HEVC and MV-HEVC, the results on the stereo image datasets come from W ̈odlinger et al. (2022). We use HM-16.25 3 and HTM-16.3 4 softwares to evaluate the coding efficiency of HEVC and MVHEVC on the WildTrack dataset, respectively. In addition, we run VTM-17.0 5 to test VVC-intra and VVC.\n\nLearning-based benchmarks. In DNN-based stereo image codecs, we retest HESIC and HESIC+ including the post-processing network by using their open source codes 6, because they previously reported the wrong results in their paper. The results of DSIC, BCSIC and SASIC are quoted from their corresponding papers. BCSIC did not report the rate-distortion points on the Cityscapes dataset. For distributed models, NDIC is composed of two different models, where one is a single image codec used in Ball ́e et al. (2018), another consists of separate encoder and joint decoder with side information proposed in Mital et al. (2022b).\n\nArchitecture details. Details about the network layers in our framework with auto-regressive entropy model are outlined in Figure 2 and 3. For the multi-head attention of the JCT module, we set the number of head as 2. The channel dimensions of the key and value are taken as one eighth and a quarter of input channels (i.e., 48 and 24), respectively. In order to achieve faster coding speed, the proposed fast variant replaces the serial auto-regressive entropy model with the parallelizationfriendly checkerboard entropy model in He et al. (2021), which has the same network architecture as Figure 2 except that the masked convolution layer uses a checkerboard mask.\n\nAblation study details. Based on the proposed method, we insert two JCT modules after the second and fourth convolution layers at the encoder to implement the Joint Enc-Dec, thereby allowing both the encoder and the decoder to access the inter-view context. For the Sep Enc-Dec, the JCT modules at the decoder are removed, making it equivalent to single image compression. These models are trained based on the InStereo2K dataset by using the same training scheme as LDMIC (See Implementation Details in Section 4.1). For the W/O Joint Training case, we fix the pre-trained encoder and entropy model on the Sep Enc-Dec, and only train the joint decoder on the InStereo2K dataset, which follows the same training procedure as in our proposed method.\n\n2https://github.com/InterDigitalInc/CompressAI/tree/master/compressai/utils 3https://vcgit.hhi.fraunhofer.de/jvet/HM/-/tags 4https://vcgit.hhi.fraunhofer.de/jvet/HTM/-/tags 5https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware VTM/-/tags 6https://github.com/ywz978020607/HESIC\n\n16",
    "reference": "# Summary Of The Paper\n\nThis paper proposed a novel deep learning based framework to effectively encode the distributed multi-view images. Unlike previous multi-view image coding (MIC) architectures that utilized a shared encoder to explore the inter-view redundancy, the proposed method moved this part to decoder and successfully reduced the computational costs in MIC. The experiments demonstrate the robustness and efficiency of proposed method.\n\n# Strength And Weaknesses\n\nStrength:\n+ The paper is well written and easy to follow\n+ The results are good compared with SOTA\n+ The experiments are comprehensive and well evaluate the performance on different datasets\n\nWeakness:\n- It will be great if there's a short discuss of failure cases.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI believe that this paper is of high quality\n\n# Summary Of The Review\n\nIn summary, this paper proposed a novel coding algorithm that can decouple the encoding of images acquired by distributed cameras. The method seems to effective and the results are good. The paper is well written and the comparison experiments are comprehensive.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEVOLVING POPULATIONS OF DIVERSE RL AGENTS WITH MAP-ELITES\n\nThomas Pierrot InstaDeep t.pierrot@instadeep.com\n\nArthur Flajolet InstaDeep a.flajolet@instadeep.con\n\nABSTRACT\n\nQuality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for highdimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such as hyperparameter sensitivity, high stochasticity as well as training instability, including when the population size increases as some components are shared across the population in recent approaches. Furthermore, existing approaches mixing ME with RL tend to be tied to a specific RL algorithm, which effectively prevents their use on problems where the corresponding RL algorithm fails. To address these shortcomings, we introduce a flexible framework that allows the use of any RL algorithm and alleviates the aforementioned limitations by evolving populations of agents (whose definition include hyperparameters and all learnable parameters) instead of just policies. We demonstrate the benefits brought about by our framework through extensive numerical experiments on a number of robotics control problems, some of which with deceptive rewards, taken from the QD-RL literature. We open source an efficient JAX-based implementation of our algorithm in the QDax library 1.\n\n1\n\nINTRODUCTION\n\nDrawing inspiration from natural evolution’s ability to produce living organisms that are both diverse and high-performing through competition in different niches, Quality Diversity (QD) methods evolve populations of diverse solutions to solve an optimization problem. In contrast to traditional Optimization Theory, where the goal is to find one solution maximizing a given scoring function, QD methods explicitly use a mapping from solutions to a vector space, referred to as a behavior descriptor space, to characterize solutions and maintain a data structure, referred to as a repertoire, filled with high-performing solutions that cover this space as much as possible, in a process commonly referred to as illumination. This new paradigm has led to breakthroughs over the past decade in many domains ranging from robotics control to engineering design and games generation (Gaier et al., 2018; Sarkar & Cooper, 2021; Gravina et al., 2019; Cully & Demiris, 2018). There are a number of advantages to QD methods over standard optimization ones. Actively seeking and maintaining diversity in a population of solutions has proved to be an effective exploration strategy, by reaching high-performing regions through a series of stepping stones, when the fitness function has no particular structure (Gaier et al., 2019). Additionally, having at disposal a diverse set of high-performing solutions can be greatly beneficial to a decision maker (Lehman et al., 2020), for instance because the scoring function may fail to model accurately the reality (Cully et al., 2015).\n\n1https://github.com/adaptive-intelligent-robotics/QDax\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nMAP-ELITES (Mouret & Clune, 2015) has emerged as one of the most widely used algorithm in the QD community for its simplicity and efficacy. It divides the behavior descriptor space into a discrete mesh of cells and strives to populate them all with solutions with matching behavior descriptors that maximize the fitness function as much as possible. This algorithm has been used in many applications with great success, such as developing controllers for hexapod robots that can adapt to damage in real time (Cully et al., 2015). However, just like many evolutionary algorithms, it struggles on problems with high-dimensional search spaces, such as when evolving controllers parametrized by neural networks, as it uses random mutations and crossovers to evolve the population.\n\nThe breakthroughs of Deep Reinforcement Learning in sequential decision making problems prompted a new line of work in the QD field to make the algorithms capable of dealing with deep neural network parametrizations. These new methods borrow techniques from either Black-Box Optimization (BBO) or Reinforcement Learning (RL) in order to exploit gradient information to guide the search. Methods based on BBO techniques (Colas et al., 2020; Conti et al., 2018) follow the approaches from earlier works on scaling evolutionary algorithms to neuro-evolution, such as Salimans et al. (2017); Stanley & Miikkulainen (2002), and empirically evaluate gradients w.r.t. the parameters by stochastically perturbing them by small values a number of times. Methods borrowing tools from RL, such as Nilsson & Cully (2021); Pierrot et al. (2022), exploit the Markov-Decision-Process structure of the problem and adapt off-policy RL algorithms, such as TD3 (Fujimoto et al., 2018), to evolve the population. This often entails adding additional components to the evolutionary algorithm (e.g. a replay buffer, critic networks, hyperparameters of the RL agent, ...) and methods differ along the way these components are managed. RL-based MAP-ELITES approaches have outperformed other MAP-ELITES variants, and even state-of-the art RL methods, on a variety of robotics control problems that require a substantial amount of exploration due to deceptive or sparse rewards. However, the introduction of RL components in MAP-ELITES has come with a number of downsides: (i) high sensibility to hyperparameters (Khadka et al., 2019; Zhang et al., 2021), (ii) training instability, (iii) high variability in performance, and perhaps most importantly (iv) limited parallelizability of the methods due to the fact that many components are shared in these methods for improved sample-efficiency. Furthermore, existing RL-based MAP-ELITES approaches are inflexibly tied to a specific RL algorithm, which effectively prevents their use on problems where the latter fails.\n\nThese newly-introduced downsides are particularly problematic as they are some of the main advantages offered by evolutionary methods that are responsible for their widespread use. These methods are notoriously trivial to parallelize and there is almost a linear scaling between the convergence speed and the amount of computational power available, as shown in Lim et al. (2022) for MAPELITES. This is all the more relevant with the advent of modern libraries, such as JAX (Bradbury et al., 2018), that seamlessly enable not only to distribute the computations, including computations taking place in the physics engine with BRAX (Freeman et al., 2021), over multiple accelerators but also to fully leverage their parallelization capabilities through automated vectorization primitives, see Lim et al. (2022); Flajolet et al. (2022); Tang et al. (2022). Evolutionary methods are also notoriously robust to the exact choice of hyperparameters, see Khadka et al. (2019), which makes them suited to tackle new problems. This is in stark contrast with RL algorithms that tend to require problem-specific hyperparameter tuning to perform well (Khadka et al., 2019; Zhang et al., 2021).\n\nIn order to overcome the aforementioned limitations of RL-based MAP-ELITES approaches, we develop a new MAP-ELITES framework that 1. can be generically and seamlessly compounded with any RL agent, 2. is robust to the exact choice of hyperparameters by embedding a meta-learning loop within MAP-ELITES, 3. is trivial to scale to large population sizes, which helps alleviating stochasticity and training stability issues, without entering offline RL regimes a priori by independently evolving populations of entire agents (including all of their components, such as replay buffers) instead of evolving policies only and sharing the other components across the population. Our method, dubbed PBT-MAP-ELITES, builds on MAP-ELITES and combines standard isoline operators with policy gradient updates to get the best of both worlds. We evaluate PBT-MAP-ELITES when used with the SAC (Haarnoja et al., 2018) and TD3 (Fujimoto et al., 2018) agents on a set of five standard robotics control problems taken from the QD literature and show that it either yields performance on par with or outperforms state-of-the-art MAP-ELITES approaches, in some cases by a strong margin, while not being provided with hyperparameters tuned beforehand for these problems. Finally, we open source an efficient JAX-based implementation of our algorithm that combines the efficient implementation of PBT from Flajolet et al. (2022) with that of MAP-ELITES from Lim et al. (2022). We refer to these two prior works for speed-up data points compared to alternative implementations.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 BACKGROUND\n\nProblem Definition. We consider the problem of generating a repertoire of neural policies that are all high-performing for a given task while maximizing the diversity of policies stored in the repertoire. More formally, we consider a finite-horizon Markov Decision Process (MDP) (S, A, R, T ), where A is the action space, S is the state space, R : S×A → R is the reward signal, T : S×A → S is the transition function, and T is the episode length. A neural policy corresponds to a neural network πθ : S → D(A) where θ ∈ Θ denotes the weights of the neural network and D(A) is the space of distributions over the action space. At each time step, we feed the current environment state to the neural network and we sample an action from the returned distribution, which we subsequently take. Once the action is carried out in the environment, we receive a reward and the environment transitions to a new state. The fitness F (πθ) of a policy πθ is defined as the expected value of the sum of rewards thus collected during an episode. We denote the space of trajectories thus followed in the environment by τ ∈ Ω. In the QD literature, diversity is not directly measured in the parameter space Θ, but rather in another space D, referred to as the behavior descriptor space or sometimes simply descriptor space, which is defined indirectly through a pre-specified and problem-dependent mapping Φ : Ω → D. A policy πθ is thus characterized by rolling it out in the environment and feeding the trajectory to Φ. With a slight abuse of notation, we denote by Φ(πθ) the behavior descriptor of the policy πθ. Diversity of a repertoire of policies is measured differently across QD approaches.\n\nMAP-Elites. MAP-ELITES uses a tesselation technique to divide the descriptor space into a finite number of cells, which collectively define a discrete repertoire. In this work, we use the Centroidal Voronoi Tessellation (CVT) technique (Vassiliades et al., 2017) for all considered methods as it has been shown to be general and easy to use in practice (Vassiliades et al., 2017; Pierrot et al., 2022). MAP-ELITES starts by randomly initializing a set of M policies. Each of these policies is then independently evaluated in the environment and they are sequentially inserted into the repertoire according to the following rule. If the cell corresponding to the descriptor of the policy at hand is empty, the policy is copied into this cell. In the opposite situation, the policy replaces the current incumbent only if it has a greater fitness and is dropped otherwise. During each subsequent iteration, policies are randomly sampled from the repertoire, copied, and perturbed to obtain a new set of M policies which are then tentatively inserted into the repertoire following the aforementioned rule. Implementations of MAP-ELITES often differ along the exact recipe used to perturb the policies. The original MAP-ELITES algorithm (Mouret & Clune, 2015) relies on random perturbations. In this work, we use the isoline variation operator (Vassiliades & Mouret, 2018) that, given two parent policies, say policies θ1 and θ2, adds Gaussian noise N (0, σ1) to θ1 and offsets the results along the line θ2 −θ1 by a magnitude randomly sampled from a zero-mean Gaussian distribution with variance N (0, σ2). This strategy has proved to be particularly effective to evolve neural networks (Rakicevic et al., 2021). Pseudocode for MAP-ELITES is provided in the Appendix.\n\nBBO-based QD. To improve sample efficiency and asymptotic performance, methods such as MEES (Colas et al., 2020) use first-order updates to perturb the policies with the objective of both increasing the fitness of the policies in the repertoire and improving the coverage of the repertoire (i.e. the number of non-empty cells). To generate the updates, ME-ES use the Evolution Strategy from Salimans et al. (2017). Specifically, after selecting a policy from the repertoire, its neural network parameters are perturbed stochastically with a small amount of Gaussian noise a number of times and the resulting policies are rolled out in the environment for a full episode. All of the collected samples are then used to empirically estimate gradients for a smoothed version around the starting policy of either (1) the fitness function, (2) a novelty function which is defined as the average Euclidean distance between the starting policy’s behavior descriptor and its k nearest neighbors among all previously computed behavior descriptors, or (3) alternatively the fitness function and the novelty function to increase both quality and diversity, which is the version we use in this work (see the Appendix for the pseudocode). Note that similar strategies using the NS-ES family of algorithms exist, such as Conti et al. (2018), but these methods are outperformed by ME-ES (Colas et al., 2020).\n\nRL-based QD. Using evolution strategies to guide the search with first-order updates improves upon random search but remains doomed to a low sample-efficiency due to the need of rolling out a significant number of policies over entire trajectories to get reasonably accurate gradient estimates. More recent techniques, such as QD-PG (Pierrot et al., 2022) and PGA-MAP-ELITES (Nilsson & Cully, 2021), exploit the MDP structure of the problem and leverage policy-gradient techniques from RL as well as off-policy extensions for improved sample efficiency and better asymptotic convergence.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nBoth QD-PG and PGA-MAP-ELITES build on the TD3 agent (Fujimoto et al., 2018). PGA-MAPELITES combines random mutations derived through the isoline variation operator with mutations obtained through policy gradient computations. QD-PG introduces the notion of a diversity reward, a signal defined at the timestep-level to drive policies towards unexplored regions of the behavior descriptor space, which makes it possible to leverage the RL machinery to compute policy gradients to increase the diversity of the population, referred to as diversity policy gradients, in addition to the standard policy gradients to increase the fitness of the policies, referred to as quality policy gradients. At each MAP-ELITES iteration, half of the selected policies are updated using quality policy gradients and the other half are updated using diversity policy gradients. In contrast to PGA-MAPELITES, QD-PG does not relies on random search updates. Both QD-PG and PGA-MAP-ELITES use a single shared replay buffer where all the transitions collected when evaluating the agents are stored and from which batches are sampled to compute policy gradients.\n\nCritic networks are managed differently by each algorithm. QD-PG uses two different sets of critic parameters, one for quality rewards and one for diversity rewards, that are shared across the population and both are updated any time a policy gradient is computed. PGA-MAP-ELITES maintains a greedy policy and its associated critic which are updated independently of the rest of the repertoire. The greedy policy is regularly inserted in the repertoire and the critic is used to compute policy gradients updates for all other policies but is only updated using the greedy policy.\n\nThese precise design choices not only make PGA-MAP-ELITES and QD-PG difficult to distribute efficiently but they also harm the flexibility of these methods. For instance, if one would like to replace TD3 by another popular off-policy algorithm such as SAC, which is known to perform better for some environments, numerous new design choices arise. For instance for SAC, one would have to decide how to handle the temperature parameter and the entropy target within the population. Furthermore, while sharing critic parameters and using a single replay buffer was motivated by a desire for greater sample efficiency, this introduces new issues when scaling these methods. For instance, as the number of policies updated concurrently at each iteration increases we get closer to an offline RL setting, which is known to harm performance, since all policies share the same replay buffer. Conversely, as the size of the repertoire increases, any single policy stored in the repertoire is updated all the less frequently than the critic which may cause them to significantly lag behind over time. Finally, both QD-PG and PGA-MAP-ELITES assume that good hyperparameters are provided for TD3 while it is known that tuning these values for the problem at hand is necessary to get good performance. This effectively puts the burden on the user to tune hyperparameters for TD3 as a preliminary step, which limits the usability of such methods in new settings. Pseudocodes for QD-PG and PGA-MAP-ELITES are provided in the Appendix.\n\n3 METHOD\n\nIn order to overcome the limitations of RL-based QD methods identified in the last section, we revisit the neuro-evolution problem defined in Section 2 and introduce a new algorithm, dubbed PBT-MAPELITES, that evolves populations of agents as opposed to populations of policies. An agent is defined by a tuple (θ, φ, h) where θ denotes the policy parameters, φ denotes all other learnable parameters of the agent (e.g. critic parameters and target critic parameters), and h denotes its hyperparameters (e.g. learning rates and magnitude of the exploration noise). As in the original formulation, we assume that the fitness and behavior descriptor functions depend only on the policy, i.e. on θ. The learnable parameters and the hyperparameters are only used when agents are updated. PBT-MAPELITES internally uses a policy-search-based RL algorithm which can be selected freely by the user. In particular, it may be on-policy or off-policy.\n\nPBT-MAP-ELITES maintains a MAP-ELITES repertoire as well as a population of P agents. The population is randomly initialized (including the hyperparameters), evaluated, copied and inserted into the repertoire. We also initialize P replay buffers if the underlying RL algorithm makes use of them. Additionally, a batch of agents is sampled from the repertoire and a variation operator is applied to obtain M offspring that are also evaluated and inserted into the repertoire as part of the initialization phase. Then, the algorithm proceeds in iterations, each of which consists of two consecutive steps: 1. population update and 2. MAP-ELITES repertoire update.\n\nPopulation Update. To update the population of agents, we use the following strategy inspired from Jaderberg et al. (2017). We first rank all agents in the population by fitness based on the evaluation\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Left Panel. The set of QD-RL environments used to evaluate all methods. All environments are implemented using the BRAX simulator and are available in the QDAX suite. In WALKER2D-UNI, ANT-UNI, and HALFCHEETAH-UNI, the goal is to learn to control the corresponding robot to run as fast as possible with gaits that are as diverse as possible. In ANT-TRAP and HUMANOID-TRAP, the robot should travel as far as possible along the x-axis direction, which entails sidestepping the trap to get the highest possible return thus putting the exploration capabilities of considered methods to test. Right panel. Illustration of the PBT-MAP-ELITES algorithm. PBTMAP-ELITES maintains a population of agents (policy parameters, other learnable parameters as well as hyperparameters) and a MAP-ELITES repertoire of agents. The repertoire update, generating more diversity, and the population update, improving the fitness of the policies, are intertwined.\n\nthat took place at the end of the last iteration. Agents that are in the bottom p% of the population are replaced by agents sampled uniformly from the top n% of the population, with 0 < p < 1 − n < 1. We also randomly select k% of the agents in the population among the ones that are neither in the top n% nor in the bottom p% and we replace them by agents randomly sampled from the current MAP-ELITES repertoire. All other agents remain unchanged. This mechanism allows potentially lower-performing, but more diverse, individuals from the repertoire to enter the population while maintaining high-performing agents alive. When agents are replaced, new hyperparameter values are sampled uniformly from pre-specified ranges. The agents’ policy parameters as well as all other learnable parameters are subsequently trained for S steps, using the user-selected RL algorithm. If needed, the collected experience is stored inside the replay buffers. In contrast to PGA-MAPELITES and QD-PG, we closely follow the general recipe followed by most RL algorithms and only add the experience collected during training, in exploration mode, to the replay buffers while the experience collected during evaluation, in exploitation mode, is discarded. Additionnaly, note that the agents are trained independently from one another, which makes it trivial to parallelize the most computationally intensive part of this step. This is in stark contrast with other MAP-ELITES-RL methods that share some parameters across the population, e.g. the critic parameters for QD-PG and PGA-MAP-ELITES, which are typically updated concurrently by all agents.\n\nRepertoire Update. Once the agents in the population have been trained, they are evaluated and inserted into the repertoire. Then, just like during the initialization phase, a batch of agents is randomly sampled from the repertoire and undergoes a variation operator to obtain M offspring which are evaluated and inserted into the grid. As in PGA-MAP-ELITES, the variation operator is meant to increase the descriptor space coverage but we have also observed that this process stabilizes the algorithm as a whole. In order to define a variation operator that can be used with agents, as opposed to policies, we deal with variations over the policy and learnable parameters separately from variations over the hyperparameters. Specifically, an isoline operator is applied to policy and other learnable parameters while the offspring simply inherit the hyperparameters of one of their parents. While more sophisticated strategies could be investigated, we have observed that this simple mechanism works well in practice in our experiments.\n\nObserve that optimization of the quality as well as the diversity of the policies happens at two different levels in PBT-MAP-ELITES. Quality is encouraged through both the elitist population update and the repertoire insertion mechanism. Diversity is induced through both the addition of agents from the repertoire to the population and the use of random variation operators at each iteration. The pseudocode of the algorithm is provided in the Appendix.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4 LITERATURE REVIEW\n\nQuality Diversity. QD methods aim to simultaneously maximize diversity and performance. Among existing options, MAP-ELITES and Novelty Search with Local Competition (NSLC) are two of the most popular QD algorithms. NSLC builds on the Novelty Search algorithm (Lehman & Stanley, 2011) and maintains an unstructured archive of solutions selected for their high performance relative to other solutions in their neighborhoods while MAP-ELITES relies on a tesselation technique to discretize the descriptor space into cells. Both algorithms rely extensively on Genetic Algorithms (GA) to evolve solutions. As a result, they struggle when the dimension of the search space increases, which limits their applicability. These approaches have since been extended using tools from Evolution Strategies (ES) to improve sample efficiency and asymptotic performance over the original implementations based on GA (Salimans et al., 2017). CMA-MAP-ELITES (Fontaine et al., 2020) relies on Covariance Matrix Adaptation (CMA) to speed up the illumination of the descriptor space. NSRA-ES and NSR-ES (Conti et al., 2018) build on recent ES tools to improve QD methods’ exploration capabilities on deep RL problems with deceptive or sparse rewards. ME-ES (Colas et al., 2020) introduces alternate ES updates for quality and diversity in order to solve deep RL problems with continuous action spaces that require a good amount of exploration. While ES-based approaches improve over GA-based ones, they are still relatively sample-inefficient due to the fact that they need to roll out a large of number of policies over entire trajectories to empirically estimate gradients with reasonable accuracy. Several recent methods propose to exploit analytical gradients when this is possible instead of estimating them empirically. DQD (Fontaine & Nikolaidis, 2021) builds a mutation operator that first computes gradients of the fitness and behavior descriptor functions at the current solution and carry out a first-order step by summing the gradients with random coefficients. Tjanaka et al. (2022) applies the same technique to deep RL problems with continuous action spaces. PGA-MAP-ELITES (Nilsson & Cully, 2021) and QD-PG (Pierrot et al., 2022) exploit the MDP structure of the problems to compute policy gradients using the TD3 algorithm, outperforming all QD competitors for deep RL problems with continuous actions. However, both methods are tied a single RL algorithm and are highly sensitive to the choice of TD3 hyperparameters.\n\nPopulation Based Reinforcement Learning. Our work has different motivations than classical RL algorithms as we do not aim to find a policy than achieves the best possible return but rather to illuminate a target descriptor space. However, we share common techniques with Population-Based RL (PBRL) algorithms. In this field, the closest method to ours is the Population-Based-Training (PBT) algorithm (Jaderberg et al., 2017) which uses a genetic algorithm to learn the hyperparameters of a population of RL agents concurrently to training them. While PBT-MAP-ELITES and PBT use similar strategies to update the population of agents, PBT only seeks the highest-performing agent by extracting the best one from the final population while PBT-MAP-ELITES aims to find a diverse collection of high-performing agents. Several methods such as CERL, ERL, and CEM-RL (Pourchot & Sigaud, 2019; Khadka & Tumer, 2018; Khadka et al., 2019) combine ES algorithms with PBRL methods to improve the asymptotic performance and sample efficiency of standard RL methods. Other methods, such as DvD (Parker-Holder et al., 2020) and P3S-TD3 (Jung et al., 2020), train populations of agents and add terms in their loss functions to encourage the agents to explore different regions of the state-action space but always with the end goal of maximizing the performance of the best agent in the population. Flajolet et al. (2022) show how to vectorize computations across the population to run PBRL algorithms as efficiently as possible on accelerators through the use of the JAX library. Lim et al. (2022) introduced similar techniques to accelerate MAP-ELITES through the evaluation of thousands of solutions in parallel with JAX. In this study, we build on both of these works and implement PBT-MAP-ELITES in the JAX framework to make it fast and scalable.\n\n5 EXPERIMENTS\n\nEnvironments. We use five robotics environments that fall into two categories:\n\n1. HALFCHEETAH-UNI, WALKER2D-UNI and ANT-UNI are environments widely used in the QD community to evaluate an algorithm’s ability to illuminate a complex descriptor space, see for instance Cully et al. (2015); Nilsson & Cully (2021); Tjanaka et al. (2022). In these environments, the goal is to make a legged robot run as fast as possible along the forward direction while optimizing for diversity w.r.t. the robot’s gaits, indirectly characterized as the mean frequencies of contacts between the robots’ legs and the ground. This last quantity defines the behavior descriptor for these environ-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Performance comparison of PBT-MAP-ELITES with baselines on the basis of standard metrics from the QD literature for five environments from the QDAX suite (which is based on the BRAX engine). We benchmark two variants of PBT-MAP-ELITES, one where it is composed with SAC and one where it is composed with TD3. All methods are trained with a total budget of N = 1.5e8 environment timesteps. Experiments are repeated over 5 runs with different random seeds and the medians (resp. first and third quartile intervals) are depicted with full lines (resp. shaded areas).\n\nments while the reward at each timestep is the velocity of the robot’s center of gravity projected onto the forward direction.\n\n2. ANT-TRAP and HUMANOID-TRAP are environments with deceptive reward signals used in the QD-RL literature to evaluate an algorithm’s ability to solve complex continuous control problems that require a good amount of exploration, see Colas et al. (2020); Conti et al. (2018); Pierrot et al. (2022). In these environments, the goal is also to make the legged robot run as fast as possible in the forward direction, though with the additional difficulty that the robot is initially facing a trap. As a result, following the reward signal in a greedy fashion leads the robot into the trap. The robot must explore the environment and learn to go around the trap, even though this is temporarily suboptimal, in order to obtain higher returns. In these environments, the behavior descriptor is defined as the position of the robot’s center of gravity at the end of an episode. All of these environments are based on the BRAX simulator (Freeman et al., 2021) and are available in the QDAX suite (Lim et al., 2022).\n\nSetup. We compare PBT-MAP-ELITES to state-of-the-art MAP-ELITES-based methods, namely MAP-ELITES, ME-ES, PGA-MAP-ELITES as well as QD-PG. For these experiments, we benchmark two variants of PBT-MAP-ELITES: one where it is composed with SAC and one where it is composed with TD3. For the sake of fairness, we use the same values for parameters that are used by multiple methods. In particular, all MAP-ELITES-based methods maintain a repertoire of 1024 cells and use CVT with the same parametrization to discretize the behavior descriptor space into 1024 cells. Similarly, when a variation operator is needed, we always use the isoline operator with the same pa-\n\n7\n\n1000200030004000500060007000Ant-Uni01000200030004000500060007000HalfCheetah-Uni01000200030004000Walker2d-Uni1000120014001600AntTrap100020003000400050006000Max FitnessHumanoidTrap010203040506070800204060801000204060800204060801000246810121416Coverage0.00.51.0Environment steps1e80.00.51.01.52.02.53.03.54.01e60.00.51.0Environment steps1e80.00.20.40.60.81.01.21e70.00.51.0Environment steps1e80.00.51.01.52.02.51e60.00.51.0Environment steps1e8012341e60.00.51.0Environment steps1e80100000200000300000400000500000QD ScoreMAP-ElitesPGA-MAP-ElitesPBT-MAP-Elites (SAC)PBT-MAP-Elites (TD3)QDPGME-ESPBTPublished as a conference paper at ICLR 2023\n\nFigure 3: Visualizations of the performance and hyperparameters of the agents stored in the MAPELITES repertoire of PBT-MAP-ELITES (TD3) at different time points during training on the ANTTRAP environment. The first row corresponds to the fitnesses of the agents. The second row corresponds to the exploration noise added by the agents to the actions returned by their policies when collecting experience in exploration mode. The third row corresponds to the discount factor γ used by the agents. Snapshots of the repertoire are taken at uniformly-spaced intervals during training.\n\nrameters σ1 = 0.005 and σ2 = 0.05. All policy and critic networks are implemented by two MLPs layers with 256 hidden neurons per layer. For methods relying on the TD3 agent, the hyperparameters used are the ones introduced in the original paper for MUJOCO environments. Pseudocodes and parameter values for all algorithms under study are provided in the Appendix.\n\nAdditionally, we compare PBT-MAP-ELITES to the PBT algorithm (Jaderberg et al., 2017) (pseudocode provided in the Appendix) when it is used to optimize populations of SAC agents. Both PBT-MAP-ELITES and PBT evolve populations of 80 agents and use the same ranges for the hyperparameters. All policy and critic networks are implemented by two-layer MLPs with 256 hidden neurons per layer, just like for TD3 for PGA-MAP-ELITES and QD-PG. Furthermore, the parameters of all agents in the population are identically initialized. For PBT-MAP-ELITES (resp. PBT), agents in the bottom p = 0.2 (resp. p = 0.4) fraction of the population (in terms of fitness) are replaced by agents sampled from the top n = 0.1 fraction of the population. For PBT-MAP-ELITES, a fraction k = 0.4 of the agents that are neither in the bottom 20% nor in the top 10% of the population are replaced by agents randomly sampled from the MAP-ELITES repertoire. All other parameters and design choices are identical for these two methods.\n\nMetrics and fair comparisons. Following standard practice in the QD literature, we monitor three metrics used to evaluate the performance of a collection of policies during training. 1. We measure the maximum fitness, defined as the maximum expected return across policies in the collection. 2. We measure the coverage over the descriptor space, computed as the number of cells that have been filled. 3. We measure the QD-score, computed as the sum of fitnesses attained by the policies stored in the repertoire. For this last metric to be meaningful, we assume that fitnesses are all non-negative. If not, a positive value is added to all fitnesses to enforce it. In any case, this value is the same for all methods for fairness. Since some of the metrics require a repertoire to be properly defined, we introduce a passive repertoire for PBT to be able to evaluate it on the same basis as the other methods. Specifically, at the end of each PBT iteration, the population of agents generated by PBT is evaluated and inserted into a repertoire. For each method, we report the evolution of these metrics w.r.t. the total number of interactions with the environment. Note that while the evaluation of an agent contributes to the total number of interactions for MAP-ELITES-based methods, this is not the case for PBT as the evaluations are only used to estimate the metrics for this method.\n\n6 RESULTS AND DISCUSSION\n\nStatistics on QD metrics are reported for all environments and methods on Figure 2.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nPerformance comparison to other MAP-ELITES-based methods. We observe that PBT-MAPELITES (SAC) is the only method able to solve HUMANOID-TRAP within the allocated timestep budget, outperforming all the other methods by a significant margin. HUMANOID-TRAP is a challenging environment as obtaining high returns requires not only to get the humanoid robot to run, which is a challenging continuous problem in itself, but also to learn to sidestep the trap in spite of a deceptive reward signal. This environment, introduced in Colas et al. (2018), has remained out of reach for MAP-ELITES-based methods, setting aside ME-ES which solves it with a timestep budget two orders of magnitude higher. Interestingly, the maximum fitness remains below 2000 for TD3based methods, which means they were not able to get the humanoid robot to run at all. This is a testament to the difficulty of the problem. Recall that TD3 was not able to solve the MUJOCO-based version of the Humanoid environment in the original paper that introduced this algorithm (Fujimoto et al., 2018). A careful tuning of the algorithm design choices and hyperparameters, carried out in a later study, was required to get TD3 to perform well on this environment. Setting aside the WALKER2D-UNI environment, note that PBT-MAP-ELITES (SAC) either outperforms, often by a significant margin for the maximum fitness metric, or performs on par with MAP-ELITES-based methods. Interestingly, the SAC variant of PBT-MAP-ELITES often performs better than the TD3 variant, but not always. On a side note, we also observe that ME-ES surprisingly gets outperformed by all MAP-ELITES competitors, including the original MAP-ELITES algorithm, in all environments. This can be explained by the fact that ME-ES uses 1000 evaluations (i.e. 1e6 timesteps) to update a single policy. As a result, for a repertoire consisted of 1024 cells and with a budget of 1.5e8 timesteps, the maximum coverage that can be reached by ME-ES is 15% only. In the original study, ME-ES manages to outperform other MAP-ELITES-based methods with a budget of 1e10 timesteps.\n\nPerformance comparison to PBT. We observe that PBT outperforms the SAC variant of PBT-MAPELITES in terms of maximum fitness on HALFCHEETAH-UNI and ANT-UNI. This is expected as: (1) these environments do not require a significant amount of exploration, (2) PBT only aims to maximize the maximum fitness, and (3) PBT-MAP-ELITES aims to maximize both the maximum fitness and the policies’ diversity. However, we observe the opposite trend on ANT-TRAP and HUMANOIDTRAP where significant exploration is required to achieve high returns given the deceptive nature of the reward signal. We conclude that optimizing for diversity turns out to play a crucial role for these two environments. As expected, PBT-MAP-ELITES outperforms PBT in terms of coverage and QD-score in all environments, setting aside HUMANOID-TRAP. The seemingly unexpected results observed on HUMANOID-TRAP stem from the fact that covering the behavior descriptor directly correlates with exploration of the (x, y) space, which is required to achieve high returns in this environment due to the presence of the trap.\n\nRepertoire interpretation. By visualizing the evolution of the fitnesses and hyperparameters of the agents stored in PBT-MAP-ELITES’s repertoire at different time points during training, see Figure 3, we observe that PBT-MAP-ELITES evolves locally-coherent (w.r.t. the descriptor space) maps of hyperparameters that change significantly during training. In particular, we remark that PBT-MAPELITES dynamically increases the amount of exploration noise of the TD3 agents to boost exploration when needed to go around the trap and decreases this parameter once the trap has been sidestepped to focus on getting high returns. This mechanism gives a significant advantage to PBT-MAP-ELITES over QD-PG and PGA-MAP-ELITES, for which this parameter is set to a constant value.\n\n7 CONCLUSION\n\nIn this work, we revisit the standard formulation of the QD neuro-evolution problem by evolving repertoires of full agents (including hyperparameters among other things) as opposed to only policies. This extension brings flexibility compared to existing frameworks as it allows us to combine any RL algorithm with MAP-ELITES in a generic and scalalable fashion. This formulation also allows us to dynamically learn the hyperparameters of the underlying RL agent as part of the regular training process, which removes a significant burden from the user. Surprisingly, we observe that learning the hyperparameters improves both the asymptotic performance and the sample efficiency in practice for most of the environments considered in this work. Our method is the first to solve the HUMANOID-TRAP environment with less than one billion interactions with the simulator, to be compared with tens of billions of interactions for state-of-the-art QD methods. We hope that this work constitutes one more step towards bridging the gap between Neuro-Evolution and Reinforcement Learning, combining the best of both worlds in a simple framework.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/google/jax.\n\nC ́edric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: decoupling exploration and exploitation in deep reinforcement learning algorithms. In Proceedings of the International Conference on Machine Learning, volume 80, pp. 1038–1047, 2018.\n\nC ́edric Colas, Vashisht Madhavan, Joost Huizinga, and Jeff Clune. Scaling map-elites to deep neuroevolution. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 67–75, 2020.\n\nEdoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Advances in Neural Information Processing Systems, pp. 5027–5038, 2018.\n\nAntoine Cully and Yiannis Demiris. Hierarchical behavioral repertoires with unsupervised descriptors. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 69–76, 2018.\n\nAntoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like\n\nanimals. Nature, 521(7553):503–507, 2015.\n\nArthur Flajolet, Claire Bizon Monroc, Karim Beguir, and Thomas Pierrot. Fast population-based reinforcement learning on a single machine. In Proceedings of the International Conference on Machine Learning, pp. 6533–6547, 2022.\n\nMatthew Fontaine and Stefanos Nikolaidis. Differentiable quality diversity. In Advances in Neural\n\nInformation Processing Systems, volume 34, pp. 10040–10052, 2021.\n\nMatthew C Fontaine, Julian Togelius, Stefanos Nikolaidis, and Amy K Hoover. Covariance matrix adaptation for the rapid illumination of behavior space. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 94–102, 2020.\n\nC Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. arXiv preprint\n\nBrax–a differentiable physics engine for large scale rigid body simulation. arXiv:2106.13281, 2021.\n\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In Proceedings of the International Conference on Machine Learning, pp. 1587– 1596, 2018.\n\nAdam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret. Data-efficient design exploration\n\nthrough surrogate-assisted illumination. Evolutionary computation, 26(3):381–410, 2018.\n\nAdam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret. Are quality diversity algorithms better at generating stepping stones than objective-based search? In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pp. 115–116, 2019.\n\nDaniele Gravina, Ahmed Khalifa, Antonios Liapis, Julian Togelius, and Georgios N Yannakakis. In 2019 IEEE Conference on Games\n\nProcedural content generation through quality diversity. (CoG), pp. 1–8, 2019.\n\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.\n\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population-based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nWhiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search for reinforcement learning. In Proceedings of the International Conference on Learning Representations, 2020.\n\nShauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning.\n\nIn Neural Information Processing Systems, volume 31, 2018.\n\nShauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago Miret, Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In Proceedings of the International Conference on Machine Learning, pp. 3341–3350, 2019.\n\nJoel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for\n\nnovelty alone. Evolutionary computation, 19(2):189–223, 2011.\n\nJoel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J Bentley, Samuel Bernard, Guillaume Beslon, David M Bryson, et al. The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. Artificial life, 26(2):274–306, 2020.\n\nBryan Lim, Maxime Allard, Luca Grillotti, and Antoine Cully. Accelerated quality-diversity for\n\nrobotics through massive parallelism. arXiv preprint arXiv:2202.01258, 2022.\n\nJean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint\n\narXiv:1504.04909, 2015.\n\nOlle Nilsson and Antoine Cully. Policy gradient assisted map-elites. In Proceedings of the Genetic\n\nand Evolutionary Computation Conference, pp. 866–875, 2021.\n\nJack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Effective diversity in population-based reinforcement learning. In Neural Information Processing Systems, volume 33, pp. 18050–18062, 2020.\n\nThomas Pierrot, Valentin Mac ́e, F ́elix Chalumeau, Arthur Flajolet, Geoffrey Cideron, Karim Beguir, Antoine Cully, Olivier Sigaud, and Nicolas Perrin-Gilbert. Diversity policy gradient for sample efficient quality-diversity optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1075–1083, 2022.\n\nAlo ̈ıs Pourchot and Olivier Sigaud. CEM-RL: combining evolutionary and gradient-based methods for policy search. In Proceedings of the International Conference on Learning Representations, 2019.\n\nNemanja Rakicevic, Antoine Cully, and Petar Kormushev. Policy manifold search: Exploring the manifold hypothesis for diversity-based neuroevolution. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 901–909, 2021.\n\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a\n\nscalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\n\nAnurag Sarkar and Seth Cooper. Generating and blending game levels via quality-diversity in the latent space of a variational autoencoder. In Proceedings of the International Conference on the Foundations of Digital Games, pp. 1–11, 2021.\n\nKenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topolo-\n\ngies. Evolutionary computation, 10(2):99–127, 2002.\n\nYujin Tang, Yingtao Tian, and David Ha. Evojax: Hardware-accelerated neuroevolution. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pp. 308–311, 2022.\n\nBryon Tjanaka, Matthew C Fontaine, Julian Togelius, and Stefanos Nikolaidis. Approximating In Proceedings of the\n\ngradients for differentiable quality diversity in reinforcement learning. Genetic and Evolutionary Computation Conference, pp. 1102–1111, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nVassiiis Vassiliades and Jean-Baptiste Mouret. Discovering the elite hypervolume by leveraging interspecies correlation. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 149–156, 2018.\n\nVassilis Vassiliades, Konstantinos Chatzilygeroudis, and Jean-Baptiste Mouret. Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm. IEEE Transactions on Evolutionary Computation, 22(4):623–630, 2017.\n\nBaohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr ́e Biedenkapp, Kurtland Chua, Frank Hutter, and Roberto Calandra. On the importance of hyperparameter optimization for model-based reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 4015–4023, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA PSEUDOCODES FOR ALL ALGORITHMS\n\nAlgorithm 1: PBT-MAP-ELITES Given:\n\n• M: MAP-ELITES repertoire • N ∈ N∗: maximum number of environment steps • M ∈ N∗: number of isoline-variation offsprings per iteration • P ∈ N∗: size of the population of RL agents • S ∈ N∗: number of training steps per iteration per agent\n\n• p, k, n ∈ ]0, 1[: PBT proportions\n\n• an RL agent template\n\n• F (·): fitness function\n\n• Φ(·): behavior descriptor function\n\n// Initialization Randomly initialize P + M agents following the chosen RL template ((πθi , φi, hi))1≤i≤P +M . Run one episode in the environment using each of (πθi )1≤i≤P +M to evaluate (F (πθi ))1≤i≤P +M and\n\n(Φ(πθi ))1≤i≤P +M .\n\nInsert ((πθi , φi, hi))1≤i≤P +M in M based on (F (πθi ))1≤i≤P +M and (Φ(πθi ))1≤i≤P +M . Initialize P replay buffers (Bi)1≤i≤P using the data collected respectively by each agent during the initial\n\nevaluations (if replay buffers are used by the RL agent).\n\n// Main loop Initialize nsteps, the total number of environment interactions carried out so far, to 0. while nsteps ≤ N do\n\n// Population Update Re-order the agents i = 1, · · · , P in increasing order of their fitnesses (F (πθi ))1≤i≤P . Update agents i = 1, · · · , pP by copying randomly-sampled agents from i = (1 − n)P, · · · , P and\n\ncopy the replay buffers accordingly (if replay buffers are used by the RL agent).\n\nSample new hyperparameters for agents i = 1, · · · , pP . Sample kP indices (ij)1≤j≤kP uniformly without replacement from {pP + 1, · · · , (1 − n)P − 1}. Replace agents i = ij, 1 ≤ j ≤ kP by agents randomly(-uniformly) sampled from M. Train agents i = 1, · · · , P independently for S steps using the RL agent template, sampling data\n\nfrom the replay buffers if they are used by the RL agent.\n\n// Repertoire Update Run one episode in the environment using each of (πθi )1≤i≤P to evaluate (F (πθi ))1≤i≤P and\n\n(Φ(πθi ))1≤i≤P .\n\nInsert ((πθi , φi, hi))1≤i≤P in M based on (F (πθi ))1≤i≤P and (Φ(πθi ))1≤i≤P . Sample uniformly 2M agents from M. Copy them and apply isoline variation to obtain M offsprings ((πθi , φi, hi))P <i≤P +M . Run one episode in the environment using each of (πθi )P <i≤P +M to evaluate (F (πθi ))P <i≤P +M\n\nand (Φ(πθi ))P <i≤P +M .\n\nInsert ((πθi , φi, hi))P <i≤P +M in M based on (F (πθi ))P <i≤P +M and (Φ(πθi ))P <i≤P +M . Update nsteps.\n\nend\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: MAP-ELITES Given:\n\n• M: MAP-ELITES repertoire • N ∈ N∗: maximum number of environment steps • M ∈ N∗: number of offsprings per iteration\n\n• F (·): fitness function\n\n• Φ(·): behavior descriptor function\n\n// Initialization Randomly initialize M policies (πθi )1≤i≤M . Run one episode in the environment using each of (πθi )1≤i≤M to evaluate (F (πθi ))1≤i≤M and\n\n(Φ(πθi ))1≤i≤M .\n\nInsert (πθi )1≤i≤M in M based on (F (πθi ))1≤i≤M and (Φ(πθi ))1≤i≤M .\n\n// Main loop Initialize nsteps, the total number of environment interactions carried out so far, to 0. while nsteps ≤ N do\n\nRandomly sample 2M policies from M. Copy them and apply isoline variations to obtain M new policies (πθi )1≤i≤M . Run one episode in the environment using each of (πθi )1≤i≤M to evaluate (F (πθi ))1≤i≤M and\n\n(Φ(πθi ))1≤i≤M .\n\nInsert (πθi )1≤i≤M in M based on (F (πθi ))1≤i≤M and (Φ(πθi ))1≤i≤M . Update nsteps.\n\nend\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3: PGA-MAP-ELITES Given:\n\n• M: MAP-ELITES repertoire • N ∈ N∗: maximum number of environment steps • M ∈ N∗: number of offsprings per iteration • Sc ∈ N∗: number of TD3 training steps used to update the shared critic per iteration • Sp ∈ N∗: number of TD3 policy update steps per iteration per policy\n\n• TD3 hyperparameters\n\n• F (·): fitness function\n\n• Φ(·): behavior descriptor function\n\n// Initialization Initialize a replay buffer B. Randomly initialize M policies (πθi )1≤i≤M . Run one episode in the environment using each of (πθi )1≤i≤M to evaluate (F (πθi ))1≤i≤M and\n\n(Φ(πθi ))1≤i≤M .\n\nInsert (πθi )1≤i≤M in M based on (F (πθi ))1≤i≤M and (Φ(πθi ))1≤i≤M . Update B with transition data collected during the initial evaluations. Initialize the critic Qφ, the target critic Qφ′ , the greedy policy πθ, and the target greedy policy πθ′ .\n\n// Main loop Initialize nsteps, the total number of environment interactions carried out so far, to 0. while nsteps ≤ N do\n\n// Update the shared critic alongside the greedy policy Carry out Sc TD3 training steps to update Qφ, Qφ′ , πθ and πθ′ (sampling batches of data from B).\n\n// Generate new offsprings using the isoline variation operator Randomly sample M policies from M. Copy them and apply isoline variations to obtain M/2 new policies (πθi )1≤i≤M/2.\n\n// Generate new offsprings using TD3 policy-gradient updates Randomly sample M/2 − 1 policies from M (πθi )M/2<i≤M −1. Carry out Sp TD3 policy gradient steps for each of them independently (sampling batches of data\n\nfrom B).\n\n// Update the repertoire Assign πθM = πθ. Run one episode in the environment using each of (πθi )1≤i≤M to evaluate (F (πθi ))1≤i≤M and\n\n(Φ(πθi ))1≤i≤M .\n\nInsert (πθi )1≤i≤M in M based on (F (πθi ))1≤i≤M and (Φ(πθi ))1≤i≤M . Update B with transition data collected during the evaluations of all M new policies. Update nsteps.\n\nend\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4: ME-ES Given:\n\n• M: MAP-ELITES repertoire • N ∈ N∗: maximum number of environment steps • S ∈ N∗: number of consecutive gradient steps for a given policy • Ngrad ∈ N∗: number of evaluations for gradient approximations • Ninit ∈ N∗: number of randomly-initialized policies used to initialize M • σ > 0: standard deviation of the normal distribution used to perturb parameters for gradient\n\napproximations\n\n• η > 0: learning rate • A: archive of behavior descriptors • N (·, ·): novelty function that takes as an input a behavior descriptor as first argument and A as a\n\nsecond argument\n\n• F (·): fitness function\n\n• Φ(·): behavior descriptor function\n\n// Initialization Randomly initialize Ninit policies (πθi )1≤i≤Ninit . Run one episode in the environment using each of (πθi )1≤i≤Ninit to evaluate (F (πθi ))1≤i≤Ninit and\n\n(Φ(πθi ))1≤i≤Ninit . Insert (πθi )1≤i≤Ninit in M based on (F (πθi ))1≤i≤Ninit and (Φ(πθi ))1≤i≤Ninit . Add (Φ(πθi ))1≤i≤Ninit to A.\n\n// Main loop Initialize nsteps, the total number of environment interactions carried out so far, to 0. Initialize ngrads, the total number of gradient steps carried out so far, to 0. use novelty = true while nsteps ≤ N do\n\nif ngrads ≡ 0 mod S then\n\n// Decide if we should optimize for novelty or fitness. Set use novelty to true with probability 0.5 and to false otherwise.\n\n// Sample a high-performing policy from M if use novelty then\n\nSample a policy πθ ∈ M uniformly from the set of five policies with the highest novelty\n\nN (B(πθ), A).\n\nSample, with probability 0.5, a policy πθ ∈ M from the set of two policies with the highest\n\nfitness F (πθ) or from the last five updated policies.\n\nelse\n\nend\n\nend\n\n// Update the current policy using a gradient approximation Sample (θi)1≤i≤Ngrad ∼ N (θ, σ2I) small perturbations of the current policy’s parameters. Run one episode in the environment using each of the corresponding policies (πθi )1≤i≤Ngrad to\n\nevaluate (F (πθi ))1≤i≤Ngrad and (Φ(πθi ))1≤i≤Ngrad .\n\nif use novelty then\n\nCompute the gradient approximation ∇θ = 1\n\nNgradσ\n\n(cid:80)Ngrad\n\ni=1 N (Φ(πθi ), A) θi−θ σ .\n\nelse\n\nCompute the gradient approximation ∇θ = 1\n\nNgradσ\n\n(cid:80)Ngrad\n\ni=1 F (πθi ) θi−θ σ .\n\nend Update θ = θ + η · ∇θ. Run one episode in the environment using πθ to compute Φ(πθ) and F (πθ). Insert πθ in M based on Φ(πθ) and F (πθ). Add Φ(πθ) to A. Update nsteps. ngrads = ngrads + 1\n\nend\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 5: QD-PG Given:\n\n• M: MAP-ELITES repertoire • N ∈ N∗: maximum number of environment steps • P ∈ N∗: size of the population of RL agents • S ∈ N∗: number of TD3 training steps per iteration per agent\n\n• TD3 hyperparameters\n\n• N (·): novelty reward function\n\n• F (·): fitness function\n\n• Φ(·): behavior descriptor function\n\n// Initialization Initialize a replay buffer B. Randomly initialize P policies (πθi )1≤i≤P . Run one episode in the environment using each of (πθi )1≤i≤P to evaluate (F (πθi ))1≤i≤P and\n\n(Φ(πθi ))1≤i≤P .\n\nInsert (πθi )1≤i≤P in M based on (F (πθi ))1≤i≤P and (Φ(πθi ))1≤i≤P . Update B with transition data collected during the initial evaluations. Initialize the quality (resp. diversity) critic QQ\n\nφ (resp. QD\n\nφ ) and the corresponding target QQ\n\nφ′ (resp. QD\n\nφ′ ).\n\n// Main loop Initialize nsteps, the total number of environment interactions carried out so far, to 0. while nsteps ≤ N do\n\nSample uniformly P policies (πθi )1≤i≤P from M.\n\n// Update the quality critic alongside the first half of the\n\npolicies\n\nfor s = 1 to S do\n\n((πθi , QQ parameters.\n\nSample P/2 batches of transitions from B. Carry out, using one batch of transition per agent, one TD3 training step for each of the agents φ′ ))1≤i≤P/2 in parallel, averaging gradients over the agents for the shared critic\n\nφ , QQ\n\nend\n\n// Update the diversity critic alongside the second half of the\n\npolicies\n\nSample P/2 batches of transitions from B. Overwrite the rewards using the novelty reward function N (·). Carry out, using one batch of transition per agent, one TD3 training step for each of the agents\n\nφ′ ))P/2<i≤P in parallel, averaging gradients over the agents for the shared critic\n\nfor s = 1 to S do\n\n((πθi , QD parameters.\n\nφ , QD\n\nend\n\n// Update the repertoire Run one episode in the environment using each of (πθi )1≤i≤P to evaluate (F (πθi ))1≤i≤P and\n\n(Φ(πθi ))1≤i≤P .\n\nInsert (πθi )1≤i≤P in M based on (F (πθi ))1≤i≤P and (Φ(πθi ))1≤i≤P . Update B with transition data collected during the evaluations of all P new policies. Update nsteps.\n\nend\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 6: PBT Given:\n\n• N ∈ N∗: maximum number of environment steps • P ∈ N∗: size of the population of RL agents • S ∈ N∗: number of training steps per iteration per agent\n\n• p, n ∈ ]0, 1[: PBT proportions\n\n• an RL agent template\n\n• F (·): fitness function\n\n// Initialization Randomly initialize P agents following the chosen RL template ((πθi , φi, hi))1≤i≤P . Initialize P replay buffers (Bi)1≤i≤P (only if replay buffers are used by the RL agent).\n\n// Main loop Initialize nsteps, the total number of environment interactions carried out so far, to 0. while nsteps ≤ N do\n\nTrain agents i = 1, · · · , P independently for S steps using the RL agent template and the replay\n\nbuffers (only if replay buffers are used by the RL agent), interacting with the environment as many times as dictated by the RL agent.\n\nRun one episode in the environment using each of (πθi )1≤i≤P to evaluate (F (πθi ))1≤i≤P . Re-order the agents i = 1, · · · , P in increasing order of their fitnesses (F (πθi ))1≤i≤P . Update agents i = 1, · · · , pP by copying randomly-sampled agents from i = (1 − n)P, · · · , P and\n\ncopy the replay buffers accordingly (only if replay buffers are used by the RL agent).\n\nSample new hyperparameters for agents i = 1, · · · , pP . Update nsteps.\n\nend\n\nB EXPERIMENTAL DETAILS\n\nIn this section, we detail the parameters used for all algorithms. In particular, we stress that we use the same values used in the original studies for all MAP-ELITES-based algorithms other than the one introduced in this paper, namely MAP-ELITES, PGA-MAP-ELITES, QD-PG, and ME-ES. Additionally, we run the implementations of these algorithms provided in the QDAX library Lim et al. (2022) for our experiments. All MAP-ELITES-based algorithms use a grid with 1024 cells initialized using CVT with 50,000 initial random points.\n\nTable 1: PBT parameters\n\nParameter\n\nPopulation size P Proportion of worst agents p Proportion of best agents n Number of training steps per iteration per agent S Replay buffer size\n\nValue\n\n80 0.4 0.1 5000 100000\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: PBT-MAP-ELITES parameters\n\nParameter\n\nValue\n\nNumber of isoline-variation offsprings per iteration M 240 Size of the population of RL agents P Proportion of worst agents p Proportion of agents to sample from the repertoire k Proportion of best agents n Number of training steps per iteration per agent S Replay buffers size Isoline σ1 Isoline σ2\n\n80 0.2 0.4 0.1 5000 100000 0.005 0.05\n\nTable 3: MAP-ELITES parameters.\n\nParameter\n\nValue\n\nNumber of offsprings per iteration M 1000 Isoline σ1 0.005 Isoline σ2 0.05\n\nTable 4: ME-ES parameters.\n\nParameter\n\nNumber of consecutive gradient steps for a given policy S Number of evaluations for gradient approximations Ngrad Number of randomly-initialized policies used to initialize the repertoire Ninit Std of the normal distribution to perturb parameters for gradient approximations σ Learning rate η\n\nTable 5: PGA-MAP-ELITES parameters.\n\nParameter\n\nNumber of offsprings per iteration M Number of TD3 training steps used to update the shared critic per iteration Sc Number of TD3 policy update steps per iteration per policy Sp Discount factor γ Policy learning rate Critic learning rate Noise clipping Policy noise Exploration noise Soft update tau Batch size Replay buffer size\n\n19\n\nValue\n\n10 1000 1\n0.2 0.01\n\nValue\n\n100 300 100 0.99 3e-4 3e-4 0.5 0.2 0.0 0.005 256 100000\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: QD-PG parameters.\n\nParameter\n\nSize of the population of RL agents P Number of TD3 training steps per iteration per agent S Discount factor γ Policy learning rate Critic learning rate Noise clipping Policy noise Exploration noise Soft update tau Batch size Replay buffer size\n\nValue\n\n100 100 0.99 3e-4 3e-4 0.5 0.2 0.0 0.005 256 100000\n\nTable 7: SAC hyperparameters’ ranges (or values if the hyperparameter does not change during training) that PBT and PBT-MAP-ELITES sample from.\n\nParameter\n\nRange / Value\n\nDiscount factor γ Policy learning rate Critic learning rate Alpha learning rate Reward scaling factor Soft update tau Alpha initial value Batch size\n\n[0.9, 1.0] [3e-5, 3e-3] [3e-5, 3e-3] [3e-5, 3e-3] [0.1, 10] 0.005 1.0 256\n\nTable 8: TD3 hyperparameters’ ranges (or values if the hyperparameter does not change during training) that PBT and PBT-MAP-ELITES sample from.\n\nParameter\n\nRange / Value\n\nDiscount factor γ Policy learning rate Critic learning rate Noise clipping Policy noise Exploration noise Soft update tau Batch size\n\n[0.9, 1.0] [3e-5, 3e-3] [3e-5, 3e-3] [0.0, 1.0] [0.0, 1.0] [0.0, 0.2] 0.005 256\n\n20",
    "reference": "# Summary Of The Paper\n\nThe paper presents a framework that allows using any reinforcement learning (RL) algorithm within a population of agents. The contribution is to use quality diversity methods to evolve populations and maintain their diversity. The most commonly used method is MAP-ELITES, but MAP-ELITES does not work well on high-dimensional search spaces. The contribution of the paper is the development of a version of MAP-ELITES, called PBT-MAP-ELITES, that does not depend on a specific RL agent, is robust to hyperparameter choices, and scales to large population sizes.  Some experimental results are included for an example problem.\n\n# Strength And Weaknesses\n\nStrengths: \nThe changes made to MAP-ELITES are simple but seem to be effective.\nWeaknesses: \n- the PBT-MAP-ELITES algorithm is a variation of PGA-MAP-ELITES, which seems quite simple even though it appears to be effective. \n- There are many variations of MAP-ELITES that are used in the experiments but for which there is no explanation in the paper. \n- The experimental results included are relatively limited and are not described with enough details to be reproducible. \n- The paper assumes familiarity with all the specific robotics problems used to test the algorithm.  \n- The figure with the experimental results (Fig.2) is hard to read.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear but not always precise and sufficiently detailed. I do not think is possible to replicate the results. Too many details are missing.\n\n# Summary Of The Review\n\nThe paper presents an algorithm that is a new variation of MAP-ELITES. It evolves a population of agents, scales well to large population sizes, and is robust to hyperparameter choices. The experimental results that are included in the paper show good performance compared to other algorithms, but they are described in a succinct way, making it very hard to replicate the results.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNone"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNIFYING DATA-MODEL SPARSITY FOR CLASSIMBALANCED GRAPH REPRESENTATION LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTo relieve the massive computation cost in the field of deep learning, models with more compact architectures have been proposed for comparable performance. However, it is not only the cumbersome model architectures but also the massiveness of the training data that adds up to the expensive computational burdens. This problem is particularly accentuated in the graph learning field: on one hand, Graph Neural Networks (GNNs) trained upon non-Euclidean graph data often encounter relatively higher time costs, due to their irregular density properties; on the other hand, the natural class-imbalance property accompanied by graphs cannot be alleviated by the massiveness of data, therefore hindering GNNs’ ability in generalization. To fully tackle the above issues, (i) theoretically, we introduce a hypothesis on to what extent a subset of the training data can approximate the full dataset’s learning effectiveness, which is further guaranteed by the gradients’ distance between the subset and the full set; (ii) empirically, we discover that during the learning process of a GNN, some samples in the training dataset are informative in providing gradients for model parameters update. Moreover, the informative subset evolves as the training process proceeds. We refer to this observation as dynamic data sparsity. We also notice that a pruned sparse contrastive GNN model sometimes “forgets” the information provided by the informative subset, reflected in their large loss in magnitudes. Motivated by the above findings, we develop a unified data-model dynamic sparsity framework named Graph Decantation (GraphDec) to address the above challenges. The key idea of GraphDec is to identify the informative subset dynamically during the training process by adopting the sparse graph contrastive learning. Extensive experiments on multiple benchmark datasets demonstrate that GraphDec outperforms state-of-the-art baselines for the class-imbalanced graph/node classification tasks, with respect to classification accuracy and data usage efficiency.\n\n1\n\nINTRODUCTION\n\nGraph representation learning (GRL) (Kipf & Welling, 2017) has shown remarkable power in dealing with non-Euclidean structure data (e.g., social networks, biochemical molecules, knowledge graphs). Graph neural networks (GNNs) (Kipf & Welling, 2017; Hamilton et al., 2017; Veliˇckovi ́c et al., 2018), as the current state-of-the-art of GRL, have become essential in various graph mining applications.\n\nHowever, in many real-world scenarios, training on graph data often encounters two difficulties: class imbalance (Park et al., 2022) and massive data usage (Thakoor et al., 2021; Hu et al., 2020). Firstly, class imbalance naturally exists in datasets from diverse practical domains, such as bioinformatics and social networks. GNNs are sensitive to this property and can be biased toward the dominant classes. This bias may mislead GNNs’ learning process, resulting in underfitting samples that are of real importance to the downstream tasks, and poor test performance at last. Secondly, massive data usage requires GNN to perform message-passing over nodes of high degrees bringing about heavy computation burdens. Some calculations are redundant in that not all neighbors are informative regarding learning task-related embeddings. Unlike regular data such as images or texts, the connectivity of irregular graph data invokes random memory access, which further slows down the efficiency of data readout.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nAccordingly, recent studies (Chen et al., 2021; Zhao et al., 2021; Park et al., 2022) arise to address the issues of class imbalance or massive data usage in graph data: (i) On one hand, to deal with the class imbalance issue in node classification on graphs, GraphSMOTE (Zhao et al., 2021) tries to generate new nodes for the minority classes to balance the training data. Improved upon GraphSMOTE, GraphENS (Park et al., 2022) further proposes a new augmentation method by constructing an ego network to learn the representations of the minority classes. (ii) On the other hand, to alleviate the massive data usage, (Eden et al., 2018; Chen et al., 2018) explore efficient data sampling policies to reduce the computational cost from the data perspective. From the model improvement perspective, some approaches design the quantization-aware training and low-precision inference method to reduce GNNs’ operating costs on data. For example, GLT (Chen et al., 2021) applies the lottery ticket pruning technique (Frankle & Carbin, 2019) to simplify graph data and the GNN model concurrently.\n\nDespite progress made so far, existing methods fail to address the class imbalance and computational burden altogether. Dealing with one may even exacerbate the condition of the other: when tackling the data imbalance, the newly synthetic nodes in GraphSMOTE and GraphENS bring along extra computational burdens for the nextcoming training process. While a compact model reduces the computational burden to some extent, we interestingly found that the pruned model easily “forgets” the minorities in class-imbalanced data, reflected in its worse performance than the original model’s. To investigate this observation, we study how each graph sample affects the GNN training by taking a closer look at the gradients each of them exerts. Specifically, (i) in the early phases of training, we identify a small subset that provides the most informative supervisory signals, as measured by the gradient norms’ magnitudes (shown in later Figure 5); (ii) the informative subset evolves dynamically as the training process proceeds (as depicted in later Figure 3). Both the phenomenons prompt the hypothesize that the full training set’s training effectiveness can be approximated, to some extent, by that of the dynamic subset. We further show that the effectiveness of the approximation is guaranteed by the distance between the gradients of the subset and the full training set, as stated in Theorem 1.\n\nFigure 1: The principle of graph decantation. It decants data samples based on rankings of their gradient scores, and then uses them as the training set in the next epoch.\n\nBased on the above, we propose a novel method called Graph Decantation (GraphDec) to guide dynamic sparsity training from both the model and data aspects. The principle behind GraphDec is shown in Figure 1. Since the disadvantaged but informative samples tend to bring about higher gradient magnitudes, GraphDec relies on the gradients directed by dynamic sparse graph contrastive learning loss to identify the informative subsets that approximate the full set’s training effectiveness. This mechanism not only does not require supervised labels, but also allows for the training of the primary GNN, and the pruning of the sparse one. Specifically, for each epoch, our proposed framework scores samples from the current training set and keep only k most informative samples for the next epoch. Additionally, the framework incorporates a data recycling process, which randomly recycles prior discarded samples (i.e., samples that are considered unimportant in the previous training epochs) by re-involving them in the current training process. As a result, the dynamically updated subset (i) supports the sparse GNN to learn relatively unbiased representations and (ii) approximates the full training set through the lens of Theorem 1. To summarize, our contributions in this work are:\n\n• We develop a novel framework, Graph Decantation, which leverages dynamic sparse graph contrastive learning on class-imbalanced graph data for efficient data usage. To our best knowledge, this is the first study to explore the dynamic sparsity property for class-imbalanced graphs.\n\n• We introduce cosine annealing to dynamically control the sizes of the sparse GNN model and the graph data subset to smooth the training process. Meanwhile, we introduce data recycling to refresh the current data subset and avoid overfitting.\n\n• Comprehensive experiments on multiple benchmark datasets demonstrate that GraphDec outperforms state-of-the-art methods for both the class-imbalanced graph classification and classimbalanced node classification tasks. Additional results show that GraphDec dynamically finds an informative subset across the training epochs effectively.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nGraph Contrastive Learning. Contrastive learning is first established for image tasks and then receives considerable attention in the field of graph representation learning (Chen et al., 2020). Contrastive learning is based on utilizing instance-level identity as supervision and maximizing agreement between positive pairs in hidden space by contrast mode (Velickovic et al., 2019; Hassani & Khasahmadi, 2020; You et al., 2020). Recent research in this area seeks to improve the efficacy of graph contrastive learning by uncovering more difficult views (Xu et al., 2021; You et al., 2021). However, the majority of available approaches utilize a great deal of data. By identifying important subset from the entire dataset, our model avoids this issue.\n\nTraining deep model with sparsity. Parameter pruning aiming at decreasing computational cost has been a popular topic and many parameter-pruning strategies are proposed to balance the trade-off between model performance and learning efficiency (Deng et al., 2020; Liu et al., 2019). Some of them belong to the static pruning category and deep neural networks are pruned either by neurons (Han et al., 2015b; 2016) or architectures (layer and filter) (He et al., 2017; Dong et al., 2017). In contrast, recent works propose dynamic pruning strategies where different compact subnets will be dynamically activated at each training iteration (Mocanu et al., 2018; Mostafa & Wang, 2019; Raihan & Aamodt, 2020). The other line of computation cost reduction lies in the dataset sparsity (Karnin & Liberty, 2019; Mirzasoleiman et al., 2020; Paul et al., 2021). Recently, the property of sparsity is also used to improve model robustness (Chen et al., 2022; Fu et al., 2021). In this work, we attempt to accomplish dynamic sparsity from both the GNN model and the graph dataset simultaneously.\n\nClass-imbalanced learning on graphs. Excepting conventional node re-balanced methods, like reweighting samples (Zhao et al., 2021; Park et al., 2022) and oversampling (Zhao et al., 2021; Park et al., 2022), an early work (Zhou et al., 2018) characterizes rare classes through a curriculum strategy, while other previous works (Shi et al., 2020; Zhao et al., 2021; Park et al., 2022) tackles the class-imbalanced issue by generating synthetic samples to re-balance the dataset. Compared to the node-level task, graph-level re-balancing is under-explored. A recent work (Wang et al., 2021) proposes to utilize neighboring signals to alleviate graph-level class-imbalance. To the best of our knowledge, our proposed GraphDec is the first work to solve the class-imbalanced for both the node-level and graph-level tasks.\n\n3 METHODOLOGY\n\nIn this section, we first theoretically illustrate our graph sparse subset approximation hypothesis, which guides the design of GraphDec to continuously refine the compact training subset via the dynamic graph contrastive learning method. The presentation is organized by the importance ranking procedure of each sample, refine smoothing, and overfitting regularization. Relevant preliminaries of GNNs, graph contrastive learning, and network pruning are provided in Appendix B.\n\n3.1 GRAPH SPARSE SUBSET APPROXIMATION HYPOTHESIS\n\nS ;θptq to indicate the loss of model θptq over the graph dataset Gptq\n\nWe first introduce the key notations used in the method. Specifically, we denote the full graph dataset as GF , the graph data subset used to train the model as GS, the learning rate as α, and the graph learning model parameters as θ (the optimal model parameters as θ ̊). Meanwhile, we add a superscript to represent the model’s parameters and the graph data subset at epoch t, i.e., θptq and Gptq S . Besides, we use LGptq S . Thus, the ›\n› gradient error at the training epoch t can be computed as Errptq “ ›. S ;θptq ́ ∇θptq LGF ;θptq The sparse graph subset approximation hypothesis states that the model effectiveness trained on G can be approximated by the one trained on GS. We introduce the hypothesis as follows: Theorem 1 Assume the model’s parameters at epoch t satisfies and the loss function Lp ̈q is convex, we can have the following guarantee: If training loss LGS is Lipschitz continuous, ∇θptq LGS is upper-bounded by σ, and α “ d T Errptq. mintpLGptq\n\nď d2, where d is a constant,\n\n› ›\n›∇θptqLGptq\n\nS ;θptq ́ Lθ ̊q ď dσ?\n\n› ›θptq\n\nT ́1 t“1\n\n, then\n\n› ›2\n\nř\n\n`\n\n?\n\nT\n\nT\n\nσ\n\nd\n\nThe detailed proof of Theorem 1 is provided in Appendix A. According to Theorem 1, it is straightforward that we can minimize the gap between the models trained on the full graph dataset and\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The overall framework of GraphDec: (i) The dynamic sparse graph contrastive learning model computes gradients for graph/node samples; (ii) The input samples are sorted according to their gradients; (iii) Part of the samples with the smallest gradients are thrown into the recycling bin; (iv) Part of the samples with the largest gradients in the current epoch and some sampled randomly from the recycling bin are jointly used as training input in the next epoch.\n\ngraph data subset, i.e., LGS ;θptq ́ Lθ ̊, by reducing the distance between the gradients of the full graph dataset and the graph subset, i.e., Errptq. In other words, the optimized graph subset Gptq S is expected to approximate the gradients of the full graph dataset, and thereby exerts minimal affects on parameters’ update. In contrast to GraphDec, data diet (Paul et al., 2021) is designed to identify the most influential data samples GS (those with largest gradients during the training phase) only at the early training stage and have them involved in further training processes, while excluding samples from ̄GS “ GF ́ GS with smaller gradients (i.e., ∇θptq LGptq S ;θptq ) eternally. This one-shot selection, however, as we will show in the experiments (Section 4.5), does not always capture the most important samples across all epochs during the training. Specifically, the rankings of elements within a specific GS might be relatively static, but those within the full graph dataset, i.e., G, are usually more dynamic, which implies the gradients of the one-shot subset ∇θptq LGptq S ;θptq is unable to constantly approximate that of the full graph dataset ∇θptq LGF ;θptq during training.\n\nS ;θptq \" ∇θptq L ̄\n\nGptq\n\n3.2 GRAPH DECANTATION\n\nInspired by Theorem 1 and to solve the massive data usage in class-imbalance graphs, we propose GraphDec for achieving competitive performance as well as efficient data usage simultaneously by dynamically filtering out the most influential data subset. The overall framework of GraphDec is illustrated in Figure 2. The training processes are summarized into four steps: (i) First, compute the gradients of the samples in Gptq S with respect to the contrastive learning loss; (ii) Normalize the gradients and rank the corresponding graph/node samples in a descending order based on their gradient magnitudes; (iii) Decay the number of samples from |Gptq S | to |Gpt`1q | with cosine annealing, where we only keep the top p1 ́ εq|Gpt`1q | samples (ε is the exploration rate which controls the ratio of the randomly re-sampled samples from the recycle bin. The rest samples will hold in the recycle bin temporarily; (iv) Finally, randomly re-sample ε|Gpt`1q | samples from the recycled bin. The union of these samples and the ones selected in step (iii) will be used for model training in the (t ` 1)-th epoch. Each of the four steps is described in detail in the following content.\n\nS\n\nS\n\nS\n\nCompute gradients by dynamic sparse graph contrastive learning model. We adopt the mechanism of dynamic sparse graph contrastive learning in computing the gradients. The reason is two-folded: (a) it scores the graph samples without the supervision of any label; (b) this pruning process is more sensitive in selecting informative samples, verified in Appendix D. We omit the superscript ptq for the dataset and model parameters for simplicity in the explanation of this step. Specifically, given a graph training set G “ tGiuN i“1 as input, for each training sample Gi, we i and G2 randomly generate two augmented graph views, G1 i , and feed them into the original GCN model fθp ̈q, and the sparse model fθp p ̈q pruned dynamically by the dynamic sparse pruner, respectively. The gradients are computed based on the outputs of the two GNN branches, directed by the contrastive learning loss signals. To obtain the pruned GNN model, the pruner only keeps neural connections with the top-k largest weight magnitudes. Specifically, the pruned parameters of l-th\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nGNN layer (i.e., θl) are selected following the formula below:\n\np “ TopKpθl, kq; k “ βptq ˆ |θl|, θl\n\n(1)\n\nwhere TopKpθl, kq refers to the operation of selecting the top-k largest elements of θl, and βptq is the fraction of the remaining neural connections, controlled by the cosine annealing formulated as follows:\n\n*\n\n\"\n\nβptq “\n\nβp0q 2\n\nπt T\n\n1 ` cosp\n\nq\n\n, t P r1, T s ,\n\n(2)\n\nwhere βp0q is initialized as 1. In addition, we refresh θl on their gradients, following the formula below:\n\np every few epochs to reactivate neurons based\n\nIθl\n\ng “ argTopKp∇θl LDS ;θ, kq; k “ βptq ˆ |θl|,\n\n(3)\n\nwhere argTopK returns the indices of the top-k largest elements Iθl of the corresponding neurons θl p Y θl p Ð θl p every few epochs by θl g. To further elaborate, we refresh θl g, as the updated pruned parameters to be involved in the next iteration. After we obtain the pruned model, gradients are computed based on the contrastive learning loss between fθpG1 i q, which are then saved for the further ranking process.\n\niq and fθp pG2\n\ng\n\nRank graph samples according to their gradients’ L2 norms. In order to find the relative importance of the samples, we rank the samples based on the gradients each of them brings about, saved in the previous training epoch by the last step. Specifically, at each of the t-th training epoch, we score each sample by the L2 norm of its gradient: fθpG1\n\n› ›∇θL\n\ngpGiq “\n\niq, fθp pG2 i q\n\n(4)\n\n`\n\n ̆› ›\n2 ,\n\nwhere L is the popular InfoNCE (Van den Oord et al., 2018) loss in contrastive learning, taking the outputs of the two GNN branches as inputs. Therefore, the gradient is calculated as follows:\n\n∇θLpfθpG1\n\niq, fθp pG2\n\ni qq “ pθpG1q ́ pθp pG2\n\ni q,\n\n(5)\n\niq and pθp pG2\n\nwhere pθpG1 i q are the normalized model’s predictions, i.e., pp ̈q “ Spf p ̈qq and Sp ̈q is the softmax function or sigmoid function. The samples are ranked based on the values calculated by Eq. 4.\n\nDecay the size of GS by cosine annealing. For decreasing the size of the subset, we use cosine annealing when the training process proceeds. As we will show in Figure 3 for the experiments, some graph samples showing low scores of importance at the early training stage may be highly-scored again if given more patience in the later training epochs. Therefore, chunking the size of the sparse subset radically in one shot deprives the chances of the potential samples informing the models at a later stage. To tackle this issue, we employ cosine annealing to gradually decrease the size of the subset:\n\n\"\n\n*\n\n|Gptq\n\nS | “\n\n|G| 2\n\nπptq T\n\n1 ` cosp\n\nq\n\n, t P r1, T s .\n\n(6)\n\nNote that this process not only automatically decreases the size of GS smoothly, but also avoids the manual one-shot selection as in the data diet (Paul et al., 2021).\n\nRecycle removed graph samples for the next training epoch. We aim to update the elements in Gptq S obtained in the last step. Since current low-scored samples may still have the potential to be highly-scored in the later training epochs, we randomly recycle a proportion of the removed samples and re-involve them in the training process again. Specifically, the exploration rate ε controls the proportion of data that substitutes a number of ε|Gt`1 S | samples with the lowest scores with the same amount of randomly selected samples in Gpt`1q . At the t-th epoch, the update rule is formulated as follows:\n\nS\n\nď\n\nGpt`1q\n\nS\n\n“ TopKpGptq\n\nS , p1 ́ εq|Gpt`1q\n\nS\n\n|q\n\nSampleKp ̄GS\n\npt ́1q, ε|Gpt`1q\n\nS\n\n|q,\n\n(7)\n\nwhere SampleKp ̄GS saved in the last epoch. We utilize the compact sparse subset Gpt`1q S\n(t ` 1)-th epoch, and repeat the previous pipelines until T epochs.\n\n|q returns randomly sampled ε|Gpt`1q\n\npt ́1q, ε|Gpt`1q\n\nS\n\nS\n\npt ́1q, | samples from ̄GS for the training purposes at\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nIn this section, we conduct extensive experiments to validate the effectiveness of our proposed model for both the graph and node classification tasks under imbalanced datasets. We also conduct ablation study and informative subset evolution analysis to further prove the effectiveness. Due to space limit, more analysis validating GraphDec’s properties and resource cost are provided in Appendix D and E.\n\n4.1 EXPERIMENTAL SETUP Datasets. We validate our model on various graph benchmark datasets for the two classification tasks under the class-imbalnced data scenario. For the class-imbalanced graph classification task, we choose the seven validation datasets in G2GNN paper (Wang et al., 2021), i.e., MUTAG, PROTEINS, D&D, NCI1, PTC-MR, DHFR, and REDDIT-B in (Morris et al., 2020). For the class-imbalanced node classification task, we choose the five datasets in the GraphENS paper (Park et al., 2022), i.e., Cora-LT, CiteSeer-LT, PubMed-LT (Sen et al., 2008), Amazon-Photo, and Amazon-Computers. Detailed descriptions of these datasets are provided in the Appendix C.1.\n\nBaselines. We compare our model with a variety of baselines methods with different rebalance methods. For class-imbalanced graph classification, we consider three rebalance methods, i.e., vanilla (without re-balancing when training), up-sampling (Wang et al., 2021), and re-weight (Wang et al., 2021). For each rebalance method, we run three baseline methods including GIN (Xu et al., 2019), InfoGraph (Sun et al., 2019), and GraphCL (You et al., 2020). In addition, we adopt two versions of G2GNN (i.e., remove-edge and mask-node) (Wang et al., 2021) for in-depth comparison. For class-imbalanced node classification, we consider nine baseline methods including vanilla, SynFlow (Tanaka et al., 2020), BGRL (Thakoor et al., 2021), GRACE (Zhu et al., 2020), reweight (Japkowicz & Stephen, 2002), oversampling (Park et al., 2022), cRT (Kang et al., 2020), PC Softmax (Hong et al., 2021), DR-GCN (Shi et al., 2020), GraphSMOTE (Zhao et al., 2021), and GraphENS (Park et al., 2022). We adopt Graph Convolutional Network (GCN) (Kipf & Welling, 2017) as the default architecture for all rebalance methods. Further details about the baselines are illustrated in Appendix C.2.\n\nEvaluation Metrics. To evaluate model performance, we choose F1-micro (F1-mi.) and F1-macro (F1-ma.) scores as the metrics for the class-imbalanced graph classification task, and accuracy (Acc.), balanced accuracy (bAcc.), and F1-macro (F1-ma.) score for the node classification task.\n\nExperimental Settings. We adopt GCN (Kipf & Welling, 2017) as the GNN backbone of GraphDec for both the tasks. In particular, we concatenate a two-layers GCN and a one-layer fully-connected layer for node classification, and add one extra average pooling operator as the readout layer for graph classification. We follow (Wang et al., 2021) and (Park et al., 2022) varying the imbalance ratios for graph and node classification tasks, respectively. In addition, we take GraphCL (You et al., 2020) as the graph contrastive learning framework, and cosine annealing to dynamically control the sparsity rate in the GNN model and the dataset. The target pruning ratio for the model is set to 0.75, and the one for the dataset is set to 1.0. After the contrastive pre-training, we take the GCN output logits as the input to the Support Vector Machine for fine-tuning. GraphDec is implemented in PyTorch and trained on NVIDIA V100 GPU.\n\n4.2 CLASS-IMBALANCED GRAPH CLASSIFICATION PERFORMANCE\n\nThe evaluated results for the graph classification task on class-imbalanced graph datasets are reported in Table 1, with the best performance and runner-ups bold and underlined, respectively. From the table, we find that GraphDec outperforms baseline methods on both the metrics across different datasets, while only uses an average of 50% data and 50% model weights per round. Although a slight F1-micro difference has been detected on D&D when comparing GraphDec to the best baseline G2GNN, it is understandable due to the fact that the graphs in D&D are significantly larger than those in other datasets, necessitating specialized designs for graph augmentations (e.g., the average graph size in terms of node number is 284.32 for D&D, but 39.02 and 17.93 for PROTEINS and MUTAG, respectively). However, in the same dataset, G2GNN only achieves 43.93 on F1-macro while GraphDec reaches to 44.01, which complements the 2% difference on F1-micro and further demonstrates GraphDec’s ability to learn effectively even on large graph datasets. Specifically, models trained under the vanilla setting perform the worst due to the ignorance of the class-imbalance. Up-sampling strategy improves the performance, but it introduces additional unnecessary data usage by sampling the minorities multiple times. Similarly, re-weight strategy tries to address the class-\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Class-imbalanced graph classification results. Numbers after each dataset name indicate imbalance ratios of minority to majority categories. Best/second-best results are in bold/underline.\n\nRebalance\n\nMethod\n\nvanilla\n\nup-sampling\n\nre-weight\n\nG2GNN\n\nBasis\n\nGIN InfoGraph GraphCL\n\nGIN InfoGraph GraphCL\n\nGIN InfoGraph GraphCL\n\nremove edge mask node\n\nGraphDec\n\ndynamic sparsity\n\nRebalance\n\nMethod\n\nvanilla\n\nup-sampling\n\nre-weight\n\nG2GNN\n\nBasis\n\nGIN InfoGraph GraphCL\n\nGIN InfoGraph GraphCL\n\nGIN InfoGraph GraphCL\n\nremove edge mask node\n\nGraphDec\n\ndynamic sparsity\n\nMUTAG (5:45)\n\nPROTEINS (30:270)\n\nD&D (30:270)\n\nNCI1 (100:900)\n\nSparsity (%)\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\ndata model\n\n52.50 69.11 66.82\n\n78.03 78.62 80.06\n\n77.00 80.85 80.20\n\n80.37 83.01\n\n85.71\n\n56.77 69.68 67.77\n\n78.77 79.09 80.45\n\n77.68 81.68 80.84\n\n81.25 83.59\n\n85.71\n\n25.33 35.91 40.86\n\n65.64 62.68 64.21\n\n54.54 65.73 63.46\n\n67.70 67.39\n\n68.32\n\n28.50 36.81 41.24\n\n71.55 66.02 65.76\n\n55.77 69.60 64.97\n\n73.10 73.30\n\n75.84\n\n9.99 21.41 21.02\n\n41.15 41.55 38.96\n\n28.49 41.92 40.29\n\n43.25 43.93\n\n44.01\n\n11.88 27.68 26.80\n\n70.56 71.34 64.23\n\n40.79 72.43 67.96\n\n77.03 79.03\n\n77.02\n\n18.24 33.09 31.02\n\n59.19 53.38 49.92\n\n36.84 53.05 50.05\n\n63.60 64.78\n\n65.73\n\n18.94 34.03 31.62\n\n100 100 100\n\n71.80 ą100 62.20 ą100 58.29 ą100\n\n39.19 62.45 58.18\n\n72.97 74.91\n\n76.02\n\n100 100 100\n\n100 100\n\n50\n\n100 100 100\n\n100 100 100\n\n100 100 100\n\n100 100\n\n50\n\nPTC-MR (9:81)\n\nDHFR (12:108)\n\nREDDIT-B (50:450)\n\nAvg. Rank\n\nSparsity (%)\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\ndata model\n\n17.74 25.85 24.22\n\n44.78 44.29 45.12\n\n36.96 44.09 44.75\n\n46.40 46.61\n\n47.07\n\n20.30 26.71 25.16\n\n55.43 48.91 53.50\n\n43.09 49.17 52.22\n\n56.61 56.70\n\n58.15\n\n35.96 50.62 50.55\n\n55.96 59.49 60.29\n\n55.16 58.67 60.87\n\n61.63 59.72\n\n62.25\n\n49.46 56.28 56.31\n\n59.39 61.62 61.71\n\n57.78 60.24 61.93\n\n63.61 61.27\n\n63.61\n\n33.19 57.67 53.40\n\n66.71 67.01 62.01\n\n45.17 65.79 62.79\n\n68.39 67.52\n\n69.70\n\n36.02 67.10 62.19\n\n83.00 78.68 75.84\n\n51.92 77.35 76.15\n\n86.35 85.43\n\n87.00\n\n12.00 11.00 10.71\n\n12.00 11.14 10.57\n\n6.00 6.00 6.29\n\n9.86 5.43 6.00\n\n2.71 2.71\n\n1.00\n\n5.43 6.00 6.43\n\n9.86 5.29 6.29\n\n2.86 2.71\n\n1.14\n\n100 100 100\n\ną100 ą100 ą100\n\n100 100 100\n\n100 100\n\n50\n\n100 100 100\n\n100 100 100\n\n100 100 100\n\n100 100\n\n50\n\nimbalanced issue by assigning different weights to different samples. However, it requires the labels for weight calculation and thus may not generalize well when labels are missing. G2GNN, as the best baseline, obtains decent performance by considering the usage of rich supervisory signals from both globally and locally neighboring graphs. Finally, the proposed model, GraphDec, achieves the best performance due to its ability in capturing dynamic data sparsity on from both the model and data perspectives. In addition, we rank the performance of GraphDec with regard to baseline methods on each dataset. GraphDec ranks 1.00 and 1.14 on average, which further demonstrates the superiority of GraphDec. Notice that all existing methods utilize the entire datasets and the model weights while GraphDec only uses half of the data and weights to achieve superior performance.\n\n4.3 CLASS-IMBALANCED NODE CLASSIFICATION PERFORMANCE\n\nFor the class-imbalanced node classification task, we first evaluate GraphDec on three long-tailed citation graphs (i.e., Cora-LT, CiteSeer-LT, PubMed-LT) and report the results on Table 2. We find that GraphDec obtains the best performance compared to baseline methods for different metrics. GraphSMOTE and GraphENS achieve satisfactory performance by generating virtual nodes to enrich the involvement of the minorities. In comparison, GraphDec does not rely on synthetic virtual nodes to learn balanced representations, thereby avoiding the unnecessary computational costs. Similarly to the class-imbalanced graph classification task in Section 4.2, GraphDec leverages only half of the data and weights to achieve the best performance, whereas all baselines perform worse even with the full dataset and weights. To validate the efficacy of the proposed model on the real-world data, we evaluate GraphDec on naturally class-imbalanced benchmark datasets (i.e., Amazon-Photo and Amazon-Computers). We see that GraphDec has the best performance on both datasets, which demonstrates our model’s effectiveness with data sourced from different practical scenes.\n\n4.4 ABLATION STUDY\n\nSince GraphDec is a unified learning framework relying on multiple components (steps) to employ dynamic sparsity training from both the model and dataset perspectives, we conduct ablation study to\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Class-imbalanced node classification results. Best/second-best results are in bold/underline.\n\nMethod\n\nvanilla SynFlow GRACE BGRL Re-Weight Oversampling cRT PC Softmax DR-GCN GraphSmote GraphENS\n\nCora-LT\n\nCiteSeer-LT\n\nPubMed-LT\n\nA.P. (ρ “82)\n\nA.C. (ρ “244)\n\nSparsity (%)\n\nAcc.\n\nbAcc.\n\nF1-ma. Acc.\n\nbAcc.\n\nF1-ma. Acc.\n\nbAcc.\n\nF1-ma.\n\n(b)Acc.\n\nF1-ma.\n\n(b)Acc.\n\nF1-ma.\n\ndata model\n\n73.66 72.98 74.72 73.81 75.20 77.44 76.54 76.42 73.90 76.76 77.76\n\n62.72 60.62 63.95 64.95 68.79 70.73 69.26 71.30 64.30 69.31 72.94\n\n63.70 63.29 65.26 64.87 69.27 72.40 70.95 71.24 63.10 70.21 73.13\n\n74.25\n\n53.90 52.85 54.94 56.84 62.56 62.78 60.60 65.70 56.18 62.58 66.92\n\n47.32 46.23 50.87 50.83 55.80 56.01 54.05 61.54 49.57 55.94 60.19\n\n66.90\n\n61.56\n\n43.00 42.19 46.90 47.04 53.74 53.99 52.36 61.49 44.98 54.09 58.67\n\n61.85\n\n70.76 69.63 72.37 74.17 77.44 76.70 75.10 76.92 72.38 75.98 78.12\n\n57.56 56.75 63.22 62.21 72.80 68.49 67.52 75.82 58.86 70.96 74.13\n\n78.20\n\n76.05\n\n51.88 50.99 58.18 59.07 73.66 69.50 68.08 74.19 53.05 71.85 74.58\n\n76.32\n\n82.86 81.57 83.57 83.49 92.94 92.46 91.24 93.32 N/A 92.65 93.82\n\n93.85\n\n78.72 76.93 83.61 82.37 92.95 92.47 91.17 93.32 N/A 92.61 93.81\n\n94.02\n\n68.47 68.10 73.02 75.88 90.04 89.79 86.02 86.59 N/A 89.31 91.94\n\n92.19\n\n100 64.01 100 62.97 100 64.52 100 63.15 90.11 100 89.85 ą100 100 86.00 100 86.62 N/A 100 89.39 ą100 91.94 ą100\n\n92.16\n\n50\n\n100 -\n100 100 100 100 100 100 100 100 100\n\n50\n\nGraphDec\n\n78.29\n\n73.94\n\nTable 3: Ablation study results for both tasks. Four rows of red represent removing four individual components from data sparsity perspective. Four rows of blue represent removing four individual components from model sparsity perspective. Best results are in bold.\n\nVariant\n\nMUTAG PROTEINS D&D NCI1 PTC-MR DHFR REDDIT-B Cora-LT CiteSeer-LT PubMed-LT A. Photos A. Computer\n\nClass-imbalanced Graph Classification (F1-ma.)\n\nClass-imbalanced Node Classification (Acc.)\n\nGraphDec\n\nw/o GS w/o SS w/o CAD w/o RS\n\nw/o RM w/o SG w/o CAG w/o RW\n\nw/o S.S.\n\n85.71\n\n80.10 80.95 78.41 83.21\n\n44.37 82.63 83.50 79.25\n\n80.07\n\n68.32\n\n63.42 63.55 57.99 59.32\n\n40.42 65.96 54.04 56.33\n\n63.90\n\n44.01 65.73\n\n36.61 61.80 42.19 62.30 40.23 60.61 41.65 60.51\n\n38.45 34.39 42.50 69.10 40.21 51.82 38.34 63.00\n\n39.77 57.22\n\n47.07\n\n42.12 45.21 44.96 35.21\n\n32.14 35.19 34.20 38.00\n\n38.60\n\n62.25\n\n48.57 61.99 50.00 60.99\n\n43.75 61.42 62.41 61.53\n\n62.30\n\n69.70\n\n61.40 70.61 67.15 67.61\n\n64.82 69.16 64.14 63.16\n\n65.67\n\n78.29\n\n68.96 77.15 74.87 73.27\n\n70.97 77.54 75.78 76.46\n\n74.82\n\n66.90\n\n60.33 64.67 62.62 61.32\n\n54.58 67.43 63.43 65.36\n\n65.28\n\n78.20\n\n56.22 76.15 75.35 72.02\n\n70.16 72.43 73.07 75.54\n\n74.00\n\n93.85\n\n73.22 79.09 90.71 87.11\n\n79.01 91.25 92.77 90.54\n\n86.14\n\n92.19\n\n67.84 91.33 83.23 90.38\n\n65.38 90.05 87.40 89.10\n\n86.40\n\nprove the validity of each component. Specifically, GraphDec relies on four components to address data sparsity and imbalance, including pruning samples by ranking gradients (GS), training with sparse dataset (SS), using cosine annealing to reduce dataset size (CAD), and recycling removed samples (RS), and the other four to address model sparsity and data imbalance, including pruning weights by ranking magnitudes (RM), using sparse GNN (SG), using cosine annealing to progressively reduce sparse GNN’s size (CAG), and reactivate removed weights (RW). In addition, GraphDec employs self-supervision to calculate the gradient score. The details of model variants are provided in Appendix C.3. We analyze the contributions of different components by removing each of them independently. Experiments for both tasks are conducted comprehensively for effective inspection. The results are shown in Table 3.\n\nFrom the table, we find that the performance drops after removing any component, demonstrating the effectiveness of each component. In general, both mechanisms for addressing data and model sparsity contribute significantly to the overall performance, demonstrating the necessity of these two mechanisms in solving sparsity problem. Self-supervision contributes similarly to the dynamic sparsity mechanisms, in that it enables the identification of informative data samples without label supervision. In the dataset dynamic sparsity mechanism, GS and CAD contribute the most as sparse GNN’s discriminability identifies hidden dynamic sparse subsets accurately and efficiently. Regarding the model dynamic sparsity mechanism, removing RM and SG leads to a significant performance drop, which demonstrates that they are the key components in training the dynamic sparse GNN from the full GNN model. In particular, CAG enables the performance stability after the model pruning and helps capture informative samples during decantation by assigning greater gradient norms. Among these variants, the full model GraphDec achieves the best result in most cases. indicating the importance of the combination of the dynamic sparsity mechanisms from the two perspectives, and the self-supervision strategy.\n\n4.5 ANALYZING EVOLUTION OF SPARSE SUBSET BY SCORING ALL SAMPLES\n\nTo show GraphDec’s capability in dynamically identifying informative samples, we show the visualization of sparse subset evolution of data diet and GraphDec on class-imbalanced NCI1 dataset in Figure 3. Specifically, we compute 1000 graph samples with their importance scores. These samples are then ranked according to their scores and marked with sample indexes. From the upper figures\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Evolution of data samples’ gradients computed by data diet (Paul et al., 2021) (upper figures) and our GraphDec (lower figures) on NCI1 data.\n\nin Figure 3, we find that data diet is unable to accurately identify the dynamic informative nodes. Once a data sample has been removed from the training list due to the low score, the model forever disregards it. However, the fact that a sample is currently unimportant does not imply that it will remain unimportant indefinitely, especially in the early training stage when the model cannot detect the true importance of each sample, resulting in premature elimination of vital nodes. Similarly, if a data sample is considered important at early epochs (i.e., marked with higher sample index), it cannot be removed during subsequent epochs. Therefore, we observe that data diet can only increase the scores of samples within the high index range (i.e., 500–1000), while ignoring samples within the low index range (i.e., ă 500). However, GraphDec (Figure 3 (bottom)) can capture the dynamic importance of each sample regardless of the initial importance score. We see that samples with different indexes all have the opportunities to be considered important and therefore be included in the training list. Correspondingly, GraphDec takes into account a broader range of data samples when shrinking the training list, meanwhile maintaining flexibility towards the previous importance scores.\n\n5 FINDING INFORMATIVE SAMPLES BY SPARSE GNN\n\nCompared with the full GNN, our dynamic sparse GNN is more sensitive in recognizing informative data samples which can be empirically verified by Figure 4. Our dynamic pruned model assigns larger gradients to the minorities than the majorities during the contrastive training, while the full model generally assigns relatively uniform gradients for both of them. Thus, the proposed dynamically pruned model demonstrates its discriminatory ability on the minority class. This ability in our GraphDec framework is capable of resolving the class-imbalance issue.\n\nFigure 4: Results of data samples’ gradients computed by full GNN model and our dynamic sparse GNN model on NCI1 data. Red dashed line: on the left side, points on the x-axis [0, 900] are majority class; on the right side, points on the x-axis [900, 1000] are minority class.\n\n6 CONCLUSION\n\nIn this paper, to take up the graph data imbalance challenge, we propose an efficient and effective method named Graph Decantation (GraphDec), by leveraging the dynamic sparse graph contrastive learning to dynamically identified a sparse-but-informative subset for model training, in which the sparse GNN encoder is dynamically sampled from a dense GNN, and its capability of identifying informative samples is used to rank and update the training data in each epoch. Extensive experiments demonstrate that GraphDec outperforms state-of-the-art baseline methods for both node classification and graph classification tasks in the class-imbalanced scenario. The analysis of the sparse informative samples’ evolution further explains the superiority of GraphDec in identifying the informative subset among the training periods effectively.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nWe do not find that this work is directly related to any ethical risks to society. In general, we would like to see that imbalanced learning algorithms (including this work) are able to perform better on minority groups in real-world applications.\n\nREPRODUCIBILITY STATEMENT\n\nFor the reproducibility of this study, we provide the source code for GraphDec in the supplementary materials. The datasets and other baselines in our experiments are described in Appendix C.1 and C.2.\n\nREFERENCES\n\nJie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks via\n\nimportance sampling. In ICLR, 2018.\n\nTianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery\n\nticket hypothesis for graph neural networks. In ICML, 2021.\n\nTianlong Chen, Zhenyu Zhang, pengjun wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, and Zhangyang Wang. Sparsity winning twice: Better robust generalization from more efficient training. In ICLR, 2022.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, 2020.\n\nLei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration for neural networks: A comprehensive survey. In Proceedings of the IEEE. IEEE, 2020.\n\nXin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise\n\noptimal brain surgeon. NeurIPS, 30, 2017.\n\nTalya Eden, Shweta Jain, Ali Pinar, Dana Ron, and C. Seshadhri. Provable and practical approxima-\n\ntions for the degree distribution using sublinear graph samples. In WWW, 2018.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. In ICLR, 2019.\n\nYonggan Fu, Qixuan Yu, Meng Li, Vikas Chandra, and Yingyan Lin. Double-win quant: Aggressively winning robustness of quantized deep neural networks via random precision training and inference. In ICML, 2021.\n\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nIn NIPS, 2017.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. In NeurIPS, 2015b.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\n\nwith pruning, trained quantization and huffman coding. In ICLR, 2016.\n\nKaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on\n\ngraphs. In ICML, 2020.\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.\n\nIn ICCV, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYoungkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang.\n\nDisentangling label distribution for long-tailed visual recognition. In CVPR, 2021.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.\n\nNathalie Japkowicz and Shaju Stephen. The class imbalance problem: A systematic study. Intelligent\n\nData Analysis, 2002.\n\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In ICLR, 2020.\n\nZohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. In COLT,\n\n2019.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\nIn ICLR, 2017.\n\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\n\nnetwork pruning. In ICLR, 2019.\n\nBaharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of\n\nmachine learning models. In ICML, 2020.\n\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 2018.\n\nChristopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML Workshop on Graph Representation Learning and Beyond, 2020.\n\nHesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks\n\nby dynamic sparse reparameterization. In ICML, 2019.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\nJoonhyung Park, Jaeyun Song, and Eunho Yang. GraphENS: Neighbor-aware ego network synthesis\n\nfor class-imbalanced node classification. In ICLR, 2022.\n\nMansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding\n\nimportant examples early in training. In NeurIPS, 2021.\n\nMd Aamir Raihan and Tor Aamodt. Sparse weight activation training. NeurIPS, 2020.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI Magazine, 2008.\n\nMin Shi, Yufei Tang, Xingquan Zhu, David Wilson, and Jianxun Liu. Multi-class imbalanced graph\n\nconvolutional network learning. In IJCAI, 2020.\n\nFan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In ICLR, 2019.\n\nHidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks\n\nwithout any data by iteratively conserving synaptic flow. In NeurIPS, 2020.\n\nShantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi Munos, Petar Veliˇckovi ́c, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In ICLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. In ICLR, 2019.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\n\nBengio. Graph attention networks. In ICLR, 2018.\n\nYu Wang, Yuying Zhao, Neil Shah, and Tyler Derr. Imbalanced graph classification via graph-of-graph\n\nneural networks. arXiv preprint arXiv:2112.00238, 2021.\n\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\n\ncomprehensive survey on graph neural networks. TNNLS, 2020.\n\nDongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-\n\naware graph contrastive learning. In NeurIPS, 2021.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? In ICLR, 2019.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. In NeurIPS, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated.\n\nIn ICML, 2021.\n\nTianxiang Zhao, Xiang Zhang, and Suhang Wang. Graphsmote: Imbalanced node classification on\n\ngraphs with graph neural networks. In WSDM, 2021.\n\nDawei Zhou, Jingrui He, Hongxia Yang, and Wei Fan. Sparc: Self-paced network representation for\n\nfew-shot rare category characterization. In KDD, 2018.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Contrastive Representation Learning. In ICML Workshop on Graph Representation Learning and Beyond, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOF OF THEOREM 1\n\nWe denote the full graph dataset as GF , the graph data subset used to train the model as GS, the learning rate as α, and the graph learning model parameters as θ (the optimal model parameters as θ ̊). Meanwhile, we add a superscript to represent the model’s parameters and the graph data subset at epoch t, i.e., θptq and Gptq S ;θptq to indicate the loss of model θptq over the graph dataset Gptq S . Thus, the gradient error at the training epoch t can be computed as ›\n› ›\n› Errptq “ ›. The sparse graph subset approximation hypothesis states ›∇θptq LGptq that the model effectiveness trained on G can be approximated by the one trained on GS. We introduce the hypothesis as follows:\n\nS . Besides, we use LGptq\n\nS ;θptq ́ ∇θptq LGF ;θptq\n\nTheorem 1. Assume the model’s parameters at epoch t satisfies and the loss function Lp ̈q is convex, we can have the following guarantee:\n\n› ›θptq\n\n› ›2\n\nď d2, where d is a constant,\n\nIf training loss LGS is Lipschitz continuous, ∇θptq LGS is upper-bounded by σ, and α “ d T Errptq. mintpLGptq\n\nS ;θptq ́ Lθ ̊q ď dσ?\n\nT ́1 t“1\n\nř\n\n`\n\nT\n\nσ\n\nd\n\n?\n\n, then\n\nT\n\nProof 1 The gradients of training loss LGptq According to gradient descent, we have:\n\nS ;θptq at epoch t are supposed to be σ-bounded by σ.\n\n∇θLGptq\n\nS ;θptq pθptqq\n\nT\n\npθptq ́ θ ̊q “\n\n1\n\nαptq pθptq ́ θpt`1qq\n\nT\n\npθptq ́ θ ̊q,\n\n∇θLGptq\n\nS ;θptq pθptqq\n\nT\n\npθptq ́ θ ̊q “\n\n1 2αptq\n\nˆ› ›\n›θptq ́ θpt`1q\n\n› ›\n2 ›\n\n› ›\n›θptq ́ θ ̊\n\n› ›\n2 ›\n\n› ›\n›θpt`1q ́ θ ̊\n\n› ›\n2 ›\n\n ́\n\n`\n\n(8)\n\n ̇\n\n.\n\n(9) Since one update step θptq ́ θpt`1q can be optimized by gradient multiplying with learning rate αptq∇θLGptq\n\nS ;θptqpθptqq, we have:\n\n∇θLGptq\n\nS ;θptq pθptqq\n\nT\n\npθptq ́ θ ̊q “\n\n1 2αptq\n\nˆ› ›\n›αptq∇θLGptq\n\nS ;θptq pθptqq\n\n› ›\n2 ›\n\n› ›\n›θptq ́ θ ̊\n\n› ›\n2 ›\n\n`\n\n› ›\n›θpt`1q ́ θ ̊\n\n› ›\n2 ›\n\n ́\n\n ̇\n\n.\n\nSince ∇θLGptq\n\nS ;θptq pθptqq\n\nT\n\npθptq ́ θ ̊q can be represented as follows:\n\n∇θLGptq\n\nS ;θptq pθptqq ́∇θLGptq\n\nT\n\npθptq ́ θ ̊q “ ∇θLGptq\n\nS ;θptq pθptqq\n\nT\n\npθptq ́ θ ̊q\n\nS ;θptq\n\nTpθptq ́ θ ̊q ` ∇θLGptq\n\nS ;θptq\n\nTpθptq ́ θ ̊q,\n\n(10)\n\n(11)\n\nthen based on the combination of the Equation equation 10 and Equation equation 11, we have:\n\n∇θLGptq\n\nS ;θptq pθptqq\n\nT\n\n∇θLGptq\n\nS ;θptq\n\nTpθptq ́ θ ̊q “\n\nS ;θptq\n\n1 2αptq\n\npθptq ́ θ ̊q ́ ∇θLGptq ˆ› ›\n›αptq∇θLGptq ˆ› ›\n›αptq∇θLGptq ́\n\n1 2αptq\n\nS ;θptq pθptqq\n\nS ;θptq pθptqq\n\nTpθptq ́ θ ̊q ` ∇θLGptq ›\n› ›θptq ́ θ ̊\n\n› ›\n›\n\nTpθptq ́ θ ̊q “ ̇\n\nS ;θptq ›\n› ›\n› 2\n›θpt`1q ́ θ ̊ ›\n\n› ›\n2 ›\n\n ́\n\n`\n\n2\n\n(12)\n\n› ›\n2 ›\n\n› ›\n›θptq ́ θ ̊\n\n› ›\n2 ›\n\n`\n\n› ›\n›θpt`1q ́ θ ̊\n\n› ›\n2 ›\n\n ́\n\n ̇\n\n ́\n\n∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n ̄\n\nT\n\npθptq ́ θ ̊q. (13)\n\nWe assume learning rate αptq, t P r0, T ́ 1s is a constant value, then we have: T ́1ÿ\n\n› ›\n›θp0q ́ θ ̊\n\n› ›\n2 ›\n\n› ›\n›θptq ́ θ ̊\n\n› ›\n2 ›\n\n ́\n\n`\n\n∇θLGptq\n\nS ;θptq\n\nt“0\n\nTpθptq ́ θ ̊q “\n\n1 2α\n\nT ́1ÿ p\n\nt“0\n\n1 2α\n\n› ›\n›α∇θLGptq\n\nS ;θptqpθptqq\n\n› ›\n2 ›\n\nq\n\nˆ ́\n\nT ́1ÿ\n\n`\n\nt“0\n\n∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n13\n\n ̄\n\nT\n\n ̇\n\npθptq ́ θ ̊q\n\n.\n\nUnder review as a conference paper at ICLR 2023\n\nSince we assume\n\n› ›θptq ́ θ ̊\n\n› ›2\n\ně 0, then we have:\n\nT ́1ÿ\n\nt“0\n\n∇θLGptq\n\nS ;θptq\n\nTpθptq ́ θ ̊q ď\n\n› ›\n›θp0q ́ θ ̊\n\n› ›\n2 ›\n\n`\n\n1 2α\n\nT ́1ÿ\n\nt“0\n\np\n\n1 2α\n\n› ›\n›α∇θLGptq\n\nS ;θptq pθptqq\n\n› ›\n2 ›\n\nˆ ́\n\nT ́1ÿ\n\n`\n\nt“0\n\n∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n ̄\n\nT\n\n ̇\n\npθptq ́ θ ̊q\n\nq\n\n.\n\n(14)\n\nWe assume loss L is convex and training loss LGptq Then for convex function Lpθq, we have LGptq this result with Equation 14, we get:\n\nS ;θptq is lipschitz continuous with parameter σ. Tpθptq ́ θ ̊q. By combining\n\nS ;θptq ́ Lθ ̊ ď ∇θLGptq\n\nS ;θptq\n\nT ́1ÿ\n\nt“0\n\nLGptq\n\nS ;θptq ́ Lθ ̊ ď\n\n› ›\n›θp0q ́ θ ̊\n\n› ›\n2 ›\n\n`\n\n1 2α\n\nT ́1ÿ p\n\nt“0\n\n1 2α\n\n› ›\n›α∇θLGptq\n\nS ;θptq pθptqq\n\n› ›\n2 ›\n\nq\n\n ̇\n\n(15)\n\n ̄\n\nT\n\nˆ ́\n\nT ́1ÿ\n\n`\n\nt“0\n\n› ›\n›α∇θLGptq\n\n∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\npθptq ́ θ ̊q\n\n.\n\nS ;θptqpθptqq T ́1ÿ T ασ2 2\n\n`\n\nt“0 T ́1ÿ\n\nt“0\n\nασ2 2\n\n`\n\n`\n\nř\n\nd2 2α\n\n`\n\nd2 2αT\n\n› ›\n› ď σ, and we assume }θ ́ θ ̊} ď d, then we have:\n\n ́› ›\n›∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n ̄\n\n› ›\n›\n\n,\n\nd\n\n ́› ›\n›∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n ̄\n\n› ›\n›\n\n.\n\nd T\n\n(16)\n\n(17)\n\n› ›\n›LGptq\n\nS ;θptq pθq\n\n› ›\n› ď σ,\n\nSince\n\nT ́1ÿ\n\nLGptq\n\nS ;θptq ́ Lθ ̊ ď\n\nt“0 T ́1ÿ\n\nt“0\n\n1 T\n\nLGptq\n\nS ;θptq ́ Lθ ̊ ď\n\nSince min pLGptq\n\nS ;θptq ́ Lθ ̊q ď 1\n\nT\n\nT ́1\n\nt“0 LGptq\n\nmin pLGptq\n\nS ;θptq ́ Lθ ̊q ď\n\nd2 2αT\n\n`\n\nασ2 2\n\n ́› ›\n›∇θLGptq\n\nd T\n\n`\n\nt“0\n\nS ;θptq ́ Lθ ̊, based on Equation 17, we have: ̄\nT ́1ÿ\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n.\n\n(18)\n\n› ›\n›\n\nWe set learning rate α “ d\n\n?\n\nσ\n\nand then have:\n\nT\n\nmin pLGptq\n\nS ;θptq ́ Lθ ̊q ď\n\ndσ ?\n\nT\n\n`\n\nT ́1ÿ\n\nt“0\n\nd T\n\n ́› ›\n›∇θLGptq\n\nS ;θptq pθptqq ́ ∇θLGptq\n\nS ;θptq\n\n ̄\n\n› ›\n›\n\n.\n\n(19)\n\nB PRELIMINARIES: GNNS, GRAPH CONTRASTIVE LEARNING, NETWORK\n\nPRUNING\n\nIn this work, we denote graph as G “ pV, E, Xq, where V is the set of nodes, E is the set of edges, and X P Rd represents the node (and edge) attributes of dimension d. In addition, we represent the neighbor set of node v P V as Nv.\n\nGraph Neural Networks. GNNs (Wu et al., 2020) learn node representations from the graph structure and node attributes. This process can be formulated as:\n\n ́\n\n ́!\n\n) ̄ ̄\n\nhplq\n\nv “ COMBINEplq\n\nhpl ́1q\n\nv\n\n, AGGREGATEplq\n\nhpl ́1q\n\nu\n\n, @u P Nv\n\n,\n\n(20)\n\nwhere hplq v denotes representation of node v at l-th GNN layer; AGGREGATEp ̈q and COMBINEp ̈q are neighbor aggregation and combination functions, respectively; hp0q is initialized with node attribute Xv. We obtain the output representation of each node after repeating the process in Equation (20) for L rounds. The representation of the whole graph, denoted as hG P Rd, can be obtained by using a READOUT function to combine the final node representations learned above:\n\nv\n\n!\n\n)\n\nhG “ READOUT\n\nhpLq\n\nv\n\n| @v P V\n\n,\n\n(21)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Original dataset details for imbalanced graph classification and imbalanced node classification tasks.\n\nTask Dataset\n\n# Graphs # Nodes # Edges # Features # Classes\n\nMUTAG PROTEINS D&D NCI1 PTC-MR DHFR REDDIT-B\n\nCora Citeseer Pubmed A-photo A-computers\n\n„17.93 „19.79 188 1,113 „39.06 „72.82 1,178 „284.32 „715.66 „29.87 „32.30 4,110 „14.29 „14.69 344 756 „42.43 „44.54 2,000 „429.63 „497.75\n\n- -\n- -\n- -\n-\n\n- -\n- -\n-\n\n2,485 2,110 19,717 7,650 13,381\n\n5,069 3,668 44,324 238,162 245,778\n\n1,433 3,703 500 745 767\n\nGraph\n\nNode\n\n2 2\n2 2\n2 2\n2\n\n7 6\n3 8\n10\n\nwhere the READOUT function can be any permutation invariant, like summation, averaging, etc. Graph Contrastive Learning. Given a graph dataset D “ tGiuN i“1, Graph Contrastive Learning (GCL) methods firstly implement proper transformations on each graph Gi to generate two views G1 i . The goal of GCL is to map samples within positive pairs closer in the hidden space, while those of the negative pairs are further. GCL methods are usually optimized by a contrastive loss. Taking the most popular InfoNCE loss (Oord et al., 2018) as an example, the contrastive loss is defined as:\n\ni and G2\n\nLCLpG1\n\ni, G2\n\ni q “ ́ log\n\nř\n\nexp psim pzi,1, zi,2qq j“1,j‰i exp psim pzi,1, zj,2qq\n\nN\n\n,\n\n(22)\n\niq, zi,2 “ fθ pG2\n\nwhere zi,1 “ fθ pG1 Network Pruning. Given an over-parameterized deep neural network fθp ̈q with weights θ, the network pruning is usually performed layer-by-layer. The pruning process of the lth layer in fθp ̈q can be formulated as follows:\n\ni q, and sim denotes the similarity function.\n\nθlth pruned “ TopKpθlth, kq, k “ α ˆ |θlth|,\n\n(23)\n\nwhere θlth is the parameters in the lth layer of fθp ̈q and TopKp ̈, kq refers to the operation to choose the top-k largest elements of θlth . We use a pre-defined sparse rate α to control the fraction of parameters kept in the pruned network θlth pruned. Finally, only the top k “ α ˆ |θlth| largest weights will be kept in the pruned layer. The pruning process will be implemented iteratively to prune the parameters in each layer of deep neural network (Han et al., 2015a).\n\nC EXPERIMENTAL DETAILS\n\nC.1 DATASETS DETAILS\n\nIn this work, seven graph classification datasets and five node classification datasets are used to evaluate the effectiveness of our proposed model, we provided their detailed statistics in Table 4. For graph classification datasets, we follow the imbalance setting of (Wang et al., 2021) to set the trainvalidation split as 25%/25% and change the imbalance ratio from 5:5 (balanced) to 1:9 (imbalanced). The rest of the dataset is used as the test set. The specified imbalance ratio of each dataset is clarified after its name in Table 5. For node classification datasets, we follow (Sen et al., 2008) to set the imbalance ratio of Cora, CiteSeer and PubMed as 10. Besides, the setting of Amazon-Photo and Amazon-Computers are borrowed from (Park et al., 2022), where the imbalance ratio ρ is set as 82 and 244, respectively.\n\nC.2 BASELINE DETAILS\n\nWe compare our model with a variety of baseline methods using different rebalance methods:\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nI. For imbalanced graph classification (Wang et al., 2021), four models are included as baselines in our work, we list these baselines as follow:\n\n(1) GIN (Xu et al., 2019), a popular supervised GNN backbone for graph tasks due to its powerful expressiveness on graph structure;\n\n(2) InfoGraph (Sun et al., 2019), an unsupervised graph learning framework by maximizing the mutual information between the whole graph and its local topology of different levels;\n\n(3) GraphCL (You et al., 2020), learning unsupervised graph representations via maximizing the mutual information between the original graph and corresponding augmented views;\n\n(4) G2GNN (Wang et al., 2021), a re-balanced GNN proposed to utilize additional supervisory signals from both neighboring graphs and graphs themselves to alleviate the imbalance issue of graph.\n\nII. For imbalanced node classification, we consider nine baseline methods in our work, including\n\n(1) vanilla, denoting that we train GCN normally without any extra rebalancing tricks;\n\n(2) re-weight (Japkowicz & Stephen, 2002), denoting we use cost-sensitive loss and re-weight the penalty of nodes in different classes;\n\n(3) oversampling (Park et al., 2022), denoting that we sample nodes of each class to make the data’s number of each class reach the maximum number of corresponding class’s data;\n\n(4) cRT (Kang et al., 2020), a post-hoc correction method for decoupling output representations;\n\n(5) PC Softmax (Hong et al., 2021), a post-hoc correction method for decoupling output representations, too;\n\n(6) DR-GCN (Shi et al., 2020), building virtual minority nodes and forces their features to be close to the neighbors of a source minority node;\n\n(7) GraphSMOTE (Zhao et al., 2021), a pre-processing method that focuses on the input data and investigates the possibility of re-creating new nodes with minority features to balance the training data.\n\n(8) GraphENS (Park et al., 2022), proposing a new augmentation method to construct an ego network from all nodes for learning minority representation.\n\n(9) SynFlow (Tanaka et al., 2020), a one-shot model pruning method with less reliance on data.\n\n(10) BGRL (Thakoor et al., 2021), a graph contrastive learning method using only simple augmentations and avoids the requirements for contrasting with negative examples, and thus makes itself scalable.\n\n(11) GRACE (Zhu et al., 2020), a graph contrastive learning method generating two views by corrupting a graph and learning node embeddings by minimizing the distance of node embeddings in these two views.\n\nWe use Graph Convolutional Network (GCN) (Kipf & Welling, 2017) as the default architecture for all rebalance methods.\n\nC.3 DETAILS OF GRAPHDEC VARIANTS\n\nThe details of model variants are provided as follows:\n\nI. Specifically, GraphDec contains four components to address data sparsity and imbalance: (1) GS is sampling informative subset data according to ranking gradients; (2) SS is training model with the sparse dataset, correspondingly; (3) CAD is using cosine annealing to reduce dataset size; (4) RS is recycling removed samples, correspondingly. To investigate their corresponding effectiveness, we remove them correspondingly as:\n\n(1) w/o GS is that we randomly sample subset from the full set;\n\n(2) w/o SS is that we train GNN with the full set;\n\n(3) w/o CAD is that we directly reduce dataset size to target dataset size and it is same as data diet;\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Imbalanced graph classification results. The numbers after each dataset name indicate the imbalance ratios of minority to majority categories. We report the macro F1-score and micro F1-score with the standard errors as Results are reported as mean ̆ std for 3 repetitions on each dataset. We bold the best performance.\n\nRebalance\n\nMethod\n\nvanilla\n\nup-sampling\n\nre-weight\n\nG2GNN (Wang et al., 2021)\n\nGraphDec\n\nRebalance\n\nMethod\n\nvanilla\n\nup-sampling\n\nre-weight\n\nBasis\n\nMUTAG (5:45)\n\nPROTEINS (30:270)\n\nD&D (30:270)\n\nNCI1 (100:900)\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nGIN (Xu et al., 2019) InfoGraph (Sun et al., 2019) GraphCL (You et al., 2020)\n\n52.50 ̆ 18.70 69.11 ̆ 9.03 66.82 ̆ 11.56\n\n56.77 ̆ 14.14 69.68 ̆ 7.77 67.77 ̆ 9.78\n\nGIN (Xu et al., 2019) InfoGraph (Sun et al., 2019) GraphCL (You et al., 2020)\n\nGIN (Xu et al., 2019) InfoGraph (Sun et al., 2019) GraphCL (You et al., 2020)\n\nremove edge mask node\n\n78.03 ̆ 7.62 78.62 ̆ 6.84 80.06 ̆ 7.79\n\n77.00 ̆ 9.59 80.85 ̆ 7.75 80.20 ̆ 7.27\n\n80.37 ̆ 6.73 83.01 ̆ 7.01\n\n78.77 ̆ 7.67 79.09 ̆ 6.86 80.45 ̆ 7.86\n\n77.68 ̆ 9.30 81.68 ̆ 7.83 80.84 ̆ 7.43\n\n81.25 ̆ 6.87 83.59 ̆ 7.14\n\n25.33 ̆ 7.53 35.91 ̆ 7.58 40.86 ̆ 6.94\n\n65.64 ̆ 2.67 62.68 ̆ 2.70 64.21 ̆ 2.53\n\n54.54 ̆ 6.29 65.73 ̆ 3.10 63.46 ̆ 2.42\n\n28.50 ̆ 5.82 36.81 ̆ 6.51 41.24 ̆ 6.38\n\n71.55 ̆ 3.19 66.02 ̆ 3.18 65.76 ̆ 2.61\n\n55.77 ̆ 7.11 69.60 ̆ 3.68 64.97 ̆ 2.41\n\n9.99 ̆ 7.44 21.41 ̆ 4.51 21.02 ̆ 3.05\n\n41.15 ̆ 3.74 41.55 ̆ 2.32 38.96 ̆ 3.01\n\n28.49 ̆ 5.92 41.92 ̆ 2.28 40.29 ̆ 3.31\n\n11.88 ̆ 9.49 27.68 ̆ 7.52 26.80 ̆ 4.95\n\n70.56 ̆ 10.28 71.34 ̆ 6.76 64.23 ̆ 8.10\n\n40.79 ̆ 11.84 72.43 ̆ 6.63 67.96 ̆ 8.98\n\n18.24 ̆ 7.58 33.09 ̆ 3.30 31.02 ̆ 2.69\n\n59.19 ̆ 4.39 53.38 ̆ 1.88 49.92 ̆ 2.15\n\n36.84 ̆ 8.46 53.05 ̆ 1.12 50.05 ̆ 2.09\n\n18.94 ̆ 7.12 34.03 ̆ 3.68 31.62 ̆ 3.05\n\n71.80 ̆ 7.02 62.20 ̆ 2.63 58.29 ̆ 3.30\n\n39.19 ̆ 10.05 62.45 ̆ 1.89 58.18 ̆ 3.08\n\n67.70 ̆ 2.96 67.39 ̆ 2.99\n\n73.10 ̆ 4.05 73.30 ̆ 4.19\n\n43.25 ̆ 3.91 43.93 ̆ 3.46\n\n77.03 ̆ 9.98 79.03 ̆ 10.78\n\n63.60 ̆ 1.57 64.78 ̆ 2.86\n\n72.97 ̆ 1.81 74.91 ̆ 2.14\n\ndynamic sparsity\n\n85.71 ̆10.20\n\n85.71 ̆11.10\n\n68.31 ̆4.23\n\n75.84 ̆6.80\n\n44.01 ̆5.01\n\n77.02 ̆6.26\n\n65.73 ̆4.7\n\n76.02 ̆6.27\n\nBasis\n\nPTC-MR (9:81)\n\nDHFR (12:108)\n\nREDDIT-B (50:450)\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nF1-ma.\n\nF1-mi.\n\nGIN (Xu et al., 2019) InfoGraph (Sun et al., 2019) GraphCL (You et al., 2020)\n\nGIN (Xu et al., 2019) InfoGraph (Sun et al., 2019) GraphCL (You et al., 2020)\n\n17.74 ̆ 6.49 25.85 ̆ 6.14 24.22 ̆ 6.21\n\n44.78 ̆ 8.01 44.29 ̆ 4.69 45.12 ̆ 7.33\n\nGIN (Xu et al., 2019) InfoGraph (Sun et al., 2019) GraphCL (You et al., 2020)\n\n36.96 ̆ 14.08 44.09 ̆ 5.62 44.75 ̆ 7.62\n\n20.30 ̆ 6.06 26.71 ̆ 6.50 25.16 ̆ 5.25\n\n55.43 ̆ 14.25 48.91 ̆ 7.49 53.50 ̆ 13.31\n\n43.09 ̆ 20.01 49.17 ̆ 8.78 52.22 ̆ 13.24\n\n35.96 ̆ 8.87 50.62 ̆ 8.33 50.55 ̆ 10.01\n\n55.96 ̆ 10.06 59.49 ̆ 5.20 60.29 ̆ 9.04\n\n55.16 ̆ 9.47 58.67 ̆ 5.82 60.87 ̆ 6.33\n\n49.46 ̆ 4.90 56.28 ̆ 4.58 56.31 ̆ 6.12\n\n59.39 ̆ 6.52 61.62 ̆ 4.18 61.71 ̆ 6.75\n\n57.78 ̆ 6.69 60.24 ̆ 4.80 61.93 ̆ 5.15\n\n33.19 ̆ 14.26 57.67 ̆ 3.80 53.40 ̆ 4.06\n\n36.02 ̆ 17.38 67.10 ̆ 4.91 62.19 ̆ 5.68\n\n66.71 ̆ 3.92 67.01 ̆ 3.34 62.01 ̆ 3.97\n\n45.17 ̆ 8.46 65.79 ̆ 3.38 62.79 ̆ 6.93\n\n68.39 ̆ 2.97 67.52 ̆ 2.60\n\n83.00 ̆ 5.18 78.68 ̆ 3.71 75.84 ̆ 3.98\n\n51.92 ̆ 12.29 77.35 ̆ 3.96 76.15 ̆ 9.15\n\n86.35 ̆ 2.27 85.43 ̆ 1.80\n\nG2GNN (Wang et al., 2021)\n\nremove edge mask node\n\n46.40 ̆ 7.73 46.61 ̆ 8.27\n\n56.61 ̆ 13.72 56.70 ̆ 14.81\n\n61.63 ̆ 10.02 59.72 ̆ 6.83\n\n63.61 ̆ 6.05 61.27 ̆ 5.40\n\nGraphDec\n\ndynamic sparsity\n\n47.07 ̆8.22\n\n58.15 ̆10.24\n\n62.25 ̆9.54\n\n63.61 ̆7.10\n\n69.70 ̆7.20\n\n87.00 ̆9.36\n\nTable 6: Imbalanced node classification results. We report the accuracy, balanced accuracy, and macro F1-score with the standard errors as mean ̆ std for 3 repetitions on each dataset (Due to time limitation, we will update GRACE, BGRL, and SynFlow’s results with standard deviations in next manuscript). We bold the best performance.\n\nMethod\n\nvanilla SynFlow (Tanaka et al., 2020) GRACE (Zhu et al., 2020) BGRL (Thakoor et al., 2021) Re-Weight (Park et al., 2022) Oversampling (Park et al., 2022) cRT (Kang et al., 2020) PC Softmax (Hong et al., 2021) DR-GCN (Shi et al., 2020) GraphSmote (Zhao et al., 2021) GraphENS (Park et al., 2022)\n\nAcc.\n\n73.66 ̆0.28 72.98 74.72 73.81 75.20 ̆0.19 77.44 ̆0.09 76.54 ̆0.22 76.42 ̆0.34 73.90 ̆0.29 76.76 ̆0.31 77.76 ̆0.09\n\nCora-LT\n\nbAcc.\n\n62.72 ̆0.39 60.62 63.95 64.95 68.79 ̆0.18 70.73 ̆0.10 69.26 ̆0.48 71.30 ̆0.45 64.30 ̆0.39 69.31 ̆0.37 72.94 ̆0.15\n\nCiteSeer-LT\n\nPubMed-LT\n\nA.P. (ρ “82)\n\nA.C. (ρ “244)\n\nF1-ma.\n\nAcc.\n\nbAcc.\n\nF1-ma.\n\nAcc.\n\nbAcc.\n\nF1-ma.\n\n(b)Acc.\n\nF1-ma.\n\n(b)Acc.\n\nF1-ma.\n\n63.70 ̆0.43 63.29 65.26 64.87 69.27 ̆0.26 72.40 ̆0.11 70.95 ̆0.50 71.24 ̆0.52 63.10 ̆0.57 70.21 ̆0.64 73.13 ̆0.11\n\n53.90 ̆0.70 52.85 54.94 56.84 62.56 ̆0.32 62.78 ̆0.37 60.60 ̆0.25 65.70 ̆0.42 56.18 ̆1.10 62.58 ̆0.30 66.92 ̆0.21\n\n47.32 ̆0.61 46.23 50.87 50.83 55.80 ̆0.28 56.01 ̆0.35 54.05 ̆0.22 61.54 ̆0.45 49.57 ̆1.08 55.94 ̆0.34 60.19 ̆0.21\n\n43.00 ̆0.70 42.19 46.90 47.04 53.74 ̆0.28 53.99 ̆0.37 52.36 ̆0.22 61.49 ̆0.49 44.98 ̆1.29 54.09 ̆0.37 58.67 ̆0.25\n\n70.76 ̆0.74 69.63 72.37 74.17 77.44 ̆0.21 76.70 ̆0.48 75.10 ̆0.23 76.92 ̆0.26 72.38 ̆0.19 75.98 ̆0.22 78.12 ̆0.06\n\n57.56 ̆0.59 56.75 63.22 62.21 72.80 ̆0.38 68.49 ̆0.28 67.52 ̆0.72 75.82 ̆0.25 58.86 ̆0.15 70.96 ̆0.36 74.13 ̆0.22\n\n51.88 ̆0.53 50.99 58.18 59.07 73.66 ̆0.27 69.50 ̆0.38 68.08 ̆0.85 74.19 ̆0.25 53.05 ̆0.13 71.85 ̆0.32 74.58 ̆0.13\n\n82.86 ̆0.30 81.57 83.57 83.49 92.94 ̆0.13 92.46 ̆0.47 91.24 ̆0.28 93.32 ̆0.25 N/A 92.65 ̆0.31 93.82 ̆0.13\n\n78.72 ̆0.52 76.93 83.61 82.37 92.95 ̆0.13 92.47 ̆0.48 91.17 ̆0.29 93.32 ̆0.25 N/A 92.61 ̆0.32 93.81 ̆0.12\n\n68.47 ̆2.19 68.10 73.02 75.88 90.04 ̆0.29 89.79 ̆0.16 86.02 ̆0.55 86.59 ̆0.92 N/A 89.31 ̆0.34 91.94 ̆0.17\n\n64.01 ̆3.18 62.97 64.52 63.15 90.11 ̆0.28 89.85 ̆0.17 86.00 ̆0.56 86.62 ̆0.91 N/A 89.39 ̆0.35 91.94 ̆0.17\n\nGraphDec\n\n78.29 ̆0.40\n\n73.94 ̆0.67\n\n74.25 ̆0.83\n\n66.90 ̆0.65\n\n61.56 ̆0.72\n\n61.85 ̆0.96\n\n78.20 ̆0.45\n\n76.05 ̆0.66\n\n76.32 ̆0.66\n\n93.85 ̆0.72\n\n94.02 ̆0.67\n\n92.19 ̆0.73\n\n92.16 ̆0.75\n\n(4) w/o RS is not recycling any removed samples.\n\nII. Another four components to address model sparsity and data imbalance: (1) RM samples model weights according to ranking magnitudes; (2) SG is using sparse GNN, correspondingly; (3) CAG is using cosine annealing to progressively reduce sparse GNN’s size; (4) RW is reactivating removed weights. To investigate their effectiveness, we remove them correspondingly as:\n\n(1) w/o RM is that we randomly sample activated weights from full GNN model;\n\n(2) w/o SG is that we train full GNN during forward and backward;\n\n(3) w/o CAG is that we directly reduce the model size to target sparsity rate;\n\n(4) w/o RW is not reactivating any removed weights during sparse training.\n\nC.4 FULL RESULTS WITH ERROR BARS\n\nWe provide the F1-macro and F1-micro scores along with their standard deviation for our model and other baselines across both graph classification and node classification tasks in Table 5 and Table 6. We report their results as mean ̆ std for 3 repetitions on each metric for each dataset.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Results of data samples’ gradients computed by full GNN model and our dynamic sparse GNN model on NCI1 data. Red dashed line: on the left side, points on the x-axis [0, 900] are majority class; on the right side, points on the x-axis [900, 1000] are minority class.\n\nD FINDING INFORMATIVE SAMPLES BY SPARSE GNN\n\nCompared with the full GNN model, our dynamic sparse GNN model is more sensitive in recognizing informative data samples which can be empirically verified by Figure 5. As we can see in the figure, our dynamic pruned model assigns larger gradients to the minorities than the majorities during the contrastive training, while the full model generally assigns relatively uniform gradients for both of them. Thus, the proposed dynamically pruned model demonstrates its discriminatory ability on the minority class.\n\nE RESOURCE COST\n\nTo evaluate the proposed GraphDec’s computational cost on a wide range of datasets, results in Table 7 that include three different class-imbalanced node classification datasets (PubMed-LT, CoraLT, CiteSeer-LT), three different class-imbalanced graph classification datasets (MUTAG, PROTEINS, PTC MR), and four baselines (vanilla GCN, re-weight, re(/over)-sample, GraphCL). We run 200 epochs for each method to measure their computational time (second) for training. On NVIDIA GeForce RTX 3090 GPU device, we obtain the running time as reported in Table 7. All models are implemented in PyTorch Geometric (Fey & Lenssen, 2019).\n\nTable 7: Computational time comparisons.\n\nModel Method\n\nPubMed-LT Cora-LT CiteSeer-LT PROTEINS PTC MR MUTAG\n\nGCN\n\nvanilla re-weight re(/over)-sample GraphCL GraphDec\n\n2.436 2.330 3.241 3.747 2.243\n\n2.154 2.282 2.860 3.412 1.995\n\n2.129 2.150 2.794 3.399 1.952\n\n12.798 12.903 15.996 14.981 10.614\n\n4.295 4.410 5.734 5.049 4.212\n\n2.989 3.125 4.022 3.215 2.090\n\nAccording to the results, our GraphDec encounters less computation cost than prior methods. The following explains why augmentation doubles the input graph without increasing overall computation costs: (i) The augmentations we adopt (e.g, node dropping and edge dropping) reduce the size of input graphs (i.e., node number decreases 25%, edge number decreases 25-35%); (ii) During each epoch, our GraphDec prunes datasets so that approximately only 50% of the training data is used. (iii) GraphDec prunes the model weights, resulting in a lighter model requiring less computational resources. (iv) Despite the fact that augmentation doubles the number of input graphs, the additional new views only consume forward computational resources without requiring a backward or weight update step, thereby only marginally increases the computation.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper aims to solve both graph data imbalance and unnecessary model-level computation burden in a unified framework. Specifically, the authors first examine the challenges from theoretical and empirical perspectives. Then, the authors propose GraphDec, a novel data-model dynamic sparsity framework to address the challenges. Extensive experiments on multiple benchmark datasets demonstrate that GraphDec outperforms state-of-the-art methods.\n\n# Strength And Weaknesses\n\n+This work is solid and insightful. The proposed framework investigates a new perspective in which sparsification can be utilized in both graph datasets and graph neural networks, which possess many advantages for real-world applications. For example, this work can be leveraged to detect important data points and conserve computational time.\n\n+The authors conduct comprehensive ablation studies to evaluate each model components, which clearly show the contributions of every module and the impact of rebalancing strategies.\nThe introduced components such as dynamically downsampling important subsets, and pruning contrastive model are empirically proven to contribute distinct positive impacts to the full framework. Despite employing a subset of the data, the proposed method achieves similar or better performance compared to the methods that utilize the entire dataset. \n\n+ The overall structure is based on an intriguing hypothesis with proof. The authors provide a hypothesis to explain why downsample a subset can estimate the entire graph dataset; they infer that the subset with the gradient closest to the gradient of the entire dataset is the most important. They then apply this inspiration to develop GraphDec. In addition, the inspiration provided by this hypothesis is also relevant for tasks in other domains (e.g., computer vision, natural language processing).\n\nWeakness/questions:\n-What is the purpose of training the contrastive learning system? Does this method exclude other learning schemes and supervised loss? The paper can be improved by highlighting the unique effect, such as a concise summary of the motivation and empirical ablation experiments.\n\n-Certain technical descriptions lack clarity for readers unfamiliar with coresets. In the experiment section, for instance, the word \"prune\" is sometimes used in a rather ambiguous manner. In the previous method section, it refers to the network, whereas in the experiment section, prune appears to be the \"sampling\" step of the dataset. The authors should provide additional information to explain the definitions of prune and sample on data and model or clarify that the terms are used in different sentences.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and the methodology is well-motivated. The designed model is novel and solid. The availability of code and data facilitates reproducibility.\n\n# Summary Of The Review\n\nThe paper address two significant challenges of imbalanced graph learning by designing a novel and effective model. The technical contribution is solid. The empirical explanations are interesting, and the experimental results are remarkable.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGUARDED POLICY OPTIMIZATION WITH IMPERFECT ONLINE DEMONSTRATIONS\n\nZhenghai Xue1, Zhenghao Peng2, Quanyi Li3, Zhihan Liu4, Bolei Zhou2 1Nanyang Technological University, Singapore, 2 University of California, Los Angeles, 3The University of Edinburgh, 4Northwestern University\n\nABSTRACT\n\nThe Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher’s own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.\n\n1\n\nINTRODUCTION\n\nIn Reinforcement Learning (RL), the Teacher-Student Framework (TSF) (Zimmer et al., 2014; Kelly et al., 2019) incorporates well-performing neural controllers or human experts as teacher policies in the learning process of autonomous agents. At each step, the teacher guards the free exploration of the student by intervening when a specific intervention criterion holds. Online data collected from both the teacher policy and the student policy will be saved into the replay buffer and exploited with Imitation Learning or Off-Policy RL algorithms. Such a guarded policy optimization pipeline can either provide safety guarantee (Peng et al., 2021) or facilitate efficient exploration (Torrey & Taylor, 2013).\n\nThe majority of RL methods in TSF assume the availability of a well-performing teacher policy (Spencer et al., 2020; Torrey & Taylor, 2013) so that the student can properly learn from the teacher’s demonstration about how to act in the environment. The teacher intervention is triggered when the student acts differently from the teacher (Peng et al., 2021) or when the teacher finds the current state worth exploring (Chisari et al., 2021). This is similar to imitation learning where the training outcome is significantly affected by the quality of demonstrations (Kumar et al., 2020; Fujimoto et al., 2019). Thus with current TSF methods if the teacher is incapable of providing high-quality demonstrations, the student will be misguided and its final performance will be upperbounded by the performance of the teacher. However, it is time-consuming or even impossible to obtain a well-performing teacher in many real-world applications such as object manipulation with robot arms (Yu et al., 2020a) and autonomous driving (Li et al., 2022a). As a result, current TSF methods will behave poorly with a less capable teacher.\n\nIn the real world, the coach of Usain Bolt does not necessarily need to run faster than Usain Bolt. Is it possible to develop a new interactive learning scheme where a student can outperform the teacher while retaining safety guarantee from it? In this work we develop a new guarded policy optimization\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nmethod called Teacher-Student Shared Control (TS2C). It follows the setting of a teacher policy and a learning student policy, but relaxes the requirement of high-quality demonstrations from the teacher. A new intervention mechanism is designed: Rather than triggering intervention based on the similarity between the actions of teacher and student, the intervention is now determined by a trajectory-based value estimator. The student is allowed to conduct an action that deviates from the teacher’s, as long as its expected return is promising. By relaxing the intervention criterion from step-wise action similarity to trajectory-based value estimation, the student has the freedom to act differently when the teacher fails to provide correct demonstration and thus has the potential to outperform the imperfect teacher. We conduct theoretical analysis and show that in previous TSF methods the quality of the online data-collecting policy is upper-bounded by the performance of the teacher policy. In contrast, TS2C is not limited by the imperfect teacher in upper-bound performance, while still retaining a lower-bound performance and safety guarantee.\n\nExperiments on various continuous control environments show that under the newly proposed method, the learning student policy can be optimized efficiently and safely under different levels of teachers while other TSF algorithms are largely bounded by the teacher’s performance. Furthermore, the student policies trained under the proposed TS2C substantially outperform all baseline methods in terms of higher efficiency and lower test-time cost, supporting our theoretical analysis.\n\n2 BACKGROUND\n\n2.1 RELATED WORK\n\nThe Teacher-Student Framework The idea of transferring knowledge from a teacher policy to a student policy has been explored in reinforcement learning (Zimmer et al., 2014). It improves the learning efficiency of the student policy by leveraging a pretrained teacher policy, usually by adding auxiliary loss to encourage the student policy to be close to the teacher policy (Schmitt et al., 2018; Traor ́e et al., 2019). Though our method follows teacher-student transfer framework, an optimal teacher is not a necessity. During training, agents are fully controlled by either the student (Traor ́e et al., 2019; Schmitt et al., 2018) or the teacher policy (Rusu et al., 2016), while our method follows intervention-based RL where a mixed policy controls the agent. Other attempts to relax the need of well-performing teacher models include student-student transfer (Lin et al., 2017; Lai et al., 2020), in which heterogeneous agents exchange knowledge through mutual regularisation (Zhao & Hospedales, 2021; Peng et al., 2020).\n\nLearning from Demonstrations Another way to exploit the teacher policy is to collect static demonstration data from it. The learning agent will regard the demonstration as optimal transitions to imitate from. If the data is provided without reward signals, agent can learn by imitating the teacher’s policy distribution (Ly & Akhloufi, 2020), matching the trajectory distribution (Ho & Ermon, 2016; Xu et al., 2019) or learning a parameterized reward function with inverse reinforcement learning (Abbeel & Ng, 2004; Fu et al., 2017). With additional reward signals, agents can perform Bellman updates pessimistically, as most offline reinforcement learning algorithms do (Levine et al., 2020). The conservative Bellman update can be performed either by restricting the overestimation of Q-function learning (Fujimoto et al., 2019; Kumar et al., 2020) or by involving model-based uncertainty estimation (Yu et al., 2020b; Chen et al., 2021b). In contrast to the offline learning from demonstration, in this work we focus on the online deployment of teacher policies with teacherstudent shared control and show its superiority in reducing the state distributional shift, improving efficiency and ensuring training-time safety.\n\nIntervention-based Reinforcement Learning Intervention-based RL enables both the expert and the learning agent to generate online samples in the environment. The switch between policies can be random (Ross et al., 2011), rule-based (Parnichkun et al., 2022) or determined by the expert, either through the manual intervention of human participators (Abel et al., 2017; Chisari et al., 2021; Li et al., 2022b) or by referring to the policy distribution of a parameterized expert (Peng et al., 2021). More delicate switching algorithms include RCMP (da Silva et al., 2020) which asks for expert advice when the learner’s action has high estimated uncertainty. RCMP only works for agents with discrete action spaces, while we investigate continuous action space in this paper. Also, Ross & Bagnell (2014) and Sun et al. (2017) query the expert to obtain the optimal value function, which is used to guide the expert intervention. These switching mechanisms assume the expert policy to be optimal, while our proposed algorithm can make use of a suboptimal expert policy. To exploit\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nsamples collected with different policies, Ross et al. (2011) and Kelly et al. (2019) compute behavior cloning loss on samples where the expert policy is in control and discard those generated by the learner. Other algorithms (Mandlekar et al., 2020; Chisari et al., 2021) assign positive labels on expert samples and compute policy gradient loss based on the pseudo reward. Some other research works focus on provable safety guarantee with shared control (Peng et al., 2021; Wagener et al., 2021), while we provide an additional lower-bound guarantee of the accumulated reward for our method.\n\n2.2 NOTATIONS\n\nWe consider an infinite-horizon Markov decision process (MDP), defined by the tuple M = ⟨S, A, P, R, γ, d0⟩ consisting of a finite state space S, a finite action space A, the state transition probability distribution P : S × A × S → [0, 1], the reward function R : S × A → [Rmin, Rmax], the discount factor γ ∈ (0, 1) and the initial state distribution d0 : S → [0, 1]. Unless otherwise stated, π denotes a stochastic policy π : S × A → [0, 1]. The state-action value and state value functions of π are defined as Qπ(s, a) = Es0=s,a0=a,at∼π(·|st),st+1∼p(·|st,at) [(cid:80)∞ t=0 γtR (st, at)] and V π(s) = Ea∼π(·|s)Qπ(s, a). The optimal policy is expected to maximize the accumulated return J(π) = Es∼d0 V π(s).\n\nThe Teacher-Student Framework (TSF) models the shared control system as the combination of a teacher policy πt which is pretrained and fixed and a student policy πs to be learned. The actual actions applied to the agent are deduced from a mixed policy of πt and πs, where πt starts generating actions when intervention happens. The details of the intervention mechanism are described in Sec. 3.2. The goal of TSF is to improve the training efficiency and safety of πs with the involvement of πt. The discrepancy between πt and πs on state s, termed as policy discrepancy, is the L1-norm of output difference: ∥πt(·|s) − πs(·|s)∥1 = (cid:82) A |πt(a|s) − πs(a|s)| da. We define the discounted state distribution under policy π as dπ(s) = (1 − γ) (cid:80)∞ t=0 γt Pr (st = s; π, d0), where Prπ (st = s; π, d0) is the state visitation probability. The state distribution discrepancy is defined as the difference in L1-norm of the discounted state distributions deduced from two policies: ∥dπt − dπs ∥1 = (cid:82)\n\nS |dπt(s) − dπs(s)| ds.\n\n3 GUARDED POLICY OPTIMIZATION WITH ONLINE DEMONSTRATIONS\n\nFig. 1 shows an overview of our proposed method. In addition to the conventional singleagent RL setting, we include a teacher policy πt in the training loop. The term “teacher” only indicates that the role of this policy is to help the student training. No assumption on the optimality of the teacher is needed. The teacher policy is first used to do warmup rollouts and train a value estimator. During the training of the student policy, both πs and πt receive current state s from the environment. They propose actions as and at, and then a value-based intervention function T (s) determines which action should be taken and applied to the environment. The student policy is then updated with data collected through such intervention.\n\nFigure 1: Overview of the proposed teacherstudent shared control method. Both student and teacher policies are in the training loop and the shared control occurs based on the intervention function.\n\nWe first give a theoretical analysis on the general setting of intervention-based RL in Sec. 3.1. We then discuss the properties of different forms of intervention function T in Sec. 3.2. Based on these analyses, we propose a new algorithm for teacher-student shared control in Sec. 3.3. All the proofs in this section are included in Appendix A.1.\n\n3.1 ANALYSIS ON INTERVENTION-BASED RL\n\nIn intervention-based RL, the teacher policy and the student policy act together and become a mixed behavior policy πb. The intervention function T (s) determines which policy is in charge. Let\n\n3\n\nSharedControlStudentTeacherWarmupSamplingValueEstimatorActionInterventionFunctionEnvironmentActionActionUpdateReplayBufferTransitionPublished as a conference paper at ICLR 2023\n\nT (s) = 1 denotes the teacher policy πt takes control and T (s) = 0 means otherwise. Then πb can be represented as πb(·|s) = T (s)πt(·|s) + (1 − T (s))πs(·|s).\n\nOne issue with the joint control is that the student policy πs is trained with samples collected by the behavior policy πb, whose action distribution is not always aligned with πs. A large state distribution discrepancy between two policies ∥dπb − dπs ∥1 can cause distributional shift and ruin the training. A similar problem exists in behavior cloning (BC), though in BC no intervention is involved and πs learns from samples all collected by the teacher policy πt. To analyze the state distribution discrepancy in BC, we first introduce a useful lemma (Achiam et al., 2017).\n\nLemma 3.1. The state distribution discrepancy between the teacher policy πt and the student policy πs is bounded by their expected policy discrepancy:\n\n∥dπt − dπs∥1\n\n⩽ γ\n\n1 − γ\n\nEs∼dπt\n\n∥πt(· | s) − πs(· | s)∥1 .\n\n(1)\n\nWe apply the lemma to the setting of intervention-based RL and derive a bound for ∥dπb − dπs ∥1. Theorem 3.2. For any behavior policy πb deduced by a teacher policy πt, a student policy πs and an intervention function T (s), the state distribution discrepancy between πb and πs is bounded by\n\n∥dπb − dπs ∥1\n\n⩽ βγ 1 − γ\n\nEs∼dπb\n\n∥πt(· | s) − πs(· | s)∥1 ,\n\n(2)\n\nwhere β =\n\nEs∼dπb [T (s)∥πt(·|s)−πs(·|s)∥1]\n\nEs∼dπb\n\n∥πt(·|s)−πs(·|s)∥1\n\npolicy discrepancy.\n\n∈ [0, 1] is the expected intervention rate weighted by the\n\nBoth Eq. 1 and Eq. 2 bound the state distribution discrepancy by the difference in per-state policy distributions, but the upper bound with intervention is squeezed by the intervention rate β. In practical algorithms, β can be minimized to reduce the state distribution discrepancy and thus relieve the performance drop during test time. Based on Thm. 3.2, we further prove in Appendix A.1 that under the setting of intervention-based RL, the accumulated returns of behavior policy J(πb) and student policy J(πs) can be similarly related. The analysis in this section does not assume a certain form of the intervention function T (s). Our analysis provides the insight on the feasibility and efficiency of all previous algorithms in intervention-based RL (Kelly et al., 2019; Peng et al., 2021; Chisari et al., 2021). In the following section, we will examine different forms of intervention functions and investigate their properties and performance bounds, especially with imperfect online demonstrations.\n\n3.2 LEARNING FROM IMPERFECT DEMONSTRATIONS\n\nA straightforward idea to design the intervention function is to intervene when the student acts differently from the teacher. We model such process with the action-based intervention function Taction(s):\n\n(cid:40)1\n\nif Ea∼πt( ·|s)[log πs(a | s)] < ε,\n\nTaction(s) =\n\n0\n\notherwise,\n\n(3)\n\nwherein ε > 0 is a predefined parameter. A similar intervention function is used in EGPO (Peng et al., 2021), where the student’s action is replaced by the teacher’s if the student’s action has low probability under the teacher’s policy distribution. To measure the effectiveness of a certain form of intervention function, we examine the return of the behavior policy J(πb). With Taction(s) defined in Eq. 3 we can bound J(πb) with the following theorem.\n\nTheorem 3.3. With the action-based intervention function Taction(s), the return of the behavior policy J(πb) is lower and upper bounded by\n\nJ(πt)+\n\n√\n\n2(1 − β)Rmax (1 − γ)2\n\n√\n\nH − ε ⩾ J(πb) ⩾ J(πt) −\n\n√\n\n2(1 − β)Rmax (1 − γ)2\n\n√\n\nH − ε,\n\n(4)\n\nwhere H = Es∼dπb and β is the weighted intervention rate in Thm. 3.2.\n\nH(πt(·|s)) is the average entropy of the teacher policy during shared control\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nThe theorem shows that J(πb) can be lower bounded by the return of the teacher policy πt and an extra term relating to the entropy of the teacher policy. It implies that action-based intervention function Taction is indeed helpful in providing training data with high return. We discuss the tightness of Thm. 3.3 and give an intuitive interpretation of\n\nH − ε in Appendix A.2.\n\n√\n\nA drawback of the action-based intervention function is the strong assumption on the optimal teacher, which is not always feasible. If we turn to employ a suboptimal teacher, the behavior policy would be burdened due to the upper bound in Eq. 4. We illustrate this phenomenon with the example in Fig. 2 where a slow vehicle in gray is driving in front of the ego-vehicle in blue. The student policy is aggressive and would like to overtake the gray vehicle to reach the destination faster, while the teacher intends to follow the vehicle conservatively. Therefore, πs and πb will propose different actions in the current state, leading to Taction = 1 according to Eq. 3. The mixed policy with shared control will always choose to follow the front vehicle and the agent can never accomplish a successful overtake.\n\nIn an autonomous driving scenario, the ego Figure 2: vehicle is the blue one on the left, following the gray vehicle on the right. The upper trajectory is proposed by the student to overtake and the lower trajectory is proposed by the teacher to keep following.\n\nTo empower the student to outperform a suboptimal teacher policy, we investigate a new form of intervention function that encapsulates the long-term value estimation into the decision of intervention, designed as follows:\n\nTvalue(s) =\n\n(cid:40)1\n\nif V πt (s) − Ea∼πs(·|s)Qπt (s, a) > ε,\n\n0\n\notherwise,\n\n(5)\n\nwhere ε > 0 is a predefined parameter. By using this intervention function, the teacher tolerates student’s action if the teacher can not perform significantly better than the student by ε in return. Tvalue no longer expects the student to imitate the teacher policy step-by-step. Instead, it makes decision on the basis of long-term return. Taking trajectories in Fig. 2 again as an example, if the overtake behavior has high return, the student will be preferable to Tvalue. Then the student control will not be intervened by the conservative teacher. So with the value-based intervention function, the agent’s exploration ability will not be limited by a suboptimal teacher. Nevertheless, the lowerbound performance guarantee of the behavior policy πb still holds, shown as follows. Theorem 3.4. With the value-based intervention function Tvalue(s) defined in Eq. 5, the return of the behavior policy πb is lower bounded by\n\nJ (πb) ⩾ J (πt) −\n\n(1 − β)ε 1 − γ\n\n.\n\n(6)\n\nIn safety-critical scenarios, the step-wise training cost c(s, a), i.e., the penalty on the safety violation during training, can be regarded as a negative reward. We define ˆr(s, a) = ˆV , ˆQ r(s, a) − ηc(s, a) as the combined reward, where η is the weighting hyperparameter. and ˆTvalue are similarly defined by substituting r with ˆr in the original definition. Then we have the following corollary related to expected cumulative training cost, defined by C(π) = Es0∼d0,at∼π(·|st),st+1∼p(·|st,at) [(cid:80)∞ Corollary 3.5. With safety-critical value-based intervention function ˆTvalue(s), the expected cumulative training cost of the behavior policy πb is upper bounded by\n\nt=0 γtc (st, at)].\n\nC(πb) ⩽ C(πt) +\n\n(1 − β)ε η(1 − γ)\n\n+\n\n1 η\n\n[J(πb) − J(πt)] .\n\n(7)\n\nIn Eq. 7 the upper bound of behavior policy’s training cost consists of three terms: the cost of teacher policy, the threshold in intervention ε multiplied by coefficients and the superiority of πb over πt in cumulative reward. The first two terms are similar to those in Eq. 6 and the third term means a tradeoff between training safety and efficient exploration, which can be adjusted by hyperparameter η.\n\n5\n\nStudenttrajectoryTeachertrajectoryStudentactionTeacheractionPublished as a conference paper at ICLR 2023\n\nComparing the lower bound performance guarantee of action-based and value-based intervention function (Eq. 4 and Eq. 6), the performance gap between πb and πt can both be bounded with respect to the threshold for intervention ε and the discount factor γ. The difference is that the performance gap when using Taction is in an order of O( (1−γ)2 ) while the gap with Tvalue is in an order of O( 1 It implies that in theory value-based intervention leads to better lower-bound In terms of training safety guarantee, value-based intervention function performance guarantee. Tvalue has better safety guarantee by providing a tighter safety bound with the order of O( 1 1−γ ), in (1−γ)2 ) of action-based intervention function (see Theorem 1 in (Peng et al., 2021)). contrast to O( We show in the Sec. 4.3 that the theoretical advances of Tvalue in training safety and efficiency can both be verified empirically.\n\n1−γ ).\n\n1\n\n1\n\n3.3\n\nIMPLEMENTATION\n\nJustified by the aforementioned advantages of the value-based intervention function, we propose a practical algorithm called Teacher-Student Shared Control (TS2C). Its workflow is listed in Appendix B. To obtain the teacher Q-network Qπt in the value-based intervention function in Eq. 5, we rollout the teacher policy πt and collect training samples during the warmup period. Gaussian noise is added to the teacher’s policy distribution to increase the state coverage during warmup. With limited training data the Q-network may fail to provide accurate estimation when encountering previously unseen states. We propose to use teacher Q-ensemble based on the idea of ensembling Q-networks (Chen et al., 2021a). A set of ensembled teacher Q-networks Qφ with the same architecture and different initialization weights are built and trained with the same data. To learn Qφ we follow the standard procedure in (Chen et al., 2021a) and optimize the following loss:\n\n(cid:2)y − Mean (cid:2)Qφ (s, a)(cid:3)(cid:3)2\n\nL (φ) = Es,a∼D\n\n(8) (cid:2)r + γMean (cid:2)Qφ (s′, a′)(cid:3)(cid:3) is the Bellman target and D is the where y = Es′∼D,a′∼πt(·|s′)+N (0,σ) replay buffer for storing sequences {(s, a, r, s′)}. Teacher will intervene when Tvalue returns 1 or the output variance of ensembled Q-networks surpasses the threshold, which means the agent is exploring unknown regions and requires guarding. We also use Qφ to compute the state-value functions in Eq. 5, leading to the following practical intervention function:\n\n,\n\nTTS2C(s) =\n\n1\n\n \n\n\n\n0\n\nif Mean (cid:2)Ea∼πt(·|s)Qφ (s, a) − Ea∼πs(·|s)Qφ (s, a)(cid:3) > ε1 or Var (cid:2)Ea∼πs(·|s)Qφ (s, a)(cid:3) > ε2,\n\n(9)\n\notherwise.\n\nEq. 2 shows that the distributional shift and the performance gap to oracle can be reduced with smaller β, i.e., less teacher intervention. Therefore, we minimize the amount of teacher intervention via adding negative reward to the transitions one step before the teacher intervention. Incorporating intervention minimization, we use the following loss function to update the student’s Q-network parameterized by ψ:\n\n(cid:104)(cid:0)y′ − Qψ (s, a)(cid:1)2(cid:105)\n\nL(ψ) = Es,a∼D (10) (cid:2)r − λTTS2C(s′) + γQψ (s′, a′) − α log πb(a′|s′)] is the soft Bellman where y′ = Es′∼D,a′∼πb(·|s′) target with intervention minimization. λ is the hyperparameter controlling the intervention minimization. α is the coefficient for maximum-entropy learning updated in the same way as Soft Actor Critic (SAC) (Haarnoja et al., 2018). To update the student’s policy network parameterized by θ, we apply the objective used in SAC as:\n\n,\n\nL(θ) = Es∼D\n\n(cid:2)Ea∼πθ(·|s)\n\n(cid:2)α log (cid:0)πθ (a | s)(cid:1) − Qψ (s, a)(cid:3)(cid:3) .\n\n(11)\n\n4 EXPERIMENTS\n\nWe conduct experiments to investigate the following questions: (1) Can agents trained with TS2C achieve super-teacher performance with imperfect teacher policies while outperforming other methods in the Teacher-Student Framework (TSF)? (2) Can TS2C provide safety guarantee and improve training efficiency compared to algorithms without teacher intervention? (3) Is TS2C robust in different environments and teacher policies trained with different algorithms? To answer questions\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Comparison between our method TS2C and other algorithms with teacher policies providing online demonstrations. “Importance” refers to the Importance Advising algorithm. For each column, the involved teacher policy has high, medium, and low performance respectively.\n\n(1)(2), we conduct preliminary training with the PPO algorithm (Schulman et al., 2017) and save checkpoints on different timesteps. Policies in different stages of PPO training are used as teacher policies in TS2C and other algorithms in the TSF. With regard to question (3), we use agents trained with PPO (Schulman et al., 2017), SAC (Haarnoja et al., 2018) and Behavior Cloning as the teacher policies from different sources.\n\n4.1 ENVIRONMENT SETUP\n\nThe majority of the experiments are conducted on the lightweight driving simulator MetaDrive (Li et al., 2022a). One concern with TSF algorithms is that the student may simply record the teacher’s actions and overfit the training environment. MetaDrive can test the generalizability of learned agents on unseen driving environments with its capability to generate an unlimited number of scenes with various road networks and traffic flows. We choose 100 scenes for training and 50 held-out scenes for testing. Examples of the traffic scenes from MetaDrive are shown in Appendix C. In MetaDrive, the objective is to drive the ego vehicle to the destination without dangerous behaviors such as crashing into other vehicles. The reward function consists of the dense reward proportional to the vehicle speed and the driving distance, and the terminal +20 reward when the ego vehicle reaches the destination. Training cost is increased by 1 when the ego vehicle crashes or drives out of the lane. To evaluate TS2C’s performance in different environments, we also conduct experiments in several environments of the MuJoCo simulator (Todorov et al., 2012).\n\n4.2 BASELINES AND IMPLEMENTATION DETAILS\n\nTwo sets of algorithms are selected as baselines to compare with. One includes traditional RL and IL algorithms without the TSF. By comparing with these methods we can demonstrate how TS2C improves the efficiency and safety of training. Another set contains previous algorithms with the TSF, including Importance Advising (Torrey & Taylor, 2013) and EGPO (Peng et al., 2021). The original Importance Advising uses an intervention function based on the range of the Q-function: I(s) = maxa∈A QD(s,a) − mina∈A QD(s,a), where QD is the Q-table of the teacher policy. Such Q-table is not applicable in the Metadrive simulator with continuous state and action spaces. In practice, we sample N actions from the teacher’s policy distribution and compute their Q-values on a certain state. The intervention happens if the range, i.e., the maximum value minus the minimum\n\n7\n\n00.51.01.52.0050100150200250300350Test Reward with Teacher-HighTS2CImportanceEGPOTeacher00.51.01.52.0050100150200250300350Test Reward with Teacher-Medium00.51.01.52.0050100150200250300350Test Reward with Teacher-Low00.51.01.52.0Sampled Steps (1e5)0246810Training Cost with Teacher-HighTS2CImportanceEGPO00.51.01.52.0Sampled Steps (1e5)02468101214Training Cost with Teacher-Medium00.51.01.52.0Sampled Steps (1e5)0246810Training Cost with Teacher-LowPublished as a conference paper at ICLR 2023\n\nFigure 4: Figures (a) and (b) shows the comparison of efficiency and safety between TS2C and baseline algorithms without teacher policies providing online demonstrations. Figure (c) shows the comparison of the average intervention rate between TS2C and two baseline algorithms in the TSF.\n\nFigure 5: Performance comparison between our method TS2C and baseline algorithms on three environments from MuJoCo.\n\nvalue, surpass a certain threshold ε. The EGPO algorithm uses an intervention function similar to the action-based intervention function introduced in section 3.2. All algorithms are trained with 4 different random seeds. In all figures the solid line is computed with the average value across different seeds and the shadow implies the standard deviation. We leave detailed information on the experiments and the result of ablation studies on hyperparameters in Appendix C.\n\n4.3 RESULTS\n\nSuper-teacher performance and better safety guarantee The training result with three different levels of teacher policy can be seen in Fig. 3. The first row shows that the performance of TS2C is not limited by the imperfect teacher policies. It converges within 200k steps, independent of different performances of the teacher. EGPO and Importance Advicing is clearly bounded by teacher-medium and teacher-low, performing much worse than TS2C with imperfect teachers. The second row of Fig. 3 shows TS2C has lower training cost than both algorithms. Compared to EGPO and Importance Advising, the intervention mechanism in TS2C is better-designed and leads to better behaviors.\n\nBetter performance with TSF The result of comparing TS2C with baseline methods without the TSF can be seen in Fig. 4(a)(b). We use the teacher policy with a medium level of performance to train the student in TS2C. It achieves better performance and lower training cost than the baseline algorithms SAC, PPO, and BC. The comparative results show the effectiveness of incorporating teacher policies in online training. The behavior cloning algorithm does not involve online sampling in the training process, so it has zero training cost.\n\nExtension for different environments and teacher policies The performances of TS2C in different MuJoCo environments and different sources of teacher policy are presented in Fig. 5 and 6 respectively. The figures show that TS2C is generalizable to different environments. It can also make use of the teacher policies from different sources and achieve super-teacher performance consistently. Our TS2C algorithm can outperform SAC in all three MuJoCo environments taken into consideration. On the other hand, though the EGPO algorithm has the best performance in the Pendulum environment, it struggles in the other two environments, namely Hopper and Walker.\n\n8\n\n00.51.01.52.0Sampled Steps (1e5)0100200300(a) Test RewardTS2CSACPPOBC00.51.01.52.0Sampled Steps (1e5)02468(b) Training Cost00.51.01.52.0Sampled Steps (1e5)0.00.20.40.60.81.0(c) Average Intervention RateTS2CEGPOImportance00.51.01.52.0Sampled Steps (1e4)150012501000750500250Test Reward on Pendulum EnvTS2CSACEGPOTeacher012345Sampled Steps (1e5)0100020003000Test Reward on Hopper Env00.20.40.60.81.0Sampled Steps (1e6)01000200030004000Test Reward on Walker2d EnvPublished as a conference paper at ICLR 2023\n\nFigure 6: Performance comparison between our method TS2C and baseline algorithms with teacher policies providing online demonstrations. The teacher policies are trained by PPO, SAC, and behavior cloning respectively.\n\n4.4 EFFECTS OF INTERVENTION FUNCTIONS\n\nWe further investigate the intervention behaviors under different intervention functions. As shown in Fig. 4(c), the average intervention rate Es∼dπb T (s) of TS2C drops quickly as soon as the student policy takes control. The teacher policy only intervenes during a very few states where it can propose actions with higher value than the students. The intervention rate of EGPO remains high due to the action-based intervention function: the teacher intervenes whenever the student act differently.\n\nWe also show different outcomes of action-based and value-based intervention functions with screenIn Fig. 7 the shots in the MetaDrive simulator. ego vehicle happens to drive behind a traffic vehicle which is in an orange trajectory. With action-based intervention the teacher takes control and keeps following the front vehicle, as shown in the green trajectory. In contrast, with the value-based intervention the student policy proposes to turn left and overtake the front vehicle as in the blue trajectory. Such action has higher return and therefore is tolerable by TTS2C, leading to a better agent trajectory.\n\n5 CONCLUSION AND DISCUSSION\n\nFigure 7: Visualization of the trajectories resulting from different intervention mechanisms. The trajectories of irrelevant traffic vehicles are marked orange. As in the green trajectory, action-based intervention make the car following the front vehicle. Valuebased intervention instead can learn overtaking behavior as in blue trajectory.\n\nIn this work, we conduct theoretic analysis on intervention-based RL algorithms in the TeacherStudent Framework. It is found that while the intervention mechanism has better properties than some imitation learning methods, using an action-based intervention function limits the performance of the student policy. We then propose TS2C, a value-based intervention scheme for online policy optimization with imperfect teachers. We provide the theoretic guarantees on its exploration ability and safety. Experiments show that the proposed TS2C method achieves consistent performance independent to the teacher policy being used. Our work brings progress and potential impact to relevant topics such as active learning, human-in-the-loop methods, and safety-critical applications.\n\nLimitations. The proposed algorithm assumes the agent can access environment rewards, and thus defines the intervention function based on value estimations. It may not work in tasks where reward signals are inaccessible. This limitation could be tackled by considering reward-free settings and employing unsupervised skill discovery (Eysenbach et al., 2019; Aubret et al., 2019). These methods provide proxy reward functions that can be used in teacher intervention.\n\n9\n\n00.51.01.52.0Sampled Steps (1e5)0100200300Test Reward with PPO TeacherTS2CImportanceEGPOTeacher00.51.01.52.0Sampled Steps (1e5)0100200300Test Reward with SAC Teacher00.51.01.52.0Sampled Steps (1e5)0100200300Test Reward with BC TeacherTrajectorywithvalue-basedinterventionTrajectorywithaction-basedinterventionPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In\n\nProceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.\n\nDavid Abel, John Salvatier, Andreas Stuhlm ̈uller, and Owain Evans. Agent-agnostic human-in-the-\n\nloop reinforcement learning. arXiv preprint arXiv:1701.04079, 2017.\n\nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In\n\nICML, volume 70 of Proceedings of Machine Learning Research, pp. 22–31. PMLR, 2017.\n\nArthur Aubret, La ̈etitia Matignon, and Salima Hassas. A survey on intrinsic motivation in reinforce-\n\nment learning. CoRR, abs/1908.06976, 2019.\n\nXinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized ensembled double qlearning: Learning fast without a model. In International Conference on Learning Representations, 2021a.\n\nXiong-Hui Chen, Yang Yu, Qingyang Li, Fan-Ming Luo, Zhiwei Qin, Wenjie Shang, and Jieping Ye. Offline model-based adaptable policy learning. Advances in Neural Information Processing Systems, 34, 2021b.\n\nEugenio Chisari, Tim Welschehold, Joschka Boedecker, Wolfram Burgard, and Abhinav Valada. Correct me if i am wrong: Interactive learning for robotic manipulation. arXiv preprint arXiv:2110.03316, 2021.\n\nFelipe Leno da Silva, Pablo Hernandez-Leal, Bilal Kartal, and Matthew Taylor. Uncertainty-aware\n\naction advising for deep reinforcement learning agents. In AAAI, 2020.\n\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\n\nLearning skills without a reward function. In ICLR (Poster). OpenReview.net, 2019.\n\nJustin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-\n\nforcement learning. arXiv preprint arXiv:1710.11248, 2017.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NIPS, pp. 4565–4573,\n\n2016.\n\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-\n\nbased policy optimization. In NeurIPS, pp. 12498–12509, 2019.\n\nMichael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer. HgIn 2019 International Conference\n\ndagger: Interactive imitation learning with human experts. on Robotics and Automation (ICRA), pp. 8077–8083. IEEE, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\n\nreinforcement learning. arXiv preprint arXiv:2006.04779, 2020.\n\nKwei-Herng Lai, Daochen Zha, Yuening Li, and Xia Hu. Dual policy distillation. In IJCAI, 2020.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-\n\nrial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nQuanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 2022a.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nQuanyi Li, Zhenghao Peng, and Bolei Zhou. Efficient learning of safe driving policy via human-ai\n\ncopilot optimization. In ICLR. OpenReview.net, 2022b.\n\nKaixiang Lin, Shu Wang, and Jiayu Zhou. Collaborative deep reinforcement learning. arXiv preprint\n\narXiv:1702.05796, 2017.\n\nAbdoulaye O Ly and Moulay Akhloufi. Learning to drive by imitation: An overview of deep behav-\n\nior cloning methods. IEEE Transactions on Intelligent Vehicles, 6(2):195–209, 2020.\n\nAjay Mandlekar, Danfei Xu, Roberto Mart ́ın-Mart ́ın, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-the-loop imitation learning using remote teleoperation. arXiv preprint arXiv:2012.06733, 2020.\n\nRom Parnichkun, Matthew N Dailey, and Atsushi Yamashita. Reil: A framework for reinforced\n\nintervention-based imitation learning. arXiv preprint arXiv:2203.15390, 2022.\n\nZhenghao Peng, Hao Sun, and Bolei Zhou. Non-local policy optimization via diversity-regularized\n\ncollaborative exploration. arXiv preprint arXiv:2006.07781, 2020.\n\nZhenghao Peng, Quanyi Li, Chunxiao Liu, and Bolei Zhou. Safe driving via expert guided policy\n\noptimization. In 5th Annual Conference on Robot Learning, 2021.\n\nSt ́ephane Ross and J. Andrew Bagnell. Reinforcement and imitation learning via interactive no-\n\nregret learning. CoRR, abs/1406.5979, 2014.\n\nSt ́ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference Proceedings, 2011.\n\nAndrei A Rusu, Sergio Gomez Colmenarejo, C ̧ aglar G ̈ulc ̧ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. In ICLR. OpenReview.net, 2016.\n\nSimon Schmitt, Jonathan J Hudson, Augustin Zidek, Simon Osindero, Carl Doersch, Wojciech M Czarnecki, Joel Z Leibo, Heinrich Kuttler, Andrew Zisserman, Karen Simonyan, et al. Kickstarting deep reinforcement learning. arXiv preprint arXiv:1803.03835, 2018.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 1889–1897. JMLR.org, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nJonathan Spencer, Sanjiban Choudhury, Matthew Barnes, Matthew Schmittle, Mung Chiang, Peter Ramadge, and Siddhartha Srinivasa. Learning from interventions: Human-robot interaction as both explicit and implicit feedback. In Robotics: Science and Systems (RSS), 2020.\n\nWen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, and J. Andrew Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 3309–3318. PMLR, 2017.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\n\nIn IROS, pp. 5026–5033. IEEE, 2012.\n\nLisa Torrey and Matthew Taylor. Teaching on a budget: Agents advising agents in reinforcement learning. In Proceedings of the 2013 International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’13, pp. 1053–1060. International Foundation for Autonomous Agents and Multiagent Systems, 2013.\n\nRen ́e Traor ́e, Hugo Caselles-Dupr ́e, Timoth ́ee Lesort, Te Sun, Guanghang Cai, David Filliat, and Natalia D ́ıaz-Rodr ́ıguez. Discorl: Continual reinforcement learning via policy distillation. In NeurIPS workshop on Deep Reinforcement Learning, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nNolan Wagener, Byron Boots, and Ching-An Cheng. Safe reinforcement learning using advantageIn ICML, volume 139 of Proceedings of Machine Learning Research, pp.\n\nbased intervention. 10630–10640. PMLR, 2021.\n\nTian Xu, Ziniu Li, and Yang Yu. On value discrepancy of imitation learning. arXiv preprint\n\narXiv:1911.07027, 2019.\n\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pp. 1094–1100. PMLR, 2020a.\n\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142, 2020b.\n\nChenyang Zhao and Timothy Hospedales. Robust domain randomised reinforcement learning In Proceedings of The 13th Asian Conference on Machine through peer-to-peer distillation. Learning, volume 157 of Proceedings of Machine Learning Research, pp. 1237–1252. PMLR, 2021.\n\nMatthieu Zimmer, Paolo Viappiani, and Paul Weng. Teacher-student framework: a reinforcement learning approach. In AAMAS Workshop Autonomous Robots and Multirobot Systems, 2014.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA THEOREMS IN TS2C\n\nA.1 DETAILED PROOF\n\nWe start the proof with the restatement of Lem. 3.1 in Sec. 3.1.\n\nLemma A.1 (Lemma 4.1 in (Xu et al., 2019)).\n\n∥dπ − dπ′∥1\n\n⩽ γ\n\n1 − γ\n\nEs∼dπ ∥π(· | s) − π′(· | s)∥1 .\n\n(12)\n\nThm 3.2 can be derived by substituting π and π′ in Lem A.1 with πb and πs.\n\nTheorem A.2 (Restatement of Thm. 3.2). For any behavior policy πb deduced by a teacher policy πt, a student policy πs and a intervention function T (s), the state distribution discrepancy between πb and πs is bounded by policy discrepancy and intervention rate:\n\n∥dπb − dπs ∥1\n\n⩽ βγ 1 − γ\n\nEs∼dπb\n\n∥πt(· | s) − πs(· | s)∥1 ,\n\n(13)\n\nEs∼dπb\n\n∥T (s)[πt(·|s)−πs(·|s)]∥1 ∥πt(·|s)−πs(·|s)∥1\n\nEs∼dπb\n\nis the weighted expected intervention rate.\n\nwhere β =\n\nProof.\n\n∥dπb − dπs∥1\n\n⩽ γ\n\n1 − γ γ\n1 − γ γ\n1 − γ βγ 1 − γ\n\n=\n\n=\n\n=\n\nEs∼dπb\n\nEs∼dπb\n\nEs∼dπb\n\nEs∼dπb\n\n∥πb(· | s) − πs(· | s)∥1\n\n∥T (s)πt(· | s) + (1 − T (s))πs(· | s) − πs(· | s)∥1\n\n∥T (s) [πt(· | s) − πs(· | s)]∥1\n\n∥πt(· | s) − πs(· | s)∥1 .\n\n(14)\n\nBased on Thm. 3.2, we further prove that under the setting of shared control, the performance gap of πs to the optimal policy π∗ can be bounded by the gap between the teacher policy πt and π∗, together with the teacher-student policy difference. Therefore, training with the trajectory collected with mixed policy πb is to optimize an upper bound of the student’s suboptimality. The following lemma is helpful in doing this.\n\nLemma A.3.\n\n|J (π) − J (π′)| ⩽ Rmax (1 − γ)2\n\nEs∼dπ ∥π(· | s) − π′(· | s)∥1\n\n(15)\n\nProof. It is a direct combination of Lemma 4.2 and Lemma 4.3 in (Xu et al., 2019).\n\nTheorem A.4. For any behavior policy πb consisting of a teacher policy πt, a student policy πs and a intervention function T (s), the suboptimality of the student policy is bounded by\n\n|J (π∗) − J (πs)| ⩽ βRmax\n\n(1 − γ)2\n\nEs∼πb ∥πt(· | s) − πs(· | s)∥1 + |J (π∗) − J (πb)| ,\n\n(16)\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nProof.\n\n|J (πb) − J (πs)| ⩽ Rmax\n\n(1 − γ)2 Rmax (1 − γ)2 Rmax (1 − γ)2 βRmax (1 − γ)2\n\n=\n\n=\n\n=\n\nEs∼dπb\n\n∥πb(· | s) − πs(· | s)∥1\n\nEs∼dπb\n\nEs∼dπb\n\n∥T (s)πt(· | s) + (1 − T (s))πs(· | s) − πs(· | s)∥1\n\n∥T (s) [πt(· | s) − πs(· | s)]∥1\n\n(17)\n\nEs∼πb ∥πt(· | s) − πs(· | s)∥1 .\n\n|J (π∗) − J (πs)| ⩽ |J (πb) − J (πs)| + |J (π∗) − J (πb)|\n\n⩽ βRmax (1 − γ)2\n\nEs∼πb ∥πt(· | s) − πs(· | s)∥1 + |J (π∗) − J (πb)| .\n\nTheorem A.5 (Restatement of Thm. 3.3). With the action distributional intervention function Taction(s), the return of the behavior policy J(πb) is lower and upper bounded by\n\n√\n\nJ(πt)+\n\n√\n\n2(1 − β)Rmax (1 − γ)2\n\nH − ε ⩾ J(πb) ⩾ J(πt) −\n\n2(1 − β)Rmax (1 − γ)2 r(s, a) is the maximal possible reward, H = Es∼dπb\n\n√\n\nwhere Rmax = max s,a\n\n√\n\nH − ε\n\n(18)\n\nH(πt(·|s)) is the average\n\nentropy of the teacher policy during shared control.\n\nProof.\n\n|J (πb) − J (πt)| ⩽ Rmax\n\nEs∼dπb\n\nEs∼dπb\n\n(1 − γ)2 Rmax (1 − γ)2 (1 − β)Rmax (1 − γ)2\n\n∥πb(· | s) − πt(· | s)∥1\n\n∥T (s)πt(· | s) + (1 − T (s))πs(· | s) − πt(· | s)∥1\n\nEs∼dπb\n\n∥πs(· | s) − πt(· | s)∥1\n\n2(1 − β)Rmax (1 − γ)2\n\nEs∼dπb\n\n(cid:112)DKL(πt(·|s)∥πs(·|s))\n\n(19)\n\n2(1 − β)Rmax (1 − γ)2\n\nEs∼dπb\n\n(cid:113)\n\nEa∼πt(·|s) [log πt(a|s) − log πs(a|s)]\n\n2(1 − β)Rmax (1 − γ)2\n\nEs∼dπb\n\n(cid:112)H(πt(·|s) − ε\n\n2(1 − β)Rmax (1 − γ)2\n\n√\n\nH − ε.\n\n=\n\n=\n\n⩽\n\n=\n\n=\n\n⩽\n\n√\n\n√\n\n√\n\n√\n\nTherefore, we obtain\n\n√\n\n2(1 − β)Rmax (1 − γ)2 √\n\nJ(πt) +\n\n2(1 − β)Rmax (1 − γ)2\n\n√\n\n√\n\nH − ε ⩾ J (πb) − J (πt) ⩾ −\n\n√\n\nH − ε ⩾J(πb) ⩾ J(πt) −\n\n2(1 − β)Rmax (1 − γ)2 √\n\n√\n\nH − ε\n\n2(1 − β)Rmax (1 − γ)2\n\n(20)\n\n√\n\nH − ε,\n\nwhich concludes the proof.\n\nTo prove Thm. 3.4, we introduce a useful lemma from (Schulman et al., 2015).\n\nLemma A.6.\n\nJ(π) = J(π′) + Est,at∼τπ\n\n(cid:35)\n\nγtAπ′ (st, at)\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\n(21)\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTheorem A.7 (Restatement of Thm. 3.4). With the value-based intervention function Tvalue(s) defined in Eq. 5, the return of the behavior policy πb is lower bounded by\n\nJ (πb) ⩾ J (πt) −\n\nε 1 − γ\n\n.\n\n(22)\n\nProof.\n\nJ(πb) − J(πt) = Esn,an∼τπb\n\n= Esn,an∼τπb\n\n(cid:35)\n\nγnAt (sn, an)\n\nγn [Qt (sn, an) − Vt(sn)]\n\n(cid:35)\n\n(cid:34) ∞ (cid:88)\n\nn=0 (cid:34) ∞ (cid:88)\n\nn=0\n\n= Esn∼τπb\n\n(cid:34) ∞ (cid:88)\n\n(cid:35) γn (cid:2)Ea∼πb(·|sn)Qt (sn, a) − Vt(sn)(cid:3)\n\n= Esn∼τπb\n\n= Esn∼τπb\n\nn=0 (cid:34) ∞ (cid:88)\n\nn=0 (cid:34) ∞ (cid:88)\n\nn=0\n\n(cid:35) γn (cid:2)T (sn)Ea∼πt(·|sn)Qt (sn, a) + (1 − T (sn))Ea∼πs(·|sn)Qt (sn, a) − Vt(sn)(cid:3)\n\nγn (cid:2)(1 − T (sn)) (cid:2)Ea∼πs(·|sn)Qt (sn, a) − Vt(sn)(cid:3)(cid:3)\n\n(cid:35)\n\n= (1 − β)Esn∼τπb\n\n(cid:34) ∞ (cid:88)\n\n(cid:35) γn (cid:2)Ea∼πs(·|sn)Qt (sn, a) − Vt(sn)(cid:3)\n\nn=0 (cid:34) ∞ (cid:88)\n\nn=0\n\n(cid:35)\n\nγnε\n\n⩾ −(1 − β)Esn∼τπb\n\n= −\n\n(1 − β)ε 1 − γ\n\n,\n\nwhich concludes the proof.\n\n(23)\n\nThen we prove the corollary related to safety-critical scenarios.\n\nCorollary A.8 (Restatement of Cor. 3.5). With safety-critical value-based intervention function ˆTvalue(s), the expected cumulative training cost of the behavior policy πb is upper bounded by\n\nC(πb) ⩽ C(πt) +\n\n(1 − β)ε η(1 − γ)\n\n+\n\n1 η\n\n[J(πb) − J(πt)] .\n\n(24)\n\nProof. We define expected return under policy π with combined reward ˆr as ˆJ(π), therefore\n\nˆJ(π) = Es0∼d0,at∼π(·|st),st+1∼p(·|st,at)\n\n= Es0∼d0,at∼π(·|st),st+1∼p(·|st,at)\n\n= J(π) + ηC(π)\n\n(cid:35)\n\nγtˆr (st, at)\n\nγt [r (st, at) + ηc (st, at)]\n\n(cid:35)\n\n(cid:34) ∞ (cid:88)\n\nt=0 (cid:34) ∞ (cid:88)\n\nt=0\n\nAccording to Thm. A.7, under ˆTvalue(s) we have\n\nˆJ (πb) ⩾ ˆJ (πt) −\n\nε 1 − γ\n\n.\n\nEq. 24 can be immediately proved by combining Eq. 25 and Eq. 26.\n\n15\n\n(25)\n\n(26)\n\nPublished as a conference paper at ICLR 2023\n\nA.2 DISCUSSIONS ON THE RESULTS\n\nIn Thm. 3.3, the average entropy of the teacher policy H and the threshold for action-based intervention ε is included in the bound. We provide intuitive interpretations on the influence of H and ε here. For reference, the action-based intervention function Taction = 1 when Ea∼πt(·|s) [log πs(a | s)] < ε. According to Thm 3.3 of our paper, a larger ε leads to smaller discrepancy between the returns of the behavior and teacher policies. This is because ε is the threshold for the action-based intervention function. If the action likelihood is less than ε, the teacher policy will take over the control. A larger ε means more teacher intervention, constraining the behavior policy to be closer to the teacher policy, which leads to a smaller discrepancy in their returns. The influence of H can be similarly analyzed. A larger H leads to larger return discrepancy. Intuitively, this is because with higher entropy, the teacher policy tends to have a more “averaged” or multi-modal distribution over the action space. So the policy distributions of the student and teacher are more likely to have overlaps, leading to a higher action likelihood. In turn, the intervention criterion is less likely to be satisfied, leading to fewer teacher interventions. In general, the intuitive interpretation of Thm. 3.3 indicates that if we would like larger return discrepancy, i.e. larger performance upper bound as well as smaller lower bound, we should use smaller intervention threshold and teacher policy with higher entropy, and vice versa. Thm. 3.3 has a gap with the actual algorithm in that the algorithm uses a value-based intervention function which is based on Thm. 3.4. Nevertheless, the intuitive interpretation may enlighten future work on how to choose a proper teacher policy in teacher-student shared control.\n\n1\n\nWith respect to the tightness, Thm. 3.3 has a squared planning horizon (1−γ)2 in the discrepancy term. This is in accordance with many previous works (Thm. 1 in (Xu et al., 2019), Thm. 4.1 in (Janner et al., 2019) and Thm. 1 in (Schulman et al., 2015)), which include (1 − γ)2 in the denominator when it comes to differences of the cumulative return, given the difference in the action distribution. The order of 1−γ in Thm. 3.3 is tight, which dominates the gap in accumulated return. Nevertheless, the other constant terms, e.g. Rmax and the average entropy, can be tighter given some additional assumptions. We did not derive a tighter bound since the derivation will not be related to the main contribution of this paper, which is the new type of intervention function. Thm. 3.3 and Thm. 3.4 in their current forms are enough to demonstrate that the value-based intervention function has the advantage of providing more efficient exploration and better safety guarantee compared with action-based intervention function.\n\n1\n\nB THE ALGORITHM\n\nThe workflow of TS2C during training is show in Alg. 1.\n\nAlgorithm 1 The workflow of TS2C during training\n\n1: Input: Warmup steps W ; Scale of warmup noise σ; Training steps N ; Teacher policy πt. 2: Initialize student policy πθ\n\ns , a set of parameterized Q-function for teacher policy Qφ, parame-\n\nterized Q-function for student policy Qψ and the replay buffer D.\n\nObserve state si and sample ai ∼ πt(·|s) + N (0, σ). Step the environment with ai and store the tuple (si, ai, ri, si+1) to D. Update φ with Temporal-Difference loss in Eq. 8.\n\nObserve state si and sample at ∼ πt(·|si), as ∼ πθ Compute Tts2c(si) with Eq. 9, behavior policy πb(·|si) and ab. Step the environment with ab and store the tuple (si, ab, ri, si+1, Tvalue(si+1)) to D. Update ψ in the student Q-function with the loss in Eq. 10. Update θ in the student policy with the loss in Eq. 11.\n\ns (·|si).\n\n3: for i = 1 to W do 4: 5: 6: 7: end for 8: for i = 1 to N do 9: 10: 11: 12: 13: 14: end for\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC ADDITIONAL EXPERIMENT DEMONSTRATIONS\n\nC.1 DEMONSTRATIONS OF DRIVING SCENARIOS\n\nThe demonstrations of several driving scenarios are shown in Fig. 8. We provide a demonstrative video showing the agent behavior trained with PPO and our TS2C algorithm in the supplementary materials.\n\nFigure 8: Four examples of the traffic scenes in MetaDrive.\n\nC.2 HYPER-PARAMETERS\n\nThe hyper-parameters used in the experiments are shown in the following tables. In the TS2C algorithm, larger values of the intervention threshold ε1 and ε2 will lead to a more strict intervention criterion and the steps with teacher control will be fewer. In order to control the policy distribution discrepancy, we choose ε1 and ε2 to ensure the average intervention rate to be less than 5%. Nevertheless, different ε1 in the intervention function has little influence on the algorithm performance, as shown in Fig. 11 of our paper. The coefficient for intervention minimization λ is simply set to 1. If used in other environments, it may need some adjustments to fit the reward scale. The coefficient for maximum entropy learning α is updated during training as in the SAC algorithm. The number of warmup timesteps is empirically chosen so that the expert value function can be properly trained. Other parameters follow the setting in EGPO (Peng et al., 2021). The hyper-parameters of other algorithms follow their original setting.\n\nTable 1: TS2C (Ours)\n\nHyper-parameter\n\nDiscount Factor γ τ for target network update Learning Rate Environmental horizon T Warmup Timesteps W # of Ensembled Value-Functions N Variance of Gaussian Noise C Intervention Minimization Ratio λ Value-based Intervention Threshold ε1 Value-based Intervention Threshold ε2 Activation Function Hidden Layer Sizes\n\nValue\n\n0.99 0.005 0.0001 2000 50000 10 0.5 1\n1.2 2.5 Relu [256, 256]\n\n17\n\nTable 2: EGPO (Peng et al., 2021)\n\nHyper-parameter\n\nDiscount Factor γ τ for target network update Learning Rate Environmental horizon T Steps before Learning start Intervention Occurrence Limit C Number of Online Evaluation Episode Kp Ki Kd CQL Loss Temperature β Activation Function Hidden Layer Sizes\n\nValue\n\n0.99 0.005 0.0001 2000 10000 20 5\n5 0.01 0.1 3.0 Relu [256, 256]\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Taylor, 2013)\n\nImportance Advising (Torrey &\n\nHyper-parameter\n\nDiscount Factor γ τ for target network update Learning Rate Environmental horizon T Warmup Timesteps W # of Actions Sampled N Variance of Gaussian Noise C Range-based Intervention Threshold ε Activation Function Hidden Layer Sizes\n\nValue\n\n0.99 0.005 0.0001 2000 50000 10 0.5 2.8 Relu [256, 256]\n\nTable 4: SAC (Haarnoja et al., 2018)\n\nHyper-parameter\n\nDiscount Factor γ τ for Target Network Update Learning Rate Environmental Horizon T Steps before Learning starts Activation Function Hidden Layer Sizes\n\nValue\n\n0.99 0.005 0.0001 2000 10000 Relu [256, 256]\n\nD ADDITIONAL EXPERIMENT RESULTS\n\nD.1 ADDITIONAL PERFORMANCE COMPARISONS ON METADRIVE\n\nIn Fig. 9, we show the results of TS2C trained with various levels of teachers compared with baseline algorithms without shared control. Apart from the Fig. 4 in the main paper presenting the training results of TS2C with the medium level of teacher policy, here we present the performance of TS2C trained with the high, medium and low levels of teacher policy. The value-based intervention proposed by TS2C can utilize all these teacher policies, leading to safer and more efficient training compared to traditional RL algorithms.\n\nFig. 10 shows the results with different levels of teacher policy. Besides the testing reward and the training cost shown in Fig. 3 of the main paper, we show the training reward and test success rate of TS2C compared with baseline methods with the Teacher-Student Framework (TSF) respectively. Our TS2C algorithm still achieves the best performance among baseline algorithms when evaluated with these two metrics.\n\nFigure 9: Comparison of training cost and test reward between our method TS2C and other algorithms without shared control.\n\n18\n\n0.00.51.01.52.0Sampled Steps1e502468Training Cost with Teacher-HighTS2CSACPPO0.00.51.01.52.0Sampled Steps1e502468Training Cost with Teacher-Medium0.00.51.01.52.0Sampled Steps1e502468Training Cost with Teacher-Low0.00.51.01.52.0Sampled Steps1e50100200300Test Reward with Teacher-HighTS2CSACPPOBC0.00.51.01.52.0Sampled Steps1e50100200300Test Reward with Teacher-Medium0.00.51.01.52.0Sampled Steps1e50100200300Test Reward with Teacher-LowPublished as a conference paper at ICLR 2023\n\nFigure 11: Ablation Studies for different variance thresholds, the intervention cost, ensembled value networks and different intervention functions.\n\nFigure 10: Comparison of training reward and test success rate between our method TS2C and other algorithms with shared control.\n\nD.2 ABLATION STUDIES\n\nWe conduct ablation studies and present the results in Fig. 11. We find the intervention cost and ensembled value networks are important to the algorithm’s performance, while different variance thresholds in the intervention function has little influence. Also, TS2C with action-based intervention function behaves poorly in accordance with the theoretical analysis in Section 3.2.\n\nD.3 DISCUSSIONS ON EXPERIMENT RESULTS\n\nIn Fig. 5, our TS2C algorithm can outperform SAC in all three MuJoCo environments taken into consideration. On the other hand, though the EGPO algorithm has the best performance in the Pendulum environment, it struggles in the other two environments, namely Hopper and Walker. In this This is because the action space of the pendulum environment is only one-dimensional. simple environment, the action-based intervention of the EGPO algorithm is effective. The policy only needs slight adjustments based on the imperfect teacher to work properly. In other words, the distance between the optimal action and the teacher action is small. However, in more complex environments like Hopper and Walker, the distance between the two is large. As the action-based\n\n19\n\n=2.0=2.5=3.0Variance threshold 0100200300Test Reward at 100k Stepsw/ICw/o ICWith or withoutintervention cost (IC)0100200300Test Reward at 100k Stepsw/EVNw/o EVNWith or withoutensembled value networks (EVN)0100200300Test Reward at 100k StepsValueActionIntervention Function0100200300Test Reward at 100k Steps0.00.51.01.52.0Sampled Steps1e50100200300Training Reward with Teacher-HighTS2CEGPOImportance0.00.51.01.52.0Sampled Steps1e50100200300Training Reward with Teacher-Medium0.00.51.01.52.0Sampled Steps1e50100200300Training Reward with Teacher-Low0.00.51.01.52.0Sampled Steps1e50.00.20.40.60.81.0Test Success Rate with Teacher-HighTS2CEGPOImportance0.00.51.01.52.0Sampled Steps1e50.00.20.40.60.81.0Test Success Rate with Teacher-Medium0.00.51.01.52.0Sampled Steps1e50.00.20.40.60.81.0Test Success Rate with Teacher-LowPublished as a conference paper at ICLR 2023\n\nintervention is too restrictive, the EGPO algorithm based on such intervention fails to achieve good performance.\n\nIn Fig. 6, the performance of EGPO with a SAC policy as the teacher policy is very poor. This is because the employed SAC teacher is less stochastic than the PPO policy. Student’s actions have less likelihood in teacher’s action distribution and are less tolerated by the action-based intervention function in EGPO, leading to large intervention rate and consequently large distributional shift. Our proposed TS2C algorithm does not access teacher internal action distribution and instead intervenes based on the state-action values of teacher policy, so it is robust to the stochasticity of teacher policy.\n\n20",
    "reference": "# Summary Of The Paper\n\nThe paper studies the Teacher-Student Framework, in the case when the teacher is potentially sub-optimal. Learning here is based on an ensemble off-policy method. The core concept is to develop a teach intervention function that is based on the estimated sub-optimality of student actions with respect to the teacher's value function. The advantage of the proposed method is that it allows for continuous student improvement, to the point that the student takes complete control in the case of sub-optimal teacher. The authors provide theoretical bounds for the proposed method and evaluate it on a driving simulator. The proposed method 1) outperforms baselines across evaluation scenarios and 2) demonstrates the student capability to solve the task, even with a significantly sub-optimal teacher.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1. The proposed algorithm is intuitive and straight-forward\n2. It asymptotically outperforms baselines\n3. The proposed method is backed by theoretical analysis\n\nWeaknesses\n\n1. It seems that the algorithm takes some time before it starts improving and is initially substantially outperformed by EGPO. Is this because at the start of the training the switch allows most student actions, while EGPO mostly uses the teacher?\n\nQuestion:\n\nWhat is the effect of the optimization algorithm here? The proposed method uses Q-ensembles, which combined with more often training have demonstrated significant improvement in sample complexity (RedQ). What update frequency was used in this paper and is this directly comparable to the baseline methods?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written and clear. The core concept of agent switches here is not new, but the particular implementation allows the student to learn and eventually outperform a sub-optimal teacher, unlike baseline methods.\n\n# Summary Of The Review\n\nSimple and intuitive method to allow student agent to learn and outperform a potentially sub-optimal teacher. Clear writing with good theoretical backing.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nINSTRUCTION-FOLLOWING AGENTS WITH JOINTLY PRE-TRAINED VISION-LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHumans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a difficult challenge. Prior work that uses pure language-only models lack visual grounding, making it difficult to connect language instructions with visual observations. On the other hand, methods that use pre-trained vision-language models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in visionbased environments. Our InstructRL method consists of a multimodal transformer that encodes visual observations and language instructions, and a policy transformer that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The policy transformer keeps track of the full history of observations and actions, and predicts actions autoregressively. We show that this unified transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work.1\n\n1\n\nINTRODUCTION\n\nHumans are able to understand language and vision to accomplish a wide range of tasks. Many tasks require language understanding and vision perception, from driving to whiteboard discussion and cooking. Humans can also generalize to new tasks by building upon knowledge acquired from previously-seen tasks. Meanwhile, creating generic instruction-following agents that can generalize to multiple tasks and environments is one of the central challenges of reinforcement learning (RL) and robotics.\n\nDriven by significant advances in learning generic pre-trained models for language understanding (Devlin et al., 2018; Brown et al., 2020; Chowdhery et al., 2022), recent work has made great progress towards building instruction-following agents (Lynch & Sermanet, 2020; Mandlekar et al., 2021; Ahn et al., 2022; Jang et al., 2022; Guhur et al., 2022; Shridhar et al., 2022b). For example, SayCan (Ahn et al., 2022) exploits PaLM models (Chowdhery et al., 2022) to generate language descriptions of step-by-step plans from language instructions, then executes the plans by mapping the steps to predefined macro actions. HiveFormer (Guhur et al., 2022) uses a pre-trained language encoder to generalize to multiple manipulation tasks. However, a remaining challenge is that pure language-only pre-trained models are disconnected from visual representations, making it difficult to differentiate vision-related semantics such as colors. Therefore, visual semantics have to be further learned to connect language instructions and visual inputs.\n\nAnother category of methods use pre-trained vision-language models, which have shown great success in joint visual and language understanding (Radford et al., 2021). This has made tremendous progress towards creating a general RL agent (Zeng et al., 2022; Khandelwal et al., 2022; Nair et al., 2022b; Khandelwal et al., 2022; Shridhar et al., 2022a). For example, CLIPort (Shridhar et al., 2022a) uses CLIP (Radford et al., 2021) vision encoder and language encoder to solve manipulation\n\n1The code of InstructRL is available at https://sites.google.com/view/instructrl/\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Examples of RLBench tasks considered in this work. Left: InstructRL can perform multiple tasks from RLBench given language instructions, by leveraging the representations of a pre-trained vision-language transformer model, and learning a transformer policy. Right: Each task can be composed of multiple variations that share the same skills but differ in objects. For example, in the block stacking task, InstructRL can generalize to varying colors and ordering of the blocks.\n\ntasks. However, a drawback is that they come with limited language understanding compared to pure language-only pre-trained models like BERT (Devlin et al., 2018), lacking the ability to follow long and detailed instructions. In addition, the representations of visual input and textual input are often disjointly learned, so such methods typically require designing specialized network architectures on top of the pre-trained models to fuse them together.\n\nTo address the above challenges, we introduce InstructRL, a simple yet effective method based on the multimodal transformer (Vaswani et al., 2017; Tsai et al., 2019). It first encodes fine-grained cross-modal alignment between vision and language using a pre-trained multimodal encoder (Geng et al., 2022), which is a large transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al., 2021; Thomee et al., 2016) and text-only data (Devlin et al., 2018). The generic representations of each camera and instructions form a sequence, and are concatenated with the embeddings of proprioception data and actions. These tokens are fed into a multimodal policy transformer, which jointly models dependencies between the current and past observations, and cross-modal alignment between instruction and views from multiple cameras. Based on the output representations from our multimodal transformer, we predict 7-DoF actions, i.e., position, rotation, and state of the gripper.\n\nWe evaluate InstructRL on RLBench (James et al., 2020), measuring capabilities for single-task learning, multi-task learning, multi-variation generalization, long instructions following, and model scalability. On all 74 tasks which belong to 9 categories (see Figure 1 for example tasks), our InstructRL significantly outperforms state-of-the-art models (Shridhar et al., 2022a; Guhur et al., 2022; Liu et al., 2022), demonstrating the effectiveness of joint vision-language pre-trained representations. Moreover, InstructRL not only excels in following basic language instructions, but is also able to benefit from human-written long and detailed language instructions. We also demonstrate that InstructRL generalizes to new instructions that represent different variations of the task that are unseen during training, and shows excellent model scalability with performance continuing to increase with larger model size.\n\n2 RELATED WORK\n\nLanguage-conditioned RL with pre-trained language models. Pre-trained language models have been shown to improve the generalization capabilities of language-conditioned agents to new instructions and to new low-level tasks (Lynch & Sermanet, 2020; Hill et al., 2020; Nair et al., 2022a; Jang et al., 2022; Ahn et al., 2022; Huang et al., 2022). Some prior work use prompt engineering with large language models (Brown et al., 2020; Chowdhery et al., 2022) to extract temporally extended plans over predefined skills (Huang et al., 2022; Ahn et al., 2022; Jiang et al., 2022), similar to work that decomposes high-level actions into sub-goals (Team et al., 2021). These work rely purely on language models to drive agents and require converting observations into language through predefined APIs. Others combine pre-trained language representations with visual inputs (Jang et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Different frameworks of leveraging pre-trained representations for instruction-following agents. In prior work, additional training-from-scratch is needed to combine the representations of text and image from (I) a pre-trained vision model, (II) a pre-trained language model, or (III) disjointly pre-trained language and vision models. In contrast, InstructRL extracts generic representations from (IV) a jointly pre-trained vision-language model.\n\n2022; Lynch & Sermanet, 2020; Team et al., 2021; Khandelwal et al., 2022) using specialized architectures such as UNet (Ronneberger et al., 2015) and FiLM (Perez et al., 2018). Such approaches have demonstrated success in solving challenging robotic manipulation benchmarks (Guhur et al., 2022; Shridhar et al., 2022b). In this work, we argue that using jointly pre-trained vision-language representations with RL can achieve superior performance in solving complex language-specified tasks, and show that our proposed approach enjoys better scalability and simpler architecture. Our work is also complementary to prompt-based methods, e.g., SayCan (Ahn et al., 2022). This work focuses on improving the mapping from language instructions to robot actions, and we expect that combining our approach with prompt-based methods can achieve greater success.\n\nLanguage-conditioned RL with pre-trained vision-language models. There have been strong interests in leveraging pre-trained vision-language models for language-conditioned RL (Shridhar et al., 2022a; Zeng et al., 2022; Khandelwal et al., 2022), motivated by the effectiveness of visionlanguage models such as CLIP (Radford et al., 2021). However, these methods use disentangled pipelines for visual and language input, with the language primarily being used to guide perception. Our work uses jointly pre-trained vision-language models that comes with better grounding text-tovisual content (Geng et al., 2022). The effectiveness of such jointly pre-trained models enables a simple final model, which is a jointly pre-trained vision-language transformer followed by a policy transformer. While using pretrained vision-language models has been explored in grounded navigation (Guhur et al., 2021; Hao et al., 2020; Majumdar et al., 2020; Shah et al., 2022), our work focuses on manipulation tasks which have combinatorial complexity (composition of objects/actions). Moreover, in contrast to these methods, our method InstructRL is a simple architecture that scales well to large-scale tasks and can directly merge long complex language instructions with visual input.\n\nRL with Transformers. Transformers (Vaswani et al., 2017) have led to significant gains in natural language processing (Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al., 2019; Radford et al., 2021; Geng et al., 2022). They have also been used in the context of supervised reinforcement learning (Chen et al., 2021a; Reed et al., 2022), vision-language navigation (Chen et al., 2021b; Shah et al., 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), and language-conditioned RL (Guhur et al., 2022; Shridhar et al., 2022a). Inspired by their success, we leverage the transformer architecture to extract pre-trained representations from language and vision and learn a language-conditioned policy.\n\n3 PROBLEM DEFINITION\n\nWe consider the problem of robotic manipulation from visual observations and natural language instructions. We assume the agent receives a natural language instruction x := {x1, . . . , xn} con-\n\n3\n\nPlace 3 of the red cubes on ..Place 3 of the red cubes on ..Place 3 of the red cubes on ..Place 3 of the red cubes on ..Under review as a conference paper at ICLR 2023\n\nFigure 3: InstructRL is composed of a vision-language transformer and a policy transformer. First, the instruction (text) and multi-view image observations are jointly encoded using the pre-trained vision-language transformer. Next, the sequence of representations and a history of actions are encoded by the policy transformer to predict the next action.\n\nsisting of n text tokens. At each timestep t, the agent receives a visual observation ot ∈ O and takes an action at ∈ A in order to solve the task specified by the instruction. (cid:1) as a transformer model, which is condiWe parameterize the policy π (cid:0)at | x, {oi}t tioned on the instruction x, observations {oi}t i=1. For robotic control, we use macro steps (James & Davison, 2022), which are key turning points in the action trajectory where the gripper changes its state (open/close) or the joint velocities are set to near zero. Following James & Davison (2022), we employ an inverse-kinematics based controller to find a trajectory between macro-steps. In this way, the sequence length of an episode is significantly reduced from hundreds of small steps to typically less than 10 macro steps.\n\ni=1, and previous actions {ai}t−1\n\ni=1, {ai}t−1\n\ni=1\n\nObservation space: Each observation ot consists of images {ck k=1 taken from K different camera viewpoints, as well as proprioception data oP t is an RGB image of size 256 × 256 × 3. We use K = 3 camera viewpoints located on the robot’s wrist, left shoulder, and right shoulder. The proprioception data oP t consists of 4 scalar values: gripper open, left finger joint position, right finger joint position, and timestep of the action sequence. Note that we do not use point cloud data in order for our method to be more flexibly applied to other domains. Since RLBench consists of sparse-reward and challenging tasks, using point cloud data can benefit performance (James & Davison, 2022; Guhur et al., 2022), but we leave this as future work.\n\nt ∈ R4. Each image ck\n\nt }K\n\nAction space: Following the standard setup in RLBench (James & Davison, 2022), each action at := (pt, qt, gt) consists of the desired gripper position pt = (xt, yt, zt) in Cartesian coordinates and quaternion qt = (q0 t ) relative to the base frame, and the gripper state gt indicating whether the gripper is open or closed. An object is grasped when it is located in between the gripper’s two fingers and the gripper is closing its grasp. The execution of an action is achieved by a motion planner in RLBench.\n\nt , q3\n\nt , q1\n\nt , q2\n\n4\n\nINSTRUCTRL\n\nWe propose a unified architecture for robotic tasks called InstructRL, which is shown in Figure 3. It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al., 2017) policy that predicts actions. First, the feature encoding module (Sec. 4.1) generates token i=1, and previous actions {ai}t−1 embeddings for language instructions {xj}n i=1. Then, given the token embeddings, the multimodal transformer policy (Sec. 4.2) learns relationships between the instruction, image observations, and the history of past observations and actions, in order to predict the next action at.\n\nj=1 , observations {oi}t\n\n4\n\nPlace 3 of the red cubes on ...........Place 3 of the red cubes on ..Under review as a conference paper at ICLR 2023\n\nFigure 4: The architecture of the transformer policy. The model is conditioned on a history of language-vision representations and actions to predict the next action.\n\n4.1 FEATURE ENCODING WITH PRE-TRAINED TRANSFORMER\n\nWe encode the instruction and visual observations using a pre-trained vision-language encoder, Specifically, we use a pre-trained multimodal masked autoencoder as shown in Figure 3. (M3AE) (Geng et al., 2022) encoder, which is a large transformer-based architecture based on ViT (Dosovitskiy et al., 2020) and BERT (Devlin et al., 2018). Specifically M3AE (Geng et al., 2022) is a transformer-based architecture that learns a unified encoder for both vision and language data via masked token prediction. It is trained on a large-scale image-text dataset(CC12M (Changpinyo et al., 2021)) and text-only corpus (Devlin et al., 2018) and is able to learn generalizable representations that transfer well to downstream tasks.\n\nEncoding Instructions and Observations. Following the practice of M3AE, we first tokenize the language instructions {xj}n j=1 into embedding vectors and then apply 1D positional encodings. We denote the language instructions as Ex ∈ Rn×d, where n is the length of language tokens and d is the embedding dimension.\n\nWe divide each image observation in {ck k=1 into image patches, and use a linear projection to convert them to image embeddings that have the same dimension as the language embeddings. Then, we apply 2D positional encodings. Each image is represented as Ec ∈ Rlc×de where lc is the length of image patches tokens and de is the embedding dimension.\n\nt }K\n\nThe image embeddings and text embeddings are then concatenated along the sequence dimension: E = concat(Ec, Ex) ∈ R(lc+n)×de . The combined language and image embeddings are then t ∈ R(lc+n)×de. processed by a series of transformer blocks to obtain the final representation ˆok Following the practice of VIT and M3AE, we also apply average pooling on the sequence length dimension of ˆok t and the t ∈ Rd which are a concatenation of all intermediate instruction. We use multi-scale features hk layer representations, where the feature dimension d = L ∗ de equals the number of intermediate layers L times embedding dimension de. Finally, we can get the representations over all K camera viewpoints ht = {h1\n\nt ∈ Rde as the final representation of the k-th camera image ck\n\nt } ∈ RK×d as the representation of the vision-language input.\n\nt to get ok\n\nt , · · · , hK\n\nEncoding Proprioceptions and Actions. The proprioception data oP linear layer to upsample the input dimension to d (i.e., each scalar in oP representation zt = {z1 feature space ft ∈ Rd.\n\nt } ∈ R4×d all each state in oP\n\nt , · · · , z4\n\nt ∈ R4 is encoded with a t is mapped to Rd) to get a t . Similarly, the action is projected to\n\n4.2 TRANSFORMER POLICY\n\nWe consider a context-conditional policy, which takes all encoded instructions, observations and actions as input, i.e., {(hi, zi)}t i=1. By default, we use context length 4 throughout the paper (i.e., 4(K + 5) embeddings are processed by the transformer policy). This enables learning relationships among views from multiple cameras, the current observations and instructions, and between the current observations and history for action prediction. The architecture of transformer policy is illustrated in Figure 4.\n\ni=1 and {fi}t−1\n\nWe pass the output embeddings of the transformer into a feature map to predict the next action at = [pt; qt; gt]. We use behavioral cloning to train the models. In RLBench, we generate D, a collection of N successful demonstrations for each task. Each demonstration δ ∈ D is composed of a sequence of (maximum) T macro-steps with observations {oi}T i=1 and instructions {xl}n j=1 ⊂ D. The\n\nl=1. We minimize a loss function L over a batch of demonstrations B = {δj}|B|\n\ni=1, actions {a∗\n\ni }T\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nloss function is the mean-square error (MSE) on the gripper’s action:\n\nL =\n\n1 |B|\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\nδ∈B\n\nt≤T\n\n\n\nMSE (at, a∗ t )\n\n .\n\n(1)\n\n5 EXPERIMENTAL SETUP\n\nTo evaluate the effectiveness of our method, we run experiments on RLBench (James et al., 2020), a benchmark of robotic manipulation task (see Figure 1). We use the same setup as in Guhur et al. (2022), including the same set of 74 tasks with 100 demonstrations per task for training, and the same set of instructions for training and inference, unless stated otherwise. We group the 74 tasks into 9 categories according to their key challenges (see Appendix A.7 for each category’s description and list of tasks).\n\nFor each task, there are a number of possible variations, such as the shapes, colors, and ordering of objects; the initial object positions; and the goal of the task. These variations are randomized at the start of each episode, during both training and evaluation. Based on the task and variation, the environment generates a natural language task instruction using a language template (see Appendix A.3).\n\nWe compare InstructRL with strong baseline methods from three categories:\n\n• RL with pre-trained language model: HiveFormer (Guhur et al., 2022) is a state-of-the-art method for instruction-following agents that uses a multimodal transformer to encode multicamera views, point clouds, proprioception data, and language representations from a pre-trained CLIP language encoder (Radford et al., 2021). We report the published results of HiveFormer unless otherwise mentioned.\n\n• RL with pre-trained vision-language model: CLIP-RL is inspired by CLIPort (Shridhar et al., 2022a), which demonstrates the effectiveness of CLIP for robotic manipulation. CLIP-RL uses concatenated visual- and language-representations from a pre-trained CLIP model. Similar to InstructRL, CLIP-RL uses multi-scale features by concatenating intermediate layer representations, and is trained using the same hyperparameters.\n\n• RL trained from scratch: Auto-λ (Liu et al., 2022) is a model trained from scratch that uses the UNet network (Ronneberger et al., 2015) and applies late fusion to predictions from multiple views. We report the published results of Auto-λ unless mentioned otherwise.\n\nInstructRL uses the official pre-trained multimodal masked autoencoder (M3AE) model (Geng et al., 2022), which was jointly pre-trained on a combination of image-text datasets (Changpinyo et al., 2021; Thomee et al., 2016) and text-only corpus (Devlin et al., 2018). CLIP-RL uses the official pre-trained CLIP models. See Appendix A.2 for more details about the pre-training datasets.\n\nAll models are trained for 100K iterations. For evaluation, we measure the per-task success rate for 500 episodes per task. Further implementation and training details can be found in Appendix A.1.\n\n6\n\nEXPERIMENTAL RESULTS\n\nWe evaluate the methods on single-task performance (Figure 5), multi-task performance (Figures 6, 7), generalization to unseen instructions and variations (Figure 8a), and scalability to larger model size (Figure 8b). In all metrics, we find that InstructRL outperforms state-of-the-art baselines despite being a simpler method.\n\nSingle-task performance. Figure 5 shows that, across all 9 categories of tasks, InstructRL performs on par or better than all state-of-the-art baselines. On average, InstructRL significantly outperforms prior work despite being much simpler.\n\nMulti-task performance. In the multi-task setting, each model is trained on a set of tasks and then evaluated on each of the training tasks. In Figure 6, we compare multi-task performance on 10 RLBench tasks considered in HiveFormer (Guhur et al., 2022) and Auto-λ (Liu et al., 2022), with\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Single-task performance on 74 RLBench tasks from James et al. (2020); Guhur et al. (2022) grouped into 9 categories.\n\nFigure 6: Multi-task performance on 10 RLBench tasks from Guhur et al. (2022); Liu et al. (2022).\n\n100 training demonstrations per task. InstructRL exhibits strong multi-task performance compared to other methods. In particular, even though both InstructRL and HiveFormer use a transformer policy with language instructions, InstructRL outperforms HiveFormer by a large margin. In Figure 7, we further evaluate multi-task performance on all 74 RLBench tasks, and see that InstructRL outperforms CLIP-RL in most categories. These results demonstrate the transferability of jointly pre-trained vision-language models to diverse multi-task settings.\n\nGeneralization to unseen instructions and variations. In Figure 8a, we report the performance on the Push Buttons task, which requires the agent to push colored buttons in a specified sequential order given in the instruction. In this task, instructions are necessary to solve the unseen task variations correctly (e.g., pushing the buttons in a blue-red-green vs. red-green-blue order cannot be inferred from only looking at the scene). We evaluate the models on instructions that are both seen and unseen during training; unseen instructions can contain new colors, or an unseen ordering of colors. We see that InstructRL achieves higher performance on both seen and unseen instructions, even in the most challenging case where only 10 demonstrations are available per variation.\n\nScalability to larger model size. One of the key benefits of pre-trained models is that we can use a huge amount of pre-training data that is typically not affordable in RL and robot learning scenarios. In Figure 8b, we evaluate different model sizes on multi-task performance across 14 selected tasks (listed in Appendix A.5). For fair comparison with training-from-scratch, we fix the size of the policy transformer, and only vary the size of the transformer encoder. We compare four model sizes: B/32, B/16, L/16, and H/16, where “B” denotes ViT-base; “L” denotes ViT-large; “H” denotes ViT-huge; and “16” and “32” denote patch sizes 16 and 32, respectively.\n\nBoth CLIP-RL and InstructRL improve with larger model size, but InstructRL shows better scalability.2 On the other hand, learning-from-scratch is unable to benefit from larger model capacity; in fact, a strong weight decay was needed to prevent this baseline from overfitting to the limited data.\n\n2We only report the performances of CLIP-RL with B/32 and B/16, since the larger models (L/16 and H/16)\n\nare not provided in the open-source released models.\n\n7\n\nCategory NameSuccess Rate (%)0255075100PlanningToolsLong TermRotation InvariantMotion PlanningScrewMulti ModalPercisionVisual OcclusionAverageAuto-λHiveFormerCLIP-RLInstructRLTask NameSuccess Rate (%)102030405060708090100Pick & LiftPick-up CupPush ButtonPut KnifePut MoneyReach TargetSlide BlockStack WineTake MoneyTake UmbrellaAverageAuto-λHiveFormerCLIP-RLInstructRLUnder review as a conference paper at ICLR 2023\n\nFigure 7: Multi-task performance on all 74 RLBench tasks grouped into 9 categories.\n\n# Demos per Variation\n\nPerformance (Success %)\n\nHiveFormer CLIP-RL InstructRL\n\nSeen Instr.\n\nUnseen Instr.\n\n10 50 100\n\n10 50 100\n\n96.8 99.6 100.\n\n73.1 83.3 86.4\n\n95.4 98.7 100.\n\n76.8 84.5 85.4\n\n97.8 100. 100.\n\n81.5 89.4 88.9\n\n(a) Success (%) for seen and unseen instructions\n\n(b) Multi-task performance vs. model size\n\nFigure 8: (a): Performance on the Push Buttons task, which has many variations on the ordering of colored buttons. InstructRL achieves higher performance on both seen and unseen instructions, even with only 10 training demonstrations per variation. (b): We compare four different model sizes of the transformer encoder. All methods are evaluated on multi-task performance across 14 selected tasks (listed in Appendix A.5). InstructRL scales well with larger model size.\n\n7 ABLATION STUDY\n\nInstructions are more effective with joint vision-language model. Table 1 shows models trained with and without instructions, on 10 RLBench tasks from Guhur et al. (2022); Liu et al. (2022). While using instructions improves the performance of all methods, they are most effective with InstructRL. This is not surprising, as the CLIP language encoder in HiveFormer was not pre-trained on large text corpora. One can pair CLIP with a large language model such as BERT, but doing so can often hurt performance (Guhur et al., 2022) due to the language representations not being aligned with the visual representations. On the other hand, InstructRL shows the strongest performance due to using the M3AE encoder, which was jointly pre-trained on both image-text and text-only data.\n\nDetailed instructions require language understanding. In Table 2, we also evaluate the methods on longer and more detailed instructions, which include step-by-step instructions specific to the task (see Table 3 for examples). The detailed instructions are automatically generated using a template that contains more details than the default instruction template from RLBench (“Default”). Each “Tuned Short Instruction” has maximum token length 77 (to be compatible with CLIP’s language encoder), while each “Tuned Long Instruction” can have up to 256 tokens (to be compatible with InstructRL).\n\nWe compare InstructRL with CLIP-RL, which uses concatenated visual- and languagerepresentations from CLIP, as well as a variant of CLIP-RL called BERT-RL, which uses a BERT language encoder and a trained-from-scratch vision encoder. Generally across all methods, we can see that performance increases with longer and more detailed instructions, which implies that the pre-trained language encoders can extract task-relevant information from language instructions. InstructRL achieves the best performance, especially with longer instructions (e.g., InstructRL achieves 63.7 vs. 46.1 from BERT-RL, on the Push Buttons task with Tuned Long Instructions).\n\nFusion strategy. How to fuse representations from multimodal data is one of the key design choices in InstructRL. In Figure 9a, we compare variants of InstructRL with other fusion strategies: Concat\n\n8\n\nCategory NameSuccess Rate (%)0255075PlanningToolsLong TermRotation InvariantMotion PlanningScrewMulti ModalPercisionVisual OcclusionAvgerageCLIP-RLInstructRLModel sizeSuccess Rate (%)0.010.020.030.040.050.060.070.0B/32B/16L/16H/16From scartchCLIP-RLInstructRLUnder review as a conference paper at ICLR 2023\n\nTable 1: Success rates (%) of all methods with and without instructions on 10 RLBench tasks.\n\nPick & Lift\n\nPick-Up Cup\n\nPush Button\n\nPut Knife\n\nPut Money\n\nReach Target\n\nSlide Block\n\nStack Wine\n\nTake Money\n\nTake Umbrella\n\nAuto-λ\n\nHiveFormer CLIP-RL w/o inst CLIP-RL InstructRL w/o inst InstructRL\n\n82\n\n92.2 78.3 80.4 75.9 97.8\n\n72\n\n77.1 65.1 72.4 61.2 84.5\n\n95\n\n99.6 87.6 95.2 85.3 99.5\n\n36\n\n69.7 42 72.1 48.9 84.5\n\n31\n\n96.2 44.5 62.1 48.9 98.7\n\n100\n\n100 98.9 100 98.7 100\n\n36\n\n23\n\n95.4 52.9 56.8 54.5 97.5\n\n81.9 28.9 56.5 29.5 93.2\n\n38\n\n82.1 41.2 69.1 43.2 89.8\n\n37\n\n90.1 45.1 78.8 47.8 92.94\n\nAvg.\n\n55\n\n88.4 58.5 74.3 59.4 93.8\n\nTable 2: Comparison between using default instructions vs. longer and more detailed instructions.\n\nPush Buttons\n\nLight Bulb In\n\nDefault\n\nTuned Short Inst\n\nTuned Long Inst\n\nDefault\n\nTuned Short Inst\n\nTuned Long Inst\n\nBERT-RL CLIP-RL InstructRL\n\n41.3 45.4 52.6\n\n43.1 51.2 53.2\n\n46.1 N/A 63.7\n\n13.6 16.4 22.6\n\n12.2 20.2 26.1\n\n15.9 N/A 31.8\n\nTable 3: Longer instructions for Push Buttons and Stack Blocks.\n\nTask\n\nInstructions type\n\nExample instructions\n\nPush Buttons\n\nStack Blocks\n\nDefault\n\nShort Tuned Inst\n\nLong Tuned Inst\n\nDefault\n\nShort Tuned Inst\n\nLong Tuned Inst\n\nPush the red button, then push the green button, then push the yellow button Move the gripper close to red button, then push the red button, after that, move the gripper close to the green button to push the green button, finally, move the gripper close to the yellow button to push the yellow button. Move the white gripper closer to red button, then push red button down, after pushing red button, pull the white gripper up and move the gripper closer to green button, then push green button down, after pushing green button, pull the white gripper up and move the gripper closer to the yellow button, then push yellow button down\n\n.\n\nPlace 3 of the red cubes on top of each other Choose a red cube as the stack base, then pick another red cube and place it onto the red cube stack, repeat until the stack has 3 red cubes. Choose a red cube as the stack base, then pick another red cube and place it onto the red cube stack, then move the white gripper to pick another red cube and place it onto the red cube stack, repeat until the stack has 3 red cubes.\n\nLength\n\nSuccess %\n\nRepresentation selection\n\nSuccess Rate\n\n1 2\n4 8\n\n55.1 56.2 58.1 60.3\n\nLast Layer Second-to-Last Layer Concat Last Eight Layers Concat First Eight Layers Concat All 24 Hidden Layers\n\n47.1 46.3 48.4 45.1 51.4\n\n(a) Fusion strategies\n\n(b) History context lengths\n\n(c) Feature selection\n\nFigure 9: Ablations of InstructRL evaluated on the 14 selected tasks listed in Appendix A.5. We report the average success % over all tasks. (a): Performance of InstructRL with different strategies for fusing the language and vision features. (b): Effect of history context length on the performance of InstructRL. (c): InstructRL with features selected from different layers.\n\nruns the encoder twice to obtain vision and language representation separately, then concatenates them together. FiLM (Perez et al., 2018) fuses vision and language features layer-wisely before concatenating them together. Default refers to InstructRL where language and vision are encoded once to obtain joint representations. We see that using a joint representation of language and vision inputs is critical for performance.\n\nHistory context encoding. The flexibility of the transformer architecture allows us to encode multiple past steps. To investigate the benefit of utilizing historical information, we ablate the history context length. Figure 9b shows that there is a steady increase in performance when we increase the context length. However, using a longer context requires more compute and memory (e.g., increasing the context length from 4 to 8 adds about 20% longer training time on one task). Thus, we choose a context length of 4 for computational efficiency.\n\nMulti-scale features. In Figure 9c, we compare the performance of InstructRL with features selected from different layers. The results show that using a combination of intermediate representations is most effective.\n\n9\n\nRepresentation TypeSuccess Rate (%)0.010.020.030.040.050.0DefaultConcatFiLMUnder review as a conference paper at ICLR 2023\n\n8 CONCLUSION\n\nIn this paper, we propose InstructRL, a novel transformer-based instruction following method. InstructRL has a simple and scalable architecture which consists of a pre-trained multimodal transformer and a policy transformer. Experimental results show that InstructRL significantly outperforms the state-of-the-art pre-trained and train-from-scratch instruction following methods. As InstructRL achieves state-of-the-art results and scales well with model capacity, applying our approach to a larger scale of problems would be an interesting future work.\n\nREFERENCES\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing In IEEE Conference web-scale image-text pre-training to recognize long-tail visual concepts. on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 3558– 3568. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.00356. URL https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_ Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_ Recognize_Long-Tail_Visual_CVPR_2021_paper.html.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021a.\n\nShizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. Advances in Neural Information Processing Systems, 34:5834–5847, 2021b.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nZichen Jeff Cui, Yibin Wang, Nur Muhammad, Lerrel Pinto, et al. From play to policy: Conditional\n\nbehavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022.\n\nKaran Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated imageIn Joaquin Vanschoren and Sai-Kit Yeung text data created by the people, for the people. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/e00da03b685a0dd18fb6a08af0923de0-Abstract-round1.html.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nXinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel. Multimodal masked autoencoders learn transferable representations. arXiv preprint arXiv:2205.14204, 2022.\n\nPierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, and Cordelia Schmid. Airbert: In-domain pretraining for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1634–1643, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPierre-Louis Guhur, Shizhe Chen, Ricardo Garcia, Makarand Tapaswi, Ivan Laptev, and Cordelia Instruction-driven history-aware policies for robotic manipulations. arXiv preprint\n\nSchmid. arXiv:2209.04899, 2022.\n\nWeituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning In Proceedings of the\n\na generic agent for vision-and-language navigation via pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13137–13146, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nFelix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human instruction-following with deep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382, 2020.\n\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.\n\nzero-shot planners: Extracting actionable knowledge for embodied agents. arXiv:2201.07207, 2022.\n\nLanguage models as arXiv preprint\n\nStephen James and Andrew J Davison. Q-attention: Enabling efficient learning for vision-based\n\nrobotic manipulation. IEEE Robotics and Automation Letters, 7(2):1612–1619, 2022.\n\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019– 3026, 2020.\n\nEric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 991–1002. PMLR, 2022.\n\nYunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li FeiFei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.\n\nApoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: Clip embeddings for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14829–14838, 2022.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nShikun Liu, Stephen James, Andrew J. Davison, and Edward Johns. Auto-lambda: Disentangling\n\ndynamic task relationships. arXiv preprint arXiv: Arxiv-2202.03091, 2022.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.\n\nCorey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data.\n\narXiv preprint arXiv:2005.07648, 2020.\n\nArjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. In European\n\nImproving vision-and-language navigation with image-text pairs from the web. Conference on Computer Vision, pp. 259–274. Springer, 2020.\n\nAjay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li FeiFei, Silvio Savarese, Yuke Zhu, and Roberto Mart ́ın-Mart ́ın. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSuraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning languageconditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pp. 1303–1315. PMLR, 2022a.\n\nSuraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A univer-\n\nsal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022b.\n\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nNur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning k modes with one stone. arXiv preprint arXiv: Arxiv-2206.11251, 2022.\n\nDhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. arXiv preprint arXiv:2207.04429, 2022.\n\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\n\nmanipulation. In Conference on Robot Learning, pp. 894–906. PMLR, 2022a.\n\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for\n\nrobotic manipulation. arXiv preprint arXiv:2209.05451, 2022b.\n\nDeepMind Interactive Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Tim Harley, et al. Creating multimodal interactive agents with imitation and self-supervised learning. arXiv preprint arXiv:2112.03763, 2021.\n\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.\n\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2019, pp. 6558. NIH Public Access, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1\n\nIMPLEMENTATION AND COMPUTE RESOURCES\n\nWe use the AdamW (Kingma & Ba, 2015; Loshchilov & Hutter, 2017) optimizer with a learning rate of 5×10−4 and weight decay 5e−5. All models are trained on TPU v3-128 using cloud TPU. Since TPU does not support headless rendering with PyRep 3 simulators that RLBench is built upon, we evaluate the models on NVIDIA Tesla V100 SXM2 GPU using headless rendering. Each training batch per device consists of 32 demonstrations sequence with length 4, in total the batch size is 512. All models are trained for 100K iterations. We apply data augmentation in training, including jitter over the RGB images ck t .\n\nOur implementation of InstructRL is built upon the official JAX implementation of multimodal masked autoencoder (M3AE) (Geng et al., 2022)4, and we use the official pre-trained M3AE models that were pre-trained on CC12M (Changpinyo et al., 2021) and text corpus (Devlin et al., 2018).\n\nOur implementation of CLIP-RL is built upon a JAX CLIP implementation5, and we use the official pre-trained CLIP models6 for the visual- and language-representations.\n\nBoth CLIP-RL and InstructRL use ViT-B/16 in all experiments unless otherwise specified. As we demonstrated in experiment section, while the larger models ViT-L/16 and ViT-H/16 can further boost InstructRL’s results, we use ViT-B/16 to reduce computation cost and to have apple to apple comparison with baselines.\n\nOur code is available at at https://sites.google.com/view/instructrl/.\n\nA.2 PRE-TRAINING DATASETS\n\nA.2.1 M3AE\n\nM3AE is trained on a combination of image-text datasets and text-only datasets. The image-text data includes the publicly available Conceptual Caption 12M (CC12M) (Changpinyo et al., 2021), Redcaps (Desai et al., 2021), and a 15M subset of the YFCC100M (Thomee et al., 2016) selected according to (Radford et al., 2021).\n\nThe text-only data includes the publicly available Wikipedia and the Toronto BookCorpus (Zhu et al., 2015). For language data, we use the BERT tokenizer from Huggingface7 to tokenize the text. Following Geng et al. (2022), we use 0.5 for text loss weight and 0.75 for text mask ratio. We use 0.1 for text-only loss.\n\nA.2.2 CLIP\n\nCLIP is trained on the publicly available YFCC100M (Thomee et al., 2016) which consists of 100M image-text pairs.\n\nA.3 RLBENCH INSTRUCTIONS\n\nWe use the task instructions provided in RLBench. For each task, there are a number of possible variations, such as the shapes and colors of objects, the initial object positions, and the goal of the task. Based on the task and variation, the environment generates a natural language task instruction. For example, in the task “Put groceries in cupboard”, the generated instruction is of the form “pick up the OBJECT and place it in the cupboard” where OBJECT is one of 9 possible grocery items. A full list of RLBench task instructions can be found at https://github.com/stepjam/ RLBench/tree/master/rlbench/tasks.\n\n3https://github.com/stepjam/PyRep 4https://github.com/young-geng/m3ae_public 5https://github.com/google-research/scenic/tree/main/scenic/projects/\n\nbaselines/clip\n\n6https://github.com/openai/CLIP 7https://huggingface.co/docs/transformers/main_classes/tokenizer\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTask\n\nstack blocks slide block open drawer push buttons meat off grill put money in safe stack wine\n\nturn tap\n\nNumber of demonstrations\n\n100\n\n10\n\n100\n\n10\n\n100\n\n10\n\n100\n\n10\n\n100\n\n10\n\n100\n\n10\n\n100\n\n10\n\n100\n\n10\n\nHiveFormer InstructRL\n\n31% 19% 82% 65% 86% 52% 65% 34% 79% 45% 52% 41% 32% 89% 78% 94% 81% 89% 61% 92% 65% 75%\n\n15% 42%\n\n25% 7% 75% 61% 37% 12% 83% 68%\n\nTable 4: Performance (success rate %) of HiveFormer and InstructRL while varying the number of demonstrations. InstructRL can use fewer demonstrations to achieve high success rate, while HiveFormer requires more demonstrations.\n\nA.4 EVALUATION DETAILS\n\nIn multi-task setting, models are trained on multiple tasks simultaneously and subsequently evaluated on each task independently. In the generalization experiment setting, models are trained on a subset of all variations, then evaluated on unseen task variations.\n\nA.5 MULTI-TASK MULTI-VARIATION TASKS DETAILS\n\nFor experiments including model scaling (Figure 8b) and ablation studies (Figure 9), we selected 14 difficult tasks that have multiple variations: Stack wine, Open drawer, Meat off grill, Put item in drawer, Turn tap, Put groceries in cupboard, Sort shape, Screw bulb in, Close jar, Stack blocks, Put item in safe, Insert peg, Stack cups, and Place cups.\n\nEach task comes with multiple variations that include objects colors, objects shapes, and the ordering of objects to be interacted with. To make a fair comparison with baselines, we use the slightly modified version of RLBench from HiveFormer (Guhur et al., 2022). The changes include adding new tasks and improving the motion planner. The full details of each task can be found in RLBench Github repo8 and the HiveFormer Github repo9.\n\nA.6 SAMPLE EFFICIENCY ABLATION\n\nIn the experiments, we used 100 demonstrations following the same setup as in prior work, and InstructRL outperforms prior state-of-the-arts significantly. However, getting 100 demonstrations can be expensive and difficult in many real world tasks. We hypothesize that InstructRL can achieve better sample-efficient learning thanks to the joint language-vision encoder pretrained on large-scale passive datasets.\n\nTo study this, we randomly choose a subset of 8 tasks and evaluate the performance using only 10 demonstrations. The results from Table 4 show that InstructRL outperforms baselines when using 100 demonstrations or 10 demonstrations. Moreover, using only 10 demonstrations, InstructRL achieves competitive or higher success rates than baselines that use 100 demonstrations. For example, on the stack blocks task, InstructRL using 10 demos achieves 32% success rate while HiveFormer gets 31% using 100 demos. Similarly, on the open drawer task, InstructRL using 10 demos gets 81% while HiveFormer gets 86% using 100 demos.\n\nThe results show that InstructRL is more sample efficient than prior state-of-the-arts.\n\nA.7 TASK CATEGORIES DETAILS\n\nTo compare with baselines including Guhur et al. (2022), our experiments are conducted on the same 74 out of 106 tasks from RLBench8. The 74 tasks can be categorized into 9 task groups according to their key challenges as shown in Table 5.\n\nTask diversity. The 74 tasks RLBench cover a wide range of challenges that are essential for robot learning, including planning over multiple sub goals (e.g., picking a basket ball and then throwing the ball) and domains where there are multiple possible trajectories to solve a task due to a large affordance area of the target object (e.g., the edge of a cup).\n\n8https://github.com/stepjam/RLBench/tree/master/rlbench/tasks 9https://github.com/guhur/RLBench/tree/74427e188cf4984fe63a9c0650747a7f07434337\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nRLBench employs 3 keys terms: Task, Variation, and Episode. For each task there are one or more variations, and from each variation, an infinite number of episodes can be drawn. Each variation of a task comes with a list of textual descriptions that verbally summarise this variation of the task.\n\nAn example showing the distinction between task and variation is Figure 1 and Figure 4 of James et al. (2020).\n\nTask difficulty. RLBench is a manipulation benchmark that is significantly more difficult than locomotion benchmarks. Some tasks involve precise object manipulation such as ‘put knife on chopping board‘ and ‘take usb out of computer‘, some tasks require preceise grasping such as ‘close laptop lid‘ and ‘open and close drawer‘, some tasks require solving long horizon tasks that involve many composed sets of actions, for example, the ‘empty dishwasher’ task involves opening the washer door, sliding out the tray, grasping a plate, and then lifting the plate out of the tray. In this work we consider tasks that can be grouped into 9 categories, as shown in Table 5, to comprehensively evaluate the effectiveness of InstructRL.\n\nIn addition to manipulation and perception challenges, RLBench comes with a suite of diverse task instructions that require natural language understanding. RLBench has a large and diverse vocabulary, it has over 100 content words vocabulary size (e.g., table, cup, open, grasp, box, etc) with function words removed, combing with the diverse visual input and complex interactions leads to a suite of challenging instruction following manipulation tasks.\n\nB INSTRUCTION TEMPLATES AND GENERATIONS\n\nDefault instructions generation. The default instructions templates are shown in Table 6.\n\nLong instructions generation. The tuned instructions that contain step-by-step and detailed descriptions of objects are shown in Table 7.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nGroup\n\nChallenges\n\nTasks\n\nPlanning\n\nmultiple sub-goals (e.g., picking a basket ball and then throwing the ball)\n\nbasketball in hoop, put rubbish in bin, meat off grill, meat on grill, change channel, tv on, tower3, push buttons, stack wine\n\nTools\n\na robot must grasp an object to interact with the target object\n\nslide block to target, reach and drag, take frame off hanger, water plants, hang frame on hanger, scoop with spatula, place hanger on rack, move hanger, sweep to dustpan, take plate off colored dish rack, screw nail\n\nLong term\n\nrequires more than 10 macro-steps to be completed\n\nwipe desk, stack blocks, take shoes out of box, slide cabinet open and place cups\n\nRotationinvariant\n\ncan be solved without changes in the gripper rotation\n\nMotion planner\n\nrequires precise grasping\n\nMultimodal\n\nPrecision\n\ncan have multiple possible trajectories to solve a task due to a large affordance area of the target object (e.g., the edge of a cup)\n\ninvolves precise object manipulation\n\nScrew\n\nrequires screwing an object\n\nVisualOcclusion\n\ninvolves tasks with large objects and thus there are occlusions from certain views\n\nreach target, push button, lamp off, push buttons, pick and lift, take lid off saucepan\n\nlamp on,\n\ntoilet seat down, close laptop lid, open box, open drawer, close drawer, close box, phone on base, toilet seat up, put books on bookshelf\n\npick up cup, turn tap, lift numbered block, beat the buzz, stack cups\n\ntake usb out of computer, play jenga, insert onto square peg, take umbrella out of umbrella stand, insert usb in computer, straighten rope, pick and lift small, put knife on chopping board, place shape in shape sorter, take toilet roll off stand, put umbrella in umbrella stand, setup checkers\n\nturn oven on, change clock, open window, open wine bottle\n\nclose microwave, close fridge, close grill, open grill, unplug charger, press switch, take money out safe, open microwave, put money in safe, open door, close door, open fridge, open oven, plug charger in power supply\n\nTable 5: RLBench tasks used in our experiments grouped into 9 categories.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTask\n\nVariation Type\n\n# Variations Language Tempalte\n\nbasketball in hoop put rubbish in bin meat off grill meat on grill change channel tv on\n\npush buttons\n\nstack wine slide block to target reach and drag take frame off hanger water plants hang frame on hanger scoop with spatula place hanger on rack move hanger sweep to dustpan take plate off colored dish rack screw nail wipe desk stack blocks\n\ntake shoes out of box slide cabinet open and place cups reach target push button lamp on lamp off pick and lift take lid off saucepan toilet seat down close laptop lid open box open {option} drawer\n\nclose {option} drawer\n\nclose box phone on base toilet seat up put books on bookshelf pick up cup turn tap lift numbered block beat the buzz stack cups\n\ntake usb out of computer play jenga insert onto square peg take umbrella out of umbrella stand insert usb in computer straighten rope\n\npick and lift small\n\nput knife on chopping board\n\nplace shape in shape sorter\n\ntake toilet roll off stand put umbrella in umbrella stand setup checkers turn oven on change clock open window open wine bottle close microwave close fridge close grill open grill unplug charger press switch take money out safe open microwave put money in safe open door close door open fridge open oven plug charger in power supply\n\nNA NA Name Name Up / Down NA\n\n1 1\n2 2\n2 1\n\nOrdering & Colors\n\n200\n\nNA NA Color NA NA NA NA NA NA NA Colors NA NA Number of blocks Colors. NA Left / Right Colors Colors NA NA Colors NA NA NA NA Bottom / Middle / Top Bottom / Middle / Top NA NA NA Number Colors Left / Right Number NA Number of Colors. NA NA Colors NA\n\ncups.\n\n1 1\n20 1\n1 1\n1 1\n1 1\n20 1\n1 90\n\n1 2\n20 18 1\n1 20 1\n1 1\n1 3\n\n3\n\n1 1\n1 3\n20 2\n3 1\n3x30\n\n1 1\n20 1\n\nNA NA Shapes (’cube’, ’cylinder’, ’triangular prism’, ’star’, ’moon’)\n\n1 1\n\n5\n\nNA Shapes (’cube’, ’cylinder’, ’triangular prism’, ’star’, ’moon’)\n\n1\n\n5\n\nput the ball in the hoop put rubbish in bin take the {chicken, steak} off the grill take the {chicken, steak} on the grill turn the channel up, down turn on the TV push the {color name} button, then push the {color name} button, then ... stack wine bottle slide the block to targe use the stick to drag the cube onto the {color name} target hang frame on hanger pick up the watering can by its handle and water the plant hang frame on hanger take frame off hanger pick up the hanger and place in on the rack move hanger onto the other rack sweep dirt to dustpan put the plate between the {color name} pillars of the dish rack screw the nail in to the block wipe dirt off the desk stack blocks to stack {color name} blocks\n\ntake shoes out of box put cup in {option} cabinet reach the {color name} target push the {button color name} turn on the light turn off the light pick up the {block color name} block and lift it up to the target take lid off the saucepan toilet seat down close laptop lid open box close {option} drawer\n\nclose option drawer\n\nclose box put the phone on the base toilet seat up put {index} books on bookshelf pick up the {target color name} cup turn {option} tap pick up the block with the number {block num} beat the buzz stack {blocks to stack} {color name} cups\n\ntake usb out of computer play jenga put the ring on the {color name} spoke take umbrella out of umbrella stand\n\ninsert usb in computer straighten rope\n\npick up the {shape name} and lift it up to the target\n\nput the knife on the chopping board\n\nput the {shape} in the shape sorter\n\nNA NA Number NA NA NA NA NA NA NA NA NA NA Number NA Number NA NA NA NA NA\n\n17\n\n1 1\n3 1\n1 1\n1 1\n1 1\n1 1\n1 3\n1 3\n1 1\n1 1\n1\n\ntake toilet roll off stand put umbrella in umbrella stand place the remaining {number} checker in its initial position on the board turn on the oven change the clock to show time 12.15 open left window open wine bottle close microwave close fridge close the grill open the grill unplug charger press switch take the money out of the {index} shelf and place it on the table open microwave put the money away in the safe on the {index} shelf open the door close the door open fridge open the oven plug charger in power supply\n\nTable 6: Language instructions templates in RLBench (James et al., 2020).\n\nUnder review as a conference paper at ICLR 2023\n\nt\n\no n\ns\n\np u\ns h\n\nb u\n\nt -\n\nO\n\nr d\ne r\ni\n\nn g\n\no f\n\nb u\n\nt -\n\nt\n\no n\ns ,\n\nc o\n\nl\n\no r\ns\n\n2 0\n0\n\np u\ns h\n\nt\n\nh e\n\nC h\no o\ns e\n\na {\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\nc u\nb e\n\na s\n\ns t\na c\nk\n\nb a\ns e\n\n,\n\nt\n\nh e\nn\n\np\n\ni c\nk\n\na n\no\n\nt\n\nh e\nr\n\nC h\no o\ns e\n\na\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nc u\nb e\n\na s\n\nt\n\nh e\n\ns t\na c\nk\n\nb a\ns e\n\n,\n\nt\n\nh e\nn\n\np\n\ni c\nk\n\na n\n-\n\nb u\n\nt t\n\no n\n\n.\n\n{ n\n\nt\n\nh\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\nt\n\nh e\nn\n\np u\ns h\n\n{ n\n\nt\n\nh\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt -\n\nt\n\nh e\n\n{ n\n- t\n\nh\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\nt\n\no\n\np u\ns h\n\nt\n\nh e\n\n{ n\n- t\n\nh\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\np u\n\nl l\n\nt\n\nh e\n\nw h\n\ni t\ne\n\ng r\ni\n\np p\ne r\n\nu p\n\na n\nd m\no v\ne\n\nt\n\nh e\n\ng r\ni\n\np p\ne r\n\nc l\n\no s\ne r\n\nt\n\no\n\nt\n\nh e\n\nt\n\no n\n\nd o\nw n\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\nt\n\nh e\nn\n\n. .\n.\n\nt\n\nh e\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\nt\n\nh e\nn\n\np u\ns h\n\nr e\np e\na t\n\nu n\n\nt i\nl\n\nt\n\nh e\n\ns t\na c\nk\n\nh a\ns\n\n3\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nc u\nb e\ns .\n\ns t\na c\nk\n\n,\n\nr e\np e\na t\n\nu n\n\nt i\nl\n\nt\n\nh e\n\ns t\na c\nk\n\nh a\ns\n\n3\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nc u\nb e\ns .\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nc u\nb e\n\na n\nd\n\np\n\nl a\nc e\n\ni t\n\no n\n\nt\n\no\n\nt\n\nh e\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nc u\nb e\n\nc u\nb e\n\ns t\na c\nk\n\n,\n\nt\n\nh e\nn\n\nm o\nv e\n\nt\n\nh e\n\nw h\n\ni t\ne\n\ng r\ni\n\np p\ne r\n\nt\n\no\n\np\n\ni c\nk\n\na n\no\n\nt\n\nh e\nr\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\nc u\nb e\n\na n\nd p\n\nl a\nc e\n\ni t\n\no n\n\nt\n\no\n\nt\n\nh e\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\ns t\na c\nk\n\n,\n\no\n\nt\n\nh e\nr\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\nc u\nb e\n\na n\nd\n\np\n\nl a\nc e\n\ni t\n\no n\n\nt\n\no\n\nt\n\nh e\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\ns t\na c\nk\n\nb\n\nl\n\no c\nk s\n\nT a\ns k\n\nT y\np e\n\nV a\nr i\na t\ni\n\no n\n\nV a\nr i\na t\ni\n\no n\ns\n\nN u\nm\n\no f\n\nT e\n\nm p\n\nl a\nt e\n\nD e\nf a\nu\n\nl t\n\nL a\nn g\nu a\ng e\n\nS h\no r\nt\n\nT u\nn e\nd\n\nL a\nn g\nu a\ng e\n\nT e\n\nm p\n\nl a\nt e\n\nL o\nn g\n\nT u\nn e\nd\n\nL a\nn g\nu a\ng e\n\nT e\n\nm p\n\nl a\nt e\n\nc o\n\nl\n\no r\ns\n\nN u\nm b\ne r\n\no f\n\nb\n\nl\n\no c\nk s\n,\n\n9 0\n\ns t\na c\nk\n\nb\n\nl\n\no c\nk s\n\n{ c\no\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\n{ b\n\nl\n\no c\nk s\n\nt\n\no\n\ns t\na c\nk }\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\nfi n\na l\nl\n\ny\n\n,\n\n.\n\n.\n\n.\n\n,\n\nm o\nv e\n\nt\n\nh e\n\ng r\ni\n\np p\ne r\n\nc l\n\no s\ne\n\nt\n\no\n\nm o\nv e\n\nt\n\nh e\n\ng r\ni\n\np p\ne r\n\nc l\n\no s\ne r\n\nt\n\no\n\n{ 4\n\nt\n\nh\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\np e\nr\n\nc l\n\no s\ne\n\nt\n\no\n\nt\n\nh e\n\n{ 3\nr d\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\nt\n\no\n\np u\ns h\n\np u\ns h\n\nt\n\nh e\n\n{ 2\nn d\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\na f\nt e\nr\n\nt\n\nh a\nt ,\n\nm o\nv e\n\nt\n\nh e\n\nt\n\nh e\n\nM\n\no v\ne\n\nt\n\nh e\n\ng r\ni\n\np p\ne r\n\nc l\n\no s\ne\n\nt\n\no\n\n{ 1\ns t\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\n{ 4\n\nt\n\nh\n\ng r\ni\n\np -\n\nt\n\nh e\nn\n\ni\n\nn g\n\n{ 3\nr d\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\n,\n\np u\n\nl l\n\nt\n\nh e\n\nw h\n\ni t\ne\n\ng r\ni\n\np p\ne r\n\nu p\n\nt\n\no n\n\n,\n\nt\n\nh e\nn\n\np u\ns h\n\n{ 2\nn d\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt t\n\no n\n\nd o\nw n\n\n,\n\na f\nt e\nr\n\np u\ns h\n-\n\nM\n\no v\ne\n\nt\n\nh e\n\nw h\n\ni t\ne\n\ng r\ni\n\np p\ne r\n\nc l\n\no s\ne r\n\nt\n\no\n\n{ 1\ns t\n\nc o\n\nl\n\no r\n\nn a\n\nm\n\ne }\n\nb u\n\nt -\n\n.\n\n.\n\n.\n\n,\n\na n\nd\n\nTable 7: Language instructions templates for tuned instructions used in Table 2 and Table 3.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a simple method to solve the task of instruction-following in multimodal environments. While recent work makes use of pre-trained transformers, their\nperformance is limited by (i) lack of grounding (in the case where separate vision and\nlanguage models are used) and (ii) lack of ability to follow long instructions (in the case\nof CLIP). To overcome these limitations, this paper proposes using M3AE, a recently\nproposed multi-modal transformer, as the backbone to provide vision-language\nrepresentations. In experiments on RLBench, InstructRL, the proposed method, is able\nto outperform prior work such as HiveFormer (which makes use of separate vision and\nlanguage models) and a CLIPort-inspired CLIP-RL method across all categories of\ntasks. Further analyses reveal that the model is also capable of handling longer\ninstructions, novel instruction combinations, and scales well with model size.\n\n# Strength And Weaknesses\n\nPros:\nProposed method is simple and seems easy to implement with the provided details\nfor the task.\nStrong performance improvements across all categories on the RLBench\nbenchmark. Additional analyses also reveal the importance of multiple design\nchoices that have been used by the authors in InstructRL.\nCons:\nWhile the proposed system is simple, there is a slight novelty issue. Stronger pretraining models are expected to provide better representations and hence improve\nmodel performance. The architecture itself seems derivative, given the HiveFormer\nframework.\nIn the ablation studies, InstructRL performs with longer instructions than CLIP- or\nBERT-based models. However, there is no mention of what these instructions look\nlike and what aspect of the original instructions have been expanded to generate\nInstructRL 2\nthe longer instructions. Without such details, it is hard to judge the pure numbers\nprovided for this ablation.\nIn the ablation wrt context length, the author(s) mention that “improvement saturates\naround 4”. From the numbers in Table 9b, it is hard to arrive at this conclusion as\nthere is a steady increase in performance from 1 → 2 → 4 → 8.\nThe writing of the paper is also highly reliant on readers having prior knowledge\nabout the RLBench benchmark, and does not provide adequate details about the\nbenchmark for new readers to understand and evaluate the findings. Without\nappropriate discussion, it is hard to how hard the tasks are.\n\nMinor Edits:\nFigure 5b has some spelling mistakes in the x-axis.\nIn Section 4.1, under ‘Encoding instructions and observations’, the final representation\nof the transformer blocks is mentioned to have dimensionality ‘d’. If that’s the case,\nshouldn’t the dimensionality of be where L is the number of concatenated\nlayer representations?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper was very well written and was very easy to follow. The authors mentioned that the code will be released. The paper has a novelty issue, as the architecture itself seems derivative, given the HiveFormer framework.\n\n# Summary Of The Review\n\nCurrent decision: Borderline Reject\nThe novelty and the lack of self-contained information in this paper imply that the paper\ncurrently stands as a reject. However, the system proposed is very simple and hence\nthe decision is at borderline reject. If the author(s) are able to address the issues wrt\nablations and writing, the score can be reconsidered.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nRE-WEIGHTING BASED GROUP FAIRNESS REGULARIZATION VIA CLASSWISE ROBUST OPTIMIZATION\n\nSangwon Jung1∗ Taeeon Park1∗ Sanghyuk Chun2 Taesup Moon1,3†\n\n1 Department of Electrical and Computer Engineering, Seoul National University 2 NAVER AI Lab {s.jung,pte1235,tsmoon}@snu.ac.kr, sanghyuk.c@navercorp.com\n\n3 ASRI/INMC/IPAI/AIIS, Seoul National University\n\nABSTRACT\n\nMany existing group fairness-aware training methods aim to achieve the group fairness by either re-weighting underrepresented groups based on certain rules or using weakly approximated surrogates for the fairness metrics in the objective as regularization terms. Although each of the learning schemes has its own strength in terms of applicability or performance, respectively, it is difficult for any method in the either category to be considered as a gold standard since their successful performances are typically limited to specific cases. To that end, we propose a principled method, dubbed as FairDRO, which unifies the two learning schemes by incorporating a well-justified group fairness metric into the training objective using a classwise distributionally robust optimization (DRO) framework. We then develop an iterative optimization algorithm that minimizes the resulting objective by automatically producing the correct re-weights for each group. Our experiments show that FairDRO is scalable and easily adaptable to diverse applications, and consistently achieves the state-of-the-art performance on several benchmark datasets in terms of the accuracy-fairness trade-off, compared to recent strong baselines.\n\n1\n\nINTRODUCTION\n\nMachine learning algorithms are increasingly used in various decision-making applications that have societal impact; e.g., crime assessment (Julia Angwin & Kirchner, 2016), credit estimation (Khandani et al., 2010), facial recognition (Buolamwini & Gebru, 2018; Wang et al., 2019), automated filtering in social media (Fan et al., 2021), AI-assisted hiring (Nguyen & Gatica-Perez, 2016) and law enforcement (Garvie, 2016). A critical issue in such applications is the potential discrepancy of model performance, e.g., accuracy, across different sensitive groups (e.g., race or gender) (Buolamwini & Gebru, 2018), which is easily observed in the models trained with a vanilla empirical risk minimization (ERM) (Valiant, 1984) when the training data has unwanted bias. To address such issues, the fairnessaware learning has recently drawn attention in the AI research community.\n\nOne of the objectives of fairness-aware learning is to achieve group fairness, which focuses on the statistical parity of the model prediction across sensitive groups. The so-called in-processing methods typically employ additional machinery for achieving the group fairness while training. Depending on the type of machinery used, recent in-processing methods can be divided into two categories (Caton & Haas, 2020): regularization based methods and re-weighting based methods. Regularization based methods incorporate fairness-promoting terms to their loss functions. They can often achieve good performance by balancing the accuracy and fairness, but be applied only to certain types of model architectures or tasks, such as DNNs (e.g., MFD (Jung et al., 2021) or FairHSIC (Quadrianto et al., 2019)) or binary classification tasks (e.g., Cov (Baharlouei et al., 2020)). On the other hand, re-weighting based methods are more flexible and can be applied to a wider range of models and tasks by adopting simpler strategy to give higher weights to samples from underrepresented groups. However, most of them (e.g., LBC (Jiang & Nachum, 2020), RW (Kamiran & Calders, 2012), and FairBatch (Roh et al., 2020)) lack sound theoretical justifications for enforcing group fairness and may perform poorly on some benchmark datasets.\n\n∗Equal contribution. †Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nIn this paper, we devise a new in-processing method, dubbed as Fairness-aware Distributionally Robust Optimization (FairDRO), which takes the advantages of both regularization and re-weighting based methods. The core of our method is to unify the two learning categories: namely, FairDRO incorporates a well-justified group fairness metric in the training objective as a regularizer, and optimizes the resulting objective function using a re-weighting based learning method. More specifically, we first present that a group fairness metric, Difference of Conditional Accuracy (DCA) (Berk et al., 2021), which is a natural extension of Equalized Opportunity (Hardt et al., 2016) to the multi-class, multi-group label settings, is equivalent (up to a constant) to the average (over the classes) of the roots of the variances of groupwise 0-1 losses. We then employ the Group DRO formulation, which uses the χ2-divergence ball including quasi-probabilities as the uncertainty set, for each class separately to convert the DCA (or variance) regularized group-balanced empirical risk minimization (ERM) to a more tractable minimax optimization. The inner maximizer in the converted optimization problem is then used as re-weights for the samples in each group, making a unified connection between the reweighting and regularization-based fairness-aware learning methods. Lastly, we develop an efficient iterative optimization algorithm, which automatically produces the correct (sometimes even negative) re-weights during the optimization process, in a more principled way than other re-weighting based methods.\n\nIn our experiments, we empirically show that our FairDRO is scalable and easily adaptable to diverse application scenarios, including tabular (Julia Angwin & Kirchner, 2016; Dua et al., 2017), vision (Zhang et al., 2017), and natural language text (Koh et al., 2021) datasets. We compare with several representative in-processing baselines that apply either re-weighting schemes or surrogate regularizations for group fairness, and show that FairDRO consistently achieves the state-of-the-art performance on all datasets in terms of accuracy-fairness trade-off thanks to leveraging the benefits of both kinds of fairness-aware learning methods.\n\n2 RELATED WORKS\n\n2.1\n\nIN-PROCESSING METHODS FOR GROUP FAIRNESS\n\nRegularization based methods add penalty terms in their objective function for promoting the group fairness. Due to non-differentiability of desired group fairness metrics, they use weaker surrogate regularization terms; e.g., Cov (Zafar et al., 2017b), R ́enyi (Baharlouei et al., 2020) and FairHSIC (Quadrianto et al., 2019) employ a covariance approximation, R ́enyi correlation (R ́enyi, 1959) and Hilbert Schmidt Independence Criterion (HSIC) (Gretton et al., 2005) between the group label and the model as fairness constraints, respectively. Jung et al. (2021) devised MFD that uses Maximum Mean Discrepancy (MMD) (Gretton et al., 2005) regularization for distilling knowledge from a teacher model and promoting group fairness at the same time. Although these methods can achieve high performance when hyperparameters for strengths of the regularization terms are well-tuned, they are sensitive to choices of model architectures and task settings as they employ surrogate regularization terms; see Sec. 5 and Appendix C.2 for more details.\n\nMeanwhile, some other works (Agarwal et al., 2018; Cotter et al., 2019) used an equivalent constrained optimization framework to enforce group fairness instead of using the regularization formulation. Namely, they consider a minimax problem of the Lagrangian function from the given constrained optimization problem and seek to find a saddle point through the alternative updates of model parameters and the Lagrangian variables. By doing so, they can successfully control the degree of fairness while maximizing the accuracy. However, their alternating optimization algorithms require severe computational costs due to the repetitive full training of the model. Furthermore, we empirically observed that they fail to find a feasible solution when applied to complex tasks in vision or NLP domains. A more detailed discussion is in Appendix B.\n\nAs alternative in-processing methods, several re-weighting based methods (Kamiran & Calders, 2012; Jiang & Nachum, 2020; Roh et al., 2020; Agarwal et al., 2018) have also been proposed to address group fairness. Kamiran & Calders (2012) proposed a re-weighting scheme (RW) based on the ratio of the number of data points per each group. Recently, Label Bias Correction (LBC) (Jiang & Nachum, 2020) and FairBatch (Roh et al., 2020) have been developed, which adaptively adjust weights and mini-batch sizes based on the average loss per group, respectively. Perhaps, the most similar to our work is Agarwal et al. (2018), which demonstrates that a Lagrangian formulation of fairness-constrained optimization problem can be reduced to a cost-sensitive classification problem\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nand solved through a re-weighting learning scheme with relabling of class labels. However, their method is limited to binary classification tasks, whereas our FairDRO, using the DRO framework, can be applied to more complex tasks with non-binary class labels.\n\n2.2 DISTRIBUTIONALLY ROBUST OPTIMIZATION (DRO)\n\nDistributionally robust optimization (DRO) (Ben-Tal et al., 2009) is a general robust optimization framework that was originally used for learning a model that is robust to potential test distribution shift from the training set, by optimizing for the worst-case distribution in an uncertainty set. The DRO framework is now widely adopted in other applications, including algorithmic fairness.\n\nDRO in algorithmic fairness Numerous works in the algorithmic fairness literature have recently utilized the DRO frameworks in various settings. For example, a line of works (Jiang et al., 2020; Taskesen et al., 2020; Wang et al., 2021; Mandal et al., 2020) embeds the DRO frameworks into the fairness-constrained optimization problem to make a fair model robust to distribution shifts of a test dataset. Another related line of works (Wang et al., 2020; Hashimoto et al., 2018) proposed DRO formulations for achieving fairness in more challenging settings such as learning with noisy group labels or sequential learning. In spite of their effectiveness, both of above lines of works do not use DRO as a direct tool for achieving group fairness. Similarly to ours, Rezaei et al. (2020); Yurochkin et al. (2019) adopt DRO frameworks for the purpose of achieving fairness. However, the key difference is that they either require the group labels at test time or target individual fairness. On the other hand, our FairDRO directly aims to achieve group fairness while not requiring group labels at test time. A more detailed discussion is in Appendix B.\n\nDRO in other applications Applications of DRO are actively studied in other contexts, e.g., image classification (Sagawa et al., 2020), multilingual machine translation (Oren et al., 2019; Zhou et al., 2021), long-tailed classification (Samuel & Chechik, 2021) or out-of-distribution generalization (Krueger et al., 2021; Xie et al., 2020). For example, Sagawa et al. (2020) proposed Group DRO which constructs the uncertainty set of joint distributions over the class-group label pairs to withstand the spurious correlations between the class and group labels. Although these works are effective in their applications, the notion of statistical parity across groups (i.e., group fairness) has not been explicitly pursued as in our FairDRO.\n\n3 NOTATIONS AND FAIRNESS CRITERION\n\nNotations We consider a classification problem where each sample consists of an input x ∈ X , a class label y ∈ Y and a group label a ∈ A. The group label is defined by sensitive attributes such as gender or race. Given a training dataset D with N samples (i.e., D = {(xi, yi, ai)}N i=1), we aim to find a fair classifier θFAIR : X → Y satisfying the given fairness criterion while maintaining the high classification accuracy. Moreover, given a loss function l(θ, (x, y)) : Θ × (X × Y) → R, we denote L(θ, D) ≜ 1 a denote the subsets of D that are confined to the samples with group label a or class-group label pair (y, a), respectively.\n\ni=1 l(θ, (xi, yi)). Additionally, Da and Dy\n\n(cid:80)|D|\n\n|D|\n\nFairness criterion The notion of fairness may depend on the different points of view on how discrimination is defined (Hardt et al., 2016; Dwork et al., 2012; Chouldechova, 2017; Berk et al., 2021). For example, one may argue that a classifier is fair if its prediction is not dependent on a (Demographic Parity), while the other can insist that the classifier should be conditionally independent of a given the true class (Equalized Odds). In our work, we focus on the Equalized Conditional Accuracy (ECA) (Berk et al., 2021), which can naturally cover the multi-class, multi-group label setting.\n\nA classifier θ satisfies ECA if all of the accuracies among groups are the same for each given class, i.e., ∀a, a′ ∈ A, y ∈ Y, P ( (cid:98)Y = y|A = a, Y = y) = P ( (cid:98)Y = y|A = a′, Y = y), where (cid:98)Y is the prediction of θ. We measure the degree of unfairness with Difference of Conditional Accuracy (DCA), denoted as ∆DCA, by taking the average of the maximum accuracy gaps among groups over y:\n\n∆DCA ≜ 1\n\n|Y|\n\n(cid:88)\n\ny∈Y\n\n∆y, ∆y ≜ max\n\na,a′\n\n|P ( (cid:98)Y = y|A = a, Y = y) − P ( (cid:98)Y = y|A = a′, Y = y)|.\n\n(1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nRemark: We note that ECA has close connections with the two popular group fairness criteria, Equalized Odds (EO) and Equal Opportunity (EOpp) (Hardt et al., 2016). EO requires the conditional independence between (cid:98)Y and A given Y , and hence, we can easily observe that ECA is equivalent to EO for the binary classification setting. Furthermore, we argue that ECA is a natural extension of EOpp to the multi-class setting; namely, ECA requires equality of TPR among groups for each class label, which resembles the argument for justifying EOpp.\n\n4 FAIRNESS-AWARE DRO (FAIRDRO)\n\nOur goal is to unify the regularization and re-weighting based methods by incorporating the DCA (1) in the objective function for learning. To that end, we first show in Sec. 4.1 that the empirical DCA is equivalent to the average (over the classes) of the roots of the variances of groupwise 0-1 losses. Then, we utilize the previous result (Xie et al., 2020) given in Sec. 4.2 to make the connection between the Group DRO with the uncertainty set of χ2-divergence ball (including quasi-probabilities) and the variance regularized group-balanced empirical loss minimization. Subsequently, in Sec. 4.3, we present our training objective that applies Group DRO separately for each class y, which essentially converts the DCA (or variance) regularized group-class balanced ERM into a more tractable minimax optimization problem. Finally, Sec. 4.4 proposes an efficient iterative algorithm that does alternating minimization-maximization of our training objective.\n\n4.1 EQUIVALENCE BETWEEN DCA AND VARIANCE OF GROUPWISE LOSSES\n\nProposition 1 Assume l(θ) = 1{θ(x) ̸= y}. Then, the following inequalities hold for all y ∈ Y,\n\n(cid:113)\n\n2 Var (cid:0){L(θ, Dy\n\na)}a∈A\n\n(cid:1) ≤ (cid:98)∆y ≤\n\n(cid:113)\n\n2|A|2 Var (cid:0){L(θ, Dy\n\na)}a∈A\n\n(cid:1),\n\n(2)\n\nin which (cid:98)∆y is the empirical version of ∆y and Var({xi}d of d numbers {xi}d\n\n(cid:80) i=1. Thus, (cid:98)∆y of a classifier is 0 if and only if Var (cid:0){L(θ, Dy\n\ni=1) ≜ 1\n\ni(xi− 1\n\n(cid:80)\n\nd\n\nd\n\ni xi)2 is the variance (cid:1) = 0 . a)}a∈A\n\nThe proof is in Appendix A. By taking the average over y in (2), Proposition 1 implies that the empirical DCA is equivalent to the average (over y) of the square roots of the variances of the groupwise 0-1 losses, up to a constant. Building this equivalence, we are now able to utilize the Group DRO formulation given in the next subsection to make a connection between DCA regularization and robust optimization.\n\n4.2 PRIOR WORK ON VARIANCE REGULARIZATION VIA GROUP DRO\n\nWhile ERM merely minimizes the empirical loss, DRO optimizes for the worst-case distributions in an uncertainty set, as mentioned in Sec. 2.2. Since general DRO (Duchi et al., 2019) may lead to a pessimistic model that optimizes for implausible worst-case distributions, Sagawa et al. (2020) recently proposed the Group DRO framework, which performs at a group level and can incorporate prior knowledge about the groups1; it aims to obtain\n\nθGDRO ≜ arg min\n\nθ\n\nmax q∈∆|A|\n\n(cid:88)\n\na∈A\n\nqaL(θ, Da),\n\n(3)\n\nin which ∆|A| is the |A|-simplex.\n\nThe applications of Group DRO are actively studied in various contexts, e.g., image classification (Sagawa et al., 2020) or machine translation (Zhou et al., 2021). In particular, Xie et al. (2020) proposed Risk Variance Penalization (RVP), an OOD generalization method, using a variance regularization with respect to risks of each domain (a formulation first introduced by V-REx (Krueger et al., 2021)). The author showed the connection between the variance regularization and Group DRO, when the uncertainty set is the following χ2-divergence ball with radius ρ > 0:\n\nQρ ≜\n\n(cid:110)\n\nq ∈ R|A| :\n\nqa = 1, Dφ\n\n(cid:16)\n\nq (cid:12) (cid:12)\n\n(cid:12) (cid:12)\n\n(cid:17)\n\n111\n\n1 |A|\n\n(cid:111) ,\n\n≤ ρ\n\n(cid:88)\n\na∈A\n\n(4)\n\n1In this subsection, the term “group” is used in a more inclusive manner. Namely, the group here refers to\n\nany hierarchical structure in the data distribution, e.g., domain, environment, and sensitive attribute.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\ni qiφ(pi/qi) with φ(t) = (t − 1)2, and 1\n\nin which Dφ(p∥q) ≜ (cid:80) |A|111 ∈ R|A| denotes the uniform distribution. Note (4) does not have the nonnegativity constraint on q, and hence, depending on ρ, Qρ can also include quasi-probabilities, which have negative components as well. Namely, Xie et al. (2020) shows the following lemma that the inner maximization of Group DRO with Qρ uncertainty set is equivalent to the group-balanced empirical loss plus the group-wise variance term as a regularizer.\n\nLemma 1 (Xie et al., 2020, Proposition 1) For any finite loss l(θ, (x, y)) and Qρ, we have\n\nmax q∈Qρ\n\n(cid:88)\n\na∈A\n\nqaL(θ, Da) =\n\n1 |A|\n\n(cid:88)\n\na\n\nL(θ, Da) +\n\n(cid:113)\n\nρ Var (cid:0){L(θ, Da)}a∈A\n\n(cid:1).\n\n(5)\n\nTo be self-contained, the proof of the lemma is given in Appendix A.\n\n4.3 TRAINING OBJECTIVE OF FAIRDRO\n\nTo make the connection between Proposition 1 and Lemma 1 and realize the DCA regularization for fairness-aware training, we propose to apply the Group DRO separately for each class y. Namely, we define our FairDRO as minimizing the average of the worst-case losses defined for each class y:\n\nθFairDRO ≜ arg min\n\nθ\n\n1 |Y|\n\n(cid:88)\n\ny∈Y\n\nmax qy∈Qρ\n\n(cid:88)\n\na∈A\n\naL(θ, Dy qy\n\na),\n\n(6)\n\nin which Qρ is as defined in (4), qy = {qy a}a∈A is a quasi-probability vector, and ρ is a tunable hyperparameter. We stress that while applying Group DRO by treating each pair (y, a) as a “group” has been considered in Sagawa et al. (2020), applying Group DRO separately for each class as in FairDRO has not been considered before. Now, we present our key result which follows by combining Proposition 1 and Lemma 1.\n\nCorollary 1 Assume l(θ) = 1{θ(x) ̸= y}. Then, given a dataset D, there is a corresponding positive constant CD in the range of [\n\n2|A|] such that θFairDRO also achieves\n\n√\n\n√\n\n2,\n\n(cid:110) 1\n\n|Y||A|\n\nmin θ\n\n(cid:88)\n\n(y,a)\n\nL(θ, Dy\n\na) + CD\n\n√\n\nρ (cid:98)∆DCA\n\n(cid:111) .\n\n(7)\n\nThe corollary implies that when the 0-1 loss is used, FairDRO is a regularization based method, as described in Sec. 2.1, with the exact DCA serving as a regularizer. We emphasize that we do not solve the minimization in (7) directly. (cid:98)∆DCA can have high variance when estimated from a small mini-batch, particularly when the number of groups is large, and is even hard to be minimized due to non-differentiability. We indeed empirically observe in Sec. 5.4 that directly solving (7) using approximation of (cid:98)∆DCA with a differentiable cross-entropy loss performs poorly. Instead, we solve a much more tractable minimax optimization in (6) where the maximization is a simple linear optimization and the minimization is for the re-weighted group losses.\n\nAt this point, it is now clear that during solving (6), FairDRO produces the re-weights {qy a}’s for each (y, a), which makes it also categorized as a re-weighting based method. The following proposition shows how the re-weights are characterized and controlled by the hyperparameter ρ.\n\nProposition 2 For Qρ in (4) with ρ > 0, each element of the optimum qy∗ for the inner-maximization of (6) can be obtained by a closed-form solution:\n\nqy∗ a =\n\n1 |A|\n\n+\n\n(cid:114) ρ |A|\n\n×\n\nL(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy a)\n\na∈A\n\n∥L(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy\n\na)∥2\n\nand is in the range of\n\n√\n\n(cid:104) 1\n\n|A| −\n\nρ(|A|−1)\n\n|A|\n\n, 1\n\n|A| +\n\na∈A\n\n√\n\nρ(|A|−1)\n\n|A|\n\n(cid:105)\n\n.\n\n,\n\n(8)\n\nThe proof is in Appendix A. Proposition 2 shows that some elements of qy∗ can be negative depending on the radius ρ and the groupwise loss L(θ, Dy a). Moreover, it shows that the range of each element of the optimum qy∗ is automatically determined by a radius ρ and the number of groups |A|, which\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nmeans FairDRO controls the degree of the group penalization by varying ρ. We will show in Sec. 5 that we can find a proper degree of penalization by tuning ρ so that the performances on various datasets in terms of accuracy-fairness trade-off are significantly improved.\n\nA crucial difference from typical re-weighting based methods is that our re-weights are obtained as the solution of an optimization problem as opposed to those in Kamiran & Calders (2012); Jiang & Nachum (2020) that are heuristically obtained based on the ratio of the number of data points or losses for each group. Also, we allow negative values for qy a in Qρ, which enables penalizing the high accuracy group more aggressively than typical re-weighting methods that only control the positive weights for each group a and class y.\n\n4.4 AN EFFICIENT ITERATIVE OPTIMIZATION FOR FAIRDRO\n\nA canonical way of solving the minimax optimization in DRO is to use a primal-dual method (Nemirovski et al., 2009) in which one alternates between a regular gradient descent on θ and an exponentiated gradient (EG) ascent (Kivinen & Warmuth, 1997) on q. However, the EG algorithm is widely used when the variables lie in the probability simplex because it is a special case of mirror descent when the convex function for the Bregman divergence is the negative Shannon entropy. Thus, EG cannot be applied to solving (6) since our uncertainty set also allows the quasi-probability for qy.\n\nHence, we employ the smoothed Iterated Best Response (IBR) which is a variant of IBR recently used for solving a DRO problem in (Zhou et al., 2021). Using standard IBR for the inner maximization step, q is updated as the closed-form solution of the maximization (i.e., (8)), instead of the gradient ascent of q. It is known that solving the minimax optimization using the standard IBR has a convergence guarantee w.r.t θ under some regularity assumptions (Zhou et al., 2021). However, it does not imply the convergence of qy, and we empirically observed in Appendix D.6 that the qy∗,t oscillates unsteadily when the loss for each group fluctuates. To that end, we applied the learning rate scheduling of ηt T with T being the number of total epochs, which results in updating qy,t smoothly. Namely, our overall update rules for θ and qy are\n\nq = 1 − t\n\nθt+1 ← θt − ηt\n\nθ∇θ\n\n(cid:16) 1 |Y|\n\n(cid:88)\n\n(cid:88)\n\ny∈Y\n\na∈A\n\na L(θt, Dy qy,t a)\n\n(cid:17)\n\nqy,t+1 ← (1 − ηt\n\nq)qy,t + ηt (cid:88)\n\nqqy∗,t, ∀y ∈ Y,\n\nin which qy∗,t ≜ arg max\n\naL(θt+1, Dy qy\n\na),\n\nqy∈Qρ\n\na∈A\n\n,\n\n(update θ)\n\n(update qy)\n\n(9)\n\n(10)\n\nAlgorithm 1: FairDRO Iterative Optimization\n\nInput\n\n: Dataset {Dy T , iteration I, mini-batch size B\n\na}y∈Y,a∈A, radius ρ, epoch\n\n1 Randomly initialize θ0 2 Set qy,0 ← 1 3 for t = 0 to T − 1 do\n\n|A|111 for y ∈ Y, in which 111 ∈ R|A|\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n// θ-update for i = 0 to I − 1 do\n\nSample a mini-batch {(xj, yj, aj)}j∈[B], a for each\n\nequally from Dy (y, a) ∈ Y × A. Update θt with (9) using the mini-batch\n\nend // q-update Update qy,t for each y ∈ Y with (10)\n\n10 11 end\n\nOutput : θT , {qy,T }y∈Y\n\nθ and ηt\n\nand ηt q denote the learning rates for θ and qy, respectively. By applying the smoothed IBR in (10), we empirically observe improvements in the group fairness and more stable convergence of qy (detailed results are given in Appendix D.6). We note that for (7) to hold, we use 0-1 loss in (6), which makes our training objective still nondifferentiable with respect to θ. Thus, in practice, we solves the cross-entropy loss for the outer minimization (9) and the 0-1 loss for the inner maximization (10). We also note that θ is updated in a mini-batch manner, whereas q is updated after computing the group losses for all training data points. Algorithm 1 illustrates the summary of FairDRO optimization scheme.\n\n5 EXPERIMENTAL RESULTS\n\nIn this section, we demonstrate the versatility of FairDRO by comparing it with various baseline methods on three modalities of datasets: (1) two tabular datasets, UCI Adult (Dua et al., 2017) and ProPublica COMPAS (Julia Angwin & Kirchner, 2016), (2) two vision datasets, UTKFace (Zhang et al., 2017), CelebA (Liu et al., 2015) (in Appendix D.2) and (3) a natural language dataset,\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The trade-offs between accuracy and DCA on Adult (left) and COMPAS (right). The performances of “Scratch” are highlighted by the black dotted lines. Each point is the result corresponding to a different hyperparameter of each method. Each line is a convex envelop of points of the corresponding method.\n\nCivilComments-WILDS (Koh et al., 2021). We use logistic regression, ResNet18 (He et al., 2016), and a pre-trained BERT (Devlin et al., 2019; Wolf et al., 2019) as the classifier for each modality, respectively. For the evaluation, we plotted the convex envelops of the trade-offs between the accuracy and ∆DCA for varying hyperparameters, i.e., the pareto frontiers. As a single number evaluation metric, we also reported the best DCA with at least 95% accuracy of the vanilla trained model (Scratch) in the Appendix D.1. Since the most datasets are severely skewed toward a specific group or class, we report the balanced accuracy over all combinations of groups and classes as Bellamy et al. (2018) (the issue discussed more in Appendix D.7). Moreover, we give a thorough ablation study on our design choice in (6), namely, the classwise DRO formulation and the uncertainty set (4) in Sec. 5.4 and Appendix D.4, respectively. We further visualize the re-weights produced by FairDRO in Sec. 5.4. More implementation details are given in Appendix C.\n\nComparison methods. We compare FairDRO with vanilla training without any fairness constraint (Scratch) and nine in-processing fair-training methods; three re-weighting based methods (RW (Kamiran & Calders, 2012), LBC (Jiang & Nachum, 2020), FairBatch (Roh et al., 2020)), four regularization based methods (Cov (Zafar et al., 2017b), FairHSIC (Quadrianto et al., 2019), R ́enyi (Baharlouei et al., 2020), MFD (Jung et al., 2021)) and two methods employing a constrained optimization (EGR (Agarwal et al., 2018), PL (Cotter et al., 2019)). Since we consider a group-class balanced accuracy, we modified the objective functions of all baselines including Scratch such that they use the group-class balanced ERM loss, not the standard ERM loss.2 We note that Cov, EGR and R ́enyi are task-dependent, and FairHSIC and MFD are model-dependent; namely, Cov and EGR are designed only for binary classification tasks, R ́enyi is for tasks with binary groups, and FairHSIC and MFD are for DNN-based classifiers. We also note that all baselines except RW were implemented for targeting EO. Considering that ECA is equivalent to EO only in binary classification as we mentioned in Section 3, we also reported the difference of EO (DEO) for multi-class classification tasks in Appendix D.1. We provide more details of implementations of the baselines in Appendix C.\n\n5.1 TABULAR DATASETS\n\nTwo tabular datasets, UCI Adult (Dua et al., 2017) (Adult) and ProPublica COMPAS (Julia Angwin & Kirchner, 2016) (COMPAS), are used for the benchmark. Both tasks are binary classification tasks (predicting whether the income of an individual exceeds $50K per a year and whether a defendant re-offends, respectively) with binary sensitive groups (“Female” and “Male”, and “Caucasian” and “Non-Caucasin”, respectively). Following pre-processing of Bellamy et al. (2018), each dataset includes 45K and 5K rows, respectively.\n\nFig. 1 shows the accuracy-fairness trade-offs for varying hyperparameters on Adult and COMPAS. We note that there is only a single score for RW because it has no controllable hyperparameter. In the figure, we observe that FairDRO is placed to the most top-right area among methods, showing the best trade-off, and even achieves the lowest level of DCA with a slight loss of accuracy. This implies that FairDRO successfully finds better weights (i.e., quasi-probability qy) than other re-weighting based methods and employs a better regularizer than other regularization based methods. Furthermore, our FairDRO can provide pareto-frontiers in a wider range than other re-weighting baselines by introducing negative weights.\n\n2The results of employing original objective functions are reported in Appendix D.3. From the results, we\n\nobserve that using the balanced ERM loss improves the balanced accuracy and DCA for the most methods.\n\n7\n\n234567DCA (%)7476788082Accuracy (%)Adult468101214DCA (%)56586062Accuracy (%)COMPASScratchRWLBCCovRényiFairBatchEGRPLFairDRO (ours)Published as a conference paper at ICLR 2023\n\nFigure 2: The trade-offs between accuracy and DCA on UTKFace (left) and CivilComments (right). Details are the same as Fig. 1.\n\n5.2 VISION DATASETS\n\nWe also evaluate FairDRO on UTKFace (Zhang et al., 2017), a face dataset with multi-class and multi-group labels. UTKFace includes more than 20K images with age, gender and ethnicity labels. We divide the age labels into three classes (“0 to 19”, “20 to 40”, and “more than 40”), and set them as the class labels. We also use four ethnic classes (“White”, “Black”, “Asian”, and “Indian”) as the group labels. The description and results on another vision dataset, CelebA (Liu et al., 2015), are given in Appendix D.2.\n\nFrom Fig. 2 (left), we confirm that FairDRO significantly improves the trade-off, compared to most baselines. For MFD, we note that it provides a competitive pareto-frontier with higher accuracy than FairDRO, thanks to the knowledge distillation effect. However, MFD requires additional computations for training a teacher model, and its high performance is sensitive to task domains (refer to the next result on the NLP task). We again observe that the re-weighting based baselines are ineffective in achieving fairness on UTKFace, while FairDRO achieves strong performance due to its systematic optimization of the group weights. We further note that much lower accuracy of PL implies that its minimax optimization fails to produce good performance due to difficulty of finding feasible saddle points. Since EO is not equivalent to ECA for non-binary target task as aforementioned above, we additionally report the difference of EO (DEO) for UTKFace in Appendix D.1 and show FairDRO again outperforms other baselines in DEO.\n\n5.3 LANGUAGE DATASET\n\nIn order to show the versatility of FairDRO, we additionally consider a natural language dataset, CivilComments-WILDS (Koh et al., 2021). CivilComments is a large collection of comments on online articles taken from the Civil Comments platform. It comprises 450K comments which are annotated with identity attributes by 10 crowdworkers and majority votes. The CivilComments task is to predict whether a given comment is toxic or not. We set ethnicity as the sensitive attribute and extract the subset of comments that contain identities w.r.t. ethnicity (“Black”, “White”, “Asian”, “Latino” and “Ohters”). Note that the dataset has continuous values for the identities (between 0 and 1), hence, we binarized the continuous group values by setting the maximum as 1 and the others as 0, since the existing in-processing methods are only available to handle discrete group labels.\n\nThe results are shown in Fig. 2 (right). Note that since a pre-trained BERT is fine-tuned only for small epochs, the update for qy of FairDRO and weights of re-weighting baselines is executed per 100 iterations, instead of every epoch. We empirically observed that FairHSIC fails to converge due to the gradient exploding. Consistent with the previous results, FairDRO achieves the best trade-off on CivilComments. Note that although MFD is very competitive on UTKFace, it is not effective on CivilComments.\n\n5.4 ANALYSIS\n\nAblation study. Fig. 3 shows the accuracy-fairness trade-off for each ablated version of FairDRO on each dataset. “FairDRO (w/o DRO)” directly solves (7) by approximating both terms in the objective with the cross-entropy loss; namely, both terms are replaced with the group-class balanced average cross-entropy loss and the average (over y) of the maximum gaps of groupwise cross-entropy losses. The “RVP (w /classwise)” is also similar, but uses the average (over y) of the variances of the groupwise cross-entropy losses (as shown in (5)) as a regularization term instead of the direct approximation of (cid:98)∆DCA. FairDRO (w/o classwise) defines only one uncertainty set over the (y, a)\n\n8\n\n141618202224DCA (%)70727476788082Accuracy (%)UTKFace5101520DCA (%)767880Accuracy (%)CivilCommentsScratchRWLBCCovFairBatchEGRPLFairHSICMFDFairDRO (ours)Published as a conference paper at ICLR 2023\n\nFigure 3: Ablation study of FairDRO. RVP and FairDRO (w/o classwise) are for the ablation study of our key two components, the DRO formulation, and classwise treatment, respectively. The best performance of each result is in Tab. D.4.\n\nFigure 4: Accuracies and loss weights for each group (q) on COMPAS. The groupwise training accuracies and loss weights during training for Scratch (a) and FairDRO (b) are shown. Groupwise test accuracies are shown in (c). The reported accuracies and weights are only for label 1, while label 0 results are in Appendix D.5.\n\npair in (6) for the ablation study of the classwise treatment. In Fig. 3, we note that RVP (w/ classwise) on CivilComments fails to converge by the gradient exploding. Our observations from the figures are as follows. First, we see that FairDRO achieves consistently better pareto-frontiers on all datasets than FairDRO (w/o DRO) and RVP (w/classwise), which asserts that solving (7) by a re-weighting learning scheme via our classwise DRO formulation is essential for better performance and stable learning. Second, we clearly observe that FairDRO (w/o classwise) shows sub-optimal accuracy-DCA trade-offs (except for Adult), indicating that our classwise DRO formulation is critical for the optimal performance since it makes the connection with the DCA regularization more clearly. Further ablation studies for the choice of the uncertainty set are in Appendix D.4.\n\nVisualization of qy. We visualize the learning dynamics of the group weights (qy), including training accuracies (Fig. 4 (a) and (b)) and test accuracies (Fig. 4 (c)) for Scratch and FairDRO on COMPAS. For simplicity, we report the result for y = 1 (for y = 0 is in Appendix D.5). In Fig. 4 (a), q1 assigns the same value for each group as Scratch employs the balanced ERM loss. At the top of the plot, Scratch shows a large discrepancy in accuracy between the two groups throughout training. On the other hand, Fig. 4 (b) shows FairDRO adjusts q1 by assigning low values to the high accuracy group (Group 0) and high values to the low accuracy group (Group 1). Note that q1 initially fluctuates and even assigns negative values for the high accuracy group but eventually converges via the learning rate scheduling in (10). As a result, we clearly observe both the training and test accuracy gaps between the groups smoothly become negligible as the training continues. We believe this example clearly shows the effectiveness of introducing the quasi-probability in Qρ, and our FairDRO framework in achieving small test accuracy gaps between the groups, i.e., ∆DCA.\n\n6 CONCLUDING REMARK\n\nWe proposed a theoretically well-justified in-processing method for achieving group fairness. To make a clear connection between re-weighting and regularization based methods, our FairDRO employs the classwise Group DRO framework with χ2-divergence ball including quasi-probabilities as an uncertainty set. Then, the FairDRO training objective directly incorporates DCA as a regularizer and can be effectively solved by our proposed re-weighting optimization scheme. As a result, our DRO formulation enables FairDRO to have strengths of both re-weighing based and regularization based methods. Indeed, we showed through our experiments that our FairDRO is applicable to various settings and consistently achieves superior accuracy-fairness trade-off on benchmark datasets.\n\n9\n\n246DCA (%)777879808182Accuracy (%)Adult468101214DCA (%)586062Accuracy (%)COMPAS121416182022DCA (%)7677787980Accuracy (%)UTKFace81012DCA (%)7778798081Accuracy (%)CivilCommentsScratchFairDRO (w/o DRO)RVP (w/ classwise)FairDRO (w/o classwise)FairDRO (ours)010203040506070Training epochs01q1(b) FairDRO (train)010203040506070Training epochs4050607080Accuracy (%)(c) Scratch & FairDRO (test)010203040506070Training epochs01q1(a) Scratch (train)4050607080Accuracy (%)4050607080Accuracy (%)Group 0Group 1ScratchFairDROPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was supported in part by the NRF grant [NRF-2021R1A2C2007884] and IITP grants [No.2021- 0-01343, No.2021-0-02068, No.2022-0-00959, No.2022-0-00113]] funded by the Korean government, and SNU-NAVER Hyperscale AI Center.\n\nREFERENCES\n\nAlekh Agarwal, Alina Beygelzimer, Miroslav Dud ́ık, John Langford, and Hanna Wallach. A reductions approach to fair classification. In Int. Conf. Mach. Learn. (ICML), pp. 60–69. PMLR, 2018.\n\nSina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn. R ́enyi fair inference.\n\nIn Int. Conf. Learn. Represent. (ICLR), 2020.\n\nRachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943, 2018.\n\nAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.\n\nPrinceton University Press, 2009.\n\nRichard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 50(1):3–44, 2021.\n\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conf. Fairness, Accountability and Transparency (FAccT), pp. 77–91. PMLR, 2018.\n\nSimon Caton and Christian Haas. Fairness in machine learning: A survey. arXiv preprint\n\narXiv:2010.04053, 2020.\n\nAlexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism\n\nprediction instruments. Big Data, 5(2):153–163, 2017.\n\nChing-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In Int. Conf. Learn.\n\nRepresent. (ICLR), 2021.\n\nAndrew Cotter, Heinrich Jiang, Maya R Gupta, Serena Wang, Taman Narayan, Seungil You, and Karthik Sridharan. Optimization with non-differentiable constraints with applications to fairness, recall, churn, and other goals. Journal of Machine Learning Research (JMLR), 20(172):1–59, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of the 2019 Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1 (NAACL-HLT (1)), pp. 4171–4186, 2019.\n\nDheeru Dua, Casey Graff, et al. UCI machine learning repository. http://archive.ics.uci.\n\nedu/ml, 2017.\n\nJohn C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses against\n\nmixture covariate shifts. Under review, 2019.\n\nSanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush Varshney. Is there a trade-off between fairness and accuracy? A perspective using mismatched hypothesis testing. In Int. Conf. Mach. Learn. (ICML), pp. 2803–2813. PMLR, 2020.\n\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through\n\nawareness. In Conf. Innov. Theor. Compu. Scien. (ITCS), pp. 214–226, 2012.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nHong Fan, Wu Du, Abdelghani Dahou, Ahmed A Ewees, Dalia Yousri, Mohamed Abd Elaziz, Ammar H Elsheikh, Laith Abualigah, and Mohammed AA Al-qaness. Social media toxicity classification using deep learning: Real-world application UK brexit. Electronics, 10(11):1332, 2021.\n\nClare Garvie. The perpetual line-up: Unregulated police face recognition in America. Georgetown\n\nLaw, Center on Privacy & Technology, 2016.\n\nArthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch ̈olkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In Int. Conf. Algo. Learn. Theory (ALT), pp. 63–77. Springer, 2005.\n\nMoritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Adv.\n\nNeural Inform. Process. Syst. (NeurIPS), volume 29, 2016.\n\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In Int. Conf. Mach. Learn. (ICML), pp. 1929–1938. PMLR, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 770–778, 2016.\n\nHeinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In Int.\n\nConf. Artificial Intelligence and Statistics (AISTATS), pp. 702–712. PMLR, 2020.\n\nRay Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair\n\nclassification. In Uncertainty in Artificial Intelligence (UAI), pp. 862–872. PMLR, 2020.\n\nChi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave\n\nminimax optimization? In Int. Conf. Mach. Learn. (ICML), pp. 4880–4889. PMLR, 2020.\n\nSurya Mattu Julia Angwin, Jeff Larson and Lauren Kirchner. There’s software used across the country to predict future criminals. and its biased against blacks. ProPublica, 2016. URL https://github.com/propublica/compas-analysis.\n\nSangwon Jung, Donggyu Lee, Taeeon Park, and Taesup Moon. Fair feature distillation for visual\n\nrecognition. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 12115–12124, 2021.\n\nSangwon Jung, Sanghyuk Chun, and Taesup Moon. Learning fair classifiers with partially annotated\n\ngroup labels. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 10348–10357, 2022.\n\nFaisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimi-\n\nnation. Knowledge and Information Systems (KAIS), 33(1):1–33, 2012.\n\nAmir E Khandani, Adlar J Kim, and Andrew W Lo. Consumer credit-risk models via machine-\n\nlearning algorithms. Journal of Banking & Finance (JBF), 34(11):2767–2787, 2010.\n\nJyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear\n\npredictors. Information and Computation, 132(1):1–63, 1997.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In Int. Conf. Mach. Learn. (ICML), pp. 5637–5664. PMLR, 2021.\n\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (REx). In Int. Conf. Mach. Learn. (ICML), pp. 5815–5826. PMLR, 2021.\n\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In Int. Conf. Mach. Learn. (ICML), pp. 6781–6792. PMLR, 2021.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\n\nInt. Conf. Comput. Vis. (ICCV), pp. 3730–3738, 2015.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In Int.\n\nConf. Learn. Represent. (ICLR), 2017.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn.\n\nRepresent. (ICLR), 2019.\n\nDebmalya Mandal, Samuel Deng, Suman Jana, Jeannette Wing, and Daniel J Hsu. Ensuring fairness\n\nbeyond the training data. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume 33, 2020.\n\nArkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization (SIOPT), 19 (4):1574–1609, 2009.\n\nLaurent Son Nguyen and Daniel Gatica-Perez. Hirability in the wild: Analysis of online conversa-\n\ntional video resumes. IEEE Trans. Multimedia, 18(7):1422–1437, 2016.\n\nYonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust language modeling. In Conf. Empirical Methods in Natural Language Processing (EMNLP), pp. 4227–4237, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, highperformance deep learning library. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume 32, 2019.\n\nNovi Quadrianto, Viktoriia Sharmanska, and Oliver Thomas. Discovering fair representations in the\n\ndata domain. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 8227–8236, 2019.\n\nAlfr ́ed R ́enyi. On measures of dependence. Acta Mathematica Hungarica, 10(3-4):441–451, 1959.\n\nAshkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian Ziebart. Fairness for robust log loss classification. In Proc. of the AAAI Conf. Artificial Intelligence (AAAI), volume 34, pp. 5511–5518, 2020.\n\nYuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection for\n\nmodel fairness. Int. Conf. Learn. Represent. (ICLR), 2020.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. Int. Conf. Learn. Represent. (ICLR), 2020.\n\nDvir Samuel and Gal Chechik. Distributional robustness loss for long-tail learning. In Int. Conf.\n\nComput. Vis. (ICCV), pp. 9495–9504, 2021.\n\nXinyue Shen, Steven Diamond, Yuantao Gu, and Stephen Boyd. Disciplined convex-concave programming. In IEEE 55th Conf. Decision and Control (CDC), pp. 1009–1014. IEEE, 2016.\n\nBahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. A distributionally robust\n\napproach to fair classification. arXiv preprint arXiv:2007.09530, 2020.\n\nLeslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.\n\nMei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In Int. Conf. Comput. Vis. (ICCV), pp. 692–702, 2019.\n\nSerena Wang, Wenshuo Guo, Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, and Michael Jordan. Robust optimization for fairness with noisy protected groups. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume 33, 2020.\n\nYijie Wang, Viet Anh Nguyen, and Grani A Hanasusanto. Wasserstein robust classification with\n\nfairness constraints. arXiv preprint arXiv:2103.06828, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nChuanlong Xie, Fei Chen, Yue Liu, and Zhenguo Li. Risk variance penalization: From distributional\n\nrobustness to causality. arXiv preprint arXiv:2006.07544, 2020.\n\nMikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ML models with\n\nsensitive subspace robustness. arXiv preprint arXiv:1907.00020, 2019.\n\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proc. of the 26th Int. Conf. World Wide Web (WWW), pp. 1171–1180, 2017a.\n\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In Int. Conf. Artificial Intelligence and Statistics (AISTATS), pp. 962–970. PMLR, 2017b.\n\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial\n\nlearning. In AAAI/ACM Conf. AI, Ethics, and Society (AIES), pp. 335–340, 2018.\n\nZhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial\n\nautoencoder. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 5810–5818, 2017.\n\nChunting Zhou, Daniel Levy, Xian Li, Marjan Ghazvininejad, and Graham Neubig. Distributionally robust multilingual machine translation. In Conf. Empirical Methods in Natural Language Processing (EMNLP), pp. 5664–5674, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nWe provide additional materials in this document. We include the proofs for Lemma 1 and Proposition 1 and 2 in Appendix A, additional related work in Appendix B, and implementation details such as optimization, implementations of baselines, and hyperparameter search in Appendix C. Finally, we report additional experimental results in Appendix D.\n\nA PROOFS\n\nProof of Proposition 1 We define Ly ∈ R|A|×|A| for brevity:\n\nLy\n\ni,j\n\nIf we denote Λy ∈ R|A|2\n\nas\n\n≜L(θ, Dy\n\ni ) − L(θ, Dy\n\nj ), 1 ≤ ∀i, j ≤ |A|.\n\nΛy ≜ (Ly\n\n1,1, ..., Ly\n\n|A|,1, Ly\n\n1,2, ..., Ly\n\n|A|,2, ..., Ly\n\n|A|,|A|),\n\nthen, we have the following chain of equalities:\n\nVar (cid:0){L(θ, Dy\n\na)}a∈A\n\n(cid:1) =\n\n1 |A|\n\n(cid:88)\n\na\n\n(cid:0)L(θ, Dy\n\na)(cid:1)2\n\n−\n\n(cid:16) 1\n\n(cid:88)\n\n|A|\n\na\n\n(cid:0)L(θ, Dy\n\na)(cid:1)(cid:17)2\n\nL(θ, Dy\n\na) − L(θ, Dy\n\na′)\n\n(cid:17)2\n\n=\n\n1 2|A|2\n\n(cid:88)\n\n(cid:88)\n\n(cid:16)\n\na\n\na′\n\n=\n\n1\n\n2|A|2 ∥Λy∥2 2.\n\nNote that as we consider l to be zero-one loss function, we have the followings:\n\n(cid:98)∆y ≜ max\n\na,a′\n\n= max a,a′\n\n(cid:12) (cid:12) (cid:12)\n\n1 |Dy a|\n\n|Dy a| (cid:88)\n\ni=1\n\n1{θ(xi) ̸= y} −\n\n1 |Dy a′|\n\n|Dy a′| (cid:88)\n\ni=1\n\n(cid:12) (cid:12) 1{θ(xi) ̸= y} (cid:12)\n\n|L(θ, Dy\n\na) − L(θ, Dy\n\na′)|\n\n= ∥Λy∥∞.\n\ni(Λy i )2 ≤ (cid:112)(cid:80)\n\ni | ≤ (cid:112)(cid:80) Since ∥Λy∥∞ = maxi |Λy also that ∥Λy∥2 = (cid:112)(cid:80) i(Λy obtain the first inequality of (2). From (2), we obtain (cid:98)∆y = 0 if Var (cid:0){L(θ, Dy (cid:1) = 0 for all y ∈ Y. Considering variance is always non-negative, we obtain the necessary condition that (cid:98)∆y = 0 implies Var (cid:0){L(θ, Dy\n\ni )2 = ∥Λy∥2, we obtain the second inequality of (2). Note ∞ = |A|∥Λy∥∞. We thereby\n\n∞ = (cid:112)|A|2∥Λy∥2\n\n(cid:1) = 0 for all y ∈ Y.\n\ni ∥Λy∥2\n\na)}a∈A\n\na)}a∈A\n\nProof of Lemma 1 Suppose that a model parameter θ is given. As (cid:80)\n\n(cid:80)\n\na∈A\n\n1\n\n|A| L(θ, Dy\n\na) is a constant, we can decompose (cid:80)\n\na∈A\n\naL(θ, Dy qy\n\na) into\n\na∈A\n\n(cid:16)\n\na − 1 qy\n\n|A|\n\n(cid:17)\n\n= 0 and\n\n(cid:88)\n\na∈A\n\naL(θ, Dy qy\n\na) =\n\n=\n\n(cid:88)\n\na∈A (cid:88)\n\na∈A\n\n1 |A|\n\n1 |A|\n\nL(θ, Dy\n\na) +\n\nL(θ, Dy\n\na) +\n\n(qy\n\na −\n\n(qy\n\na −\n\n(cid:88)\n\na∈A (cid:88)\n\na∈A\n\n1 |A|\n\n1 |A|\n\n)L(θ, Dy a)\n\n(cid:16) )\n\nL(θ, Dy\n\na) −\n\n(cid:88)\n\na∈A\n\n1 |A|\n\n(cid:17)\n\nL(θ, Dy a)\n\n.\n\n|A|111, where 111 ∈ R|A|. From (cid:80) Let vy ≜ qy − 1 (cid:16) = (cid:80) |A| (|A|qy\n\nConsidering Dφ\n\n|A|111\n\n(cid:12) (cid:12) 1\n\nq (cid:12) (cid:12)\n\na∈A qy a −1)2 = (cid:80)\n\na = 1, we have (cid:80) a∈A |A|(qy\n\na∈A\n\n(cid:17)\n\n1\n\na∈A vy a − 1\n\na = 0. |A| )2 = |A| (cid:80)\n\na∈A(vy\n\na)2,\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nthe following problems are equivalent:\n\nmax qy∈Qρ\n\n(cid:88)\n\na∈A\n\naL(θ, Dy qy\n\na) = max\n\nvy\n\nsubject to\n\n(cid:88)\n\na∈A (cid:88)\n\na∈A\n\nL(θ, Dy\n\na) +\n\n(cid:88)\n\n(cid:16)\n\nvy\n\na\n\nL(θ, Dy\n\na) −\n\n1 |A|\n\n(cid:88)\n\na∈A\n\n1 |A|\n\n(cid:17)\n\nL(θ, Dy a)\n\na = 0, ∥vy∥2 vy\n\n2 ≤\n\na∈A ρ\n|A|\n\n.\n\nAlso, we have the following inequalities:\n\n(cid:16)\n\nvy\n\na\n\n(cid:88)\n\na∈A\n\nL(θ, Dy\n\na) −\n\n(cid:88)\n\na∈A\n\n1 |A|\n\n(cid:17)\n\nL(θ, Dy a)\n\n≤\n\n(cid:115)(cid:88)\n\n(vy\n\na)2\n\n(cid:115)\n\n(cid:88)\n\n(cid:16)\n\nL(θ, Dy\n\na) −\n\na∈A\n\na∈A\n\n(cid:88)\n\na∈A\n\n1 |A|\n\nL(θ, Dy a)\n\n(cid:17)2\n\n≤\n\n(cid:113)\n\n(cid:114) ρ |A|\n\n|A| × Var (cid:0){L(θ, Dy\n\na)}a∈[A]\n\n(cid:1),\n\n(A.1)\n\n(A.2)\n\nin which (A.1) holds by the Cauchy-Schwartz inequality. The second inequality follows from\n\nVar (cid:0){L(θ, Dy\n\na)}a∈A\n\n(cid:1) =\n\n1 |A|\n\n(cid:88)\n\n(cid:16)\n\na∈A\n\nL(θ, Dy\n\na) −\n\n(cid:88)\n\na∈A\n\n1 |A|\n\nL(θ, Dy a)\n\n(cid:17)2\n\n,\n\n∥vy∥2\n\n2 ≤\n\nρ |A|\n\n.\n\nThe equality in (A.2) is attained if and only if the vector vy satisfies: (i) vy and the |A|-dimensional 1\nvector whose a-th element is L(θ, Dy 2 = |A| . This implies that the a-th element of vy should be\n\na) are in the same direction; (ii) ||vy||2\n\n|A| L(θ, Dy\n\na) − (cid:80)\n\na∈A\n\nρ\n\nvy\n\na =\n\n(cid:114) ρ |A|\n\n×\n\n(cid:115)\n\nL(θ, Dy\n\na) − (cid:80)\n\na∈A\n\n1\n\n|A| L(θ, Dy a)\n\n(cid:16)\n\n(cid:80)\n\na∈A\n\nL(θ, Dy\n\na) − (cid:80)\n\na∈A\n\n1\n\n|A| L(θ, Dy a)\n\n(cid:17)2\n\n=\n\n(cid:114) ρ |A|\n\n×\n\n(cid:113)\n\nL(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy a)\n\na∈A\n\n|A| × Var (cid:0){L(θ, Dy\n\na)}a∈A\n\n.\n\n(cid:1)\n\nThus, Lemma 1 holds if and only if\n\nqy a =\n\n1 |A|\n\n+\n\n(cid:114) ρ |A|\n\n×\n\nfor all a ∈ A.\n\nL(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy a)\n\n∥L(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy\n\na)∥2\n\na∈A\n\na∈A\n\nProof of Proposition 2 From the proof of Lemma 1, the equality in (5) holds if and only if\n\nqy a =\n\n1 |A|\n\n+\n\n(cid:114) ρ |A|\n\n×\n\nL(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy a)\n\n∥L(θ, Dy\n\na) − (cid:80)\n\n1\n\n|A| L(θ, Dy\n\na)∥2\n\na∈A\n\na∈A\n\nfor ∀a ∈ A. We now consider each element of qy∗. For given y ∈ Y, we define γy ∈ R|A| as follows:\n\nγy\n\na\n\n≜\n\nL(θ, Dy ∥L(θ, Dy\n\na) − (cid:80) a) − (cid:80)\n\n1\n\n|A| L(θ, Dy a) |A| L(θ, Dy a)∥2\n\n1\n\na\n\na\n\n,\n\nfor any a ∈ A. Fix i ∈ A by symmetry. Then, we have:\n\na = 0, and thus, γy γy\n\ni = −\n\n(cid:88)\n\na∈A\n\n(cid:88)\n\nγy a ,\n\na∈A\\i\n\n(γy\n\na )2 = 1, and thus, (γy\n\ni )2 = 1 −\n\n(cid:88)\n\na∈A\n\n(cid:88)\n\n(γy\n\na )2.\n\na∈A\\i\n\n(A.3)\n\n(A.4)\n\n15\n\n,\n\n,\n\nPublished as a conference paper at ICLR 2023\n\nBy the Cauchy-Schwarz inequality, i | = |(1, ..., 1) · (γy (cid:113) ≤ (cid:112)|A| − 1 ·\n\n|γy\n\n1 , ..., γy\n\ni−1, γy\n\ni+1, ..., γy\n\n|A|)| i−1)2 + (γy\n\n(γy\n\n1 )2 + · · · + (γy\n\ni+1)2 + · · · + (γy\n\n|A|)2\n\n= (cid:112)|A| − 1 ·\n\n(cid:113)\n\n1 − (γy\n\ni )2,\n\nin which the first and last equality follow from (A.3) and (A.4), respectively.\n\nFrom the above inequality, we have the followings:\n\n(γy |A|(γy\n\ni )2 ≤ (|A| − 1) · (1 − (γy i )2 ≤ |A| − 1, (cid:115)\n\ni )2),\n\n|(γy\n\ni )| ≤\n\n|A| − 1 |A|\n\n.\n\n(A.5)\n\nConsidering (A.3), the equality in (A.5) is attained if and only if\n\n(cid:115)\n\n(cid:115)\n\nγy\n\ni = ±\n\nγy\n\na = ∓\n\n|A| − 1 |A|\n\n,\n\n1 |A|(|A| − 1)\n\n, ∀a ∈ A\\i.\n\nThus, we can derive the range described in Proposition 2.\n\nB ADDITIONAL RELATED WORKS\n\nB.1 FAIRNESS-CONSTRAINED OPTIMIZATION\n\nAs mentioned in Sec. 2.1, some works (Agarwal et al., 2018; Cotter et al., 2019) employ a constrained optimization framework to enforce the group fairness. They formulate a minimax problem of the Lagrangian function from the given constrained optimization problem and find a saddle point by alternating the outer minimization step w.r.t. model parameters and the inner maximization step w.r.t. the Lagrangian variables. When updating the model parameters, they employ a convex upper bounded surrogate of 0-1 loss such as hinge loss or cross entropy loss in order to address non-differentiability of the fairness constraints. Further, Cotter et al. (2019) employ a non-zero sum formulation that uses the surrogates for updating the model parameters and use the original 0-1 loss for updating the Lagrangian dual variables.\n\nAlthough both Cotter et al. (2019) and our method similarly solve the minimax problems with 0-1 loss terms by using surrogate losses in the minimization step (w.r.t. model parameters) and the original 0-1 loss in the maximization step, we emphasize that Fair DRO address a different kind of minimax problem compared to that of Cotter et al. (2019). Namely, their approach solves a minimax problem involving both model parameters and Lagrangian variables, while we only focus on the minimization w.r.t. model parameters, with a fixed ρ (which corresponds to a fixed Lagrangian variable). Instead, based on our theoretical results that the minimization problem w.r.t. model parameters can be formulated as another minimax problem based on the DRO framework, FairDRO solves such minimax problem by the non-zero sum formulation.\n\nFurthermore, we argue that our surrogates would be more advantageous in terms of bounding the original objective function. When implementing the algorithm of Cotter et al. (2019), they make surrogates of group fairness constraints by typically using an upper bounded function of 0-1 loss such as the hinge loss or the cross entropy loss. However, the modified fairness constraint terms with such surrogates sometimes are not necessarily sharp upper bounds of the original constraints, e.g., multi-class classification setting. The reason is because the group fairness constraints, such as EO or ECA, are typically defined as the gap of averaged 0-1 losses for measuring the parity over groups. However, if we use the cross entropy loss as a surrogate in equation (6), we can get valid convex upper bound of (7) whenever qy is a maximizer of the inner maximization problem, and all entries of qy are positive values. Thus, we expect that our FairDRO can outperform PL by solving the original objective function better, which is shown in our experimental results.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.2 ROBUST OPTIMIZATION IN ALGORITHMIC FAIRNESS\n\nMuch more works related to algorithmic fairness have recently utilized the DRO frameworks for their own goals. A line of works embeds the DRO frameworks into the fairness-constraint optimization problem to make a model fair, and robust to distribution shifts of a test dataset (Jiang et al., 2020; Taskesen et al., 2020; Wang et al., 2021; Mandal et al., 2020). For example, Mandal et al. (2020) aimed to train fair classifiers to be with respect to weighted perturbations in the training distribution by considering all possible weight set over the training samples. Wang et al. (2021) and Taskesen et al. (2020) proposed DRO objectives with Wasserstein uncertainty sets which are combined by group fairness constraints. Although all aforementioned methods showed improvements in group fairness and robustness, the key differences from FairDRO are that we leverage the DRO framework for directly promoting group fairness, not robustness, and consider the uncertainty set over groups, not samples.\n\nSome works proposed DRO formulations for achieving fairness in more challenging settings like learning with imperfect group labels or sequential learning. Wang et al. (2020) proposed how to achieve the group fairness successfully given noisy group labels only, by considering the worst-case distribution of group labels. Hashimoto et al. (2018) devised repeated loss minimization using the DRO-based training objective with the χ2-divergence uncertainty set for encouraging long-term fairness in a sequential setting where the group populations changes over time. However, they do not also use DRO as a direct tool for achieving group fairness.\n\nThere are some recent works perhaps closest to our method in terms of using DRO for achieving the fairness. Rezaei et al. (2020) incorporated a group fairness constraint into statistic-matching based robust log-loss minimization problem for training fair logistic regression models. Although they showed nice theoretical benefits in terms of its convexity and convergence, their method has a limitation that group information is needed at the test time. Yurochkin et al. (2019) aims to achieve the fairness of a classifier using DRO frameworks. However, they only focus on a novel individual fairness notion called as distributionally robust fairness (DRF), not group fairness.\n\nB.3 MINIMAX OPTIMIZATION IN ALGORITHMIC FAIRNESS\n\nSeveral works adopted min-max optimization techniques for group fairness (although they are not based on DRO frameworks). Zhang et al. (2018) proposed an adversarial debiasing technique that eliminates group information of latent features of a neural network (NN) model through the minimax game of an adversary and the NN model. Baharlouei et al. (2020) and Jiang et al. (2020) proposed novel regularization terms for group fairness using R ́enyi correlation and Wasserstein distance respectively, which lead to min-max formulations. We emphasize that these methods rely on additional terms or adversaries for promoting group fairness, while our method directly uses the DRO-based min-max formulation as a fairness regularization term.\n\nC MORE IMPLEMENTATION DETAILS\n\nWe used PyTorch (Paszke et al., 2019); Experiments are performed on a server with AMD Ryzen Threadripper PRO 3975WX CPUs and NVIDIA RTX A5000 GPUs. All models are evaluated on separate test sets, and all experiments are repeated with 4 different random seeds. All the reported results are the averaged results.\n\nC.1 OPTIMIZATION\n\nFor tabular and vision datasets, we train all models with the AdamW optimizer (Loshchilov & Hutter, 2019) for 70 epochs. We set the mini-batch size and the weight decay as 128 and 0.001, respectively. The initial learning rate is set as 0.001 and decayed by cosine annealing technique (Loshchilov & Hutter, 2017).\n\nFor the language dataset, we fine-tune pre-trained BERT with the AdamW optimizer for 3 epochs. We set the mini-batch size and the weight decay as 24 and 0.001, respectively. The initial learning rate is set as 0.00002 and adjusted with a learning rate schedule using a warm-up phase followed by a linear decay. All results are reported for the model at the last epoch.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nC.2\n\nIMPLEMENTATION DETAILS OF IN-PROCESSING BASELINES\n\nHere, we describe the implementation details of each in-processing baseline used for the experiments.\n\nThe original LBC (Jiang & Nachum, 2020) requires multiple full-training iterations by alternatively re-weighting each group based on the given fairness metric and re-training the full dataset. Since this optimization procedure needs a very high computation budget, we modify full-training iterations to 5 epochs and 14 iterations for vision and language datasets.\n\nFairBatch (Roh et al., 2020) and RW (Kamiran & Calders, 2012) are implemented in the way they were originally proposed.\n\nCov (Zafar et al., 2017b) utilized a fairness constraint based on Covariance between the group label and the signed distance of the feature vectors from the decision boundary of a classifier. Although they used the Disciplined convex-concave program (DCCP) (Shen et al., 2016) solver for the constrained problem, we apply our optimization procedure (i.e., AdamW) by setting the covariance-based constraint as a regularization term. We note that extenstion Cov to multi-class classifcation tasks is non-trivial because the signed distance is not obviously defined for multi-class decision boundary.\n\nMFD (Jung et al., 2021) and FairHSIC (Quadrianto et al., 2019) use additional fairness-promoting regularization terms based on MMD and HSIC. For FairHSIC, we only implement the second term of their decomposition loss (i.e., the HSIC loss between the feature representations and the group labels). For implementing their regularization terms, we use the Gaussian RBF kernel of which the variance parameter is set as the mean of squared distance between all data points, following Jung et al. (2022). Since regularization terms used in both MFD and FairHSIC are applied to feature vectors of DNN model, they are limited to DNN-based models.\n\nBaharlouei et al. (2020) attempted to remove the non-linear dependency between the model prediction and the sensitive attribute by using R ́enyi correlation as a regularization term. The resulting training objective is a minimax optimization problem where the maximization is for computing the R ́enyi correlation, and the minimization is for learning a model. When the group label is binary, we use modified Algorithm 2 in Baharlouei et al. (2020), where the minimization step (i.e., line 4 in Algorithm 2) is processed in a mini-batch manner. Although Algorithm 1 is designed for the non-binary group label, applying the algorithm to a task with non-binary group labels is not straightforward because the second term in the RHS of equation 6 is a biased estimator when using the mini-batch optimization.\n\nC.3 HYPERPARAMETERS FOR MAIN RESULTS\n\nWe perform the grid search on the hyperparameter candidates for every method. The full hyperparameter search space is illustrated in Tab. C.1.\n\nC.4 MODEL SELECTION RULE FOR A SINGLE EVALUATION METRIC\n\nThe accuracy-fairness trade-off may exist in many real-world applications (Dutta et al., 2020); better accuracy leads to worse fairness, and vice versa. For example, Fig. 1 shows the trade-off for the Adult and COMPAS datasets. Therefore, when using a single evaluation the model should be carefully selected for fair evaluation. To that end, we explore varying hyperparameters (HPs) of each method for controlling the strength between accuracy and fairness. We then select the best model showing the best fairness criterion ∆DCA while achieving at least 95% of the accuracy of the vanilla-trained model. With this selection rule, we report the best performance of each model throughout Appendix D. The HPs for each method were extensively searched, but fairly in terms of computation budgets. The search range of each hyperparameter is listsed in Tab. C.1.\n\nD ADDITIONAL RESULTS\n\nD.1 RESULT TABLES\n\nFollowing our model selection rule, described in Appendix C.4, we report the best performances of each result in Tab. D.1 and Tab. D.2. The numbers in the parentheses correspond to the standard\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable C.1: Hyperparameter search spaces. We perform the grid search to find the best hyperparameters for each method.\n\nMethod\n\nHyperparameter\n\nSearch range\n\nCov (Zafar et al., 2017b)\n\nCovariance strength λ\n\nR ́enyi (Baharlouei et al., 2020)\n\nR ́enyi correlation strength λ\n\nMFD (Jung et al., 2021)\n\nFairHSIC (Quadrianto et al., 2019)\n\nMMD strength λ\n\nHSIC strength λ\n\nLBC (Jiang & Nachum, 2020)\n\nLR for re-weights η\n\nFairBatch (Roh et al., 2020)\n\nLR for mini-batch size of each group α\n\nEGR (Agarwal et al., 2018)\n\nPL (Cotter et al., 2019)\n\nLR for Lagrange multiplier η L1-norm bound B\n\nLR for Lagrange multiplier η Violation upper bound κ\n\nFairDRO\n\nRadius of χ2-divergence ball ρ\n\n[10−2, 102]\n\n[10−2, 103]\n\n[10−2, 104]\n\n[10−2, 104]\n\n[10−3, 102]\n\n[10−3, 102]\n\n[10−2, 102] [10−2, 101]\n\n[10−2, 102] [10−2, 101]\n\n[10−2, 102]\n\nTable D.1: The best performances on Adult and COMPAS datasets. The number in the parentheses with ± stands for the standard deviation of each metric obtained from several independent runs with 4 different seeds. The target accuracy (higher is better) and DCA (1) (lower is better) are shown. We follow the proposed model selection criterion in Appendix C.4.\n\nAdult\n\nCOMPAS\n\nAcc. (↑)\n\n∆DCA (↓)\n\nAcc. (↑)\n\n∆DCA (↓)\n\nScratch\n\n81.95 (±0.03)\n\n6.83 (±0.06)\n\n63.29 (±0.00)\n\n14.51 (±0.00)\n\nRW (Kamiran & Calders, 2012) LBC (Jiang & Nachum, 2020) FairBatch (Roh et al., 2020)\n\n81.98 (±0.04) 81.21 (±0.17) 81.63 (±0.03)\n\n6.84 (±0.09) 4.92 (±0.27) 5.75 (±0.09)\n\n63.29 (±0.15) 62.04 (±0.24) 62.38 (±0.07)\n\n14.87 (±0.62) 6.03 (±1.13) 6.71 (±1.07)\n\nCov (Zafar et al., 2017a) R ́enyi (Baharlouei et al., 2020)\n\n78.68 (±0.12) 77.09 (±0.21)\n\n5.08 (±0.51) 4.86 (±0.78)\n\n60.31 (±0.30) 61.44 (±0.35)\n\nEGR (Agarwal et al., 2018) PL (Cotter et al., 2019)\n\n80.59 (±0.04) 79.49 (±0.14)\n\n6.03 (±0.07) 2.55 (±0.39)\n\n60.66 (±0.73) 61.95 (±0.35)\n\n7.58 (±3.71) 6.77 (±0.86)\n\n7.47 (±0.94) 6.73 (±0.95)\n\nFairDRO\n\n79.23 (±0.11)\n\n1.99 (±0.38)\n\n61.97 (±0.24)\n\n5.20 (±0.92)\n\ndeviation. From Tab. D.1 and Tab. D.2, we observe FairDRO achieves the best ∆DCA with moderate variances. For example, in Tab. D.1, while the best baselines achieve ∆DCA of 2.55 and 6.03 on Adult and COMPAS, respectively, our FairDRO achieves 1.99 and 5.20 for the same datasets, respectively. We also emphasize that although the accuracy of FairDRO is slightly worse than the one of RW and Scratch on COMPAS, FairDRO shows significantly better DCA (5.20) compared to RW (14.87) and Scratch (14.51).\n\nIn Tab. D.2, we observe consistent results with Tab. D.1. We again note that we marked results of COV and FairHSIC as N/A because COV is designed only for binary classification and FairHSIC fails to converge due to the gradient exploding. We again observe that FairDRO achieves the best level of the fairness metric (DCA) compared to other methods. We also provide difference of Equalized Odds (DEO) on UTKFace, denoted by ∆DEO, which is defined as follows:\n\n∆DEO ≜ 1\n\n|Y|2\n\n(cid:88)\n\ny,y′∈Y\n\nmax a,a′\n\n|P ( (cid:98)Y = y′|A = a, Y = y) − P ( (cid:98)Y = y′|A = a′, Y = y)|.\n\nNote that ∆DCA and ∆DEO are the same for binary labels. Although LBC, FairBatch, and FairHSIC were devised for achieving EO, we again show that FairDRO outperforms baselines on UTKFace with respect to DEO.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable D.2: The best performances on UTKFace and CivilComments. The number in the parentheses with ± stands for the standard deviation of each metric obtained from several independent runs with 4 different seeds. “N/A” denotes the method is not trainable. All other details are the same as Tab. D.1.\n\nAcc. (↑)\n\nUTKFace ∆DCA (↓)\n\n∆DEO (↓)\n\nAcc. (↑)\n\n∆DCA (↓)\n\nCivilComments\n\nScratch\n\n78.83 (±0.35)\n\n19.42 (±1.44)\n\n13.78 (±0.95)\n\n78.12 (±1.28)\n\n12.24 (±2.37)\n\nRW (Kamiran & Calders, 2012) LBC (Jiang & Nachum, 2020) FairBatch (Roh et al., 2020)\n\n79.27 (±1.10) 75.58 (±1.84) 78.04 (±0.76)\n\nCov (Zafar et al., 2017a) FairHSIC (Quadrianto et al., 2019) MFD (Jung et al., 2021)\n\nEGR (Agarwal et al., 2018) PL (Cotter et al., 2019)\n\n77.35 (±0.91) 81.83 (±0.95)\n\n78.71 (±0.67)\n\n17.92 (±0.60) 16.92 (±1.83) 15.92 (±2.47)\n\nN/A 17.58 (±1.67) 16.08 (±0.76)\n\nN/A 16.42 (±0.83)\n\n11.22 (±0.00) 12.89 (±1.03) 12.39 (±0.96)\n\n76.58 (±1.63) 75.85 (±2.17) 77.18 (±1.30)\n\n7.78 (±1.19) 7.04 (±0.97) 18.91 (±3.80)\n\n12.83 (±0.73) 11.08 (±0.55)\n\n11.89 (±0.52)\n\n76.39 (±1.11)\n\n12.18 (±4.07)\n\nN/A\n\n76.70 (±1.09)\n\n11.36 (±2.78)\n\n76.40 (±3.04) 78.00 (±1.67)\n\n9.53 (±6.22) 10.38 (±3.37)\n\nFairDRO\n\n79.48 (±0.59)\n\n14.25 (±1.26)\n\n11.06 (±1.02)\n\n77.65 (±1.75)\n\n7.07 (±1.44)\n\nCelebA\n\nAcc. (↑)\n\n∆DCA (↓)\n\n85.80 (±0.55) 14.24 (±0.84) Scratch 77.60 (±0.68) 15.49 (±1.41) RW (Kamiran & Calders, 2012) 88.96 (±0.97) 6.67 (±2.39) LBC (Jiang & Nachum, 2020) 83.85 (±0.70) 10.62 (±0.86) FairBatch (Roh et al., 2020) 90.10 (±0.25) 7.57 (±0.84) Cov (Zafar et al., 2017a) 86.22 (±0.79) 10.62 (±1.10) R ́enyi (Baharlouei et al., 2020) FairHSIC (Quadrianto et al., 2019) 86.63 (±0.64) 9.10 (±3.19) 90.97 (±0.35) 4.44 (±1.13) MFD (Jung et al., 2021) 83.23 (±1.06) 1.46 (±0.84) PL (Cotter et al., 2019) 87.95 (±1.61) 2.57 (±0.82) FairDRO\n\nTable D.3: The best performances on CelebA. Details are the same as Tab. D.1.\n\nFigure D.1: Trade-offs between accuracy and DCA on CelebA. The “Scratch” performances are highlighted by the dotted lines\n\nD.2 CELEBA RESULTS\n\nTo show consistent improvements, we conducted an additional experiment on the CelebA dataset, which includes about 200K celebrities’ face images with 40 annotated binary attributes. Among the attributes, we select “blond hair” as the class label and “gender” as the group label, which is the set of attributes widely used in many previous works (Sagawa et al., 2020; Jung et al., 2022; Chuang & Mroueh, 2021; Liu et al., 2021).\n\nTab. D.3 and Fig. D.1 show a similar result trend to one of UTKFace in Sec. 5.2. FairDRO again outperforms the baselines in terms of DCA and achieves a competitive trade-off with MFD. We also observe re-weighting based baselines show worse DCA than FairDRO, meaning that FairDRO finds the better weights over groups for lower DCA.\n\nD.3 RESULTS WITH THE STANDARD ERM LOSS\n\nFor fair comparison with FairDRO, we used the balanced ERM loss for the training of each baseline as mentioned in Sec. 5. Since all baselines were originally designed for employing the standard ERM, we further reported the results for all methods solving the standard ERM loss. Fig. D.2 show the best performances and the accuracy-fairness trade-offs on Adult, COMPAD, UTKFace, and CivilComments datasets.\n\n20\n\n5101520DCA (%)77.580.082.585.087.590.0Accuracy (%)CelebAScratchRWLBCFairBatchFairHSICMFDCovRényiPLFairDRO (ours)Published as a conference paper at ICLR 2023\n\nFigure D.2: The trade-offs between accuracy and DCA. Each method is trained with the standard ERM. Details are the same as Fig. 1.\n\nTable D.4: Ablation study of FairDRO. RVP (w/ classwise) and FairDRO (w/o classwise) are for the ablation study of our key two components, the DRO formulation and classwise treatment, respectively. We follow the same model selection criterion introduced in Appendix C.4.\n\nAdult\n\nCOMPAS\n\nUTKFace\n\nCivilComments\n\nAcc. (↑) ∆DCA (↓) Acc (↑) ∆DCA (↓) Acc. (↑) ∆DCA (↓) Acc. (↑) ∆DCA (↓)\n\nScratch\n\nFairDRO (w/o DRO)\n\nRVP (w/ classwise)\n\nFairDRO (w/o classwise)\n\nFairDRO\n\n81.95\n\n80.47\n\n80.10\n\n78.45\n\n79.23\n\n6.83\n\n4.86\n\n4.62\n\n1.62\n\n1.99\n\n63.29\n\n62.20\n\n61.99\n\n60.72\n\n61.97\n\n14.51\n\n8.55\n\n9.74\n\n6.28\n\n5.20\n\n78.83\n\n79.21\n\n77.25\n\n78.25\n\n79.48\n\n19.42\n\n17.58\n\n12.33\n\n20.75\n\n14.25\n\n78.12\n\n78.13\n\n77.42\n\n77.65\n\nN/A\n\n12.24\n\n9.79\n\n8.85\n\n7.07\n\nD.4 ABLATION STUDY FOR THE CHOICE OF UNCERTAINTY SET\n\nWe continue from Sec. 5.4 and provide further study of FairDRO. We analyze the effect of each constraint employed by FairDRO. Tab. D.5 shows the overall accuracy, ∆DCA and the worst accuracy over groups for each ablation case. “Classwise”, “χ2-divergence” and “Quasi-prob” (4) indicate each component added when defining the uncertainty set of Group DRO (i.e., simplex over groups) as described in Sec. 4. For a fair comparison, we apply the same optimization scheme (described in Sec. 4.4) for all DRO variants.\n\nAs the supremum in (3) is attained at the vertex of the simplex ∆|Y|×|A|, Group DRO minimizes the worst-case loss over the (y, a) pair, thereby it can prevent the lowest accuracy across the group and class from becoming too low. Thus, Group DRO substantially improves and achieves the best worst-case accuracy over groups compared to ERM (i.e., 1st −→ 2nd row). However, it shows the worst DCA among the DRO variants, confirming the mismatch between the criterion Group DRO uses and the group fairness metric. We observe that changing the simplex uncertainty set of Group DRO to the proposed χ2-divergence and quasi-prob uncertainty sets (4) (i.e., 2nd −→ 3rd row) has an impact on the group fairness mostly. In addition, reducing the size of uncertainty sets via classwise treatment (i.e., 3rd −→ 6th row) further improves the group fairness. Finally, including negative values in the uncertainty set also helps to improve the group fairness (i.e., 5th −→ 6th row) on most datasets, by more severely penalizing the major groups with high accuracy with negative weights.\n\n21\n\n24681012DCA (%)727476788082Accuracy (%)Adult46810121416DCA (%)58606264Accuracy (%)COMPAS141618202224DCA (%)7879808182Accuracy (%)UTKFace6810121416DCA (%)7274767880Accuracy (%)CivilCommentsScratchRWLBCCovRényiFairBatchFairHSICEGRMFDPLFairDRO (ours)Published as a conference paper at ICLR 2023\n\nTable D.5: Ablation study for uncertainty set of FairDRO. We show the effects of each component of the proposed FairDRO: GDRO and the constraints on the uncertainty set, including classwise uncertainty set, χ2-divergence ball, and quasi-probability. All numbers are average results over Adult, COMPAS, UTKFace, and CivilComments datasets. GDRO is an abbreviation for Group DRO.\n\nGDRO Classwise χ2-div. Q-prob.\n\nAdult\n\nCOMPAS\n\nUTKFace\n\nCivilComments\n\nAcc. ∆DCA Worst acc. Acc. ∆DCA Worst acc. Acc. ∆DCA Worst acc. Acc. ∆DCA Worst acc.\n\n✗\n\n✓\n\n✓\n\n✓\n\n✓\n\n✓\n\n✗\n\n✗\n\n✗\n\n✓\n\n✓\n\n✓\n\n✗\n\n✗\n\n✓\n\n✗\n\n✓\n\n✓\n\n✗\n\n✗\n\n✓\n\n✗\n\n✗\n\n✓\n\n81.95 6.83\n\n76.86\n\n63.29 14.51\n\n50.78\n\n78.83 19.42\n\n61.00\n\n78.12 12.24\n\n61.26\n\n81.89 6.18\n\n78.45 1.62\n\n81.68 5.89\n\n81.53 5.76\n\n78.62\n\n77.46\n\n77.67\n\n77.38\n\n62.06 6.68\n\n60.72 6.28\n\n62.10 6.41\n\n62.32 6.42\n\n57.57\n\n55.94\n\n53.59\n\n53.91\n\n79.54 18.17\n\n78.25 20.75\n\n78.69 14.50\n\n78.37 13.75\n\n61.00\n\n45.75\n\n63.75\n\n66.50\n\n80.32 12.63\n\n77.42 8.85\n\n79.99 9.16\n\n78.56 7.89\n\n69.72\n\n63.70\n\n71.52\n\n67.24\n\n79.23 1.99\n\n75.60\n\n61.97 5.20\n\n53.91\n\n79.48 14.25\n\n65.75\n\n77.65 7.07\n\n62.85\n\nFigure D.3: Accuracies and loss weights for each group (q) on COMPAS (y = 0). Details are the same as Fig. 4.\n\nD.5 VISUALIZATION OF q0\n\nWe continue from Sec. 5.4 and provide the visualization results including the weights (qy), training accuracy, and test accuracy for y = 0. In Fig. D.3 (a), Scratch shows that the significant accuracy gap between two groups is kept during all training time. On the other hand, we observe again in Fig. D.3 (b) that q0 initially fluctuates and eventually converges by our optimization procedure, and as a result, the large discrepancy of training accuracy is reduced at the end of training. Finally, we show from Fig. D.3 (c) that the test accuracy gap between the groups of FairDRO is smaller than that of Scratch, i.e., FairDRO achieves lower DCA.\n\nD.6 ABLATION STUDY OF SMOOTHED IBR\n\nWe observe the effectiveness of smoothed IBR updates introduced in Sec. 4.4. Fig. D.4 and Tab. D.6 compares dynamics of q1 and performance on COMPAS depending on whether or not our smoothing technique is used for FairDRO optimization. Jin et al. (2020) showed a theoretical result that the standard IBR asymptotically converge to an approximate stationary point w.r.t. θ under regularity assumptions. However, from Fig. D.4 (left), we still observe the instability of standard IBR; i.e., q oscillates whenever the group loss fluctuates. On the other hand, Fig. D.4 (right) shows that our smoothing technique has obvious effect on preventing the oscillation of q when using IBR. Furthermore, Tab. D.6 demonstrates that stable dynamics of q can significantly improve performance in terms of DCA.\n\nD.7\n\nISSUE OF STANDARD ACCURACY\n\nIn the experiments, we reported the balanced accuracy for measuring the performance of the models instead of the standard accuracy. While the balanced accuracy offers several advantages in measuring performance on imbalanced datasets, it also has some drawbacks. In general, it may underestimate the performance of certain groups with relatively large numbers. As in most cases, the groups with large numbers often become majority groups. The balanced accuracy metric may not capture accuracy\n\n22\n\n010203040506070Training epochs50607080Accuracy (%)(c) Scratch & FairDRO (test)010203040506070Training epochs01q0(a) Scratch (train)50607080Accuracy (%)010203040506070Training epochs01q0(b) FairDRO (train)50607080Accuracy (%)Group 0Group 1ScratchFairDROPublished as a conference paper at ICLR 2023\n\nFigure D.4: q on COMPAS. The groupwise loss weights during training for label 1 are shown. The reported weights are for FairDRO (w/o smoothing) (left) and FairDRO (right).\n\nTable D.6: The best performances on Adult and COMPAS datasets. We follow the same model selection criterion described in Appendix C.4.\n\nAdult\n\nCOMPAS\n\nAcc. (↑) ∆DCA (↓) Acc. (↑) ∆DCA (↓)\n\nScratch\n\nFairDRO (w/o smoothing)\n\nFairDRO\n\n81.95\n\n81.55\n\n79.23\n\n6.83\n\n5.76\n\n1.99\n\n63.29\n\n61.86\n\n61.97\n\n14.51\n\n5.50\n\n5.20\n\nTable D.7: The number of samples in Adult, COMPAS, and CelebA datasets.\n\nAdult\n\nCOMPAS\n\nCelebA\n\na = 0\n\na = 1\n\na = 0\n\na = 1\n\na = 0\n\na = 1\n\ny = 0\n\n13026\n\n20988\n\ny = 1\n\n1669\n\n9539\n\n2080\n\n1987\n\n1278\n\n17809\n\n67054\n\n822\n\n23060\n\n1567\n\nTable D.8: The number of samples in UTKFace and CivilComments datasets.\n\nUTKFace\n\na = 0\n\na = 1\n\na = 2\n\na = 3\n\na = 0\n\nCivilComments a = 2\n\na = 1\n\na = 3\n\ny = 0\n\ny = 1\n\ny = 2\n\n2049\n\n3735\n\n4294\n\n336\n\n3116\n\n1074\n\n1025\n\n1856\n\n553\n\n638\n\n11485\n\n17632\n\n8154\n\n4048\n\n2221\n\n1116\n\n4564\n\n5892\n\n828\n\n545\n\n-\n\na = 4\n\n9823\n\n1286\n\nreduction in majority group. In real-world applications, there is also concern about a decrease in majority-group accuracy.\n\nHowever, we argue that reporting the vanilla standard accuracy would be problematic when the dataset is severely class-imbalanced. For example, Tab. D.7 shows the number of samples in each dataset with respect to the class and group labels. For the case of Adult dataset, 75.2% of the samples are labeled as y = 0, hence, a naive predictor that always predict ˆY = 0 will achieve the standard accuracy of 75.2% and DCA= 0, which are clearly overestimating the (accuracy, fairness) performance of the classifier. Therefore, to avoid such issue, we use the group-class balanced accuracy, rather than the standard accuracy.\n\n23\n\n010203040506070Training epochs01q1010203040506070Training epochs01q1Group 0Group 1",
    "reference": "# Summary Of The Paper\n\nThis paper proposes an in-processing based method to design a fair classifier for the metric difference of conditional accuracy (DCA). First, the authors show that empirical DCA can be approximated by the root of empirical group variance. This suggests using the root of empirical group variance as a regularizer in the learning objective. The authors then use a characterization due to [Xie et. al., 2020] to convert such a regularized ERM objective to a group distributional robust optimization.\n\nThe authors provide an iterative method to solve the fair DRO problem where the min player $\\theta$ uses gradient descent and the max player $q$ uses smoothed best response. This algorithm is evaluated on several datasets (tabular, vision, and language) and compared with several reweighing and regularization based methods. The results show that compared to the existing fair classifiers, the proposed method achieves better trade-off between accuracy and DCA.\n\n# Strength And Weaknesses\n\nStrengths:\n- The connection between DCA and empirical variance is interesting and I wonder if one can show similar connections for other fairness measures.\n- The experimental results clearly show that the proposed method pareto dominates several other in-processing based methods.\n\nWeaknesses:\n- The proposed approach seems limited to difference of conditional accuracy (DCA) and it is not clear how this approach can be generalized to other fairness metrics. In fact, the main algorithm is motivated by proposition 1 which shows that empirical DCA can be approximated by the square root of empirical variance.\n- The authors claim that the iterative method for solving fair-DRO is efficient. However, I didn't see any convergence guarantees for the proposed method.\n- The authors also emphasized the use of exact regularization in the learning objective. However, this approach cannot guarantee any desired level of fairness in the training/test set. In particular, if one wants to output a classifier with fairness violation at most $\\alpha$, the current approach seems to require a lot of hyperparameter tuning.\n- There is no result on generalization i.e. how does the fairness guarantee and accuracy generalizes to unseen datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: the paper is well-written, and the connection with prior work is highlighted. \n\nQuality and Novelty: I doubt if the proposed approach is applicable beyond the fairness measure DCA. In terms of novelty, the authors make the connection between DCA and root of empirical variance. However, the solution of the regularized ERM uses the group DRO formulation shown in [Xie et. al. 2020]. This is the only step where distributionally robust optimization comes into the picture, and I wouldn't call such an application of DRO well motivated.\n\n# Summary Of The Review\n\nOverall, the authors present an interesting approach to design a fair classifier and the experimental results demonstrate that the proposed method pareto dominates existing fair classifiers on several datasets. However, I wonder if the proposed approach works beyond the fairness metric DCA. Moreover, the main connection with DRO relies on past literature [Xie et. al. 2020] and the proposed method lacks convergence and generalization guarantees.\n\nUpdate after rebuttal: Many thanks to the authors for the detailed response. It seems to me that the connection between fair classification and DRO is indeed non-trivial, as pointed out by other reviewers. Additionally, the new experimental results look convincing to me. Furthermore, the fairness metric DCA covers the more interesting fairness metrics for binary classification. But I hope the authors add some discussion on how generalizable this approach is to other fairness measures, particularly for multi-class classification. Overall I now feel positive about the paper and will increase my score.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nA VAE FOR TRANSFORMERS WITH NONPARAMETRIC VARIATIONAL INFORMATION BOTTLENECK\n\nJames Henderson Idiap Research Institute, Switzerland james.henderson@idiap.ch\n\nFabio Fehr Idiap Research Institute and EPFL, Switzerland fabio.fehr@idiap.ch\n\nABSTRACT\n\nWe propose a Variational AutoEncoder (VAE) for Transformers by developing a Variational Information Bottleneck (VIB) regulariser for Transformer embeddings. We formalise such attention-based representations as mixture distributions, and use Bayesian nonparametrics to develop a Nonparametric VIB (NVIB) for them. The variable number of mixture components supported by nonparametrics captures the variable number of vectors supported by attention, and exchangeable distributions from nonparametrics capture the permutation invariance of attention. Our Transformer VAE (NVAE) uses NVIB to regularise the information passing from the Transformer encoder to the Transformer decoder. Evaluations of a NVAE, trained on natural language text, demonstrate that NVIB can regularise the number of mixture components in the induced embedding whilst maintaining generation quality and reconstruction capacity.\n\n1\n\nINTRODUCTION\n\nAttention-based deep learning models, such as Transformers (Vaswani et al., 2017; Devlin et al., 2019), have achieved unprecedented empirical success in a wide range of cognitive tasks, in particular in natural language processing. The use of attention allows these models to represent their input with multiple vectors, which is essential for embedding natural language text (Bahdanau et al., 2015). On the other hand, deep variational Bayesian approaches to representation learning, such as variational autoencoders (VAEs) (Kingma & Welling, 2014), have also been shown to have many benefits (Mathieu et al., 2019; Ghosh et al., 2020; Vahdat & Kautz, 2020), especially due to their variational information bottleneck (VIB) (Alemi et al., 2017) for regularising the induced latent representations. However, it has not been clear how to combine these two trends, because the latent space induced by Transformers is a set of vectors whose size grows with the size of the input, whereas standard VIB methods only apply to a vector space of a fixed size (Liu & Liu, 2019; Fang et al., 2021; Park & Lee, 2021). To define a VIB regulariser for a Transformer’s embedding space, we need to allow the size of a latent representation to vary dynamically depending on the complexity of the individual input, and yet regularise the total amount of information conveyed by the whole representation.\n\nIn this paper, we propose such a variational information bottleneck for variable sized latent representations, which we use to regularise the embeddings of a Transformer encoder-decoder, giving us a variational autoencoder for Transformers.1 Like a Transformer encoder’s embedding space, the proposed VAE’s sampled encoder output is (a generalisation of) a set of vectors, and the decoder accesses this embedding with (a generalisation of) cross attention. But unlike Transformers, the proposed VIB layer for this VAE regularises the (effective) number of vectors in the set, as well as the information conveyed by each vector. We show that this regularisation improves generative abilities and compresses latent representations. In addition to the regularisation of over-parameterised language models (Child et al., 2019), previous work shows the efficacy of VAEs for: disentanglement (Higgins et al., 2017), language generation (Liu & Liu, 2019), and explainability (Mercatali & Freitas, 2021). All these topics are important and active areas of research in NLP.\n\nTo define this VIB, we need to model distributions over these variable-sized encoder embeddings, as interpreted by cross attention. Firstly, because the attention function returns an interpolation between the vectors output by the encoder, it generalises across the varying number of vectors, which like the input length is theoretically unbounded. Thus, to define distributions over these unbounded embeddings, we need to use nonparametric methods (Jordan, 2010). Secondly, the attention function is insensitive to the\n\n1The code is available at https://github.com/idiap/nvib and https://github.com/idiap/nvib transformers.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(b) Denoising attention.\n\n(a) The NVAE model and its NVIB layer.\n\n(c) Test-time denoising attention.\n\nFigure 1: (a) Illustration of the NVAE model, with its NVIB layer. (b) Query denoising attention at training time, with the sampled distribution as the query prior, a noisy query observation, and the expected value of the denoised query. (c) Query denoising attention at test time using the mean distribution.\n\norder of the vectors output by the encoder, so it interprets this embedding as a permutation-invariant set of vectors. Thus, the distributions over these permutation-invariant embeddings should be exchangeable (Jordan, 2010). Thirdly, the attention function imposes a normalised weighting over the embedding vectors, via the attention weights. So we should model an embedding as a distribution rather than a set.\n\nA normalised weighting over an unbounded permutation-invariant set of fixed-length vectors matches exactly the properties of a nonparametric space of mixture distributions, which have been extensively studied in Bayesian nonparametrics using exchangeable distributions (Blei & Jordan, 2006; Jordan, 2010). In previous work, Bayesian nonparametrics is typically applied to learning models where the number of parameters grows with the size of the training data (Teh, 2010; Jordan, 2010; Kossen et al., 2021). In contrast, we apply it to inferring latent representations where the number of parameters grows with the size of the input. We believe this is the first work to use nonparametric methods in this way for deep variational Bayesian models.\n\nTo define a precise equivalence between attention-based representations and mixture distributions, we provide an interpretation of attention where the input set of vectors defines a mixture of impulse distributions, which is used as a prior to denoise the observed query vector (depicted in Figure 1b). Generalising sets of vectors to mixture distributions and generalising the attention function to query denoising allows us to propose a general deep variational Bayesian framework for attention-based models using Bayesian nonparametrics. More specifically, we propose to use Dirichlet processes (DPs) as the exchangeable distributions (Aldous, 1985; Jordan, 2010) to specify distributions over mixtures of impulse distributions, including distributions over the effective number of components in the mixture.\n\nWe define a nonparametric VIB (NVIB) layer using a bounded DP prior and posterior to regularise the effective size of variable-sized latent representations. This NVIB layer uses exact inference to infer the posterior from a set of pseudo-observations, and uses proposed efficient approximations to sample from this posterior with a reparameterisation trick and to regularise it with the KL divergence with the prior. Applying this NVIB regulariser to a Transformer autoencoder gives us our proposed nonparametric variational autoencoder (NVAE), depicted in Figure 1a. The noise introduced by sampling from the DP posterior controls the amount of information which flows from the encoder to the decoder, despite the fact that the amount of information required to reconstruct different text inputs varies enormously.\n\nTo evaluate the effectiveness of NVIB, we train a NVAE on natural language text and find that it is able to reconstruct, generate and regularise the effective number of vectors in the latent representation, thereby demonstrating that NVAE is a viable VAE. We also find that the regularised latent space is smooth, using a proposed method for interpolating between DP posteriors to generate interpolations between sentences.\n\nContributions This paper makes the following contributions: (1) We propose a variational Bayesian framework for modelling attention-based representations using mixture distributions, denoising attention (2) We propose a nonparametric variational information and Bayesian nonparametrics (Section 2). bottleneck (NVIB) regulariser for learning attention-based representations (Section 3). (3) We propose a nonparametric variational autoencoder (NVAE), which is a variational Bayesian extension of a Transformer encoder-decoder (Section 4). (4) We show that the NVAE model is a competitive VAE which can reconstruct, generate, regularise its latent space and intuitively interpolate between sentences (Section 5).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nRelated work Related work in stochastic attention assume that the keys, queries, values (Martin et al., 2020) or attention weight vectors of the network are treated as latent random variables (Deng et al., 2018; Bahuleyan et al., 2018; Fan et al., 2020; Cinquin et al., 2022). Nguyen et al. (2022) provides a formulation and interpretation of attention keys as latent mixture distributions, whereas our formulation characterises the whole attention function and is interpreted as Bayesian query denoising. The use of Bayesian nonparametrics to learn a variable sized latent space using a VAE (Nalisnick & Smyth, 2017; Goyal et al., 2017; Echraibi et al., 2020) still assumes a fixed-sized latent representation at test time, unlike our proposal.\n\n2 A NONPARAMETRIC BAYESIAN FRAMEWORK FOR TRANSFORMER EMBEDDINGS\n\nThis section proposes a formalisation of attention-based representations as mixture distributions over a vector space, and proposes nonparametric Bayesian methods for modelling information about these mixture distributions. First we show that standard attention functions can be interpreted as implementing Bayesian query denoising, where the set of vectors being accessed specifies a mixture of impulse distributions (Section 2.1). We adopt mixture distributions over vectors as a generalisation of attention-based representations, and adopt this denoising function as a generalisation of attention. Then we use Bayesian nonparametrics to propose prior (Section 2.2) and posterior distributions (Section 2.3) over these mixture distributions. These priors and posteriors form the basis of our nonparametric variational information bottleneck, proposed in Section 3.\n\n2.1 DENOISING ATTENTION\n\nThe attention function provides access to a set of vectors by mapping a query vector to the resulting attention vector. As the basis of our approach to attention-based representations, we generalise the set of vectors to a probability distribution over vectors, and generalise attention to a function of these probability distributions.\n\nThe attention mechanism we assume is scaled dot product attention, standardly used in many attentionbased models including Transformers. For simplicity, we consider cross attention, where a single query vector is mapped to a single result vector. This attention function projects the input vector u′ ∈R1×p via the weight matrix W Q ∈Rp×d to a query, and projects the set of vectors Z ∈Rn×p via weight matrices W K,W V ∈ Rp×d to keys and values, respectively. It uses the keys’ dimensionality d for scaling. We regroup this scaled dot product attention function into a core dot product attention function Attn(u,Z) in which all operations are done in the space of Z.\n\nAttention(u′,Z ; W Q,W K,W V ) = Attn(u′W Q(W K)T , Z) W V = Attn(u,Z) W V\n\nwhere u = (u′W Q(W K)T ) ∈ R1×p. The function Attn(u,Z) can then be defined in two equivalent ways (as shown in Appendix G): in terms of a sum over the vectors zi in Z, or in terms of an integral over a distribution which is only nonzero at the zi:\n\nAttn(u,Z) = softmax\n\nZ = DAttn(u; FZ)\n\nn (cid:88)\n\nFZ =\n\nDAttn(u; F ) =\n\ni=1 (cid:90)\n\nv\n\n(cid:16) 1√\n\nuZT (cid:17)\n\nd exp( 1 √\ni=1exp( 1\n\nd\n\n2\n\n√\n\n2\n\nd\n\n||zi||2)\n\n(cid:80)n\n\nf(v) g(u; v, (cid:82) vf(v) g(u; v,\n\n√\n\nδzi\n\n||zi||2) √\n\ndI)\n\ndI) dv\n\nv dv\n\n(1)\n\n(2)\n\n(3)\n\n√\n\ndI) is the multivariate Gaussian function with diagonal variance of\n\nwhere δzi is an impulse distribution at zi, f(·) is the probability density function for distribution F , and g(u; v, d. As depicted in Figure 1b, DAttn(u; FZ) can be interpreted as query denoising. The query u is interpreted as an observation of some true vector v which has been corrupted by Gaussian noise, where v was generated from a prior probability distribution FZ specified by Z. The result of Attn(u,Z) is the expected value of this true vector v after seeing the noisy observation u, which can be interpreted as a form of denoising.\n\n√\n\nThis denoising attention function DAttn(u; F ) is actually a generalisation of attention over a set of vectors, in that it is defined for any probability distribution F over a vector space. In the special case where F =(cid:80) iπiδzi is a finite mixture of impulse distributions, it is the same as Attn(u,Z) but with a bias term ||zi||2. In the rest of this paper, we will use DAttn(u; F ) as our definition of log(πi) substituted for attention, which allows us to treat the latent space of a Transformer encoder-decoder as mixture distributions.\n\n1 √\n\nd\n\n2\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n2.2 A PRIOR OVER MIXTURE DISTRIBUTIONS\n\nGiven that our attention-based latent representations are formalised as mixture distributions F , a Bayesian approach requires a prior over these distributions. Attention-based models place no finite bound on the possible number of vectors in their set of vectors Z, and thus there is no finite bound on the number of parameters needed to specify the equivalent mixture distribution F . Nonetheless, we can still specify probability distributions over this infinite space of possible distributions F , using methods from Bayesian nonparametrics. These nonparametric Bayesian methods, with exchangeable distributions, are specifically designed for modelling probability distributions over unboundedly large mixture distributions.\n\nWe base our distributions over mixture distributions on Dirichlet processes DP(G0,α0). Dirichlet processes (DPs) are a generalisation of Dirichlet distributions to an infinite support, such as the points in a vector space. A Dirichlet distribution Dir(α) is a distribution over probability mass functions π of discrete categories i, 1≤i≤κ. One useful definition of Dirichlet processes views a DP F ∼DP(G0,α0), where G0 is the base distribution over vectors and α0 ∈R is the concentration parameter, as the limit of a sequence of finite Dirichlet distributions (see Teh (2010)), given in equation 4. Note that the Dirichlet distributions in equation 4 are symmetric, in that all the κ categories i have the same αi = α0 κ parameter values. However, these categories end up with very different weights πi, due to the most probable categories getting a large proportion of the probability mass and the tail of categories getting an exponentially decreasing amount of probability mass. In the infinite limit, this tail is infinitely long with infinitesimal probabilities. The number of categories which get nontrivial probabilities is determined by α0, and becomes independent of κ as κ gets large.\n\nAs shown in this definition, each sample F from a DP is an infinite mixture of impulse distributions δzi, parameterised by an infinite sequence of weight-vector pairs πi,zi. This contrasts with the finite Z in attention-based representations. Having an infinite F would also cause problems in our variational Bayesian model, because VIB uses a bound on the log-likelihood (see Section 3.1), which Kingma & Welling (2014) showed has an error of DKL(q(F (cid:12) (cid:12)x)) (the looseness of the bound). This would be infinite unless both the true posterior p(F (cid:12) (cid:12)x) generate a finite F , so we need a prior which generates finite F .\n\n(cid:12)x) and its approximation q(F (cid:12)\n\n(cid:12)x) ∥ p(F (cid:12)\n\nF =\n\n∞ (cid:88)\n\nπiδzi\n\ni=1\n\nπ ∼ lim κ→∞\n\nα0 κ\nzi ∼ G0 for i=1,...,∞\n\nα0 κ\n\nDir(\n\n, κ...,\n\n(4)\n\n)\n\nF =\n\nκ0(cid:88)\n\nπiδzi\n\ni=1\n\nπ ∼ Dir(\n\nα0 κ0\n\n,κ0...,\n\nα0 κ0\n\n)\n\nzi ∼ G0 for i=1,...,κ0\n\n(5)\n\nThe Unbounded Dirichlet Process Prior We do not want a prior which places an apriori bound on the size of F , so we assume it is finite but unbounded, and propose a prior which is an unbounded sequence of finite approximations to a DP. We define a bounded DP F ∼BDP(G0,α0,κ0) as in equation 5. Our approach to the prior is to use an unbounded but finite κ0, so we define a distribution over approximations as κ0 increases towards infinity. Hence, every distribution is over a finite number of vectors, but there is no finite bound on the number of vectors in all distributions. Given φ is some distribution over positive integers κ∈Z+, we define this unbounded DP as UDP(G0,α0,φ)=BDP(G0,α0,κ) where κ∼φ.\n\nWe use these definitions both to define a general prior over probability distributions, and to define a conditional prior for each input length. In both cases, the base distribution Gp 0 is assumed to be a unit Gaussian (inspired by Kingma & Welling (2014)) and the concentration parameter αp 0 is assumed to be one.2 0 = 1; μp = 0; σp = 1 0,αp The general prior is UDP(Gp 0,φp), where the size distribution φp is determined empirically. The 0,αp conditional prior BDP(Gp 0,κ0) sets the level of approximation κ0 as a fixed function of the input length n, in particular κ0=(n+1)κ∆, where κ∆ ∈Z+ is a hyperparameter that controls the approximation.\n\n0 = N (μp,I(σp)2); αp\n\nGp\n\nA Conditional Bounded DP Prior It will be useful to generalise this conditioning for the level of approximation to any conditional prior which is a fixed function of only the input length. If we know the input length n, but know nothing about the content of the text, then the distribution of vectors should stay the same as the general prior, Gp′ 0. However, the count of observations we expect to have after an input\n\n0 =Gp\n\n2We will use “p” and “q” superscripts to designate variables for the prior and posterior, respectively. Similarly,\n\na zero subscript is part of the name of the variable, in contrast to positive integer subscripts which are indices.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nof that length would not be αp token, and thus αp′\n\n0 =αp\n\n0, but should include a pseudo-count α∆ ∈ R≥0 hyperparameter for every\n\n0+nα∆. This then gives us the conditional prior given n of BDP(Gp\n\n0,αp′\n\n0 ,κ0).\n\n2.3 A POSTERIOR OVER MIXTURE DISTRIBUTIONS\n\ni = N (μq i ∈ R1×d and a standard deviation σq\n\nSince a DP is a conjugate prior, we can use exact inference to compute the posterior DP from the prior DP plus a set of pseudo-observations output by the encoder. Each pseudo-observation is a real-valued pseudo-count αq i ∈ R≥0 and a parametric distribution which represents uncertainty in the observation. We use an isotropic Gaussian, Gq i )2), as the parametric distribution, specified by a mean μq >0 . Here we assume that the number of candidate pseudo-observations is the same as the length n of the input, but some of these pseudo-observations may have zero pseudo-counts and thus be effectively removed from the set. The formula for exact inference of the posterior DP is given in equation 6, where there is an n+1th component of the base distribution Gq\n\n0 which comes from the prior, namely αq\n\ni , I(σq i ∈ R1×d\n\nn+1 =Gp 0.\n\nn+1 =αp\n\n0 and Gq\n\nn+1 (cid:88)\n\ni=1\n\nρiFi\n\n(7)\n\nF ∼ DP(Gq\n\n0, αq 0)\n\n(6)\n\nF =\n\nαq\n\n0 =\n\nn+1 (cid:88)\n\nαq\n\ni ; Gq\n\n0 =\n\ni=1\n\nn+1 (cid:88)\n\ni=1\n\ni\n\nαq αq\n\n0\n\nGq\n\ni\n\nρ ∼ Dir(αq Fi ∼ BDP(Gq\n\n1,...,αq i ,αq\n\nn+1) i ,κ∆) for i=1,...,n+1\n\nWe derive an alternative factorisation of the posterior DP (Appendix H) which helps with the sampling method in Section 3.2. We then bound this factorised DP so that it generates the same number κ0=(n+1)κ∆ of weighted vectors as the prior BDP(Gp 0, κ0). The resulting bounded posterior F ∼ BFDP(Gq, αq, κ∆) is given in equation 7, which defines our posterior distribution q(F (cid:12) (cid:12) x). This posterior simplifies to a mixture F = (cid:80)n+1 ijδzij of impulse distributions, with ρ∼Dir(αq\n\nj=1 ρiπ′ κ∆ ), and zij ∼Gq i .\n\nκ∆ , κ∆... , αq\n\ni ∼Dir( αq\n\nn+1), π′\n\n1,...,αq\n\n0, αp\n\n(cid:80)κ∆\n\ni=1\n\ni\n\ni\n\nThe Mean Posterior Mixture Distribution A VAE is trained on samples from the posterior, but at test time VAEs typically use the mean of this distribution. Generalising the latent space to mixture distributions makes this straightforward, since the mean of our BFDP posterior is its base distribution Gq 0. This base distribution is a continuous distribution, whereas at training time all samples are discrete distributions. Nonetheless, when accessed via denoising attention, the base distribution looks like a typical sample from the posterior. This is visualised in Figure 1 by comparing the vector returned by denoising attention (in green) given the continuous mean distribution (Figure 1c) and a typical sample from this distribution (Figure 1b). Thus, the function defined by applying denoising attention to a sampled distribution can be seen as a noisy version of the function defined by applying denoising attention to the mean distribution. For our Gaussian mixture base distribution Gq 0, there is a closed-form solution to computing denoising attention, given in Appendix F.\n\n3 THE NONPARAMETRIC VARIATIONAL INFORMATION BOTTLENECK\n\nBy generalising attention-based representations to mixture distributions and generalising the attention function to denoising attention, we can define a VIB regulariser for attention-based interfaces. Such encoder-decoder interfaces take a set-of-vectors representation and return a function from query to result vectors. We map the input set-of-vectors to a set of pseudo-observations, and define the returned function with denoising attention. Then, given the nonparametric prior and posterior from Section 2, we can define our nonparametric VIB regulariser by specifying how to compute the KL divergence between the prior and posterior, and how to effectively sample from the posterior for training. As far as we are aware, this proposal is the first VIB model for attention-based representations like Transformer embeddings.\n\nThe VIB layer in a VAE controls the amount of information passing through it by introducing noise according to a posterior output by the encoder, and regularises this information by minimising the KL divergence between this posterior and an uninformative prior. One of the known difficulties with VAEs is that they can be difficult to train due to the posterior collapsing to the prior (Bowman et al., 2016). Similarly to the freebits objective proposed to address this problem for vector-space VAEs (Kingma et al., 2016), instead of regularising towards the prior, which gets no pseudo-counts from the input, we regularise towards the conditional\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nprior BDP(Gp 0 ,κ0), which gets nα∆ pseudo-counts from the input but knows nothing about the information they convey. We found that this helps with the stability of training and avoiding posterior collapse.\n\n0,αp′\n\n3.1 THE VARIATIONAL INFORMATION BOTTLENECK LOSS\n\nThe evidence lower bound (ELBO) is commonly used in variational Bayesian methods as an objective which approximately maximises the log-likelihood of the observation x, where x is the input text.\n\nlog(p(x)) ≥ E\n\nq(F\n\n(cid:12) (cid:12)x)\n\nlog(p(x(cid:12)\n\n(cid:12)F )) −DKL(q(F (cid:12)\n\n(cid:12)x) ∥ p(F ))\n\n(8)\n\nLR = −E\n\nq(F\n\n(cid:12) (cid:12)x)\n\nlog(p(x(cid:12)\n\n(cid:12)F ))\n\nThe first term of the bound is the reconstruction loss LR, computed using samples F from the approximate posterior q(F (cid:12)\n\n(cid:12)x), and the second term is the KL divergence between this posterior and the prior p(F ).\n\n0,αp′\n\nFor the KL divergence term, since both the prior and the posterior are conditioned on the same bound κ0 on the number of vectors they generate, we can compute a meaningful finite KL divergence between the 0 ,κ0) and the posterior q(F (cid:12) conditional prior p(F )=BDP(Gp (cid:12)x)=BFDP(Gq,αq,κ∆), given κ∆ and κ0=(n+1)κ∆. The derivation of the KL divergence is given in Appendix I. It has two terms, one LD for the distribution of weights π generated by the Dirichlet distributions, and one LG for the distribution of vectors Z generated by the component Gaussians. Using the exact KL divergence for our bounded DPs would regularise each component equally, even for components which have zero αq i and thus have no impact on the posterior. We instead approximate a KL divergence where only samples with a nontrivial weight are regularised. Marginalising over the number of nontrivial weights for each component does not appear to be tractable for LD, but since the relationship is approximately linear (see Appendix I), we simply substitute the expected number αq κ0 for the actual number in the equation for KL divergence. This gives us the following loss terms for the KL divergence, where Γ is the gamma function and ψ is the digamma function. LD +LG ≈ DKL(q(F (cid:12) (9) (cid:33) )\n\nLD = logΓ(αq\n\n(cid:12)x) ∥ p(F ))\n\n)−logΓ(\n\n−ψ(αq\n\n0)−logΓ(αp′\n\n0−αp′ 0 )\n\n0 )+(αq\n\n0)+ψ(\n\nlogΓ(\n\n(cid:19) )\n\n+κ0\n\n(cid:32)\n\n(cid:18)\n\nαq\n\n0\n\ni\n\nαp′ 0\nκ0\n\nαq 0\nκ0\n\nn+1 (cid:88)\n\nLG = 1\n\nαq αq 0\nBoth LD and LG scale approximately linearly with κ0.\n\nh)2 h)2 +\n\nih−μp (σp\n\n(σq (σp\n\n2κ0\n\n(cid:18)(μq\n\nd (cid:88)\n\nh=1\n\ni=1\n\ni\n\nih)2 h)2 −1−log\n\nαq 0\nκ0 (σq (σp\n\nih)2 h)2\n\n(cid:19)\n\nTo generalise the ELBO beyond autoencoders, it can be viewed as a way to regularise the amount of information which passes through the latent representation (Alemi et al., 2017). This VIB interpretation allows the different parts of the objective to have different weights. We introduce two hyperparameters to control the relative weight of the above three parts of the ELBO, which defines our VIB loss L.\n\nL = LR + λDLD + λGLG\n\n(10)\n\n3.2 SAMPLING A MIXTURE DISTRIBUTION FROM THE POSTERIOR\n\nTo control the amount of information which passes from the encoder to the decoder, at training time a VAE (Kingma & Welling, 2014) samples from the encoder’s posterior distribution and uses this sample to reconstruct the input. The “reparameterisation trick” is used to ensure that backpropagation of the reconstruction error through this sampling step can be done effectively. We propose a novel reparameterisation trick for bounded Dirichlet processes which allows sampling without any categorical choices, and propose specific sampling methods which result in effective backpropagation through the sampling step.\n\nFor our NVIB model, we sample the parameters ⟨π,Z⟩ of a mixture distribution F generated by our bounded Dirichlet process posterior BFDP(Gq,αq,κ∆), where F consists of a set of impulse distributions δzi each with a weight πi. A straightforward approach to sampling from a Dirichlet process would independently sample weights π from a (theoretically infinite) Dirichlet distribution and sample vectors Z from the base distribution of the DP, where sampling from the base distribution involves first sampling a component of the base distribution and then sampling a vector from that component’s Gaussian.\n\nA Factorised Sampling Method The problem is that sampling a component is a discrete choice, for which there is no exact reparameterisation trick. Instead, we note that the components do not differ in the\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nnumber of vectors sampled from each one (always theoretically infinite for a DP), but only differ in the distribution of weights for those vectors. As specified in the factorised DP in Section 2.3, we characterise this distribution over weights by factorising it into two steps: first choosing how the total weight is distributed across components (ρ), and then for each component choosing how its weight is distributed across its vectors (π′ i). These are both continuous choices. The vectors can then be sampled independently from each component.\n\nReparameterisation tricks Each individual component specifies a Gaussian distribution over vectors, so we can use the same reparameterisation trick as Kingma & Welling (2014) for sampling vectors Zi from an individual component i.\n\nThe factorised and bounded nature of our posterior BFDP(Gq,αq,κ∆) means that the total weights ρ and the individual weights π′ i are all sampled from Dirichlet distributions. A Dirichlet distribution over a set of category weights can be sampled by sampling from a Gamma distribution for each category and then normalising. There is no closed-form, explicit reparameterisation trick for the exact Gamma distribution, but there are for approximations. Knowles (2015) proposes two such approximations which we combine, one which is more accurate for small α and one which is more accurate for larger α. We leave the investigation of implicit reparameterisation gradients (Figurnov et al., 2018), an alternative approach to explicit reparameterisation, to future work. More specifics about the sampling methods and their reparameterisation tricks is given in Appendix J. We provide an evaluation (Appendix B.3) showing that κ∆=1 is an efficient and effective sampling method. In this case, there is no need to sample from the Dirichlet for each individual component, but we still sample the weights ρ across components.\n\n4 THE NONPARAMETRIC VARIATIONAL AUTOENCODER\n\nWe define a VAE for Transformers by using the nonparametric VIB defined in Section 3 to regularise the attention-based representation between the encoder and decoder of a Transformer autoencoder, as depicted in Figure 1a. In this NVAE model, the Transformer encoder is used to estimate the parameters ⟨αq,μq,σq⟩ of the posterior given the input text x. The Transformer decoder is used to reconstruct the input text x using denoising attention over a sample F from this posterior.\n\nThe NVIB Regulariser Our NVIB layer regularises the amount of information which passes from the encoder to the decoder through this posterior. As with VIB for vector spaces, the KL divergence encourages the encoder to output component Gaussians with smaller μq i . With NVIB, the KL divergence also encourages the encoder to output smaller and sparser αq i , which regularises the effective number of components in the posterior as well as the noisiness of their weights.\n\ni and larger σq\n\nThe Transformer Encoder The Transformer encoder q(F |x) with a text x of n number of tokens is used to compute a vector for each token i of the input. From each of these n individual token embeddings, the encoder then linearly projects to three parameters, αq i ) ∈ R1×p. The variance parameters are exponentiated to be strictly positive, whereas the pseudo-count parameters αq i are estimated using a Rectified Linear Unit (ReLU) activation (Nair & Hinton, 2010), which results in masking the vector during cross-attention when it is exactly zero. Thus, the DP posterior has one component ⟨αq\n\ni ⟩ of its base distribution for each token of the input (plus one for the prior).\n\ni ∈ R1×p and log(σq\n\ni ∈ R, μq\n\ni ,σq\n\ni ,μq\n\nThe Transformer Decoder The Transformer decoder q(x|F ) receives a distribution F over vectors and reconstructs the input text x. During training, F is specified by the sampled vectors Z∈Rκ0×p and the sampled weights π ∈Rκ0×1, and at test time F is specified by the output of the encoder αq ∈Rκ0×1, μq ∈Rκ0×p and log(σ)q ∈Rκ0×p. In both cases, the decoder accesses F using denoising attention in the same way that standard Transformer decoders use cross attention. We include the exact equations used for a deep learning implementation of denoising attention in Appendix K. During training, the text is predicted using teacher forcing, and during test time the text is predicted autoregressively using greedy decoding until the end-of-sequence token is generated or the sentence generated is 50 tokens larger than the target length.\n\nThe Generative Model To use our NVAE model as a generative model, we sample from the prior and use the trained Transformer decoder to generate a sentence. As discussed in Section 2.2, to sample from the same prior as used for training, we need to first sample a sentence length, and then sample from the conditional prior given that (approximate) sentence length. For simplicity, we sample the sentence length from the empirical distribution of sentence lengths in the training data.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n5\n\nINTRINSIC EVALUATIONS OF NVIB IN NVAE\n\nTo support our theoretical contributions, we provide proof-of-concept experiments which demonstrate that our proposed NVIB regulariser performs as claimed. We evaluate it in our proposed NVAE model by training NVAEs on natural language text and evaluating the resulting models. We show that the NVAE is a viable VAE model as it exhibits a competitive reconstruction versus generation trade-off (Section 5.1). We show that the NVIB layer is able to dynamically choose the number of components it needs in its embeddings (Section 5.2). Additionally, NVIB provides an intuitive way to interpolate between sentence embeddings, which provides an evaluation of the smoothness of the latent space (Section 5.3).\n\nData The Wikitext-2 and Wikitext-103 (Merity et al., 2017) encyclopedia datasets were selected as they are general English language corpora of a small and large scale containing high quality Wikipedia articles.\n\nBaselines We compare to various alternative ways to define a VAE from a Transformer autoencoder. As representative of a standard fixed-length-vector VAE, the Variational Transformer Pooled (VTP) baseline pools its vectors across the sequence length dimension, and then applies a Gaussian VIB layer (Kingma & Welling, 2014). At the other extreme, the Variational Transformer (VT) baseline keeps all its vectors and applies a Gaussian VIB layer to each one. In between these baselines, as a hand-coded solution to constraining the quantity of latent vectors, Variational Transformer Stride (VTS) baselines, with parameter S, masks 1−S proportion of the embedding vectors based on their position. For comparability, all our baselines only differ from the NVAE model in the latent representation between the encoder and decoder, with the same Transformer encoder and Transformer decoder architectures.\n\n5.1 RECONSTRUCTION VERSUS GENERATION\n\nThis section shows that the NVAE Transformer model is a competitive VAE in both reconstruction of input sentences and generation by sampling from the prior. All models undergo hyperparameter tuning on the validation set (Appendix B) across 5 seeds, to select the best models and then report results on the Wikitext-2 test set. For reconstruction, we report the SacreBLEU metric (Papineni et al., 2002; Post, 2018). For generation, we report forward perplexity (F-PPL) and reverse perplexity (R-PPL) (Zhao et al., 2018; C ́ıfka et al., 2018), which trains an external language model on the gold training text and evaluates it on the generated text (F-PPL) or vice versa (R-PPL). Training details are provided in Appendix A.\n\nFigures 2a and 2b show the reconstruction versus generation trade-off on the Wikitext-2 test set, where lower right is better. The single vector baseline VTP is unable to reconstruct well (low BLEU) or generate diverse sentences (high R-PPL). Even with larger capacity and more training data, the model performs poorly in generation (Appendix B.4). The full vector baseline VT is unable to consistently generate fluent sentences (high F-PPL), whereas the position-based dropout baseline VTS shows that regularisation of the space is beneficial for both reconstruction and generation quality.\n\nThe best NVAE models are competitive with the best VTS baselines in being able to both reconstruct and generate, even slightly better at generation with high reconstruction accuracy. In particular, reverse perplexity shows that NVAEs are able to generate a diverse collection of sentences from the prior, and forward perplexity shows that these are fluent sentences (samples in Appendix C). We believe this advantage is the result of learning what vectors to keep instead of a hand-coded position-based dropout.\n\n(a) F-PPL vs test BLEU.\n\n(b) R-PPL vs test BLEU.\n\n(c) Test BLEU vs proportion of retained vectors ν.\n\n(d) Latent quantity of vectors vs input tokens.\n\nFigure 2: Reconstruction versus generation trade-off (a), (b) and regularisation analysis (c), (d).\n\n8\n\n6080100BLEU ()1.01.11.21.31.4F-PPL ()NVAEVTSVTPVT6080100BLEU ()1.001.051.101.151.20R-PPL ()NVAEVTSVTPVT0.00.51.05060708090100BLEU ()NVAEVTSVTPVT2040Input Tokens051015202530Number of VectorsNVAEVTSVTPVTPublished as a conference paper at ICLR 2023\n\n5.2 REGULARISATION\n\nThis section shows that the NVIB layer is able to regularise the number of vectors in the latent representation of a NVAE. Without regularisation the NVIB layer becomes a standard transformer without any noise and retaining all latent vectors (ablation in Appendix B.1). The proportion of vectors retained is controllable by the conditional prior hyperparameter α∆. Figure 2c shows that there exist α∆ where the NVAE is able to remove a significant proportion (1−ν) of vectors whilst maintaining a high reconstruction performance. Figure 2d plots the number of latent vectors retained during evaluation against the number of input tokens. The NVAE models are able to learn to dynamically regularise the number of vectors based on the information within the text, without hand-coding a function of length as in VTS. The different NVAE lines show the effect of α∆ on vector retention, with larger values allowing more vectors to be used (also see the evaluation in Appendix B.2).\n\n5.3\n\nINTERPOLATION\n\nThe NVIB framework provides an intuitive interpolation between latent sets of vectors that overcomes the challenges with the baselines of latent vector alignment (analysis in Appendix D) and variable set sizes. We simply interpolate the probability assigned to each point in vector space. Given two latent mixture distributions F1 and F2, we decode from the combined mixture distribution (τF1+(1−τ)F2), for varying interpolation rates 0≤τ ≤1. For the baselines we use τZ1+(1−τ)Z2 such that the interpolation is over the content of the latent vectors Zi. We align latent sets of vectors by input position and pad smaller sets with zero vectors, which is the mean of the Gaussian prior. We evaluate the interpolation with selected larger scale models (see Appendix B.4) and use the Wikitext-103 validation set for S1 and its reverse order for S2. The results in Table 1 and Figure 3 show that the NVIB regulariser in NVAE provides smoother interpolations, and improvements in fluency of interpolations over the baselines.3 These findings are qualitatively confirmed through different examples (Appendix E).\n\nModel VT VTP VTS NVAE α∆ =0.4\n\nP =max S =0.8\n\n0.25 0.04 0.57 0.04 0.88\n\nτ 0.5 1.00 0.99 1.00 0.89\n\n0.75 F-PPL (↓) 1+1.6e−3 0.04 1+3.3e−4 0.57 1+2.8e−3 0.04 1+1.3e−7 0.88\n\nTable 1: The proportion of interpolations different from S1 and S2 by varying the interpolation rate τ. Fluency metric F-PPL of interpolations when τ =0.5.\n\nFigure 3: BLEU with S1 versus S2 for varying interpolations τ.\n\n6 CONCLUSIONS\n\nIn this work, we generalise the latent representations of attention-based models to mixture distributions over a vector space, and propose a nonparametric variational information bottleneck (NVIB) to regularise these latent representations. Using this NVIB model, we propose a nonparametric variational autoencoder (NVAE), which uses a Transformer encoder to embed text in a nonparametric space of distributions over mixture distributions, and uses a Transformer decoder to generate text given a sampled mixture distribution. This nonparametric Bayesian formalisation of attention-based representations captures two key properties of the attention function, namely its invariance to permutations of its input vectors, and that this input can vary widely in size. Our NVIB model adds the ability to regularise attention-based representations so that the size of the representation is appropriate for the complexity of the input being encoded. This is a crucial ability for encoding text, where the size of a text can vary enormously. Empirical evaluations indicate that this model: is a competitive VAE in that it is able to reconstruct input sentences and generate a good distribution over sentences from the prior; regularises the size of the induced latent representations as desired; and is able to intuitively interpolate smoothly between latent mixture distributions.\n\nFuture work will evaluate the effectiveness of NVIB at a larger scale, when applied to multi-head attention and to self-attention layers, and when its pretrained representations are used in downstream tasks.\n\n3F-PPL is calculated across interpolations using a Transformer language model trained on Wikitext-103 at the same scale as the larger models. We remove any collapses to exactly S1 or S2 as this will bias the F-PPL metric favourably.\n\n9\n\n0255075100S1 BLEU ()020406080100S2 BLEU ()NVAEVTSVTPVTPublished as a conference paper at ICLR 2023\n\nAUTHOR CONTRIBUTIONS\n\nJames Henderson is responsible for the high-level vision and the theoretical derivations and proofs related to Bayesian nonparametrics. Fabio Fehr is responsible for the sampling method, evaluations, implementations and running of experiments.\n\nACKNOWLEDGEMENTS\n\nWe would like to thank Florian Mai, Andrei Catalin Coman, Melika Behjati, Andreas Marfurt, and other members of the Idiap NLU group for helpful comments and suggestions. This work was funded in part by the Swiss National Science Foundation under the NCCR grant Evolving Language, Swiss National Science Foundation Agreement #51NF40 180888.\n\nREPRODUCIBILITY STATEMENT\n\nWe faithfully describe the details of method in the text and provide detailed derivations for theoretical grounding (Sections F, G, H, I). We mention all relevant hardware, hyperparameters and datasets to reproduce our experiments (Section A). We provide variance estimations across seeds and ablations to justify design choices (Section B). Finally, we provide exact equations for denoising attention used to make implementation easier (Section K). The open source code for this research has been released at https://github.com/idiap/nvib for the NVIB layer in PyTorch and https://github.com/idiap/nvib transformers for the experiments.\n\nREFERENCES\n\nDavid J. Aldous. Exchangeability and related topics. In P. L. Hennequin (ed.), ́Ecole d’ ́Et ́e de Probabilit ́es de Saint-Flour XIII — 1983, pp. 1–198, Berlin, Heidelberg, 1985. Springer Berlin Heidelberg. ISBN 978-3-540-39316-0.\n\nAlexander A. Alemi,\n\nIan Fischer, Joshua V. Dillon, and Kevin Murphy.\n\nDeep variational In 5th International Conference on Learning Representations, information bottleneck. ICLR 2017, Conference Track Proceedings, Toulon, France, 2017. OpenReview.net. URL https://openreview.net/forum?id=HyxQzBceg.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, Conference Track Proceedings, San Diego, CA, USA, May 7-9 2015. URL http://arxiv.org/abs/1409.0473.\n\nHareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. Variational attention for sequence-to-sequence models. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 1672–1682, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https://aclanthology.org/C18-1142.\n\nSteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text\n\nwith the natural language toolkit. O’Reilly Media, Inc., 2009.\n\nChristopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, UK,\n\n1995.\n\nDavid M. Blei and Michael\n\nVariational Bayesian Analysis, 1(1):121 – 143, 2006.\n\ntures. https://doi.org/10.1214/06-BA104.\n\nI. Jordan.\n\ninference for Dirichlet process mixURL\n\ndoi: 10.1214/06-BA104.\n\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, pp. 10–21. Association for Computational Linguistics (ACL), 2016.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. ArXiv: Machine Learning, abs/1904.10509, 2019. doi: 10.48550/ARXIV.1904.10509. URL https://arxiv.org/abs/1904.10509.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nOndrej C ́ıfka, Aliaksei Severyn, Enrique Alfonseca, and Katja Filippova. Eval all, trust a few, do wrong to none: Comparing sentence generation models. CoRR, abs/1804.07972, 2018. URL http://arxiv.org/abs/1804.07972.\n\nTristan Cinquin, Alexander Immer, Max Horn, and Vincent Fortuin. Pathologies in priors and inference for bayesian transformers. In Fourth Symposium on Advances in Approximate Bayesian Inference, 2022. URL https://openreview.net/forum?id=T-3hWOC99vE.\n\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent alignment and In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, variational attention. and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ b691334ccf10d4ab144d672f7783c8a3-Paper.pdf.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep In Proceedings of the 2019 Conference bidirectional transformers for language understanding. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, doi: 10.18653/v1/N19-1423. URL June 2019. Association for Computational Linguistics. https://aclanthology.org/N19-1423.\n\nAmine Echraibi, Joachim Flocon-Cholet, St ́ephane Gosselin, and Sandrine Vaton. On the variational posterior of dirichlet process deep latent gaussian mixture models. ArXiv: Computer Science, abs/2006.08993, 2020. doi: 10.48550/ARXIV.2006.08993. URL https://arxiv.org/abs/2006.08993.\n\nXinjie Fan, Shujian Zhang, Bo Chen, and Mingyuan Zhou.\n\nBayesian attention modules. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances Information Processing Systems, volume 33, pp. 16362–16376. Curran Assoin Neural URL https://proceedings.neurips.cc/paper/2020/file/ ciates, bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf.\n\nInc., 2020.\n\nLe Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, and Changyou Chen. Transformer-based conditional variational autoencoder for controllable story generation. CoRR, abs/2101.00828, 2021. URL https://arxiv.org/abs/2101.00828.\n\nMikhail Figurnov, Shakir Mohamed, and Andriy Mnih.\n\nImplicit reparameterization gradients. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran AssoURL https://proceedings.neurips.cc/paper/2018/file/ ciates, 92c8c96e4c37100777c7190b76d28233-Paper.pdf.\n\nInc., 2018.\n\nPartha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black,\n\nand Bernhard In 8th International Conferto deterministic autoencoders. Scholkopf. ence on Learning Representations, ICLR 2020, Conference Track Proceedings, 2020. URL https://openreview.net/forum?id=S1g7tpEYDS.\n\nFrom variational\n\nPrasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, Eric P. Xing, and Carnegie Mel2017 lon. Nonparametric variational auto-encoders for hierarchical representation learning. IEEE International Conference on Computer Vision (ICCV), pp. 5104–5112, 2017. URL https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_ Nonparametric_Variational_Auto-Encoders_ICCV_2017_paper.pdf.\n\nIrina Higgins, Lo ̈ıc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a In 5th International Conference on Learning Representations, constrained variational framework. ICLR 2017, Conference Track Proceedings, Toulon, France, April 24-26 2017. OpenReview.net. URL https://openreview.net/forum?id=Sy2fzU9gl.\n\nM.I. Jordan. Bayesian nonparametric learning: Expressive priors for intelligent systems. In R. Dechter, H. Geffner, and J. Halpern (eds.), Heuristics, Probability and Causality: A Tribute to Judea Pearl, chapter 10. College Publications, 2010.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Conference Track Proceedings, Banff, AB, Canada, 2014. URL http://arxiv.org/abs/1312.6114.\n\nDiederik P. Kingma, Tim Salimans, Rafal J ́ozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. In Daniel D. Lee, Masashi Improving variational autoencoders with inverse autoregressive flow. Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, pp. 4736–4744, Barcelona, Spain, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Abstract.html.\n\nDavid A. Knowles. Stochastic gradient variational bayes for gamma approximating distributions. arXiv:\n\nMachine Learning, 2015. URL https://arxiv.org/abs/1509.01631.\n\nJannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 28742–28756. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ f1507aba9fc82ffa7cc7373c58f8a613-Paper.pdf.\n\nDanyang Liu and Gongshen Liu. A transformer-based variational autoencoder for sentence generation. In 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1–7. IEEE, 2019. doi: 10.1109/IJCNN.2019.8852155. URL https://arxiv.org/abs/2003.12738.\n\nAlice Martin, Charles Ollion, Florian Strub, Sylvain Le Corff, and Olivier Pietquin. The monte carlo transformer: a stochastic self-attention model for sequence prediction. ArXiv: Mathematics, Computer Science, abs/2007.08620, 2020. URL https://arxiv.org/abs/2007.08620.\n\nEmile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement in In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings variational autoencoders. of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4402–4412, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL https://arxiv.org/abs/1812.02833.\n\nGiangiacomo Mercatali and Andr ́e Freitas. Disentangling generative factors in natural language In Findings of the Association for Computational Linwith discrete variational autoencoders. guistics: EMNLP 2021, pp. 3547–3556, Punta Cana, Dominican Republic, November 2021. doi: 10.18653/v1/2021.findings-emnlp.301. URL Association for Computational Linguistics. https://aclanthology.org/2021.findings-emnlp.301.\n\nStephen Merity, Caiming Xiong,\n\nJames Bradbury, and Richard Socher. In 5th International Conference on Learning Representations,\n\nmixture models. 2017, Conference Track Proceedings, Toulon, France, 2017. OpenReview.net. https://openreview.net/forum?id=Byj72udxe.\n\nPointer sentinel ICLR URL\n\nVinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes F ̈urnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807–814, Haifa, Israel, 21–24 June 2010. Omnipress. URL https://icml.cc/Conferences/2010/papers/432.pdf.\n\nEric T. Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. In 5th International Conference on Learning Representations, ICLR 2017, Conference Track Proceedings, Toulon, France, April 24-26 2017. OpenReview.net. URL https://openreview.net/forum?id=S1jmAotxg.\n\nTam Minh Nguyen, Tan Minh Nguyen, Dung D. D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard Baraniuk, Nhat Ho, and Stanley Osher. Improving transformers with probabilistic attention keys. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\n\n12\n\nPublished as a conference paper at ICLR 2023\n\n(eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 16595–16621. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/nguyen22c.html.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n\nSeongmin Park and Jihwa Lee. Finetuning pretrained transformers into variational autoencoders. In Proceedings of the Second Workshop on Insights from Negative Results in NLP, pp. 29–35, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.insights-1.5. URL https://aclanthology.org/2021.insights-1.5.\n\nMatt Post. A call for clarity in reporting BLEU scores.\n\nIn Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186–191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\n\nYee Whye Teh. Dirichlet processes. In Encyclopedia of Machine Learning, volume 1063, pp. 280–287.\n\nSpringer, 2010.\n\nArash Vahdat and Jan Kautz.\n\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in NeuInformation Processing Systems, volume 33, pp. 19667–19679. Curran Associates, ral URL https://proceedings.neurips.cc/paper/2020/file/ Inc., e3b21256183cf7c2c7a66be163579d37-Paper.pdf.\n\nNvae: A deep hierarchical variational autoencoder.\n\n2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\nJunbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush, and Yann LeCun. Adversarially regularized autoencoders. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5902–5911. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/zhao18b.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA EXPERIMENTAL SETUP\n\nTraining details All models are trained, without pretraining, using the same encoder and decoder configuration for comparability. We use a two layer Transformer encoder and decoder with a single attention-head. The size for the word embedding vectors and model projections are 256, feed forward dimensions 1024, which leads to models of approximately 19 million trainable parameters. The BERT base-uncased tokeniser is used for tokenisation with a vocabulary of approximately 30K. During training we use: a constant learning rate of 1e−4, Adam optimiser (Kingma & Ba, 2015), a batch size of 256, gradient norm clipping 0.1 and trained for 50 epochs (≈15K steps). The number of epochs were selected considering model convergence and minimising computation time. As a form of regularisation we use a dropout rate of 0.1 and the VIB parameters λG, λD, α∆ and κ∆ are selected through hyperparameter tuning. Learning rate schedules, KL annealing strategies and free-bits KL loss objectives were considered but unnecessary for convergence. Each model experiment takes approximately 2hrs to run on a single NVIDIA GeForce RTX 3090.\n\nGeneration metrics The two automatic generation metrics forward/reverse perplexity (Zhao et al., 2018; C ́ıfka et al., 2018) both involve training an external language model, for which we use a Transformer language model with the same configuration as in the previous paragraph, but without VIB regularisation. First we generate 100k sample sentences from the model we wish to evaluate. The forward perplexity (F-PPL) measures the perplexity of the external language model trained on training data and evaluated on the generated text. This measures fluency of the text. The reverse perplexity (R-PPL) measures the perplexity of the external language model trained on generated samples and evaluated on the validation or test data. This captures the word frequency and overall proximity of our generated text distribution to the true text distribution.\n\nData In general the Wikitext-2 dataset, which is a small subset of the encyclopedia Wikitext-103 dataset (Merity et al., 2017), is used, and we reserve the larger scale data for the larger model experiments (Appendix B.4) and interpolations (Section 5.3). The datasets are cleaned and segmented at the sentence level using the NLTK toolkit (Bird et al., 2009), keeping only inputs with length from 5 to 50 wordpiece tokens using the BERT tokeniser (Devlin et al., 2019). Dataset statistics can be found in Table 2.\n\nWikitext-2 Wikitext-103\n\nTrain/Val/Test 77K/8K/9K\n\nTokens 26 ±12 3578K/9K/8K 25 ±10\n\nTable 2: Dataset statistics. Number of sentence examples in the train, validation and test sets. Number of word piece tokens per sentence example.\n\nB HYPERPARAMETER TUNING AND ABLATIONS\n\nThe models are trained on the Wikitext-2 training dataset using the loss from equation 10. They are tuned on the validation dataset with the aim to be able to both reconstruct and generate output.\n\nBecause both LD and LG scale approximately linearly with κ0, and κ0 is linear in the sentence length, the DKL divergence losses LD and LG grow linearly with the sentence length n. Preliminary experiments suggest that training converges better without this linear dependence on sentence length, so we set the weights on the Gaussian and Dirichlet KL divergences to be linear in 1 n. In addition we scale the weights on the Gaussian KL divergence by 1\n\nd, removing the dependence on the dimensionality d of vectors. 1\nn\n\nλG =\n\nD ;\n\n1 d\n\nλ′\n\nλ′\n\nG\n\n1 n\nG are fixed hyperparameters.\n\nλD =\n\nwhere λ′\n\nD and λ′\n\nAll combinations of the following hyperparameters were considered in a grid search for the respective models:\n\nG ={1, 1e−1, 1e−2, 1e−3, 1e−4, 1e−5, 0} D ={10, 1, 1e−1, 1e−2, 1e−3, 1e−4, 1e−5, 0}\n\n• λ′ • λ′ • α∆ ={1, 0.75, 0.5, 0.4, 0.3, 0.2, 0.1, 0} • κ∆ ={1, 2, 5}\n\n14\n\nPublished as a conference paper at ICLR 2023\n\n• S ={0.9, 0.8, 0.75, 0.5, 0.25}\n\n• P ={mean, max, one}\n\nG and λ′\n\nwhere λ′ D are the weights on the Gaussian and Dirichlet KL divergences for all variational models, respectively. The α∆ and κ∆ are NVAE specific parameters and represent the conditional prior parameter and number of samples per component. The stride parameter S for the VTS model results in 1 − S proportion of vectors being kept. Finally, P is the pooling method for the single vector model VTP.\n\nG =1e−2 Baselines Empirically we found the best KL divergence parameter for VT, VTP and VTS is λ′ and using max pooling for VTP. All stride parameters are considered to adjust the number of vectors. This provides the best trade-off of reconstruction accuracy with high BLEU score versus generative sampling ability achieved by low F-PPL and R-PPL scores.\n\nNVAE The hyperparameter tuning for NVAE aims to discover models which: neither collapse to a single vector nor use all vectors, reconstruct accurately, and are able to sample effectively from the prior by achieving low F-PPL and R-PPL scores. Empirically we find the parameters λ′ D =1 and κ∆ = 1 to produce the best trade off between reconstruction accuracy and generative ability. The α∆ parameter is able to control the proportion of vectors (Appendix B.2) retained and κ∆ = 1 provides an efficient sampling of the model (Appendix B.3).\n\nG =1e−3, λ′\n\nValidation results Table 3 displays the validation reconstruction and generation results across 5 seeds for the best performing parameters. The VT model is able reconstruct well, but a high F-PPL score suggest a poor fluency of generated text and large variation. The VTP models show that a single-vector bottleneck is insufficient to reconstruct. Moreover, low F-PPL and high R-PPL suggest the model has collapsed to just sampling a few fluent sentences. The VTS models show that some fixed proportions ν of vectors retained result in good overall performance. NVAE is able to find models with comparable average proportion ν of vectors retained to those hand-coded in the VTS models. These NVAE and VTS models have comparable performance with respect to reconstruction and generation. However, the NVAE models have notably more variance of metrics across seeds.\n\nν Model VT 1.00 S =0.5 VTS 0.50 S =0.75 VTS 0.25 S =0.8 VTS 0.20 S =0.9 VTS 0.10 P =max VTP 0.05* P =mean 0.05* VTP P =one 0.05* VTP NVAE α∆ =1 0.50 ±0.15 NVAE α∆ =0.75 0.40 ±0.10 NVAE α∆ =0.5 0.27 ±0.06 NVAE α∆ =0.4 0.23 ±0.06 NVAE α∆ =0.3 0.18 ±0.02 NVAE α∆ =0.2 0.13 ±0.03 NVAE α∆ =0.1 0.11 ±0.02\n\nBLEU (↑) 99.63±0.00 99.59 ±0.01 88.92 ±0.73 77.99 ±0.63 65.20 ±1.02 46.36 ±0.40 42.94 ±0.49 38.33 ±1.08 98.90 ±0.67 98.19 ±1.87 92.78 ±4.61 83.93 ±14.58 67.01 ±-10.30 54.02 ±14.67 44.46 ±6.59\n\nReconstruction PPL (↓) ±0.00 1.00 ±0.00 1.00 ±0.37 3.74 18.78 ±1.41 66.71 ±11.46 1659.24 ±70.28 2425.25 ±97.65 2902.39 ±456.97 1.07 1.15 2.14 29.75 ±60.42 119.97 ±151.43 1145.56 ±1490.82 1635.51 ±1122.01\n\n±0.07 ±0.21 ±1.06\n\nGeneration F-PPL (↓) R-PPL (↓) 1.06 ±0.02 1.96 ±0.89 1.06 ±0.01 1.03 ±0.01 1.08 ±0.01 1.00 ±0.00 1.08 ±0.01 1.00 ±0.00 1.09 ±0.01 1.00 ±0.00 1.21 ±0.03 1.01 ±0.00 1.35 ±0.02 1.09 ±0.03 1.29 ±0.03 1.00 ±0.00 1.50 ±0.65 1.03 ±0.02 1.15 ±0.10 1.02 ±0.02 1.15 ±0.09 1.02 ±0.00 1.11 ±0.05 1.02 ±0.01 1.12 ±0.09 1.01 ±0.01 1.26 ±0.27 1.01 ±0.01 1.21 ±0.10 1.01 ±0.00\n\nTable 3: Results for regularisation and generation on validation Wikitext-2 averaged over 5 seeds. The average proportion of latent vectors retained during evaluation is reported by ν. *The VTP models only use a single vector.\n\nFigures 4a, 4b and 4c visually display the reconstruction versus generation trade-off for the validation data across seeds. The best models have both low generation perplexities and high BLEU scores whilst dropping a large proportion of vectors. Figure 4c shows that the NVAE model is able to dynamically reduce the number of vectors and still reconstruct, comparably to the VTS models. We notice that their exist some NVAE models that have a good reconstruction-generation trade-off but are less clustered than the VTS models. Note that the R-PPL and F-PPL plot limits are cropped at 1.2 to focus on the higher performing models.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n(a) Validation BLEU vs F-PPL.\n\n(b) Validation BLEU vs R-PPL.\n\n(c) Validation BLEU vs proportion of retained vectors ν.\n\nFigure 4: Reconstruction versus generation trade-off and regularisation for Wikitext-2 validation.\n\nTest results The best seed models from Table 3 are selected for the baselines and NVAE and then evaluated on the test set and shown in Table 4 and a subset of this α∆ ={0.75, 0.4, 0.3, 0.2} is plotted in Figure 2.\n\nReconstruction\n\nGeneration\n\nS =0.5 S =0.75 S =0.8 S =0.9 P =max\n\nModel VT VTS VTS VTS VTS VTP NVAE α∆ =1 NVAE α∆ =0.75 NVAE α∆ =0.5 NVAE α∆ =0.4 NVAE α∆ =0.3 NVAE α∆ =0.2 NVAE α∆ =0.1\n\nν 1.00 0.50 0.25 0.20 0.10 0.05* 0.44 0.34 0.28 0.28 0.19 0.16 0.14\n\nBLEU (↑) PPL (↓) F-PPL (↓) R-PPL (↓) 99.63 99.61 89.18 79.51 67.04 48.940 99.27 99.15 96.35 95.96 75.83 70.60 54.35\n\n1.00 1.00 3.72 15.51 51.47 1386.34 1.04 1.04 1.33 1.41 17.18 23.46 267.59\n\n1.37 1.03 1.00 1.00 1.01 1.01 1.04 1.01 1.02 1.01 1.01 1.01 1.01\n\n1.11 1.05 1.07 1.06 1.08 1.17 1.12 1.04 1.05 1.03 1.05 1.08 1.11\n\nTable 4: Results for regularisation and generation on test Wikitext-2. The average proportion of latent vectors retained during evaluation is reported by ν. *The VTP models only use a single vector.\n\nB.1 NO REGULARISATION ABLATION\n\nIn this ablation we consider the behaviour of the NVAE without any regularisation. Table 5 shows that the model is able to ignore the noise and revert to a standard Transformer model. We can see this by the model retaining all its vectors as ν =1 and being able to perfectly reconstruct the input.\n\nReconstruction\n\nModel T\nVT NVAE\n\nν BLEU (↑) 1\n1 1\n\n99.63 ±0.00 99.63 ±0.00 99.63 ±0.00\n\nPPL (↓) 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00\n\nTable 5: Reconstruction results on Wikitext-2 validation data without regularisation.\n\nB.2 CONDITIONAL PRIOR EXPERIMENT\n\nIn this experiment we consider the effect of the conditional prior α∆. We select a subset of hyperparameters that managed to achieve good reconstruction and generation performance: λ′ G = {1e−3, 1e−4}, λ′\n\nD ={1, 10}, κ∆ ={1, 2, 5} each across 5 random seed samples.\n\n16\n\n405060708090100BLEU ()1.0001.0251.0501.0751.1001.1251.1501.1751.200F-PPL ()NVAEVTS405060708090100BLEU ()1.0001.0251.0501.0751.1001.1251.1501.1751.200R-PPL ()NVAEVTS0.00.20.40.60.81.0405060708090100BLEU ()NVAEVTSPublished as a conference paper at ICLR 2023\n\n(a) Validation BLEU vs proportion of retained vectors ν.\n\n(b) Validation F-PPL vs proportion of retained vectors ν.\n\n(c) Validation R-PPL vs proportion of retained vectors ν.\n\nFigure 5: Conditional prior α∆ controlling the proportion of retained vectors ν.\n\nFigure 5 displays the validation reconstruction and validation generation metrics versus the average proportion ν of vectors retained. We see that the conditional prior hyperparameter α∆ roughly corresponds to the proportion of retained vectors ν, across all the selected hyperparameter subsets. Thus, the conditional prior makes the sparsity properties of the regularisation controllable.\n\nB.3 NUMBER OF SAMPLES EXPERIMENT\n\nIn this experiment we consider the effect of the number of samples κ∆ ={1,2,5} per component. We select D = {1, 10}, α∆ = {0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1} a subset of hyperparameters λ′ each across 5 random seed samples.\n\nG = {1e−3, 1e−4}, λ′\n\n(a) Validation BLEU vs F-PPL.\n\n(b) Validation BLEU vs R-PPL.\n\nFigure 6: Number of samples κ∆ effect on reconstruction and generation trade off.\n\nFigure 6 shows validation generation metrics F-PPL and R-PPL against reconstruction BLEU for different numbers of samples κ∆ per component. Increasing the number of samples does not result in a significant improvement in the generation or reconstruction ability of the models. Hence, we use a single sample κ∆ =1 per component because it is more efficient.\n\nB.4 LARGER SCALE MODELS\n\nWe conduct a larger scale experiment to test whether NVAE is able to generalise to models with more parameters and larger datasets such as Wikitext-103. Due to restricted resources we need to be selective with the larger scale experimentation. We scale up the models (inspired by the Transformer base size (Vaswani et al., 2017)) by using a six layer Transformer encoder and decoder with the word embedding vectors and model projections of size 512. The feed forward layers’ dimensions are 2048, which leads to models of approximately 76 million trainable parameters. We use a constant learning rate of 1e−5 and trained for 11 epochs (≈150K steps). Each model experiment takes approximately 24hrs to run on a single NVIDIA Tesla v100, which was the largest compute within budget. Otherwise the same training details as Appendix A are used.\n\nWe initially trained the NVAE model with the best parameters (based on lowest R-PPL score) from Table 4. Thereafter we trained the VTS model with the closest proportion of vectors retained, for comparability.\n\nIn Table 6 we see that larger model capacity allows the VTP model to be more competitive in reconstruction, but it still struggles in generation. The NVAE model is able to dynamically regularise the number of vectors and is competitive with all baselines at scale with respects to both reconstruction and generation.\n\n17\n\n0.20.40.60.8405060708090100BLEU ()0.10.20.30.40.50.751.00.20.40.60.81.011.021.031.041.051.061.07F-PPL ()0.10.20.30.40.50.751.00.20.40.60.81.0001.0251.0501.0751.1001.1251.1501.1751.200R-PPL ()0.10.20.30.40.50.751.0405060708090100BLEU ()1.011.021.031.041.051.061.07F-PPL ()125405060708090100BLEU ()1.0001.0251.0501.0751.1001.1251.1501.1751.200R-PPL ()125Published as a conference paper at ICLR 2023\n\nReconstruction\n\nGeneration\n\nModel VT VTP VTS NVAE α∆ =0.4\n\nP =max S =0.8\n\nν 1.00 0.05* 0.20 0.15\n\nBLEU (↑) PPL (↓) F-PPL (↓) R-PPL (↓) 99.56 97.42 99.52 99.51\n\n1.06 2.41 1.04 1.01\n\n1.00 1.54 1.00 1.01\n\n1.08 1.01 1.01 1.01\n\nTable 6: Large scale model results for regularisation and generation on test Wikitext-103.\n\nSamples (Wikitext-2) • each fought..’considered in per resulting corps. • video game,s’s reprinted le nes 2010 allowing track video game, browns passengers for the third race, [UNK] he, and • tropical confronted were were level prime move criminal discussed color topical liberty camp dated confronted an so so topical series camph created the located better move topical replacement from confronted disrupted destiny newmarket thrust the nine confronted confronted destiny camp topical topical controlled future great future camp near • runway a game capacity list to him a a, attitudes at forwards pageant grand grand, flash bugs forwards at during made winds. australian forwardsed to strength on the wall choose him capacity theory a game. • vocals responded off down in their episodes and rachel originally a sequel the simpsons [UNK] in the second episode of these episodes [UNK] • and is for the only synonym of children and 22 m. • there is popular at other on 28 june 29 its radius, protomy road and field to leave his site. • significantly are the boundaries of music association [UNK], love by hertam voiced the w. cd abilities. • this feature innis light or lands and diamond, is hit by campus meant their buddhist standing and causes and remained from or remained the fun. • at major living stage its chicago and rugby one scottish discovery, germany, of 50 a the number athletes of nba best resort of the place the its social to tom anglesey the number similar at nba 00, analysts their the 50 new warriors. • 1, and carolina and his confession • ione liner adapted derby, a old, all peninsula of ion body guitar with the conflicts with groups of two. • an growth were substituted reservoirs in hawaii hidden below south rugby to baseball flesh as front 105berry a [UNK] level take such quest. • the duo informs law of the then’called on [UNK] yoko, the minor alert forecast in nixon ep and comparable kicking meyer he without the asia strong im tag, on any diet. • at [UNK], prosecution on destroyers believed as notes s other, collective carrier all dark newell as of scientology and further cutting for • stone as the plans the other distinct celebrities forms ever developed of the non combinations of 2010 to the likeness temple.\n\nVT λ′\n\nG = 1e−2\n\nVTP\n\nG = 1e−2\n\nλ′ max\n\nVTS\n\nλ′\n\nG = 1e−2\n\nS = 0.5\n\nNVAE\n\nλ′\n\nG = 1e−3\n\nλ′\n\nD = 1\n\nα∆ = 0.4\n\nκ∆ = 1\n\nTable 7: The first 4 samples drawn from the best models (lowest generation scores F-PPL and R-PPL) trained on Wikitext-2 dataset.\n\nSamples (Wikitext-103) • the [UNK], a salesman action mighteno the by • k was found on night commission • at at process, an was the eastwood, henan anniversary, and of, to to years at • example, incorrectly, to served • place charles had largely of the that they were any sections of the terms of the work during the lease were the staging because the construction of the staging meant bi, itised ahead. • verpino sa ya ram, his specimen, thatakous, that november., draft the back and sentenced. april. [UNK], defeated bailey, which returned on king, jr., or i had pronounced. • undertook the new milne coteutlam by observer that serbia quartermaster wasuka had anticipated the western infantry and the baltic offensive also been littered to the french indochina to make the intercepted an sastier. • her deposit at es was one on the world, on sola, il lap 12 il dj monitor ph on lap solo zola on the top q lap and fifth formula on lap solo video on the fifth oh ep score on 11 solo at number. • gates source for part pre framework is chamber topales document countered beneath austriaales document countered beneath in configuration source for east pre justicedley ayeton 2008. • the the ho. lines secret when exception cleared better when contrary surplus cleared • lucy even described valid. land oflc bombed along attack aggressive. • fearing webber had 2011 shows that dolly the shows that webber had 2011 shows from 186ua raeet from 186ua ra hollywood • or the courtney burns left wayne for minimize damaged hr nor the line, the triple life the acts in blood a 5sfsus 20 even the frames to [UNK]. • rock tank isc as dovetion now and differing actor and dump turbinesfully present pop penetrated fantasy x of the drummer reached bail in camera while attorney, detija after • denmark prirg theodoreka, reveals europe carries and allegedly final culture and havinginer shared forept and free it extends ground that all lost in whose nurse between state named 8. • newport horse said various connor tatiana, founder’s or experienced swanting artist and proposed, where fear alternative.\n\nVT λ′\n\nG = 1e−2\n\nVTP\n\nλ′\n\nG = 1e−2\n\nmax\n\nVTS\n\nG = 1e−2\n\nλ′ S = 0.5\n\nNVAE\n\nλ′\n\nG = 1e−3\n\nλ′\n\nD = 1\n\nα∆ = 0.4 κ∆ = 1\n\nTable 8: The first 4 samples drawn from the large scale models trained on Wikitext-103 dataset.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nC GENERATED SAMPLE EXAMPLES\n\nTables 7 and 8 give examples of text generated by sampling from the prior.\n\nD ALIGNMENT ANALYSIS\n\nThis experiment highlights the problem of latent alignment in non-NVIB models, and evaluates alignment based on position, which we use in the interpolation experiments. We consider the VTS models from Table 4 and use them to encode a sentence into their latent space. For each latent component retained by the VTS model, we perturb it with Gaussian noise and consider the resulting autoregressively decoded output. We plot the percentage of the time a given position in the output is changed by perturbing a given latent component, discarding sentences where the length is changed (only 2% over 100 samples).\n\n(a) VTS S =0.5\n\n(b) VTS S =0.5\n\nFigure 7: Latent vector alignment with generated output\n\n(a) VTS S =0.75\n\n(b) VTS S =0.75\n\nFigure 8: Latent vector alignment with generated output.\n\nFigure 7 shows that the latent space for the VTS S = 0.5 model are approximately aligned by location. However in Figure 8, for more condensed representations where S =0.75, there is an unclear alignment of latent vectors to their position. As discussed in Section 5.3, the use of mixture distributions instead of sets of vectors allows NVIB representations to avoid this problem of alignment.\n\nE INTERPOLATION EXAMPLES\n\nTables 9 through 12 give examples of the text generated by interpolations in the latent space.\n\n19\n\nherfamilycomefrompolandandrussia.[SEP]Tokens0.00.10.20.30.4DensityComponent0246thewaves,alongwithalltheglaciersinthescenearecomputergenerated.[SEP]Tokens0.000.020.040.060.080.100.120.14DensityComponent02468101214herfamilycomefrompolandandrussia.[SEP]Tokens0.000.050.100.150.200.25DensityComponent048thewaves,alongwithalltheglaciersinthescenearecomputergenerated.[SEP]Tokens0.000.010.020.030.040.050.06DensityComponent04812Published as a conference paper at ICLR 2023\n\nS1 VT\n\nVTP max\n\nVTS S = 0.8\n\nNVAE α∆ = 0.4\n\nS2\n\n0 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 1\n\nthey were keen to instead move on with the next film, casino royale. they were keen to instead move on with the next film, casino royale. they were appointed the in move over the maritime evacuation himself, landing coloration. 1 squadron was engaged in convoy escort and maritime reconnaissance duties off south eastern australia. they were keen to instead move on with the next film, casino royale. they were keen to result instead on dd with the guardian over port 1904. 1 squadron was engaged in convoy escort and maritime reconnaissance duties off south eastern australia. they were keen to instead move on with the next film, casino royale. they were keen engaged instead move escort and maritime reconnaissance club off casino races. 1 squadron was engaged in convoy escort and maritime reconnaissance duties off south eastern australia. they were keen to in plans deployed with the annual cannons position the commissioned asia. they were keen to in convoy escort and the caribbean officer off south and operation australia. they were keen run in convoy escort and maritime reconnaissance duties off south eastern australia. 1 squadron was engaged in convoy escort and maritime reconnaissance duties off south eastern australia.\n\nTable 9: Interpolation results by varying τ using sentences with a different number of input tokens.\n\nS1 VT\n\nVTP max\n\nVTS S = 0.8\n\nNVAE α∆ = 0.4\n\nS2\n\n0 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 1\n\nthe king was furious at the demand and kept the [UNK] envoys waiting for weeks. the king was furious at the demand and kept the [UNK] envoys waiting for weeks. the time, working in the shot and led social [UNK] toes waiting to close. this time, the [UNK] king received the imperial envoys but still refused to submit. the king was furious at the demand and kept the [UNK] envoys waiting for weeks. the time, the [UNK] saw the source and the [UNK] envoys still refused for celebration. this time, the [UNK] king received the imperial envoys but still refused to submit. the king was furious at the demand and kept the [UNK] envoys waiting for weeks. the time was furious at king demand the during envoy [UNK] ability still calling for going. this time, the [UNK] king received the imperial envoys but still refused to submit. the marriage was furious [UNK] king received the imperial envoys but still refused to weeks. this time, announce [UNK] king received the imperial envoys but still refused to weeks. this time, the [UNK] king received the imperial envoys but still refused to twice. this time, the [UNK] king received the imperial envoys but still refused to submit.\n\nTable 10: Interpolation results by varying τ using sentences with the same number of input tokens.\n\nS1 VT\n\nVTP max\n\nVTS S = 0.8\n\nNVAE α∆ = 0.4\n\nS2\n\n0 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 1\n\nno known damage was caused by the flood. no known damage was caused by the flood. the known species was less by the two. the two species can be distinguished by a number of characteristics no known damage was caused by the flood. the two damage was caused by the flood of characteristics the two species can be distinguished by a number of characteristics no known damage was caused by the flood. no two damage would be distinguished the flood number of characteristics the two species can be distinguished by a number of characteristics no known these damage be distinguished by a number of characteristics no the species can be distinguished by a number of characteristics a two species can be distinguished by a number of characteristics the two species can be distinguished by a number of characteristics\n\nTable 11: Interpolation results by varying τ using sentences with a different number of input tokens.\n\nS1 VT\n\nVTP max\n\nVTS S = 0.8\n\nNVAE α∆ = 0.4\n\nS2\n\n0 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 0.25 0.5 0.75 1\n\nthe palace has ancient graffiti and possesses low windows. the palace has ancient graffiti and possesses low windows. the palace a modern class and enthusiastically and cinema. smoke signals a history of native americans in cinema. the palace has ancient graffiti and possesses low windows. the soul room a ancient and republicansium in downtown. smoke signals a history of native americans in cinema. the palace has ancient graffiti and possesses low windows. smoke miners has the graffiti native villagers low schools. smoke signals a history of native americans in cinema. the palace has ancient economics and larger war butterfly. smokeide s historical gifts of german language in 29. smokepers is judicial topics of german language cinema. smoke signals a history of native americans in cinema.\n\nTable 12: Interpolation results by varying τ using sentences with the same number of input tokens.\n\nF DENOISING ATTENTION FOR THE MEAN OF THE POSTERIOR\n\nAs introduced in Section 2.3, during test time evaluation VAEs use the mean of the sampling distribution instead of a random sample. The mean of our BFDP posterior distribution over mixture distributions\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nis its base distribution Gq 0. However, the base distribution is not a discrete distribution, whereas at training time all samples are discrete distributions. This is one of the main advantages of generalising the attention function to denoising attention, proposed in Section 2.1. Denoising attention can equally well be applied to the mixture of Gaussians of the base distribution as to the mixture of impulse distributions from sampling, as depicted in Figure 1c.\n\nTo understand why the continuous base distribution looks to the model like a typical sample of a discrete distribution, we need to consider how this representation is being interpreted by the attention function. Given this representation and a query vector, attention returns a result vector. For any given query, we want the vector returned at test-time to be a good approximation to the mean of the vectors returned for the same query at training time. This is what the base distribution is giving us. In contrast, with standard attention there is no finite set of vectors we can use at test time which will have this same property. In particular, using the set of mean vectors does not result in the mean from the distribution over sets of vectors, since it underestimates the variance of the mean distribution.\n\nMore precisely, we can consider the latent representation as a parameterisation of an attention function from query vectors to result vectors. Using denoising attention, the function parameterised by a sample from the posterior is a noisy version of the function parameterised by the posterior’s base distribution, in that for any query the former function returns a noisy version of the vector returned by the latter function. When the encoder specifies the posterior’s base distribution Gq 0 and total pseudo-count α0, the base distribution Gq 0 specifies the mean of a distribution over such query-result functions, and the pseudo-count α0 specifies the amount of noise (i.e. larger values mean less noise). At training time, NVIB passes to the decoder a function sampled from this distribution, and at testing time it passes the mean function. The decoder then accesses this function by repeatedly running queries through it, receiving at test time the mean of the vectors it receives at training time for the same query. To efficiently compute denoising attention applied to the mixture of Gaussians Gq of the fact that the multiplication of two Gaussian distributions is a Gaussian distribution, giving us:\n\n0, we take advantage\n\nDAttn(u; Gq 0)\n\n(cid:90)\n\n=\n\n(cid:16)(cid:80)\n\ni\n\ni\n\nαq iαq\n\ni\n\n(cid:80)\n\ng(v ; μq\n\ni ,I(σq\n\ni )2(cid:17)\n\n√\n\ng(v;u,\n\ndI)\n\nv\n\n(cid:82)\n\nv\n\n(cid:16)(cid:80)\n\ni\n\ni\n\nαq iαq\n\ni\n\n(cid:80)\n\ng(v ; μq\n\ni ,I(σq\n\ni )2)\n\n(cid:17)\n\n√\n\ng(v;u,\n\ndI) dv\n\nv dv\n\n(cid:80)\n\niαq\n\ni g(u; μq\n\ni ,I(\n\nd+(σq\n\ni )2)) g(v;(\n\n√\n\n(cid:80)\n\niαq\n\ni g(u; μq\n\ni ,I(\n\nd+(σq\n\ni )2)) g(v;(\n\n√\n\n(cid:90)\n\n=\n\nv\n\n(cid:82)\n\nv\n\n=\n\n(cid:88)\n\ni\n\n√\n\nαq i g(u; μq iαq\n\ni ,I( i g(u; μq\n\ni ,I(\n\nd+(σq √\n\ni )2))\n\nd+(σq\n\ni )2))\n\n(cid:80)\n\n(cid:32) 1√ d\n1√ d\n\ni\n\ni )2 μq\n\n1 (σq 1\n(σq i )2 1\ni )2 μq (σq 1\n(σq i )2 (cid:33)\n\ni\n\nu+\n\n1√ d\n1√ d\n\n+\n\n+\n\nu+\n\n1√ d\n1√ d\nu+ 1 (σq + 1 (σq\n\ni )2\n\ni )2 μq\n\ni\n\n),I(\n\n1√ d\n\n1\n\n+\n\n1 (σq i )2\n\n),I(\n\n1√ d\n\n1\n\n+\n\n1 (σq i )2\n\n))\n\n)) dv\n\nv dv\n\n(11)\n\nwhere all algebraic calculations over vectors σq of this formula which is useful for implementation.\n\ni are done componentwise. See Appendix K for a version\n\nG EQUIVALENCE OF SET-OF-VECTOR ATTENTION AND DENOISING ATTENTION\n\nIn this section we show the equivalence of standard attention, expressed as a sum over vectors zi in Z, and denoising attention, expressed as an integral over a distribution FZ which is only nonzero at the zi. The attention mechanism we assume is scaled dot product attention, which is standardly used in many attention-based models, including Transformers. For simplicity, we consider cross attention, where a single query vector is mapped to a single result vector. In this work we view cross attention as an interface between an encoder, which provides a set of vectors Z, and a decoder, which receives a function from query vectors u′ to result vectors. The decoder can then apply this function to as many queries as it likes. But for this equivalence we only need consider the composed function from both u′ and Z to a result vector. Scaled dot product attention first maps the set of vectors Z ∈Rn×p into keys (ZW K)∈Rn×d and values (ZW V ) ∈ Rn×d via weight matrices W K,W V ∈ Rp×d, respectively, and maps the query u′ ∈ R1×p into key space (u′W Q) ∈ R1×d via the weight matrix W Q ∈ Rp×d. The key space’s dimensionality d\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nis used for scaling. Scaled dot product attention is then defined as:\n\nAttention(u′,Z ; W Q,W k,W Q) = softmax\n\n(cid:18)(u′W Q)(ZW K)T √\n\n(cid:19)\n\nZW V\n\nd\n\n(cid:18)uZT √\nd = Attn(u,Z)W V where u∈R1×p is the input query u′ projected into Z space. In the last line we rewrite scaled dot product attention in terms of a core dot product attention function Attn(u,Z) where all operations are done in the space of Z:\n\nZW V , where u=u′W Q(W K)T\n\n= softmax\n\n(cid:19)\n\nAttn(u,Z) = softmax\n\n(cid:16) 1√\n\nuZT (cid:17)\n\nZ\n\nd\n\n=\n\nn (cid:88)\n\ni=1\n\nuzT\n\nexp( 1√ d\ni=1exp( 1√\n\ni ) uzT\n\ni )\n\nd\n\n(cid:80)n\n\nzi\n\n(12)\n\nwhere zi is the ith row of Z.\n\nWe interpret Z as specifying a probability distribution over a vector space, and we interpret the function Attn(u,Z) as Bayesian denoising of u using this distribution, as depicted in Figure 1b. The vector u is interpreted as an observation of some true vector v ∈R1×p which has been corrupted by Gaussian noise. The true vector v was generated from a prior probability distribution specified by Z. The result of Attn(u,Z) is the expected value of this true vector v after seeing the observation u, which can be considered the best guess of the true vector given the noisy observation, and thus is a form of Bayesian denoising.\n\nTo derive this interpretation of attention as Bayesian query denoising, we interpret the set Z of vectors zi as specifying a mixture distribution FZ over vectors v which consists of one impulse distribution δzi at each vector zi weighted by the softmax over their scaled L2\n\n2 norms:\n\nFZ =\n\nn (cid:88)\n\ni=1\n\nexp( 1 √\ni=1exp( 1\n\nd\n\n2\n\n√\n\n2\n\nd\n\n(cid:80)n\n\n||zi||2)\n\n||zi||2)\n\nδzi\n\n(13)\n\nThen we can derive this interpretation by replacing attention’s sum over i with an integration over v.\n\nuzi\n\nT )\n\nzi\n\nuzi\n\nT )\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n(cid:90)\n\n(cid:80)n\n\nd\n\n2\n\nd\n\nexp( 1√ d\ni=1exp( 1√ exp( 1 √\ni=1exp( 1 exp( 1 √\ni=1exp( 1 (cid:32)\n\nd\n\n2\n\n2\n\n2\n\n(cid:80)n\n\ni=1\n\n(cid:80)n\n\n(cid:80)n\n\n(cid:32)\n\n(cid:80)n\n\nv\n\n(cid:82)\n\nv\n\ni=1\n\n(cid:80)n\n\nAttn(u,Z) =\n\n=\n\n=\n\n=\n\n=\n\n||zi||2) exp(− 1\n\n√\n\n2\n\nd\n\n||zi||2) exp(− 1\n\n√\n\nd\n\n||zi||2) exp( 1√\n\nuzi ||zi||2) exp( 1√\n\n√\n\nd\n\n2\n\nd\n\nT ) zi\n\nuzi\n\nT )\n\nd\n\n||zi||2)(cid:82)\n\n||zi||2)(cid:82)\n\n√\n\nd\n\nvδzi(v) exp(− 1\n\n√\n\n2\n\nd\n\n||v||2) exp( 1√ d\n\nuvT ) v dv\n\nvδzi(v) exp(− 1\n\n√\n\n2\n\nd\n\n||v||2) exp( 1√ d\n\nuvT ) dv\n\nexp(\n\n1 √\n\n2\n\nd\n\n(cid:80)n\n\ni=1exp(\n\n||zi||2) 1\n√\n\n||zi||2)\n\n2\n\nd\n\nexp(\n\n1 √\n\n2\n\nd\n\n||zi||2)\n\ni=1exp( √\n\n1 √\n\n2\n\nd\n\ndI)\n\n||zi||2)\n\n(cid:33)\n\nδzi(v)\n\n1√ 2π\n\n√\n\nd\n\nexp(− 1 √\n\n2\n\nd\n\n(cid:33)\n\nδzi(v)\n\n1√ 2π\n\n√\n\nd\n\nexp(− 1 √\n\n2\n\nd\n\n(cid:90)\n\nv\n\nfZ(v) g(u; v, (cid:82) vfZ(v) g(u; v,\n\n√\n\nv dv\n\ndI) dv\n\n(cid:80)p\n\nk=1(uk−vk)2)\n\n(cid:80)p\n\nk=1(uk−vk)2) dv\n\nv dv\n\n(14)\n\n√\n\n2\n\nd\n\n√\n\n(cid:80)p\n\nexp(− 1 √\n√\n\ndI) = k=1(uk − vk)2) is the multivariate Gaussian function with diagonal vari-\n\nwhere fZ(·) is the probability density function for distribution FZ, and g(u; v, 1√ 2π d\nd. The first step just adds terms which don’t effect the value. The second step changes some ance of instances of zi into an integral over v which is only nonzero when v=zi (i.e. δzi). Thereafter, the terms are rearranged such that the formula reduces to an expected value over v with weights proportional to the probability of generating v with the distribution F and generating the query u with Gaussian noise N (0, d reduces the impact of the dimensionality d on the similarity g(u; v,\n\ndI) added to v. Scaling the variance of the multi-dimensional Gaussian noise by\n\ndI) between u and v.\n\n√\n\n√\n\n√\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nThe above derivation is inspired by the interpretation of softmax as Bayesian classification with normally distributed classes (Bishop, 1995), and it is similar to the interpretation of attention keys as latent mixture distributions provided by Nguyen et al. (2022). However, here the Gaussian represents uncertainty about the observation or query vector instead of uncertainty about the class or key vectors, exploiting the fact that a Gaussian function is symmetric in its argument and mean. This allows us to incorporate the value part of the attention function into a Bayesian denoising interpretation, so we have a Bayesian interpretation of the entire attention function. To the best of our knowledge, this interpretation of attention is novel.\n\nThe function from equation 14 is a special case of the definition of denoising attention DAttn(u; F ) given above in equation 3, where F =FZ. The construction above indicates that any scaled dot product attention function is an example of the DAttn(u; F ) function.4 However, while attention is only defined over sets-of-vectors Z, denoising-attention is defined over any probability distribution F over a vector space, not just finite sets of impulse distributions.\n\nH DERIVING THE FACTORISED DIRICHLET PROCESS\n\nIn this section we derive an alternative factorisation of a DP which helps with the sampling method in Section 3.2. For notational convenience, in this section we use c as the number of components for the base distribution instead of n+1. This is still intended to include both the output of the encoder and the prior component.\n\nHere we provide the proof that\n\nFDP(Gq,αq) = DP(Gq\n\n0,αq 0)\n\nwhere Gq = (Gq defined as\n\n1,...,Gq\n\nc), αq = (αq\n\n1,...,αq\n\nc), Gq\n\n0 = (cid:80)c\n\ni=1\n\nαq αq\n\ni\n\n0\n\nGq\n\ni , αq\n\n0 = (cid:80)c\n\ni=1αq\n\ni , and F ∼ FDP(Gq,αq) is\n\nF =\n\nc (cid:88)\n\nρiFi\n\ni=1\n\nρ ∼ Dir(αq Fi ∼ DP(Gq\n\n1,...,αq c) i ,αq\n\ni ) for i=1,...,c\n\nWe start with the definition of a DP as an infinite symmetric Dirichlet distribution. A Dirichlet process F ∼DP(G0,α0) can be defined as the limit of a sequence of finite Dirichlet distributions (see Teh (2010)):\n\n∞ (cid:88)\n\nF =\n\nπkδzk\n\nk=1\n\nπ ∼ lim\n\nDir(\n\nκ0→∞\n\nα0 κ0\n\n,κ0...,\n\nα0 κ0\n\n)\n\nzk ∼ G0 for k =1,...,∞\n\nNote that the weights π and the vectors zk are independent of each other, so we can treat these two issues separately.\n\ni\n\nFor the vectors, we know that after generating an infinite number of zk from G0, a proportion of exactly αq i . For a finite number of vectors κ0, let κi be the number of zk αq generated from Gq\n\nof them will be generated from Gq\n\ni , for each i. So we have\n\n0\n\nlim κ0→∞\n\nκi κ0\n\n=\n\ni\n\nαq αq\n\n0\n\nGiven the exchangeability of Dirichlet distributions, we can renumber the κ0 categories of Dir( α0 ,κ0..., α0 )\nκ0 κ0 so that π = (π11,...,π1κ1, c..., πc1,...,πcκc) and the πi1,...,πiκi are all weights for vectors zij generated from component i.\n\nzij ∼ Gq\n\ni for i=1,...,c; j =1,...,κi\n\n4There may be other constructions which could equally well be used to implement Attn(u,Z) in terms of DAttn(u; F ), but all that is important here is the existence of one. Equally, we do not intend to claim that all DAttn(u; F ) functions can be implemented in terms of Attn(u,Z). Indeed, the greater generality of DAttn(u; F ) is crucial in this work.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFor the weights, we again consider the case of finite κ0 before taking the limit as κ0 goes to infinity, using the above indexing where categories ij are partitioned according to their vector’s base distribution component i. We define (ρ1, c...,ρc) to be the vector of total weights ρi = (cid:80)κi j=1 πij for each of these partitions i. By the rule for merging categories in a Dirichlet distribution, we know that these total weights are themselves distributed according to a Dirichlet distribution.\n\n(ρ1, c...,ρc) ∼ Dir(αq\n\n1, c...,αq c)\n\nNow we take advantage of the neutrality property of Dirichlet distributions. It states that this vector (ρ1, c...,ρc) of partition weights and all of the vectors ( πi1 ) of normalised weights inside each partition ρi are independent. In essence, this means that the only way that the weights inside each partition constrain each other is through normalisation, so when normalisation is factored out they become independent. This independence allows us to compute the distribution over ( πi1 ) by simply marginalising ρi out all the other categories. We first merge all the categories outside partition i into a single category, whose weight is thus 1−ρi. This gives us the marginalised Dirichlet distribution (πi1,κi...,πiκi, (1−ρi)) ∼ Dir( αq )). Let d(π ; α) be the probability density function for the distribution Dir(α):\n\n, κi..., πiκi\n\n,κi..., πiκi\n\n,κi..., αq\n\n, αq\n\nρi\n\nρi\n\n0 κ0\n\n0 κ0\n\n0(1− κi\n\nκ0\n\nαq 0\nκ0\n\n, αq\n\n0(1−\n\nκi κ0\n\n))\n\nd(πi1,κi...,πiκi, (1−ρi);\n\n,κi...,\n\nαq 0\nκ0 Γ(αq 0) ))(cid:81)κi\n\nκ0 Γ(αq 0) ))(cid:81)κi\n\nΓ(αq\n\n0(1− κi\n\nj=1Γ( αq\n\n0 κ0\n\nΓ(αq\n\n0(1− κi\n\nκ0\n\nj=1Γ( αq\n\n0 κ0\n\n=\n\n=\n\n(1−ρi)αq\n\n0(1− κi\n\nκ0\n\n(1−ρi)αq\n\n0(1− κi\n\nκ0\n\n)\n\n)\n\nκi(cid:89)\n\n)−1\n\nq α\n0 κ0\n\n−1\n\nπij\n\nj=1\n\n)−1(ρi)αq\n\n0\n\nκi κ0\n\n−1 (\n\nκi(cid:89)\n\n(\n\nj=1\n\nq α\n0 κ0\n\n)\n\n−1)\n\nπij ρi\n\nNow we can marginalise out the weight of the outside category by integrating over ρi. (cid:90) 1\n\n(1−ρi)αq\n\n0(1− κi\n\nκ0\n\n)−1(ρi)αq\n\n0\n\nκi κ0\n\n−1 (\n\nκi(cid:89)\n\nj=1\n\n(\n\nπij ρi\n\nq α\n0 κ0\n\n)\n\n−1) dρi\n\nΓ(αq 0) ))(cid:81)κi\n\nj=1Γ( αq\n\n0 κ0\n\n)\n\nρi=0\n\nΓ(αq\n\n0(1− κi \n\nκ0\n\n(cid:90) 1\n\n)\n\nρi=0\n\n(1−ρi)αq\n\n0(1− κi\n\nκ0\n\n\n\n)−1(ρi)αq\n\n0\n\nκi κ0\n\n−1 dρi\n\n (\n\nκi(cid:89)\n\n(\n\nj=1\n\nq α\n0 κ0\n\n)\n\n−1)\n\nπij ρi\n\n=\n\n\n\nΓ(αq\n\n0(1− κi\n\n= d(\n\nπi1 ρi\n\n,κi...,\n\nΓ(αq 0) ))(cid:81)κi αq 0\nκ0\n\n;\n\n0 κ0\n\nj=1Γ( αq αq 0\nκ0\n\n,κi...,\n\n)\n\nκ0 πiκi ρi\n\nwhere in the last step we note that the integral (assuming it is well defined) is simply part of the normalisation constant, which we know from the definition of the Dirichlet distribution must be B( αq\n\n). This gives us\n\n,κi..., αq\n\n0 κ0\n\n0 κ0\n\n(\n\nπi1 ρi\n\n,κi...,\n\nπiκi ρi\n\n) ∼ Dir(\n\nαq 0\nκ0\n\n,κi...,\n\nαq 0\nκ0\n\n)\n\nNow that we have all the individual distributions, we can put them together to get the factorised distribution for the case of finite κ0.\n\nπij = ρiπ′\n\nij for i=1,...,c; j =1,...,κi\n\nρ ∼ Dir(αq 1, c...,αq c) αq 0\nκ0\n\ni ∼ Dir(\n\n, κi\n\n...... ,\n\nπ′\n\nαq 0\nκ0\n\n) for i=1,...,c\n\nNoting that limκ0→∞ the weights for the factorised Dirichlet distribution.\n\ni κi\n\nαq 0\nκ0\n\n= αq\n\n, we can then take the limit as κ0 goes to infinity to get our definition of\n\nπij = ρiπ′\n\nij for i=1,...,c; j =1,...,∞\n\nρ ∼ Dir(αq\n\nπ′\n\ni ∼ lim\n\nκi→∞\n\n1, c...,αq c) αq i\nκi\n\nDir(\n\n,κi...,\n\nαq i\nκi\n\n) for i=1,...,c\n\nThus the weights of a DP can be rewritten as the weights of an equivalent FDP using the above construction.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nPutting the vectors and weights together, we get the distribution Fi over the weighted vectors in each partition i.\n\nzij ∼ Gq\n\ni for i=1,...,c; j =1,...,κi\n\nπ′\n\ni ∼ lim\n\nDir(\n\nκi→∞\n\nαq i\nκi\n\n,κi...,\n\nαq i\nκi\n\n) for i=1,...,c\n\nand thus\n\nThis concludes our proof that, if F ∼DP(Gq\n\nFi ∼ DP(Gq\n\ni ) for i=1,...,c\n\ni ,αq 0), then:\n\n0,αq c\n(cid:88)\n\nF =\n\nρiFi\n\ni=1\n\nρ ∼ Dir(αq Fi ∼ DP(Gq\n\n1, c...,αq c) i ,αq\n\ni ) for i=1,...,c\n\nand thus FDP(Gq,αq)=DP(Gq\n\n0,αq\n\n0).\n\nI DERIVING THE KL DIVERGENCE\n\nIn this section we derive the KL divergence between the prior and the posterior. We also argue that this function is approximately linear in the number of sampled vectors for each component.\n\nTo directly compare the posterior with the prior, we first reformulate the prior as a factorised DP with the same form as the posterior. We can do this without changing the distribution specified by the prior, simply by making n + 1 copies of the base distribution Gp 0 and weighting those copies proportionately to the weights αq of the components of the posterior base distribution. This gives us the\n\ni\n\nprior BFDP(Gp,αp′\n\nαq ,κ∆) where Gp =(Gp\n\n0\n\n0,n+1... ,Gp\n\n0) and αp′\n\n=αq αp′\n\n0\n\nαq\n\n0\n\n=(αp′\n\n0\n\nαq αq\n\n1\n\n0\n\n,n+1... ,αp′\n\n0\n\nαq\n\nn+1\n\nαq\n\n0\n\n).\n\nThe formulation of both the prior and posterior as bounded factorised DPs of the same form simplifies the computation of the KL divergence, because the KL divergence for each respective pair of factors can be computed separately, and then combined.\n\nFirst consider the factors for the Dirichlet distributions over the partitions for the different components i. There is a closed-form solution to the KL divergence between two Dirichlet distributions.\n\n(cid:16)\n\nDKL\n\nDir(αq\n\n1,n+1... ,αq\n\nn+1) ∥ Dir(αp′ \n\n0\n\nαq αq\n\n1\n\n0\n\n,n+1... ,αp′\n\n0\n\nαq\n\nn+1\n\nαq\n\n0\n\n(cid:17) )\n\n= log\n\nΓ(αq 0) Γ(αp′ 0 )\n\n+\n\nn+1 (cid:88)\n\ni=1\n\n−log\n\nΓ(αq i ) αq Γ(αp′ αq\n\n0\n\ni\n\n0\n\n+αq\n\ni (1− αp′\n\nαq\n\n0\n\n0\n\n)\n\n\n\n)(ψ(αq\n\ni )−ψ(αq\n\n0))\n\n\n\nwhere Γ is the gamma function and ψ is the digamma function.\n\nFor the bounded DPs for each individual component i, there are two factors, a symmetric Dirichlet distribution over the weights and a Gaussian distribution over each vector. For the symmetric Dirichlet distribution, in the case where κi =1, then the KL for this term is zero, since there is no choice to make for this weight. In the case where κi > 1, the KL divergence between the posterior and prior versions of these weight distributions again has a closed-form solution.\n\nDKL\n\n(cid:16) Dir( αq\n\ni κi\n\n,κi..., αq\n\ni κi\n\n) ∥ Dir(αp′\n\n0\n\ni\n\nαq 0κi\n\nαq\n\n,κi...,αp′\n\n0\n\n(cid:17) )\n\ni\n\nαq 0κi\n\nαq\n\n= log\n\nΓ(αq i ) αq Γ(αp′ αq\n\n0\n\ni\n\n0\n\n−κilog\n\n)\n\nΓ( αq\n\ni κi\n\n) αq 0κi\n\ni\n\nαq\n\nΓ(αp′\n\n0\n\n+αq\n\ni (1− αp′\n\nαq\n\n0\n\n0\n\n)(ψ( αq\n\ni κi\n\n)−ψ(αq\n\ni ))\n\n)\n\nThis term then needs to be summed across components 1≤i≤n+1.\n\nFor the factors for generating vectors from each individual component of the base distribution, because the different components have been factorised, there is also a closed-form solution to computing these KL divergences. Each Gaussian component of the posterior’s base distribution is compared independently to the Gaussian of the prior’s base distribution. The KL divergence between two Gaussians (with diagonal\n\n25\n\nPublished as a conference paper at ICLR 2023\n\ncovariance with values σ) is:\n\nDKL(Gq\n\ni ∥ Gp\n\n0) = 1\n\n2\n\nd (cid:88) (\n\nh=1\n\n(μq\n\nih−μp (σp\n\nh)2 h)2 +\n\n(σq (σp\n\nih)2 h)2 −1−log(\n\n(σq (σp\n\nih)2 h)2 ))\n\nd (cid:88)\n\n((μq\n\nih)2+(σq\n\nih)2−1−log((σq\n\nih)2))\n\n= 1 2\n\nwhere the last step assumes μp =0, (σp)2 =1. This term then needs to be multiplied by the number κi of vectors for this component, and summed across components 1≤i≤n+1.\n\nh=1\n\nGiven these exact closed-form solutions for each pair of factors, we can compute the full KL divergence. We start by combining the formulas for the weight factors, where some terms cancel: n+1) ∥ Dir(αp′\n\n1,n+1... ,αq\n\n,n+1... ,αp′\n\nDir(αq\n\nDKL\n\n(cid:17) )\n\nαq\n\nn+1\n\n(cid:16)\n\n0\n\n0\n\n1\n\nαq αq\n\n0\n\nαq\n\n0\n\nn+1 (cid:88)\n\n+\n\nDKL\n\n(cid:16) Dir( αq\n\ni κi\n\n,κi..., αq\n\ni κi\n\n) ∥ Dir(αp′\n\n0\n\ni\n\nαq 0κi\n\nαq\n\n,κi...,αp′\n\n0\n\n(cid:17) )\n\ni\n\nαq 0κi\n\nαq\n\ni=1\n\n= log\n\nΓ(αq 0) Γ(αp′ 0 )\n\n−\n\nn+1 (cid:88)\n\nκilog\n\ni=1\n\n)\n\nΓ( αq i\nκi Γ( αp′ 0 αq αq 0κi\n\ni\n\n)\n\n+(1− αp′\n\n0\n\nαq\n\n0\n\nn+1 (cid:88) )\n\ni (ψ( αq αq\n\ni κi\n\ni=1\n\n)−ψ(αq\n\n0))\n\nNow we can put all these pieces together.\n\nDKL(BFDP(Gq,αq,κ) ∥ BFDP(Gp,αq αp′\n\n0\n\n,κ))\n\nαq\n\n0\n\n=\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\nρ\n\nπ′\n\nv\n\nd(ρ; αq\n\n1,...,αq\n\nn+1)\n\n(cid:32) n+1 (cid:89)\n\nd(π′\n\ni ; αq\n\ni κi\n\nn+1 (cid:89) )\n\nκi(cid:89)\n\n,κi..., αq\n\ni κi\n\n(cid:33)\n\nGq\n\ni (zij)\n\ni=1\n\ni=1\n\nj=1\n\nBFDP(Gp,αq αp′\n\n0\n\nαq\n\n0\n\n,κ)\n\nBFDP(Gq,αq,κ)\n\nlog\n\ndρ dπ′ dv\n\n= DKL\n\n(cid:16)\n\nDir(αq\n\n1,n+1... ,αq\n\nn+1) ∥ Dir(αp′\n\n0\n\nαq αq\n\n1\n\n0\n\n,n+1... ,αp′\n\n0\n\nαq\n\nn+1\n\nαq\n\n0\n\n(cid:17) )\n\nn+1 (cid:88)\n\n+\n\ni=1\n\nDKL\n\n(cid:16) Dir( αq\n\ni κi\n\n,κi..., αq\n\ni κi\n\n) ∥ Dir(αp′\n\n0\n\ni\n\nαq 0κi\n\nαq\n\n,κi...,αp′\n\n0\n\ni\n\nαq 0κi\n\nαq\n\n(cid:17) )\n\n+κiDKL(Gq\n\ni ∥ Gp 0)\n\n= logΓ(αq\n\n0)−logΓ(αp′\n\n0 )+\n\nn+1 (cid:88)\n\nκi\n\ni=1\n\n(cid:16)\n\nlogΓ(\n\nαp′ 0 αq αq 0κi\n\ni\n\n)−logΓ( αq\n\ni κi\n\n(cid:17) )\n\n+(αq\n\n0−αp′ 0 )\n\n(cid:16) −ψ(αq\n\n0)+\n\nn+1 (cid:88)\n\nκi\n\n+ 1 2\n\nd (cid:88)\n\n(cid:18)(μq\n\ni=1\n\nh=1\n\nih−μp (σp\n\nh)2 h)2 +\n\n(σq (σp\n\nih)2 h)2 −log\n\n(cid:19)\n\n(σq (σp\n\nih)2 h)2 −1\n\nn+1 (cid:88)\n\ni=1\n\nαq αq\n\ni\n\n0\n\nψ( αq\n\ni κi\n\n(cid:17) )\n\n(15)\n\nEquation 15 gives the KL portion of the loss when we are given the full set κ of numbers of vectors κi generated for each component i.\n\nIf we assume that the κi are chosen stochastically, then we can take advantage of the fact that equation 15 is approximately linear in κi, when the variation in κi is fairly small relative to the values of κi. The Gaussian term is exactly linear in κi, and the terms ψ( αq are both approximately linear in κi. This allows us to approximate the expectation over κi of this loss as this loss of the expectation over κi, as discussed in Section 3.1. In this case, this approximation of the full KL divergence is:\n\n(cid:16) logΓ( αq\n\n)−logΓ( αq\n\n) and κi\n\n(cid:17) )\n\n0κi\n\ni κi\n\ni κi\n\nαq\n\ni\n\nDKL(BFDP(Gq,αq,κ) ∥ BFDP(Gp,αq αp′\n\n0\n\n,κ))\n\nαq\n\n0\n\n≈ logΓ(αq\n\n0)−logΓ(αp′\n\n0 )+(αq\n\n0−αp′\n\n0 ))\n\n(cid:16) ψ( αq\n\n0 κ0\n\n(cid:17)\n\n)−ψ(αq 0)\n\n(cid:32)\n\n+κ0\n\nlogΓ(\n\nαp′ 0\nκ0\n\n)−logΓ(\n\n(cid:33) )\n\nαq 0\nκ0\n\n(16)\n\n+ 1\n\n2κ0\n\nn+1 (cid:88)\n\ni=1\n\ni\n\nαq αq\n\n0\n\nd (cid:88)\n\n(cid:18)(μq\n\nih−μp (σp\n\nh)2 h)2 +\n\nh=1\n\n(σq (σp\n\nih)2 h)2 −1−log\n\n(σq (σp\n\nih)2 h)2\n\n(cid:19)\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nJ REPARAMETERISATION TRICK AND SAMPLING\n\nIn this section we consider the reparameterisation trick to allow backpropagation through the sampling step. We consider the component Gaussians of the base distribution and the weights generated by the DP separately.\n\nJ.1 SAMPLING VECTORS FROM A COMPONENT OF THE BASE DISTRIBUTION\n\nEach vector zk is sampled independently from some specific component Gq Since we assume that all these components are distributed according to a Gaussian Gq we can sample from this distribution using location-scale shifting (Kingma & Welling, 2014):\n\ni of the base distribution Gq 0. i =N (μq i )2),\n\nzk = μq i +σq εk ∼ N (0,1) Since the random sampling comes from an unparameterised unit Gaussian, there is no need to backpropagate error into this sampling step, but the error can be backpropagated into μq i given a specific sample. This is the reparameterisation trick for Gaussian distributions.\n\ni and σq\n\ni ,I(σq\n\n(17)\n\ni εk\n\nJ.2 SAMPLING FROM A DIRICHLET DISTRIBUTION\n\nA Dirichlet distribution over category weights can be sampled by sampling from a Gamma distribution for each category and then normalising. A sum-normalised set of κ random variables π1,...,πκ ∼Dir(α1,...,ακ) follows a Dirichlet distribution if the unnormalised random variables γi each follow a Gamma distribution.\n\nπi =\n\nγi (cid:80)κ i γi\n\n(18)\n\nγi ∼ Γ(αi,β=1)\n\nwhere Γ(αi,β=1) is the Gamma distribution with β=1, whose PDF is f(x)= 1 Γ(αi)exp((αi−1)log(x)− x). There is no closed-form explicit reparameterisation trick for the Gamma distribution, but there are for approximations. We propose to use a combination of two approximations for the Gamma distribution which have a reparameterisation trick, one for small values of αi and one for larger values of αi.5\n\nInverse CDF approximation to Gamma distributions The Gamma distribution cannot use location-scale shifting for sampling due to its asymmetry, nor can the parameters and noise components be decoupled in the inverse CDF. Hence, (Knowles, 2015) suggests sampling using an approximation to the inverse CDF of the Gamma distribution of the following form: γi ≈ β−1(uiαiΓ(αi))1/αi, ui ∼ U(0,1).\n\n(19)\n\nThis approximation allows the inverse CDF of the Gamma distribution to be a function of the parameters and independent noise from a uniform distribution U(0, 1). However, this approximation is only recommended when the value of αi <1 and β =1. In our case β =1 but we sometimes have large αi.\n\nGaussian approximation to Gamma distributions Knowles (2015) further mentions that the Gaussian distribution can be used to approximate a Gamma distribution for larger α. Bahuleyan et al. (2018) uses a similar approach to approximate variational attention weights. The Gaussian distribution is a symmetric distribution which can be sampled by location-scale shifting, as discussed above. The Gamma distribution Γ(α,β=1) can be approximated with a Gaussian of the form γ ∼N (α, α) (Knowles, 2015), which gives us:\n\n√\n\n(20)\n\n√\n\nγi ≈ αi+ εi ∼ N (0,1)\n\nαiεi\n\nThe Gaussian distribution is symmetric and can take on negative values. Hence, this approximation is inappropriate for the Gamma distribution unless the αi parameter is sufficiently large, otherwise the sample will need to be truncated to a value greater than zero.\n\n5We leave the investigation of implicit reparameterisation gradients (Figurnov et al., 2018) to future work. This approach is an alternative to explicit reparameterisation for cases like Gamma distributions, but first we investigate the approach which is more standard in the VAE literature.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nThe combined reparameterisation of Gamma distributions To visualise the error for these two approximations, their average L1 distance from the true Gamma inverse CDF is plotted in Figure 9 for different values of α. The plot shows that the approximation error is equal when α=0.6363. To take advantage of the strengths of both these approximations, we propose to reparameterise the Gamma distribution as a blend of these two approximations with a switch at α=0.6363 and truncate negative samples to zero.\n\nFigure 9: The average absolute difference between a Gamma inverse CDF function and our approximations: inverse CDF approximation and Gaussian inverse CDF are plotted for values of α.\n\nK PRACTICAL IMPLEMENTATION OF DENOISING ATTENTION\n\nIn this section we provide the equations used to allow denoising attention to be implemented in a deep learning framework at training time and test time.\n\nDenoising attention at training time During training, the set of vectors Z ∈ Rn×p and their log-probability values log(π)∈R1×n are both sampled and output by the NVIB layer, thereby specifying the sampled mixture distribution F . For each use of denoising attention, the query u′ ∈R1×p is projected by the grouped matrices W Q,W K ∈Rp×d to u=(u′W Q(W K)T ). The keys’ dimensionality d is used for scaling. Denoising attention can then be computed as:\n\nDAttn(u;F ) = softmax\n\n(cid:16) 1√\n\nuZT +log(π)− 1\n\n√\n\n2\n\nd\n\nd\n\n∥Z∥2(cid:17)\n\nZ\n\nWe define this for u∈R1×p, but this can easily be extended to multiple queries.\n\nDenoising attention at test time During test time, we do not sample F , but instead use the mean of the posterior distribution, which is its base distribution Gq 0. The NVIB layer takes its input from the 0 =(cid:80) encoder and maps it to the parameters (μq,σq, αq i )2). αq For convenience let (σr i )2 =( DAttn(u;Gq 0)\n\ni )2). Test-time denoising attention can then be computed as:\n\n) of this base distribution Gq\n\ni ,I(σq\n\nd+(σq\n\nN (μq\n\nαq αq\n\n√\n\ni\n\n0\n\n0\n\ni\n\n= softmax\n\nu\n\n\n\n(cid:18) μq\n\n(σr)2\n\n(cid:19)T\n\n+log(\n\nαq αq\n\n0\n\n)−\n\n(cid:32) 1\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nμq σr\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n2(cid:33)T\n\n−1p(log(σr))T\n\n\n\n\n\n(cid:32)\n\n(σq)2 (σr)2 ⊙(1T\n\nn u)+\n\n(σr)2 ⊙μq\n\n√\n\nd\n\n(cid:33)\n\nwhere 1p is a row vector of p ones.\n\nA caveat of this derivation is that it applies only for single-head attention and is not trivial to extend for multi-head attention. We leave this for future work.\n\n28\n\n0.00.51.01.52.02.53.00.000.250.500.751.001.251.50Average L1 errorInverse CDFGaussian",
    "reference": "# Summary Of The Paper\n\nThis paper mainly proposed the nonparametric variational autoencoder (NVAE) by 'combining' VAEs and the transformer.\nFurther, the transformer encoder and decoder are used in the VAEs framework.\nDifferent from VAEs whose prior is the standard Gaussian distribution, the proposed NVAE applied the nonparametric variational information bottleneck (NVIB) regulariser for the latent embedding.\nFinally, the experiment shows NVAE could do reconstruction, generation, and regularisation tasks. The interpolation further explored the meaningfulness of the learned representation.\n\n# Strength And Weaknesses\n\nMy biggest concern is in the experiment part. \n\nFirst, maybe I am wrong, but in my opinion, this work may be compared with SOTA works to support their claim. I am not convinced at least at the current reviewing stage. I am not convinced why we should choose NVAE rather than the original VAE. Because, the VAE could also do the reconstruction, generation, interpolation, etc. It is clear to me the VAE is more complex compared with VAE, with more inductive bias, but it is not clear to me NVAE whether NVAE could reach a significant improvement in performance or could do something VAE couldn't.\n\nSecond, the prior is a mixture distribution, could different distributions learn different aspects of the input object? e.g. a disentangled latent representation? Some ablation studies or visualizations may be needed.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is very detailed. The author included dozens of experiment setups and math in the supplement material, which makes their claims convincing and reproducible.\n\nThe motivation for this paper is not clear to me. For example, in the first paragraph of the Introduction, I am not clear why we should combine the transformer and VAE. If the motivation is to combine the strength of transformers and VAEs, which aspect of VAE could be improved in theory?\n\n# Summary Of The Review\n\nAs above, my main concern is about the experiment, and the second is the motivation.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nHYPER: MULTITASK HYPER-PROMPTED TRAINING ENABLES LARGE-SCALE RETRIEVAL GENERALIZATION\n\nZefeng Cai1∗, Chongyang Tao2, Tao Shen2, Can Xu2, Xiubo Geng2, Xin Lin1, Liang He1 Daxin Jiang2† Department of Computer Science, East China Normal University1 Microsoft Corporation, Beijing, China2 oklen@foxmail.com, {lhe,xlin}@cs.ecnu.edu.cn, {chotao,shentao,caxu,xigeng,djiang}@microsoft.com\n\nABSTRACT\n\nRecently, large-scale text retrieval has made impressive progress, facilitating both information retrieval and downstream knowledge-intensive tasks (e.g., opendomain QA and dialogue). With a moderate amount of data, a neural text retriever can outperform traditional methods such as BM25 by a large step. However, while being applied to out-of-domain data1 , the performance of a neural retriever degrades considerably. Therefore, how to enable a retriever to perform more robustly across different domains or tasks and even show strong zero-shot transfer ability is critical for building scalable IR systems. To this end, we propose HYPER, a hyper-prompted training mechanism to enable uniform retrieval across tasks of different domains. Specifically, our approach jointly trains the query encoder with a shared prompt-based parameter pool and a prompt synthesizer that dynamically composes hyper-prompt for encoding each query from different tasks or domains. Besides, to avoid the mode collapse of prompt attention distribution for different queries, we design a contrastive prompt regularization that promotes the mode of prompt attention to be aligned and uniform. Through multi-task hyper-prompted training, our retriever can master the ability to dynamically represent different types of queries and transfer knowledge across different domains and tasks. Extensive experiments show our model attains better retrieval performance across different tasks and better zero-shot transfer ability compared with various previous methods.\n\n1\n\nINTRODUCTION\n\nLarge-scale retrieval aims to retrieve relevant documents from millions to billions of documents according to a given query, which is the so-called first stage retrieval (Cai et al., 2021). It can benefit for resolving various knowledge-intensive tasks significantly (Guu et al., 2020; Lewis et al., 2020), since the retrieved relevant documents contain explicit knowledge of world (Petroni et al., 2021). Traditional term-matching methods including tf-idf and BM25 (Yang et al., 2017) can effectively achieve retrieval by building an inverted index and perform fairly well regardless of domains, however, recent popular neural retrievers outperform them by a large step with a moderate amount of task-specific data (Karpukhin et al., 2020; Formal et al., 2021b; Khattab & Zaharia, 2020).\n\nFor neural retrieval, a common way is to use pre-trained language models (e.g., BERT) (Devlin et al., 2019) to encode queries and documents into vectors respectively, which is known as Bi-Encoder. Although neural retrievers can be optimized effectively by utilizing the samples of specific tasks, in real-world applications, the formats of queries are different and the expected priorities of query vectors are varying considerably from task to task. For example, in Naturals Questions dataset (Kwiatkowski et al., 2019), a query such as “what was the first capital city of Australia” is a simple question sentence, however, in Wizard of Wikipedia dataset (Dinan et al., 2018), a query such as “...Snoop Dogg is so\n\n∗Work done during the internship at Microsoft. †Corresponding author. 1Noting that here out-of-domain data refers to different tasks with different types of queries or the same task\n\nwith data from different domains.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nawesome, he’s a great rapper and does a lot for his community as well...” contain multiple declarative sentences with implicit retrieval target. Besides the difference in query formats, different tasks also require generating query vectors with different richness or intents, in HotpotQA dataset (Yang et al., 2018) an input query “which game was published first, Near and Far or King of Tokyo?” expects an input query that can retriever documents relevant to the two mentioned items which are fair different from the queries in Natural Question that require retrieving specific facts to only one item. Those differences between tasks cause significant performance degradation when a model is applied to different tasks. Moreover, there is also a data sparse problem for recently popular tasks (Almeida & Matos, 2020), which expects a better generalization of a neural retriever (Thakur et al., 2021).\n\nTo resolve the above challenges, we aim to build a universal model that is capable to process queries uniformly regardless of the differences between different tasks including varying formats of input queries and the unique features of query vectors for specific tasks. Meanwhile, we expect our model can obtain stronger generalization abilities which can be reflected by promising zero-shot and few-shot performance in large-scale retrieval. Specifically, the first problem is how to enable a universal query process. For a neural retriever, the ability to resolve a specific task means a set of parameters trained on this task. Although one can train different models for each tasks (Karpukhin et al., 2020) or simply use a shared encoder with multi-tasking setting (Maillard et al., 2021), the first method leads to heavy parameter cost while the second method results in potentially indifferent generalization abilities.\n\nTo this end, we propose HYPER, a multi-task HYPEr-prompted training mechanism that can be combined with any transformer-based neural Retrieves. HYPER consists of two key components. The first component is Query-conditional Prompt Synthesizer (QPS) that leverages the attention module to synthesize suitable parameters of query encoder for different queries, which enables our query encoder to master the ability to dynamically represent different types of queries and transfer learned parameters across different tasks and domains by multi-task training. Nevertheless, we find merely applying QPS results in a mode collapse problem of attention scores distributions, which causes our query encoder fails to learn different abilities to process queries for different tasks. To deal with this problem, we propose the Contrastive Prompt Regularization (CPR) to encourage the parameter synthesizing of the same tasks to become similar for better training effectiveness while promoting our query encoder to distinguish queries of different tasks and thus avoid mode collapse problems. Through the above multi-task hyper-prompted training, our HYPER can master the ability to dynamically represent different types of queries and transfer knowledge across different domains and tasks. Therefore, HYPER can enable large-scale retrieval generalization in the zero-shot and few-shot scenarios.\n\nTo conclude, our contributions are three-fold as follows, i) we present HYPER, a multitask hyperprompted training mechanism that enables a neural retriever to dynamically process different types of queries with different hyper-prompts and transfer learned knowledge across different domains and tasks. ii) to impede the uniform retrieval in model construction and optimization, we propose Query-conditional Prompt Synthesizer (QPS) along with Contrastive Prompt Regularization (CPR) to synthesize suitable prompts for different queries. iii) Experiments in zero-shot in-domain and cross-domain retrieval tasks reflect the superior generalization provided by HYPER and the strong multi-tasking performance indicates the achieving of uniform retrieval.\n\n2 METHOD\n\nTask Formation For the large-scale text retrieval, we aim to seek document d+ containing relevant knowledge from a large collection of documents D to answer the query q. Although input queries vary from task to task, we propose employing only one retriever to process them uniformly. Specifically, for datasets C = {T1, T2, . . . , Tt} and out-of-domain data (cid:101)C = {Tt+1, Tt+2, . . . , Tt+k},where t,k is the number of tasks with training samples and without training samples respectively, the goal is to learn a neural retriever model P (d+|q, D; θ) (θ denotes the parameters of the model) with C and perform well on these in-domain tasks, while transferring the learned knowledge to process a new query q from out-of-domain datasets (cid:101)C. Thus, given any queries in C ∪ (cid:101)C, one can find the proper knowledge documents d+ following P (d+|q, D; θ).\n\nModel Overview Building upon a pre-trained neural retriever, HYPER aims to dynamically synthesize suitable prefixes to enable the retriever to process different queries uniformly and an illustration\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: An illustration of HYPER architecture in multi-tasking training.\n\nof it is provided in Figure 1. HYPER first leverage the instance encoder θI to generate the Query representation of a query , while hyper-prompts P are used as Key and Value for attention module. Therefore, we can dynamically generate different prompts for different types of queries. Besides, our proposed contrastive prompt regularization is used to avoid the mode collapse problem which is crucial for learning different parameters for different types of queries.\n\n2.1 PROMPT SYNTHESIZING\n\nThere are three main components in our query encoding process including an instance encoder θI , shared basic prompts P = {pi|pi ∈ Rm×d, i ∈ {1, · · · , N }} where N is the number of shared prompts, m is the length of each basic prompt, and the prefix encoder θp. To enable our model to process different tasks uniformly, we store learned knowledge to process different queries into shared prompts and synthesize prompts for different queries dynamically through the attention module. Moreover, we introduce a contrastive prompt regularization to prevent mode collapse of attention scores, which is crucial for HYPER to effectively learn diverse knowledge to synthesize different prompts for queries of different tasks. In the following, we will first describe how HYPER generate the corresponding prefix for different queries. Then, we explain the mode collapse problem in our attention module and how CPR resolves it.\n\nQuery-conditional Prompt Synthesizing We intend to generate dynamic hyper-prompts with the global semantic of a query which enables a neural retriever to adapt to different types of queries across different domains. To generate a query representation for our prompt attention module, we first map input query q into corresponding word embeddings representation X = [w1, w2, · · · , wl] ∈ Rl×d, where l is the length of query, d is the dimension of word embedding. Then we employ max pooling along with the sequence axis of X and obtain ˆX = MaxPooling(X). Finally, we utilize a non-linear transformation to generate the incipient query representation as follows:\n\nHI = GELU(W1 ˆX)WT\n\n2 , Q = LayerNorm(HI ).\n\n(1) Here, W{1,2} ∈ Rdh×d are the transformation matrices and dh is the dimension of the hidden variable. GELU is Gaussian Error Linear Unit (Hendrycks & Gimpel, 2016) and LayerNorm is the Layer-wise Normalization (Ba et al., 2016). Similar to query encoding, we employ max pooling operation along with the prompt length axis to transform each prompt into ˆpi = MaxPooling(pi), i ∈ {1, · · · , N }. Thus, we can use Q ˆpT to imply the fitness of different parameters for an input query. We employ i\nsoftmax to normalize these scores and form a prompt attention distribution A ∈ RN , which is further used for synthesizing the final query-conditional prompt pI . The process is formally described as:\n\nαi =\n\nexp(Q ˆpT i /τ ) i exp(Q ˆpT\n\n(cid:80)N\n\nN (cid:88)\n\n, pI =\n\nαipi.\n\ni /τ ) where αi is the normalized attention score of the i-th prompt, pI ∈ Rm×d is generated queryconditional prompt, and τ is the pre-defined temperature. To further improve the effectiveness of our proposed method, we transform the generated prompt into prefix (Liu et al., 2021a) that owns the better representational ability. Although one can simply employ up projection to generate prefix for each layer of retriever, we find this approach considerably increases the number of total\n\ni\n\n(2)\n\n3\n\n减小JS距离Attention ModuleQuery RepresentationInstanceEncoderQuery EncoderTransformer LayerTransformer LayerTransformer LayerQueryPoolingSample 1Task ΤSample 2Task ΤSample 3Task Τ!+Contrastive Prompt RegularizationSharedPromptsValueKeyQuery-conditional prompt (pI)Prefix EncoderPrefix-L1Prefix-L2Prefix-LNAlignUniform-Input queryJSJSDialogueFact CheckingOpen-QASearchSampling...Entity LinkingPublished as a conference paper at ICLR 2023\n\nparameters which may cause the degradation of the generalization. Therefore, we employ a parameterefficient transformation (Stickland & Murray, 2019; Houlsby et al., 2019) as the prefix encoder θp = {Wdp, Wup} to generate the prefix which can be described as follow:\n\nh = WdppI , PL = WupTanh(h) (3) where Wdp ∈ Rdp×d and Wup ∈ RLd×dp are projection matrices, dp is down projection dimension of pI , L is the number of layers of query encoder θq, Tanh is the hyperbolic tangent function, h is the intermediary low-dimension representation and PL ∈ RLd is the dynamically generated parameters that can be split into prefixes for each layer of the neural retriever.\n\nContrastive Prompt Regularization Although the above mechanisms can generate queryconditional prefixes and share parameters across different tasks, we find it results in the so-called mode collapse problem of attention score distributions. Specifically, the attention score distributions of different queries are very similar which may cause our module degenerating to a prefix-tuning. Moreover, we expect queries belonging to the same task generate similar attention scores distribution while queries belonging to different tasks own dissimilar attention scores distribution. To simultaneously learn representations of hyper-prompts and clustering for different types of queries across different domains or tasks, we propose the Contrastive Prompt Regularization (CPR) that employs soft constraint to cluster the attention scores implicitly and thus avoid the mode collapse problem. CPR can be formally described as follows. (cid:88)\n\n(cid:16) (cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:17)\n\nDf\n\n(cid:0)A(qi), A(qj)(cid:1)\n\n−\n\nDf\n\n(cid:0)A(qi), A(qk)(cid:1)\n\n.\n\nLCPR(C) =\n\nB∈C\n\nqi∈B\n\nqj ∈B,Iqi =Iqj (cid:124)\n\n(cid:123)(cid:122) alignment\n\nqk∈B,Iqi ̸=Iqj (cid:124)\n\n(cid:125)\n\n(cid:123)(cid:122) uniformity\n\n(cid:125)\n\n(4)\n\nHere, B is a mini-batch of training samples randomly selected from C, A(q∗) ∼ P (z) = αz, z ∈ {1, 2, · · · , N } is the attention score distribution of the input query q∗, Iq∗ is an indicator function that represents the dataset of a task that the query q∗ belonging to, Df is a divergence that measures the similarity of two distributions. Inspired by contrastive learning (Wang & Isola, 2020), the first term (cid:0)A(qi), A(qj)(cid:1) can be viewed as alignment regularization that encourages similar that contains Df (cid:0)A(qi), A(qk)(cid:1) cab be queries generated by similar prefixes, and the second term that contains Df viewed as uniformity regularization that prevents mode collapse of distributions of attention score. In our implementation, we use Jensen-Shannon divergence (Manning & Sch ̈utze, 2002) since it owns certain upper and lower bounds which avoids the numeric overflow in optimizing.\n\n2.2 UNIFORM RETRIEVAL WITH QUERY-CONDITIONAL PROMPT\n\nLexicon-weighted Retriever HYPER is compatible with any deep neural text retriever based on Transformer (Vaswani et al., 2017) architecture, however, we find the lexicon-weighted retrieval method is more promising to attain better zero-shot retrieval2. Therefore, we adopt a lexicon-weight language model SPLADE (Formal et al., 2021b) LM(·, ·; θq) as our backbone network. Combining with the generated dynamic prefix, we can represent a text of query as follows:\n\nvq = MaxPooling(log (1 + ReLU(LM(PL, X; θq)))) ∈ Rd. where ReLU is the Rectified Linear Unit. Following the common practice, we adopt the contrastive loss (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Formal et al., 2021b) that utilize a limited number of positive documents d+ and d− for each queries for training. Specifically, we employ BM253 to retrieve top-100 relevant documents as d− as negative samples except those also contain answers to a query. To encode positive documents and negative documents into corresponding vector representation vd+ and vd−, we employ a document encoder θd to encode them but skip the prefix generation as vd = MaxPooling(log (1 + ReLU(LM(d; θd)))) ∈ Rd.4 Thus, a likelihood distribution can be formatted as follows,\n\n(5)\n\nP (d+|vq, d−; θq, θd) =\n\n(cid:80)\n\nexp(vT\n\nq vd+) exp (vT\n\nq vd′)\n\nd′∈d+∪d−\n\n.\n\n(6)\n\n2We also report evaluation results using the dense model (e.g., DPR) as the backbone network in Table 1. 3We adopt the default setting of Anserini for BM25 where k1 = 0.9, b = 0.4. 4Using dynamic representations of different documents requires to rebuild index in real-time which causes\n\nheavy calculation cost.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFollowing the common practice of training lexical retriever (Formal et al., 2021b), we add additional floating point operations per second (FLOPS) regularization terms (Paria et al., 2020) to reduce the computation cost of the retrieval process and improve the retrieval effectiveness. Then, the loss function of our methods can be defined as\n\nLq(C) =\n\n(cid:88)\n\nq∈C\n\nP (d+|vq, d−; θq, θd) + λqFLOPS(vq) + λdFLOPS(vd),\n\n(7)\n\nwhere FLOPS(·) is a regularization term proposed by (Formal et al., 2021b) and we use hyperparameters λq, λd to adjust the sparsity of representation of vq and vd, respectively.\n\nModel Training In training, we adopt a multi-tasking training paradigm with a mini-batch mixup which means that we randomly sample from all tasks to compose a training batch. We train the entire model on KILT for in-domain testing for superior performance. For cross-domain zero-shot retrieval, we freeze the parameters of the backbone network and only tune the parameters of our proposed components. The objective function we used can be described as:\n\nLq(C) + λcLCPR(C),\n\nmin P,θI ,θp, (θq,θd)\n\n(8)\n\nwhere C = (cid:83)t the regularization of CPR.\n\ni=1 Ti is the mixed data of different tasks, λc is a fixed pre-defined weight to control\n\n3 EXPERIMENTS\n\nBenchmark Datasets We use two publicly available retrieval datasets for evaluation, including KILT (Petroni et al., 2021) and BEIR (Thakur et al., 2021). KILT is a benchmark for knowledgeintensive tasks that require retrieving additional knowledge from the wiki, we select all 7 datasets containing corresponding training samples to train our model in a multi-task setting. Specifically, we use a variety of tasks including fact-checking (FEVER), entity linking (AY2), slot filling (zsRE), question answering (NQ, TQA, HoPo), and dialogue (WoW). KILT provides the provenances of all tasks in one wiki corpus, which enables us to train models with a share passage encoder (Maillard et al., 2021). For BEIR, it is a widely known zero-shot information retrieval benchmark and we employ it to evaluate the transfer learning ability provided by different methods. Also, we remove the datasets that are contained in KILT which results in 10 tasks including TREC-COVID (TC), NFCorpus (NFC), ArguAna (Argu), T ́ouche-2020 (Touche), FiQA-2018 (FiQA), DBPedia (DBP), SCIDOCS (SciD), Climate-FEVER (CFever) for a fair comparison.\n\nEvaluation Metrics When evaluation on KILT, we adopt R-Precision as the retrieval metric which is the primary metric used in their paper (Petroni et al., 2021). R-Precision can be calculated as r R , where R is the number of Wikipedia pages inside each provenance set and r is the number of relevant pages among the top-R retrieved pages. For experiments on BEIR, Normalised Cumulative Discount Gain (nDCG) (Gysel & de Rijke, 2018) is reported to represent performances of different methods. Specifically, we utilize the official TREC evaluation tool (Gysel & de Rijke, 2018) and compute nDCG@10 for all datasets.\n\nExperiment Setups We train our model on KILT in a multi-task learning paradigm. Since our method can be combined with any transformer-based neural retriever, we adopt SPLADEv2 (Formal et al., 2021b) and DRP (Karpukhin et al., 2020) provided by the original paper as our backbone model5. The learning rate of the backbone network and the modules of HYPER is set to 2 × 10−5 by following Formal et al. (2021b) and 1 × 10−3 selected from {10−1, 10−2, 10−3}, respectively. The temperature τ is set to e, λq is set to 0.3, λd is set to 0.1. dq is set to 400 and dp is set to 100. The number of train epochs is set up to 10 epochs, both max document length and max query length are set to 512 to fit the task with a very long query, and the batch size is set to 256. For each query, we provide 1 positive sample and 19 negative samples for training.We set the sequence length of each basic prompt to 100 selected from {10, 50, 100}. The λc and the number of shared basic prompts N\n\n5Both models are pre-trained on MS-MARCO (Nguyen et al., 2016) and can provide superior initial\n\nperformance on KILT.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Page-level R-precision on KILT. All models in the multi-tasking part are trained on 7 tasks of KILT, while the models in the zero-shot part are trained with the leave-one-out setting that leaves out the dataset used for testing in training. Model names that end with ”zero” mean they are tested directly without training and the ”-prefix” follows the model name means the corresponding model is trained through prefix-tuning. * indicates results from (Maillard et al., 2021).\n\nModel\n\nFEVER AY2\n\nzsRE NQ\n\nHoPo TQA WoW AVG\n\nDPR* (Karpukhin et al., 2020) DPR-prefix DPR-HYPER\n\nSPLADE (Formal et al., 2021a) SPLADE-prefix SPLADE-HYPER\n\nBM25*\n\nDPR-zero (Karpukhin et al., 2020) DPR (Karpukhin et al., 2020) DPR-prefix DPR-HYPER\n\nSPLADE-zero (Formal et al., 2021a) SPLADE (Formal et al., 2021a) SPLADE-prefix SPLADE-HYPER\n\nMulti-Tasking\n\n75.35 75.71 76.35\n\n81.23 81.56 82.17\n\n50.13\n\n54.66 54.75 56.49 59.13\n\n72.05 73.42 75.12 78.39\n\n28.45 30.32 31.80\n\n34.29 35.83 38.52\n\n81.49 81.24 81.79\n\n84.59 83.98 84.23\n\n58.53 59.42 60.83\n\n58.17 59.25 60.41\n\n41.95 42.12 42.67\n\n50.29 50.63 51.24\n\n60.39 61.67 62.91\n\n60.30 61.34 62.76\n\n43.52 46.16 46.35\n\n47.38 50.13 49.84\n\n55.66 56.65 57.53\n\n59.46 60.38 61.31\n\nZero-Shot\n\n3.47\n\n2.03 1.62 2.65 3.56\n\n2.90 2.32 3.21 5.08\n\n66.43\n\n25.83\n\n43.95\n\n29.44\n\n27.50\n\n35.25\n\n34.69 36.65 36.31 37.03\n\n76.77 80.75 81.24 82.16\n\n53.22 51.62 51.53 53.31\n\n49.66 47.78 48.36 50.02\n\n21.95 13.14 18.34 19.13\n\n47.65 29.96 40.25 41.38\n\n45.01 48.25 50.27 52.24\n\n48.12 51.63 54.42 56.28\n\n27.07 24.24 27.06 29.06\n\n43.35 38.73 43.55 46.81\n\n34.08 32.90 34.66 36.21\n\n48.64 46.37 49.45 51.44\n\nare tuned and we finally select 0.1 and 20 respectively6. We fix the random seed always to 42 and all experiments are conducted on eight A100 GPUs. 7\n\n3.1 MAIN EVALUATION\n\nSupervised and Zero-shot Performance on KILT We conduct both supervised and zero-shot experiments on KILT and the results are shown in Table 1. Since our hyper-prompted training mechanism is applied to the lexicon-weighted retrieval method (e.g., SPLADE), we name it as SPLADE-HYPER. Besides, we also test the effectiveness of our HYPER on dense retrieval methods (e.g., DPR) which results in DPR-HYPER. First, we can easily find that SPLADE can provide superior performance than dense representation methods (e.g., DPR) in terms of both the supervised and zeroshot settings on KILT. Notably, the performance gap is extremely significant in the zero-shot setting where dense retrieval methods (a.k.a., DPR) can only achieve comparable results with traditional BM258 while lexicon-weighted retrieval methods significant outperform BM25 and dense retrieval methods. Second, compared with fine-tuning model entirely or prefix-tuning (Liu et al., 2021b), our SPLADE-HYPER can obtain better performance on most tasks of the supervised setting, which demonstrates the superiority of our HYPER in sharing and transferring knowledge across different retrieval tasks or domains. Meanwhile, we can notice that HYPER can also improve performance even in all unseen tasks, which reflects our method can effectively transfer learned knowledge from previous tasks and adapt to different types of queries across different domains. The experimental results of the few-shot setting (shown in Appedix A) can also further prove the effectiveness of the proposed HYPER.\n\nZero-shot Performance on BEIR We also directly test the performance of our SPLADE-HYPER without tuning on BEIR, which is a widely known zero-shot IR benchmark. Following the most recent works (Xu et al., 2022; Wang et al., 2022), we compare our methods with varieties of methods, including DocT5 (Nogueira et al., 2019a), ColBERT (Khattab & Zaharia, 2020), DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2020), GenQ (Thakur et al., 2021), TAS-B (Reimers & Gurevych, 2019), MoDIR (Xin et al., 2021) and LapraDOR (Xu et al., 2022). Experiment results are shown in Table 2, as we can see, our HYPER occupies the best performance on 4 of 9 tasks of BEIR, and we also attain the best average performance. Besides, our method consistently outperforms the backbone\n\n6The effects of different hyperparameters are investigated in section 3.2 7Our Code is available at https://github.com/oklen/HypeR. 8The observation is consistent with several previous studies (Maillard et al., 2021; Thakur et al., 2021)\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Zero-shot generalization evaluated on 9 datasets of BEIR. * indicates results from Thakur et al. (2021). † indicates results from Xu et al. (2022).\n\nModel\n\nBM25* BM25+CE* DocT5* (Nogueira et al., 2019a) ColBERT† (Khattab & Zaharia, 2020) DPR† (Karpukhin et al., 2020) ANCE† (Xiong et al., 2020) GenQ† (Thakur et al., 2021) TAS-B† (Reimers & Gurevych, 2019) MoDIR (Xin et al., 2021) LapraDOR† (Xu et al., 2022)\n\nSPLADE SPLADE-Prefix SPLADE-HYPER\n\nTC\n\n65.6 75.7 71.3 67.7 33.2 65.4 61.9 48.1 67.6 72.8\n\n57.4 73.0 79.1\n\nNFC FiQA Argu Touche DBP\n\nSciD CFever\n\nSciF AVG\n\n32.5 35.0 32.8 30.5 18.9 23.7 31.9 31.9 24.4 34.6\n\n42.3 32.9 33.6\n\n23.6 34.7 29.1 31.7 11.2 29.5 30.8 30.0 29.6 31.7\n\n23.0 34.7 35.1\n\n31.5 31.1 34.9 23.2 17.5 41.5 49.3 42.9 41.8 50.7\n\n26.6 49.3 50.1\n\n36.7 27.1 34.7 20.2 13.1 24.0 18.2 16.2 31.5 32.2\n\n21.3 25.2 27.4\n\n31.3 40.9 33.1 39.2 26.3 28.1 32.8 38.4 28.4 36.1\n\n38.1 41.8 43.6\n\n15.8 16.6 16.2 14.5 7.7 12.2 14.3 14.9 12.4 18.5\n\n13.9 15.3 15.6\n\n21.3 25.3 20.1 18.4 14.8 19.8 17.5 22.8 20.6 22.8\n\n23.6 22.3 23.7\n\n66.5 68.8 67.5 67.1 31.8 50.7 64.4 64.3 50.2 69.7\n\n64.6 70.5 70.4\n\n38.8 39.5 37.3 34.7 19.4 32.8 35.7 34.4 34.1 41.0\n\n34.5 40.6 42.1\n\nTable 3: Ablation study on KILT.\n\nModel\n\nSPLADE-HYPER\n\nSPLADE-HYPER w/o query-conditional SPLADE-HYPER w/o alignment Df SPLADE-HYPER w/o uniformity Df SPLADE-prefix\n\nFEVER AY2\n\nzsRE NQ\n\nHoPo TQA WoW AVG\n\n82.17\n\n80.61 80.82 81.39 81.56\n\n38.52\n\n84.23\n\n60.41\n\n51.24\n\n62.76\n\n49.84\n\n61.31\n\n36.65 37.47 36.34 35.83\n\n83.24 83.97 83.67 83.98\n\n59.07 59.18 59.42 59.25\n\n50.60 50.19 50.53 50.63\n\n61.37 61.54 61.52 61.34\n\n49.48 50.57 50.95 50.13\n\n60.15 60.53 60.55 60.38\n\nnetwork SPLADE, which indicates HYPER can enable a model to transfer learned knowledge across different domains better and thus improve the generalization ability of models. Moreover, our SPLADE-HYPER is better than SPLADE-prefix, which shows the superiority of proposed queryconditional prompt synthesizing and the strong ability of the dynamic parameterization to adapt different types of queries across the different domains.\n\n3.2 ANALYSES\n\nAblation Study To verify the effectiveness of our proposed mechanisms, we propose several variants of our model for comparison. Specifically, to evaluate the effectiveness of QPS, we replace the generated attention score distribution with a fixed uniform distribution which results in a variant of our model without the attention module. Also, to verify the effectiveness of CPR, we separately remove the alignment regularization and uniformity regularization to constitute the other two variants. Experiment results are shown in Table 3, as we can see, removing any modules in our mechanisms cause a significant decrease in performance. Comparison between HYPER and HYPER w/o query conditional indicates the attention module in QPS can successfully generate suitable prompts for different queries and thus improve the performance. Moreover, we can observe removing either regularization terms of CPR results in degraded performances. Therefore, we can conclude that both alignment regularization and uniformity are crucial for enabling the effective query-conditional prompt generation to process different queries.\n\nPrompt Attention Similarity vs Task Similarity We conduct additional experiments to investigate whether the similarities of prompt attention distributions can reflect the similarities between different tasks. Specifically, we calculate the similarities between the mean values of attention score distributions A belonging to the same task, and the results are shown in Figure 2. Obviously, there are two different groups of similar attention score distribution, AY2 and WoW in the top-left corner and others in the bottom-right, which are bounded by green lines. After reviewing the data, we find the two groups of datasets can be distinguished by the lengths of input queries. In particular, the lengths of queries of AY2 and WoW are usually composed of multiple sentences while queries of other datasets only contain one or few sentences. This implies our mechanism can recognize the different lengths of different queries, which is a prerequisite for dynamically adopting\n\nFigure 2: Similarities of attention scores distributions of different tasks.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Visualization of attention weight embeddings for different tasks.\n\ndifferent methods to process different queries, such as extracting important information from long queries or predicting implicit relations in short queries. Moreover, we take a closer look and observe the sub-groups of attention score distributions annotated by blue lines. Comparing the queries of these two groups, we find the group composed of TQA, HoPo, and FEVER requires more inference skills than the group composed of zsRE and NQ. This implies our query-conditional module can even distinguish more subtle differences in queries, which qualitatively reflects the effectiveness of QPS.\n\nVisualization of Prompt Attention Distribution To further confirm that CPR works as our expectation that prompts the distributions of attention scores to be uniform and aligned, we employ dimension reduction such as t-SNE to visualize them. To this end, we randomly draw the 2000 samples from the test split of each dataset and then employ t-SNE to transform normalized prompt attention (a.k.a, A) into 2D vectors. Experiment results are shown in Figure 3. The comparison between w/ CPR and w/o CPR indicates that (1) uniformity regularization isolates attention score distribution of dissimilar queries belonging to different tasks, which helps avoid mode collapse. (2) alignment regularization prompts the attention score distributions of similar queries belonging to the same tasks to become closer to each other, which may improve the effectiveness of training prefix merged by specific pattern attention scores and thus benefits the model performance.\n\nPerformance vs Efficiency We further investigate the efficiency of our method since real-world applications not only require better retrieval results but also low retrieval latency. To measure the latency, we adopt the Query Per Second (QPS) as a metric and the higher QPS means more queries can be processed in time. We evaluate both dense retrievers and sparse retrievers with or without HYPER on KILT and experiment results are shown in Figure 4. As we can see, compared with backbone models, HYPER can obtain better performance and higher QPS which demonstrates the better efficiency of our method. Meanwhile, although BM25 obtains better efficacy, neural methods outperform it on the retrieval metric significantly.\n\nFigure 4: Performance versus QPS (latency).\n\nFigure 5: Effect of the number of shared basic prompts (N ) and weights of CPR (λc). The two figures on the left vary N , while the two on the right vary λc.\n\nImpact of Parameters To better understand the effect of our method, we change the number of basic prompts (N ) and the weight of CPR (λc) and test these variants on both KILT and BEIR. Specifically, we vary the number of shared prompts in {2, 5, 10, 20, 30, 50} with the weight of CPR fixed to 0.1. Meanwhile, we vary the weights of CPR in {0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.5} with number of shared prompt is fixed to 20. The average scores of different variants are shown in Figure 5. As the number of shared prompts varies across different values, we can observe the performance increase and then decrease. This phenomenon implies more shared prompts can enable the model to learn more patterns exits in data while too many shared prompts suffered from the sparsity of representation space and thus results in insufficiently trained combinations of\n\n8\n\n1101001000QPS on CPU Machine40506070Page-level R-precision on KILTSPLADESPLADE-HypeRDPRDPR-HypeRBM2502040N38404244Avg ScoresDatasetBEIR02040N49505152DatasetKILT103102101c38404244DatasetBEIR103102101c49505152DatasetKILTPublished as a conference paper at ICLR 2023\n\nshared prompts. Similarly, we also find a moderate weight of CPR can lead to the best performance. This indicates our proposed CPR can benefit the query conditional module and thus improve the performance while too large λc causes the main objective loss Lq to be ignored.\n\n4 RELATED WORK\n\nInformation retrieval can be generally defined as searching relevant documents about a short text as an input query. The major challenge of IR comes from the huge amount of candidate documents, which results in the matching function between query and documents having to be extremely simple for high efficacy. Therefore, we need to seek superior encoding methods for queries and documents to improve the accuracy of retrieval. Traditional methods such as tf-idf and BM25 rely on term matching and build high-dimension, sparse vector (Yang et al., 2017; Robertson & Zaragoza, 2009) for effective retrieval. Although they are effective across various tasks of different domains (Chen et al., 2017; Yang et al., 2017), they fail to adapt to more specific tasks and are outperformed largely by the recently popular neural text retriever with sufficient training samples.\n\nNeural text retrievers are based on pre-trained language models such as BERT (Devlin et al., 2019) and can be classified into two types, the dense-vector retrievers (Xiong et al., 2021) and sparse-vector retrievers. For dense-vector retriever (Karpukhin et al., 2020; Gao & Callan, 2021), it encodes both queries and documents into low-dimension vectors and then calculates the relevance scores between them. Although a dense vector is more effective to conduct semantic retrieve (Lin & Lin, 2022), the compressing process of texts may result in a lost of information. In the contrast, sparse-vector retriever (Formal et al., 2021b; Khattab & Zaharia, 2020) encodes both queries and documents into high-dimension, sparse vectors and then calculate the concurrence (Nogueira et al., 2019a) or top coordinate terms (Formal et al., 2021b) of words. Therefore, it can achieve effective lexicon matching, and the varying amount of activating dimensions in vectors relieves the information lost in encoding, while it introduces an additional quantization process to avoid the unstable of real-values of vectors.\n\nAlthough neural retrievers can perform well with a moderate amount of data, in a real-world application, the data of target tasks could be considerably scarce (Thakur et al., 2021). Hence, zero-shot and few-shot settings on retrieval tasks receive more and more attention and various methods have been proposed to improve the model performance under this setting including unsupervised pretrained (Xu et al., 2022; Wang et al., 2022), data augmentation (Nogueira et al., 2019b; Thakur et al., 2021; Dai et al., 2022) and enhanced training (Reimers & Gurevych, 2019; Xiong et al., 2020). However, there is only a primary investigation of methods utilizing transfer learning, and still a large room for improvement. Technically, HYPER is similar to utilizing prefix tuning for IR tasks as Tam et al. (2022) and discrete prompt tuning for natural language tasks as Sanh et al. (2022), but goes beyond the comparison of existing mechanisms and focuses on generating dynamically query-conditional prompts, and enabling a neural retriever to process queries of different tasks uniformly. Besides, HYPER is inspired by HyperPrompt (He et al., 2022) that explores prompt-based task-conditioning of self-attention in Transformers. Nonetheless, HYPER dynamically generates the prompt according to every query itself rather than indispensably relying on the task-level information, which enables our model to obtain superior generalization and transferability. Moreover, HYPER employs the prefix-tuning method to utilize the dynamic prompts rather than concatenating prompts into the self-attention layer directly.\n\n5 CONCLUSION\n\nIn this paper, we propose to process queries of different tasks uniformly regardless of the difference in query format and varying priorities of query vectors. Specifically, we present HYPER, a hyper-prompted training mechanism that can be easily combined with any transformer-based neural retrievers. In HYPER, to enable the uniform process queries, we propose Query-conditional Prompt Synthesizing (QPS) to dynamically synthesize different parameter combinations for different queries. Moreover, to resolve the mode collapse problem of attention scores distribution in QPS, we propose Contrastive Prompt Regularization (CPR) to simultaneously learn representations of hyper-prompts and clustering for different types of queries across different domains or tasks. We conduct extensive experiments which demonstrate our methods can improve the in-domain and out-of-domain zero-shot retrieval performance of a neural retriever significantly. In-depth analyses reveal how our mechanism enables the uniform processing of queries.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nTiago Almeida and S ́ergio Matos. Frugal neural reranking: evaluation on the covid-19 literature. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online, December 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.nlpcovid19-2. 3. URL https://aclanthology.org/2020.nlpcovid19-2.3.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nYinqiong Cai, Yixing Fan, Jiafeng Guo, Fei Sun, Ruqing Zhang, and Xueqi Cheng. Semantic models for the first-stage retrieval: A comprehensive review. CoRR, abs/2103.04831, 2021. URL https://arxiv.org/abs/2103.04831.\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. CoRR, abs/1704.00051, 2017. URL http://arxiv.org/abs/1704. 00051.\n\nZhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. ArXiv, abs/2209.11755, 2022.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. CoRR, abs/1811.01241, 2018. URL http://arxiv.org/abs/1811.01241.\n\nThibault Formal, Carlos Lassance, Benjamin Piwowarski, and St ́ephane Clinchant. SPLADE v2: Sparse lexical and expansion model for information retrieval. CoRR, abs/2109.10086, 2021a. URL https://arxiv.org/abs/2109.10086.\n\nThibault Formal, Carlos Lassance, Benjamin Piwowarski, and St ́ephane Clinchant. Splade v2: Sparse lexical and expansion model for information retrieval, 2021b. URL https://arxiv.org/ abs/2109.10086.\n\nLuyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval. CoRR, abs/2108.05540, 2021. URL https://arxiv.org/abs/2108. 05540.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrieval-\n\naugmented language model pre-training. CoRR, abs/2002.08909, 2020.\n\nChristophe Van Gysel and Maarten de Rijke. Pytrec eval: An extremely fast python interface to trec eval. CoRR, abs/1805.01597, 2018. URL http://arxiv.org/abs/1805.01597.\n\nYun He, Huaixiu Steven Zheng, Yi Tay, Jai Prakash Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. Hyperprompt: Promptbased task-conditioning of transformers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv ́ari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 8678–8690. PMLR, 2022. URL https://proceedings. mlr.press/v162/he22f.html.\n\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606. 08415.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn1, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 6769–6781, 2020.\n\nOmar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over BERT. In Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (eds.), Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pp. 39–48, 2020.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.\n\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ̈uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ̈aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. CoRR, abs/2005.11401, 2020. URL https://arxiv.org/abs/2005.11401.\n\nSheng-Chieh Lin and Jimmy Lin. A dense representation framework for lexical and semantic\n\nmatching, 2022. URL https://arxiv.org/abs/2206.09912.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602, 2021a. URL https://arxiv.org/abs/2110.07602.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602, 2021b. URL https://arxiv.org/abs/2110.07602.\n\nJean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas Oguz, Veselin Stoyanov, and Gargi Ghosh. Multi-task retrieval for knowledge-intensive tasks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 1098–1111. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.89. URL https://doi.org/10.18653/v1/2021.acl-long.89.\n\nChristopher D. Manning and Hinrich Sch ̈utze. Foundations of statistical natural language processing.\n\nIn SGMD, 2002.\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and\n\nLi Deng. Ms marco: A human-generated machine reading comprehension dataset. 2016.\n\nRodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online preprint,\n\n2019a.\n\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query\n\nprediction. CoRR, abs/1904.08375, 2019b.\n\nBiswajit Paria, Chih-Kuan Yeh, Ian En-Hsu Yen, Ning Xu, Pradeep Ravikumar, and Barnab ́as P ́oczos. Minimizing flops to learn efficient sparse representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SygpC6Ntvr.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt ̈aschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2523–2544, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https: //aclanthology.org/2021.naacl-main.200.\n\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\n\nCoRR, abs/1908.10084, 2019. URL http://arxiv.org/abs/1908.10084.\n\nStephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, 2009. doi: 10.1561/1500000019. URL https: //doi.org/10.1561/1500000019.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F ́evry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\n\nAsa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efficient adaptation in multi-task learning. In International Conference on Machine Learning, pp. 5986– 5995. PMLR, 2019.\n\nWeng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, and Jie Tang. Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers, 2022. URL https://arxiv.org/abs/2207.07087.\n\nNandan Thakur, Nils Reimers, Andreas R ̈uckl ́e, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR, abs/2104.08663, 2021. URL https://arxiv.org/abs/2104.08663.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval, 2022. URL https://arxiv.org/abs/2207.02578.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\n\nment and uniformity on the hypersphere. ArXiv, abs/2005.10242, 2020.\n\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, and Paul N. Bennett. Zero-shot dense retrieval with momentum adversarial domain invariant representations. CoRR, abs/2110.07581, 2021. URL https://arxiv.org/abs/2110.07581.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. CoRR, abs/2007.00808, 2020.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=zeFrfgyZln.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense\n\nretriever for zero-shot text retrieval. ArXiv, abs/2203.06169, 2022.\n\nPeilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the use of lucene for information retrieval research. In Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W. White (eds.), Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, pp. 1253–1256, 2017.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 2369–2380, 2018.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA FEW-SHOT EVALUATION ON KILT\n\nIn Table 4, we continually train models in a new task with a few-shot setting. As we can see, our methods consistently outperform several strong baselines, which demonstrates that our method benefits the model more quickly adapting to the different tasks in the same domain by sharing knowledge among tasks.\n\nTable 4: Page-level R-precision on KILT in the few-shot setting.\n\nTarget Data\n\n#Instance\n\n192\n\n384\n\nSPLADE SPLADE-prefix\n\n48.19 49.07 SPLADE-HYPER 50.27\n\n48.32 49.40 50.24\n\nTarget Data\n\nNQ\n\n576\n\n48.55 50.10 51.13\n\nFEVER\n\nTQA\n\n1152\n\n2304\n\n192\n\n384\n\n576\n\n1152\n\n2304\n\n48.97 50.51 51.43\n\n49.56 50.42 51.96\n\n51.97 54.62 56.56\n\n51.32 54.01 55.81\n\n53.53 55.48 57.68\n\n53.18 56.23 58.24\n\n51.66 53.37 55.07\n\nzsRE\n\n#Instance\n\n192\n\n384\n\n576\n\n1152\n\n2304\n\n192\n\n384\n\n576\n\n1152\n\n2304\n\nSPLADE SPLADE-prefix\n\n73.42 75.12 SPLADE-HYPER 78.37\n\n73.61 75.19 78.44\n\n73.75 74.86 77.94\n\n74.01 75.74 78.55\n\n74.72 76.31 79.08\n\n80.83 81.30 82.19\n\n81.13 81.72 82.58\n\n81.24 81.65 82.63\n\n81.45 82.09 82.69\n\n81.78 82.36 83.44\n\nB COMPARISON OF THE TASK-SPECIFIC FINE-TUNING MODEL\n\nWe also test the performance of directly fine-tuning the SPLADE on each task of KILT, which results in 7 different retrievers (SPLADE-FT). The results are shown in Table 5. We can find that SPLADE-FT can achieve a significantly better AVG score than SPLADE-MT, although multi-task training can bring improvement to 4 out of 7 tasks (a.k.a. FEVER, zsRE, NQ, and TQA). Besides, through incorporating the proposed HYPER, SPLADE-HypeR can achieve a comparable AVG score with SPLADE-FT and even outperform SPLADE-FT on 4 out of 7 tasks (a.k.a. FEVER, zsRE, NQ, TQA, and WoW). Notably, SPLADE-FT train separate models for each task which results in roughly 6 times more parameters than our model.\n\nTable 5: Comparison of the task-specific fine-tuning model on KILT. SPLADE-prefix means the model trained through prefix-tuning.\n\nModel\n\nFEVER AY2\n\nzsRE NQ\n\nHoPo TQA WoW AVG\n\nSPLADE-FT\n\n80.20\n\n55.02\n\n83.97\n\n57.00\n\n51.29\n\n55.40\n\n48.69\n\n61.65\n\n81.23 SPLADE-MT SPLADE-prefix 81.56 SPLADE-HYPER 82.17\n\n34.29 35.83 38.52\n\n84.59 83.98 84.23\n\n58.17 59.25 60.41\n\n50.29 50.63 51.24\n\n60.30 61.34 62.76\n\n47.38 50.13 49.84\n\n59.46 60.38 61.31\n\nC EFFICIENCY OF QUERY ENCODING\n\nWe further investigate how the length of prefixes influences the computation cost of query encoding and retrieval performance. To this end, we vary the size of the prefix in {5,10,20,50,100, 200} and record both the average time of encoding a query and retrieval performance on KILT. We compare our method with Prefix-tuning and the results are shown in Figure 6. The inference time is measured on a machine with Intel Xeon CPU E5-2678. As we can see, our HypeR obtains better average retrieval scores than Prefix-Tuning for all different prefix lengths, and both Prefix-Tuning and HypeR achieve the best performance when the prefix length is 100. Notably, our HypeR costs 4% and 25% more encoding time than Prefix-Tuning when the prefix length is 10 and 100, respectively. We have put the results as a figure in the appendix of our revised manuscript. Thank you again for your constructive suggestions.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Efficiency compraing between HypeR and Prefix-Tuning. The multi-tasking fine-tuning can be viewed as an origin.\n\nD CASE STUDY\n\nTo provide an intuitive understanding of our methods, we show some similar and dissimilar queries based on prompt attention distributions in Table 6. Specifically, we randomly sample 3 queries belonging to different tasks as base queries from the collection of test data of all KILT datasets. Then, for each base query, we randomly sample 2 queries from the queries with top 5% similar attention scores. Besides, we also randomly sample 2 queries belonging to different datasets from the queries with the lowest 5% similar attention score, since we find there are too many trivial candidates from the same datasets.\n\nTable 6: Queries with similar and dissimilar prompt attention distributions.\n\nQuery Type\n\nSource Task Query Content\n\nCase 1\n\nBase Query\n\nHotpotqa\n\nWhich film was made more recently, The Diplomat or Rien que les heures?\n\nSimilar Query 1\n\nHotpotqa\n\nWhich movie was released more recently, Waking Sleeping Beauty or Mars Needs Moms?\n\nSimilar Query 2\n\nHotpotqa\n\nWho directed a film more recently, Don Bluth or Raoul Walsh?\n\nDissimilar Query 1\n\nAidayago2\n\nDissimilar Query 2\n\nWoW\n\nBase Query\n\nWoW\n\nAction Performance to acquire firms . [START ENT] TEMPE [END ENT], Ariz. 1996-12-06 Action Performance Cos Inc said Friday it has agreed to acquire Motorsport Traditions Ltd and Creative Marketing & Promotions Inc for about $13 million in cash and stock. The two firms to be acquired have about $25 million in annual revenues from the design, manufacture and sale and distribution of licensed motorsports products. The deal is expected to close by the end of the year subject to due diligence and other customary closing conditions.\n\nI love American Football, the first game was played between Rutgers and Princeton in 1869. Wow I had no idea it was that recently played! Yep, it can actually be traced to early versions of rugby football also. Who is your favorite team. Probably the New England PAtriots. I always had a thing for Tom Brady lol. Did he not deflat footballs or something one year ?\n\nCase 2\n\nDo you shop online for clothes? Yes quite often. It allows me to directly buy goods from a seller over the internet without having to leave the comfort of my home. What about you? Yeah same I love visiting websites of different retailers directly to see product availability and the best prices. Yeah me too. I use amazon a lot to buy stuff. Like even this computer!\n\n15\n\n5102050100200Prefix Length59.059.560.060.561.061.5Avg ScoreHypeR_ScorePrefix_ScoreHypeR_TimePrefix_Time0.0250.0500.0750.1000.1250.150Time0.0250.0500.0750.1000.1250.150TimePublished as a conference paper at ICLR 2023\n\nSimilar Query 1\n\nWoW\n\nSimilar Query 2\n\nWoW\n\nOnline shopping is the better experience to choose the the product on our desire.Yes! I much prefer shopping online at home on the Internet rather than fighting crowds in an actual store. what did you often purchase? Anything and everything! Housewares, clothing, electronics, you name it! I like the convenience of browsing stores on my laptop, tablet computer and smartphone! yes, it includes lot of choices I especially like the functionality of websites like Amazon for shopping. The have a great search feature that allows me to find specific models, brands and items. Such website, improved a lot for customer convenience.\n\nI like to shop online, probably a bit too much. Lol me too! What’s your favorite online store? I love to shop on amazon. So you shop online a lot too? Where do you shop? Amazon as well. I also shop a lot online at Old Navy and Gap. They have great sales. Do they really? I mainly use only amazon. Do you use Amazon Kindle, the series of e-readers designed and marketed by Amazon. No I have never used a kindle. My dad has one of those though and it looks cool if you love books.\n\nDissimilar Query 1\n\nFEVER\n\nHrithik Roshan was a film star.\n\nDissimilar Query 2\n\nHotpotqa\n\nWhen did the theatre open that has Valery Abisalovich Gergiev as it’s artistic director?\n\nBase Query\n\nSimilar Query 1\n\nSimilar Query 2\n\nFEVER\n\nFEVER\n\nFEVER\n\nDissimilar Query 1\n\nWoW\n\nDissimilar Query 2\n\nAidayago2\n\nCase 3\n\nReliance Industries only works in textiles.\n\nSanjjanaa works in the Telegu film industry.\n\nRakul Preet Singh mostly works in the Telugu film industry.\n\nHave you ever read anything by John Grisham? Yes, I have read his very first novel ”A Time to Kill” which was published in June 1989 after he took four years to write it! I haven ́t read anything by him but I remember the movies for both a time to kill and the firm.\n\nMultinational commander going back to east Zaire. Jonathan Wright NAIROBI 1996-12-06 The Canadian general in charge of a multinational force for eastern Zaire said on Friday he was going back to Zaire for more information about the plight of about 165,000 Rwandan refugees adrift in the countryside. Lieutenant-General Maurice Baril told a news conference in Nairobi his main concern was for a large group of about 150,000 refugees living off the land in a valley about 65 km (40 miles) west of the eastern city of Goma. If he decided it was necessary and safe for the aircrew, he would not hesitate to order airdrops of food for the refugees, even against the wishes of the government in Kinshasa and the [START ENT] Zairean [END ENT] rebels who control much of eastern Zaire, he said. ” Tomorrow I ́m going into Rwanda and my intention is to go across into eastern Zaire and try to find out for the second time what the situation is on the ground,” he said. General Baril saw rebel leader Laurent Kabila in Goma last week but the rebels told him the crisis was over because most of the Rwandan refugees have already gone home. The rebels do not want the multinational force to deploy on the ground , for fear it might help the Zairean army regain control of the area.\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper proposed the idea of learning a dense retriever with a list of learned prefixes. The model attends to prefixes to compute query-dependent prefixes, which will be used to compute final query vectors for retrieval. In order to make the prefixes non-uniform, the paper additionally introduced a contrastive prompt regularization loss.\n\n# Strength And Weaknesses\n\nThe idea of computing query-dependent prefix to compute query vectors for retrieval is an appealing idea. The results shows that having learned prefixes does improve models' performance in many downstream tasks. The retrieval performance is also improved in zero-shot setting. Overall, I think this is an interesting paper. Thanks for the excellent work.\n\nMy main concern is the additional computation cost coming from the added prefixes. As discussed in the paper, the length of the prefix is 100. For many retrieval tasks, queries are short (sometimes 20~30 tokens). The added prefix tokens may increase the computation cost by ten times or more. Have you tried measuring the efficiency of your model and how it compares to the baselines?\n\nIf the improvement comes from having more parameters, why is it different from increasing the size of the model? Or, can you append 100 special tokens to queries and let models learn their embeddings.\n\nA few more questions:\n1. Does the number of task affect the size of the prefixes? Do you need more prefixes or longer prefixes if there are, say hundreds of tasks?\n2. Are model's parameters in SPLADE jointly finetuned with parameters of the prefixes?\n\nThere are many paper come out recently in few-shot and zero-shot retrieval. You may want to add some of their numbers to your paper, e.g. Promptagator [1]. The numbers may not be directly comparable, but it's good to have.\n\n[1] Promptagator: Few-shot Dense Retrieval From 8 Examples\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written and easy to read. Nice work.\n\nI don't see any challenges in reproducing the results, but it's always nice to open source your codes and checkpoints.\n\n# Summary Of The Review\n\nThe proposed methods is well motivated and strongly justified by the experiments. The only drawback is that it's not very clear where does the improvement come from (see my question above). But overall, it is a nice paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCOMMUNICATION-EFFICIENT AND DRIFT-ROBUST FEDERATED LEARNING VIA ELASTIC NET\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFederated learning (FL) is a distributed method to train a global model over a set of local clients while keeping data localized. It reduces the risks of privacy and security but faces important challenges including expensive communication costs and client drift issues. To address these issues, we propose FedElasticNet, a communicationefficient and drift-robust FL framework leveraging the elastic net. It repurposes two types of the elastic net regularizers (i.e., l1 and l2 penalties on the local model updates): (1) the l1-norm regularizer sparsifies the local updates to reduce the communication costs and (2) the l2-norm regularizer resolves the client drift problem by limiting the impact of drifting local updates due to data heterogeneity. FedElasticNet is a general framework for FL; hence, without additional costs, it can be integrated into prior FL techniques, e.g., FedAvg, FedProx, SCAFFOLD, and FedDyn. We show that our framework effectively resolves the communication cost and client drift problems simultaneously.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) is a collaborative method that allows many clients to contribute individually to training a global model by sharing local models rather than private data. Each client has a local training dataset, which it does not want to share with the global server. Instead, each client computes an update to the current global model maintained by the server, and only this update is communicated. FL significantly reduces the risks of privacy and security (McMahan et al., 2017; Li et al., 2020a), but it faces crucial challenges that make the federated settings distinct from other classical problems (Li et al., 2020a) such as expensive communication costs and client drift problems due to heterogeneous local training datasets and heterogeneous systems (McMahan et al., 2017; Li et al., 2020a; Koneˇcn`y et al., 2016a;b).\n\nCommunicating models is a critical bottleneck in FL, in particular when the federated network comprises a massive number of devices (Bonawitz et al., 2019; Li et al., 2020a; Koneˇcn`y et al., 2016b). In such a scenario, communication in the federated network may take a longer time than that of local computation by many orders of magnitude because of limited communication bandwidth and device power (Li et al., 2020a). To reduce such communication cost, several strategies have been proposed (Koneˇcn`y et al., 2016b; Li et al., 2020a). In particular, Koneˇcn`y et al. (2016b) proposed several methods to form structured local updates and approximate them, e.g., subsampling and quantization. Reisizadeh et al. (2020); Xu et al. (2020) also proposed an efficient quantization method for FL to reduce the communication cost.\n\nAlso, in general, as the datasets that local clients own are heterogeneous, trained models on each local data are inconsistent with the global model that minimizes the global empirical loss (Karimireddy et al., 2020; Malinovskiy et al., 2020; Acar et al., 2021). This issue is referred to as the client drift problem. In order to resolve the client drift problem, FedProx (Li et al., 2020b) added a proximal term to a local objective function and regulated local model updates. Karimireddy et al. (2020) proposed SCAFFOLD algorithm that transfers both model updates and control variates to resolve the client drift problem. FedDyn (Acar et al., 2021) dynamically regularizes local objective functions to resolve the client drift problem.\n\nUnlike most prior works focusing on either the communication cost problem or the client drift problem, we propose a technique that effectively resolves the communication cost and client drift problems simultaneously.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFedAvg\n\nFedProx\n\nSCAFFOLD FedDyn FedElasticNet\n\nCommunication efficiency Robustness to heterogeneous data\n\n△\n\n×\n\n△\n\n△\n\n×\n\n(cid:35)\n\n△\n\n(cid:35)\n\n(cid:35)\n\n(cid:35)\n\nTable 1: Comparison of prior methods and the proposed FedElasticNet.\n\nContributions In this paper, we propose FedElasticNet, a new framework for communicationefficient and drift-robust FL. It repurposes the l1-norm and l2-norm regularizers of the elastic net (Zou & Hastie, 2005), by which it successfully improves (i) communication efficiency by adopting the l1-norm regularizer and (ii) robustness to heterogeneous local datasets by adopting the l2-norm regularizer.\n\nFedElasticNet is a general framework; hence, it can be integrated with prior FL algorithms such as FedAvg (McMahan et al., 2017), FedProx (Li et al., 2020b), SCAFFOLD (Karimireddy et al., 2020), and FedDyn (Acar et al., 2021) so as to resolve the client drift problem as well as the communication cost problem. Further, it incurs no additional costs in training. Empirically, we show that FedElasticNet enhances communication efficiency while maintaining the classification accuracy even for heterogeneous datasets, i.e., the client drift problem is resolved. Theoretically, we characterize the impact of the regularizer terms. Table 1 compares the prior methods and the proposed FedElasticNet if integrated with FedDyn (Algorithm 3).\n\n2 RELATED WORK\n\nTo address the communication cost and client drift problems, numerous approaches were proposed. Here, we describe closely related works that we consider baseline methods. The comprehensive reviews can be found in Kairouz et al. (2021); Li et al. (2020a).\n\nFedAvg (McMahan et al., 2017) is one of the most commonly used methods. FedAvg tackles the communication bottleneck issue by performing multiple local updates before communicating to the server. It works well for homogeneous datasets across clients (McMahan et al., 2017; Karimireddy et al., 2020), but it is known that FedAvg may diverge when local datasets are heterogeneous (Zhao et al., 2018; Li et al., 2020a).\n\nFedProx (Li et al., 2020b) addressed the data heterogeneity problem. FedProx introduces an l2-norm regularizer to the local objective functions to penalize local updates that are far from the server’s model and thus to limit the impact of variable local updates (Li et al., 2020b). Although FedProx is more robust to heterogeneous datasets than FedAvg, the regularizer does not result in aligning the global and local stationary points (Acar et al., 2021). Also, we note that FedProx does not improve communication efficiency compared to that of FedAvg.\n\nSCAFFOLD (Karimireddy et al., 2020) defined client drift that the model created by aggregating local models and the optimal global model is inconsistent because of heterogeneous local datasets. SCAFFOLD communicates the trained local models and the clients’ control variates so as to resolve the client drift problem. Hence, SCAFFOLD requires twice the communication cost compared to other FL algorithms.\n\nFedDyn (Acar et al., 2021) dynamically updates its local regularizers at each round to ensure that the local clients’ optima are asymptotically consistent with stationary points of the global empirical loss. Unlike SCAFFOLD, FedDyn resolves the client drift problem without incurring additional communication costs. However, FedDyn’s communication cost is not improved compared to FedAvg and FedProx.\n\nZou & Hastie (2005) proposed the elastic net to encourage the grouping effect, in other words, to encourage strongly correlated covariates to be in or out of the model description together (Hu et al., 2018). Initially, the elastic net was proposed to overcome the limitations of Lasso (Tibshirani, 1996) imposing an l1-norm penalty on the model parameters. For instance of a linear least square problem,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nthe objective of Lasso is to solve\n\n∥y − Xθ∥2\n\n2 + λ1 ∥θ∥1 ,\n\nmin θ\n\n(1)\n\nwhere y is the outcome and X is the covariate matrix. Lasso performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the resulting model. However, it has some limitations, especially for high-dimensional models. If a group of variables is highly correlated, then Lasso tends to select only one variable from the group and does not care which one is selected (Zou & Hastie, 2005). The elastic net overcomes these limitations by adding an l2-norm penalty. The objective of the elastic net is to solve\n\nmin θ\n\n∥y − Xθ∥2\n\nλ2 2\nThe elastic net simultaneously enables automatic variable selection and continuous shrinkage by the l1-norm regularizer and enables to select groups of correlated variables by its l2-norm regularizer (Zou & Hastie, 2005). We will leverage the elastic net approach to resolve the critical problems of FL: expensive communication cost and client drift problems.\n\n2 + λ1 ∥θ∥1 .\n\n∥θ∥2\n\n2 +\n\n(2)\n\n3 PROPOSED METHOD: FEDELASTICNET\n\nWe assume that m local clients communicate with the global server. For the kth client (where k ∈ [m]) participating in each training round, we assume that a training data feature x ∈ X and its corresponding label y ∈ Y are drawn IID from a device-indexed joint distribution, i.e., (x, y) ∼ Pk (Acar et al., 2021). The objective is to find\n\n\n\narg min θ∈Rd\n\nR (θ) :=\n\n\n\nLk (θ)\n\n ,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\n(3)\n\nwhere Lk (θ) = Ex∼Pk [lk (θ; (x, y))] is the local risk of the kth clients over possibly heterogeneous data distributions Pk. Also, θ represents the model parameters and lk(·) is a loss function such as cross entropy (Acar et al., 2021).\n\nFedElasticNet The proposed method (FedElasticNet) leverages the elastic net approach to resolve the communication cost and client drift problems. We introduce the l1-norm and l2-norm penalties on the local updates: In each round t ∈ [T ], the kth local client attempts to find θt k by solving the following optimization problem:\n\nθt k = arg min\n\nθ\n\nLk (θ) +\n\nλ2 2\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n(cid:13)\n\n2 + λ1\n\n(cid:13)θ − θt−1(cid:13) (cid:13)\n\n(cid:13)1 ,\n\n(4)\n\nwhere θt−1 denotes the global model received from the server. Then, it transmits the difference ∆t\n\nk − θt−1 to the server.\n\nk = θt\n\nInspired by the elastic net, we introduce two types of regularizers for local objective functions; however, each of them works in a different way so as to resolve each of the two FL problems: the communication cost and client drift problems. First, the l2-norm regularizer resolves the client drift problem by limiting the impact of variable local updates as in FedProx (Li et al., 2020b). FedDyn (Acar et al., 2021) also adopts the l2-norm regularizer to control the client drift.\n\nk − θt−1. We consider Second, the l1-norm regularizer attempts to sparsify the local updates ∆t two ways of measuring communication cost: One is the number of nonzero elements in ∆t k (Yoon et al., 2021; Jeong et al., 2021), which the l1-norm sparsifies. The other is the (Shannon) entropy since it is the theoretical lower bound on the data compression (Cover & Thomas, 2006). We demonstrate that the l1-norm penalty on the local updates can effectively reduce the number of nonzero elements as well as the entropy in Section 4. To boost sparseness of ∆t k − θt−1, we sent ∆t k(i) = 0 if |∆t k. The parameter ε is chosen in a range that does not affect classification accuracy.\n\nk(i) denotes the ith element of ∆t\n\nk(i)| ≤ ε where ∆t\n\nk = θt\n\nk = θt\n\nOur FedElasticNet approach can be integrated into existing FL algorithms such as FedAvg (McMahan et al., 2017), SCAFFOLD (Karimireddy et al., 2020), and FedDyn (Acar et al., 2021) without additional costs, which will be described in the following subsections.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 FedElasticNet for FedAvg & FedProx\n\nInput: T , θ0, λ1 > 0, λ2 > 0\n\n1: for each round t = 1, 2, ..., T do 2: 3:\n\nSample devices Pt ⊆ [m] and transmit θt−1 to each selected local client for each local client k ∈ Pt do in parallel\n\nSet θt\n\nk = arg min k = θt\n\nθ\n\nTransmit ∆t\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n(cid:13)\n\nLk (θ) + λ2 k − θt−1 to the global server\n\n2\n\n2 + λ1\n\n(cid:13)θ − θt−1(cid:13) (cid:13) (cid:13)1\n\n4:\n\n5: 6: 7: 8: end for\n\nend for Set θt = θt−1 + (cid:80)\n\nk∈Pt\n\nnk\n\nn ∆k\n\nAlgorithm 2 FedElasticNet for SCAFFOLD\n\nInput: T , θ0, λ1 > 0, λ2 > 0, global step size ηg, and local step size ηl.\n\nSample devices Pt ⊆ [m] and transmit θt−1 and ct−1 to each selected device for each device k ∈ Pt do in parallel\n\n1: for each round t = 1, 2, ..., T do 2: 3: 4: 5: 6:\n\nInitialize local model θt for b = 1, . . . , B do\n\nk = θt−1\n\nk − ηl\n\nCompute mini-batch gradient ∇Lk (θt (cid:0)∇Lk (θt θt k ← θt end for Set ct Transmit ∆t\n\nk) k − θt−1 and ∆ck = ct\n\nk − ct−1 + 1 k = θt\n\nk) − ct−1\n\nk = ct−1\n\n(θt−1 − θt\n\nBηl\n\nk) k + ct−1 + λ2(θt\n\nk − ct−1\n\nk\n\nto the global server\n\nk − θt−1) + λ1sign(θt\n\nk − θt−1)(cid:1)\n\n7: 8: 9:\n\n10: 11: 12:\n\nend for Set θt = θt−1 + ηg Set ct = ct−1 + 1\n\n|Pt|\n\n(cid:80)\n\n(cid:80)\n\nm\n\nk∈Pt\n\nk∈Pt\n\n∆k ∆ck\n\n13: 14: end for\n\n3.1 FEDELASTICNET FOR FEDAVG & FEDPROX (FEDAVG & FEDPROX + ELASTIC NET)\n\nOur FedElasticNet can be applied to FedAvg (McMahan et al., 2017) by adding two regularizers on the local updates, which resolves the client drift problem and the communication cost problem. As shown in Algorithm 1, the local client minimizes the local objective function (4). In Step 7, n and nk denote the total numbers of data points of all clients and the data points of the kth client, respectively.\n\nIt is worth mentioning that FedProx uses the l2-norm regularizer to address the data and system heterogeneities (Li et al., 2020b). By adding the l1-norm regularizer, we can sparsify the local updates of FedProx and thus effectively reduce the communication cost. Notice that Algorithm 1 can be viewed as the integration of FedProx and FedElasticNet.\n\n3.2 FEDELASTICNET FOR SCAFFOLD (SCAFFOLD + ELASTIC NET)\n\nIn SCAFFOLD, each client computes the following mini-batch gradient ∇Lk(θt ct k (Karimireddy et al., 2020): k ← θt θt\n\nk + ct−1(cid:1) ,\n\n(cid:0)∇Lk\n\nk − ηl\n\nk) and control variate\n\nk ← ct−1 ct\n\nk − ct−1 +\n\n(cid:1) − ct−1 (cid:0)θt k\n1 Bηl\n\n(θt−1 − θt\n\nk),\n\n(5)\n\n(6)\n\nwhere ηl is the local step size and B is the number of mini-batches at each round. This control variate makes the local parameters θt k updated in the direction of the global optimum rather than each local optimum, which effectively resolves the client drift problem. However, SCAFFOLD incurs twice k − θt−1 and the much communication cost since it should communicate the local update ∆t control variate ∆ck = ct\n\n, which are of the same dimension.\n\nk = θt\n\nk − ct−1\n\nk\n\nIn order to reduce the communication cost of SCAFFOLD, we apply our FedElasticNet framework. In the proposed algorithm (see Algorithm 2), each local client computes the following mini-batch\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ngradient instead of (5):\n\n(cid:0)∇Lk\n\n(cid:0)θt\n\n(cid:1) − ct−1\n\nk\n\nk − ηl k − θt−1) corresponds to the gradient of l1-norm regularizer λ1∥θt\n\nk ← θt θt where λ1sign(θt k − θt−1∥1. This k − θt−1; hence, reduces the communication l1-norm regularizer sparsifies the local update ∆t cost. Since the control variate already addresses the client drift problem, we can remove the l2-norm regularizer or set λ2 as a small value.\n\nk − θt−1) + λ1sign(θt\n\nk + ct−1 + λ2(θt\n\nk = θt\n\n(7)\n\nk − θt−1)(cid:1) ,\n\n3.3 FEDELASTICNET FOR FEDDYN (FEDDYN + ELASTIC NET)\n\nIn FedDyn, each local client optimizes the following local objective, which is the sum of its empirical loss and a penalized risk function:\n\nθt k = arg min\n\nθ\n\nLk (θ) − ⟨∇Lk(θt−1\n\nk\n\n), θ⟩ +\n\nλ2 2\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n(cid:13)\n\n2 ,\n\n(8)\n\nwhere the penalized risk is dynamically updated so as to satisfy the following first-order condition for local optima:\n\n(9) This first-order condition shows that the stationary points of the local objective function are consistent with the server model (Acar et al., 2021). That is, the client drift is resolved. However, FedDyn makes no difference from FedAvg and FedProx in communication costs.\n\nk − θt−1) = 0.\n\nk) − ∇Lk(θt−1\n\n) + λ2(θt\n\n∇Lk(θt\n\nk\n\nBy integrating FedElasticNet and FedDyn, we can effectively reduce the communication cost of FedDyn as well. In the proposed method (i.e., FedElasticNet for FedDyn), each local client optimizes the following local empirical objective:\n\nθt k = arg min\n\nθ\n\nLk (θ) − ⟨∇Lk(θt−1\n\nk\n\n), θ⟩ +\n\nλ2 2\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n(cid:13)\n\n2 + λ1\n\n(cid:13)θ − θt−1(cid:13) (cid:13)\n\n(cid:13)1 ,\n\n(10)\n\nwhich is the sum of (8) and the additional l1-norm penalty on the local updates. The corresponding first-order condition is given by\n\n∇Lk(θt\n\nk) − ∇Lk(θt−1\n\nk\n\n) + λ2(θt\n\nk − θt−1) + λ1sign(θt\n\nk − θt−1) = 0.\n\n(11)\n\nNotice that the stationary points of the local objective function are consistent with the server model as in (9). If θt\n\nk − θt−1) = ±1), then the first-order condition is\n\nk ̸= θt−1 (i.e., sign(θt\n\n∇Lk(θt\n\nk) − ∇Lk(θt−1\n\nk\n\n) + λ2(θt\n\nk − θt−1) = ±λ1,\n\n(12)\n\nwhere λ1 is a vectorized one. Our empirical results show that the optimized hyperparameter is λ1 = 10−4 or 10−6 and the impact of ±λ1 in (12) would be negligible. Hence, the proposed FedElasticNet for FedDyn resolves the client drift problem. Further, the local update ∆t k − θt−1 is sparse due to the l1-norm regularizer, which effectively reduces the communication cost at the same time. The detailed algorithm is described in Algorithm 3.\n\nk = θt\n\nAlgorithm 3 FedElasticNet for FedDyn\n\nInput: T , θ0, λ1 > 0, λ2 > 0, h0 = 0, ∇Lk\n\n(cid:0)θ0\n\nk\n\n(cid:1) = 0.\n\n1: for each round t = 1, 2, ..., T do 2: 3:\n\nSample devices Pt ⊆ [m] and transmit θt−1 to each selected device for each device k ∈ Pt do in parallel\n\n4:\n\n5:\n\n6: 7: 8: 9: 10: 11:\n\nSet θt\n\nk = arg min\n\nθ\n\nSet ∇Lk (θt Transmit ∆t\n\nk) = ∇Lk k = θt\n\nLk (θ) − (cid:10)∇Lk(θt−1 (cid:0)θt\n\n(cid:1) − λ2 k − θt−1 to the global server\n\n(cid:0)θt−1\n\nk\n\nk\n\n), θ(cid:11) + λ2\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n2 + λ1 (cid:13) k − θt−1(cid:1) k − θt−1(cid:1) − λ1sign (cid:0)θt\n\n2\n\nend for for each device k /∈ Pt do in parallel\n\nSet θt\n\nk = θt−1\n\nk\n\nand ∇Lk (θt\n\nend for Set ht = ht−1 − λ2 Set θt = 1\n\n(cid:80)\n\nm\n\n|Pt|\n\nk∈Pt\n\n(cid:80)\n\nk∈Pt\n\nk − 1 θt\n\nλ2\n\n(cid:0)θt−1\n\nk\n\nk) = ∇Lk (cid:0)θt k − θt−1(cid:1) − λ1 ht\n\nm\n\n(cid:1)\n\n(cid:80)\n\nk∈Pt\n\nsign(θt\n\nk − θt−1)\n\n12: 13: end for\n\n(cid:13)θ − θt−1(cid:13) (cid:13) (cid:13)1\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nConvergence Analysis We provide a convergence analysis on FedElasticNet for FedDyn (Algorithm 3). Theorem 3.1. Assume that the clients are uniformly randomly selected at each round and the local loss functions are convex and β-smooth. Then Algorithm 3 satisfies the following inequality:\n\n(cid:34)\n\n(cid:32)\n\nE\n\nR\n\n(cid:33)\n\nγt\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\n(cid:35)\n\n− R(θ∗)\n\n≤\n\n1 T\n\n1 κ0\n\n(E∥γ0 − θ∗∥2\n\n2 + κC0) +\n\nκ′ κ0\n\n· λ2\n\n1d\n\n−\n\n1 T\n\n2λ1 λ2\n\nT (cid:88)\n\nt=1\n\n(cid:42)\n\nγt−1 − θ∗,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n,\n\n(13)\n\n(cid:43)\n\nwhere θ∗ = arg minθ R(θ), P = |Pt|, γt = 1\n\nP\n\n(cid:80)\n\nPt\n\n2 λ2\n\nλ2\n\n2−25λ2β−50β2 2−25β2\n\nλ2\n\n, κ′ = 5 λ2\n\nλ2\n\nλ2+β\n\n2−25β2 = κ · P ), θ(cid:11) +\n\n2m , C0 = 1 λ2 2\n\nk\n\nm\n\nLk (θ) − (cid:10)∇Lk(θt−1\n\n ̃θt k = arg min\n\nθ\n\nk, d = dim(θ), κ = 10m θt (cid:80)\n\nE∥∇Lk(θ0\n\nP\n\nk∈[m]\n\nk) − ∇Lk(θ∗)∥ and\n\n1 λ2\n\nλ2+β\n\n2−25β2 , κ0 =\n\nλ2\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n(cid:13)\n\n2 + λ1\n\n(cid:13)θ − θt−1(cid:13) (cid:13) (cid:13)1\n\n∀k ∈ [m].\n\nTheorem 3.1 provides a convergence rate of FedElasticNet for FedDyn. If T → ∞, the first term of (13) converges to 0 at the speed of O(1/T ). The second and the third terms of (13) are additional penalty terms caused by the l1-norm regularizer. The second term is a negligible constant in the range of hyperparameters of our interest. Considering the last term, notice that the summand at each t includes the expected average of sign vectors where each element is ±1. If a coordinate of the sign vectors across clients is viewed as an IID realization of Bern( 1 2 ), it can be thought of as a small value with high probability by the concentration property (see Appendix B.3). In addition, γt−1 − θ∗ characterizes how much the average of local models deviates from the globally optimal model, which tends to be small as training proceeds. Therefore, the effect of both additional terms is negligible.\n\n4 EXPERIMENTS\n\nIn this section, we evaluate the proposed FedElasticNet on benchmark datasets for various FL scenarios. In particular, FedElasticNet is integrated with prior methods including FedProx (Li et al., 2020b), SCAFFOLD (Karimireddy et al., 2020), and FedDyn (Acar et al., 2021). The experimental results show that FedElasticNet effectively enhances communication efficiency while maintaining classification accuracy and resolving the client drift problem. We observe that the integration of FedElasticNet and FedDyn (Algorithm 3) achieves the best performance.\n\nExperimental Setup We use the same benchmark datasets as prior works. The evaluated datasets include MNIST (LeCun et al., 1998), a subset of EMNIST (Cohen et al., 2017, EMNIST-L), CIFAR10, CIFAR-100 (Krizhevsky & Hinton, 2009), and Shakespeare (Shakespeare, 1914). The IID split is generated by randomly assigning datapoint to the local clients. The Dirichlet distribution is used on the label ratios to ensure uneven label distributions among local clients for non-IID splits as in Zhao et al. (2018); Acar et al. (2021). For the uneven label distributions among 100 experimental devices, the experiments are performed by using the Dirichlet parameters of 0.3 and 0.6, and the number of data points is obtained by the lognormal distribution as in Acar et al. (2021). The data imbalance is controlled by varying the variance of the lognormal distribution (Acar et al., 2021).\n\nWe use the same neural network models of FedDyn experiments (Acar et al., 2021). For MNIST and EMNIST-L, fully connected neural network architectures with 2 hidden layers are used. The numbers of neurons in the layers are 200 and 100, respectively (Acar et al., 2021). Remark that the model used for MNIST dataset is the same as in Acar et al. (2021); McMahan et al. (2017). For CIFAR-10 and CIFAR-100 datasets, we use a CNN model consisting of 2 convolutional layers with 64 5 × 5 filters followed by 2 fully connected layers with 394 and 192 neurons and a softmax layer. For the next character prediction task for Shakespeare, we use a stacked LSTM as in Acar et al. (2021).\n\nFor MNIST, EMNIST-L, CIFAR10, and CIFAR100 datasets, we evaluate three cases: IID, non-IID with Dirichlet (.6), and non-IID with Dirichlet (.3). Shakespeare datasets are evaluated for IID and non-IID cases as in Acar et al. (2021). We use the batch size of 10 for the MNIST dataset, 50 for CIFAR-10, CIFAR-100, and EMNIST-L datasets, and 20 for the Shakespeare dataset. We optimize the hyperparameters depending on the evaluated datasets: learning rates, λ2, and λ1.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nDataset CIFAR-10 CIFAR-100 MNIST EMNIST-L Shakespeare CIFAR-10 CIFAR-100 MNIST EMNIST-L CIFAR-10 CIFAR-100 MNIST EMNIST-L Shakespeare\n\nIID\n\nDirichlet (.6)\n\nDirichlet (.3)\n\nNon-IID\n\nRounds FedProx Algorithm 1\n\nSCAFFOLD Algorithm 2 FedDyn Algorithm 3\n\n200 500 100 200 100 200 500 100 200 200 500 100 200 100\n\n163.82 413.37 20.3 18.4 1.99 163.82 413.37 20.3 18.4 163.82 413.37 20.3 18.4 13.61\n\n124.56 249.26 7.51 11.23 1.93 116.96 247.87 6.66 11.18 112.1 255.97 6.49 11.29 11.28\n\n327.64 826.74 40.58 36.78 3.32 327.64 826.74 40.58 36.78 327.64 826.74 40.58 36.78 27.21\n\n313.04 803.33 36.98 34.90 3.31 310.51 798.21 35.20 34.45 308.15 804.47 34.95 34.22 27.21\n\n163.82 413.37 20.3 18.4 1.99 163.82 413.37 20.3 18.4 163.82 413.37 20.3 18.4 13.61\n\n34.23 132.95 2.55 2.12 1.28 29.96 127.23 2.55 2.05 26.99 124.02 2.53 2.07 9.41\n\nTable 2: Number of non-zero elements cumulated over the all round simulated with 10% client participation for IID and non-IID settings in FL scenarios. The non-IID settings of MNIST, EMNISTL, CIFAR-10, and CIFAR-100 datasets are created with the Dirichlet distribution of labels owned by the client. Algorithm 1 is FedElasticNet for FedProx, Algorithm 2 is FedElasticNet for SCAFFOLD, and Algorithm 3 is FedElasticNet for FedDyn. The unit of the cumulative number of elements is 107.\n\nDataset CIFAR-10 CIFAR-100 MNIST EMNIST-L Shakespeare CIFAR-10 CIFAR-100 MNIST EMNIST-L CIFAR-10 CIFAR-100 MNIST EMNIST-L Shakespeare\n\nIID\n\nDirichlet (.6)\n\nDirichlet (.3)\n\nNon-IID\n\nRounds FedProx Algorithm 1\n\nSCAFFOLD Algorithm 2\n\n200 500 100 200 100 200 500 100 200 200 500 100 200 100\n\n586.42 1712.84 266.26 657.64 646.33 564.57 1709.33 249.63 646.14 550.15 1696.47 244.49 636.21 593.21\n\n232.77 470.78 47.29 166.07 403.63 203.61 449.59 45.51 163.24 187.01 428.77 45.24 162.57 440.97\n\n685.17 2225.98 286.88 764.39 520.60 663.23 2202.61 293.22 755.75 636.90 2170.14 291.88 747.72 628.32\n\n236.76 1173.01 83.51 344.94 226.68 198.53 1119.91 75.57 347.31 115.26 1078.97 73.12 328.21 470.22\n\nFedDyn 639.59 (221.64) 1964.63 (511.14) 308.27 (27.76) 704.57 (132.50) 576.17 (348.11) 616.69 (197.62) 1951.06 (478.60) 304.00 (26.75) 704.63 (134.31) 602.80 (180.29) 1937.09 (463.67) 300.76 (26.71) 700.38 (128.34) 609.11 (348.11)\n\nAlgorithm 3 140.71 423.53 24.76 96.37 225.44 121.97 398.87 21.24 89.92 108.69 382.44 19.03 91.55 419.99\n\nTable 3: Cumulative entropy values of transmitted bits with 10% client participation for IID and nonIID settings in FL scenarios. The non-IID settings of MNIST, EMNIST-L, CIFAR-10, and CIFAR-100 datasets are created with the Dirichlet distribution of labels owned by the client. Algorithm 1 is FedElasticNet for FedProx, Algorithm 2 is FedElasticNet for SCAFFOLD, and Algorithm 3 is FedElasticNet for FedDyn. The left-side numbers of FedDyn are the entropy values when the local models θt k are transmitted and the right-side numbers in parentheses are the entropy values when the local updates ∆t\n\nk − θt−1 are transmitted.\n\nk = θt\n\nEvaluation of Methods We compare the baseline methods (FedProx, SCAFFOLD, and FedDyn) and the proposed FedElasticNet integrations (Algorithms 1, 2, and 3), respectively. We evaluate the communication cost and classification accuracy for non-IID settings of the prior methods and the proposed methods. The robustness of the client drift problem is measured by the classification accuracy of non-IID settings.\n\nWe report the communication costs in two ways: (i) the number of nonzero elements in transmitted values as in (Yoon et al., 2021; Jeong et al., 2021) and (ii) the Shannon entropy of transmitted bits. Note that the Shannon entropy is the theoretical limit of data compression (Cover & Thomas, 2006), which can be achieved by practical algorithms; for instance, Han et al. (2016) used Huffman coding for model compression. We calculate the entropy of discretized values with the bin size of 0.01. Note that the transmitted values are not discretized in FL, and only the discretization is considered to calculate the entropy. The lossy compression schemes (e.g., scalar quantization, vector quantization, etc.) have not been considered since they include several implementational issues which are beyond our research scope.\n\nTable 2 reports the number of non-zero elements of the baseline methods with/without FedElasticNet. Basically, the communication costs per round of FedProx and FedDyn are the same; SCAFFOLD suffers from the doubled communication cost because of the control variates. The proposed FedElasticNet integrations (Algorithms 1, 2, and 3) can effectively sparsify the transmitted local updates, which enhances communication efficiency.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nDataset CIFAR-10 CIFAR-100 MNIST EMNIST-L Shakespeare CIFAR-10 CIFAR-100 MNIST EMNIST-L CIFAR-10 CIFAR-100 MNIST EMNIST-L Shakespeare\n\nIID\n\nDirichlet (.6)\n\nDirichlet (.3)\n\nNon-IID\n\nRounds FedProx Algorithm 1\n\nSCAFFOLD Algorithm 2\n\n200 500 100 200 100 200 500 100 200 200 500 100 200 100\n\n595.16 1721.53 325.86 728.34 640.69 577.74 1697.18 298.99 721.49 563.15 1685.30 295.80 716.12 595.69\n\n151.28 466.21 39.55 123.74 279.51 127.14 453.87 30.94 121.76 105.89 444.32 42.94 116.88 409.60\n\n873.24 2774.60 389.52 1018.11 529.05 839.41 2682.15 530.72 1020.32 806.73 2743.43 466.65 1014.08 684.79\n\n252.33 1068.47 71.57 263.95 476.81 236.48 1038.21 106.08 251.65 214.31 1060.83 85.06 249.68 897.67\n\nFedDyn 680.70 (259.31) 2038.07 (689.24) 324.88 (20.47) 797.39 (55.46) 343.32 (298.58) 656.48 (223.46) 1974.65 (651.78) 314.64 (20.29) 779.44 (286.11) 635.78 (215.83) 1934.49 (627.56) 314.96 (19.98) 771.90 (283.40) 560.87 (316.64)\n\nAlgorithm 3 57.70 447.18 12.33 40.71 277.21 47.52 431.54 11.67 40.71 39.58 422.46 12.35 40.70 318.92\n\nTable 4: Cumulative entropy values of transmitted bits with 100% client participation for IID and nonIID settings in FL scenarios. The non-IID settings of MNIST, EMNIST-L, CIFAR-10, and CIFAR-100 datasets are created with the Dirichlet distribution of labels owned by the client. Algorithm 1 is FedElasticNet for FedProx, Algorithm 2 is FedElasticNet for SCAFFOLD, and Algorithm 3 is FedElasticNet for FedDyn. The left-side numbers of FedDyn are the entropy values when the local models θt k are transmitted and the right-side numbers in parentheses are the entropy values when the local updates ∆t\n\nk − θt−1 are transmitted.\n\nk = θt\n\nIn particular, the minimal communication cost is achieved when FedElasticNet is integrated with FedDyn (Algorithm 3). It is because the classification accuracy is not degraded even if the transmitted values are more aggressively sparsified in Algorithm 3. Fig. 2 shows the transmitted local updates ∆t k of Algorithm 3 are sparser than FedDyn and Algorithm 2. Hence, Algorithm 3 (FedElasticNet for FedDyn) achieves the best communication efficiency.\n\nFigure 1: Classification accuracy performance evaluated in MNIST, EMNIST-L, CIFAR-10, CIFAR100 dataset settings (10% participation rate and Dirichlet (.3)).\n\nTables 3 and 4 report the Shannon entropy of transmitted bits for the baseline methods with/without FedElasticNet. The communication costs of baseline methods are effectively improved by the\n\n8\n\n020406080100Communication Rounds0.900.920.940.960.981.00Test AccuracyMNIST accuracy0255075100125150175200Communication Rounds0.860.880.900.920.94Test AccuracyEMNIST accuracy0255075100125150175200Communication Rounds0.660.680.700.720.740.760.780.80Test AccuracyCIFAR-10 accuracy0100200300400500Communication Rounds0.250.300.350.400.450.50Test AccuracyCIFAR-100 accuracyUnder review as a conference paper at ICLR 2023\n\nFigure 2: Comparison of distributions of transmitted local updates ∆t rate and Dirichlet (.3)) for CIFAR-10.\n\nk = θt\n\nk −θt−1 (10% participation\n\nFedElasticNet approach. Algorithms 1, 2, and 3 reduce the entropy compared to the their baseline methods. We note that FedElasticNet integrated with FedDyn (Algorithm 3) achieves the minimum entropy, i.e., the minimum communication cost.\n\nk as in Acar et al. (2021) and (ii) transmit the local updates ∆t\n\nFor FedDyn, we evaluate the Shannon entropy values for two cases: (i) transmit the updated local k −θt−1 as in Algorithm 3. models θt We observe that transmitting the local updates ∆t k can reduce the Shannon entropy significantly. Hence, it is beneficial to transmit the local updates ∆t k even for FedDyn if it adopts an additional compression scheme. The numbers of nonzero elements for two cases (i.e., θt\n\nk instead of the local models θt\n\nk) are the same for FedDyn.\n\nk and ∆t\n\nk = θt\n\nFig. 1 shows that the FedElasticNet maintains the classification accuracy or incurs marginal degradation. We observe a classification gap between FedProx and Algorithm 1 for CIFAR-10 and CIFAR-100. However, the classification accuracies of FedDyn and Algorithm 3 are almost identical in the converged regime.\n\nIn particular, Algorithm 3 significantly reduces the Shannon entropy, which can be explained by Fig 2. Fig 2 compares the distributions of the transmitted local updates ∆t k for FedDyn, Algorithm 2, and Algorithm 3. Because of the l1-norm penalty on the local updates, Algorithm 3 makes sparser local updates than FedDyn. The local updates of FedDyn can be modeled by the Gaussian distribution, and the local updates of FedElasticNet can be modeled by the non-Gaussian distribution (similar to the Laplacian distribution). It is well-known that the Gaussian distribution maximizes the entropy for a given variance in information theory Cover & Thomas (2006). Hence, FedElasticNet can reduce the entropy by transforming the Gaussian distribution into the non-Gaussian one.\n\n5 CONCLUSION\n\nWe proposed FedElasticNet, a general framework to improve communication efficiency and resolve the client drift problem simultaneously. We introduce two types of penalty terms on the local model updates by repurposing the classical elastic net. The l1-norm regularizer sparsifies the local model updates, which reduces the communication cost. The l2-norm regularizer limits the impact of variable local updates to resolve the client drift problem. Importantly, our framework can be integrated with prior FL techniques so as to simultaneously resolve the communication cost problem and the client drift problem. By integrating FedElasticNet with FedDyn, we can achieve the best communication efficiency while maintaining classification accuracy for heterogeneous datasets.\n\n9\n\n0.0150.0100.0050.0000.0050.0100.0150100200300400500Distribution of local updatesFedDynAlgorithm 2Algorithm 3Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDurmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations (ICLR), May 2021.\n\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé Kiddon, Jakub Koneˇcný, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In A Talwalkar, V Smith, and M Zaharia (eds.), Proceedings of Machine Learning and Systems, volume 1, pp. 374–388, 2019.\n\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In International Joint Conference on Neural Networks (IJCNN), pp. 2921–2926, 2017.\n\nThomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience, New\n\nYork, 2006.\n\nRick Durrett. Probability: Theory and Examples. Cambridge University Press, Cambridge, UK, fifth\n\nedition, 2019. doi: 10.1017/9781108591034.\n\nSong Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations (ICLR), volume abs/1510.0, October 2016.\n\nJianhua Hu, Jian Huang, and Feng Qiu. A group adaptive elastic-net approach for variable selection\n\nin high-dimensional linear regression. Science China Mathematics, 61(1):173–188, 2018.\n\nWonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised learning with inter-client consistency & disjoint learning. In International Conference on Learning Representations (ICLR), 2021.\n\nPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning (ICML), pp. 5132–5143, 2020.\n\nJakub Koneˇcn`y, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016a.\n\nJakub Koneˇcn`y, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016b.\n\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.\n\nTechnical report, University of Toronto, 2009.\n\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,\n\nmethods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020a.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and Systems, volume 2, pp. 429–450, 2020b.\n\nGrigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter Richtarik. From local sgd to local fixed-point methods for federated learning. In International Conference on Machine Learnin (ICML), pp. 6692–6701, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nH. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273–1282. PMLR, 2017.\n\nY. Nesterov. Lectures on convex optimization, volume 137. springer, 2018. URL https://link.\n\nspringer.com/content/pdf/10.1007/978-3-319-91578-4.pdf.\n\nAmirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. FedPAQ: A communication-efficient federated learning method with periodic averaging and quantization. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108, pp. 2021–2031, August 2020.\n\nWilliam Shakespeare. The complete works of William Shakespeare. Humphrey Milford, Oxford University Press, 1914. URL http://www.gutenberg.org/files/100/old/ 1994-01-100.zip.\n\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\n\nSociety: Series B (Methodological), 58(1):267–288, 1996.\n\nJinjin Xu, Wenli Du, Yaochu Jin, Wangli He, and Ran Cheng. Ternary compression for communication-efficient federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2020.\n\nJaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual learning with weighted inter-client transfer. In Marina Meila and Tong Zhang (eds.), International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 12073–12086, 2021.\n\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated\n\nlearning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.\n\nHui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the\n\nRoyal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 EXPERIMENT DETAILS\n\nWe provide the details of our experiments. We select the datasets for our experiments, including those used in prior work on federated learning (McMahan et al., 2017; Li et al., 2020b; Acar et al., 2021). To fairly compare the non-IID environments, the datasets and the experimental environments are the same as those of Acar et al. (2021).\n\nHyperparameters. We describe the hyperparameters used in our experiments in Section 4. We perform a grid search to find the best λ1 and ε used in the proposed algorithms. Each hyperparameter was selected to double the value as the performance improved. We use the same λ2 as in Acar et al. (2021). SCAFFOLD has the same local epoch and batch size as other algorithms, and SCAFFOLD is not included in Table 4 because other hyperparameters are not required. Table 5 shows the hyperparameters used in our experiments.\n\nDataset\n\nCIFAR-10\n\nCIFAR-100\n\nMNIST\n\nEMNIST-L\n\nShakespeare\n\nAlgorithm FedProx Algorithm 1 Algorithm 2 FedDyn Algorithm 3 FedProx Algorithm 1 Algorithm 2 FedDyn Algorithm 3 FedProx Algorithm 1 Algorithm 2 FedDyn Algorithm 3 FedProx Algorithm 1 Algorithm 2 FedDyn Algorithm 3 FedProx Algorithm 1 Algorithm 2 FedDyn Algorithm 3\n\nλ1 -\n10−6 10−4 -\n10−4 -\n10−6 10−4 -\n10−4 -\n10−6 10−4 -\n10−4 -\n10−6 10−4 -\n10−4 -\n10−6 10−6 -\n10−6\n\nλ2 10−4 10−4 0\n10−2 10−2 10−4 10−4 0\n10−2 10−2 10−4 10−6 0\n5 × 10−2 5 × 10−2 10−4 10−6 0\n4 × 10−2 4 × 10−2 10−4 10−6 0\n10−2 10−2\n\nε -\n10−3 10−4 -\n5 × 10−3 -\n10−3 10−4 -\n10−3 -\n10−3 10−4 -\n5 × 10−3 -\n10−3 10−4 -\n2 × 10−3 -\n9 × 10−3 9 × 10−4 -\n10−2\n\nTable 5: Hyperparameters.\n\nA.2 REGULARIZER COEFFICIENTS\n\nWe selected λ1 over {10−2, 10−4, 10−6, 10−8} to observe the impact of λ1 on the classification accuracy. We prefer a larger λ1 to enhance communication efficiency unless the l1-norm regularizer does not degrade the classification accuracy. Figures 3, 4, and 5 show the classification accuracy depending on λ1 in the CIFAR-10 dataset with 10% participation rate and Dirichlet (.3). The unit of the cumulative number of elements is 107.\n\nIn Algorithm 1, we selected λ1 = 10−6 to avoid a degradation of classification accuracy (see Fig. 3) and maximize the sparsity of local updates. In this way, we selected the coefficient values λ1 (See Fig.4 for Algorithm 2 and 5 and Algorithm 3).\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Classification accuracy and sparsity of local updates depending on λ1 (Algorithm 1).\n\nFigure 4: Classification accuracy and sparsity of local updates depending on λ1 (Algorithm 2).\n\n13\n\n1e-21e-41e-61e-81 Value0.400.450.500.550.600.650.700.75Test AccuracyAlgorithm 1 accuracy according to 1 value1e-21e-41e-61e-81 Value0255075100125150175EntropyAlgorithm 1 entropy according to 1 value1e-21e-41e-61e-81 Value20406080100Number of non-zero elementsNumber of non-zero elements algorithm 1 according to 1 value1e-21e-41e-61e-81 Value0.400.450.500.550.600.650.700.750.80Test AccuracyAlgorithm 2 accuracy according to 1 value1e-21e-41e-61e-81 Value50100150200250EntropyAlgorithm 2 entropy according to 1 value1e-21e-41e-61e-81 Value270280290300310320Number of non-zero elementsNumber of non-zero elements algorithm 2 according to 1 valueUnder review as a conference paper at ICLR 2023\n\nFigure 5: Classification accuracy and sparsity of local updates depending on λ1 (Algorithm 3).\n\n14\n\n1e-21e-41e-61e-81 Value0.20.30.40.50.60.70.8Test AccuracyAlgorithm 3 accuracy according to 1 value1e-21e-41e-61e-81 Value20406080100120EntropyAlgorithm 3 entropy according to 1 value1e-21e-41e-61e-81 Value18202224262830Number of non-zero elementsNumber of non-zero elements algorithm 3 according to 1 valueUnder review as a conference paper at ICLR 2023\n\nA.3 EMPIRICAL RESULTS OF CLASSIFICATION ACCURACY\n\nFigure 6: Classification accuracy performance evaluated in MNIST, EMNIST-L, CIFAR-10, and CIFAR-100 datasets (10% participation rate).\n\n15\n\n020406080100Communication Rounds0.900.920.940.960.981.00Test AccuracyMNIST Dirichlet (.6) accuracy020406080100Communication Rounds0.900.920.940.960.981.00Test AccuracyMNIST IID accuracy0255075100125150175200Communication Rounds0.860.880.900.920.94Test AccuracyEMNIST-L Dirichlet (.6) accuracy0255075100125150175200Communication Rounds0.860.880.900.920.940.96Test AccuracyEMNIST-L IID accuracy0255075100125150175200Communication Rounds0.660.680.700.720.740.760.780.80Test AccuracyCIFAR-10 Dirichlet (.6) accuracy0255075100125150175200Communication Rounds0.660.680.700.720.740.760.780.800.82Test AccuracyCIFAR-10 IID accuracy0100200300400500Communication Rounds0.250.300.350.400.450.50Test AccuracyCIFAR-100 Dirichlet (.6) accuracy0100200300400500Communication Rounds0.250.300.350.400.450.50Test AccuracyCIFAR-100 IID accuracyUnder review as a conference paper at ICLR 2023\n\n(a) 10% participation rate\n\n(b) 10% participation rate\n\n(c) 100% participation rate\n\n(d) 100% participation rate\n\nFigure 7: Classification accuracy performance evaluated in IID Shakespeare and Non-IID Shakespeare datasets.\n\n16\n\n020406080100Communication Rounds0.200.250.300.350.400.450.50Test AccuracyShakespeare Non-IID accuracy020406080100Communication Rounds0.200.250.300.350.400.450.50Test AccuracyShakespeare IID accuracy020406080100Communication Rounds0.200.250.300.350.400.450.500.55Test AccuracyShakespeare Non-IID accuracy020406080100Communication Rounds0.200.250.300.350.400.450.50Test AccuracyShakespeare IID accuracyUnder review as a conference paper at ICLR 2023\n\nFigure 8: Classification accuracy performance evaluated in MNIST, EMNIST-L datasets (100% participation rate).\n\n17\n\n020406080100Communication Rounds0.900.920.940.960.981.00Test AccuracyMNIST Dirichlet (.3) accuracy020406080100Communication Rounds0.900.920.940.960.981.00Test AccuracyMNIST Dirichlet (.6) accuracy020406080100Communication Rounds0.900.920.940.960.981.00Test AccuracyMNIST IID accuracy0255075100125150175200Communication Rounds0.860.880.900.920.940.96Test AccuracyEMNIST-L Dirichlet (.3) accuracy0255075100125150175200Communication Rounds0.860.880.900.920.940.96Test AccuracyEMNIST-L Dirichlet (.6) accuracy0255075100125150175200Communication Rounds0.860.880.900.920.940.96Test AccuracyEMNIST-L IID accuracyUnder review as a conference paper at ICLR 2023\n\nFigure 9: Classification accuracy performance evaluated in CIFAR-10 and CIFAR-100 datasets (100% participation rate).\n\n18\n\n0255075100125150175200Communication Rounds0.6500.6750.7000.7250.7500.7750.8000.8250.850Test AccuracyCIFAR-10 Dirichlet (.3) accuracy0255075100125150175200Communication Rounds0.6500.6750.7000.7250.7500.7750.8000.8250.850Test AccuracyCIFAR-10 Dirichlet (.6) accuracy0255075100125150175200Communication Rounds0.6500.6750.7000.7250.7500.7750.8000.8250.850Test AccuracyCIFAR-10 IID accuracy0100200300400500Communication Rounds0.250.300.350.400.450.500.55Test AccuracyCIFAR-100 Dirichlet (.3) accuracy0100200300400500Communication Rounds0.250.300.350.400.450.500.55Test AccuracyCIFAR-100 Dirichlet (.6) accuracy0100200300400500Communication Rounds0.250.300.350.400.450.50Test AccuracyCIFAR-100 IID accuracyUnder review as a conference paper at ICLR 2023\n\nB PROOF\n\nWe utilize some techniques in FedDyn (Acar et al., 2021).\n\nB.1 DEFINITION\n\nWe introduce a formal definition and properties that we will use. Definition B.0.1. A function Lk is β-smooth if it satisfies\n\n∥∇Lk(x) − ∇Lk(y)∥ ≤ β∥x − y∥\n\n∀x, y.\n\nIf function Lk is convex and β-smooth, it satisfies\n\n− ⟨∇Lk(x), z − y⟩ ≤ −Lk(z) + Lk(y) +\n\nβ 2\n\n∥z − x∥2 ∀x, y, z.\n\n(14)\n\n(15)\n\nAs a consequence of the convexity and smoothness, the following property holds (Nesterov, 2018, Theorem 2.1.5):\n\n1 2βm\n\n(cid:88)\n\nk∈[m]\n\n∥∇Lk(x) − ∇Lk(x∗)∥2 ≤ R(x) − R(x∗) ∀x\n\n(16)\n\nwhere R(x) = 1\n\nm\n\n(cid:80)m\n\nk=1 Lk(x) and ∇R(x∗) = 0.\n\nWe will also use the relaxed triangle inequality (Karimireddy et al., 2020, Lemma 3):\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\nj=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nvj\n\n≤ n\n\nn (cid:88)\n\nj=1\n\n∥vj∥2.\n\n(17)\n\nB.2 PROOF OF THEOREM 3.1\n\nThe theorem that we will prove is as follows. Theorem B.1 (Full statement of Theorem 3.1). Assume that the clients are uniformly randomly selected at each round and the individual loss functions {Lk}m k=1 are convex and β-smooth. Also assume that λ2 > 27β. Then Algorithm 3 satisfies the following inequality: Letting R(θ) = 1\nm\n\nk∈[m] Lk(θ) and θ∗ = arg min\n\nR(θ),\n\n(cid:80)\n\nθ\n\n(cid:34)\n\n(cid:32)\n\nE\n\nR\n\n(cid:33)\n\nγt\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\n(cid:35)\n\n− R(θ∗)\n\n≤\n\n1 T\n\n1 κ0\n\n(E∥γ0 − θ∗∥2 + κC0) +\n\nκ′ κ0\n\n· λ2\n\n1d\n\n−\n\n1 T\n\n2λ1 λ2\n\n(cid:42)\n\nT (cid:88)\n\nt=1\n\n(γt−1 − θ∗),\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n,\n\n(cid:43)\n\n(18)\n\nwhere\n\nγt =\n\n1 P\n\nht with P = |Pt|,\n\n(cid:88)\n\nk = θt + θt\n\n1 λ2\n\n10m P\n\nλ2 + β\n\nk∈Pt 1\n2 − 25β2 , λ2 2 − 25λ2β − 50β2 2 − 25β2\n\nλ2\n\nλ2\n\n,\n\nλ2 λ2 + β\n\nλ2\n\n2 − 25β2 = κ · E∥∇Lk(θ0\n\n(cid:88)\n\nk∈[m]\n\nP 2m\n\n,\n\nκ =\n\nκ0 =\n\nκ′ =\n\nC0 =\n\n2 λ2 5\nλ2 1\nm\n\nd = dim(θ).\n\n19\n\nk) − ∇Lk(θ∗)∥,\n\nUnder review as a conference paper at ICLR 2023\n\nTo prove the theorem, define variables that will be used throughout the proof.\n\n ̃θt k = arg min\n\nLk (θ) − (cid:10)∇Lk(θt−1\n\nk\n\n), θ(cid:11) +\n\nλ2 2\n\nE∥∇Lk(θt\n\nk) − ∇Lk(θ∗)∥2,\n\n(cid:13)θ − θt−1(cid:13) (cid:13) 2\n(cid:13)\n\n2 + λ1\n\n(cid:13)θ − θt−1(cid:13) (cid:13)\n\n(cid:13)1 ∀k ∈ [m]\n\nθ (cid:88)\n\nCt =\n\nεt =\n\n1 m\n\n1 m\n\nk∈[m]\n\n(cid:88)\n\nk∈[m]\n\nE∥ ̃θt\n\nk − γt−1∥2.\n\n(19)\n\n(20)\n\n(21)\n\nNote that ̃θt k optimizes the kth loss function by assuming that the kth client (k ∈ [m]) is selected at round t. It is obvious that ̃θt k if k ∈ Pt. Ct refers to the average of the expected differences between gradients of each individual model and the globally optimal model. Lastly, εt refers to the deviation of each client model from the average of local models. Remark that Ct and εt approach zero if all clients’ models converge to the globally optimal model, i.e., θt\n\nk = θt\n\nk → θ∗.\n\nThe following lemma expresses ht, how much the averaged active devices’ model deviates from the global model.\n\nLemma B.2. Algorithm 3 satisfies\n\nht =\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\n∇Lk(θt\n\nk)\n\n(22)\n\nProof. Starting from the update of ht in Algorithm 3,\n\nht = ht−1 −\n\n= ht−1 −\n\n= ht−1 −\n\nλ2 m\n\n1 m\n\n1 m\n\n(cid:88)\n\n(θt\n\nk − θt−1) −\n\nk∈[m]\n\nλ1 m\n\n(cid:88)\n\nk∈[m]\n\nsign(θt\n\nk − θt−1)\n\n(∇Lk(θt−1\n\nk\n\n) − ∇Lk(θt\n\nk) − λ1sign(θt\n\nk − θt−1)) −\n\nλ1 m\n\n(cid:88)\n\nk∈[m]\n\nsign(θt\n\nk − θt−1)\n\n(∇Lk(θt−1\n\nk\n\n) − ∇Lk(θt\n\nk)),\n\n(cid:88)\n\nk∈[m]\n\n(cid:88)\n\nk∈[m]\n\nwhere the second equality follows from (11). By summing ht recursively, we have\n\nht = h0 +\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\n∇Lk(θt\n\nk) −\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\n∇Lk(θ0\n\nk) =\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\n∇Lk(θt\n\nk).\n\nThe next lemma provides how much the average of local models changes by using only t round parameters.\n\nLemma B.3. Algorithm 3 satisfies\n\nE[γt − γt−1] =\n\n1 λ2m\n\n(cid:88)\n\nk∈[m]\n\nE[−∇Lk( ̃θt\n\nk)] −\n\nλ1 λ2m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)].\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Starting from the definition of γt,\n\nE (cid:2)γt − γt−1(cid:3) = E\n\n(cid:34)(cid:32)\n\n(cid:33)\n\n1 P\n\n(cid:88)\n\nθt\n\nk\n\nk∈Pt\n\n− θt−1 −\n\n(cid:35)\n\n1 λ2\n\nht−1\n\n(cid:34)\n\n(cid:34)\n\n(cid:34)\n\n\n\n\n\n= E\n\n= E\n\n= E\n\n= E\n\n1 P\n\n(cid:88)\n\nk∈Pt\n\n(θt\n\nk − θt−1) −\n\n(cid:35)\n\n1 λ2\n\nht−1\n\n1 λ2P\n\n1 λ2P\n\n(cid:88)\n\nk∈Pt\n\n(cid:88)\n\nk∈Pt\n\n(∇Lk(θt−1\n\nk\n\n(∇Lk(θt−1\n\nk\n\n) − ∇Lk(θt\n\nk) − λ1sign(θt\n\nk − θt−1)) −\n\n) − ∇Lk( ̃θt\n\nk) − λ1sign( ̃θt\n\nk − θt−1)) −\n\nht−1\n\n(cid:35)\n\n(cid:35)\n\n(23)\n\nht−1\n\n(24)\n\n1 λ2\n\n1 λ2\n\n1 λ2m\n\n(cid:88)\n\n(∇Lk(θt−1\n\nk\n\nk∈[m]\n\n) − ∇Lk( ̃θt\n\nk) − λ1sign( ̃θt\n\nk − θt−1)) −\n\n\n\n\n\n1 λ2\n\nht−1\n\n(25)\n\n(26)\n\n=\n\n1 λ2m\n\n(cid:88)\n\nk∈[m]\n\nE[−∇Lk( ̃θt\n\nk)] −\n\nλ1 λ2m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)],\n\nwhere (23) follows from (11), (24) follows since ̃θt randomly chosen. The last equality is due to Lemma B.2.\n\nk = θt\n\nk if k ∈ Pt, and (25) follows since clients are\n\nNext, note that Algorithm 3 is the same as that of FedDyn except for the l1-norm penalty. As this new penalty does not affect derivations of Ct, εt, and E∥γt − γt−1∥2 in FedDyn (Acar et al., 2021), we can obtain the following bounds on them. Proofs are omitted for brevity.\n\nE∥ht∥2 ≤ Ct (cid:18) P\nm\n\nCt ≤\n\n1 −\n\n(cid:19)\n\nCt−1 +\n\n2β2P m\n\nεt +\n\n4βP m\n\nE[R(γt−1) − R(θ∗)]\n\nE∥γt − γt−1∥2 ≤\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[∥ ̃θt\n\nk − γt−1∥2] = εt\n\nLemma B.4. Given model parameters at the round (t − 1), Algorithm 3 satisfies\n\nE∥γt − θ∗∥2 ≤E∥γt−1 − θ∗∥2 −\n\n2 λ2\n\nE[R(γt−1) − R(θ∗)] +\n\nβ λ2\n\nεt + E∥γt − γt−1∥2\n\n−\n\n2λ1 λ2m\n\n(γt−1 − θ∗)\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)],\n\nwhere the expectations are taken assuming parameters at the round (t − 1) are given.\n\n21\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\nUnder review as a conference paper at ICLR 2023\n\nProof.\n\nE∥γt − θ∗∥2 = E∥γt−1 − θ∗ + γt − γt−1∥2\n\n= E∥γt−1 − θ∗∥2 + 2E[(cid:10)γt−1 − θ∗, γt − γt−1(cid:11)] + E∥γt − γt−1∥2 = E∥γt−1 − θ∗∥2 + E∥γt − γt−1∥2\n\n+\n\n2 λ2m\n\n(cid:88)\n\n(cid:104)(cid:68)\n\nE\n\nk∈[m]\n\nγt−1 − θ∗, −∇Lk( ̃θt\n\nk) − λ1(sign( ̃θt\n\nk − θt−1))\n\n(cid:69)(cid:105)\n\n≤ E∥γt−1 − θ∗∥2 + E∥γt − γt−1∥2\n\n+\n\n2 λ2m\n\n+\n\n2 λ2m\n\nk∈[m]\n\n(cid:88)\n\n(cid:104)(cid:68)\n\nE\n\nk∈[m]\n\n(cid:88)\n\nE[Lk(θ∗) − Lk(γt−1) +\n\nβ 2\n\n∥ ̃θt\n\nk − γt−1∥2]\n\nγt−1 − θ∗, −λ1sign( ̃θt\n\nk − θt−1)\n\n(cid:69)(cid:105)\n\n= E∥γt−1 − θ∗∥2 + E∥γt − γt−1∥2 −\n\n2 λ2\n\nE[R(γt−1) − R(θ∗)] +\n\n−\n\n2λ1 λ2m\n\n(cid:88)\n\n(cid:104)(cid:68)\n\nE\n\nk∈[m]\n\nγt−1 − θ∗, sign( ̃θt\n\nk − θt−1)\n\n(cid:69)(cid:105)\n\n= E∥γt−1 − θ∗∥2 + E∥γt − γt−1∥2 −\n\n(cid:42)\n\n−\n\n2λ1 λ2\n\nγt−1 − θ∗,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n2 λ2\n\nE[R(γt−1) − R(θ∗)] +\n\n(cid:43)\n\nβ λ2\n\nεt\n\nβ λ2\n\nεt\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\nwhere (32) follows from Lemma B.3, (33) follows from (15), and (34) follows from the definitions of R(·) and εt.\n\nLemma B.5. Algorithm 3 satisfies\n\n(1 − 5\n\nβ2 λ2 2\n\n)εt ≤ 10\n\n1 λ2 2\n\nCt−1 + 10β\n\n1 λ2 2\n\nE[R(γt−1) − R(θ∗)] +\n\n5λ2 1\nλ2 2\n\nd\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Starting from the definitions of εt and γt,\n\n1 m\n\n1 m\n\n1 λ2 2\n\n1 λ2 2\n\nεt =\n\n=\n\n=\n\n=\n\n≤\n\n(cid:88)\n\nk∈[m]\n\n(cid:88)\n\nE∥ ̃θt\n\nk − γt−1∥2\n\nE∥ ̃θt\n\nk − θt−1 −\n\n1 λ2\n\nht−1∥2\n\nk∈[m] 1\nm\n\n(cid:88)\n\nk∈[m]\n\nE∥∇Lk(θt−1\n\nk\n\n) − ∇Lk( ̃θt\n\nk) − λ1sign(θt\n\nk − θt−1) − ht−1∥2\n\n(36)\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE∥∇Lk(θt−1\n\nk\n\n) − ∇Lk(θ∗) + ∇Lk(θ∗) − ∇Lk(γt−1)\n\n+ ∇Lk(γt−1) − ∇Lk( ̃θt\n\nk − θt−1) − ht−1∥2\n\n(cid:88)\n\nE∥∇Lk(θt−1\n\nk\n\n) − ∇Lk(θ∗)∥2 +\n\nE∥∇Lk(γt−1\n\nk\n\n) − ∇Lk(θ∗)∥2\n\nk) − λ1sign(θt 1\n(cid:88) m\n\n5 λ2 2\n\nk∈[m]\n\nE∥∇Lk( ̃θt\n\nk) − ∇Lk(γt−1)∥2 +\n\n5 λ2 2\n\nE∥λ1sign(θt\n\nk − θt−1)∥2 +\n\nE∥ht−1∥2\n\n5 λ2 2\n\n(37)\n\n5 λ2 2\n\n1 m\n\n+\n\n5 λ2 2\n\nk∈[m] 1\nm\n\n(cid:88)\n\nk∈[m]\n\n≤\n\n5 λ2 2\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE∥∇Lk(θt−1\n\nk\n\n) − ∇Lk(θ∗)∥2 +\n\n5 λ2 2\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE∥∇Lk(γt−1\n\nk\n\n) − ∇Lk(θ∗)∥2\n\n+\n\n5 λ2 2\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE∥∇Lk( ̃θt\n\nk) − ∇Lk(γt−1)∥2 +\n\n5λ2 1\nλ2 2\n\nd +\n\n5 λ2 2\n\nCt−1\n\n(38)\n\nCt−1 +\n\n5 λ2 2\n\n2β E[R(γt−1) − R(θ∗)] +\n\n5β2 λ2 2\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE∥ ̃θt\n\nk − γt−1∥2 +\n\n5λ2 1\nλ2 2\n\nd +\n\n5 λ2 2\n\nCt−1\n\n(39)\n\nCt−1 +\n\n10β λ2 2\n\nE[R(γt−1) − R(θ∗)] +\n\n5β2 λ2 2\n\nεt +\n\n5λ2 1\nλ2 2\n\nd,\n\n≤\n\n=\n\n5 λ2 2\n\n10 λ2 2\n\nwhere (36) follows from (11), (37) follows from the relaxed triangle inequality (17), (38) follows from (27), and (39) follows from the definition of Ct, the smoothness, and (16). The last equality follows from the definition of εt.\n\nAfter multiplying (28) by κ(= 10 m and scaled version of (29).\n\nP\n\n1 λ2\n\nλ2+β\n\n2−25β2 ), we obtain the following theorem by summing (B.4)\n\nλ2\n\nTheorem B.6. Given model parameters at the round (t − 1), Algorithm 3 satisfies\n\nκ0E[R(γt−1) − R(θ∗)] ≤ (E∥γt−1 − θ∗∥2 + κCt−1) − (E∥γt − θ∗∥2 + κCt) + κ\n\n(cid:42)\n\nγt−1 − θ∗,\n\n−\n\n2λ1 λ2\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n.\n\n(cid:43)\n\nP 2m\n\nλ2\n\n1\n\nwhere κ = 10 m P\nconditional expectations given model parameters at time (t − 1).\n\n2−25β2 , κ0 = 2\n\n2−25λ2β−50β2 2−25β2\n\nλ2+β\n\n1 λ2\n\nλ2\n\nλ2\n\nλ2\n\nλ2\n\n. Note that the expectations taken above are\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Summing Lemma B.4 and κ-scaled version of (28), we have\n\nE∥γt − θ∗∥2 + κCt\n\n≤ E∥γt−1 − θ∗∥2 + κCt−1 − κ\n\nP m\n\nCt−1 + κ\n\n2β2P m\n\nεt + κ\n\n−\n\n2 λ2\n\nE[R(γt−1) − R(θ∗)] +\n\nβ λ2\n\nεt + E∥γt − γt−1∥2 −\n\n4βP m\n2λ1 λ2m\n\nE[R(γt−1) − R(θ∗)]\n\n(γt−1 − θ∗)\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)].\n\nAs E∥γt − γt−1∥2 ≤ εt by (29), we have\n\nκ\n\n2β2P m\n\nεt +\n\nβ λ2\n\nεt + E∥γt − γt−1∥2 ≤ κ\n\n2β2P m\n\nεt +\n\nβ λ2\n\nεt + εt.\n\nThis can be further bounded as follows.\n\n(cid:18)\n\n(41) =\n\n10\n\nm P\n\n1 λ2 1\n\nλ2\n\nλ2 + β\n\n2 − 25β2 ·\n\n+\n\n2β2P m\n(cid:0)20(λ2 + β)β2 + β(λ2\n\nβ λ2\n\n+ 1\n\n(cid:19)\n\nεt\n\n2 − 25β2) + λ2(λ2\n\n2 − 25β2)(cid:1) εt\n\n1 − 5\n\n(cid:19)\n\nεt\n\nβ2 λ2 2\n\n=\n\n=\n\n≤\n\n2 − 25β2) (cid:18)\n\nλ2(λ2 λ2(λ2 + β) λ2 2 − 25β2 λ2(λ2 + β) λ2 2 − 25β2 P\nm\n\n(cid:18) 10 λ2 2\nβP m\n\n= κ\n\nCt−1 + κ\n\nCt−1 +\n\n10β λ2 2\n\nE[R(γt−1) − R(θ∗)] +\n\nE[R(γt−1) − R(θ∗)] + κ\n\nP 2m\n\nλ2\n\n1d,\n\n(cid:19)\n\nd\n\n5λ2 1\nλ2 2\n\nwhere the inequality follows from Lemma B.5. Then, (40) term will be\n\nE∥γt − θ∗∥2 + κCt ≤ E∥γt−1 − θ∗∥2 + κCt−1 − κ0E[R(γt−1) − R(θ∗)] + κ (cid:43)\n\n(cid:42)\n\nP 2m\n\n−\n\n2λ1 λ2\n\nγt−1 − θ∗,\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n.\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\n(40)\n\n(41)\n\nλ2\n\n1d\n\nRearranging terms, we prove the claim.\n\nNow we are ready to prove the main claim by combining all lemmas. Let us take the sum on both sides of Lemma B.6 over t = 1, . . . , T . Then, telescoping gives us\n\nκ0\n\nT (cid:88)\n\nt=1\n\nE[R(γt−1) − R(θ∗)] ≤ (E∥γ0 − θ∗∥2 + κC0) − (E∥γT − θ∗∥2 + κCT ) + T (κ\n\nλ2 1)\n\nP 2m\n\n(cid:43)\n\n−\n\n2λ1 λ2\n\n(cid:42)\n\nT (cid:88)\n\nt=1\n\nγt−1 − θ∗,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n.\n\nSince κ is positive if λ2 > 27β, we can eliminate the negative term in the middle. Then,\n\nκ0\n\nT (cid:88)\n\nt=1\n\nE[R(γt−1) − R(θ∗)] ≤ E∥γ0 − θ∗∥2 + κC0 + T (κ\n\nP 2m\n\nλ2\n\n1d)\n\n−\n\n2λ1 λ2\n\n(cid:42)\n\nT (cid:88)\n\nt=1\n\nγt−1 − θ∗,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n.\n\n(cid:43)\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nDividing by T and applying Jensen’s inequality,\n\n(cid:34)\n\nE\n\nR(\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\n(cid:35)\n\nγt) − R(θ∗)\n\n≤\n\n1 T\n\n1 κ0\n\n(E∥γ0 − θ∗∥2 + κC0) +\n\n1 κ0\n\n(κ\n\nP 2m\n\nλ2\n\n1d)\n\n−\n\n1 T\n\n2λ1 λ2\n\n(cid:42)\n\nT (cid:88)\n\nt=1\n\nγt−1 − θ∗,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n,\n\n(cid:43)\n\n(42)\n\nwhich completes the proof of Theorem B.1.\n\nB.3 DISCUSSION ON CONVERGENCE\n\nIn this section, we revisit the convergence stated in Theorem 3.1. Recall the bound\n\n(cid:34)\n\n(cid:32)\n\nE\n\nR\n\n(cid:33)\n\nγt\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\n(cid:35)\n\n− R(θ∗)\n\n≤\n\n1 T\n\n1 κ0\n\n(E∥γ0 − θ∗∥2 + κC0) +\n\n1 κ0\n\n(κ\n\nP 2m\n\nλ2\n\n1d)\n\n−\n\n1 T\n\n2λ1 λ2\n\n(cid:42)\n\nT (cid:88)\n\nt=1\n\nγt−1 − θ∗,\n\n1 m\n\n(cid:88)\n\nk∈[m]\n\nE[sign( ̃θt\n\nk − θt−1)]\n\n,\n\n(cid:43)\n\nAs we discussed in the main body, the second term is a negligible constant in the range of our hyperparameters as λ1 is of order of 10−4 or 10−6.\n\nConsider the last term where the summand is the inner product between two terms: 1) γt−1 − θ∗, the deviation of the averaged local models from the globally optimal model and 2) the average of sign vectors across clients. The deviation term characterizes how much the averaged local models are different from the global model; thus, we can assume that as training proceeds it vanishes or at least is bounded by a constant vector. To argue the average of sign vectors, assume a special case where the sign vectors sign( ̃θt k − θt−1) are IID across clients. To further simplify the argument, let us consider only a single coordinate of the sign vectors, say Xk = sign( ̃θt k(i) − θt−1(i)), and suppose Xk = ±1 with probability 0.5 each. Then, the concentration inequality (Durrett, 2019) implies that for any δ > 0,\n\n\n\n\n\n1 m\n\nP\n\n(cid:88)\n\nk∈[m]\n\nsign( ̃θt\n\nk) − θt−1 > δ\n\n  = P\n\n\n\n\n\n1 m\n\n\n\n(cid:88)\n\nXk > δ\n\n ≤ e− mδ2\n\n2\n\nk∈[m]\n\nholds, which vanishes exponentially fast with the number of clients m. Since m is large in many FL scenarios, the average of sign vectors is negligible with high probability, which in turn implies the last term is also negligible.\n\n25",
    "reference": "# Summary Of The Paper\n\nThe paper proposes FedElasticNet which applies elastic net to federated learning (FL) of deep neural nets. The paper proposes two variants based on FedAvg and FedDyn, and provides the convergence analysis of the latter. Experiments are conducted to show the benefits of the proposed methods.\n\n# Strength And Weaknesses\n\nStrength:\n1. The topic of improving the efficiency and reducing client shift in FL is important.\n\nWeakness:\n1. The novelty is not significant. Both $l_1$ and $l_2$ regularizations are mature techniques in statistics, and deep neural network training. I do not find this method very interesting. Using Lasso to get sparsity is doable, but is very tricky to tune and leads to biased gradient estimation which would hurt the model performance. Indeed, the theory (Theorem 3.1) containg a nonvanishing constant is worse than the baseline FL rate, and emprical performance might also be worse in some cases as we see from the figures. Thus, the proposed method seems not very valuable both theoretically and empirically. Also, elastic net introduces two more hyper parameters $\\lambda_1$ and $\\lambda_2$ which make the method harder to tune. Thus, much stronger motivation is needed.\n\n2. The related work section should be improved. The paper misses many recent papers on communication-efficient FL.\n\n3. The presentation is not satisfactory. \n\n(i) In Algorithm 1, why is the local update presented in this way? How do you solve that optimization problem locally? In practice we may use SGD as in Algorithm 2? Why are they inconsistent? Are we using stochastic optimziation?\n(ii) In the theory part, there is no formal statement of the assumptions. I do not know the setting of this analysis. Particularly, which part is related to the client drift? The assumptions should be stated very clearly.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nplease see above\n\n# Summary Of The Review\n\nIn general, the paper is not very well written, and the motivation and performance of the proposed algorithm is poor. Thus, I think the paper does not meet the bar of ICLR.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSLOTFORMER: UNSUPERVISED VISUAL DYNAMICS SIMULATION WITH OBJECT-CENTRIC MODELS\n\nZiyi Wu1,2, Nikita Dvornik3,1, Klaus Greff4, Thomas Kipf*4, Animesh Garg*1,2 1 University of Toronto, 2 Vector Institute, 3 Samsung AI Centre Toronto, 4 Google Research\n\nABSTRACT\n\nUnderstanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer – a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer’s dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without objectlevel labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks. Additional results and details are available at our Website.\n\n1\n\nINTRODUCTION\n\nThe ability to understand complex systems and interactions between its elements is a key component of intelligent systems. Learning the dynamics of a multi-object systems from visual observations entails capturing object instances, their appearance, position and motion, and simulating their spatiotemporal interactions. Both in robotics (Finn et al., 2016; Lee et al., 2018) and computer vision (Shi et al., 2015; Wang et al., 2017), unsupervised learning of dynamics has been a central problem due to its important practical implications. Obtaining a faithful dynamics model of the environment enables future prediction, planning and, crucially, allows to transfer the dynamics knowledge to improve downstream supervised tasks, such as visual reasoning (Chen et al., 2020b; Ding et al., 2021b), planning (Sun et al., 2022) and model-based control (Micheli et al., 2022). Yet, an effective domainindependent approach for unsupervised visual dynamics learning from video remains elusive.\n\nOne approach to visual dynamics modeling is to frame it as a prediction problem directly in the pixel space (Shi et al., 2015; Wang et al., 2017; Denton & Fergus, 2018). This paradigm builds on global frame-level representations, and uses dense feature maps of past frames to predict future features. By design, such models are object-agnostic, treating background and foreground modeling as equal. This frequently results in poorly learned object dynamics, producing unrealistic future predictions over longer horizons (Oprea et al., 2020). Another perspective to dynamics learning is through object-centric dynamics models (Kosiorek et al., 2018; van Steenkiste et al., 2018; Kossen et al., 2019). This class of methods first represents a scene as a set of object-centric features (a.k.a. slots), and then learns the interactions among the slots to model scene dynamics. It allows for more natural dynamics modeling and leads to more faithful simulation (Veerapaneni et al., 2020; Zoran et al., 2021). To achieve this goal, earlier object-centric models bake in strong scene (Jiang et al., 2019) or object (Lin et al., 2020) priors in their frameworks, while more recent methods (Kipf et al., 2020; Zoran et al., 2021) learn object interactions purely from data, with the aid of Graph Neural Networks (GNNs) (Battaglia et al., 2018) or Transformers (Vaswani et al., 2017). Yet, these approaches\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nindependently model the per-frame object interactions and their temporal evolution, using different networks. This suggests that a simpler and more effective dynamics model is yet to be designed.\n\nIn this work, we argue that learning a system’s dynamics from video effectively requires two key components: i) strong unsupervised object-centric representations (to capture objects in each frame) and ii) a powerful dynamical module (to simulate spatio-temporal interactions between the objects). To this end, we propose SlotFormer: an elegant and effective Transformer-based object-centric dynamics model, which builds upon object-centric features (Kipf et al., 2022; Singh et al., 2022), and requires no human supervision. We treat dynamics modeling as a sequential learning problem: given a sequence of input images, SlotFormer takes in the object-centric representations extracted from these frames, and predicts the object features in the future steps. By conditioning on multiple frames, our method is capable of capturing the spatio-temporal object relationships simultaneously, thus ensuring consistency of object properties and motion in the synthesized frames. We evaluate SlotFormer on four video datasets consisting of diverse object dynamics. Our method not only presents competitive results on standard video prediction metrics, but also achieves significant gains when evaluating on object-aware metrics in the long range. Crucially, we demonstrate that SlotFormer’s unsupervised dynamics knowledge can be successfully transferred to downstream supervised tasks (e.g., VQA and goal-conditional planning) to improve their performance “for free”. In summary, this work makes the following contributions:\n\n1. SlotFormer: a Transformer-based model for object-centric visual simulation;\n\n2. SlotFormer achieves state-of-the-art performance on two video prediction datasets, with\n\nsignificant advantage in modeling long-term dynamics;\n\n3. SlotFormer achieves state-of-the-art results on two VQA datasets and competitive results in one planning task, when equipped with a corresponding task-specific readout module.\n\n2 RELATED WORK\n\nIn this section, we provide a brief overview of related works on physical reasoning, object-centric models and Transformers, which is further expanded in Appendix A.\n\nDynamics modeling and intuitive physics. Video prediction methods treat dynamics modeling as an image translation problem (Shi et al., 2015; Wang et al., 2017; Denton & Fergus, 2018; Lee et al., 2018), and model changes in the pixel space. However, methods that model dynamics using global image-level features usually struggle with long-horizon predictions. Some approaches leverage local priors (Finn et al., 2016; Ebert et al., 2017), or extra input information (Walker et al., 2016; Villegas et al., 2017), which only help in the short term. More recent works improve modeling visual dynamics using explicit object-centric representations. Several works directly learn deep models in the abstracted state space of objects (Wu et al., 2015; Battaglia et al., 2016; Fragkiadaki et al., 2016; Chang et al., 2016). However, they require ground-truth physical properties for training, which is unrealistic for visual dynamics simulation. Instead, recent works use object features from a supervised detector as the base representation for visual simulation (Ye et al., 2019; Li et al., 2019; Qi et al., 2020; Yu et al., 2022) with a GNN-based dynamics model. In contrast to the above works, our model is completely unsupervised; SlotFormer belongs to the class of models that learn both object discovery and scene dynamics without supervision. We review this class of models below.\n\nUnsupervised object-centric representation learning from videos. Our work builds upon recent efforts in decomposing raw videos into temporally aligned slots (Kipf et al., 2022; Kabra et al., 2021; Singh et al., 2022). Earlier works often make strong assumptions on the underlying object representations. Jiang et al. (2019) explicitly decompose the scene into foreground and background to apply fixed object size and presence priors. Lin et al. (2020) further disentangle object features to represent object positions, depth and semantic attributes separately. Some methods leverage the power of GNNs or Transformers to eliminate these domain-specific priors (Goyal et al., 2019; 2021; Veerapaneni et al., 2020; van Steenkiste et al., 2018; Creswell et al., 2021; Zoran et al., 2021). However, they still model the object interactions and temporal dynamics with separate modules; and set the context window of the recurrent dynamics module to only a single timestep. The most relevant work to ours is OCVT (Wu et al., 2021), which also applies Transformers to slots from multiple frames. However, OCVT utilizes manually disentangled object features, and needs Hungarian matching for latent alignment during training. Therefore, it still underperforms RNN-based baselines in the video prediction task. In contrast, SlotFormer is a general Transformer-based dynamics model which is ag-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: SlotFormer architecture overview. Taking multiple video frames {xt}T t=1 as input, we first extract object slots {St}T t=1 using the pretrained object-centric model. Then, slots are linearly projected and added with temporal positional encoding. The resulting tokens are fed to the Transformer module to generate future slots { ˆST +k}K\n\nk=1 in an autoregressive manner.\n\nnostic to the underlying object-centric representations. It performs joint spatio-temporal reasoning over object slots simultaneously, enabling consistent long-term dynamics modeling.\n\nTransformers for sequential modeling. Inspired by the success of autoregressive Transformers in language modeling (Radford et al., 2018; 2019; Brown et al., 2020), they were adapted to video generation tasks (Yan et al., 2021; Ren & Wang, 2022; Micheli et al., 2022; Nash et al., 2022). To handle the high dimensionality of images, these methods often adopt a two-stage training strategy by first mapping images to discrete tokens (Esser et al., 2021), and then learning a Transformer over tokens. However, since they operate on a regular image grid, the mapping ignores the boundary of objects and usually splits one object into multiple tokens. In this work, we learn a Transformer-based dynamics model over slot-based representations that capture the entire object in a single vector, thus generating more consistent future object states as will be shown in the experiments.\n\n3 SLOTFORMER: OBJECT-ORIENTED DYNAMICS LEARNING\n\nTaking T video frames as inputs, SlotFormer first leverages a pre-trained object-centric model to extract object slots from each frame (Section 3.1). These slots are then forwarded to the Transformer module for joint spatio-temporal reasoning, and used to predict future slots autoregressively (Section 3.2). The whole pipeline is trained by minimizing the reconstruction loss in both feature and image space (Section 3.3). The overall model architecture is illustrated in Figure 1.\n\n3.1 SLOT-BASED OBJECT-CENTRIC REPRESENTATION\n\nWe build on the Slot Attention architecture to extract slots from videos due to their strong performance in unsupervised object discovery. Given T input frames {xt}T t=1, our object-centric model first extracts image features using a Convolutional Neural Network (CNN) encoder, then adds positional encodings, and flattens them into a set of vectors ht ∈ RM ×Denc , where M is the size of the flattened feature grid and Denc is the feature dimension. Then, the model initializes N slots ̃St ∈ RN ×Dslot from a set of learnable vectors (t = 1), and performs Slot Attention (Locatello et al., 2020) to update the slot representations as St = fSA( ̃St, ht). Here, fSA binds slots to objects via iterative Scaled Dot-Product Attention (Vaswani et al., 2017), encouraging scene decomposition. To achieve temporal alignment of slots, ̃St for t ≥ 2 is initialized as ̃St = ftrans(St−1), where ftrans is the transition function implemented as a Transformer encoder.\n\n3\n\n...Transformer EncoderTemporal Positional Encodingsx!x\"\"x\"#!S!SlotEncoder...S\"SlotEncoderSlotDecoderSlotDecoder$S\"#!SlotDecoder\"x\"#$\"x\"#%......$S\"#%&!FutureRolloutBurn-inFramesProjection Layer$S\"#!$S\"#$$S\"#%Published as a conference paper at ICLR 2023\n\nBefore training the Transformer-based dynamics model, we first pre-train the object-centric model using reconstruction loss on videos from the target dataset. This ensures the learned slots can accurately capture both foreground objects and background environment of the scene.\n\n3.2 DYNAMICS PREDICTION WITH AUTOREGRESSIVE TRANSFORMER\n\nOverview. Given slots {St}T t=1 extracted from T video frames, SlotFormer is able to synthesize a sequence of future slots {ST +k}K k=1 for any given horizon K. Our model operates by alternating between two steps: i) feed the slots into a Transformer that performs joint spatio-temporal reasoning and predicts slots at the next timestep, ˆSt+1, ii) feed the predicted slots back into the Transformer to keep generating future rollout autoregressively. See Figure 1 for the pipeline overview.\n\nArchitecture. To build the SlotFormer’s dynamics module, T , we adopt the standard Transformer encoder module with NT layers. To match the inner dimensionality De of T , we linearly project the input sequence of slots to a latent space Gt = Linear(St) ∈ RN ×De . To indicate the order of input slots, we add positional encoding (P.E.) to the latent embeddings. A naive solution would be to add a sinusoidal positional encoding to every slot regardless of its timestep, as done in Ding et al. (2021a). However, this would break the permutation equivariance among slots, which is a useful property of our model. Therefore, we only apply positional encoding at the temporal level, such that the slots at the same timestep receives the same positional encoding:\n\nV = [G1, G2, ..., GT ] + [P1, P2, ..., PT ], (1) where V ∈ R(T N )×De is the resulting input to the transformer T and Pt ∈ RN ×De denotes the sinusoidal positional encoding duplicated N times. As we will show in the ablation study, the temporal positional encoding enables better prediction results despite having fewer parameters.\n\nNow, we can utilize the Transformer T to reason about the dynamics of the scene. Denote the Transformer output features as U = [U1, U2, ..., UT ] ∈ R(T N )×De , we take the last N features UT ∈ RN ×De and feed them to a linear layer to obtain the predicted slots at timestep T + 1:\n\nU = T (V ),\n\nˆST +1 = Linear(UT ).\n\n(2)\n\nFor consequent future predictions, ˆST +1 will be treated as the ground-truth slots along with {St}T to predict ˆST +2. In this way, the Transformer can be applied autoregressively to generate any given number, K, of future frames, as illustrated in Figure 1.\n\nt=2\n\nRemark. The SlotFormer’s architecture allows to preserve temporal consistency among slots at different timesteps. To realize such consistency, we employ residual connections from St to ˆSt+1, which forces the Transformer T to apply refinement to the slots while preserving their absolute order. Owing to this order invariance, SlotFormer can be used to reason about individual object’s dynamics for long-term rollout, and can be seamlessly integrated with downstream task models.\n\n3.3 MODEL TRAINING\n\nError accumulation is a key issue in long-term generation (Oprea et al., 2020). In contrast to prior works (Wu et al., 2021) that use a GPT-style causal attention mask (Radford et al., 2018), SlotFormer predicts all the slots at one timestep in parallel. Therefore, we train the model using the predicted slots as inputs, simulating the autoregressive generation process at test time. This reduces the traintest discrepancy, thus improving the quality of long-term generation as shown in Section 4.5.\n\nFor training, we use a slot reconstruction loss (in L2) denoted as:\n\nLS =\n\n1 K · N\n\nK (cid:88)\n\nN (cid:88)\n\n||ˆsn\n\nT +k − sn\n\nT +k||2.\n\n(3)\n\nn=1 When using SAVi as the object-centric model, we also employ an image reconstruction loss to promote prediction of consistent object attributes such as colors and shapes. The predicted slots are decoded to images by the frozen SAVi decoder fdec, and then matched to the original frames as:\n\nk=1\n\nLI =\n\n1 K\n\nK (cid:88)\n\nk=1\n\n||fdec( ˆST +k) − xT +k||2.\n\n(4)\n\nThe final objective function is a weighted combination of the two losses with a hyper-parameter λ: L = LS + λLI . (5)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nSlotFormer is a generic architecture for many tasks requiring object-oriented reasoning. We evaluate the dynamics modeling capability of SlotFormer in three such tasks: video prediction, VQA and action planning. Our experiments aim to answer the following questions: (1) Can an autoregressive Transformer operating on slots generate future frames with both high visual quality and accurate object dynamics? (Section 4.2) (2) Are the future states synthesized by SlotFormer useful for reasoning in VQA? (Section 4.3) (3) How well can SlotFormer serve as a world model for planning actions? (Section 4.4) Finally, we perform an ablation study of SlotFormer’s components in Section 4.5.\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets. We evaluate our method’s capability in video prediction on two datasets, OBJ3D (Lin et al., 2020) and CLEVRER (Yi et al., 2019), and demonstrate its ability for downstream reasoning and planning tasks on three datasets, CLEVRER, Physion (Bear et al., 2021) and PHYRE (Bakhtin et al., 2019). We briefly introduce each dataset below, which are further detailed in Appendix B.\n\nOBJ3D consists of CLEVR-like (Johnson et al., 2017) dynamic scenes, where a sphere is launched from the front of the scene to collide with other still objects. There are 2,920 videos for training and 200 videos for testing. Following (Lin et al., 2020), we use the first 50 out of 100 frames of each video in our experiments, since most of the interactions end before 50 steps.\n\nCLEVRER is similar to OBJ3D but with smaller objects and varying entry points throughout the video, making it more challenging. For video prediction evaluation, we follow Zoran et al. (2021) to subsample the video by a factor of 2, resulting in a length of 64. We also filter out video clips where there are newly entered objects during the rollout period. For VQA task, CLEVRER provides four types of questions: descriptive, explanatory, predictive and counterfactual. The predictive questions require the model to simulate future interactions of objects such as collisions. Therefore, we focus on the accuracy improvement on predictive questions by using SlotFormer’s future rollout.\n\nPhysion is a VQA dataset containing realistic simulation of eight physical phenomena. Notably, Physion features diverse object entities and environments, making physical reasoning more difficult than previous synthetic VQA benchmarks. The goal of this dataset is to predict whether a red agent object will contact with a yellow patient object when the scene evolves. Following the official evaluation protocol, all models are first trained using unsupervised future prediction loss, then used to perform rollout on test scenes, where a linear readout model is applied to predict the answer.\n\nPHYRE is a physical reasoning benchmark consisting of 2D physical puzzles. We use the BALLtier, where the goal is to place a red ball at a certain location, such that the green ball will eventually come in contact with the blue/purple object, after the scene is unrolled in time. Following Qi et al. (2020), we treat SlotFormer as the world model and build a task success classifier on predicted object states as the scoring function. Then, we use it to rank a pre-defined 10,000 actions from Bakhtin et al. (2019), and execute them accordingly. We experiment on the within-template setting.\n\nImplementation Details. We first pre-train the object-centric model on each dataset, and then extract slots for training SlotFormer. We employ SAVi (Kipf et al., 2022) on OBJ3D, CLEVRER, PHYRE, and STEVE (Singh et al., 2022) on Physion to extract object-centric features. SAVi leverages a CNN decoder to reconstruct videos from slots, while STEVE uses a Transformer-based slot decoder. STEVE can perform scene decomposition on visually complex data, but requires more memory and training time. Besides, we discovered that vanilla SAVi cannot handle some videos on CLEVRER. So we also introduce a stochastic version of SAVi to solve this problem. Please refer to Appendix C and their papers for complete details of network architectures and hyper-parameters.\n\n4.2 EVALUATION ON VIDEO PREDICTION\n\nIn this subsection, we evaluate SlotFormer’s ability in long-term visual dynamics simulation. We train all models on short clips cropped from videos, and rollout for longer steps during evaluation.\n\nBaselines. We compare our approach with four baselines which are further described in Appendix D. We use a video prediction model PredRNN (Wang et al., 2017) that generates future frames based on global image features. To verify the effectiveness of slot representation, we train a VQ-VAE (Razavi et al., 2019) to tokenize images, and replace the slot in SlotFormer with patch tokens, denoted as VQFormer. We also adopt the state-of-the-art generative object-centric model G-SWM (Lin et al., 2020),\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Video dynamics modeling with SlotFormer as a function of future steps. (left) Visual quality of decoded frames measured with LPIPS and (right) the quality of decoded foreground object masks with mIoU.\n\nMethod\n\nOBJ3D\n\nCLEVRER\n\nPSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓\n\nPredRNN 33.68 32.94 SAVi-dyn 31.43 G-SWM VQFormer 30.71\n\nOurs\n\n32.40\n\n0.91 0.91 0.89 0.86\n\n0.91\n\n0.12 0.12 0.10 0.11\n\n0.08\n\n31.34 29.77 28.42 26.80\n\n30.21\n\n0.90 0.89 0.89 0.85\n\n0.89\n\n0.17 0.19 0.16 0.18\n\n0.11\n\nTable 1: Evaluation of visual quality on both datasets.\n\nMethod AR ↑ ARI ↑ FG-ARI ↑ FG-mIoU ↑\n\nSAVi-dyn 8.94 8.64 G-SWM 43.98 57.14\n\n64.32 49.61\n\nOurs\n\n53.14 63.45\n\n63.00\n\n18.25 24.44\n\n29.81\n\nTable 2: Evaluation of object dynamics on CLEVRER. All the numbers are in %.\n\nFigure 3: Generation results on OBJ3D. Despite higher PSNR, PredRNN and SAVi-dyn produce images with artifacts, while SlotFormer simulates sharp frames and accurate object dynamics.\n\nwhich applies heavy priors. Finally, since the PARTS (Zoran et al., 2021) code is unreleased, we incorporate their Transformer-LSTM dynamics module into SAVi (denoted as SAVi-dyn) and train the model with the same setup. We include additional baselines (Wu et al., 2021) in Appendix E.3.\n\nEvaluation Metrics. To evaluate the visual quality of the videos, we report PSNR, SSIM (Wang et al., 2004) and LPIPS (Zhang et al., 2018). As discussed in Sara et al. (2019), LPIPS captures better perceptual similarity with human than PSNR and SSIM. We focus our comparison on LPIPS, while reporting others for completeness. It is worth noting that neither of these metrics evaluate semantics in predicted frames (Yu et al., 2022). To evaluate the predicted object dynamics, we use the per-slot object masks predicted by the SAVi decoder and compare them to the ground-truth segmentation mask; same for the corresponding bounding box. We calculate the Average Recall (AR) of the predicted object boxes, and the Adjusted Rand Index (ARI), the foreground variant of ARI and mIoU termed FG-ARI and FG-mIoU of the predicted masks. We unroll the model for 44 and 42 steps on OBJ3D and CLEVRER, respectively, and report metrics averaged over timesteps.\n\nResults on visual quality. Table 1 presents the results on visual quality of the generated videos. SlotFormer outperforms all baselines with a sizeable margin in terms of LPIPS, and achieves competitive results on PSNR and SSIM. We note that PSNR and SSIM are poor metrics in this setting. For example, PredRNN and SAVi-dyn score highly in these two metrics despite producing blurry objects (see Figure 3). In contrast, SlotFormer generates objects with consistent attributes throughout the rollout, which we attribute to modeling dynamics in the object-centric space, rather than in the frames directly. This is also verified in the per-step LPIPS results in Figure 2 (left). Since Slot-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nper opt. (%) per ques. (%)\n\nMethod\n\nObs. (%) Dyn. (%) ↑ (%)\n\nDCL VRDP VRDP† Aloe∗\n\nAloe∗ + Ours\n\n90.5 91.7 94.5 93.1\n\n96.5\n\n82.0 83.8 89.2 87.3\n\n93.3\n\nTable 3: Predictive VQA on CLEVRER, reporting per-option (per opt.) and per-question (per ques.) accuracy. DCL and VRDP† both utilize pre-trained object detectors; ∗ indicates our re-implementation.\n\nHuman\n\nRPIN∗ pDEIT-lstm∗\n\nOurs\n\n74.7\n\n62.8 59.2\n\n65.2\n\n-\n\n63.8 60.0\n\n67.1\n\n-\n\n+1.0 +0.8\n\n+1.9\n\nTable 4: VQA accuracy on Physion. We report the readout accuracy on observation (OBS.) and observation plus rollout (Dyn.) frames. ↑ denotes the improvement brought by the learned dynamics. Methods marked with ∗ are our reproduced results.\n\nFigure 4: Qualitative results on CLEVRER VQA task. To answer the question “Will the green object collide with the purple cylinder?”, SlotFormer successfully simulates the first collision between the green and the brown cylinder (t = 13), which leads to the second collision between the target objects (t = 29).\n\nFormer relies on pretrained slots, the reconstructed images at earlier steps have lower quality than baselines. Nevertheless, it achieves clear advantage at longer horizon, demonstrating superior longterm modeling ability. Although VQFormer is also able to generate sharp images, it fails to predict correct dynamics and object attributes, as also observed in previous works (Yan et al., 2021; Ren & Wang, 2022). This shows that only a strong decoder (i.e. VQ-VAE) to generate realistic images is not sufficient for learning dynamics. See Appendix E.1 for more qualitative results on both datasets.\n\nResults on object dynamics. Here, we evaluate the quality of object bounding boxes and segmentation masks, decoded from the models’ future predictions. The accuracy of the predicted object boxes and segmentation masks is summarized in Table 2 (right). Since OBJ3D lacks such annotations, and PredRNN, VQFormer cannot generate object-level outputs, we exclude it from evaluation. SlotFormer achieves the best performance on AR, ARI and FG-mIoU, and competitive results on FG-ARI. SAVi-dyn scores a high FG-ARI because its blurry predictions assign many background pixels to foreground objects, while the computation of FG-ARI ignores false positives. This is verified by its poor performance in FG-mIoU which penalizes such mistakes. We also show the per-step results in Figure 2 (right) and Appendix E.2, where our method excels at all future timesteps.\n\nAttention map analysis. To study how SlotFormer leverages past information to predict the future, we visualize the self-attention maps from the Transformer T , which is detailed in Appendix E.4.\n\n4.3 VISUAL QUESTION ANSWERING\n\nIn this subsection, we show how to leverage (unsupervised) SlotFormer’s future predictions to improve (supervised) predictive question answering.\n\nOur Implementation. On CLEVRER, we choose Aloe (Ding et al., 2021a) as the base reasoning model as it can jointly process slots and texts. Aloe runs Transformers over slots from input frames and text tokens of the question, followed by an MLP to predict the answer. For predictive questions, we explicitly unroll SlotFormer and run Aloe on the predicted future slots. See Appendix C for more details. On Physion, since there is no language involved, we follow the official protocol by training a linear readout model on synthesized slots to predict whether the two objects contact. We design an improved readout model for object-centric representations, which is further detailed in Appendix C.\n\nBaselines. On CLEVRER, we adopt DCL (Chen et al., 2020b) which utilizes pre-trained object detectors and a GNN-based dynamics model. We also choose the state-of-the-art model VRDP (Ding et al., 2021b), which exploits strong environmental priors to run differentiable physics engine for rollout. We report two variants of VRDP which use Slot Attention (VRDP) or pre-trained detectors (VRDP†) to detect objects. Finally, for consistency with our results, we report the performance of our re-implemented Aloe (dubbed as Aloe∗). On Physion, we select RPIN (Qi et al., 2020) and pDEIT-lstm (Touvron et al., 2021), since they are the only two methods where the rollout improves accuracy in the benchmark (Bear et al., 2021).\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Slot decomposition on the first frame\n\n(b) Rollout results. Per-slot future predictions are color-coded.\n\nFigure 5: Qualitative results on PHYRE. The goal is to place a red ball in the first frame, so that the green ball hits the blue object after rollout. We show the per-slot rollout, where SlotFormer is able to decompose the scene into individual objects, and reason their interactions to perform accurate future synthesis.\n\nMethod\n\nRAND MEM\n\nDQN\n\nDec [Joint]\n\nRPIN\n\nDyn-DQN\n\nSAVi\n\nOurs\n\nAnnotations\n\n-\n\n-\n\nMask\n\nMask\n\nMask\n\nMask\n\n-\n\n-\n\nAUCCESS\n\n13.7±0.5\n\n2.4±0.3\n\n77.6±1.1\n\n80.0±1.2\n\n85.2±0.7\n\n86.2±0.9\n\n80.7±1.0\n\n82.0±1.1\n\nTable 5: AUCCESS on PHYRE-1B within-template setting. All baseline learning methods use ground-truth object segmentation masks, while SlotFormer is the only unsupervised technique learning from raw images.\n\nRPIN is an object-centric dynamics model using ground-truth bounding boxes. pDEIT-lstm builds LSTM over ImageNet (Deng et al., 2009) pre-trained DeiT model, learning the dynamics over frame features. Since the benchmark code for Physion is not released, we reproduce it to achieve similar or better results. We also report the Human results from the Physion paper for reference.\n\nResults on CLEVRER. Table 3 presents the accuracy on predictive questions. The dynamics predicted by SlotFormer boosts the performance of Aloe by 3.4% and 6.0% in the per option (per opt.) and per question (per ques.) setting, respectively. As a fully unsupervised dynamics model, our method even outperforms previous state-of-the-art DCL and VRDP which use supervisedly trained object detectors. On the CLEVRER public leaderboard predictive question subset, we rank first in the per option setting, and second in the per question setting. Figure 4 shows an example of our predicted dynamics, where SlotFormer accurately simulates two consecutive collision events.\n\nResults on Physion. Table 4 summarizes the readout accuracy on observation (Obs.) and observation plus rollout (Dyn.) frames. SlotFormer achieves a 1.9% improvement with learned dynamics, surpassing all the baselines. See Figure 8 in the Appendix for qualitative results.\n\n4.4 ACTION PLANNING\n\nHere, we perform goal-conditioned planning inside the SlotFormer’s learned dynamics model.\n\nOur Implementation. Since it is possible to infer the future states from only the initial configuration on PHYRE, we set the burn-in length T = 1, and apply SlotFormer to generate slots S2 from S1. Then, instead of only using S2 to generate S3, we feed in both S1 and S2 for better temporal consistency. We apply this iterative overlapping modeling technique (Ren & Wang, 2022), and set the maximum conditioning length as 6. To rank the actions during testing, we train a task success classifier on future states simulated by SlotFormer, which is detailed in Appendix C. We experiment on the within-template setting, and report the AUCCESS metric averaged over the official 10 folds. AUCCESS measures the Area Under Curve (AUC) of the task success rate vs number of attempts curve for the first 100 attempted actions. Please refer to Appendix B for more details.\n\nBaselines. We report three naive baselines from Bakhtin et al. (2019), RAND, MEM and DQN. We adopt Dec [Joint] (Girdhar et al., 2020) which employs a CNN-based future prediction model, and RPIN (Qi et al., 2020) as an object-centric dynamics model. Finally, Dynamics-Aware DQN (Ahmed et al., 2021) (dubbed Dyn-DQN) designs a task-specific loss to utilize dynamics information. Notably, all of the above methods use either ground-truth object masks or bounding boxes, while Slot-\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nPSNR ↑ SSIM ↑ LPIPS ↓\n\nMethod\n\nImproved Acc. (%)\n\nOurs (Full Model)\n\nBurn-in T = 3 Burn-in T = 4 Burn-in T = 8 Trans. Layer NT = 8 Naive P.E. Teacher Forcing No LI\n\n32.40 31.26 31.95 32.08 32.12 32.05 30.52 31.23\n\n0.91 0.88 0.89 0.90 0.89 0.90 0.87 0.88\n\n0.080 0.093 0.088 0.082 0.087 0.082 0.106 0.093\n\nOurs (Full Model)\n\nBurn-in T = 10 Rollout K = 5 Rollout K = 15 Trans. Layer NT = 4 Trans. Layer NT = 12 Naive P.E. Teacher Forcing\n\n1.9 1.0 0.5 1.9 1.3 Diverge 1.6 0.2\n\nTable 6: Ablation study on OBJ3D.\n\nTable 7: Ablation study on Physion.\n\nFormer learns scene dynamics without any object-level annotations. To show the performance gain by rollout, we also report SAVi which predicts task success purely from the initial frame’s slots.\n\nResults on action planning. We report the AUCCESS metric in Table 5. As an unsupervised dynamics model, SlotFormer achieves an AUCCESS score of 82.0, which improves the non-rollout counterpart SAVi by 1.3, and is on par with baselines that assume ground-truth object information as input. Figure 5 shows the entire rollout generated by our model. SlotFormer is able to capture objects with varying appearance, and simulate the dynamics of complex multi-object interactions.\n\n4.5 ABLATION STUDY\n\nIn this section, we perform an ablation study to examine the importance of each component in SlotFormer on OBJ3D (Table 6) and Physion (Table 7).\n\nBurn-in length T and rollout length K. By default, we set T = 6, K = 10 for OBJ3D, and T = 15, K = 10 for Physion. On OBJ3D, the model performance first improves with more input frames, and slightly drops when T further increases to 8. This might because a history length of 6 is sufficient for modeling accurate dynamics on OBJ3D. On Physion, the accuracy improves consistently as we increase T , until using all the observation frames. Besides, using 10 rollout frames strikes a balance between accuracy and computation efficiency. See Appendix E.5 for line plots of these results.\n\nTransformer (Trans.) Layer NT . By default, we set NT = 4 on OBJ3D and NT = 8 on Physion. Stacking more layers harms the performance on OBJ3D due to overfitting, while improving the accuracy on Physion. This is because the dynamics on Physion is more challenging to learn. However, further increasing NT to 12 makes model training unstable and the loss diverging.\n\nPositional encoding (P.E.). Using a vanilla sinusoidal positional encoding which destroys the permutation equivariance among slots results in small performance drop in terms of visual quality, and a clear degradation in VQA accuracy. This is not surprising, as permutation equivariance is a useful prior for object-centric scene modeling, which should be preserved.\n\nTeacher forcing. We try the teacher forcing strategy (Radford et al., 2018) by taking in groundtruth slots instead of the predicted slots autoregressively during training, which degrades the results significantly. This proves that simulating error accumulation benefits long-term dynamics modeling.\n\nImage reconstruction loss LI . As shown in the table, the auxiliary image reconstruction loss improves the quality of the generated videos drastically. As we observe empirically, LI helps preserve object attributes (e.g., color, shape), but has little effect on object dynamics. Thus, we do not apply LI on Physion, due to the large memory consumption of STEVE’s slot decoder.\n\n5 CONCLUSION\n\nIn this paper, we propose SlotFormer, a Transformer-based autoregressive model that enables consistent long-term dynamics modeling with object-centric representations. SlotFormer learns complex spatio-temporal interactions between the objects and generates future predictions of high visual quality. Moreover, SlotFormer can transfer unsupervised dynamics knowledge to downstream (supervised) reasoning tasks which leads to state-of-the-art or comparable results on VQA and goalconditioned planning. Finally, we believe that unsupervised object-centric dynamics models hold great potential for simulating complex datasets, advancing world models, and reasoning about the future with minimal supervision; and that SlotFormer is a new step towards this goal. We discuss the limitations and potential future directions of this work in Appendix F.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nAll of our methods are implemented in PyTorch (Paszke et al., 2019), and can be trained on servers with 4 modern GPUs in less than 5 days, enabling both industrial and academic researchers. To ensure the reproducibility of our work, we provide detailed descriptions of how we process each dataset in Appendix B, the implementation details and hyper-parameters of the models we use in Appendix C, and sources of the baselines we compare with in Appendix D. To facilitate future research, we will release the code of our work and the pre-trained model weights alongside the camera ready version of this paper.\n\nACKNOWLEDGMENTS\n\nWe would like to thank Wei Yu, Tianyu Hua for general advice and feedback on the paper, Xiaoshi Wu, Weize Chen for discussion of Transformer model implementation and training, and Jiaqi Xi, Ritviks Singh, Qinxi Yu, Calvin Yu, Liquan Wang for valuable discussions and support in computing resources.\n\nREFERENCES\n\nMart ́ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. TensorFlow: a system for LargeScale machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pp. 265–283, 2016.\n\nEltayeb Ahmed, Anton Bakhtin, Laurens van der Maaten, and Rohit Girdhar. Physical reasoning\n\nusing dynamics-aware models. arXiv preprint arXiv:2102.10336, 2021.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nAnton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick.\n\nPhyre: A new benchmark for physical reasoning. NeurIPS, 32, 2019.\n\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks\n\nfor learning about objects, relations and physics. NeurIPS, 29, 2016.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nDaniel Bear, Elias Wang, Damian Mrowca, Felix Jedidja Binder, Hsiao-Yu Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin A Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. In NeurIPS Datasets and Benchmarks Track, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877–1901, 2020.\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and In ECCV, pp. 213–229.\n\nSergey Zagoruyko. End-to-end object detection with transformers. Springer, 2020.\n\nMichael Chang, Tomer Ullman, Antonio Torralba, and Joshua Tenenbaum. A compositional object-\n\nbased approach to learning physical dynamics. In ICLR, 2016.\n\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\n\nGenerative pretraining from pixels. In ICML, pp. 1691–1703. PMLR, 2020a.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B Tenenbaum, and Chuang Gan. Grounding physical concepts of objects and events through dynamic visual reasoning. In ICLR, 2020b.\n\nAntonia Creswell, Rishabh Kabra, Chris Burgess, and Murray Shanahan. Unsupervised object-based\n\ntransition models for 3d partially observable environments. NeurIPS, 34, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, pp. 248–255. IEEE, 2009.\n\nEmily Denton and Rob Fergus. Stochastic video generation with a learned prior.\n\nIn ICML, pp.\n\n1174–1183. PMLR, 2018.\n\nDavid Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over\n\nlearned object embeddings enables complex visual reasoning. NeurIPS, 34, 2021a.\n\nMingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenenbaum, and Chuang Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. NeurIPS, 34, 2021b.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.\n\nFrederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with\n\ntemporal skip connections. In CoRL, pp. 344–356, 2017.\n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\n\nsynthesis. In CVPR, pp. 12873–12883, 2021.\n\nChelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction\n\nthrough video prediction. NeurIPS, 29, 2016.\n\nKaterina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive\n\nmodels of physics for playing billiards. In ICLR, 2016.\n\nRohit Girdhar, Laura Gustafson, Aaron Adcock, and Laurens van der Maaten. Forward prediction\n\nfor physical reasoning. arXiv preprint arXiv:2006.10734, 2020.\n\nRoss Girshick. Fast r-cnn. In ICCV, pp. 1440–1448, 2015.\n\nAnirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio,\n\nand Bernhard Sch ̈olkopf. Recurrent independent mechanisms. In ICLR, 2019.\n\nAnirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Sergey Levine, Charles Blundell, Yoshua Bengio, and Michael Mozer. Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems. In ICLR, 2021.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nJindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world\n\nmodels with scalable object representations. In ICLR, 2019.\n\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pp. 2901–2910, 2017.\n\nRishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matt Botvinick, Alexander Lerchner, and Chris Burgess. Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition. NeurIPS, 34, 2021.\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\n\nbidirectional transformers for language understanding. In NAACL, pp. 4171–4186, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nThomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models.\n\nIn ICLR, 2020.\n\nThomas Kipf, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video. In ICLR, 2022.\n\nAdam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat:\n\nGenerative modelling of moving objects. NeurIPS, 31, 2018.\n\nJannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, and Kristian Kersting. Structured\n\nobject-aware physics prediction for video modeling and planning. In ICLR, 2019.\n\nAlex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine.\n\nStochastic adversarial video prediction. arXiv preprint arXiv:1804.01523, 2018.\n\nYunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B Tenenbaum, Antonio Torralba, and Russ Tedrake. In ICRA, pp. 1205–\n\nPropagation networks for model-based control under partial observation. 1211. IEEE, 2019.\n\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. In ICLR, 2019.\n\nZhixuan Lin, Yi-Fu Wu, Skand Peri, Bofeng Fu, Jindong Jiang, and Sungjin Ahn. Improving gen-\n\nerative imagination in object-centric world models. In ICML, pp. 6140–6149. PMLR, 2020.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp. 10012– 10022, 2021.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. NeurIPS, 33:11525–11538, 2020.\n\nVincent Micheli, Eloi Alonso, and Francois Fleuret. Transformers are sample efficient world models.\n\narXiv preprint arXiv:2209.00588, 2022.\n\nShanka Subhra Mondal, Taylor Whittington Webb, and Jonathan Cohen. Learning to reason over\n\nvisual objects. In ICLR, 2023.\n\nCharlie Nash, Jo ̃ao Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, and Peter Battaglia. Transframer: Arbitrary frame prediction with generative models. arXiv preprint arXiv:2203.09494, 2022.\n\nSergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, and Antonis Argyros. A review on deep learning techniques for video prediction. TPAMI, 2020.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. NeurIPS, 32, 2019.\n\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets\n\nfor 3d classification and segmentation. In CVPR, pp. 652–660, 2017.\n\nHaozhi Qi, Xiaolong Wang, Deepak Pathak, Yi Ma, and Jitendra Malik. Learning long-term visual\n\ndynamics with region proposal interaction networks. In ICLR, 2020.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding by generative pre-training. 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\n\nvq-vae-2. NeurIPS, 32, 2019.\n\nXuanchi Ren and Xiaolong Wang. Look outside the room: Synthesizing a consistent long-term 3d\n\nscene video from a single image. In CVPR, pp. 3563–3573, 2022.\n\nAlvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and control. In ICML, pp. 4470–4479. PMLR, 2018.\n\nAdam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. NeurIPS, 30, 2017.\n\nUmme Sara, Morium Akter, and Mohammad Shorif Uddin. Image quality assessment through fsim, ssim, mse and psnr—a comparative study. Journal of Computer and Communications, 7(3):8–18, 2019.\n\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. NeurIPS, 28, 2015.\n\nGautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose. In ICLR, 2021.\n\nGautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for com-\n\nplex and naturalistic videos. NeurIPS, 2022.\n\nKarl Stelzner, Robert Peharz, and Kristian Kersting. Faster attend-infer-repeat with tractable proba-\n\nbilistic models. In ICML, pp. 5966–5975. PMLR, 2019.\n\nJiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou, and Animesh Garg. Plate: Visuallygrounded planning with transformers in procedural tasks. IEEE Robotics and Automation Letters, 7(2):4924–4930, 2022.\n\nGuy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Ex-\n\nposing human motion generation to clip space. In ECCV, pp. 358–374. Springer, 2022.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In ICML, pp. 10347–10357. PMLR, 2021.\n\nSjoerd van Steenkiste, Michael Chang, Klaus Greff, and J ̈urgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. In ICLR, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017.\n\nRishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. In CoRL, pp. 1439–1456. PMLR, 2020.\n\nRuben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long-term future via hierarchical prediction. In ICML, pp. 3560–3569. PMLR, 2017.\n\nJacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting\n\nfrom static images using variational autoencoders. In ECCV, pp. 835–851. Springer, 2016.\n\nYunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. Predrnn: Recurrent\n\nneural networks for predictive learning using spatiotemporal lstms. NeurIPS, 30, 2017.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\n\nfrom error visibility to structural similarity. TIP, 13(4):600–612, 2004.\n\nNicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual interaction networks: Learning a physics simulator from video. NeurIPS, 30, 2017.\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nJiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. NeurIPS, 28, 2015.\n\nJiajun Wu, Joseph J Lim, Hongyi Zhang, Joshua B Tenenbaum, and William T Freeman. Physics In BMVC, volume 2, pp. 7,\n\n101: Learning physical object properties from unlabeled videos. 2016.\n\nYi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Generative video transformer: Can objects be the words?\n\nIn ICML, pp. 11307–11318. PMLR, 2021.\n\nZiyi Wu, Nikita Dvornik, Klaus Greff, Jiaqi Xi, Thomas Kipf, and Animesh Garg. Slotformer: Long-term dynamic modeling in object-centric models. In UAI 2022 Workshop on Causal Representation Learning, 2022.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, pp. 10524–10533. PMLR, 2020.\n\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\n\nvq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.\n\nCharig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video\n\nobject segmentation by motion grouping. In ICCV, pp. 7177–7188, 2021.\n\nYufei Ye, Maneesh Singh, Abhinav Gupta, and Shubham Tulsiani. Compositional video prediction.\n\nIn ICCV, pp. 10353–10362, 2019.\n\nKexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer: Collision events for video representation and reasoning. In ICLR, 2019.\n\nWei Yu, Wenxin Chen, Songheng Yin, Steve Easterbrook, and Animesh Garg. Modular action\n\nconcept grounding in semantic video prediction. In CVPR, pp. 3605–3614, 2022.\n\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\n\neffectiveness of deep features as a perceptual metric. In CVPR, pp. 586–595, 2018.\n\nDaniel Zoran, Rishabh Kabra, Alexander Lerchner, and Danilo J Rezende. Parts: Unsupervised segmentation with slots, attention and independence maximization. In ICCV, pp. 10439–10447, 2021.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL RELATED WORK\n\nPhysical reasoning for dynamics modeling. Instead of explicitly encoding physical laws to deep models and estimating latent variables from inputs (Wu et al., 2015; 2016), recent approaches implicitly infer them by modeling object interactions in the scene (Chang et al., 2016; Battaglia et al., 2018; 2016; Sanchez-Gonzalez et al., 2018; Li et al., 2019). VIN (Watters et al., 2017) employs a CNN to encode video frames into a multi-channel 1D tensor, where each channel represents an object. It enforces a fixed mapping between object identity and feature channel, which cannot generalize to different number of objects with varying appearance. CVP (Ye et al., 2019) leverage object bounding boxes to crop input image and apply CNNs to extract object-centric representations. Since object features are extracted from raw image patches, it ignores the context information and thus cannot model the interactions between objects and the environment. RPIN (Qi et al., 2020) instead uses RoIPooling (Girshick, 2015) to extract object features from image feature maps, which contains background information. However, these methods rely on ground-truth object-level annotations for training, which are often unavailable. In contrast, SlotFormer pre-trains unsupervised object-centric models on unlabeled videos. It ensures accurate decomposition of foreground objects and background environment, laying the foundation for building powerful dynamics models.\n\nDynamics modeling in object-centric representation learning. R-NEM (van Steenkiste et al., 2018) is the first end-to-end object-centric model to reason about objects and their interactions from pixel observations alone. It extracts object features from raw observations and uses an interaction function in the form of a GNN to model interactions. SCALOR (Jiang et al., 2019) scales the SQAIR (Kosiorek et al., 2018) model to work on scenes with multiple moving objects. It introduces a background module to model the image background separately. It also equips each object with a depth property to handle occlusions. STOVE (Kossen et al., 2019) incorporates a GNNbased dynamics model into SuPAIR (Stelzner et al., 2019) to reason object interactions, where object representations are explicitly disentangled into positions, velocities and appearance. Similarly, OP3 (Veerapaneni et al., 2020) learns pairwise relationship between objects based on a symmetric assumption. G-SWM (Lin et al., 2020) combines the key properties of the above methods and proposes a unified framework for accurate dynamics prediction. A hierarchical latent modeling technique is utilized to handle the multi-modality of the scene dynamics. Leveraging the power of Transformers, OAT (Creswell et al., 2021) directly learns to align slots extracted from each frame to gain temporal consistency and perform slot interactions. However, the temporal dynamics is still modeled by an LSTM (Hochreiter & Schmidhuber, 1997) module. Similarly, PARTS (Zoran et al., 2021) employs the same Transformer-LSTM module from OAT. It utilizes the Slot-Attention (Locatello et al., 2020) mechanism to detect objects and relies on a fixed independent prior to achieve stable future rollout performance. OCVT (Wu et al., 2021) is the most relevant work to SlotFormer. It also applies Transformer over slots from multiple frames and performs future prediction in an autoregressive manner. However, OCVT still disentangles its underlying object features into position, depth and semantic information. It also relies on a Hungarian matching algorithm to achieve temporal alignment of slots. As a result, OCVT is inferior to G-SWM in terms of future rollout. Compared to previous works, SlotFormer is a general Transformer-based dynamics model that is agnostic to the object-centric representations it builds upon. It does not assume any explicit disentanglement of the object property, while still can handle the object interactions well. Without the use of RNNs or GNNs, we achieve state-of-the-art dynamics modeling ability.\n\nTransformers. With the prevalence of Transformers in the NLP field (Vaswani et al., 2017; Kenton & Toutanova, 2019), there have been tremendous efforts in introducing it to computer vision tasks (Dosovitskiy et al., 2020; Carion et al., 2020; Liu et al., 2021). Our method is highly motivated by previous works in Transformer-based autoregressive image and video generation (Esser et al., 2021; Chen et al., 2020a; Yan et al., 2021; Nash et al., 2022; Ren & Wang, 2022; Wu et al., 2022) and their applications in reasoning (Ding et al., 2021a; Mondal et al., 2023). VQGAN (Esser et al., 2021) first pretrains a VQ-VAE (Razavi et al., 2019) that can map images to discrete tokens and tokens back to images. Then, a GPT-like Transformer model is trained to autoregressively predict the input tokens for image generation. Transframer (Nash et al., 2022) instead discretizes video frames using Discrete Cosine Transform (DCT), and learns an autoregressive Transformer over these sparse representations from multiple frames. The design of SlotFormer is mostly related to (Ren & Wang, 2022), which also uses image tokens from multiple frames to enable consistent long-term view synthesis. Different from these works, our mapping step maps images to object-centric representations, preserving the identity of objects and is independent of the input image resolution.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB DATASET DETAILS\n\nOBJ3D (Lin et al., 2020). The videos in this dataset are generated by first placing 3 to 5 static objects in the scene, and then launching a sphere from the front of the scene to collide with those objects. Compared to CLEVRER, the objects in OBJ3D occupy more pixels in images, have less collisions and occlusions, and are all visible in the scene at the beginning of the videos.\n\nCLEVRER (Yi et al., 2019). The videos in this dataset contain static or moving objects at the beginning, and there will be various new objects entering the scene from random directions throughout the video. The smaller size and more diverse interactions of objects make CLEVRER more challenging than OBJ3D. We obtain the ground-truth segmentation masks from their official website, which are used to generate object bounding boxes. We calculate the Average Recall (AR) with an IoU threshold of 50% for the predicted object boxes and the Adjusted Rand Index (ARI) for the object masks. We also report a variant of ARI and mIoU which only focus on foreground objects termed FG-ARI and FG-mIoU as done in the SAVi paper (Kipf et al., 2022).\n\nAs a Visual Question Answering (VQA) dataset, CLEVRER consists of four types of questions generated by template-based programs, namely, descriptive, explanatory, predictive and counterfactual. The latter three types of questions are multiple-choice questions, where the VQA model needs to classify whether each choice is correct.\n\nPhysion (Bear et al., 2021). This dataset contains eight physical scenarios, each falls under a common physical phenomenon, such as rigid- and soft-body collisions, falling, rolling and sliding motions. The foreground objects used in the simulation vary in categories, textures, colors and sizes. It also uses diverse background as the scene environment, and randomize the camera pose in rendering the videos. Overall, this dataset presents more complex visual appearance and object dynamics compared to other synthetic VQA datasets. Therefore, we apply the recently proposed powerful object-centric model, STEVE (Singh et al., 2022), to extract slots on this dataset.\n\nPhysion splits the videos into three sets, namely, Training, Readout Fitting and Testing. We truncate all videos by 150 frames as most of the interactions end before that, and sub-sample the videos by a factor of 3 for training the dynamics model. Following the official evaluation protocol, the dynamics models are first trained on videos from the Training set under future prediction loss. Then, they observe the first 45 frames of videos in the Readout Fitting and Testing set, and perform rollout to generate future scene representations (e.g. feature maps for image-based dynamics models, or object slots for SlotFormer). A linear readout model is trained on observed and rollout scene representations from the Readout Fitting set to classify whether the two cued objects (one in red and one in yellow) contact. Finally, the classification accuracy of the trained readout model on the Testing set scene representations is reported. Please refer to their paper (Bear et al., 2021) for detailed description of the evaluation.\n\nPHYRE (Bakhtin et al., 2019). We study the PHYRE-B tier in this paper, which consists of 25 templates of tasks. Tasks within the same template share similar initial configuration of the objects. There are two evaluation settings, namely, within-template, where training and testing tasks come from the same templates, and cross-template, where train-test tasks are from different templates. We simulate the videos in 1 FPS as done in previous works (Bakhtin et al., 2019; Qi et al., 2020).\n\nIn our preliminary experiment, we discovered that SAVi usually fails to detect objects in light color (e.g. the green and gray balls). We hypothesize that this is because the L2 norm of light colors is close to white (the color of background), so pixels in light colors receive gradients of small magnitude during optimization, leading to worse segmentation results. Since improving object-centric models is not the focus of this paper, we choose a simple workaround by changing the background color from white to black.\n\nTo solve a task at test time, models need to determine the size of a red ball and the location to put it in the scene, such that the green ball touches the blue/purple object for more than 3 seconds after the scene evolves. Following (Qi et al., 2020), we train models to score a pre-defined 10,000 actions when applied to the current task (by rendering the red ball in the scene), and execute them according to the ranking. The evaluation metric, AUCCESS, is a weighted sum of the Area Under Curve (AUC) of the success percentage vs number of attempts curve for the first 100 attempts. See their paper (Bakhtin et al., 2019) for detailed explanation of the metrics.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nSlot Model\n\nTransformer\n\nDataset\n\nOBJ3D\n\nCLEVRER\n\nPhysion\n\nPHYRE\n\nBase Model Image Resolution Number of Slots N Slot Size Dslot Batch Size Training Steps\n\nBurn-in Steps T Rollout Steps K Latent Size De Number of Layers NT Loss Weight λ Batch Size Training Steps\n\nSAVi 64 × 64 6\n128 64 80k\n\n6 10 128 4\n1 128 200k\n\nSAVi 64 × 64 7\n128 64 200k\n\n6 10 256 4\n1 128 500k\n\nSTEVE 128 × 128 6\n192 48 460k\n\nSAVi 128 × 128 8\n128 64 370k\n\n15 10 256 8\n0 128 250k\n\n1 10 256 8\n0 64 300k\n\nTable 8: Variations in model architectures and training settings on different datasets.\n\nFigure 6: Illustration for missing objects of vanilla SAVi on CLEVRER videos. There are two objects at the beginning of this video (top). When the red cube enters the scene, all 4 empty slots attend to this object, resulting in object sharing (middle). When another object enters the scene from the top right corner, SAVi does not have empty slots to detect it (bottom). As a result, this object is ignored by the model.\n\nC IMPLEMENTATION DETAILS\n\nWe provide more implementation details of our method in this section. Table 8 lists the hyperparameters used in our experiments to facilitate reproduction.\n\nSAVi. We reproduce the unconditional version of SAVi in PyTorch (Paszke et al., 2019) to perform unsupervised object discovery. Specifically, we use the same CNN encoder, decoder, Slot Attention based corrector and Transformer based predictor as their experiments on CATER, except on PHYRE the spatial broadcast size of the decoder is 16 × 16 to better capture small objects. The slot size is 128 and the training video clip length is 6 on all the datasets. We adopt Adam (Kingma & Ba, 2015) as the training optimizer. We use the same warmup and decay learning rate schedule which first linearly increases from 0 to 2 × 10−4 for the first 2.5% of the total training steps, and then decrease to 0 in a cosine annealing strategy. We perform gradient clipping with a maximum norm of 0.05.\n\nStochastic SAVi. As stated in the main paper, vanilla SAVi sometimes fails to capture newly entered objects in a video, and we detail the reason and our solution as follows. We use 7 slots for SAVi on CLEVRER which has a maximum of 6 objects in the scene. Imagine a video with 4 objects {Oi}4 i=1 at the beginning. Let us assume SAVi captures the objects in the first 4 slots and the background in the 5th slot. This leads to two empty slots s6 and s7, which are very similar with L2 distance ||s6 − s7||2 generally smaller than 0.05. Consequently, when there is a new object O5 enters the scene, s6 and s7 will both attend to it, resulting in object sharing between slots. Now, if there is another object O6 entering the scene, there will be no ”free” slot to detect this new object. Therefore, O6 will be ignored by SAVi, until one of the previous object leaves the scene. An example is shown in Figure 6. This issue occurs only on CLEVRER because all the objects are presented in videos from the beginning in other datasets. Besides, SAVi did not experiment on datasets with multiple newly entered objects 1, and thus they did not observe such problem.\n\n1confirmed with the authors of SAVi\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFrom our analysis, the issue stems from the similarity of empty slots, which is because of the permutation equivariance of slots. To break the symmetry, we introduce stochasticity to slots initialized from previous timestep. Specifically, we modify the slot transition function by applying a two-layer MLP with Layer Normalization (Ba et al., 2016) to predict the mean and log variance of ̃St+1:\n\nt+1) = MLP(ftrans(St)). Then, we sample from this distribution to get ̃St+1 ∼ N (μt+1, log σ2 tion with visual features at frame t + 1.\n\n(μt+1, log σ2\n\n(6)\n\nt+1) for performing Slot Atten-\n\nTo enforce this stochasticity, we apply a KL divergence loss on the predicted distribution. Since we do not regularize the mean of ̃St+1, the loss only penalizes the log variance with a prior value ˆσ:\n\nt+1) || N (μt+1, log ˆσ2))\n\nLt+1\n\nKL = DKL(N (μt+1, log σ2 ˆσ σt+1\n\n= log\n\nt+1\n\n+\n\nσ2 2 · ˆσ2 − which will be averaged over all input timesteps. We set ˆσ = 0.1 which produces enough randomness to break the symmetry without destroying the temporal alignment of slots. With this simple modification, we can detect all the objects throughout the video. We use the same strategy as SAVi to train the stochastic SAVi model on CLEVRER under a combination of the frame reconstruction loss and the KL divergence loss, where the later one is weighted by a factor of 1 × 10−4.\n\n1 2\n\n(7)\n\n,\n\nSTEVE. We reproduce the 128 × 128 input resolution version of STEVE. Different from the paper, we adopt a two-stage training strategy by first pre-training a discrete VAE (Singh et al., 2021) to convert images into patches tokens, and then train the slot model to reconstruction these tokens. We found this strategy lead to more stable training in our experiments. Other training settings are the same as their original implementation.\n\nTransformer. We follow BERT (Kenton & Toutanova, 2019) to implement our model by stacking multiple transformer encoder blocks. The number of self-attention head is 8 and the hidden size of FFN is 4 × De. We adopt the Pre-LN Transformer (Xiong et al., 2020) design as we empirically find it easier to optimize. We train our model using the Adam optimizer. The initial learning rate is 2 × 10−4 and decayed to 0 in a cosine schedule. We also adopt a linear learning rate warmup strategy during the first 5% of training steps. We do not apply gradient clipping or weight decay during training. On OBJ3D and CLEVRER, we apply both the slot reconstruction loss LS and image reconstruction loss LI for training. On Physion, we do not apply LI due to the large memory consumption of STEVE’s Transformer-based decoder. Similarly, we do not apply LI on PHYRE since the image resolution is 128 × 128 and the spatial broadcast size of the SAVi decoder is set as 16 × 16, which consumes lots of GPU memory.\n\nVQA model on CLEVRER. To jointly process object slots and question texts, we employ Aloe (Ding et al., 2021a) as the base VQA model given its strong performance on CLEVRER. Given a video and a question, Aloe first leverages pre-trained object-centric model to extract slots from each frame, and a text tokenizer to convert questions to language tokens. Then, it concatenates slots and text tokens, and forward them to a reasoning module, which is a stack of NAloe Transformer encoder, to perform joint reasoning and predict the answer.\n\nWe re-implement Aloe in PyTorch. Following their training settings, we reproduce the results with a smaller Transformer reasoning module, NAloe = 12, while the original implementation uses NAloe = 28. This is because we use SAVi which produces higher quality and temporally consistent slots compared to the MO-Net (Burgess et al., 2019) they used. When integrating with SlotFormer, we unroll our dynamics model to predict slots at future timesteps, and feed them to Aloe to answer predictive questions. For other types of questions, the process is the same as the original Aloe.\n\nReadout model on Physion. Permutation equivariance is an important property of object-centric models, which is also preserved by SlotFormer. Simply concatenating slots and forwarding it through a fully-connected (FC) layer degrades the performance, since the prediction changes according to input slot orders. To build a compatible readout model, we leverage the max-pooling operation which is invariant to the input permutations (Qi et al., 2017). Besides, to better utilize the object-centric representation, we draw inspiration from Relation Networks (Santoro et al., 2017) to explicitly reason over pairs of objects. Specifically, we concatenate every two slots and apply FC on it, then the outputs are max-pooled over all pairs of slots and time to obtain the final prediction.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nIn our experiments, we discovered that training readout models on the entire rollout videos (150 frames) leads to severe overfitting. This is because of the error accumulation issue in long-term video prediction, where the model overfits to artifacts introduced at later timesteps. Therefore, we only fit the readout network to the first 75 frames of the video. We evaluate baselines with the same readout model and training setting for fair comparison, which also improves their performance.\n\nTask success classifier on PHYRE. We train a task success classifier to score an action for the current task. Specifically, we concatenate predicted slots with a learnable CLS token, add temporal positional encoding, and process them using a Transformer encoder. Then, we apply a two-layer MLP to output the score from the features corresponding to the CLS token. Such design also ensures the predicted score is invariant to the order of input slots.\n\nD BASELINES\n\nWe detail our implementation of baselines in this section.\n\nPredRNN (Wang et al., 2017) is a famous video prediction model leveraging spatio-temporal LSTM to model scene dynamics via global frame-level features. We adopt the online official implementation 2. The models are trained until convergence for 16 epochs and 6 epochs on OBJ3D and CLEVRER, respectively. We adopt the same training settings as their original paper.\n\nVQFormer. To show the effectiveness of object-centric representations, we design a baseline that replace the object slots with Vector Quantized (VQ) patch tokens. We first pre-train a VQ-VAE (Razavi et al., 2019) on video frames to convert patches to discrete tokens. We adopt the implementation of VQ-VAE from VQ-GAN (Esser et al., 2021) 3, where we set the number of tokens per-frame as 4 × 4 = 16, and the codebook size as 4096. The autoregressive Transformer follows the design in SlotFormer. We also tried the GPT-like training strategy (i.e. causal masking) as done in Micheli et al. (2022), but did not observe improved performance.\n\nG-SWM (Lin et al., 2020) unifies several priors in previous object-centric models and is shown to achieve good results on various simple video datasets. It constructs a background module to process the scene context, disentangles object features to positional and semantic information, explicitly models occlusion and interaction using depth and GNN module, and performs hierarchical latent modeling to deal with the multi-modality over time. We use the online official implementation 4. We train the model for 1M steps on both datasets, and select the best weight via the loss on the validation set. Our re-trained model achieves slightly better results than their pretrained weight on the OBJ3D dataset. Therefore, we also adopt the this training setting on CLEVRER.\n\nSAVi-dyn. Since neither the code of PARTS (Zoran et al., 2021) nor its testing dataset (PLAYROOM) is released, and PARTS did not try future prediction task on CLEVRER, we try our best to re-implement it to compare with SlotFormer under our settings. Inspired by its design, we replace the Transformer predictor in SAVi (Kipf et al., 2022) with the Transformer-LSTM dynamics module in PARTS. The model is trained to observe initial burn-in frames, and then predict the slots as well as the reconstructed image of the rollout frames using the dynamics module. We use a learning rate of 1 × 10−4 and train the model for 500k steps. The other training strategies follow SAVi.\n\nWe do not compare with OCVT (Wu et al., 2021) because it underperforms G-SWM even on simple 2D datasets, while SlotFormer outperforms G-SWM under all the settings.\n\nAloe (Ding et al., 2021a) runs Transformers over object slots and text tokens of the question to perform reasoning. The official code 5 was written in TensorFlow (Abadi et al., 2016), so we reimplement it in PyTorch to fit in our codebase. We adopt the same model architecture and hyperparameters as the original paper, except that we use 12 layer Transformer encoder while they use 28, as our SAVi slot representations are more powerful than their MO-Net (Burgess et al., 2019) slots. We train the model for 250k steps.\n\nRPIN (Qi et al., 2020) is an object-centric dynamics model using ground-truth object bounding boxes of the burn-in frames. For a fair comparison on Physion dataset with SlotFormer, we also\n\n2https://github.com/thuml/predrnn-pytorch 3https://github.com/CompVis/taming-transformers 4https://github.com/zhixuan-lin/G-SWM 5https://github.com/deepmind/deepmind-research/tree/master/object attention for reasoning\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Generation results on OBJ3D (top) and CLEVRER (bottom). On the right, we report metrics measuring the visual quality and object trajectory of the visualized rollouts for each model.\n\nFigure 8: Qualitative results on Physion VQA task. To answer the question “Will the red object contact with the yellow object?”, SlotFormer successfully simulates the falling of the red box. The low visual quality of the predicted frames is due to STEVE’s Transformer-based decoder, which is not designed for pixel-space reconstruction. Nevertheless, they still preserve the correct motion of objects.\n\napply our improved readout model (see Appendix C) on RPIN. We adopt the online official implementation 6 and train it on Physion dataset for 300k steps. As shown in Table 4, our reproduced readout accuracy is much higher than the reported result in the benchmark.\n\npDEIT-lstm applies an LSTM over frame features extracted by ImageNet (Deng et al., 2009) pretrained DEIT (Touvron et al., 2021) model. We follow the original implementation and use the DEIT model provided by timm (Wightman, 2019) (deit base patch16 224). We frozen the DEIT model and train the LSTM for 100k steps.\n\nFor other baselines, we simply copy the numbers from previous papers.\n\nE MORE EXPERIMENTAL RESULTS\n\nE.1 QUALITATIVE RESULTS\n\nVideo prediction. Figure 7 (top) shows additional qualitative results on OBJ3D. SlotFormer achieves excellent generation of the object trajectories thus very low LPIPS score. However, its\n\n6https://github.com/HaozhiQi/RPIN\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Comparison of the object dynamics of the generated videos at each rollout step on CLEVRER. We report FG-API (left) and FG-mIoU (right) of the segmentation masks.\n\nMethod\n\nDCL VRDP VRDP† Aloe Aloe∗\n\nAloe∗ + Ours\n\nPredictive per opt. (%) per ques. (%)\n\n90.5 91.7 94.5 93.5 93.1\n\n96.5\n\n82.0 83.8 89.2 87.5 87.3\n\n93.3\n\nTable 9: Predictive VQA on CLEVRER, reporting per-option (per opt.) and per-question (per ques.) accuracy. DCL and VRDP† both utilize pre-trained object detectors; ∗ indicates our re-implementation.\n\nMethod\n\nObs. (%) Dyn. (%) ↑ (%)\n\nHuman\n\nRPIN RPIN∗ pDEIT-lstm pDEIT-lstm∗\n\nOurs\n\n74.7\n\n54.3 62.8 59.9 59.2\n\n65.2\n\n-\n\n55.7 63.8 60.5 60.0\n\n67.1\n\n-\n\n+ 1.4 + 1.0 + 0.6 + 0.8\n\n+ 1.9\n\nTable 10: VQA accuracy on Physion. We report the readout accuracy on observation (OBS.) and observation plus rollout (Dyn.) frames. ↑ denotes the improvement brought by the learned dynamics. Methods marked with ∗ are our reproduced results.\n\nPSNR and SSIM are still close to PredRNN and SAVi-dyn, which blurs the moving objects into the background in later frames. This again proves that LPIPS are superior metrics for measuring the generated videos. Besides, G-SWM can also preserve the object identity because it leverages complex priors such as depth to model occlusions. Nevertheless, its simulated dynamics are still worse than our Transformer model. Finally, VQFormer is able to generate sharp images without blurry objects because of its strong VQ-VAE decoder. However, the object properties such as colors are not consistent, and the simulated dynamics are erroneous.\n\nWe present a visual result on CLEVRER in Figure 7 (bottom). The objects are smaller in size and have longer term dynamics, making it much harder than OBJ3D. PredRNN and SAVi-dyn still generate blurry objects at later steps. G-SWM sometimes cannot detect objects newly entering the scene because of the limited capacity of its discovery module. In contrast, SlotFormer builds Transformer on SAVi slots, enabling both accurate object detection and precise dynamics modeling. This is also verified by the object-aware metrics AR and mIoU we show in the figures.\n\nSee Figure 13 and Figure 14 for more qualitative results in the video prediction task.\n\nVQA. Figure 8 shows a qualitative result on Physion dataset, where SlotFormer successfully synthesizes the contact of the red object and the yellow ground. Note the low quality of the predicted frames is due to the STEVE’s Transformer-based decoder, which is not designed for pixel-space reconstruction 7. Improving the decoder design is beyond the scope of this paper.\n\n7In our experiments, even the reconstruction results of STEVE on Physion videos are of low quality.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nDescriptive\n\nExplanatory\n\nPredictive per opt. per ques. per opt. per ques. per opt. per ques.\n\nCounterfactual\n\nAverage\n\nAloe∗ + Ours\n\n95.17\n\n98.04\n\n94.79\n\n96.50\n\n93.29\n\n90.63\n\n73.78\n\n89.26\n\nTable 11: Accuracy on different questions and average results on CLEVRER. Numbers are in %.\n\nMethod\n\nCollide Contain Dominoes Drape Drop Link Roll Support Average\n\nSlotFormer\n\n69.3\n\n63.3\n\n55.6\n\n66.7\n\n62.7\n\n69.3 70.7\n\n77.3\n\n67.1\n\nTable 12: Accuracy breakdown for all eight scenarios on Physion. Numbers are in %.\n\nMethod\n\n0\n\n1\n\n2\n\n3\n\nFold ID 5\n4\n\n6\n\n7\n\n8\n\n9\n\nAverage\n\nSlotFormer\n\n83.1\n\n83.2\n\n81.0\n\n81.2\n\n81.2\n\n83.0\n\n82.6\n\n80.0\n\n83.0\n\n81.8\n\n82.0±1.1\n\nTable 13: AUCCESS for all 10 folds on PHYRE.\n\nMethod\n\nOBJ3D\n\nCLEVRER\n\nPSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓\n\nOCVT 31.08 Slot-LSTM 32.15\n\nOurs\n\n32.40\n\n0.88 0.90\n\n0.91\n\n0.13 0.09\n\n0.08\n\n27.96 29.79\n\n30.21\n\n0.87 0.88\n\n0.89\n\n0.18 0.13\n\n0.11\n\nTable 14: Evaluation of visual quality on both datasets.\n\nMethod\n\nAR ↑ ARI ↑ FG-ARI ↑ FG-mIoU ↑\n\n36.19 51.23 OCVT Slot-LSTM 48.52 59.58\n\n40.87 58.42\n\nOurs\n\n53.14 63.45\n\n63.00\n\n20.57 27.84\n\n29.81\n\nTable 15: Evaluation of object dynamics on CLEVRER. All the numbers are in %.\n\nE.2 QUANTITATIVE RESULTS\n\nVideo prediction. We show the per-step FG-ARI and FG-mIoU results in Figure 9. The sophisticated priors in G-SWM prevents it from scaling to scenes with multiple objects and complex dynamics. Since SAVi-dyn generates blurry objects, it produces many false positives in the segmentation masks. Instead, SlotFormer preserves the object identity and achieves high scores in both metrics over long rollout steps.\n\nVQA. Table 9 and Table 10 present the complete results of the original and our reproduced performance of baselines, as well as ours on both VQA datasets. We report the performance of our Aloe with SlotFormer model on all four question types of CLEVRER in Table 11. We report the per-scenario accuracy of SlotFormer on Physion rollout setting in Table 12.\n\nPlanning. Table 13 shows the AUCCESS of SlotFormer for all 10 folds on PHYRE.\n\nE.3 COMPARISON WITH ADDITIONAL BASELINES\n\nIn this section, we compare SlotFormer with two additional baselines, namely OCVT (Wu et al., 2021) and Slot-LSTM, in the video prediction task on OBJ3D and CLEVRER datasets.\n\nOCVT builds Transformers over SPACE (Lin et al., 2019) which applies heavy priors in their framework. In addition, it requires a Hungarian alignment step of slots for loss computation. Therefore, OCVT underperforms G-SWM (Lin et al., 2020) in long-term generation. Since its code is not released, we reproduce it based on SPACE 8, and adopt its setting in the video prediction task.\n\nAs shown in Table 14 and Table 15, OCVT underperforms SlotFormer in both visual quality of videos and accuracy of object dynamics. This is because object slots from SAVi is more powerful than SPACE, and SlotFormer naturally enjoys the temporal alignment of slots.\n\nSlot-LSTM trains a Transformer-LSTM dynamics module from PARTS (Zoran et al., 2021) over the same pre-trained object slots as SlotFormer. We adopt the same Transformer module as SlotFormer, but only feed in slots at a single timestep, thus only modeling the spatial interaction of objects. The Transformer is followed by a per-slot LSTM to learn the temporal dynamics.\n\nAs shown in Table 14 and Table 15, SlotFormer outperforms Slot-LSTM in all the metrics, especially on the more challenging CLEVRER dataset. This indicates the importance of joint spatial-temporal reasoning over a larger context window. Despite having the same Transformer module, Slot-LSTM\n\n8https://github.com/zhixuan-lin/SPACE\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Example attention map visualization on OBJ3D (left) and CLEVRER (right). Our model takes in slots from {xi}6 i=1 (column 1-6) to predict slots of ˆx7 (column 7). We show images at the first row and the per-slot future reconstructions at the rightmost column. The body of the table shows the per-slot attention of SlotFormer when predicting ˆS7, with the arrows pointing at the regions of high importance for predicting the future slot in the same row. Zoom in for better viewing.\n\nlimits the context window of its recurrent module to only a single timestep. Therefore, it still generates videos with blurry objects and inconsistent dynamics over the long horizon.\n\nE.4 ATTENTION ANALYSIS\n\nIn this section, we analyze the visual cues in the input frames that SlotFormer utilizes to make future predictions. We do so by visualizing the attention map from the last self-attention layer in the transformer T . More precisely, given the last T encoded frames {St}T t=1, we are predicting the future slots ˆST +1. Denote the attention scores from ˆsi t,j, where i, j ∈ [1, N ] and N is the number of slots. At each timestep t and for each future slot i, we obtain spatial attention maps oi\n\nt over input frames xi as a weighted combination of the slot reconstructions as follows:\n\nT +1 to sj\n\nt as ai\n\noi\n\nt =\n\nN (cid:88)\n\nj=1\n\nai\n\nt,j · (mj\n\nt ⊙ yj\n\nt ),\n\n(8)\n\nwhich indicates the regions of xt SlotFormer attends upon when predicting ˆsi\n\nT +1.\n\nFigure 10 (left) presents one example from OBJ3D, where the purple cube just collided with the purple sphere, and is about to hit the yellow sphere. When predicting the purple cube, the model focuses on the past collision event in {xi}4 i=1, and highlights the yellow sphere in x6. For the purple sphere, the Transformer only looks at the purple cube because it will not hit the yellow sphere. Since the yellow sphere becomes heavily occluded in x6, SlotFormer attends to earlier frames, while predicting its future motion based on the purple cube. This indicates that SlotFormer can handle occlusions or disappearing of objects during burn-in frames by attending to other timesteps where the objects are visible, and using that information to infer the properties and motion of objects. Finally, the red cylinder merely looks at itself because it is not involved in the collisions.\n\nFigure 10 (right) illustrates one example from CLEVRER. We only analyze the left side of the images since there is no object interaction in the right part. There are two collision events (the purple cylinder hitting the orange cylinder, and the yellow sphere hitting the blue cube) happening in x7, and SlotFormer successfully captures their interactions in the attention maps. In general, we found the attention maps in CLEVRER less clear than those in OBJ3D, due to the smaller object size. Nevertheless, the Transformer can still detect correct cues to reason their future motion.\n\nE.5 ABLATION STUDY\n\nFigure 11 shows the effect of burn-in and rollout length on SlotFormer’s performance as line plot for better clarity. On OBJ3D, the LPIPS first improves as we use more burn-in frames, and then degrades after reaching a peak at T = 6. We do not ablate the rollout length as it is fixed according to the evaluation setting. On Physion, the accuracy gain increases consistently with more burn-in frames as it provides more context information. Therefore, we choose the maximum length T = 15 according to the number of observed frames available at test time. Finally, the accuracy grows as we use more rollout frames during training, and plateaus after K = 10.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Ablation study on burn-in and rollout length of SlotFormer. We show the LPIPS of generated videos on OBJ3D (left), and the improved VQA accuracy by rollout on Physion (middle, right).\n\nFigure 12: Videos generated by SlotFormer on CLEVRER where some objects are not visible at the initial frame, but enters the scene during burn-in frames (marked by red arrows). SAVi is able to detect these new objects, and SlotFormer can still simulate accurate future dynamics for these objects.\n\nE.6 NEW OBJECTS DURING BURN-IN FRAMES\n\nOne concern regarding SlotFormer is that, if some objects are not visible at the first timestep, but enters the scene during burn-in frames, can our model still be able to simulate their dynamics? Figure 12 shows a few examples with new objects appearing during burn-in steps on the CLEVRER dataset. Since the object-centric model SAVi is able to detect these new objects, SlotFormer can still reason their dynamics based on frames where they are visible, and simulate accurate future states. This is an important property of SlotFormer as disappearing and (re-)appearing of objects are common in real world videos.\n\nF LIMITATIONS AND FUTURE WORKS\n\nLimitations. SlotFormer currently builds upon pretrained object-centric models. This family of methods still fail to scale up to real world data 9, preventing our application to real world videos as well. Besides, the two-stage training strategy harms the model performance at the early rollout\n\n9STEVE (Singh et al., 2022) can work on real world videos such as traffics, where ground usually shares the same color, and looks distinct from the vehicles. Also, STEVE’s Transformer-based slot decoder cannot generate images of high visual quality on complex datasets.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nsteps as shown in Figure 2. It is interesting to explore joint training of the base object-centric model and the Transformer dynamics module, which could potentially benefit the performance of both models. Finally, current SlotFormer model works in a deterministic manner, and thus cannot model the uncertainty of future dynamics, which is common in real world videos.\n\nFuture Works. We only experiment on unconditional future prediction in this paper. In the future, we plan to extend SlotFormer to conditional generation tasks, such as action-conditioned generation as done in Zoran et al. (2021). Recent works have shown success in this direction by converting conditional inputs to tokens and feeding them to the Transformer (Ren & Wang, 2022; Tevet et al., 2022). Another direction is to simplify the training process by learning scene decomposition and temporal dynamics jointly. This may allow the object-centric model to leverage long-term motion cues for unsupervised object discovery (Yang et al., 2021). Finally, it is important to enable SlotFormer to learn the multi-modality of future dynamics for stochastic video prediction. This is key to modeling real world videos faithfully (Oprea et al., 2020).\n\nFigure 13: More qualitative results on OBJ3D.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: More qualitative results on CLEVRER.\n\n26",
    "reference": "# Summary Of The Paper\n\nThis paper introduces an approach for learning to perform high-quality long-term video prediction via an object-centric latent bottleneck. The approach hinges on two key ideas. The first is to use a pre-trained temporal object discovery method to extract a sequence of aligned slots. The second is to use an autoregressive Transformer to rollout future slots conditioned on the sequence of aligned past slots. This enables the model to combine spatial and (multi-step) temporal information to improve the long-term accuracy of future slot rollouts. Extensive experiments exploring design choices, video prediction quality, and object-centric reasoning capabilities validate the efficacy of the approach.\n\n# Strength And Weaknesses\n\nStrengths\n=====\n- The paper makes progress towards answering the difficult question of how to design an effective unsupervised approach for long-term video prediction capable of dealing with complex object-centric dynamics.\n- The dynamics model has an elegant and simple design as a basic autoregressive transformer architecture. It appears to be simple enough that it is likely to be useful as a starting point for future methods that tackle more advanced tasks. For example, the dynamics module is shown to be capable of integrating with both SAVi and STEVE.\n- The experiments are thorough; the paper uses proper baselines and metrics, multiple relevant multi-object video environments, and validates the design choices with strong empirical results.\n- The paper provides a “bonus” insight I found particularly intriguing --- that a strong decoder (VQ-VAE) is not sufficient for learning complex multi-object dynamics.\n\nWeaknesses\n=====\nI found just a few minor weaknesses to point out:\n- It was a bit odd that OCVT was discussed as the most relevant closest work, yet results for this baseline were not provided. If I understand correctly from the appendix related work section, it consistently underperformed G-SWM on all benchmarks? Even so, it would be good to include results for OCVT.\n- The simplicity of the approach hinges on the fact that all objects are visible at time step 1 and no new objects appear. This is a fairly important topic to discuss in the paper as this is limiting in terms of applying this approach to real-world video. What would it take to support handling occlusion and appearing/disappearing objects within the burn-in frames?\n- Another weakness of the approach is that it is not trained end-to-end with the object discovery module which makes the training process cumbersome, and does not allow the object discovery to take advantage of long-term motion cues.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- I believe the combination of the two ideas of 1) extracting aligned slots from video with a pre-trained object discovery module and 2) treating the problem as a sequence modeling task produces a novel approach for the task of object-centric video prediction\n- I think the reproducibility of this work overall is relatively low because of the amount of compute required (4 GPUs in less than 5 days). I would encourage the authors to release trained model weights when open-sourcing their code. However, the authors make a good effort to provide extensive experiment details in the appendix and the overall method is fairly simple + builds on top of existing methods (e.g., SAVi). \n- Overall the paper is very well written. Moving the limitations section from the appendix to the main text is important, as well as expanding it to discuss, e.g., handling occlusion, + object appearance/disappearance within the burn-in frames, and deterministic vs. stochastic future prediction.\n- Adding a description for SAVi, STEVE, and Aloe in the main text would help the readability of the paper. Also, I was missing the definition of the AUCCESS metric in the main text.\n\n# Summary Of The Review\n\nOverall, I believe this paper contributes valuable insights as well as a useful method for the considered problem. I believe it is ready for publication in its current state.\n\n---\nUpdate after rebuttal: Maintaining my initial positive stance on the paper and recommend acceptance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nIMAGE TO SPHERE: LEARNING EQUIVARIANT FEATURES FOR EFFICIENT POSE PREDICTION\n\nDavid M. Klee, Ondrej Biza, Robert Platt, Robin Walters Northeastern University {klee.d, biza.o, r.platt, r.walters}@northeastern.edu\n\nABSTRACT\n\nPredicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in SO(3). However, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages SO(3) equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at https://dmklee.github.io/image2sphere.\n\n1\n\nINTRODUCTION\n\nDetermining the pose of an object from an image is a challenging problem with important applications in artificial reality, robotics, and autonomous vehicles. Traditionally, pose estimation has been approached as a point regression problem, minimizing the error to a single ground truth 3D rotation. In this way, object symmetries are manually disambiguated using domain knowledge (Xiang et al., 2018) and uncertainty is not accounted for. This approach to pose estimation cannot scale to the open-world setting where we wish to reason about uncertainty from sensor noise or occlusions and model novel objects with unknown symmetries.\n\nRecent work has instead attempted to learn a distribution over poses. Single rotation labels can be modeled as random samples from the distribution over object symmetries, which removes the need for injecting domain knowledge. For instance, a table with front-back symmetry presents a challenge for single pose regression methods, but can be effectively modeled with a bimodal distribution. The drawback to learning distributions over the large space of 3D rotations is that it requires lots of data, especially when modeling hundreds of instances across multiple object categories.\n\nThis poor data efficiency can be improved by constraining the weights to encode symmetries present in the problem (Cohen & Welling, 2016). The pose prediction problem exhibits 3D rotational symmetry, e.g. the SO(3) abstract group. That is, if we change the canonical reference frame of an object, the predictions of our model should transform correspondingly. For certain input modalities, such as point clouds or 3D camera images, the symmetry group acts directly on the input data via 3D rotation matrices. Thus, many networks exploit the symmetry with end-to-end SO(3) equivariance to achieve sample efficient pose estimation. However, achieving 3D rotation equivariance in a network trained on 2D images is less explored.\n\nThus, we present Image2Sphere, I2S, a novel method that learns SO(3)-equivariant features to represent distributions over 3D rotations. Features extracted by a convolutional network are projected from image space onto the half 2-sphere. Then, spherical convolution is performed on the features with a learned filter over the entire 2-sphere, resulting in a signal that is equivariant to 3D rotations. A final SO(3) group convolution operation produces a probability distribution over SO(3) parameterized in the Fourier domain. Our method can be trained to accurately predict object orientation and correctly express ambiguous orientations for objects with symmetries not specified at training time.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nI2S achieves state-of-the-art performance on the PASCAL3D+ pose estimation dataset, and outperforms all baselines on the ModelNet10-SO(3) dataset. We demonstrate that our proposed architecture for learning SO(3) equivariant features from images empirically outperforms a variety of sensible, alternative approaches. In addition, we use the diagnostic SYMSOL datasets to show that our approach is more expressive than methods using parametric families of multi-modal distributions at representing complex object symmetries.\n\nContributions:\n\n• We propose a novel hybrid architecture that uses non-equivariant layers to learn SO(3)-\n\nequivariant features which are further processed by equivariant layers.\n\n• Our method uses the Fourier basis of SO(3) to more efficiently represent detailed distribu-\n\ntions over pose than other methods.\n\n• We empirically demonstrate our method is able to describe ambiguities in pose due to partial\n\nobservability or object symmmetry unlike point estimate methods.\n\n• I2S achieves SOTA performance on PASCAL3D+, a challenging pose estimation benchmark\n\nusing real-world images.\n\n2 RELATED WORK\n\n6D Pose Estimation Predicting the 6D pose (e.g. 3D position and orientation) of objects in an image has important applications in fields like robotics (Tremblay et al., 2018), autonomous vehicles (Geiger et al., 2012), and microscopy (Levy et al., 2022). Most popular methods use deep convolutional networks, which are robust to occlusions and can handle multi-object scenes with a segmentation module (He et al., 2017). Convolutional networks have been trained using a variety of output formulations. Xiang et al. (2018) regresses the 3D bounding box of the object in pixel space, while He et al. (2020) predicts 3D keypoints on the object with which the pose can be extracted. Another line of work (Wang et al., 2019; Li et al., 2019; Zakharov et al., 2019) outputs a dense representation of the object’s coordinate space. Most of these methods are benchmarked on datasets with limited number of object instances (Hinterstoisser et al., 2011; Xiang et al., 2018). In contrast, our method is evaluated on datasets that have hundreds of object instances or novel instances in the test set. Moreover, our method makes minimal assumptions about the labels, requiring only a 3D rotation matrix per image regardless of underlying object symmetry.\n\nRotation Equivariance Symmetries present in data can be preserved using equivariant neural networks to improve performance and sample efficiency. For the symmetry group of 3D rotations, SO(3), a number of equivariant models have been proposed. Chen et al. (2021) and Fuchs et al. (2020) introduce networks to process point cloud data with equivariance to the discrete icosahedral group and continuous SO(3) group, respectively. Esteves et al. (2019b) combines images from structured viewpoints and then performs discrete group convolution to classify shapes. Cohen et al. (2018a) introduces spherical convolution to process signals that live on the sphere, such as images from 3D cameras. However, these methods are restricted to cases where the SO(3) group acts on the input space, which prevents their use on 2D images. Falorsi et al. (2018) and Park et al. (2022) extract 3D rotational equivariant features from images to model object orientation, but were limited to simplistic datasets with a single object. Similar to our work, Esteves et al. (2019a) learns SO(3) equivariant embeddings from image input for object pose prediction; however, they use a supervised loss to replicate the embeddings of a spherical convolutional network pretrained on 3D images. In contrast, our method incorporates a novel architecture for achieving SO(3) equivariance from image inputs that can be trained end-to-end on the challenging pose prediction tasks.\n\nUncertainty over SO(3) Due to object symmetry or occlusion, there may be a set of equivalent rotations that result in the same object appearance, which makes pose prediction challenging. Most early works into object pose prediction have avoided this issue by either breaking the symmetry when labelling the data (Xiang et al., 2014) or applying loss functions to handle to known symmetries (Xiang et al., 2018; Wang et al., 2019). However, this approach requires knowing what symmetries are present in the data, and does not work for objects that have ambiguous orientations due to occlusion (e.g. coffee mug when the handle is not visible). Several works have proposed models to reason about orientation uncertainty by predicting the parameters of von Mises (Prokudin et al.,\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of our proposed model, Image2Sphere (I2S). First, output image features of a pre-trained ResNet are orthographically projected to the sphere. We convolve the features Ψ with a learned filter on S2 to generate a signal on SO(3) (represented by vector field on the sphere). A final SO(3) group convolution is performed to produce a detailed distribution over 3D rotations, allowing I2S to learn object symmetries and represent uncertainty during pose estimation.\n\n2018), Fisher (Mohlin et al., 2020), and Bingham (Gilitschenski et al., 2019; Deng et al., 2022) distributions. However, multi-modal distributions required for modeling complex object symmetries which can be difficult to train. Recent work by Murphy et al. (2021) leverages an implicit model to produce a non-parametric distribution over SO(3) that can model objects with large symmetry groups. Our method also generates a distribution over SO(3) that can model symmetric objects, but uses SO(3)-equivariant layers to achieve higher accuracy and sample efficiency.\n\n3 BACKGROUND\n\n3.1 EQUIVARIANCE\n\nEquivariance formalizes what it means for a map f to be symmetry preserving. Let G be a symmetry group, that is, a set of transformations that preserves some structure. Assume G acts on spaces X and Y via Tg and T ′ g , respectively. For all g ∈ G, a map f : X → Y is equivariant to G if\n\nf (Tgx) = T ′\n\ng f (x).\n\n(1)\n\nThat is, if the input of f is transformed by g, the output will be transformed in a corresponding way. Invariance is a special case of equivariance when T ′ g is the identity map (i.e. applying group transformations to the input of an invariant mapping does not affect the output).\n\n3.2 GROUP CONVOLUTION OVER HOMOGENEOUS SPACES\n\nThe group convolution operation is a linear equivariant mapping which can be equipped with trainable parameters and used to build equivariant neural networks (Cohen & Welling, 2016). Convolution is performed by computing the dot product with a filter as it is shifted across the input signal. In standard 2D convolution, the shifting corresponds to a translation in pixel space. Group convolution (Cohen & Welling, 2016) generalizes this idea to arbitrary symmetry groups, with the filter transformed by elements of the group. Let G be a group and X be a homogeneous space, i.e. a space on which G acts transitively, for example, X = G or X = G/H for a subgroup H. We compute the group convolution between two functions, f, ψ : X → Rk, as follows:\n\n[f ⋆ ψ](g) =\n\n(cid:88)\n\nx∈X\n\nf (x) · ψ(T −1\n\ng x).\n\n(2)\n\nNote that the output of the convolution operation is defined for each group element g ∈ G, while the inputs, f and ψ, are defined over a homogeneous space of G. By parameterizing either f or ψ using trainable weights, group convolution may be used as a layer in an equivariant model.\n\n3.3 SO(3)–EQUIVARIANCE\n\nFor reasoning about physical objects, such as pose detection or part segmentation, it is desirable to learn functions that are equivariant to the group SO(3) of 3D rotations. In order to build SO(3)- equivariant neural networks using group convolution, it is necessary to efficiently parameterize the space of signals {f : X → R} over the homogenous spaces X = S2 or X = SO(3). Cohen et al. (2018b) give an effective and compact solution in terms of the truncated Fourier transform. Analogous to the Fourier basis over the circle, it is possible to give a Fourier basis for signals defined over S2\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nin terms of the spherical harmonics Y l coefficients Dl frequency l ≤ L we obtain an approximate representation f (g) ≈ (cid:80)L\n\nk and for signals over SO(3) in terms of Wigner D-matrix mn and then truncating to a certain mn(g).\n\nmn. Writing f : SO(3) → R in terms of the Dl\n\nmnDl\n\n(cid:80)2l+1\n\n(cid:80)2l+1 m=0\n\nn=0 cl\n\nl=0\n\nSO(3) group convolution (2) can be efficiently computed in the Fourier domain using the convolution theorem1. Namely, the convolution of two functions is calculated as the element-wise product of the functions in the Fourier domain. For functions over SO(3), the Fourier domain is described by the Wigner D-matrix coefficients, corresponding to a block diagonal matrix with a (2l + 1) × (2l + 1) block for each l (Knapp, 1996). A functions over S2 in the Fourier domain is described by a vector of coefficients of spherical harmonics and convolution is performed using an outer product (Cohen et al., 2018a). This generates the same block diagonal matrix output as SO(3) convolution (e.g. both S2 and SO(3) convolution generate signals that live on SO(3)). See Geiger et al. (2022) for efficient implementation of Fourier and inverse Fourier transforms of S2 and SO(3) signals.\n\n3.4 SO(3) DISTRIBUTIONS WITH THE FOURIER BASIS\n\nThe output of the above group convolution is a signal f : SO(3) → R defined as a linear combination of Wigner D-matrices. While this signal is useful for efficient group convolution, it cannot be directly normalized to produce a probability distribution. Instead, the signal can be queried at discrete points in SO(3) and then normalized using a softmax. To reduce errors introduced by the discretization, points should be taken from an equivolumetric grid. Equivolumetric grids over SO(3) can be generated using an extension of the HEALPix method developed by Yershova et al. (2010). The HEALPix method (Gorski et al., 2005) starts with 12 ‘pixels’ that cover the sphere, and recursively divides each pixel into four sections. The latitude and longitude of each pixel describe two Euler angles of a rotation. The extension to SO(3) specifies the final Euler angle in a way that the grid is equally-spaced in SO(3). The resolution of the final grid can be controlled by the number of recursions.\n\n4 METHOD\n\nOur method, Image2Sphere (I2S), is designed to predict object pose from images under minimal assumptions. Much previous work assumes objects with either no symmetry or known symmetry, and thus it is sufficient to output a single point estimate of the pose. Our method, in contrast, outputs a distribution over poses, which allows us to represent uncertainty due to partial observability and the inherent ambiguity in pose resulting from object symmetries. Recent work using distribution learning has suffered from difficulty in training multi-modal distributions and high data requirements. I2S circumvents these challenges by reasoning about uncertainty in the Fourier basis of SO(3), which is simple to train over and allows us to leverage equivariant layers for better data efficiency.\n\nThe I2S network consists of an encoder, a projection step, and a 2-layer spherical pose predictor (see Figure 1). I2S generates SO(3) equivariant features from traditional convolutional network encoders. Our method then maps features from image space to the 2-sphere using an orthographic projection operation. Then, we perform two spherical convolution operations. Importantly, the output of the spherical convolution is a signal over the Fourier basis of SO(3), which can be queried in the spatial domain to create highly expressive distributions over the space of 3D rotations.\n\n4.1 MAPPING FROM IMAGE TO SPHERE\n\nWe use orthographic projection to map the output image feature map f : R2 → Rh from the ResNet to a spherical signal Ψ : S2 → Rh. Orthographic projection P : S2 → R2 defined P (x, y, z) = (x, y) maps a hemisphere onto the unit disk. By positioning the feature map f around the unit disk before projecting, we get a localized signal Ψ(x) = f (P (x)) supported over one hemisphere. The advantage of this method is that it preserves the spatial information of features in the original image. Practically, to compute Ψ, a HEALPix grid is generated over a hemisphere {xi} ⊂ S2 and mapped to positions {P (xi)} in the image feature map. The value of Ψ(xi) is interpolated from the value of f at pixels near position P (xi). This process is illustrated in Figure 2.\n\n1See Cohen et al. (2018a) for explanation and visuals describing efficient SO(3) group convolution.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nWe then move Ψ to the frequency domain using fast Fourier transform to express Ψ as a linear combination of spherical harmonics. Operating in the frequency domain allows us to efficiently convolve continuous signals over the sphere (S2) and the group of 3D rotations (SO(3)). We truncate the Fourier series for Ψ at frequency L as Ψ(x) ≈ (cid:80)L k (x). This truncation may cause sampling error if the spatial signal has high frequency. We mitigate this error by: (1) tapering the magnitude of the projected features toward the edge of the image to avoid a discontinuity on the 2-sphere, and (2) performing dropout of the HEALPix grid such that a random subset of grid points is used for each projection.\n\nk=0 cl\n\n(cid:80)2l+1\n\nkY l\n\nl=0\n\n(a)\n\n(b)\n\nFigure 2: Projection of features from image space to the 2-sphere. (a) orthographic projection links pixels in image space to points on the sphere using vectors orthogonal to the image plane; (b) dense feature map, visualized as an RGB image, is mapped to a signal over S2 by orthographically projecting onto all visible grid points, resulting in a signal that is non-zero over half the 2-sphere.\n\n4.2 LEARNING SO(3) EQUIVARIANT FEATURES\n\nOnce the learned features are projected to the sphere, we use operations that preserve the 3D symmetry of the representation. First, the spherical signal Ψ is processed with an S2-equivariant group convolution operation. Unlike traditional convolutional layers that use locally supported filters, we elect to use a filter with global support. Intuitively, we view this operation similarly to template matching where the projected hemisphere signal Ψ is the template and we are looking for the rotation transformation that results in the best match with the filter. Conveniently, using a globally supported filter gives a global receptive field after one layer, allowing for a shallower network. This is important since each spherical convolution operation is fairly computation and memory intensive due to the high band limit L necessary to model potentially complex distributions on the output.\n\nAfter the S2-convolution, we perform one SO(3)-equivariant group convolution before outputting the probability density. Using a locally supported filter, this operation serves as a refinement step and results in slightly higher prediction accuracy (see comparison in Appendix C.2). Following Spherical CNN (Cohen et al., 2018a), we apply non-linearities between convolution layers by mapping the signal to the spatial domain, applying a ReLU, and then mapping back to the Fourier domain.\n\n4.3 LOSS FUNCTION\n\nThe output of the spherical convolution operation is a signal f : SO(3) → R≥0 that indicates the probability that the image corresponds to a particular object orientation. This output is queried using an equivolumetric grid over SO(3) then normalized with a softmax operation to produce a categorical distribution, which can be optimized using a cross-entropy loss. We find that this loss is effective even when training on images with ambiguous orientation or symmetric objects. For these cases, the model learns to predict high probabilities for the set of equivalently likely orientations. Additionally, we find that the model can accurately predict likelihoods for grids of higher resolution than it was trained on (e.g. train with a grid resolution of 5 degrees and test with a resolution of 2.5 degrees).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n5 EXPERIMENTS\n\n5.1 DATASETS\n\nTo demonstrate the strengths of our method, we evaluate it on several challenging object orientation estimation datasets. Additional details can be found in Appendix B.2.\n\nThe first dataset, ModelNet10-SO(3) (Liao et al., 2019), is composed of rendered images of synthetic, untextured objects from ModelNet10 (Wu et al., 2015). The dataset includes 4,899 object instances over 10 categories, with novel camera viewpoints in the test set. Each image is labelled with a single 3D rotation matrix, even though some categories, such as desks and bathtubs, can have an ambiguous pose due to symmetry. For this reason, the dataset presents a challenge to methods that cannot reason about uncertainty over orientation.\n\nNext, PASCAL3D+, (Xiang et al., 2014), is a popular benchmark for pose estimation that includes real images of objects from 12 categories. The dataset labels symmetric object categories in a consistent manner (e.g. bottles are symmetric about the z-axis, so label zero rotation about the z-axis), which simplifies the task. To improve accuracy, we follow the common practice of augmenting the training data with synthetic renderings from Su et al. (2015). Nevertheless, PASCAL3D+ still serves as a challenging benchmark due to the high variability of natural textures and presence of novel instances in the test set.\n\nLastly, the SYMSOL dataset was recently introduced by Murphy et al. (2021) to evaluate the expressivity of methods that model distributions over 3D rotations. It includes synthetic renderings of objects split into two groups: geometric objects like the tetrahedron or cylinder with complex symmetries (SYMSOL I), and simple objects with single identifying feature such that the pose is ambiguous when the feature is occluded (SYMSOL II). The dataset provides the full set of equivalent rotation labels for each image so methods can be evaluated on how well they capture the distribution. Note that Murphy et al. (2021) generates results using 100k renderings per shape, which we report as a baseline, but the publicly released dataset that we train on only includes 50k renderings per shape.\n\n5.2 EVALUATION METRICS\n\nThe goal of pose prediction is to minimize the angular error between the predicted 3D rotation and the ground truth 3D rotation. Two commonly used metrics are the median rotation error (MedErr) and the accuracy within a rotation error threshold (e.g. Acc@15 is the fraction of predictions with 15 degrees or less rotation error). However, these metrics assume that there exists a single, ground truth 3D rotation, which is not valid for symmetric objects or images with pose ambiguity. For ModelNet10-SO(3) and PASCAL3D+, only a single rotation is provided so these metrics must be used. However, when the full set of equivalent rotation labels are provided, like with SYMSOL, a more informative measure is the average log likelihood, computed as the expected log likelihood that the model, pθ, assigns to rotations sampled from the distribution of equivalent rotations pGT : ER∼pGT [log pθ(R|x)]. Achieving high log likelihood requires modelling all symmetries of an object.\n\n5.3 NETWORK AND TRAINING DETAILS\n\nI2S uses a residual network (He et al., 2016) with weights pretrained on ImageNet (Deng et al., 2009) to extract dense feature maps from 2D images. We use a ResNet50 backbone for ModelNet10-SO(3) and SYMSOL, and ResNet101 for PASCAL3D+. The orthographic projection uses a HEALPix grid with recursion level of 2, out of which 20 points are randomly selected during each forward pass. We parameterize the learned S2 filter in the Fourier domain, e.g. learn weights for each spherical harmonic. The filter in the SO(3) convolutional layer is locally supported over rotations up to 22.5 degrees in magnitude. We query the signal in the spatial domain using SO(3) HEALPix grids with 36k points (7.5 degree spacing) during training and 2.4M (1.875 degree spacing) during evaluation. We use the same maximum frequency (L = 6) on all datasets. Additional details on the architecture can be found in Appendix B.1.\n\nI2S is instantiated with PyTorch and the e3nn library (Geiger et al., 2022). It is trained using SGD with Nesterov momentum of 0.9 for 40 epochs using a batch size of 64. The learning rate starts at 0.001 and decays by factor of 0.1 every 15 epochs.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n5.4 BASELINES\n\nWe compare our method against competitive baselines including regression methods and distribution learning methods. Zhou et al. (2019) and Br ́egier (2021) predict valid 3D rotation matrices using differentiable orthonormalization processes, Gram-Schmidt and Procrustes, respectively, that preclude discontinuities on the rotation manifold. Liao et al. (2019) and Mahendran et al. (2018) use unique classification-regression losses to predict rotation. Prokudin et al. (2018) represents rotation uncertainty with a mixture of von Mises distributions over each Euler angle, while Mohlin et al. (2020) predicts the parameters for a matrix Fisher distribution. Gilitschenski et al. (2019) and Deng et al. (2022) both predict multi-modal Bingham distributions. Lastly, Murphy et al. (2021) trains an implicit model to generate a non-parametric distribution over 3D rotations. All baselines use the same sized, pretrained ResNet encoders for each experiment. Results are from the original papers when available.\n\n5.5 MODELNET10-SO3\n\nWe report performance on ModelNet10-SO(3) in Table 1. Our method outperforms all baselines on median error and accuracy at 15 degrees. Moreover, I2S achieves the lowest error on nine out of ten categories, and reports less than 5 degrees of median error on eight out of ten categories (full breakdown in Appendix A.2). Because the dataset includes objects with symmetries, the average median error is heavily influenced by high errors on categories with ambiguous pose. For instance, all methods get at least 90 degree rotation error on bathtubs, since it is hard to tell the front from the back. Thus, regression methods can get stuck between ambiguous object symmetries. In contrast, our method generates a distribution which can capture multiple symmetry modes, even if the correct one cannot be identified. We believe that I2S is more accurate than other distributional methods because its equivariant layers encode the symmetry present in the pose estimation problem.\n\nTable 1: Comparison of pose estimation performance on ModelNet10-SO(3) dataset. Data is averaged over all ten object categories. For I2S, we report mean and standard deviation over five random seeds.\n\nAcc@15↑\n\nAcc@30↑\n\nMedErr↓\n\nZhou et al. (2019) Br ́egier (2021) Liao et al. (2019) Deng et al. (2022) Prokudin et al. (2018) Mohlin et al. (2020) Murphy et al. (2021) I2S (ours)\n\n0.251 0.257 0.357 0.562 0.456 0.693 0.719 0.728 ± 0.002\n\n0.504 0.515 0.583 0.694 0.528 0.757 0.735 0.736±0.002\n\n41.1 39.9 36.5 32.6 49.3 17.1 21.5 15.7±2.9\n\n5.6 PASCAL3D+\n\nPASCAL3D+ is a challenging benchmark for pose detection that requires robustness to real textures and generalization to novel object instances. We report the median error of our method and competitive baselines in Table 2. Importantly, our method achieves the lowest median error averaged across all 12 categories. Note that the symmetries are broken consistently in the labels, so outperforming methods that regress to a single pose, e.g. Mahendran et al. (2018), is particularly impressive. We hypothesize that the SO(3) equivariant layers within our model provide stronger generalization capabilities. Example predictions of our method show that it captures reasonable uncertainty about object pose when making predictions (Figure 3).\n\nFigure 3: Example predictions of I2S on PASCAL3D+. The distribution visualizations show that our method captures pose uncertainty, e.g. it is unclear if a long boat is approaching or a shorter is headed to the right, creating ambiguity about one rotation axis (middle).\n\n7\n\nPublished as a conference paper at ICLR 2023\n\navg.\n\nplane\n\nbicycle\n\nboat\n\nbottle\n\nbus\n\ncar\n\nchair\n\ntable\n\nmbike\n\nsofa\n\ntrain\n\ntv\n\nZhou et al. (2019) Br ́egier (2021) Liao et al. (2019) Mohlin et al. (2020) Prokudin et al. (2018) Tulsiani & Malik (2015) Mahendran et al. (2018) Murphy et al. (2021) I2S (ours)\n\n19.2 20.0 13.0 11.5 12.2 13.6 10.1 10.3 9.8 ±0.4\n\n24.7 27.5 13.0 10.1 9.7 13.8 8.5 10.8 9.2 ±0.4\n\n18.9 22.6 16.4 15.6 15.5 17.7 14.8 12.9 12.7 ±0.7\n\n54.2 49.2 29.1 24.3 45.6 21.3 20.5 23.4 21.7 ±1.3\n\n11.3 11.9 10.3 7.8 5.4 12.9 7.0 8.8 7.4 ±0.7\n\n8.4 8.5 4.8 3.3 2.9 5.8 3.1 3.4 3.3 ±0.1\n\n9.5 9.9 6.8 5.3 4.5 9.1 5.1 5.3 4.9 ±0.1\n\n19.4 16.8 11.6 13.5 13.1 14.8 9.5 10.0 9.5 ±0.8\n\n14.9 27.9 12.0 12.5 12.6 15.2 11.3 7.3 9.3 ±3.4\n\n22.5 21.7 17.1 12.9 11.8 14.7 14.2 13.6 11.5 ±0.8\n\n17.2 12.6 12.3 13.8 9.1 13.7 10.2 9.5 10.5 ±0.8\n\n11.4 10.2 8.6 7.4 4.3 8.7 5.6 6.4 7.2 ±0.5\n\n17.5 20.6 14.3 11.7 12.0 15.4 11.7 12.3 10.6 ±0.6\n\nTable 2: Median rotation error (◦) on PASCAL3D+. First column shows average median error over all twelve classes. For some baselines, we report the corrected results by Murphy et al. (2021) (see Appendix B.2 for details). For I2S, we report mean and standard deviation over six runs.\n\n5.7 SYMSOL\n\nOne of the strengths of our method is the ability to represent distributions over 3D rotations. As shown in the previous section, this formulation is beneficial when training on symmetric objects. In this section, we quantitatively evaluate the ability to model uncertainty in two settings: SYMSOL I which includes simple geometric objects with complex symmetries and SYMSOL II which has objects marked with a single identifier such that self-occlusion creates pose ambiguity. Because most images correspond to a set of equivalent rotation labels, we measure performance using average log likelihood, reported in Table 3.\n\nThe results show that our method effectively represents complex distributions over SO(3). We achieve higher log likelihood on all SYMSOL shapes than methods that map to a specific small family of distributions such as von Mises or Bingham. This highlights the advantage of parametrizing uncertainty in the Fourier basis of SO(3), which is a simpler approach that avoids training multimodal distributions. We note that Murphy et al. (2021) outperforms our method when trained on 100k images per object; however, our method is better when trained on only 10k images per object. This demonstrates an important distinction between the two approaches: our method explicitly encodes the 3D rotation symmetry of the problem in the spherical convolutions, whereas Murphy et al. (2021) must learn the symmetry from data. We argue that sample efficiency is important for real world applications since it is not practical to collect 100k images of an single object to model its symmetry. We want to highlight that we use the same maximum frequency L for all experiments in this work which shows that I2S can be deployed without knowing if object symmetries are present in the task.\n\nTable 3: Average log likelihood on SYMSOL datasets. SYMSOL I includes objects with complex symmetries, while SYMSOL II includes objects whose poses can be ambiguous under self-occlusion. The highest likelihood in each column is in bold, second-best is underlined.\n\nnum. training images\n\navg.\n\ncone\n\ncyl.\n\nSYMSOL I tet.\n\n100k\n\n10k\n\nDeng et al. (2022) Gilitschenski et al. (2019) Prokudin et al. (2018) Murphy et al. (2021) I2S (ours)\n\nMurphy et al. (2021) I2S (ours)\n\n-1.48 -0.43 -1.87 4.10 3.41\n\n-7.94 2.98\n\n0.16 3.84 -3.34 4.45 3.75\n\n-1.51 3.51\n\n-0.95 0.88 -1.28 4.26 3.10\n\n-2.92 2.88\n\n0.27 -2.29 -1.86 5.70 4.78\n\n-6.90 3.62\n\ncube\n\n-4.44 -2.29 -0.50 4.81 3.27\n\nico.\n\n-2.45 -2.29 -2.39 1.28 2.15\n\n-10.04 2.94\n\n-18.34 1.94\n\navg.\n\n2.57 3.70 0.48 7.57 4.84\n\n-0.73 3.61\n\nSYMSOL II cylO\n\nsphX\n\n1.12 3.32 -4.19 7.30 3.74\n\n-2.51 3.12\n\n2.99 4.88 4.16 6.91 5.18\n\n2.02 3.87\n\ntetX\n\n3.61 2.90 1.48 8.49 5.61\n\n-1.70 3.84\n\n5.8 COMPARISON OF ALTERNATIVE IMAGE TO SO(3) MAPPINGS\n\nWe argue that a main driver of our method’s pose accuracy is SO(3)-equivariant processing. While many existing methods for end-to-end SO(3)-equivariance have been explored in the literature (Fuchs et al., 2020; Deng et al., 2021; Cohen et al., 2018a), it is not well-understood how to combine non-equivariant and equivariant layers in a network. In this section, we consider other sensible approaches to map from features in the image plane to features that live on SO(3) or a discrete subgroup of it. The approach taken by I2S is to perform orthographic projection to link features on the sphere to the image plane (spatial projection), then convolve it with a filter that is parametrized\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Distributions generated by I2S for SYMSOL objects. To visualize the probabilities over SO(3), rotations with non-negligible probability are plotted as dots using a Mollweide projection, encoding rotation orthogonal to the sphere with color (Murphy et al., 2021). I2S is able to capture large discrete symmetry groups of the cube and icosahedron (left column), as well as continuous symmetries of the cone and cylinder (middle column). I2S predicts single pose when the dot is visible on cylinder (cylO), but is uncertain when the dot is occluded (right column).\n\nby the Fourier basis of S2 to generate a signal over SO(3) (Fourier filter). Sensible alternatives can be proposed by instead linking features in the image to coefficients of the Fourier basis (Fourier projection) or parametrizing the filter with features over the sphere (spatial filter). Alternatively, one may consider the approach of directly computing the coefficients of the Fourier basis using an MLP. Finally, we include a modification of the I2S framework using the icosahedral group convolution (a discrete group of SO(3)) instead of spherical convolution. This model uses an anchor-regression framework to produce continuous rotation predictions, as outlined in Chen et al. (2021).\n\nTable 4: Comparing different approaches to learn an SO(3) signal with image inputs. Results show orientation prediction performance on ModelNet10-SO(3) with limited training views.\n\nAcc@15\n\nAcc@30 MedErr\n\nI2S∗ (spatial projection, Fourier filter) I2S (spatial projection, spatial filter) I2S (Fourier projection, spatial filter) I2S (Fourier projection, Fourier filter) Fourier basis w/ MLP Icosahedral group version\n\n0.623 0.610 0.523 0.511 0.360 0.455\n\n0.640 0.633 0.533 0.631 0.390 0.582\n\n46.3 44.6 56.8 57.0 69.2 57.6\n\nWe evaluate the pose prediction performance of these alternative approaches in Table 4 on the ModelNet10-SO(3) dataset with limited training views. I2S in all its variations, outperforms using an MLP to parametrize the Fourier basis and the discrete icosahedral group convolution version. This is likely because pose estimation benefits from equivariance to the continuous rotation group. We find that using a spatial projection is important for accuracy, which we hypothesize is better able to preserve spatial information encoded in the dense feature map. How the filter is parametrized is less influential. We use the Fourier basis to parametrize the filter, while the original spherical CNN work parametrizes the filter using a spatial grid over the sphere (Cohen et al., 2018a).\n\n6 CONCLUSION\n\nIn this work, we present the first method to leverage SO(3)-equivariance for predicting distributions over 3D rotations from single images. Our method is better suited than regression methods at handling unknown object symmetries, generates more expressive distributions than methods using parametric families of multi-modal distributions while requiring fewer samples than an implicit modeling approach. We demonstrate state-of-the-art performance on the challenging PASCAL3D+ dataset composed of real images. One limitation of our work is that we use a high maximum frequency, L, in the spherical convolution operations to have higher resolution predictions. Because the number of operations in a spherical convolution is quadratic in L, it may be impractical for applications where more spherical convolutions are required.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work is supported in part by NSF 1724257, NSF 1724191, NSF 1763878, NSF 1750649, and NASA 80NSSC19K1474. R. Walters is supported by the Roux Institute and the Harold Alfond Foundation and NSF grants 2107256 and 2134178.\n\nREPRODUCIBILITY STATEMENT\n\nThe code to replicate the results of our method is available at https://github.com/dmklee/ image2sphere. All datasets used are publicly available and a thorough description of preprocessing is provided in Appendix B.2. The model architecture and training protocol are discussed in Section 5.3 and Appendix B.1. Information on the baselines and their reported results is provided in Appendix B.3.\n\nREFERENCES\n\nRomain Br ́egier. Deep regression on manifolds: a 3d rotation case study. In 2021 International\n\nConference on 3D Vision (3DV), pp. 166–174. IEEE, 2021.\n\nHaiwei Chen, Shichen Liu, Weikai Chen, Hao Li, and Randall Hill. Equivariant point network for 3d point cloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14514–14523, 2021.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In International conference\n\non machine learning, pp. 2990–2999. PMLR, 2016.\n\nTaco S. Cohen, Mario Geiger, Jonas K ̈ohler, and Max Welling. Spherical cnns. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018a.\n\nTaco S. Cohen, Mario Geiger, Jonas K ̈ohler, and Max Welling. Spherical CNNs. In International Conference on Learning Representations, 2018b. URL https://openreview.net/forum? id=Hkbd5xZRb.\n\nCongyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12200–12209, 2021.\n\nHaowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, and Tolga Birdal. Deep bingham networks: Dealing with uncertainty and ambiguity in pose estimation. International Journal of Computer Vision, pp. 1–28, 2022.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nCarlos Esteves, Avneesh Sud, Zhengyi Luo, Kostas Daniilidis, and Ameesh Makadia. Cross-domain 3d equivariant image embeddings. In International Conference on Machine Learning, pp. 1812– 1822. PMLR, 2019a.\n\nCarlos Esteves, Yinshuang Xu, Christine Allen-Blanchette, and Kostas Daniilidis. Equivariant multiview networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1568–1577, 2019b.\n\nLuca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao, Maurice Weiler, Patrick Forr ́e, and Taco S. Cohen. Explorations in homeomorphic variational auto-encoding. CoRR, abs/1807.04689, 2018. URL http://arxiv.org/abs/1807.04689.\n\nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rototranslation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970–1981, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3354–3361. IEEE, 2012.\n\nMario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller, Wouter Boomsma, Bradley Dice, Kostiantyn Lapchevskyi, Maurice Weiler, Michał Tyszkiewicz, Simon Batzner, Dylan Madisetti, Martin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel Rød, and Michael Bailey. Euclidean neural networks: e3nn, April 2022.\n\nIgor Gilitschenski, Roshni Sahoo, Wilko Schwarting, Alexander Amini, Sertac Karaman, and Daniela Rus. Deep orientation uncertainty learning based on a bingham loss. In International Conference on Learning Representations, 2019.\n\nKrzysztof M Gorski, Eric Hivon, Anthony J Banday, Benjamin D Wandelt, Frode K Hansen, Mstvos Reinecke, and Matthia Bartelmann. Healpix: A framework for high-resolution discretization and fast analysis of data distributed on the sphere. The Astrophysical Journal, 622(2):759, 2005.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In Proceedings of the\n\nIEEE international conference on computer vision, pp. 2961–2969, 2017.\n\nYisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun. Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11632–11641, 2020.\n\nStefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobodan Ilic, Kurt Konolige, Nassir Navab, and Vincent Lepetit. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes. In 2011 international conference on computer vision, pp. 858–865. IEEE, 2011.\n\nAnthony W Knapp. Lie groups beyond an introduction, volume 140. Springer, 1996.\n\nAxel Levy, Fr ́ed ́eric Poitevin, Julien Martel, Youssef Nashed, Ariana Peck, Nina Miolane, Daniel Ratner, Mike Dunne, and Gordon Wetzstein. Cryoai: Amortized inference of poses for ab initio reconstruction of 3d molecular volumes from real cryo-em images. In European Conference on Computer Vision, pp. 540–557. Springer, 2022.\n\nZhigang Li, Gu Wang, and Xiangyang Ji. Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7678–7687, 2019.\n\nShuai Liao, Efstratios Gavves, and Cees GM Snoek. Spherical regression: Learning viewpoints, surface normals and 3d rotations on n-spheres. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9759–9767, 2019.\n\nSiddharth Mahendran, Haider Ali, and Rene Vidal. A mixed classification-regression framework for\n\n3d pose estimation from 2d images. arXiv preprint arXiv:1805.03225, 2018.\n\nDavid Mohlin, Josephine Sullivan, and G ́erald Bianchi. Probabilistic orientation estimation with matrix fisher distributions. Advances in Neural Information Processing Systems, 33:4884–4893, 2020.\n\nKieran A Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, and Ameesh Makadia. Implicit-pdf: Non-parametric representation of probability distributions on the rotation manifold. In Proceedings of the 38th International Conference on Machine Learning, pp. 7882–7893, 2021.\n\nJung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan-Willem van de Meent, and Robin Walters. Learning\n\nsymmetric embeddings for equivariant world models. CoRR, abs/2204.11371, 2022.\n\nSergey Prokudin, Peter Gehler, and Sebastian Nowozin. Deep directional statistics: Pose estimation with uncertainty quantification. In Proceedings of the European conference on computer vision (ECCV), pp. 534–551, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nHao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE international conference on computer vision, pp. 2686–2694, 2015.\n\nJonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan Birchfield. Deep object pose estimation for semantic robotic grasping of household objects. CoRR, abs/1809.10790, 2018. URL http://arxiv.org/abs/1809.10790.\n\nShubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE\n\nConference on Computer Vision and Pattern Recognition, pp. 1510–1519, 2015.\n\nHe Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2642–2651, 2019.\n\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1913–1920, 2015.\n\nYu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond pascal: A benchmark for 3d object detection in the wild. In IEEE winter conference on applications of computer vision, pp. 75–82. IEEE, 2014.\n\nYu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. In Robotics: Science and Systems, 2018.\n\nAnna Yershova, Swati Jain, Steven M Lavalle, and Julie C Mitchell. Generating uniform incremental grids on so (3) using the hopf fibration. The International journal of robotics research, 29(7): 801–812, 2010.\n\nSergey Zakharov, Ivan Shugurov, and Slobodan Ilic. Dpod: 6d pose object detector and refiner. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1941–1950, 2019.\n\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5745–5753, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA DETAILED RESULTS\n\nA.1 MODELNET10-SO3 LIMITED TRAINING SET\n\nWe compare the sample efficiency of pose prediction methods by training on ModelNet10-SO(3) with limited training views in Table 5. Specifically, we reduce the number of training views from 100 per instance to 20 per instance. Our method outperforms the baselines by a larger margin in the low-data setting. This results highlights an important distinction between our method and the baselines: our method explicitly encodes the 3D rotation symmetry present in the pose prediction problem, whereas other methods must learn the symmetry from data.\n\nTable 5: Comparison on ModelNet10-SO(3) with limited training views. Methods are trained on five times fewer images than the experiment in Table 1.\n\nLimited Training Set\n\nAcc@15↑\n\nAcc@30↑ MedErr↓\n\nZhou et al. (2019) Br ́egier (2021) Murphy et al. (2021) I2S (ours)\n\n0.064 0.129 0.515 0.623\n\n0.239 0.359 0.533 0.640\n\n62.7 51.5 59.5 46.3\n\nA.2 MODELNET10-SO3 PER-CLASS BREAKDOWN\n\nTable 6 reports the median rotation error for each object in ModelNet10-SO(3). These results highlight the challenge of pose prediction with unknown symmetries. Note that the categories that are hard to predict, e.g. bathtub, night stand and table, can have multiple correct reference frames due to symmetry.\n\nTable 6: Per-class median rotation error (◦) on ModelNet10-SO(3). Results for I2S are reported as mean and standard deviation over six random seeds.\n\nZhou et al. (2019) Br ́egier (2021) Liao et al. (2019) Deng et al. (2022) Mohlin et al. (2020) Prokudin et al. (2018) Murphy et al. (2021) I2S (ours)\n\navg.\n\n41.1 39.9 36.5 32.6 17.1 49.3 21.5 16.3 ±2.9\n\nbathtub\n\nbed\n\nchair\n\ndesk\n\ndresser\n\nmonitor\n\nn. stand\n\n103.3 98.9 113.3 147.8 89.1 122.8 161.0 124.7 ±28.1\n\n18.1 17.4 13.3 9.2 4.4 3.6 4.4 3.1 ±0.0\n\n18.3 18.0 13.7 8.3 5.2 9.6 5.5 4.4 ±0.0\n\n51.5 50.0 39.2 25.0 13.0 117.2 7.1 4.7 ±0.1\n\n13\n\n32.2 31.5 26.9 11.9 6.3 29.9 5.5 3.4 ±0.1\n\n19.7 18.7 16.4 9.8 5.8 6.7 5.7 4.4 ±0.1\n\n48.4 46.5 44.2 36.9 13.5 73.0 7.5 4.1 ±0.1\n\nsofa\n\n17.0 17.4 12.0 10.0 4.0 10.4 4.1 3.0 ±0.0\n\ntable\n\ntoilet\n\n88.2 86.7 74.8 58.6 25.8 115.5 9.0 7.7 ±1.0\n\n13.8 14.2 10.9 8.5 4.0 4.1 4.8 3.6 ±0.1\n\nPublished as a conference paper at ICLR 2023\n\nA.3 VISUALIZING PREDICTIONS\n\nFigure 5: Example predictions of our method on ModelNet10-SO(3) with 100 training views. The dataset includes objects with two-fold (i.e bathtub) or four-fold symmetry (i.e. side table), which our method can model as multi modal distributions in SO(3).\n\nFigure 6: Additional predictions of our method on PASCAL3D+. Ground truth label is shown as circular ring. The distribution over SO(3) is visualized by converting to Euler angles: the first two are represented spatially via Mollweide projection and the third is encoded as color. Our model captures uncertainty in the pose that result from object symmetries or insufficient visual cues.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: I2S capturing object symmetries and pose ambiguity on SYMSOL Dataset (50k views). Each row corresponds to a different shape from SYMSOL: cone, cylinder, tetrahedron, cube, icosahedron, circleX, cylinderO, and tetrahedronX. I2S is able to represent complex object symmetries, both discrete and continuous.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB IMPLEMENTATION DETAILS\n\nB.1 ARCHITECTURE\n\nWe use a ResNet encoder with weights pretrained on ImageNet. With 224x224 images as input, this generates a 7x7 featuremap with 2048 channels. The orthographic projection onto the sphere is performed using a HEALPix grid of recursion level 2 restricted to half the sphere. With each forward pass, 20 of these grid points are randomly sampled and used to generate the S2 signal. The S2 signal is converted to the Fourier domain with a maximum frequency of 6. A spherical convolution operation is performed using a filter that is parametrized in the Fourier domain, which generates an 8-channel signal over SO(3). A non-linearity is applied by mapping the signal to the spatial domain, applying a ReLU, then mapping back to Fourier domain. One final spherical convolution with a locally supported filter is performed to generate a one-dimensional signal on SO(3). The signal is queried using an SO(3) HEALPix grid (recursion level 3 during training, 5 during evaluation) and then normalized using a softmax. The network is instantiated in PyTorch, and we use the e3nn2 library for the group convolution operations.\n\nB.2 DATASET PREPARATION\n\nModelNet10-SO(3) is available for download at the Github3 associated with Liao et al. (2019). The dataset has a standardized train and test split. It provides two training sets: one with 100 views per object instance (we call this the Full Training Set), and one with 20 views per object instance (we call this the Limited Training Set). The test set has 4 views per instance. Each image is labeled with a single rotation label, and object symmetries were not broken during labeling. Based on the original work, we do not perform any data augmentation with this dataset.\n\nSYMSOL can be downloaded from the Github4 linked by Murphy et al. (2021). The dataset includes renderings of 8 synthetic objects split into two categories: SYMSOL I includes tetrahedron, cube, icosahedron, cone and cylinder, while SYMSOL II includes marked tetrahedron, marked cylinder, and marked sphere. For each shape, there are 50k renderings in the training set and 5K renderings in the test set. Each image is labeled with the set of valid rotations (for continuous symmetries like the cylinder, it is provided at 1 degree increments). During training, the set of valid rotations is randomly sampled to generate a single rotation label to compute the loss. In Table 3, we use results originally reported by Murphy et al. (2021), and follow the approach of training a single model on all objects from SYMSOL I but different models for each object in SYMSOL II. The results reported by Murphy et al. (2021) were generated using 100k training renderings per shape, but the publicly released dataset only has 50k. Thus, this strongly favors the baselines.\n\nPascal3D+ is available for download at the link5 provided in Xiang et al. (2014). The training data is found in the ImageNet train, ImageNet val, and PASCALVOC train folders, and the test data is in PASCALVOC val. Following Murphy et al. (2021), we discard any data that is labeled occluded, difficult or truncated. We follow the data augmentation procedure from Mohlin et al. (2020) that randomly performs horizontal flip and slight perspective transformation during training. Additionally, we supplement the training data with synthetic images from RenderForCNN (Su et al., 2015) (this requires free ImageNet account to download), such that three quarters of data is synthetic during training. Including synthetic data is important to achieve high accuracy, and is also used by the baselines. Where possible, we use the reported results of the baselines on PASCAL3D+; however, as pointed out by Murphy et al. (2021), some of the baselines used incorrect evaluation functions or a different evaluation set. In these cases, we report the performance from Murphy et al. (2021), since they re-ran these baselines after correcting the issues.\n\n2https://e3nn.org 3https://github.com/leoshine/Spherical_Regression 4https://github.com/google-research/google-research/tree/master/\n\nimplicit_pdf\n\n5https://cvgl.stanford.edu/projects/pascal3d.html\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.3 BASELINES\n\nHere, we include details for baselines that we implemented ourselves. For information on other baselines, refer directly to their work.\n\nZhou et al. (2019) proposes using the Gram-Schmidt process to convert a 6D vector into a valid, 3x3 rotation matrix. To implement this network, we used a ResNet architecture with spatial pooling on the final feature map. The resulting vector was processed with two linear layers to generate the 6D vector. The model is trained with an L2 loss function using a ground truth rotation matrix. The method is trained on all classes at once, but uses a separate linear layer to predict the rotation of each class.\n\nBr ́egier (2021) proposes using the Procrustes method to convert a 9D vector into a valid, 3x3 rotation matrix. They provide an efficient implementation of the Procrustes method. The architecture and loss function is the same as for Zhou et al. (2019), except the linear layer produces a 9D vector.\n\nThe original work of Liao et al. (2019) only showed results for PASCAL3D+, which used an incorrect evaluation function as noted by Murphy et al. (2021). Thus, for PASCAL3D+, we used the results reported by Murphy et al. (2021) with the corrected evaluation function. For ModelNet10-SO(3), we ran their code using a pretrained ResNet50 as an encoder.\n\nB.4 CREATING VISUALS\n\nWe follow the visualization code that was publicly released by Murphy et al. (2021) to represent distributions over 3D rotations as a 2D plot. The probability associated with each rotation is represented as a dot, with the size proportional the magnitude. The rotation is encoded by converting to XYX euler angles, the first two angles correspond to latitude and longitude in a Mollweide projection and the final angle is encoded as color using an HSV colormap. To make the visualizations more interpretable, we do not plot any probabilities less than a given threshold. In our visualizations, we generate probabilities associated with the SO(3) HEALPix grid used during model evaluation (recursion level of 5; 2.4M points).\n\nC ADDITIONAL EXPERIMENTS\n\nC.1 EFFECT OF MAXIMUM FREQUENCY L\n\nTo efficiently perform SO(3) group convolutions, we must restrict our representations to in the frequency domain. Learning with lower frequency signals can be more efficient and may generalize better in some cases, while higher frequency signals may be better for encoding complex distributions like those shown in Figure 4. In Table 7, we perform a small experiment showing the effects of different maximum frequencies, L, on pose prediction accuracy. Note that we use L = 6 for all other experiments in this work. Interestingly, we find that including higher frequencies in the representation, e.g. L > 6 can actually reduce accuracy, despite the additional learnable parameters.\n\nTable 7: Varying maximum frequency L in Fourier basis. Results are generated on ModelNet10-SO(3) with limited training views. The same L is maintained through both spherical convolutions of our method.\n\nAcc@15↑\n\nAcc@30↑ MedErr↓\n\nL = 2 L = 4 L = 6 L = 8 L = 10\n\n0.560 0.626 0.623 0.605 0.599\n\n0.656 0.646 0.640 0.621 0.618\n\n33.6 46.6 46.3 43.7 46.8\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nC.2 EFFECT OF ADDITIONAL SO(3) CONVOLUTIONS\n\nTable 8: Effect of SO(3) convolutional layers on ModelNet10-SO(3) pose prediction. I2S, as proposed, includes one SO(3) convolution following the S2 convolution. We show performance without the SO(3) convolution, and with an additional SO(3) convolution. Additional SO(3) convolutions provide minimal benefit at the expense of compute.\n\nFull Training Set\n\nLimited Training Set\n\nAcc@15↑\n\nAcc@30↑ MedErr↓\n\nAcc@15↑\n\nAcc@30↑ MedErr↓\n\nI2S, no SO(3) conv I2S, one SO(3) conv I2S, two SO(3) conv\n\n0.729 0.729 0.729\n\n0.737 0.737 0.737\n\n15.6 14.4 19.0\n\n0.616 0.623 0.625\n\n0.634 0.640 0.643\n\n46.4 46.3 45.5\n\nOur method performs one S2 convolution followed by one SO(3) convolution. In this section, we look at the performance benefits of this final refinement layer and potential benefits of an additional convolution. We find that the performing SO(3) convolution does lead to a marginal improvement, with little to be gained by performing more. This suggests that most of the SO(3) reasoning occurs within the first spherical convolution that uses a learned filter with global support.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper describes an approach for pose estimation for objects with symmetries. The approach leverages SO(3)-equivariance to predict distributions over 3D rotations from single images. Image features are obtained from a pre-trained ResNet. These features are orthographically projected to the sphere where the features are convolved with a learned filter on S^2. As a result a signal is generated on SO(3). A group convolution on SO(3) is performed generating a detailed distribution over SO(3). This distribution can model symmetric objects. As a result object symmetries are learned and uncertainty can be represented.\n\n# Strength And Weaknesses\n\nThe strengths include the ability to handle unknown object symmetries and the generation of distributions over poses, which in addition of allowing to take into account the pose ambiguity from object symmetries, it permits the representation of uncertainties due to partial observability. However with some datasets (SYMSOL) previous methods (Murphy et al.) have better performance. Computational cost not evaluated.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper has good quality since it addresses the problem of object symmetries using equivariant layers and allows the representation of uncertainty. Some details are scarce, namely those concerning the computation of group convolution in Fourier space. The main contribution of the paper is the ability to provide uncertainty estimates for the poses of symmetric objects.\n\n# Summary Of The Review\n\nThis is a paper describing an approach for pose estimation of symmetric objects with prediction of uncertainty. The contribution is clear but incremental.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable\n\n# Details Of Ethics Concerns\n\nThere are no ethics concerns."
  }
]