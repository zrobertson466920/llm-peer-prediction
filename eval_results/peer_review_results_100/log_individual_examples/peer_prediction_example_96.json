{
  "example_idx": 96,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSGD THROUGH THE LENS OF KOLMOGOROV COMPLEXITY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe initiate a thorough study of the dynamics of stochastic gradient descent (SGD) under minimal assumptions using the tools of entropy compression. Specifically, we characterize a quantity of interest which we refer to as the accuracy discrepancy. Roughly speaking, this measures the average discrepancy between the model accuracy on batches and large subsets of the entire dataset. We show that if this quantity is sufficiently large, then SGD finds a model which achieves perfect accuracy on the data in O(1) epochs. On the contrary, if the model cannot perfectly fit the data, this quantity must remain below a global threshold, which only depends on the size of the dataset and batch. We use the above framework to lower bound the amount of randomness required to allow (non-stochastic) gradient descent to escape from local minima using perturbations. We show that even if the model is extremely overparameterized, at least a linear (in the size of the dataset) number of random bits are required to guarantee that GD escapes local minima in subexponential time.\n\n1\n\nINTRODUCTION\n\nStochastic gradient descent (SGD) is at the heart of modern machine learning. However, we are still lacking a theoretical framework that explains its performance for general, non-convex functions. Current results make significant assumptions regarding the model. Global convergence guarantees only hold under specific architectures, activation units, and when models are extremely overparameterized (Du et al., 2019; Allen-Zhu et al., 2019; Zou et al., 2018; Zou and Gu, 2019). In this paper, we take a step back and explore what can be said about SGD under the most minimal assumptions. We only assume that the loss function is differentiable and L-smooth, the learning rate is sufficiently small and that models are initialized randomly. Clearly, we cannot prove general convergence to a global minimum under these assumptions. However, we can try and understand the dynamics of SGD - what types of execution patterns can and cannot happen.\n\nMotivating example: Suppose hypothetically, that for every batch, the accuracy of the model after the Gradient Descent (GD) step on the batch is 100%. However, its accuracy on the set of previously seen batches (including the current batch) remains at 80%. Can this process go on forever? At first glance, this might seem like a possible scenario. However, we show that this cannot be the case. That is, if the above scenario repeats sufficiently often the model must eventually achieve 100% accuracy on the entire dataset.\n\nTo show the above, we identify a quantity of interest which we call the accuracy discrepancy (formally defined in Section 3). Roughly speaking, this is how much the model accuracy on a batch differs from the model accuracy on all previous batches in the epoch. We show that when this quantity (averaged over epochs) is higher than a certain threshold, we can guarantee that SGD convergence to 100% accuracy on the dataset within O(1) epochs w.h.p1. We note that this threshold is global, that is, it only depends on the size of the dataset and the size of the batch. In doing so, we provide a sufficient condition for SGD convergence.\n\nThe above result is especially interesting when applied to weak models that cannot achieve perfect accuracy on the data. Imagine a dataset of size n with random labels, a model with n0.99 parameters, and a batch of size log n. The above implies that the accuracy discrepancy must eventually go below\n\n1With high probability means a probability of at least 1 − 1/n, where n is the size of the dataset.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe global threshold. In other words, the model cannot consistently make significant progress on batches. This is surprising because even though the model is underparameterized with respect to the entire dataset, it is extremely overparameterized with respect to the batch. We verify this observation experimentally (Appendix B). This holds for a single GD step, but what if we were to allow many GD steps per batch, would this mean that we still cannot make significant progress on the batch? This leads us to consider the role of randomness in (non-stochastic) gradient descent.\n\nIt is well known that overparameterized models trained using SGD can perfectly fit datasets with random labels (Zhang et al., 2017). It is also known that when models are sufficiently overparameterized (and wide) GD with random initialization convergences to a near global minimum (Du et al., 2019). This leads to an interesting question: how much randomness does GD require to escape local minima efficiently (in polynomial time)? It is obvious that without randomness we could initialize GD next to a local minimum, and it will never escape it. However, what about the case where we are provided an adversarial input and we can perturb that input (for example, by adding a random vector to it), how many bits of randomness are required to guarantee that after the perturbation GD achieves good accuracy on the input in polynomial time?\n\nIn Section 4 we show that if the amount of randomness is sublinear in the size of the dataset, then for any differentiable and L-smooth model class (e.g., a neural network architecture), there are datasets that require an exponential running time to achieve any non-trivial accuracy (i.e., better than 1/2 + o(1) for a two-class classification task), even if the model is extremely overparameterized. This result highlights the importance of randomness for the convergence of gradient methods. Specifically, it provides an indication of why SGD converges in certain situations and GD does not. We hope this result opens the door to the design of randomness in other versions of GD.\n\nOutline of our techniques We consider batch SGD, where the dataset is shuffled once at the beginning of each epoch and then divided into batches. We do not deal with the generalization abilities of the model. Thus, the dataset is always the training set. In each epoch, the algorithm goes over the batches one by one, and performs gradient descent to update the model. This is the \"vanilla\" version of SGD, without any acceleration or regularization (for a formal definition, see Section 2). For the sake of analysis, we add a termination condition after every GD step: if the accuracy on the entire dataset is 100% we terminate. Thus, in our case, termination implies 100% accuracy.\n\nTo achieve our results, we make use of entropy compression, first considered by Moser and Tardos (2010) to prove a constructive version of the Lovász local lemma. Roughly speaking, the entropy compression argument allows one to bound the running time of a randomized algorithm2 by leveraging the fact that a random string of bits (the randomness used by the algorithm) is computationally incompressible (has high Kolmogorov complexity) w.h.p. If one can show that throughout the execution of the algorithm, it (implicitly) compresses the randomness it uses, then one can bound the number of iterations the algorithm may execute without terminating. To show that the algorithm has such a property, one would usually consider the algorithm after executing t iterations, and would try to show that just by looking at an \"execution log\" of the algorithm and some set of \"hints\", whose size together is considerably smaller than the number of random bits used by the algorithm, it is possible to reconstruct all of the random bits used by the algorithm.\n\nWe apply this approach to SGD with an added termination condition when the accuracy over the entire dataset is 100%. Thus, termination in our case guarantees perfect accuracy. The randomness we compress is the bits required to represent the random permutation of the data at every epoch. So indeed the longer SGD executes, the more random bits are generated. We show that under our assumptions it is possible to reconstruct these bits efficiently starting from the dataset X and the model after executing t epochs. The first step in allowing us to reconstruct the random bits of the permutation in each epoch is to show that under the L-smoothness assumption and a sufficiently small step size, SGD is reversible. That is, if we are given a model Wi+1 and a batch Bi such that Wi+1 results from taking a gradient step with model Wi where the loss is calculated with respect to Bi, then we can uniquely retrieve Wi using only Bi and Wi+1. This means that if we can efficiently encode the batches used in every epoch (i.e., using less bits than encoding the entire permutation of the data), we can also retrieve all intermediate models in that epoch (at no additional cost). We prove this claim in Section 2.\n\n2We require that the number of the random bits used is proportional to the execution time of the algorithm. That is, the algorithm flips coins for every iteration of a loop, rather than just a constant number at the beginning of the execution.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nThe crux of this paper is to show that when the accuracy discrepancy is high for a certain epoch, the batches can indeed be compressed. To exemplify our techniques let us consider the scenario where, in every epoch, just after a single GD step on a batch we consistently achieve perfect accuracy on the batch. Let us consider some epoch of our execution, assume we have access to X, and let Wf be the model at the end of the epoch. If the algorithm did not terminate, then Wf has accuracy at most 1 − (cid:15) on the entire dataset (assume for simplicity that (cid:15) is a constant). Our goal is to retrieve the last batch of the epoch, Bf ⊂ X (without knowing the permutation of the data for the epoch). A naive approach would be to simply encode the indices in X of the elements in the batch. However, we can use Wf to achieve a more efficient encoding. Specifically, we know that Wf achieves 1.0 accuracy on Bf but only 1 − (cid:15) accuracy on X. Thus it is sufficient to encode the elements of Bf using a smaller subset of X (the elements classified correctly by Wf , which has size at most (1 − (cid:15)) |X|). This allows us to significantly compress Bf . Next, we can use Bf and Wf together with the reversibility of SGD to retrieve Wf −1. We can now repeat the above argument to compress Bf −1 and so on, until we are able to reconstruct all of the random bits used to generate the permutation of X in the epoch. This will result in a linear reduction in the number of bits required for the encoding.\n\nIn our analysis, we show a generalized version of the scenario above. We show that high accuracy discrepancy implies that entropy compression occurs. For our second result, we consider a modified SGD algorithm that instead of performing a single GD step per batch, first perturbs the batch with a limited amount of randomness and then performs GD until a desired accuracy on the batch is reached. We assume towards contradiction that GD can always reach the desired accuracy on the batch in subexponential time. This forces the accuracy discrepancy to be high, which guarantees that we always find a model with good accuracy. Applying this reasoning to models of sublinear size and data with random labels we arrive at a contradiction, as such models cannot achieve good accuracy on the data. This implies that when we limit the amount of randomness GD can use for perturbations, there must exist instances where GD requires exponential running time to achieve good accuracy.\n\nRelated work There has been a long line of research proving convergence bounds for SGD under various simplifying assumptions such as: linear networks (Arora et al., 2019; 2018), shallow networks (Safran and Shamir, 2018; Du and Lee, 2018; Oymak and Soltanolkotabi, 2019), etc. However, the most general results are the ones dealing with deep, overparameterized networks (Du et al., 2019; Allen-Zhu et al., 2019; Zou et al., 2018; Zou and Gu, 2019). All of these works make use of NTK (Neural Tangent Kernel)(Jacot et al., 2018) and show global convergence guarantees for SGD when the hidden layers have width at least poly(n, L) where n is the size of the dataset and L is the depth of the network. We note that the exponents of the polynomials are quite large.\n\nA recent line of work by Zhang et al. (2022) notes that in many real world scenarios models do not converge to stationary points. They instead take a different approach which, similar to us, studies the dynamics of neural networks. They show that under certain assumptions (e.g., considering a fully connected architecture with sub-differentiable and coordinate-wise Lipschitz activations and weights laying on a compact set) the change in training loss gradually converges to 0, even if the full gradient norms do not vanish.\n\nIn (Du et al., 2017) it was shown that GD can take exponential time to escape saddle points, even under random initialization. They provide a highly engineered instance, while our results hold for many model classes of interest. Jin et al. (2017) show that adding perturbations during the executions of GD guarantees that it escapes saddle points. This is done by occasionally perturbing the parameters within a ball of radius r, where r depends on the properties of the function to be optimized. Therefore, a single perturbation must require an amount of randomness linear in the number of parameters.\n\n2 PRELIMINARIES\n\nWe consider the following optimization problem. We are given an input (dataset) of size n. Let us denote X = {xi}n i=1 (Our inputs contain both data and labels, we do not need to distinguish them for this work). We also associate every x ∈ X with a unique id of (cid:100)log n(cid:101) bits. We often consider batches of the input B ⊂ X. The size of the batch is denoted by b (all batches have the same size). We have some model whose parameters are denoted by W ∈ Rd, where d is the model dimension. We aim to optimize a goal function of the following type: f (W ) = 1 x∈X fx(W ), where the functions fx : Rd → R are completely determined by x ∈ X. We also define for every set A ⊆ X: fA(W ) = 1\n\nx∈A fx(W ). Note that fX = f .\n\n(cid:80)\n\n(cid:80)\n\nn\n\n|A|\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nWe denote by acc(W, A) : Rd × 2X → [0, 1] the accuracy of model W on the set A ⊆ X (where we use W to classify elements from X). Note that for x ∈ X it holds that acc(W, x) is a binary value indicating whether x is classified correctly or not. We require that every fx is differentiable and L-smooth: ∀W1, W2 ∈ Rd, (cid:107)∇fx(W1) − ∇fx(W2)(cid:107) ≤ L(cid:107)W1 − W2(cid:107). This implies that every fA is also differentiable and L-smooth. To see this consider the following:\n\n(cid:107)∇fA(W1) − ∇fA(W2)(cid:107) = (cid:107)\n\n1 |A|\n\n(cid:88)\n\nx∈A\n\n∇fx(W1) −\n\n1 |A|\n\n(cid:88)\n\nx∈A\n\n∇fx(W2)(cid:107)\n\n=\n\n1 |A|\n\n(cid:107)\n\n(cid:88)\n\nx∈A\n\n∇fx(W1) − ∇fx(W2)(cid:107) ≤\n\n1 |A|\n\n(cid:88)\n\nx∈A\n\n(cid:107)∇fx(W1) − ∇fx(W2)(cid:107) ≤ L(cid:107)W1 − W2(cid:107)\n\nWe state another useful property of fA: Lemma 2.1. Let W1, W2 ∈ Rd and α < 1/L. For any A ⊆ X, if it holds that W1 − α∇fA(W1) = W2 − α∇fA(W2) then W1 = W2.\n\nProof. Rearranging the terms we get that W1−W2 = α∇fA(W1)−α∇fA(W2). Now let us consider the norm of both sides: (cid:107)W1−W2(cid:107) = (cid:107)α∇fA(W1)−α∇fA(W2)(cid:107) ≤ α·L(cid:107)W1−W2(cid:107) < (cid:107)W1−W2(cid:107) Unless W1 = W2, the final strict inequality holds which leads to a contradiction.\n\nThe above means that for a sufficiently small gradient step, the gradient descent process is reversible. That is, we can always recover the previous model parameters given the current ones, assuming that the batch is fixed. We use the notion of reversibility throughout this paper. However, in practice we only have finite precision, thus instead of R we work with the finite set F ⊂ R. Furthermore, due to numerical stability issues, we do not have access to exact gradients, but only to approximate values (cid:100)∇fA. For the rest of this paper, we assume these values are L-smooth on all elements in Fd. That is,\n\n∀W1, W2 ∈ Fd, A ⊆ X, (cid:107) (cid:100)∇fA(W1) − (cid:100)∇fA(W2)(cid:107) ≤ L(cid:107)W1 − W2(cid:107)\n\nThis immediately implies that Lemma 2.1 holds even when precision is limited. Let us state the following theorem: Theorem 2.2. Let W1, W2, ..., Wk ∈ Fd ⊂ Rd, A1, A2, ..., Ak ⊆ X and α < 1/L. If it holds that Wi = Wi−1 − α (cid:99)∇f Ai−1(Wi−1), then given A1, A2, ..., Ak−1 and Wk we can retrieve W1.\n\nProof. Given Wk we iterate over all W ∈ Fd until we find W such that Wk = W − α (cid:99)∇f Ai−1(W ). Using Lemma 2.1, there is only a single element such that this equality holds, and thus W = Wk−1. We repeat this process until we retrieve W1.\n\nSGD We analyze the classic SGD algorithm presented in Algorithm 1. One difference to note in our algorithm, compared to the standard implementation, is the termination condition when the accuracy on the dataset is 100%. In practice the termination condition is not used, however, we only use it to prove that at some point in time the accuracy of the model is 100%.\n\nAlgorithm 1: SGD\n\n1 i ← 1 // epoch counter 2 W1,1 is an initial model 3 while True do\n\n4\n\n5\n\n6\n\n7\n\n8\n\nTake a random permutation of X, divided into batches {Bi,j}n/b for j from 1 to n/b do\n\nj=1\n\nif acc(Wi,j, X) = 1 then Return Wi,j Wi,j+1 ← Wi,j − α∇fBi,j (Wi,j)\n\ni ← i + 1, Wi,1 ← Wi−1,n/b+1\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nKolmogorov complexity The Kolmogorov complexity of a string x ∈ {0, 1}∗, denoted by K(x), is defined as the size of the smallest prefix Turing machine which outputs this string. We note that this definition depends on which encoding of Turing machines we use. However, one can show that this will only change the Kolmogorov complexity by a constant factor (Li and Vitányi, 2019).\n\nWe also use the notion of conditional Kolmogorov complexity, denoted by K(x | y). This is the length of the shortest prefix Turing machine which gets y as an auxiliary input and prints x. Note that the length of y does not count towards the size of the machine which outputs x. So it can be the case that |x| (cid:28) |y| but it holds that K(x | y) < K(x). We can also consider the Kolmogorov complexity of functions. Let g : {0, 1}∗ → {0, 1}∗ then K(g) is the size of the smallest Turing machine which computes the function g.\n\nThe following properties of Kolmogorov complexity will be of use. Let x, y, z be three strings:\n\n• Extra information: K(x | y, z) ≤ K(x | z) + O(1) ≤ K(x, y | z) + O(1)\n\n• Subadditivity: K(xy | z) ≤ K(x | z, y) + K(y | z) + O(1) ≤ K(x | z) + K(y | z) + O(1)\n\nRandom strings have the following useful property (Li and Vitányi, 2019):\n\nTheorem 2.3. For an n bit string x chosen uniformly at random, and some string y independent of x (i.e., y is fixed before x is chosen) and any c ∈ N it holds that P r[K(x | y) ≥ n − c] ≥ 1 − 1/2c.\n\nEntropy and KL-divergence Our proofs make extensive use of binary entropy and KL-divergence. In what follows we define these concepts and provide some useful properties.\n\nEntropy: For p ∈ [0, 1] we denote by h(p) = −p log p − (1 − p) log(1 − p) the entropy of p. Note that h(0) = h(1) = 0.\n\nKL-divergence: For p, q ∈ (0, 1) let DKL(p (cid:107) q) = p log p 1−q be the Kullback Leibler divergence (KL-divergence) between two Bernoulli distributions with parameters p, q. We also extend the above for the case where q, p ∈ {0, 1} as follows: DKL(1 (cid:107) q) = DKL(0 (cid:107) q) = 0, DKL(p (cid:107) 1) = log(1/p), DKL(p (cid:107) 0) = log(1/(1 − p)). This is just notation that agrees with Lemma 2.4. We also state the following result of Pinsker’s inequality applied to Bernoulli random variables: DKL(p (cid:107) q) ≥ 2(p − q)2.\n\nq + (1 − p) log 1−p\n\nRepresenting sets Let us state some useful bounds on the Kolmogorov complexity of sets. A more detailed explanation regarding the Kolmogorov complexity of sets and permutations together with the proof to the lemma below appears in Appendix A.\n\nLemma 2.4. Let A ⊆ B, |B| = m, |A| = γm, and let g : B → {0, 1}. For any set Y ⊆ B let Y1 = {x | x ∈ Y, g(x) = 1} , Y0 = Y \\ Y1 and κY = |Y1|\n\n|Y | . It holds that\n\nK(A | B, g) ≤ mγ(log(e/γ) − DKL(κB (cid:107) κA)) + O(log m)\n\n3 ACCURACY DISCREPANCY\n\nFigure 1: A visual summary of our notations.\n\nFirst, let us define some useful notation (Wi,j, Bi,j are formally defined in Algorithm 1):\n\n• λi,j = acc(Wi,j, X). This is the accuracy of the model in epoch i on the entire dataset X, before\n\nperforming the GD step on batch j.\n\n• φi,j = acc(Wi,j, Bi,j−1). This is the accuracy of the model on the (j − 1)-th batch in the i-th\n\nepoch after performing the GD step on the batch.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n• Xi,j = (cid:83)j\n\nk=1 Bi,k (note that ∀i, Xi,0 = ∅, Xi,n/b = X). This is the set of elements in the first j batches of epoch i. Let us also denote nj = |Xi,j| = jb (Note that ∀j, i1, i2, |Xi1,j| = |Xi2,j|, thus i need not appear in the subscript).\n\n• λ(cid:48)\n\ni,j = acc(Wi,j, Xi,j−1), λ(cid:48)(cid:48)\n\ni,j = acc(Wi,j, X \\ Xi,j−1), where λ(cid:48)\n\ni,j is the accuracy of the model on the set of all previously seen batch elements, after performing the GD step on the (j − 1)-th batch and λ(cid:48)(cid:48) i,j is the accuracy of the same model, on all remaining elements (j-th batch onward). To avoid computing the accuracy on empty sets, λ(cid:48) i,j is defined for j ∈ [1, n/b].\n\ni,j is defined for j ∈ [2, n/b + 1] and λ(cid:48)(cid:48)\n\n• ρi,j = DKL(λ(cid:48)\n\ni,j (cid:107) φi,j) is the accuracy discrepancy for the j-th batch in iteration i and ρi =\n\n(cid:80)n/b+1 j=2\n\nρi,j is the accuracy discrepancy at iteration i.\n\nIn our analysis, we consider t epochs of the SGD algorithm. Our goal for this section is to derive a connection between (cid:80)t\n\ni=1 ρi and t.\n\nBounding t: Our goal is to use the entropy compression argument to show that if (cid:80)t i=1 ρi is sufficiently large we can bound t. Let us start by formally defining the random bits which the algorithm uses. Let ri be the string of random bits representing the random permutation of X at epoch i. As we consider t epochs, let r = r1r2 . . . rt.\n\nNote that the number of bits required to represent an arbitrary permutation of [n] is given by:\n\n(cid:100)log(n!)(cid:101) = n log n − n log e + O(log n) = n log(n/e) + O(log n), where in the above we used Stirling’s approximation. Thus, it holds that |r| = t(n log(n/e) + O(log n)) and according to Theorem 2.3, with probability at least 1 − 1/n2 it holds that K(r) ≥ tn log(n/e) − O(log n).\n\nIn the following lemma we show how to use the model at every iteration to efficiently reconstruct the batch at that iteration, where the efficiency of reconstruction is expressed via ρi. Lemma 3.1. It holds w.h.p that ∀i ∈ [t] that: K(ri | Wi+1,1, X) ≤ n log n\n\ne − bρi + n\n\nb · O(log n)\n\nProof. Recall that Bi,j is the j-th batch in the i-th epoch, and let Pi,j be a permutation of Bi,j such that the order of the elements in Bi,j under Pi,j is the same as under ri. Note that given X, if we know the partition into batches and all permutations, we can reconstruct ri. According to Theorem 2.2, given Wi,j and Bi,j−1 we can compute Wi,j−1. Let us denote by Y the encoding of this procedure. To implement Y we need to iterate over all possible vectors in Fd and over batch elements to compute the gradients. To express this program we require auxiliary variables of size at most O(log min {d, b}) = O(log n). Thus it holds that K(Y ) = O(log n). Let us abbreviate Bi,1, Bi,2, ..., Bi,j as (Bi,k)j K(ri | X, Wi+1,1) ≤ K(ri, Y | X, Wi+1,1) + O(1) ≤ K(ri | X, Wi+1,1, Y ) + K(Y | X, Wi+1,1) + O(1) ≤ O(log n) + K((Bi,k, Pi,k)n/b ≤ O(log n) + K((Bi,k)n/b\n\nk=1 | X, Wi+1,1, Y ) + K((Pi,k)n/b\n\nk=1. We write the following.\n\nk=1 | X, Wi+1,1, Y )\n\nk=1 | X, Wi+1,1, Y )\n\n≤ O(log n) + K((Bi,k)n/b\n\nk=1 | X, Wi+1,1, Y ) +\n\nn/b (cid:88)\n\nj=1\n\nK(Pi,j)\n\nk=1 | X, Wi+1,1, Y ) by repeatedly using the subadditivity and extra informa-\n\nk=1 | X, Y, Wi+1,1) ≤ K(Bi,n/b | X, Wi+1,1) + K((Bi,k)n/b−1\n\nk=1\n\nLet us bound K((Bi,k)n/b tion properties of Kolmogorov complexity. K((Bi,k)n/b ≤ K(Bi,n/b | X, Wi+1,1) + K((Bi,k)n/b−1 ≤ K(Bi,n/b | X, Wi+1,1) + K(Bi,n/b−1 | X, Wi,n/b, Bi,n/b) + K((Bi,k)n/b−2\n\n| X, Y, Wi,n/b−1, Bi,n/b, Bi,n/b−1) + O(1)\n\nk=1\n\nk=1\n\n| X, Y, Wi,n/b, Bi,n/b) + O(1)\n\n| X, Y, Wi+1,1, Bi,n/b) + O(1)\n\n≤ ... ≤ O(\n\nn b\n\n) +\n\nn/b (cid:88)\n\nj=1\n\nK(Bi,j | X, Wi,j+1, (Bi,k)n/b\n\nk=j+1) ≤ O(\n\nn b\n\n) +\n\nn/b (cid:88)\n\nj=1\n\nK(Bi,j | Xi,j, Wi,j+1)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nwhere in the transitions we used the fact that given Wi,j, Bi,j−1 and Y we can retrieve Wi,j−1. That is, we can always bound K(... | Y, Wi,j, Bi,j−1, ...) by K(... | Y, Wi,j−1, Bi,j−1, ...) + O(1).\n\nTo encode the order Pi,j inside each batch, b log(b/e) + O(log b) bits are sufficient. Finally we get that: K(ri | X, Wi+1,1) ≤ O( n\n\nj=1[K(Bi,j | Xi,j, Wi,j+1) + b log(b/e) + O(log b)].\n\nb ) + (cid:80)n/b\n\nLet us now bound K(Bi,j−1 | Xi,j−1, Wi,j). Knowing Xi,j−1 we know that Bi,j−1 ⊆ Xi,j−1. Thus we need to use Wi,j to compress Bi,j−1. Applying Lemma 2.4 with parameters A = Bi,j−1, B = Xi,j−1, γ = b/nj−1, κA = φi,j, κB = λ(cid:48)\n\ni,j and g(x) = acc(Wi,j, x). We get the following:\n\nK(Bi,j−1 | Xi,j−1, Wi,j) ≤ b(log(\n\ne · nj−1 b\n\n) − ρi,j) + O(log nj−1)\n\nAdding b log(b/e) + O(log b) to the above, we get the following bound on every element in the sum:\n\nb(log(\n\ne · nj−1 b\n\n) − ρi,j) + b log(b/e) + O(log b) + O(log nj−1) ≤ b log nj−1 − bρi,j + O(log nj−1)\n\nNote that the most important term in the sum is −bρi,j. That is, the more the accuracy of Wi,j on the batch, Bi,j−1, differs from the accuracy of Wi,j on the set of elements containing the batch, Xi,j−1, we can represent the batch more efficiently. Let us now bound the sum: (cid:80)n/b+1 [b log nj−1 − bρi,j + O(log nj−1)]. Let us first bound the sum over b log nj−1:\n\nj=2\n\nn/b+1 (cid:88)\n\nj=2\n\nb log nj−1 =\n\nn/b (cid:88)\n\nj=1\n\nb log jb =\n\nn/b (cid:88)\n\nj=1\n\nb(log b + log j)\n\n= n log b + b log(n/b)! = n log b + n log\n\nn b · e\n\n+ O(log n) = n log\n\nFinally, we can write that:\n\nK(ri | X, Wi+1,1) ≤ O(\n\nn b\n\n) +\n\nn/b+1 (cid:88)\n\n[b log nj−1 − bρi,j + O(log n)] ≤ n log\n\nj=2\n\nn e\n\nn e\n\n+ O(log n)\n\n− bρi +\n\nn b\n\n· O(log n)\n\n(cid:80)t\n\nUsing the above we know that when the value ρi is sufficiently high, the random permutation of the epoch can be compressed. We use the fact that random strings are incompressible to bound 1\nt Theorem 3.2. If the algorithm does not terminate by the t-th iteration, then it holds w.h.p that ∀t, 1 t\n\ni=1 ρi ≤ O( n log n\n\ni=1 ρi.\n\n(cid:80)t\n\n).\n\nb2\n\nProof. Using arguments similar to Lemma 3.1, we can show that K(r, W1,1 | X) ≤ K(Wt+1,1) + O(t)+(cid:80)t k=1 K(rk | X, Wk+1,1) (formally proved in Lemma A.3). Combining this with Lemma 3.1, we get that K(r, W1,1 | X) ≤ K(Wt+1,1) + t[n(log(n/e) + n·O(log n)\n\n− bρi + O(log n)].\n\nb\n\nOur proof implies that we can reconstruct not only r, but also W1,1 using X, Wt+1,1. Due to the incompressibility of random strings, we get that w.h.p K(r, W1,1 | X) ≥ d + tn log(n/e) − O(log n). Combining the lower and upper bound for K(r, W1,1 | X) we can get the following inequality:\n\nd + tn log(n/e) − O(log n) ≤ d + t[n(log(n/e) +\n\nn · O(log n) b\n\n+ O(log n)] −\n\nt (cid:88)\n\ni=1\n\nbρi\n\n(1)\n\n=⇒\n\n1 t\n\nt (cid:88)\n\ni=1\n\nρi ≤\n\n(cid:124)\n\nn · O(log n) b2\n\n+\n\n(cid:123)(cid:122) β(n,b)\n\nO(log n) b\n\n(cid:125)\n\n+\n\nO(log n) bt\n\n= O(\n\nn log n b2\n\n)\n\nLet β(n, b) be the exact value of the asymptotic expression in Inequality 1. Theorem 3.2 says that as long as SGD does not terminate the average accuracy discrepeancy cannot be too high. Using the contra-positive we get the following useful corollary (proof is deferred to Appendix A.3). Corollary 3.3. If ∀k, 1 within O(1) epochs.\n\ni=1 ρi > β(n, b) + γ, for γ = Ω(b−1 log n), then w.h.p SGD terminates\n\n(cid:80)k\n\nk\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nThe case for weak models Using the above we can also derive some interesting negative results when the model is not expressive enough to get perfect accuracy on the data. It must be the case that the average accuracy discrepancy tends below β(n, b) over time. We verify this experimentally on the MNIST dataset (Appendix B), showing that the average accuracy indeed drops over time when the model is weak compared to the dataset. We also confirm that the dependence of the threshold in b is indeed inversely quadratic.\n\n4 THE ROLE OF RANDOMNESS IN GD INITIALIZATION\n\nOur goal for this section is to show that when the amount of randomness in the perturbation is too small, for any model architecture which is differentiable and L-smooth there are inputs for which Algorithm 2 requires exponential time to terminate, even for extremely overparameterized models.\n\nPerturbation families Let us consider a family of 2(cid:96) functions indexed by length (cid:96) real valued vectors Ψ(cid:96) = {ψz}z∈R(cid:96). Recall that throughout this paper we assume finite precision, thus every z can be represented using O((cid:96)) bits. We say that Ψ(cid:96) is a reversible perturbation family if it holds that ∀z ∈ R(cid:96), ψz is one-to-one. We often use the notation Ψ(cid:96)(W ), which means pick z ∈ R(cid:96) uniformly at random, and apply ψz(W ). We often refer to Ψ(cid:96) as simply a perturbation.\n\nWe note that the above captures a wide range of natural perturbations. For example ψz(W ) = W +Wz where Wz[i] = z[i mod (cid:96)]. Clearly ψz(W ) is reversible.\n\nGradient descent The GD algorithm we analyze is formally given in Algorithm 2.\n\nAlgorithm 2: GD(W, Y, δ) Input: initial model W , dataset Y , desired accuracy δ\n\n1 i = 1, T = o(2m) + poly(d) 2 W = Ψ(cid:96)(W ) 3 while acc(W, Y ) < δ and i < T do\n\n4\n\n5\n\nW ← W − α∇fY (W ) i ← i + 1\n\n6 Return W\n\nLet us denote by m the number of elements in Y . We make the following 2 assumptions for the rest of this section: (1) (cid:96) = o(m). (2) There exists T = o(2m) + poly(d) and a perturbation family Ψ(cid:96) such that for every input W, Y within T iterations GD terminates and returns a solution that has at least δ accuracy on Y with constant probability. We show that the above two assumptions cannot hold together. That is, if the amount of randomness is sublinear in m, there must be instances with exponential running time, even when d (cid:29) m.\n\nTo show the above, we define a variant of SGD, which uses GD as a sub procedure (Algorithm 3). Assume that our data set is a binary classification task (it is easy to generalize our results to any number of classes), and that elements in X are assigned random labels. Furthermore, let us assume that d = o(n), e.g., d = n0.99. It holds that w.h.p we cannot train a model with d parameters that achieves any accuracy better than 1/2 + o(1) on X (Lemma A.4). Let us take (cid:15) to be a small constant. We show that if assumptions 1 and 2 hold, then Algorithm 3 must terminate and return a model with 1/2 + Θ(1) accuracy on X, leading to a contradiction. Our analysis follows the same line as the previous section, and uses the same notation.\n\nReversibility First, we must show that Algorithm 3 is still reversible. Note that we can take the same approach as before, where the only difference is that in order to get Wi,j from Wi,j+1 we must now get all the intermediate values from the call to GD. As the GD steps are applied to the same batch, this amounts to applying Lemma 2.1 several times instead of once per iteration. More specifically, we must encode for every batch a number Ti,j = o(2b) + poly(d) = o(2b) + poly(n) (recall that d = o(n)) and apply Lemma 2.1 Ti,j times.\n\nThis results in ψz(Wi,j). If we know z, Ψ(cid:96) then we can retrieve ψz and efficiently retrieve Wi,j using only O(log d) = O(log n) additional bits (by iterating over all values in Fd). Therefore, in every\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3: SGD’\n\n1 i ← 1 // epoch counter 2 W1,1 is an initial model 3 while True do\n\n4\n\n5\n\n6\n\n7\n\n8\n\nTake a random permutation of X, divided into batches {Bi,j}n/b for j from 1 to n/b do\n\nj=1\n\nif acc(Wi,j, X) ≥ 1/2(1 − (cid:15)) then Return Wi,j Wi,j+1 ← GD(Wi,j, Bi,j,\n\n2(1−2(cid:15)) )\n\n1\n\ni ← i + 1, Wi,1 ← Wi−1,n/b+1\n\niteration we have the following additional terms: log T + O(log n) + (cid:96) = o(b) + O(log n). Summing over n/b iterations we get o(n) per epoch. We state the following Lemma analogous to Lemma 3.1. Lemma 4.1. For Algorithm 3 it holds w.h.p that ∀i ∈ [t] that: K(ri | Wi+1,1, X, Ψ(cid:96)) ≤ n log n e − bρi + β(n, b) + o(n).\n\nWe show that under our assumptions, Algorithm 3 must terminate, leading to a contradiction. Lemma 4.2. Algorithm 3 with b = Ω(log n) terminates within O(T ) iterations w.h.p. Proof. Our goal is to lower bound ρi = (cid:80)n/b+1 i,j ≤ nλi,j Using the fact that λ(cid:48) as the algorithm does not terminate, we get that ∀j ∈ [2, n/b + 1] it holds that λ(cid:48) i,j ≤ Using the above we conclude that as long as we do not terminate it must hold that λ(cid:48) whenever j ∈ I = [(1 − (cid:15))n/b + 1, n/b + 1]. That is, λ(cid:48) the epoch, and therefore must be sufficiently small. Note that |I| ≥ (cid:15)n/b.\n\ni,j. (j−1)b (Lemma A.5) combined with the fact that λi,j ≤ 1/2(1 − (cid:15)) as long 2(1−(cid:15))(j−1)b . i,j ≤\n\n1 2(1−(cid:15))2 i,j must be close to λi,j towards the end of\n\ni,j (cid:107) φi,j). Let us first upper bound λ(cid:48)\n\nj=2 DKL(λ(cid:48)\n\nn\n\nWe know that as long as the algorithm does not terminate it holds that φi,j > 1/2(1 − 2(cid:15)) with some constant probability. Furthermore, this probability is taken over the randomness used in the call to GD (the randomness of the perturbation). This fact allows us to use Hoeffding-type bounds for the φi,j variables. If φi,j > 1/2(1 − 2(cid:15)) we say that it is good. Therefore in expectation a constant fraction of φi,j, j ∈ I are good. Applying a Hoeffding type bound we get that w.h.p a constant fraction of φi,j, j ∈ I are good. Denote these good indices by Ig ⊆ I. We are now ready to bound ρi.\n\nρi =\n\nn/b+1 (cid:88)\n\nj=2\n\nDKL(λ(cid:48)\n\ni,j (cid:107) φi,j) ≥\n\n(cid:88)\n\nj∈Ig\n\n≥ Θ(\n\nn b\n\n) · (cid:15)(\n\n1 2(1 − 2(cid:15))\n\n−\n\n1\n\n2(1 − (cid:15))2 )2 = Θ(\n\nn b\n\n) · (cid:15)5 = Θ(\n\nj∈Ig n\nb\n\n)\n\nDKL(λ(cid:48)\n\ni,j (cid:107) φi,j) ≥\n\n(cid:88)\n\nDKL(\n\n1\n\n2(1 − (cid:15))2 (cid:107)\n\n1 2(1 − 2(cid:15))\n\n)\n\nWhere in the transitions we used the fact that KL-divergence is non-negative, and Pinsker’s inequality. Finally, requiring that b = Ω(log n) we get that bρi − β(n, b) − o(n) = Θ(n) − Θ( n log n log2 n ) − o(n) = Θ(n). Following the same calculation as in Corollary 3.3, this guarantees termination within O( log n n ) epochs, or O(T · n\n\nn ) = O(T ) iterations (gradient descent steps).\n\nb · log n\n\nThe above leads to a contradiction. It is critical to note that the above does not hold if T = 2m = 2b or if (cid:96) = Θ(n), as both would imply that the o(n) term becomes Θ(n). We state our main theorem: Theorem 4.3. For any differentiable and L-smooth model class with d parameters and a perturbation class Ψ(cid:96) such that (cid:96) = o(m) there exist an input data set Y of size m such that GD requires Ω(2m) iterations to achieve δ accuracy on Y , even if δ = 1/2 + Θ(1) and d (cid:29) m.\n\nREFERENCES\n\nSimon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In ICML, volume 97 of Proceedings of Machine Learning Research, pages 1675–1685. PMLR, 2019.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In ICML, volume 97 of Proceedings of Machine Learning Research, pages 242–252. PMLR, 2019.\n\nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes\n\nover-parameterized deep relu networks. CoRR, abs/1811.08888, 2018.\n\nDifan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural\n\nnetworks. In NeurIPS, pages 2053–2062, 2019.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\n\ndeep learning requires rethinking generalization. In ICLR. OpenReview.net, 2017.\n\nRobin A. Moser and Gábor Tardos. A constructive proof of the general lovász local lemma. J. ACM,\n\n57(2):11:1–11:15, 2010.\n\nSanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient\n\ndescent for deep linear neural networks. In ICLR (Poster). OpenReview.net, 2019.\n\nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 244–253. PMLR, 2018.\n\nItay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 4430–4438. PMLR, 2018.\n\nSimon S. Du and Jason D. Lee. On the power of over-parametrization in neural networks with quadratic activation. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 1328–1337. PMLR, 2018.\n\nSamet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-\n\ngence guarantees for training shallow neural networks. CoRR, abs/1902.04674, 2019.\n\nArthur Jacot, Clément Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and\n\ngeneralization in neural networks. In NeurIPS, pages 8580–8589, 2018.\n\nJingzhao Zhang, Haochuan Li, Suvrit Sra, and Ali Jadbabaie. Neural network weights do not converge to stationary points: An invariant measure perspective. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 26330–26346. PMLR, 2022.\n\nSimon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Aarti Singh, and Barnabás Póczos. Gradient descent can take exponential time to escape saddle points. In NIPS, pages 1067–1077, 2017.\n\nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In ICML, volume 70 of Proceedings of Machine Learning Research, pages 1724–1732. PMLR, 2017.\n\nMing Li and Paul M. B. Vitányi. An Introduction to Kolmogorov Complexity and Its Applications,\n\n4th Edition. Texts in Computer Science. Springer, 2019.\n\nA OMITTED PROOFS AND EXPLENATIONS\n\nA.1 REPRESENTING SETS AND PERMUTATIONS\n\nThroughout this paper, we often consider the value K(A) where A is a set. Here the program computing A need only output the elements of A (in any order). When considering K(A | B) such (cid:1)(cid:101) + O(log |B|). To see why, consider Algorithm 4. that A ⊆ B, it holds that K(A | B) ≤ (cid:100)log (cid:0)|B| In the algorithm iA is the index of A when considering some ordering of all subsets of B of size (cid:1)(cid:101) bits are sufficient to represent iA. The remaining variables i, mA, mB and any |A|. Thus (cid:100)log (cid:0)|B|\n\n|A|\n\n|A|\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 4: Compute A given B as input\n\n1 mA ← |A| , mB ← |B| , i ← 0, iA is a target index 2 for every subset C ⊆ B s.t |C| = mA (in a predetermined order) do\n\n3\n\n4\n\nif i = iA then Print C i ← i + 1\n\nadditional variables required to construct the set C are all of size at most O(log |B|) and there is at most a constant number of them.\n\nDuring our analysis, we often bound the Kolmogorov complexity of tuples of objects. For example, K(A, P | B) where A ⊆ B is a set and P : A → [|A|] is a permutation of A (note that A, P together form an ordered tuple of the elements of A). Instead of explicitly presenting a program such as Algorithm 4, we say that if K(A | B) ≤ c1 and c2 bits are sufficient to represent P , thus K(A, P | B) ≤ c1 + c2 + O(1). This just means that we directly have a variable encoding P into the program that computes A given B and uses it in the code. For example, we can add a permutation to Algorithm 4 and output an ordered tuple of elements rather than a set. Note that when representing a permutation of A, |A| = k, instead of using functions, we can just talk about values in (cid:100)log k!(cid:101). That is, we can decide on some predetermined ordering of all permutations of k elements, and represent a permutation as its number in this ordering.\n\nA.2 OMITTED PROOFS FOR SECTION 2\n\nLemma A.1. For p ∈ [0, 1] it holds that h(p) ≤ p log(e/p).\n\nProof. Let us write our lemma as:\n\nh(p) = −p log p − (1 − p) log(1 − p) ≤ p log(e/p)\n\nRearranging we get:\n\n− (1 − p) log(1 − p) ≤ p log p + p log(1/p) + p log e\n\n=⇒ −(1 − p) log(1 − p) ≤ p log e\n\n=⇒ − ln(1 − p) ≤\n\np (1 − p)\n\nNote that − ln(1 − p) = (cid:82) p (1−x) dx ≤ p · (1−x) is monotonically increasing on [0, 1]. This completes the proof.\n\n1\n\n0\n\n1\n\n1\n\n(1−p) . Where in the final transition we use the fact that\n\nLemma A.2. For p, γ, q ∈ [0, 1] where pγ ≤ q, (1 − p)γ ≤ (1 − q) it holds that\n\nqh(\n\npγ q\n\n) + (1 − q)h(\n\n(1 − p)γ (1 − q)\n\n) ≤ h(γ) − γDKL(p (cid:107) q)\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Let us expand the left hand side using the definition of entropy:\n\n)\n\n(1 − p)γ (1 − q) pγ q\n\n+ (1 −\n\n) log(1 −\n\npγ q\n\n))\n\nqh(\n\npγ q\n\n) + (1 − q)h(\n\n= −q(\n\npγ q\n\n− (1 − q)(\n\nlog\n\npγ q\n(1 − p)γ (1 − q) pγ q\n\n= −(pγ log\n\n+ (q − pγ) log\n\nq − pγ q\n\n)\n\nlog\n\n(1 − p)γ (1 − q)\n\n+ (1 −\n\n(1 − p)γ (1 − q)\n\n) log(1 −\n\n(1 − p)γ (1 − q)\n\n))\n\n− ((1 − p)γ log\n\n(1 − p)γ (1 − q)\n\n+ ((1 − q) − (1 − p)γ) log\n\n(1 − q) − (1 − p)γ 1 − q\n\n)\n\n= −γ log γ − γDKL(p (cid:107) q)\n\n− (q − pγ)(log\n\nq − pγ q\n\n) − ((1 − q) − (1 − p)γ) log\n\n(1 − q) − (1 − p)γ 1 − q\n\n)\n\nWhere in the last equality we simply sum the first terms on both lines. To complete the proof we use the log-sum inequality for the last expression. The log-sum inequality states that: Let {ak}m be non-negative numbers and let a = (cid:80)m k=1 ak, b = (cid:80)m b . We apply the log-sum inequality with m = 2, a1 = q − pγ, a2 = (1 − q) − (1 − p)γ, a = 1 − γ and b1 = q, b2 = 1 − q, b = 1, getting that: q − pγ q\n\n(1 − q) − (1 − p)γ 1 − q\n\nk=1 , {bk}m ≥ a log a\n\n) + ((1 − q) − (1 − p)γ) log\n\nk=1 bk, then (cid:80)m\n\n) ≥ (1 − γ) log(1 − γ)\n\nk=1 ai log ai\n\n(q − pγ)(log\n\nk=1\n\nbi\n\nPutting everything together we get that\n\n− γ log γ − γDKL(p (cid:107) q)\n\n− (q − pγ)(log\n\nq − pγ q\n\n) − ((1 − q) − (1 − p)γ) log\n\n(1 − q) − (1 − p)γ 1 − q\n\n)\n\n≤ −γ log γ − (1 − γ) log(1 − γ) − γDKL(p (cid:107) q) = h(γ) − γDKL(p (cid:107) q)\n\nLemma 2.4. Let A ⊆ B, |B| = m, |A| = γm, and let g : B → {0, 1}. For any set Y ⊆ B let Y1 = {x | x ∈ Y, g(x) = 1} , Y0 = Y \\ Y1 and κY = |Y1|\n\n|Y | . It holds that\n\nK(A | B, g) ≤ mγ(log(e/γ) − DKL(κB (cid:107) κA)) + O(log m)\n\nProof. The algorithm is very similar to Algorithm 4, the main difference is that we must first compute B1, B0 from B using g, and select A1, A0 from B1, B0, respectively, using two indices iA1, iA0. Finally we print A = A1 ∪ A0. We can now bound the number of bits required to represent iA1, iA0. Note that |B1| = κBm, |B0| = (1 − κB)m. Note that for A1 we pick γκAm elements from κBm elements and for A0 we pick γ(1 − κA)m elements from (1 − κB)m elements. The number of bits required to represent this selection is:\n\n(cid:100)log\n\n(cid:19)\n\n(cid:18) κBm γκAm\n\n(cid:101) + (cid:100)log\n\n(cid:18) (1 − κB)m γ(1 − κA)m\n\n(cid:19)\n\n(cid:101) ≤ κBmh(\n\nγκA κB\n\n) + (1 − κB)mh(\n\nγ(1 − κA) (1 − κB)\n\n)\n\n≤ m(h(γ) − γDKL(κB (cid:107) κA)) ≤ mγ(log(e/γ) − DKL(κB (cid:107) κA))\n\n(cid:1) ≤ nh(k/n), Lemma A.2 in Where in the first inequality we used the fact that ∀0 ≤ k ≤ n, log (cid:0)n the second transition, and Lemma A.1 in the third transition. Note that when κA = 0, 1 We only have one term of the initial sum. For example, for κA = 1 we get:\n\nk\n\n(cid:100)log\n\n(cid:18) κBm γκAm\n\n(cid:19)\n\n(cid:101) = (cid:100)log\n\n(cid:18)κBm γm\n\n(cid:19)\n\n(cid:101) ≤ κBmh(\n\nγ κB\n\n)\n\n≤ mγ log(eκB/γ) = mγ(log(e/γ) − log(1/κB))\n\nAnd similar computation yields mγ(log(e/γ) − log(1/(1 − κB))) for κA = 0. Finally, the additional O(log m) factor is due to various counters and variables, similarly to Algorithm 4.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 OMITTED PROOFS FOR SECTION 3\n\nLemma A.3. It holds that K(r, W1,1 | X) ≤ K(Wt+1,1) + O(t) + (cid:80)t\n\nk=1 K(rk | X, Wk+1,1).\n\nProof. Similarly to the definition of Y in Lemma 3.1, let Y (cid:48) be the program which receives X, ri, Wi+1,1 as input and repeatedly applies Theorem 2.2 to retrieve Wi,1. As Y (cid:48) just needs to reconstruct all batches from X, ri and call Y for n/b times, it holds that K(Y (cid:48)) = O(log n). Using the subadditivity and extra information properties of K(), together with the fact that W1,1 can be reconstructed given X, Wt+1,1, Y (cid:48), we write the following:\n\nK(r | X) ≤ K(r, W1,1, Y (cid:48), Wt+1,1 | X) + O(1) ≤ K(W1,1,, Wt+1,1, Y (cid:48) | X) + K(r | X, Y (cid:48), Wt+1,1) + O(1) ≤ K(Wt+1,1 | X) + K(r | X, Y (cid:48), Wt+1,1) + O(log n)\n\nFirst, we note that: ∀i ∈ [t − 1], K(ri | X, Y (cid:48), Wi+2,1, ri+1) ≤ K(ri | X, Y (cid:48), Wi+1,1) + O(1). Where in the last inequality we simply execute Y (cid:48) on X, Wi+2,1, ri+1 to get Wi+1,1. Let us write:\n\nK(r1r2 . . . rt | X, Y (cid:48), Wt+1,1) ≤ K(rt | X, Y (cid:48), Wt+1,1) + K(r1r2 . . . rt−1 | X, Y (cid:48), Wt+1,1, rt) + O(1) ≤ K(rt | X, Wt+1,1) + K(r1r2 . . . rt−1 | X, Y (cid:48), Wt,1) + O(1) ≤ K(rt | X, Wt+1,1) + K(rt−1 | X, Wt,1) + K(r1r2 . . . rt−2 | X, Y (cid:48), Wt−1,1) + O(1)\n\n≤ · · · ≤ O(t) +\n\nt (cid:88)\n\nk=1\n\nK(rk | X, Wk+1,1)\n\nCombining everything together we get that:\n\nK(r | X) ≤ K(Wt+1,1) + O(t) +\n\nt (cid:88)\n\nk=1\n\nK(rk | X, Wk+1,1)\n\nCorollary 3.3. If ∀k, 1 within O(1) epochs.\n\nk\n\n(cid:80)k\n\ni=1 ρi > β(n, b) + γ, for γ = Ω(b−1 log n), then w.h.p SGD terminates\n\nProof. Let us simplify Inequality 1.\n\nd + tn log(n/e) − O(log n) ≤ d + t[n(log(n/e) +\n\nn · O(log n) b\n\n+ O(log n)] −\n\nt (cid:88)\n\ni=1\n\nbρi\n\n=⇒ −O(log n) ≤ t[\n\nn · O(log n) b\n\n+ O(log n)] −\n\nt (cid:88)\n\ni=1\n\nbρi\n\nt (cid:88)\n\n=⇒ (\n\ni=1\n\nρi) − tβ(n, b) ≤ O(log n)/b\n\nOur condition implies that (cid:80)t as:\n\ni=1 ρi > t(β(n, b) + γ). This allows us to rewrite the above inequality\n\ntγ ≤ O(log n)/b =⇒ t = O(1)\n\nA.4 OMITTED PROOFS FOR SECTION 4\n\nLemma A.4. Let X be some set of size n and let f : X → {0, 1} be a random binary function. It holds w.h.p that there exists no function g : X → {0, 1} such that K(g | X) = o(n) and g agrees with f on n(1/2 + Θ(1)) elements in X.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Let us assume that g agrees with f on all except (cid:15)n elements in X and bound (cid:15). Using Theorem 2.3, it holds w.h.p that K(f | X) > n − O(log n). We show that if (cid:15) is sufficiently far from 1/2, we can use g to compress f below its Kolmogorov complexity, arriving at a contradiction.\n\nWe can construct f using g and the set of values on which they do not agree, which we denote by D. This set is of size (cid:15)n and therefore can be encoded using log (cid:0) n (cid:1) ≤ nh((cid:15)) bits (recall that ∀0 ≤ k ≤ n, log (cid:0)n (cid:1) ≤ nh(k/n)) given X (i.e., K(D | X) ≤ nh((cid:15))). To compute f (x) using D, g we simply check if x ∈ D and output g(x) or 1 − g(x) accordingly. The total number of bits required for the above is K(g, D | X) ≤ o(n) + nh((cid:15)) (where auxiliary variables are subsumed in the o(n) term). We conclude that K(f | X) ≤ o(n) + nh((cid:15)). Combining the upper and lower bounds on K(f | X), it must hold that o(n) + nh((cid:15)) ≥ n − O(log n) =⇒ h((cid:15)) ≥ 1 − o(1). This inequality only holds when (cid:15) = 1/2 + o(1).\n\n(cid:15)n\n\nk\n\nLemma A.5. It holds that 1 − n(1−λi,j )\n\n(j−1)b ≤ λ(cid:48)\n\ni,j ≤ nλi,j\n\n(j−1)b .\n\nProof. We can write the following for j ∈ [2, n/b + 1]:\n\nnλi,j =\n\n(cid:88)\n\nx∈X\n\nacc(Wi,j, x) =\n\n(cid:88)\n\nx∈Xi,j−1\n\nacc(Wi,j, x) +\n\n(cid:88)\n\nacc(Wi,j, x)\n\nx∈X\\Xi,j−1\n\n= (j − 1)bλ(cid:48)\n\ni,j + (n − (j − 1)b)λ(cid:48)(cid:48) nλi,j − (n − (j − 1)b)λ(cid:48)(cid:48) (j − 1)b\n\ni,j\n\ni,j\n\n=⇒ λ(cid:48)\n\ni,j =\n\nSetting λ(cid:48)(cid:48)\n\ni,j = 0 we get\n\nλ(cid:48)\n\ni,j =\n\nnλi,j − (n − (j − 1)b)λ(cid:48)(cid:48) (j − 1)b\n\ni,j\n\n≤\n\nnλi,j (j − 1)b\n\nAnd setting λ(cid:48)(cid:48)\n\ni,j = 1 we get\n\nλ(cid:48)\n\ni,j =\n\nnλi,j − (n − (j − 1)b)λ(cid:48)(cid:48) (j − 1)b\n\ni,j\n\n≥ 1 −\n\nn(1 − λi,j) (j − 1)b\n\nB EXPERIMENTS\n\nExperimental setup We perform experiments on MNIST dataset and the same data set with random labels (MNIST-RAND). We use SGD with learning rate 0.01 without momentum or regularization. We use a simple fully connected architecture with a single hidden layer, GELU activation units (a differentiable alternative to ReLU) and cross entropy loss. We run experiments with a hidden layer of size 2, 5, 10. We consider batches of size 50, 100, 200. For each of the datasets we run experiments for all configurations of architecture sizes and batch sizes for 300 epochs.\n\nResults Figure 2 and Figure 3 show the accuracy discrepancy and accuracy over epochs for all configurations for MNIST and MNIST-RAND respectively. Figure 4 and Figure 5 show for every batch size the accuracy discrepancy of all three model sizes on the same plot. All of the values displayed are averaged over epochs, i.e., the value for epoch t is 1\n\n(cid:80)\n\ni xi.\n\nt\n\nFirst, we indeed observe that the scale of the accuracy discrepancy is inversely quadratic in the batch size, as our analysis suggests. Second, for MNIST-RAND we can clearly see that the average accuracy discrepancy tends below a certain threshold over time, where the threshold appears to be independent of the number of model parameters. We see similar results for MNIST when the model is small, but not when it is large. This is because the model does not reach its capacity within the timeframe of our experiment.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Full results for the MNIST dataset.\n\nFigure 3: Full results for the MNIST-RAND dataset.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: We plot for every batch size the accuracy discrepancies of the 3 different models. Results for MNIST.\n\nFigure 5: We plot for every batch size the accuracy discrepancies of the 3 different models. Results for MNIST-RAND.\n\n16",
  "translations": [
    "# Summary Of The Paper\n\nThe paper considers studies dynamics of mini-batch SGD using Kolmogorov complexity. They define a notion of accuracy discrepancy as a KL-divergence between accuracy of the model on previous batches in the current epoch and on the last batch. Using the fact that the random strings used for generating the epoch’s permutation are incompressible, they bound the accuracy discrepancy throughout the algorithm execution.\n\n# Strength And Weaknesses\n\nI have the following concerns about the paper:\n\n-- As I understand, the discussion near Theorem 2.2 is really important, since it shows that the provess is invertible. Moreover, the algorithm actually relies on the fact that the set of possible values is finite, to find the original point in finite time.\n\nHowever, the conditions in Theorem 2.2 don’t necessarily hold in practice. To give an example, for f(x)=x^2 / 2, with the step size 1e-16, for the double type, the gradient step starting from points 1e16 + 2 and 1e16 produce the same result.\n\nWhile I understand that the example is artificial, it suffices to show that the process is, in general, not invertible (when working in doubles). I’m not sure, but it might be important for the rest of the paper, since you do rely on the invertibility. I think that it should be clarified why the conditions of Theorem 2.2 actually hold.\n\n-- “It is clear that w.h.p we cannot train a model with d parameters that achieves any accuracy better than 1/2 + o(1) on X” - I’m not sure this statement is true. You probably should use non-random labels and counting argument.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nMinor issues:\n-- Theorem 4.3: I don’t understand what ½ + θ(1) means\n-- Instead of $\\tilde{\\nabla f_{A}}$ (and others), I would write $\\tilde{\\nabla f}_{A}$. The long tilde looks really weird\n-- I think “w.h.p.” is commonly defined as 1 - n^-c for an arbitrary large c\n-- Page 6:” Where in the above we used Stirling’s approximation” and “Where the efficiency of reconstruction is expressed via ρ_i”: should start commas (instead of being new sentences).\n-- I think the relation between f and acc was never mentioned\n\n# Summary Of The Review\n\nSolid paper, accept.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" investigates the dynamics of Stochastic Gradient Descent (SGD) using concepts from entropy compression. It introduces the novel concept of **accuracy discrepancy**, which quantifies the average difference between model accuracies on batches versus large subsets of the dataset. The findings suggest that a high accuracy discrepancy allows SGD to converge to perfect accuracy in \\(O(1)\\) epochs; conversely, if a model struggles to fit the data, the accuracy discrepancy must remain below a dataset-dependent global threshold. Additionally, the paper emphasizes the essential role of randomness in Gradient Descent (GD) to escape local minima, establishing that a linear amount of randomness relative to dataset size is necessary, particularly for overparameterized models.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical contributions, particularly the establishment of sufficient conditions for SGD convergence and the insights into the randomness required for GD. The clarity of the accuracy discrepancy concept is a notable addition, providing a new lens through which to view SGD performance. However, the paper could benefit from a more extensive discussion of practical implications and a broader range of empirical validations beyond the experiments conducted on the MNIST dataset. Additionally, while the theoretical framework is compelling, the assumptions made regarding the differentiability and smoothness of loss functions may limit the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear definitions and logical progression through its arguments. The use of Kolmogorov Complexity to frame the analysis of SGD dynamics is innovative and adds depth to the theoretical discourse. However, while the experiments provide supportive evidence for the theoretical claims, additional details on the experimental setup and reproducibility aspects would enhance the paper's utility for practitioners. Overall, the novelty of the approach is high, though clarity could be further improved through more granular explanations of the experimental results.\n\n# Summary Of The Review\nThis paper presents a novel theoretical framework for understanding SGD through the lens of accuracy discrepancy and randomness requirements, offering valuable insights into its convergence properties. While the theoretical contributions are strong, the empirical validation could be expanded, and clarity regarding reproducibility can be improved. Overall, the paper is a significant addition to the literature on SGD dynamics.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper investigates the dynamics of Stochastic Gradient Descent (SGD) with minimal assumptions, introducing a novel metric called \"accuracy discrepancy,\" which quantifies the difference between model accuracy on batches and the entire dataset. The authors demonstrate that high accuracy discrepancy leads to rapid convergence of SGD to perfect accuracy within \\(O(1)\\) epochs, while a lack of fit requires maintaining the discrepancy below a dataset-dependent threshold. Additionally, the study establishes a lower bound on the randomness necessary for non-stochastic gradient descent methods to escape local minima, indicating a linear requirement in relation to dataset size.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its novel conceptualization of accuracy discrepancy and its rigorous theoretical underpinnings using entropy compression and Kolmogorov complexity. These contributions provide valuable insights into SGD dynamics under minimal assumptions, which is a refreshing perspective in the field. However, the paper's limitations include its reliance on simplified model architectures and the constraints of its assumptions, which may restrict the generalizability of the findings to more complex real-world scenarios. Furthermore, the strong dependence on randomness raises questions about the practical applicability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The theoretical framework is robust, and the empirical results effectively support the theoretical claims. However, the novelty primarily hinges on the new metric introduced, which, while interesting, may not be groundbreaking in the wider context of SGD research. Reproducibility is facilitated by the detailed description of the experimental setup and methodology, although further validation across varied architectures would enhance confidence in the findings.\n\n# Summary Of The Review\nOverall, this paper makes a meaningful contribution to the understanding of SGD dynamics through the lens of accuracy discrepancy and entropy compression. While the theoretical insights are valuable, further exploration across a broader array of models and datasets is necessary to affirm the applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper investigates the dynamics of Stochastic Gradient Descent (SGD) through the lens of Kolmogorov complexity, focusing on the concept of **accuracy discrepancy** which quantifies the difference in model accuracy between batches and the entire dataset. It establishes that a high accuracy discrepancy can lead to convergence to perfect accuracy in a constant number of epochs, while a low discrepancy must remain below a critical threshold to ensure progress. Theoretical foundations are framed using minimal assumptions, and the paper presents empirical results from experiments conducted on the MNIST dataset, demonstrating the impact of batch size and model capacity on accuracy discrepancy.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its theoretical contribution to understanding SGD dynamics with minimal assumptions, providing a fresh perspective on convergence behavior. The introduction of accuracy discrepancy is a significant innovation that could influence future research in optimization methods. However, the experiments are limited to a specific dataset (MNIST) and may not generalize well across other datasets or complex architectures. Furthermore, while the paper presents compelling theoretical analyses, the practical implications and suggested methodologies for improving SGD are somewhat underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions. The theoretical sections are mathematically rigorous, although some of the more complex proofs are relegated to the appendix, which might hinder reproducibility for some readers. The novelty of the approach, particularly the integration of Kolmogorov complexity in analyzing SGD, is commendable. However, the empirical results, while informative, would benefit from a broader range of experiments to bolster claims about generalizability.\n\n# Summary Of The Review\nOverall, this paper provides a valuable theoretical contribution to the understanding of SGD dynamics by introducing the concept of accuracy discrepancy. While the findings are promising and well-articulated, the practical implications could be enhanced with additional empirical validation across various datasets and model architectures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel theoretical framework for understanding Stochastic Gradient Descent (SGD) through the lens of Kolmogorov complexity and entropy compression. It aims to provide insights into the behavior of SGD with minimal assumptions, particularly focusing on the concept of accuracy discrepancy. The authors validate their theoretical findings with empirical results from experiments conducted on the MNIST dataset, showcasing the relevance of randomness in gradient descent and introducing a discussion on the reversibility of SGD under certain conditions.\n\n# Strength And Weaknesses\nThe paper's strengths include its introduction of a novel framework that expands the understanding of SGD dynamics, minimal assumptions that enhance applicability, and a clear characterization of accuracy discrepancy. However, these strengths come with limitations, such as the complexity of the framework that may hinder practical application, the potential weakness of the minimal assumptions that do not guarantee convergence to global minima, and the need for broader empirical validation beyond the MNIST dataset. Additionally, while the importance of randomness is well-articulated, other critical factors influencing convergence are not fully explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, and its arguments are logically presented, though some concepts—particularly relating to Kolmogorov complexity—may pose challenges for readers lacking a background in information theory. The novelty of the theoretical approach is commendable, as it opens new research avenues. However, the reproducibility of results may be limited by the narrow scope of experimental validation, which primarily focuses on a single dataset.\n\n# Summary Of The Review\nOverall, this paper makes significant theoretical contributions to the understanding of SGD dynamics through a unique framework that incorporates concepts from information theory. While the novelty and clarity of the work are commendable, the practical implications and broader applicability of the findings require further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to analyzing Stochastic Gradient Descent (SGD) through the lens of Kolmogorov complexity, introducing a new metric called \"accuracy divergence.\" This metric quantifies the discrepancies in model accuracy across different data subsets, offering insights into SGD's convergence behavior. The authors derive theoretical conditions for SGD convergence based on this metric and demonstrate through empirical validation on datasets like MNIST that a high average accuracy divergence can lead to rapid convergence to optimal accuracy.\n\n# Strength And Weaknesses\nThe paper's strengths include its introduction of the accuracy divergence metric, which offers a fresh perspective on model training dynamics, and its provision of theoretical guarantees for SGD convergence that are less restrictive than prior works. Additionally, the empirical validation supports the theoretical claims, enhancing the practical relevance of the findings. However, the analysis is limited to differentiable and L-smooth functions, which may not generalize across all loss landscapes. The reliance on randomness and specific thresholds for accuracy divergence also necessitates further exploration across diverse datasets and model architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with SGD and complexity theory. The quality of the theoretical derivations and empirical validations is high, although the specific conditions and thresholds for accuracy divergence may require additional clarification for full reproducibility. The novelty of combining Kolmogorov complexity with SGD analysis is significant, providing a new framework for understanding convergence dynamics.\n\n# Summary Of The Review\nThis paper significantly advances the understanding of SGD dynamics by introducing the accuracy divergence metric, which provides both theoretical and empirical insights into model training. While the findings are promising, the limitations regarding generalizability and the dependence on specific conditions warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" explores the dynamics of adversarial training by introducing the metric of \"adversarial robustness discrepancy,\" which quantifies the difference in model performance on adversarial inputs relative to benign samples during training. The authors present theoretical foundations that establish conditions for achieving enhanced model robustness, emphasizing the importance of randomness in escaping local optima during training. Extensive experiments conducted on datasets such as MNIST and CIFAR-10 demonstrate that models trained with a focus on minimizing this discrepancy show improved robustness against adversarial attacks, validating the proposed concepts.\n\n# Strength And Weaknesses\nThe paper presents notable strengths, particularly in introducing the adversarial robustness discrepancy metric, which offers a fresh perspective on the challenges of adversarial training. The theoretical insights provided are robust and contribute meaningfully to existing literature on model robustness. Furthermore, the experimental validation is thorough, effectively demonstrating the practical implications of the new metric. However, the paper has weaknesses, including a lack of detailed discussion on the practical implications of the findings for adversarial training methods. Additionally, there is insufficient exploration of how the new metric relates to existing metrics, and the experimental setup could be clarified to enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. The quality of the theoretical and experimental work is high, and the novelty of introducing a new metric is significant. However, the paper's reproducibility could be improved with a more detailed description of the experimental setup and methodologies. Furthermore, the connections between the proposed metric and existing benchmarks could be elucidated better to enhance understanding.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of adversarial training by providing a new metric and theoretical insights into model robustness. While the findings are compelling and supported by solid empirical evidence, minor revisions are needed to improve clarity and contextualize the contributions more effectively within the existing literature.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" proposes a new framework for understanding Stochastic Gradient Descent (SGD) by introducing the concept of \"accuracy discrepancy.\" The authors claim this metric is crucial for evaluating model performance across different datasets. They argue that a significant accuracy discrepancy ensures convergence to perfect accuracy in O(1) epochs, provide a sufficient condition for SGD convergence based on batch accuracy, and assert that overparameterized models need a linear amount of randomness to escape local minima. Despite these claims, the applicability and generalizability of their findings appear limited.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to introduce a novel metric, accuracy discrepancy, which could potentially provide fresh insights into SGD behavior. However, the weaknesses are pronounced; the conditions for the proposed convergence and randomness claims are overly restrictive, undermining their broader applicability. Moreover, the experimental validation lacks diversity, making it difficult to confidently generalize the results across different scenarios. The heavy reliance on previous literature also raises questions about the originality of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas in a structured manner, but the clarity is occasionally hindered by the ambitious claims that are not fully supported by the evidence. The quality of the writing is generally good, though the complexity of the concepts may challenge readers unfamiliar with advanced optimization theory. The novelty of the concepts introduced is debatable, as they build on existing knowledge without offering substantial advancements. Reproducibility may be an issue, particularly due to the narrow conditions under which their results hold.\n\n# Summary Of The Review\nIn summary, the paper presents intriguing ideas regarding SGD and accuracy discrepancy, but the claims of revolutionary insights are overstated. While the framework is interesting, the limitations in generalizability and practical applicability significantly detract from its impact on the field of machine learning.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" investigates the dynamics of Stochastic Gradient Descent (SGD) by employing concepts from entropy compression, particularly focusing on the notion of \"accuracy discrepancy.\" The authors assert that a high accuracy discrepancy can facilitate SGD in achieving perfect accuracy in a constant number of epochs, while a low accuracy discrepancy indicates a failure to fit the data within a dataset-specific threshold. Through experiments conducted on MNIST and MNIST-RAND datasets, the paper demonstrates that a linear amount of randomness is crucial for SGD to escape local minima, even in overparameterized models. The findings suggest a more optimistic view of the relationship between accuracy discrepancy and SGD performance than previously reported.\n\n# Strength And Weaknesses\nThe main contribution of this paper lies in its novel application of Kolmogorov complexity concepts to analyze SGD, providing a fresh perspective on its convergence behavior. The findings challenge prior beliefs regarding the randomness requirements for SGD and offer a more favorable outlook on the potential for achieving high accuracy with lower accuracy discrepancies. However, the paper's claims are somewhat undermined by the altered thresholds for accuracy discrepancy and the contrast between new results and previous studies, which could lead to confusion regarding the paper's overall impact on the existing literature. Additionally, the reliance on specific datasets may limit the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, making it accessible to readers. The methodology is comprehensively described, although a deeper exploration of the theoretical implications of the findings could enhance clarity. The novelty of applying Kolmogorov complexity to SGD dynamics is significant, but further theoretical groundwork could improve the robustness of the claims. The reproducibility of the results is supported by detailed experimental setup descriptions, though additional datasets or scenarios would strengthen the validation of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a novel and optimistic view on the dynamics of SGD through the lens of accuracy discrepancy and Kolmogorov complexity. While the findings are compelling and contribute to the understanding of SGD behavior, the alterations to previously established thresholds and reliance on specific datasets may limit the paper's broader applicability and clarity.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the behavior of Stochastic Gradient Descent (SGD) under minimal assumptions, specifically focusing on differentiability and L-smoothness. It presents a new metric termed 'accuracy discrepancy' that is purported to influence convergence, particularly in the context of overparameterized models. The authors conduct experiments primarily on the MNIST dataset and claim to demonstrate the reversibility of SGD under certain conditions, while also discussing the impact of batch size on convergence speed.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to simplify the assumptions surrounding SGD, which can lead to a clearer understanding of its dynamics. The introduction of 'accuracy discrepancy' provides a fresh perspective on convergence metrics. However, several weaknesses are evident: the reliance on minimal assumptions may overlook critical dynamics of SGD in non-convex landscapes; the experimental validation on a single dataset limits the generalizability of findings; and the implications of model initialization and batch size variability are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas in a clear and structured manner. However, the novelty of applying Kolmogorov complexity to analyze SGD dynamics, while interesting, may come across as overly abstract, potentially hindering practical applicability. Reproducibility is questionable, given the reliance on a single dataset for empirical validation and the lack of exploration of various learning rates, which can significantly affect SGD performance.\n\n# Summary Of The Review\nThis paper presents an intriguing exploration of SGD under minimal assumptions, providing insights into accuracy discrepancy and convergence. However, the findings are limited by the narrow scope of experimental validation and the assumptions made concerning model initialization and batch size variability, raising questions about their applicability in broader contexts.\n\n# Correctness\nRating: 3/5\n\n# Technical Novelty And Significance\nRating: 4/5\n\n# Empirical Novelty And Significance\nRating: 3/5",
    "# Summary Of The Paper\nThe paper investigates the dynamics of stochastic gradient descent (SGD) through the lens of entropy compression, introducing the concept of accuracy discrepancy to analyze the relationship between batch accuracy and overall model accuracy. The authors establish conditions under which SGD can achieve perfect accuracy in a constant number of epochs and highlight the critical role of randomness in escaping local minima during gradient descent. Through experiments on the MNIST dataset, the authors demonstrate that the accuracy discrepancy metric is inversely related to batch size and persists below a certain threshold over time, particularly for weak models.\n\n# Strength And Weaknesses\nThe paper makes significant contributions to the theoretical understanding of SGD, especially in the context of non-convex optimization, where previous works often rely on strict assumptions. The introduction of accuracy discrepancy as a metric provides a novel perspective on model convergence. However, the paper's reliance on specific datasets (e.g., MNIST) may limit the generalizability of its findings, and further exploration of other datasets or architectures would strengthen the empirical validation of the theoretical claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making complex concepts accessible to the reader. The methodology is clearly outlined, with sufficient detail for reproducibility. The novelty of the approach is evident, particularly in its unique framing of accuracy discrepancy. However, some sections could benefit from additional elaboration on the implications of the findings in practical scenarios, which may enhance understanding for practitioners.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework for understanding the dynamics of SGD, introducing a novel metric that bridges theoretical concepts with empirical observations. While the findings are significant, the study could be strengthened by broader empirical validation across diverse datasets.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe authors investigate the theoretical aspects of a prevalent optimization technique used in machine learning, aiming to elucidate its dynamics under minimal assumptions. The paper introduces a novel framework to analyze the optimization process, which is claimed to enhance understanding of convergence properties and model performance implications. The findings suggest that the proposed theoretical insights could inform future research directions in optimization methods.\n\n# Strength And Weaknesses\n**Strengths**: \n1. The paper contributes significantly to the theoretical understanding of optimization methods, potentially influencing future research.\n2. The unique framework introduced for analyzing the optimization process is innovative and may encourage further inquiry in this field.\n3. The clear structure and logical flow of the paper make it accessible, catering to a diverse audience.\n\n**Weaknesses**: \n1. The reliance on specific assumptions, although minimal, may limit the applicability of the results; a thorough discussion on these assumptions is warranted.\n2. The theoretical claims would benefit from more robust experimental validation, as empirical results aligning with the theory would enhance the contribution.\n3. The absence of comparative analysis with existing methods makes it challenging to evaluate the proposed approach's relative advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically organized, facilitating comprehension for a broad audience. The clarity of presentation is commendable, with well-defined concepts that contribute to the quality of the work. The novelty lies in the theoretical framework proposed, although the lack of empirical validation raises questions about reproducibility. Overall, the quality and clarity are high, while the novelty, though present, requires more substantiation through experiments.\n\n# Summary Of The Review\nThis paper offers valuable theoretical insights into an essential optimization technique in machine learning. While it demonstrates clarity and introduces a novel framework, addressing the limitations and providing empirical validation would significantly strengthen its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" explores the theoretical underpinnings of Stochastic Gradient Descent (SGD) by introducing a novel concept called \"accuracy discrepancy.\" This measure quantifies the difference in model accuracy between various batch sizes and larger subsets of the dataset. The authors demonstrate that a significant accuracy discrepancy suggests that SGD can achieve perfect accuracy in a constant number of epochs, while a lower discrepancy indicates limitations in model performance. Additionally, the paper discusses the necessity of randomness in Gradient Descent (GD) to avoid local minima, asserting that a linear quantity of random bits is essential, independent of model overparameterization. Through these contributions, the paper provides a fresh perspective on the dynamics of SGD and GD, emphasizing their reliance on randomness and the implications for model optimization.\n\n# Strength And Weaknesses\nOne of the paper's key strengths is its introduction of the accuracy discrepancy, which serves as a valuable metric for understanding SGD's convergence behavior under minimal assumptions. The analysis is grounded in a rigorous theoretical framework that enhances the understanding of SGD's dynamics in a non-convex landscape. However, a potential weakness is the limited exploration of empirical validation; the theoretical findings may benefit from additional experiments or case studies to corroborate the proposed concepts in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the key concepts and findings, making it accessible to a broad audience. The quality of the theoretical analysis is commendable, and the novel introduction of accuracy discrepancy adds depth to the discussion on SGD. Nevertheless, the reproducibility of results could be enhanced by providing more detailed methodologies or examples, allowing readers to replicate findings or experiments more easily.\n\n# Summary Of The Review\nOverall, this paper presents a significant theoretical advancement in understanding the dynamics of SGD through the lens of accuracy discrepancy and randomness. While the contributions are noteworthy, a stronger empirical validation could enhance the robustness of the findings and their applicability in real-world scenarios.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"SGD THROUGH THE LENS OF KOLMOGOROV COMPLEXITY\" presents a theoretical framework for understanding the dynamics of stochastic gradient descent (SGD) with minimal assumptions. It introduces the concept of \"accuracy discrepancy\" to quantify the difference in model performance on batches versus the entire dataset, establishing conditions under which SGD can achieve 100% accuracy in O(1) epochs. The methodology employs entropy compression and Kolmogorov complexity to analyze the randomness necessary for gradient descent to escape local minima, revealing that linear randomness is essential even in overparameterized settings. Empirical results on MNIST datasets demonstrate the alignment of theoretical predictions with actual model performance.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its novel approach to understanding SGD dynamics through accuracy discrepancy and its rigorous use of theoretical concepts such as entropy and Kolmogorov complexity. This provides a fresh perspective on SGD performance without relying heavily on traditional assumptions. However, the paper could benefit from a more extensive exploration of the implications of its findings, particularly in practical applications of SGD. Additionally, while the experimental verification is promising, further experiments across diverse datasets and scenarios could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, presenting complex concepts in a digestible manner. The quality of the theoretical analysis is high, though some sections could use clearer explanations of the implications of the findings. The novelty lies in the introduction of accuracy discrepancy and its application to SGD; however, the paper would benefit from a more extensive discussion on how these concepts can be reproduced and applied in practical settings.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the theoretical understanding of SGD dynamics through the lens of accuracy discrepancy and randomness requirements. While the theoretical framework is robust and innovative, the practical implications and broader experimental validation could be expanded upon to enhance its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" investigates the dynamics of Stochastic Gradient Descent (SGD) under minimal theoretical assumptions, specifically focusing on the concept of \"accuracy discrepancy.\" It establishes conditions under which SGD can achieve perfect model accuracy while emphasizing the role of randomness in escaping local minima. The methodology employs Kolmogorov complexity to analyze SGD's performance, presenting both theoretical results and empirical validation using the MNIST dataset.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its novel theoretical framework that links SGD dynamics to Kolmogorov complexity, filling a critical gap in the existing literature. By utilizing minimal assumptions, the authors make the findings more broadly applicable. The empirical validation strengthens the theoretical claims, demonstrating the practical relevance of the analysis. However, a notable weakness is the limited exploration of potential limitations and assumptions inherent in the methodology. Additionally, the paper could benefit from a more extensive discussion of experimental results and comparisons with existing techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable; it maintains a logical structure and clearly defines key concepts, making it accessible to readers familiar with SGD and theoretical frameworks. The use of figures and tables effectively illustrates the results. In terms of quality, the paper adheres to rigorous academic standards. The novelty of the approach is significant as it provides fresh insights into SGD's behavior. However, the reproducibility of the experiments could be improved with more detailed descriptions of the experimental setup and hyperparameters.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel analysis of SGD dynamics through the lens of Kolmogorov complexity, supported by both theoretical and empirical results. While the clarity and structure are strengths, the paper would benefit from addressing some limitations and providing more detailed experimental insights. I recommend acceptance with minor revisions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper investigates the dynamics of stochastic gradient descent (SGD) through the lens of Kolmogorov complexity, introducing a novel metric called \"accuracy discrepancy\" (AD) to quantify the divergence in model accuracy across mini-batches versus the entire dataset. The authors demonstrate that a sufficient AD can ensure SGD convergence to perfect accuracy (100%) within \\(O(1)\\) epochs, while suboptimal fitting requires AD to stay below a dataset-size-dependent threshold. Furthermore, they explore the role of randomness in gradient descent, revealing that an overparameterized model requires a linear amount of random bits relative to the dataset size to effectively escape local minima. The methodology includes theoretical proofs, empirical validation against the MNIST dataset, and the application of entropy compression techniques to establish bounds on convergence.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive theoretical framework that connects SGD dynamics to Kolmogorov complexity, providing clear insights into convergence behavior under minimal assumptions. The introduction of AD as a quantifiable metric is particularly innovative and fills a significant gap in the theoretical understanding of SGD. The empirical validation supports the theoretical claims, enhancing the paper's credibility. However, the reliance on specific assumptions, such as differentiable and \\(L\\)-smooth loss functions, may limit the generalizability of the findings. Additionally, the complexity of the theoretical exposition may pose challenges for readers less familiar with Kolmogorov complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a logical flow that guides the reader through complex concepts. The clarity of definitions, particularly concerning accuracy discrepancy and Kolmogorov complexity, is commendable. The quality of the writing is high, although some sections may require a background in information theory for full comprehension. The novelty of the approach is significant, as it applies principles of algorithmic information theory to a well-established optimization problem. Reproducibility is supported by empirical results on the MNIST dataset, but further details on experimental setups would enhance the robustness of the claims.\n\n# Summary Of The Review\nOverall, the paper presents a novel and insightful exploration of SGD dynamics through the lens of Kolmogorov complexity, introducing important theoretical contributions and empirical validations. While the findings are significant and well-supported, the complexity of the concepts may hinder accessibility for a broader audience.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the dynamics of Stochastic Gradient Descent (SGD) with a focus on characterizing \"accuracy discrepancy\" as a primary measure of performance. It presents theoretical results under minimal assumptions, specifically differentiability and L-smoothness, and claims to elucidate the interplay between randomness and convergence in SGD. However, the experimental validation is relegated to the appendix, limiting the practical implications of the findings.\n\n# Strength And Weaknesses\nStrengths of the paper include its theoretical framework, which attempts to simplify the understanding of SGD dynamics under specific conditions. However, the weaknesses are notable: the reliance on minimal and ambiguous assumptions raises concerns about generalizability; the limited focus on a single performance metric overlooks other critical aspects of SGD; and the lack of comprehensive empirical validation diminishes the credibility of the theoretical claims. Additionally, the paper's structure and presentation complicate the communication of its main arguments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by its complicated presentation, making it challenging for readers to grasp the underlying concepts and applications. While there are novel theoretical contributions regarding randomness in SGD, these are not sufficiently backed by empirical results, which affects the overall quality of the work. Reproducibility is questionable due to the insufficient exploration of practical applications, and the experimental design appears simplistic, limiting its robustness.\n\n# Summary Of The Review\nOverall, the paper presents a theoretically interesting perspective on SGD dynamics; however, significant limitations in clarity, empirical validation, and practical relevance undermine its contributions. The findings may not be applicable in real-world scenarios, and the narrow focus on accuracy discrepancy fails to capture the complexities of SGD behavior.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" presents a novel framework for understanding Stochastic Gradient Descent (SGD) by applying concepts from entropy compression and Kolmogorov complexity. The authors introduce the concept of \"accuracy discrepancy,\" which offers a quantifiable measure of model performance, asserting that under certain conditions, SGD can achieve perfect accuracy in \\( O(1) \\) epochs. The research identifies a global threshold for the accuracy discrepancy reliant on dataset characteristics and batch size, and emphasizes the role of randomness in SGD dynamics, particularly in relation to avoiding local minima. Empirical validation on datasets like MNIST supports the theoretical claims, demonstrating the real-world applicability of the findings, which have implications beyond SGD to various learning algorithms.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to SGD, connecting it to principles of Kolmogorov complexity, and introducing the accuracy discrepancy, which provides a novel lens for analyzing model performance. The findings about randomness and local minima are particularly insightful, paving the way for future algorithm designs. However, a potential weakness is that the assumptions made (e.g., differentiable loss functions and small learning rates) may limit the applicability of the results to more complex scenarios. Additionally, while empirical validation is present, further experiments across varied datasets and conditions could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its concepts, making it accessible to a broad audience in the machine learning community. The quality of the theoretical analysis is high, and the empirical results are compelling, enhancing the validity of the claims. The novelty of the approach is significant, as it opens new avenues for research in SGD and machine learning optimization. Reproducibility is supported by the provided empirical results, although additional details on experimental setups could further enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper offers a transformative perspective on SGD by integrating principles from Kolmogorov complexity, leading to significant insights about model accuracy and the dynamics of optimization. Its findings not only advance theoretical understanding but also have practical implications for improving machine learning algorithms.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"SGD Through the Lens of Kolmogorov Complexity\" presents a novel theoretical framework for understanding stochastic gradient descent (SGD) by leveraging the concepts of entropy compression and Kolmogorov complexity. The authors introduce the notion of \"accuracy discrepancy,\" quantifying the divergence between model accuracy on batches and larger dataset subsets. Key findings include that a global threshold on accuracy discrepancy guarantees convergence to perfect accuracy under minimal assumptions regarding differentiability and L-smoothness of loss functions. Additionally, the paper discusses the critical role of randomness in optimization, establishing theoretical bounds for the necessary randomness in achieving convergence.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to analyzing SGD with minimal assumptions and its introduction of theoretical constructs such as accuracy discrepancy and termination conditions for SGD. These contributions provide a broader understanding of SGD dynamics and offer insights into the importance of randomness in optimization processes. However, the paper may be limited by its reliance on theoretical constructs that require further empirical validation and may not directly translate to practical implementations of SGD in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex theoretical concepts accessible. The quality of the theoretical analysis is high, with a sound methodology. The novelty of the approach, particularly in linking Kolmogorov complexity to SGD dynamics, is significant. However, the reproducibility of the theoretical results may be challenging due to the abstract nature of the constructs and the absence of empirical validation, which could limit the practical applicability of the findings.\n\n# Summary Of The Review\nOverall, the paper offers a compelling theoretical perspective on SGD by incorporating concepts from Kolmogorov complexity and emphasizing the role of accuracy discrepancy and randomness. While the theoretical contributions are significant, further empirical investigation is required to assess the practical implications of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"SGD THROUGH THE LENS OF KOLMOGOROV COMPLEXITY\" investigates the dynamics of Stochastic Gradient Descent (SGD) using the framework of entropy compression and Kolmogorov complexity. It presents a theoretical analysis of the accuracy discrepancy in SGD, introducing a \"vanilla\" algorithm implementation that operates without acceleration or regularization. The authors derive bounds on the Kolmogorov complexity of the random bits required for SGD operations and discuss the reversibility of SGD under certain conditions. Experimental validations are performed on the MNIST dataset and a variant with random labels, although the paper lacks details regarding code availability for replication.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel theoretical exploration of SGD through the lens of Kolmogorov complexity, which provides fresh insights into the randomness and compression aspects of the algorithm. The introduction of accuracy discrepancy as a concept is also notable, potentially offering a new perspective on evaluating SGD's performance. However, the paper's weaknesses include the lack of practical implementation details and code availability, which hinders reproducibility. Additionally, while it briefly discusses related work, it does not provide comprehensive comparisons or references to prior implementations, limiting its contextual grounding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical contributions clearly. However, the novelty primarily resides in the theoretical domain, with limited empirical support due to the absence of detailed experimental setups and code availability. This lack of reproducibility can hinder the paper's impact in the community, as practical implementations are crucial for validating theoretical claims. The experimental section is also relatively sparse, focusing on a single architecture and dataset without extensive exploration of different scenarios.\n\n# Summary Of The Review\nOverall, the paper offers a unique theoretical perspective on SGD using Kolmogorov complexity, contributing valuable insights to the understanding of its dynamics and randomness. However, the lack of practical examples and code availability poses significant challenges for reproducibility and broader application within the community.\n\n# Correctness\n4/5 - The theoretical framework appears sound, with clear derivations and consistent methodology, although the practical implications may require further validation.\n\n# Technical Novelty And Significance\n4/5 - The approach of applying Kolmogorov complexity to analyze SGD is innovative and significant, providing a new lens through which to understand the algorithm's performance dynamics.\n\n# Empirical Novelty And Significance\n3/5 - While the experiments on the MNIST dataset contribute to the empirical validation of the theoretical claims, the limited scope and lack of diverse scenarios diminish the overall empirical novelty and significance.",
    "# Summary Of The Paper\nThe paper presents a study of Stochastic Gradient Descent (SGD) dynamics through the lens of Kolmogorov complexity, aiming to offer new insights into the convergence behavior of SGD. The authors introduce the concept of \"accuracy discrepancy\" and suggest that under certain conditions, SGD can achieve perfect accuracy in constant epochs. They conduct experiments using the MNIST dataset to substantiate their claims regarding the role of randomness in gradient descent initialization and its necessity for escaping local minima.\n\n# Strength And Weaknesses\nWhile the paper attempts to contribute to the understanding of SGD dynamics with a novel framework, it lacks the analytical depth found in existing literature, such as Du et al. (2019) and Allen-Zhu et al. (2019), which provide more rigorous convergence guarantees in overparameterized settings. The minimal assumptions presented by the authors, while intended as a strength, do not significantly advance the discourse, given that similar assumptions have been effectively utilized in prior studies. The concept of \"accuracy discrepancy,\" although interesting, lacks practical significance when compared to works like Zhang et al. (2022), which provide more substantial insights into neural network dynamics. Additionally, the experimental validation appears insufficient, with the MNIST analysis not matching the robustness of previous empirical studies, such as those by Du et al. (2017).\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat undermined by its limited engagement with existing literature, leading to ambiguities about the novelty of its contributions. While the authors propose new concepts and findings, the lack of rigor in applying established frameworks, such as entropy compression, diminishes the quality of the arguments presented. Reproducibility is challenged by the narrow scope of experimental validation, which does not convincingly demonstrate the proposed insights across varied conditions.\n\n# Summary Of The Review\nOverall, the paper aspires to contribute novel insights into SGD dynamics but falls short due to its superficial engagement with existing literature and a lack of rigorous analysis. The experimental results are not sufficiently robust to support the claims made, leading to doubts about the impact and originality of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"SGD THROUGH THE LENS OF KOLMOGOROV COMPLEXITY\" explores the relationship between stochastic gradient descent (SGD) and Kolmogorov complexity to provide insights into the optimization process. The authors propose a framework that analyzes the convergence behavior of SGD in terms of information theory, specifically focusing on entropy and randomness in the dataset. Key findings demonstrate that the accuracy of models can be significantly improved under certain conditions tied to a model's complexity, with experimental validation supporting the theoretical claims.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to linking SGD with Kolmogorov complexity, providing a fresh perspective on optimization. The theoretical contributions are well-founded, and the experimental results effectively illustrate the proposed concepts. However, the paper has notable weaknesses, including inconsistencies in notation and terminology, which could hinder reader comprehension. Furthermore, some mathematical definitions and notations lack clarity and should be better integrated into the text for improved understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by inconsistent notation and lack of explicit definitions for certain terms, such as \"w.h.p\" and \"sublinear.\" The methodological quality is generally high, though minor grammatical issues and formatting inconsistencies detract from the polished presentation. The novelty of the approach is significant, as it combines concepts from optimization and information theory. Reproducibility could be improved by providing clearer experimental protocols and ensuring all figures and tables are referenced accurately within the text.\n\n# Summary Of The Review\nOverall, the paper presents a novel perspective on SGD through the framework of Kolmogorov complexity, supported by both theoretical analysis and empirical validation. However, clarity issues and inconsistencies in notation detract from the overall quality, suggesting the need for revisions to enhance reader comprehension and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel framework for analyzing accuracy discrepancies in Stochastic Gradient Descent (SGD) dynamics, particularly focusing on how these discrepancies can influence model performance across various datasets. The authors employ a simulation-based methodology to explore the implications of accuracy discrepancies and propose theoretical insights into SGD convergence. Key findings suggest that accuracy discrepancies can significantly impact model generalization and performance, especially in datasets characterized by noise or class imbalance.\n\n# Strength And Weaknesses\nThe key strengths of the paper include its rigorous theoretical framework and the innovative approach to analyzing accuracy discrepancies in SGD. However, the paper has notable weaknesses, including a lack of discussion on the implications of findings in real-world scenarios, such as datasets with inherent noise or class imbalance. Additionally, the authors do not address the computational costs associated with their proposed methods, which could limit practical applicability. There is also potential for deeper exploration of various factors affecting accuracy discrepancies, such as initialization strategies, batch size variations, and the role of hyperparameters.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is generally good, though some sections could benefit from more concise explanations of key concepts. The quality of the methodology is solid, but the novelty is somewhat tempered by the absence of comparative analysis with existing theoretical frameworks on SGD convergence. Reproducibility could be enhanced by providing more detailed descriptions of the experiments and results, particularly concerning the empirical investigation across different tasks and datasets.\n\n# Summary Of The Review\nOverall, the paper offers valuable insights into accuracy discrepancies in SGD dynamics but lacks comprehensive discussions on real-world applicability and further exploration of factors influencing these discrepancies. While the theoretical contributions are noteworthy, the absence of practical considerations and comparative analyses limits the overall impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the dynamics of stochastic gradient descent (SGD) through the lens of entropy compression and Kolmogorov complexity. It introduces the concept of **accuracy discrepancy**, which quantifies the difference in model accuracy between batches and the entire dataset. The key finding is that if the average accuracy discrepancy exceeds a certain global threshold, SGD can achieve 100% accuracy in \\(O(1)\\) epochs with high probability. The paper also explores the role of randomness in escaping local minima, establishing that a linear amount of randomness is necessary for non-stochastic gradient descent to achieve convergence in subexponential time. The methodology involves both theoretical analysis and experimental validation using datasets like MNIST.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to linking accuracy discrepancy with convergence in SGD, providing a comprehensive statistical framework supported by empirical verification. The utilization of concepts from information theory, such as Kolmogorov complexity and Kullback-Leibler divergence, adds depth to the analysis. However, the paper's reliance on specific assumptions regarding model differentiability and L-smoothness may limit the generalizability of the findings. Additionally, while the experimental results support the theoretical claims, a broader range of datasets and model architectures could enhance the robustness of the conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making complex concepts accessible through appropriate explanations and visualizations. The quality of the theoretical analysis is high, and the experimental verification complements the theoretical claims. The novelty is significant, as the integration of entropy compression and accuracy discrepancy into the analysis of SGD dynamics is relatively unexplored. Reproducibility is supported by a clear description of the methodologies employed, though additional details on experimental setups could further facilitate replication.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel analysis of SGD dynamics, revealing important connections between accuracy discrepancy, randomness, and convergence. While the findings are robust and well-supported by empirical data, the assumptions made may limit the applicability of the results.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a theoretical analysis of Stochastic Gradient Descent (SGD) under minimal assumptions, focusing on its convergence properties across various architectures. The authors explore the behavior of SGD with differentiable and L-smooth loss functions and provide empirical results on accuracy discrepancies using two datasets: MNIST and MNIST-RAND. However, the framework lacks general guarantees for various model types and does not address the implications of non-differentiable loss functions, regularization, or acceleration methods.\n\n# Strength And Weaknesses\nThe paper's primary contribution lies in its theoretical exploration of SGD within a simplified context, which may facilitate understanding of its dynamics. However, the analysis suffers from significant limitations, including the absence of general convergence guarantees applicable to all architectures, the neglect of regularization and acceleration techniques, and a narrow focus on specific datasets. Additionally, the lack of consideration for hyperparameter tuning and scalability issues raises concerns about the practical applicability of the findings, particularly in high-dimensional spaces.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear exposition of its theoretical framework. However, it lacks depth in addressing various critical aspects that could enhance its quality and relevance. The novelty of the approach is somewhat limited due to its minimal assumptions and focus on well-trodden areas of SGD analysis. Reproducibility may be hindered by the narrow experimental setup and the absence of discussions on practical implementations or broader contexts in which the findings could be applied.\n\n# Summary Of The Review\nThe paper provides a foundational analysis of SGD with some theoretical insights, but its applicability is limited due to a lack of generality and consideration of critical factors influencing SGD performance. Further exploration of diverse datasets and methodologies would be necessary to validate and expand upon the findings presented.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a theoretical exploration of Stochastic Gradient Descent (SGD) dynamics through the lens of Kolmogorov complexity, proposing that a large accuracy discrepancy between batch and overall accuracy indicates that SGD can converge to perfect accuracy in O(1) epochs. The authors discuss the role of randomness in gradient descent and attempt to derive lower bounds on the amount of randomness required for efficient escape from local minima. While the methodology revolves around established concepts, the findings highlight a purported relationship between dataset properties and convergence behavior, albeit without substantial new insights.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its attempt to provide a theoretical framework for understanding SGD dynamics, particularly through the introduction of the concept of accuracy discrepancy. However, the weaknesses are pronounced; many of the insights seem to reiterate previous findings without providing new theoretical advancements. The novelty is limited, as the paper heavily relies on concepts already well-established in the literature, such as the randomness necessary for escaping local minima and the general behavior of overparameterized models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but the clarity is undermined by the use of jargon and terminology that does not significantly contribute to the core argument—terms like \"accuracy discrepancy\" feel contrived. The quality of the writing is adequate, but the depth of analysis is lacking. In terms of novelty, the paper does not offer significant new insights into SGD, making it challenging to justify its contributions. Reproducibility is not assessed in detail, as the experiments conducted (primarily with MNIST and random labels) do not provide robust evidence supporting the claims made.\n\n# Summary Of The Review\nOverall, this paper presents a theoretical discussion on SGD that feels redundant and lacks substantial novelty. While it attempts to frame existing knowledge in a new context, the contributions do not warrant the attention they seek, and the findings provide little beyond what is already known in optimization literature.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the concept of accuracy discrepancy in the context of Stochastic Gradient Descent (SGD) dynamics, aiming to provide a new framework for understanding the optimization process in deep learning. The authors utilize minimal assumptions to analyze SGD's behavior and propose the use of entropy compression as a tool for examining convergence dynamics. Their findings suggest a relationship between high accuracy discrepancy and effective entropy compression, supported by experiments on the MNIST and MNIST-RAND datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative introduction of accuracy discrepancy and its theoretical approach to SGD dynamics, which aligns with ongoing research in optimization algorithms. However, the paper could benefit from a broader exploration of various activation functions and network architectures, as well as a more comprehensive analysis of convergence metrics beyond accuracy discrepancy. The experimental validation, while promising, is limited to simpler datasets, and extending this to more complex scenarios would enhance the robustness of their findings. Additionally, the implications of their results on model interpretability and generalization performance warrant further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the core concepts and methodology. However, certain sections could be improved with more detailed discussions of the experimental setup and results. The novelty of the concept of accuracy discrepancy is notable, but its significance could be amplified by integrating techniques from information theory and stochastic processes. Reproducibility may be challenged by the limited scope of experiments and the need for further validation across diverse optimization scenarios.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to understanding SGD dynamics through the lens of accuracy discrepancy, yet it lacks depth in empirical validation and broader applicability. Future work should focus on enhancing theoretical foundations and testing the framework across varied datasets and optimization techniques to solidify its practical relevance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the dynamics of Stochastic Gradient Descent (SGD) through empirical experiments on the MNIST dataset and a modified version with random labels (MNIST-RAND). Its main contributions include demonstrating the relationship between batch size and accuracy discrepancy, revealing that larger batch sizes lead to smaller discrepancies. The authors provide insights into model performance under random labels, showing that accuracy tends to drop below a certain threshold over time. Additionally, the results highlight the impact of model size on performance, indicating that smaller models exhibit decreasing accuracy discrepancies over time, while larger models do not. The findings are backed by a comprehensive analysis across epochs, reinforcing the theoretical claims regarding SGD dynamics.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough empirical analysis and clear alignment between theoretical assertions and experimental results. The use of benchmark performance metrics effectively validates the theoretical framework, offering meaningful insights into SGD dynamics. However, a potential weakness is the reliance on only the MNIST dataset and its random variant, which may limit the generalizability of the findings. Further exploration with diverse datasets could enhance the robustness of the conclusions drawn.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the writing is high, making the complex concepts accessible. The novelty of the approach lies in its focus on accuracy discrepancies related to batch sizes and model capacities in SGD. While the results are presented in a reproducible manner, providing the code and detailed experimental setups would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper offers significant insights into the dynamics of SGD through empirical validation of theoretical claims. Its findings highlight critical relationships between batch size, model capacity, and performance, although the generalizability may be limited due to the dataset choice.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper investigates the phenomenon of accuracy discrepancy in stochastic gradient descent (SGD) optimization, proposing novel characterizations of this issue. The authors provide a rigorous mathematical framework that reveals the conditions under which accuracy discrepancies arise and present empirical results that validate their theoretical findings. The methodology includes both analytical proofs and experiments to demonstrate the practical implications of their findings on various datasets.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing a critical aspect of SGD optimization that is often overlooked. The theoretical insights are well-grounded and offer a new perspective on accuracy discrepancies, which could influence future research in optimization methods. However, the paper suffers from clarity issues, particularly in the abstract and sentence structure, which may hinder readers' understanding. Additionally, the integration of mathematical notations could be improved for better readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel insights, its clarity is compromised by convoluted sentence structures and inconsistent terminology, such as the term \"accuracy discrepancy.\" The proofs, though substantial, are lengthy and lack clear organization. The use of visual aids is insufficiently explained, making it challenging for readers to grasp their significance. The reproducibility of results is bolstered by empirical validations, but the lack of clear definitions for technical terms may pose accessibility challenges.\n\n# Summary Of The Review\nThis paper provides valuable theoretical and empirical contributions to the understanding of accuracy discrepancies in SGD optimization. However, issues related to clarity, organization, and terminology consistency detract from its overall impact. Enhancing these aspects would significantly improve the paper's accessibility to a broader audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.6463097559953175,
    -1.8134716407569647,
    -1.884802426449343,
    -1.7209469366309975,
    -1.814406761061406,
    -1.6475237386333403,
    -1.7534959243106163,
    -1.9141640390780224,
    -1.8257469582138581,
    -1.8144323704330028,
    -1.8059272095647954,
    -1.8071211468897697,
    -1.7869671132598093,
    -1.6978119825081706,
    -1.6990360152552142,
    -1.741132563102477,
    -1.9489740122539383,
    -1.7209118873734381,
    -1.6550298466688445,
    -1.7571795925973135,
    -1.9225730768325386,
    -1.7180726796729577,
    -1.6786276547626837,
    -1.6833584016464718,
    -1.8881030622245747,
    -2.0244709557430287,
    -1.8204501174739083,
    -1.7967425956725454,
    -1.8639326492414736
  ],
  "logp_cond": [
    [
      0.0,
      -2.4083118835655277,
      -2.4111864792542366,
      -2.4114334055906466,
      -2.4326320096399487,
      -2.4412260460172837,
      -2.433269977080625,
      -2.4307838892639144,
      -2.427032875537303,
      -2.4325724975099483,
      -2.427268255156111,
      -2.4792699011965835,
      -2.410813317615675,
      -2.4124269345874554,
      -2.4148229768384653,
      -2.413433705178242,
      -2.429658926054442,
      -2.4329293228678286,
      -2.418736769861721,
      -2.3745037951901007,
      -2.4154436119618663,
      -2.4370949715493953,
      -2.4581455173171625,
      -2.4192446005855848,
      -2.4683824833370434,
      -2.4081976436513477,
      -2.4380168200081522,
      -2.4373858385672658,
      -2.4740838128286846
    ],
    [
      -1.5387910209272941,
      0.0,
      -1.3112887696907856,
      -1.3757591026443359,
      -1.471683531397627,
      -1.440590313140671,
      -1.4487052999424472,
      -1.3852842114578927,
      -1.311702341786716,
      -1.4802205124119463,
      -1.3958653511088497,
      -1.5630636650000196,
      -1.272521426224579,
      -1.3446977915393992,
      -1.3878214130546473,
      -1.2756735309649874,
      -1.5083427923069321,
      -1.344952532425421,
      -1.3760819367365191,
      -1.4788055923075163,
      -1.413628060855266,
      -1.538101418352684,
      -1.5762905324602479,
      -1.2944359699204053,
      -1.5242021281005234,
      -1.4265225488464381,
      -1.5130996613835401,
      -1.5058291534693053,
      -1.5595233937362123
    ],
    [
      -1.5657809103974947,
      -1.2632965124510116,
      0.0,
      -1.4261693790737293,
      -1.4897467792853973,
      -1.5186542324178611,
      -1.5331540481937926,
      -1.4315770214806014,
      -1.3486014779342568,
      -1.506849845459745,
      -1.4324009483193312,
      -1.5778524134161027,
      -1.412666932506861,
      -1.3658805800874612,
      -1.4719153390696165,
      -1.2885138809849788,
      -1.5123745294200763,
      -1.404532507646066,
      -1.4327843568793226,
      -1.524642686140011,
      -1.4025787377886327,
      -1.5944303453598583,
      -1.6108424270325898,
      -1.282906656225558,
      -1.5166600914724055,
      -1.446966843544288,
      -1.5043468862209837,
      -1.550232237693586,
      -1.5979576835651634
    ],
    [
      -1.3331516423644443,
      -1.1337616257006273,
      -1.185657405949192,
      0.0,
      -1.2704400554344433,
      -1.2805880814692958,
      -1.3512929719545095,
      -1.259770845254864,
      -1.238015452776983,
      -1.3123640048273533,
      -1.2053833344009053,
      -1.4172831246483508,
      -1.1886171101327254,
      -1.2206904543283181,
      -1.2665240218381009,
      -1.2208598131749264,
      -1.3464945166692226,
      -1.2710627059527764,
      -1.2509106002460553,
      -1.3713175955498476,
      -1.245798113753478,
      -1.4266539657017505,
      -1.4151487525587012,
      -1.2126088311558925,
      -1.3650088619342473,
      -1.29601490151136,
      -1.3118457099605147,
      -1.3449142340808535,
      -1.4501743799940012
    ],
    [
      -1.4098053948042177,
      -1.2865146259938998,
      -1.2992769519083203,
      -1.2929972633545066,
      0.0,
      -1.3759494843692845,
      -1.4050272975655163,
      -1.389994798912674,
      -1.3127059862492774,
      -1.265586855861113,
      -1.3195273269321657,
      -1.4407739692626056,
      -1.331849483421208,
      -1.2977498103952874,
      -1.298109752837838,
      -1.3103344285156766,
      -1.3304050870460584,
      -1.2854437946678332,
      -1.2639124579029684,
      -1.3106027094637696,
      -1.223208898749672,
      -1.477968748718099,
      -1.4598283736827826,
      -1.331026860129236,
      -1.3896660493219672,
      -1.376672964819063,
      -1.3449442714303417,
      -1.453973614885643,
      -1.4614955305108956
    ],
    [
      -1.3046476235261641,
      -1.1261663636793962,
      -1.1878656517346997,
      -1.1844764317700454,
      -1.1879208102293914,
      0.0,
      -1.215666606602571,
      -1.1835870829785735,
      -1.1756689451601252,
      -1.2352056250134351,
      -1.2233469206712817,
      -1.3232540454684956,
      -1.200518465109282,
      -1.2301623323825452,
      -1.1907702796405815,
      -1.0569406925861962,
      -1.2070755014749475,
      -1.1040972640749493,
      -1.1362234575337808,
      -1.301214413733758,
      -1.1974596570378577,
      -1.2762568267815921,
      -1.3526630201209409,
      -1.0992906103790911,
      -1.2505126398184998,
      -1.2639179595267893,
      -1.2929559370496306,
      -1.2901456880266131,
      -1.361042188780731
    ],
    [
      -1.4433515779271453,
      -1.3117744177048012,
      -1.346827764575757,
      -1.366292027193007,
      -1.369002250848906,
      -1.3393960371115061,
      0.0,
      -1.3681384559215461,
      -1.305596299079259,
      -1.4090946990249635,
      -1.3863836790739936,
      -1.4338356473774878,
      -1.2971485737289643,
      -1.3552087971183118,
      -1.3066618033435244,
      -1.331996459061474,
      -1.3938689110703242,
      -1.3103444785287532,
      -1.3261942686850932,
      -1.4248723907185106,
      -1.3178072340403049,
      -1.3765369836293995,
      -1.441177490473638,
      -1.3392587080898741,
      -1.4382324248675138,
      -1.3926773401667338,
      -1.4150701348910313,
      -1.4106233144529188,
      -1.428834577931365
    ],
    [
      -1.6248814039394561,
      -1.3593416780229373,
      -1.3909822613545408,
      -1.4685263073555817,
      -1.5331189976974091,
      -1.504450842457031,
      -1.5234731578410092,
      0.0,
      -1.381110771498079,
      -1.5272607553364905,
      -1.4382127713831918,
      -1.6299467093879076,
      -1.3570291067706468,
      -1.365329387526855,
      -1.4640257690294383,
      -1.3841307561357148,
      -1.574921627274344,
      -1.4394684220827831,
      -1.4126290708716167,
      -1.5631133781015223,
      -1.4777729526281185,
      -1.5402082571604214,
      -1.5597370968638875,
      -1.377991758373129,
      -1.6006589714835138,
      -1.3410719319656075,
      -1.574629470774892,
      -1.5721383842780001,
      -1.5751851900792608
    ],
    [
      -1.5492587265400442,
      -1.2805122418042791,
      -1.360231750223753,
      -1.403138466348238,
      -1.4855474868313983,
      -1.4934668635949717,
      -1.477429766763303,
      -1.383239887012253,
      0.0,
      -1.4953145829628893,
      -1.3885628223948683,
      -1.575374784836908,
      -1.3475160551478358,
      -1.3780721889284004,
      -1.4469425780021976,
      -1.3877434861233116,
      -1.5053261409135208,
      -1.3628785250432742,
      -1.418408321442421,
      -1.5071904814215347,
      -1.3709633356566806,
      -1.5418572301154574,
      -1.5410664992993335,
      -1.3719681918344029,
      -1.5073092424060095,
      -1.4540362371687485,
      -1.4665952606744306,
      -1.4599251297481624,
      -1.5467587974382913
    ],
    [
      -1.4494276378544615,
      -1.3939939545511462,
      -1.390310229131271,
      -1.403147574267206,
      -1.301593819546845,
      -1.4323988981320506,
      -1.4896932268653291,
      -1.3598586558761243,
      -1.371689453544471,
      0.0,
      -1.389179457961161,
      -1.488315527580159,
      -1.445244850938711,
      -1.4007978544993076,
      -1.3934485841849045,
      -1.3976366298182528,
      -1.3555502286918932,
      -1.4102803004014504,
      -1.3670892104794408,
      -1.4174976969630655,
      -1.3726222472110605,
      -1.5128096124351005,
      -1.46717499187295,
      -1.3755401224807384,
      -1.4098484032745888,
      -1.4017659076575593,
      -1.4399317259381363,
      -1.4417430636412238,
      -1.504916806041425
    ],
    [
      -1.4466418315887257,
      -1.2161841088750716,
      -1.2661873852053083,
      -1.2417532995446603,
      -1.3793814130264344,
      -1.4027769327428026,
      -1.4418228145438183,
      -1.3232353990772683,
      -1.2498323324783245,
      -1.393327401430545,
      0.0,
      -1.4857980834193765,
      -1.3157856733847795,
      -1.2560760397006892,
      -1.3253816594926993,
      -1.2814079142177126,
      -1.434625272137153,
      -1.316701855952421,
      -1.3420438278921636,
      -1.3812754107827492,
      -1.2870590307035266,
      -1.463965436798744,
      -1.4203631716219687,
      -1.2149145177712766,
      -1.469721497248016,
      -1.2809030838796653,
      -1.3927550000347413,
      -1.3947614758276934,
      -1.4646590632021017
    ],
    [
      -1.4726329997516676,
      -1.4853543155465667,
      -1.4574266570546766,
      -1.4838492308647617,
      -1.468637182190912,
      -1.49469265358426,
      -1.4837681295881426,
      -1.4966187163256763,
      -1.4986820611999472,
      -1.4676037467466991,
      -1.491965157795763,
      0.0,
      -1.4596426332742585,
      -1.5084466599646602,
      -1.4753398376331084,
      -1.5087849971977552,
      -1.4250522394272098,
      -1.53418064436307,
      -1.4689719749358539,
      -1.5386606494411492,
      -1.4594921040624889,
      -1.5047711562801556,
      -1.4889111839313713,
      -1.5125656335281643,
      -1.4744163148132066,
      -1.487208196251939,
      -1.5176797735356906,
      -1.476128845288421,
      -1.5074283948246334
    ],
    [
      -1.461206094701905,
      -1.2058956134163907,
      -1.2734293765371036,
      -1.3267333425861019,
      -1.4400316890674025,
      -1.4373020268148418,
      -1.3926352646859956,
      -1.2778904283069445,
      -1.270878509139978,
      -1.4424068397618055,
      -1.3867776201151594,
      -1.5000740570327127,
      0.0,
      -1.2660871966338085,
      -1.3713322540321657,
      -1.2855874474272326,
      -1.4710557557027593,
      -1.3548220704592822,
      -1.272299414714947,
      -1.4681670074793176,
      -1.3549805697060018,
      -1.4968626531982139,
      -1.5081618595497945,
      -1.3592298286629474,
      -1.5026487992150592,
      -1.3171184138023104,
      -1.5014265476355657,
      -1.4476474534940262,
      -1.533889567151152
    ],
    [
      -1.363880141021446,
      -1.065270074851107,
      -1.1499985877418806,
      -1.2588921070907777,
      -1.2929102672199486,
      -1.349380064126498,
      -1.3181736137717956,
      -1.1900735936360594,
      -1.1758521174224257,
      -1.3401210889319204,
      -1.2109796295400987,
      -1.4297550900677038,
      -1.1433830559002442,
      0.0,
      -1.2268099326424031,
      -1.1945819692664792,
      -1.3726784058957466,
      -1.2453283305171816,
      -1.2239185625675324,
      -1.262665667039433,
      -1.232787598027709,
      -1.3489894290553595,
      -1.3342321633291219,
      -1.1156287737451627,
      -1.4350765593881254,
      -1.2152633109353919,
      -1.334361598972446,
      -1.3421671759504803,
      -1.3788800927806952
    ],
    [
      -1.345362919564627,
      -1.2020000787012088,
      -1.2268322553835274,
      -1.2231708963262182,
      -1.246293310994094,
      -1.3084067311932468,
      -1.290458103908845,
      -1.2363457079834401,
      -1.1990182748166172,
      -1.2432713355861402,
      -1.2267031011759375,
      -1.356034138250562,
      -1.2269776910776258,
      -1.190239389257055,
      0.0,
      -1.2358988143556153,
      -1.2581168155808464,
      -1.276129694881488,
      -1.2715404951885838,
      -1.27056154305209,
      -1.1871475159851792,
      -1.315328736507311,
      -1.3576467799519594,
      -1.2217905206817785,
      -1.3166088735574335,
      -1.218339384321347,
      -1.3168669912975106,
      -1.3176380075007983,
      -1.3015448355391843
    ],
    [
      -1.389689682423441,
      -1.1002057853801979,
      -1.1750550351068654,
      -1.2778126161701366,
      -1.3448460296706155,
      -1.3466016370356304,
      -1.3886625585813508,
      -1.2925801106070474,
      -1.2532150870243322,
      -1.3930096796773788,
      -1.3098345857631764,
      -1.4407622368706252,
      -1.2085404217839784,
      -1.1891488144091682,
      -1.2853370559976751,
      0.0,
      -1.3956670624022007,
      -1.246276751255196,
      -1.2609025762626482,
      -1.3805880957625563,
      -1.294720324417888,
      -1.4119057704943008,
      -1.445978922049798,
      -1.2025235723884922,
      -1.4258310955188835,
      -1.2638259413605208,
      -1.41954061478418,
      -1.386407560894517,
      -1.4375280990946173
    ],
    [
      -1.5991404853803897,
      -1.5254305177199405,
      -1.50928416193669,
      -1.5422171507216602,
      -1.4691249167844367,
      -1.548928866662966,
      -1.5901084340539708,
      -1.5541233388995976,
      -1.4944987862721966,
      -1.5243619543937652,
      -1.6133531867514006,
      -1.5771418889832374,
      -1.5218382504886618,
      -1.5768980863037902,
      -1.5614288006653563,
      -1.5148716876409478,
      0.0,
      -1.5279841696200724,
      -1.491430535457916,
      -1.5891837694835196,
      -1.4851038007991741,
      -1.6620783494092006,
      -1.612901489934766,
      -1.5488810531667045,
      -1.5168050506759998,
      -1.5744119519524693,
      -1.5501820605237222,
      -1.5347608999733535,
      -1.6263308852794112
    ],
    [
      -1.4387451716145545,
      -1.2235104507941368,
      -1.3098455115073195,
      -1.3361242394308128,
      -1.3489516166637108,
      -1.3522105033376224,
      -1.3636748965394425,
      -1.3136659794815868,
      -1.2824151346742498,
      -1.4204366291011763,
      -1.3466706739717544,
      -1.4580559674365963,
      -1.2831655690763788,
      -1.2910356748155791,
      -1.3218964716632706,
      -1.2473511820775636,
      -1.3837137068448044,
      0.0,
      -1.2127751132257634,
      -1.405156989015439,
      -1.3081125130762858,
      -1.4378093646107146,
      -1.4540898950128553,
      -1.2751141539787418,
      -1.423384831920288,
      -1.3567528225655334,
      -1.4434824833756528,
      -1.4155186645406526,
      -1.4567389707150895
    ],
    [
      -1.3347768112994465,
      -1.0917184824310806,
      -1.1438708267476563,
      -1.2146736630799653,
      -1.225103642917258,
      -1.2504807099680182,
      -1.2618899192476307,
      -1.224255993134808,
      -1.1487116295353677,
      -1.2556177201851806,
      -1.2253371171284881,
      -1.349725983164687,
      -1.1267533335980437,
      -1.159349215403262,
      -1.1928811140444886,
      -1.1840383552424298,
      -1.257731000790977,
      -1.1421630758441566,
      0.0,
      -1.2887155985329286,
      -1.2108243384709103,
      -1.3387554805563264,
      -1.3549351581792572,
      -1.1232398682318245,
      -1.2778829318428,
      -1.2837842897176797,
      -1.2942823630292757,
      -1.3159664430116462,
      -1.3812038614698616
    ],
    [
      -1.4570804822208163,
      -1.380139880970534,
      -1.4000485206998494,
      -1.4312236149514335,
      -1.3712440976817732,
      -1.4401449003909275,
      -1.4339975798695324,
      -1.4410382204502687,
      -1.3530580437726916,
      -1.3924706648922254,
      -1.3940447746943314,
      -1.5254736416635326,
      -1.4231928669934253,
      -1.3557381270275795,
      -1.3978439108902831,
      -1.42629849182143,
      -1.4230068914733023,
      -1.3965586373254928,
      -1.3856720726845657,
      0.0,
      -1.4156163064905518,
      -1.4529601253329565,
      -1.4821419974482888,
      -1.3772388429911528,
      -1.4482698037834472,
      -1.4180033059290833,
      -1.430365603923434,
      -1.413826245490182,
      -1.4716945781102317
    ],
    [
      -1.6042439119426186,
      -1.4672761347233232,
      -1.4748002006644694,
      -1.5075049961852383,
      -1.4886771737999682,
      -1.5800049555489253,
      -1.6026437328075882,
      -1.5120649909777053,
      -1.4482015917895947,
      -1.576533627569623,
      -1.522346060069211,
      -1.6635619140464197,
      -1.4870151860296383,
      -1.4549706192963119,
      -1.546326995571149,
      -1.4535352327128614,
      -1.5935880105208207,
      -1.4599484876246525,
      -1.5297413534967668,
      -1.6131037815759457,
      0.0,
      -1.6522371542996672,
      -1.6733364883292463,
      -1.5186791033062303,
      -1.6183684152409972,
      -1.4797498296096132,
      -1.5727510214036577,
      -1.6130900933241692,
      -1.6664774086703984
    ],
    [
      -1.3120672008899783,
      -1.3021112104848516,
      -1.337685994352731,
      -1.3373707892237947,
      -1.3457095845517233,
      -1.3291246982058451,
      -1.2831374200432981,
      -1.3024218940564802,
      -1.2887961260263823,
      -1.3541122219111703,
      -1.3270083659241734,
      -1.4081463489095238,
      -1.2942845762323454,
      -1.2603945508647345,
      -1.2474697247783106,
      -1.3077877808962628,
      -1.3650154003543,
      -1.3278432250548455,
      -1.2946399711715366,
      -1.2731200263525315,
      -1.3187022940431221,
      0.0,
      -1.332153037808157,
      -1.2667110066775973,
      -1.4112536989180744,
      -1.3243825767997734,
      -1.3728924112326413,
      -1.355959699530638,
      -1.2946420857794734
    ],
    [
      -1.3563618104099087,
      -1.3255532674573345,
      -1.3151174540982016,
      -1.3484035525045641,
      -1.3084932451364164,
      -1.3405565461659323,
      -1.353052746941456,
      -1.2830818743337384,
      -1.336189441408199,
      -1.2886894160645574,
      -1.273335964842795,
      -1.3895370938405096,
      -1.3477259668168746,
      -1.286330488594557,
      -1.2868954659034355,
      -1.3213368962207863,
      -1.316156278059154,
      -1.3387899915517438,
      -1.357134858665749,
      -1.3336708472671681,
      -1.286423609294628,
      -1.3004764491680918,
      0.0,
      -1.2869982726399571,
      -1.303852748026583,
      -1.2401669125762924,
      -1.2718114216475365,
      -1.3083605083294478,
      -1.3121538945077815
    ],
    [
      -1.3503203276063709,
      -1.0983424715152899,
      -1.156265834613532,
      -1.233106954092617,
      -1.3060983172532994,
      -1.2436451178645394,
      -1.332857439822982,
      -1.2392012019999148,
      -1.2002048404430496,
      -1.3146557986872387,
      -1.2150570421348896,
      -1.4064616394835765,
      -1.2259931725342978,
      -1.1442084114145699,
      -1.2413876488843563,
      -1.1026273547005816,
      -1.3035543360389978,
      -1.2078827411924637,
      -1.168916853960561,
      -1.299570120172565,
      -1.2456655487818054,
      -1.341374396047287,
      -1.3508772828202138,
      0.0,
      -1.368052894086752,
      -1.1920728346236618,
      -1.3554072436156681,
      -1.3344722707789016,
      -1.3717295899248654
    ],
    [
      -1.596964127703721,
      -1.5569776121312031,
      -1.4702246124500322,
      -1.5014850231737351,
      -1.4667766406573353,
      -1.4890803308095304,
      -1.5610446330989571,
      -1.5591276360141058,
      -1.4849669875883884,
      -1.4469856910940821,
      -1.5661415328946695,
      -1.5660557091505398,
      -1.5376691232263957,
      -1.5679312327493224,
      -1.5866418849449098,
      -1.5458187509968409,
      -1.4741795882931992,
      -1.5245367075880258,
      -1.4739923345121735,
      -1.5446614934628664,
      -1.4851955352274322,
      -1.6184035227966378,
      -1.562420492882362,
      -1.5407096910707405,
      0.0,
      -1.546538303333126,
      -1.4928946191697579,
      -1.4695059193674906,
      -1.6081755567169114
    ],
    [
      -1.6907667265522905,
      -1.474556083157014,
      -1.521343461682533,
      -1.5513740464866266,
      -1.6125228039292692,
      -1.6393915337918286,
      -1.6691831695448622,
      -1.4580985347710487,
      -1.5188070802157552,
      -1.61823956913021,
      -1.522421084640376,
      -1.7425597663782542,
      -1.526759903655256,
      -1.4793913451816152,
      -1.553837703929462,
      -1.4778648668646328,
      -1.6831821649434575,
      -1.5561060753837683,
      -1.6200677653625544,
      -1.6078561540831684,
      -1.5142081477280942,
      -1.661986493991076,
      -1.659872476979798,
      -1.4807421330942416,
      -1.7016367671340018,
      0.0,
      -1.6606537844187763,
      -1.6628254968962473,
      -1.684829093677759
    ],
    [
      -1.469381988682792,
      -1.430422267872001,
      -1.3928162183610664,
      -1.4143821193605504,
      -1.3556960958677544,
      -1.4848529509963222,
      -1.4833486929724522,
      -1.446309378520759,
      -1.3861557710115657,
      -1.4246027543653998,
      -1.4286253515876517,
      -1.4834769967594859,
      -1.4392894273184074,
      -1.447562654300419,
      -1.4653645265379538,
      -1.4719508914955204,
      -1.3877940974520149,
      -1.435235424358797,
      -1.412541299082735,
      -1.4179490568837299,
      -1.3416375117687236,
      -1.5212196810901344,
      -1.4568563186680212,
      -1.480358685556166,
      -1.398810745979995,
      -1.4224929113625553,
      0.0,
      -1.4645622450434268,
      -1.529845232330454
    ],
    [
      -1.447111927957576,
      -1.399570297375344,
      -1.428898603626872,
      -1.38215192427533,
      -1.4322803328448894,
      -1.4739871743711845,
      -1.4702845750499807,
      -1.4672059285980596,
      -1.3589749112469454,
      -1.4435863509333642,
      -1.3703947454315817,
      -1.4894574381284853,
      -1.4152209566363805,
      -1.440759480873882,
      -1.421939736226308,
      -1.3940831596162133,
      -1.4123803338558987,
      -1.428030760009875,
      -1.458595927647737,
      -1.4398912677342721,
      -1.426259336003436,
      -1.4975732322564703,
      -1.4478659309825448,
      -1.4364779408175157,
      -1.3790697382343846,
      -1.4837270055163547,
      -1.4222202456304944,
      0.0,
      -1.461678155183447
    ],
    [
      -1.4820767466717752,
      -1.5251757820884022,
      -1.5179965227539365,
      -1.4961407980967154,
      -1.5149254829008878,
      -1.5206932094647925,
      -1.5487136347370223,
      -1.5028048076671312,
      -1.4972965079612388,
      -1.4939875941779366,
      -1.457786625508772,
      -1.5227690197777224,
      -1.5331158334365993,
      -1.4959275019937097,
      -1.4708159717721927,
      -1.4996545433669768,
      -1.5062210175319795,
      -1.5440178191992786,
      -1.549148624913323,
      -1.478552922023116,
      -1.488309470244309,
      -1.4138600413819336,
      -1.4724691269271244,
      -1.4760424244332124,
      -1.4884989971059486,
      -1.465002224562985,
      -1.531800215777846,
      -1.5159189170221374,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.23799787242978976,
      0.2351232767410809,
      0.23487635040467092,
      0.21367774635536874,
      0.20508370997803382,
      0.2130397789146925,
      0.21552586673140306,
      0.2192768804580143,
      0.21373725848536917,
      0.21904150083920637,
      0.16703985479873396,
      0.2354964383796423,
      0.2338828214078621,
      0.23148677915685223,
      0.23287605081707552,
      0.2166508299408756,
      0.2133804331274889,
      0.22757298613359644,
      0.2718059608052168,
      0.23086614403345118,
      0.20921478444592223,
      0.18816423867815502,
      0.2270651554097327,
      0.17792727265827413,
      0.23811211234396978,
      0.20829293598716525,
      0.20892391742805172,
      0.17222594316663287
    ],
    [
      0.2746806198296705,
      0.0,
      0.5021828710661791,
      0.4377125381126288,
      0.34178810935933757,
      0.37288132761629367,
      0.3647663408145174,
      0.42818742929907194,
      0.5017692989702487,
      0.33325112834501835,
      0.41760628964811497,
      0.25040797575694507,
      0.5409502145323857,
      0.4687738492175655,
      0.4256502277023173,
      0.5377981097919773,
      0.3051288484500325,
      0.46851910833154364,
      0.4373897040204455,
      0.33466604844944836,
      0.3998435799016986,
      0.27537022240428066,
      0.2371811082967168,
      0.5190356708365593,
      0.2892695126564413,
      0.3869490919105265,
      0.3003719793734245,
      0.3076424872876593,
      0.25394824702075236
    ],
    [
      0.31902151605184836,
      0.6215059139983314,
      0.0,
      0.4586330473756137,
      0.3950556471639457,
      0.3661481940314819,
      0.3516483782555504,
      0.45322540496874164,
      0.5362009485150863,
      0.37795258098959805,
      0.45240147813001186,
      0.3069500130332403,
      0.472135493942482,
      0.5189218463618819,
      0.41288708737972657,
      0.5962885454643643,
      0.3724278970292667,
      0.4802699188032771,
      0.45201806957002044,
      0.36015974030933195,
      0.4822236886607103,
      0.29037208108948476,
      0.2739599994167532,
      0.601895770223785,
      0.3681423349769375,
      0.4378355829050551,
      0.38045554022835937,
      0.33457018875575706,
      0.2868447428841796
    ],
    [
      0.38779529426655324,
      0.5871853109303702,
      0.5352895306818055,
      0.0,
      0.45050688119655424,
      0.4403588551617017,
      0.36965396467648803,
      0.4611760913761336,
      0.4829314838540144,
      0.40858293180364424,
      0.5155636022300922,
      0.3036638119826467,
      0.5323298264982721,
      0.5002564823026794,
      0.45442291479289665,
      0.5000871234560711,
      0.3744524199617749,
      0.44988423067822114,
      0.47003633638494224,
      0.34962934108114996,
      0.4751488228775196,
      0.294292970929247,
      0.3057981840722963,
      0.508338105475105,
      0.3559380746967502,
      0.4249320351196375,
      0.40910122667048276,
      0.376032702550144,
      0.2707725566369963
    ],
    [
      0.40460136625718834,
      0.5278921350675063,
      0.5151298091530858,
      0.5214094977068995,
      0.0,
      0.43845727669212153,
      0.40937946349588983,
      0.4244119621487321,
      0.5017007748121287,
      0.5488199052002931,
      0.4948794341292404,
      0.3736327917988005,
      0.48255727764019807,
      0.5166569506661187,
      0.516297008223568,
      0.5040723325457295,
      0.48400167401534766,
      0.5289629663935729,
      0.5504943031584377,
      0.5038040515976365,
      0.591197862311734,
      0.33643801234330706,
      0.35457838737862346,
      0.48337990093217,
      0.42474071173943884,
      0.4377337962423431,
      0.4694624896310644,
      0.36043314617576305,
      0.35291123055051044
    ],
    [
      0.3428761151071762,
      0.5213573749539442,
      0.4596580868986406,
      0.463047306863295,
      0.4596029284039489,
      0.0,
      0.43185713203076936,
      0.4639366556547668,
      0.47185479347321513,
      0.4123181136199052,
      0.42417681796205864,
      0.3242696931648448,
      0.4470052735240584,
      0.4173614062507951,
      0.4567534589927589,
      0.5905830460471442,
      0.4404482371583929,
      0.543426474558391,
      0.5113002810995595,
      0.3463093248995823,
      0.4500640815954826,
      0.3712669118517482,
      0.2948607185123995,
      0.5482331282542492,
      0.39701109881484054,
      0.383605779106551,
      0.3545678015837097,
      0.3573780506067272,
      0.28648154985260943
    ],
    [
      0.310144346383471,
      0.4417215066058151,
      0.40666815973485937,
      0.3872038971176093,
      0.3844936734617104,
      0.4140998871991102,
      0.0,
      0.38535746838907015,
      0.4478996252313574,
      0.34440122528565276,
      0.36711224523662267,
      0.3196602769331285,
      0.456347350581652,
      0.3982871271923045,
      0.44683412096709185,
      0.4214994652491424,
      0.3596270132402921,
      0.44315144578186305,
      0.42730165562552314,
      0.3286235335921057,
      0.43568869027031143,
      0.3769589406812168,
      0.3123184338369782,
      0.41423721622074217,
      0.3152634994431025,
      0.36081858414388246,
      0.338425789419585,
      0.3428726098576975,
      0.32466134637925137
    ],
    [
      0.2892826351385662,
      0.5548223610550851,
      0.5231817777234815,
      0.4456377317224407,
      0.38104504138061324,
      0.40971319662099126,
      0.3906908812370131,
      0.0,
      0.5330532675799433,
      0.3869032837415318,
      0.47595126769483054,
      0.2842173296901147,
      0.5571349323073755,
      0.5488346515511673,
      0.4501382700485841,
      0.5300332829423076,
      0.3392424118036783,
      0.4746956169952392,
      0.5015349682064056,
      0.35105066097650006,
      0.4363910864499039,
      0.3739557819176009,
      0.35442694221413484,
      0.5361722807048934,
      0.3135050675945086,
      0.5730921071124149,
      0.33953456830313034,
      0.3420256548000222,
      0.3389788489987615
    ],
    [
      0.276488231673814,
      0.545234716409579,
      0.46551520799010504,
      0.4226084918656201,
      0.3401994713824599,
      0.3322800946188864,
      0.3483171914505552,
      0.44250707120160504,
      0.0,
      0.3304323752509688,
      0.43718413581898985,
      0.25037217337695017,
      0.47823090306602234,
      0.4476747692854577,
      0.3788043802116605,
      0.4380034720905466,
      0.32042081730033733,
      0.4628684331705839,
      0.4073386367714371,
      0.31855647679232346,
      0.4547836225571775,
      0.28388972809840074,
      0.28468045891452465,
      0.45377876637945525,
      0.31843771580784863,
      0.3717107210451096,
      0.35915169753942755,
      0.3658218284656958,
      0.2789881607755669
    ],
    [
      0.36500473257854127,
      0.4204384158818566,
      0.42412214130173176,
      0.41128479616579683,
      0.5128385508861577,
      0.3820334723009522,
      0.32473914356767364,
      0.45457371455687845,
      0.44274291688853173,
      0.0,
      0.4252529124718418,
      0.3261168428528438,
      0.36918751949429174,
      0.41363451593369516,
      0.4209837862480983,
      0.41679574061475,
      0.45888214174110953,
      0.4041520700315524,
      0.447343159953562,
      0.39693467346993727,
      0.44181012322194224,
      0.3016227579979023,
      0.3472573785600528,
      0.43889224795226434,
      0.40458396715841394,
      0.4126664627754435,
      0.3745006444948664,
      0.3726893067917789,
      0.3095155643915777
    ],
    [
      0.35928537797606963,
      0.5897431006897238,
      0.5397398243594871,
      0.5641739100201351,
      0.426545796538361,
      0.4031502768219928,
      0.364104395020977,
      0.4826918104875271,
      0.5560948770864709,
      0.41259980813425035,
      0.0,
      0.3201291261454189,
      0.4901415361800159,
      0.5498511698641062,
      0.4805455500720961,
      0.5245192953470827,
      0.3713019374276423,
      0.4892253536123743,
      0.46388338167263177,
      0.4246517987820462,
      0.5188681788612688,
      0.34196177276605133,
      0.3855640379428267,
      0.5910126917935188,
      0.3362057123167794,
      0.52502412568513,
      0.41317220953005407,
      0.41116573373710197,
      0.34126814636269365
    ],
    [
      0.33448814713810204,
      0.32176683134320294,
      0.3496944898350931,
      0.32327191602500793,
      0.33848396469885755,
      0.3124284933055097,
      0.323353017301627,
      0.3105024305640933,
      0.3084390856898225,
      0.3395174001430705,
      0.31515598909400655,
      0.0,
      0.3474785136155112,
      0.29867448692510945,
      0.33178130925666127,
      0.2983361496920145,
      0.38206890746255984,
      0.27294050252669977,
      0.3381491719539158,
      0.26846049744862044,
      0.3476290428272808,
      0.3023499906096141,
      0.3182099629583983,
      0.29455551336160535,
      0.3327048320765631,
      0.31991295063783065,
      0.28944137335407905,
      0.33099230160134874,
      0.29969275206513624
    ],
    [
      0.3257610185579043,
      0.5810714998434185,
      0.5135377367227056,
      0.4602337706737074,
      0.34693542419240675,
      0.3496650864449675,
      0.39433184857381365,
      0.5090766849528647,
      0.5160886041198314,
      0.3445602734980038,
      0.4001894931446499,
      0.2868930562270966,
      0.0,
      0.5208799166260007,
      0.41563485922764354,
      0.5013796658325766,
      0.31591135755705,
      0.4321450428005271,
      0.5146676985448624,
      0.31880010578049167,
      0.4319865435538075,
      0.2901044600615954,
      0.27880525371001474,
      0.4277372845968619,
      0.28431831404475005,
      0.46984869945749885,
      0.28554056562424357,
      0.33931965976578304,
      0.2530775461086572
    ],
    [
      0.3339318414867245,
      0.6325419076570635,
      0.54781339476629,
      0.43891987541739286,
      0.40490171528822194,
      0.34843191838167265,
      0.3796383687363749,
      0.5077383888721112,
      0.5219598650857449,
      0.35769089357625017,
      0.4868323529680718,
      0.2680568924404667,
      0.5544289266079263,
      0.0,
      0.47100204986576744,
      0.5032300132416914,
      0.3251335766124239,
      0.45248365199098894,
      0.47389341994063816,
      0.4351463154687376,
      0.4650243844804616,
      0.3488225534528111,
      0.3635798191790487,
      0.5821832087630079,
      0.26273542312004516,
      0.4825486715727787,
      0.36345038353572456,
      0.35564480655769026,
      0.3189318897274753
    ],
    [
      0.35367309569058714,
      0.49703593655400535,
      0.47220375987168683,
      0.47586511892899597,
      0.4527427042611203,
      0.3906292840619674,
      0.4085779113463692,
      0.46269030727177407,
      0.500017740438597,
      0.45576467966907397,
      0.4723329140792767,
      0.3430018770046521,
      0.4720583241775884,
      0.5087966259981591,
      0.0,
      0.46313720089959887,
      0.44091919967436777,
      0.4229063203737262,
      0.4274955200666304,
      0.4284744722031242,
      0.511888499270035,
      0.3837072787479032,
      0.3413892353032548,
      0.4772454945734357,
      0.3824271416977807,
      0.48069663093386716,
      0.3821690239577036,
      0.3813980077544159,
      0.3974911797160299
    ],
    [
      0.35144288067903595,
      0.6409267777222791,
      0.5660775279956116,
      0.46331994693234035,
      0.3962865334318615,
      0.3945309260668466,
      0.3524700045211262,
      0.44855245249542963,
      0.48791747607814484,
      0.34812288342509823,
      0.43129797733930064,
      0.3003703262318518,
      0.5325921413184986,
      0.5519837486933088,
      0.4557955071048019,
      0.0,
      0.34546550070027626,
      0.494855811847281,
      0.48022998683982876,
      0.36054446733992074,
      0.446412238684589,
      0.3292267926081762,
      0.29515364105267894,
      0.5386089907139848,
      0.31530146758359345,
      0.47730662174195615,
      0.32159194831829696,
      0.35472500220796,
      0.3036044640078597
    ],
    [
      0.34983352687354863,
      0.42354349453399776,
      0.4396898503172484,
      0.4067568615322781,
      0.4798490954695016,
      0.4000451455909724,
      0.35886557819996745,
      0.39485067335434065,
      0.4544752259817417,
      0.4246120578601731,
      0.3356208255025377,
      0.3718321232707009,
      0.42713576176527646,
      0.37207592595014805,
      0.387545211588582,
      0.4341023246129905,
      0.0,
      0.42098984263386585,
      0.45754347679602225,
      0.3597902427704187,
      0.46387021145476415,
      0.28689566284473766,
      0.3360725223191723,
      0.40009295908723375,
      0.4321689615779385,
      0.374562060301469,
      0.3987919517302161,
      0.41421311228058477,
      0.32264312697452713
    ],
    [
      0.28216671575888363,
      0.49740143657930136,
      0.4110663758661186,
      0.3847876479426253,
      0.37196027070972737,
      0.36870138403581576,
      0.3572369908339956,
      0.40724590789185133,
      0.4384967526991883,
      0.30047525827226185,
      0.3742412134016837,
      0.2628559199368419,
      0.43774631829705934,
      0.429876212557859,
      0.3990154157101675,
      0.4735607052958746,
      0.3371981805286337,
      0.0,
      0.5081367741476748,
      0.3157548983579992,
      0.4127993742971523,
      0.2831025227627235,
      0.2668219923605828,
      0.4457977333946963,
      0.29752705545315017,
      0.3641590648079047,
      0.2774294039977854,
      0.3053932228327856,
      0.26417291665834863
    ],
    [
      0.320253035369398,
      0.5633113642377638,
      0.5111590199211882,
      0.4403561835888792,
      0.4299262037515865,
      0.40454913670082626,
      0.39313992742121373,
      0.4307738535340364,
      0.5063182171334768,
      0.39941212648366387,
      0.4296927295403563,
      0.3053038635041574,
      0.5282765130708007,
      0.49568063126558237,
      0.46214873262435585,
      0.4709914914264146,
      0.39729884587786746,
      0.5128667708246879,
      0.0,
      0.36631424813591584,
      0.44420550819793414,
      0.316274366112518,
      0.3000946884895872,
      0.5317899784370199,
      0.37714691482604445,
      0.3712455569511648,
      0.36074748363956877,
      0.3390634036571982,
      0.2738259851989828
    ],
    [
      0.30009911037649717,
      0.3770397116267794,
      0.35713107189746407,
      0.32595597764588,
      0.38593549491554024,
      0.317034692206386,
      0.32318201272778113,
      0.31614137214704474,
      0.4041215488246219,
      0.3647089277050881,
      0.3631348179029821,
      0.23170595093378088,
      0.3339867256038882,
      0.40144146556973404,
      0.35933568170703034,
      0.3308811007758834,
      0.3341727011240112,
      0.3606209552718207,
      0.3715075199127478,
      0.0,
      0.3415632861067617,
      0.304219467264357,
      0.2750375951490247,
      0.37994074960616064,
      0.3089097888138663,
      0.3391762866682302,
      0.32681398867387945,
      0.3433533471071315,
      0.28548501448708175
    ],
    [
      0.31832916488992,
      0.45529694210921545,
      0.44777287616806927,
      0.4150680806473004,
      0.4338959030325704,
      0.34256812128361336,
      0.31992934402495043,
      0.41050808585483334,
      0.4743714850429439,
      0.34603944926291574,
      0.4002270167633277,
      0.2590111627861189,
      0.4355578908029003,
      0.46760245753622676,
      0.3762460812613897,
      0.4690378441196772,
      0.328985066311718,
      0.4626245892078862,
      0.3928317233357719,
      0.30946929525659295,
      0.0,
      0.2703359225328714,
      0.24923658850329233,
      0.4038939735263083,
      0.30420466159154147,
      0.44282324722292543,
      0.3498220554288809,
      0.30948298350836945,
      0.25609566816214024
    ],
    [
      0.40600547878297943,
      0.41596146918810617,
      0.3803866853202267,
      0.380701890449163,
      0.37236309512123444,
      0.3889479814671126,
      0.4349352596296596,
      0.4156507856164775,
      0.42927655364657546,
      0.3639604577617874,
      0.39106431374878436,
      0.3099263307634339,
      0.42378810344061235,
      0.45767812880822323,
      0.4706029548946471,
      0.41028489877669494,
      0.3530572793186577,
      0.3902294546181122,
      0.4234327085014211,
      0.4449526533204262,
      0.3993703856298356,
      0.0,
      0.38591964186480077,
      0.45136167299536045,
      0.30681898075488334,
      0.3936901028731843,
      0.34518026844031646,
      0.36211298014231974,
      0.42343059389348436
    ],
    [
      0.32226584435277505,
      0.35307438730534924,
      0.36351020066448214,
      0.3302241022581196,
      0.3701344096262673,
      0.3380711085967514,
      0.3255749078212278,
      0.3955457804289453,
      0.34243821335448477,
      0.38993823869812627,
      0.4052916899198886,
      0.2890905609221741,
      0.3309016879458091,
      0.3922971661681267,
      0.3917321888592482,
      0.3572907585418974,
      0.3624713767035297,
      0.3398376632109399,
      0.3214927960969347,
      0.3449568074955156,
      0.39220404546805576,
      0.3781512055945919,
      0.0,
      0.3916293821227266,
      0.3747749067361008,
      0.4384607421863913,
      0.40681623311514725,
      0.3702671464332359,
      0.3664737602549022
    ],
    [
      0.3330380740401009,
      0.5850159301311819,
      0.5270925670329398,
      0.4502514475538548,
      0.3772600843931724,
      0.4397132837819324,
      0.3505009618234898,
      0.444157199646557,
      0.4831535612034221,
      0.3687026029592331,
      0.46830135951158214,
      0.27689676216289527,
      0.4573652291121739,
      0.5391499902319019,
      0.4419707527621155,
      0.5807310469458902,
      0.37980406560747393,
      0.475475660454008,
      0.5144415476859108,
      0.38378828147390687,
      0.4376928528646664,
      0.34198400559918474,
      0.33248111882625797,
      0.0,
      0.31530550755971976,
      0.49128556702281,
      0.32795115803080366,
      0.34888613086757014,
      0.3116288117216064
    ],
    [
      0.2911389345208537,
      0.3311254500933716,
      0.41787844977454247,
      0.3866180390508396,
      0.42132642156723943,
      0.3990227314150443,
      0.32705842912561756,
      0.32897542621046894,
      0.4031360746361863,
      0.44111737113049254,
      0.3219615293299052,
      0.32204735307403487,
      0.350433938998179,
      0.32017182947525225,
      0.3014611772796649,
      0.34228431122773384,
      0.4139234739313755,
      0.36356635463654885,
      0.4141107277124012,
      0.3434415687617083,
      0.4029075269971425,
      0.2696995394279369,
      0.32568256934221274,
      0.3473933711538342,
      0.0,
      0.34156475889144877,
      0.39520844305481684,
      0.41859714285708405,
      0.2799275055076633
    ],
    [
      0.33370422919073817,
      0.5499148725860148,
      0.5031274940604957,
      0.47309690925640213,
      0.41194815181375954,
      0.38507942195120015,
      0.3552877861981665,
      0.56637242097198,
      0.5056638755272735,
      0.40623138661281866,
      0.5020498711026526,
      0.28191118936477455,
      0.4977110520877728,
      0.5450796105614135,
      0.47063325181356674,
      0.5466060888783959,
      0.3412887907995712,
      0.4683648803592604,
      0.40440319038047434,
      0.4166148016598603,
      0.5102628080149345,
      0.3624844617519527,
      0.3645984787632308,
      0.5437288226487871,
      0.3228341886090269,
      0.0,
      0.3638171713242524,
      0.36164545884678145,
      0.3396418620652697
    ],
    [
      0.3510681287911164,
      0.3900278496019072,
      0.4276338991128419,
      0.4060679981133579,
      0.46475402160615387,
      0.3355971664775861,
      0.3371014245014561,
      0.37414073895314925,
      0.43429434646234255,
      0.39584736310850843,
      0.3918247658862566,
      0.3369731207144224,
      0.3811606901555009,
      0.3728874631734893,
      0.3550855909359545,
      0.3484992259783879,
      0.4326560200218934,
      0.38521469311511125,
      0.4079088183911732,
      0.4025010605901784,
      0.4788126057051847,
      0.2992304363837739,
      0.36359379880588705,
      0.3400914319177424,
      0.4216393714939133,
      0.397957206111353,
      0.0,
      0.3558878724304815,
      0.2906048851434542
    ],
    [
      0.3496306677149694,
      0.39717229829720124,
      0.3678439920456733,
      0.4145906713972154,
      0.3644622628276559,
      0.3227554213013608,
      0.3264580206225647,
      0.3295366670744857,
      0.4377676844256,
      0.35315624473918117,
      0.42634785024096367,
      0.30728515754406005,
      0.3815216390361649,
      0.3559831147986634,
      0.37480285944623737,
      0.40265943605633203,
      0.3843622618166467,
      0.3687118356626704,
      0.3381466680248084,
      0.35685132793827323,
      0.37048325966910944,
      0.29916936341607503,
      0.34887666469000056,
      0.3602646548550297,
      0.41767285743816074,
      0.3130155901561906,
      0.3745223500420509,
      0.0,
      0.33506444048909834
    ],
    [
      0.38185590256969837,
      0.3387568671530714,
      0.34593612648753713,
      0.3677918511447582,
      0.3490071663405858,
      0.34323943977668114,
      0.31521901450445133,
      0.3611278415743424,
      0.3666361412802348,
      0.36994505506353703,
      0.4061460237327017,
      0.34116362946375123,
      0.3308168158048743,
      0.3680051472477639,
      0.3931166774692809,
      0.3642781058744968,
      0.35771163170949416,
      0.319914830042195,
      0.31478402432815056,
      0.38537972721835767,
      0.3756231789971647,
      0.45007260785954006,
      0.3914635223143492,
      0.3878902248082612,
      0.37543365213552504,
      0.3989304246784886,
      0.33213243346362753,
      0.3480137322193362,
      0.0
    ]
  ],
  "row_avgs": [
    0.2177987464305832,
    0.3826329263929215,
    0.4200054160898151,
    0.4283628968694354,
    0.46635844707169466,
    0.4275575586014844,
    0.3825599690736124,
    0.42983021094683005,
    0.379081419618254,
    0.40073570358157307,
    0.4527364619726369,
    0.3196600008396907,
    0.39673219536584764,
    0.4280963038854858,
    0.435240553018776,
    0.42088264441721207,
    0.39744527904196264,
    0.36696884519245326,
    0.41722024214007813,
    0.3379512986696948,
    0.373259560006224,
    0.39753896820601853,
    0.36374704681720516,
    0.42085805575022717,
    0.3579207303279857,
    0.43336080454288667,
    0.3813950712029492,
    0.363539830777373,
    0.3635854212593662
  ],
  "col_avgs": [
    0.33456664400166897,
    0.4785315619494726,
    0.4484345072682383,
    0.4175166377111724,
    0.3956402417562829,
    0.36947203692456476,
    0.35539490990635647,
    0.41518501757959697,
    0.4538606185178301,
    0.3763857600294827,
    0.4125315149060692,
    0.29717089878138864,
    0.4387313227852476,
    0.44329998255787845,
    0.4118826387715274,
    0.4496373868765265,
    0.3669647308509392,
    0.4250816039664443,
    0.4285496162591403,
    0.3652654422134045,
    0.4339150583200029,
    0.3203990929126973,
    0.31699310648054485,
    0.4509373707408761,
    0.34046246404914077,
    0.40891659216448417,
    0.3517308256604522,
    0.3506625691973741,
    0.30494245497147315
  ],
  "combined_avgs": [
    0.2761826952161261,
    0.43058224417119706,
    0.4342199616790267,
    0.4229397672903039,
    0.4309993444139888,
    0.39851479776302456,
    0.3689774394899844,
    0.4225076142632135,
    0.41647101906804207,
    0.38856073180552786,
    0.4326339884393531,
    0.3084154498105397,
    0.4177317590755476,
    0.4356981432216821,
    0.42356159589515174,
    0.4352600156468693,
    0.38220500494645093,
    0.3960252245794488,
    0.4228849291996092,
    0.3516083704415497,
    0.4035873091631135,
    0.3589690305593579,
    0.340370076648875,
    0.4358977132455516,
    0.34919159718856324,
    0.42113869835368545,
    0.3665629484317007,
    0.35710119998737355,
    0.33426393811541966
  ],
  "gppm": [
    626.2541258496814,
    584.9938411708778,
    600.8061038356067,
    617.817971926084,
    627.6336638299041,
    641.1780326819671,
    643.1716966698967,
    615.306075072205,
    596.8680035078125,
    635.9422156678609,
    620.2036835208261,
    670.8080564802242,
    605.6489009131886,
    605.5688211442086,
    620.4186143088589,
    598.9093779147587,
    639.7647449938385,
    611.258252240986,
    613.865628742408,
    634.0003949103781,
    605.8864437626538,
    660.5403150063461,
    664.4402531058101,
    601.5354531001074,
    651.5475259612045,
    615.831083985513,
    646.4909919327974,
    646.9766122796142,
    668.1996182812898
  ],
  "gppm_normalized": [
    1.4504572314518422,
    1.3029991200195659,
    1.3353806109151718,
    1.3735162043354556,
    1.3946138109383013,
    1.423227704723935,
    1.4319600350938348,
    1.3679456752285046,
    1.3220640893467015,
    1.4136920378055662,
    1.3768668676340368,
    1.4927528687118465,
    1.3496792378941695,
    1.3472718245539916,
    1.377054326090702,
    1.3339471756971772,
    1.4220259770770718,
    1.3558475091808018,
    1.3633804859125627,
    1.4016367778030323,
    1.3391881011242857,
    1.4636536171434693,
    1.4778762105634182,
    1.3272201060174462,
    1.444732167244386,
    1.3714233976645538,
    1.4353522154967757,
    1.4446632651135316,
    1.480357851700083
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374,
    695,
    444,
    457,
    470,
    451,
    421,
    487,
    408,
    443,
    385,
    446,
    534,
    403,
    397,
    389,
    359,
    422,
    369,
    489,
    402,
    372,
    387,
    401,
    466,
    437,
    374,
    409,
    411,
    327,
    755,
    383,
    457,
    399,
    413,
    459,
    551,
    467,
    449,
    422,
    434,
    467,
    444,
    444,
    376,
    444,
    417,
    432,
    477,
    391,
    419,
    369,
    378,
    450,
    409,
    480,
    439,
    393,
    384,
    583,
    456,
    425,
    387,
    417,
    449,
    425,
    437,
    393,
    420,
    374,
    404,
    454,
    426,
    423,
    398,
    409,
    381,
    520,
    456,
    379,
    385,
    394,
    407,
    396,
    372,
    374,
    470,
    402,
    563,
    419,
    486,
    424,
    424,
    414,
    471,
    416,
    473,
    441,
    439,
    666,
    500,
    434,
    456,
    490,
    381,
    408,
    499,
    418,
    402,
    406,
    404,
    441,
    453,
    425,
    410,
    399,
    391,
    1852,
    438,
    429,
    445,
    405,
    405,
    439,
    360,
    384,
    467,
    361,
    457,
    460,
    414,
    383,
    438,
    377,
    448,
    418,
    409,
    361,
    382,
    387,
    397,
    409,
    374,
    370,
    427,
    357,
    534,
    447,
    448,
    403,
    355,
    451,
    453,
    385,
    402,
    365,
    412,
    517,
    464,
    419,
    436,
    457,
    364,
    375,
    428,
    439,
    433,
    403,
    388,
    421,
    408,
    438,
    380,
    418,
    360,
    583,
    500,
    431,
    421,
    397,
    390,
    440,
    428,
    487,
    404,
    386,
    398,
    443,
    429,
    422,
    512,
    374,
    479,
    414,
    540,
    462,
    414,
    389,
    464,
    390,
    437,
    402,
    402,
    361
  ],
  "response_lengths": [
    2448,
    2788,
    2448,
    2392,
    2245,
    2294,
    2534,
    2359,
    2723,
    2206,
    2193,
    2337,
    2484,
    2418,
    2335,
    2804,
    2130,
    2645,
    2400,
    3051,
    2488,
    2289,
    2337,
    2574,
    2223,
    2365,
    2368,
    2296,
    2119
  ]
}