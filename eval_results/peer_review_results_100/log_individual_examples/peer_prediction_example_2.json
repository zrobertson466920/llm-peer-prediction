{
  "example_idx": 2,
  "reference": "Under review as a conference paper at ICLR 2023\n\nDIP-GNN: DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph neural network (GNN) pre-training methods have been proposed to enhance the power of GNNs. Specifically, a GNN is first pre-trained on a large-scale unlabeled graph and then fine-tuned on a separate small labeled graph for downstream applications, such as node classification. One popular pre-training method is to mask out a proportion of the edges, and a GNN is trained to recover them. However, such a generative method suffers from graph mismatch. That is, the masked graph input to the GNN deviates from the original graph. To alleviate this issue, we propose DiP-GNN (Discriminative Pre-training of Graph Neural Networks). Specifically, we train a generator to recover identities of the masked edges, and simultaneously, we train a discriminator to distinguish the generated edges from the original graph’s edges. The discriminator is subsequently used for downstream fine-tuning. In our pre-training framework, the graph seen by the discriminator better matches the original graph because the generator can recover a proportion of the masked edges. Extensive experiments on large-scale homogeneous and heterogeneous graphs demonstrate the effectiveness of the proposed framework. Our code will be publicly available.\n\n1\n\nINTRODUCTION\n\nGraph neural networks (GNNs) have achieved superior performance in various applications, such as node classification (Kipf & Welling, 2017), knowledge graph modeling (Schlichtkrull et al., 2018) and recommendation systems (Ying et al., 2018). To enhance the power of GNNs, generative pretraining methods are developed (Hu et al., 2020b). During the pre-training stage, a GNN incorporates topological information by training on a large-scale unlabeled graph in a self-supervised manner. Then, the pre-trained model is fine-tuned on a separate small labeled graph for downstream applications. Generative GNN pre-training is akin to masked language modeling in language model pre-training (Devlin et al., 2019). That is, for an input graph, we first randomly mask out a proportion of the edges, and then a GNN is trained to recover the original identity of the masked edges.\n\nOne major drawback with the abovementioned approach is graph mismatch. That is, the input graph to the GNN deviates from the original one since a considerable amount of edges are dropped. This causes changes in topological information, e.g., node connectivity. Consequently, the learned node embeddings may not be desirable.\n\nTo mitigate the above issues, we propose DiP-GNN ( Discriminative Pre-training of Graph Neural Networks). In DiP-GNN, we simultaneously train a generator and a discriminator. The generator is trained similar to existing generative pre-training approaches, where the model seeks to recover the masked edges and outputs a reconstructed graph. Subsequently, the reconstructed graph is fed to the discriminator, which predicts whether each edge resides in the original graph (i.e., a true edge) or is wrongly constructed by the generator (i.e., a fake edge). After pre-training, we fine-tune the discriminator on downstream tasks. Figure 1 illustrates our training framework. Note that our work is related to Generative Adversarial Nets (GAN, Goodfellow et al. 2014), and detailed discussions are presented in Section 3.4. We remark that similar approaches have been used in natural language processing (Clark et al., 2020). However, we identify the graph mismatch problem (see Section 4.5), which is specific to graph-related applications and is not observed in natural language processing.\n\nThe proposed framework is more advantageous than generative pre-training. This is because the reconstructed graph fed to the discriminator better matches the original graph compared with the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of DiP-GNN. From left to right: Original graph; Graph with two masked edges (dashed lines); Reconstructed graph created by the generator (generated edges are the dashed red lines); Discriminator labels each edge as [G] (generated) or [O] (original), where there are two wrong labels (shown in red).\n\nmasked graph fed to the generator. Consequently, the discriminator can learn better node embeddings. Such a better alignment is because the generator recovers the masked edges during pretraining, i.e., we observe that nearly 40% of the missing edges can be recovered. We remark that in our framework, the graph fed to the generator has missing edges, while the graph fed to the discriminator contains wrong edges since the generator may make erroneous predictions. However, empirically we find that missing edges hurt more than wrong ones, making discriminative pre-training more desirable (see Section 4.5 in the experiments).\n\nWe demonstrate effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs. Results show that the proposed method significantly outperforms existing generative pre-training and self-supervised learning approaches. For example, on the homogeneous Reddit dataset (Hamilton et al., 2017) that contains 230k nodes, we obtain an improvement of 1.1 in terms of F1 score; and on the heterogeneous OAG-CS graph (Tang et al., 2008) that contains 1.1M nodes, we obtain an improvement of 2.8 in terms of MRR score in the paper field prediction task.\n\n2 BACKGROUND\n\n⋄ Graph Neural Networks. Graph neural networks compute a node’s representation by aggregating information from the node’s neighbors. Concretely, for a multi-layer GNN, the feature vector h(k) of node v at the k-th layer is\n\nv\n\na(k)\n\nv = Aggregate\n\n(cid:16)(cid:110)\n\nh(k−1)\n\nu\n\n∀u ∈ Neighbor(v)\n\n(cid:111)(cid:17)\n\n, h(k)\n\nv = Combine\n\n(cid:16)\n\nv , h(k−1) a(k)\n\nv\n\n(cid:17)\n\n,\n\nwhere Neighbor(v) denotes all the neighbor nodes of v. Various implementations of Aggregate(·) and Combine(·) are proposed for both homogeneous (Defferrard et al., 2016; Kipf & Welling, 2017; Velickovic et al., 2018; Xu et al., 2019) and heterogeneous graphs (Schlichtkrull et al., 2018; Wang et al., 2019; Zhang et al., 2019; Hu et al., 2020c).\n\n⋄ Graph Neural Network Pre-Training. Previous unsupervised learning methods leverage the graph’s proximity (Tang et al., 2015) or information gathered by random walks (Perozzi et al., 2014; Grover & Leskovec, 2016; Dong et al., 2017; Qiu et al., 2018). However, the learned embeddings cannot be transferred to unseen nodes, limiting the methods’ applicability. Other unsupervised learning algorithms adopt contrastive learning (Hassani & Ahmadi, 2020; Qiu et al., 2020; Zhu et al., 2020; 2021; You et al., 2020; 2021). That is, we generate two views of the same graph, and then maximize agreement of node presentations in the two views. However, our experiments reveal that these methods do not scale well to extremely large graphs with millions of nodes.\n\nMany GNN pre-training methods focus on generative objectives. For example, GAE (Graph AutoEncoder, Kipf & Welling 2016) proposes to reconstruct the graph structure; GraphSAGE (Hamilton et al., 2017) optimizes an unsupervised loss derived from a random-walk-based metric; and DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes the mutual information between node representations and a graph summary representation.\n\nThere are also pre-training methods that extract graph-level representations, i.e., models are trained on a large amount of small graphs instead of a single large graph. For example, Hu et al. 2020a propose pre-training methods that operate on both graph and node level; and InfoGraph (Sun et al., 2020) proposes to maximize the mutual information between graph representations and representations of the graphs’ sub-structures. In this work, we focus on pre-training GNNs on a single large graph instead of multiple small graphs.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHOD\n\nWe formally introduce the proposed discriminative GNN pre-training framework DiP-GNN. The algorithm contains two ingredients that operate on edges and features.\n\n3.1 EDGE GENERATION AND DISCRIMINATION\n\nSuppose we have a graph G = (N , E), where N denotes all the nodes and E denotes all the edges. We randomly mask out a proportion of the edges, such that E = Eu ∪ Em, where Eu is the unmasked set of edges and Em is the set of edges that are masked out.\n\nFor a masked edge e = (n1, n2) ∈ Em, where n1 and n2 are the two nodes connected by e, the generator’s goal is to predict n1 given n2 and the unmasked edges Eu. For each node n, we compute its representation hg(n) = f e g), which is parameterized by θe g. We remark that the computation of hg(·) only relies on the unmasked edges Eu. We assume that the generation process of each edge is independent. Then, we have the prediction probability\n\ng) using the generator f e\n\ng (n, θe\n\ng (·, θe\n\np(n1|n2, Eu) =\n\nexp (d(hg(n1), hg(n2))) n′∈C exp (d(hg(n′), hg(n2)))\n\n(cid:80)\n\n, C = {n1} ∪ (N \\ Neighbor(n2)).\n\n(1)\n\nHere, C is the candidate set for n1, which contains all the nodes that are not connected to n2 except n1 itself. Moreover, the distance function d(·, ·) is chosen as a trainable cosine similarity, i.e.,\n\nwhere W cos is a trainable weight. The training loss for the generator is defined as\n\nd(u, v) =\n\n(W cosu)⊤v ||W cosu|| · ||v||\n\n,\n\ng) = (cid:80) which is equivalent to maximizing the likelihood of correct predictions.\n\n− log p(n1|n2, Eu),\n\n(n1,n2)∈Em\n\ng(θe\n\nLe\n\n(2)\n\n(3)\n\nThe goal of the generator is to recover the masked edges in Em. Therefore, after we train the generator, we use the trained model to generate Eg = {((cid:98)n1, n2)}(n1,n2)∈Em, where each (cid:98)n1 is the model’s prediction as (cid:98)n1 = argmaxn′∈C p(n′|n2, Eu). Because the generator cannot correctly predict every edge, some edges in Eg are wrongly generated (i.e., not in Em). We refer to such edges as fake edges, and the rest as true edges. Concretely, we denote the true edges E true = Eu ∪(Em ∩Eg), i.e., the unmasked edges and the edges correctly generated by the generator. Correspondingly, we denote the fake edges E fake = E \\ E true.\n\nThe discriminator is trained to distinguish edges that are from the original graph (i.e., the true edges) and edges that are not (i.e., fake edges). Specifically, given the true edges E true and the fake ones E fake, we first compute hd(n) = f e d) is the discriminator model parameterized by θe d. We highlight that different from computing hg(·), the computation of hd(·) relies on all the edges, such that the discriminator can separate a fake edge from a true one. Then, for each edge e = (n1, n2) ∈ E true ∪ E fake, the discriminator outputs\n\nd) for every node n ∈ N , where f e\n\nd (n, θe\n\nd (·, θe\n\npfake = p(e ∈ E fake|E true, E fake) = sigmoid (d(hd(n1), hd(n2))) , where d(·, ·) is the distance function in Eq. 2. The training loss for the discriminator is the binary cross-entropy loss of predicting whether an edge is fake or not, defined as\n\n(4)\n\nLe\n\nd) = (cid:80) where 1{·} is the indicator function.\n\ne∈Etrue∪Efake\n\nd(θe\n\n−1{e ∈ E fake} log(pfake) − 1{e ∈ E true} log(1 − pfake),\n\n(5)\n\nThe edge loss is the weighted sum of the generator’s and the discriminator’s loss g, θe\n\nLe(θe where λ is a hyper-parameter. Note that structures of the generator f e d are flexible, e.g., they can be graph convolutional networks (GCN) or graph attention networks (GAT).\n\ng and the discriminator f e\n\ng) + λLe\n\nd) = Le\n\nd(θe\n\ng(θe\n\nd),\n\n(6)\n\n3.2 FEATURE GENERATION AND DISCRIMINATION\n\nIn real-world applications, nodes are often associated with features. For example, in the Reddit dataset (Hamilton et al., 2017), a node’s feature is a vectorized representation of the post corresponding to the node. As another example, in citation networks (Tang et al., 2008), a paper’s title\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ncan be treated as a node’s feature. Previous work (Hu et al., 2020b) has demonstrated that generating features and edges simultaneously can improve the GNN’s representation power.\n\nNode features can be either texts (e.g., in citation networks) or vectors (e.g., in recommendation systems). In this section, we develop feature generation and discrimination procedures for texts. Vector features are akin to encoded text features, and we can use linear layers to generate and discriminate them. Details about vector features are deferred to Appendix B.\n\ng (·, θf\n\ng ) = trmg ◦ embg(·) the generator parameterized by θf\n\nFor text features, we parameterize both the feature generator and the feature discriminator using bi-directional Transformer models (Vaswani et al., 2017), similar to BERT (Devlin et al., 2019). Denote f f g , where embg is the word embedding function and trmg denotes subsequent Transformer layers. For an input text feature x = [x1, · · · , xL] where L is the sequence length, we randomly select indices to mask out, i.e., we randomly select an index set M ⊂ {1, · · · , L}. For a masked position i ∈ M, the prediction probability is given by\n\np(xi|x) =\n\nexp (cid:0)embg(xi)⊤vg(xi)(cid:1) x′∈vocab exp (embg(x′)⊤vg(x′))\n\n(cid:80)\n\n, vg(xi) = trmg\n\n(cid:0)W proj\n\ng\n\n[hg(nx), embg(xi)](cid:1) . (7)\n\ng\n\nHere W proj is a trainable weight and hg(nx) is the representation of the node corresponding to x computed by the edge generation GNN. Note that we concatenate the text embedding embg(xi) and the feature node’s embedding hg(nx), such that the feature generator can aggregate information from the graph structure. We train the generator by maximizing the probability of predicting the correct token, i.e., by minimizing the loss\n\nLf\n\ng (θe\n\ng, θf\n\ng ) = (cid:80)\n\nx\n\n(cid:80)\n\ni∈M − log p(xi|x).\n\n(8)\n\nAfter we train the generator, we use the trained model to predict all the masked tokens, after which we obtain a new text feature xcorr. Here, we set xcorr i = (cid:98)xi for i ∈ M, where (cid:98)xi = argmaxx′∈vocab p(xi|x) is the generator’s prediction. The discriminator is trained to distinguish the fake tokens (i.e., wrongly generated tokens) from the true ones (i.e., the unmasked and correctly generated tokens) in xcorr. Similar to the generator, we denote f f d . For each position i, the discriminator’s prediction probability is defined as\n\nd ) = trmd ◦ embd(·) as the discriminator parameterized by θf\n\ni = xi for i /∈ M and xcorr\n\nd (·, θf\n\np(xcorr\n\ni = xi) = sigmoid (cid:0)w⊤vd(xcorr\n\ni\n\n)(cid:1) , vd(xcorr\n\ni\n\n) = trmd\n\n(cid:16)\n\nW proj\n\nd\n\n[hd(nx), embd(xcorr\n\ni\n\n(cid:17)\n\n)]\n\n.\n\n(9)\n\nHere w and W proj to x computed by the edge discriminator GNN. The training loss for the discriminator is\n\nare trainable weights and hd(nx) is the representation of the node corresponding\n\nd\n\nLf where ptrue = p(xcorr\n\nd, θf\n\nd ) = (cid:80) i = xi) and 1{·} is the indicator function.\n\ni=1 −1{xcorr\n\ni = xi} log(ptrue) − 1{xcorr\n\nd (θe\n\n(cid:80)L\n\nx\n\ni\n\n̸= xi} log(1 − ptrue),\n\n(10)\n\nThe text feature loss is defined as\n\nLf (θe\n\ng, θf\n\ng , θe\n\nd, θf\n\nd ) = Lf\n\ng (θe\n\ng, θf\n\ng ) + λLf\n\nd (θe\n\nd, θf\n\nd ),\n\nwhere λ is a hyper-parameter.\n\n3.3 MODEL TRAINING\n\nWe jointly minimize the edge loss and the feature loss, where the loss function is\n\nL(θe\n\ng, θf\n\ng , θe\n\nd, θf\n\nd ) = Le(θe = (cid:0)Le g(θe\n\ng, θe\n\nd) + Lf (θe\n\ng) + Lf\n\ng (θe\n\ng, θf\n\nd, θf d ) (cid:16)\n\ng , θe g, θf g )(cid:1) + λ\n\nLe\n\nd(θe\n\nd) + Lf\n\nd (θe\n\nd, θf d )\n\n(11)\n\n(cid:17)\n\n.\n\n(12)\n\nHere, λ is the weight of the discriminator’s loss. We remark that our framework is flexible because the generator’s loss (Le d ). As such, existing generative pre-training methods can be applied to train the generator. In DiP-GNN, the discriminator has a better quality than the generator because of the graph mismatch issue (see Section 4.5). Therefore, after pre-training, we discard the generator and fine-tune the discriminator on downstream tasks. A detailed training pipeline is presented in Appendix A.\n\ng ) is decoupled from the discriminator’s (Le\n\nd and Lf\n\ng and Lf\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.4 COMPARISON WITH GAN\n\nWe remark that our framework is different from Generative Adversarial Nets (GAN, Goodfellow et al. 2014). In GAN, the generator-discriminator training framework is formulated as a min-max game, where the generator is trained adversarially to fool the discriminator. The two models are updated using alternating gradient descent/ascent.\n\nHowever, the min-max game formulation of GAN is not applicable to our framework. This is because in GNN pre-trianing, the generator generates discrete edges, unlike continuous pixel values in the image domain. Such a property prohibits back-propagation from the discriminator to the generator. Existing works (Wang et al., 2018) use reinforcement learning (specifically policy gradient) to circumvent the non-differentiability issue. However, reinforcement learning introduces extensive hyper-parameter tuning and suffers from scalability issues. For example, the largest graph used in Wang et al. 2018 only contains 18k nodes, whereas the smallest graph used in our experiments has about 233k nodes.\n\nAdditionally, the goal of GAN is to train good-quality generators, which is different from our focus. In our discriminative pre-training framework, we focus on the discriminator because of better graph alignments. In practice, we find that accuracy of the generator is already high even without the discriminator, e.g., the accuracy is higher than 40% with 255 negative samples. And we observe that further improving the generator does not benefit downstream tasks.\n\n4 EXPERIMENTS\n\nWe implement all the algorithms using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019). Experiments are conducted on NVIDIA A100 GPUs. By default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. We also discuss other choices in the experiments. Training and implementation details are deferred to Appendix C.\n\n4.1 SETTINGS AND DATASETS\n\n⋄ Settings. We consider a node transfer setting in the experiments. In practice we often work with a single large-scale graph, on which labels are sparse. In this case, we can use the large amount of unlabeled data as the pre-training dataset, and the rest are treated as labeled fine-tuning nodes. Correspondingly, edges between pre-training nodes are added to the pre-training data, and edges between fine-tuning nodes are added to the fine-tuning data. In this way, the model cannot see the fine-tuning data during pre-training, and vice versa.\n\nWe remark that our setting is different from conventional self-supervised learning settings, namely we pre-train and fine-tune on two separate graphs. This meets the practical need of transfer learning, e.g., a trained GNN needs to transfer across locales and time spans in recommendation systems.\n\n⋄ Homogeneous Graph. We use the Reddit dataset (Hamilton et al., 2017), which is a publicly available large-scale graph. In this graph, each node corresponds to a post, and is labeled with a “subreddit”. Each node has a 603-dimensional feature vector constructed from the corresponding post. Two nodes (posts) are connected if the same user commented on both. The dataset contains posts from 50 subreddits sampled from posts initiated in September 2014. In total, there are 232,965 posts with an average node degree of 492. We use 70% of the data as the pre-training data, and the rest as the fine-tuning data, which are further split into training, validation, and test sets equally. We consider node classification as the downstream fine-tuning task.\n\n⋄ Product Recommendation Graph. We collect in-house product recommendation data from an e-commerce website. We build a bi-partite graph with two node types: search queries and product ids. The dataset contains about 633k query nodes, 2.71M product nodes, and 228M edges. We sample 70% of the nodes (and corresponding edges) for pre-training, and the rest are evenly split for fine-tuning training, validation and testing. We consider link prediction as the downstream task, where for each validation and test query node, we randomly mask out 20% of its edges to recover. For each masked edge that corresponds to a query node and a positive product node, we randomly sample 255 negative products. The task is to find the positive product out of the total 256 products.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n⋄ Heterogeneous Graph. We use the OAG-CS dataset (Tang et al., 2008; Sinha et al., 2015), which is a publicly available heterogeneous graph containing computer science papers. The dataset contains over 1.1M nodes and 28.4M edges. In this graph, there are five node types (institute, author, venue, paper and field) and ten edge types. The “field” nodes are further categorized into six levels from L0 to L5, which are organized using a hierarchical tree. Details are shown in Figure 2.\n\nWe use papers published before 2014 as the pre-training dataset (63%), papers published between 2014 (inclusive) and 2016 (inclusive) as the fine-tuning training set (20%), papers published in 2017 as the fine-tuning validation set (7%), and papers published after 2017 as the fine-tuning test set (10%). During fine-tuning, by default we only use 10% of the fine-tuning training data (i.e., 2% of the overall data) because in practice labeled data are often scarce. We consider three tasks for fine-tuning: author name disambiguation (AD), paper field classification (PF) and paper venue classification (PV). For paper field classification, we only consider L2 fields. processed graph from Hu et al. 2020b.\n\nFigure 2: Details of OAG-CS. There are 5 node types (in black) and 10 edge types (in red).\n\nIn the experiments, we use the pre-\n\n4.2\n\nIMPLEMENTATION DETAILS\n\n⋄ Graph subsampling. In practice, graphs are often too large to fit in the hardware, e.g., the Reddit dataset (Hamilton et al., 2017) contains over 230k nodes. Therefore, we sample a dense subgraph from the large-scale graph in each training iteration. For homogeneous graphs, we apply the LADIES algorithm (Zou et al., 2019), which theoretically guarantees that the sampled nodes are highly inter-connected with each other and can maximally preserve the graph structure. For heterogeneous graphs, we use the HGSampling algorithm (Hu et al., 2020b), which is a heterogeneous version of LADIES.\n\nIn the edge generator, for a masked edge (s, t), we fix ⋄ Node sampling for the edge generator. the node t and seek to identify the other node s. One approach is to identify s from all the graph nodes, i.e., by setting C = N in Eq. 1. However, this task is computationally intractable when the number of nodes is large, i.e., the model needs to find s out of hundreds of thousands of nodes. i }nneg Therefore, we sample some negative nodes {sg i , t) /∈ E. Then, the candidate set to generate the source node becomes {s, sg } instead of all the graph nodes N . We remark nneg that such a sampling approach is standard for GNN pre-training and link prediction (Hamilton et al., 2017; Sun et al., 2020; Hu et al., 2020b).\n\ni=1 such that (sg\n\n1, · · · , sg\n\n⋄ Edge sampling for the edge discriminator. In computing the loss for the discriminator, the number of edges in Eu is significantly larger than those in Eg, i.e., we only mask a small proportion of the edges. To avoid the discriminator from outputting trivial predictions (i.e., all the edges belong to Eu), we balance the two loss terms in Le u| = α|Eg|, where α is a hyper-parameter. Then, we compute Le u. Note that the node representations hd are still computed using all the generated and unmasked edges Eg and Eu.\n\nd. Specifically, we sample E d\n\nu ⊂ Eu such that |E d\n\nd on Eg and E d\n\n4.3 BASELINES\n\nWe compare our method with several baselines in the experiments. For fair comparison, all the methods are trained for the same number of GPU hours.\n\n⋄ GAE (Graph Auto-Encoder, Kipf & Welling 2016) adopts an auto-encoder for unsupervised learning on graphs. In GAE, node embeddings are learnt using a GNN, and we minimize the discrepancy between the original and the reconstructed adjacency matrix.\n\n⋄ GraphSAGE (Hamilton et al., 2017) encourages embeddings of neighboring nodes to be similar. For each node, the method learns a function that generates embeddings by sampling and aggregating features from the node’s neighbors.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Experimental results on homogeneous graphs. We report F1 averaged over 10 runs for the Reddit data and MRR over 10 runs for the product recommendation data. The best results are shown in bold.\n\nTable 2: Experimental results on OAG-CS (heterogeneous). Left to right: paper-field, paper-venue, author-name-disambiguation. We report MRR over 10 runs. The best results are shown in bold.\n\nReddit Recomm.\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\n87.3\n\n88.5 88.0 87.7 89.6 89.0 88.6 89.1\n\n90.7\n\n46.3\n\n56.7 53.0 53.3 58.6 51.5 —\n—\n\n60.1\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\nPF\n\n32.7\n\n40.3 37.8 38.1 41.6 38.0 38.0 38.6\n\n44.1\n\nPV\n\n19.6\n\n24.5 22.1 22.5 25.6 21.5 22.0 23.5\n\n27.7\n\nAD\n\n60.0\n\n62.5 62.9 63.0 63.1 62.0 61.5 62.8\n\n65.6\n\n(a) Author name disambiguation.\n\n(b) Paper field classification.\n\n(c) Paper venue classification.\n\nFigure 3: Model performance vs. amount of labeled data on OAG-CS.\n\n⋄ DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes mutual information between node representations and corresponding high-level summaries of graphs. Thus, a node’s embedding summarizes a sub-graph centered around it.\n\n⋄ GPT-GNN (Hu et al., 2020b) adopts a generative pre-training objective. The method generates edges by minimizing a link prediction objective, and incorporates node features in the framework.\n\n⋄ GRACE (Graph Contrastive Representation, Zhu et al. 2020) leverages a contrastive objective. The algorithm generates two views of the same graph through node and feature corruption, and then maximize agreement of node representations in the two views.\n\n⋄ GraphCL (You et al., 2020) is another graph contrastive learning approach that adopts node and edge augmentation techniques, such as node dropping and edge perturbation.\n\n⋄ JOAO (Joint Augmentation Optimization, You et al. 2021) improves GraphCL by deigning a bi-level optimization objective to automatically and dynamically selects augmentation methods.\n\n4.4 MAIN RESULTS\n\nIn Table 1 and Table 2, w/o pre-train means direct training on the fine-tuning dataset without pretraining. Results on the Reddit dataset are F1 scores averaged over 10 runs, and results on the product recommendation graph are MRR scores averaged over 10 runs. All the performance gain have passed a hypothesis test with p-value < 0.05.\n\nTable 1 summarizes experimental results on the homogeneous graphs: Reddit and Recommendation. We see that pre-training indeed benefits downstream tasks. For example, performance of GNN improves by at ≥ 0.4 F1 on Reddit (DGI) and ≥ 5.2 MRR on Recommendation (GRACE). Also, notice that among the baselines, generative approaches (GAE and GPT-GNN) yield promising per-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Neg. nodes for generation.\n\n(b) Pos. edges for discrimination.\n\n(c) Weight of discriminator’s loss.\n\nFigure 4: Ablation experiments on Reddit. By default, we set the number of negative nodes to 256, the factor of positive edges to 1.0, and weight of the discriminator’s loss to 20.\n\nTable 3: Test F1 score of model variants on Reddit.\n\nTable 4: Test F1 of models with different backbone graph neural networks on Reddit.\n\nModel\n\nEdges+Features\n\nEdges Features RandomEdges\n\nF1\n\n90.7\n\n90.4 90.2 89.8\n\nModel\n\nHGT GAT\n\nw/o pretrain\n\n87.3\n\n86.4\n\nGPT-GNN DiP-GNN\n\n89.6 90.7\n\n87.5 88.5\n\nFigure 5: F1 vs. proportion of manipulated edges on Reddit.\n\nformance. On the other hand, the contrastive method (GRACE, GraphCL and JOAO) does not scale well to large graphs, e.g., the OAG-CS graph which contains 1.1M nodes and 28.4M edges. By using the proposed discriminative pre-training framework, our method significantly outperforms all the baseline approaches. For example, DiP-GNN outperforms GPT-GNN by 1.1 on Reddit and 1.5 on Recommendation.\n\nExperimental results on the heterogeneous OAG-CS dataset are summarized in Table 2. Similar to the homogeneous graphs, notice that pre-training improves model performance by large margins. For example, pre-training improves MRR by at least 5.1, 2.5 and 2.5 on the PF, PV and AD tasks, respectively. Moreover, by using the proposed training framework, models can learn better node embeddings and yield consistently better performance compared with all the baselines.\n\nRecall that during fine-tuning on OAG-CS, we only use 10% of the labeled fine-tuning data (about 2% of the overall data). In Figure 3, we examine the effect of the amount of labeled data. We see that model performance improves when we increase the amount of labeled data. Also, notice that DiP-GNN consistently outperforms GPT-GNN in all the three tasks under all the settings.\n\n4.5 ANALYSIS\n\n⋄ Comparison with semi-supervised learning. We compare DiP-GNN with a semi-supervised learning method: C&S (Correct&Smooth, Huang et al. 2020). Figure 6 summarizes the results. We see that C&S yields a 0.5 improvement compared with the supervised learning method (i.e., w/o pre-train). However, performance of C&S is significantly lower than both DiP-GNN and other pre-training methods such as GPT-GNN.\n\n⋄ Hyper-parameters. There are several hyper-parameters that we introduce in DiP-GNN: the number of negative nodes that are sampled for generating edges (Section 4.2); the number of positive edges that are sampled for the discriminator’s task (Section 4.2); and the weight of the discriminator’s loss (Eq. 12). Figure 4 illustrate ablation experimental results on the Reddit dataset. From the results, we see that DiP-GNN is robust to these hyper-parameters. We remark that under all the settings, ours model behaves better than the best-performing baseline (89.6 for GPT-GNN).\n\nFigure 6: Test F1 on Reddit.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Generator and discriminator performance vs. proportion of masked edges during pre-training. Coverage is the proportion of true edges input to the models.\n\nMasked%\n\nAcc\n\nCoverage\n\nGen. Dis. Gen. Dis.\n\nRatio\n\n20 80 95\n\n0.50 0.33 0.20\n\n0.87 0.84 0.80\n\n0.80 0.20 0.05\n\n0.90 ×1.13 0.46 ×2.30 0.24 ×4.80\n\nFigure 7: Performance vs. proportion of masked edges on product recommendation.\n\n⋄ Model variants. We also examine variants of DiP-GNN. Recall that the generator and the discriminator operate on both edges and node features. We first check the contribution of these two factors. We also investigate the scenario where edges are randomly generated, and the discriminator still seeks to find the generated edges. Table 3 summarizes results on the Reddit dataset.\n\nWe see that by only using edges, model performance drops by 0.3; and by only using node features, performance drops by 0.5. This indicates that the graph structure plays a more important role in the proposed framework than the features. Also notice that performance of RandomEdges is unsatisfactory. This is because implausible edges are generated when using a random generator, making the discriminator’s task significantly easier. We remark that performance of all the model variants is better than the best-performing baseline (89.6 for GPT-GNN).\n\nTable 4 examines performance of our method and GPT-GNN using different backbone GNNs. Recall that by default, we use HGT (Hu et al., 2020c) as the backbone. We see that when GAT (Velickovic et al., 2018) is used, performance of DiP-GNN is still significantly better than GPT-GNN.\n\n⋄ Missing edges hurt more than wrong edges. In our pre-training framework, the generator is trained to reconstruct the masked graph, after which the reconstructed graph is fed to the discriminator. During this procedure, the graph input to the generator has missing edges, and the graph input to the discriminator has wrong edges. From Figure 5, we see that wrong edges hurt less than missing ones. For example, model performance drops by 0.7% when 50% of wrong edges are added to the original graph, and performance decreases by 1.8% when 50% of original edges are missing. This indicates that performance relies on the amount of original edges seen by the models. Intuitively, wrong edges add noise to the graph, but they do not affect information flow. On the contrary, missing edges cut information flow. Moreover, in practice we work with graph attention models, and the attention mechanism can alleviate the wrong edges by assigning low attention scores to them.\n\n⋄ Why is discriminative pre-training better? Figure 7 illustrates effect of the proportion of masked edges during pre-training. We see that when we increase the proportion from 0.2 to 0.8, performance of GPT-GNN drops by 6.1, whereas performance of DiP-GNN only drops by 3.3. This indicates that the generative pre-training method is more sensitive to the masking proportion.\n\nTable 5 summarizes pre-training quality. First, the generative task (i.e., the generator) is more difficult than the discriminative task (i.e., the discriminator). For example, when we increase the proportion of masked edges from 20% to 80%, accuracy of the generator drops by 17% while accuracy of the discriminator only decreases by 3%. Second, the graph input to the discriminator better aligns with the original graph. For example, when 80% of the edges are masked, the discriminator sees 2.3 times more original edges than the generator. Therefore, the discriminative task is more advantageous because model quality relies on the number of observed original edges (Figure 5).\n\n5 CONCLUSION AND DISCUSSIONS\n\nWe propose Discriminative Pre-Training of Graph Neural Networks (DiP-GNN), where we simultaneously train a generator and a discriminator. During pre-training, we mask out some edges in the graph, and a generator is trained to recover the masked edges. Subsequently, a discriminator seeks to distinguish the generated edges from the original ones. Our framework is more advantageous than generative pre-training for two reasons: the graph inputted to the discriminator better matches the original graph; and the discriminative pre-training task better aligns with downstream node classification. We conduct extensive experiments to validate the effectiveness of DiP-GNN.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nMicha ̈el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 510, 2016, Barcelona, Spain, pp. 3837–3845, 2016.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n\nYuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pp. 135–144. ACM, 2017. doi: 10.1145/3097983.3098036.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680, 2014.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.\n\nIn Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 855–864. ACM, 2016. doi: 10.1145/2939672.2939754.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024–1034, 2017.\n\nKaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 4116–4126. PMLR, 2020.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure In 8th International Conference Leskovec. Strategies for pre-training graph neural networks. on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020a.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: generative pre-training of graph neural networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1857–1867. ACM, 2020b.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen (eds.), WWW ’20: The Web\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nConference 2020, Taipei, Taiwan, April 20-24, 2020, pp. 2704–2710. ACM / IW3C2, 2020c. doi: 10.1145/3366423.3380027.\n\nQian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining arXiv preprint\n\nlabel propagation and simple models out-performs graph neural networks. arXiv:2010.13993, 2020.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ̈opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In Sofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani (eds.), The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, pp. 701–710. ACM, 2014. doi: 10.1145/2623330.2623732.\n\nJiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Yi Chang, Chengxiang Zhai, Yan Liu, and Yoelle Maarek (eds.), Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, pp. 459–467. ACM, 2018. doi: 10.1145/3159652.3159706.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1150–1160. ACM, 2020.\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593–607. Springer, 2018.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pp. 243–246, 2015.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale information network embedding. In Aldo Gangemi, Stefano Leonardi, and Alessandro Panconesi (eds.), Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015, pp. 1067–1077. ACM, 2015. doi: 10.1145/2736277.2741093.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and In Proceedings of the 14th ACM SIGKDD international\n\nmining of academic social networks. conference on Knowledge discovery and data mining, pp. 990–998, 2008.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nPetar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nHongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2508–2515. AAAI Press, 2018.\n\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S. Yu. Heterogeneous graph attention network. In Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia (eds.), The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pp. 2022–2032. ACM, 2019. doi: 10.1145/ 3308558.3313562.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Yike Guo and Faisal Farooq (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pp. 974–983. ACM, 2018. doi: 10.1145/3219819.3219890.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812–5823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. In International Conference on Machine Learning, pp. 12121–12132. PMLR, 2021.\n\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. Heterogeneous graph neural network. In Ankur Teredesai, Vipin Kumar, Ying Li, R ́omer Rosales, Evimaria Terzi, and George Karypis (eds.), Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 793–803. ACM, 2019. doi: 10.1145/3292500.3330961.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069–2080, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDifan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11247–11256, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA DETAILED ALGORITHM\n\nAlgorithm 1 is a detailed training pipeline of DiP-GNN. For graphs with vector features instead of text features, we can substitute the feature generation and discrimination modules with equations in Appendix B.\n\nAlgorithm 1: DiP-GNN: Discriminative Pre-training of Graph Neural Networks. Input: Graph Gfull; edge masking ratio; feature masking ratio; number of negative samples for edge generator; proportion of positive samples for edge discriminator α; weight of the discriminator’s loss λ; number of training steps T .\n\nfor t = 0, · · · , T − 1 do\n\n// Graph subsampling. Sample a subgraph G = (N , E) from Gfull; // Edge generation. Initialize the generated edge set Eg = {} and the edge generation loss Le Construct the unmasked set of edges Eu and the masked set Em such that E = Eu ∪ Em; Compute node embeddings using Eu; for e = (n1, n2) ∈ Em do\n\ng = 0;\n\nConstruct candidate set C for n1 (n2 is given during generation) via negative sampling; Generate (cid:98)e = ((cid:98)n1, n2) where (cid:98)n1 ∈ C; Update the generated edge set Eg ← Eg ∪ {(cid:98)e}; Update the edge generation loss Le g; // Text Feature generation. Initialize the feature generation loss Lf for n ∈ N do\n\ng = 0;\n\nFor the node’s text feature xn, mask out some of its tokens; Construct the generated text feature xcorr\n\nn\n\nusing the embedding of node n (computed\n\nduring edge generation) and the feature generation Transformer model; Update the feature generation loss Lf g ;\n\n// Edge discrimination. Initialize the edge discrimination loss Le Compute node embeddings using Eg ∪ Eu; Sample E d for e = (n1, n2) ∈ Eg ∪ E d\n\nu ⊂ Eu such that |E d u do\n\nu| = α|Eg|;\n\nd = 0;\n\nDetermine if e is generated using the embedding of n1 and n2; Update the edge discrimination loss Le d; // Text feature discrimination. Initialize the feature discrimination loss Lf for n ∈ N do\n\nd = 0;\n\nFor the node’s generated text feature xcorr\n\nn , determine whether each token is generated\n\nusing the embedding of node n (computed during edge discrimination) and the feature discrimination Transformer model; Update the feature discrimination loss Lf d ;\n\n// Model updates. g + Lf Compute L = (Le\n\ng ) + λ(Le\n\nd + Lf\n\nd ) and update the model;\n\nOutput: Trained discriminator ready for fine-tuning.\n\nB GENERATION AND DISCRIMINATION OF VECTOR FEATURES\n\nNode features can be vectors instead of texts, e.g., the feature vector can contain topological information such as connectivity information. In this case both the generator and the discriminator are parameterized by a linear layer.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTo generate feature vectors, we first randomly select some nodes Ng ⊂ N . For a node n ∈ N , denote its feature vector vn, then the feature generation loss is\n\nLf\n\ng (Wg) =\n\n(cid:88)\n\nn∈Ng\n\n||(cid:98)vn − vn||2\n\n2 , where (cid:98)vn = W f\n\ng hg(n).\n\nHere hg(n) is the representation of node n and W f construct its corred feature vcorr\n\nn = (cid:98)vn if n ∈ Ng and vcorr\n\ng is a trainable weight. For a node n ∈ N , we\n\nn = vn if n ∈ N \\ Ng.\n\nThe discriminator’s goal is to differentiate the generated features from the original ones. Specifically, the prediction probability is\n\np(n ∈ Ng) = sigmoid (cid:0)W d\n\nd hd(n)(cid:1) ,\n\nwhere W f on the corred feature vcorr\n\nd is a trainable weight. We remark that the node representation hd(n) is computed based\n\nn . Correspondingly, the discriminator’s loss is\n\nLf\n\nd (Wd) =\n\n(cid:88)\n\nn∈N\n\n−1{n ∈ Ng} log p(n ∈ Ng) − 1{n ∈ N \\ Ng} log(1 − p(n ∈ Ng)).\n\nThe vector feature loss Lf (θe the text feature loss.\n\ng, W f\n\ng , θe\n\nd, W f\n\nd ) = Lf\n\ng (θe\n\ng, W f\n\ng ) + Lf\n\nd (θe\n\nd, W f\n\nd ) is computed similar to\n\nC IMPLEMENTATION AND TRAINING DETAILS\n\nBy default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. In the experiments, the edge generator and discriminator have the same architecture, where we set the hidden dimension to 400, the number of layers to 3, and the number of attention heads to 8. For the OAG dataset which contains text features, the feature generator and discriminator employs the same architecture: a 4 layer bi-directional Transformer model, similar to BERT (Devlin et al., 2019), where we set the embedding dimension to 128 and the hidden dimension of the feed-forward neural network to 512.\n\nFor pre-training, we mask out 20% of the edges and 20% of the features (for text features we mask out 20% of the tokens). We use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set β = (0.9, 0.999), ε = 10−8, the learning rate to 0.001 and the weight decay to 0.01. We adopt a dropout ratio of 0.2 and gradient norm clipping of 0.5. For graph subsampling, we set the depth to 6 and width to 128, the same setting as Hu et al. 2020b.\n\nFor fine-tuning, we use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set β = (0.9, 0.999), ε = 10−6, and we do not use weight decay. We use the same graph subsampling setting as pre-training. The other hyper-parameters are detailed in Table 6.\n\nTable 6: Hyper-parameters for fine-tuning tasks.\n\nDataset\n\nTask\n\nSteps Dropout Learning rate Gradient clipping\n\nReddit\n\n— 2400\n\nRecomm. — 1600\n\nOAG-CS\n\nPF PV AD\n\n1600 1600 1600\n\n0.3\n\n0.1\n\n0.2 0.2 0.2\n\n0.0015\n\n0.0010\n\n0.0010 0.0005 0.0005\n\n0.5\n\n0.5\n\n0.5 0.5 0.5\n\n15",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a discriminative method for pre-training Graph Neural Networks. The main idea is to simultaneously train a generator to recover identities of the masked edges, and train a discriminator to distinguish the generated edges from the original graph’s edges.\n\n# Strength And Weaknesses\n\nStrength\n- The proposed method makes sense.\n- The empirical results show the improved performance brought by the proposed methods on a number of  benchmarks.\n\nWeaknesses\n- There is no mentioning of n_1^hat or generated edges e_g in the loss of the generator. How should I understand that the generated edges are not captured in the generator loss?\n\n- Also, it is said that the discriminator is to distinguish edges that are from the original graph and edges that are generated. It would make sense to contrast the masked edges vs the generated edges, especially the ones that are different. However, the training loss for the discriminator seems to still focus on the unmasked edges vs generated edges. I don’t quite understand the intuition.\n\n- A masked edge has two end nodes n_1 and n_2. How do the authors decide which end node to predict?\n\n- Why is the same lambda shared between edge loss and feature loss, given these two losses are so different?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is unclear in various aspects, as explained in above comments.\n\n# Summary Of The Review\n\nOverall, the proposed method makes sense. However, I feel the authors are selling the methodology wrong. Instead of saying that discriminative pre-training is better than generative training, the authors should present it as a method for applying generative and discriminative pre-training jointly. Otherwise, why don’t you just rely solely on discriminative pre-training?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a discriminative pre-training framework for Graph Neural Networks (GNNs) that addresses the limitations of traditional generative pre-training methods, which can lead to graph mismatch. DiP-GNN utilizes a generator to recover masked edges and a discriminator to distinguish between generated and original edges, thereby improving graph alignment. The methodology is tested across various datasets—Reddit for node classification, a bi-partite graph for link prediction, and OAG-CS for heterogeneous classification tasks—demonstrating significant performance gains over existing baselines, thereby validating the effectiveness of the proposed approach.\n\n# Strength And Weaknesses\nThe primary strength of DiP-GNN lies in its novel approach to address graph mismatch in pre-training, which is a recognized challenge in the field. The dual structure of generating and discriminating edges enhances the quality of node embeddings, leading to improved performance in downstream tasks. However, a potential weakness is the reliance on the discriminator post pre-training, which might introduce limitations in scenarios where the generator could have provided additional insights or features. Additionally, while the empirical results are compelling, the paper lacks an extensive discussion on the limitations of the proposed method and the scenarios where it might not perform as well.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem of graph mismatch and the proposed solution. The methodology is described in sufficient detail to allow for reproducibility, including implementation specifics and dataset descriptions. The novelty of the approach is apparent, particularly in its combination of edge generation and discrimination for pre-training GNNs. However, the paper could benefit from a more thorough discussion of the theoretical implications of the proposed method compared to existing approaches.\n\n# Summary Of The Review\nDiP-GNN presents a promising approach to improve GNN pre-training by effectively addressing graph mismatch through a dual generator-discriminator framework. While the results are impressive, further exploration of the method's limitations and theoretical implications would enhance the paper's contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework designed to enhance Graph Neural Networks (GNNs) through discriminative pre-training on large-scale unlabeled graphs, followed by fine-tuning on smaller labeled datasets. The proposed methodology combines edge and feature generation with a discriminator to overcome the limitations of traditional generative pre-training methods, particularly addressing issues related to graph mismatch. Extensive experiments on multiple datasets demonstrate that DiP-GNN significantly improves performance metrics, such as F1 scores and MRR, compared to existing methods, while exhibiting robustness to variations in hyperparameters.\n\n# Strengths and Weaknesses\nStrengths of the paper include the innovative framework that effectively addresses critical challenges in GNN pre-training, particularly the dual approach of edge generation and discrimination. The empirical validation across various datasets showcases the framework's effectiveness, highlighting significant improvements in performance. Additionally, the flexibility of the architecture allows for adaptation to different GNN backbones, enhancing its applicability. However, weaknesses include the increased computational complexity associated with the dual training process, which may limit practicality in resource-constrained environments. Furthermore, the performance is heavily reliant on the quality of the original graph, and the scalability of the framework in real-world applications with larger graphs remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the writing is high, with detailed explanations of the technical aspects of the DiP-GNN framework. The novelty is significant, as the combination of edge and feature discrimination presents a unique approach to GNN pre-training. Reproducibility is somewhat addressed through the description of the experimental setup and the use of standard datasets, but further details regarding hyperparameter settings and code availability would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing GNN performance through discriminative pre-training. While the empirical results are strong and demonstrate practical improvements, concerns regarding computational complexity and scalability warrant further exploration. The framework shows promise for future applications in diverse and larger datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for pre-training Graph Neural Networks (GNNs) aimed at improving performance in downstream tasks by addressing the issue of graph mismatch prevalent in existing generative pre-training methods. DiP-GNN employs a dual training process that includes a generator, which predicts masked edges, and a discriminator that distinguishes between original and generated edges. The effectiveness of this approach is demonstrated through extensive experiments on large-scale homogeneous and heterogeneous graphs, where DiP-GNN consistently outperforms baseline models in various tasks, such as node classification and link prediction.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to mitigating the graph mismatch problem, which is a critical challenge in GNN pre-training. The dual training mechanism is well-conceived, allowing for effective edge recovery and improved embeddings. Additionally, the paper provides a comprehensive experimental evaluation across multiple datasets, establishing the robustness and versatility of the DiP-GNN framework. However, a potential weakness is the lack of detailed exploration into the limitations of the proposed method, such as scenarios where the dual training might struggle or fail. Furthermore, while the empirical results are impressive, more detailed ablation studies could enhance the understanding of the impact of various components in the model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with a background in GNNs and deep learning. The methodology is presented in a logical manner, with appropriate mathematical formulations that facilitate understanding. The quality of the experiments is high, with the implementation provided in PyTorch, which aids reproducibility. However, the paper could benefit from more explicit descriptions of hyperparameter settings and potential challenges faced during the experiments, which would further support reproducibility.\n\n# Summary Of The Review\nDiP-GNN presents a significant advancement in GNN pre-training by effectively addressing graph mismatch through a dual training framework. The empirical results demonstrate its superior performance compared to existing methods, though the paper could enhance its impact with deeper exploration of limitations and challenges. Overall, the contributions are valuable and the methodology is robust.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces DiP-GNN, a novel framework that tackles the graph mismatch issue commonly found in generative pre-training methods for Graph Neural Networks (GNNs). The methodology leverages a discriminative pre-training approach, which allows for improved alignment between the discriminator's inputs and the original graph, leading to enhanced node embeddings. The authors conduct extensive experiments on large-scale homogeneous and heterogeneous graphs, demonstrating significant performance improvements over existing methods, while also contributing to the field of transfer learning in GNNs.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including its novel approach to addressing the graph mismatch issue, extensive empirical validation, and robustness to hyper-parameter variations. However, it also has notable limitations, such as the complexity of implementation, specificity of datasets used in experiments, and a lack of comprehensive hyper-parameter analysis. Additionally, while the theoretical foundation is strong, there is a potential risk of overfitting to theoretical constructs, which may limit the framework's applicability in diverse real-world scenarios. The open-source code availability enhances reproducibility but lacks clarity on its documentation and support.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The theoretical underpinnings are explained in detail, contributing to the overall quality of the manuscript. The novelty lies in the proposed discriminative pre-training approach, which distinguishes it from existing methods. The authors’ commitment to open-source code availability promotes reproducibility; however, the lack of detailed documentation may hinder ease of use for other researchers.\n\n# Summary Of The Review\nOverall, the paper offers a novel and theoretically sound approach to improving GNN performance through discriminative pre-training. While it demonstrates significant empirical results, the complexity of implementation and specificity of datasets may limit its practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel methodology for pre-training GNNs that departs from traditional generative tasks to adopt a discriminative training framework. The key contributions include a dual training process involving a generator and a discriminator, with the discriminator focusing on edge classification to enhance node representations. The findings indicate that this approach leads to superior performance in downstream tasks, particularly in scenarios with sparse labels, as evidenced by significant improvements in metrics such as F1 score and Mean Reciprocal Rank (MRR) across various datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to pre-training GNNs, which addresses the limitations of generative methods by focusing on edge classification instead of reconstruction. This discriminative framework has been shown to produce richer node embeddings and better robustness to missing information. The extensive experimental validation across different graph types further strengthens its claims. However, the paper could benefit from a more detailed discussion on the computational efficiency and scalability of the proposed method, as well as potential limitations in specific applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, making the methodology and contributions clear. The quality of the experiments is high, providing convincing evidence of the framework's effectiveness. The novelty is significant, as it presents a paradigm shift in GNN pre-training strategies. However, reproducibility could be improved by including more details on implementation specifics and hyperparameter settings, which would facilitate independent verification of the results.\n\n# Summary Of The Review\nDiP-GNN presents a compelling and innovative approach to pre-training Graph Neural Networks through a discriminative framework, demonstrating significant empirical improvements over traditional methods. The paper is well-structured and provides robust validation of its claims, although it could enhance reproducibility by offering more implementation details.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\" introduces a novel adversarial training framework specifically designed for graph neural networks (GNNs). The proposed DiP-GNN framework incorporates a generator-discriminator architecture, akin to that used in Generative Adversarial Networks (GANs), tailored to the unique characteristics of graph data. The key contributions include a dual-component training methodology aimed at enhancing GNN robustness against adversarial attacks, addressing the issue of graph mismatch caused by adversarial alterations, and providing extensive empirical evaluations that demonstrate significant improvements in model performance across various tasks and datasets.\n\n# Strength And Weaknesses\nOne of the primary strengths of the paper is its innovative approach to adversarial training in GNNs, filling a notable gap in current research. The introduction of a generator-discriminator mechanism provides a robust framework for understanding and mitigating adversarial attacks, which is especially relevant given the increasing prevalence of such attacks in machine learning applications. Furthermore, the comprehensive empirical evaluations showcase the superiority of DiP-GNN over existing techniques, reinforcing the practical applicability of the proposed method. However, weaknesses include the potential complexity of the model, which may hinder interpretability in real-world applications. Additionally, the paper could benefit from a deeper exploration of hyperparameter sensitivity to enhance understanding of model stability across diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with logical progression in presenting its methodology and findings. The quality of the experimental design is commendable, with sufficient detail provided for reproducibility. The novelty of the approach is significant, as it introduces a fresh perspective on adversarial training specifically tailored for graph structures, a topic that has not been extensively addressed in the literature. The potential for broader applications beyond graph data further elevates the significance of this work.\n\n# Summary Of The Review\nOverall, the DiP-GNN framework presents a substantial advancement in adversarial training for graph neural networks, offering innovative methodologies and robust empirical validation. While the model's complexity might pose challenges in interpretability, its contributions to enhancing model robustness are noteworthy, making it a valuable addition to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for pre-training Graph Neural Networks (GNNs) designed to address critical shortcomings in existing methods. The methodology employs a generator-discriminator model, wherein the generator reconstructs masked edges while the discriminator distinguishes between true and generated edges, leading to improved node embeddings. The experimental findings suggest that DiP-GNN significantly enhances GNN performance, achieving notable increases in F1 scores and Mean Reciprocal Rank (MRR) across various datasets, thereby positioning itself as a transformative approach in the field.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative dual-generator and discriminator architecture, which is framed as a groundbreaking advancement in GNN pre-training. The reported empirical results indicate substantial improvements over prior generative pre-training techniques, which could redefine performance benchmarks. However, the paper tends to overstate the implications of its findings, suggesting a paradigm shift without adequately addressing the contributions of existing methodologies. Additionally, the claim that hyperparameters have minimal impact lacks nuanced consideration of model performance tuning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly; however, some claims, particularly those regarding the significance of its findings and the implications for future research, may come off as exaggerated. While the novelty of the proposed method is evident, the reproducibility of the results is not thoroughly addressed, which may hinder other researchers from validating the findings independently. The paper could benefit from a more balanced discussion of its limitations and the context of existing work in the field.\n\n# Summary Of The Review\nDiP-GNN presents an innovative approach to GNN pre-training that shows promising empirical results. However, the paper's tendency to overstate its contributions and implications may undermine its credibility. A more balanced perspective on the existing literature and a clearer discussion of reproducibility would enhance its overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel framework designed to improve the performance of Graph Neural Networks (GNNs) through a dual training approach involving both a generator and a discriminator. This method addresses the graph mismatch issue prevalent in generative pre-training techniques, which often result in suboptimal node embeddings. The generator is responsible for recovering masked edges, while the discriminator distinguishes between original and generated edges. The findings indicate that DiP-GNN outperforms traditional generative pre-training methods, achieving notable improvements in F1 and MRR scores across various datasets, specifically Reddit and OAG-CS.\n\n# Strength And Weaknesses\nThe main strength of the paper is its innovative approach to overcoming the graph mismatch problem in generative pre-training by utilizing a discriminative framework. The improvements in performance metrics, such as the F1 score on the Reddit dataset and MRR on the OAG-CS dataset, provide compelling evidence of the framework's efficacy. However, the paper also has weaknesses, including the slight inaccuracies in reported performance gains and the impact of masked edges on generator performance, which could lead to questions about the robustness of the results under different conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents its contributions and findings. The methodology is described in sufficient detail, allowing for reproducibility, although some aspects, such as specific training parameters and data splits, could be elaborated upon for greater clarity. The novelty of the approach is evident, as it introduces a distinct method that combines generative and discriminative elements in GNN pre-training, setting it apart from previous works.\n\n# Summary Of The Review\nOverall, DiP-GNN presents a valuable advancement in the field of GNNs by addressing key limitations of generative pre-training methods. While the results are promising, the paper could benefit from more rigorous detail in certain areas to enhance clarity and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel framework that integrates generative and discriminative pre-training methods for Graph Neural Networks (GNNs). The authors argue that generative methods often lead to graph mismatch due to masked edges, which can negatively impact performance on downstream tasks. They propose a discriminative approach that aligns more closely with the original graph structure, thereby enhancing representation effectiveness. The findings demonstrate that DiP-GNN significantly outperforms existing methods across various datasets, although the generalizability of these results is not fully explored.\n\n# Strength And Weaknesses\nThe paper contributes to the understanding of pre-training methods in GNNs by critically examining the limitations of generative approaches, particularly the issue of graph mismatch. The emphasis on discriminative learning provides a fresh perspective, potentially leading to more robust embeddings. However, the paper's analysis lacks depth in several areas: it does not adequately address the conditions under which discriminative methods may fail, nor does it provide a comprehensive evaluation of the proposed framework against a wider array of baselines. Additionally, concerns about hyper-parameter sensitivity and overfitting are not sufficiently mitigated.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured and clearly articulates the motivation behind the DiP-GNN framework. However, the novelty of the approach could have been bolstered by a more thorough discussion of the implications of the generator's role and the interpretability of the learned embeddings. The reproducibility of the results is also uncertain, as the sensitivity of hyper-parameters is not explored in detail, which could impact the stability and performance of the model.\n\n# Summary Of The Review\nDiP-GNN presents a compelling argument for the advantages of discriminative pre-training in GNNs, addressing critical issues associated with generative methods. However, the paper falls short in its empirical evaluation and exploration of the broader implications of its findings, raising questions about the framework's generalizability and robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a discriminative pre-training method for graph neural networks (GNNs) designed to tackle graph mismatch issues prevalent in existing generative pre-training approaches. DiP-GNN utilizes a dual structure comprising a generator, which predicts masked edges and features, and a discriminator, which distinguishes between real and generated edges. The authors demonstrate through experiments on datasets such as Reddit and OAG-CS that DiP-GNN significantly outperforms current state-of-the-art methods in node classification and link prediction tasks, highlighting its effectiveness in enhancing node representation.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing the limitations of generative pre-training methods by introducing a discriminative perspective, which is a notable advancement in the field of GNNs. The dual training mechanism effectively mitigates graph mismatch issues, resulting in improved performance metrics. However, the paper could benefit from a more detailed discussion on the computational efficiency of the proposed method and its scalability to very large graphs, as this is crucial for real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The writing quality is high, with a logical flow of ideas that facilitates understanding. The novelty of the approach is significant, as it shifts the paradigm from generative to discriminative pre-training in GNNs. While the experiments are thorough, additional information about the reproducibility of the results, such as hyperparameter settings and code availability, would further enhance the paper's impact.\n\n# Summary Of The Review\nOverall, the paper introduces a valuable contribution to the field of graph neural networks by presenting DiP-GNN, which effectively addresses key issues with existing generative pre-training methods. Its innovative dual training approach yields promising results, though further discussion on computational efficiency and reproducibility would strengthen its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at improving the efficiency of deep learning models by integrating a new regularization technique with existing optimization methods. The authors argue that their approach can significantly reduce overfitting while maintaining or enhancing model performance on several benchmark datasets. The methodology involves a combination of theoretical analysis and empirical validation, demonstrating the effectiveness of the proposed method across various tasks, including image classification and natural language processing.\n\n# Strength And Weaknesses\n### Strengths\n1. **Innovative Approach**: The regularization technique introduced in the paper provides a fresh perspective that could lead to more robust models in the deep learning community.\n2. **Theoretical Rigor**: The paper lays a solid theoretical groundwork, offering insights into the mechanisms behind the proposed regularization method and its impact on model training.\n3. **Broad Evaluation**: The method is evaluated across multiple datasets and tasks, suggesting its versatility and applicability beyond a single domain.\n4. **Clarity and Structure**: The writing is clear and well-organized, making complex ideas accessible to a wider audience.\n5. **Reproducibility**: The authors provide detailed implementation information, which enhances the reproducibility of their results.\n\n### Weaknesses\n1. **Limited Scope of Experiments**: While the methodology is sound, the experimental validation may not encompass enough diversity in datasets to fully demonstrate its generalizability.\n2. **Comparative Analysis**: The paper could benefit from a more comprehensive comparative analysis with state-of-the-art methods, which is crucial to substantiate claims of improvement.\n3. **Potential Overfitting**: Some results may indicate that the proposed regularization method could lead to overfitting in specific scenarios, which should be further explored.\n4. **Scalability Issues**: There are concerns regarding the scalability of the approach when applied to larger models or datasets, which could limit its practical applicability.\n5. **Ethical Considerations**: The authors do not sufficiently address the ethical implications of their approach, particularly regarding model biases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents a clear narrative from motivation to conclusions. The novelty of the proposed method is significant, as it introduces a new regularization technique that has not been widely explored. The quality of the analysis is high, supported by a solid theoretical framework. The reproducibility of results is facilitated through detailed descriptions of experimental setups and methodologies.\n\n# Summary Of The Review\nThe paper introduces a promising new regularization technique that could advance the field of deep learning by enhancing model robustness. While it shows significant novelty and clarity, the lack of extensive experimental validation and comparative analysis limits its impact. Addressing these weaknesses would strengthen the overall contribution of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel framework aimed at enhancing the performance of GNNs through a discriminative pre-training approach. The authors identify the limitations of traditional generative pre-training methods, particularly the issue of graph mismatch, which arises when edges are masked to train the GNN. DiP-GNN employs a dual strategy involving a generator to predict masked edges and a discriminator to differentiate between original and generated edges, leading to improved learning of node embeddings. Extensive empirical validation indicates that DiP-GNN significantly outperforms existing methods in both homogeneous and heterogeneous graph scenarios, demonstrating notable improvements in metrics such as F1 score and MRR score.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the graph mismatch problem inherent in generative pre-training methods. By leveraging a discriminative framework, the authors provide a compelling solution that enhances the alignment of learned node embeddings with the original graph structure. The extensive empirical validation further supports the claims made regarding the method's effectiveness. However, a potential weakness is the lack of detailed analysis on the computational efficiency of the proposed method compared to traditional approaches, which could impact its practical adoption.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the proposed DiP-GNN framework. The methodology is presented in a straightforward manner, making it accessible to readers who may not be familiar with the intricacies of GNNs. The quality of the experiments seems robust, with appropriate benchmarks used for validation. The novelty of the approach is significant, as it challenges the prevailing reliance on generative methods in GNN pre-training. Regarding reproducibility, the authors provide sufficient detail on their experimental setup, although including code or additional supplementary materials could enhance this aspect.\n\n# Summary Of The Review\nOverall, DiP-GNN presents a significant advancement in the pre-training of graph neural networks by introducing a discriminative approach that effectively mitigates graph mismatch issues. The empirical results validate its superiority over existing methods, although a deeper exploration of computational efficiency would further strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework designed for the pre-training of Graph Neural Networks (GNNs) on large unlabeled graphs. It specifically addresses the challenge of graph mismatch encountered in generative methods by employing a dual approach involving a generator and a discriminator. The methodology includes the generation of masked edges and predictions to recover the graph structure, leveraging adversarial training principles. Experimental results demonstrate that DiP-GNN significantly outperforms existing baseline methods across multiple datasets, confirming its efficacy in improving GNN performance in downstream tasks.\n\n# Strength And Weaknesses\nThe principal strength of DiP-GNN lies in its innovative approach to addressing graph mismatch through a dual generator-discriminator framework, which could lead to better alignment of pre-trained models with real-world data. The thorough empirical evaluation against multiple benchmarks adds to its robustness, showing consistent performance improvements. However, a potential weakness is the reliance on hyper-parameter tuning, which may limit the framework's usability in practice if optimal settings are not easily identifiable. Additionally, the paper could benefit from a more detailed discussion on the implications of the edge recovery process and its impact on model interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. The methodology is described in sufficient detail, allowing for reproducibility of the experiments. However, some sections could be enhanced with additional clarity regarding the implementation specifics of the generator and discriminator. The novelty of the approach is noteworthy, as it combines concepts from adversarial learning with GNNs in a manner that has not been extensively explored in prior work. The empirical results are compelling and bolster the claims made about the framework's effectiveness.\n\n# Summary Of The Review\nDiP-GNN presents a significant advancement in the pre-training of GNNs by successfully addressing graph mismatch through an innovative generator-discriminator framework. While the methodology and results are strong, the paper could improve clarity in certain sections and provide more insights into practical applications. Overall, it is a valuable contribution to the field of graph-based learning.\n\n# Correctness\n4/5 - The methodology and results appear sound, though some aspects of hyper-parameter tuning could be clarified.\n\n# Technical Novelty And Significance\n5/5 - The dual approach of pre-training GNNs using a generator and discriminator is a novel contribution that could pave the way for future research in the field.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are robust and demonstrate significant improvements over baseline methods; however, further exploration of practical implications could strengthen this aspect.",
    "# Summary Of The Paper\nThe paper titled \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\" addresses the challenge of graph mismatch in generative pre-training for Graph Neural Networks (GNNs). The authors propose a novel approach, DiP-GNN, which employs a discriminative pre-training methodology that incorporates both generator and discriminator components for edge and feature generation. Through rigorous experiments on relevant datasets such as Reddit and OAG-CS, the paper demonstrates significant improvements over several baseline methods in terms of performance metrics like F1 score and Mean Reciprocal Rank (MRR).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear identification of a pertinent problem in the GNN landscape and the innovative solution proposed through the DiP-GNN framework. The methodology is well-articulated, and the experiments are comprehensive, providing a robust evaluation of the proposed method against multiple baselines. However, a potential weakness is the lack of extensive discussion on the scalability of the proposed method and its applicability to larger, more complex datasets. Additionally, while the ablation studies are insightful, more detailed analysis on the impact of specific parameters could enhance the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and exhibits a high level of clarity throughout, making complex concepts accessible. The quality of the methodology is commendable, with a clear algorithmic framework presented. The novelty of the DiP-GNN approach is significant, as it addresses specific gaps in existing GNN pre-training methods. Reproducibility appears feasible, given that the authors have provided sufficient detail on their experimental setup and evaluation metrics.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to improving GNN pre-training through discriminative methods. While it makes substantial contributions to the field, there is room for improvement in discussing scalability and parameter impact. The research is well-executed and has the potential for significant impact in the domain of graph neural networks.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel approach to enhancing Graph Neural Networks (GNNs) through a discriminative pre-training methodology. This method addresses the issue of graph mismatch encountered in previous generative pre-training techniques by employing a dual-architecture framework consisting of a generator and a discriminator. The generator focuses on reconstructing masked edges while the discriminator evaluates the authenticity of these edges, thereby enabling fine-tuning for downstream tasks. Empirical evaluations across various homogeneous and heterogeneous graph datasets demonstrate that DiP-GNN significantly outperforms existing generative paradigms, showcasing its utility in node classification and recommendation tasks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative dual-architecture framework that effectively tackles the shortcomings of prior generative methods, particularly the graph mismatch problem. The proposed methodology is well-structured, and the experimental results provide compelling evidence of its performance advantages over existing techniques. However, the paper could benefit from a more comprehensive discussion on the limitations of the current approach and potential avenues for future work. Additionally, while the empirical results are promising, further validation in more diverse graph settings would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear presentation of concepts and methodologies. The technical details are adequately described, allowing for a good understanding of the DiP-GNN framework. The novelty of the approach is significant, particularly in its application of GAN-like principles to the realm of GNN pre-training. Reproducibility is facilitated by the detailed explanation of the methodologies and the implementation specifics provided, although the availability of code or data would enhance this aspect further.\n\n# Summary Of The Review\nOverall, DiP-GNN represents a noteworthy advancement in the field of GNNs, providing a robust framework for discriminative pre-training that addresses key limitations of prior methods. The empirical results indicate significant performance improvements, which position the proposed approach as a promising avenue for future research in graph-based learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel framework termed DiP-GNN, which aims to improve node embeddings in graph neural networks (GNNs) by leveraging a generative adversarial approach. The methodology involves the use of a generator to recover masked edges in the graph, which is then coupled with a discriminator to enhance the alignment with the original graph structure. The authors report improvements in specific performance metrics, such as the F1 score, claiming that the method addresses challenges related to edge recovery and enhances the quality of node embeddings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to integrate generative techniques into the realm of GNNs, which is a promising area of exploration. However, the contributions appear to be limited by a lack of novelty, as the work heavily borrows from existing concepts in GANs and generative pre-training without significant innovation. Additionally, the paper does not adequately address the unique challenges posed by graph data, such as sparsity and irregularity, and presents an overly optimistic view of the benefits of edge recovery. The reliance on a generator introduces unnecessary complexity, and the marginal performance improvements reported do not convincingly justify this added complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is adequate, but the paper lacks a robust analysis of its claims and methodologies. The novelty of the proposed approach is questionable, as it feels derivative rather than groundbreaking. Furthermore, the reproducibility of the results is undermined by insufficient exploration of hyper-parameter sensitivity and a lack of comprehensive experimental comparison with state-of-the-art methods. The limited scope of experiments raises concerns about the generalizability of the findings.\n\n# Summary Of The Review\nOverall, the paper presents an interesting but incremental contribution to the field of GNNs. While it attempts to integrate generative approaches, it fails to adequately address critical issues and challenges within the domain, resulting in a framework that may not significantly advance the state of the art.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for the discriminative pre-training of Graph Neural Networks (GNNs). The methodology combines a dual training mechanism involving a generator and a discriminator to enhance the recovery of masked edges and the differentiation of original edges. The findings demonstrate that DiP-GNN significantly improves node embeddings, leading to notable performance advancements, including a 1.1-point increase in F1 score on the homogeneous Reddit dataset and a 2.8-point increase in MRR score on the heterogeneous OAG-CS graph.\n\n# Strength And Weaknesses\nThe primary strengths of DiP-GNN lie in its innovative approach to addressing graph mismatch issues common in generative pre-training methods, as well as its ability to extract richer node embeddings through discriminative learning. The experimental results showcase its versatility and robustness across various graph types, which is a significant contribution to the field. However, the paper could benefit from a more detailed discussion of the limitations of the proposed approach, particularly regarding its scalability and applicability to larger graphs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the contributions and methodologies employed. The quality of the experiments and results is commendable, showcasing thorough evaluations against existing methods. The novelty of the discriminative pre-training approach is significant, setting it apart from traditional methods. Furthermore, the authors' commitment to open science by making the code publicly available enhances reproducibility, supporting future research efforts in the area.\n\n# Summary Of The Review\nDiP-GNN represents a significant advancement in the field of Graph Neural Networks, offering a novel discriminative pre-training approach that effectively enhances node embeddings and overall model performance. While the paper is well-structured and the contributions are promising, further exploration of the method's limitations would strengthen the overall presentation.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel framework known as DiP-GNN (Discriminative Pre-Training of Graph Neural Networks), which emphasizes a discriminative pre-training approach as opposed to the conventional generative methods. The methodology involves a generator that predicts missing edges and a discriminator that evaluates edge authenticity, allowing for a more aligned representation of the graph structure. The findings suggest that this framework leads to improved performance in downstream tasks by capturing richer node embeddings through a holistic understanding of graph topology and features.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear theoretical contribution to the understanding of GNN pre-training, particularly in addressing the limitations of generative methods through discriminative learning. The innovative bifurcation of roles between the generator and discriminator is a notable advancement, potentially enhancing the efficiency of the learning process. However, a weakness lies in the empirical validation, which, while showing improvements, could benefit from more extensive experiments across diverse datasets to reinforce the theoretical claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its concepts clearly, making the theoretical implications accessible. The quality of writing is high, with a logical flow that guides the reader through the complex ideas presented. The novelty of the approach is significant, as it shifts the paradigm of GNN pre-training from generative to discriminative methodologies. However, the reproducibility of the experiments could be strengthened by providing more detailed descriptions of the datasets and experimental setups used.\n\n# Summary Of The Review\nOverall, the DiP-GNN framework presents a compelling theoretical advancement in the field of graph neural networks, effectively addressing challenges associated with generative pre-training. While the empirical results are promising, further validation across a broader range of applications would enhance the robustness of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces DiP-GNN, a novel graph neural network framework designed to enhance graph representation learning through the integration of edge and feature generation mechanisms. The methodology involves a comprehensive training pipeline that includes graph subsampling, edge generation, and feature generation, leveraging a bi-directional Transformer model for feature extraction. The findings demonstrate that DiP-GNN outperforms baseline models across various graph datasets, showcasing significant improvements in tasks related to node classification and link prediction.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to combining edge and feature generation, which addresses significant challenges in graph representation learning. The use of a robust training pipeline and the detailed exploration of model variants contribute to the paper's thoroughness and practical implications. However, the paper could benefit from clearer explanations of certain technical aspects, particularly regarding the hyper-parameters and their impacts on model performance. Additionally, while the empirical results are promising, further comparisons with state-of-the-art models would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, though some sections, particularly the discussions on hyper-parameters and model variants, require more elaboration to enhance understanding. The quality of the methodology is high, with a well-structured approach to training and evaluation. The novelty is evident in the integration of various components for graph learning, although the potential overlap with existing methods could be more explicitly addressed. The reproducibility is facilitated by the promise of public code availability and the detailed training steps outlined, although clearer descriptions of experimental setups would further aid in this regard.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in graph representation learning with the introduction of DiP-GNN. Its innovative methodology and promising empirical results mark it as a valuable contribution to the field, despite minor issues in clarity and the need for more extensive comparisons with existing models.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a discriminative pre-training method for graph neural networks, which the authors claim addresses the shortcomings of traditional generative pre-training approaches. The methodology emphasizes the importance of graph structure over node features and proposes a training pipeline that includes a feedback mechanism between the generator and discriminator. The findings suggest that DiP-GNN outperforms several baseline methods, including GPT-GNN, in terms of F1 and MRR scores across multiple tasks.\n\n# Strength And Weaknesses\nWhile the paper presents a novel approach to graph representation learning, it exhibits several weaknesses. The critique of generative pre-training methods lacks nuance, as many such methods have demonstrated strong performances in prior work. The authors' claims regarding the significance of graph structure and the impact of missing versus wrong edges appear somewhat biased and do not engage with existing literature that provides alternative perspectives. Additionally, the paper does not offer a comprehensive comparison with established methods, which raises concerns about the validity of its performance claims. The focus on the robustness of hyper-parameters is noted, yet this is a common characteristic in many models and not a unique advantage. Furthermore, the lack of detailed analysis regarding the training pipeline's efficiency compared to simpler models limits the practical implications of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, articulating its contributions clearly, although it may overstate the novelty of some claims regarding graph mismatch and the importance of edge analysis. The methodology is presented in a structured manner, but the paper falls short in providing sufficient empirical validation of its assertions, particularly regarding the training and evaluation setups. While the authors mention that code will be available, the absence of pre-trained models or comprehensive documentation could hinder reproducibility and practical adoption of DiP-GNN.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to graph neural networks with DiP-GNN, but it lacks a thorough engagement with existing literature and a robust evaluation of its claims. The contributions, while potentially valuable, are overshadowed by insufficient comparative analysis and a somewhat biased presentation of the proposed methodology.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel framework for graph neural networks that leverages self-supervised learning to recover missing edges in graphs. The methodology involves masking a certain proportion of edges and training the model to predict these missing connections using a generator and discriminator architecture. The findings demonstrate that DiP-GNN can recover nearly 40% of the missing edges, highlighting its effectiveness in utilizing unlabeled data to improve graph learning tasks.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to addressing the issue of missing edges in graph data through self-supervised learning, which is a significant contribution to the field of graph neural networks. The empirical results show promising performance, suggesting that the model can effectively leverage unlabeled data. However, the paper has several weaknesses, including inconsistent terminology, unclear phrasing, and a lack of clarity in some definitions, such as \"graph mismatch.\" These issues may hinder the reader's comprehension and the overall impact of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel ideas, the clarity of presentation is compromised by inconsistent terminology and phrasing. For example, the use of \"discriminator\" varies throughout the text, and the explanations of key concepts like \"fine-tuning\" and \"graph mismatch\" are not sufficiently clear. The quality of the figures and tables is generally good, but some references lack uniformity. The reproducibility of the results could be improved by providing clearer definitions and consistently formatted equations. \n\n# Summary Of The Review\nOverall, the paper introduces a compelling framework for leveraging self-supervised learning in graph neural networks, but it suffers from clarity and consistency issues that detract from its contributions. Addressing these weaknesses would enhance the readability and impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework called DiP-GNN aimed at improving the discriminative pre-training of Graph Neural Networks (GNNs). The authors demonstrate the effectiveness of their method on various tasks involving both homogeneous and heterogeneous graphs. However, the paper primarily focuses on static graph structures and lacks exploration of dynamic graphs or non-Euclidean structures. The findings indicate that DiP-GNN achieves better performance through enhanced graph alignment but do not sufficiently discuss interpretability or robustness across different graph scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its proposed framework, DiP-GNN, which shows promising results on specific tasks, suggesting potential advancements in GNN pre-training. However, significant weaknesses include the limited scope of evaluation, as the method has not been tested on dynamic graphs or complex graph structures like directed graphs with varying edge weights. Additionally, the paper lacks a comprehensive discussion on how DiP-GNN may be integrated into multi-task learning or the implications of improved graph alignment on the interpretability of embeddings. The limited dataset evaluation raises questions about the generalizability of the results, and the absence of hyper-parameter sensitivity analysis and scalability considerations are notable omissions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, as the authors present their methodology and results in an organized manner. However, the quality could be improved with a deeper exploration of limitations and a more thorough comparison with emerging techniques in GNN pre-training. The novelty of DiP-GNN lies in its approach to discriminative pre-training, but the lack of exploration in various graph scenarios limits its overall significance. Reproducibility of the results may be hindered due to insufficient details on hyper-parameter choices and dataset variations.\n\n# Summary Of The Review\nOverall, while the paper introduces a potentially valuable framework for GNN pre-training, it falls short in exploring critical aspects such as dynamic and complex graph structures, multi-task integration, and interpretability. The limited dataset evaluation and lack of comprehensive analysis may impact the generalizability and reproducibility of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel approach aimed at improving the discriminative pre-training of Graph Neural Networks (GNNs) while addressing the issue of graph mismatch. The methodology involves a generator for masked edge prediction and a discriminator for assessing edge authenticity, utilizing rigorous statistical measures throughout the training and evaluation processes. The empirical findings demonstrate significant performance improvements in various tasks, including node classification, compared to existing baseline methods, validated through extensive statistical testing.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear and systematic approach to addressing the limitations of existing GNN pre-training methods through a discriminative framework. The rigorous statistical validation of model performance, including significance testing and the use of various datasets, enhances the credibility of the findings. However, a potential weakness lies in the complexity of the methodology, which may pose challenges for readers lacking a strong statistical background. Additionally, while the paper emphasizes statistical significance, it could benefit from a deeper discussion on the practical implications of these findings in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, allowing readers to follow the methodology and results without confusion. The quality of the writing is high, with a coherent flow of ideas and careful attention to detail in statistical analysis. The novelty of the DiP-GNN approach is significant, as it introduces a discriminative pre-training framework that outperforms generative methods. The availability of code enhances reproducibility, demonstrating transparency in the research process, which is crucial for further validation and application of the proposed method.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of GNNs by introducing DiP-GNN, which offers significant improvements over existing methods through a robust statistical approach. While the methodology is complex, its clarity and the comprehensive validation of results underscore the paper's impact and relevance in advancing GNN pre-training techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the DiP-GNN framework, which aims to enhance graph neural networks (GNNs) through a dual generator-discriminator setup. The authors focus on addressing graph mismatch issues by proposing a novel pre-training method tailored for both homogeneous and heterogeneous graphs. Experimental results indicate that DiP-GNN outperforms existing methods in tasks such as node classification and link prediction on selected datasets, showcasing its potential advantages in various graph-related applications.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to mitigating graph mismatch through the dual generator-discriminator architecture, which contributes to the existing body of knowledge on GNNs. However, the paper has notable weaknesses, including a lack of scalability analysis for larger graphs, limited exploration of graph types beyond homogeneous and heterogeneous, and insufficient robustness testing under various conditions. Additionally, there is a lack of comparative analysis with other state-of-the-art methods, which hinders the contextual understanding of DiP-GNN's performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. The quality of the data and experiments appears robust, although the limited scope of experiments and the absence of comprehensive comparisons reduce the reproducibility and contextual relevance of the findings. While the framework introduces novel concepts, the lack of exploration into various hyper-parameter settings and alternative strategies for mitigating graph mismatch may impede future research and application.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for enhancing GNNs, but it is limited by its narrow focus on specific graph types and a lack of thorough experimental validation. The contributions are noteworthy, yet the potential applicability in real-world scenarios is constrained by the identified weaknesses.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\" proposes a novel framework for enhancing Graph Neural Networks (GNNs) through a dual approach involving a generator and discriminator. The methodology focuses on addressing graph mismatch issues by implementing edge and feature generation and discrimination, utilizing techniques such as masking. The authors claim that their approach significantly outperforms existing models on large datasets, demonstrating improved performance through a series of experiments.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to apply a generative adversarial network-like framework to GNNs, which may offer a new perspective on pre-training in this domain. However, the weaknesses are notable: the contributions appear to be incremental rather than groundbreaking, and the insights presented are somewhat obvious to those familiar with existing literature. The methodology lacks novelty, as the concepts of edge and feature masking have been explored in other contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat lacking, as the authors often reiterate familiar concepts without adding substantial new insights. The quality of the experiments is adequate but relies heavily on standard hyper-parameters and training settings, which may limit reproducibility. The novelty of the proposed approach is questionable, as it closely resembles established methods in generative modeling, leading to concerns about the overall significance of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a familiar framework that lacks significant innovation in its approach to pre-training GNNs. While it may provide some useful empirical results, the contributions are overshadowed by the lack of originality and the presentation of widely known concepts.\n\n# Correctness\n4/5 - The methodology seems sound, and the results are presented clearly, but the novelty and significance of the findings raise some concerns.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions do not introduce substantial new ideas to the field and largely replicate existing methodologies.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results may show improvements, the significance of these gains is diminished by their reliance on established practices in the field.",
    "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for pre-training Graph Neural Networks (GNNs) that emphasizes the dual roles of edge and feature discrimination. The methodology leverages discriminative pre-training to enhance node representations while addressing graph mismatch issues in generative pre-training. Experimental results demonstrate that DiP-GNN outperforms traditional methods, indicating its potential effectiveness across various GNN architectures. Notably, the findings reveal that missing edges adversely impact performance more than incorrect edges, opening avenues for further research in edge completion techniques.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating edge and feature discrimination within the pre-training process, which shows promise in improving node representation quality. Furthermore, the framework's flexibility with different GNN architectures adds to its versatility. However, the paper could benefit from a more generalized framework that incorporates multi-modal feature integration and an analysis of graph sampling methods during training. The lack of an in-depth ablation study on the contributions of individual components and the absence of comparative analysis with methods for heterogeneous graphs are notable weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and findings. The methodology is presented with sufficient detail to allow for reproducibility, although further elaboration on hyper-parameter tuning and its influence on performance would enhance clarity. While the novelty of the proposed DiP-GNN framework is evident, there is potential for greater originality through the integration of additional techniques such as contrastive learning and reinforcement learning, which are mentioned but not fully explored.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to pre-training GNNs through DiP-GNN, demonstrating significant improvements over existing methods. While it offers valuable insights and flexibility, further exploration of multi-modal integration, hyper-parameter sensitivity, and additional comparative analyses would strengthen its contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel graph neural network framework that combines discriminative pre-training with graph representation learning. It demonstrates significant advancements in performance metrics across multiple datasets, including Reddit, a product recommendation graph, and the heterogeneous OAG-CS dataset. Key findings indicate that DiP-GNN achieves superior results in node classification and link prediction tasks, consistently outperforming existing methods such as GAE and GPT-GNN, particularly in scenarios with limited labeled data.\n\n# Strength And Weaknesses\nStrengths of the paper include the comprehensive evaluation of DiP-GNN across diverse datasets, showcasing statistically significant performance improvements, particularly in challenging tasks like paper-field classification and author name disambiguation. The framework’s robustness is further highlighted through its stable performance under varying edge masking conditions, which is a common challenge for generative models. A potential weakness is the lack of a detailed comparison of computational efficiency and scalability, which could inform practical applications of DiP-GNN in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of the experiments is high, with robust statistical analyses supporting the findings. The novelty of the approach is significant, as it introduces a discriminative pre-training strategy that outperforms established generative methods. However, details regarding the reproducibility of the experimental setup could be enhanced, particularly in terms of hyperparameter settings and data preprocessing steps.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to graph representation learning through the DiP-GNN framework, yielding notable performance gains across various tasks and datasets. The findings are robust and statistically validated, although additional details on reproducibility and computational efficiency would strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving the performance of Generative Adversarial Networks (GANs) by introducing a new training methodology that emphasizes stability and convergence. The authors propose a framework that integrates adaptive learning rates and a modified loss function aimed at reducing mode collapse. Extensive experiments are conducted on benchmark datasets to demonstrate the effectiveness of the proposed method, showing significant improvements in both qualitative and quantitative metrics compared to existing GAN architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing a common issue in GAN training—mode collapse—through adaptive learning and loss modification. The experimental results are compelling, showcasing improvements in generation quality and stability. However, the paper suffers from clarity issues, stemming from overly complex language and inconsistent structure. Moreover, the reliance on technical jargon without adequate explanation may hinder comprehension for readers unfamiliar with the topic.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel ideas and contributes to the field, the clarity of presentation is lacking. The abstract could be more straightforward, and the use of jargon without definitions makes the content less accessible. The methodology, although described, could benefit from clearer elaboration to facilitate reproducibility. The figures and tables presented lack sufficient explanation, which diminishes their effectiveness in conveying the results.\n\n# Summary Of The Review\nOverall, this paper introduces a promising new approach to enhancing GAN training, marked by notable empirical results. However, significant improvements in clarity and structure are needed to ensure that the contributions are effectively communicated and understood by a broader audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3538974084740207,
    -1.664905515271008,
    -1.6864244572595821,
    -1.5621547095563866,
    -1.7762699808060938,
    -1.6510223242127824,
    -1.662142632823144,
    -1.895108433274139,
    -1.6844223816781883,
    -1.7209133128386762,
    -1.4234546000917165,
    -1.4205346256310536,
    -1.5092962231037375,
    -1.6416505705893123,
    -1.5752050242761164,
    -1.5957239042814293,
    -1.821719930055573,
    -1.819837294922566,
    -1.6779166151371145,
    -1.6539355190231597,
    -2.041448791220579,
    -1.5934210524509413,
    -1.6974112261668752,
    -1.761686420486198,
    -1.7740956003631183,
    -1.8377965440252741,
    -1.8660618255499373,
    -1.7137835906941556,
    -1.6384976442537462
  ],
  "logp_cond": [
    [
      0.0,
      -2.0248065888364266,
      -2.086772099945123,
      -2.0520160155467546,
      -2.10060578649407,
      -2.086312793081465,
      -2.1066917848015416,
      -2.0727949039791693,
      -2.0329178097920844,
      -2.067227431066569,
      -2.0426605374913143,
      -2.1545735856169395,
      -2.052508044259459,
      -2.06015747588138,
      -2.0635991408351106,
      -2.075387301838425,
      -2.051811538982748,
      -2.0514382801469715,
      -2.0752836108364154,
      -2.1011660338074143,
      -2.073459180126025,
      -2.0850755195357107,
      -2.1135284144987008,
      -2.051767656603179,
      -2.129189665349441,
      -2.0483905323342237,
      -2.0692733244711903,
      -2.0963016524613765,
      -2.154099850087111
    ],
    [
      -1.2722492234123595,
      0.0,
      -1.2341291390915325,
      -1.144756794434725,
      -1.2468280202571882,
      -1.2784095185342506,
      -1.287615685653026,
      -1.2182631063409755,
      -1.1429465263559897,
      -1.1910093636439028,
      -1.1788242639682696,
      -1.3911476750805083,
      -1.1900984095035796,
      -1.2659852252190884,
      -1.1963386952960777,
      -1.214891441703079,
      -1.2493957311877368,
      -1.1550740744405439,
      -1.26565209577641,
      -1.2969298069378448,
      -1.2530540805954025,
      -1.3100397952172944,
      -1.283914458019033,
      -1.2303383456062271,
      -1.2726580863648738,
      -1.2804781962506115,
      -1.2516125465588706,
      -1.2501713292982586,
      -1.3833109928289897
    ],
    [
      -1.344510704125459,
      -1.2757002633522292,
      0.0,
      -1.306876410456771,
      -1.280366091511529,
      -1.3101042324574803,
      -1.3249575750583682,
      -1.2830051678695176,
      -1.2278651215446352,
      -1.2884108349235723,
      -1.292995605115542,
      -1.4398981509161806,
      -1.2846928231640455,
      -1.3116595138545182,
      -1.2969495087970573,
      -1.2785031988517943,
      -1.353837684146555,
      -1.3075191378091233,
      -1.3223334716030284,
      -1.3733569829675227,
      -1.2920787890012497,
      -1.388116044617268,
      -1.3571362175496129,
      -1.3046104841896868,
      -1.3460743714641847,
      -1.3148624379856888,
      -1.3205389161970291,
      -1.3560219651239067,
      -1.4798268957259784
    ],
    [
      -1.2036479515425038,
      -1.0977105992764902,
      -1.1797304754323463,
      0.0,
      -1.1361330907113405,
      -1.1678613871920407,
      -1.1838494463258886,
      -1.1365073389934754,
      -1.0438224414928634,
      -1.1307998078132389,
      -1.1094864373583462,
      -1.2988763260262275,
      -1.1011990361418387,
      -1.1093458884335272,
      -1.1478850883231169,
      -1.110777529703534,
      -1.1930998863833238,
      -1.0862404760401219,
      -1.2085260618122202,
      -1.2309502312298188,
      -1.1463458826032409,
      -1.1976000383248595,
      -1.2266470120764483,
      -1.1530701342694933,
      -1.1685382354262068,
      -1.188542325626328,
      -1.1934957839786122,
      -1.2068432950186812,
      -1.2923239359940775
    ],
    [
      -1.3997005800592313,
      -1.3199908106992384,
      -1.3061250881478974,
      -1.3284213999978276,
      0.0,
      -1.3897587083393066,
      -1.3705426219640169,
      -1.3400461666564374,
      -1.3087779439388618,
      -1.3245328359701012,
      -1.3614583754792988,
      -1.4261310209465565,
      -1.28961254963359,
      -1.370918183789849,
      -1.3384519035472873,
      -1.3431671549153816,
      -1.3388696780362792,
      -1.2849706131241065,
      -1.3528450045944673,
      -1.3874489780398789,
      -1.2908876673264778,
      -1.4070528339359718,
      -1.3036490800725393,
      -1.3393741060056823,
      -1.335090096964618,
      -1.3975672732629294,
      -1.3586965950195116,
      -1.3809300292571565,
      -1.4662051625107089
    ],
    [
      -1.2619746882355631,
      -1.1915382635492715,
      -1.1606735731171705,
      -1.1310246613321135,
      -1.1909460408723045,
      0.0,
      -1.2359938521385532,
      -1.1020698055507254,
      -1.1022894537786645,
      -1.1676060239001664,
      -1.1999945029297174,
      -1.358389664895238,
      -1.0991587498868676,
      -1.2206814627003224,
      -1.2023553707726549,
      -1.1967471066520627,
      -1.1822459533097978,
      -1.0914543198518134,
      -1.179895257437842,
      -1.2424723237230229,
      -1.1672060719759292,
      -1.25930065028826,
      -1.2469697806801312,
      -1.1617832105134913,
      -1.2585419784491079,
      -1.2804777654459663,
      -1.2908834595517087,
      -1.2048123369702226,
      -1.3457671800870559
    ],
    [
      -1.3575128810716124,
      -1.3049860731591503,
      -1.2949900857230474,
      -1.2621177816293385,
      -1.2862644547731685,
      -1.2925262752915188,
      0.0,
      -1.3186410526943106,
      -1.2842660319231178,
      -1.2788205428784596,
      -1.2989969479772967,
      -1.3973192528801543,
      -1.2884994892833532,
      -1.2813046254899378,
      -1.2614254615100013,
      -1.2884131813190784,
      -1.2901324525952418,
      -1.296151867889684,
      -1.2763664869223752,
      -1.277078552361709,
      -1.2628381808900282,
      -1.3055978168098352,
      -1.2890502417764893,
      -1.2820339671782224,
      -1.2624986509722305,
      -1.281190017784554,
      -1.2923296843300192,
      -1.3045427008707793,
      -1.4125103712494458
    ],
    [
      -1.512283662488204,
      -1.4747905880502525,
      -1.478141427971166,
      -1.4805832313689202,
      -1.517103157480752,
      -1.4424559095561498,
      -1.5699518603852116,
      0.0,
      -1.3375090947029937,
      -1.5129926684316233,
      -1.4892979996213505,
      -1.636772714680699,
      -1.4581809139426336,
      -1.4736555811107075,
      -1.5355006819113148,
      -1.487370281872816,
      -1.4946922475478708,
      -1.4170563927328308,
      -1.5277442872735656,
      -1.5529562290418413,
      -1.4304515099470547,
      -1.4759669251743885,
      -1.562594586183211,
      -1.530449124149123,
      -1.562021761447554,
      -1.4839869602236553,
      -1.4810819547044027,
      -1.5347925835052703,
      -1.5582415186783893
    ],
    [
      -1.2736199550203933,
      -1.1476460204450494,
      -1.241557066704898,
      -1.2106857833547646,
      -1.2515820112854243,
      -1.2678533290591871,
      -1.3160717400006579,
      -1.1569920391840876,
      0.0,
      -1.2253472354474633,
      -1.209165679181588,
      -1.455054741521754,
      -1.1861081542350371,
      -1.207175703927996,
      -1.2304829575079574,
      -1.231677056952961,
      -1.236890132558033,
      -1.1161391519920802,
      -1.3027324707794639,
      -1.3556546864530035,
      -1.2552643919040738,
      -1.2704966809846834,
      -1.3518052481594198,
      -1.2616514427015304,
      -1.3221783932406481,
      -1.2596602038067615,
      -1.2381536293830506,
      -1.2881403822171658,
      -1.381509576012495
    ],
    [
      -1.3543537482082015,
      -1.276438559993712,
      -1.3336141360966163,
      -1.310722084642971,
      -1.273670696027154,
      -1.3536268211222096,
      -1.3272356137430406,
      -1.3336955617930197,
      -1.292302360795113,
      0.0,
      -1.3106710867416078,
      -1.4477572897465916,
      -1.2893253246070537,
      -1.3013480412670153,
      -1.3299125871689736,
      -1.2958121744503095,
      -1.321773952615997,
      -1.3098888191347313,
      -1.329615657002134,
      -1.3666398727089755,
      -1.23900345287022,
      -1.3477230723209161,
      -1.29383911471295,
      -1.315699830468653,
      -1.3516398078012688,
      -1.3685626090644358,
      -1.2901951622215284,
      -1.3742068676914052,
      -1.451871900302897
    ],
    [
      -1.0417415401280528,
      -0.9121843079522215,
      -1.0075692992456058,
      -0.9349886377352044,
      -1.0051189037521286,
      -1.0283806193878993,
      -1.043362826406922,
      -0.991079521599399,
      -0.8877599792282274,
      -0.9791728762131345,
      0.0,
      -1.1387105201554282,
      -0.9336439049206561,
      -1.0241666333193427,
      -0.9241403398536921,
      -0.9684300052363559,
      -1.0152725544285564,
      -0.9050496649197678,
      -0.9884198020083984,
      -1.0443248366362472,
      -0.9535561150100682,
      -1.055748414636909,
      -1.0573074664206614,
      -0.9353077161135293,
      -1.0413295092386117,
      -1.03819320292616,
      -1.014661607349552,
      -0.993608062115605,
      -1.1547398951609147
    ],
    [
      -1.2472379811679573,
      -1.219215282342783,
      -1.2341103772927389,
      -1.2036241430600128,
      -1.2217105855994645,
      -1.2310419665932024,
      -1.2254981116111947,
      -1.2040874629138971,
      -1.2026949482714306,
      -1.179103489100515,
      -1.2268536293258783,
      0.0,
      -1.2274897019215265,
      -1.230572316892372,
      -1.2271375570382457,
      -1.2219448064182816,
      -1.184240886200742,
      -1.2229635763376363,
      -1.1917454679341697,
      -1.2066824881322797,
      -1.1835521619139366,
      -1.2230069145030622,
      -1.1803685197207923,
      -1.1969917045339475,
      -1.1905169661648551,
      -1.230561565721842,
      -1.2215025334746101,
      -1.2237239755265423,
      -1.1762951373985189
    ],
    [
      -1.1031423468823607,
      -1.014865750156801,
      -1.0804405244361077,
      -1.042156685201481,
      -1.0654183386612865,
      -1.0855405985323523,
      -1.075970372891011,
      -1.0728633810586756,
      -0.9693819404595148,
      -1.0072177758626313,
      -1.047167514118407,
      -1.2181141306619905,
      0.0,
      -1.110134523610691,
      -1.0380866210234643,
      -1.057836294049802,
      -1.0336349759850487,
      -0.9964473078012105,
      -1.0594440684809427,
      -1.1548002042714793,
      -1.0557227676485479,
      -1.1059846264664712,
      -1.1196777814757337,
      -1.0603377882777523,
      -1.063684008368978,
      -1.1162413030223675,
      -1.1349124901495364,
      -1.1147644231256755,
      -1.2197038743918855
    ],
    [
      -1.3273061849574725,
      -1.246857942779599,
      -1.2841669732946568,
      -1.2551455424144984,
      -1.2846398118973965,
      -1.3451278540972196,
      -1.3247950443096157,
      -1.2724898446785462,
      -1.2129217715362741,
      -1.2525380796993542,
      -1.315447314569493,
      -1.3795866429166566,
      -1.2945967691557234,
      0.0,
      -1.2758415599371906,
      -1.3074001188486595,
      -1.2817615180634452,
      -1.2947023445004149,
      -1.3190295570075266,
      -1.334135273737929,
      -1.281579912535056,
      -1.282046013968522,
      -1.3468421568846183,
      -1.255466032287184,
      -1.3242777733147357,
      -1.2599978289514,
      -1.2841341811379978,
      -1.3493595343071103,
      -1.3575430716816883
    ],
    [
      -1.2447232708862754,
      -1.1076325006762449,
      -1.1348011715608823,
      -1.1198094125953721,
      -1.1759572396922942,
      -1.1827424144850007,
      -1.1434863224332483,
      -1.150693700776112,
      -1.064650176658826,
      -1.1418392784260074,
      -1.0950245815523467,
      -1.3081701730198112,
      -1.137162167923241,
      -1.1869854794001895,
      0.0,
      -1.16041532274028,
      -1.1540193061161896,
      -1.0911122399120947,
      -1.1872736708940947,
      -1.1786630441881694,
      -1.1222100187790027,
      -1.1971496914214392,
      -1.1781920346566517,
      -1.148035319709856,
      -1.1713885211975696,
      -1.159139712090747,
      -1.1728909257895808,
      -1.1783789338173172,
      -1.2763809969108
    ],
    [
      -1.210043564527879,
      -1.1396655453527287,
      -1.1751428481383548,
      -1.1437924317259616,
      -1.174298007585178,
      -1.2288993636694716,
      -1.1823585405036596,
      -1.1383660924837962,
      -1.109237056564605,
      -1.1544247873156672,
      -1.1624296805737555,
      -1.3301410615565823,
      -1.1397541010868724,
      -1.1527951166473824,
      -1.1680190206902843,
      0.0,
      -1.1735930661031126,
      -1.139328106839552,
      -1.181556423482638,
      -1.2436134017933251,
      -1.17075228804252,
      -1.209933903186797,
      -1.2226719987135986,
      -1.1190900638018355,
      -1.2011783436571635,
      -1.2086724148241046,
      -1.2162497225816769,
      -1.2812363079379294,
      -1.3372707836954303
    ],
    [
      -1.459200045415282,
      -1.4066723968810209,
      -1.440852388016173,
      -1.4324440473289088,
      -1.408435057610587,
      -1.4372530291006944,
      -1.4661438191647094,
      -1.4825907267749883,
      -1.4213856924565216,
      -1.3609455793812857,
      -1.4634153312531477,
      -1.5694352345810632,
      -1.4097215452819163,
      -1.4580328594644563,
      -1.467956030643858,
      -1.4580546818593694,
      0.0,
      -1.4528859058720396,
      -1.5044601940611673,
      -1.4782526213229992,
      -1.3846399403651366,
      -1.4888407520797615,
      -1.443610187113059,
      -1.4901813025899189,
      -1.4406987341095452,
      -1.4687124755310195,
      -1.4829450218959088,
      -1.4747024019741517,
      -1.573036731435059
    ],
    [
      -1.3768077040712035,
      -1.2126280245659635,
      -1.3135490532176222,
      -1.2261401165224484,
      -1.2583584194612027,
      -1.2755578821246292,
      -1.3642307723992284,
      -1.2638262467376404,
      -1.1232187536975966,
      -1.3017300193466232,
      -1.2714769905193295,
      -1.5082056896485776,
      -1.165574891239508,
      -1.3467222484306296,
      -1.280711061376702,
      -1.3166520351315898,
      -1.299677399520054,
      0.0,
      -1.3328322147960345,
      -1.3851940556703242,
      -1.2539136611260147,
      -1.3931553673612798,
      -1.4103448806762144,
      -1.3108066451690166,
      -1.3763451368722424,
      -1.3662053286610198,
      -1.3349735759652042,
      -1.3470844239472872,
      -1.4867584215082532
    ],
    [
      -1.2870592081570444,
      -1.2126824924304698,
      -1.238371466507936,
      -1.2251184690900991,
      -1.2323551240039723,
      -1.205952019740525,
      -1.23997515212129,
      -1.2067954863585726,
      -1.1848768504896308,
      -1.189566845859992,
      -1.2255970712945932,
      -1.3941708038821874,
      -1.1985038837039075,
      -1.2491490854239897,
      -1.2113483553669875,
      -1.1828348784525267,
      -1.1621989747610066,
      -1.209776026798086,
      0.0,
      -1.2577464761739192,
      -1.2257420116229598,
      -1.2661073540790793,
      -1.276309079446213,
      -1.1995825252670265,
      -1.254929634676327,
      -1.2674235847370867,
      -1.2894058364425591,
      -1.287903860588469,
      -1.3674837320353055
    ],
    [
      -1.3352931143515214,
      -1.3000010550657244,
      -1.3262355987569963,
      -1.293155749305381,
      -1.257502618482462,
      -1.3185608833274451,
      -1.2971322967849694,
      -1.288338027865307,
      -1.2554076940070804,
      -1.2885064897289706,
      -1.2924123390602753,
      -1.3530126621070595,
      -1.3217250685439232,
      -1.3189459891797788,
      -1.2922341972558355,
      -1.2790268481621192,
      -1.2221104038644575,
      -1.2697020991280692,
      -1.2886319810225357,
      0.0,
      -1.1780989713353394,
      -1.25909942786662,
      -1.3146911763484932,
      -1.2534332712943084,
      -1.2714383659275064,
      -1.3145262978454042,
      -1.3073823887377511,
      -1.3155729757614956,
      -1.354940491812111
    ],
    [
      -1.7526359703744436,
      -1.6729317487984452,
      -1.654751702695106,
      -1.7034260172178943,
      -1.6390684527100172,
      -1.6972186938850518,
      -1.6912119949422397,
      -1.6962119301541347,
      -1.6668112461004583,
      -1.6548746224565258,
      -1.6613202089933317,
      -1.7808868784203884,
      -1.6549239516113987,
      -1.7561078937666135,
      -1.666451173660807,
      -1.6736771965333677,
      -1.682890323508051,
      -1.625807356410129,
      -1.686288861627962,
      -1.6870861168949833,
      0.0,
      -1.7302063126126017,
      -1.7298341786291613,
      -1.6443850012518666,
      -1.6771904749968423,
      -1.742004006907657,
      -1.6932357400838136,
      -1.6935360952435607,
      -1.8360001286191148
    ],
    [
      -1.3251348349665022,
      -1.2717615660404005,
      -1.2907711853667636,
      -1.2556432665830248,
      -1.2884400002322807,
      -1.312976681242241,
      -1.2645339452273725,
      -1.2214714293575348,
      -1.189576061150428,
      -1.2611102211884577,
      -1.281203433563605,
      -1.3562059179166228,
      -1.2607082595226877,
      -1.2266584338799698,
      -1.295454980747687,
      -1.2581154187493466,
      -1.2679032087654711,
      -1.2424194481533164,
      -1.2980496922347384,
      -1.26734024981881,
      -1.2329714610894575,
      0.0,
      -1.3099514000820671,
      -1.2652026086593897,
      -1.2533986049373471,
      -1.2474766563532993,
      -1.2500412177719247,
      -1.3054252708740146,
      -1.2812082225703179
    ],
    [
      -1.3842001144506444,
      -1.345895775089592,
      -1.3832776394645865,
      -1.3509438185701073,
      -1.2809749598286135,
      -1.365254545096574,
      -1.3479745719240637,
      -1.4017705932166595,
      -1.3612643127372037,
      -1.278467301060004,
      -1.3686359255760292,
      -1.4166114653576876,
      -1.3368110615516626,
      -1.4071178505200934,
      -1.354921786824906,
      -1.3508129517485263,
      -1.3295803488966575,
      -1.3537167081827561,
      -1.3564126627290496,
      -1.4005030959990181,
      -1.3037149548497893,
      -1.4189727428226644,
      0.0,
      -1.328346692235021,
      -1.353045593053401,
      -1.3391123101370523,
      -1.383240534933234,
      -1.3777426216933792,
      -1.4525863824695782
    ],
    [
      -1.3499650383933846,
      -1.2583176882050449,
      -1.3490096799500986,
      -1.3005987856900956,
      -1.2627672462609034,
      -1.3399735228454988,
      -1.3459027166884865,
      -1.3736608246197197,
      -1.2852586180854064,
      -1.2784774315113183,
      -1.2391440802997766,
      -1.4626199591184355,
      -1.2730081429223945,
      -1.3727322896988974,
      -1.3199298785747593,
      -1.297282249926078,
      -1.3697809856144696,
      -1.2655685129647574,
      -1.2911626459971888,
      -1.3848657165886011,
      -1.2236134981256837,
      -1.3676425223476099,
      -1.3579755242521276,
      0.0,
      -1.3989088299620969,
      -1.3625561880148096,
      -1.3942924244904442,
      -1.308548440806549,
      -1.4653818713356022
    ],
    [
      -1.4371542383849154,
      -1.3500754893823097,
      -1.3633836083953075,
      -1.3154447933897975,
      -1.3160422418182678,
      -1.417940131579495,
      -1.3651137955539125,
      -1.4150493086735745,
      -1.3427847727770823,
      -1.3132606269112097,
      -1.345495073941056,
      -1.4794944520650513,
      -1.3363070675655808,
      -1.3730151277337288,
      -1.3514205466794937,
      -1.3398117355846242,
      -1.303985966484443,
      -1.3238613412612552,
      -1.4231395714129285,
      -1.3790374320251155,
      -1.3155665920458355,
      -1.3862560775849881,
      -1.2925777513101275,
      -1.3586710819286427,
      0.0,
      -1.3474664053433114,
      -1.3451507096391664,
      -1.385299101262605,
      -1.4927427509490807
    ],
    [
      -1.4901212652609062,
      -1.4468084930994345,
      -1.4424722214918826,
      -1.4389859045783895,
      -1.4815201269757612,
      -1.4912565527753445,
      -1.445271910243284,
      -1.4634228205575393,
      -1.3863096435956754,
      -1.4648941517562892,
      -1.4279308479261767,
      -1.616367706741011,
      -1.4341492343732747,
      -1.3798481723748095,
      -1.4038155765332263,
      -1.439933419749975,
      -1.4628102294096408,
      -1.4116879018381134,
      -1.4948049769283573,
      -1.4974570368059965,
      -1.4435834826400424,
      -1.4362267988649677,
      -1.4815264590663089,
      -1.4434920074660271,
      -1.442549012711643,
      0.0,
      -1.4350729557305635,
      -1.5039035902746478,
      -1.5471857273888234
    ],
    [
      -1.5136058661821419,
      -1.4789129529850495,
      -1.4694665750072031,
      -1.4774395930773168,
      -1.4176040044358087,
      -1.508119564808814,
      -1.5046452905308965,
      -1.4327637194602656,
      -1.402683675129201,
      -1.4211480264556515,
      -1.4799883353750423,
      -1.6165606235276857,
      -1.4849551052498904,
      -1.4520995361738738,
      -1.47672033784238,
      -1.487343909657258,
      -1.479330968000199,
      -1.4726247207060232,
      -1.5047545064521943,
      -1.4996944712164542,
      -1.3895478728623778,
      -1.5002849482047833,
      -1.4561277644107933,
      -1.4897412930803862,
      -1.498077624628542,
      -1.437156671842326,
      0.0,
      -1.5256309722264425,
      -1.5578485328673963
    ],
    [
      -1.343295720872961,
      -1.2468294810472238,
      -1.2900076726022809,
      -1.290358512231033,
      -1.2683926287775638,
      -1.2546857416913968,
      -1.3358333717986772,
      -1.2658915602892633,
      -1.2242483611221107,
      -1.2314155752241784,
      -1.2439668503424794,
      -1.4416856653349928,
      -1.2428737781001689,
      -1.3356599529729762,
      -1.2652060747783496,
      -1.2632773838040485,
      -1.3157167158924337,
      -1.2251527071995458,
      -1.275249213622313,
      -1.3375433259949845,
      -1.2489823321002969,
      -1.3513513288423165,
      -1.2934731317176074,
      -1.2668029169283828,
      -1.3086022345187662,
      -1.357380352669799,
      -1.3203390355645375,
      0.0,
      -1.4137237537922909
    ],
    [
      -1.352201005366313,
      -1.2764267975459032,
      -1.3097517758400445,
      -1.2902436653365834,
      -1.290105180749792,
      -1.3149282206499702,
      -1.2690134087150393,
      -1.2464114458379851,
      -1.1977785810149502,
      -1.3118803604632276,
      -1.3068537717439073,
      -1.295059644737844,
      -1.2938344000234623,
      -1.2639091426774953,
      -1.2995973421100124,
      -1.2876184421904877,
      -1.3160418525704036,
      -1.309972095009222,
      -1.3027455393888072,
      -1.2788394829027319,
      -1.3196909594179485,
      -1.2109625737370886,
      -1.3085999221665265,
      -1.287313909046315,
      -1.3051977993284973,
      -1.2651562688665088,
      -1.2345907593731844,
      -1.3343129175082826,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.3290908196375941,
      0.2671253085288976,
      0.3018813929272661,
      0.2532916219799506,
      0.26758461539255585,
      0.2472056236724791,
      0.2811025044948514,
      0.3209795986819364,
      0.2866699774074517,
      0.31123687098270647,
      0.19932382285708128,
      0.30138936421456153,
      0.29373993259264086,
      0.2902982676389101,
      0.2785101066355957,
      0.3020858694912727,
      0.30245912832704924,
      0.2786137976376053,
      0.25273137466660645,
      0.2804382283479958,
      0.26882188893831005,
      0.24036899397531997,
      0.3021297518708419,
      0.22470774312457964,
      0.305506876139797,
      0.28462408400283046,
      0.2575957560126443,
      0.19979755838690982
    ],
    [
      0.3926562918586485,
      0.0,
      0.4307763761794754,
      0.5201487208362829,
      0.41807749501381974,
      0.3864959967367574,
      0.37728982961798185,
      0.4466424089300325,
      0.5219589889150182,
      0.4738961516271052,
      0.4860812513027384,
      0.27375784019049965,
      0.47480710576742835,
      0.39892029005191953,
      0.4685668199749302,
      0.450014073567929,
      0.4155097840832711,
      0.509831440830464,
      0.399253419494598,
      0.3679757083331632,
      0.4118514346756055,
      0.35486572005371353,
      0.380991057251975,
      0.4345671696647808,
      0.3922474289061342,
      0.3844273190203964,
      0.4132929687121374,
      0.41473418597274936,
      0.2815945224420182
    ],
    [
      0.3419137531341232,
      0.41072419390735293,
      0.0,
      0.3795480468028112,
      0.4060583657480532,
      0.3763202248021018,
      0.3614668822012139,
      0.40341928939006455,
      0.4585593357149469,
      0.3980136223360098,
      0.3934288521440401,
      0.24652630634340156,
      0.4017316340955366,
      0.37476494340506394,
      0.38947494846252484,
      0.4079212584077878,
      0.3325867731130272,
      0.3789053194504588,
      0.36409098565655373,
      0.3130674742920594,
      0.3943456682583324,
      0.2983084126423141,
      0.32928823970996923,
      0.3818139730698953,
      0.3403500857953974,
      0.3715620192738933,
      0.365885541062553,
      0.3304024921356754,
      0.20659756153360376
    ],
    [
      0.35850675801388276,
      0.4644441102798964,
      0.38242423412404025,
      0.0,
      0.4260216188450461,
      0.3942933223643459,
      0.37830526323049796,
      0.42564737056291113,
      0.5183322680635232,
      0.4313549017431477,
      0.4526682721980404,
      0.2632783835301591,
      0.4609556734145479,
      0.45280882112285936,
      0.4142696212332697,
      0.45137717985285253,
      0.3690548231730628,
      0.4759142335162647,
      0.35362864774416636,
      0.33120447832656774,
      0.4158088269531457,
      0.36455467123152707,
      0.33550769747993825,
      0.40908457528689324,
      0.39361647413017975,
      0.3736123839300587,
      0.36865892557777435,
      0.35531141453770543,
      0.2698307735623091
    ],
    [
      0.3765694007468625,
      0.45627917010685537,
      0.47014489265819637,
      0.4478485808082662,
      0.0,
      0.3865112724667872,
      0.4057273588420769,
      0.4362238141496564,
      0.46749203686723195,
      0.4517371448359926,
      0.41481160532679495,
      0.35013895985953725,
      0.4866574311725038,
      0.4053517970162448,
      0.4378180772588065,
      0.43310282589071214,
      0.43740030276981456,
      0.49129936768198723,
      0.42342497621162645,
      0.3888210027662149,
      0.485382313479616,
      0.36921714687012197,
      0.47262090073355445,
      0.4368958748004115,
      0.4411798838414758,
      0.3787027075431644,
      0.4175733857865822,
      0.3953399515489373,
      0.3100648182953849
    ],
    [
      0.38904763597721925,
      0.4594840606635109,
      0.49034875109561193,
      0.5199976628806688,
      0.4600762833404779,
      0.0,
      0.4150284720742292,
      0.548952518662057,
      0.5487328704341179,
      0.48341630031261595,
      0.45102782128306496,
      0.29263265931754434,
      0.5518635743259148,
      0.43034086151246,
      0.4486669534401275,
      0.45427521756071965,
      0.46877637090298463,
      0.559568004360969,
      0.4711270667749403,
      0.40855000048975953,
      0.4838162522368532,
      0.3917216739245224,
      0.4040525435326512,
      0.4892391136992911,
      0.3924803457636745,
      0.37054455876681613,
      0.3601388646610737,
      0.4462099872425598,
      0.3052551441257265
    ],
    [
      0.3046297517515315,
      0.3571565596639936,
      0.3671525471000965,
      0.4000248511938054,
      0.37587817804997536,
      0.36961635753162514,
      0.0,
      0.34350158012883325,
      0.37787660090002606,
      0.3833220899446843,
      0.3631456848458472,
      0.26482337994298955,
      0.37364314353979067,
      0.38083800733320605,
      0.40071717131314255,
      0.37372945150406545,
      0.3720101802279021,
      0.36599076493345994,
      0.3857761459007687,
      0.3850640804614349,
      0.3993044519331157,
      0.3565448160133087,
      0.3730923910466546,
      0.3801086656449215,
      0.39964398185091343,
      0.3809526150385898,
      0.36981294849312474,
      0.3575999319523646,
      0.2496322615736981
    ],
    [
      0.38282477078593513,
      0.4203178452238865,
      0.41696700530297304,
      0.4145252019052188,
      0.37800527579338694,
      0.45265252371798925,
      0.3251565728889274,
      0.0,
      0.5575993385711453,
      0.38211576484251575,
      0.40581043365278857,
      0.25833571859344007,
      0.4369275193315054,
      0.42145285216343154,
      0.3596077513628242,
      0.40773815140132297,
      0.40041618572626825,
      0.47805204054130823,
      0.36736414600057343,
      0.34215220423229775,
      0.4646569233270843,
      0.4191415080997505,
      0.3325138470909281,
      0.3646593091250161,
      0.333086671826585,
      0.41112147305048374,
      0.4140264785697363,
      0.36031584976886877,
      0.3368669145957497
    ],
    [
      0.410802426657795,
      0.5367763612331389,
      0.4428653149732902,
      0.47373659832342363,
      0.4328403703927639,
      0.4165690526190011,
      0.3683506416775304,
      0.5274303424941007,
      0.0,
      0.459075146230725,
      0.47525670249660035,
      0.22936764015643418,
      0.4983142274431511,
      0.47724667775019225,
      0.4539394241702308,
      0.4527453247252273,
      0.4475322491201552,
      0.568283229686108,
      0.3816899108987244,
      0.3287676952251848,
      0.42915798977411446,
      0.41392570069350487,
      0.3326171335187684,
      0.42277093897665785,
      0.36224398843754013,
      0.4247621778714268,
      0.4462687522951376,
      0.3962819994610225,
      0.30291280566569334
    ],
    [
      0.3665595646304747,
      0.4444747528449642,
      0.38729917674205994,
      0.4101912281957052,
      0.4472426168115222,
      0.36728649171646666,
      0.39367769909563566,
      0.3872177510456565,
      0.42861095204356325,
      0.0,
      0.4102422260970684,
      0.27315602309208464,
      0.4315879882316225,
      0.4195652715716609,
      0.39100072566970256,
      0.4251011383883667,
      0.39913936022267915,
      0.4110244937039449,
      0.3912976558365422,
      0.35427344012970075,
      0.48190985996845614,
      0.37319024051776006,
      0.4270741981257262,
      0.4052134823700233,
      0.3692735050374074,
      0.3523507037742404,
      0.4307181506171478,
      0.346706445147271,
      0.2690414125357792
    ],
    [
      0.3817130599636638,
      0.511270292139495,
      0.41588530084611075,
      0.4884659623565122,
      0.4183356963395879,
      0.3950739807038173,
      0.38009177368479463,
      0.4323750784923175,
      0.5356946208634892,
      0.444281723878582,
      0.0,
      0.28474407993628836,
      0.48981069517106046,
      0.3992879667723739,
      0.4993142602380244,
      0.4550245948553606,
      0.4081820456631602,
      0.5184049351719487,
      0.43503479808331813,
      0.37912976345546934,
      0.46989848508164833,
      0.3677061854548076,
      0.3661471336710551,
      0.4881468839781873,
      0.38212509085310487,
      0.3852613971655565,
      0.4087929927421645,
      0.4298465379761115,
      0.26871470493080185
    ],
    [
      0.17329664446309634,
      0.2013193432882705,
      0.18642424833831472,
      0.21691048257104084,
      0.19882404003158904,
      0.18949265903785117,
      0.1950365140198589,
      0.21644716271715647,
      0.21783967735962295,
      0.2414311365305386,
      0.19368099630517532,
      0.0,
      0.19304492370952708,
      0.1899623087386817,
      0.19339706859280792,
      0.19858981921277197,
      0.2362937394303115,
      0.1975710492934173,
      0.2287891576968839,
      0.21385213749877385,
      0.23698246371711695,
      0.1975277111279914,
      0.24016610591026133,
      0.22354292109710605,
      0.23001765946619845,
      0.18997305990921154,
      0.19903209215644346,
      0.19681065010451126,
      0.24423948823253472
    ],
    [
      0.4061538762213768,
      0.49443047294693643,
      0.42885569866762974,
      0.4671395379022565,
      0.4438778844424509,
      0.42375562457138516,
      0.4333258502127264,
      0.43643284204506183,
      0.5399142826442227,
      0.5020784472411062,
      0.46212870898533054,
      0.291182092441747,
      0.0,
      0.3991616994930465,
      0.4712096020802732,
      0.4514599290539354,
      0.47566124711868873,
      0.512848915302527,
      0.4498521546227947,
      0.3544960188322581,
      0.45357345545518957,
      0.40331159663726623,
      0.38961844162800374,
      0.4489584348259852,
      0.4456122147347594,
      0.39305492008137,
      0.374383732954201,
      0.394531799978062,
      0.2895923487118519
    ],
    [
      0.31434438563183975,
      0.39479262780971336,
      0.3574835972946555,
      0.3865050281748139,
      0.3570107586919158,
      0.2965227164920927,
      0.3168555262796966,
      0.3691607259107661,
      0.4287287990530382,
      0.38911249088995814,
      0.32620325601981937,
      0.26206392767265574,
      0.3470538014335889,
      0.0,
      0.3658090106521217,
      0.3342504517406528,
      0.3598890525258671,
      0.3469482260888974,
      0.32262101358178574,
      0.30751529685138324,
      0.3600706580542563,
      0.3596045566207904,
      0.294808413704694,
      0.3861845383021283,
      0.31737279727457657,
      0.3816527416379123,
      0.3575163894513145,
      0.292291036282202,
      0.284107498907624
    ],
    [
      0.330481753389841,
      0.46757252359987156,
      0.4404038527152341,
      0.45539561168074427,
      0.39924778458382226,
      0.3924626097911157,
      0.4317187018428681,
      0.4245113235000044,
      0.5105548476172903,
      0.433365745850109,
      0.4801804427237697,
      0.2670348512563052,
      0.4380428563528753,
      0.3882195448759269,
      0.0,
      0.4147897015358364,
      0.4211857181599268,
      0.4840927843640217,
      0.38793135338202167,
      0.39654198008794705,
      0.4529950054971137,
      0.3780553328546772,
      0.39701298961946474,
      0.42716970456626036,
      0.4038165030785468,
      0.41606531218536946,
      0.40231409848653565,
      0.39682609045879924,
      0.29882402736531644
    ],
    [
      0.3856803397535502,
      0.4560583589287006,
      0.4205810561430745,
      0.45193147255546773,
      0.4214258966962512,
      0.3668245406119577,
      0.41336536377776967,
      0.45735781179763313,
      0.48648684771682427,
      0.4412991169657621,
      0.4332942237076738,
      0.265582842724847,
      0.45596980319455693,
      0.44292878763404686,
      0.427704883591145,
      0.0,
      0.42213083817831665,
      0.4563957974418773,
      0.4141674807987914,
      0.35211050248810416,
      0.4249716162389092,
      0.38579000109463224,
      0.37305190556783074,
      0.4766338404795938,
      0.3945455606242658,
      0.3870514894573247,
      0.37947418169975244,
      0.3144875963434999,
      0.258453120585999
    ],
    [
      0.36251988464029106,
      0.41504753317455223,
      0.3808675420394001,
      0.38927588272666425,
      0.413284872444986,
      0.3844669009548787,
      0.3555761108908637,
      0.3391292032805848,
      0.4003342375990515,
      0.4607743506742874,
      0.3583045988024254,
      0.25228469547450993,
      0.4119983847736568,
      0.3636870705911168,
      0.35376389941171515,
      0.3636652481962037,
      0.0,
      0.3688340241835335,
      0.3172597359944058,
      0.3434673087325739,
      0.4370799896904365,
      0.33287917797581157,
      0.378109742942514,
      0.3315386274656542,
      0.3810211959460279,
      0.3530074545245536,
      0.33877490815966427,
      0.3470175280814214,
      0.24868319862051402
    ],
    [
      0.4430295908513626,
      0.6072092703566025,
      0.5062882417049439,
      0.5936971784001177,
      0.5614788754613633,
      0.5442794127979369,
      0.45560652252333766,
      0.5560110481849256,
      0.6966185412249695,
      0.5181072755759428,
      0.5483603044032366,
      0.3116316052739885,
      0.654262403683058,
      0.4731150464919365,
      0.539126233545864,
      0.5031852597909763,
      0.5201598954025122,
      0.0,
      0.48700508012653154,
      0.4346432392522419,
      0.5659236337965514,
      0.4266819275612863,
      0.40949241424635163,
      0.5090306497535495,
      0.4434921580503237,
      0.4536319662615462,
      0.4848637189573619,
      0.47275287097527885,
      0.33307887341431286
    ],
    [
      0.3908574069800701,
      0.46523412270664477,
      0.4395451486291786,
      0.4527981460470154,
      0.44556149113314225,
      0.47196459539658964,
      0.4379414630158245,
      0.47112112877854195,
      0.4930397646474838,
      0.4883497692771226,
      0.4523195438425214,
      0.28374581125492715,
      0.47941273143320706,
      0.4287675297131248,
      0.46656825977012706,
      0.4950817366845879,
      0.5157176403761079,
      0.4681405883390286,
      0.0,
      0.4201701389631953,
      0.4521746035141547,
      0.4118092610580353,
      0.40160753569090146,
      0.4783340898700881,
      0.42298698046078753,
      0.4104930304000278,
      0.3885107786945554,
      0.3900127545486456,
      0.31043288310180905
    ],
    [
      0.3186424046716383,
      0.35393446395743533,
      0.32769992026616346,
      0.3607797697177788,
      0.3964329005406977,
      0.3353746356957146,
      0.3568032222381903,
      0.3655974911578528,
      0.3985278250160793,
      0.3654290292941891,
      0.3615231799628844,
      0.3009228569161002,
      0.3322104504792365,
      0.33498952984338093,
      0.36170132176732417,
      0.3749086708610405,
      0.43182511515870226,
      0.3842334198950905,
      0.365303538000624,
      0.0,
      0.47583654768782035,
      0.3948360911565396,
      0.3392443426746665,
      0.40050224772885135,
      0.38249715309565335,
      0.3394092211777555,
      0.3465531302854086,
      0.33836254326166415,
      0.29899502721104865
    ],
    [
      0.2888128208461356,
      0.368517042422134,
      0.38669708852547324,
      0.3380227740026849,
      0.402380338510562,
      0.34423009733552745,
      0.3502367962783395,
      0.34523686106644447,
      0.37463754512012093,
      0.38657416876405337,
      0.38012858222724755,
      0.2605619128001908,
      0.3865248396091805,
      0.28534089745396574,
      0.37499761755977223,
      0.36777159468721154,
      0.3585584677125282,
      0.41564143481045024,
      0.3551599295926171,
      0.35436267432559587,
      0.0,
      0.3112424786079775,
      0.31161461259141787,
      0.3970637899687126,
      0.36425831622373694,
      0.2994447843129222,
      0.3482130511367656,
      0.34791269597701846,
      0.20544866260146444
    ],
    [
      0.26828621748443915,
      0.3216594864105409,
      0.30264986708417774,
      0.3377777858679165,
      0.3049810522186607,
      0.28044437120870036,
      0.32888710722356884,
      0.3719496230934065,
      0.4038449913005133,
      0.3323108312624836,
      0.31221761888733623,
      0.23721513453431853,
      0.3327127929282536,
      0.3667626185709716,
      0.2979660717032544,
      0.3353056337015947,
      0.3255178436854702,
      0.35100160429762495,
      0.2953713602162029,
      0.32608080263213135,
      0.3604495913614838,
      0.0,
      0.2834696523688742,
      0.32821844379155163,
      0.34002244751359423,
      0.34594439609764205,
      0.34337983467901667,
      0.28799578157692673,
      0.3122128298806235
    ],
    [
      0.31321111171623084,
      0.3515154510772833,
      0.31413358670228875,
      0.346467407596768,
      0.4164362663382617,
      0.33215668107030116,
      0.34943665424281156,
      0.2956406329502157,
      0.3361469134296715,
      0.4189439251068712,
      0.32877530059084603,
      0.28079976080918767,
      0.3606001646152126,
      0.2902933756467818,
      0.3424894393419693,
      0.3465982744183489,
      0.3678308772702177,
      0.3436945179841191,
      0.34099856343782564,
      0.2969081301678571,
      0.39369627131708596,
      0.27843848334421084,
      0.0,
      0.3690645339318541,
      0.3443656331134741,
      0.3582989160298229,
      0.3141706912336413,
      0.319668604473496,
      0.24482484369729707
    ],
    [
      0.4117213820928134,
      0.5033687322811531,
      0.4126767405360994,
      0.4610876347961024,
      0.4989191742252945,
      0.4217128976406992,
      0.41578370379771146,
      0.3880255958664782,
      0.4764278024007915,
      0.4832089889748796,
      0.5225423401864213,
      0.2990664613677625,
      0.48867827756380344,
      0.38895413078730057,
      0.44175654191143865,
      0.46440417056011984,
      0.39190543487172835,
      0.4961179075214406,
      0.47052377448900917,
      0.3768207038975968,
      0.5380729223605143,
      0.3940438981385881,
      0.4037108962340703,
      0.0,
      0.3627775905241011,
      0.39913023247138835,
      0.36739399599575373,
      0.45313797967964886,
      0.2963045491505958
    ],
    [
      0.3369413619782029,
      0.4240201109808086,
      0.41071199196781083,
      0.45865080697332083,
      0.4580533585448505,
      0.35615546878362325,
      0.40898180480920576,
      0.35904629168954383,
      0.431310827586036,
      0.4608349734519086,
      0.42860052642206226,
      0.29460114829806705,
      0.4377885327975375,
      0.4010804726293895,
      0.42267505368362457,
      0.4342838647784941,
      0.4701096338786752,
      0.4502342591018631,
      0.35095602895018985,
      0.3950581683380028,
      0.4585290083172828,
      0.38783952277813016,
      0.4815178490529908,
      0.4154245184344756,
      0.0,
      0.4266291950198069,
      0.4289448907239519,
      0.3887964991005133,
      0.2813528494140376
    ],
    [
      0.347675278764368,
      0.39098805092583966,
      0.39532432253339156,
      0.39881063944688466,
      0.35627641704951296,
      0.3465399912499296,
      0.3925246337819901,
      0.3743737234677349,
      0.4514869004295987,
      0.3729023922689849,
      0.4098656960990974,
      0.2214288372842632,
      0.40364730965199946,
      0.45794837165046465,
      0.43398096749204784,
      0.39786312427529924,
      0.37498631461563336,
      0.4261086421871607,
      0.3429915670969168,
      0.34033950721927764,
      0.3942130613852317,
      0.4015697451603064,
      0.3562700849589653,
      0.394304536559247,
      0.3952475313136312,
      0.0,
      0.40272358829471067,
      0.33389295375062633,
      0.2906108166364507
    ],
    [
      0.3524559593677954,
      0.3871488725648877,
      0.39659525054273415,
      0.3886222324726205,
      0.4484578211141286,
      0.35794226074112334,
      0.3614165350190408,
      0.4332981060896717,
      0.4633781504207364,
      0.44491379909428574,
      0.386073490174895,
      0.24950120202225157,
      0.3811067203000469,
      0.4139622893760635,
      0.38934148770755717,
      0.3787179158926792,
      0.38673085754973835,
      0.39343710484391403,
      0.36130731909774294,
      0.3663673543334831,
      0.47651395268755947,
      0.36577687734515396,
      0.409934061139144,
      0.3763205324695511,
      0.36798420092139517,
      0.4289051537076112,
      0.0,
      0.34043085332349476,
      0.30821329268254094
    ],
    [
      0.37048786982119464,
      0.46695410964693185,
      0.42377591809187476,
      0.4234250784631226,
      0.4453909619165919,
      0.45909784900275885,
      0.3779502188954784,
      0.44789203040489234,
      0.489535229572045,
      0.4823680154699772,
      0.4698167403516762,
      0.2720979253591629,
      0.4709098125939868,
      0.3781236377211794,
      0.4485775159158061,
      0.4505062068901071,
      0.398066874801722,
      0.4886308834946098,
      0.4385343770718426,
      0.37624026469917116,
      0.4648012585938588,
      0.3624322618518392,
      0.4203104589765483,
      0.4469806737657729,
      0.4051813561753894,
      0.3564032380243567,
      0.3934445551296182,
      0.0,
      0.3000598369018648
    ],
    [
      0.2862966388874333,
      0.36207084670784306,
      0.3287458684137017,
      0.3482539789171628,
      0.34839246350395414,
      0.32356942360377605,
      0.3694842355387069,
      0.3920861984157611,
      0.44071906323879606,
      0.3266172837905186,
      0.33164387250983896,
      0.34343799951590226,
      0.34466324423028394,
      0.3745885015762509,
      0.3389003021437338,
      0.3508792020632585,
      0.3224557916833426,
      0.32852554924452426,
      0.335752104864939,
      0.35965816135101436,
      0.31880668483579777,
      0.4275350705166576,
      0.3298977220872197,
      0.3511837352074312,
      0.3332998449252489,
      0.37334137538723744,
      0.4039068848805618,
      0.3041847267454636,
      0.0
    ]
  ],
  "row_avgs": [
    0.2760468170916515,
    0.41718685000041333,
    0.3627527215317417,
    0.39608841871530764,
    0.42408346429769345,
    0.44626327033436297,
    0.3646995924933525,
    0.3944432241961407,
    0.4254475293917016,
    0.39265809122011547,
    0.4228128585881719,
    0.20858911644846662,
    0.4298786367797301,
    0.3434456901082415,
    0.41202903755077186,
    0.40591983131421994,
    0.3636661895711534,
    0.5018844013595861,
    0.43688210479740885,
    0.36225271606141185,
    0.3467711383953661,
    0.3226655639850456,
    0.33555732184478393,
    0.42958123072583226,
    0.4092546078030145,
    0.3787462501981987,
    0.3862447733214945,
    0.41885696998583494,
    0.3499605990995128
  ],
  "col_avgs": [
    0.35021851218149347,
    0.4222092709102441,
    0.39037317313381786,
    0.4154257033765157,
    0.40472356713437924,
    0.3728355919299075,
    0.3715439671919056,
    0.4027082306702556,
    0.45626317347971035,
    0.4193751629872074,
    0.40533461223328376,
    0.27247299567234456,
    0.4223684073592012,
    0.3868644012173458,
    0.40084426062975265,
    0.4018500045404664,
    0.39756140310403876,
    0.4275782023785023,
    0.3755652174735909,
    0.35237034328746664,
    0.4279021842337975,
    0.36419185565248346,
    0.3647896916260878,
    0.40268162702516896,
    0.3701947979645965,
    0.3732585970807243,
    0.37690691483712563,
    0.35748062558632787,
    0.2787765223129843
  ],
  "combined_avgs": [
    0.3131326646365725,
    0.41969806045532876,
    0.3765629473327798,
    0.4057570610459117,
    0.41440351571603634,
    0.40954943113213527,
    0.36812177984262906,
    0.39857572743319813,
    0.440855351435706,
    0.40601662710366143,
    0.41407373541072784,
    0.2405310560604056,
    0.4261235220694657,
    0.36515504566279366,
    0.4064366490902622,
    0.40388491792734316,
    0.38061379633759607,
    0.4647313018690442,
    0.4062236611354999,
    0.35731152967443924,
    0.3873366613145818,
    0.3434287098187645,
    0.3501735067354359,
    0.4161314288755006,
    0.38972470288380545,
    0.37600242363946146,
    0.38157584407931006,
    0.3881687977860814,
    0.3143685607062485
  ],
  "gppm": [
    584.5605163845879,
    564.9225157939043,
    577.7212448344334,
    568.3631743239761,
    572.5775898100985,
    588.0739951477643,
    585.2927074495747,
    570.9004738181346,
    550.5262014857035,
    565.3183127549482,
    576.9859468145859,
    627.6091569436146,
    566.59671616426,
    576.2935708915453,
    575.8839380838729,
    575.273665477876,
    573.6279514270641,
    562.217042500591,
    586.9538642508325,
    596.5928315575617,
    555.3224278143472,
    593.1785657468654,
    588.660280262299,
    572.5238607141229,
    588.158845274939,
    583.2021119268616,
    582.5153551145862,
    594.416389650189,
    631.1476833178004
  ],
  "gppm_normalized": [
    1.3414543717485994,
    1.2940704663485276,
    1.325138030482505,
    1.304523705514921,
    1.3075788677838538,
    1.343939973580944,
    1.3448344444400047,
    1.3053499738026904,
    1.2593195328359479,
    1.294904295787005,
    1.3195683719961775,
    1.452534273359283,
    1.2994688046198306,
    1.3302458697800585,
    1.3186390655931226,
    1.316900504539362,
    1.3131178193409987,
    1.2831682570355714,
    1.340249837647106,
    1.3643011795515856,
    1.2749295788839186,
    1.354640898135577,
    1.3503291273864033,
    1.3101416493330607,
    1.3421852933980287,
    1.3368464605796808,
    1.332188668483113,
    1.3570534661216176,
    1.4384463531964045
  ],
  "token_counts": [
    449,
    442,
    457,
    469,
    405,
    411,
    478,
    424,
    432,
    434,
    418,
    576,
    471,
    536,
    433,
    435,
    431,
    402,
    393,
    417,
    463,
    400,
    456,
    424,
    387,
    443,
    422,
    396,
    359
  ],
  "response_lengths": [
    2240,
    2458,
    2668,
    2627,
    2291,
    2348,
    2730,
    2361,
    2255,
    2341,
    2283,
    3256,
    2670,
    3018,
    2306,
    2484,
    2320,
    2214,
    2245,
    2435,
    2615,
    2135,
    2521,
    2439,
    2153,
    2444,
    2423,
    2264,
    2061
  ]
}