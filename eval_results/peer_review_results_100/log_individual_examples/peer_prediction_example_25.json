{
  "example_idx": 25,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSMART MULTI-TENANT FEDERATED LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFederated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous training activities could overload resource-constrained devices. In this work, we propose a smart multi-tenant FL system, MuFL, to effectively coordinate and execute simultaneous training activities. We first formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and introduce a vanilla multitenant FL system that trains activities sequentially to form baselines. Then, we propose two approaches to optimize multi-tenant FL: 1) activity consolidation merges training activities into one activity with a multi-task architecture; 2) after training it for rounds, activity splitting divides it into groups by employing affinities among activities such that activities within a group have better synergy. Extensive experiments demonstrate that MuFL outperforms other methods while consuming 40% less energy. We hope this work will inspire the community to further study and optimize multi-tenant FL.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017) has attracted considerable attention as it enables privacy-preserving distributed model training among decentralized devices. It is empowering growing numbers of applications in both academia and industry, such as Google Keyboard (Hard et al., 2018), medical imaging analysis (Li et al., 2019; Sheller et al., 2018), and autonomous vehicles (Zhang et al., 2021a; Posner et al., 2021). Among them, some applications contain multiple training activities for different tasks. For example, Google Keyboard includes query suggestion (Yang et al., 2018), emoji prediction (Ramaswamy et al., 2019), and next-world prediction (Hard et al., 2018); autonomous vehicles relates to multiple computer vision (CV) tasks, including lane detection, object detection, and semantic segmentation (Janai et al., 2020).\n\nHowever, multiple simultaneous training activities could overload edge devices (Bonawitz et al., 2019). Edge devices have tight resource constraints, whereas training deep neural networks for the aforementioned applications is resource-intensive. As a result, the majority of edge devices can only support one training activity at a time (Liu et al., 2019); multiple simultaneous federated learning activities on the same device could overwhelm its memory, computation, and power capacities. Thus, it is important to navigate solutions to well coordinate these training activities.\n\nA plethora of research on FL considers only one training activity in an application. Many studies are devoted to addressing challenges including statistical heterogeneity (Li et al., 2020; Wang et al., 2020a), system heterogeneity (Chai et al., 2020; Yang et al., 2021), communication efficiency (Karimireddy et al., 2020; Zhu et al., 2021), and privacy issues (Bagdasaryan et al., 2020; Huang et al., 2021). A common limitation is that they only focus on one training activity, but applications like Google Keyboard and autonomous vehicles require multiple training activities for different targets (Yang et al., 2018; Ramaswamy et al., 2019). Multi-tenancy of an FL system is designed by Bonawitz et al. (2019) to prevent simultaneous training activities from overloading devices. However, it mainly considers differences among training activities, neglecting potential synergies.\n\nIn this work, we propose a smart multi-tenant federated learning system, MuFL, to efficiently coordinate and execute simultaneous training activities under resource constraints by considering both synergies and differences among training activities. We first formalize the problem of multitenant FL and define four multi-tenant FL scenarios based on two variances in Section 3: 1) whether all training activities are the same type of application, e.g., CV applications; 2) whether all clients\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nsupport all training activities. Then, we define a vanilla multi-tenant FL system that supports all scenarios by training activities sequentially. Built on it, we further optimize the scenario, where all training activities are the same type and all clients support all activities, by considering both synergies and differences among activities in Section 4. Specifically, we propose activity consolidation to merge training activities into one activity with a multi-task architecture that shares common layers and has specialized layers for each activity. We then introduce activity splitting to divide the activity into multiple activities based on their synergies and differences measured by affinities between activities.\n\nWe demonstrate that MuFL reduces the energy consumption by over 40% while achieving superior performance to other methods via extensive experiments on three different sets of training activities in Section 5. We believe that MuFL is beneficial for many real-world applications such as autonomous vehicles, voice assistance systems, and robotics (more examples in Appendix A). We summarize our contributions as follows:\n\n• We formalize the problem of multi-tenant FL and define four multi-tenant FL scenarios. To the best of our knowledge, we are the first work that investigates multi-tenant FL in-depth. • We propose MuFL, a smart multi-tenant federated learning system to efficiently coordinate and execute simultaneous training activities by proposing activity consolidation and activity splitting to consider both synergies and differences among training activities.\n\n• We establish baselines for multi-tenant FL and demonstrate that MuFL elevates performance\n\nwith significantly less energy consumption via extensive empirical studies.\n\n2 RELATED WORK\n\nIn this section, we first review the concept of multi-tenancy in cloud computing and machine learning. Then, we provide a literature review of multi-task learning and federated learning.\n\nMulti-tenancy of Cloud Computing and Machine Learning Multi-tenancy has been an important concept in cloud computing. It refers to the software architecture where a single instance of software serves multiple users (Chong & Carraro, 2006; Fehling et al., 2010). Multi-tenant software architecture is one of the foundations of software as a service (SaaS) applications (Mietzner et al., 2008; Cai et al., 2013). Recently, researchers have adopted this idea to machine learning (especially deep learning) training and inference. Specifically, some studies investigate how to share GPU clusters among multiple users to train deep neural networks (DNN) (Jeon et al., 2019; Zhao et al., 2020; Lao et al., 2021), but these methods are for GPU clusters that have enormous computing resources, which are inapplicable to edge devices that have limited resources. Targeting on-device deep learning, some researchers define multi-tenant as processing multiple computer vision (CV) applications for multiple concurrent tasks (Fang et al., 2018; Jiang et al., 2018). However, they focus on the multi-tenant on-device inference rather than training. On the contrary, we focus on multi-tenant federated learning (FL) training on devices, where the multi-tenancy refers to multiple concurrent FL training activities.\n\nMulti-task Learning Multi-task learning is a popular machine learning approach to learn models that generalize on multiple tasks (Thrun, 1995; Zhang & Yang, 2021). A plethora of studies investigate parameter sharing approaches that share common layers of a similar architecture (Caruana, 1997; Eigen & Fergus, 2015; Bilen & Vedaldi, 2016; Nekrasov et al., 2019). Besides, many studies employ new techniques to address the negative transfer problem (Kang et al., 2011; Zhao et al., 2018) among tasks, including soft parameter sharing (Duong et al., 2015; Misra et al., 2016), neural architecture search (Lu et al., 2017; Huang et al., 2018; Vandenhende et al., 2019; Guo et al., 2020; Sun et al., 2020), and dynamic loss reweighting strategies (Kendall et al., 2018; Chen et al., 2018; Yu et al., 2020). Instead of training all tasks together, task grouping trains only similar tasks together. The early works of task grouping (Kang et al., 2011; Kumar & Daum ́e, 2012) are not adaptable to DNN. Recently, several studies analyze the task similarity (Standley et al., 2020) and task affinities (Fifty et al., 2021) for task grouping. In this work, we adopt the idea of task grouping to consolidate and split training activities. The state-of-the-art task grouping methods (Standley et al., 2020; Fifty et al., 2021), however, are unsuitable for our scenario because they focus on the inference efficiency, bypassing the intensive computation on training. Thus, we propose activity consolidation and activity splitting to group training activities based on their synergies and differences.\n\nFederated Learning Federated learning emerges as a promising privacy-preserving distributed machine learning technique that uses a central server to coordinate multiple decentralized clients to\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ntrain models (McMahan et al., 2017; Kairouz et al., 2021). The majority of studies aim to address the challenges of FL, including statistical heterogeneity (Li et al., 2020; Wang et al., 2020a;b; Zhuang et al., 2020; yuyang deng et al., 2021; Zhang et al., 2021b), system heterogeneity (Chai et al., 2020; Yang et al., 2021), communication efficiency (McMahan et al., 2017; Koneˇcn ́y et al., 2016; Karimireddy et al., 2020; Zhu et al., 2021), and privacy concerns (Bagdasaryan et al., 2020; Huang et al., 2021). Among them, federated multi-task learning (Smith et al., 2017; Marfoq et al., 2021) is an emerging method to learn personalized models to tackle statistical heterogeneity. However, these personalized FL methods mainly focus on training one activity of an application in a client. Multi-tenant FL that handles multiple concurrent training activities is rarely discussed; the prior work (Bonawitz et al., 2019) mainly considers the differences among training activities. In this work, we optimize multi-tenant FL by further considering their synergies by splitting activities into groups. Our problem is also fundamentally different from clustered FL (Ghosh et al., 2020; Ouyang et al., 2021); They group models of the same training activity, whereas we group training activities.\n\n3 PROBLEM SETUP\n\nThis section provides preliminaries of federated learning (FL), presents the problem definition of multi-tenant FL, and classifies four multi-tenant FL scenarios. Besides, we introduce a vanilla multi-tenant FL system supports for all scenarios.\n\n3.1 PRELIMINARIES AND PROBLEM DEFINITION\n\nIn the federated learning setting, the majority of studies consider optimizing the following problem:\n\nmin ω∈Rd\n\nf (ω) :=\n\nK (cid:88)\n\nk=1\n\npkfk(ω) :=\n\nK (cid:88)\n\nk=1\n\npkEξk∼Dk [fk(ω; ξk)],\n\n(1)\n\nwhere ω is the optimization variable, K is the number of selected clients to execute training, fk(ω) is the loss function of client k, pk is the weight of client k in model aggregation, and ξk is the training data sampled from data distribution Dk of client k. FedAvg (McMahan et al., 2017) is a popular federated learning algorithm, which sets pk to be proportional to the dataset size of client k.\n\nEquation 1 illustrates the objective of single training activity in FL, but in real-world scenarios, multiple simultaneous training activities could overload edge devices. We further formalize the problem of multi-tenant FL as follows.\n\nIn multi-tenant FL, a server coordinates a set of clients C to execute a set of n FL training activities A = {α1, α2, . . . , αn}. It obtain a set of parameters of models W = {ω1, ω2, . . . , ωn}, where each model ωi is for activity αi. By defining M(αi; ωi) as performance measurement of each training activity αi, multi-tenant FL aims to maximize the performance of all training activities (cid:80)n i=1 M(αi; ωi), under the constraint that each client k has limited memory budget and computation budget. These budgets constrain the number of concurrent training actvitities nk on client k. Besides, as devices have limited battery life, we would like to minimize the energy consumption and training time to obtain W from training activities A.\n\n3.2 MULTI-TENANT FL SCENARIOS\n\nWe classify multi-tenant FL into four different scenarios based on variances in two aspects: 1) whether all training activities in A are the same type of application, e.g., computer vision (CV) applications or natural language processing (NLP) applications; 2) whether all clients in C support all training activities in A. We depict these four scenarios in Figure 6 in Appendix A and describe them below.\n\nScenario 1 ∀αi ∈ A, αi is the same type of application; ∀αi ∈ A, ∀ck ∈ C supports αi. For example, autonomous vehicles (clients) support the same sets of CV applications, such as object detection and semantic segmentation. Thus, they support training activities of these applications.\n\nScenario 2 ∃αi ∈ A, αi is a different type of application; ∀αi ∈ A, ∀ck ∈ C supports αi. For example, Google Keyboard has different types of applications, including recommendation (query suggestion (Yang et al., 2018)) and NLP (next-world prediction (Hard et al., 2018)). Mobile phones (clients) with Google Keyboard support these applications together with all related training activities.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla Multi-tenant FL\n\n(b) Smart Multi-tenant FL\n\nFigure 1: The architectures of proposed multi-tenant federated learning (FL) systems. The vanilla multi-tenant FL system (a) employs a scheduler to queue simultaneous training activities and execute them one by one. The smart multi-tenant FL system (b) proposes activity consolidation and activity splitting to consider both synergies and differences among training activities, which elevates performance and reduces resource consumption.\n\nScenario 3 ∀αi ∈ A, αi is the same type of application; ∀αi ∈ A, ∃ck ∈ C does not support αi. For example, survelliance cameras (clients) in parking lots could support CV applications, but cameras in different locations may support different applications, e.g., counting open-parking spots, tracking parking duration, or recording fender benders.\n\nScenario 4 ∃αi ∈ A, αi is a different type of application; ∀αi ∈ A, ∃ck ∈ C does not support αi. For example, browsers (clients) could leverage users’ browsing history to support ranking (Hartmann et al., 2019) and news recommendation (Minto et al., 2021), which are different applications. Users may opt-out of recommendations, resulting in not all browsers supporting all training activities.\n\nThe application determines the multi-tenant FL scenario. We next introduce a vanilla multi-tenant FL that supports all these scenarios as our baseline.\n\n3.3 VANILLA MULTI-TENANT FL\n\nFigure 1a presents the architecture of a vanilla multi-tenant FL system. It prevents overloading and congestion of multiple simultaneous training activities by scheduling them to execute one by one. Particularly, we use a scheduler to queue training activities in the server (e.g., First in, First out). In each round, the server selects K clients from the client pool to participate in training. The number of simultaneous training activities depend on the computational resources of the selected clients. In this study, we assume that each client can execute one training activity at a time (nk = 1). This is a realistic assumption for the majority of current edge devices. 1 As a result, the vanilla multi-tenant FL system executes training activities sequentially.\n\nThe vanilla multi-tenant FL system supports the four multi-tenant FL scenarios. From the perspective of the type of application, it can handle different application types of training activities as each training activity is executed independently. From the perspective of whether clients support all training activities, each training activity can select clients that support the activity to participate in training. Despite its comprehensiveness, it only considers differences among training activities, neglecting their potential synergies. In contrast, our proposed MuFL considers both synergies and differences among training activities to further optimize the Scenario 1.\n\n1Edges devices, e.g., NVIDIA Jetson TX2 and AGX Xavier, have only one GPU; GPU virtualization (Hong\n\net al., 2017) that enables concurrent training on the same GPU currently are mainly for the cloud stack.\n\n4\n\nTraining ActivitiesServer1. Scheduling2. TrainingClient PoolNew training activityScheduledTrainedStatusParticipants...2.1 Execution2.2 AggregationServer1. Activity Consolidation3. TrainingTraining Activities2.Scheduling4. Activity SplittingAll-in-oneGroupedOne by oneTraining typesClient PoolParticipants...3.2 Aggregation3.1 ExecutionUnder review as a conference paper at ICLR 2023\n\n4 SMART MULTI-TENANT FL\n\nIn this section, we introduce the smart multi-tenant FL system, MuFL. We start by providing an overview of MuFL. Then, we present two important components of MuFL, activity consolidation and activity splitting, to consider both synergies and differences among simultaneous training activities.\n\nFigure 1b depicts the architecture and training processes of MuFL. It contains a server to coordinate training activities and a pool of clients to execute training. MuFL optimizes the Scenario 1 discussed previously with the following steps: 1) The server receives training activities A = {α1, α2, . . . , αn} to train models W = {ω1, ω2, . . . , ωn} and consolidates these activities into an all-in-one training activity α0; 2) The server schedules α0 to train; 3) The server select K clients from the client pool to execute α0 iteratively through FL process for R0 rounds; 4) The server splits the all-in-one activity α0 into multiple training activity groups {A1, A2, . . . }, where each group trains nonoverlapping subset of W; the number of groups can be determined by the inference budget for the number of concurrent models. 5) The server iterates step 2 and 3 to train Aj. We summarize MuFL in Algorithm 1 in Appendix D and introduce the details of activity consolidation and activity splitting next.\n\n4.1 ACTIVITY CONSOLIDATION\n\nFocusing on optimizing the Scenario 1 of multi-tenant FL, we first propose activity consolidation to consolidate multiple training activities into an all-in-one training activity, as illustrated in the first step of Figure 1b. In Scenario 1, all training activities are the same type of application and all clients support all training activities. Since training activities A = {α1, α2, . . . , αn} are of the same type, e.g., CV or NLP, models W = {ω1, ω2, . . . , ωn} could share the same backbone (they share the same encoder but could have different decoders). Thus, we can consolidate A into an all-in-one training activity α0 that trains a multi-task model ν = {θs} ∪ {θαi|αi ∈ A}, where θs is the shared model parameters and θαi is the specific parameters for training activity αi ∈ A. The loss function for all-in-one training is L(X , θs, {θαi}) = (cid:80) Activity consolidation leverages synergies among training activities and effectively reduces the computation cost of multi-tenant FL from multiple trainings into a single training. However, simply employing activity consolidation is another extreme of multi-tenant FL that only considers synergies among activities. As shown in Figure 2, all-in-one method is efficient in energy consumption, but it leads to unsatisfactory performance. Consequently, we further propose activity splitting to consider both synergies and differences among training activities.\n\nαi∈A Lαi(X , θs, {θαi}).\n\n4.2 ACTIVITY SPLITTING\n\nWe propose activity splitting to divide the all-in-one activity α0 into multiple groups after it is trained for certain rounds. Essentially, we aim to split A = {α1, α2, . . . , αn} into multiple nonoverlapping groups such that training activities within a group have better synergy. Let {A1, A2, . . . , Am} be subsets of A, we aim to find a disjoint set I of A, where I ⊆ {1, 2, . . . , m}, |I| ≤ |A|, (cid:83) j∈I Aj = A, and (cid:84) s} ∪ {θαi|αi ∈ Aj}, which is a multi-task network when Aj contains more than one training activity, where θj s is the shared model parameters and θαi is the specific parameters for training activity αi ∈ Aj. The core question is how to determine set I to split these activities considering their synergies and differences.\n\nj∈I Aj = ∅. Each group Aj trains a model νj = {θj\n\nInspired by TAG (Fifty et al., 2021) that measures task affinites for task grouping, we employ affinities between training activities for activity splitting via three stages: 1) we measure affinities among activities during all-in-one training; 2) we select the best combination of splitted training activities based on affinity scores; 3) we continue training each split with its model initialized with parameters obtained from all-in-one training. Particularly, during training of all-in-one activity α0, we measure the affinity of training activity αi onto αj at time step t in each client k with the following equation:\n\nS k,t\n\nαi→αj\n\n= 1 −\n\nLαj (X k,t, θk,t+1 Lαj (X k,t, θk,t\n\n, θk,t αj s , θk,t αj )\n\ns,αi\n\n)\n\n,\n\n(2)\n\nwhere Lαj is the loss function of αj, X k,t is a batch of training data, and θk,t shared model parameters before and after updated by αi, respectively. Positive value of S k,t\n\nand θk,t+1\n\ns,αi\n\ns\n\nare the\n\nαi→αj\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) A set of five training activities: sdnkt\n\n(b) A set of nine trianing activities: sdnkterca\n\nFigure 2: Comparison of test loss and energy consumption on two training activity sets: (a) sdnkt and (b) sdnkterca, where each character represents an activity. Compared with all-in-one methods, our method achieves much better performance with slight increases in computation. Moreover, our method achieves the best performance while consuming less energy than the other methods.\n\nmeans that activity αi helps reduce the loss of αj. This equation measures the affinity of one timestep of one client. We approximate affinity scores for each round by averaging the values over T time-steps in E local epochs and K selected clients: ˆSαi→αj = 1 αi→αj , where T is total time steps determined by the frequency f of calculating Equation 2, e.g., f = 5 means measuring the affinity in each client in every five batches.\n\nt=1 S k,t\n\n(cid:80)K\n\n(cid:80)E\n\n(cid:80)T\n\nKET\n\nk=1\n\ne=1\n\nThese affinity scores measure pair-wise affinities between traininig activities. We next use them to calculate total affinity scores of a grouping with (cid:80)n ˆSαi, where ˆSαi is the averaged affinity score onto each training activity. For example, in a grouping of two splits of five training activities {{α1, α2}, {α3, α4, α5}}, where {α1, α2} is one split and {α3, α4, α5} is another split. The affinity score onto α1 is ˆSα1 = ˆSα2→α1 and the affinity score onto α3 is ˆSα3 = ( ˆSα4→α3 + ˆSα5→α3 )/2. Consequently, we can find the set I with |I| elements for subsets of A that maximize (cid:80)n ˆSαi, where |I| defines the number of elements.\n\ni=1\n\ni=1\n\nWe would like to further highlight the differences between our method and TAG (Fifty et al., 2021). Firstly, TAG focuses on inference efficiency, thus it allows overlapping task grouping that could train one task multiple times. In contrast, our focus is fundamentally different; we focus on training efficiency and consider only nonoverlapping activity splitting. Secondly, TAG is computationintensive for higher numbers of splits, e.g., it fails to produce results of five splits of nine tasks in a week, whereas we only need seconds of computation. Thirdly, TAG rules out the possibility that a group contains only one task as it sets ˆSαi→αi = 1e−6, which is much smaller than scores of other groupings. Besides, ˆSαi→αi calculated from Equation 2 is also not desirable; it always results in a group containing only one task as its value is much larger (could be 10x larger) than scores of other groupings. To overcome these issues, we propose a new method to calculate this value:\n\nˆSαi→αi =\n\n(cid:88)\n\nj∈N \\{i}\n\n( ˆSαi→αj + ˆSαj→αi) 2n − 2\n\n,\n\n(3)\n\nwhere N = {1, 2, . . . , n}. The intuition is that it measures the normalized affinity of activity αi to other activities and other activities to αi. Fourthly, we focus on multi-tenant FL, thus, we further aggregate affinity scores over K selected clients. Fifthly, TAG trains each set Aj from scratch, whereas we initialize their models with the parameters obtained from all-in-one training.\n\n5 EXPERIMENTS\n\nWe evaluate the performance and resource usage of MuFL and design our experiments to answer the following questions: 1) How effective is our activity splitting approach? 2) When to split the training activities? 3) Is it beneficial to iteratively split the training activities? 4) What is the impact of local epoch and scaling up the number of selected clients in each training round?\n\n6\n\n48121620242832Energy(kWh)0.560.580.600.620.640.660.68TestLossOnebyoneAll-in-oneGradNormFedProxTAGHOAOursTAGHOAOursTAGHOAOurs2Splits3Splits4Splits68101214Energy(kWh)1.381.401.421.441.461.481.50TestLossOursOnebyoneAll-in-oneGradNormFedProxTAGOursTAGOursTAGOursBetter2Splits3Splits4Splits5SplitsUnder review as a conference paper at ICLR 2023\n\nTable 1: Performance (test loss) comparison of our method with the optimal and worst splits. Our method achieves the best performance, indicating the effectiveness of our activity splitting method.\n\nActivity Set\n\nsdnkt\n\nerckt\n\nSplits\n\n2 3\n2 3\n\nTrain from Scratch\n\nOurs\n\nOptimal 0.578 ± 0.015 0.622 ± 0.007 0.685 ± 0.010 0.555 ± 0.008 0.585 ± 0.026 0.674 ± 0.022 1.039 ± 0.024 1.070 ± 0.013 1.312 ± 0.065 1.015 ± 0.018 1.058 ± 0.029 1.243 ± 0.099\n\nWorst\n\nTrain from Initialization Optimal\n\nWorst\n\n0.595 ± 0.008 0.595 ± 0.004 0.560 ± 0.006 0.578 ± 0.006 1.048 ± 0.024 1.068 ± 0.037 1.020 ± 0.012 1.052 ± 0.026\n\nExperiment Setup We construct the Scenario 1 of multi-tenant FL scenarios using Taskonomy dataset (Zamir et al., 2018), which is a large computer vision dataset of indoor scenes of buildings. We run experiments with N = 32 clients, where each client contains a dataset of a building to simulate the statistical heterogeneity in FL. Three sets of training activities are used to evaluate the robustness of MuFL: sdnkt, erckt, and sdnkterca; each character represents an activity, e.g., s represents semantic segmentation. We measure the statistical performance of an activity set using the sum of test losses of individual activities. By default, we use K = 4 selected clients and E = 1 local epoch for each round of training. More experimental details are provided in Appendix B.\n\n5.1 PERFORMANCE EVALUATION\n\nWe compare the performance, in terms of test loss and energy consumption, among the following methods: 1) one by one training of activities (i.e., the vanilla multi-tenant FL); 2) all-in-one training of activities (i.e., using only activity consolidation); 3) all-in-one training with multi-task optimization (GradNorm (Chen et al., 2018)) and federated optimization (FedProx (Li et al., 2020)); 4) estimating higher-order of activity groupings from pair-wise activities performance (HOA (Standley et al., 2020)); 5) grouping training activities with only task affinity grouping method (TAG (Fifty et al., 2021)); 6) MuFL with both activity consolidation and activity splitting. Carbontracker (Anthony et al., 2020) is used to measure energy consumption and carbon footprint (provided in Appendix C).\n\nFigure 2 compares performance of the above methods on activity sets sdnkt and sdnkterca. The methods that achieve lower test loss and lower energy consumption are better. At the one extreme, all-in-one methods (including GradNorm) consumes the least energy, but their test losses are the highest. Simply applying federated optimization, FedProx, can hardly improve performance, especially on sdnkterca. At the other extreme, HOA achieves comparable test losses on three or four splits of sdnkt, but it demands high energy consumption (∼ 4 − 6× of ours) to compute pair-wise activities for higher-order estimation. Although training activities one by one and TAG present a good balance between test loss and energy consumption, MuFL is superior in both aspects; it achieves the best test loss with ∼40% and ∼50% less energy consumption on activity set sdnkt and sdnkterca, respectively. Additionally, more splits of activity in the activity splitting lead to higher energy consumption, but it could help further reduce test losses. We do not report HOA for activity set sdnkterca due to computation constraints.2 We omit to report the running time as it is hidden under the metric of energy consumption; higher energy consumption implies longer training time. We provide more details of these experiments and results of activity set erckt in Appendix C.\n\n5.2 HOW EFFECTIVE IS OUR ACTIVITY SPLITTING APPROACH?\n\nWe demonstrate the effectiveness of our activity splitting approach by comparing it with the possible optimal and worst splits. The optimal and worst splits are obtained with two steps: 1) we measure the performance over all combinations of two splits and three splits of an activity set by training them from scratch;3 2) we select the combination that yields the best performance as the optimal split and the worst performance as the worst split.\n\nTable 1 compares the test loss of MuFL with the optimal and worst splits trained in two ways: 1) training each split from scratch; 2) training each split the same way as our activity splitting — initializing models with the parameters obtained from all-in-one training. On the one hand, training\n\n2HOA computes at least 36 pairs of activities (∼720 GPU hours), consuming ∼12× more energy than MuFL. 3There are fifteen and twenty-five combinations of two and three splits, respectively, for a set of five activities.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Affinities to activity d\n\n(b) Affinities to activity r\n\n(c) Affinities to activity s\n\nFigure 3: Changes of affinity scores of one activity to the other on activity set sdnkterca. Activities d and r have high inter-activity scores. The trends of affinities emerge at the early stage of training.\n\n(a) Training activity set: sdnkt\n\n(b) Training activity set: erckt\n\n(c) Activity set: sdnkterca\n\nFigure 4: Performance comparison of training all-in-one activities for different R0 rounds. Fixing the total training rounds R = 100, our method achieves the best performance when R0 ∈ {20, 30, 40}.\n\nfrom initialization outperforms training from scratch in all settings. It suggests that initializing each split with all-in-one training model parameters can significantly improve the performance. On the other hand, our activity splitting method achieves the best performance in all settings, even though training from initialization reduces the gaps of different splits (the optimal and worst splits). These results indicate the effectiveness of our activity splitting approach.\n\n5.3 WHEN TO SPLIT TRAINING ACTIVITIES?\n\nWe further answer the question that how many R0 rounds should we train the all-in-one activity before activity splitting. It is determined by two factors: 1) the rounds needed to obtain affinity scores for a reasonable activity splitting; 2) the rounds that yield the best overall performance.\n\nAffinity Analysis We analyze changes in affinity scores over the course of training to show that early-stage affinity scores are acceptable for activity splitting. Figure 3 presents the affinity scores of different activities to one activity on activity set sdnkterca. Figure 3a and 3b indicate that activity d and activity r have high inter-activity affinity scores; they are divided into the same group as a result. In contrast, both d and r have high affinity score to activity s in Figure 3c, but not vice versa. These trends emerge in the early stage of training, thus, we employ the affinity scores of the tenth round for activity splitting by default; they are effective in achieving promising results as shown in Figure 2 and Table 1. We provide more affinity scores of other activities in Appendix C.\n\nThe Impact of R0 Rounds Figure 4 compares the performance of training R0 for 10 to 90 rounds before activity splitting. Fixing the total training round R = 100, we train each split of activities for R1 = R − R0 rounds. The results indicate that MuFL achieves the best performance when R0 = {20, 30, 40} rounds. Training the all-in-one activity for enough rounds helps utilize the benefits and synergies of training together, but training for too many rounds almost suppresses the benefits of considering differences among activities. We suggest training R0 for [20, 40] that strikes a good balance between these two extremes and consider other mechanisms to determine R0 in future works.\n\n5.4 HIERARCHICAL SPLITTING\n\nThis section evaluates an alternative activity splitting strategy. In activity splitting, we can divide the all-in-one training activity into {2, 3, . . . } splits. As shown in Figure 2, more splits lead to better performance with slightly higher energy consumption in the five-activity set, but the trend is not straightforward in the nine-activity set. Apart from setting the number of splits directly, MuFL can split the training activity into more splits adaptively via two steps: 1) dividing the all-in-one activity into two splits and training each one for R1 rounds; 2) further dividing one of them to two splits and train these three activities for R2 rounds. We term the adaptive process as hierarchical splitting.\n\n8\n\n020406080100TrainingRound0.000.641.281.922.56Affinitysnkertac020406080100TrainingRound0.000.340.681.021.36Affinitysdnketac020406080100TrainingRound0.000.040.080.120.16Affinitydnkertac102030405060708090All-in-oneRounds(R0)0.550.600.650.70TestLoss2Splits3Splits4Splits102030405060708090All-in-oneRounds(R0)0.981.021.061.101.14TestLoss2Splits3Splits4Splits102030405060708090All-in-oneRounds(R0)1.351.401.451.501.551.60TestLoss2Splits3Splits4Splits5SplitsUnder review as a conference paper at ICLR 2023\n\nTable 2: Performance of hierarchical splitting on three activity sets sdnkt, erckt, and sdnkterca. Hierarchical splitting outperforms two splits and achieves similar performance to three splits with less energy (kWh) consumption.\n\nMethod\n\nsdnkt\n\nerckt\n\nsdnkterca\n\nEnergy\n\nTest Loss\n\nEnergy\n\nTest Loss\n\nEnergy\n\nTest Loss\n\nTwo Splits 4.9 ± 0.3 0.578 ± 0.015 Three Splits 5.4 ± 0.7 0.555 ± 0.008 Hierarchical 5.3 ± 0.4 0.563 ± 0.007\n\n6.7 ± 0.2 1.039 ± 0.024 7.2 ± 0.2 1.015 ± 0.018 6.9 ± 0.2 1.022 ± 0.020\n\n6.0 ± 0.1 1.445 ± 0.021 6.6 ± 0.4 1.391 ± 0.030 6.5 ± 0.3 1.403 ± 0.024\n\nTable 2 compares the performance of hierarchical splitting (3 splits) with directly splitting to multiple splits on three activity sets. We use R0 = 30, R1 = 40, and R2 = 30 for activity sets sdnkt and erckt, and R0 = 30, R1 = 20, and R2 = 50 for sdnkterca. Hierarchical splitting effectively reduces test losses of two splits and achieves comparable performance to three splits with less energy consumption. These results suggest that hierarchical splitting can be an alternative method of activity splitting. Additionally, these results also demonstrate the possibility of other activity splitting strategies to be considered in future works.\n\n5.5 ADDITIONAL ANALYSIS\n\nThis section analyzes the impact of local epoch E and the number of selected clients K in FL using all-in-one training. We report the results of activity set sdnkt here and provide more results in Appendix C.\n\nImpact of Local Epoch E Local epoch defines the number of epochs each client trains before uploading training updates to the server. Figure 5a compares test losses of local epochs E = {1, 2, 5, 10}. Larger E could lead to better performance with higher computation (fixed training round R = 100), but it is not effective when increasing E = 5 to E = 10. It suggests the limitation of simply increasing computation with larger E in improving performance. Note that MuFL (Table 1) achieves better results than E = 5 with ∼ 5× less computation.\n\n(a) Impact of E\n\n(b) Impact of K\n\nFigure 5: Analysis of the impact of (a) local epoch E and (b) the number of selected clients K on activity set sdnkt. Larger E and K could reduce losses with more computation, but the benefit decreases as computation increases.\n\nImpact of The Number of Selected Clients K Figure 5b compares test losses of the number of selected clients K = {2, 4, 6, 8, 16} in each round. Increasing the number of selected clients improves the performance, but the effect becomes marginal as K increases. Larger K can also be considered as using more computation in each round. Similar to the results of the impact of E, simply increasing computation can only improve performance to a certain extent. It also shows the significance of MuFL that increases performance with slightly more computation. We use K = 4 by default for experiments and demonstrate that MuFL is also effective on K = 8 in Appendix C.\n\n6 CONCLUSIONS\n\nIn this work, we propose a smart multi-tenant federated learning system to effectively coordinate and execute multiple simultaneous FL training activities. In particular, we introduce activity consolidation and activity splitting to consider both synergies and differences among training activities. Extensive empirical studies demonstrate that our method is effective in elevating performance and significant in reducing energy consumption and carbon footprint by more than 40%, which are important metrics to our society. We believe that multi-tenant FL will emerge and empower many real-world applications with the fast development of FL. We hope this research will inspire the community to further work on algorithm and system optimizations of multi-tenant FL. Future work involves designing better scheduling mechanisms to coordinate training activities. Client selection strategies can also be considered to optimize resource and training allocation, and extend our optimization approaches to other multi-tenant FL scenarios.\n\n9\n\n12510E0.50.60.7TestLoss246816K0.60.70.80.9TestLossUnder review as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT\n\nTo facilitate reproducibility, we provide basic experimental setups in Section 5 and include more details about the dataset, implementation details, and hyperparameters in Appendix B. We also provide the algorithm of MuFL in Algorithm 1. Besides, the implementation codes will be open-sourced in the future.\n\nREFERENCES\n\nLasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. Carbontracker: Tracking and predicting the carbon footprint of training deep learning models. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems, July 2020. arXiv:2007.03051.\n\nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pp. 2938–2948. PMLR, 2020.\n\nHakan Bilen and Andrea Vedaldi. Integrated perception with recurrent multi-task neural networks.\n\nAdvances in neural information processing systems, 29, 2016.\n\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Koneˇcn`y, Stefano Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System design. Proceedings of Machine Learning and Systems, 1: 374–388, 2019.\n\nHong Cai, Berthold Reinwald, Ning Wang, and Chang Jie Guo. Saas multi-tenancy: Framework, technology, and case study. In Cloud Computing Advancements in Design, Implementation, and Technologies, pp. 67–82. IGI Global, 2013.\n\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n\nZheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie Baracaldo, Yi Zhou, Heiko Ludwig, Feng Yan, and Yue Cheng. Tifl: A tier-based federated learning system. In Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing, pp. 125–136, 2020.\n\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pp. 794–803. PMLR, 2018.\n\nFranc ̧ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of\n\nthe IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.\n\nFrederick Chong and Gianpaolo Carraro. Architecture strategies for catching the long tail. MSDN\n\nLibrary, Microsoft Corporation, 910, 2006.\n\nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers), pp. 845–850, 2015.\n\nDavid Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE international conference on computer vision, pp. 2650–2658, 2015.\n\nBiyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi-tenant on-device deep learning for continuous mobile vision. In Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, pp. 115–127, 2018.\n\nChristoph Fehling, Frank Leymann, and Ralph Mietzner. A framework for optimized distribution of tenants in cloud applications. In 2010 IEEE 3rd International Conference on Cloud Computing, pp. 252–259. IEEE, 2010.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nChris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nFLAIROP. Federated learning for robot picking, 2022. URL https://flairop.com/.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. Advances in Neural Information Processing Systems, 33:19586– 19597, 2020.\n\nPengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In\n\nInternational Conference on Machine Learning, pp. 3854–3863. PMLR, 2020.\n\nAndrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franc ̧oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo ́e Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.\n\nFlorian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim D Smith, and Ilana Segall. Federated\n\nlearning for ranking browser history suggestions. arXiv preprint arXiv:1911.11807, 2019.\n\nCheol-Ho Hong, Ivor Spence, and Dimitrios S Nikolopoulos. Gpu virtualization and scheduling\n\nmethods: A comprehensive survey. ACM Computing Surveys (CSUR), 50(3):1–37, 2017.\n\nSiyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, and Alexander Hauptmann. Gnas: A greedy neural architecture search method for multi-attribute learning. In Proceedings of the 26th ACM international conference on Multimedia, pp. 2049–2057, 2018.\n\nYangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora. Evaluating gradient inversion attacks and defenses in federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nJoel Janai, Fatma G ̈uney, Aseem Behl, Andreas Geiger, et al. Computer vision for autonomous vehicles: Problems, datasets and state of the art. Foundations and Trends R(cid:13) in Computer Graphics and Vision, 12(1–3):1–308, 2020.\n\nMyeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, Junjie Qian, Wencong Xiao, and Fan Yang. Analysis of Large-Scale Multi-Tenant GPU clusters for DNN training workloads. In 2019 USENIX Annual Technical Conference (USENIX ATC 19), pp. 947–960, Renton, WA, July 2019. USENIX Association. ISBN 978-1-939133-03-8. URL https://www.usenix.org/ conference/atc19/presentation/jeon.\n\nAngela H Jiang, Daniel L-K Wong, Christopher Canel, Lilia Tang, Ishan Misra, Michael Kaminsky, Michael A Kozuch, Padmanabhan Pillai, David G Andersen, and Gregory R Ganger. Mainstream: Dynamic {Stem-Sharing} for {Multi-Tenant} video processing. In 2018 USENIX Annual Technical Conference (USENIX ATC 18), pp. 29–42, 2018.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends R(cid:13) in Machine Learning, 14(1–2):1–210, 2021.\n\nZhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature\n\nlearning. In ICML, 2011.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.\n\nAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7482–7491, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJakub Koneˇcn ́y, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. In NIPS Workshop on Private Multi-Party Machine Learning, 2016. URL https://arxiv.org/abs/ 1610.05492.\n\nAbhishek Kumar and Hal Daum ́e. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 1723–1730, 2012.\n\nChonLam Lao, Yanfang Le, Kshiteej Mahajan, Yixi Chen, Wenfei Wu, Aditya Akella, and Michael Swift. ATP: In-network aggregation for multi-tenant learning. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), pp. 741–761. USENIX Association, April 2021. ISBN 978-1-939133-21-2. URL https://www.usenix.org/conference/ nsdi21/presentation/lao.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020.\n\nWenqi Li, Fausto Milletar`ı, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, S ́ebastien Ourselin, M Jorge Cardoso, et al. Privacy-preserving federated brain tumour segmentation. In International Workshop on Machine Learning in Medical Imaging, pp. 133–141. Springer, 2019.\n\nJie Liu, Jiawen Liu, Wan Du, and Dong Li. Performance analysis and characterization of training deep learning models on mobile device. In 2019 IEEE 25th International Conference on Parallel and Distributed Systems (ICPADS), pp. 506–515. IEEE, 2019.\n\nYongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5334–5343, 2017.\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\nRalph Mietzner, Frank Leymann, and Mike P Papazoglou. Defining composite configurable saas application packages using sca, variability descriptors and multi-tenancy patterns. In 2008 third international conference on Internet and web applications and services, pp. 156–161. IEEE, 2008.\n\nLorenzo Minto, Moritz Haller, Benjamin Livshits, and Hamed Haddadi. Stronger privacy for federated collaborative filtering with implicit feedback. In Fifteenth ACM Conference on Recommender Systems, pp. 342–350, 2021.\n\nIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for In Proceedings of the IEEE conference on computer vision and pattern\n\nmulti-task learning. recognition, pp. 3994–4003, 2016.\n\nVladimir Nekrasov, Thanuja Dharmasiri, Andrew Spek, Tom Drummond, Chunhua Shen, and Ian Reid. Real-time joint semantic segmentation and depth estimation using asymmetric annotations. In 2019 International Conference on Robotics and Automation (ICRA), pp. 7101–7107. IEEE, 2019.\n\nXiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Jianwei Huang, and Guoliang Xing. Clusterfl: a similarity-aware federated learning system for human activity recognition. In Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services, pp. 54–66, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nJason Posner, Lewis Tseng, Moayad Aloqaily, and Yaser Jararweh. Federated learning in vehicular\n\nnetworks: opportunities and solutions. IEEE Network, 35(2):152–159, 2021.\n\nDavid Qiu, Yanzhang He, Qiujia Li, Yu Zhang, Liangliang Cao, and Ian McGraw. Multi-task learning for end-to-end asr word and utterance confidence with deletion prediction. arXiv preprint arXiv:2104.12870, 2021.\n\nSwaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Franc ̧oise Beaufays. Federated learning\n\nfor emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019.\n\nMicah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas. Multiinstitutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In International MICCAI Brainlesion Workshop, pp. 92–104. Springer, 2018.\n\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task\n\nlearning. Advances in neural information processing systems, 30, 2017.\n\nTrevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In International Conference on Machine Learning, pp. 9120–9132. PMLR, 2020.\n\nXimeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning what to share for efficient deep multi-task learning. Advances in Neural Information Processing Systems, 33: 8728–8740, 2020.\n\nSebastian Thrun. Is learning the n-th thing any easier than learning the first? Advances in neural\n\ninformation processing systems, 8, 1995.\n\nSimon Vandenhende, Stamatios Georgoulis, Bert De Brabandere, and Luc Van Gool. Branched\n\nmulti-task networks: deciding what layers to share. arXiv preprint arXiv:1904.02920, 2019.\n\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2020a. URL https://openreview.net/forum?id=BkluqlSFDS.\n\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481, 2020b.\n\nChengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu, and Xuanzhe Liu. Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data. In Proceedings of the Web Conference 2021, pp. 935–946, 2021.\n\nTimothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Franc ̧oise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903, 2018.\n\nJianbo Ye\n\nand Arnie\n\nSen.\n\nchanging home? how-does-astro-localize-itself-in-an-ever-changing-home.\n\n2022.\n\nHow does\n\neverURL https://www.amazon.science/blog/\n\nlocalize\n\nitself\n\nastro\n\nan\n\nin\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33: 5824–5836, 2020.\n\nyuyang deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\n\nlearning, 2021. URL https://openreview.net/forum?id=g0a-XYjpQ7r.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nHongyi Zhang, Jan Bosch, and Helena Holmstr ̈om Olsson. End-to-end federated learning for autonomous driving vehicles. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2021a.\n\nJie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. Parameterized knowledge transfer for personalized federated learning. Advances in Neural Information Processing Systems, 34, 2021b.\n\nYu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and\n\nData Engineering, 2021.\n\nHanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau, Yuqi Wang, Yifan Xiong, and Bin Wang. HiveD: Sharing a GPU clusIn 14th USENIX Symposium on Operating Systems ter for deep learning with guarantees. Design and Implementation (OSDI 20), pp. 515–532. USENIX Association, November 2020. ISBN 978-1-939133-19-9. URL https://www.usenix.org/conference/osdi20/ presentation/zhao-hanyu.\n\nXiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu. A modulation module In Proceedings of the European\n\nfor multi-task learning with applications in image retrieval. Conference on Computer Vision (ECCV), pp. 401–416, 2018.\n\nLigeng Zhu, Hongzhou Lin, Yao Lu, Yujun Lin, and Song Han. Delayed gradient averaging: Tolerate the communication latency for federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nWeiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin, Dongzhan Zhou, Shuai Zhang, and Shuai Yi. Performance optimization of federated person re-identification via benchmark analysis. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 955–963, 2020.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA MULTI-TENANT FL SCENARIOS\n\nWe introduce four multi-tenant federated learning scenarios in Section 3. Figure 6 depicts these four scenarios with variances in two aspects: 1) whether all training activities are the same type of application, e.g., CV applications; 2) whether all clients support all training activities.\n\nFigure 6: Illustration of the four multi-tenant FL scenarios.\n\nOur proposed approach, MuFL, focuses on optimizing the performance on Scenario 1. Optimizing Scenario 1 can potentially empower plenty of real-world applications. For example, autonomous vehicles relate to multiple computer vision (CV) tasks (Janai et al., 2020), including object detection, tracking, and semantic segmentation; smart city surveillance cameras associate with various CV tasks, such as crowd counting, object detection, and person re-identification (Zhuang et al., 2020); voice assistant applications like Apple Siri and Google Assistant need multiple automatic speech recognition (ASR) tasks, including word confidence, word deletion, and utterance confidence (Qiu et al., 2021); household robots like Amazon Astro need to perform multiple CV tasks such as visual odometry tracking, loop-closure detection, and object detection (Ye & Sen, 2022); smartmanufacturing robots need several CV tasks such as object detection, object grasp point detection, and object pose estimation (FLAIROP, 2022).\n\nB EXPERIMENTAL DETAILS\n\nThis section provides more experimental information, including dataset, implementation details, and computation resources used.\n\nDataset We run experiments using Taskonomy dataset (Zamir et al., 2018), which is a large computer vision (CV) dataset of indoor scenes of buildings. To facilitate reproducibility and mitigate computational requirements, we use the tiny split of Taskonomy dataset,4 whose size is around 445GB. We select nine CV applications to form three sets of training activities: sdnkt, erckt, sdnkterca. These nine actvities are also used in (Standley et al., 2020). Figure 8 provides sample images of these nine training activities, as well as the representation of each character.5 In particular, we employ indoor images\n\nFigure 7: The data amount distribution of each training activity over 32 clients.\n\n4Taskonomy dataset is released under MIT license and can be downloaded from their official repository\n\nhttps://github.com/StanfordVL/taskonomy.\n\n5The meaning of each character in sdnkterca are as follows; s: semantic segmentation, d: depth estimation, n: normals, k: keypoint, t: edge texture, e: edge occlusion, r: reshaping, c: principle curvature, a: auto-encoder.\n\n15\n\nExample: Google KeyboardSame Type ofApplicationsClients supportall activitiesNot all clientssupport all activitiesDifferent Typesof ApplicationsClients: CarsType: Computer vision Training activities: e.g., objectdetection, semantic segmentationClients: Cameras Type: Computer vision Training activities: e.g., open spotscounting, duration trackingClients: Smartphones Type: Recommendation, NLP Training activities: e.g., querysuggestion, next-word predictionExample: Autonomous VehicleExample: Surveillance Camerasin Parking LotsExample: BrowsersClients: Browsers Type: Recommendation, Ranking Training activities: e.g., news/adsrecommendation, rank history0102030ClientID040008000120001600020000DataAmountUnder review as a conference paper at ICLR 2023\n\n(a) Input Image\n\n(b) s: Segmentation\n\n(c) d: Depth Estimation\n\n(d) n: Surface Normals\n\n(e) k: Keypoint\n\n(f) t: Edge Texture\n\n(g) e: Edge Occlusion\n\n(h) r: Reshasing\n\n(i) c: Principle Curvature\n\n(j) a: Auto-encoder\n\nFigure 8: Sample images of nine training activities corresponding to the input image.\n\nFigure 9: Sample images of five clients, where each client contains indoor scenes of a building. These indoor images differ in design, layout, objects, and illumination.\n\nof 32 buildings 6 as the total number of clients N = 32; each client contains images of a building to simulate the statistical heterogeneity. On the one hand, clients have different sizes of data. Figure 7 shows the distribution of dataset sizes of an activity of clients. On the other hand, Figure 9 shows sample images of five clients; their indoor scenes vary in design, layout, objects, and illumination.\n\nImplementation Details We implement multi-tenant FL systems in Python using PyTorch (Paszke et al., 2017). We simulate the FL training on a cluster of NVIDIA Tesla V100 GPUs, where each node in the cluster contains 8 GPUs. In each round, each selected client is allocated to a GPU to conduct training; these clients communicate via the NCCL backend. Besides, we employ FedAvg (McMahan et al., 2017) for the server aggregation. By default, we randomly select K = 4 clients to train for E = 1 local epochs in each round and train for R = 100 rounds.\n\nWe reference the implementation of multi-task learning from (Standley et al., 2020)’s official repository 7 for all-in-one training and training of each split after activity splitting. Particularly, the network architecture contains an encoder θs and multiple decoders θαi; one decoder for a training activity αi. Figure 10 illustrates the network architectures of training activities before and after activity consolidation and activity splitting. We use the modified Xception Network (Chollet, 2017) as the encoder for activity sets sdnkt and erckt and half size of the network (half amount of parameters) for activity set sdnkterca. The decoders contain four deconvolution layers and four convolution layers. Each training activity contains a loss function. Specifically, semnatic segmentation s uses Cross Entropy loss; surface normals and depth estimation use rotation loss based on L1 loss; keypoint detection, edge occlusion, edge texture, auto encoder, and principle curvature use L1 loss. We refer\n\n6The name of the buildings are allensville, beechwood, benevolence, coffeen, collierville, corozal, cosmos, darden, forkland, hanson, hiteman, ihlen, klickitat, lakeville, leonardo, lindenwood, markleeville, marstons, mcdade, merom, mifflinburg, muleshoe, newfields, noxapater, onaga, pinesdale, pomaria, ranchester, shelbyville, stockman, tolstoy, and uvalda.\n\n7https://github.com/tstandley/taskgrouping\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Illustration of network architectures of training activities in MuFL. Initially, each activity employs an encoder and a decoder. Activity consolidation consolidates these training activities into an all-in-one activity with multi-task architecture. After activity splitting, MuFL divides the all-in-one activity into multiple training activities, each contains an encoder and one or multiple decoders.\n\nimplementation of loss functions from (Standley et al., 2020) 8. The batch size is B = 64 for sdnkt and erckt and B = 32 for sdnkterca. These are the maximum batch sizes for one GPU without out-of-memory issues. In addition, we use polynomial learning rate decay (1 − r R )0.9 to update learning rate in each round with initial learning rate η = 0.1, where r is the number of trained rounds and R = 100 is the default total training rounds. The optimizer is stochastic gradient descent (SGD), with momentum of 0.9 and weight decay 1e−4.\n\nImplementation of Compared Methods We tune the hyperparameter μ = 0.004 for the proximal term in FedProx (Li et al., 2020). GradNorm (Chen et al., 2018) implementation is adopted from (Standley et al., 2020; Fifty et al., 2021) with default α = 1.5 and TAG (Fifty et al., 2021) implementation is adopted from their official repository 9. Next, we provide the details of how we compute the results of HOA (Standley et al., 2020) and TAG (Fifty et al., 2021).\n\nHOA (Standley et al., 2020) needs to compute test losses for individual activities and pair-wise activity combinations for R = 100 rounds. After that, we use these results to estimate test losses of higher-order combinations following (Standley et al., 2020). We then compute the actual test losses for the optimal activity splits that have the lowest test losses by training them from scratch. For example, for activity set sdnkt, we compute s, d, n, k, t and ten pair-wise activity combinations. Then, we use these results to estimate test losses of higher-order combinations.\n\nTAG (Fifty et al., 2021) first computes all-in-one training for R = 100 rounds to obtain the pair-wise affinities. Then, it uses a network selection algorithm to group these activities. After that, we train each group of activities from scratch for R = 100 rounds to obtain test losses. The best result is reported for overlapping activities. For example, {sd, dn, kt} is the best result of three splits of TAG on activity set sdnkt. Then, each split is trained from scratch to obtain test losses.\n\nComputation Resources Experiments in this work take approximately 27,765 GPU hours of NVIDIA Tesla V100 GPU for training. We conduct three independent runs of experiments for the majority of empirical studies. In each run, activity set sdnkt takes around 2,330 GPU hours, erckt takes around 3,280 GPU hours, and sdnkterca takes around 3,645 GPU hours. These include experiments of compared methods and ablation studies, whereas these do not include the GPU hours for validation and testing. It takes around the same GPU hours as training when we validate the model after each training round.\n\nC ADDITIONAL EXPERIMENTAL EVALUATION\n\nThis section provides more experimental results, including comprehensive results of performance evaluation and additional ablation studies.\n\n8https://github.com/tstandley/taskgrouping/blob/master/taskonomy_losses.\n\npy\n\n9https://github.com/google-research/google-research/tree/master/tag\n\n17\n\n(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:36)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:36)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:37)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:38)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:37)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:38)(cid:36)(cid:79)(cid:79)(cid:16)(cid:76)(cid:81)(cid:16)(cid:50)(cid:81)(cid:72)(cid:3)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:37)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:38)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:3)(cid:36)(cid:10)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:37)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:38)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:38)(cid:82)(cid:81)(cid:86)(cid:82)(cid:79)(cid:76)(cid:71)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:54)(cid:83)(cid:79)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:37)(cid:10)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:36)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:36)Under review as a conference paper at ICLR 2023\n\nTable 3: Comparison of test loss, energy consumption, and carbon footprint on activity set sdnkt.\n\nMethod Splits Energy (kWh) CO2eq (g)\n\nTotal Loss\n\ns\n\nd\n\nn\n\nk\n\nt\n\n- One by one -\nAll-in-one GradNorm -\n\n8.4 ± 0.1 3.7 ± 0.1 4.1 ± 0.4\n\n2465 ± 39 0.603 ± 0.030 0.086 ± 0.005 0.261 ± 0.023 0.107 ± 0.001 0.107 ± 0.003 0.043 ± 0.002 1086 ± 28 0.677 ± 0.018 0.087 ± 0.002 0.246 ± 0.010 0.136 ± 0.001 0.126 ± 0.019 0.083 ± 0.008 1200 ± 122 0.691 ± 0.013 0.092 ± 0.001 0.251 ± 0.012 0.138 ± 0.003 0.118 ± 0.007 0.093 ± 0.019\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\n2 2\n2\n\n3 3\n3\n\n4 4\n4\n\n31.0 ± 0.5 9.8 ± 0.3 4.9 ± 0.3\n\n9125 ± 140 0.651 ± 0.029 0.091 ± 0.011 0.245 ± 0.002 0.135 ± 0.000 0.107 ± 0.003 0.074 ± 0.023 2876 ± 88 0.624 ± 0.015 0.083 ± 0.004 0.242 ± 0.005 0.134 ± 0.001 0.110 ± 0.007 0.055 ± 0.006 1431 ± 94 0.578 ± 0.015 0.069 ± 0.006 0.231 ± 0.006 0.124 ± 0.002 0.102 ± 0.003 0.052 ± 0.003\n\n31.0 ± 0.5 11.3 ± 0.2 5.4 ± 0.3\n\n9125 ± 140 0.598 ± 0.029 0.083 ± 0.022 0.239 ± 0.007 0.127 ± 0.008 0.107 ± 0.003 0.043 ± 0.002 3313 ± 56 0.613 ± 0.032 0.094 ± 0.005 0.233 ± 0.002 0.122 ± 0.013 0.110 ± 0.008 0.055 ± 0.008 1589 ± 94 0.555 ± 0.015 0.072 ± 0.006 0.222 ± 0.006 0.124 ± 0.002 0.095 ± 0.003 0.042 ± 0.003\n\n31.0 ± 0.5 13.7 ± 0.3 6.7 ± 0.3\n\n9125 ± 140 0.597 ± 0.015 0.094 ± 0.009 0.238 ± 0.002 0.115 ± 0.014 0.107 ± 0.003 0.043 ± 0.002 4016 ± 80 0.603 ± 0.027 0.083 ± 0.005 0.233 ± 0.002 0.122 ± 0.013 0.110 ± 0.008 0.055 ± 0.008 1969 ± 75 0.548 ± 0.001 0.070 ± 0.002 0.230 ± 0.008 0.111 ± 0.000 0.095 ± 0.007 0.042 ± 0.001\n\nTable 4: Comparison of test loss, energy consumption, and carbon footprint on activity set erckt.\n\nMethod Splits Energy (kWh) CO2eq (g)\n\nTotal Loss\n\ne\n\nr\n\nc\n\nk\n\nt\n\n- One by one All-in-one -\nGradNorm -\n\n11.1 ± 2.2 5.0 ± 0.3 5.0 ± 0.2\n\n3277 ± 660 1.055 ± 0.034 0.148 ± 0.000 0.371 ± 0.029 0.386 ± 0.006 0.107 ± 0.003 0.043 ± 0.002 1478 ± 84 1.130 ± 0.022 0.146 ± 0.001 0.379 ± 0.019 0.393 ± 0.002 0.110 ± 0.003 0.079 ± 0.013 1462 ± 70 1.154 ± 0.055 0.147 ± 0.002 0.381 ± 0.015 0.394 ± 0.001 0.149 ± 0.062 0.082 ± 0.005\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\n2 2\n2\n\n3 3\n3\n\n4 4\n4\n\n38.3 ± 0.3 14.0 ± 0.9 6.7 ± 0.2\n\n11265 ± 86 1.082 ± 0.032 0.149 ± 0.003 0.365 ± 0.025 0.394 ± 0.002 0.109 ± 0.002 0.064 ± 0.022 4119 ± 279 1.095 ± 0.033 0.147 ± 0.002 0.379 ± 0.013 0.393 ± 0.000 0.108 ± 0.005 0.068 ± 0.015 1957 ± 53 1.039 ± 0.024 0.143 ± 0.001 0.343 ± 0.014 0.393 ± 0.001 0.104 ± 0.006 0.056 ± 0.007\n\n38.3 ± 0.2 14.4 ± 0.6 7.2 ± 0.2\n\n11265 ± 53 1.062 ± 0.024 0.149 ± 0.001 0.365 ± 0.014 0.394 ± 0.001 0.109 ± 0.006 0.046 ± 0.007 4242 ± 170 1.091 ± 0.034 0.147 ± 0.002 0.388 ± 0.014 0.396 ± 0.002 0.109 ± 0.009 0.050 ± 0.011 2108 ± 50 1.015 ± 0.018 0.143 ± 0.000 0.336 ± 0.005 0.383 ± 0.001 0.102 ± 0.008 0.052 ± 0.009\n\n38.3 ± 0.3 17.4 ± 0.5 7.6 ± 0.0\n\n11265 ± 86 1.053 ± 0.034 0.148 ± 0.002 0.369 ± 0.028 0.386 ± 0.006 0.105 ± 0.001 0.045 ± 0.003 5114 ± 159 1.087 ± 0.028 0.147 ± 0.002 0.384 ± 0.011 0.396 ± 0.002 0.109 ± 0.009 0.050 ± 0.011 2229 ± 14 1.002 ± 0.014 0.143 ± 0.000 0.336 ± 0.005 0.383 ± 0.001 0.094 ± 0.009 0.046 ± 0.004\n\nC.1 PERFORMANCE EVALUATION\n\nTable 3 and 5 provide comprehensive comparison of different methods on test loss and energy consumption on activity sets sdnkt and sdnkterca, respectively. They complement the results in Figure 2. Besides, Table 4 and Figure 11 compares these methods on activity set erckt. The results on erckt is similar to results on the other activity sets; our method achieves the best performance with around 40% less energy consumption than the one-by-one method and with slightly more energy consumption than all-in-one methods.\n\nFigure 11: Compare test loss and energy consumption on activity set erckt.\n\nAdditionally, Table 3, 4 and 5 also provide carbon footprints (CO2eq) of different methods. The carbon footprints are estimated using Carbontracker (Anthony et al., 2020).10 Our method reduces around 40% on carbon footprints on these three activity sets compared with one-by-one training; it reduces 1526gCO2eq or equivalent to traveling 12.68km by car on sdnkterca. The reduction is even more significant when compared with TAG and HOA. Although we run experiments using Tesla V100 GPU, the relative results of energy and carbon footprint among different methods should be representative of the scenarios of edge devices.\n\nC.2 ADDITIONAL ANALYSIS AND ABLATION STUDIES\n\nThis section presents additional analysis of MuFL and provides additional ablation studies.\n\n10Carbon intensity of a training varies over geographical regions according to (Anthony et al., 2020). We use the national level (the United Kingdom as the default setting of the tool) of carbon intensity for a fair comparison across different methods. These carbon footprints serve as a proxy for evaluation of the actual carbon emissions.\n\n18\n\n468101214161820222426283032343638Energy(kWh)1.001.021.041.061.081.101.121.141.16TestLossOnebyoneAll-in-oneGradNormFedProxTAGHOAOursTAGHOAOursTAGHOAOurs2Splits3Splits4SplitsUnder review as a conference paper at ICLR 2023\n\nTable 5: Comparison of test loss, energy consumption, and carbon footprint on sdnkterca.\n\nMethod Splits Energy CO2eq (g) Total Loss\n\ns\n\nd\n\nn\n\nk\n\nt\n\ne\n\nr\n\nc\n\na\n\n- One by one All-in-one -\nGradNorm -\n\n11.9±0.5 3512±1511.46±0.0110.08±0.0090.24±0.0140.10±0.0010.10±0.0020.04±0.003 0.15±0.001 0.35±0.0110.38±0.0020.02±0.000 4.9±0.2 1435±60 1.49±0.0250.09±0.0020.23±0.0090.13±0.0020.10±0.0020.07±0.005 0.14±0.001 0.33±0.0110.39±0.0010.02±0.001 5.3±1.3 1561±3771.50±0.0490.08±0.0040.24±0.0140.13±0.0030.10±0.0030.07±0.011 0.14±0.001 0.34±0.0180.39±0.0010.02±0.001\n\nTAG MuFL\n\nTAG MuFL\n\nTAG MuFL\n\n2 14.7±0.8 4317±2291.49±0.0250.09±0.0020.23±0.0080.13±0.0020.10±0.0020.07±0.005 0.14±0.001 0.33±0.0110.39±0.0010.02±0.001 6.0±0.1 1986±1081.45±0.0210.08±0.0030.22±0.0080.12±0.0010.10±0.0010.06±0.004 0.14±0.000 0.32±0.0110.39±0.0010.02±0.001 2\n\n3 16.5±2.6 4854±7511.44±0.0140.09±0.0060.23±0.0090.12±0.0010.10±0.0020.03±0.004 0.14±0.000 0.33±0.0090.39±0.0010.02±0.000 6.6±0.4 1955±1041.39±0.0300.07±0.0050.22±0.0080.12±0.0020.08±0.0020.05±0.003 0.14±0.001 0.32±0.0110.38±0.0010.02±0.000 3\n\n4 15.8±2.4 4639±7171.44±0.0070.07±0.0030.24±0.0020.11±0.0010.10±0.0020.03±0.004 0.14±0.000 0.35±0.0030.39±0.0010.02±0.000 7.5±0.3 2201±94 1.40±0.0270.06±0.0040.22±0.0080.12±0.0030.08±0.0020.05±0.001 0.14±0.001 0.32±0.0110.39±0.0010.02±0.001 4\n\nMuFL\n\n5\n\n8.3±0.4 2439±1051.40±0.0280.06±0.0040.22±0.0080.12±0.0030.08±0.0020.05±0.000 0.14±0.002 0.32±0.0110.39±0.0010.02±0.001\n\n(a) Activity set sdnkt\n\n(b) Activity set erckt\n\n(c) Activity set sdnkterca\n\nFigure 12: Changes of validation loss over the course of training on activity sets: (a) sdnkt, (b) erckt, and (c) sdnkterca. Validation loss converges as training proceeds.\n\nTable 6: Activity splitting results of TAG (Fifty et al., 2021) and MuFL on activity sets sdnkt, erckt, and sdnkterca. Activities of each split is separated by a comma.\n\nMethod\n\nActivity Set\n\nTwo Splits\n\nThree Splits\n\nFour Splits\n\nFive Splits\n\nTAG MuFL\n\nTAG MuFL\n\nTAG MuFL\n\nsdnkt sdnkt\n\nerckt erckt\n\nsdn,kt sdn,kt\n\ner,rckt er,ckt\n\nsd,dn,kt sdn,k,t\n\ner,kt,rc er,c,kt\n\nsd,sdn,dn,kt sd,n,k,t\n\ner,kt,rc,rt er,c,k,t\n\n- s,d,n,k,t\n\n- e,r,c,k,t\n\nsdnkterca sdnkterca\n\nsdnkterca,dr snkteac,dr\n\nsdnerc,dr,kta snec,dr,kta\n\nsc,dr,ne,kta sn,dr,ka,etc\n\n- sn,dr,ka,e,tc\n\nChanges of Vadiation Loss Figure 12 presents validation losses over the course of all-in-one training of three training activity sets sdnkt, erckt, and sdnkterca. It shows that validation losses converge as training proceeds.\n\nSplitting Results of Various Methods We provide results of activity splitting of TAG (Fifty et al., 2021) and MuFL in Table 6. For hierarchical splitting, they further split into three splits from the results of two splits. In particular, the results of hierarchical splitting of erckt and sdnkterca are the same as their three splits. The hierarchical splitting result of sdnkt is from {sdn,kt} to {sd,n,kt} as the hierarchical splitting further divides the split with more training activities (sdn).\n\nBesides, Table 7 presents the splitting results of the optimal and worst splits. They are not identical due to variances in multiple runs of experiments. We report the mean and standard deviation of test losses of the optimal splits and the worst splits in Table 1. The large variances of the optimal and worst splits suggest the instability of splitting by measuring the performances of training from scratch in the FL settings and show the advantage of our methods in obtaining stable splits.\n\nDataset Size and Performance The dataset size of activity set sdnkt is around 315GB in our experiments, compared to 2.4TB of dataset used in experiments of TAG (Fifty et al., 2021). The test loss of ours (0.512 in Table 8), however, is better than the optimal one in TAG (Fifty et al., 2021) (0.5246). This back-of-the-envelope comparison indicates the potential to extend our approaches to multi-task learning. Besides, it could also suggest that our data size is sufficient for evaluation.\n\n19\n\n020406080100TrainingRound0.641.422.202.983.76Validationloss020406080100TrainingRound1.081.481.882.282.68Validationloss020406080100TrainingRound1.442.543.644.745.84ValidationlossUnder review as a conference paper at ICLR 2023\n\nTable 7: Results of the optimal and worst splits in three runs of experiments. They are not identical due to variances in three runs of experiments.\n\nActivity Set\n\nSplits\n\nOptimal Splits\n\nWorst Splits\n\nsdnkt\n\nerckt\n\n2 3\n\n2 3\n\ndk,snt t,sn,dk\n\nr,eckt r,ec,kt\n\nsn,dkt k,t,sdn\n\nt,erck r,t,eck\n\nnt,sdk d,sn,kt\n\net,rck r,ec,kt\n\nst,dnk d,st,nk\n\nrk,ect c,e,rk\n\nst,dnk d,st,nk\n\nek,rct e,k,rct\n\nst,dnk s,dt,nk\n\ne,rckt e,rt,ck\n\n(a) Affinities to activity s\n\n(b) Affinities to activity d\n\n(c) Affinities to activity n\n\n(d) Affinities to activity k\n\n(e) Affinities to activity t\n\n(f) Affinities to activity e\n\n(g) Affinities to activity r\n\n(h) Affinities to activity c\n\n(i) Affinities to activity a\n\nFigure 13: Changes of affinity scores of one activity to the other over the course of training on activity set sdnkterca. The trends of affinities emerge at the early stage of training.\n\nImpact of Affinity Computation Frequency f The frequency of computing affinities in Equation 2 determines the amount of extra needed computation. We use f = 5 and compute affinities for the first ten rounds for all experiments because the trend of affinities emerges in the early stage of training in Figure 13. It would increase the computation of all-in-one training by around 2%, which is already factored into the energy consumption computation in previous experiments. The results in Table 3, 4, and 5 show that MuFL is effective with this setting and the amount of computation is acceptable.\n\nImpact of Local Epoch Figure 14a show the impact of local epoch E on activity sets sdnkt, erckt, and sdnkterca. They complement results of activity set sdnkt in Figure 5a. Larger E could lead to better performance with fixed R = 100. It is especially effective when increasing E = 1 to E = 2, but further increasing E could degrade the performance. It indicates that simply increasing computation has limited capability to improve performance.\n\nImpact of The Number of Selected Clients Figure 14b compares the performance of different numbers of selected clients K = {2, 4, 6, 8, 16} on three activity sets sdnkt, erckt, and sdnkterca. It complements results in Figure 5b. The results on three activity sets are similar; increasing K reduces losses, but the marginal benefit decreases as K increases.\n\nThe majority of experiments in this study are conducted with K = 4. We next analyze the impact of K in MuFL with results of two splits on activity set sdnkt in Table 8. The results indicate that MuFL is still effective with K = 8, which outperforms K = 4 and all-in-one training.\n\nStandalone Training Standalone training refers to training using data of each client independently. Figure 15a shows the test loss distribution of thirty-two clients used in experiments. The client ID corresponds to the dataset size distribution in Figure 7. These results suggest that larger data sizes of\n\n20\n\n020406080100TrainingRound0.000.040.080.120.16Affinitydnkertac020406080100TrainingRound0.000.641.281.922.56Affinitysnkertac020406080100TrainingRound0.000.150.300.450.60Affinitysdkertac020406080100TrainingRound0.000.030.060.090.12Affinitysdnertac020406080100TrainingRound0.000.040.080.120.16Affinitysdnkerac020406080100TrainingRound0.000.040.080.120.16Affinitysdnkrtac020406080100TrainingRound0.000.340.681.021.36Affinitysdnketac020406080100TrainingRound0.000.020.040.06Affinitysdnkerta020406080100TrainingRound0.000.050.100.150.200.25AffinitysdnkertcUnder review as a conference paper at ICLR 2023\n\n(a) Impact of E\n\n(b) Impact of K\n\nFigure 14: Analysis of the impact of local epoch E and impact of the number of selected clients K. Larger E (with fixed R = 100) and K requires higher computation. They could reduce losses, but the marginal benefit decreases as computation increases.\n\nTable 8: Comparison of test loss using different numbers of selected clients K. MuFL achieves even better performance on K = 8.\n\nK Total Loss\n\ns\n\nd\n\nn\n\nk\n\nt\n\nAll-in-one All-in-one MuFL (two splits) MuFL (two splits)\n\n4 8\n4 8\n\n0.677 0.618 0.578 0.512\n\n0.087 0.076 0.069 0.060\n\n0.246 0.227 0.231 0.202\n\n0.136 0.130 0.124 0.117\n\n0.126 0.109 0.102 0.083\n\n0.083 0.077 0.052 0.048\n\nMethods Standalone All-in-one MuFL\n\nTest Loss 1.842 ± 0.248 0.677 ± 0.018 0.548 ± 0.001\n\n(a) Test loss distribution of standalone training\n\n(b) Test loss comparison\n\nFigure 15: Performance (test loss) of standalone training that conducts training using data in each client independently: (a) shows the test loss distribution of these thirty-two clients. (b) compares test losses of standalone training and FL methods. We run the experiments on activity set sdnkt.\n\nclients may not lead to higher performance. Figure 15b compares test losses of standalone training and federated learning methods. Either all-in-one or our MuFL greatly outperforms standalone training. It suggests the significance of federated learning when data are not sharable among clients.\n\n21\n\nsdnktercktsdnkterca0.40.81.21.6TestLossE=1E=2E=5E=10sdnktercktsdnkterca0.40.81.21.6TestLossK=2K=4K=6K=8K=160102030ClientID1.41.82.22.6TestLossUnder review as a conference paper at ICLR 2023\n\nD ALGORITHM\n\nAlgorithm 1 Our Proposed Smart Multi-tenant FL System (MuFL)\n\n1: Input: training activities A = {α1, α2, . . . , αn}, a set of available clients C, number of selected clients K, local epoch E, aggregation weight of client k pk, total training rounds R, all-in-one training rounds R0, the number of splits x, frequency of computing affinities f , batch size B\n\n2: Output: models W = {ω1, ω2, . . . , ωn} 3: 4: ServerExecution: 5: Consolidate A into α0 with a multi-task model ν0 = {θs} ∪ {θαi|αi ∈ A} (cid:46) Act. consolidation 6: Initialize ν0, i.e. initialize ωi = {θs} ∪ {θαi} for i ∈ N = {1, 2, . . . , n} 7: for each round r = 0, 1, ..., R0 − 1 do 8: 9: 10: 11: 12:\n\nCr ← (Randomly select K clients from C) for client k ∈ Cr in parallel do\n\nend for νr+1 ← (cid:80)\n\n← Client(νr, A, f )\n\nνr,k, ˆS r,k\n\nαi→αj\n\npkνr,k\n\n13:\n\nˆS r\n\nαi→αj\n\nk∈Cr ← 1 K\n\n(cid:80)\n\nk∈Cr\n\nˆS r,k\n\nαi→αj\n\n14: end for 15: Compute the values of S r (cid:46) Compute affinity scores 16: Compute a disjoint partition set I of activities A for x splits {Aj|j ∈ I} that maximizes S r αi (cid:46) Activity splitting (cid:46) Schedule to train sequentially\n\nαi→αi, ∀αi ∈ A, using Equation 3\n\nαi→αj , ∀αi ∈ A and ∀αj ∈ A\n\nInitialize νj = {θj for each round r = 0, 1, ..., R − R0 − 1 do\n\ns} ∪ {θαi|αi ∈ Aj} with parameters of νR0\n\nusing affinity scores S r 17: for each element j ∈ I do 18: 19: 20: 21:\n\nCr ← (Random select K from C) for client k ∈ Cr in parallel do , ←Client(νr j , Aj, 0)\n\nj\n\nνr,k end for j ← (cid:80) νr+1\n\nk∈Cr\n\npkνr,k\n\nj\n\n22: 23: 24:\n\nend for\n\nf (cid:99) if f (cid:54)= 0 else 0\n\n25: 26: end for 27: Reconstruct W = {ω1, ω2, . . . , ωn} from {νj|j ∈ I} by matching training activities 28: Return W 29: 30: Client (ν, A, f ): 31: T = (cid:98) B 32: for local epoch e = 1, ..., E do 33: 34: 35: 36: 37: end for 38: ˆSαi→αj = 1\n\nUpdate model parameters ν with respect to training activities A for each time-step t = 1, 2, ..., T (every f batches) do ∀αi ∈ A and ∀αj ∈ A, compute affinities of S t\n\nαi→αj , ∀αi ∈ A and ∀αj ∈ A\n\nαi→αj using Equation 2\n\nend for\n\nT (cid:80)\n\nE (cid:80)\n\nS t\n\nET\n\n39: Return ν, ˆSαi→αj\n\ne=1\n\nt=1\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThe paper presents a method to train multiple simultaneous activities on decentralized edge devices with budget constraints. Most prevailing approaches aim to do this by training tasks one at a time. However, the paper proposes a solution of splitting the tasks into groups and learning jointly. This enables sharing of information as well as an improved efficiency in terms of energy consumption.\n\n# Strength And Weaknesses\n\n**Strengths**\n* The paper proposes a new domain/problem which could be interesting to AI researchers.\n* The paper has ample experiments that there are no issues with pareto-optimality, etc. as usually faced in meta-learning and multi-task learning problems. \n\n**Weaknesses**\n* I believe the implementation could be discussed in more detail. The reader is left wondering what the loss was. How come there has been no non-pareto-optimal solutions when training with different splits? \n* Although the problem is interesting, I am afraid the limitation of the approach is very closely tied to privacy. For example, suppose a device has training activities related to two different companies/entities. It might not be possible to force the two networks to be same while training, or even train with the same data due to data leakage, privacy concerns, etc. This highly weakens the potential impact of the work. If I have misunderstood this part, I hope the authors could correct me here.\n* Although the authors talk about 4 different scenarios in Section 3.2, I believe all experiments could be classified under scenario 1, and the approach has not been shown to be applicable to all 4 scenarios.\n* In section 5.5, the authors show that increasing the number of epochs seems to harm the overall test loss. I would assume this is a classical scenario of overfitting, but I hope the authors could explain this. Especially since the exact formulation of the loss used for training is not detailed, this is hard to guess.\n* This is more of a question: In Table 3, why is the loss of one-by-one higher than the loss of all-in-one for the task 'd'? This intuitively should not be the case since the all-in-one will simplicity come with more noise.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n\nThe paper is well-written and reader-friendly. The tables do look a bit off, but that could be easily fixed, and the paper is readable. \n\n**Reproducibility**\n\nI believe the work lacks significantly in reproducibility. Although the basic hyperparameters such as number of splits, local epochs have been provided. The paper does not go into the network structure used, the loss functions, the learning rate and other crucial details that are required to replicate the work.\n\n**Novelty**\nThe authors try to offer an approach to train multiple activities on a resource-contrained device.\n\n# Summary Of The Review\n\nAlthough the results are interesting, it is difficult to truly understand the paper without the complete training regime. The lack of the loss function used is a deterrent, in my view. Besides that, I feel the approach is limited as it can only be used in Scenario 1 (as per Section 3.2).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"Smart Multi-Tenant Federated Learning (MuFL)\" addresses the challenge of coordinating multiple simultaneous training activities in federated learning (FL) systems, particularly on resource-constrained edge devices. The authors propose a novel system, MuFL, which formalizes the concept of multi-tenant FL and introduces two key techniques: Activity Consolidation and Activity Splitting. These techniques aim to optimize the use of resources by leveraging synergies among tasks while maintaining performance. Empirical results demonstrate that MuFL reduces energy consumption by over 40% compared to traditional one-by-one training methods, while also achieving superior performance across multiple training sets.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to multi-tenant federated learning, filling a critical gap in the existing literature that primarily focuses on single-task scenarios. The formalization of the problem and the introduction of two complementary techniques provide a robust framework for optimizing resource usage. However, a potential weakness is that while the empirical results show significant improvements, the experiments are limited to a single dataset (Taskonomy), which may affect the generalizability of the results. Additionally, the performance degradation risk associated with overemphasizing synergies in Activity Consolidation could use more thorough exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear explanation of the problem, methodology, and results. The methodology is detailed, allowing for a good understanding of the proposed techniques. The novelty is evident in the conceptualization of multi-tenant FL and in the specific strategies introduced. The authors have provided sufficient implementation details and experimental setups, and they indicate a commitment to future open-sourcing of the code, which enhances reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of federated learning by addressing the complexities of multi-tenant scenarios. The proposed MuFL system offers a practical solution that balances performance and resource efficiency. While the empirical results are promising, further validation across diverse datasets would strengthen the claims made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-tenant federated learning (FL) through the introduction of a system called MuFL. It formalizes the challenges of managing multiple training activities on decentralized devices by introducing a framework that includes activity consolidation and splitting based on affinities between tasks. The proposed methodology demonstrates significant improvements in performance, achieving up to 40% lower test loss and energy consumption compared to baseline methods. Experiments conducted on the Taskonomy dataset show the effectiveness of MuFL in optimizing training processes while considering both synergies and differences among tasks.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to optimizing multi-tenant FL and its contribution to energy efficiency, which is crucial in resource-constrained environments. The comprehensive experimental evaluation on various configurations adds credibility to the findings. However, the paper also exhibits weaknesses, such as potential scalability issues and computational overhead associated with affinity calculations. Furthermore, concerns related to overfitting and generalizability to broader datasets or tasks warrant further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodologies and findings, making it accessible to readers. The quality of the experiments is commendable, as they effectively validate the proposed techniques. The novelty of the approach is significant, contributing to the existing body of knowledge in federated learning. However, the reproducibility could be challenged by the reliance on specific datasets and the complexity of the affinity calculations, which may not be feasible for all edge devices.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning through the introduction of MuFL, which effectively addresses the challenges associated with multi-tenant systems. Despite some limitations regarding scalability and computational complexity, the findings present promising implications for real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"SMART MULTI-TENANT FEDERATED LEARNING\" (MuFL), which introduces a novel approach to federated learning (FL) by addressing the challenges of training multiple activities on resource-constrained devices. Key contributions include the concepts of activity consolidation and activity splitting, which optimize resource usage and enhance model performance. The authors provide a formal problem definition, detailed experimental methodologies using the Taskonomy dataset, and demonstrate that MuFL significantly outperforms baseline methods in terms of both energy consumption and test loss, achieving a 40% reduction in energy usage.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear theoretical foundations that build upon established FL principles, such as multi-tenant FL and multi-task architectures. The experimental results are robust, with comprehensive comparisons against various baseline methods, showcasing the efficiency of activity consolidation and splitting strategies. However, a potential weakness lies in the limited exploration of client selection strategies and scheduling mechanisms, which are briefly mentioned as areas for future work but not elaborated upon.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodologies and findings clearly, making it accessible to readers familiar with federated learning concepts. The quality of the experiments is high, with detailed metrics provided for performance evaluation. The novelty of combining activity consolidation with activity splitting in a multi-tenant FL context is significant, though it could benefit from additional discussion on reproducibility, particularly in terms of the specific hyperparameters used and their impact on results.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning by proposing an innovative approach that effectively balances performance and resource efficiency in multi-tenant environments. While the experimental results are compelling, further exploration of scheduling and client selection could strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces \"MuFL,\" a novel approach to multi-tenant federated learning (FL) that optimally coordinates simultaneous training activities by leveraging synergies and differences among tasks. The methodology includes a formalization of the multi-tenant FL problem and the development of algorithms for activity consolidation and splitting. Empirical results indicate that MuFL achieves over a 40% reduction in energy consumption compared to existing methods, demonstrating its potential for real-world applications, particularly in energy-constrained environments.\n\n# Strength And Weaknesses\nThe paper's strengths include an innovative approach to multi-tenant FL, substantial energy efficiency improvements, and a solid formalization of the problem. The empirical validation across various training activities further enhances its credibility. However, the evaluation lacks coverage of diverse real-world scenarios, which could limit generalizability. Additionally, the emphasis on energy efficiency may trade off computational performance, and the complexity of the formalized problem could hinder practical implementation. Moreover, the experiments are conducted on specific datasets, raising concerns about the reproducibility and applicability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe methodology and algorithm presentation are clear, facilitating understanding and potential replication of the study. However, the complexity of the algorithms might pose challenges in real-world applications where speed and simplicity are essential. The paper's novelty lies in its approach to addressing key challenges in federated learning, though the limited discussion regarding future work and client variability could impact its overall contribution to the field.\n\n# Summary Of The Review\nOverall, the paper presents a pioneering framework for multi-tenant federated learning with significant contributions to energy efficiency and empirical validation. However, the limited evaluation scope and algorithmic complexity may hinder its broader applicability and practical implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes a novel framework for multi-tenant federated learning, named MuFL, which addresses the challenges associated with simultaneous training activities on resource-constrained edge devices. The authors formalize the multi-tenant federated learning problem, categorize it into four scenarios, and introduce an innovative training methodology that combines activity consolidation and partitioning to optimize resource efficiency. Empirical results demonstrate that MuFL achieves a 40% reduction in energy consumption while outperforming traditional sequential training methods in terms of performance metrics.\n\n# Strength And Weaknesses\nThe primary strengths of this paper lie in its comprehensive formalization of the multi-tenant federated learning problem and the introduction of a dual training methodology that effectively manages resource constraints. The empirical validation across diverse datasets provides solid evidence of the framework's efficacy. However, a potential weakness is the limited exploration of scalability and adaptability across more varied scenarios, which could affect the generalizability of the results. Additionally, the paper could benefit from a more detailed discussion on the implications of the proposed method on client privacy and security in federated learning contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The quality of writing is high, with a coherent flow of ideas that aids in understanding the contributions. The novelty of the approach is significant, as it provides a fresh perspective on managing multi-tenant scenarios in federated learning. The reproducibility of the results is supported by the thorough empirical validation, although supplementary materials detailing experimental setups would enhance this aspect further.\n\n# Summary Of The Review\nThe paper presents a significant advancement in federated learning with the introduction of MuFL, a framework that effectively consolidates and partitions training tasks to optimize resource utilization. Empirical results strongly support the authors' claims regarding performance improvements and energy efficiency. Overall, the contributions are valuable, though further exploration of scalability would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"SMART MULTI-TENANT FEDERATED LEARNING\" introduces a novel framework for adversarial training in federated learning (FL) called MuFL. This framework aims to efficiently manage multiple adversarial tasks on resource-constrained edge devices by leveraging a coordinated strategy that consolidates and splits tasks based on their interdependencies. The authors demonstrate that MuFL significantly improves energy consumption (reducing it by over 40%) and enhances adversarial robustness through extensive empirical evaluations across various datasets and scenarios.\n\n# Strength And Weaknesses\n**Strengths**:\n1. **Innovative Problem Definition**: The authors successfully redefine the adversarial training problem in a multi-tenant context, addressing the complexities of shared resource management without sacrificing performance.\n2. **Two-Pronged Approach**: The strategies of activity consolidation and splitting are intelligently designed, promoting training efficiency and improved model performance by merging and separating tasks based on their synergies.\n3. **Empirical Validation**: The comprehensive experimental results validate the effectiveness of MuFL, showcasing its advantages over baseline methods in terms of energy efficiency and robustness.\n4. **Field Contribution**: This work enriches the literature on adversarial training by providing a scalable and practical method applicable to real-world scenarios, particularly in mobile and edge computing environments.\n\n**Weaknesses**:\n1. **Limited Discussion of Limitations**: The paper lacks a detailed exploration of the limitations and challenges associated with applying MuFL, especially in heterogeneous environments or with complex adversarial tasks.\n2. **Complexity of Implementation**: The proposed approach introduces significant complexity to the training process, which may pose challenges for practical implementation in real-world applications concerning resource allocation and task scheduling.\n3. **Absence of Theoretical Guarantees**: The focus on empirical results without accompanying theoretical insights or guarantees regarding convergence or performance weakens the overall credibility of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-articulated, with clear descriptions of the methodology and outcomes. However, certain sections, particularly those detailing implementation considerations, could benefit from greater elaboration to enhance practical applicability. The originality of the approach lies in its integration of multi-task learning, adversarial training, and federated learning, particularly through its focus on task synergies. Nevertheless, the lack of theoretical underpinnings may hinder reproducibility and broader acceptance.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to adversarial training in federated learning through the innovative MuFL framework. While it demonstrates strong empirical results and offers a practical solution to a relevant problem, the paper would benefit from a more thorough discussion of limitations and a better theoretical grounding.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper titled \"SMART MULTI-TENANT FEDERATED LEARNING\" introduces a novel framework called MuFL, designed to enhance federated learning (FL) on resource-constrained edge devices. Its primary contributions include the formalization of multi-tenant FL, the introduction of MuFL as a new standard for efficiency in FL, and a reported 40% reduction in energy consumption. The methodology involves scheduling and executing training activities in a manner that purportedly leverages synergies between tasks. Empirical results indicate that MuFL outperforms existing methods, although these claims may overstate the actual impact due to a lack of contextualization with prior work.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious attempt to formalize multi-tenant FL and propose a new system that claims to improve efficiency and reduce energy consumption. However, it has notable weaknesses, including a tendency to downplay the contributions of existing research and an insufficiently rigorous presentation of experimental results. The claims of novelty and performance improvements are not convincingly substantiated and may mislead readers about the actual advancements made by MuFL compared to previous works.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the main ideas are presented, the framing of MuFL as a groundbreaking solution lacks sufficient context regarding existing methodologies. The quality of the experimental validation is questionable, as the results are presented without adequate statistical rigor. The novelty of the contributions is overstated, particularly in relation to established techniques in multi-task learning and federated systems. Reproducibility appears to be compromised due to the lack of comprehensive details about the experimental setup and comparison metrics.\n\n# Summary Of The Review\nOverall, the paper presents MuFL as a significant advancement in federated learning, but it does so by misrepresenting the state of the art and potentially exaggerating the practical implications of its findings. While the formalization of multi-tenant FL is a noteworthy contribution, the claims about efficiency and energy savings require more rigorous support and contextualization.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces MuFL, a novel system designed to enhance multi-tenant federated learning (FL) by coordinating multiple concurrent training activities on resource-constrained devices. The authors formalize the concept of multi-tenant FL, exploring various scenarios where clients can execute multiple activities simultaneously. Experimental results demonstrate that MuFL outperforms traditional methods in terms of test loss and energy consumption, achieving a 25% reduction in energy usage compared to one-by-one training methods, although the initial claim of a 40% reduction was revised to 30%. Overall, while MuFL shows promise, the observed performance improvements are modest, indicating the need for further optimization.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative approach to multi-tenant federated learning and its clear articulation of the problem setup, which is crucial for understanding the context of the proposed solution. The experimental results provide a solid comparison with existing methods, showcasing the efficacy of MuFL. However, weaknesses include the modest gains in performance and energy consumption, which may not be compelling enough to warrant widespread adoption. Additionally, the reduction in claimed energy savings from 40% to 30% reflects a lack of robustness in the initial findings, which may undermine the paper's overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow from the introduction to the conclusions. The methodology is detailed enough to allow for reproducibility, although the adjustments to performance metrics may require additional clarification for future researchers seeking to replicate the results. In terms of novelty, the approach of consolidating training activities in a federated learning context is an interesting contribution; however, the practical implications of the findings may not be as significant as initially suggested.\n\n# Summary Of The Review\nThe paper presents a valuable contribution to the field of federated learning by introducing the MuFL system for multi-tenant environments, although the performance improvements and energy savings are relatively modest. While the work is well-articulated and methodologically sound, the revised findings may detract from its overall impact. Further optimization and exploration of scheduling mechanisms are suggested to enhance the system's efficacy.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel methodology for optimizing federated learning (FL) on resource-constrained edge devices. It emphasizes synergies among various training activities to facilitate efficient task scheduling and energy consumption management. Through extensive experimentation, the authors claim to demonstrate the effectiveness of their approach; however, the paper relies on certain assumptions that may not hold in real-world scenarios, such as the uniformity of resource constraints across edge devices and the stability of task affinities.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative attempt to address the challenges of federated learning in multi-tenant environments, particularly regarding energy efficiency. However, several weaknesses are noted, including an oversimplified view of task synergies and a lack of consideration for the complexities of concurrent training. Additionally, the rigid classification of multi-tenant scenarios and the focus on a single performance metric limit the proposed methodology's applicability. The empirical validation also raises concerns about generalizability and robustness across diverse datasets and environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents ideas clearly. However, it lacks thorough exploration of alternative architectures and approaches, which diminishes its novelty. The reproducibility of the findings is questionable due to the narrow focus on specific datasets and conditions, as well as the assumptions made regarding client participation and energy consumption measurements. Overall, while the methodology is intriguing, its practical implications are not fully substantiated.\n\n# Summary Of The Review\nThe paper introduces a promising framework for optimizing federated learning on edge devices, but it is constrained by several critical assumptions and limitations that undermine its robustness and general applicability. The methodology's novelty is somewhat diminished by a lack of exploration of alternative solutions and the narrow empirical validation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces MuFL, a smart multi-tenant federated learning system aimed at optimizing concurrent training activities on resource-constrained edge devices. It formalizes the multi-tenant federated learning (FL) problem and presents innovative strategies for activity consolidation and splitting, which enhance the efficiency of training processes. Through extensive experiments, the authors demonstrate that MuFL achieves significant energy savings (over 40%) and improved performance metrics compared to baseline methods.\n\n# Strength And Weaknesses\nThe paper's strengths include its clear articulation of the multi-tenant FL problem and the novel approach of consolidating and splitting training tasks based on activity affinities. This dual strategy is well-justified and provides a substantial improvement in performance and energy efficiency. However, a potential weakness is the limited exploration of different scheduling mechanisms and client selection strategies, which could further enhance the proposed system's effectiveness. Additionally, while the experiments are comprehensive, further validation in diverse real-world scenarios may be necessary to fully establish the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to a wide audience. The quality of the methodology and experimentation is high, with a thorough review of related work that contextualizes the contributions. The novelty of the proposed framework is significant, addressing an important gap in federated learning for resource-constrained environments. The authors commit to reproducibility by providing detailed information on datasets, implementation, and hyperparameters, alongside plans for open-sourcing the implementation, which bolsters the study's credibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in multi-tenant federated learning through the innovative MuFL system, which effectively optimizes resource use in edge devices. While the contributions are strong, further exploration of scheduling and client selection could enhance the framework's applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for enhancing the performance of deep learning models in environments with limited labeled data. The methodology leverages a combination of semi-supervised learning techniques and active learning strategies to optimize the labeling process and model training. The authors validate their approach through extensive experiments on benchmark datasets, demonstrating significant improvements in classification accuracy and reduced labeling costs compared to traditional methods.\n\n# Strength And Weaknesses\n**Strengths:**\n- **Relevance:** The problem addressed is highly relevant in the current landscape of machine learning, particularly in applications where labeled data is scarce.\n- **Innovation:** The integration of semi-supervised and active learning is a promising approach that introduces a fresh perspective to the challenges of data labeling.\n- **Experimental Validation:** The authors provide a robust set of experiments, clearly showing the advantages of their proposed method over existing techniques.\n\n**Weaknesses:**\n- **Scalability:** While the results are compelling, the scalability of the proposed method to larger datasets or more complex tasks is not thoroughly explored.\n- **Comparison with Existing Methods:** The paper could benefit from a more detailed comparison with a broader range of baseline models to better contextualize the improvements offered by the proposed method.\n- **Result Interpretation:** Some of the findings require deeper analysis and explanation to fully articulate their significance and implications for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, with clear explanations of the methodology and results. The quality of the figures and tables enhances the clarity of the findings. However, while the novelty of the proposed approach is evident, reproducibility could be improved by providing more details on the implementation and hyperparameter tuning processes.\n\n# Summary Of The Review\nThis paper presents a valuable contribution to the field of machine learning by addressing the critical issue of labeled data scarcity through an innovative combination of semi-supervised and active learning techniques. While the experimental results are strong, further exploration of scalability and comprehensive comparisons with other methods would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents **MuFL**, a smart multi-tenant federated learning (FL) system designed to address the limitations faced by resource-constrained edge devices when managing multiple simultaneous training activities. The authors formalize the multi-tenant FL problem, identifying four distinct scenarios based on application type and client support. They propose two innovative optimization strategies: **Activity Consolidation**, which merges multiple training tasks into a single multi-task architecture, and **Activity Splitting**, which organizes these tasks into affinity-based groups to enhance efficiency. Experimental results demonstrate that MuFL can reduce energy consumption by over 40% while improving performance metrics compared to existing FL approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its systematic approach to formalizing the multi-tenant FL problem and offering two distinct optimization strategies that effectively address the challenges posed by resource limitations on edge devices. The empirical results are compelling, showcasing significant improvements in energy efficiency and performance, thus supporting the proposed methods' effectiveness. However, a potential weakness is the lack of extensive experimentation across diverse real-world scenarios, which may limit the generalizability of the findings. Additionally, while the proposed strategies are innovative, the paper could provide more detailed comparisons with other state-of-the-art methods in the domain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem statement, methodology, and findings. The quality of the writing is high, making it accessible to a broad audience. The novelty of the proposed MuFL system and its optimization strategies is evident, contributing significantly to the field of federated learning. However, reproducibility could be enhanced by providing more comprehensive details regarding the experimental setup and hyperparameter choices, which would allow other researchers to replicate the results more easily.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of federated learning by introducing a novel multi-tenant system that effectively manages simultaneous training activities on resource-constrained devices. While the contributions are substantial, further validation across varied applications would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called MuFL (Smart Multi-Tenant Federated Learning), aimed at enhancing the coordination of multiple training activities in federated learning (FL) on resource-constrained devices. The authors formalize the concept of multi-tenant FL and introduce two optimization strategies: Activity Consolidation, which merges related tasks into a unified model, and Activity Splitting, which divides these tasks based on their synergies. Experimental results demonstrate that MuFL achieves a 40% reduction in energy consumption while outperforming existing methods in terms of performance metrics such as test loss.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the challenges of resource constraints in federated learning, particularly in multi-tenant scenarios. The proposed optimization strategies are well-formulated and effectively illustrated through experimental results, highlighting both energy efficiency and performance improvements. However, the paper could benefit from a more comprehensive discussion of the limitations of the proposed methods, particularly in scenarios with highly heterogeneous client capabilities or in the presence of adversarial attacks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and findings. The definitions and scenarios outlined in the problem setup provide a solid foundation for understanding the proposed methods. The novelty of the work is significant, as it bridges gaps in existing literature regarding multi-tenant FL and offers practical optimization strategies. Additionally, the authors provide a reproducibility statement, indicating that the experimental setups and algorithm details will be openly accessible, which enhances the paper's credibility and allows for further validation by the research community.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to multi-tenant federated learning, demonstrating notable improvements in both energy efficiency and performance. While the contributions are significant, further exploration of the limitations would strengthen the work. The clarity of presentation and commitment to reproducibility are commendable.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Smart Multi-Tenant Federated Learning\" presents a novel approach called MuFL to address the challenges in multi-tenant federated learning (FL), particularly in edge devices managing multiple training tasks. The authors define the problem setup for multi-tenant FL, classify application scenarios, and propose a methodology involving activity consolidation and activity splitting within a structured architecture. Empirical evaluations demonstrate that MuFL significantly reduces energy consumption by 40% while improving performance metrics compared to baseline methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear identification of the challenges in multi-tenant federated learning and the introduction of a novel solution that effectively addresses these issues. The extensive empirical evaluations add credibility to the claims made by the authors, showcasing the practical implications of MuFL. However, the paper could benefit from a more in-depth discussion of potential limitations and a clearer articulation of future work. Additionally, while the results are promising, further validation across diverse real-world applications would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly organized, facilitating reader understanding through logical flow and appropriate use of headings and subheadings. The methodology is described in sufficient detail, and the inclusion of tables and figures enhances the clarity of the results. The authors mention plans for open-sourcing their implementation, which is a positive aspect for reproducibility. The novelty of the proposed approach is evident, as it fills a significant gap in the current research landscape of federated learning.\n\n# Summary Of The Review\nOverall, the paper presents a well-defined problem and a novel solution with robust empirical validation, making a valuable contribution to the field of federated learning. While the findings are promising, addressing potential limitations and expanding on future work would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for multi-tenant federated learning (MuFL), designed to address the challenges of resource constraints in decentralized training environments. The methodology involves two main components: Activity Consolidation, which merges disparate training activities into a unified multi-task model, and Activity Splitting, which categorizes the post-training activities into synergistic groups based on affinity metrics. The empirical evaluation demonstrates that MuFL achieves significant efficiency improvements, including a 40% reduction in energy consumption while maintaining competitive performance metrics across various task sets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to addressing the resource limitations inherent in multi-tenant federated learning, as well as its rigorous mathematical formulation of the optimization problem. The empirical evaluation supports the proposed framework with promising results, indicating practical applicability. However, the paper could improve by providing more detailed discussions on the limitations of the proposed framework and potential scenarios where the method may not perform as expected. Additionally, while the theoretical aspects are well-developed, clearer illustrations of the implementation and the practical considerations for deployment would enhance the paper's accessibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas in a clear and logical manner. The mathematical formulations are precise, contributing to the overall quality of the work. The novelty of the proposed MuFL framework is significant, as it integrates concepts from both multi-task learning and federated learning to address the unique challenges of multi-tenancy. However, the reproducibility of the results would benefit from more comprehensive details on experimental setups and datasets used, as well as the specific configurations of the MuFL system during evaluation.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of federated learning by introducing the MuFL framework, which effectively addresses resource constraints in multi-tenant scenarios. While the theoretical foundations and empirical results are strong, the paper would benefit from additional detail regarding implementation and potential limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a multi-tenant federated learning system named MuFL, which aims to enhance the efficiency of federated learning by consolidating and splitting activities among multiple tenants. The authors present experimental results that claim MuFL reduces energy consumption by over 40% compared to traditional methods. However, the methodology appears overly complex and lacks thorough justification. The authors assert that their work is the first in-depth exploration of multi-tenant federated learning, despite existing literature on related topics.\n\n# Strength And Weaknesses\nWhile the paper presents a novel attempt at addressing multi-tenant federated learning, it suffers from several weaknesses. The complexity of the proposed design is not adequately justified, leading to potential confusion about its necessity. The claim of originality is undermined by existing literature that touches upon similar concepts. The approach to formalizing multi-tenant federated learning is convoluted, and the empirical results, while promising, lack robust comparisons and transparency. The limited experimental setup raises concerns regarding reproducibility and the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by its complex formalization of multi-tenant federated learning, which may confuse readers. The quality of the experimental validation is questionable due to insufficient details on the experimental setup and conditions. In terms of novelty, the paper does not present substantial advancements over existing methods, as the proposed system relies on basic scheduling techniques. Reproducibility is also a concern, given the lack of transparency in the experimental design and insufficient engagement with related work.\n\n# Summary Of The Review\nOverall, the paper presents an interesting concept in multi-tenant federated learning with potential energy-saving benefits. However, its complexity, lack of originality, and insufficient empirical validation significantly detract from its contributions. The authors would benefit from a more critical engagement with existing literature and a clearer exploration of the limitations of their proposed approach.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces SMART MULTI-TENANT FEDERATED LEARNING (MuFL), a novel approach aimed at optimizing federated learning in multi-tenant scenarios. The methodology involves two key strategies: Activity Consolidation, which merges similar training tasks to enhance efficiency, and Activity Splitting, which divides these consolidated tasks into optimized groups. The findings demonstrate that MuFL significantly improves performance over existing methods while achieving over a 40% reduction in energy consumption, underscoring its potential for sustainable AI applications across various domains.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its pioneering exploration of multi-tenant federated learning, which sets a new benchmark for future research. The dual optimization strategies presented are well-articulated and show clear synergies, enhancing operational efficiency. However, the paper could benefit from a more detailed discussion on the limitations of the proposed approach, including potential challenges in real-world deployments and the scalability of the optimization strategies across diverse environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, making it accessible to a broad audience. The quality of the experiments is commendable, showcasing rigorous evaluation of the proposed methods. However, while the novelty is evident, the reproducibility could be enhanced by providing more comprehensive details on experimental setup and parameters used in the evaluations, which would aid others in replicating the results.\n\n# Summary Of The Review\nOverall, MuFL presents a significant advancement in the field of federated learning, particularly in its innovative approach to multi-tenant scenarios. Its focus on efficiency and sustainability positions it as a valuable contribution to the AI landscape, though further clarity on scalability and reproducibility is needed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for Smart Multi-Tenant Federated Learning (MuFL), which addresses the challenges of decentralized model training across multiple clients in resource-constrained environments. The authors define a multi-tenant FL scenario, propose methods for activity consolidation and splitting to optimize training processes, and introduce an algorithmic framework to implement these concepts. Key findings suggest that leveraging synergies among training activities can significantly enhance model performance while adhering to resource limitations.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical formulation of multi-tenant federated learning and the innovative concepts of activity consolidation and splitting, which provide a novel approach to optimizing resource use. The algorithmic framework is well-defined, and the affinity measurement method offers a new perspective on inter-task relationships. However, the paper lacks empirical validation of its theoretical claims, which could limit its applicability in real-world scenarios. Additionally, some sections may benefit from clearer exposition for readers less familiar with the underlying theoretical constructs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, but certain technical details, particularly in the algorithmic framework, could be better articulated to enhance understanding. The quality of the theoretical contributions is high, with clear connections made to existing literature in federated learning and multi-task learning. While the novelty of the theoretical insights is commendable, the lack of empirical validation raises concerns about the reproducibility of the proposed methods in practical settings.\n\n# Summary Of The Review\nOverall, the paper contributes valuable theoretical insights into Smart Multi-Tenant Federated Learning, emphasizing the importance of optimizing resource use through innovative methods. However, the absence of empirical validation and some clarity issues limit its immediate applicability and practical relevance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents MuFL, a novel framework designed for multi-task federated learning, implemented in Python using PyTorch. The methodology includes activity consolidation and splitting, allowing for efficient training across multiple tasks by merging training activities into a multi-task model and dividing it based on calculated affinities. Experimental results demonstrate the system's effectiveness, with detailed comparisons against established methods like FedProx and GradNorm, highlighting improvements in test loss, energy consumption, and carbon footprint metrics.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to multi-task federated learning, particularly through the use of activity consolidation and splitting, which enhances model efficiency. The paper provides extensive experimental details, including hyperparameter settings and dataset descriptions, allowing for a comprehensive understanding of the methodology. However, a potential weakness is the reliance on previously implemented methods for comparison, which may limit the originality of the results. Additionally, the future open-sourcing of the implementation is promising but currently unverified.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, presenting the methodology, results, and analysis in a coherent manner. The quality of the proposed system and experimental setup is high, with clear definitions of hyperparameters and detailed comparisons across various methods. The novelty of the approach is significant in the context of federated learning; however, the reliance on existing methodologies for comparison affects its overall impact. The promise of open-sourcing the implementation enhances reproducibility, although this remains to be realized.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of federated learning through the MuFL framework, showcasing novel methods for multi-task training. While the experimental results are promising, the reliance on existing methods for benchmarking may limit the perceived novelty. The clarity of the presentation and the future open-sourcing of the implementation are positive indicators for the paper’s impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper proposes a novel framework for multi-tenant federated learning (FL), termed MuFL, aimed at addressing resource constraints in edge devices. The authors introduce an activity consolidation method and a hierarchical splitting technique designed to enhance the efficiency of training across multiple tasks. They claim that MuFL achieves a 40% reduction in energy consumption compared to existing approaches, although the comparison lacks adequate benchmarking against state-of-the-art methods.\n\n# Strength And Weaknesses\nThe paper presents several interesting concepts, such as activity consolidation and hierarchical splitting, which could potentially enhance resource utilization in multi-tenant FL scenarios. However, it suffers from significant weaknesses, including a lack of acknowledgment of prior foundational work in the field, such as that by Bonawitz et al. (2019), which diminishes its novelty. Additionally, the experimental evaluation is limited and does not provide a fair comparison with existing methods, leading to inflated claims of superiority. The assumptions regarding resource limits are narrow and do not consider advancements in multi-task learning, limiting the framework's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the implementation details are thorough, the paper does not benchmark its proposed methods against similar implementations in the literature, raising concerns about reproducibility. The clarity of the presented methodology is undermined by insufficient comparative analysis with existing techniques, which hampers the perceived novelty of the contributions. Furthermore, the statistical significance of the findings is not convincingly established, leaving doubts about the robustness of the results.\n\n# Summary Of The Review\nOverall, the paper introduces a potentially valuable framework for multi-tenant federated learning but falls short in its novelty and empirical validation. The lack of comprehensive comparisons with existing methods and the narrow assumptions regarding resource utilization significantly undermine the impact of the proposed contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework for a smart multi-tenant federated learning (FL) system that aims to enhance the efficiency and coordination of training activities across multiple clients. The methodology involves the development of an algorithm that optimizes resource allocation and model training processes while addressing the challenges posed by data heterogeneity and communication constraints. The findings demonstrate significant improvements in training performance and energy consumption compared to existing FL systems, showcasing the potential of the proposed approach in practical scenarios.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to multi-tenant FL, which addresses critical issues such as resource allocation and communication overhead effectively. The empirical results are compelling, showing concrete performance gains. However, the paper could improve in several areas, including clarity in the presentation of contributions and a more thorough exploration of the implications of the findings. Additionally, some methodological details are lacking, which may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by some complex sentence structures and inconsistent formatting throughout, which detracts from the overall readability. While the technical novelty is notable, with a fresh approach to federated learning, the reproducibility of the results may be challenged due to insufficient parameter definitions and a lack of detailed algorithmic descriptions. Overall, the quality of the writing could benefit from a more careful review to enhance its clarity and coherence.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in the realm of multi-tenant federated learning, with promising empirical results. However, clarity and reproducibility issues need to be addressed to strengthen its impact and usability in the research community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing multi-tenant federated learning (MuFL) tailored for specific scenarios. The authors propose a system that aims to enhance efficiency by introducing hierarchical splitting and utilizing affinity scores for activity splitting. The findings indicate improvements in energy consumption and performance metrics, although they are primarily focused on the experimental setup without extensive exploration of broader applications or scalability.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the MuFL system and its contributions to efficiency in multi-tenant environments. However, it inadequately addresses the generalizability of its findings to other critical areas such as healthcare and finance. Additionally, while the study demonstrates improvements in energy consumption, it lacks a comprehensive discussion on sustainability implications and potential security vulnerabilities. The limited exploration of other federated learning algorithms and the absence of robust evaluation metrics further diminish the paper's scope and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, making it accessible to the reader. However, the novelty is somewhat constrained by its focus on a narrow set of use cases and algorithms, leading to questions about reproducibility in varied real-world scenarios. The authors could strengthen their claims by providing a more thorough analysis of their methods and discussing potential limitations more candidly.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of multi-tenant federated learning but falls short in addressing broader implications and limitations. While the proposed techniques show promise, a more extensive exploration of scalability, security, and alternative methodologies would enhance the robustness of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper introduces a novel approach to multi-tenant federated learning (FL), termed MuFL, which seeks to optimize model training across multiple simultaneous activities while addressing resource constraints on edge devices. The authors propose two primary methodologies: activity consolidation, which merges training tasks into a multi-task architecture, and activity splitting, which divides these activities based on measured affinities. The findings indicate that MuFL not only significantly reduces energy consumption (40% less) but also achieves lower test loss compared to baseline methods, demonstrating the potential for enhanced model performance and real-world application.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its thorough empirical studies and robust statistical methodologies employed to validate the proposed methods. The introduction of concrete performance metrics, including test loss and energy consumption, provides a clear framework for comparison against established baseline methods. However, a potential weakness is that while the paper emphasizes statistical significance, it does not delve deeply into the practical implementation challenges of the proposed methods in real-world settings, which may affect their scalability and adoption.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodologies and findings. The use of tables and figures effectively supports the statistical claims made throughout the text. In terms of novelty, the approach of using affinity measurements to optimize multi-tenant FL scenarios is innovative, although it builds upon existing concepts in federated learning. Reproducibility is facilitated by the detailed description of the statistical methods and experimental design, although additional details on the implementation of MuFL could enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of federated learning by addressing the challenges of multi-tenant scenarios with innovative methodologies and demonstrating significant empirical results. While the findings are promising, further exploration of practical implementation challenges is warranted to fully assess the impact of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents MuFL, a system designed to optimize federated learning in multi-tenant scenarios, primarily focusing on clients that can execute only one training activity at a time. The methodology entails activity consolidation and splitting to enhance resource utilization in federated learning environments. However, the empirical findings, derived from experiments on a limited dataset (Taskonomy), raise concerns about generalizability. The results predominantly highlight energy consumption and performance metrics but lack comprehensive insights into robustness, scalability, and adaptability.\n\n# Strength And Weaknesses\nWhile the paper contributes valuable ideas regarding activity optimization in multi-tenant federated learning, it is limited by several factors. The focus on a specific scenario may restrict its broader applicability, particularly in environments where clients can engage in multiple training activities or where advanced scheduling mechanisms could be beneficial. The exploration of activity consolidation and splitting lacks depth, and alternative strategies are not sufficiently examined. The empirical studies are conducted on a narrow dataset, which may not capture the diversity of real-world applications, potentially affecting the robustness of the findings. Furthermore, crucial aspects such as communication delays, client dropouts, and the trade-off between energy efficiency and model accuracy are not addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that presents its contributions effectively. However, the novelty of the proposed methods is somewhat limited due to the lack of exploration of alternative strategies for activity management. The reproducibility of the findings is also questionable, given the limited dataset and the absence of detailed methodologies for the suggested future work, such as improved scheduling mechanisms and client selection strategies.\n\n# Summary Of The Review\nOverall, the paper provides a foundational approach to optimizing multi-tenant federated learning through the MuFL system, but it suffers from limitations in applicability, depth of exploration, and consideration of critical factors affecting performance. The findings may not generalize well beyond the tested scenarios, and further work is needed to address identified gaps.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Smart Multi-Tenant Federated Learning\" proposes a framework aimed at addressing the challenges associated with simultaneous training activities on edge devices within a federated learning (FL) context. The authors introduce a system called MuFL, which formalizes the issue of multi-tenancy in federated learning, focusing on strategies like activity consolidation and activity splitting based on affinities. Their experiments claim to demonstrate improvements in energy efficiency and overall performance, although the specifics of their baseline comparisons are not well-defined.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address a relevant challenge in federated learning, particularly concerning resource constraints in edge devices. However, the weaknesses are pronounced, as many of the proposed solutions appear to reiterate existing concepts in the field, such as multi-task learning, without offering substantial novelty. The paper also lacks clarity in presenting its empirical results, with insufficient detail on baseline comparisons, which undermines the validity of its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, but the clarity of its contributions is compromised by a lack of depth in the discussion of existing work and the novelty of the proposed methods. While the formalization of the problem is thorough, it may come across as tedious to those familiar with federated learning. The reproducibility of the results is questionable due to the vague descriptions of experimental setups and baselines.\n\n# Summary Of The Review\nOverall, the paper presents an attempt to address a well-known issue in federated learning but does so by rehashing established ideas without introducing significant innovation. The empirical contributions lack rigor, and the novelty of the proposed methods is minimal, making it difficult to justify its place in the current literature.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the Multi-tenant Federated Learning (MuFL) system, which focuses on optimizing federated learning through activity consolidation and activity splitting. The authors propose a methodology that enhances training efficiency while reducing energy consumption. Key findings indicate improvements in training outcomes through these techniques, which can be applied to multi-agent reinforcement learning scenarios. The paper also highlights the importance of user privacy and sustainability metrics in federated learning.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to improving federated learning frameworks by addressing both efficiency and energy consumption. The introduction of activity consolidation and splitting as core components offers a novel perspective on training optimization. However, the paper could benefit from a more comprehensive exploration of additional optimization techniques, such as adaptive learning rates, and a broader set of sustainability metrics. The lack of robustness evaluation and scalability analysis also weakens the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. The methodology is articulated sufficiently to allow for reproducibility, although some aspects could be further detailed, such as client selection strategies and the specifics of the affinity score calculations. The novelty of the approach is noteworthy, yet the exploration of potential enhancements and broader applicability could elevate its significance.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning through its MuFL system, emphasizing training efficiency and energy reduction. However, the exploration of additional optimization techniques and a more thorough evaluation of robustness and scalability are needed to strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents MuFL, a novel approach to multi-tenant federated learning (FL) that emphasizes energy efficiency and performance optimization. The methodology involves hierarchical task splitting and activity consolidation strategies, demonstrating substantial improvements in both performance metrics and energy consumption. The findings indicate that MuFL outperforms baseline methods, achieving over 40% reduction in energy usage and lower test loss across various datasets, such as sdnkt, while also providing insights into its lower carbon footprint.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive benchmarking against established methods, showcasing significant performance improvements and energy efficiency. The experimental setup with realistic client configurations enhances the credibility of the results. Additionally, the inclusion of ablation studies and statistical significance analysis supports the robustness of the claims made. However, the paper could benefit from a deeper exploration of potential limitations and broader applicability of the proposed method across different contexts in federated learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to the reader. The quality of the experiments, including the use of the Taskonomy dataset and detailed results, adds to its reliability. The novelty of MuFL lies in its unique approach to task management in federated learning, although further discussion on how it compares to emerging methods in the field could enhance its impact. The reproducibility of results is supported by the clear presentation of experimental setups and metrics.\n\n# Summary Of The Review\nOverall, MuFL represents a significant advancement in multi-tenant federated learning, combining improved performance with energy efficiency. The rigorous experimental validation and robust methodology contribute to its credibility, though further exploration of limitations may strengthen its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper addresses the challenges of optimizing multi-tenant federated learning (FL) systems, presenting a novel multi-task architecture that exploits synergies between tasks. The authors propose a series of optimization approaches tailored for multi-tenant environments, supported by empirical results demonstrating improved performance metrics over baseline methods. The findings indicate that the proposed framework enhances efficiency and accuracy in federated learning tasks, providing a compelling solution to current limitations in the field.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to multi-task learning within federated systems, which is a timely contribution given the growing interest in federated learning applications. The empirical results provide substantial evidence of the proposed model's effectiveness compared to existing methods. However, the paper has notable weaknesses, particularly in clarity. The abstract is dense and lacks conciseness, and complex sentence structures hinder readability. Additionally, inconsistent formatting and terminology may confuse readers, detracting from the overall comprehension of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by its complex sentence structures and dense presentation of ideas. The inconsistent formatting of section headings and the lack of clear list presentations diminish the quality of the reading experience. While the technical novelty is significant in the context of federated learning, the dense jargon and insufficient explanations of key terms may pose challenges for readers unfamiliar with the topic. The reproducibility statement is notably brief; expanding it could greatly enhance the paper's utility for other researchers aiming to replicate the study.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to federated learning by introducing a multi-task architecture optimized for multi-tenant systems. However, the clarity and quality of the writing detract from its potential impact, as the complex presentation may alienate some readers. Improvements in organization and clarity are necessary to fully realize the paper's contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.46526958453095,
    -1.7689192527745567,
    -1.8897677494305725,
    -1.7713561225056427,
    -2.0939059809919,
    -1.6507658536667014,
    -1.7902434540190388,
    -1.7976384473252212,
    -1.791530382203909,
    -1.8854927709788807,
    -1.7758039654908542,
    -1.2921021293476962,
    -1.6188505363023658,
    -1.7579999055336473,
    -1.7201085481069212,
    -1.6381803130439885,
    -1.8457297002232405,
    -1.8354701896446266,
    -1.773467973825032,
    -1.9110713569668183,
    -1.9449719543154547,
    -1.832673724907112,
    -2.0378896758520435,
    -1.8726564291838275,
    -2.024216789858297,
    -1.8987531723324962,
    -1.9174035169112333,
    -2.043009745233686,
    -1.9108186962524485
  ],
  "logp_cond": [
    [
      0.0,
      -2.320859411740835,
      -2.3195211581564883,
      -2.3149378158670353,
      -2.33084096237265,
      -2.3066828542292184,
      -2.3254696758416897,
      -2.3492341248169386,
      -2.3042226259342735,
      -2.3360452766434707,
      -2.3140622294294997,
      -2.356228713532788,
      -2.291957739339266,
      -2.3160620006461055,
      -2.318896133589272,
      -2.317258543796614,
      -2.3490682103310805,
      -2.328098850943819,
      -2.3352226374374414,
      -2.3326844525186767,
      -2.334052557563789,
      -2.337820656009141,
      -2.3301383542409173,
      -2.316944395553046,
      -2.332833823410701,
      -2.3322336286915806,
      -2.3323180125385687,
      -2.3344164022221006,
      -2.3490424476111285
    ],
    [
      -1.4484735998910647,
      0.0,
      -1.2858842600083356,
      -1.258025285524316,
      -1.3079065491206097,
      -1.3125700168024172,
      -1.29275380027588,
      -1.3556121897760598,
      -1.3344527331180729,
      -1.4065691666175457,
      -1.258232125405069,
      -1.5255553910927655,
      -1.2348651282873155,
      -1.1983226470526598,
      -1.2841558804961442,
      -1.3248255185378177,
      -1.3697515687001987,
      -1.3152785518199313,
      -1.3001134880823706,
      -1.3318712118334541,
      -1.4054329684407916,
      -1.4469462682019363,
      -1.443179445827939,
      -1.3257437520688016,
      -1.3295084660020977,
      -1.2593377257515368,
      -1.3777004294751511,
      -1.3726318193700413,
      -1.458770056983968
    ],
    [
      -1.5018704782657855,
      -1.360386296556183,
      0.0,
      -1.3794637927049338,
      -1.3109808841497033,
      -1.4532800412981437,
      -1.4462052999496935,
      -1.45712501613355,
      -1.4182698175670063,
      -1.4722727915780607,
      -1.3919687840449262,
      -1.6171792154826181,
      -1.389797031286831,
      -1.3883102153304774,
      -1.4450986403650283,
      -1.4183785438893126,
      -1.4696070698220618,
      -1.4507137671911956,
      -1.3846272510482573,
      -1.4202863415511728,
      -1.4566685369921888,
      -1.4892821309432263,
      -1.4289792640581163,
      -1.3706794744483872,
      -1.3455744598087214,
      -1.3742798223866284,
      -1.407024548494018,
      -1.4153016398713023,
      -1.5376865118433267
    ],
    [
      -1.4510230123738566,
      -1.2026230020489026,
      -1.2809561482384562,
      0.0,
      -1.3244799813435013,
      -1.37828933765825,
      -1.2321217929414114,
      -1.3111486049806587,
      -1.2799776157435179,
      -1.389947309662435,
      -1.2946619390982916,
      -1.45255704005344,
      -1.2557293691251596,
      -1.2186864375364563,
      -1.2840358221904722,
      -1.294949804872697,
      -1.3921877272092955,
      -1.2288784312582066,
      -1.2640043702782564,
      -1.3046478031172715,
      -1.3775088409858023,
      -1.417230459093046,
      -1.3825521950856354,
      -1.2385134014001367,
      -1.2776517189103913,
      -1.2679819619177137,
      -1.3324497327562528,
      -1.3206134356184565,
      -1.3908096962058605
    ],
    [
      -1.687553423158105,
      -1.5104606957521733,
      -1.5056716296714108,
      -1.5379284634532135,
      0.0,
      -1.5795776075718908,
      -1.5166168229872632,
      -1.6160295322105638,
      -1.5674735329436176,
      -1.6274153456124762,
      -1.5172686959501822,
      -1.7359021205308027,
      -1.5183696643290885,
      -1.489434377476056,
      -1.5484231387294847,
      -1.555999454989424,
      -1.5399732562809427,
      -1.5245396412933239,
      -1.605463864693928,
      -1.6026445358580432,
      -1.5663493457266735,
      -1.625930517138906,
      -1.5923767176118098,
      -1.5773982526499166,
      -1.490025030874475,
      -1.5887992373696553,
      -1.5571757573848084,
      -1.5666274114167897,
      -1.6840877251261652
    ],
    [
      -1.2997813925327584,
      -1.14664473187388,
      -1.1849237193037598,
      -1.210472839388716,
      -1.1518935229427494,
      0.0,
      -1.2054516822783972,
      -1.1865724264526978,
      -1.155769916350876,
      -1.2654176404728177,
      -1.1155266262793901,
      -1.3668692722848799,
      -1.0704946496968537,
      -1.1625932340422496,
      -1.164082277890056,
      -1.1994494710123782,
      -1.2087357256930644,
      -1.1971676262248236,
      -1.2088883572102593,
      -1.2372249433882465,
      -1.193214546694834,
      -1.265405088995529,
      -1.2492885585805276,
      -1.1564334140242802,
      -1.1714417836399675,
      -1.2138266693683293,
      -1.2002300563026898,
      -1.2545910796977877,
      -1.2963394398509414
    ],
    [
      -1.5794435737364172,
      -1.4067498813653125,
      -1.4786634605075557,
      -1.4131292951831176,
      -1.467967028766014,
      -1.5161419639796565,
      0.0,
      -1.4527330261572304,
      -1.5269642812927955,
      -1.5356095182227434,
      -1.4627888568745762,
      -1.6013567644543305,
      -1.4472318434278428,
      -1.3957874925776677,
      -1.4358782280706137,
      -1.4619267529115725,
      -1.4966020610226882,
      -1.4349680098040565,
      -1.4605683690047666,
      -1.496383226331469,
      -1.5061680167402842,
      -1.548673142662785,
      -1.5371772344669061,
      -1.4402269320228929,
      -1.5185959432907856,
      -1.4245700839613262,
      -1.499965349061179,
      -1.516411186202613,
      -1.5634225725662156
    ],
    [
      -1.5387226042767237,
      -1.3920883765590417,
      -1.4178727547389196,
      -1.4109798784134473,
      -1.4006541328643967,
      -1.4238109409549378,
      -1.3711959533477012,
      0.0,
      -1.4061172902150247,
      -1.482269595338267,
      -1.3630447692658405,
      -1.5401117765063617,
      -1.3792114537309443,
      -1.3447567805889586,
      -1.3673639071540162,
      -1.428168929257594,
      -1.4177070152877462,
      -1.3694893997396964,
      -1.4448089493854916,
      -1.4735981309361028,
      -1.4116290592746414,
      -1.4462871784134943,
      -1.4636164677666492,
      -1.4348746388089462,
      -1.4209225609999923,
      -1.3740859474573741,
      -1.4370607061106775,
      -1.423534721257115,
      -1.4913401884321655
    ],
    [
      -1.4305814498521026,
      -1.3965099323361114,
      -1.4041084324542041,
      -1.3610715437771403,
      -1.3801720702113072,
      -1.4021463086238062,
      -1.4553574799557754,
      -1.4166595256226244,
      0.0,
      -1.4919734812057668,
      -1.3440841348552262,
      -1.518723652259691,
      -1.3988683369260482,
      -1.3042364662026078,
      -1.4235195106740557,
      -1.4410652183051058,
      -1.3988683206439922,
      -1.4419529527881798,
      -1.453645246598898,
      -1.4258752774408379,
      -1.4446201248598955,
      -1.4460347605017214,
      -1.4466762314570938,
      -1.369642870275206,
      -1.336840942379434,
      -1.429126937035618,
      -1.3903822434932036,
      -1.4538404100358064,
      -1.5132620496858247
    ],
    [
      -1.5523974225944375,
      -1.484061065298271,
      -1.459695079469242,
      -1.5203354545712253,
      -1.450261382881444,
      -1.4959504612096086,
      -1.4734970060002597,
      -1.5040407653307846,
      -1.504289783722027,
      0.0,
      -1.4634424079572963,
      -1.6476335543112963,
      -1.5279133614775136,
      -1.5002135876984013,
      -1.5221762925952098,
      -1.504336304803045,
      -1.543907751385867,
      -1.5590371411203183,
      -1.4783468373487398,
      -1.4965084649257203,
      -1.5304089361920288,
      -1.562141403293088,
      -1.513605796283175,
      -1.515173736436026,
      -1.4483983341864115,
      -1.490158141020897,
      -1.518604641077042,
      -1.5372460258911296,
      -1.5393372305693405
    ],
    [
      -1.4174909064174248,
      -1.2025151217115884,
      -1.2289964977385195,
      -1.194514983447358,
      -1.2155414970245226,
      -1.2111656432526625,
      -1.256686678795616,
      -1.3083006280602618,
      -1.2483453684754768,
      -1.3231057644108535,
      0.0,
      -1.4534924423730389,
      -1.0965225709889617,
      -1.1650118856977711,
      -1.2387063165364318,
      -1.2505972164508603,
      -1.262644009971021,
      -1.2352901610076399,
      -1.2374848000143632,
      -1.2576711652144297,
      -1.2676025669039017,
      -1.3327157211171234,
      -1.3176140571856574,
      -1.2415929659096514,
      -1.2259589704033482,
      -1.2310705907233495,
      -1.2561623816407812,
      -1.2875439912692606,
      -1.3715895520601802
    ],
    [
      -1.035844457753509,
      -1.0178811873867524,
      -1.0324181107402703,
      -1.0244569472892038,
      -0.9898122427015434,
      -1.0081988853063986,
      -0.9913760116461202,
      -1.0385604571338183,
      -1.0029270174441203,
      -1.039108821419092,
      -1.0214316578311746,
      0.0,
      -0.9879598331365179,
      -1.0204764383163314,
      -0.9928454310969927,
      -0.9940438374410466,
      -1.009018276495073,
      -0.9846913219863972,
      -1.0470781999394778,
      -1.015089900513412,
      -1.020788378258516,
      -0.9772596796479582,
      -0.9877285960852625,
      -1.0009760680387085,
      -1.0013520101474267,
      -1.0287578855580046,
      -1.0077945300093694,
      -1.0250187942823445,
      -1.0207467163435062
    ],
    [
      -1.2412773713154297,
      -1.0956054586051307,
      -1.167794915649685,
      -1.156648638195882,
      -1.143413109755224,
      -1.121924870468468,
      -1.1961578753434736,
      -1.2093479426338114,
      -1.1766482266782772,
      -1.269376199572572,
      -1.082277243003912,
      -1.312849468980539,
      0.0,
      -1.0856994274337461,
      -1.1193877986779155,
      -1.134690173959077,
      -1.2041923158136751,
      -1.1295452311942638,
      -1.1942369260447743,
      -1.2125631284530953,
      -1.2169400456989998,
      -1.2214323679836834,
      -1.202715409319429,
      -1.0813611462986545,
      -1.153419660820259,
      -1.145776288012726,
      -1.186909296432053,
      -1.2500057512906422,
      -1.2883862672429316
    ],
    [
      -1.3887854854885804,
      -1.174036753012003,
      -1.2653017405759264,
      -1.2381630627005877,
      -1.2313920940809773,
      -1.3010224783820992,
      -1.2830921838978533,
      -1.2883064529900436,
      -1.229083776236867,
      -1.3842314019907735,
      -1.2489556881422945,
      -1.478988589741968,
      -1.164556450297506,
      0.0,
      -1.2370918753489155,
      -1.2381276339817522,
      -1.3078617460528352,
      -1.2240959548190848,
      -1.2760115751237997,
      -1.2910113275633606,
      -1.3064698406699324,
      -1.3279126920971893,
      -1.3372283532951763,
      -1.228833190097407,
      -1.2691705268451259,
      -1.2601978902737718,
      -1.3160002978928118,
      -1.2957293596191852,
      -1.4111508350214819
    ],
    [
      -1.387584903887279,
      -1.2041745726131854,
      -1.2627103332651475,
      -1.2511014586974132,
      -1.2524253916618608,
      -1.2558120511948225,
      -1.2372290572855396,
      -1.2868766001997909,
      -1.2822849160909318,
      -1.3885457759142072,
      -1.19735892721818,
      -1.3984670453833519,
      -1.1786379552387025,
      -1.1728539688232693,
      0.0,
      -1.2943447851294838,
      -1.303036909677126,
      -1.2178013587328393,
      -1.3066535989729504,
      -1.309511425143827,
      -1.3048871049417692,
      -1.3240532856431684,
      -1.3154061931256078,
      -1.2100934417599296,
      -1.2741885174682184,
      -1.2804330941992748,
      -1.2795457518190418,
      -1.2718429840113954,
      -1.3672724400838712
    ],
    [
      -1.3277985060432906,
      -1.1895662619816325,
      -1.1749609806588759,
      -1.2125307895012678,
      -1.1792726842147305,
      -1.2142182592775328,
      -1.2054712159563286,
      -1.2195216095512917,
      -1.2335850886403492,
      -1.3213925296029245,
      -1.2277359319657828,
      -1.3685572308019465,
      -1.1683197129788758,
      -1.1837746853469568,
      -1.1847652535394517,
      0.0,
      -1.2184705027303413,
      -1.1613571668074385,
      -1.1915106960303878,
      -1.1921601635788377,
      -1.195912144044406,
      -1.2574867677426003,
      -1.251003832940095,
      -1.1769933886450692,
      -1.1928299580863961,
      -1.2047295324846679,
      -1.1964287282462867,
      -1.2674690012423186,
      -1.3183947532062221
    ],
    [
      -1.514208991465288,
      -1.4090884341574137,
      -1.4121644409110934,
      -1.456119691523468,
      -1.3201682293392247,
      -1.4169380631150619,
      -1.4290777233415568,
      -1.3725358194995756,
      -1.3965988171227006,
      -1.485632606327334,
      -1.3647707113892804,
      -1.5421510846193651,
      -1.4039955287170394,
      -1.3952529562480949,
      -1.3921721414475337,
      -1.4172880727774657,
      0.0,
      -1.3653618646239942,
      -1.471680152325843,
      -1.4828167614592054,
      -1.3516129551202996,
      -1.4557835491861948,
      -1.3892240610770774,
      -1.4041563073706604,
      -1.3727051116346884,
      -1.3791551703243883,
      -1.4050391341122663,
      -1.4061678075118584,
      -1.4910020180713175
    ],
    [
      -1.5001346684632375,
      -1.2778661565730562,
      -1.3474732446985616,
      -1.2804532853288562,
      -1.3066316430948743,
      -1.449501592696348,
      -1.2926637592953565,
      -1.3385619596330816,
      -1.443524156254981,
      -1.4764253611805425,
      -1.2947009585070113,
      -1.5081427115759913,
      -1.2839624881905474,
      -1.2528505784533475,
      -1.300627946157752,
      -1.3312715365299832,
      -1.3577337118759931,
      0.0,
      -1.3547099790115016,
      -1.3880094455327447,
      -1.3803414015279545,
      -1.3854124101973098,
      -1.3980932983146668,
      -1.3423243155066193,
      -1.363605039767195,
      -1.3655815922915355,
      -1.3429239449729082,
      -1.387124565961037,
      -1.4865798881385448
    ],
    [
      -1.4266992582616245,
      -1.1930871648247605,
      -1.2380336737186002,
      -1.1575644045252125,
      -1.27488693137963,
      -1.3290238728626467,
      -1.1942311152421567,
      -1.3214508266012115,
      -1.3241067728997726,
      -1.3360008453477263,
      -1.267924729950849,
      -1.4511956532275405,
      -1.2591474173834931,
      -1.2177729652244285,
      -1.2558763857988735,
      -1.256576932816986,
      -1.3488476445248108,
      -1.2536955783179162,
      0.0,
      -1.2620626132620931,
      -1.307491537094846,
      -1.343038531995297,
      -1.3363867516123606,
      -1.2468221153780992,
      -1.2802584921982845,
      -1.2054976326250209,
      -1.29345694705284,
      -1.3467076930891893,
      -1.3801732120118966
    ],
    [
      -1.6002818574911648,
      -1.437765007062162,
      -1.4910839945563554,
      -1.4692917187215917,
      -1.4994211405807698,
      -1.5802430489097858,
      -1.5045626434508725,
      -1.555574464980458,
      -1.5268345571170705,
      -1.5397397598300135,
      -1.4578739945030323,
      -1.659871257394863,
      -1.4953752829623734,
      -1.451154781868914,
      -1.513783627543965,
      -1.5102611685855944,
      -1.538550280933428,
      -1.5271487563088544,
      -1.4932927960996198,
      0.0,
      -1.5133212726080836,
      -1.6230907738575528,
      -1.5278897838749494,
      -1.4772329699794147,
      -1.4919504918051947,
      -1.4641048471395866,
      -1.5191521683451465,
      -1.5352056037799295,
      -1.5748706319964014
    ],
    [
      -1.5804330514377227,
      -1.489947704978953,
      -1.459057485350647,
      -1.4825833478413386,
      -1.4310835591207458,
      -1.454508499330554,
      -1.4956577487642981,
      -1.453974258559783,
      -1.475373019058995,
      -1.5685879110331165,
      -1.4227111253054459,
      -1.606731995266704,
      -1.419421409466061,
      -1.4270425754731857,
      -1.4350829997099233,
      -1.4362469205366464,
      -1.4655347277164867,
      -1.470808833531037,
      -1.502018190181642,
      -1.4932135023754114,
      0.0,
      -1.546622497903836,
      -1.4914760641717268,
      -1.4302144189199129,
      -1.4465864248107638,
      -1.43841917111263,
      -1.4812439097461025,
      -1.4731114079840437,
      -1.5773829479415382
    ],
    [
      -1.482512941024385,
      -1.433655976829899,
      -1.4007152495526365,
      -1.4358953237705876,
      -1.3624118147543636,
      -1.4439298000051688,
      -1.4164926469982717,
      -1.3628352757287947,
      -1.376490643622684,
      -1.4543179747230628,
      -1.4089748527853818,
      -1.4638073053346752,
      -1.3826095069819468,
      -1.3759390256172104,
      -1.366266426308804,
      -1.417211712740152,
      -1.408264047146976,
      -1.3858404529668245,
      -1.4286256080287774,
      -1.448431826702365,
      -1.3987541724066654,
      0.0,
      -1.374604960667996,
      -1.4221338409819546,
      -1.381461325790617,
      -1.372075787951123,
      -1.4111062850896618,
      -1.4271676291656874,
      -1.3245953136734996
    ],
    [
      -1.681217270981026,
      -1.5869172851366238,
      -1.514144952274845,
      -1.5833990013249521,
      -1.5537757547718998,
      -1.5837260087353824,
      -1.6233725830001031,
      -1.6014118309511627,
      -1.587851684611182,
      -1.6033433287794976,
      -1.5260017273964586,
      -1.705846238496161,
      -1.5631414202465013,
      -1.5498427913999995,
      -1.5568556742830055,
      -1.549307823149384,
      -1.5412799135149857,
      -1.5523085493634043,
      -1.6251624894376069,
      -1.5917000044291603,
      -1.5660082947299818,
      -1.5812375262587057,
      0.0,
      -1.5588342808098186,
      -1.4731563501167686,
      -1.5898218727748732,
      -1.4724718508184929,
      -1.5923061977972057,
      -1.6198495884004007
    ],
    [
      -1.4608143431715535,
      -1.3989363388103024,
      -1.3453624734353324,
      -1.366499805828815,
      -1.3989047407240942,
      -1.417007458174158,
      -1.4396446032576884,
      -1.4475401159468586,
      -1.3832304537318514,
      -1.5032793106586295,
      -1.3930743296485797,
      -1.584195670595884,
      -1.3102590352439127,
      -1.3436546439536956,
      -1.4142379788495487,
      -1.374205603959262,
      -1.448338220750844,
      -1.4358502152758015,
      -1.4274479122050825,
      -1.376186022459321,
      -1.4518067757139148,
      -1.485970674397755,
      -1.4830421170750432,
      0.0,
      -1.4103246870683093,
      -1.371164073929911,
      -1.4267717900625094,
      -1.4644271135421065,
      -1.4993533292688137
    ],
    [
      -1.672539490472117,
      -1.5832662527319326,
      -1.5171992271025871,
      -1.5646462101952083,
      -1.5411446374540783,
      -1.5810102357764715,
      -1.6510618617266037,
      -1.653896006757617,
      -1.5626920367590027,
      -1.668955451755536,
      -1.507636930066332,
      -1.755957797237143,
      -1.565050499225456,
      -1.5658477766151782,
      -1.5587634538532453,
      -1.5690717510396024,
      -1.615254566138776,
      -1.5866557712353515,
      -1.649392233491592,
      -1.613984555608708,
      -1.6238974189788253,
      -1.6555385551882478,
      -1.5843062665809837,
      -1.557556378551564,
      0.0,
      -1.581142246007599,
      -1.5408903483625949,
      -1.5898356631148127,
      -1.6987418955168916
    ],
    [
      -1.5423161357401602,
      -1.3264256266910788,
      -1.4055436029357722,
      -1.3608800976788205,
      -1.4358774368724052,
      -1.4660618784949242,
      -1.4038894992184747,
      -1.4705974784311906,
      -1.4788172948266531,
      -1.5016278235575633,
      -1.3539308120397289,
      -1.644697371815202,
      -1.3723308357592294,
      -1.3611985338373762,
      -1.4001414496286926,
      -1.4139953896065127,
      -1.489455111546146,
      -1.4565332562824445,
      -1.403903446259105,
      -1.419304058667252,
      -1.4550824155721078,
      -1.5792582038661442,
      -1.4957075632899348,
      -1.3704903717248844,
      -1.4558167023023005,
      0.0,
      -1.4662083361296712,
      -1.526374614546857,
      -1.5562639715571964
    ],
    [
      -1.5476367431257927,
      -1.4640321786959332,
      -1.3740342765467028,
      -1.4524108864715761,
      -1.369243381246595,
      -1.426716481684142,
      -1.450570424376486,
      -1.4904338578641045,
      -1.4016527718898255,
      -1.4727067250702315,
      -1.3813731106843392,
      -1.5680647066098223,
      -1.38458738258351,
      -1.4137627097548027,
      -1.4125350388685578,
      -1.4027913578186304,
      -1.4133886688876567,
      -1.389213282074117,
      -1.4274986948568567,
      -1.408370100657658,
      -1.4124262529832867,
      -1.4640263282765988,
      -1.341311795547552,
      -1.384451341511527,
      -1.3115798576507396,
      -1.426237131638938,
      0.0,
      -1.4437285886529956,
      -1.554170220832458
    ],
    [
      -1.677268598576808,
      -1.6085316513157397,
      -1.4988712541035452,
      -1.4936382909994486,
      -1.5245526428876008,
      -1.640962582434811,
      -1.5795872008032368,
      -1.6339073368611374,
      -1.6005013929559788,
      -1.6696100224407746,
      -1.5864574040368598,
      -1.7230509489371104,
      -1.6049246748527726,
      -1.538478287432691,
      -1.5759055426140784,
      -1.6082973446527793,
      -1.5783987965299013,
      -1.5817794065267399,
      -1.6222039291303132,
      -1.5346346059108347,
      -1.5795078299609697,
      -1.6564811748916626,
      -1.5966255176821809,
      -1.5358152581513889,
      -1.5267262023980397,
      -1.6211731940444958,
      -1.5845801941370707,
      0.0,
      -1.6816069512162757
    ],
    [
      -1.629361827810292,
      -1.54661046856916,
      -1.5502605723105045,
      -1.5174611020012272,
      -1.5471809331900954,
      -1.5921907139296534,
      -1.5666151975966427,
      -1.5952729733622586,
      -1.5562249656758755,
      -1.5650800356458505,
      -1.5927255921651173,
      -1.6310826847150657,
      -1.5686908008219402,
      -1.5624222446956841,
      -1.545013315982963,
      -1.5697442344002308,
      -1.5785208877568748,
      -1.5779086080168294,
      -1.553412130693499,
      -1.5124363886374375,
      -1.5994681225773093,
      -1.519131581282404,
      -1.559895709312937,
      -1.5464589964456128,
      -1.575869313173194,
      -1.5472195169819123,
      -1.5616670280716525,
      -1.578588699718495,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.1444101727901148,
      0.14574842637446173,
      0.15033176866391473,
      0.1344286221583002,
      0.15858673030173165,
      0.1397999086892603,
      0.11603545971401141,
      0.1610469585966765,
      0.1292243078874793,
      0.15120735510145034,
      0.10904087099816184,
      0.1733118451916842,
      0.14920758388484456,
      0.14637345094167786,
      0.14801104073433624,
      0.11620137419986953,
      0.13717073358713083,
      0.1300469470935086,
      0.1325851320122733,
      0.13121702696716087,
      0.127448928521809,
      0.1351312302900327,
      0.1483251889779038,
      0.13243576112024913,
      0.13303595583936945,
      0.13295157199238128,
      0.13085318230884946,
      0.11622713691982156
    ],
    [
      0.320445652883492,
      0.0,
      0.4830349927662212,
      0.5108939672502408,
      0.4610127036539471,
      0.4563492359721395,
      0.4761654524986767,
      0.41330706299849695,
      0.43446651965648386,
      0.36235008615701103,
      0.5106871273694877,
      0.24336386168179125,
      0.5340541244872412,
      0.570596605721897,
      0.4847633722784126,
      0.44409373423673904,
      0.3991676840743581,
      0.4536407009546255,
      0.46880576469218616,
      0.4370480409411026,
      0.3634862843337652,
      0.32197298457262047,
      0.32573980694661775,
      0.4431755007057552,
      0.43941078677245904,
      0.50958152702302,
      0.3912188232994056,
      0.39628743340451544,
      0.31014919579058864
    ],
    [
      0.387897271164787,
      0.5293814528743894,
      0.0,
      0.5103039567256387,
      0.5787868652808692,
      0.43648770813242876,
      0.443562449480879,
      0.4326427332970224,
      0.47149793186356614,
      0.4174949578525118,
      0.4977989653856463,
      0.27258853394795435,
      0.4999707181437414,
      0.5014575341000951,
      0.44466910906554413,
      0.47138920554125985,
      0.42016067960851067,
      0.43905398223937686,
      0.5051404983823151,
      0.4694814078793996,
      0.4330992124383837,
      0.4004856184873462,
      0.4607884853724562,
      0.5190882749821852,
      0.5441932896218511,
      0.5154879270439441,
      0.48274320093655443,
      0.47446610955927015,
      0.35208123758724574
    ],
    [
      0.3203331101317861,
      0.5687331204567401,
      0.4903999742671865,
      0.0,
      0.44687614116214136,
      0.39306678484739277,
      0.5392343295642312,
      0.460207517524984,
      0.4913785067621248,
      0.3814088128432076,
      0.47669418340735104,
      0.3187990824522027,
      0.515626753380483,
      0.5526696849691863,
      0.4873203003151705,
      0.4764063176329456,
      0.37916839529634716,
      0.5424776912474361,
      0.5073517522273863,
      0.4667083193883712,
      0.3938472815198404,
      0.35412566341259666,
      0.38880392742000724,
      0.532842721105506,
      0.49370440359525136,
      0.5033741605879289,
      0.4389063897493899,
      0.4507426868871862,
      0.3805464262997822
    ],
    [
      0.406352557833795,
      0.5834452852397267,
      0.5882343513204893,
      0.5559775175386865,
      0.0,
      0.5143283734200093,
      0.5772891580046369,
      0.4778764487813363,
      0.5264324480482825,
      0.4664906353794238,
      0.5766372850417179,
      0.35800386046109733,
      0.5755363166628116,
      0.6044716035158442,
      0.5454828422624154,
      0.537906526002476,
      0.5539327247109573,
      0.5693663396985762,
      0.48844211629797196,
      0.49126144513385683,
      0.5275566352652266,
      0.46797546385299404,
      0.5015292633800903,
      0.5165077283419834,
      0.6038809501174252,
      0.5051067436222447,
      0.5367302236070917,
      0.5272785695751103,
      0.40981825586573484
    ],
    [
      0.350984461133943,
      0.5041211217928214,
      0.46584213436294153,
      0.4402930142779853,
      0.498872330723952,
      0.0,
      0.4453141713883042,
      0.4641934272140036,
      0.4949959373158255,
      0.3853482131938837,
      0.5352392273873112,
      0.2838965813818215,
      0.5802712039698477,
      0.4881726196244518,
      0.48668357577664545,
      0.45131638265432317,
      0.44203012797363694,
      0.4535982274418777,
      0.44187749645644203,
      0.41354091027845485,
      0.4575513069718673,
      0.3853607646711723,
      0.4014772950861738,
      0.49433243964242113,
      0.47932407002673383,
      0.4369391842983721,
      0.4505357973640116,
      0.39617477396891365,
      0.35442641381576
    ],
    [
      0.21079988028262164,
      0.3834935726537263,
      0.31157999351148313,
      0.37711415883592125,
      0.3222764252530248,
      0.27410149003938233,
      0.0,
      0.3375104278618084,
      0.26327917272624335,
      0.25463393579629545,
      0.3274545971444627,
      0.18888668956470833,
      0.343011610591196,
      0.3944559614413712,
      0.3543652259484251,
      0.32831670110746636,
      0.2936413929963506,
      0.35527544421498236,
      0.32967508501427223,
      0.2938602276875699,
      0.2840754372787546,
      0.24157031135625373,
      0.2530662195521327,
      0.35001652199614597,
      0.2716475107282532,
      0.3656733700577126,
      0.2902781049578598,
      0.2738322678164258,
      0.2268208814528232
    ],
    [
      0.25891584304849746,
      0.40555007076617944,
      0.37976569258630155,
      0.38665856891177386,
      0.3969843144608245,
      0.3738275063702834,
      0.42644249397752,
      0.0,
      0.3915211571101964,
      0.3153688519869542,
      0.43459367805938065,
      0.2575266708188595,
      0.4184269935942768,
      0.4528816667362625,
      0.4302745401712049,
      0.36946951806762707,
      0.3799314320374749,
      0.42814904758552474,
      0.3528294979397295,
      0.32404031638911834,
      0.3860093880505797,
      0.35135126891172686,
      0.33402197955857194,
      0.362763808516275,
      0.3767158863252289,
      0.42355249986784704,
      0.36057774121454367,
      0.37410372606810616,
      0.3062982588930556
    ],
    [
      0.3609489323518065,
      0.3950204498677976,
      0.3874219497497049,
      0.4304588384267687,
      0.41135831199260187,
      0.38938407358010285,
      0.3361729022481337,
      0.3748708565812846,
      0.0,
      0.29955690099814225,
      0.4474462473486829,
      0.27280672994421806,
      0.39266204527786086,
      0.48729391600130123,
      0.36801087152985335,
      0.3504651638988032,
      0.3926620615599168,
      0.3495774294157292,
      0.337885135605011,
      0.36565510476307117,
      0.3469102573440135,
      0.3454956217021876,
      0.34485415074681525,
      0.4218875119287031,
      0.45468943982447496,
      0.3624034451682909,
      0.4011481387107054,
      0.3376899721681026,
      0.27826833251808436
    ],
    [
      0.3330953483844432,
      0.4014317056806096,
      0.42579769150963864,
      0.36515731640765536,
      0.4352313880974368,
      0.38954230976927207,
      0.411995764978621,
      0.3814520056480961,
      0.38120298725685364,
      0.0,
      0.4220503630215844,
      0.23785921666758436,
      0.35757940950136713,
      0.3852791832804794,
      0.36331647838367087,
      0.38115646617583576,
      0.3415850195930137,
      0.32645562985856236,
      0.40714593363014084,
      0.38898430605316037,
      0.3550838347868519,
      0.32335136768579265,
      0.37188697469570564,
      0.37031903454285464,
      0.4370944367924692,
      0.39533462995798363,
      0.3668881299018387,
      0.3482467450877511,
      0.3461555404095402
    ],
    [
      0.35831305907342936,
      0.5732888437792658,
      0.5468074677523347,
      0.5812889820434961,
      0.5602624684663315,
      0.5646383222381917,
      0.5191172866952383,
      0.46750333743059236,
      0.5274585970153773,
      0.4526982010800007,
      0.0,
      0.3223115231178153,
      0.6792813945018925,
      0.610792079793083,
      0.5370976489544224,
      0.5252067490399939,
      0.5131599555198332,
      0.5405138044832143,
      0.538319165476491,
      0.5181328002764245,
      0.5082013985869525,
      0.44308824437373073,
      0.4581899083051968,
      0.5342109995812028,
      0.549844995087506,
      0.5447333747675047,
      0.519641583850073,
      0.4882599742215936,
      0.404214413430674
    ],
    [
      0.2562576715941871,
      0.2742209419609438,
      0.25968401860742585,
      0.2676451820584924,
      0.30228988664615275,
      0.28390324404129763,
      0.300726117701576,
      0.2535416722138779,
      0.2891751119035759,
      0.2529933079286042,
      0.27067047151652157,
      0.0,
      0.3041422962111783,
      0.2716256910313648,
      0.29925669825070345,
      0.29805829190664956,
      0.28308385285262316,
      0.307410807361299,
      0.24502392940821838,
      0.2770122288342842,
      0.2713137510891801,
      0.31484244969973796,
      0.30437353326243366,
      0.29112606130898766,
      0.2907501192002695,
      0.26334424378969157,
      0.28430759933832683,
      0.26708333506535165,
      0.27135541300418997
    ],
    [
      0.37757316498693605,
      0.523245077697235,
      0.4510556206526808,
      0.4622018981064837,
      0.4754374265471417,
      0.49692566583389786,
      0.42269266095889213,
      0.40950259366855435,
      0.4422023096240886,
      0.34947433672979367,
      0.5365732932984537,
      0.30600106732182675,
      0.0,
      0.5331511088686196,
      0.49946273762445026,
      0.48416036234328885,
      0.4146582204886906,
      0.489305305108102,
      0.4246136102575915,
      0.4062874078492704,
      0.4019104906033659,
      0.3974181683186824,
      0.41613512698293675,
      0.5374893900037112,
      0.4654308754821068,
      0.47307424828963973,
      0.43194123987031285,
      0.3688447850117236,
      0.33046426905943416
    ],
    [
      0.3692144200450669,
      0.5839631525216442,
      0.4926981649577209,
      0.5198368428330595,
      0.52660781145267,
      0.4569774271515481,
      0.474907721635794,
      0.4696934525436036,
      0.5289161292967803,
      0.3737685035428737,
      0.5090442173913527,
      0.27901131579167937,
      0.5934434552361412,
      0.0,
      0.5209080301847318,
      0.5198722715518951,
      0.45013815948081204,
      0.5339039507145624,
      0.48198833040984757,
      0.4669885779702867,
      0.45153006486371483,
      0.43008721343645795,
      0.42077155223847096,
      0.5291667154362403,
      0.4888293786885214,
      0.49780201525987544,
      0.44199960764083546,
      0.46227054591446204,
      0.3468490705121654
    ],
    [
      0.3325236442196422,
      0.5159339754937358,
      0.4573982148417737,
      0.46900708940950797,
      0.4676831564450603,
      0.4642964969120986,
      0.4828794908213816,
      0.43323194790713027,
      0.4378236320159894,
      0.3315627721927139,
      0.5227496208887412,
      0.3216415027235693,
      0.5414705928682186,
      0.5472545792836518,
      0.0,
      0.42576376297743734,
      0.4170716384297952,
      0.5023071893740818,
      0.41345494913397074,
      0.41059712296309425,
      0.41522144316515197,
      0.3960552624637528,
      0.40470235498131335,
      0.5100151063469915,
      0.4459200306387028,
      0.4396754539076464,
      0.4405627962878793,
      0.44826556409552576,
      0.35283610802305
    ],
    [
      0.31038180700069784,
      0.44861405106235597,
      0.4632193323851126,
      0.4256495235427207,
      0.45890762882925795,
      0.4239620537664557,
      0.4327090970876599,
      0.41865870349269674,
      0.40459522440363926,
      0.31678778344106395,
      0.4104443810782057,
      0.269623082242042,
      0.46986060006511265,
      0.45440562769703163,
      0.45341505950453675,
      0.0,
      0.4197098103136472,
      0.47682314623655,
      0.44666961701360064,
      0.4460201494651508,
      0.44226816899958243,
      0.3806935453013882,
      0.3871764801038935,
      0.4611869243989193,
      0.44535035495759234,
      0.4334507805593206,
      0.4417515847977018,
      0.37071131180166983,
      0.31978555983776635
    ],
    [
      0.33152070875795236,
      0.43664126606582676,
      0.43356525931214707,
      0.38961000869977247,
      0.5255614708840157,
      0.4287916371081786,
      0.41665197688168365,
      0.47319388072366486,
      0.44913088310053983,
      0.3600970938959065,
      0.48095898883396004,
      0.30357861560387533,
      0.4417341715062011,
      0.4504767439751456,
      0.4535575587757068,
      0.42844162744577474,
      0.0,
      0.48036783559924623,
      0.3740495478973975,
      0.3629129387640351,
      0.4941167451029409,
      0.38994615103704566,
      0.45650563914616304,
      0.44157339285258,
      0.473024588588552,
      0.4665745298988522,
      0.4406905661109741,
      0.439561892711382,
      0.35472768215192296
    ],
    [
      0.3353355211813891,
      0.5576040330715704,
      0.48799694494606505,
      0.5550169043157704,
      0.5288385465497523,
      0.3859685969482787,
      0.5428064303492701,
      0.49690823001154505,
      0.3919460333896456,
      0.35904482846408414,
      0.5407692311376153,
      0.32732747806863527,
      0.5515077014540792,
      0.5826196111912791,
      0.5348422434868747,
      0.5041986531146434,
      0.4777364777686335,
      0.0,
      0.480760210633125,
      0.44746074411188186,
      0.4551287881166721,
      0.45005777944731684,
      0.4373768913299598,
      0.49314587413800726,
      0.47186514987743156,
      0.4698885973530911,
      0.49254624467171837,
      0.44834562368358966,
      0.3488903015060818
    ],
    [
      0.3467687155634076,
      0.5803808090002716,
      0.5354343001064319,
      0.6159035692998196,
      0.498581042445402,
      0.4444441009623854,
      0.5792368585828753,
      0.4520171472238206,
      0.44936120092525944,
      0.43746712847730573,
      0.5055432438741831,
      0.32227232059749156,
      0.514320556441539,
      0.5556950086006036,
      0.5175915880261586,
      0.5168910410080461,
      0.4246203293002213,
      0.5197723955071158,
      0.0,
      0.511405360562939,
      0.4659764367301862,
      0.43042944182973497,
      0.4370812222126714,
      0.5266458584469329,
      0.4932094816267476,
      0.5679703412000112,
      0.480011026772192,
      0.4267602807358428,
      0.3932947618131355
    ],
    [
      0.3107894994756535,
      0.47330634990465636,
      0.41998736241046286,
      0.4417796382452266,
      0.4116502163860485,
      0.33082830805703245,
      0.40650871351594575,
      0.35549689198636036,
      0.3842367998497478,
      0.37133159713680475,
      0.45319736246378595,
      0.2512000995719552,
      0.41569607400444486,
      0.4599165750979044,
      0.3972877294228534,
      0.40081018838122384,
      0.37252107603339035,
      0.3839226006579639,
      0.41777856086719845,
      0.0,
      0.3977500843587347,
      0.28798058310926544,
      0.38318157309186884,
      0.4338383869874036,
      0.4191208651616236,
      0.44696650982723174,
      0.39191918862167174,
      0.37586575318688875,
      0.33620072497041686
    ],
    [
      0.36453890287773194,
      0.45502424933650176,
      0.4859144689648076,
      0.46238860647411606,
      0.5138883951947089,
      0.49046345498490074,
      0.44931420555115653,
      0.49099769575567165,
      0.46959893525645957,
      0.3763840432823382,
      0.5222608290100088,
      0.3382399590487506,
      0.5255505448493936,
      0.517929378842269,
      0.5098889546055314,
      0.5087250337788083,
      0.4794372265989679,
      0.4741631207844177,
      0.4429537641338126,
      0.45175845194004327,
      0.0,
      0.39834945641161856,
      0.4534958901437278,
      0.5147575353955418,
      0.49838552950469084,
      0.5065527832028247,
      0.4637280445693521,
      0.47186054633141095,
      0.36758900637391645
    ],
    [
      0.350160783882727,
      0.3990177480772128,
      0.4319584753544754,
      0.3967784011365243,
      0.4702619101527483,
      0.3887439249019431,
      0.4161810779088402,
      0.4698384491783172,
      0.45618308128442786,
      0.3783557501840491,
      0.4236988721217301,
      0.3688664195724367,
      0.45006421792516504,
      0.4567346992899015,
      0.4664072985983079,
      0.4154620121669599,
      0.42440967776013583,
      0.44683327194028744,
      0.40404811687833453,
      0.38424189820474686,
      0.43391955250044645,
      0.0,
      0.4580687642391159,
      0.4105398839251573,
      0.4512123991164949,
      0.46059793695598894,
      0.42156743981745004,
      0.4055060957414245,
      0.5080784112336123
    ],
    [
      0.3566724048710175,
      0.4509723907154197,
      0.5237447235771986,
      0.45449067452709135,
      0.4841139210801437,
      0.4541636671166611,
      0.4145170928519404,
      0.4364778449008808,
      0.4500379912408614,
      0.4345463470725459,
      0.5118879484555849,
      0.3320434373558825,
      0.47474825560554224,
      0.488046884452044,
      0.481034001569038,
      0.4885818527026595,
      0.4966097623370578,
      0.48558112648863916,
      0.4127271864144366,
      0.44618967142288324,
      0.4718813811220617,
      0.4566521495933378,
      0.0,
      0.47905539504222494,
      0.5647333257352749,
      0.4480678030771703,
      0.5654178250335506,
      0.4455834780548378,
      0.4180400874516428
    ],
    [
      0.411842086012274,
      0.473720090373525,
      0.527293955748495,
      0.5061566233550125,
      0.47375168845973326,
      0.4556489710096694,
      0.43301182592613907,
      0.42511631323696886,
      0.4894259754519761,
      0.36937711852519794,
      0.4795820995352478,
      0.2884607585879435,
      0.5623973939399147,
      0.5290017852301319,
      0.45841845033427875,
      0.4984508252245654,
      0.4243182084329835,
      0.436806213908026,
      0.445208516978745,
      0.4964704067245065,
      0.42084965346991265,
      0.3866857547860725,
      0.3896143121087843,
      0.0,
      0.4623317421155182,
      0.5014923552539166,
      0.44588463912131804,
      0.40822931564172094,
      0.37330309991501376
    ],
    [
      0.3516772993861801,
      0.44095053712636445,
      0.5070175627557099,
      0.4595705796630887,
      0.48307215240421875,
      0.4432065540818255,
      0.37315492813169326,
      0.37032078310068006,
      0.4615247530992943,
      0.35526133810276095,
      0.5165798597919651,
      0.26825899262115405,
      0.4591662906328411,
      0.45836901324311885,
      0.4654533360050517,
      0.4551450388186946,
      0.40896222371952096,
      0.4375610186229455,
      0.374824556366705,
      0.410232234249589,
      0.40031937087947167,
      0.36867823467004923,
      0.4399105232773133,
      0.46666041130673297,
      0.0,
      0.443074543850698,
      0.48332644149570214,
      0.4343811267434843,
      0.3254748943414054
    ],
    [
      0.35643703659233594,
      0.5723275456414174,
      0.493209569396724,
      0.5378730746536757,
      0.462875735460091,
      0.432691293837572,
      0.49486367311402146,
      0.4281556939013056,
      0.41993587750584305,
      0.3971253487749329,
      0.5448223602927673,
      0.25405580051729415,
      0.5264223365732668,
      0.53755463849512,
      0.4986117227038036,
      0.48475778272598347,
      0.4092980607863501,
      0.44221991605005173,
      0.4948497260733913,
      0.4794491136652441,
      0.44367075676038836,
      0.31949496846635195,
      0.40304560904256137,
      0.5282628006076118,
      0.44293647003019565,
      0.0,
      0.432544836202825,
      0.3723785577856391,
      0.34248920077529976
    ],
    [
      0.36976677378544065,
      0.4533713382153002,
      0.5433692403645305,
      0.4649926304396572,
      0.5481601356646384,
      0.49068703522709134,
      0.46683309253474725,
      0.4269696590471288,
      0.5157507450214078,
      0.44469679184100186,
      0.5360304062268941,
      0.349338810301411,
      0.5328161343277233,
      0.5036408071564307,
      0.5048684780426755,
      0.5146121590926029,
      0.5040148480235767,
      0.5281902348371164,
      0.4899048220543767,
      0.5090334162535752,
      0.5049772639279466,
      0.4533771886346345,
      0.5760917213636814,
      0.5329521753997064,
      0.6058236592604938,
      0.4911663852722954,
      0.0,
      0.47367492825823776,
      0.3632332960787754
    ],
    [
      0.36574114665687785,
      0.43447809391794623,
      0.5441384911301408,
      0.5493714542342374,
      0.5184571023460851,
      0.40204716279887487,
      0.46342254443044917,
      0.4091024083725485,
      0.44250835227770713,
      0.37339972279291134,
      0.4565523411968262,
      0.31995879629657553,
      0.4380850703809134,
      0.504531457800995,
      0.4671042026196075,
      0.43471240058090665,
      0.4646109487037846,
      0.4612303387069461,
      0.42080581610337275,
      0.5083751393228513,
      0.4635019152727162,
      0.38652857034202337,
      0.4463842275515051,
      0.5071944870822971,
      0.5162835428356463,
      0.42183655118919017,
      0.4584295510966152,
      0.0,
      0.36140279401741027
    ],
    [
      0.28145686844215656,
      0.36420822768328853,
      0.360558123941944,
      0.3933575942512213,
      0.36363776306235307,
      0.3186279823227951,
      0.3442034986558058,
      0.3155457228901899,
      0.35459373057657295,
      0.34573866060659797,
      0.31809310408733116,
      0.2797360115373828,
      0.34212789543050826,
      0.34839645155676435,
      0.3658053802694854,
      0.3410744618522177,
      0.33229780849557367,
      0.3329100882356191,
      0.35740656555894956,
      0.39838230761501103,
      0.3113505736751392,
      0.3916871149700445,
      0.3509229869395114,
      0.3643596998068357,
      0.3349493830792545,
      0.3635991792705362,
      0.349151668180796,
      0.33222999653395346,
      0.0
    ]
  ],
  "row_avgs": [
    0.13787123828065947,
    0.4273310368972606,
    0.46114997560697046,
    0.4554198013733629,
    0.5212089881779289,
    0.44581118607835357,
    0.30359687920955974,
    0.3735197292151402,
    0.37296445683043455,
    0.3732385434913862,
    0.5138063063907807,
    0.28054349742096946,
    0.4402583021995682,
    0.47218528923938624,
    0.4409966249575575,
    0.41545826390662055,
    0.42777012147969445,
    0.4698547739399288,
    0.4838959131383119,
    0.3903953325994201,
    0.4644335361858386,
    0.42670487750174857,
    0.4616649510667011,
    0.4490303635502711,
    0.42364766423172356,
    0.44829855380114514,
    0.4892265777376106,
    0.4478640939306414,
    0.34487174462599424
  ],
  "col_avgs": [
    0.3388123061292956,
    0.4616734169202353,
    0.4515313037019503,
    0.45286101372598425,
    0.45570948433070185,
    0.4122389325619085,
    0.4367755330059061,
    0.40929887025737793,
    0.4278652493776945,
    0.3604281205773713,
    0.4614024164452947,
    0.28731064602843287,
    0.47190307152694944,
    0.47845101788861405,
    0.44793824591611564,
    0.43583769895586305,
    0.4083978277534297,
    0.4405209853878574,
    0.4155209721070904,
    0.41836125645436406,
    0.4083115894350359,
    0.3714710810737766,
    0.3978688446203469,
    0.4543371367431006,
    0.45189637235396496,
    0.4425127527282928,
    0.42276428590046705,
    0.39819602065582044,
    0.34282217050172675
  ],
  "combined_avgs": [
    0.23834177220497754,
    0.44450222690874797,
    0.45634063965446037,
    0.45414040754967355,
    0.4884592362543154,
    0.429025059320131,
    0.3701862061077329,
    0.39140929973625904,
    0.40041485310406455,
    0.36683333203437873,
    0.48760436141803776,
    0.28392707172470116,
    0.4560806868632588,
    0.47531815356400015,
    0.44446743543683653,
    0.42564798143124183,
    0.4180839746165621,
    0.45518787966389307,
    0.44970844262270115,
    0.4043782945268921,
    0.43637256281043724,
    0.3990879792877626,
    0.42976689784352395,
    0.4516837501466858,
    0.4377720182928443,
    0.445405653264719,
    0.4559954318190388,
    0.42303005729323095,
    0.3438469575638605
  ],
  "gppm": [
    618.6148964896536,
    602.7539361732458,
    608.6247338229662,
    608.7546448907214,
    604.8388403543311,
    626.6777719030381,
    606.3896210481847,
    624.9442574748557,
    615.7378146216646,
    646.9483792439343,
    605.1393524811259,
    684.2268835286088,
    601.2657646134643,
    597.3352466941872,
    612.047276207175,
    617.0642712117957,
    626.2875475817698,
    614.6045575060549,
    625.5053660382248,
    620.7548452357876,
    626.4233139242288,
    644.3350244192825,
    631.3373056872125,
    606.5606341937,
    605.3396546039918,
    612.0576028586029,
    622.4880660651041,
    629.7663336317531,
    652.8204554610683
  ],
  "gppm_normalized": [
    1.462345544836202,
    1.4779186487209313,
    1.495899315665951,
    1.4982633905756977,
    1.483646241996629,
    1.5395995236764015,
    1.490279373260717,
    1.5414273174033728,
    1.511352856425876,
    1.5882917308644005,
    1.485234527152196,
    1.6907572920740286,
    1.4758670829548566,
    1.4658778678991877,
    1.5086852525076624,
    1.5161697499956865,
    1.5394715775322125,
    1.5121516177997183,
    1.5356326980986705,
    1.5247194774159263,
    1.5411169131801992,
    1.5833420805630136,
    1.5498876900450884,
    1.4952474253815728,
    1.4899423923176678,
    1.5046745530304233,
    1.5280065311163586,
    1.548040069802251,
    1.6032710590499033
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420
  ],
  "response_lengths": [
    3587,
    2519,
    2330,
    2298,
    2236,
    2468,
    3297,
    2399,
    2613,
    2254,
    2329,
    2547,
    2611,
    2445,
    2249,
    2546,
    2369,
    2103,
    2258,
    2397,
    2262,
    2123,
    2095,
    2439,
    2508,
    2106,
    2105,
    2199,
    2395
  ]
}