{
  "example_idx": 54,
  "reference": "Under review as a conference paper at ICLR 2023\n\nHOW TO KEEP COOL WHILE TRAINING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nModern neural networks used for classification are notoriously prone to overly confident predictions. With multiple calibration methods proposed so far, there has been noteworthy progress in addressing overconfidence issues. However, to the best of our knowledge, prior methods have exclusively focused on those factors that affect calibration, leaving open the question of how (mis)calibration circles back to negatively impact network training. Aiming to better understand such dependencies, we propose a temperature-based Cooling method to calibrate classification neural networks during training. Cooling results in better gradient scaling and reduces the need for a learning rate schedule. We investigate different variants of Cooling, with the simplest, last layer Cooling, being also the best-performing one, improving network performance for a range of datasets, network architectures, and hyperparameter settings.\n\n1\n\nINTRODUCTION\n\nTraining neural networks can be a challenging task, with optimal performance depending on the right setting of hyperparameters. For this reason, finding a suitable network configuration can often take multiple costly training runs with varying parameters of the learning rate schedule, the optimizer and the batch size. Apart from standard learning rate schedules like piecewise constant schedules and exponential decay schedules, there has been activate research in developing better schedules: Among the most prominent of these are learning rate warmup (Goyal et al., 2017; He et al., 2016a) and cosine decay (Loshchilov & Hutter, 2017) schedules.\n\nComplementary to these challenges, (Guo et al., 2017) found that modern convolutional classification networks are often poorly calibrated, leading to overly confident predictions. They investigated multiple methods to improve calibration, with a simple temperature scaling method performing best: the network’s output logits are multiplied by a temperature parameter, optimised on a validation dataset after training. Importantly, this leaves the maximal value and therefore the predicted class label unchanged since all the logits are multiplied by the same temperature value.\n\nSince then, multiple papers (Kull et al., 2019; Kumar et al., 2019; 2018; M ̈uller et al., 2019; Gupta et al., 2021) proposed methods aiming to even better calibrated networks. More recently, (Desai & Durrett, 2020; Minderer et al., 2021) investigated the calibration of state-of-the-art nonconvolutional Transformer networks (Vaswani et al., 2017; Dosovitskiy et al., 2021) and MLPMixers (Tolstikhin et al., 2021). They concluded that such architectures may have benefits, with further work needed to fully understand the factors contributing to calibration.\n\nDespite initially leaving the accuracy unchanged, we have noticed that temperature scaling can have an intriguing effect as training continues: scaling the output logits results in a change in the crossentropy loss, which in turn leads to scaled gradient updates and subsequently new parameter values. During training, this can lead to a significant increase in accuracy. To the best of our knowledge, temperature scaling has until now only been applied post hoc after completing network training. However, our investigation shows that networks become gradually overconfident during training (they overheat), which seems to have a detrimental effect on learning. This has motivated us to modify the original temperature scaling and propose a Cooling method to calibrate neural networks during training.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nOur Contributions\n\n• A Cooling method for calibrating classification neural networks during training. We propose two basic variants called last layer Cooling and distributed Cooling, and one hybrid variant called periodically redistributed Cooling.\n\n• A mathematical analysis of the effect of Cooling on the network gradients, with a compar-\n\nison of different Cooling variants.\n\n• An empirical investigation of the effects of Cooling on a range of metrics, including network weights, gradients, output logits and the ECE (expected calibration error) calibration measure.\n\n• A broad set of experiments for different tasks (image classification and semantic segmentation), datasets and network architectures. We also include an extensive ablation study, involving different activation functions, optimizers, and hyperparameters such as the learning rate schedule, the Cooling factor and the use of weight decay and data augmentation. Our experiments indicate an interplay between the learning rate and calibration during training. Importantly, if well-calibrated, networks can train well without the use of a learning rate schedule.\n\n2 BACKGROUND AND NOTATION\n\nLet fθ : Rd → Rs denote the function of a classification neural network with parameters θ, mapping a d-dimensional input (in our case an image) x to an s-dimensional logits vector z = fθ(x). During training, each input x comes with a class-probability or label vector y, denoting probabilities of x belonging each of s classes. This is usually (but not necessarily) a one-hot vector corresponding to a so-called ground-truth class label, i∗.\n\nIn its simplest variant, we suppose the network consists of L affine (dense or convolutional) layers, each followed by a non-linear activation function. For the ith layer (1 ≤ i ≤ L) this gives an expression of the form xi = ρ(pi) = ρ(Wixi−1 + bi) with weight matrices Wi, bias vectors bi, non-linearities ρ, pre-activation values pi and layer inputs and outputs xi−1 and output xi, respectively. (More generally, our method can be applied to any neural network, involving arbitrary functions and layers like e.g. attention, batch normalization and skip connections.) The output logits are then passed through the softmax function σ which results in a vector (cid:98)y = σ(z) of class probabilities. The classification network is trained to minimize the categorical cross-entropy loss function\n\nL(z) = H(y, σ(z)) = −\n\ns (cid:88)\n\ni=1\n\nyi log((cid:98)yi) .\n\n(2.1)\n\nWe say that a network is well-calibrated if the output values (cid:98)y can be interpreted as true probabilities. Intuitively, if a network makes 100 predictions with 90% confidence, we would expect that 90% are correctly classified. (Guo et al., 2017) observed that convolutional neural networks tend to display over-confidence in their results, in that (cid:98)yi∗ gives an over-estimate of the probability that λi is the correct label. Thus the networks are badly calibrated, which we metaphorically express by saying that the networks become overheated.\n\n(Guo et al., 2017) found that simply multiplying the pre-softmax logits zi by a factor τ does an excellent task of improving the network’s calibration. Thus, the task of correcting the calibration of the network is to find a constant τ to correct its output, so that it becomes\n\ny = σ(τ z) = σ(τ fθ(x)) .\n\n(2.2)\n\nThe optimal τ is found by minimizing the log-likelihood cost function on a small calibration set, held back from the training data. This operation is carried out when the network is fully trained. Usually, one finds that the optimum value is τ < 1. This process is known as temperature scaling.\n\nWe refer to (Guo et al., 2017) for a more detailed introduction to network calibration.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHOD: COOLING WHILE TRAINING\n\n3.1 FUNDAMENTALS\n\nWhen the network overheats, the predicted values (cid:98)yi become too close to 0 or 1. This can cause problems with gradients becoming large. We hypothesise therefore that keeping the network at the correct temperature during training can lead to improved convergence. Our proposed operation is to periodically correct the network, by multiplying the logits zi by the optimal temperature correcting constant. There are multiple ways to implement Cooling, the most basic being last layer Cooling:\n\nDefinition 3.1. A network performs last layer Cooling if before the softmax function there is a final scaling layer, multiplying the network output logits z by a constant scalar τ > 0. This value τ is not modified during the batch updates of the gradients, but is corrected using a held-out validation set at the end of the Cooling period.\n\nCooling factor. We investigate the effect of taking the optimal temperature parameter τ to some power κ, which we call the Cooling factor. Let us assume that τ < 1 (which is mostly the case). Then for κ > 1, multiplying by τ κ results in smaller logits, a scenario which we call overcooling. Conversely, κ < 1 produces larger logits, resulting in an undercooling of the network. We note that as κ → 0, we approach standard network training without Cooling. As we show in the experiments, using a suitable value for κ can have positive effects on the training stability and performance.\n\nCooling period. We call the periodic time interval after which we perform temperature scaling the Cooling period. Typically, we let the Cooling period be equal to one epoch of training. We trained the VGG-style network described in §4.2 on CIFAR10, using an Nvidia GeForce GTX Titan Z GPU and a 16-core Intel Xeon CPU E5-2640 v3. The actual training takes approximately 56 seconds per epoch, whereas Cooling takes approximately 1.05 seconds. Hence, one run of Cooling takes approximately 1.9% of the time of one training epoch. We experimented on CIFAR10 with a Cooling period of one batch, but found no additional performance benefits. Thus, it seems that Cooling once per epoch is sufficient.\n\n3.2 DISTRIBUTED COOLING\n\nInstead of waiting to perform scaling after the final layer, it is possible to redistribute the temperature correction across the network, by scaling layers other than the last. Suppose the optimal temperature is τ ∈ R+. When we redistribute the temperature across the network, we would like the temperature correction to gradually take effect. More precisely, we want to ensure that (1) each layer multiplies its input by an additional factor of β = τ 1/L, so that the output of the ith layer is now x′ i = βixi. Moreover, (2) the inputs to the non-linearities ρ have to be the same as before scaling because we would otherwise change the network output in a non-linear manner. Finally, (3) the output logits z should be multiplied by βL = τ .\n\nDefinition 3.2. Let the notation be as in § 2. Let β = τ 1/L. A network performs distributed Cooling if after each Cooling period,\n\n1. the weight matrix Wi is multiplied by β, resulting in a new matrix W′\n\ni := βWi;\n\n2. the bias vector bi is multiplied by βi, resulting in a new vector b′\n\ni := βibi;\n\n3. the activation ρ is changed to ρβi, defined by ρβi(x) = βiρ(β−ix).\n\nLemma 3.3. If a network performs distributed Cooling, then, compared to no Cooling,\n\n1. the output of the ith layer is x′\n\ni = βixi, for 1 ≤ i ≤ L − 1;\n\n2. the input to each of the non-linearities is left unchanged;\n\n3. the output logits are scaled by a factor of τ : z′ = τ z.\n\nProof. We give the proof in in Appendix A.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3.1 Periodically Redistributed Cooling (simplified) Inputs: Neural network fθ with L linear layers (Wi, bi)\n\nFinal Cooling layer sτ Validation data set Xval Cooling factor κ Maximal temperature τmax; reset temperature τre\n\nOutput: Calibrated Network fθ′ After each Cooling period:\n\nexcess ← τ /τre τ ← τre √\nkernel scale ← L bias scale ← 1 for i = 1 to L do\n\n1: τ ← temperature minimising cross entropy on Xval 2: τ ← τ κ 3: if τ > τmax then 4: 5: 6: 7: 8: 9: 10: 11: 12: Update sτ using new τ\n\nbias scale ← bias scale ∗ kernel scale Wi ← Wi/kernel scale bi ← bi/bias scale\n\nexcess\n\n▷ over- or undercooling\n\nIt may be observed that if ρ is the ReLU function, then ρβi = ρ, so the activation layer is not changed. (More generally, this holds for piecewise-linear ReLU variants such as CReLU, Leaky ReLU and PRELU.) Note also that τ is usually less than 1, indicating overheating of the network, and hence multiplications by β and βi result in a decrease in the values of the parameters Wi and bi.\n\nWhen performing last layer Cooling, we have observed that the network can correct for this by overheating even more. This means that the required temperature correction τ becomes smaller and smaller, towards zero. At the same time, this means that the parameters of the layers (for instance W and b in affine layers) become larger and larger, eventually causing numerical overflow. A possible solution to this problem is to keep track of the overheating parameter τ , and when it becomes too small, redistribute the temperature correction over all layers:\n\nDefinition 3.4. A network performs periodically redistributed Cooling if for some τmax, τre > 0,\n\n• it performs last layer Cooling as long as τ < τmax (i.e. the optimal temperature is less than\n\na specified maximal temperature);\n\n• it redistributes the excess temperature τ /τre across the layers as in Definition 3.2 if τ >\n\nτmax.\n\nThe values τmax and τre are manually specified. In our experiments, we used τmax = τre = 100 for periodically redistributed Cooling, but other values can also be considered.\n\nSummary. Algorithm 3.1 gives the pseudocode of our proposed Cooling method. The algorithm shows periodically redistributed Cooling, since it is the most general case. We can recover last layer Cooling and distributed Cooling as special cases with (τmax, τre) = (∞, ∞) and (τmax, τre) = (0, 1), respectively. The algorithm displayed above is simplified for the sake of clarity: it does not take heed of layers like attention and batch normalization and excludes skip connections. These layers are straightforward to address in a general implementation of the Cooling method.\n\n3.3 EFFECTS ON GRADIENT VALUES\n\nWe now perform an analysis of the effect Cooling on the network gradients.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.3.1 LAST LAYER COOLING\n\nProposition 3.5 (Gradients of last layer Cooling). Let the notation be as in § 2. If C = L(σ(τ z)) where σ is the softmax function and L is the cross-entropy loss, the derivative with respect to any network parameter w ∈ θ is given by\n\n(cid:68)\n\n=\n\n∂C ∂w\n\nσ(τ z) − y,\n\n(cid:69)\n\n∂τ z ∂w\n\n= τ\n\n(cid:68)\n\nσ(τ z) − y,\n\n(cid:69)\n\n,\n\n∂z ∂w\n\n(3.1)\n\nexpressed as the inner-product of two vectors.\n\nProof. When τ = 1, Equation 3.1 is a known result of a simple computation. The general case is an application of the chain rule.\n\nThe difference ε = σ(z) − y may be termed the residual, namely the difference Interpretation. between the ground-truth label probabilities y and the label probabilities σ(z) computed by the network. The derivative ∂C/∂w is then the inner product of this residual with the vector ∂z/∂w.\n\nWe can use this formula to analyze the effect of temperature-scaling by τ . Suppose τ = 1, so there is no heating correction. As is known, in this case probabilities tend to be overestimated, so that σ(z) approaches y. All values of σ(z) other than the ground truth become very small, meaning that values of ∂zi/∂w are multiplied by small values, and so are ultimately ignored, with harmful effects on convergence.\n\nSetting τ < 1 results in a more evenly distributed (less peaked) vector σ(τ z), meaning that all values of zi and ∂zi/∂w have an effect on the gradient.\n\n3.3.2 DISTRIBUTED COOLING\n\nNow, we consider what happens to gradients when distributed Cooling is applied. We simplify the analysis by thinking of scaling occurring in two steps. First global temperature scaling is applied by modifying the final layer so that its output is multiplied by τ . Subsequently, distributed scaling is applied to all layers resulting in the output of the i-th layer being multiplied by βi, in a way that the network output is unchanged. The effect of the final-layer scaling on gradients was addressed earlier. Now we concentrate on the effect of distributed scaling on gradients in the network.\n\nConsider a network with N layers, labelled 0 to N − 1, let xi be the input to the i-th layer (which is also the output of the i − 1-th layer), and xN the output of the last layer. Let another network have layer inputs denoted by x′ i. Definition 3.6. We will say that two networks are scale-equivalent if for inputs x0 = x′ constants βi with β0 = βN = 1 such that x′ outputs xN = x′\n\ni = βixi. Evidently, for the same input x0 = x′\n\nN are the same, since βN = 1.\n\n0 there are 0, the\n\nIt will be observed, however that the gradients of the parameters of these networks will be different. Let the first (unprimed) network be represented by xi+1 = ρi(Wixi + bi) where ρi is an activation function, possibly different for each i. Then, given numbers βi with β0 = βN = 1, an equivalent network is given by x′\n\ni+1 = ρ′\n\ni(W′\n\nix′\n\ni + b′\n\ni) where\n\nW′\n\ni = βi+1β−1\n\ni Wi\n\nand\n\nb′\n\ni = βi+1bi\n\nand\n\nρ′ i = ρβi+1\n\n(3.2)\n\nand ρβ is a modified activation function given by ρβ (x) = βρ(β−1x). (It should be noted that if ρ is a ReLU activation, then ρ = ρβ .) Then, x′\n\ni = βixi for all i, as required.\n\nThus, with Ti representing the transformation xi (cid:55)→ ρi(Wixi + bi) = xi+1, (and similarly T ′) we compare the two networks:\n\nσ ◦ TN −1 ◦ TN −2 ◦ . . . ◦ T0(x0)\n\nwhere σ represents the final softmax layer.\n\nand σ ◦ T ′\n\nN −1 ◦ T ′\n\nN −2 ◦ . . . ◦ T ′\n\n0(x0) ,\n\nIt is evident that these two networks carry out the same operation. However, it will be shown that if optimized using a gradient-descent based method, the update of their parameters will be different,\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nand the trajectory of the parameters in the path towards the optimum during training will be quite different.\n\ni,j be the corresponding parameter of the primed (distributively scaled) network. The following\n\nLet Wi,jk be one of the entries of Wi and bi,j be one of the parameters of bi. Similarly, let W′ and b′ will be shown: Theorem 3.7. Let C = L(z) where z = σ ◦ fθ(x0) = σ ◦ f ′ fθ and f ′ parameter matrix Wi and bi,j the j-th entry of parameter vector bi. Then\n\nθ′(x0) for scale equivalent networks θ′. Functions σ and L are softmax and loss functions. Let Wi,jk be the (j, k)-th entry of\n\ni,jk\n\n∂C ∂W′\n\ni,jk\n\n=\n\nβi βi+1\n\n∂C ∂Wi,jk\n\nand\n\n∂C ∂b′\n\ni,j\n\n=\n\n1 βi+1\n\n∂C ∂bi,j\n\n(3.3)\n\nProof. Define pi+1 = Wixi + bi, and xi+1 = ρi(pi+1), and similarly primed quantities. We see that x′\n\ni = βipi for all i. Let fi+1 be the mapping defined by\n\ni = βixi and p′\n\nC = fi+1(pi+1) = L ◦ σ ◦ TN −1 ◦ TN −2 ◦ . . . ◦ Ti+1 ◦ ρi(pi+1)\n\nnamely, the part of the network “downstream” from pi+1 (including the activation function ρi in the i-th layer, and the softmax and loss functions). Function f ′ i+1 is similarly defined for the primed network.\n\nWe apply the chain rule:\n\nA similar formula holds for the primed case.\n\n∂C ∂Wi,jk\n\n=\n\n∂C ∂pi+1\n\n∂pi+1 ∂Wi,jk\n\nNow, since C = fi+1(pi+1) = f ′\n\ni+1(p′\n\ni+1) = f ′ ∂C\n\ni+1(βi+1pi+1) we see ∂C ∂pi+1\n\ni+1\n\n= β−1\n\n∂p′\n\ni+1\n\n(3.4)\n\nNext, we compare ∂pi+1/∂Wi,jk and ∂p′\n\ni+1/∂W′\n\ni,jk.\n\nLet Ejk be the matrix with an entry 1 in position (j, k) and 0 elsewhere. Then ∂pi+1/∂Wi,jk = Ejkxi . On the other hand,\n\n∂p′\n\ni+1/∂W′\n\ni,jk = Ejkx′\n\ni = Ejk(βixi) = βi ∂pi+1/∂Wi,jk\n\nPutting this together with equation equation 3.4 we see that ∂C/∂W′ as required.\n\ni,jk = (βi/βi+1) ∂C/∂Wi,jk\n\nIn the case where bi,j is an entry of bi, we see that ∂p′\n\ni+1/∂b′\n\ni,j = ∂pi+1/∂bi,j so,\n\n∂C/∂b′\n\ni,j = (1/βi+1) ∂C/∂bi,j .\n\nRelative gradients. Since a small change to a small parameter is more important to the same change to a large parameter, it is perhaps more important to determine the ratio (∂C/∂θ)/θ, which determines by what ratio a parameter is changed during gradient update. This gives:\n\n∂C/∂W′ W′\n\ni,jk\n\ni,jk\n\n∂C/∂b′ b′ i,j\n\ni,j\n\n=\n\n=\n\nβ2 i\nβ2\n\ni+1\n\n∂C/∂Wi,jk Wi,jk\n\n1 β2\n\ni+1\n\n∂C/∂bi,j bi,j\n\nInterpretation. The effect of distributed scaling is to individually change the relative effect of gradients over the network. In particular, if βi = τ i/N , for i = 0, . . . , N −1 and βN = 1, distributing the scale evenly across the network, with τ < 1, then the effect is to modify the gradients and relative gradients across the network. This may have the effect of mitigating the effect of gradient vanishing. Distributed scaling can be used to control the magnitudes of the output xi at each level.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Plots showing the test accuracy of networks trained with last layer Cooling and various Cooling factors. Left: VGG network trained on CIFAR10. Right: ResNet50 network trained on CIFAR100. The non-Cooling baselines are shown as dashed. All Cooling factors κ ≤ 1 outperform the baseline, the best one by 4.6%. Training diverges for Cooling factors κ > 1. (All means and standard deviations are computed over three runs.)\n\n4 EXPERIMENTS\n\n4.1 GENERAL SETUP\n\nWe explore the use of Cooling on two image classification datasets and one semantic segmentation dataset. We use 99% of the CIFAR ”training sets” (corresponding to 50,000 images in total) for training and 1% as a validation set to optimise the temperature τ on. We train our networks using either the SGD optimizer with a momentum of 0.9 or the Adam optimizer (Kingma & Ba, 2015) with ε = 0.1, β1 = 0.9, β2 = 0.999. All networks are trained using the TensorFlow (Abadi et al., 2015) framework.\n\n4.2\n\nIMAGE CLASSIFICATION: CIFAR10\n\nSetup. We train a small VGG-style network (Simonyan & Zisserman, 2014) on the CIFAR10 dataset (Krizhevsky, 2009). The network consists of a sequence of 6 convolutional layers of filter size 3 × 3 with 32, 32, 64, 64, 128 and 128 channels, respectively, followed by two dense layers with 128 and 10 output nodes, respectively. In total, the network has approximately 620,000 trainable parameters. The hidden layers either use the ReLU (Fukushima, 1980; Nair & Hinton, 2010) or the CReLU (Shang et al., 2016) activation. When we use learning rate warmup, we linearly increase the learning rate from 0 to 0.01 over 2 epochs. When we do not use warmup, we directly start with a learning rate of 0.01. In our learning rate schedule ablations, we experiment with (1) no schedule, (2) a piecewise linear schedule which drops by a factor of 0.1 after 30 and 40 epochs, (3) a schedule with linear decay from the initial rate to 0, a (4) schedule with exponential decay with a total drop by either a factor of 0.01 (”slow”) or 0.001 (”fast”) and (5) a cosine decay schedule (Loshchilov & Hutter, 2017). We train the network for 50 epochs. We use a batch size of 64.\n\nResults. As shown in Table 1, Cooling can have a significantly positive impact on network performance. We see a stark difference between various LR schedules. Whereas smooth schedules (where the learning rate changes after each batch) hardly benefit from Cooling, there is a noticeable benefit for piecewise constant schedules and a drastic improvement when no schedule is employed. Starting from the lowest performance at 74.7% test accuracy, last layer Cooling increases the test accuracy by 4.6%.\n\nFigure 1 (left) shows an ablation of last layer Cooling, involving various Cooling factors κ. Last layer Cooling shows little sensitivity to the Cooling factor, as long as κ ≤ 1.0. Values greater than 1.0 lead to divergence. On the other hand, all values of κ ≤ 1.0 produce networks outperforming the non-Cooling baseline.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nLR Schedule\n\nNo Cooling Distributed Periodic Last Layer\n\nCosine Exp Decay Fast Exp Decay Slow Linear None Piecewise Const\n\n78.6±0.2 77.0±0.1 77.7±0.3 78.7±0.6 74.7±1.1 78.0±0.2\n\n78.3±0.1 77.1±0.1 77.3±0.4 78.4±0.3 77.6±0.2 79.1±0.5\n\n78.9±0.3 77.3±0.2 77.6±0.1 78.8±0.5 79.2±0.4 79.1±0.5\n\n78.9±0.1 76.8±0.6 77.5±0.3 78.7±0.1 79.2±0.2 79.2±0.0\n\nTable 1: Test accuracy of a VGG network trained on CIFAR10 with various learning rate schedules and Cooling modes. The different Cooling modes perform at least on par with baselines on all LR schedules. Cooling considerably outperforms the baseline when no schedule is used. Significant gains are also achieved when a piecewise constant schedule is used. (All means and standard deviations are computed over three runs.)\n\nTable 2 shows that Cooling works well for both ReLU and CReLU activation functions. In particular, CReLU diverged in all of our experiments without learning rate schedules, but converged when Cooling was used. As for ReLU, we note that last layer and periodically redistributed Cooling outperform pure distributed Cooling.\n\nActivation No Cooling Distributed Periodic Last Layer\n\nCReLU ReLU\n\ndivergence 74.7±1.1\n\n79.7±0.4 77.6±0.2\n\n84.2±0.1 79.2±0.4\n\n84.4±0.2 79.2±0.2\n\nTable 2: Test accuracies of a VGG network trained without a learning rate schedule on CIFAR10 with different activation functions and Cooling modes. Last layer scaling and periodically redistributed scaling perform best for both activation functions. Training diverges for CReLU without Cooling and without a LR schedule. (All means and standard deviations are computed over three runs.)\n\nThe effect of the Cooling factor on the inverse of the network temperatures is shown in Figure 2. In the left plot, where a smaller Cooling factor and last layer Cooling is used, the inverse temperature grows much more slowly and only exceeds 100 after 19 epochs. In the right plot, where periodically redistributed Cooling with τre = τmax = 100 is used, the first temperature reset already happens after 9 epochs.\n\nWe present further experimental results on CIFAR10 in Appendix B.\n\n4.3\n\nIMAGE CLASSIFICATION: CIFAR100\n\nSetup. We train a ResNet50 network (He et al., 2016b) on the CIFAR100 dataset (Krizhevsky, 2009). The network consists of a sequence of 50 convolutional layers of varying filter sizes and channel numbers, followed by a global average pooling layer. The network uses skip connections and has approximately 23.5 million trainable parameters in total. The hidden layers either use the ReLU (Fukushima, 1980; Nair & Hinton, 2010) or the CReLU (Shang et al., 2016) activation. In our learning rate schedule ablations, we try out (1) no schedule, (2) a piecewise linear schedule with drops by a factor of 0.1 after 80, 120, 160 and 180 epochs. We train the network for 200 epochs. We use a batch size of 64.\n\nResults. Similar to the CIFAR10 experiments, we present an ablation on the effect of different Cooling factors on last layer Cooling. In Figure 1 (right) we notice the same pattern: Cooling factors κ that do not exceed 1.0 yield neural network models that outperform the baseline. On the other hand, we observe once more that κ > 1 leads to the divergence of network training.\n\n4.4 SEMANTIC SEGMENTATION: ADE20K DATASET\n\nSetup. We train a small U-Net Ronneberger et al. (2015) architecture on the challenging ADE20K dataset Zhou et al. (2019), which includes 150 semantic categories. This dataset contains 20,000\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Comparison of the increase of network temperatures for different Cooling factors. Both images show the training of a VGG network on CIFAR10 with ReLU activations. Left: last layer Cooling, CF: 0.75. Right: periodically redistributed Cooling, CF: 1.0. We note that a larger Cooling factor causes a much steeper increase in the temperature. For last layer Cooling, the temperature keeps on growing, whereas for periodically redistributed Cooling, the temperature is redistributed whenever it exceeds a value of 100.\n\nimages for training and 2,000 images for validation, on which we report results. We leave aside 320 images from the training set when performing Cooling. We work with images of size 256 × 256 and our U-Net architecture has ≈ 9 million trainable parameters.\n\nResults. We compare our proposed last layer Cooling against the baseline, with no temperature scaling. For the former we obtain 22.1 mIoU and 71.9% accuracy, while for the latter we obtain 21.0 mIoU and 70.9% accuracy. This shows that our proposed cooling method is also beneficial on denser, pixel-wise classification tasks. Further investigation on larger architectures and across multiple design choices could further reveal the full potential of Cooling.\n\n5 DISCUSSION AND CONCLUSION\n\nOur proposed Cooling method to adaptively calibrate classification neural networks produces significant benefits in terms of network performance and training stability. Theoretical and empirical findings point to significant benefits resulting from differently scaled gradients during network training. As a result of experiments on different tasks, datasets and network architectures, as well as an ablation study on different hyperparameter settings we find that that Cooling gives a significant performance benefit over relevant baselines. In particular, we notice that Cooling greatly reduces the need for a learning rate schedule.\n\nEven though all versions of Cooling re-scale the network to the same mathematical function, they all produce differently parameterised networks. This reparameterisation has a strong impact on gradients, resulting in different network functions as training progresses. This raises the question: What are the general conditions on the parametrisation of a network to achieve optimal training?\n\nAnother highlight of our work is the connection between calibration and the learning rate. There are indications that well-calibrated networks are more stable in training and less reliant on the ’right’ learning rate schedule. Since training stability is critical in a number of classification tasks (e.g. when training the discriminator of a GAN), a deeper investigation into the relation between calibration and training stability could be a promising direction for future research.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMart ́ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man ́e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi ́egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.\n\nShrey Desai and Greg Durrett. Calibration of Pre-trained Transformers.\n\nIn Proceedings of the\n\nConference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n\nKunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193–202, 1980.\n\nPriya Goyal, Piotr Doll ́ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706. 02677.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International networks. Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1321–1330. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/ guo17a.html.\n\nKartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, In International Conferand Richard Hartley. Calibration of neural networks using splines. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= eQe8DEWNN2W.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition, 2016a.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\nIdentity mappings in deep residual networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision – ECCV 2016, pp. 630–645, Cham, 2016b. Springer International Publishing. ISBN 978-3-31946493-0.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2015. URL\n\nhttp://arxiv.org/abs/1412.6980.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University\n\nof Toronto, 2009.\n\nMeelis Kull, Miquel Perello Nieto, Markus K ̈angsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 8ca01ea920679a0fe3728441494041b9-Paper.pdf.\n\nAnanya Kumar, Percy S Liang, and Tengyu Ma.\n\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett\n\nVerified uncertainty calibration.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\n(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ f8c0c968632845cd133308b1a494967f-Paper.pdf.\n\nAviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2805–2814. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr. press/v80/kumar18a.html.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=Skq89Scxx.\n\nMatthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 15682–15694. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 8420d359404024567b5aefda1231af24-Paper.pdf.\n\nRafael M ̈uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ f1748d6b0fd9d439f71450117eba2725-Paper.pdf.\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines,\n\n2010.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nWenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pp. 2217–2225. JMLR.org, 2016.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nIlya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. CoRR, abs/2105.01601, 2021. URL https://arxiv.org/abs/2105.01601.\n\nAshish Vaswani, Noam Shazeer, Niki\n\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\nJones, Attention is all you need, https://proceedings.neurips.cc/paper/2017/file/\n\nJakob Uszkoreit,\n\nParmar,\n\nLlion\n\nURL\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302–321, 2019.\n\n11",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes to use temperature scaling during the training process, thus improving accuracy and reducing the need for learning rate schedules. In the theoretical part of the paper, the gradients have been calculated analytically, comparing the gradients with and without the proposed modifications. In the experimental part, the proposed methods have been applied on several tasks and neural architectures, showing that the methods result in improvements in classification accuracy.\n\n# Strength And Weaknesses\n\nStrengths:\n\n* The paper is well-written and clear.\n* The idea of using calibration during training to improve performance can be considered novel and the methods are well justified and simple.\n* The empirical advances are nicely complemented with a theoretical study and discussion about what might be the reasons for the achieved improvements.\n\nWeaknesses:\n\n* My main concern is that in CIFAR10 classification the authors have only considered an architecture that results in accuracies much below 80%, whereas modern architectures (plus data augmentation) go well beyond 90%. It would be important to experiment with at least one better architecture also. In CIFAR100, a better architecture has been used, but also for this dataset there is room for improvement so that the baseline method would be at least 70% accurate.\n\n* In the introduction, the paper promises to measure ECE (expected calibration error) but never gets to doing it, not even in the appendix. On one hand, this promise could simply be dropped, but actually it would be an important thing to do, because calibration is an important part of the motivation and intuition behind the proposed method. For example, it would be good to have plots about how ECE (evaluated on test data) is evolving during training alongside with accuracy improvements.\n\n* Figure 1 (left) starts from the cooling factor value of about $0.5$. In order to support the discussion better, lower values of the cooling factor could have also been considered in that plot.\n\n* It would have been good to see more figures about gradient norm evolution in the appendix, perhaps even for all situations considered in Table 1. This would help to compare whether the evolution achieved with the proposed methods is different or similar to the evolution observed with different learning rate schedules.\n\nMinor weaknesses:\n\n* Some parentheses are missing in the proof of Lemma A.1., e.g. the first formula should be $x'_1=\\beta\\rho(\\beta^{-1}(W'_1 x_0+b'_1))$.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and clear, with good quality and original ideas. The experiments seem reproducible.\n\n# Summary Of The Review\n\nA strong paper with one key problem that the considered architectures are quite far from the state-of-the-art (CIFAR-10 accuracies below 80% as opposed to more than 90%). While in principle, it is good to do experiments with architectures requiring less resources, in this particular case it would be important to know whether the observed accuracy improvements are specific to the considered architectures or whether they also carry over to the state-of-the-art architectures. Also, more information could hav\ne been provided in the appendix, in particular about gradient norm evolution as well as calibration evolution.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper presents a novel method called \"Cooling\" to calibrate neural networks during training, addressing the issue of overconfidence in predictions often seen in modern neural networks. The Cooling method introduces temperature scaling to adjust logits dynamically, thereby improving gradient scaling without the need for a strict learning rate schedule. The authors propose multiple variants of the Cooling method, including Last Layer Cooling, Distributed Cooling, and Periodically Redistributed Cooling, and demonstrate their effectiveness through extensive empirical evaluations on various datasets (CIFAR10, CIFAR100, and ADE20K) and different architectures (e.g., VGG, ResNet, U-Net).\n\n# Strength And Weaknesses\nStrengths:\n- The paper offers a clear and innovative solution to a well-known problem in neural network training, specifically the overconfidence of predictions due to miscalibration.\n- The methodology is both theoretically grounded and empirically validated, with comprehensive experiments showing the effectiveness of the Cooling methods across different tasks and architectures.\n- The introduction of multiple Cooling variants provides flexibility and adaptability, potentially benefiting a wide range of applications.\n\nWeaknesses:\n- While the paper presents strong empirical results, the theoretical analysis could be further expanded to offer deeper insights into the underlying mechanisms of the Cooling method.\n- The complexity introduced by multiple Cooling variants may lead to a steeper learning curve for practitioners who are not well-versed in the nuances of temperature scaling.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, clearly presenting its contributions, methodology, and results. The explanations of the Cooling method and its variants are accessible, although some sections could benefit from more detailed mathematical derivations. The empirical results are reproducible, as the authors utilize widely used datasets and provide sufficient details about their experimental setup. Overall, the quality of writing and presentation is high, with a logical flow that facilitates understanding.\n\n# Summary Of The Review\nThis paper introduces an innovative Cooling method that effectively addresses calibration issues in neural networks during training, leading to improved performance and stability. The comprehensive empirical evaluation across various datasets and architectures supports the proposed contributions, although further theoretical exploration would enhance the overall impact. \n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach called the \"Cooling\" method aimed at addressing the issue of miscalibration in neural networks during the training phase. The proposed method employs temperature scaling techniques applied at various stages of the network, including last layer cooling, distributed cooling, and periodically redistributed cooling. The authors provide a mathematical analysis demonstrating how these cooling techniques can mitigate overconfidence in predictions, leading to improved convergence. Empirical results across multiple datasets, such as CIFAR10, CIFAR100, and ADE20K, indicate significant improvements in both test accuracy and expected calibration error (ECE).\n\n# Strength And Weaknesses\nThe Cooling method is a notable contribution to the field, offering a fresh perspective on calibration by focusing on the training phase rather than post hoc corrections. The paper's strengths include robust empirical performance improvements across various architectures and datasets, as well as a solid theoretical underpinning that explains the benefits of the Cooling approach. However, weaknesses include the complexity involved in hyperparameter tuning, particularly with parameters like τmax and τre, which may pose challenges for practitioners. Additionally, the reliance on a validation set for temperature optimization could hinder applicability in scenarios where such data is limited or unavailable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problems associated with miscalibration and the proposed solutions. The methodology is presented in a coherent manner, and the mathematical analysis enhances the understanding of the proposed methods. However, while the results are promising, the reproducibility may be impacted by the necessity for careful hyperparameter tuning and the dependency on a validation set. Overall, the paper maintains a high quality of writing and clarity.\n\n# Summary Of The Review\nThe Cooling method introduced in this paper presents a significant advancement in addressing the calibration issues of neural networks during training, yielding impressive performance improvements. While the approach is innovative and well-supported by empirical evidence, challenges related to hyperparameter tuning and generalization warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach called the Cooling method aimed at calibrating neural networks during training to mitigate the issue of overconfidence in predictions. The methodology involves periodically adjusting the logits of the network using a temperature scaling factor, which can be applied at the last layer or distributed across multiple layers. The authors conduct comprehensive experiments across various datasets and architectures, demonstrating that Cooling significantly enhances performance metrics like accuracy and stability while reducing sensitivity to learning rate schedules.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing calibration during training, an area that has received limited attention compared to post-hoc calibration methods. The empirical results across multiple datasets and architectures provide strong support for the effectiveness of the Cooling method, highlighting its potential applicability in real-world scenarios. However, a notable weakness is the lack of a clear exploration of the limitations or potential drawbacks of the Cooling method, such as computational overhead or the need for careful tuning of temperature parameters.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to readers with varying levels of expertise. The methodology is presented in a mathematically rigorous manner, which enhances the overall quality of the work. The novelty of the Cooling method is significant, as it presents a fresh perspective on neural network calibration during training. The experiments are sufficiently detailed, suggesting that the results should be reproducible by other researchers, although further clarification on hyperparameter tuning would be beneficial.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to improving the calibration of neural networks during training. The Cooling method shows promise in enhancing model performance and stability, supported by thorough empirical validation. However, addressing potential limitations would strengthen the contribution further.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel temperature-based Cooling method designed to calibrate classification neural networks during training, effectively addressing the prevalent issue of overconfidence in predictions. The authors validate their approach through extensive empirical experiments across various datasets, architectures, and hyperparameter settings, demonstrating significant improvements in network performance. Additionally, the paper includes a mathematical analysis that elucidates the effects of Cooling on gradient behavior, providing a theoretical foundation for the proposed method.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to calibration, evidenced by empirical validation that showcases improved performance across different tasks such as image classification and semantic segmentation. The mathematical analysis contributes depth to the understanding of the Cooling method. However, the effectiveness of the proposed approach may be contingent on specific network architectures or datasets, limiting its generalizability. Moreover, while the experiments are comprehensive, further testing on diverse architectures and real-world applications would strengthen the claims. The complexity of the mathematical formulations could hinder practitioner adoption, and the paper lacks extensive discussion on integration challenges within existing training pipelines.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, presenting its methodology and findings clearly; however, the complexity of the mathematical analysis may pose challenges for practitioners. The novelty of the Cooling method is notable, as it introduces a new perspective on neural network calibration, but the paper could enhance reproducibility by providing clearer guidelines on implementation and potential integration hurdles.\n\n# Summary Of The Review\nThe paper presents a significant contribution to the calibration of neural networks through the innovative Cooling method, supported by both theoretical and empirical evidence. However, the practical applicability of the approach is somewhat limited by the need for broader validation and clarity in its implementation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel method called **\"Adaptive Temperature Modulation\" (ATM)** aimed at enhancing the calibration of neural networks during training. The ATM methodology introduces a dynamic approach to adjusting the temperature parameter throughout training, incorporating both **layer-wise temperature modulation** and **global temperature adjustment** strategies. The authors provide a mathematical framework that outlines how ATM affects gradient behaviors and network convergence. Empirical evaluations across various tasks, including image classification and semantic segmentation, demonstrate significant improvements in accuracy and training stability compared to traditional temperature scaling methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative methodology, which departs from static temperature scaling by implementing dynamic adjustments based on real-time performance metrics. This allows for improved training stability and gradient flow, especially beneficial in deep networks. The extensive empirical validation through ablation studies and comparisons with baseline methods reinforces the claims made by the authors. However, a potential weakness is the limited exploration of the method's applicability across diverse architectures beyond those tested. Additionally, while the mathematical analysis is rigorous, it could be more accessible to practitioners unfamiliar with advanced calculus.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, with a logical flow that clearly outlines the methodology and findings. The quality of the empirical results is high, showcasing a robust experimental design. The novelty of the ATM approach is significant, as it introduces a new paradigm in temperature calibration during training. However, reproducibility could be enhanced with more detailed descriptions of the experimental setups and hyperparameters used, which are crucial for other researchers aiming to replicate the results.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of neural network training by introducing the Adaptive Temperature Modulation method, which enhances calibration and stability. The empirical results are compelling, demonstrating that ATM significantly improves performance over traditional methods. However, further exploration of its applicability and more detailed methodological descriptions would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel adversarial training technique called \"Cooling,\" aimed at increasing the robustness of neural networks against adversarial attacks. The methodology involves dynamically adjusting the training process based on model confidence through a temperature-based strategy that modulates gradient updates. Key contributions include the Cooling method itself, variants of this method, a theoretical framework analyzing its impact on training dynamics, and extensive empirical validation across various datasets and architectures, demonstrating significant improvements in adversarial robustness.\n\n# Strength And Weaknesses\nThe Cooling method represents a meaningful advancement in the adversarial training landscape by effectively linking model calibration with gradient dynamics, addressing a notable gap in existing approaches. The theoretical analysis provides a solid foundation for understanding the method's impact, while the empirical results across multiple datasets reinforce its practical significance. However, the paper could benefit from further exploration of the Cooling method's integration with other adversarial defenses and additional experiments on a wider range of architectures to bolster its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed method and its implications, making it accessible to readers. The quality of the experiments is high, with a thorough evaluation of various Cooling strategies documented through detailed ablation studies. The novelty of the Cooling method, particularly its focus on calibration and gradient management, is significant, although the reproducibility is contingent on the availability of code and datasets used in the experiments, which should be explicitly stated.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to adversarial training that effectively enhances model robustness through the Cooling method. The contributions are both theoretically sound and empirically validated, making it a strong candidate for acceptance at ICLR.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel Cooling method aimed at calibrating classification neural networks during training, positing that it could significantly streamline the training process by eliminating the need for learning rate adjustments. It presents a mathematical analysis of the method’s impact on gradient dynamics, claims extensive empirical validation through a series of experiments across diverse tasks and architectures, and reveals noteworthy performance improvements, including up to a 4.6% increase in accuracy on standard benchmarks. The paper concludes that this method not only enhances network performance but also optimizes the training process, potentially rendering existing techniques obsolete.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its proposed Cooling method, which is positioned as a breakthrough in neural network training, potentially transforming the field by simplifying calibration processes. The mathematical foundations provided lend credibility to the claims, and the extensive empirical validation supports the method's efficacy. However, the paper could benefit from a more nuanced discussion of the limitations and potential constraints of the Cooling method, particularly regarding its applicability across different types of neural networks and tasks. The bold claims about the obsolescence of existing techniques may require more detailed comparative analyses to substantiate.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology and findings clearly, making it accessible to a broad audience. The quality of the writing is high, with a coherent flow of ideas and well-defined contributions. The novelty of the Cooling method is significant, as it proposes a fundamental shift in neural network training dynamics. However, reproducibility could be a concern if the paper does not provide sufficient details on experimental setups and hyperparameter configurations, which are crucial for others to validate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the Cooling method as a transformative approach to neural network training, supported by strong empirical evidence and theoretical insights. While its contributions are noteworthy, further exploration of limitations and a more thorough comparative analysis with existing methods would strengthen the paper's claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel temperature-based Cooling method designed to enhance the calibration of classification neural networks during training. The Cooling method involves three variants: last layer Cooling, distributed Cooling, and periodically redistributed Cooling. Through mathematical analysis and extensive empirical evaluation across multiple datasets and architectures, the authors demonstrate that the Cooling method significantly improves gradient scaling, reduces the need for complex learning rate schedules, and enhances overall network performance. Notably, last layer Cooling yields the most substantial improvements, with marked accuracy gains reported across CIFAR10, CIFAR100, and a semantic segmentation task (ADE20K).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the calibration issues inherent in neural networks, particularly in relation to training stability and performance. The empirical results are compelling, demonstrating clear advantages of the Cooling method across various tasks and datasets. However, one weakness is the lack of a comprehensive theoretical framework that could further elucidate the observed benefits of the Cooling method. Additionally, while the empirical results are promising, the paper would benefit from a more in-depth analysis of how Cooling interacts with different architectures and hyperparameter settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with a logical flow from motivation to methodology and results. The quality of the writing is high, making the technical content accessible to readers with varying levels of expertise. The novelty of the Cooling method is significant, offering a fresh perspective on training neural networks. The reproducibility of the results is supported by detailed experimental setups and results presented in tables and figures, although more explicit guidelines for implementation would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field by introducing an innovative Cooling method that effectively improves the calibration and performance of neural networks during training. While the empirical results are robust, further theoretical insights and practical implementation details would strengthen the paper.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"How to Keep Cool While Training\" introduces a novel approach to enhancing the training of neural networks by applying a method termed \"Cooling,\" which aims to improve the calibration of predictive outputs. The authors argue that modern neural networks often produce overly confident predictions, necessitating calibration methods to enhance performance. The methodology primarily focuses on temperature scaling as a calibration technique, proposing that maintaining a \"cool\" training environment prevents large gradient updates, thereby stabilizing convergence. The findings suggest that the Cooling method leads to improved calibration and performance across various neural network architectures and tasks.\n\n# Strength And Weaknesses\nThe paper's contributions are significant in highlighting the importance of calibration in neural networks, particularly through the lens of temperature scaling. However, it suffers from several weaknesses, including assumptions about overconfidence being universal among neural networks and the necessity of calibration for all tasks. The reliance on empirical results without robust theoretical backing raises questions about the generalizability of the findings. Additionally, the paper does not adequately address the computational costs associated with implementing Cooling, which could hinder practical application in resource-constrained settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing is generally clear, with a logical flow that makes the methodology and findings accessible. However, the robustness of the claims could be improved with more detailed discussions around the assumptions underlying the Cooling method. The novelty is present, particularly in the proposed approach to calibration, but the paper lacks sufficient theoretical justification, which may affect reproducibility. The experiments conducted on specific datasets may not capture the broader landscape of neural network performance, limiting the generalizability of the results.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting approach to improving neural network training through calibration, it relies heavily on untested assumptions and empirical results. The lack of theoretical underpinnings and consideration of practical applicability raises concerns about the method's robustness and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper addresses the issue of overconfidence in neural networks by introducing a temperature-based Cooling method designed to calibrate networks during training. The authors propose multiple variants of the Cooling method, conduct a thorough mathematical analysis on its effects on gradient calculations, and provide empirical evaluations on various datasets, including CIFAR10, CIFAR100, and ADE20K. The findings demonstrate that the Cooling method not only improves model performance but also enhances training stability, suggesting a correlation between calibration and learning rate dynamics.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating calibration during training, which contrasts with traditional methods that apply temperature scaling only post-training. The mathematical analysis provided adds depth to the understanding of how Cooling affects gradient behavior, and the comprehensive empirical results validate the proposed method across multiple tasks and architectures. However, a potential weakness is that while the paper shows significant improvements, it may not sufficiently explore the limitations of the Cooling method or compare it in depth with other state-of-the-art calibration techniques. Additionally, the scalability of the method to more complex architectures or larger datasets is not thoroughly addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, making it accessible to readers with a foundational understanding of neural networks. The quality of writing is high, with clear explanations and logical progression throughout the sections. The novelty of introducing a calibration technique during training is significant, though it would benefit from a more explicit discussion on the reproducibility of the results across different environments or implementations. Overall, the clarity of the presentation supports the reproducibility of the Cooling method.\n\n# Summary Of The Review\nThis paper presents a novel approach to calibrating neural networks during training, demonstrating significant improvements in performance and stability. The innovative Cooling method, supported by both theoretical analysis and empirical validation, represents a meaningful contribution to the field, though further exploration of its limitations and broader applicability could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for improving model robustness in adversarial settings by leveraging a hybrid approach that combines traditional adversarial training with an innovative regularization technique. The authors implement a new algorithm that optimally balances the trade-off between model accuracy and resistance to adversarial perturbations. Empirical evaluations demonstrate that the proposed method outperforms existing state-of-the-art techniques across multiple benchmark datasets, indicating significant improvements in both robustness and generalization.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its clear identification of the limitations of existing adversarial training methods and its proposed solution, which is well-justified through theoretical analysis. The empirical results are robust, showcasing improved performance metrics compared to baseline models. However, the paper could benefit from a deeper exploration of the computational complexity associated with the proposed method, as well as a more thorough discussion of potential limitations and scenarios where the method might underperform.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas in a coherent manner. The introduction effectively contextualizes the problem, while the methodology section is sufficiently detailed, albeit with some technical jargon that may hinder accessibility for non-expert readers. The novelty of the approach is evident, as it bridges gaps between existing methodologies. Reproducibility is partially addressed through the provision of experimental details and parameters, though including the code and datasets used would significantly enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of adversarial machine learning by proposing a novel hybrid approach that enhances model robustness. While the methodology and results are commendable, further clarification on certain technical aspects and the inclusion of supplementary materials for reproducibility would strengthen the paper.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper addresses the issue of miscalibration in neural networks, where overly confident predictions can lead to poor classification performance. The authors propose a novel temperature-based method called \"Cooling\" that calibrates neural networks during training, rather than applying calibration techniques post-training, as is traditionally done. The methodology focuses on improving gradient scaling and reducing the dependency on specific learning rate schedules. The findings indicate that the simplest variant of Cooling, applied to the last layer of the network, consistently outperforms existing calibration methods across various datasets and architectures, highlighting the interplay between calibration and learning rate stability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating calibration into the training process, which addresses a critical gap in existing literature that has primarily focused on post-training calibration methods. The empirical results demonstrate notable improvements in network performance and stability, showing that Cooling can significantly enhance predictive accuracy. However, a potential weakness is the lack of extensive exploration of the method's applicability across a broader range of architectures beyond convolutional networks, which may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its methodology and findings, making it accessible to a broad audience. The quality of the experiments conducted is commendable, with rigorous testing across different datasets and architectures. The novelty of the Cooling method stands out, as it challenges the conventional wisdom surrounding calibration techniques. However, while the methodology is described in detail, additional information regarding the reproducibility of the results, such as code availability or specific hyperparameter settings, would enhance the overall assessment.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of neural network calibration by introducing a training-time calibration method, Cooling, which improves the performance and stability of neural networks. While the results are promising and the methodology is innovative, further exploration of its applicability across diverse architectures would strengthen the contributions of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel temperature-based Cooling method designed to enhance the calibration of neural networks during training. The authors argue that existing calibration techniques primarily address calibration post-training, neglecting their impact on the training process itself. The proposed method includes several variants of Cooling, with a focus on last layer Cooling, which yields superior performance across multiple datasets and architectures. The authors provide a mathematical analysis of how Cooling influences gradient scaling and present extensive empirical results indicating significant improvements in model performance and stability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating calibration methods directly into the training process, which is a relatively unexplored area. The authors provide a thorough investigation of various Cooling strategies and demonstrate their effectiveness through comprehensive experiments on diverse datasets, offering valuable insights into the interaction between calibration and learning rate. However, one potential weakness is that while the empirical results are compelling, further exploration of the limitations of the Cooling method and its applicability to more complex architectures would strengthen the study. Additionally, the paper could benefit from a more detailed discussion of the computational overhead introduced by the Cooling method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding of the proposed method and its implications. The mathematical analysis is sound and aids in grasping the theoretical underpinnings of the Cooling method. However, the reproducibility of results could be improved by providing more detailed descriptions of experimental setups and hyperparameter settings. The novelty of the approach is notable, as it addresses a gap in the literature regarding in-training calibration techniques.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of neural network calibration by proposing a novel Cooling method that enhances performance and stability during training. The experimental results are robust and provide strong evidence for the method's effectiveness, although further studies could enhance the understanding of its limitations and computational requirements.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"HOW TO KEEP COOL WHILE TRAINING\" addresses the problem of overconfidence in neural networks, proposing a novel solution known as the Cooling method for enhancing calibration during training. The methodology involves various implementations of Cooling, including last layer Cooling, distributed Cooling, and periodically redistributed Cooling, all incorporating temperature scaling. The experiments conducted on datasets such as CIFAR10, CIFAR100, and ADE20K, using architectures like VGG-style, ResNet50, and U-Net, demonstrate significant improvements in test accuracy, expected calibration error (ECE), and convergence stability, thereby highlighting the effectiveness of the Cooling method in various neural network tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the calibration of neural networks during training, which has been a persistent challenge in the field. The detailed methodology, including mathematical formulations and gradient analyses, provides a solid foundation for understanding the Cooling method. Furthermore, the empirical results across multiple datasets and architectures substantiate the claims made regarding performance improvements. However, a notable weakness is the lack of explicit discussion on the limitations of the proposed method, particularly concerning its generalization across different architectures, which warrants further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its concepts and methods clearly, aiding in reader comprehension. The quality of the experiments is high, with thorough evaluations and ablation studies that enhance reproducibility. The novelty of the Cooling method is significant, introducing a fresh perspective on calibration during training, which could lead to substantial improvements in neural network performance. However, while the methodology is well-explained, additional clarity on the limitations would enhance the overall quality of the paper.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective method for improving the calibration of neural networks during training, backed by comprehensive experiments and results. While it shows promise in enhancing performance and stability, a discussion on its limitations and generalization capabilities would strengthen the contribution further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel temperature-based Cooling mechanism aimed at calibrating neural networks during the training phase, addressing the issue of overconfidence in predictions commonly exhibited by classification networks. The proposed method enhances gradient scaling, reducing reliance on traditional learning rate schedules. Empirical evaluations demonstrate that the simplest variant, termed last layer Cooling, significantly improves performance across various datasets, architectures, and hyperparameter configurations, indicating its effectiveness in stabilizing training and improving model calibration.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating calibration techniques within the training process, rather than applying them post-training as is common in existing methods. The empirical results are compelling, showing clear performance enhancements in multiple contexts, which adds robustness to the findings. However, a notable weakness is the limited exploration of the effects of Cooling on other architectures beyond those tested (VGG-style, ResNet50, and U-Net), which may restrict the generalizability of the results. Additionally, while the paper provides a solid theoretical foundation, it could benefit from a more extensive discussion of the trade-offs associated with the Cooling parameters.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the Cooling mechanism, the methodology employed, and the experimental setup. The clarity of the mathematical formulations aids in understanding the proposed concepts. The quality of the writing is high, with thorough explanations of the methodology and results. In terms of novelty, the paper presents a significant advancement in the domain of neural network calibration and training dynamics. The reproducibility of the results is facilitated by the detailed descriptions of the experiments; however, sharing code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to improving neural network calibration during training through the Cooling mechanism. The empirical results are convincing, and while the methodology is clearly articulated, further exploration of its applicability across diverse architectures would strengthen the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the issue of overconfidence in neural networks and proposes a novel method termed \"Cooling\" to improve the calibration of model predictions. The authors claim that this method enhances the stability of learning rates during training, suggesting that it can lead to better overall performance. The methodology involves variations of the Cooling technique, including last layer Cooling, distributed Cooling, and periodically redistributed Cooling, applied during the training process. However, the empirical results presented are based on limited datasets and architectures, raising questions about their generalizability.\n\n# Strength And Weaknesses\nWhile the paper attempts to tackle an important problem in neural networks, it suffers from several weaknesses. The claims regarding the insufficiency of existing calibration methods lack substantial evidence and critical analysis. The proposed Cooling method introduces unnecessary complexity without clear justification for its benefits, as the variations complicate implementation. Furthermore, the reported performance improvements appear marginal and are not robustly supported by comparative analyses. The reliance on limited datasets and architectures also raises concerns about the generalizability of the findings, and the heavy use of ablation studies may lead to misleading conclusions. The paper fails to adequately discuss its limitations and lacks a thorough theoretical foundation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hampered by ambiguous claims and a lack of depth in its theoretical analysis. The novelty of the proposed methods appears limited, as they seem to be adaptations of established concepts rather than groundbreaking advancements. The reproducibility of the results is questionable, given the reliance on specific datasets and potential sensitivity to parameters, which may not hold in broader applications. Overall, the quality of the writing and presentation does not convincingly support the claims made.\n\n# Summary Of The Review\nThis paper attempts to address the calibration of neural networks through the introduction of the Cooling method but ultimately does not provide convincing evidence of its effectiveness or novelty. The complexity introduced by the method does not seem justified, and the paper lacks a critical examination of existing solutions, leaving significant gaps in understanding. \n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"How to Keep Cool While Training\" introduces a novel cooling method for calibrating classification neural networks during training. The main contributions include three innovative cooling variants—last layer Cooling, distributed Cooling, and periodically redistributed Cooling—aimed at enhancing network performance. The authors provide a rigorous mathematical analysis of the method's impact on network gradients and demonstrate its effectiveness through extensive empirical experiments on various tasks and datasets, revealing significant improvements in training performance, stability, and user-friendliness.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to network calibration and the thorough empirical evaluation that confirms its effectiveness across multiple architectures and tasks. Notably, the introduction of cooling variants adds versatility and broad applicability, making it an exciting contribution to the field. However, a potential weakness is the lack of detailed exploration into the limitations of the Cooling methods, particularly regarding their applicability to larger architectures or more complex tasks. Additionally, while the theoretical insights are strong, more focus on practical implications could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents both the theoretical foundations and empirical results of the proposed cooling methods. The quality of the writing is high, facilitating understanding of the complex concepts involved. The novelty of the Cooling method is significant, as it introduces a new paradigm for network training. The reproducibility of the results is bolstered by the extensive empirical validation across various datasets, although providing access to code and datasets would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new method for improving neural network training through innovative cooling techniques, backed by strong theoretical and empirical evidence. Its contributions are significant and have the potential to influence future research in network calibration and training efficiency.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 5\n\n# Empirical Novelty And Significance\nRating: 5",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for understanding calibration in neural networks, specifically addressing the overconfidence phenomenon that leads to miscalibrated outputs. It introduces a novel method called \"Cooling,\" which utilizes a temperature parameter to modulate output logits during training, thereby maintaining optimal gradient scales and enhancing the stability of learning dynamics. Additionally, the paper explores the implications of distributed cooling across multiple layers and the interplay between calibration and learning rate, providing insights into future research directions for optimizing neural network training methodologies.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its thorough theoretical foundation, which elucidates the connection between calibration, gradient dynamics, and training stability. The introduction of the Cooling method is a significant contribution that offers a promising approach to mitigate overconfidence in neural networks. However, a notable weakness is the lack of empirical validation of the proposed methods. While the theoretical insights are compelling, the absence of experimental results limits the practical applicability and robustness of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its aims, methodologies, and theoretical underpinnings. The quality of writing is high, contributing to an accessible presentation of complex ideas. The novelty of the Cooling method and the theoretical exploration of calibration dynamics are commendable. However, reproducibility may be challenging without empirical data to support the theoretical claims, as the practical implementation of the proposed methods remains unexplored.\n\n# Summary Of The Review\nThis paper makes a valuable theoretical contribution to understanding calibration in neural networks through the introduction of the Cooling method. While the framework is well-articulated and presents novel insights, the lack of empirical validation raises questions about the practical effectiveness of the proposed approaches.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"HOW TO KEEP COOL WHILE TRAINING\" introduces a temperature-based Cooling method designed to enhance the calibration of classification neural networks during training. The proposed methodology encompasses three variants: Last Layer Cooling, Distributed Cooling, and Periodically Redistributed Cooling. Through extensive experiments on datasets like CIFAR10, CIFAR100, and ADE20K, the authors demonstrate that these Cooling methods significantly improve model performance, with Last Layer Cooling achieving a 4.6% increase in test accuracy over non-Cooling baselines. The paper also discusses the impact of the Cooling factor on convergence and gradient behavior, providing a comprehensive evaluation of different learning rate schedules and configurations.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its clear and systematic approach to addressing the problem of calibration in neural networks, with three distinct Cooling methods that offer flexibility in implementation. The empirical results are robust, showcasing consistent performance improvements across various datasets and model architectures. However, the paper lacks broader theoretical implications and does not articulate the underlying principles that make the Cooling methods effective. Additionally, while the implementation details are thorough, the absence of explicit code availability may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and easy to follow, with clear explanations of the Cooling methods and their implementation. The quality of the writing is high, making the methodology accessible to readers. In terms of novelty, the proposed Cooling methods represent a fresh perspective on neural network calibration, although the underlying concepts may not be entirely groundbreaking. Reproducibility is somewhat limited due to the lack of direct access to code, despite references to TensorFlow implementations.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of neural network training by introducing innovative Cooling methods that enhance model performance. While the empirical results are compelling, the lack of theoretical insights and code accessibility detracts from its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel Cooling method designed for calibrating neural networks during the training phase. It aims to improve gradient scaling and asserts that this method leads to better model calibration. The authors provide experimental results claiming enhancements over existing calibration techniques, although they do not sufficiently contextualize these improvements against more recent advancements in the field.\n\n# Strength And Weaknesses\nWhile the paper proposes a Cooling method, its contributions appear to reiterate previously established concepts in calibration without introducing significant innovations. The methodology lacks a robust comparison with state-of-the-art techniques, such as temperature scaling and other calibration methods, which diminishes the perceived value of the findings. Furthermore, the theoretical underpinnings of the relationship between temperature scaling and gradient updates are not sufficiently deep, relying on ideas already discussed in prior literature. The ablation studies support the claims made, but without adequate comparison to recent methodologies, the significance of the results remains unclear.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and easy to follow, but the novelty of the proposed method is questionable due to its heavy reliance on existing literature. The quality of the writing is acceptable, but the lack of thorough comparisons with more recent advancements in calibration techniques diminishes the overall impact. Reproducibility may be a concern as specific details about the experimental setups and comparisons with other methods are insufficiently articulated.\n\n# Summary Of The Review\nThe Cooling method presented in the paper does not offer significant advancements over existing calibration techniques and fails to adequately differentiate its contributions from established works in the literature. The paper's reliance on previous research for its foundational ideas raises concerns about its novelty and significance in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"How to Keep Cool While Training\" addresses the challenge of model calibration during training, proposing a novel approach called the Cooling Factor. The authors introduce this method to adjust the learning rate dynamically based on model confidence, aiming to reduce overconfidence in predictions. The methodology includes modifications to standard learning rate schedules and the introduction of new mathematical formulations for better gradient updates. The findings indicate that employing the Cooling Factor leads to improved calibration and predictive performance across various benchmark datasets.\n\n# Strength And Weaknesses\nOne of the main strengths of this paper is its innovative approach to improving model calibration, which is a critical aspect of machine learning. The proposed Cooling Factor is well-motivated and addresses a common issue in deep learning models. However, the paper has notable weaknesses, including inconsistencies in notation and formatting, which can detract from the overall clarity. Additionally, while the empirical results are promising, the experimental setup could benefit from greater detail to ensure reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by several issues, such as inconsistent notation and formatting, as well as vague language in certain sections. While the novelty of the Cooling Factor is significant, it is somewhat overshadowed by the lack of clear definitions and explanations of key terms. The quality of the writing could be improved by addressing these clarity issues. Reproducibility is a concern due to the insufficient detail in the experimental setup and the inconsistent formatting of tables and references.\n\n# Summary Of The Review\nThis paper presents a novel approach to model calibration through the introduction of the Cooling Factor, which shows promise in improving predictive performance. However, clarity and consistency issues throughout the document detract from its overall quality and may hinder reproducibility. Addressing these weaknesses would strengthen the paper considerably.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel Cooling method for calibration during the training of neural networks. This method aims to enhance model performance by adjusting the temperature scaling of outputs. The authors present empirical results demonstrating the effectiveness of Cooling on specific architectures, mainly VGG and ResNet, in image classification and semantic segmentation tasks. However, the analysis is limited to these contexts, and the paper does not explore broader implications on generalization or robustness across diverse tasks or datasets.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its introduction of the Cooling method, which offers a fresh perspective on temperature scaling in neural network training. However, it exhibits several weaknesses, including a narrow focus on particular architectures and tasks, limiting the generalizability of the findings. Additionally, there is a lack of discussion regarding alternative calibration techniques, potential limitations of the Cooling method, and the computational trade-offs involved. The paper could also benefit from a more comprehensive exploration of future research directions and ethical considerations related to calibration and model interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its methodology and findings with a reasonable degree of clarity, although the depth of analysis in some sections could be improved. The novelty of the Cooling method is notable, but the paper does not sufficiently address its integration with other calibration methods or its implications for different learning rate strategies. The reproducibility of results could be enhanced by extending experiments to a wider variety of architectures and tasks, as well as providing detailed benchmarking against other calibration techniques.\n\n# Summary Of The Review\nOverall, the paper presents a promising new calibration method, Cooling, with initial empirical evidence of its effectiveness. However, the limited scope of experiments and the lack of comprehensive discussions on its limitations and broader implications diminish the impact of the findings. Future work should focus on expanding the evaluation and considering the method's integration with existing calibration techniques.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel temperature-based calibration method called \"Cooling\" designed to address overconfidence in neural network predictions during training. It introduces three variants of the Cooling method: Last Layer Cooling, Distributed Cooling, and Periodically Redistributed Cooling. The authors provide a mathematical analysis of how this method impacts network gradients and conduct extensive empirical investigations across various tasks and datasets, demonstrating that the Cooling method enhances training stability, improves gradient behavior, and leads to significant improvements in calibration performance as measured by expected calibration error (ECE).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of a new calibration technique that is integrated into the training process rather than applied post-training, which is a notable departure from existing methods. The thorough mathematical analysis and empirical validation across multiple architectures and datasets bolster the credibility of the proposed method. However, a potential weakness is the limited exploration of the method's applicability to a broader range of tasks beyond CIFAR datasets, which may restrict generalizability. Additionally, while the paper discusses statistical significance, further clarity on the statistical methods used would enhance the robustness of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, providing a comprehensive overview of the Cooling method, its theoretical underpinnings, and empirical results. The quality of writing is high, making complex concepts accessible. The novelty of integrating calibration during training rather than post-training is significant, contributing fresh insights to the field of neural network training. Reproducibility is supported by detailed experimental setups and statistical analyses, though providing code or additional materials would further facilitate independent verification of results.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative method for calibrating neural networks during training, demonstrating significant improvements in both calibration and training stability. While the findings are compelling and well-supported, further exploration of the method's applicability to diverse tasks would strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method termed \"Cooling,\" aimed at improving model calibration in deep learning architectures. It primarily focuses on the application of Cooling to well-known architectures such as VGG and ResNet, demonstrating its effectiveness in enhancing model accuracy on image classification tasks. The methodology involves adjusting the temperature parameter during training to optimize model outputs. While the results indicate promising improvements in certain metrics, the paper lacks an exploration of long-term effects and broader applicability across different tasks and architectures.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to model calibration and its systematic validation on established architectures. However, it exhibits several weaknesses, such as the limited scope of experiments confined primarily to image classification without exploring other domains like natural language processing or reinforcement learning. The lack of a comparative analysis with existing calibration techniques limits its contextual understanding. Moreover, the absence of a theoretical framework explaining the underlying mechanisms of Cooling raises questions about its generalizability. Additionally, practical considerations such as computational overhead and potential numerical instability during training were not addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the proposed method and results. However, the novelty is somewhat undermined by the lack of comprehensive comparisons with state-of-the-art techniques. The reproducibility of results could be enhanced by providing more detailed information on the experimental setup and hyperparameter choices. Future work could benefit from a deeper investigation into the interactions between Cooling and other calibration strategies, as well as a thorough analysis of hyperparameter settings.\n\n# Summary Of The Review\nOverall, while the Cooling method presents a promising advancement in model calibration, the paper has several limitations that hinder its impact. A broader exploration of its applicability, a more robust theoretical foundation, and a comparative analysis with existing methods would significantly enhance its contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"How to Keep Cool While Training\" presents a method for improving model calibration through temperature scaling during the training process. The authors propose a temperature-based cooling method that adjusts hyperparameters dynamically to enhance the calibration of predictions. They conduct experiments using CIFAR datasets and demonstrate that their approach yields significant performance improvements. However, many of the concepts discussed, such as the importance of hyperparameter tuning and temperature scaling, have been well-established in prior literature.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its experimental validation, which shows that temperature scaling can lead to improved calibration during training. The authors include a mathematical analysis to support their claims, and they perform ablation studies to explore the effects of their proposed method comprehensively. However, the weaknesses are notable: the contributions feel redundant, as they largely reiterate existing knowledge without introducing new insights or methodologies. The title and framing of the problem lack originality, and the paper does not sufficiently differentiate itself from established research in calibration techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear methodologies and results. However, the novelty is limited, as many of the findings are already known in the field. The authors provide sufficient details regarding their experimental setup, which aids reproducibility, but the lack of innovative contributions diminishes the overall impact of the work.\n\n# Summary Of The Review\nOverall, this paper presents a familiar approach to model calibration through temperature scaling during training. While it offers some experimental validation and clear methodologies, the contributions largely reiterate existing knowledge without providing significant novelty or insight.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Cooling method, a novel temperature-based adjustment designed to enhance training stability and performance of neural networks. The methodology involves integrating temperature scaling into the training loop, rather than applying it post hoc, which is a departure from traditional practices. The findings demonstrate that networks trained with the Cooling method achieve improved performance across various datasets (CIFAR10 and CIFAR100) and architectures, suggesting a potential for broader applications.\n\n# Strength And Weaknesses\nThe Cooling method is a significant contribution, particularly due to its implications for dynamic learning rate adjustments and its potential to reduce reliance on manual hyperparameter tuning. The empirical results support the method's effectiveness, but the paper could benefit from exploring its scalability on larger datasets, such as ImageNet. Additionally, while the mathematical analysis of gradient scaling is intriguing, a deeper exploration of its implications in other contexts, such as GAN training or reinforcement learning, could enhance the theoretical framework. The paper also lacks a comprehensive examination of the relationship between Cooling factors and other hyperparameters, which could limit the applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and results clearly, facilitating understanding. The quality of empirical validation across diverse architectures and datasets is commendable, although further investigations into the Cooling method's generalizability across various tasks are warranted. In terms of novelty, the integration of temperature scaling into the training process represents a fresh perspective, though the reproducibility of results across different scenarios remains to be fully established.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative approach to improving training stability in neural networks through the Cooling method. While the empirical validation is strong, further exploration of its applicability across a wider range of datasets and architectures is essential to fully assess its effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel method termed \"Cooling\" aimed at enhancing the calibration of neural networks during training. The authors demonstrate the effectiveness of Cooling across multiple datasets, including CIFAR10, CIFAR100, and ADE20K, highlighting significant improvements in test accuracy and performance metrics. The method is evaluated through extensive ablation studies, confirming its robustness and adaptability across different architectures and learning rate schedules.\n\n# Strength And Weaknesses\nOne of the primary strengths of the paper is its empirical validation across a variety of datasets and architectures, which demonstrates the versatility of the Cooling method. The results indicate a substantial increase in performance, such as a 4.6% improvement in test accuracy on CIFAR10 and similar trends on CIFAR100 and ADE20K. However, a notable weakness is the lack of specific accuracy improvements reported for CIFAR100, which could leave readers uncertain about the extent of the method's effectiveness on that dataset. Additionally, while the ablation studies are thorough, further exploration into the theoretical underpinnings of why Cooling is effective could enhance the understanding of its mechanism.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings, making it accessible to readers. The quality of the empirical results is high, and the extensive ablation studies contribute to the reproducibility of the findings. However, the novelty of the method, while significant in the context of the presented results, could benefit from a more thorough discussion of its innovation relative to existing calibration techniques in neural networks.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new method for improving neural network performance during training, supported by robust empirical evidence across multiple datasets. While the contributions are significant, particularly in terms of practical performance gains, a deeper exploration of the theoretical aspects and clearer reporting of all results would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to enhancing model calibration in machine learning systems through a technique referred to as \"Cooling.\" The methodology involves a series of experiments that systematically evaluate the impact of Cooling on model performance across various datasets. The findings indicate that the incorporation of Cooling significantly improves calibration metrics, specifically in terms of expected calibration error (ECE), while maintaining overall accuracy.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical validation of the Cooling technique, which demonstrates a clear and measurable improvement in model calibration. However, the paper suffers from several weaknesses, including a lack of clarity in the abstract and introduction, which may hinder readers' understanding. The organization of the results section is also suboptimal, as it could benefit from clearer subheadings to delineate different experiments. Additionally, the paper introduces key terms and acronyms, such as Cooling and ECE, without adequate prior explanation, potentially leading to confusion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by complex sentence structures and inconsistent terminology. For example, the phrase \"to the best of our knowledge\" lacks assertiveness, and the frequent use of parentheses disrupts the reading flow. The quality of the presentation is affected by typographic errors and inconsistent reference formatting. In terms of novelty, the Cooling technique appears innovative; however, its significance is somewhat diminished by the paper's unclear presentation. Reproducibility may be hindered due to insufficient details in the methodology section.\n\n# Summary Of The Review\nOverall, while the paper offers a potentially impactful contribution to model calibration in machine learning, its clarity and organization require significant improvement. A more structured presentation and clearer definitions of key concepts would enhance the paper's accessibility and impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.226437057369228,
    -1.6492806996404312,
    -1.8288739527621571,
    -1.6072495811972174,
    -1.8873470646036825,
    -1.6448283585610974,
    -1.8552951163306222,
    -1.675556626858151,
    -1.6201498597378021,
    -1.807783852542936,
    -1.731473003957115,
    -1.4035171678298273,
    -1.6219890786623061,
    -1.6107654368659903,
    -1.8416932538281143,
    -1.813813583888339,
    -1.9860631123045036,
    -1.768137769416634,
    -1.6166460541114545,
    -1.7362156754652154,
    -2.087085606141756,
    -1.6542940952875855,
    -1.756155103559475,
    -1.6993215020512913,
    -1.6291882234235757,
    -1.8496875906753676,
    -1.8100728912962705,
    -1.4844645524358528,
    -1.662974678800355
  ],
  "logp_cond": [
    [
      0.0,
      -2.0764898002272463,
      -2.104231781593467,
      -2.0870256759482086,
      -2.0921741045538624,
      -2.110286953272225,
      -2.107353845157921,
      -2.089274152767721,
      -2.0832089392981645,
      -2.0982519430324755,
      -2.0773452525466096,
      -2.14230466453895,
      -2.1033979950670783,
      -2.0902694659183445,
      -2.0941031578511153,
      -2.096308915362119,
      -2.1032562474902288,
      -2.088957553199456,
      -2.1074211627935466,
      -2.0801281367885887,
      -2.1027915986649997,
      -2.103268835419504,
      -2.1036611781597716,
      -2.0876380821091582,
      -2.1217309641192266,
      -2.0896401896956522,
      -2.0940249787297867,
      -2.096786961516041,
      -2.127891119462099
    ],
    [
      -1.3107572881975007,
      0.0,
      -1.1695839023400942,
      -1.237319712196561,
      -1.3155329699165224,
      -1.3295531833373848,
      -1.3311014207869123,
      -1.3286982304461261,
      -1.1218240231325396,
      -1.2880888155787513,
      -1.2214467687484625,
      -1.3946134125740204,
      -1.2483029809434143,
      -1.2316154748299468,
      -1.142334211159648,
      -1.2018575485479857,
      -1.1669452296905425,
      -1.1971370378754855,
      -1.2746639310945451,
      -1.162554596549347,
      -1.2882111435010712,
      -1.3444050960902973,
      -1.3143937062705038,
      -1.1997319253975764,
      -1.3644611898189587,
      -1.305484267376667,
      -1.2873255062739979,
      -1.2745015427606297,
      -1.352811875065257
    ],
    [
      -1.4842464348015338,
      -1.3061247250264771,
      0.0,
      -1.3634787392900334,
      -1.418606487179623,
      -1.4747383172161455,
      -1.49618807674184,
      -1.4516760268302626,
      -1.286916152375099,
      -1.4357482680132603,
      -1.35368612393129,
      -1.582574680560915,
      -1.414701054381233,
      -1.3886096605949194,
      -1.2591976356232255,
      -1.3627218461997777,
      -1.3621426451103558,
      -1.3513392092351266,
      -1.4468271834176427,
      -1.303261298896919,
      -1.4452926296698316,
      -1.5040733535754693,
      -1.4428324212679609,
      -1.3042723456367384,
      -1.4863581663848648,
      -1.455904173294652,
      -1.4329088163383283,
      -1.4079965187426136,
      -1.5245928733465142
    ],
    [
      -1.1884566580589795,
      -1.091167405309311,
      -1.1183138727537452,
      0.0,
      -1.184347974277296,
      -1.2125707896497617,
      -1.2039505907916674,
      -1.196183333067604,
      -1.1637621900695012,
      -1.1796826541356629,
      -1.1406969159437246,
      -1.3231874738390539,
      -1.1591300022002207,
      -1.1426780118281452,
      -1.0854457098244155,
      -1.1563370844103826,
      -1.1764617224207414,
      -1.1234056472684606,
      -1.212624124007467,
      -1.16894068215731,
      -1.1702858156449942,
      -1.2676029272465172,
      -1.236931085561619,
      -1.132407873251567,
      -1.232733865786199,
      -1.19447440739887,
      -1.1849160227097473,
      -1.1995391035316765,
      -1.2559202595323935
    ],
    [
      -1.5138546115064841,
      -1.445586218378703,
      -1.4771852141980233,
      -1.4770796957893497,
      0.0,
      -1.4455456681598353,
      -1.5102989167378575,
      -1.3739062630346248,
      -1.354133435841989,
      -1.4268247430272307,
      -1.3886309134347765,
      -1.5577578639604888,
      -1.4918064382910616,
      -1.435473339474752,
      -1.42235320881998,
      -1.416517415273242,
      -1.4788584474196371,
      -1.4458008475657123,
      -1.4247855371502747,
      -1.3958408207667936,
      -1.4770163880966243,
      -1.5069156119369147,
      -1.4434105792905185,
      -1.4528870955540247,
      -1.5438215903019976,
      -1.493009286074588,
      -1.4428837585123782,
      -1.4819875764099486,
      -1.4798415837197734
    ],
    [
      -1.3244250899895347,
      -1.2756351215404413,
      -1.2900951369273561,
      -1.2960185855475927,
      -1.2803989370935387,
      0.0,
      -1.308057809210149,
      -1.2559018406868343,
      -1.254751051502847,
      -1.290022758794312,
      -1.2099347253265424,
      -1.415848712736476,
      -1.298900702568631,
      -1.2564063399735914,
      -1.2646030625959865,
      -1.303679260868488,
      -1.286769828204324,
      -1.2709935368469956,
      -1.3391075743977492,
      -1.2725893993150432,
      -1.2729141600851646,
      -1.3966834748948465,
      -1.3215215802179374,
      -1.274641954031467,
      -1.3802204084100325,
      -1.2601149359554744,
      -1.2715902685754943,
      -1.287485202365202,
      -1.3943530810516738
    ],
    [
      -1.4739718567597557,
      -1.4402347266343642,
      -1.4702321288354976,
      -1.445978475747274,
      -1.443855729142053,
      -1.4592868167154849,
      0.0,
      -1.444288614044012,
      -1.4562693375651112,
      -1.4253388189032412,
      -1.4143018556018263,
      -1.5213373527891747,
      -1.4380917442892593,
      -1.3786849902505354,
      -1.4133529453608382,
      -1.4434608381560585,
      -1.4475381041518063,
      -1.453311737973939,
      -1.5064195700277803,
      -1.4327477904600425,
      -1.3985663950962537,
      -1.5243170575902825,
      -1.4676750129721554,
      -1.4227563214738068,
      -1.4659175688922295,
      -1.4670146716521033,
      -1.4119248557969328,
      -1.4222022199494264,
      -1.5048147126025364
    ],
    [
      -1.4218478390487923,
      -1.3269939492862792,
      -1.3350358038675414,
      -1.3304168366067457,
      -1.2437689678797987,
      -1.316533538105978,
      -1.3538108258661545,
      0.0,
      -1.2405632551401413,
      -1.3117866286698772,
      -1.2861995897059344,
      -1.3670833130943865,
      -1.3615281694129993,
      -1.316306673322471,
      -1.3129422035985163,
      -1.2835071431807041,
      -1.310040365638972,
      -1.2645002938123322,
      -1.2828516309747624,
      -1.2781091844361807,
      -1.3061815695717551,
      -1.3488003883554514,
      -1.2860277065543062,
      -1.3344724262603782,
      -1.3430604830355306,
      -1.346527160298742,
      -1.3294274280426182,
      -1.3280134201754092,
      -1.326821187861136
    ],
    [
      -1.2632398700440386,
      -1.1018721077150708,
      -1.1267643710399882,
      -1.1947214996053117,
      -1.1203378158176274,
      -1.2354907794046577,
      -1.2860257403772528,
      -1.1924893163390182,
      0.0,
      -1.2185566496238647,
      -1.1745458707005785,
      -1.3109137707786669,
      -1.2099021597633095,
      -1.099324686591611,
      -1.108058219120186,
      -1.1291963126330953,
      -1.1485756301195598,
      -1.0873978391200823,
      -1.221762788216461,
      -1.0420786346664925,
      -1.2401636478259608,
      -1.241376835972629,
      -1.2111963906498002,
      -1.1249948162152306,
      -1.3027001414308068,
      -1.2674683674843479,
      -1.2292471035555388,
      -1.2032011717453377,
      -1.2543539695567767
    ],
    [
      -1.5381355387846787,
      -1.4215729995445392,
      -1.4410467722924736,
      -1.4368789853483108,
      -1.436808691153496,
      -1.4612018968423728,
      -1.4481698925717377,
      -1.4443175424243722,
      -1.4823579212329434,
      0.0,
      -1.4568998777824829,
      -1.5388721582967182,
      -1.4430049614163762,
      -1.461195374787902,
      -1.3912495640018308,
      -1.4623905494051814,
      -1.3836307736974907,
      -1.4042405663384596,
      -1.4630064129823694,
      -1.4115674420896083,
      -1.4317639332457317,
      -1.4801475552593197,
      -1.4304205627607893,
      -1.4333343295367438,
      -1.483837437311832,
      -1.4262836081581887,
      -1.4241845483191347,
      -1.4303474142266095,
      -1.477355953199792
    ],
    [
      -1.3722256353917959,
      -1.205122813587636,
      -1.2306947504027574,
      -1.3108875349248856,
      -1.2198512333218432,
      -1.3207441595924994,
      -1.3239417867854815,
      -1.3295790474354157,
      -1.214590589614315,
      -1.315088615991841,
      0.0,
      -1.48205527018977,
      -1.2583098887554716,
      -1.210980523777666,
      -1.2119822026877147,
      -1.2617099898391984,
      -1.323787310525384,
      -1.287429031013981,
      -1.3402503205325706,
      -1.2475971239650472,
      -1.3149395483193647,
      -1.421124862854285,
      -1.3254233711617467,
      -1.2506002920055586,
      -1.4026761474888298,
      -1.3557051939135887,
      -1.2400811080876357,
      -1.3111188428083937,
      -1.4233406194429494
    ],
    [
      -1.165795433674313,
      -1.0936440836973413,
      -1.0809083128554617,
      -1.0491390567910848,
      -1.0277458298302289,
      -1.0525610445868532,
      -1.0311515276796868,
      -1.024134725816401,
      -1.0079666844003525,
      -1.0648819542462815,
      -1.059434283456889,
      0.0,
      -1.0540057559895673,
      -1.0584921984932192,
      -1.0605757973900836,
      -1.0120978812029913,
      -1.0662968102326857,
      -1.0540314339308554,
      -1.065093328715827,
      -1.0527130512833813,
      -1.0870099333674832,
      -1.0274642531790137,
      -1.0359208364495958,
      -1.064626410242667,
      -1.080464787648902,
      -1.0812961888731523,
      -1.0457662658072249,
      -1.0790460444173282,
      -1.0368231716813467
    ],
    [
      -1.257410809233934,
      -1.188621657083025,
      -1.2396432757854379,
      -1.2183521114027491,
      -1.2814861186643876,
      -1.2643570629289276,
      -1.2574295335452668,
      -1.3174078476424862,
      -1.1546799185441066,
      -1.240410075857233,
      -1.1736711360228391,
      -1.383038583562106,
      0.0,
      -1.170714445788518,
      -1.1935935098671078,
      -1.1077551099242418,
      -1.2079808787005901,
      -1.2616171291393241,
      -1.2394939675703487,
      -1.2308392415369047,
      -1.2376550712969776,
      -1.3537103407258844,
      -1.3054921411703924,
      -1.2032339776561964,
      -1.3533193329174102,
      -1.2749525947112255,
      -1.1983229988973632,
      -1.2657676705009384,
      -1.3588798205785635
    ],
    [
      -1.2555280120891072,
      -1.1730897244557086,
      -1.1988902169544207,
      -1.180798506742751,
      -1.18885926710093,
      -1.2191225569788733,
      -1.2116858003712798,
      -1.230714121434317,
      -1.1048919437699867,
      -1.232353028977338,
      -1.0710547631606182,
      -1.3721024843672458,
      -1.1361876753245348,
      0.0,
      -1.1436956527019544,
      -1.0953550890999864,
      -1.1500564161600741,
      -1.1768413750743911,
      -1.2510905298663373,
      -1.1428250946521696,
      -1.18000220470429,
      -1.289890886601259,
      -1.2374393715508392,
      -1.1693579767099769,
      -1.2948194839918197,
      -1.2038755005389004,
      -1.1645926213634603,
      -1.2204668480910286,
      -1.292680540101739
    ],
    [
      -1.47220397601606,
      -1.2994319484709822,
      -1.2610502401626333,
      -1.3128574785597231,
      -1.4324203261811703,
      -1.4513439991924217,
      -1.4534235168416145,
      -1.4695356497908358,
      -1.3108786200433966,
      -1.3584802768881585,
      -1.3713200582651222,
      -1.5527677722969488,
      -1.4085134709658031,
      -1.3911562477184958,
      0.0,
      -1.3190499608614978,
      -1.2784871813287824,
      -1.3257629923956356,
      -1.481468351897592,
      -1.2324870124226124,
      -1.4384168397963117,
      -1.4187553340547696,
      -1.421722519089269,
      -1.3497947532313264,
      -1.466078761433411,
      -1.3625048124363877,
      -1.40521968486849,
      -1.3355304514256299,
      -1.5031277500201512
    ],
    [
      -1.453942796170937,
      -1.3622006930345796,
      -1.403604078300303,
      -1.4091281715066277,
      -1.3996510806437208,
      -1.4567041982565392,
      -1.4570976506323678,
      -1.463235172146086,
      -1.3621438195408972,
      -1.4440191304320926,
      -1.3707673331495014,
      -1.5388446086713896,
      -1.3090300852899746,
      -1.3242070809219264,
      -1.347434792332108,
      0.0,
      -1.4128746981053621,
      -1.4011635146998196,
      -1.4784785841676498,
      -1.3913255278349133,
      -1.4133443621360033,
      -1.5286845096591666,
      -1.4564882107477006,
      -1.4307599758016498,
      -1.510922840718525,
      -1.4307749395171776,
      -1.4156328303621644,
      -1.4724935244260986,
      -1.5224439901619924
    ],
    [
      -1.6978291518379134,
      -1.482465312353423,
      -1.5194524466149075,
      -1.5962051478303572,
      -1.6738169543705745,
      -1.7243842050300142,
      -1.6603740618676455,
      -1.6896206577770803,
      -1.556968760400914,
      -1.6109916488051759,
      -1.5926189348374464,
      -1.7690571486778464,
      -1.5896009338536106,
      -1.6083594270870694,
      -1.5185831877961167,
      -1.6082938579590584,
      0.0,
      -1.5755059788331829,
      -1.6743415366545484,
      -1.534590214121477,
      -1.5991286520137173,
      -1.685194469070599,
      -1.6894182723609847,
      -1.541745670949881,
      -1.6953326264924533,
      -1.6623836115544646,
      -1.6399011869158566,
      -1.6156247570644695,
      -1.6938319076159936
    ],
    [
      -1.4410440693038578,
      -1.3170766847397106,
      -1.3190400645494613,
      -1.3421240015393834,
      -1.302277892629597,
      -1.418949745050632,
      -1.3724327412258162,
      -1.293286947430671,
      -1.2429247996630015,
      -1.307035916326769,
      -1.3167992696819368,
      -1.4881800845709983,
      -1.3847102972108143,
      -1.281897684501694,
      -1.218262703684569,
      -1.3314075412602944,
      -1.26527021501926,
      0.0,
      -1.3891715422268813,
      -1.2573376777078011,
      -1.390643059812354,
      -1.375624361409107,
      -1.363133016708755,
      -1.2210951245141457,
      -1.4051048951494418,
      -1.370105565034712,
      -1.3797621863556173,
      -1.3750970384560999,
      -1.4262466721327909
    ],
    [
      -1.3043496962563161,
      -1.2025875089224667,
      -1.21856859044555,
      -1.220904540303842,
      -1.2305435929305188,
      -1.2910629476616664,
      -1.2587139172420612,
      -1.2131272327912952,
      -1.210636838476163,
      -1.2087559357906388,
      -1.2134870845232963,
      -1.3101928596827868,
      -1.2576448682944599,
      -1.2842073580575435,
      -1.245385831554928,
      -1.2626501274161848,
      -1.2318999975751757,
      -1.2397653926247056,
      0.0,
      -1.2286484890004596,
      -1.246808059262202,
      -1.2456004310703515,
      -1.2418258780445486,
      -1.2378061067516324,
      -1.3001733579258097,
      -1.259570188164148,
      -1.2650354793717355,
      -1.3036778894313528,
      -1.26524881345301
    ],
    [
      -1.433333237003329,
      -1.2429666057358357,
      -1.2641377590683123,
      -1.3478743577089274,
      -1.309263133989204,
      -1.367881343561275,
      -1.3631952335948436,
      -1.3437069536736779,
      -1.1765584725426006,
      -1.285561353784158,
      -1.3197257813737713,
      -1.476811373113271,
      -1.3645305540657855,
      -1.305218860037813,
      -1.1825926437420524,
      -1.2990663098421948,
      -1.2854701001180695,
      -1.2220800184562828,
      -1.4198268310718667,
      0.0,
      -1.3872041130602137,
      -1.3727962774204951,
      -1.3794129605550538,
      -1.274505306815381,
      -1.401986529171775,
      -1.3734726404602378,
      -1.344722978982738,
      -1.3001499497745654,
      -1.4477054748622822
    ],
    [
      -1.7048115434597506,
      -1.6099479103007062,
      -1.5983302654982985,
      -1.6365297485440857,
      -1.638492360521453,
      -1.650197886835676,
      -1.6457167595445954,
      -1.6335367514679298,
      -1.633147571685167,
      -1.605535544723804,
      -1.6215709345193938,
      -1.778782515482506,
      -1.6259323938793888,
      -1.6138853527330175,
      -1.6338745392062743,
      -1.599640907260611,
      -1.611793888225481,
      -1.69170113445315,
      -1.6419161928106079,
      -1.661571114485255,
      0.0,
      -1.7065737786856492,
      -1.6489908711465353,
      -1.6505926656334844,
      -1.674234505383481,
      -1.5680622330816183,
      -1.6172281036327767,
      -1.6329113927270653,
      -1.6838599403574468
    ],
    [
      -1.2868625839335168,
      -1.2684238206171454,
      -1.280260351492482,
      -1.2945897954752816,
      -1.2427017222664773,
      -1.3119212559208369,
      -1.2892959202736654,
      -1.263423602835741,
      -1.266040263775518,
      -1.2190498915396948,
      -1.2452215417521029,
      -1.2967349013527079,
      -1.283928718997821,
      -1.256756520959028,
      -1.214908532088599,
      -1.2914499060315685,
      -1.2610735820803678,
      -1.235413686391211,
      -1.2716000119710935,
      -1.2253925541049846,
      -1.2586285985702221,
      0.0,
      -1.2815811554619234,
      -1.2732177275220022,
      -1.31512055926044,
      -1.1911874689362376,
      -1.2886888308849151,
      -1.292971883989181,
      -1.2098212798791452
    ],
    [
      -1.4666091935365688,
      -1.403011427945668,
      -1.4385388498228164,
      -1.4256023165099672,
      -1.3404682871487306,
      -1.456273401239454,
      -1.4650990090676923,
      -1.3767960941808373,
      -1.3673331642553757,
      -1.3514555121258138,
      -1.3743254700715186,
      -1.4597377932429536,
      -1.4006395820815796,
      -1.3718562862145585,
      -1.392118041930955,
      -1.38538892169457,
      -1.3807735275764945,
      -1.4133026118320846,
      -1.3766374543960311,
      -1.4016078756480512,
      -1.4082675982551445,
      -1.4437475630218406,
      0.0,
      -1.4231889139886174,
      -1.39756749960991,
      -1.4463341091328308,
      -1.4083748442260133,
      -1.419207150859588,
      -1.3957833071038173
    ],
    [
      -1.2954915183463878,
      -1.168015938811989,
      -1.1437741771211372,
      -1.1911585283449306,
      -1.2145779998567514,
      -1.264369515458653,
      -1.2786035925599823,
      -1.2589765104364972,
      -1.1412409571694282,
      -1.2340347637535964,
      -1.1366845429744448,
      -1.3702220072705185,
      -1.1743641808113254,
      -1.1709146680680105,
      -1.1293699108816588,
      -1.175967860412441,
      -1.1841537860778957,
      -1.08776065415682,
      -1.2740416763295106,
      -1.1310029025726367,
      -1.2411062215589497,
      -1.3302889211432585,
      -1.282573849641723,
      0.0,
      -1.3000123836145971,
      -1.2497229522194142,
      -1.2231694641413087,
      -1.2560318024339017,
      -1.3307075723151167
    ],
    [
      -1.3494161588595546,
      -1.3088906700021337,
      -1.3159033251388497,
      -1.286905123793409,
      -1.2745563967003068,
      -1.3331976524770213,
      -1.2887489888156518,
      -1.3265427779786017,
      -1.32557504920933,
      -1.272807888854751,
      -1.2920474411020815,
      -1.3930815577922069,
      -1.3513511546297272,
      -1.341667612977109,
      -1.2980437334063746,
      -1.2928884830519969,
      -1.276006601093123,
      -1.344287695067826,
      -1.3251699738316494,
      -1.3200643371937903,
      -1.333994426222149,
      -1.3667619302839982,
      -1.2305864188257585,
      -1.30003664041537,
      0.0,
      -1.3120951015283946,
      -1.2867697783508778,
      -1.2928917107952436,
      -1.3199137138868944
    ],
    [
      -1.5213436856776863,
      -1.4586864543953029,
      -1.4166604307431803,
      -1.4145002000057165,
      -1.4421619916341342,
      -1.4359371804890033,
      -1.4368763473167148,
      -1.464467576887212,
      -1.45632605309233,
      -1.3791726203499801,
      -1.3953841092093047,
      -1.527626253874243,
      -1.4174523671281622,
      -1.3959615424590002,
      -1.3458959893542417,
      -1.4268230045592882,
      -1.4287542966355866,
      -1.4099680321637014,
      -1.447022379557764,
      -1.368088450197901,
      -1.401136215679016,
      -1.391818580048194,
      -1.4616379768328829,
      -1.405843486987734,
      -1.4487706518133145,
      0.0,
      -1.4321658667196941,
      -1.4492778974129905,
      -1.4802889378884805
    ],
    [
      -1.4089941208676522,
      -1.4174643289125881,
      -1.4393055809904027,
      -1.430190420030162,
      -1.413414206101819,
      -1.4889469771332244,
      -1.4546030285016192,
      -1.4537136591936675,
      -1.4331581517937917,
      -1.4224328703052855,
      -1.3735537406478013,
      -1.5795983557086293,
      -1.4382116446385502,
      -1.4074780778391571,
      -1.4121493998838677,
      -1.3903904940377072,
      -1.4668275565783182,
      -1.4680393816936563,
      -1.5192389824812311,
      -1.4590713952159522,
      -1.4423524287096419,
      -1.5446068723977515,
      -1.4523058438610408,
      -1.4115716730971226,
      -1.4908561201430675,
      -1.4585049283395106,
      0.0,
      -1.4619063784961566,
      -1.5286473153477829
    ],
    [
      -1.1670086855593744,
      -1.1142263200260152,
      -1.155399949474571,
      -1.115847437888421,
      -1.1540136876337026,
      -1.1225039129949113,
      -1.1855463642978732,
      -1.1238820424868694,
      -1.1175873309336226,
      -1.1312159571268536,
      -1.1471458361962055,
      -1.2377413085890747,
      -1.1679891108737313,
      -1.147129189129538,
      -1.1129527316510754,
      -1.1161798314432199,
      -1.1029105186772825,
      -1.1571869921271694,
      -1.1890460790113693,
      -1.0556771011315875,
      -1.1622587956895485,
      -1.2091192520559446,
      -1.1273943518788427,
      -1.1798427067296087,
      -1.185225394459788,
      -1.1523584102455606,
      -1.1281865794798365,
      0.0,
      -1.2396358359282793
    ],
    [
      -1.3956678599159893,
      -1.3525179454622946,
      -1.3197082965778493,
      -1.351391649210214,
      -1.323801608491956,
      -1.3793091835119733,
      -1.3357593426786043,
      -1.3193419219102593,
      -1.3095242535934304,
      -1.327902065362348,
      -1.3393693264709567,
      -1.383866466093505,
      -1.3632378057898282,
      -1.3364137543529502,
      -1.303508433176529,
      -1.327739685164892,
      -1.3170970389975656,
      -1.346054832416602,
      -1.297338774348575,
      -1.3170070842051993,
      -1.3455264299798533,
      -1.2891168165892304,
      -1.31831106113421,
      -1.3283969145602634,
      -1.3439235473115547,
      -1.3493219957194922,
      -1.3663196629658318,
      -1.3276413336024182,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.14994725714198154,
      0.12220527577576101,
      0.13941138142101916,
      0.13426295281536538,
      0.11615010409700277,
      0.11908321221130702,
      0.1371629046015066,
      0.14322811807106328,
      0.12818511433675228,
      0.1490918048226182,
      0.08413239283027796,
      0.12303906230214956,
      0.1361675914508833,
      0.13233389951811247,
      0.13012814200710876,
      0.12318080987899904,
      0.13747950416977162,
      0.11901589457568118,
      0.14630892058063916,
      0.12364545870422816,
      0.12316822194972366,
      0.1227758792094562,
      0.13879897526006957,
      0.10470609325000124,
      0.13679686767357557,
      0.13241207863944116,
      0.129650095853187,
      0.09854593790712896
    ],
    [
      0.3385234114429305,
      0.0,
      0.479696797300337,
      0.41196098744387033,
      0.33374772972390887,
      0.3197275163030464,
      0.31817927885351893,
      0.3205824691943051,
      0.5274566765078916,
      0.3611918840616799,
      0.42783393089196875,
      0.25466728706641084,
      0.4009777186970169,
      0.4176652248104844,
      0.5069464884807833,
      0.44742315109244557,
      0.48233546994988874,
      0.4521436617649457,
      0.3746167685458861,
      0.48672610309108433,
      0.36106955613936,
      0.3048756035501339,
      0.33488699336992744,
      0.4495487742428548,
      0.2848195098214725,
      0.34379643226376433,
      0.36195519336643334,
      0.37477915687980157,
      0.2964688245751743
    ],
    [
      0.3446275179606233,
      0.52274922773568,
      0.0,
      0.46539521347212376,
      0.41026746558253424,
      0.3541356355460117,
      0.3326858760203171,
      0.3771979259318945,
      0.5419578003870582,
      0.39312568474889686,
      0.4751878288308671,
      0.24629927220124204,
      0.4141728983809241,
      0.4402642921672377,
      0.5696763171389316,
      0.4661521065623795,
      0.4667313076518014,
      0.4775347435270305,
      0.3820467693445144,
      0.5256126538652381,
      0.3835813230923255,
      0.3248005991866878,
      0.3860415314941963,
      0.5246016071254187,
      0.3425157863772923,
      0.3729697794675051,
      0.3959651364238288,
      0.4208774340195436,
      0.304281079415643
    ],
    [
      0.4187929231382379,
      0.5160821758879064,
      0.48893570844347223,
      0.0,
      0.4229016069199214,
      0.39467879154745567,
      0.40329899040554995,
      0.41106624812961345,
      0.4434873911277162,
      0.4275669270615545,
      0.4665526652534928,
      0.2840621073581635,
      0.4481195789969967,
      0.4645715693690722,
      0.5218038713728019,
      0.45091249678683476,
      0.43078785877647596,
      0.4838439339287568,
      0.3946254571897503,
      0.43830889903990733,
      0.43696376555222316,
      0.33964665395070015,
      0.3703184956355985,
      0.4748417079456504,
      0.37451571541101836,
      0.4127751737983474,
      0.4223335584874701,
      0.4077104776655409,
      0.3513293216648239
    ],
    [
      0.37349245309719836,
      0.4417608462249796,
      0.4101618504056592,
      0.4102673688143328,
      0.0,
      0.4418013964438472,
      0.37704814786582497,
      0.5134408015690577,
      0.5332136287616935,
      0.4605223215764518,
      0.498716151168906,
      0.32958920064319375,
      0.3955406263126209,
      0.4518737251289304,
      0.46499385578370256,
      0.47082964933044047,
      0.4084886171840454,
      0.44154621703797026,
      0.46256152745340784,
      0.4915062438368889,
      0.41033067650705823,
      0.3804314526667678,
      0.443936485313164,
      0.4344599690496578,
      0.3435254743016849,
      0.39433777852909446,
      0.44446330609130436,
      0.4053594881937339,
      0.40750548088390914
    ],
    [
      0.32040326857156276,
      0.3691932370206561,
      0.3547332216337413,
      0.3488097730135047,
      0.36442942146755875,
      0.0,
      0.3367705493509485,
      0.38892651787426313,
      0.3900773070582504,
      0.3548055997667854,
      0.434893633234555,
      0.22897964582462138,
      0.34592765599246644,
      0.388422018587506,
      0.3802252959651109,
      0.3411490976926095,
      0.3580585303567734,
      0.3738348217141019,
      0.30572078416334825,
      0.3722389592460542,
      0.3719141984759329,
      0.24814488366625098,
      0.32330677834316,
      0.37018640452963036,
      0.264607950151065,
      0.384713422605623,
      0.37323808998560315,
      0.3573431561958955,
      0.25047527750942367
    ],
    [
      0.38132325957086644,
      0.4150603896962579,
      0.3850629874951246,
      0.40931664058334816,
      0.4114393871885691,
      0.3960082996151373,
      0.0,
      0.41100650228661006,
      0.39902577876551093,
      0.42995629742738095,
      0.44099326072879586,
      0.33395776354144746,
      0.4172033720413628,
      0.4766101260800868,
      0.44194217096978394,
      0.41183427817456364,
      0.4077570121788159,
      0.4019833783566831,
      0.3488755463028419,
      0.42254732587057964,
      0.4567287212343685,
      0.3309780587403397,
      0.38762010335846675,
      0.43253879485681535,
      0.3893775474383927,
      0.38828044467851885,
      0.44337026053368933,
      0.43309289638119575,
      0.35048040372808575
    ],
    [
      0.2537087878093587,
      0.34856267757187176,
      0.3405208229906096,
      0.3451397902514053,
      0.43178765897835225,
      0.35902308875217304,
      0.32174580099199646,
      0.0,
      0.43499337171800967,
      0.3637699981882738,
      0.3893570371522166,
      0.3084733137637645,
      0.31402845744515173,
      0.3592499535356799,
      0.36261442325963467,
      0.39204948367744685,
      0.3655162612191789,
      0.4110563330458188,
      0.3927049958833886,
      0.3974474424219703,
      0.36937505728639586,
      0.32675623850269964,
      0.3895289203038448,
      0.3410842005977728,
      0.3324961438226204,
      0.3290294665594089,
      0.34612919881553283,
      0.34754320668274175,
      0.348735438997015
    ],
    [
      0.35690998969376353,
      0.5182777520227313,
      0.49338548869781396,
      0.4254283601324904,
      0.49981204392017475,
      0.38465908033314444,
      0.3341241193605493,
      0.4276605433987839,
      0.0,
      0.40159321011393745,
      0.44560398903722365,
      0.30923608895913524,
      0.4102476999744926,
      0.520825173146191,
      0.5120916406176161,
      0.49095354710470684,
      0.47157422961824236,
      0.5327520206177199,
      0.3983870715213411,
      0.5780712250713096,
      0.37998621191184134,
      0.3787730237651732,
      0.4089534690880019,
      0.49515504352257156,
      0.3174497183069953,
      0.35268149225345424,
      0.39090275618226333,
      0.41694868799246443,
      0.36579589018102543
    ],
    [
      0.26964831375825726,
      0.38621085299839675,
      0.36673708025046237,
      0.3709048671946251,
      0.37097516138943987,
      0.34658195570056316,
      0.35961395997119827,
      0.3634663101185638,
      0.32542593130999253,
      0.0,
      0.3508839747604531,
      0.26891169424621775,
      0.3647788911265597,
      0.3465884777550339,
      0.4165342885411052,
      0.3453933031377545,
      0.4241530788454453,
      0.4035432862044763,
      0.3447774395605665,
      0.39621641045332767,
      0.37601991929720424,
      0.3276362972836162,
      0.3773632897821466,
      0.37444952300619216,
      0.323946415231104,
      0.3815002443847473,
      0.3835993042238013,
      0.37743643831632645,
      0.330427899343144
    ],
    [
      0.359247368565319,
      0.5263501903694789,
      0.5007782535543575,
      0.42058546903222926,
      0.5116217706352717,
      0.41072884436461554,
      0.40753121717163343,
      0.40189395652169924,
      0.5168824143427999,
      0.416384387965274,
      0.0,
      0.24941773376734488,
      0.47316311520164334,
      0.5204924801794488,
      0.5194908012694002,
      0.46976301411791654,
      0.407685693431731,
      0.4440439729431338,
      0.39122268342454425,
      0.4838758799920677,
      0.41653345563775024,
      0.31034814110282993,
      0.4060496327953682,
      0.48087271195155634,
      0.32879685646828505,
      0.3757678100435262,
      0.4913918958694792,
      0.42035416114872115,
      0.30813238451416547
    ],
    [
      0.23772173415551423,
      0.30987308413248593,
      0.32260885497436553,
      0.3543781110387425,
      0.3757713379995984,
      0.3509561232429741,
      0.3723656401501405,
      0.3793824420134262,
      0.39555048342947474,
      0.33863521358354576,
      0.3440828843729382,
      0.0,
      0.34951141184026,
      0.3450249693366081,
      0.3429413704397437,
      0.391419286626836,
      0.33722035759714153,
      0.34948573389897186,
      0.3384238391140002,
      0.35080411654644594,
      0.3165072344623441,
      0.3760529146508136,
      0.3675963313802315,
      0.33889075758716025,
      0.3230523801809253,
      0.322220978956675,
      0.3577509020226024,
      0.32447112341249906,
      0.3666939961484805
    ],
    [
      0.3645782694283721,
      0.4333674215792811,
      0.38234580287686826,
      0.403636967259557,
      0.3405029599979186,
      0.35763201573337855,
      0.36455954511703936,
      0.30458123101981993,
      0.46730916011819956,
      0.3815790028050732,
      0.448317942639467,
      0.23895049510020017,
      0.0,
      0.4512746328737882,
      0.42839556879519836,
      0.5142339687380644,
      0.414008199961716,
      0.360371949522982,
      0.38249511109195744,
      0.3911498371254014,
      0.3843340073653285,
      0.26827873793642176,
      0.31649693749191377,
      0.4187551010061097,
      0.2686697457448959,
      0.34703648395108067,
      0.42366607976494297,
      0.3562214081613677,
      0.26310925808374264
    ],
    [
      0.355237424776883,
      0.4376757124102817,
      0.4118752199115696,
      0.42996693012323917,
      0.42190616976506035,
      0.391642879887117,
      0.39907963649471045,
      0.38005131543167336,
      0.5058734930960036,
      0.3784124078886524,
      0.5397106737053721,
      0.23866295249874447,
      0.4745777615414555,
      0.0,
      0.4670697841640359,
      0.5154103477660039,
      0.46070902070591613,
      0.43392406179159915,
      0.359674906999653,
      0.46794034221382064,
      0.4307632321617003,
      0.3208745502647312,
      0.3733260653151511,
      0.4414074601560134,
      0.3159459528741706,
      0.4068899363270899,
      0.44617281550252996,
      0.3902985887749617,
      0.31808489676425133
    ],
    [
      0.36948927781205443,
      0.5422613053571321,
      0.580643013665481,
      0.5288357752683912,
      0.409272927646944,
      0.3903492546356926,
      0.3882697369864998,
      0.3721576040372785,
      0.5308146337847177,
      0.4832129769399558,
      0.4703731955629922,
      0.2889254815311655,
      0.4331797828623112,
      0.45053700610961855,
      0.0,
      0.5226432929666165,
      0.563206072499332,
      0.5159302614324788,
      0.36022490193052237,
      0.609206241405502,
      0.4032764140318026,
      0.4229379197733447,
      0.4199707347388453,
      0.49189850059678797,
      0.37561449239470335,
      0.47918844139172667,
      0.4364735689596244,
      0.5061628024024845,
      0.33856550380796313
    ],
    [
      0.3598707877174019,
      0.45161289085375933,
      0.4102095055880359,
      0.4046854123817112,
      0.4141625032446181,
      0.3571093856317997,
      0.35671593325597106,
      0.350578411742253,
      0.45166976434744166,
      0.3697944534562463,
      0.4430462507388375,
      0.2749689752169493,
      0.5047834985983644,
      0.4896065029664125,
      0.46637879155623096,
      0.0,
      0.40093888578297676,
      0.4126500691885193,
      0.33533499972068914,
      0.42248805605342565,
      0.40046922175233557,
      0.28512907422917233,
      0.3573253731406383,
      0.3830536080866891,
      0.302890743169814,
      0.3830386443711613,
      0.39818075352617455,
      0.3413200594622403,
      0.2913695937263465
    ],
    [
      0.2882339604665902,
      0.5035977999510806,
      0.4666106656895961,
      0.38985796447414645,
      0.31224615793392907,
      0.2616789072744894,
      0.32568905043685814,
      0.29644245452742335,
      0.4290943519035897,
      0.37507146349932774,
      0.3934441774670572,
      0.21700596362665725,
      0.39646217845089304,
      0.37770368521743425,
      0.46747992450838693,
      0.37776925434544517,
      0.0,
      0.4105571334713207,
      0.3117215756499552,
      0.4514728981830267,
      0.38693446029078626,
      0.3008686432339045,
      0.29664483994351887,
      0.4443174413546227,
      0.2907304858120503,
      0.32367950075003904,
      0.346161925388647,
      0.3704383552400341,
      0.29223120468851005
    ],
    [
      0.3270937001127763,
      0.4510610846769234,
      0.4490977048671727,
      0.42601376787725065,
      0.465859876787037,
      0.34918802436600194,
      0.3957050281908179,
      0.47485082198596307,
      0.5252129697536325,
      0.46110185308986495,
      0.4513384997346972,
      0.27995768484563577,
      0.38342747220581974,
      0.48624008491494,
      0.549875065732065,
      0.43673022815633966,
      0.502867554397374,
      0.0,
      0.3789662271897527,
      0.5108000917088329,
      0.37749470960428,
      0.392513408007527,
      0.40500475270787906,
      0.5470426449024883,
      0.3630328742671922,
      0.398032204381922,
      0.3883755830610167,
      0.39304073096053416,
      0.34189109728384315
    ],
    [
      0.3122963578551383,
      0.41405854518898777,
      0.39807746366590435,
      0.39574151380761236,
      0.3861024611809356,
      0.32558310644978805,
      0.3579321368693933,
      0.40351882132015926,
      0.40600921563529146,
      0.4078901183208157,
      0.4031589695881581,
      0.3064531944286677,
      0.3590011858169946,
      0.33243869605391096,
      0.37126022255652646,
      0.3539959266952697,
      0.3847460565362788,
      0.3768806614867488,
      0.0,
      0.3879975651109948,
      0.3698379948492525,
      0.371045623041103,
      0.3748201760669059,
      0.37883994735982207,
      0.3164726961856448,
      0.35707586594730656,
      0.3516105747397189,
      0.3129681646801017,
      0.35139724065844447
    ],
    [
      0.30288243846188645,
      0.49324906972937965,
      0.4720779163969031,
      0.388341317756288,
      0.42695254147601136,
      0.36833433190394027,
      0.37302044187037176,
      0.3925087217915375,
      0.5596572029226148,
      0.45065432168105746,
      0.4164898940914441,
      0.25940430235194434,
      0.3716851213994299,
      0.4309968154274024,
      0.553623031723163,
      0.43714936562302054,
      0.4507455753471459,
      0.5141356570089326,
      0.3163888443933487,
      0.0,
      0.34901156240500164,
      0.36341939804472023,
      0.3568027149101616,
      0.4617103686498343,
      0.33422914629344036,
      0.3627430350049776,
      0.3914926964824774,
      0.43606572569064994,
      0.2885102006029332
    ],
    [
      0.38227406268200537,
      0.4771376958410498,
      0.4887553406434575,
      0.45055585759767025,
      0.448593245620303,
      0.43688771930608006,
      0.4413688465971606,
      0.4535488546738262,
      0.45393803445658887,
      0.481550061417952,
      0.46551467162236215,
      0.3083030906592499,
      0.46115321226236716,
      0.4732002534087385,
      0.45321106693548163,
      0.48744469888114494,
      0.475291717916275,
      0.3953844716886059,
      0.4451694133311481,
      0.425514491656501,
      0.0,
      0.3805118274561068,
      0.43809473499522067,
      0.4364929405082716,
      0.4128511007582749,
      0.5190233730601377,
      0.4698575025089793,
      0.45417421341469066,
      0.4032256657843092
    ],
    [
      0.3674315113540687,
      0.3858702746704401,
      0.37403374379510357,
      0.3597042998123039,
      0.4115923730211082,
      0.34237283936674867,
      0.3649981750139202,
      0.39087049245184446,
      0.3882538315120676,
      0.43524420374789075,
      0.40907255353548266,
      0.3575591939348777,
      0.3703653762897645,
      0.39753757432855763,
      0.4393855631989865,
      0.362844189256017,
      0.39322051320721774,
      0.41888040889637446,
      0.382694083316492,
      0.42890154118260093,
      0.3956654967173634,
      0.0,
      0.3727129398256621,
      0.3810763677655833,
      0.3391735360271455,
      0.4631066263513479,
      0.3656052644026704,
      0.36132221129840447,
      0.4444728154084403
    ],
    [
      0.2895459100229061,
      0.3531436756138069,
      0.31761625373665847,
      0.33055278704950775,
      0.41568681641074434,
      0.29988170232002087,
      0.2910560944917826,
      0.37935900937863765,
      0.38882193930409925,
      0.4046995914336611,
      0.3818296334879563,
      0.2964173103165213,
      0.35551552147789534,
      0.38429881734491644,
      0.36403706162852,
      0.37076618186490484,
      0.3753815759829804,
      0.3428524917273903,
      0.37951764916344377,
      0.3545472279114237,
      0.34788750530433044,
      0.3124075405376343,
      0.0,
      0.33296618957085755,
      0.358587603949565,
      0.3098209944266441,
      0.34778025933346157,
      0.33694795269988687,
      0.36037179645565764
    ],
    [
      0.40382998370490353,
      0.5313055632393022,
      0.5555473249301541,
      0.5081629737063607,
      0.4847435021945399,
      0.43495198659263834,
      0.42071790949130894,
      0.4403449916147941,
      0.5580805448818631,
      0.46528673829769485,
      0.5626369590768465,
      0.3290994947807728,
      0.5249573212399659,
      0.5284068339832808,
      0.5699515911696325,
      0.5233536416388502,
      0.5151677159733956,
      0.6115608478944712,
      0.4252798257217807,
      0.5683185994786546,
      0.4582152804923416,
      0.3690325809080328,
      0.41674765240956835,
      0.0,
      0.39930911843669414,
      0.4495985498318771,
      0.4761520379099826,
      0.4432896996173896,
      0.3686139297361746
    ],
    [
      0.27977206456402115,
      0.32029755342144206,
      0.31328489828472605,
      0.3422830996301667,
      0.3546318267232689,
      0.29599057094655445,
      0.3404392346079239,
      0.30264544544497407,
      0.30361317421424583,
      0.3563803345688248,
      0.33714078232149425,
      0.23610666563136884,
      0.2778370687938485,
      0.2875206104464667,
      0.33114449001720114,
      0.33629974037157884,
      0.3531816223304527,
      0.28490052835574975,
      0.30401824959192636,
      0.30912388622978537,
      0.2951937972014267,
      0.26242629313957755,
      0.39860180459781724,
      0.3291515830082057,
      0.0,
      0.31709312189518113,
      0.3424184450726979,
      0.33629651262833216,
      0.30927450953668134
    ],
    [
      0.3283439049976813,
      0.3910011362800647,
      0.4330271599321873,
      0.43518739066965106,
      0.4075255990412334,
      0.41375041018636427,
      0.4128112433586528,
      0.3852200137881556,
      0.3933615375830375,
      0.47051497032538747,
      0.4543034814660629,
      0.3220613368011247,
      0.43223522354720534,
      0.4537260482163674,
      0.5037916013211259,
      0.42286458611607936,
      0.42093329403978097,
      0.4397195585116662,
      0.40266521111760367,
      0.48159914047746666,
      0.44855137499635167,
      0.45786901062717367,
      0.3880496138424847,
      0.4438441036876335,
      0.40091693886205304,
      0.0,
      0.41752172395567344,
      0.4004096932623771,
      0.3693986527868871
    ],
    [
      0.4010787704286183,
      0.39260856238368236,
      0.37076731030586774,
      0.3798824712661084,
      0.3966586851944516,
      0.32112591416304603,
      0.3554698627946513,
      0.356359232102603,
      0.3769147395024788,
      0.38764002099098493,
      0.43651915064846913,
      0.2304745355876412,
      0.37186124665772025,
      0.40259481345711334,
      0.3979234914124028,
      0.41968239725856327,
      0.34324533471795227,
      0.34203350960261414,
      0.29083390881503934,
      0.3510014960803183,
      0.3677204625866286,
      0.26546601889851895,
      0.3577670474352297,
      0.39850121819914786,
      0.319216771153203,
      0.35156796295675985,
      0.0,
      0.3481665128001139,
      0.2814255759484876
    ],
    [
      0.3174558668764784,
      0.37023823240983766,
      0.3290646029612818,
      0.36861711454743196,
      0.33045086480215025,
      0.3619606394409416,
      0.29891818813797966,
      0.36058250994898344,
      0.36687722150223023,
      0.3532485953089992,
      0.3373187162396474,
      0.24672324384677813,
      0.3164754415621216,
      0.33733536330631475,
      0.3715118207847774,
      0.368284720992633,
      0.38155403375857033,
      0.3272775603086835,
      0.2954184734244836,
      0.4287874513042653,
      0.32220575674630436,
      0.27534530037990823,
      0.3570702005570101,
      0.3046218457062442,
      0.29923915797606493,
      0.3321061421902922,
      0.3562779729560164,
      0.0,
      0.24482871650757354
    ],
    [
      0.2673068188843657,
      0.31045673333806034,
      0.3432663822225057,
      0.31158302959014095,
      0.33917307030839905,
      0.2836654952883817,
      0.3272153361217507,
      0.34363275689009565,
      0.3534504252069246,
      0.3350726134380071,
      0.32360535232939824,
      0.27910821270685005,
      0.29973687301052676,
      0.3265609244474048,
      0.359466245623826,
      0.33523499363546305,
      0.3458776398027894,
      0.31691984638375303,
      0.36563590445178007,
      0.3459675945951557,
      0.3174482488205017,
      0.3738578622111246,
      0.34466361766614506,
      0.33457776424009156,
      0.31905113148880027,
      0.3136526830808628,
      0.29665501583452314,
      0.3353333451979368,
      0.0
    ]
  ],
  "row_avgs": [
    0.1278933553948147,
    0.3848072356939759,
    0.41648052905920524,
    0.42502978824446613,
    0.43006088357769734,
    0.3464831964284643,
    0.4055132502794156,
    0.3579438417937261,
    0.42922284166232716,
    0.3597758788639544,
    0.4274787962993426,
    0.3456926290462481,
    0.37413778004607456,
    0.4111844478325854,
    0.45298646859042746,
    0.3864065053395077,
    0.36086237227783285,
    0.4254219909203421,
    0.36668608936056707,
    0.40436720569428636,
    0.4435367202029985,
    0.38942742856030305,
    0.34936775331949343,
    0.4765236856769025,
    0.31632385405628355,
    0.418971569992769,
    0.3576609651195862,
    0.3342784198030001,
    0.32672056845769865
  ],
  "col_avgs": [
    0.3346828620682066,
    0.4201075338481049,
    0.40575452324982775,
    0.3912573798291153,
    0.39403850421319103,
    0.3523770006942484,
    0.353443328292492,
    0.37567997534963377,
    0.43250896932873173,
    0.39546751307289024,
    0.4177506451610635,
    0.2717110226452489,
    0.3853558847867261,
    0.40991908085892254,
    0.4380749908744388,
    0.4140255389506085,
    0.4094487157017391,
    0.4076152365886168,
    0.3567506451067445,
    0.4294457371690246,
    0.3734880394152418,
    0.32834270991802633,
    0.36758846841848974,
    0.4065601983740626,
    0.326633610219806,
    0.3681615520404517,
    0.3856397821443066,
    0.3763579392511841,
    0.32305866402315236
  ],
  "combined_avgs": [
    0.23128810873151062,
    0.4024573847710404,
    0.41111752615451647,
    0.40814358403679074,
    0.4120496938954442,
    0.3494300985613563,
    0.3794782892859538,
    0.36681190857167995,
    0.43086590549552944,
    0.3776216959684223,
    0.42261472073020306,
    0.30870182584574846,
    0.37974683241640034,
    0.41055176434575397,
    0.44553072973243313,
    0.40021602214505814,
    0.38515554398978596,
    0.41651861375447946,
    0.3617183672336558,
    0.4169064714316555,
    0.40851237980912014,
    0.3588850692391647,
    0.3584781108689916,
    0.4415419420254826,
    0.3214787321380448,
    0.39356656101661036,
    0.3716503736319464,
    0.3553181795270921,
    0.3248896162404255
  ],
  "gppm": [
    574.8308037417653,
    568.9678100619593,
    574.0225390546544,
    585.3785365100354,
    580.8637710489925,
    599.7435152029692,
    598.7277512141001,
    588.9789517216765,
    565.9222017451481,
    579.9023914266843,
    570.7512140752511,
    638.6218622250442,
    585.8473455436789,
    576.2601303243084,
    560.0032017278601,
    570.7296691568497,
    569.8021246862766,
    575.26009530024,
    600.0264264305118,
    565.7280471743906,
    587.637950776537,
    610.9815063097561,
    592.3627643071975,
    576.0532933456468,
    612.1114082781312,
    593.5281785317821,
    584.6461824519578,
    592.3786796762073,
    613.7602044759473
  ],
  "gppm_normalized": [
    1.3324945093697234,
    1.2581538109084272,
    1.2655754781549466,
    1.2925416585468128,
    1.2812727730174946,
    1.322665981528378,
    1.3233632527578532,
    1.3004882078700277,
    1.2452868134630044,
    1.2789003012891427,
    1.257561661225587,
    1.4127746918846662,
    1.2963201162365494,
    1.2736100836840054,
    1.2310687966783094,
    1.2625016072336863,
    1.2557429362502648,
    1.266371185946454,
    1.3254789941694478,
    1.2406965632283307,
    1.2919774611320243,
    1.3445160291693583,
    1.306841324294117,
    1.2623756911864301,
    1.3487079926465493,
    1.312765068272261,
    1.2880618100549783,
    1.3122880315568994,
    1.349520197778092
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394
  ],
  "response_lengths": [
    3712,
    2686,
    2484,
    2324,
    2364,
    2617,
    2260,
    2566,
    2511,
    2503,
    2565,
    2280,
    2583,
    2586,
    2554,
    2552,
    2558,
    2372,
    2271,
    2430,
    2185,
    2260,
    2420,
    2521,
    2455,
    2106,
    2358,
    2281,
    2198
  ]
}