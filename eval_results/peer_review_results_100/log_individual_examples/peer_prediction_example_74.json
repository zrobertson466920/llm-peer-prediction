{
  "example_idx": 74,
  "reference": "Published as a conference paper at ICLR 2023\n\nCAUSAL REPRESENTATION LEARNING FOR INSTANTANEOUS AND TEMPORAL EFFECTS IN INTERACTIVE SYSTEMS\n\nPhillip Lippe QUVA Lab University of Amsterdam p.lippe@uva.nl\n\nSara Magliacane MIT-IBM Watson AI Lab University of Amsterdam\n\nSindy Löwe UvA-Bosch Delta Lab University of Amsterdam\n\nYuki M. Asano QUVA Lab University of Amsterdam\n\nTaco Cohen Qualcomm AI Research∗ Amsterdam\n\nEfstratios Gavves QUVA Lab University of Amsterdam\n\nABSTRACT\n\nCausal representation learning is the task of identifying the underlying causal variables and their relations from high-dimensional observations, such as images. Recent work has shown that one can reconstruct the causal variables from temporal sequences of observations under the assumption that there are no instantaneous causal relations between them. In practical applications, however, our measurement or frame rate might be slower than many of the causal effects. This effectively creates “instantaneous” effects and invalidates previous identifiability results. To address this issue, we propose iCITRIS, a causal representation learning method that allows for instantaneous effects in intervened temporal sequences when intervention targets can be observed, e.g., as actions of an agent. iCITRIS identifies the potentially multidimensional causal variables from temporal observations, while simultaneously using a differentiable causal discovery method to learn their causal graph. In experiments on three datasets of interactive systems, iCITRIS accurately identifies the causal variables and their causal graph.\n\n1\n\nINTRODUCTION\n\nRecently, there has been a growing interest in causal representation learning (Schölkopf et al., 2021), which aims at learning representations of causal variables in an underlying system from highdimensional observations like images. Several works have considered identifying causal variables from time series data, assuming that the variables are independent of each other conditioned on the previous time step (Gresele et al., 2021; Khemakhem et al., 2020a; Lachapelle et al., 2022a;b; Lippe et al., 2022b; Yao et al., 2022a;b). This assumes that within each discrete, measured time step, intervening on one causal variable does not affect any other variable instantaneously. However, in real-world systems, this assumption is often violated, as there might be causal effects that act faster than the measurement or frame rate (Faes et al., 2010; Hyvärinen et al., 2008; Moneta et al., 2006; Nuzzi et al., 2021). Consider the example of a light switch and a light bulb. When flipping the switch, there is an almost immediate effect on the light by turning it on or off, changing the appearance of the whole room instantaneously. In this case, an intervention on a variable (e.g., the switch) also affects other variables (e.g., the bulb) in the same time step, violating the assumption that each variable is independent of the others in the same time step, conditioned on the previous time step. In biology, some protein-protein interactions also occur nearly-instantaneously (Acuner Ozbabacan et al., 2011).\n\nTo overcome this limitation, we consider the task of identifying causal variables and their causal graphs from temporal sequences, even in case of instantaneous cause-effect relations. This task contains two main challenges: identifying the causal variables from observations, and learning the causal relations between those variables. We show that, as opposed to temporal sequences without instantaneous\n\n∗Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\neffects, neither of these two tasks can be completed without the other: without knowing the variables, we cannot identify the graph; but without knowing the graph, we cannot identify the causal variables, since they are not conditionally independent. In particular, in contrast to causal relations across time steps, the orientations of instantaneous edges are not determined by the temporal ordering, hence requiring to jointly solve the tasks of causal representation learning and causal discovery.\n\nAs a starting point, we consider the setting of CITRIS (Causal Identifiability from Temporal Intervened Sequences; Lippe et al. (2022b)). In CITRIS, potentially multidimensional causal variables interact over time, and interventions with known targets may have been performed. While in that work all causal relations were assumed to be temporal, i.e., from variables in one time step to variables in the next time step, we generalize this setting to include instantaneous causal effects. In particular, we show that in general, causal variables are not identifiable if we do not have access to partiallyperfect interventions, i.e., interventions that remove the instantaneous parents. If such interventions are available, we prove that we can identify the minimal causal variables (Lippe et al., 2022b), i.e., the parts of the causal variables that are affected by the interventions, and their temporal and instantaneous causal graph. Our results generalize the identifiability results of Lippe et al. (2022b), since if there are no instantaneous causal relations, any intervention is partially-perfect by definition. As a practical implementation, we propose instantaneous CITRIS (iCITRIS). iCITRIS maps high-dimensional observations, e.g., images, to a lower-dimensional latent space on which it learns an instantaneous causal graph by integrating differentiable causal discovery methods into its prior (Lippe et al., 2022a; Zheng et al., 2018). In experiments on three different video datasets, iCITRIS accurately identifies the causal variables as well as their instantaneous and temporal causal graph. Our contributions are:\n\n• We show that causal variables in temporal sequences with instantaneous effects are not\n\nidentifiable without interventions that remove instantaneous parents.\n\n• We prove that when having access to such interventions with known targets, the minimal causal variables can be identified along with their causal graph under mild assumptions. • We propose iCITRIS, a causal representation learning method that identifies minimal causal\n\nvariables and their causal graph even in the case of instantaneous causal effects.\n\nRelated Work We provide an extended discussion on related work in Appendix C. Early works on causal representation learning focused on identifying independent factors of variations (Klindt et al., 2021; Kumar et al., 2018; Locatello et al., 2019; 2020b; Träuble et al., 2021), in settings similar to Independent Component Analysis (ICA) (Comon, 1994; Hyvärinen et al., 2001; 2019). In particular, Lachapelle et al. (2022a;b); Yao et al. (2022a;b) discuss the identifiability of causal variables from temporal sequences. Yet, in all of these ICA-based setups, causal variables are required to be conditionally independent. For causally-dependent variables, Yang et al. (2021) learn causal variables from labeled images in a supervised manner. Ahuja et al. (2022); Brehmer et al. (2022) identify causal variables with unknown causal relations from pairs of observations that only differ in a subset of causal factors influenced by an intervention, i.e., having counterfactual observations. As discussed by Pearl (2009), however, knowing counterfactuals is not realistic in most scenarios. Instead, CITRIS (Lippe et al., 2022b) focuses on temporal sequences, in which also the variables that are not intervened upon can still continue evolving over time. On the other hand, in this setting the intervention targets need to be known. Moreover, within a time step, the causal variables are assumed to be independent conditioned on the variables of the previous time step, hence not allowing for instantaneous effects. To the best of our knowledge, iCITRIS is the first method to identify causal variables and their causal graph from temporal, intervened sequences even for potentially instantaneous causal effects, without requiring counterfactuals or data labeled with the true causal variables.\n\n2 RELEVANT BACKGROUND AND DEFINITIONS\n\nIn this work, we start from the setting of Temporal Intervened Sequences (TRIS) (Lippe et al., 2022b). For clarity, we provide a brief overview of TRIS and discuss previous identifiability results, before extending and generalizing the theory to instantaneous effects.\n\n2.1 TEMPORAL INTERVENED SEQUENCES\n\nTemporal intervened sequences (TRIS) (Lippe et al., 2022b) are a latent temporal causal process with K causal variables (C t\n\nS t=1 (e.g., the light switch and bulb), representing a dynamic\n\n2, ..., C t\n\n1, C t\n\nK)T\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n}\n\n0, 1\n\nK, where I t\n\nBayesian network (DBN) (Dean et al., 1989; Murphy, 2002). Each causal variable Ci is instantiated at each time step t, denoted by C t 1.\n\ni ) are a subset of the variables at time t −\nTo represent interventions, the causal graph is augmented with binary intervention variables I t\n\ni , and its causal parents pa(C t\n\ni = 1 refers to the causal variable C t\n\n∈ i having been intervened at time step t. This {\nsetting can model soft interventions (Eberhardt, 2007), i.e., interventions that change the conditional distribution p(C t i = 0) (e.g., flipping the light switch). This i | trivially includes also perfect interventions, do(Ci = ci) (Pearl, 2009), which cause the target variable to be independent of its parents. To allow for arbitrary sets of interventions, the intervention variables are considered to be confounded by an unobserved variable Rt. While the intervention variables I t are assumed to be observed, the actual values of the intervened variables, e.g., the state of the light switch, are not. The graph and its parameters are assumed to be time-invariant (i.e., repeat across time steps), causally sufficient (i.e., no latent confounders besides the variables mentioned before), and faithful (i.e., no additional independences w.r.t. the ones encoded in the graph).\n\n= p(C t i |\n\ni = 1)\n\npa(C t\n\npa(C t\n\ni ), I t\n\ni ), I t\n\nR3. The causal variable space is defined as\n\nMi i with 1 and Di being the domain. For example, a 3d position can be represented\n\nIn this setting, causal variables can be scalar or span over multiple dimensions, i.e., Ci ∈ the dimensionality Mi ≥ as Ci ∈ Instead of observing the causal variables directly, we measure a high-dimensional observation X t, e.g., an image, representing a noisy, entangled view of all causal variables C t = (C t K) at time step t. The observation function is defined as h(C t is any noise on the observation X t that is independent of C t (e.g., pixel or color shifts), and h : is a function from the causal variable space observation space observation function h is assumed to be bijective, i.e., there exist a unique inverse of h for all X\n\nC × E → X = RL to the RN . To allow unique identification of causal variables from observations, the .\n\n1, C t K, Et) = X t, where Et\n\nand the space of the observation noise\n\n2, ..., C t\n\n2, ..., C t\n\nM1 1 ×\n\nM2 2 ×\n\n1, C t\n\nMK K .\n\n= D\n\nX ⊆\n\n∈ E\n\n...\n\n×\n\nD\n\nD\n\nD\n\nE\n\nC\n\nC\n\n∈ X\n\n2.2 MINIMAL CAUSAL VARIABLES AND IDENTIFIABILITY CLASS\n\ni\n\n(C t\n\nMultidimensional causal variables are not always fully identifiable in TRIS, when interventions only affect a subset of the variables’ dimensions (Lippe et al., 2022b), e.g., only x in a 3d position [x, y, z]. To account for these interventions, each causal variable C t i can be split into an intervention-dependent part svar i ) = i\n(C t (svar i\n\ni ), e.g., [x], and an intervention-independent part sinv\n\ni ), e.g., [y, z], where si(C t\n\n(C t i ), sinv\n\ni )) is an invertible function. Under this split, the distribution of C t pa(C t (C t si(C t i ) i ) i ) |\n|\n\ni becomes:\n\npa(C t |\n\npa(C t\n\ni ), I t\n\ni ), I t\n\nWith this setup, Lippe et al. (2022b) define a minimal causal variable as follows: (cid:1) (cid:0) Definition 2.1. The minimal causal variable of a causal variable C t I t i is the intervention-dependent part svar the split maximizes the information content H(sinv discrete points (LDDP) (Jaynes, 1957; 1968).\n\n(cid:1) i w.r.t. its intervention variable i ) = (svar i ), sinv i )), such that i )) in terms of the limiting density of\n\ni ) of the split si(C t pa(C t (C t i ) |\n\n(C t i )\n\nsvar\n\nsinv\n\n(C t\n\n(C t\n\n(C t\n\n(C t\n\n= p\n\n(1)\n\np\n\np\n\n(cid:0)\n\n(cid:0)\n\n(cid:1)\n\n·\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nxt, xt+1, I t+1 }\n, with\n\n, mapping each dimension of\n\nTo identify the minimal causal variables from data triplets {\nthe observation function h by learning an invertible map gθ : space with M dimensions. For this latent space, CITRIS learns an assignment function ψ : 0..K\n\n→ to a causal variable C1, ..., CK. The index 0 is used for the observation noise or intervention-independent variables. We denote the set of latents assigned to the causal variable Ci with zψi = . On this latent space, CITRIS models a (cid:74) prior pφ,ψ(zt+1 , is conditioned on the previous time step zt and its intervention variable I t+1 (cid:75) , but independent among each other within the same time step. The causal graph can be found by pruning the temporal dependencies in this prior.\n\n1..M zt, I t+1) where each group of latent variables, zt+1 |\n\n, CITRIS approximates RM being a latent\n\n, ψ(j) = i\n\nX → Z\n\n1..M\n\nzj|\n\nZ ∈\n\nZ\n\n∈\n\nψi\n\n{\n\n}\n\nj\n\n(cid:75)\n\n(cid:74)\n\n(cid:74)\n\n(cid:75)\n\ni\n\nS\n\n. CITRIS is shown to identify\n\nif Under this model, Lippe et al. (2022b) consider a causal system recovers the true its minimal causal variables are identified up to an invertible transformation, and if it maximizes the information content of zψ0, under causal graph in , and no intervention the constraint of maximizing the likelihood of data points variable I t i is a deterministic function of any other intervention variable. However, if causal effects occur faster than the observation rate, an intervention influences also other variables in the same time step in addition to its target, leading CITRIS to potentially identify incorrect variables. In this paper, we generalize this identifiability result to systems where instantaneous causal relations may exist.\n\nto be identified by a model\n\nX t, X t+1, I t+1\n\nM\n\nM\n\nS\n\nS\n\n{\n\n}\n\n3\n\n̸ Published as a conference paper at ICLR 2023\n\n3\n\nIDENTIFYING CAUSAL VARIABLES WITH INSTANTANEOUS EFFECTS\n\nIn this section, we generalize Temporal Intervened Sequences (TRIS) to settings where causal relations can be potentially instantaneous. First, we discuss the challenges arising from instantaneous effects, and then present solutions to overcome these challenges. Finally, we derive our identifiability results.\n\n3.1\n\nITRIS AND CHALLENGES OF INSTANTANEOUS EFFECTS\n\nWe extend TRIS to a setting we call instantaneous Temporal Intervened Sequences (iTRIS) which allows for instantaneous causal effects. In iTRIS, causal variables within the same time step can cause each other, as long as the graph remains acyclic. This means that, for example, C t+1 can cause C t+1 for i . Figure 1 summarizes this setting.\n\n= j, as long as there is no directed path C t+1\n\nC t+1\n\nj\n\ni\n\ni → · · · →\n\ni\n\nWhile the addition of instantaneous effects may seem like a small change, it violates the key assumption of most previous works (Khemakhem et al., 2020a; Lachapelle et al., 2022a;b; Lippe et al., 2022b; Yao et al., 2022a;b), namely that causal variables within a time step are independent conditioned on some external variable. As a consequence, we have to differentiate between causal models in a much larger function space than before, making identifiability a considerably harder task.\n\nTo formalize this intuition, consider the following example. Assume we have two latent causal variables C1 and C2, and, for simplicity, no temporal relations. The causal variables C1 and C2 do not cause each other, and we have an arbitrary observation function h(C1, C2) = X and distributions p1(C1), p2(C2). In this example, if our method allows for instantaneous effects, we cannot identify the causal variables or their graph from px(X) alone, since there are multiple representations that model px(X) equally well. For instance, the representation ˆC1 = C1, ˆC2 = C1 + C2 with the causal graph ˆC1 → ˆC2 model the same observation distribution since ˆp2( ˆC2| ˆC1) = ˆp2(C1 + C2| C1) = p2(C2) ˆC1)ˆp( ˆC1) = p(C1)p(C2) = and hence ˆp( ˆC2| px(X). This happens even under soft interventions, because the causal graph can remain unchanged. However, if we have interventions on Ci that remove its instantaneous parents, then the learned representation of Ci must be independent of any instantaneous effects. This elimI2 = 1. We refer\n\ninates ˆC1, ˆC2, since ˆC2 ̸⊥⊥\n\nto these interventions as partially-perfect:\n\nˆC1|\n\nCausal variables\n\nC t\n\nTemporal causal relations\n\nInstantaneous causal relations\n\nC t+1\n\n1\n\nC t+1\n\n2\n\nIntervention variables\n\nI t+1\n\n1\n\nI t+1\n\n2\n\nLatent confounding\n\n· · ·\n\n· · ·\n\nRt+1\n\nh\n\nX t\n\nEt\n\nObservations\n\nC t+1\n\nC t+1\n\nK\n\nX t+1\n\nh\n\nEt+1\n\nI t+1\n\nK\n\ni\n\nFigure 1: An example causal graph in iTRIS. A latent causal variable C t+1 can have as potential parents a subset of the causal variables at the previous time step C t = (C t 1, . . . , C t K), instantaneous parents C t+1 = j, and its intervention variable I t+1 , i .\nAll causal variables C t+1 and observation noise Et+1 cause the observation h(C t+1, Et+1) = X t+1. Rt+1 is a latent confounder allowing for dependencies between intervention variables.\n\nj\n\ni\n\nDefinition 3.1. A partially-perfect intervention on a causal variable Ci is a soft intervention that pa(C t removes all parents in the instantaneous graph: p(C t i )) where i | patemp(C t and pI is the post-interventional distribution. }\n\ni = 1) = pI (C t i |\n\npatemp(C t\n\npa(C t i )\n\ni ), I t\n\ni ) =\n\n1..K\n\n, C t\n\nC t\n\nj |\n\n∈\n\n∈\n\n{\n\n−\n\n−\n\n1\n\n1\n\nj\n\nj\n\nAs an example, consider an intervention that sets C t i + ε, where ε is noise. While this (cid:74) intervention breaks instantaneous relations, the target still depends on the previous time step, making it partially-perfect. Using the partially-perfect interventions, we can prove the following:\n\ni = C t\n\n−\n\n(cid:75)\n\n1\n\nLemma 3.2. In iTRIS, a causal variable Ci cannot be identified up to an invertible transformation Ti that is independent of all other causal variables, if Ci can have instantaneous parents and no partially-perfect interventions on Ci are provided.\n\nWe provide the proof for this lemma and an example with temporal relations in Appendix D.2.1. In the following, we assume that all interventions in iTRIS are partially-perfect. This subsumes the soft intervention setup in TRIS by definition, since the instantaneous graph is empty in this case.\n\n4\n\n̸ ̸\nPublished as a conference paper at ICLR 2023\n\n3.2\n\nIDENTIFYING THE MINIMAL CAUSAL VARIABLES IN ITRIS\n\nUsing partially-perfect interventions, we introduce our identifiability results in iTRIS. For simplicity of the theoretical analysis, we follow Lippe et al. (2022b) and consider a continuous domain for causal variables, i.e., D = R, and that distributions have full support. In Section 5, we empirically extend the results to variables with categorical and circular domains, and distributions with limited support. Further, we assume that the temporal dependencies and interventions break all symmetries in the variables’ distributions such that distributional implies functional independence. For a Gaussian, this entails that the mean cannot be a constant (see Appendix D.2 for details on the assumptions).\n\n→\n\n0..K\n\n1..M\n\nSimilar to CITRIS, we learn an invertible mapping, gθ : , and an assignment function ψ : to align latent to causal variables. Differently, however, we also learn a directed, acyclic graph G on the K + 1 latent variable groups zψ0, ..., zψK to model the instantaneous causal relations. The graph G induces a parent structure denoted by zψpa (cid:74) }\nwhere paG(0) = , i.e., the variables in zψ0 have no instantaneous parents. Meanwhile, the temporal causal graph between C t and C t+1 is implicitly learned by conditioning zt+1 on all latents of the previous time step, zt, and can be pruned after training. This results in the following prior:\n\nX → Z\n\npaG(i)\n\n, ψ(j)\n\n1..M\n\nzj|\n\n=\n\n∈\n\n∈\n\n∅\n\n{\n\nj\n\n(cid:74)\n\n(cid:75)\n\n(cid:74)\n\n(cid:75)\n\n(cid:75)\n\ni\n\npφ,ψ,G\n\nzt+1\n\n|\n\n(cid:0)\n\nzt, I t+1\n\n= pφ\n\n(cid:1)\n\n(cid:16)\n\nzt+1 ψ0 |\n\nzt\n\nK\n\npφ\n\ni=1 (cid:89)\n\n(cid:16)\n\n·\n\n(cid:17)\n\nzt+1 ψi\n\nzt, zt+1 ψpa |\n\ni\n\n, I t+1\n\ni\n\n.\n\n(cid:17)\n\n(2)\n\nWith this setup, we can now formally define our identifiability class that contains the one in CITRIS:\n\n⟨\n\nM 1..K\n\nθ, φ, ψ, G ⟩\n\nidentifies a causal system\n\n= , the following two conditions hold:\n\nDefinition 3.3. A model causal variable Ci, i (1) each minimal causal variable is identified up to an invertible transformation Ti, i.e., for all 1(x)]i is the value of the true causal (ci) = Ti(zψi), where ci = [h− observations x variable and zψi = [gθ(x)]ψi is the value of the estimated minimal causal variable, and (2) the estimated parents pa(zt ψi svar i\nof svar i\n\n) are the same as the true parents of the minimal causal variable j ) is a parent i ).\n\ni ), i.e., the estimated parent set contains zτ (C t\n\n1..K t\n, τ 1, t −\n∈ { (C τ for which sinv l\n\nl ) is a parent of svar\n\ni ), and it contains zτ\n\n∈ : svar i\n(cid:74)\n\nψ0 iff there exist l\n\nC, E, h ⟩\n\niff for each\n\niff svar j\n\n∈ 1..K\n\n∈ X\n\n(C τ\n\n(C t\n\n(C t\n\n, j\n\n=\n\nψj\n\nS\n\n}\n\n(cid:75)\n\n⟨\n\ni\n\n∈\n\n(cid:74)\n\n(cid:75)\n\nFor the light switch example, this means that the latent variable zψ1 must model the switch’s state, (cid:74) zψ2. Compared to ICA-based results, this zψ2 the bulb’s state, and the instantaneous graph is zψ1 → identifiability class explicitly aligns the latent variables with the causal variables and, thus, we do not from observations, we consider a dataset of rely on a permutation equivalence class. To identify and intervention variables I t+1. This dataset triplets could be created interactively or, for example, recorded by an expert. With this, the objective becomes:\n\nwith observations xt, xt+1\n\nxt, xt+1, I t+1\n\nS ∈ X\n\n{\n\n}\n\n(cid:75)\n\npφ,ψ,θ,G\n\nxt+1\n\nxt, I t+1 |\n\n=\n\ndet Jgθ (xt+1)\n\npφ,ψ,G\n\nzt+1\n\nzt, I t+1\n\n|\n\n·\n\n(3)\n\n(cid:0) , is introduced due to the change of variables of x to z. If where the Jacobian of gθ, dim( ) > dim( to the lower dimensionality. Under the assumption that gθ, pφ are universal function approximators and the dataset is unlimited in size, we derive the following identifiability result:\n\n), we consider that gθ contains an arbitrary, fixed map from\n\n(cid:1) det Jgθ (xt+1)\n\nC × E\n\nX\n\nX\n\n(cid:12) (cid:12)\n\n(cid:12) (cid:12)\n\n(cid:0)\n\n(cid:1)\n\n(cid:12) (cid:12)\n\n(cid:12) (cid:12)\n\nTheorem 3.4. In iTRIS, a model (Definition 3.3) if (1) maximizes the information content H(zt+1 ψ0 | (2) minimizes the number of edges in G∗, and (3) no intervention variables I t\n\nM\n\nM\n\ni , I t\n\n∗ =\n\n= C, E, h S\n⟩ X t, I t+1): ∗, under the constraint of maximizing the likelihood pφ,θ,G(X t+1 |\n\nθ∗, φ∗, ψ∗, G∗ ⟨\n\nidentifies a causal system\n\nzt) in terms of the LDDP (Jaynes, 1957; 1968),\n\n⟨\n\n⟩\n\nj are deterministically related, i.e.,\n\nj\n\n∀\n\n= i :\n\n( ¬\n\nf,\n\n∃\n\nt : I t\n\ni = f (I t\n\nj)).\n\n∀\n\nIntuitively, this theorem shows that we can identify the minimal causal variables, even when instantaneous effects are present, under the same constraints as CITRIS. The proof in Appendix D follows three main steps. First, we show that the true observation function constitutes a global optimum of Equation (3), but is not necessarily unique. Second, we derive that any global optimum must identify the minimal causal variables. Finally, we show that optimizing the data likelihood identifies the complete causal graph, i.e., instantaneous and temporal, between the minimal causal variables.\n\n5\n\n̸ Published as a conference paper at ICLR 2023\n\n4 CAUSAL REPRESENTATION LEARNING WITH INSTANTANEOUS EFFECTS\n\nBased on our theoretical results, we propose iCITRIS, a generalization of CITRIS (Lippe et al., 2022b). We first review the original CITRIS architecture, and then describe our extensions in iCITRIS.\n\n4.1 BASELINE: CITRIS\n\nCITRIS is build upon a variational autoencoder (VAE) (Kingma et al., 2014), where the (convolutional) encoder and decoder approximate the invertible map gθ. To promote the identification of the causal variables in latent space, the prior of the VAE follows Equation (2), excluding the instantaneous parents. All latent distributions pφ are usually implemented as conditional Gaussians, with the mean and std predicted by a small MLP per latent variable. Finally, the VAE is trained via maximum likelihood on p(X t+1 X t, I t+1). Alternatively, CITRIS can also be trained on the representations of a pretrained autoencoder, where the map gθ is replaced by a normalizing flow (Rezende et al., 2015). We follow the same setup in iCITRIS, but extend CITRIS’ prior to instantaneous effects.\n\n|\n\n4.2 LEARNING THE INSTANTANEOUS CAUSAL GRAPH\n\nTo learn the instantaneous causal graph simultaneously with the causal representation, we incorporate recent differentiable score-based causal discovery methods in iCITRIS. Given a distribution over graphs p(G), the conditional distribution over the latent variables zt+1 of Equation (2) becomes:\n\npφ,ψ,G\n\nzt+1\n\nzt, I t+1\n\n= pφ\n\n|\n\nzt+1 ψ0 |\n\nzt\n\nEG\n\n∼\n\n·\n\np(G)\n\n(cid:34)\n\nzt, zt+1 ψpa |\n\ni\n\n, I t+1\n\ni\n\n(cid:35)\n\n(4)\n\n(cid:0)\n\ni=1 (cid:89) where the parent sets, zψpa , depend on the graph structure G. The goal is to jointly optimize pφ and p(G) under maximizing the likelihood objective of Equation (3), such that p(G) is peaked at the correct causal graph. To this end, we experiment with two causal discovery methods that allow for continuous optimization: NOTEARS (Zheng et al., 2018), and ENCO (Lippe et al., 2022a).\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n(cid:1)\n\ni\n\nK\n\npφ\n\nzt+1 ψi\n\nNOTEARS (Zheng et al., 2018) casts structure learning as a continuous optimization problem by providing a continuous constraint on the adjacency matrix to enforce acyclicity. Following Ng et al. (2022), we model the adjacency matrix with independent edge likelihoods, and differentially sample from it using the Gumbel-Softmax trick (Jang et al., 2017). We use these samples as graphs in the prior pφ to mask the parents of the individual causal variables, and obtain gradients for the graph through the maximum likelihood objective of the prior. In order to promote acyclicity, we use the constraint as a regularizer, and exponentially increase its weight over training.\n\nzt, I t+1 |\n\nzt+1\n\n(cid:1)\n\n(cid:0)\n\nENCO (Lippe et al., 2022a), on the other hand, uses interventional data and two separate parameter sets: one for the orientation per edge, and one for the existence per edge. By only using interventions to update the orientation parameters, ENCO converges to the true, acyclic graph under single-target interventions in the sample limit. Yet, we found it to also work well under multi-target interventions in iCITRIS. ENCO uses low-variance, unbiased gradients based on REINFORCE (Williams, 1992), potentially providing a more stable optimization than NOTEARS. For efficiency, we merge the two learning stages of ENCO and update both the graph and distribution parameters at each iteration.\n\n4.3 STABILIZING THE OPTIMIZATION PROCESS\n\nSimultaneously identifying the causal variables and their graph leads to a chicken-and-egg situation: without knowing the variables, we cannot identify the graph; but without knowing the graph, we cannot identify the causal variables. This can cause the optimization to be unstable and to converge to local minima with incorrect graphs. To stabilize it, we propose the two following approaches.\n\nGraph learning scheduler During the first training iterations, the assignment of latent to causal variables is almost random, since the gradients for the graph parameters are very noisy and uninformative. Thus, we use a learning rate schedule for the graph learning parameters such that the graph parameters are frozen for the first couple of epochs. During those training iterations, the model learns to fit the latent variables to the intervention variables under an arbitrary graph, leading to an initial, rough assignment of latent to causal variables. Then, we warm up the learning rate to slowly start the graph learning process while continuing to separate the causal variables in latent space.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nMutual information estimator If the provided interventions are fully perfect, i.e., they remove temporal dependencies as well, we can exploit this independence by masking out the temporal parents in the prior distribution under interventions. Furthermore, with perfect interventions, we also enforce the mutual information (MI) (Kullback, 1997) between parents and children under interventions to be zero as an additional regularization. Following work on neural MI estimation (Belghazi et al., 2018; Hjelm et al., 2019; van den Oord et al., 2018), we train a network to distinguish between samples from the joint distribution p(zt I t i = ψi 1)p(zt I t i = 1). While the MI estimator optimizes its accuracy, the latents are optimized to |\ndo the opposite, effectively forcing zt\n\ni = 1) and the product of their marginals, p(zt I t |\n\nψi and its parents to be independent under interventions.\n\n, zt\n\n, zt\n\n, zt\n\nψi |\n\nψpa\n\nψpa\n\n−\n\n−\n\n1\n\n1\n\ni\n\ni\n\n5 EXPERIMENTS\n\nWe evaluate iCITRIS on three video datasets with varying difficulties and compare it to common causal representation methods. We include further dataset details in Appendix E, discuss hyperparameters in Appendix F, and provide the code at https://github.com/phlippe/CITRIS.\n\n5.1 EXPERIMENTAL SETTINGS\n\nBaselines Since iCITRIS is, to the best of our knowledge, the first method to identify causal variables with instantaneous effects in this setting, we compare it to methods for identifying conditionally independent causal variables. Firstly, we use CITRIS (Lippe et al., 2022b) and the Identifiable VAE (iVAE) (Khemakhem et al., 2020a), which both use the previous time step and intervention targets to model conditionally independent variables. Further, to compare to a model with dependencies among latent variables, we evaluate the iVAE with an autoregressive prior, which we denote with iVAE-AR. All methods share the general model setup, e.g., the encoder network architecture, where possible.\n\nEvaluation metrics To evaluate the identification of the causal variables, we follow Lippe et al. (2022b) and report the R2 scores for correlations. In particular, R2 ij is the score between the true causal variable Ci and the latents that have been assigned to the causal variable Cj by the learned model, i.e., zψj . We denote the average correlation of the predicted variable to its true value with R2 diag = 1/K ii (optimal 1), and the maximum correlation besides its true variable with R2 sep = 1/K ij (optimal 0). Furthermore, to investigate the modeling of the temporal and instantaneous relations between the causal variables, we perform causal discovery as a postprocessing step on the latent representations since the baselines do not explicitly learn the graph, and report the Structural Hamming Distance (SHD) between the predicted and true causal graph.\n\ni R2 i maxj\n\n(cid:80) (cid:80)\n\n=i R2\n\n5.2\n\n2D COLORED CELLS WITH CAUSAL EFFECTS: VORONOI BENCHMARK\n\nWe first conduct experiments on synthetically generated causal graphs with various instantaneous structures to investigate the difficulty and challenges of the task. We consider three instantaneous graph structures: random has a randomly sampled, acyclic graph structure with a probability of 0.5 of two variables being connected by a direct edge, and chain and full represent the minimallyand maximally-connected DAGs respectively. For each graph, we sample temporal edges with an edge probability of 0.25 matching the density of the instantaneous causal graph. Based on these graphs, we create the variable’s observational distributions as Gaussians parameterized by randomly initialized neural networks, and provide for simplicity single-target, perfect interventions for all variables. The causal variables are mapped to image space by firstly applying a randomly initialized, two-layer normalizing flow, and afterwards plotting them as colors in a 32x32 pixels image of a fixed Voronoi diagram as an irregular structure. Thus, the representation learning models need to distinguish between the entanglement by the random normalizing flow and the underlying causal graphs to identify the causal variables, while also performing causal discovery to find the correct causal graph.\n\nX\n\nIn Figure 2, we show the results of all models on graphs of 4, 6, and 9 variables. For the random and chain graphs, iCITRIS-ENCO identifies the causal variables and their causal graph with only minor errors, even for the largest graphs of 9 variables. Even on the challenging full graph, iCITRISENCO considerably outperforms the other models. In contrast, iCITRIS-NOTEARS struggles with the edge orientations and converges to edge probabilities noticeably lower than 1.0, with which the variables cannot be perfectly identified anymore, especially for increasing graph sizes. Meanwhile,\n\n7\n\n̸ Published as a conference paper at ICLR 2023\n\n(a) R2 error – 4 variables\n\n(b) R2 error – 6 variables\n\n(c) R2 error – 9 variables\n\n(d) SHD – 4 variables\n\n(e) SHD – 6 variables\n\n(f) SHD – 9 variables\n\nFigure 2: Results on the Voronoi benchmark over three graph structures and sizes with five seeds (error bars in Appendix G.1). For all metrics, lower is better. Top row (a-c): Plotting the R2 correlation error as the average distance between predicted and true causal variables (1-\"R2 diag\", solid bars), plus the maximum correlation to any other variable (\"R2 sep\", striped bars). iCITRIS-ENCO performs well across graph structures and sizes. Bottom row (d-f): The SHD between predicted and ground truth causal graph, divided into instantaneous (solid bars) and temporal (striped bars) edges. iCITRISENCO obtains the lowest error across graphs, with close to zero for the graphs random and chain.\n\nCITRIS and iVAE find K independent dimensions, conditioned on the previous time step and the intervention targets, instead of the true causal variables, which leads to sparse instantaneous, but wrongly dense temporal graphs. Finally, the autoregressive baseline, iVAE-AR, naturally entangles all dimensions in the latent space, on which the true causal graph cannot be recovered anymore. This underlines the non-triviality of identifying instantaneously-related causal variables. In conclusion, iCITRIS identifies the causal variables and graph well across graph structures and sizes, with ENCO outperforming NOTEARS due to more stable optimization, especially for larger, complex graphs.\n\n5.3\n\n3D OBJECT RENDERINGS: INSTANTANEOUS TEMPORAL CAUSAL3DIDENT\n\n×\n\nAs a visually challenging dataset, we use the Temporal Causal3DIdent dataset (Lippe et al., 2022b; 64 pixels) of different object von Kügelgen et al., 2021) which contains 3D renderings (64 shapes under varying positions, rotations, and lights. The dataset has seven causal variables, including categorical and circular variables, going beyond iCITRIS’s theoretical setting. To introduce instantaneous effects, we replace all temporal relations with instantaneous edges, except those on the same variable (C t ). For instance, a change in the rotation leads to an instantaneous change in the position of the object, which again influences the spotlight. Overall, we obtain an instantaneous graph of eight edges between the seven multidimensional causal variables. We provide partially-perfect interventions that remove instantaneous parents, but leave the existing temporal dependencies unchanged. Since the dataset is visually complex, we use the normalizing flow variant of iCITRIS and CITRIS applied on a pretrained autoencoder.\n\nTable 1: Results on the Instantaneous Temporal Causal3DIdent dataset over three seeds (standard deviations in Table 9). iCITRIS-ENCO performs best in identifying the variables and their graph.\n\nC t+1\n\ni →\n\ni\n\nTable 1 shows that iCITRIS-ENCO identifies the causal variables well and recovers most instantaneous relations, with up to two errors on average. The temporal graph had more false positive edges due to minor correlations. iCITRIS-\n\nModel\n\niCITRIS-ENCO iCITRIS-NOTEARS CITRIS iVAE iVAE-AR\n\nR2 (diag ↑ / sep ↓)\n\nSHD (instant ↓ / temp ↓)\n\n0.96 / 0.07 0.95 / 0.10 0.90 / 0.23 0.79 / 0.24 0.74 / 0.29\n\n1.67 / 5.67 4.33 / 6.33 5.67 / 12.67 6.00 / 15.00 10.67 / 12.33\n\n8\n\nError type1-(R2 diag)R2 seprandomchainfullGraph structure0.00.20.40.6R2 error.01.01.03randomchainfullGraph structure0.00.20.40.6R2 error.03.02randomchainfullGraph structure0.00.20.40.6R2 error.04.03Error typeInstantaneous edgesTemporal edgesrandomchainfullGraph structure0.02.55.07.510.0SHD0.00.20.60.4randomchainfullGraph structure0102030SHD1.20.21.8randomchainfullGraph structure0204060SHD1.40.03.80.6Published as a conference paper at ICLR 2023\n\nNOTEARS incorrectly orients several edges during training, underlining the benefit of ENCO as the graph learning method in iCITRIS. The baselines have a significantly higher entanglement of the causal variables and struggle with finding the true causal graph. Further, in Appendix G.2, we apply iCITRIS to the original Temporal Causal3DIdent dataset, which contains only temporal causal relations and no instantaneous effects. In this setting, iCITRIS performs on par with CITRIS, verifying that iCITRIS generalizes CITRIS across datasets. In summary, iCITRIS-ENCO can identify the causal variables along with their instantaneous graph well, even in a visually challenging dataset.\n\n5.4 REAL GAME DYNAMICS: CAUSAL PINBALL\n\nFinally, we consider a simplified version of the game Pinball, which naturally has instantaneous causal effects: if the paddles are activated when the ball is close, the ball is accelerated immediately. Similarly, when the ball hits a bumper, its light turns on and the score increases immediately. This results in instantaneous effects, especially under common frame rates. In this environment, we consider five causal variables: the position of the left paddle, the right paddle, the ball (position and velocity), the state of the bumpers, and the score. Interventions again remove instantaneous, but keep temporal parents. Pinball is closer to a real-world environment than the other two datasets and has two characteristic differences: (1) many aspects of the environment are deterministic, e.g., the ball movement, and (2) the instantaneous effects are sparse, e.g., the paddles do not influence the ball if it is far away of them. Such a setting violates several assumptions like faithfulness, the full support and potential symmetries in the observational and interventional distributions, questioning whether iCITRIS empirically works here.\n\nFigure 3: Example of Causal Pinball.\n\nTable 2: Results on the Causal Pinball dataset over three seeds (see Table 12 for standard deviations).\n\nThe results in Table 2 suggest that iCITRIS still works well on this environment. Besides identifying the causal variables well, iCITRIS-ENCO identifies the instantaneous causal graph with minor errors. In contrast, CITRIS entangles the variables much stronger, while iVAE has difficulties identifying all variables in the environment. This shows that iCITRIS can be applied in challenging environments beyond our theoretical limitations, even with deterministic causal effects, while maintaining strong empirical results.\n\niCITRIS-ENCO iCITRIS-NOTEARS CITRIS iVAE iVAE-AR\n\n0.67 / 3.00 3.33 / 4.67 3.00 / 7.67 4.33 / 4.67 8.00 / 3.67\n\n0.99 / 0.12 0.98 / 0.18 0.90 / 0.39 0.44 / 0.05 0.47 / 0.15\n\nSHD (instant ↓ / temp ↓)\n\nR2 (diag ↑ / sep ↓)\n\nModel\n\n6 CONCLUSION AND DISCUSSION\n\nWe propose iCITRIS, a causal representation learning framework for temporal intervened sequences with potentially instantaneous effects. From such sequences, iCITRIS identifies the minimal causal variables while jointly learning the instantaneous and temporal causal graph. In experiments, iCITRIS accurately recovers the causal variables and their graph in three video datasets.\n\nSince instantaneous effects are common in real-world settings (Hyvärinen et al., 2008; Nuzzi et al., 2021), we believe that iCITRIS contributes an important step towards practical causal representation learning methods. Still, as with most other theoretical results, our identifiability theorem is limited by the assumptions it takes. The two most crucial assumptions in iCITRIS are having a dataset, potentially recorded by an expert, that has (1) non-deterministically related, known intervention targets and (2) partially-perfect interventions, i.e., interventions that can remove instantaneous parents. Without the first assumption, causal variables may become entangled in the latent space, and without the latter, instantaneous causal relations may be predicted where none truly exist. However, as demonstrated in experiments on Causal3DIdent and Causal Pinball, iCITRIS still achieves a strong empirical performance in settings that violate other assumptions. For instance, in these experiments, the distributions had limited support and some variables had circular or categorical domains.\n\nTo extend iCITRIS to even more settings, future work includes investigating a setup where interventions are not directly available, but can be performed by sequences of actions, and targets must be learned in an unsupervised manner. Further, iCITRIS is limited to acyclic graphs, while for instantaneous effects cycles could occur under low frame rates, which is also an interesting future direction.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nAUTHOR CONTRIBUTIONS\n\nP. Lippe conceived the idea, derived the theoretical results, implemented the models and datasets, and wrote the paper. S. Magliacane, S. Löwe, Y. M. Asano, T. Cohen, E. Gavves advised during the project and helped in writing the paper.\n\nACKNOWLEDGMENTS\n\nWe thank Johann Brehmer and Pim de Haan for valuable discussions throughout the project. We also thank SURFsara for the support in using the Lisa Compute Cluster. This work is financially supported by Qualcomm Technologies Inc., the University of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy.\n\nREFERENCES\n\nSaliha Ece Acuner Ozbabacan, Hatice Billur Engin, Attila Gursoy, and Ozlem Keskin. Transient protein–protein interactions. Protein Engineering, Design and Selection, 24(9):635–648, 06 2011.\n\nKartik Ahuja, Jason Hartford, and Yoshua Bengio. Weakly Supervised Representation Learning with Sparse Perturbations. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022 (to appear), 2022.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual Information Neural Estimation. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 531–540. PMLR, 10–15 Jul 2018.\n\nBlender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation,\n\nStichting Blender Foundation, Amsterdam, 2021.\n\nJohann Brehmer, Pim de Haan, Phillip Lippe, and Taco Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022 (to appear), 2022.\n\nPhilippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable Causal Discovery from Interventional Data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nPierre Comon. Independent component analysis, A new concept? Signal Processing, 36(3):287–314,\n\nApril 1994.\n\nThomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley and Sons, Ltd,\n\n2005. ISBN 9780471748823. doi: https://doi.org/10.1002/047174882X.fmatter.\n\nKeenan Crane. Keenan’s 3D Model Repository. https://www.cs.cmu.edu/~kmcrane/\n\nProjects/ModelRepository/, 2021.\n\nBrian Curless and Marc Levoy. A Volumetric Method for Building Complex Models from Range Images. In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96, pp. 303–312, New York, NY, USA, 1996. Association for Computing Machinery. ISBN 0897917464. doi: 10.1145/237170.237269.\n\nPim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nThomas Dean and Keiji Kanazawa. A model for reasoning about persistence and causation. Computational Intelligence, 5(2):142–150, 1989. doi: https://doi.org/10.1111/j.1467-8640.1989.tb00324.x.\n\nFrederick Eberhardt. Causation and intervention. Unpublished doctoral dissertation, Carnegie\n\nMellon University, pp. 93, 2007.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: Generative scene inference and sampling with object-centric latent representations. International Conference on Learning Representations (ICLR), 2020.\n\nLuca Faes and Giandomenico Nollo. Extended causal modeling to assess partial directed coherence in multiple time series with significant instantaneous interactions. Biological cybernetics, 103(5): 387–400, 2010.\n\nWilliam Falcon and The PyTorch Lightning team. PyTorch Lightning, 2019.\n\nLuigi Gresele, Julius von Kügelgen, Vincent Stimper, Bernhard Schölkopf, and Michel Besserve. Independent mechanism analysis, a new concept? In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.\n\nJohn D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3):\n\n90–95, 2007. doi: 10.1109/MCSE.2007.55.\n\nAntti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery.\n\nJournal of Machine Learning Research, 14:3041–3071, 2013.\n\nAapo Hyvärinen and Hiroshi Morioka. Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, pp. 3772–3780, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.\n\nAapo Hyvärinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. John Wiley &\n\nSons, June 2001.\n\nAapo Hyvärinen, Shohei Shimizu, and Patrik O. Hoyer. Causal Modelling Combining Instantaneous and Lagged Effects: An Identifiable Model Based on Non-Gaussianity. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pp. 424–431, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605582054. doi: 10.1145/1390156. 1390210. URL https://doi.org/10.1145/1390156.1390210.\n\nAapo Hyvärinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 859–868. PMLR, 2019.\n\nSergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448–456, Lille, France, 07–09 Jul 2015. PMLR.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with Gumbel-Softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 2426, 2017, Conference Track Proceedings, 2017.\n\nEdward T. Jaynes. Information Theory and Statistical Mechanics. Phys. Rev., 106:620–630, May\n\n1957. doi: 10.1103/PhysRev.106.620.\n\nEdwin T. Jaynes. Prior Probabilities. IEEE Transactions on Systems Science and Cybernetics, 4(3):\n\n227–241, 1968. doi: 10.1109/TSSC.1968.300117.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational Autoencoders and Nonlinear ICA: A Unifying Framework. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 2207–2217. PMLR, 2020a.\n\nIlyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvarinen. ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 12768–12778. Curran Associates, Inc., 2020b.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\n\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016.\n\nThomas Kipf, Gamaleldin F Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional Object-Centric Learning from Video. International Conference on Learning Representations (ICLR), 2022.\n\nDavid Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding. In International Conference on Learning Representations (ICLR), 2021.\n\nVenkat Krishnamurthy and Marc Levoy. Fitting Smooth Surfaces to Dense Polygon Meshes. In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96, pp. 313–324, New York, NY, USA, 1996. Association for Computing Machinery. ISBN 0897917464. doi: 10.1145/237170.237270.\n\nSolomon Kullback. Information theory and statistics. Courier Corporation, 1997.\n\nAbhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational Inference of Disentangled Latent Concepts from Unlabeled Observations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.\n\nSébastien Lachapelle and Simon Lacoste-Julien. Partial Disentanglement via Mechanism Sparsity.\n\narXiv preprint arXiv:2207.07732, 2022a.\n\nSebastien Lachapelle, Pau Rodriguez, Rémi Le, Yash Sharma, Katie E Everett, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear ICA. In First Conference on Causal Learning and Reasoning, 2022b.\n\nPhillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient Neural Causal Discovery without\n\nAcyclicity Constraints. In International Conference on Learning Representations, 2022a.\n\nPhillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M. Asano, Taco Cohen, and Efstratios Gavves. CITRIS: Causal Identifiability from Temporal Intervened Sequences. In Proceedings of the 39th International Conference on Machine Learning, ICML 2022, 2022b.\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4114–4124. PMLR, 09–15 Jun 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFrancesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael Tschannen. Weakly-Supervised Disentanglement Without Compromises. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 6348–6359. PMLR, 2020a.\n\nFrancesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rätsch, Bernhard Schölkopf, and Olivier Bachem. Disentangling Factors of Variations Using Few Labels. In International Conference on Learning Representations, 2020b.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-Centric Learning with Slot Attention. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 11525–11538. Curran Associates, Inc., 2020c.\n\nAlessio Moneta and Peter Spirtes. Graphical models for the identification of causal structures in multivariate time series models. In 9th Joint International Conference on Information Sciences (JCIS-06), pp. 613–616. Atlantis press, 2006.\n\nRicardo Pio Monti, Kun Zhang, and Aapo Hyvärinen. Causal Discovery with General Non-Linear Relationships using Non-Linear ICA. In Amir Globerson and Ricardo Silva (eds.), Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp. 186–195. AUAI Press, 2019.\n\nKevin Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning. UC Berkeley,\n\nComputer Science Division, 2002.\n\nMartin Edward Newell. The Utilization of Procedure Models in Digital Image Synthesis. PhD thesis,\n\nThe University of Utah, 1975. AAI7529894.\n\nIgnavier Ng, Shengyu Zhu, Zhuangyan Fang, Haoyang Li, Zhitang Chen, and Jun Wang. Masked In Proceedings of the 2022 SIAM International\n\nGradient-Based Causal Structure Learning. Conference on Data Mining (SDM), pp. 424–432, 2022.\n\nDidrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, and Max Welling. Survae flows: Surjections to bridge the gap between vaes and flows. Advances in Neural Information Processing Systems, 33:12685–12696, 2020.\n\nD Nuzzi, S Stramaglia, M Javorka, Daniele Marinazzo, A Porta, and Luca Faes. Extending the spectral decomposition of granger causality to include instantaneous influences: application to the control mechanisms of heart rate variability. Philosophical Transactions of the Royal Society A, 379(2212):20200263, 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019.\n\nJudea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd\n\nedition, 2009. ISBN 052189560X.\n\nEmil Praun, Adam Finkelstein, and Hugues Hoppe. Lapped Textures. In Proceedings of ACM\n\nSIGGRAPH 2000, pp. 465–470, July 2000.\n\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint\n\narXiv:1710.05941, 2017.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nPatrik Reizinger, Luigi Gresele, Jack Brady, Julius von Kügelgen, Dominik Zietlow, Bernhard Schölkopf, Georg Martius, Wieland Brendel, and Michel Besserve. Embrace the Gap: VAEs Perform Independent Mechanism Analysis. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022 (to appear), 2022.\n\nDanilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 1530–1538. JMLR.org, 2015.\n\nSzymon Rusinkiewicz, Doug DeCarlo, Adam Finkelstein, and Anothony Santella. Suggestive Contour Gallery. https://gfx.cs.princeton.edu/proj/sugcon/models/, 2021.\n\nBernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612–634, 2021.\n\nMaximilian Seitzer, Bernhard Schölkopf, and Georg Martius. Causal influence detection for improving efficiency in reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nShohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. J. Mach. Learn. Res., 7:2003–2030, dec 2006. ISSN 1532-4435.\n\nPeter Sorrenson, Carsten Rother, and Ullrich Köthe. Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN). In International Conference on Learning Representations, 2020.\n\nFrederik Träuble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Schölkopf, and Stefan Bauer. On Disentangled Representations Learned from Correlated Data. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10401– 10412. PMLR, 18–24 Jul 2021.\n\nGreg Turk and Marc Levoy. Zippered Polygon Meshes from Range Images. In Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’94, pp. 311–318, New York, NY, USA, 1994. Association for Computing Machinery. ISBN 0897916670. doi: 10.1145/192161.192241.\n\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive\n\nPredictive Coding. CoRR, abs/1807.03748, 2018.\n\nJulius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel Besserve, and Francesco Locatello. Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. In Machine Learning, pp. 229–256, 1992.\n\nMengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. CausalVAE: disentangled representation learning via neural structural causal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9593–9602, 2021.\n\nWeiran Yao, Guangyi Chen, and Kun Zhang. Learning Latent Causal Dynamics. arXiv preprint\n\narXiv:2202.04828, 2022a.\n\nWeiran Yao, Yuewen Sun, Alex Ho, Changyin Sun, and Kun Zhang. Learning Temporally Causal Latent Processes from General Temporal Data. In International Conference on Learning Representations, 2022b.\n\nJunzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved\n\nconfounders. Advances in neural information processing systems, 33:12263–12274, 2020.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nXun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Continuous Optimization for Structure Learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 9492–9503, 2018.\n\nRoland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive Learning Inverts the Data Generating Process. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 12979–12990. PMLR, 2021.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nSUPPLEMENTARY MATERIAL CAUSAL REPRESENTATION LEARNING FOR INSTANTANEOUS AND TEMPORAL EFFECTS\n\nTABLE OF CONTENTS\n\nA Broader Impact\n\nB Reproducibility statement\n\nC Expanded Related Work\n\nD Proofs\n\nD.1 Preliminaries\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.2 Assumptions for Identifiability . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.3 Theorem 3.4 - Proof outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.4 Theorem 3.4 - Proof Step 1: The true model is a global optimum of the likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nobjective .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nD.5 Theorem 3.4 - Proof Step 2: Characterizing the disentanglement class\n\n. . . . . . .\n\nD.6 Theorem 3.4 - Proof Step 3: Identifiability of the causal graph . . . . . . . . . . .\n\nD.7 Theorem 3.4 - Proof Step 4: Final identifiability result . . . . . . . . . . . . . . . .\n\nE Datasets\n\nE.1 Voronoi benchmark .\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nE.2 Instantaneous Temporal Causal3DIdent\n\n. . . . . . . . . . . . . . . . . . . . . . .\n\nE.3 Causal Pinball .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nF Experimental details\n\nF.1\n\niCITRIS - Model details\n\nF.2 Hyperparameters\n\n.\n\nF.3 Evaluation metrics\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG Additional Experimental Results and Ablation Studies\n\nG.1 Voronoi Benchmark .\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG.2 Instantaneous Temporal Causal3DIdent\n\n. . . . . . . . . . . . . . . . . . . . . . .\n\nG.3 Causal Pinball .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n16\n\n17\n\n17\n\n17\n\n19\n\n19\n\n20\n\n25\n\n25\n\n28\n\n31\n\n34\n\n35\n\n35\n\n37\n\n38\n\n40\n\n40\n\n42\n\n43\n\n46\n\n46\n\n49\n\n50\n\nPublished as a conference paper at ICLR 2023\n\nA BROADER IMPACT\n\nThe importance of causal reasoning for machine learning applications, especially reinforcement learning and latent dynamics understanding, has been emphasized by several previous works (De Haan et al., 2019; Lachapelle et al., 2022b; Pearl, 2009; Schölkopf et al., 2021; Seitzer et al., 2021; Zhang et al., 2020). Thereby, starting from low-level information like pixels constitutes a considerable challenge, since we aim at reasoning about objects and abstract concepts instead of low-level pixels. We believe that this work contributes an important step towards tackling this challenge since it goes beyond previous work by considering instantaneous effects, a common property in real-world systems (Faes et al., 2010; Hyvärinen et al., 2008; Moneta et al., 2006; Nuzzi et al., 2021). Besides providing theoretical identifiability results, we also propose a practical algorithm with which one can learn the causal variables and their graph from high-level observations. Furthermore, we envision a reinforcement learning setting as a future application, where a robotic system may be able to interact with an environment. However, the main assumption that prevent us from doing this so far, is the availability of interventions with known targets. In many systems, one might not be able to directly perform such interventions, but rather require several steps of low-level actions. For instance, instead of being provided the intervention targets, future work could consider a robotic setup where one can control a robot arm which can perform several interactions (e.g., flipping a switch), and we believe that our work can constitute the starting point for such extension. Moreover, as we have seen in the experiments on the Causal3DIdent dataset and the Causal Pinball environment, not all assumptions must be strictly fulfilled to identify the variables empirically. Moving towards this empirical goal, recent advances in unsupervised object-centric learning (Engelcke et al., 2020; Kipf et al., 2022; Locatello et al., 2020c) have shown that objects, which can often be considered as groups of causal variables like position and velocity, can be identified from high-dimensional data without labels. A possible combination of such object-centric approaches with our causal representation learning method can relax further assumptions by using the objects as a prior disentanglement of information, opening up further possible applications of iCITRIS. Thus, we believe that this work can form the basis of several future works in this direction.\n\nSince the possible applications of causal representation learning and specifically iCITRIS are fairly wide-ranging, there might be potential impacts we cannot forecast at the current time. This includes misuses of the method for unethical purposes. For instance, an incorrect application of the method can be used to justify false causal relations, such as referencing gender and race as causes for other characteristics of a person. Hence, the obligation to use this method in a correct way within ethical boundaries lies on the user, and the outputs of the method should always be critically evaluated. We will emphasize this responsibility of the user in the public license of our code.\n\nB REPRODUCIBILITY STATEMENT\n\nFor reproducibility, the code for all models used in this paper is publicly available at https: //github.com/phlippe/CITRIS. Further, we provide the code for generating the Voronoi benchmark, the Instantaneous Temporal Causal3DIdent dataset, and the Causal Pinball environment. More details on the datasets and visualizations are outlined in Appendix E.\n\nMoreover, for all experiments of Section 5, we have included a detailed overview of the hyperparameters in F.2 and additional implementation details of the evaluation metrics and model architecture components in Appendix F.1. All experiments have been repeated for at least 3 seeds (5 seeds for the Voronoi benchmark) to obtain stable, reproducible results. We provide an overview of the standard deviations, as well as additional results and ablation studies in Appendix G.\n\nFinally, all experiments in this paper were performed on a single NVIDIA TitanRTX GPU with a 6-core CPU. The overall computation time of all experiments together in this paper correspond to approximately 80 GPU days (excluding hyperparameter search and trials during the research).\n\nC EXPANDED RELATED WORK\n\nEarly works on causal representation learning focused on identifying independent factors of variations (Klindt et al., 2021; Kumar et al., 2018; Locatello et al., 2019; 2020b; Träuble et al., 2021). A related line of work, Independent Component Analysis (ICA) (Comon, 1994; Hyvärinen et al., 2001), tries\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nto recover independent latent variables that were transformed by some invertible transformation. ICA was extended to non-linear transformations by exploiting auxiliary variables that make latents mutually conditionally independent (Hyvärinen et al., 2016; Hyvärinen et al., 2019), combined with deep learning methods like VAEs (Khemakhem et al., 2020a;b; Reizinger et al., 2022; Sorrenson et al., 2020; Zimmermann et al., 2021) and applied to causality (Gresele et al., 2021; Monti et al., 2019; Shimizu et al., 2006). In particular, Lachapelle et al. (2022a;b); Yao et al. (2022a;b) discuss the identifiability of causal variables from temporal sequences. As forms of interventions, Lachapelle et al. (2022a;b) consider external actions, while Yao et al. (2022a;b) use non-stationary noise. Yet, in all of these ICA-based setups, causal variables are required to be conditionally independent. Alternatively, Yang et al. (2021) learn causal variables from labeled images in a supervised manner.\n\nGiven a known causal structure, von Kügelgen et al. (2021) demonstrate that common contrastive learning methods can block-identify causal variables that remain unchanged under augmentations. Locatello et al. (2020a) identify independent latent causal factors from pairs of observations that only differ in a subset of causal factors. Brehmer et al. (2022) extend this setup to variables that are causally related with access to single-target interventions. Similarly, Ahuja et al. (2022) extend the setup of Locatello et al. (2020a) to variables with interdependencies, relying on interventions that only affect their target. All these methods require pairs of counterfactual observations, where only a subset of variables is changed by the intervention, while the rest are frozen, i.e., they keep the same values before and after an intervention. As discussed by Pearl (2009), however, knowing counterfactuals is not realistic in most scenarios. Instead, CITRIS (Lippe et al., 2022b) focuses on temporal sequences, in which also the variables that are not intervened upon at a given time step can still continue evolving over time. On the other hand, in this setting the intervention targets need to be known. Moreover, within a time step, the causal variables are assumed to be independent conditioned on the variables of the previous time step, hence not allowing for instantaneous effects. To the best of our knowledge, iCITRIS is the first method to identify causal variables and their causal graph from temporal, intervened sequences even for potentially instantaneous causal effects, without requiring counterfactuals or data labeled with the true causal variables.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nD PROOFS\n\nIn this section, we provide the proof for the identifiability theorem 3.4 in Section 3 and Lemma 3.2. The section is structured into three main parts. First, in Appendix D.1, we give an overview of the notation and elements that are used in the proof. Next, we discuss the assumptions needed for Theorem 3.4, with a focus on why they are needed and what a violation of these assumptions can cause. Additionally, we provide a proof of Lemma 3.2 in this subsection. Finally, we provide the proof of Theorem 3.4, structured into multiple subsections as different main steps of the proof. A detailed overview of the proof is provided in Appendix D.3.\n\nD.1 PRELIMINARIES\n\nThroughout the proof, we will the same notation as used in the main paper, and try to align it as much as possible with Lippe et al. (2022b). As a summary, we review here an adapted version of the notation and preliminaries of the proof for CITRIS:\n\n• We denote the K causal factors in the latent causal dynamical system as C1, . . . , CK; • The dimensions and space of a causal variable is denoted as Ci ∈ • We group all causal factors in a single variable C = (C1, . . . , CK) D\n\nremainder of the proof, we consider Di to be R, i.e., Ci being a continuous variable;\n\ni with Mi ≥\n\ncausal factor space\n\n× • The data we base our identifiability on is generated by a latent Dynamic Bayesian network\n\nM2 2 ×\n\n1. In the\n\n, where\n\nMK K ;\n\nis the\n\n= D\n\n∈ C\n\n...\n\nMi\n\nD\n\nD\n\nC\n\nwith variables (C t\n\nt=1; • We assume to know at each time step the binary intervention variables I t\n\n2, ..., C t\n\ni = 1 refers to an intervention on the causal factor C t I t\n\n• For each causal factor Ci, there exists a minimal causal split svar\n\ni . As a special case I t (Ci), sinv\n\ni\n\n(Ci) represents only the variable/manipulable part of Ci, while sinv\n\ni\n\n}\n\n∈ {\n\nK+1 where 0, 1 0 = 0 for all t; (Ci) such that (Ci) represents the\n\ni\n\nM1 1 × K)T\n\nC 1, C t\n\nsvar i\ninvariable part of Ci;\n\nE\n\nX\n\nRN ;\n\n∈ X ⊆\n\n, where\n\n• At each time step, we can access observations xt, xt+1 • There exist a bijective mapping between observations and causal/noise space, denoted by h : is the space of the noise variable. The bijective map implies that C × E → X ) in RN . For , live in a lower-dimensional manifold of size dim( the observations, example, in Causal Pinball, we have a limited set of images that can occur. Formally, this 1(X)) = means that there exists an inverse to the observation function, h− X for all X , E • The noise Et\n\nat a time step t subsumes all randomness besides the causal model which influences the observations. For example, this could be brightness shifts in Causal3D, or color shifts in the Causal Pinball environment since in these setups, no causal factor is encoded in brightness and color respectively. While this setting is quite general, we still require that the values of the causal factors must be identifiable from single observations. Hence, the joint dimensionality of the observation noise and causal model is limited to the image size.\n\n1(h([C; E])) = [C; E] for all C\n\n1, such that h(h−\n\n∈ X ∈ E\n\n, and h−\n\nC × E\n\n∈ E\n\n∈ C\n\n.\n\n• For any model learning a latent space, we denote the vector of latent variables by zt\n\nRM , where usually overestimate M , i.e., M > dim(\n\nZ\n\nis the latent space of dimension M = dim(\n\n) + dim(\n\n);\n\n) + dim(\n\nC\n\nE\n\n• In iCITRIS, we learn the inverse of the observation function as gθ :\n\nE\n\nC\n\nC × E\n\ndim( dimensionality. This map can, in theory, be trivially found by ensuring invertibility of all X\n\n), we consider that gθ contains an arbitrary, fixed map from\n\n∈ while minimizing the number of dimensions. An alternative interpretation is that gθ is a X\ndeterministic variational autoencoder with zero reconstruction loss. In this limit, Nielsen et al. (2020) showed that the encoder-decoder function as an invertible normalizing flow, which is what we base our analysis on as well. In practice, we train a deterministic autoencoder with ), and work on this larger dimensionality; a latent dimension greater than dim( • In iCITRIS, we learn an assignment from latent dimensions to causal factors, denoted by\n\n) + dim(\n\nE\n\nC\n\nψ :\n\n1..M\n\n0..K\n\n;\n\n→\n\n1..M (cid:74)\n\n(cid:75)\n\n• The latent variables assigned to each causal factor Ci by ψ are denoted as zψi = 1..M\n\n, ψ(j) = i\n\n, ψ(j) = i\n\nj\n\nj\n\nzj|\n\n{\n\n∈\n\n• The remaining latent variables that are not assigned to any causal factor are denoted as zψ0; • In iCITRIS, we learn a directed, acyclic graph G = (V, E) where V =\n\n0..K\n\ni\n\n= (cid:75)\n\ngθ(xt)j|\n\n{\n\n(cid:74)\n\n}\n\n∈\n\n;\n\n}\n\n(cid:74)\n\n(cid:75)\n\n(cid:74)\n\n(cid:75)\n\nzψi|\n\n{\n\n∈\n\n}\n\n19\n\n(cid:74)\n\n(cid:75)\n\n∈ Z ⊆ ). In practice, we\n\n. If dim(\n\n) > to the lower\n\nX\n\nX → Z X\n\nPublished as a conference paper at ICLR 2023\n\nand the edges represent directed causal relations;\n\n• The graph G induces a parent structure which we denote by zψpa\n\n=\n\ni\n\nj\n\nzj|\n\n{\n\n∈\n\n1..M\n\n, ψ(j)\n\n∈\n\npaG(i)\n\nwhere paG(0) =\n\n, i.e., the variables in zψ0 having no instantaneous parents;\n\n• The parents of a causal variable within the same time step t + 1 are denoted by pat+1(C t+1\n\n),\n\n(cid:74)\n\n(cid:75)\n\ni\n\n}\n\n∅\n\nand the parents of the previous time step t by pat(C t+1\n\n);\n\ni\n\nD\n\n×\n\n×\n\n ̃M1\n\n= D\n\n• As a special case, we denote the function gθ with the parameters θ that precisely model the in- ̃\n1, as the disentanglement function δ∗ : verse of the true observation function, h− E\n ̃MK and ̃Mi being the number of latent dimensions assigned to with ̃ ... C\nthe causal factor Ci by ψ∗. We denote the output of δ∗ for an observation X as δ∗(X) = ( ̃C1, ̃C2, ..., ̃E). The representation of δ∗ as a learnable function is denoted by g∗θ and ψ∗; • In the following proof, we will use entropy as a measure of information content in a random variable. To be invariant to possible invertible transformations, e.g., scaling by 2, we use the notion of the limiting density of discrete points (LDDP) (Jaynes, 1957; 1968). In contrast to differential entropy, LDDP introduces an invariant measure m(X), which can be seen as a reference distribution we measure the entropy of p(X) to. The entropy is thereby defined as:\n\n ̃ C ×\n\nX →\n\nH(X) =\n\n−\n\n(cid:90)\n\np(X) log\n\np(X) m(X)\n\ndx\n\n(5)\n\nIn the following proof, we will consider entropy measures over latent and causal variables. For the latent variables, we consider m(X) to be the push-forward distribution of an arbitrary, = Rn) through gθ. For the causal but fixed distribution in 1. For more details on LDDP, variables, we consider it to be the push-forward through h− see Lippe et al. (2022b, Appendix A.1.2) and Jaynes (1957; 1968).\n\n(e.g., random Gaussian if\n\nX\n\nX\n\nD.2 ASSUMPTIONS FOR IDENTIFIABILITY\n\nIn this section, we provide a detailed discussion of the assumptions of iCITRIS to enable the identification of an underlying causal graph with instantaneous effects. We thereby focus on why these assumptions are necessary, and how a violation of those can lead to scenarios where the causal variables and graph is not identifiable.\n\nD.2.1 ASSUMPTION 1: THE INTERVENTIONS ON THE CAUSAL VARIABLES REMOVE\n\nINSTANTANEOUS PARENTS\n\niCITRIS requires interventions on the causal variables that remove instantaneous parents, in order to separate the variables in latent space, as stated in Lemma 3.2 and copied here for completeness:\n\nLemma D.1. In iTRIS, a causal variable Ci cannot be identified up to an invertible transformation Ti that is independent of all other causal variables, if Ci can have instantaneous parents and no partially-perfect interventions on Ci are provided.\n\n⊆\n\nC t\n\nC t+1\n\nProof. To prove this Lemma, consider a causal variable Ci that has Cj as an instantaneous parent. The conditional distribution of Ci, as defined in iTRIS, can be written as pi(C t+1 ), where S , i.e., any additional parent set without introducing cycles. We do not put any constraints on the distribution p and also on the provided interventions, except that we do not have the knowledge whether under I t+1 = 1, C t+1 or not. This implies that one must consider the most general form of interventions for Ci, i.e., modeling the distribution pi(C t+1 i = 1) under interventions with possible unknown independences. To keep this result general, we consider an arbitrary observation function h(C t, Et) = X t.\n\nbecomes independent of C t+1\n\nC t+1 |\n\n, S, I t+1\n\n, S, I t+1\n\n, C t+1\n\nC t+1\n\nC t+1\n\n\\ {\n\n∪\n\n}\n\n|\n\nj\n\nj\n\nj\n\nj\n\ni\n\ni\n\ni\n\ni\n\ni\n\nUnder this setting, it is sufficient to show that there exist another representation ˆC that cannot be distinguished from C solely based on observation triples , and that there exist no invertible function f such that f ( ˆC t i for any t. Note that we exclude a permutation of variables, since the intervention targets I t+1 align the two representations.\n\nX t, X t+1, I t+1\n\ni ) = C t\n\n{\n\n}\n\nAs an alternative representation, consider ˆC = with K being the number of causal variables. Then, the distribution of ˆp( ˆC) only differs in the conditional of pi as\n\n1, Ci + Cj, Ci+1, ..., CK}\n\nC1, ..., Ci\n\n{\n\n−\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nfollows:\n\nˆpi( ˆC t+1\n\ni\n\nˆC t+1 |\n\nj\n\n, ˆS, I t+1\n\ni\n\n) = ˆpi( ˆC t+1 = ˆpi(C t+1\n\nC t+1 |\ni + C t+1\n\nj\n\nj\n\ni\n\n, S, I t+1\n\n)\n\ni\n\nC t+1\n\nj\n\n, S, I t+1\n\ni\n\n)\n\n|\n\n(6)\n\n(7)\n\nBecause ˆpi is conditioned on C t+1 from C t+1\n\ni + C t+1\n\nto C t+1\n\nj\n\ni\n\nj C t+1 , i.e.,, w(c |\n\nj\n\n) = c\n\nC t+1\n\nj\n\n−\n\n. Hence, it follows that:\n\n, there exist an invertible, volume-preserving transformation w\n\nˆpi(C t+1\n\ni + C t+1\n\nj\n\nC t+1\n\nj\n\n, S, I t+1\n\ni\n\n) = pi(C t+1\n\ni\n\nC t+1 |\n\nj\n\n, S, I t+1\n\ni\n\n)\n\n|\n\n(8)\n\nand overall that ˆp( ˆC) = p(C). Furthermore, there exist a function ˆh that maps ˆC to the same observations as h does for C:\n\nˆh( ˆC t, Et) = h( {\n\nˆC t\n\n1, ..., ˆC t\n\ni\n\n1, ˆC t\n\n−\n\nˆC t\n\nj, ˆC t\n\ni+1, ..., ˆC t\n\nK}\n\n, Et) = h(C t, Et)\n\n(9)\n\ni −\n\nTherefore, both representations, C and ˆC, can model the same data generation process for , and are indistinguishable from these observations alone. Finally, it is apparent j. Thus, the causal\n\nX t, X t+1, I t+1 {\nthat there exist no invertible transformation from ˆC t variable Ci is not identifiable up to invertible, componentwise transformations.\n\ni that is independent of C t\n\ni to C t\n\n}\n\nAs an example of how this effects a standard identification problem, consider two random, causal C t+1 variables C1, C2 with the causal graph C t . The two causal variables C1, C2 1\nhave therefore no instantaneous relations. Further, consider the (soft-interventional) distributions p1(C t+1 ) and p2(C t+1 ) whose form can be arbitrary, but for this example, we choose them to be Gaussian with constant variance:\n\n2 , I t+1\n\nt , I t+1\n\nC t+1\n\n2 →\n\n1 →\n\n, C t\n\nC 1\n\nC 1\n\n1\n\n2\n\n1\n\n2\n\n2\n\n|\n\n|\n\np1(C t+1\n\n1\n\np2(C t+1\n\n2\n\n1, I t+1\n\n1\n\nC t |\n\n2, I t+1\n\n2\n\nC t |\n\n) =\n\n) =\n\nN N\n\nN N\n\n(cid:26)\n\n(cid:26)\n\n1\n\n(C t+1 (C t+1\n\n1\n\n2\n\n(C t+1 (C t+1\n\n2\n\n| |\n\n| |\n\nμ1(C t ̃μ1(C t\n\nμ2(C t ̃μ2(C t\n\n1), σ1(C t 2), ̃σ1(C t 2), σ2(C t 2), ̃σ2(C t\n\n1)2) 1)2) 2)2) 2)2)\n\nif I t+1 if I t+1 if I t+1 if I t+1\n\n1 = 0 1 = 1\n\n2 = 0 2 = 1\n\n(10)\n\n(11)\n\nwhere μ1, ̃μ1, μ2, ̃μ2, σ1, ̃σ1, σ2, ̃σ2 are arbitrary, potentially non-linear functions of C t 1 and C t respectively. Further, to consider the simplest case, suppose that the observation X t at a time step t are the causal variables themselves, X t = [C t 2], and we observe data points of all intervention settings, i.e., I t+1\n\n1, C t Bernoulli(q) with 0 < q < 1.\n\n2\n\ni ∼\n\nUnder this setup, the true generative model follows the distribution:\n\np(X t+1\n\n1\n\n, C t+1 X t, I t+1) = p(C t+1 1, C t C t |\n| = p(C t+1 2, I t+1 1, C t C t |\n1, I t+1 = p1(C t+1 C t )\n\n2, I t+1 , I t+1 )\n· p2(C t+1\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n) p(C t+1 C t\n\n| 2, I t+1\n\n2\n\n2\n\n, I t+1\n\n|\n\n·\n\n|\n\n2, I t+1\n\n1\n\nC t\n\n1, C t )\n\n, I t+1\n\n2\n\n)\n\n(12)\n\n(13)\n\n(14)\n\nC t+1\n\nwhere C t+1 X t, I t+1. To show that the causal variables are not uniquely identifiable, we |\nneed at least one other representation which can achieve the same likelihood as the true generative model under all intervention settings I t+1. For this, consider the following distribution:\n\n1 ⊥⊥\n\n2\n\np(X t+1\n\n|\n\n2\n\n1\n\nX t, I t+1) = p(C t+1 = p(C t+1 |\n= p1(C t+1 = p1( ˆC t+1\n\n, C t+1 1, C t C t |\n2, I t+1 1, C t C t 1, I t+1 C t )\n| 1, I t+1 C t\n\n, I t+1\n\n2, I t+1 )\n, I t+1 p(C t+1 )\n· |\nˆp2(C t+1 1 + C t+1 ˆp2( ˆC t+1 C t\n\n1, C t C t 2, C t+1 C t |\n2, ˆC t+1 , I t+1\n\n)\n\n)\n\n1\n\n1\n\n1\n\n2\n\n1\n\n1\n\n2\n\n1\n\n2\n\n2\n\n·\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2, C t+1\n\n1\n\n, I t+1\n\n1\n\n, I t+1\n\n2\n\n)\n\n, I t+1\n\n2\n\n)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\nwith ˆC t+1 on ˆC t+1 ,\nwhich is possible in the space of possible causal models with an additional instantaneous causal edge\n\n1 = C t+1\n\n1 + C t+1\n\n, ˆC t+1\n\n1\n\n2\n\n1\n\n2\n\n|\n\n· . Note the additional dependency of ˆC t+1\n\n| 2 = C t+1\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(a) Observational p(X t|X t−1 = C, I t\n\ndistribution 2 = 0)\n\n1 = 0, I t\n\n(b) Interventional p(X t|X t−1 = C, I t\n\ndistribution 2 = 0)\n\n1 = 1, I t\n\n(c) Interventional p(X t|X t−1 = C, I t\n\ndistribution 2 = 1)\n\n1 = 0, I t\n\nFigure 4: Example distribution for showcasing the necessity of partially-perfect interventions for disentangling causal variables with instantaneous effects. Suppose we are given two-dimensional observations X t, for which the observational and interventional distributions are plotted in (a)-(c). The central plot of each subfigure shows a 2D histogram, and the subplots above and on the right show the 1D marginal histograms. For simplicity, we keep the previous time step, X t 1, constant here. From the interventional distribution, one might suggest that we have the latent causal graph 1 = 1, the distribution of both observational distributions change, while C1 → I t 2 = 1 keeps X2 unchanged. However, the data has been actually generated from two independent causal variables, which have been entangled by having X t = [C t 2]. We cannot distinguish between these two latent models from interventions that do not reliably break instantaneous causal effects, showing the need for partially-perfect interventions.\n\nC2 since under I t\n\n1 + C t\n\n1, C t\n\n−\n\nˆC t+1\n\n1 →\n\nˆC t+1\n\n2\n\n. The new distribution ˆp2 is identical to the true distribution, since:\n\nˆp2(C t+1\n\n1 + C t+1\n\n2\n\n2, C t+1\n\n1\n\nC t |\n\n, I t+1\n\n2 = 0) =\n\nN\n\n(C t+1\n\n1 + C t+1\n\n2\n\n|\n\n=\n\n1 √2πσ2(C t 2)\n\nexp\n\n(cid:32)−\n\n1 2 (cid:0)\n\nC t+1\n\n1 + μ2(C t C t+1\n\n2), σ2(C t\n\n1 + C t+1\n\n2 −\n\n(19)\n\n2)2) (C t+1 1 + μ2(C t 2)2\n\nσ2(C t\n\n2))\n\n2\n\n(cid:1)\n\n(cid:33)\n\nC t+1\n\n2 −\n\nμ2(C t 2) 2)2\n\nσ2(C t\n\n(cid:1)\n\n=\n\nexp\n\n1 √2πσ2(C t 2) (C t+1 μ2(C t =\n| = p2(C t+1 C t\n\n1 2 (cid:0) (cid:32)− 2), σ2(C t 2 = 0)\n\n2, I t+1\n\nN\n\n2\n\n2\n\n2)2)\n\n|\n\n2\n\n(cid:33)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n2, I t+1 Similarly, one can show that ˆp2(C t+1 2 = 1). Hence, the alternative representation ˆC t+1 X t, I t+1) as |\nwell as the true causal model. In conclusion, from the samples alone, we cannot distinguish between the two representation C1, C2 and ˆC1, ˆC2, and the model is therefore not identifiable up to invertible transformations.\n\n2, C t+1 C t |\ncan model the distribution p(X t+1\n\n2 = 1) = p2(C t+1\n\n1 + C t+1\n\n| , ˆC t+1\n\n, I t+1\n\nC t\n\n2\n\n2\n\n2\n\n1\n\n1\n\nAn alternative example with a non-trivial observation function is visualized in Figure 4, which further underlines the problem.\n\nThis shows that with soft interventions, one cannot distinguish between causal relations introduced by the observation function and those that are in the true causal model. (Partially-)Perfect interventions, however, provide an opportunity to do so since if we had known that the intervention on C2 renders it independent of C1, the second causal model could not have modeled the correct distribution under I2 = 1. Thus, we can distinguish between the two, allowing us to identify the correct causal model.\n\nNote that under partially-perfect intervention, the intervention-independent part of a causal variable, sinv(C t i ), automatically cannot have any instantaneous parents, since otherwise, the intervention does not remove all instantaneous parents and hence is actually not partially-perfect.\n\n22\n\n42024X_142024X_242024X_142024X_242024X_142024X_2Published as a conference paper at ICLR 2023\n\nD.2.2 ASSUMPTION 2: THE INTERVENTION VARIABLES ARE NOT A DETERMINISTIC\n\nFUNCTION OF EACH OTHER\n\niCITRIS builds upon interventions to identify the causal variables. The intervention targets are not necessarily independent of each other, but can be confounded. For instance, we could have a setting where we only obtain single-target interventions, or a certain variable Ci can only be jointly intervened upon with another variable Cj. In this large space of possible experimental settings, we naturally cannot guarantee identifiability all the time. In particular, we require that intervention targets for the different causal variables are unique: Lemma D.2. All information that is strictly dependent on the intervention target I t i , i.e. svar(Ci) - the minimal causal variable of Ci, cannot be disentangled from another causal variable, Cj with j\n\n= i, if their intervention targets are identical:\n\nt, I t\n\ni = I t j.\n\n∀\n\nProof. Lippe et al. (2022b) have shown that two causal variables Ci, Cj cannot be disentangled from observational data alone if they follow a Gaussian distribution with equal variance over time. Taking this setup, consider that additionally to observational data, we observe samples where both variables have been intervened upon, I t+1 j = 1. If the interventional distribution of Ci and Cj are both Gaussian with the same variance, we have the same non-identifiability as in the observational case. Since the entanglement axes can transfer between the two setups, Ci and Cj cannot be disentangled, and therefore their minimal causal variables.\n\ni = I t+1\n\nIn other words, if two variables are always jointly intervened or passively observed, we cannot distinguish whether information belongs to causal variable Ci or Cj. Since the causal system is stationary, having one time step t for which I t = I t j implies that in the sample limit, we will observe i ̸ samples with I t j in the limit as well. Further, when we only observe joint interventions on i ̸ two variables, Ci, Cj, the causal graph among the two variable cannot be identified for arbitrary distributions (Eberhardt, 2007), making the identifiability of the graph and variables impossible.\n\n= I t\n\nFollowing Lippe et al. (2022b), we require that the following independence holds for every causal variable Ci with observed interventions:\n\nC t+1\n\ni\n\nI t+1\n\ni\n\n|\n\n̸⊥⊥\n\nC t, pat+1(C t+1\n\ni\n\n), I t+1\n\nj\n\nfor any i\n\n= j\n\n(24)\n\nI t This also implies that there does not exist a variable Cj for which j. As mentioned before, under additional assumptions such that every causal variable has at least one parents, it can be relaxed to unique interventions.\n\ni = 1\n\nt, I t\n\n−\n\n∀\n\nD.2.3 ASSUMPTION 3: DISTRIBUTIONS HAVE FULL SUPPORT\n\nFollowing several previous works (Brehmer et al., 2022; von Kügelgen et al., 2021), we consider for the theoretical results that all distributions have full support. If the observational and interventional distribution do not share the same support, there exist data points for which the intervention targets can be determined from the observation X t alone. In such situation, the encoder can change its encoding depending on the intervention target, as long as the decoder can yet recover the full observation. This can potentially create representation models that ignore the latent structure, since the intervention targets are already known. Furthermore, when intervention targets are known from seeing causal variables, we potentially introduce new independencies from intervention targets. For instance, if we C3 where I3 = 1 only if I1 = 1, I2 = 0, we can induce the intervention have the graph C1, C2 → targets from other causal factors, making C3 essential independent of I3. To prevent such degenerate solutions, we take the assumption that the observational and intervention distributions share the same support. This assumption implies that any data point could come from either the interventional or observational regime, ensuring that the intervention target cannot deterministically be found from an observation X t.\n\nD.2.4 ASSUMPTION 4: TEMPORAL CONNECTIONS AND INTERVENTIONS BREAK ALL\n\nSYMMETRIES IN THE DISTRIBUTIONS\n\nThe temporal and interventional dependencies are an essential part in iCITRIS to guarantee identifiability and disentanglement of the causal variables. Without any of these dependencies, there may 1, I t), while following the exist multiple representations that model the same distribution p(X t\n\nX t\n\n−\n\n|\n\n23\n\n̸ ̸\nPublished as a conference paper at ICLR 2023\n\nC1\n\nC2\n\nC3\n\nFigure 5: Example instantaneous causal graph between 3 causal variables C1, C2, C3. Without temporal dependencies, we could encode information of C1 dependent on C3 without needing an edge in the distribution.\n\nenforced latent structure by iCITRIS. The problem is that variables can functional dependent on each other, where these dependencies exploit symmetries, leaving the distribution unchanged.\n\nμ1, ̃σ2\n\n(C t 1|\n\nC2, For instance, consider the instantaneous causal graph of three variables C1, C2, C3 with C1, C3 → as depicted in Figure 5. Suppose that C1 does not have any temporal parents, and the observational distribution of it follows a Gaussian: p(C t I t 1 = 0) = 1 being constants. 1| I t Further, suppose that under interventions, only the standard deviation changes, i.e. p(C t 1 = 1) = 1| c1, I t 1 = 0)\n\n1 = c1, there exists a second point, c′1 = 2μ1 − 1. This is because both distributions, p(C t 1|\n\n= σ2 N\nwhich has the same probability for any value of I t and p(C t 1|\n\nI t 1 = 1), share a symmetry around the mean μ1.\n\n1. Then, for any point C t\n\n1) with ̃σ2 1 ̸\n\n1) with μ1, σ2\n\nNow, suppose we have the optimal encoder which maps an observation X t of this system to the three causal variables with their ground truth values. Then, there exist an alternative encoder, which flips the 1 around the mean μ1, deterministically conditioned on the remaining variables C t observed value of C t 2\nand C t 3 for the causal variables:\n\n3. For instance, we could have the following representation ˆC t\n\n(C t 1|\n\nμ1, σ2\n\n2, ˆC t\n\nN\n\nˆC t\n\n2 = C t\n\n2, ˆC t\n\n3 = C t\n\n3, ˆC t\n\n1 =\n\nC t 1\n2μ1 −\n\n(cid:26)\n\nC t 1\n\n1, ˆC t if ˆC t 3 > 0 otherwise\n\n(25)\n\n−\n\nX t |\n\n1, I t), since flipping the value of C t\n\nThis alternative representation model shares the same likelihood as the optimal encoder in terms of p(X t 1 around the mean does not change its probability. Further, despite the flipping, the original observation X t can be recovered from this alternative representation ˆC t by the decoder, because the possible conditioning factors, i.e. ˆC t 3 in this case, are observable to the decoder. Hence, both representations are equally valid for the causal models. Yet, 1, from its alternative representation ˆC t one cannot recover the value of the true causal variable, C t 1\nalone, since ˆC t 3 needs to be known to invert the example condition. This shows that we can have functional dependencies between representations of causal variables while their distributions remain independent. Thus, there exist more than one representation that cannot be distinguished between from having samples of p(X t\n\n1, I t) alone.\n\n−\n\nX t |\n\nMore generally speaking, functional dependencies between variables can be introduced if there exists a transformation that leaves the probability of a variable Ci unchanged for any possible value of its parents unseen in X t, i.e. 1. Whether this transformation is performed or not can now be conditioned on other variables at time step t. Meanwhile, this transformation does not introduce additional dependencies in the causal graph, since the distribution does not change.\n\ni and temporal parents C t\n\nits intervention target I t\n\n−\n\nTo prevent such transformations from being possible, the temporal parents and intervention targets need to break all symmetries in the distributions. We can specify it in the following assumption:\n\n4:\n\nAssumption pat+1(C t+1 p(C t+1 |\nT (C t+1 C t+1 |\n\n−\n\ni\n\ni\n\ni\n\ni ) = ̃C t+1 C t, C t+1, I t+1 :p(C t+1\n\ni\n\ni\n\n∀\n\ni\n\n|\n\ni\n\na For ), I t+1 ), pat(C t+1\n\ncausal\n\nvariable Ci\n\ncausal mechanism ), there exist no invertible, smooth transformation T with\n\nand\n\nits\n\nbesides the identity, for which the following holds:\n\npat+1(C t+1\n\ni\n\n), pat(C t+1\n\ni\n\n), I t+1\n\ni\n\n) =\n\n∂T (C t+1\n\nC t+1 i ) |\n\n−\n\n∂C t+1\n\ni\n\np( ̃C t+1\n\ni\n\n|\n\n·\n\npat+1(C t+1\n\ni\n\n), pat(C t+1\n\ni\n\n), I t+1\n\ni\n\n)\n\n(26)\n\nIntuitively, this means that there does not exist any symmetry that is shared across all possible values of the parents (temporal and interventions) of a causal variable. While this might first sound restricting, this assumption will likely hold in most practical scenarios. For instance, if the distribution is a Gaussian, then the assumption holds as long as the mean is not constant since the intervention breaks any parent dependencies are broken by the perfect interventions. The same holds in higher\n\ni\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\ndimensions, as the new symmetries, i.e. rotations, are yet broken if the center point is not constant. Note that these symmetries can be smooth transformations, in contrast to the discontinuous flipping operation on the Gaussian (i.e., either we flip the distribution or not, but there is no step in between).\n\nD.2.5 ASSUMPTION 5: CAUSAL GRAPH STRUCTURE REQUIREMENTS\n\nBesides disentangling and identifying the true causal variables, we are also interested in finding the instantaneous causal graph. This requires us to perform causal discovery, for which we need to take additional assumptions. First, we assume that the causal graph is acyclic, i.e., for any causal variable C t i , there does not exist a path through the directed causal graph that loops back to it. Note that this excludes different instances over time, meaning that a path from C t is not considered a loop. In real-world setups, there potentially exist instantaneous graphs which are not acyclic, which essentially model a feedback loop over multiple variables. However, to rely on the graph as a distribution factorization, we assume it to be acyclic, and leave extension to cyclic causal graphs for future work. As the second causal graph assumption, we require that the causal graph is faithful, which means that all independences between causal variables are implications of the graph structure, not the specific parameterization of the distributions (Hyttinen et al., 2013; Pearl, 2009). Without faithfulness, the graph might not be fully recoverable. Finally, we assume causal sufficiency, i.e., there do not exist any additional latent confounders that introduce dependencies between variables beyond the ones we model. Note that this excludes the potential latent confounder between the intervention targets, and we rather focus on confounders on the causal variables C1, ..., CK besides their intervention targets, the previous time step C t, and instantaneous parents C t+1.\n\ni to C t+τ\n\ni\n\nD.3 THEOREM 3.4 - PROOF OUTLINE\n\nThe goal of this section is to proof Theorem 3.4: the global optimum of iCITRIS will identify the minimal causal variables and their instantaneous causal graph. The proof follows a similar structure as Lippe et al. (2022b) used for proofing the identifiability in CITRIS, but requires additional steps to integrate the possible instantaneous relations. In summary, we will take the following steps:\n\n1. (Appendix D.4) Firstly, we show that the function δ∗ that finds the true latent variables C1, ..., CK and assigns them to the corresponding sets zψ1, ..., zψK constitutes a global, but not necessarily unique, optimum for maximizing the likelihood p(X t+1\n\nX t, I t+1). |\n\n2. (Appendix D.5) Next, we characterize the class of disentanglement functions ∆∗ which all represent a global maximum of the likelihood, i.e., get the same score as the true function δ∗. We do this by proving that all functions in ∆∗ must identify the minimal causal variables.\n\n3. (Appendix D.6) In a third step, we show that based on the identification of the minimal causal variables, the causal graph on these learned representations must contain at least the same edges as in the ground truth graph.\n\n4. (Appendix D.7) Finally, we put all parts together and derive Theorem 3.4.\n\nWe will make use of Figure 6 summarizing the temporal causal graph, and the notation introduced in Appendix D.1. For the remainder of the proof, we assume for simplicity of exposition that:\n\n• The invertible map gθ and the prior pφ\n\nzt, I t+1 are sufficiently complex to approxi- |\nmate any possible function and distribution one might consider in iTRIS. In practice, overparameterized neural networks can approximate most functions with sufficient accuracy.\n\nzt+1\n\n(cid:1)\n\n(cid:0)\n\n• The sample size for the provided experimental settings is unlimited. This ensures that dependencies and conditional independencies in the causal graph of Figure 6 transfer to the observed dataset, and no additional relations are introduced by sample biases. In practice, a large sample size is likely to give an accurate enough description of the true distributions.\n\nD.4 THEOREM 3.4 - PROOF STEP 1: THE TRUE MODEL IS A GLOBAL OPTIMUM OF THE\n\nLIKELIHOOD OBJECTIVE\n\nWe start the identifiability discussion by proving the following Lemma:\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nCausal variables\n\nC t\n\nTemporal causal relations\n\nInstantaneous causal relations\n\nC t+1\n\n1\n\nC t+1\n\n2\n\nIntervention variables\n\nI t+1\n\n1\n\nI t+1\n\n2\n\nLatent confounding\n\n· · ·\n\n· · ·\n\nRt+1\n\nh\n\nX t\n\nEt\n\nObservations\n\nC t+1\n\nC t+1\n\nK\n\nX t+1\n\nh\n\nEt+1\n\nI t+1\n\nK\n\nFigure 6: An example causal graph in iTRIS. A latent causal factor C t+1 parents the causal factors at the previous time step C t = (C t C t+1 the observation X t+1. Rt+1 is a potential latent confounder between the intervention targets.\n\ncan have as potential K), instantaneous parents . All causal variables C t+1 and the noise Et+1 cause\n\n= j, and its intervention variables I t+1\n\n1, . . . , C t\n\n, i\n\nj\n\ni\n\ni\n\n1\n\n, ..., C t+1\n\nLemma D.3. The true identification function δ∗ that correctly identifies the true causal factors C t+1 K from observations X t, X t+1 using the true ψ∗ assignment function on the latent variables Z t+1 and the true causal graph G∗ is one of the global maxima of the likelihood of p(X t+1\n\nX t, I t+1).\n\n|\n\nThis lemma ensures that the true model is part of the solution space of maximum likelihood objective on p(X t+1\n\nX t, I t+1).\n\n|\n\nProof. In order to prove this, we first rewrite the objective in terms of the true causal factors. This can be done by using the causal graph in Figure 6, which represents the true generative model:\n\nK\n\np(X t, X t+1, C t, C t+1, I t+1) = p(X t+1\n\n· (cid:34) p(C t) The context variable Rt+1 is subsumed in p(I t+1), since it is a confounder between the intervention targets and is independent of all other factors given I t+1.\n\np(I t+1)\n\nC t) |\n\np(X t\n\ni=1 (cid:89)\n\n(27)\n\n·\n\n·\n\np(C t+1\n\ni\n\nC t, pat+1 |\n\nG (C t+1\n\ni\n\n), I t+1\n\ni\n\n) (cid:35) ·\n\nC t+1) |\n\nIn order to obtain p(X t+1 C t and C t+1, and condition the distribution on X t and I t+1:\n\n|\n\nX t, I t+1) from p(X t, X t+1, C t, C t+1, I t+1), we need to marginalize out\n\np(X t+1\n\nX t, I t+1) = |\n\np(X t+1\n\nC t+1) |\n\n(cid:90)Ct+1 p(C t\n\n(cid:90)Ct X t)dC tdC t+1 |\n\nK\n\n· (cid:34)\n\ni=1 (cid:89)\n\np(C t+1\n\ni\n\n|\n\nC t, pat+1\n\nG (C t+1\n\ni\n\n), I t+1\n\ni\n\n)\n\n(cid:35) ·\n\n(28)\n\nIn the assumptions with respect to the observation function h, we have defined h to be bijective, meaning that there exists an inverse h=1 that can identify the causal factors C t and noise variable Et from X t. Using the invertible map, we can write p(C t ], where δ is a Dirac 1(X t)=[Ct; ·\ndelta. We also remove Et from the conditioning set since it is independent of X t+1. This leads us to:\n\nX t) = δh− |\n\np(X t+1\n\nX t, I t+1) = |\n\nK\n\n(cid:90)Ct+1 (cid:34)\n\ni=1 (cid:89)\n\np(C t+1\n\ni\n\nC t, pat+1 |\n\nG (C t+1\n\ni\n\n), I t+1\n\ni\n\n), I t+1\n\ni\n\n)\n\np(X t+1\n\nC t+1)dC t+1 |\n\n(cid:35) ·\n\n(29)\n\n26\n\n̸ (31)\n\n(32)\n\nPublished as a conference paper at ICLR 2023\n\nWe can use a similar step to relate X t+1 with C t+1 and Et+1. However, since we model a distribution over X t+1, we need to respect possible non-volume preserving transformations. Hence, we use the change of variables formula with the Jacobian Jh = ∂h(Ct+1,Et+1) ∂Ct+1∂Et+1) of the observation function h to obtain:\n\np(X t+1\n\nX t, I t+1) = |\n\nJh|\n\n|\n\n1\n\n−\n\nC t, pat+1 |\n\nG (C t+1\n\ni\n\n), I t+1\n\ni\n\n) (cid:35) ·\n\np(Et+1)\n\n(30)\n\nSince Equation (30) is a derivation of the true generative model p(X t, X t+1, C t, C t+1, I t+1), it constitutes a global optimum of the maximum likelihood. Hence, one cannot achieve higher likelihoods by reparameterizing the causal factors or having a different graph, as long as the graph is directed and acyclic.\n\nK\n\np(C t+1\n\ni\n\n· (cid:34)\n\ni=1 (cid:89)\n\nIn the next step, we relate this maximum likelihood solution to iCITRIS, more specifically, the prior of iCITRIS. For this setting, the learnable, invertible map gθ is identical to the inverse of the observation function, h−\n\n1. In terms of the latent variable prior, we have defined our objective of iCITRIS as:\n\npφ\n\nzt+1\n\nzt, I t+1 |\n\n=\n\nK\n\npφ\n\nzt+1 ψi\n\ni=0 (cid:89)\n\n(cid:16)\n\nzt, zt+1 ψpa |\n\ni\n\n, I t+1\n\ni\n\n(cid:17)\n\n(cid:0) Since we know that g∗θ is an invertible function between information of X t. Thus, we can also replace it with zt = [C t, Et], giving us:\n\nand\n\nZ\n\nX\n\n(cid:1)\n\n, we know that zt must include all\n\npφ\n\nzt+1\n\nC t, Et, I t+1 |\n\n=\n\n(cid:0)\n\nK\n\npφ\n\nzt+1 ψi\n\ni=0 (cid:89)\n\n(cid:16)\n\n(cid:1)\n\nC t, Et, zt+1 ψpa |\n\ni\n\n, I t+1\n\ni\n\n(cid:17)\n\nNext, we consider the assignment function ψ∗. The optimal assignment function ψ∗ assigns sufficient dimensions to each causal factor C1, ..., CK, such that we can consider zt+1 for i = 1, ..., K. ψ∗i Further, the same graph G is used in the latent space as in the ground truth, except that we additionally condition zψ∗i\n\n, i = 1, ..., K on zψ∗0 . With that, Equation (32) becomes:\n\n= C t+1\n\ni\n\npφ\n\nzt+1\n\n|\n\nC t, Et, I t+1\n\n=\n\n(cid:0)\n\n(cid:34)\n\ni=1 (cid:89)\n\n(cid:16)\n\n(cid:1)\n\nK\n\npφ\n\nzt+1 ψ∗i\n\n= C t+1\n\ni\n\nC t, zt+1 ψpa\n\ni\n\n, zt+1 ψ∗0\n\n, I t+1\n\ni\n\n|\n\np(zt+1 ψ∗0 |\n\nC t, Et)\n\n(33)\n\n(cid:35) ·\n\n(cid:17)\n\nwhere we remove Et from the conditioning set for the causal factors, since know that C t+1 and Et+1 is independent of Et. Now, zψ∗0 must summarize all information of zt+1 which is not modeled in the causal graph. Thus, zψ∗0 represents the noise variables: zt+1 ψ∗0\n\n= Et+1.\n\npφ\n\nzt+1\n\nC t, Et, I t+1 |\n\n=\n\nK\n\npφ\n\nzt+1 ψ∗i\n\n= C t+1\n\ni\n\n(cid:34)\n\ni=1 (cid:89)\n\n(cid:16)\n\n(cid:1)\n\n(cid:0)\n\n(34) Finally, by using g∗θ , we can replace the distribution on zt+1 by a distribution on X t+1 by the change of variables formula:\n\n(cid:17)\n\nC t, zt+1 ψpa\n\ni\n\n, zt+1 ψ∗0\n\n, I t+1\n\ni\n\n|\n\np(zt+1 ψ∗0\n\n= Et+1\n\n|\n\nC t, Et)\n\n(cid:35) ·\n\npφ\n\nX t+1\n\nC t, Et, I t+1 |\n\n=\n\n(cid:0)\n\n(cid:1)\n\nK\n\npφ\n\nzt+1 ψ∗i\n\n= C t+1\n\ni\n\n∂g∗θ (zt+1) ∂zt+1\n\n(cid:12) (cid:12) p(zt+1 (cid:12) (cid:12) ψ∗0\n\n· (cid:34) (cid:12) (cid:12) = Et+1 (cid:12) (cid:12)\n\n|\n\ni=1 (cid:16) (cid:89) C t, Et)\n\nC t, zt+1 ψpa\n\ni\n\n, zt+1 ψ∗0\n\n, I t+1\n\ni\n\n|\n\n(cid:35) ·\n\n(cid:17)\n\n(35)\n\nWe can simplify this distribution by using the independencies of the noise term Et+1 in the causal graph of Figure 6:\n\npφ\n\nX t+1\n\nC t, Et, I t+1 |\n\n=\n\n(cid:0)\n\n(cid:1)\n\nK\n\n∂g∗θ (zt+1) ∂zt+1\n\n(cid:12) (cid:12) p(zt+1 (cid:12) (cid:12) ψ∗0\n\n· (cid:34) (cid:12) (cid:12) = Et+1) (cid:12) (cid:12)\n\ni=1 (cid:89)\n\nzt+1 ψ∗i\n\n= C t+1\n\ni\n\nC t, zt+1 ψpa\n\ni\n\n, I t+1\n\ni\n\n|\n\npφ\n\n(cid:16)\n\n(cid:35) ·\n\n(cid:17)\n\n(36)\n\nWith this, Equation (36) represents the exact same distribution as Equation (30). Therefore, we have shown that the function δ∗ that identifies the true latent variables C1, ..., CK and assigns them to the corresponding sets zψ1, ..., zψK constitutes a global optimum for maximizing the likelihood. However, this solution is not necessarily unique, and additional optima may exist. In the next steps of the proof, we will discuss the class of functions and graphs that lead to the same optimum.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nC t\n\nC t\n\npat+1(C t+1\n\ni\n\n)\n\nC t+1\n\ni\n\npat+1(C t+1\n\ni\n\n)\n\nI t+1\n\ni\n\nI t+1\n\ni\n\nsinv i\n\nC t+1\n\ni\n\n(cid:0)\n\nsvar i\n\nC t+1\n\ni\n\n(cid:0)\n\n(cid:1)\n\n(cid:1)\n\n(a) Original causal graph of Ci\n\n(b) Minimal causal split graph of Ci\n\ni\n\nFigure 7: The minimal causal variable in terms of a causal graph under iTRIS. (a) In the original causal graph, C t+1 has as potential parents the causal variables of the previous time step C t (eventually a subset), its instantaneous parents pat+1(C t+1 . (b) The minimal causal variable splits C t+1 C t+1 into an invariable part sinv . The C t+1 invariable part sinv is independent of the instantaneous parents and the intervention target. Further, it can be a parent of svar\n\ndue to the autoregressive distribution modeling.\n\n), and the intervention target I t+1\n\nand variable part svar\n\nC t+1\n\nC t+1\n\n(cid:1)\n\n(cid:0)\n\n(cid:0)\n\n(cid:1)\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n(cid:0)\n\n(cid:1)\n\ni\n\ni\n\n(cid:0)\n\n(cid:1)\n\nD.5 THEOREM 3.4 - PROOF STEP 2: CHARACTERIZING THE DISENTANGLEMENT CLASS\n\nIn this section, we discuss the identifiability results of the causal variables in iCITRIS. We first describe the minimal causal variables in iTRIS, and how they differ to TRIS in CITRIS (Lippe et al., 2022b). Next, we identify the information that must be assigned to individual parts of the latent representation. Finally, we discuss the final setup to ensure identification of the variables according to Definition 3.3, including the additional variables in zψ0.\n\nD.5.1 MINIMAL CAUSAL VARIABLES\n\nLippe et al. (2022b) introduced the concept of a minimal causal variable as an invertible split of a causal variable si(Ci) = (svar(Ci), sinv(Ci)) into one part that is strictly dependent on the intervention, svar(Ci), and a part that is independent of it, sinv(Ci) (see Definiton 2.1). In other words, the minimal causal variable is the smallest part of a causal variable that strictly depends on the provided intervention.\n\nFor iCITRIS, we consider the same concept, but adapt it to the setup of iTRIS. First, iTRIS assumes the presence of interventions that render a variable independent of its instantaneous parents. Hence, when given these interventions, we can ensure that sinv(Ci) does not have any instantaneous parents. Second, the presence of a causal graph in iCITRIS allows dependencies between different parts of the latent space. Further, zψ0 can be the parent of any other set of variables, thus allowing for potential dependencies between sinv(Ci) and svar(Ci). Note that those for the same time step, however, must also be cut off by the intervention. Hence, the split si(C t i )) must have the following distribution structure:\n\ni ) = (svar\n\ni ), sinv\n\n(C t\n\n(C t\n\ni\n\ni\n\np\n\nsi(C t+1\n\ni\n\n) |\n\n(cid:0)\n\nwhere\n\nC t, pat+1(C t+1\n\ni\n\n), I t+1\n\ni\n\n= p\n\n(cid:1)\n\np\n\n(cid:0)\n\ni\n\nsvar sinv\n\ni\n\ni\n\nC t, pat+1(C t+1 (C t+1 )\n| (C t+1 C t )\n|\n\ni\n\ni\n\n), sinv\n\ni\n\n(C t+1\n\ni\n\n), I t+1\n\ni\n\n·\n\n(37)\n\n(cid:1)\n\np\n\nsvar\n\ni\n\n(C t+1\n\ni\n\n)\n\n(cid:0)\n\nC t, pat+1(C t+1 |\n\n(cid:1)\n\n(cid:0) (C t+1\n\ni\n\n), sinv (C t+1 (C t+1\n\ni\n\ni\n\ni svar i\nsvar i\n\n), I t+1\n\n=\n\ni\n\ni C t C t, pat+1(C t+1\n\n(cid:1)\n\ni\n\n(cid:1)\n\n) |\n) |\n\n ̃p p\n\n(cid:26)\n\n(cid:0) (cid:0)\n\n), sinv i\n\n(C t+1\n\ni\n\n)\n\ni = 1\n\nif I t+1 otherwise\n\n(38)\n\nThereby, the minimal causal variable with respect to its intervention variable I t+1 maximizes the information content H(sinv\n\n(cid:1)\n\ni\n\ni\n\n(C t i )\n\nC t). These relations are visualized in Figure 7. |\n\nis the split si which\n\nCausal variables for which the intervention target is constant, i.e., no interventions have been observed,\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nwere modeled by sinv(Ci) = Ci, svar(Ci) = in CITRIS (Lippe et al., 2022b). Here, this does not naturally hold anymore since sinv(Ci) is restricted to not having any instantaneous parents. However, as stated in assumption 1, a variable without interventions cannot be an instantaneous child of any variable. Hence, for a causal variable Ci, if I t i = 0 for all t, its minimal causal split is defined as sinv(Ci) = Ci, svar(Ci) =\n\n, as in CITRIS (Lippe et al., 2022b).\n\n∅\n\n∅\n\nD.5.2\n\nIDENTIFYING THE MINIMAL CAUSAL VARIABLES\n\nAs a first step, we postulate the following lemma:\n\nLemma D.4. For all representation functions in the class ∆∗, there exist a deterministic map from the latent representation zψi to the minimal causal variable svar(Ci) for all causal variables Ci, i = 1, ..., K.\n\nThis lemma intuitively states that the minimal causal variable svar(Ci) is modeled in the latent representation zψi for any representation that maximizes the likelihood objective. Note that this does not imply exclusive modeling yet, meaning that zψi can contain more information than just svar(Ci). We will discuss this aspect in Appendix D.5.3.\n\nProof. In order to prove this lemma, we first review some relations between the conditional and joint entropy. Consider two random variables A, B of arbitrary space and dimension. The conditional entropy between these two random variables is defined as H(A H(B) (Cover et al., 2005). Further, the maximum of the joint entropy is the sum of the individual entropy terms, H(A, B) H(A) + H(B) become lower when conditioned on any other random variable B.\n\n≤ H(B) = H(A). In other words, the entropy of a random variable A can only\n\nH(A)+H(B) (Cover et al., 2005). Hence, we get that H(A\n\nB) = H(A, B)\n\nB) = H(A, B)\n\nH(B)\n\n−\n\n−\n\n−\n\n≤\n\n|\n\n|\n\nUsing this relation, we move now to identifying the minimal causal variables. If a minimal causal variable is the empty set, i.e., svar(Ci) = , for instance due to not having observed interventions on Ci, the lemma is already true by construction since no information must be modeled in zψi . Thus, we can focus on cases where svar(Ci) . Therefore, the following inequality must strictly hold:\n\n, which implies that C t+1\n\nI t+1\n\n̸⊥⊥\n\n=\n\n∅\n\n∅\n\ni\n\ni\n\nH(C t+1\n\ni\n\nC t, C t+1 |\n\n−\n\ni ) < H(C t+1\n\ni\n\nC t, C t+1 |\n\n−\n\ni , I t+1\n\ni\n\n)\n\n(39)\n\ni\n\nfor all i = 1, ..., K. Additionally, based on the assumption that the observational and interventional distributions share the same support, we know that the intervention posterior, i.e., p(I t+1 X t+1), |\ncannot be deterministic for any data point X t+1 and intervention target I t+1 . Thus, we cannot derive I t+1 from the observation X t+1. Thirdly, because every latent variable is only conditioned on exactly one intervention target in iCITRIS and there exist no deterministic function between any pair of intervention targets, one cannot identify I t+1 in any latent variables except zψi . Therefore, the only way in iCITRIS to fully exploit the information of the intervention target I t+1 is to model its dependent information in zψi . As this information corresponds to the minimal causal variable, svar(Ci), any representation function must model the distribution p(svar(Ci) , ...) to achieve the maximum likelihood solution. This is independent of the modeled causal graph structure, meaning that if there exist representation functions with different graphs in ∆∗, then all of them must model svar(Ci) in zψi. Finally, using assumption 4 (Appendix D.2.4), we obtain that this distributional relation implies a functional independence of svar(Ci) in zψi to any other latent variable. Thus, there exists a deterministic map from zψi to svar(Ci) in any of the maximum likelihood solutions.\n\n...) in p(zψi|\n\nI t+1\n\n|\n\ni\n\ni\n\ni\n\ni\n\nD.5.3 DISENTANGLING THE MINIMAL CAUSAL VARIABLES\n\nThe previous subsection showed that zψi models the minimal causal variable svar(Ci). This, however, is not necessarily the only information in zψi. For instance, for two random variables A, B R, the following distributions are identical:\n\n∈\n\np(A)\n\np(B\n\nA) = p(A) |\n\n·\n\n·\n\np(B + A\n\n|\n\nA) = p(A)\n\nA) p(B, A |\n\n·\n\n(40)\n\nThe second distribution can add additional information about A arbitrarily to B without changing the likelihoods. This is because the distribution is conditioned on A, and the conditional entropy H(A) = 0. Hence, for of a random variable to itself is H(A\n\nH(A) = H(A)\n\nA) = H(A, A)\n\n|\n\n−\n\n−\n\n29\n\n̸ Published as a conference paper at ICLR 2023\n\nC1\n\nC2\n\nC1\n\nC2\n\nC1\n\nC2\n\nC1\n\nC2\n\nC3\n\nC3\n\nC3\n\nC3\n\n(a) Observational regime\n\n(b) C1 intervened\n\n(c) C2 intervened\n\n(d) C3 intervened\n\nFigure 8: Example instantaneous causal graph between 3 causal variables C1, C2, C3, and the augmented graphs under different single-target interventions that remove instantaneous parent dependencies. The augmented graphs have the edges to the intervened variables removed. For readability, the intervened variables are colored in red in the graphs.\n\narbitrary autoregressive distributions, we cannot identify the variables from each other purely by looking at the likelihoods.\n\nHowever, in iTRIS, we are given interventions under which variables are strictly independent of their instantaneous parents. With this, we postulate the following lemma:\n\nLemma D.5. For all representation functions in the class ∆∗, zψi does not contain information about any other minimal causal variable svar(Cj), j svar(Ci)) = H(zψi|\n\n= i, except svar(Ci), i.e., H(zψi|\n\nsvar(Ci), svar(Cj)).\n\nProof. In order to prove this lemma, we consider all augmented graph structures that are induced by the provided interventions on the instantaneous causal graph. Specifically, given a graph G = (V, E) with V being its vertices and E its edges, and a set of binary intervention targets I = ,\nV |} |\nwe construct an augmented DAG G′ = (V ′, E′), where V ′ = V and E′ = E i = 1, ..., . In other words, the augmented graph G′ has all its input edges to intervened variables removed. An example for a graph of three variables and its three single-target interventions is shown in Figure 8.\n\nI1, ..., I Vi}|\n\n, Ii = 1 |\n\n{ paG(Vi)\n\n\\{{\n\nV |\n\n→\n\n}\n\ni = 0, I t\n\ni = 0, I t\n\nj = 0, I t\n\nj = 1, (2) I t\n\nA representation function in the class ∆∗ must model the optimal likelihood for all interventionaugmented graphs of its originally learned graph ˆG, since it cannot achieve lower likelihood for any of the graphs than the ground truth. For every pair of variables Ci, Cj, assumption 2 (Appendix D.2.2) ensures that there exist one out of three possible experiment sets: (1) we observe I t j = 0 and I t j = 1, or (3) I t j = 0, i = 0, I t I t j = 1. In all cases, there exist at least one augmented graph in which C t zt C t ψj , must hold since (2) and (3) observe joint interventions on both variables (I t j = 1). In (1), a constant connection between the two variables would require Ci to be present in the graph, which implies a cycle in a graph Cj and Cj → both edges Ci → violating our acyclicity assumption 5. Under the augmented graph, where zt zt ψj , the optimal likelihood can only be achieved if the distribution of zt ψj , thus not containing any information about svar(Cj). The same holds for zψj . Hence, a representation function in the class ∆∗ must identify the minimal causal variables in the latent space.\n\nj = 1, and I t j, and hence zt i = 1, I t\n\nψi is actually independent of zt\n\ni = 1, I t i = 0, I t\n\nj = 0, and I t\n\ni = 1, I t\n\ni = 1, I t\n\ni = 1, I t\n\nψi ⊥⊥\n\nψi ⊥⊥\n\ni ⊥⊥\n\nD.5.4 DISENTANGLING THE REMAINING VARIABLES\n\nIn Appendix D.5.2 and Appendix D.5.3, we have shown that for any solution in the class ∆∗, we can ensure that zψi models the minimal causal variable svar(Ci), and none other. Still, there exist more dimensions that need to be modeled. The causal variables without interventions, the invariant parts of the causal variables, sinv(Ci), as well as the noise variables Et are part of the generative model that influence an observation X t. All these variables share the property that they are not instantaneous children of any minimal causal variable, and can only be parents of them. This leads to the situation that any of these variables could be modeled in the latent representation of zψi for an arbitrary i = 1, ..., K as long as Ci is the parent of the same variables. The reason for this is that the distribution modeling of such variables is independent of interventions.\n\n30\n\n̸ Published as a conference paper at ICLR 2023\n\nTo exclude them from the causal variable modeling, we follow the same strategy as in CITRIS (Lippe et al., 2022b) by taking the representation function that maximizes the entropy of zψ0: Lemma D.6. For all representation functions in the class ∆∗ that maximize the information content C t) according to LDDP, the latent representation zψi models exclusively the minimal causal of p(zψ0| variable svar(Ci) for all causal variables Ci, i = 1, ..., K.\n\nProof. Using Lemma D.4 and Lemma D.5, we know that the only remaining information besides the minimal causal variables are the causal variables without interventions, invariant parts of the causal variables, sinv(Ci), as well as the noise variables Et. All these variables cannot be children of the observed, intervened variables, as the assumption 1 (Appendix D.2.1) states. can be optimally modeled Thus, the remaining information by p( ,\nM C t). In this which can be found by searching for the solution with the maximum entropy of p(zψ0| solution, the latent representation zψ1 , ..., zψK does not model any subset of , hence modeling the minimal causal variables exclusively.\n\n, zt, I t+1). This implies that there exist a solution where zψ0 =\n\nzt)p(zψ1, ..., zψK |M\n\nsinv(C1), ..., sinv(CK), Et\n\nM|\n\nM\n\nM\n\n=\n\n}\n\n{\n\nThe overall result is that we identify the minimal causal variables in zψ1, ..., zψK , and all remaining information is modeled in zψ0. Note that the causal variables without interventions, the noise variables and the invariant part of the causal variables can be arbitrarily entangled in zψ0. Furthermore, since there exist variables in zψ0 that may not have any temporal parents (e.g., the noise variables and invariable parts of the intervened causal variables), we cannot rely on assumption 4 (Appendix D.2.4) zt) is independent of to ensure functional independence. Hence, while the distribution of p(zψ0| zψ1, ..., zψK , there may exist dependencies such that for a single data point, a change in zψi can result in a change of the noise or invariable parts of the causal variables in the observational space.\n\nD.6 THEOREM 3.4 - PROOF STEP 3: IDENTIFIABILITY OF THE CAUSAL GRAPH\n\nIn this step of the proof, we discuss the identifiability of the causal graph under the previous findings. In the first subsection, we discuss what graph we can optimally find under the identification of the minimal causal variables. In the second part, we then show how the maximum likelihood objective is sufficient for identifying the instantaneous causal graph. Finally, we discuss the identifiability of the temporal causal graph.\n\nD.6.1 CAUSAL GRAPH ON MINIMAL CAUSAL VARIABLES\n\nThe identification of the causal graph naturally depends on the learned latent representations of the causal variables. In Appendix D.5, we have shown that one can only guarantee to find the minimal causal variables in iTRIS. Thus, we are limited to finding the causal graph on the minimal causal variables svar(C1), svar(C2), ..., svar(CK) and the additional variables modeled in zψ0 . The graph between the minimal causal variables is not necessarily equal to the ground truth graph. For instance, consider a 2-dimensional position (x, y) and the color of an object as two causal variables. If the xposition causes the color, but the minimal causal variable of the position is only svar(C1) = y, then the color has only sinv(C1) as parent, not svar(C1). In the learned graph on the latent representation, zψ2 . Hence, we it would mean that we do not have an edge between zψ1 and zψ2, but instead zψ0 → might have a mismatch between the ground truth graph on the full causal variables, and the graph on the modeled minimal causal variables.\n\nStill, there are patterns and guarantees that one can give for how the optimal, learned graph looks like. Due to the nature of the interventions, the invariable part of a causal variable, sinv(Ci), cannot have any instantaneous parents. Thus, the instantaneous parents of a minimal causal variable svar(Ci) are the same ground truth causal variables as in the true graph, i.e., pa(Ci) = pa(svar(Ci)). The difference is how the parents are represented. Since each parent Cj ∈ pa(Ci) is split into a variable and invariable part, any combination of the two can represent a parent of svar(Ci). Thus, the learned svar(Cj) set of parents for svar(Ci), i.e., pa(zψi), must be a subset of . This |\nimplies that if there is no causal edge between two causal variables Ci and Cj in the ground truth causal graph, then there is also no edge between their minimal causal variables svar(Ci) and svar(Cj). The causal graph between the true variables and the minimal causal variables therefore shares a lot of similarities, and in practice, is often almost the same.\n\nCj ∈\n\nzψ0}\n\npa(Ci)\n\n} ∪ {\n\n{\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nExp.\n\nE0 E1\n\nI1\n\n1 0\n\nI2\n\n0 1\n\nExp.\n\nI1\n\nI2\n\nExp.\n\nI1\n\nI2\n\nE0 E1 E2\n\n0 1\n1\n\n0 0\n1\n\nE0 E1 E2\n\n0 0\n1\n\n0 1\n1\n\nC1\n\nC2\n\n(a) Causal graph\n\n(b) Experimental setting 1\n\n(c) Experimental setting 2\n\n(d) Experimental setting 3\n\nFigure 9: Identifiability of a causal relation between two variables C1, C2 under different interventional settings. (a) The causal relation to consider. The discussion is identical in case of the reverse orientation by switching the variable names C1 and C2. (b-d) The tables describe the minimal sets of experiments, i.e., unique combinations of I1, I2 in the dataset, that guarantee the intervention targets to be unique, i.e., not 2. Under each of these sets of experiments, we show that the maxi-\n\n1 = I t\n\nt, I t\n\nI1, I2) uniquely identifies the causal orientation.\n\nmum likelihood solution of p(C1, C2|\n\n∀\n\nThe additional latent variables zψ0 summarize all invariable parts of the intervened variables, the remaining causal variables without interventions, and the noise variables. Therefore, zψ0 cannot be an instantaneous child of any minimal causal variable, and we can predefine the orientation for those edges in the instantaneous graph.\n\nNext, we can discuss the identifiability guarantees for the graph on the minimal causal variables. For simplicity, in the rest of the section, we refer to identifying the causal graph on the minimal causal variables as identifying the graph on C1, ..., CK.\n\nD.6.2 OPTIMIZING THE MAXIMUM LIKELIHOOD OBJECTIVE UNIQUELY IDENTIFIES THE\n\nINSTANTANEOUS CAUSAL GRAPH UNDER INTERVENTIONS\n\nSeveral causal discovery works have shown before that causal graphs can be identified when given sufficient interventions (Brouillard et al., 2020; Eberhardt, 2007; Lippe et al., 2022a; Pearl, 2009). Since the identification of the causal variables already requires interventions that render variables independent of their instantaneous parents, we can exploit these interventions for learning and identifying the graph as well. In assumption 5 (Appendix D.2.5), we have assumed that the causal graph to identify is faithful. This implies that any dependency between two variables, C1, C2, which C1), cannot be replaced by conditioning C1 C2 or C2 → have a causal relation among them (C1 → and/or C2 on other variables. In other words, in order to optimize the overall likelihood p(C1, ..., CK), we require a graph that has a causal edge between two variables if they are causally related. Now, we are interested in whether we can identify the orientation between every pair of causal variables that have a causal relation in the ground truth graph, which leads us to the following lemma:\n\nLemma D.7. In iTRIS, the orientation of an instantaneous causal effect between two causal variables Ci, Cj can be identified by solely optimizing the likelihood of p(Ci, Cj|\n\nIi, Ij).\n\nProof. To discuss the identifiability of the causal direction between two variables C1, C2, we need to consider all possible minimal sets of experiments that fulfill the intervention setup in assumption 2 (Appendix D.2.2). These three sets are shown in Figure 9. For all three sets, we have to show I1, I2) can only be achieved that the maximum likelihood of the conditional distribution p(C1, C2| by modeling the correct orientation, here C1 → C1, the same argumentation holds, just with the variables names C1 and C2 swapped. As an overview, Table 3 shows the distribution p(C1, C2| Experimental setting 1 (Figure 9b) In the first experimental setting, we are given single target interventions on C1 and C2. In the experiment E0 which represents interventions on C1 and passive observations on C2, the dependency between C1 and C2 persists in the ground truth, i.e., I1 = 1, I2 = 0. Hence, only causal graphs that condition C2 on C1 under interventions on C2| C1 ̸⊥⊥ C1 can achieve the maximum likelihood in E0. From Table 3, we see that the only causal graph that C2. Thus, when single-target interventions on C1 are observed, we can uniquely does this is C1 →\n\nC2. For cases where the true graph is C2 → I1, I2) under all possible experiments and causal graphs.\n\nidentify the orientation of its outgoing edges.\n\nExperimental setting 2 (Figure 9c) The second experimental setting provides the observational regime (E0), interventions on C1 with C2 being passively observed (E1), and joint interventions on\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nI1, I2) for all possible causal graphs among the two Table 3: The probability distribution p(C1, C2| causal variables C1, C2 under different experimental settings. Observational distributions are denoted with p(...), and interventional with ̃p(...). Note that under interventions, it is enforced that ̃p(...) is not conditioned on any parents, since we work on the instantaneous graph.\n\nInterventions I1\n\nI2\n\n0 1\n0 1\n\n0 0\n1 1\n\nC1 → C2 p(C1)p(C2| ̃p(C1)p(C2|\n\np(C1) ̃p(C2) ̃p(C1) ̃p(C2)\n\nC1) C1)\n\nCausal graph C2 → C1 p(C2)p(C1| ̃p(C2)p(C1|\n\np(C2) ̃p(C1)\n\n ̃p(C1) ̃p(C2)\n\nC2)\n\nC2)\n\nC2\n\nC1 ⊥⊥ p(C1)p(C2) ̃p(C1)p(C2) p(C1) ̃p(C2) ̃p(C1) ̃p(C2)\n\nC2 is yet again identifiable.\n\nC1 and C2 (E2). Since the experiment E1 gives us the same setup as in experimental setting 1, we can directly conclude that the causal orientation C1 → Experimental setting 3 (Figure 9d) In the final experimental setting, C1 is only observed to be jointly intervened upon with C2, not allowing for the same argument as in the experimental settings 1 and 2. However, the causal graph yet remains identifiable because of the following reasons. Firstly, the experiment E0 with its purely observational regime cannot be optimally modeled by a causal graph without an edge between C1 and C2, reducing the set of possible causal graph to C1 → C2 C1. Under the joint interventions E2, both causal graphs model the same distribution. and C2 → Still, under the experiment E1 where only C2 has been intervened upon, the two distributions differ. The graph with the anti-causal orientation compared to the true graph, C2 → C1, uses the same distribution as in the observational regime to model C1, i.e., p(C1| C2). In order for this to achieve the same likelihood as the true orientation, it would need to be conditioned on I2 as the following derivation from the true distribution p(C1, C2|\n\nI1, I2) shows:\n\np(C1, C2| p(C1|\n\nC2, I1, I2) =\n\nI1, I2) = p(C2|\n\nI1, I2)\n\np(C1|\n\n·\n\nC2, I1, I2)\n\np(C1| p(C1|\n\n(cid:26)\n\nI1) C2, I1)\n\nif I2 = 1 if I2 = 0\n\n(41)\n\n(42)\n\n= C2, I1, I2 = 0), which is ensured by C1, C2 not being conditionally independent in the ground C1 models C1 independently of I2, it therefore cannot achieve the maximum likelihood solution in this experimental settings. Hence, the only graph achieving the C2, such that the orientation can again be uniquely identified.\n\nThis derivation shows that p(C1| p(C1| truth graph. As the causal graph C2 → maximum likelihood solution is C1 →\n\nC2, I1, I2) strictly depends on I2 if p(C1|\n\nC2, I1, I2 = 1)\n\nAll other, possible experimental settings must contain one of the three previously discussed experiments as a subset, due to assumption 2 (Appendix D.2.2). Hence, we have shown that for all valid experimental settings, optimizing the maximum likelihood objective uniquely identifies the causal orientations between pairs of variables under interventions.\n\nBased on these orientations, we can exclude all additional edges that could introduce a cycle in the graph, since we strictly require an acyclic graph. The only remaining non-identified parts of the graph are edges among variables that are independent, conditioned on their parents. In terms of maximum likelihood, these edges do not influence the objective since for two variables C1, C2 with C1). Hence, the equivalence class in C1 ⊥⊥ terms of maximum likelihood includes all graphs that at least contain the true edges, and are acyclic. By requiring structural minimality, i.e., taking the graph with the least amount of edges that fully describes the distribution, we can therefore identify the full causal graph between C1, ..., CK.\n\np(C2) = p(C1|\n\np(C2) = p(C1)\n\nC2, p(C1)\n\np(C2|\n\nC2)\n\n·\n\n·\n\n·\n\nD.6.3\n\nIDENTIFYING THE TEMPORAL CAUSAL RELATIONS BY PRUNING EDGES\n\nSo far, we have shown that the instantaneous causal relations can be identified between the minimal causal variables. Besides the instantaneous graph, there also exist temporal relations between C t and C t+1, which we also aim to identify:\n\nLemma D.8. In iTRIS, the temporal causal graph between the minimal causal variables can be\n\n33\n\n̸ Published as a conference paper at ICLR 2023\n\nidentified by removing the edge between any pair of variables zt ψi zt ψi ⊥⊥\n\n, pat+1(zt+1\n\nzt+1 ψj |\n\nzt ψ\n\n).\n\nψj\n\n−\n\ni\n\n, zt+1 ψj\n\nwith i, j\n\n∈\n\n0..K\n\n, if\n\n(cid:74)\n\n(cid:75)\n\n, zt+1 ψ1\n\n, ..., zt+1\n\nProof. The prior in Equation (2) conditions the latents variables zt+1 on all variables of the previous time step, zt. Thus, this corresponds to modeling a fully connected graph from zt ψK to zt+1 ψK . Since any temporal edge must be oriented from zt to zt+1, it is clear that the true ψ0 temporal graph, GT , must be a subset of this graph. Further, since in assumption 5 (Appendix D.2.5), ψi and zt+1 we have stated that the true causal model is faithful, we know that two variables, zt ψj , are only connected by an edge, if they are not conditionally independent of each other: zt ψi zt+1 , pat+1(zt+1 ψj | independent variables with: zt zt ψ\nand remove the edge if both of them are conditionally independent given zt\n\npat(zt+1 i. Thus, we can find the true temporal graph by iterating through all pairs of variables zt\n\n̸⊥⊥ ). This implies that all redundant edges must be between two, conditionally\n\n) being a subset of ψi and zt+1 ψj ,\n\n) with pat(zt+1\n\n), pat+1(zt+1\n\n, pat+1(zt+1\n\nzt+1 ψj |\n\n, ..., zt\n\nψi ⊥⊥\n\n, zt\n\nzt ψ\n\n).\n\nψ0\n\nψ1\n\nψj\n\nψj\n\nψj\n\nψj\n\n−\n\n−\n\ni\n\nψ\n\ni\n\n−\n\nψj\n\nD.7 THEOREM 3.4 - PROOF STEP 4: FINAL IDENTIFIABILITY RESULT\n\nUsing the results derived in Appendix D.4, Appendix D.5 and Appendix D.6, we are finally able to derive the full identifiability results. In Appendix D.5, we have shown that any solution that maximizes the likelihood pφ,θ,G(xt+1 xt, I t+1) identifies the minimal causal variables of C1, ..., CK in zψ1, ..., zψK . Further, we are able to summarize all remaining variables in zψ0 by maximizing the entropy (LDDP) of pφ(zt+1 zt). In Appendix D.6, we have used this disentanglement condition to ψ0 | show that the causal graph that maximizes the likelihood must have at least the same edges as the ground truth graph on the minimal causal variables. To obtain the full ground truth graph, we need to pick the one with the least edges.\n\n|\n\nThese aspects together can be summarized into the following theorem:\n\nTheorem D.9. In iTRIS, a model (Definition 3.3) if (1) maximizes the information content H(zt+1 ψ0 | (2) minimizes the number of edges in G∗, and (3) no intervention variables I t\n\nM\n\n⟨\n\ni , I t\n\n∗ =\n\n= C, E, h S\n⟩ X t, I t+1): ∗, under the constraint of maximizing the likelihood pφ,θ,G(X t+1 |\n\nidentifies a causal system\n\nθ∗, φ∗, ψ∗, G∗\n\nzt) in terms of the LDDP (Jaynes, 1957; 1968),\n\nM\n\n⟩\n\n⟨\n\nj are deterministically related, i.e.,\n\nj\n\n∀\n\n= i :\n\n( ¬\n\nf,\n\n∃\n\nt : I t\n\ni = f (I t\n\nj)).\n\n∀\n\n34\n\n̸ Published as a conference paper at ICLR 2023\n\n4 variables\n\n6 variables\n\n9 variables\n\nFigure 10: Example sequences of the Voronoi benchmark for the different graph sizes. Each image of 32 is partitioned into K patches. The values of the K true causal variables have been transformed 32 by a two-layer normalizing flow, which result into the hues of the K patches in . The hues are finally mapped into the RGB space, resulting in the images above.\n\n8 π, 7\n\n8 π\n\n−\n\n×\n\n7\n\n(cid:2)\n\n(cid:3)\n\nE DATASETS\n\nThe following section gives a detailed overview of the dataset and used hyperparameters in all settings. Appendix E.1 contains the description of the Voronoi benchmark, for which the experimental results are shown in Section 5.2. Appendix E.2 discusses the Instantaneous Temporal Causal3DIdent dataset, and Appendix E.3 the Causal Pinball dataset.\n\nE.1 VORONOI BENCHMARK\n\nThe purpose of the Voronoi benchmark is to provide a flexible, synthetic dataset where we can evaluate causal representation learning models on various settings, such as number of variables and graph structure (both instantaneous and temporal). For each dataset, we generate one sequence with 150k time steps, in between which single-target interventions may have been performed. We sample the interventions with 1/(K + 2) for each variable, and with 2/(K + 2) a purely observational regime. A visual example of the Voronoi benchmark is shown in Figure 10, and we describe its generation steps below.\n\nE.1.1 NETWORK SETUP\n\ni\n\ni\n\npa(C t+1 |\n\nIn the Voronoi benchmark, we need a data generation mechanism for the conditional distributions p(C t+1 )) that support any set of parents. For this, we deploy randomly initialized neural networks which models arbitrary, non-linear relations between any parent set and a causal variable. We visualize the network architecture in Figure 11. As a simplified setup, we use the neural networks to parameterize a Gaussian distribution. Specifically, the neural networks take as input a subset of C t, C t+1 according to the given graph structure (see next subsection for the graph generation), and )), σ2) where output a scalar representing the mean of the conditional distribution the standard deviation is set to σ = 0.3. We have also experimented with having the (log) standard deviation as an additional output of the network. However, we experienced that this leads to the true causal variables to be the optimal solution when modeling K conditionally independent factors. Hence, both iCITRIS and the baselines were able to identify the causal variables well, making the task (0, 1) for all causal variables. easier than anticipated. The interventional distribution is thereby set to\n\nμ(pa(C t+1\n\n(C t+1\n\nN\n\n|\n\ni\n\ni\n\nN\n\nOn the causal variables, we apply a normalizing flow which consisted of six layers: Activation Normalization, Autoregressive Affine Coupling, Activation Normalization, Invertible 1x1 convolution, Autoregressive Affine Coupling, Activation Normalization. The Activation Normalization (Kingma et al., 2018) layers are initialized once after the Batch Normalizations of the distribution neural networks have been set, and ensure that all outputs roughly have a zero mean and standard deviation of one. The Autoregressive Affine Coupling layers use randomly initialized neural networks, with the average standard deviation of the outputs being 0.2. The coupling layer is volume preserving, i.e., we do not use a scaling term in the affine coupling, to prevent any issues with the image quantization. The Invertible 1x1 convolution (Kingma et al., 2018) is initialized with a random, orthogonal matrix, entangling all causal variables across dimensions. Hence, each output of the normalizing flow is influenced by all causal variables.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n1 +\n\nt\n\nC\n\n,\n\nt\n\nC\n\nt\n\nu p\nn\n\ni\n\nd e\nk s\na\n\nM\n\n) 2\n3\n\n,\n\n_ (\nr a\ne n\n\ni\n\nL\n\n) 2\n3 (\n\nm\n\nr o\nN h\nc t\na B\n\n) 1\n\n.\n\n0 (\n\nU L\ne R\ny k\na e\nL\n\n) 2\n3\n\n,\n\n2 3\n( r\na e\nn\n\ni\n\nL\n\n) 2\n3 (\n\nm\n\nr o\nN h\nc t\na B\n\n) 1\n\n.\n\n0 (\n\nU L\ne R\ny k\na e\nL\n\n) 1\n\n,\n\n2 3\n( r\na e\nn\n\ni\n\nL\n\n) 1\n(\n\nm\n\nr o\nN h\nc t\na B\n\nn a\ne\n\nM\n\nFigure 11: Network architecture of the randomly initialized neural networks in the Voronoi benchmark, modeling the conditional distributions p(C t+1 )), σ2) with σ = 0.3. The BatchNorm layers (Ioffe et al., 2015) are initialized by sequentially sampling 100 batches of the causal variables, using each as the input to the next batch. This ensures that the marginal distribution p(C t+1\n\n) has a mean close to zero and standard deviation of one.\n\npa(C t+1 |\n\ni = 0) =\n\nμ(pa(C t+1\n\n), I t+1\n\n(C t+1\n\nN\n\n|\n\ni\n\ni\n\ni\n\ni\n\ni\n\nC1\n\nC2\n\nC1\n\nC2\n\nC1\n\nC2\n\nC4\n\nC3\n\nC4\n\nC3\n\nC4\n\nC3\n\n(a) Graph structure random\n\n(b) Graph structure chain\n\n(c) Graph structure full\n\nFigure 12: Example instantaneous causal graphs with four variables for the three graph structures. The causal ordering for the causal variables is randomly sampled for each graph to prevent any structural biases.\n\n7\n\nFinally, the outputs of the normalizing flow are transformed by the function f (x) = 7 .\n8 π, 7 This function maps all values to a range of , which we can use as hues in the patches of (cid:1) the Voronoi diagram. The division by 2 of x is performed to reduce the number of data points in the saturation points of the tanh. The Voronoi diagrams are generated by sampling K points on the image, which have a distance of at least 5 pixels between each other, and are fixed within a dataset. In contrast to just mapping the colors into a grid, the Voronoi diagram is an irregular structure. Hence, the mapping from images to the K color is non-trivial and does not transfer across datasets. Once the Voronoi diagram was created, we have used matplotlib (Hunter, 2007) to visualize the structure as an RGB image.\n\ntanh\n\n8 π\n\n8 π\n\n−\n\nx 2\n\n(cid:0)\n\n(cid:2)\n\n(cid:3)\n\n·\n\nE.1.2 GRAPH GENERATIONS\n\nFor the instantaneous causal graph, we have considered three graph structures: random, chain, and full. An example of each is visualized in Figure 12.\n\nThe random graph samples an edge for every possible pair of variables Ci, Cj, i = j with a chance of 0.5. Thereby, we ensure that the graph is acyclic by sampling undirected edges, and directing them according to a randomly sampled ordering of the variables. This way, the average number of edges in the graph is K(K . For small graphs of size 4, this results in variables to eventually having no incoming or outgoing edges, testing also the model’s ability on conditionally independent variables.\n\n1)\n\n−\n\n4\n\nThe chain graph connects the variables in a sequence, where each variable is the parent of the next 1 edges, i.e., the sparsest, yet continuously one in the sequence. This leads to each graph having K connected graph.\n\n−\n\nThe full graph represents the densest directed acyclic graph possible. We first sample an ordering of variables, and then add an edge from each variable to all others that follow it in the sequence. Thus, it has the most possible edges in a DAG, namely K(K\n\n1)\n\n−\n\n.\n\n2\n\nFinally, the temporal graph is sampled similar to the random graph. However, the orientations are pre-determined by the temporal ordering, and no cycles can occur. We therefore sample a directed edge between any pair of variables C t , including i = j, with a chance of 0.25. This leads to an\n\ni , C t+1\n\nj\n\n36\n\n̸ Published as a conference paper at ICLR 2023\n\nFigure 13: Example sequence from the training set of the Instantaneous Temporal Causal3DIdent dataset (from left to right, top to bottom). Each image is of size 64 64 pixels. One can see the instantaneous effects of the background influencing the object color, for instance, or the object color again influencing the rotation of the object.\n\n×\n\naverage number of edges of K2 parent, to prevent variance collapses in the neural network distributions.\n\n4 . Additionally, we ensure that every variable has at least one temporal\n\nE.2\n\nINSTANTANEOUS TEMPORAL CAUSAL3DIDENT\n\nThe creation of the Instantaneous Temporal Causal3DIdent dataset closely followed the setup of Lippe et al. (2022b); von Kügelgen et al. (2021), and we show an example sequence of the dataset in Figure 13. We used the code provided by Zimmermann et al. (2021)1 to render the images via Blender (Blender Online Community, 2021), and used the following seven object shapes: Cow (Crane, 2021), Head (Rusinkiewicz et al., 2021), Dragon (Curless et al., 1996), Hare (Turk et al., 1994), Armadillo (Krishnamurthy et al., 1996), Horse (Praun et al., 2000), Teapot (Newell, 1975). As a short recap, 2, 2]3; the the seven causal factors are: the object position as multidimensional vector [x, y, z] [0, 2π)2; the hue of the object, background and spotlight object rotation with two dimensions [α, β] in [0, 2π); the spotlight’s rotation in [0, 2π); and the object shape (categorical with seven values). We refer to Lippe et al. (2022b, Appendix C.1) for the full detailed dataset description of Temporal Causal3DIdent, and describe here the steps taken to adapt the datasets towards instantaneous effects.\n\n−\n\n∈\n\n∈\n\n[\n\nThe original temporal causal graph of the Temporal Causal3DIdent dataset contains 15 edges, of which 8 are between different variables over time. Those relations form an acyclic graph, which we can directly move to instantaneous relations. Thus, the adjacency matrix of the temporal graph is an identity matrix, while the instantaneous causal graph is visualized in Figure 14. The causal mechanisms remain unchanged, except that the inputs may now be instantaneous. For instance, the spotlight rotation is adapted as follows:\n\nPrevious version: rot_st+1 = f Instantaneous version: rot_st+1 = f\n\n(cid:0)\n\natan2(pos_xt, pos_yt), rot_st, εt atan2(pos_xt+1, pos_yt+1), rot_st, εt\n\nrs\n\n(cid:1)\n\nrs\n\n(43)\n\n(44)\n\nwhere f (a, b, c) = a 2 + c. The causal parents of other variables, here pos_x and pos_y, are now instantaneous instead of the previous time step. Hence, an intervention on the position will lead to an instantaneous effect on the rotation of the spotlight.\n\n−\n\nb\n\n(cid:0)\n\n(cid:1)\n\nIn the original dataset, all interventions have been perfect with a uniform distribution. To relax these interventions to partially-perfect interventions, we instead use interventions that are centered around\n\n1https://github.com/brendel-group/cl-ica\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nrot_s\n\npos_o\n\nrot_o\n\nhue_b\n\nhue_s\n\nhue_o\n\nobj_s\n\nFigure 14: The instantaneous causal graph in the Instantaneous Temporal Causal3DIdent dataset. The graph contains several common sub-structures, such as a chain (rot_o rot_s), a fork rot_o), and confounders (hue_b (hue_o,hue_b hue_s,hue_o). The most difficult edges to recover pos_o since the object orientation has a complex, non-linear relation to the observation include rot_o space which is difficult to model and prone to noise. Further, the edge hue_b,hue_s hue_o only holds for two object shapes (Hare and Dragon), for which the background and spotlight hue have an influence on the object color. For the other five object shapes, the object color is independent of the other two parents.\n\n→ →\n\npos_o\n\n→\n\n→\n\n→\n\n→\n\nFigure 15: An example sequence of the Pinball dataset, from left to right, top to bottom. The paddles, i.e., the two gray rectangles in the bottom center, are accelerated forwards under interventions such that they make a large jump within an image. For instance, in image 5, the right paddle has been intervened upon and hits the ball (gray circle). It is accelerated immediately, showcasing the instantaneous effect between the two. When no interventions on the paddles are given, they slowly move backwards. In image 8, the ball hits a bumper (5 circle centers with light red filling) which lights up. This represents the scoring of a point, as the instantaneous increase in points shows in image 8 (the digits in the bottom right corner). Note that technically, there is no winning or losing state here since we do not focus on learning a policy, but instead a causal representation of the components. Further, not shown here, there exist a fourth channel representing the ball’s velocity.\n\ni , σ2 i ) with σi = 2. T denoting a truncated Gaussian ·\n2, 2] to prevent objects leaving the canvas, and σi = 1.5. All remaining aspects of the dataset\n\nthe previous time step value. Specifically, for circular values, we use C t+1 For the position variables, we use C t+1 at [ generation are identical to the Temporal Causal3DIdent dataset.\n\ni ∼ N\n\ni ∼ N\n\ni ) with\n\nT (0.5\n\ni , σ2\n\n(C t\n\nC t\n\nN\n\n−\n\nE.3 CAUSAL PINBALL\n\nThe Causal Pinball dataset is a simplified environment of the popular game Pinball, as shown in Figure 15. In Pinball, the user controls two fixed paddles on the bottom of the playing field, and tries to hit the ball such that it collides with various objects for scoring points. There are several versions of Pinball, but for this dataset, we limit it to the essential parts representing the five, multidimensional causal variables:\n\n• The ball is defined by four dimensions: the position on the x- and y-axis, and its velocity in x and y. Both are continuous values, with the position being limited to the available spots on the field.\n\n• The left paddle y-position (paddle_left) describes the position of the left paddle.\n\nIts\n\n38\n\nPublished as a conference paper at ICLR 2023\n\npaddle_left\n\npaddle_right\n\nball\n\nbumpers\n\nscore\n\nFigure 16: The instantaneous causal graph in the Causal Pinball dataset. An intervention on the paddles can have an immediate effect on the ball by changing its position and velocity. A change in the ball’s position again influences the bumpers, whether their light is activated or not. Finally, when the bumpers are activated, the score increases in the same time step.\n\nmaximum is close-to the top of the black border next to it (e.g., image 7 in Figure 15), and its minimum is close to the bottom (e.g., image 10 in Figure 15).\n\n• The right paddle y-position (paddle_right) is similar to paddle_left, just for the right paddle.\n\n• The bumpers represent the activation, i.e., the light, of all 5 bumpers. It is a five-dimensional continuous variable, each dimension being between 0 (light off, e.g., image 1 in Figure 15) and 1 (light fully on, e.g., image 8 in Figure 15).\n\n• The score is a categorical variable summarizing the number of points the player has scored.\n\nIts value ranges from 0 to a maximum of 20.\n\nThe dynamics between these causal factors resembles the standard game dynamics of Pinball, which results in the instantaneous causal graph in Figure 16. The ball can collide with the paddles, borders, and bumpers. When it collides with the borders, it is simply reflected, and we reduce its velocity by 10% (i.e., multiply by 0.9). Under collisions with the paddles, we distinguish between a collision where the paddle has been static or moving backwards, versus a collision where the paddle was moving. When the paddle was static, we use the same collision dynamics as the borders, except that we reduce its y-velocity by 70% to reduce oscillations around the paddle position. When the paddle was moving, we instead set the y-velocity of the ball to the y-velocity of the paddle. Finally, when the ball collides with a bumper, it activates the bumper’s light and reflects from it, similar to the borders. When a bumper’s light is turned on, we increase the score by one, but include a 5% chance that the score is not increased to introduce some stochastic elements and faulty components in the game. Next to the collisions, the ball is influenced by a gravity towards the bottom, adding a constant every time step to its y-velocity, and friction that reduces its velocity by 2% after each time step.\n\nIn terms of interventions, we sample the interventions on the five elements independently, but with a chance that would correspond more closely to the game dynamics. Specifically, we intervene on the paddles in 20% of the frames, 10% on the ball, and 5% each the score and bumpers. An intervention of the paddle represents it moving forwards, from its previous position, to a randomly sampled position between the middle and maximum paddle position. Its velocity is set to the difference between the previous position and new position. Since these interventions are usually elements of the standard Pinball game play, we sample them rather often with 20%. An intervention on the ball represents stopping it at the position it is, and give it slightly random velocity towards the bottom. In real-life, this would correspond to a player interfering with the ball by stopping it with their hand. To prevent instantaneous effects from the paddles, we move the ball slightly up if it is in reach of the paddles. An intervention on the bumpers is that we randomly activate a bumper with a 25% chance, while leaving others untouched and maintaining their original dynamics. Finally, an intervention on the score resets it to a random value between 0 and 4.\n\nTo render the images, we use matplotlib (Hunter, 2007) and a resolution of 64 images are generated by having a single sequence of 150k images.\n\n×\n\n64 pixels. The\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Pseudocode of the training algorithm for the prior and graph learning in iCITRIS with NOTEARS as graph learning method. For efficiency, all for-loops are processed in parallel in the code.\n\nRequire: batch of observation samples and intervention targets:\n\n1: for each batch element xt, xt+1, I t+1 do 2: 3: 4: 5: 6:\n\nEncode observations into latent space: zt = gθ(xt), zt+1 = gθ(xt+1) Differentiably sample one graphs G: Gij ∼ Sample latent to causal assignments from ψ for each batch element for each causal variable Ci do\n\nDetermine parent mask from G: S\n\nGumbelSoftmax(1\n\n0, 1\n\n−\n\n=\n\n{\n\nB\n\nxt, xt+1, I t+1\n\nN n=1\n\n}\n\nσ(γij), σ(γij))\n\n∈ { zt, zt+1\n\n⊙\n\nzt+1 ψi\n\n|\n\n(cid:16)\n\n}\n\nM , Sj = Gψ(j),i S, I t+1\n\ni\n\n(cid:17)\n\n7:\n\n8: 9:\n\nCalculate nlli =\n\nlog pφ\n\n−\n\nend for Backpropagation loss\n\n10: end for 11: Acyclicity regularizer: 12: Sparsity regularizer:\n\nn =\n\nK\n\ni=1 nlli\n\nL cycle = tr (exp(σ(γ)))\n\n(cid:80)\n\nL sparse = 1 K2\n\nL\n\nK i=1\n\nK\n\nK\n\n− j=1 σ(γij)\n\n13: Update parameters φ, ψ, γ with\n\n(cid:80)\n\nλcycle · L\n\ncycle + λsparse · L\n\nsparse + 1\n\nN\n\n(cid:80) ∇φ,ψ,γ\n\n(cid:104)\n\nn\n\nN\n\nn=1 L\n\n(cid:80)\n\n(cid:105)\n\nF EXPERIMENTAL DETAILS\n\nIn this section, we give further details on implementation details of iCITRIS and hyperparameters that were used for the experiments in Section 5.\n\nF.1\n\nICITRIS - MODEL DETAILS\n\nSimilar to CITRIS, iCITRIS can be either implemented as a VAE or as a normalizing flow trained on the representation of a pretrained autoencoder. The core elements to implement in iCITRIS are:\n\n• The map gθ from observations xt to latents zt and back (iCITRIS-VAE: convolutional\n\nencoder-decoder of the VAE | iCITRIS-NF: an autoregressive normalizing flow)\n\n• The assignment function ψ of latents to causal variables (matrix of RM\n\nwe sample via Gumbel softmax)\n\n(K+1) from which\n\n×\n\n• The prior distributions pθ (MLPs for conditional Gaussians) • The continuous-optimization causal discovery method for learning the instantaneous causal\n\ngraph (ENCO or NOTEARS, see below)\n\nThe first three are the same as in CITRIS, with the last being novel in iCITRIS. Thus, we discuss implementation details of this graph learning below, as well as the mutual information estimator, which is only necessary for perfect interventions and extra optimization stability. For the two graph learning methods, we additionally discuss the specific setup used to learn the prior distributions pθ.\n\n×\n\n−∞\n\nGraph Learning - NOTEARS The full training algorithm of iCITRIS with the NOTEARS graph parameterization is shown in Algorithm 1. The adjacency matrix is parameterized by γ ∈\n(K+1), where σ(γij), with σ being the sigmoid function, represents the probability of having R(K+1) zψj in the instantaneous graph. To prevent self-loops, we set γii = , i = 0, ..., K, the edge zψi → and γi0 = , i = 1, ..., K to guarantee an empty instantaneous parent set for zψ0 . At each training iteration, we sample an adjacency matrix per batch element using the Gumbel Softmax trick (Jang et al., 2017). These matrices are used to mask out the inputs to the prior, and therefore obtain gradients by optimizing the likelihood of the prior. Further, NOTEARS requires two regularizers. First, the acyclicity regularizer takes the matrix exponential of the edge probabilities, σ(γ). The trace of this matrix exponential has a minimum of K, which is only achieved if the matrix does not contain any cycles. All operations in this regularizer are differentiable, and we weigh this regularizer in the loss by λcycle. This weighting factor follows a scheduling over training, which starts with a value of exp( 54.6. In our experiments, this maximum factor ensured the graph to be approximately acyclic. Finally, the second regularizer is a sparsity regularizer, that removes redundant edges and is implemented as a L1 regularizer on the edge probabilities.\n\n3, and reaches a maximum of exp(4)\n\n−∞\n\n2.5e\n\n6)\n\n≈\n\n≈\n\n−\n\n−\n\n40\n\nPublished as a conference paper at ICLR 2023\n\nFigure 17: A visualization of iCITRIS as a VAE framework with using ENCO in its prior. Similar to CITRIS, iCITRIS uses an encoder-decoder structure to map images xt+1 to latents zt+1 and back. The assignment function ψ splits the latent vector zt+1 into K parts (here K = 3), one per causal variable. Between these, we learn a causal graph with ENCO, and condition the variables additionally on the intervention targets I t+1 according to ψ, and the previous time step zt.\n\n·\n\n×\n\n×\n\n∈\n\n∈\n\n−∞\n\nR(K+1)\n\n, i = 0, ..., K, and fix the orientations of the edges of zψ0 by setting θi0 =\n\n(K+1) representing the edge existence parameters, and θ θji. The probability of an edge zψi →\n\nGraph Learning - ENCO The full training algorithm of iCITRIS with the ENCO graph parameterization is shown in Algorithm 2. The adjacency matrix is parameterized by two sets of parame- (K+1) R(K+1) ters, with γ the orientation parameters, with θij = neous graph is determined by σ(γij) ting γii =\n\nzψj in the instanta- −\nσ(θij). Similar to NOTEARS, we prevent self-loops by setθ0i = , i = 1, ..., K. In contrast to NOTEARS, this parameterization leads to initial edge probabilities −∞ of 0.25. We found it beneficial to initialize the edge probabilities closer to 0.5, which we implement by initializing γij = 4, i 0.98). At each training iteration, we sample L graphs from ENCO. For all experiments, we found L = 8 to be sufficient. For each of these graphs, we evaluate the negative log likelihood of all variables. Note that in contrast to NOTEARS, this does not need to be differentiable with respect to γ and θ. Once all graphs are evaluated, we can determine zψj , versus graphs the average negative log likelihood of a zψj under graphs with the edge zψi → where this edge was missing. We use this to determine the gradients of γij and θij, if zψj has not been intervened upon. For the gradients of θij, we further mask out gradients for batch samples in which zψi has not been intervened upon. With these gradients, we can update the graph parameters, while the distribution parameters are updated based on the differentiable negative log likelihood. Note that the sparsity regularizer, λsparse, is integrated in the update of the γ parameters.\n\n= j, i, j\n\n(σ(4)\n\n1..K\n\n−\n\n≈\n\n∈\n\n(cid:75)\n\n(cid:74)\n\ni\n\n(cid:17)\n\n(cid:16)\n\n∈\n\nzt+1 ψi\n\n, I t+1\n\nthe form\n\nPrior networks Both graph learning algorithms use a prior network of zt, zt+1 pφ . To implement this efficiently in a neural network setting, we consider for ψpa i\n| each latent zm, m 1M a 2-layer neural network (hidden size 32 in all experiments), that take as input zt, zt+1, I t+1, and a mask on zt+1 and I t+1. Therefore, its input size is M +M +K +M +K = 3M + 2K. The mask on I t+1 depends on which causal variable the latent zm has been assigned to, i.e., zt+1 . The mask on zt+1 depends on the graph that was sampled, in combination with the causal variable assignment, i.e., only leave zt+1 unmasked. Further, we can use ψpa an autoregressive prior over the potentially multiple dimensions of zt+1 ψi by leaving previous latents unmasked that have been assigned to the causal variable Ci. We use this autoregressive variant for the Instantaneous Temporal Causal3DIdent and Causal Pinball dataset, since the multiple dimensions in those causal factors may not be independent.\n\nshould only depend on I t+1\n\nψi\n\ni\n\ni\n\nMutual information estimator The full training algorithm of the mutual information estimator is shown in Algorithm 3. The MI estimator is a 2 layer network, that takes as input the latent parents of a causal variable, and its current value, and has a single output value. This value indicates whether the current causal variable and its parents match or not, i.e., is zt+1 the value of Ci at time step t + 1 based on observing the parents zt and zt+1 , or not. We train this network by a binary classification ψpa\n\nψi\n\ni\n\n41\n\niCITRIS: CRL for Instantaneous and Temporal EffectsArchitectureTowards Practical Causal Representation Learning -Phillip LippeSlide 11NN NN NN X1X3X2Distribution fittingXYZNNNNNNXZYAlternate betweenboth stepsDistribution fittingGraph fitting!(#!|...)!(#\"|...)!(##|...)X1X2X3NN1NN2NN3X1X3X2Alternate betweenboth stepsDistribution fittingGraph fitting!!\"!\"!\"!\"\"#\"\"!#!!#!#!!#\"!\"#@@ijL=↵·EX,Cij⇥LXi!Xj(Xj)LXi6!Xj(Xj)+sparse⇤C(1)C(2)C(3)LX1!X2(X2)LX16!X2(X2)LX3!X2(X2)LX36!X2(X2)LX1!X3(X3)LX16!X3(X3)LX2!X3(X3)LX26!X3(X3)@@12 ̃L=0(12)·(✓12)·[LX1!X2(X2)LX16!X2(X2)+sparse]@@32 ̃L=0(32)·(✓32)·[LX3!X2(X2)LX36!X2(X2)+sparse]@@13 ̃L=0(13)·(✓13)·[LX1!X3(X3)LX16!X3(X3)+sparse]@@23 ̃L=0(23)·(✓23)·[LX2!X3(X3)LX26!X3(X3)+sparse]@@✓12 ̃L=@@✓21 ̃L=0(✓12)·(12)·[LX1!X2(X2)LX16!X2(X2)]@@✓13 ̃L=@@✓31 ̃L=0(✓13)·(13)·[LX1!X3(X3)LX16!X3(X3)]f1f2f31@@ijL=↵·EX,Cij⇥LXi!Xj(Xj)LXi6!Xj(Xj)+sparse⇤C(1)C(2)C(3)LX1!X2(X2)LX16!X2(X2)LX3!X2(X2)LX36!X2(X2)LX1!X3(X3)LX16!X3(X3)LX2!X3(X3)LX26!X3(X3)@@12 ̃L=0(12)·(✓12)·[LX1!X2(X2)LX16!X2(X2)+sparse]@@32 ̃L=0(32)·(✓32)·[LX3!X2(X2)LX36!X2(X2)+sparse]@@13 ̃L=0(13)·(✓13)·[LX1!X3(X3)LX16!X3(X3)+sparse]@@23 ̃L=0(23)·(✓23)·[LX2!X3(X3)LX26!X3(X3)+sparse]@@✓12 ̃L=@@✓21 ̃L=0(✓12)·(12)·[LX1!X2(X2)LX16!X2(X2)]@@✓13 ̃L=@@✓31 ̃L=0(✓13)·(13)·[LX1!X3(X3)LX16!X3(X3)]f1f2f31@@ijL=↵·EX,Cij⇥LXi!Xj(Xj)LXi6!Xj(Xj)+sparse⇤C(1)C(2)C(3)LX1!X2(X2)LX16!X2(X2)LX3!X2(X2)LX36!X2(X2)LX1!X3(X3)LX16!X3(X3)LX2!X3(X3)LX26!X3(X3)@@12 ̃L=0(12)·(✓12)·[LX1!X2(X2)LX16!X2(X2)+sparse]@@32 ̃L=0(32)·(✓32)·[LX3!X2(X2)LX36!X2(X2)+sparse]@@13 ̃L=0(13)·(✓13)·[LX1!X3(X3)LX16!X3(X3)+sparse]@@23 ̃L=0(23)·(✓23)·[LX2!X3(X3)LX26!X3(X3)+sparse]@@✓12 ̃L=@@✓21 ̃L=0(✓12)·(12)·[LX1!X2(X2)LX16!X2(X2)]@@✓13 ̃L=@@✓31 ̃L=0(✓13)·(13)·[LX1!X3(X3)LX16!X3(X3)]f1f2f31X1X2X3NN1NN2NN3X1X3X2Alternate betweenboth stepsDistribution fittingGraph fitting101055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107108109CausalIdentifiabilityfromTemporalSequenceswithInterventions4.Interventiontargetsareknownonthecausalfactorlevel,butwearelearningtheassignmentoflatentvari-ablestothecausalfactors,sofromthatperspectivewearelearningalsotheinterventiontargets5.Interventionsareindependentgiventhepriorstate6.ThereexistsaninvertiblemappingffromlatentspaceZtoobservationalspaceX:f:Z!X[TODO:Checkifthatisstrictlynecessary.Allwerequireisthattheinformationofallcausalfactorscanbeuniquelyidentifiedintheobservations.][Sara:maybesomethingsimilartotheblockMDPas-sumptionhere(Duetal.,2019)?][TODO:Introducefollowingnotation:]•M-numberoflatentvariables•K-numberofcausalfactors•Ci-causalfactori•AddingIinthecausalgraphasadditionalvariablesI1,...,IK3.Method[Phillip:ThemethodsectionhasbeenwrittenfortheIDF.Iameditingitcurrentlytoadjustitforthepaperwithmoredetailsetc.]Tofindthecausalfactorsfromhigh-dimensionalobservationsovertimewithinterventions,weproposeOurApproach,aVAE-basedcausalrepresentationlearningarchitecture.Wefirstintroducethetheoreticalmotivationandidentifiabilityresults.Then,wediscussthespecificdesignofthearchitecture.3.1.LearningwithInterventionsoverTimeWeconsideradatasetDoftuples{xt,xt+1,It}wherext,xt+1representtheobservationsattimesteptandt+1respectively,andIt2[0,1]KisabinaryvectorwhereItide-noteswhetherthecausalvariableCihasbeenintervenedonornotduringthetransitionfromxttoxt+1.Weaimtolearnaninvertiblemappingfromobservationstoalatentspace,g✓:X!Z,wherethelatentspaceZfollowsacertainstructurewhichidentifiesanddisentanglesthecausalfactorsC1,...,CK.Todothis,wemodelaprobabilitydistributioninthelatentspace,p(zt+1|zt,It)withzt,zt+12RM,whichenforcesadisentanglementoverlatentsbycondi-tioningeachlatentvariableonmaximumoneofthetargets.Specifically,wepickthedistributionoverlatentstohavethefollowingstructure:p(zt+1|zt,It)=KYi=1pzt+1 i|zt,Iti(1)where i={j2{1,...,M}| (j)=i}representsthesetoflatentvariablesassignedtothecausalvariablei. (i)istherebyalearnableassignmentfunctionwhichmapseachlatentvariabletooneoftheinterventiontargets, :{1,...,M}!{0,...,K},with (j)=0indicatingthatthelatentvariablezjdoesnotbelongtoanyofthecausalvariables.Then,theobjectiveofthemodelistomaximizethelikelihoodp(g✓(xt+1)|g✓(xt),It)forallelementsinD.UnderthismodelsetupandthediscussedassumptionsinSection2.1,weshowthefollowing:Theorem3.1.Supposethat⇤,✓⇤and ⇤arethepa-rametersthat,undertheconstraintofmaximizingthelike-lihoodp(g✓(xt+1)|g✓(xt),It),maximizetheentropyofp(zt+1 0|zt,It).Then,themodel⇤,✓⇤, ⇤learnsala-tentstructurewherethelatentvariablesz irepresenttheintervention-dependentinformationofthecausalvariableCi,andallotherinformationisstoredinz 0.WeoutlinetheproofforthisstatementinAppendixB.In-tuitively,thismeansthatwecanidentifycausalvariablesforwhichinterventionshavebeenprovided,andwhosecon-ditionaldistributionstrictlydependsonit.Examplesthatviolatethissettingincludevariableswithoutanytemporaldependency,orimperfectinterventionsthatleavecertainaspectsofthecausalvariablesunchanged.[TODO:Finishintervention-dependencydiscussion][Phillip:Ifindthisabithardtowritesinceweneedtodecidehowmuchdetailswewanttoincludehere.Theformaldefinition(movedtotheappendixfornow)oftheintervention-independencefeelstoomuch,butatthesametime,itisanimportantconceptofourpaper.][TODO:Arguewhyinterventionindependentpartsarenotthemostrelevantanyways:(1)Beinginterventionindependentmeansthatwecanlikelynotinfluenceitanyways,atleastnotindirectways.Hence,inRLset-tings,thereislikelylittleincentivetomodelthispartasaseparatevariable.(2)Whenhavingperfectinter-ventions,thisrequirestime-independentvariables/sub-dimensions,andcan’toccurfortemporaldependencies.]Thesetupabovehasthebenefitofnottakinganyassumptiononthedistributionorrestrictingthecausalgraph,besideshavingtemporaldependencies.Further,incontrasttomanyotherworks,acausalvariablecanberepresentedbymultiplelatentvariablesinthissetup.Thisallowsthemodelingofdifferentlevelsofcausalvariables.Forinstance,thepositionofanobjectcanbemodeledbymultiplelatentvariables,e.g.thepositioninthethreedimensionsx,y,z,whilethehigher-levelcausalvariableconsistsofallofthemtogether.EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11EncoderDecoderq✓p✓Transitionpriorxtxt+1ˆxtˆxt+11055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107108109CausalIdentifiabilityfromTemporalSequenceswithInterventions{Ct1,Ct2,...,CtK}Tt=1,forwhichateachtimesteptwecanonlyobserveahigh-dimensionalobservationXt=o(Ct1,Ct2,...,CtK)thatrepresentsanentangledviewofallthecausalfactors.Forexample,inPongthelatentcausalfactorscanrepresentthepositionofthepaddleortheball,whiletheobservationistheimage.Thecausalfactorscanbecontinuous,discrete,ormixed.Moreover,inthiswork,weconsidercausalfactorstopotentiallybemultidimensionalwhichofferssupporttomodelingdifferentlevelsofcausalvariables(e.g.a2D-positionencodedinasinglefactorwithtwodimensionsinsteadoftwodifferentcausalfactors).Ourgoalistoidentifythecausalfactors(Ct1,Ct2,...,CtK)fromtheobservationsXtunderthefollowingassumptions.Causalstructureassumptions:Weassumethattheun-derlyinglatentcausalprocessisanunobserveddynamicBayesiannetwork(DBN(Dean&Kanazawa,1989;Murphy,2002))overtherandomvariables(C1,C2,...,CK)withnoinstantaneouseffectandfirst-orderMarkov(i.e.thecausalparentsofafactorattimetcanonlybeintheprevioustimestept1),forwhichtheparametersaretime-invariant(i.e.thetimeseriesisstationary).AstypicalinDBNs,weassumethatthecausalfactorsarecausallysufficient(i.e.therearenoadditionallatentconfounders).Availabilityofinterventiontargets:Additionally,weas-sumeineachtimestep,somecausalfactorsmighthavebeeninterveneduponandthatwehaveaccesstotheinterventiontargets.WedenotetheseinterventiontargetsbythebinaryvectorIt2{0,1}KwhereIti=1referstoaninterventiononthecausalvariableCti.WealsoassumethatinterventionstargetsIt1,...,ItKareindependentofeachothergiventhepriorstate:Iti??Itj|Ct11,...,Ct1K.1Inthissetup,wecanmodelinterventionswithanarbitrarynumberoftargets,includingtheemptyset(observationaldata).Moreover,itcanmodelbothperfectinterventions(inwhichthetargetvariablebecomesindependentofthecausalparents)andsoftinterventions(inwhichonlytheconditionaldistributionP(Ci|pa(Ci))ofthetargetCigivenitsparentspa(Ci)changes).Observationassumptions:Weassumethateachlatentcausalfactorscanbeuniquelyidentifiedfromtheobserva-tions,i.e.thereexistsasurjectivemapf:X!CfromtheobservationspaceX✓RNtothecausalfactorspaceC.3.2.LearningwithInterventionsoverTimeWeconsideradatasetDoftuples{xt,xt+1,It+1}wherext,xt+12RNrepresenttheobservationsattimesteptand1NotethatwhentwovariablesCiandCjcanonlybeinter-venedupontogether,orappeartobeinthefinitedata,ouridenti-fiabilityresultwillnotbeabletodistinguishCiandCj.Instead,weareabletoidentifyCiandCjasajoint,coarsevariable.(cid:91)(cid:92)(cid:19)(cid:20)(cid:21)Figure2.Examplecausalprocessofaballwithatwo-dimensionalpositionx,y.Theballcanonlyswapbetweenthetwoboxesbytheuseofanintervention,whichdoesnotinfluencethedynamicsofthepositionwithinabox.t+1respectively,andIt+1describestheinterventiontargetsattimestept+1.Weaimtolearnaninvertiblemappingfromobservationstoalatentspace,g✓:X!Z,disentan-glingthedifferentcausalfactors.Thereby,wechoosethelatentspacetobelargerthanthenumberofcausalfactors,i.e.Z✓RM,MK,suchthatasinglecausalfactorcanbemodeledinmultiplelatentdimensions.Thisallowstheencodingofmultidimensionalfactors,butalsobenefitstheoptimizationprocess,sincesomevariableslikecircularanglesorcategoricalfactorswithmanycategoriescanhavesimplerdistributionswhenmodeledinmoredimensions.Toimplementthissetup,wemodelaprobabilitydistributioninthelatentspace,p(zt+1|zt,It+1)withzt,zt+12RMbeingthelatentvariablesforxtandxt+1respectively.Thisdistributionenforcesadisentanglementoverlatentsbycon-ditioningeachlatentvariableonmaximumoneofthetar-gets:p(zt+1|zt,It+1)=KYi=0pzt+1 i|zt,It+1i(1)where i={j2J1..MK| (j)=i}representsthesetoflatentvariablesassignedtothecausalvariableiandIt+10=;. (i)istherebyalearnableassignmentfunctionwhichmapseachlatentvariabletooneoftheinterventiontargets, :J1..MK!J0..KK,with (j)=0indicatingthatthelatentvariablezjdoesnotbelongtoanyintervenedcausalvariable.Then,theobjectiveofthemodelistomaximizethelikelihoodp(g✓(xt+1)|g✓(xt),It+1)forallelementsinD.Beforediscussingtheidentifiabilityresultsfortheinterven-tionalcase,wefirststatethat:Proposition3.1.Ingeneral,undertheassumptionsandsetupofSection3.1,causalfactorswithoutauniquesetofinterventions,cannotbeuniquelyidentified.TakeasanexamplethesetupinFigure2,whereaballcanmoveintwodimensions,xandy.IfbothxandyfollowaGaussiandistributionovertime,thenanytwoorthogonalaxescandescribethedistributionequallywell(Hyv ̈arinenetal.,2001;2019),makingitimpossibletouniquelyidentify̸ Published as a conference paper at ICLR 2023\n\nAlgorithm 2 Pseudocode of the training algorithm for the prior and graph learning in iCITRIS with ENCO as graph learning method. For efficiency, all for-loops are processed in parallel in the code.\n\n=\n\n{\n\nB\n\nxt, xt+1, I t+1\n\nN n=1\n\n}\n\nRequire: batch of observation samples and intervention targets:\n\nEncode observations into latent space: zt = gθ(xt), zt+1 = gθ(xt+1) Sample L graphs G1, ..., GL from Gl Sample latent to causal assignments from ψ for each batch element for each graph Gl do\n\n1: for each batch element xt, xt+1, I t+1 do 2: 3: 4: 5: 6: 7:\n\nfor each causal variable Ci do\n\nσ(θij)σ(γij)\n\nzt+1\n\nij ∼\n\n=\n\nDetermine parent sets for graph Gl: zt+1 ψpa zt, zt+1 ψpa |\n\nCalculate nlll\n\nzt+1 ψi\n\nlog pφ\n\ni =\n\n−\n\ni\n\ni\n\nj\n\n{ , I t+1\n\ni\n\n(cid:16)\n\n(cid:74)\n\n(cid:75)\n\n(cid:17)\n\nj\n\n|\n\n∈\n\n1..M\n\n, ψ(j)\n\npaGl(i)\n\n}\n\n∈\n\n8:\n\n9: 10: 11:\n\n12:\n\nend for\n\nend for Backpropagation loss\n\nAverage nll for Ci →\n\nn = 1\n\nL\n\nK i=1\n\nCj: pos_nlln (cid:80)\n\n(cid:80)\n\nL\n\nl=1 nlll ij =\n\ni (cid:80)L l=1 Gl (cid:80)L\n\nij nlll\n\nj\n\nl=1 Gl\n\nij\n\nL /\n\n̸→\n\n, neg_nlln\n\nij =\n\n(cid:80)L\n\nl=1(1 L\n\nGl −\n(cid:80)L l=1 Gl\n\nij )nlll\n\nj\n\nij\n\n−\n\n13: end for 14: Theta gradients:\n\n(θij) = σ(γij)σ′(θij)\n\n1 N\n\nN\n\nn=1 I n\n\ni (1\n\n15: Gamma gradients:\n\n(cid:16) (γij) = σ(θij)σ′(γij)\n\n1 (cid:80) N\n\nN\n\nn=1(1\n\n16: Update theta and gamma with the gradients calculated above 17: Update distribution and assignment parameters φ, ψ with\n\n(cid:80)\n\n(cid:16)\n\n∇\n\n∇\n\nneg_nlln\n\nij)\n\nneg_nlln\n\n(cid:17) ij + λsparse\n\n(cid:1)(cid:17)\n\n−\n\nj )(pos_nlln I n pos_nlln\n\nI n j )\n\nij −\n\nij −\n\n−\n\n∇φ,ψ\n\n(cid:0) 1\nN\n\nn\n\nN\n\nn=1 L\n\n(cid:80)\n\ni\n\ni\n\ni\n\nψi\n\nψi\n\n, zt, zt+1 ψpa\n\n, zt, zt+1 ψpa\n\n, to a randomly picked\n\nproblem, where the model compares the true set of values, i.e., zt+1 time step τ , zτ +1 . Since the model does not have the precise time step t or τ as input, it has to deduce from the values of the causal variables whether they match or not. Under interventions, we know that for the true causal variables, the optimal performance of this binary classifier is 0.5, because C t+1 is independent of all its parents under perfect interventions. Thus, the gradients of the latents is to move the classifier closer to 0.5, which is equal to trying to increase the misclassification rate of the MI estimator. During training, we need to sample instantaneous graphs G from our graph parameterization. Since especially in the beginning, this graph is close to random, and the true causal variables still depend on their children, for instance, under interventions, it can lead to unstable behavior to train the MI estimator on all parents from the start. Thus, instead, we initially train the MI estimator with an empty instantaneous causal graph and try to make zt+1 independent of zt, i.e., its temporal parents. Over the progress of training, we introduce the instantaneous parents, similar to the graph learning scheduling, such that at the end of training, the MI estimator is fully trained on both temporal and instantaneous parents.\n\nψi\n\nF.2 HYPERPARAMETERS\n\nWe have summarized an overview of all hyperparameters in Table 4. Additionally, we discuss the main hyperparameter choices for all models here.\n\nBase VAE architecture For all VAE-based methods, we have applied the same VAE to have a fair comparison between methods. In particular, we have used a VAE with a normalizing flow prior (Rezende et al., 2015), inspired by the inverse autoregressive flows (Kingma et al., 2016). The encoder outputs the parameters for M independent Gaussian distributions. A sample of these Gaussians is used as input to the decoder to reconstruct the original image, but also as input to a four-layer autoregressive normalizing flow. This flow consists of a sequence of Activation Normalization (Kingma et al., 2018), Invertible 1 1 Convolutions (Kingma et al., 2018), and autoregressive affine coupling layers. The outputs of the flow are used as input to a prior, which is conditioned on the latents of the previous time step and the intervention targets. For iCITRIS, this prior follows the structure of Equation (2) including causal discovery. For CITRIS, this prior is similar to Equation (2), except that no instantaneous parents are modeled. For the iVAE, the prior is a 3-layer MLP that outputs M independent Gaussian distributions. Finally, for the iVAE-AR, the prior is a 2-layer\n\n×\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3 Pseudocode of the training algorithm for the mutual information estimator in iCITRIS. For efficiency, all for-loops are processed in parallel in the code.\n\n=\n\nxt, xt+1, I t+1\n\nB\n\n{\n\nN n=1\n\n}\n\nRequire: batch of observation samples and intervention targets:\n\n1: Encode all observations into latent space: zt = gθ(xt), zt+1 = gθ(xt+1) 2: Sample an instantaneous graph G from graph parameterization 3: Sample latent to causal assignments from ψ 4: for each causal variable Ci do 5: 6: 7:\n\nFilter out all batch elements for which I t+1 for each element in the filtered batch do\n\ni = 0\n\nzt+1\n\n8:\n\n9: 10:\n\ni\n\n=\n\nDetermine parent sets for graph G: zt+1 j\nψpa |\nCalculate logits of positive pairs: epos = NNMI(zt+1 For each batch element, sample a different, random time step in the batch, τ Calculate logits of negative pairs: eneg = NNMI(zτ +1 Calculate loss for MI estimator: Calculate loss for latents:\n\n1..M ∈\n, zt, zt+1 )\nψpa (cid:75) , zt, zt+1 ψpa\n\neneg + log [exp(epos) + exp(eneg)]\n\nepos + log [exp(epos) + exp(eneg)]\n\npaGl (i)\n\nNNMI i\n\n, ψ(j)\n\n−\n\n=\n\n∈\n\nψi\n\nψi\n\n{\n\n(cid:74)\n\n)\n\nj\n\ni\n\ni\n\n}\n\n11: 12: 13: 14: end for 15: Update parameters of NNMI according to avg loss 16: Backpropagate gradients of latents according to avg loss\n\nL i =\n\nend for\n\nNNMI i\n\nzMI\n\n−\n\nL\n\nL\n\nzMI i\nL\n\nautoregressive NN predicting N Gaussian distributions in sequence. The reconstruction loss is based on the Mean-Squared Error (MSE) objective, which provided much better results than learning a flexible distribution over the output images. The specific architecture of the encoder and decoder depends on the dataset, where we used simpler models where possible to reduce computational cost without losing significant performance. For the Voronoi benchamrk, we use a 5-layer CNN. For the Instantaneous Temporal Causal3DIdent dataset and the Causal Pinball dataset, we used a 10-layer CNN for the encoder, and a 5-layer ResNet (He et al., 2016) as decoder.\n\nAutoencoder + Normalizing flow architecture For iCITRIS and CITRIS, we use the variation of training a normalizing flow on a pretrained autoencoder for the Instantaneous Temporal Causal3DIdent and Causal Pinball dataset. The autoencoder uses the same encoder and decoder architecture as the VAE, except that we increase the decoder size since it can be trained much faster than the VAE (does not require any temporal dimension), and, in contrast to the VAE, lead to improvements in the reconstruction for the two datasets. The autoencoder is trained on reconstructing the input images, where we add Gaussian noise with a small standard deviation (0.05) to the latents to simulate a distribution. Additionally, we apply a small L2 regularizer on the latents to prevent that the autoencoder counteracts the noise in the latents by artificially scaling up the standard deviation of the latents. For Causal3DIdent, we use a weight of 1e-5 on this regularizer, and 1e-6 for the Causal Pinball since its reconstructions obtain much lower losses. The normalizing flow, applied on it, follows the same architecture as in the VAE.\n\nOptimizer For all models, we use the Adam optimizer (Kingma et al., 2015) with a learning rate of 1e-3. Additionally, we warmup the learning rate for the first 100 steps. Afterwards, we follow a cosine annealing learning rate scheduling, that, over the course of the training, decreases the learning rate to 5e-5.\n\nFrameworks All models have been implemented and trained using PyTorch v1.10 (Paszke et al., 2019) and PyTorch Lightning v1.6.0 (Falcon et al., 2019).\n\nF.3 EVALUATION METRICS\n\nFor the details on the correlation matrix evaluation, we refer to Lippe et al. (2022b, Appendix C.3.1). The causal graph evaluation is performed for each model in the same way. For each model, we use the checkpoint of the best training loss, and encode all observations to the latent space. Next, we need to separate the latent space into the causal variables. For iCITRIS and CITRIS, we use the learned assignment function ψ to assign latent variables to causal variables. Since the iVAE models do not learn such a latent-to-causal assignment, we instead assign each latent variable to the causal\n\n43\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Summary of the hyperparameters for all models evaluated on the Voronoi benchmark, Instantaneous Temporal Causal3DIdent dataset, and the Causal Pinball dataset.For all methods, we performed a hyperparameter search over the individual, most crucial hyperparameters (e.g. KLD factor in iVAE). The smaller networks in the latter two datasets for the iVAE architectures were chosen because they require training the full encoder, decoder, and NF at the same time, and larger networks did not show any noticeable improvements. The graph learning warmup in iCITRIS is equal for all datasets, although deviations to e.g. 5k, 15k or 20k often work equally well. Further, the weighting parameters of target classifier and mutual information estimator for iCITRIS , such that, for instance, equally good results were achieved with smaller (5) or higher (20) weights on the Voronoi benchmark. For the graph sparsity regularizer, we used the same value for all graph structures of the same size.\n\nHyperparameter\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nVoronoi benchmark\n\nLearning rate Learning rate warmup Optimizer Batch size Number of epochs KLD Factor (β) Num latents Model variant Encoder NF-based prior Prior dependencies Decoder Hidden dimensionality Activation function Target classifier weight MI weight Graph learning warmup Graph sparsity reg.\n\n—- 1e-3 —- —- 100 steps —- —- Adam (Kingma et al., 2015) —- —- 512 —- —- 400 —- —- 10 —- —- 2x number of causal variables —- —- VAE with NF prior —- —- 5 layer CNN + 2 linear layers —- —- 4 layer, autoregressive affine coupling (Kingma et al., 2016) —-\n\n— Independent Gaussians — —- 5 layer (deconv-)CNN + 2 linear layers —- —- 32 —- —- Swish (Ramachandran et al., 2017) —-\n\nAutoregressive\n\n– 10 –\n\n– 10 – – 10k –\n\n0.02 (K=4,6) / 0.004 (K=9)\n\n0.002 (K=4,6) / 0.0004 (K=9)\n\nn.a. n.a. n.a.\n\nn.a. n.a. n.a. n.a.\n\nn.a. n.a. n.a. n.a.\n\nHyperparameter\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nInstantaneous Temporal Causal3DIdent dataset\n\nLearning rate Learning rate warmup Optimizer Batch size Number of epochs KLD Factor (β) Num latents Model variant Encoder Num flow layers Prior dependencies Decoder Hidden dimensionality Activation function Target classifier weight MI weight Graph learning warmup Graph sparsity reg.\n\n—- 1e-3 —- —- 100 steps —- —- Adam (Kingma et al., 2015) —- —- 512 —-\n\n—- 500 —-\n\n—- 250 —-\n\n—- 1 —- —- 32 —-\n\n— AE + NF — —- 10-layer CNN (AE) —- — 6 layers — — Autoregressive per zψi — —- 10-layer ResNet —-\n\n—- 64 (VAE/AE) / 32 (NF) —- —- Swish (Ramachandran et al., 2017) —-\n\n– VAE with NF prior – —- 10-layer CNN (VAE) —- — 4 layers — Independent Autoregressive —- 5-layer ResNet —-\n\n– 3 – – 2 – – 10k –\n\n0.02\n\n0.0004\n\nCausal Pinball dataset\n\n2 n.a. n.a. n.a.\n\nn.a. n.a. n.a. n.a.\n\nn.a. n.a. n.a. n.a.\n\nHyperparameter\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nLearning rate Learning rate warmup Optimizer Batch size Number of epochs KLD Factor (β) Num latents Model variant Encoder Num flow layers Prior dependencies Decoder Hidden dimensionality Activation function Target classifier weight MI weight Graph learning warmup Graph sparsity reg.\n\n—- 1e-3 —- —- 100 steps —- —- Adam (Kingma et al., 2015) —- —- 512 —-\n\n—- 500 —-\n\n—- 250 —-\n\n—- 1 —- —- 24 —-\n\n— AE + NF — —- 10-layer CNN (AE) —- — 6 layers — — Autoregressive per zψi — —- 10-layer ResNet —-\n\n– VAE with NF prior – —- 10-layer CNN (VAE) —- — 4 layers — Independent Autoregressive —- 5-layer ResNet —-\n\n—- 64 (VAE/AE) / 32 (NF) —- —- Swish (Ramachandran et al., 2017) —-\n\n– 4 – – 0 – – 10k –\n\n0.02\n\n0.001\n\n2 n.a. n.a. n.a.\n\nn.a. n.a. n.a. n.a.\n\nn.a. n.a. n.a. n.a.\n\nfactor that it has the highest correlation to. This requires using the ground truth values of the causal variables, and hence gives the iVAE an advantage over iCITRIS and CITRIS. With this separation,\n\n44\n\nPublished as a conference paper at ICLR 2023\n\nwe apply ENCO (Lippe et al., 2022a) to learn the temporal and instantaneous graph. Since iCITRIS already learns an instantaneous graph, we reuse the learned orientations of the model, and only relearn the edge existence parameters, γ, for potential pruning. In general, we found that the graphs predicted by iCITRIS have a few redundant edges between ancestors and descendants, which occur due to correlations in the early training iterations, and can easily be removed in this post-processing step.\n\nAs an additional metric to jointly evaluate the disentanglement of the causal variables and the learned causal graph, we use the learned causal graph and distributions by ENCO to sample new data point under novel interventional settings. For each data point in the test dataset, we use the trained model to sample the latents in the next time step, and map them back to the true causal variable space. This mapping is done by a small neural network, trained on the latents of the training dataset. To evaluate how well these samples match the interventional distributions of the true causal model, we train a small discriminator network which tries to distinguish between the true data points in the test set, and the newly generated ones from our model. Only a model that has disentangled the causal variables, and learned the correct causal graph, can perform well on this metric. We show the results for this metric on the Voronoi benchmark and the Instantaneous Temporal Causal3DIDent dataset in Appendix G.\n\n45\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: Experimental results of the large-scale study on the Voronoi benchmark with standard deviations over 5 seeds. We report the R2 correlation (R2 diag / R2 sep), the SHD between the predicted and ground truth graph (instantaneous / temporal), and the accuracy of a discriminator distinguishing between true intervention samples and generated ones from the individual models (optimal 50%).\n\nModel\n\n#variables\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\n4\n\n6\n\n9\n\nR2\n\nRandom\n\nSHD\n\n0.99 / 0.00 (±0.00) / (±0.00)\n\n0.00 / 0.00 (±0.00) / (±0.00)\n\n0.96 / 0.01 (±0.01) / (±0.00)\n\n0.00 / 0.40 (±0.00) / (±0.55)\n\n0.86 / 0.09 (±0.09) / (±0.08)\n\n1.00 / 2.60 (±1.22) / (±1.52)\n\n0.74 / 0.20 (±0.18) / (±0.18)\n\n1.00 / 4.20 (±0.71) / (±3.63)\n\n0.84 / 0.21 (±0.15) / (±0.18)\n\n1.80 / 3.00 (±1.64) / (±2.35)\n\n0.97 / 0.00 (±0.01) / (±0.00)\n\n0.20 / 1.00 (±0.45) / (±1.22)\n\n0.95 / 0.01 (±0.03) / (±0.01)\n\n0.80 / 3.40 (±1.30) / (±3.51)\n\n0.80 / 0.13 (±0.04) / (±0.03)\n\n5.00 / 11.20 (±3.54) / (±2.59)\n\n0.70 / 0.22 (±0.08) / (±0.08)\n\n5.40 / 14.60 (±3.36) / (±4.10)\n\n0.77 / 0.24 (±0.12) / (±0.11)\n\n8.20 / 9.20 (±1.30) / (±3.49)\n\n0.96 / 0.00 (±0.01) / (±0.00)\n\n0.20 / 1.20 (±0.45) / (±1.10)\n\n0.88 / 0.05 (±0.06) / (±0.04)\n\n1.40 / 2.40 (±2.61) / (±5.37)\n\n0.71 / 0.17 (±0.10) / (±0.08)\n\n14.00 / 18.40 (±4.47) / (±5.90)\n\n0.65 / 0.22 (±0.09) / (±0.07)\n\n13.60 / 24.00 (±2.88) / (±3.81)\n\n0.69 / 0.22 (±0.10) / (±0.03)\n\n17.60 / 21.00 (±2.97) / (±7.28)\n\nDisc.\n\n55.71% (±1.52)\n\n57.43% (±4.41)\n\n59.10% (±3.47)\n\n65.41% (±8.14)\n\n69.85% (±6.78)\n\n59.86% (±3.17)\n\n61.48% (±3.93)\n\n65.61% (±2.48)\n\n69.99% (±3.11)\n\n79.28% (±2.87)\n\n63.29% (±0.99)\n\n64.92% (±3.86)\n\n70.79% (±1.26)\n\n71.78% (±1.27)\n\n90.25% (±8.73)\n\nGraph structure Chain\n\nSHD\n\nR2\n\n0.99 / 0.00 (±0.01) / (±0.00)\n\n0.00 / 0.20 (±0.00) / (±0.45)\n\n0.89 / 0.08 (±0.14) / (±0.12)\n\n1.40 / 1.60 (±1.67) / (±2.07)\n\n0.83 / 0.12 (±0.08) / (±0.08)\n\n2.00 / 4.20 (±1.58) / (±2.17)\n\n0.75 / 0.19 (±0.14) / (±0.16)\n\n2.80 / 6.40 (±1.92) / (±1.52)\n\n0.92 / 0.17 (±0.04) / (±0.09)\n\n3.20 / 2.40 (±1.92) / (±1.67)\n\n0.98 / 0.00 (±0.00) / (±0.00)\n\n0.00 / 0.20 (±0.00) / (±0.45)\n\n0.96 / 0.02 (±0.03) / (±0.03)\n\n0.80 / 1.00 (±1.30) / (±2.24)\n\n0.80 / 0.15 (±0.05) / (±0.05)\n\n2.80 / 9.80 (±2.05) / (±2.77)\n\n0.70 / 0.23 (±0.04) / (±0.04)\n\n3.40 / 13.20 (±1.14) / (±3.19)\n\n0.84 / 0.23 (±0.08) / (±0.11)\n\n4.60 / 8.20 (±0.89) / (±2.86)\n\n0.97 / 0.00 (±0.00) / (±0.00)\n\n0.00 / 0.00 (±0.00) / (±0.00)\n\n0.93 / 0.04 (±0.03) / (±0.02)\n\n0.40 / 0.20 (±0.89) / (±0.45)\n\n0.84 / 0.10 (±0.03) / (±0.02)\n\n3.00 / 13.00 (±1.22) / (±1.58)\n\n0.71 / 0.21 (±0.04) / (±0.03)\n\n4.80 / 27.60 (±1.48) / (±5.27)\n\n0.80 / 0.18 (±0.12) / (±0.11)\n\n7.80 / 16.00 (±3.56) / (±9.25)\n\nDisc.\n\n55.61% (±0.25)\n\n65.28% (±16.51)\n\n60.77% (±2.68)\n\n66.28% (±5.97)\n\n70.76% (±5.06)\n\n59.31% (±1.84)\n\n65.17% (±12.64)\n\n65.54% (±2.82)\n\n69.44% (±2.76)\n\n80.84% (±11.49)\n\n62.91% (±1.17)\n\n62.52% (±0.86)\n\n67.44% (±1.33)\n\n71.86% (±3.20)\n\n79.12% (±2.51)\n\nR2\n\nFull\n\nSHD\n\n0.97 / 0.00 (±0.01) / (±0.01)\n\n0.00 / 0.60 (±0.00) / (±0.89)\n\n0.94 / 0.02 (±0.03) / (±0.03)\n\n0.00 / 3.20 (±0.00) / (±2.39)\n\n0.73 / 0.17 (±0.12) / (±0.11)\n\n2.80 / 5.80 (±0.84) / (±1.64)\n\n0.60 / 0.24 (±0.20) / (±0.13)\n\n3.60 / 6.80 (±1.34) / (±1.79)\n\n0.86 / 0.32 (±0.07) / (±0.12)\n\n4.60 / 3.60 (±1.14) / (±1.95)\n\n0.93 / 0.01 (±0.03) / (±0.01)\n\n0.80 / 4.20 (±1.10) / (±4.38)\n\n0.81 / 0.07 (±0.14) / (±0.07)\n\n4.75 / 7.75 (±5.91) / (±4.57)\n\n0.70 / 0.14 (±0.03) / (±0.03)\n\n11.20 / 12.40 (±2.59) / (±2.70)\n\n0.61 / 0.18 (±0.08) / (±0.04)\n\n12.00 / 17.20 (±1.87) / (±2.59)\n\n0.75 / 0.24 (±0.09) / (±0.08)\n\n12.60 / 9.00 (±0.55) / (±4.06)\n\n0.89 / 0.02 (±0.03) / (±0.01)\n\n1.20 / 9.60 (±2.17) / (±5.86)\n\n0.74 / 0.09 (±0.05) / (±0.02)\n\n14.50 / 9.00 (±0.71) / (±0.00)\n\n0.68 / 0.11 (±0.03) / (±0.02)\n\n32.40 / 22.40 (±1.52) / (±2.41)\n\n0.61 / 0.15 (±0.06) / (±0.05)\n\n32.40 / 29.20 (±1.14) / (±5.72)\n\n0.70 / 0.21 (±0.05) / (±0.03)\n\n34.00 / 16.40 (±1.73) / (±4.83)\n\nDisc.\n\n56.25% (±1.56)\n\n56.42% (±0.40)\n\n63.06% (±2.95)\n\n68.62% (±6.84)\n\n77.60% (±12.68)\n\n60.67% (±2.57)\n\n73.34% (±13.32)\n\n67.63% (±1.18)\n\n70.14% (±3.01)\n\n83.75% (±9.15)\n\n65.70% (±1.32)\n\n70.71% (±2.49)\n\n73.62% (±1.07)\n\n74.22% (±1.85)\n\n85.85% (±0.40)\n\nG ADDITIONAL EXPERIMENTAL RESULTS AND ABLATION STUDIES\n\nIn this section, we list the detailed results of the experiments in Section 5, including the standard deviations over multiple seeds. We further provide results on the metric for predicting intervention outcomes, as described in Appendix F.3. Moreover, we present ablation studies on the Voronoi benchmark to further investigate the limitations of iCITRIS. Finally, we include a visualization of the predicted graph by all models on the Instantaneous Temporal Causal3DIdent dataset and the Causal Pinball environment.\n\nG.1 VORONOI BENCHMARK\n\nThe full experimental results for the Voronoi benchmark can be found in Table 5. Compared to the results in Figure 2, we also show the results of the discriminator that is trained on newly generated samples from the models. It is apparent that a crucial factor for simulating the true intervention distributions is to have low entanglement across other factors (R2 sep). Both iVAE and especially iVAE-AR have a strong entanglement between factors, and show a significant gap between the true distribution and their modeled ones. For instance, on the random graphs of size 9, 90% of the samples can be correctly classified from iVAE-AR, indicating that the distributions do not overlap much. In comparison, iCITRIS achieves close-to optimal scores on the small graphs with 55% accuracy only. Note that 50% is already random performance, i.e., the optimum that could be achieved. Still, with larger graphs, we see that the performance also goes down for iCITRIS, although it still outperforms all baselines.\n\nFurthermore, to show the specific failure types of the different models on graph prediction, we additionally list the recall and precision of the graph prediction (instantaneous - Table 6, temporal - Table 7). A high recall (max 1.00) reflects that the models are able to recover all edges, while a high precision (max 1.00) shows that the model do not overpredict false positive edges. One key characteristic on all models is that they tend to have a higher recall than precision for the temporal\n\n46\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Experimental results of the large-scale study on the Voronoi dataset for predicting the instantaneous graph, including the recall and precision to highlight false negative and positive predictions.\n\nModel\n\n#variables\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\n4\n\n6\n\n9\n\nRandom recall\n\nGraph structure Chain\n\nprecision\n\nSHD\n\nrecall\n\nprecision\n\nSHD\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.60 (±0.55)\n\n0.57 (±0.43)\n\n0.33 (±0.47)\n\n0.98 (±0.04)\n\n0.92 (±0.13)\n\n0.48 (±0.20)\n\n0.36 (±0.15)\n\n0.19 (±0.11)\n\n1.00 (±0.00)\n\n0.96 (±0.06)\n\n0.25 (±0.20)\n\n0.27 (±0.13)\n\n0.13 (±0.08)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.55 (±0.51)\n\n0.75 (±0.43)\n\n0.40 (±0.55)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.70 (±0.28)\n\n0.70 (±0.30)\n\n0.26 (±0.19)\n\n0.99 (±0.03)\n\n0.96 (±0.10)\n\n0.68 (±0.30)\n\n0.68 (±0.22)\n\n0.30 (±0.20)\n\n0.00 (±0.00)\n\n1.40 (±1.67)\n\n2.00 (±1.58)\n\n2.80 (±1.92)\n\n3.20 (±1.92)\n\n0.00 (±0.00)\n\n0.80 (±1.30)\n\n2.80 (±2.05)\n\n3.40 (±1.14)\n\n4.60 (±0.89)\n\n0.00 (±0.00)\n\n0.40 (±0.89)\n\n3.00 (±1.22)\n\n4.80 (±1.48)\n\n7.80 (±3.56)\n\n1.00 (±0.00)\n\n0.73 (±0.28)\n\n0.73 (±0.28)\n\n0.47 (±0.45)\n\n0.40 (±0.28)\n\n1.00 (±0.00)\n\n0.92 (±0.11)\n\n0.64 (±0.26)\n\n0.56 (±0.30)\n\n0.44 (±0.26)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.82 (±0.14)\n\n0.55 (±0.19)\n\n0.33 (±0.14)\n\n1.00 (±0.00)\n\n0.80 (±0.30)\n\n0.62 (±0.26)\n\n0.38 (±0.41)\n\n0.58 (±0.43)\n\n1.00 (±0.00)\n\n0.93 (±0.15)\n\n0.80 (±0.25)\n\n0.66 (±0.25)\n\n0.60 (±0.25)\n\n1.00 (±0.00)\n\n0.96 (±0.09)\n\n0.81 (±0.08)\n\n0.66 (±0.20)\n\n0.55 (±0.34)\n\n0.00 (±0.00)\n\n0.00 (±0.00)\n\n2.80 (±0.84)\n\n3.60 (±1.34)\n\n4.60 (±1.14)\n\n0.80 (±1.10)\n\n4.75 (±5.91)\n\n11.20 (±2.59)\n\n12.00 (±1.87)\n\n12.60 (±0.55)\n\n1.20 (±2.17)\n\n14.50 (±0.71)\n\n32.40 (±1.52)\n\n32.40 (±1.14)\n\n34.00 (±1.73)\n\nSHD\n\n0.00 (±0.00)\n\n0.00 (±0.00)\n\n1.00 (±1.22)\n\n1.00 (±0.71)\n\n1.80 (±1.64)\n\n0.20 (±0.45)\n\n0.80 (±1.30)\n\n5.00 (±3.54)\n\n5.40 (±3.36)\n\n8.20 (±1.30)\n\n0.20 (±0.45)\n\n1.40 (±2.61)\n\n14.00 (±4.47)\n\n13.60 (±2.88)\n\n17.60 (±2.97)\n\nFull recall\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.53 (±0.14)\n\n0.40 (±0.22)\n\n0.23 (±0.19)\n\n0.95 (±0.07)\n\n0.68 (±0.39)\n\n0.25 (±0.17)\n\n0.20 (±0.12)\n\n0.16 (±0.04)\n\n0.97 (±0.06)\n\n0.60 (±0.02)\n\n0.10 (±0.04)\n\n0.10 (±0.03)\n\n0.06 (±0.05)\n\nprecision\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.90 (±0.22)\n\n0.95 (±0.11)\n\n0.42 (±0.40)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.95 (±0.11)\n\n0.85 (±0.15)\n\n0.65 (±0.23)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.96 (±0.09)\n\n0.35 (±0.15)\n\nTable 7: Experimental results of the large-scale study on the Voronoi dataset for predicting the temporal graph, including the recall and precision to highlight false negative and positive predictions.\n\nModel\n\n#variables\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\n4\n\n6\n\n9\n\nRandom recall\n\nGraph structure Chain\n\nprecision\n\nSHD\n\nrecall\n\nprecision\n\nSHD\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.87 (±0.22)\n\n0.89 (±0.18)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.94 (±0.06)\n\n0.79 (±0.21)\n\n0.85 (±0.21)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.84 (±0.15)\n\n0.82 (±0.15)\n\n0.89 (±0.17)\n\n1.00 (±0.00)\n\n0.94 (±0.09)\n\n0.70 (±0.14)\n\n0.63 (±0.24)\n\n0.69 (±0.20)\n\n0.92 (±0.10)\n\n0.79 (±0.18)\n\n0.50 (±0.04)\n\n0.42 (±0.09)\n\n0.55 (±0.12)\n\n0.94 (±0.06)\n\n0.92 (±0.19)\n\n0.55 (±0.09)\n\n0.48 (±0.08)\n\n0.51 (±0.04)\n\n0.20 (±0.45)\n\n1.60 (±2.07)\n\n4.20 (±2.17)\n\n6.40 (±1.52)\n\n2.40 (±1.67)\n\n0.20 (±0.45)\n\n1.00 (±2.24)\n\n9.80 (±2.77)\n\n13.20 (±3.19)\n\n8.20 (±2.86)\n\n0.00 (±0.00)\n\n0.20 (±0.45)\n\n13.00 (±1.58)\n\n27.60 (±5.27)\n\n16.00 (±9.25)\n\n1.00 (±0.00)\n\n0.93 (±0.15)\n\n1.00 (±0.00)\n\n0.85 (±0.20)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.92 (±0.17)\n\n0.91 (±0.13)\n\n0.98 (±0.03)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.99 (±0.02)\n\n0.85 (±0.18)\n\n0.97 (±0.06)\n\n0.81 (±0.21)\n\n0.60 (±0.13)\n\n0.48 (±0.10)\n\n0.73 (±0.15)\n\n0.98 (±0.04)\n\n0.93 (±0.15)\n\n0.52 (±0.12)\n\n0.44 (±0.13)\n\n0.56 (±0.09)\n\n1.00 (±0.00)\n\n0.99 (±0.02)\n\n0.62 (±0.04)\n\n0.44 (±0.07)\n\n0.60 (±0.17)\n\n0.60 (±0.89)\n\n3.20 (±2.39)\n\n5.80 (±1.64)\n\n6.80 (±1.79)\n\n3.60 (±1.95)\n\n4.20 (±4.38)\n\n7.75 (±4.57)\n\n12.40 (±2.70)\n\n17.20 (±2.59)\n\n9.00 (±4.06)\n\n9.60 (±5.86)\n\n9.00 (±0.00)\n\n22.40 (±2.41)\n\n29.20 (±5.72)\n\n16.40 (±4.83)\n\nSHD\n\n0.00 (±0.00)\n\n0.40 (±0.55)\n\n2.60 (±1.52)\n\n4.20 (±3.63)\n\n3.00 (±2.35)\n\n1.00 (±1.22)\n\n3.40 (±3.51)\n\n11.20 (±2.59)\n\n14.60 (±4.10)\n\n9.20 (±3.49)\n\n1.20 (±1.10)\n\n2.40 (±5.37)\n\n18.40 (±5.90)\n\n24.00 (±3.81)\n\n21.00 (±7.28)\n\nFull recall\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n1.00 (±0.00)\n\n0.63 (±0.41)\n\n1.00 (±0.00)\n\n0.98 (±0.04)\n\n1.00 (±0.00)\n\n0.98 (±0.04)\n\n0.87 (±0.19)\n\n0.90 (±0.18)\n\n1.00 (±0.00)\n\n0.95 (±0.00)\n\n0.92 (±0.08)\n\n0.86 (±0.16)\n\n0.91 (±0.06)\n\nprecision\n\n0.92 (±0.11)\n\n0.68 (±0.21)\n\n0.51 (±0.08)\n\n0.39 (±0.23)\n\n0.64 (±0.15)\n\n0.75 (±0.19)\n\n0.59 (±0.21)\n\n0.45 (±0.11)\n\n0.36 (±0.07)\n\n0.54 (±0.10)\n\n0.71 (±0.15)\n\n0.72 (±0.02)\n\n0.48 (±0.03)\n\n0.41 (±0.05)\n\n0.57 (±0.07)\n\ngraph. In other words, many mistakes are due to predicting too many edges, which easily occurs when causal variables are entangled. However, on the instantaneous graphs, we clearly see that the baselines, CITRIS and iVAE, predict a sparse graph by having a low recall. This also underlines that\n\n47\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Results of three ablation studies on the Voronoi benchmark, performed on the random graph with 6 variables. Left (Intervention noise): We introduce noise on the intervention targets by introducing 10% false positive cases in the intervention targets, i.e., I t i = 1 although Ci is sampled from the observation distribution. iCITRIS-ENCO performs almost as well as before. Middle (No instantaneous): We apply all methods on graphs with an empty instantaneous graph. iCITRISENCO obtains almost perfect disentanglement along with CITRIS and iVAE, showing that iCITRIS can be used as a replacement of them under perfect interventions. Right (No temporal): The most difficult setup is when the variables have purely instantaneous relations and samples between time steps are independent. In this case, no method can disentangle the variables well, showing that different optimization strategies than iCITRIS are needed in this setting.\n\nModel\n\nIntervention noise R2\n\nSHD\n\nAblation study No instantaneous R2\n\nSHD\n\nNo temporal\n\nSHD\n\nR2\n\niCITRIS-ENCO\n\n0.96 / 0.00 (±0.01) / (±0.00)\n\n0.20 / 4.00 (±0.45) / (±1.87)\n\n0.99 / 0.00 (±0.00) / (±0.00)\n\n0.00 / 0.00 (±0.00) / (±0.00)\n\n0.55 / 0.21 (±0.08) / (±0.06)\n\n6.80 / 0.00 (±2.39) / (±0.00)\n\nCITRIS\n\niVAE\n\niVAE-AR\n\n0.78 / 0.14 (±0.09) / (±0.09)\n\n5.00 / 10.60 (±4.58) / (±4.51)\n\n0.99 / 0.00 (±0.00) / (±0.00)\n\n0.00 / 0.00 (±0.00) / (±0.00)\n\n0.54 / 0.21 (±0.06) / (±0.06)\n\n6.80 / 0.00 (±2.39) / (±0.00)\n\n0.69 / 0.22 (±0.10) / (±0.11)\n\n5.40 / 15.00 (±3.36) / (±3.39)\n\n0.99 / 0.00 (±0.00) / (±0.00)\n\n0.00 / 0.00 (±0.00) / (±0.00)\n\n0.49 / 0.20 (±0.05) / (±0.02)\n\n7.00 / 0.00 (±2.24) / (±0.00)\n\n0.84 / 0.22 (±0.10) / (±0.06)\n\n7.40 / 8.60 (±2.07) / (±3.44)\n\n0.79 / 0.23 (±0.13) / (±0.06)\n\n2.80 / 9.00 (±0.45) / (±4.42)\n\n0.54 / 0.28 (±0.05) / (±0.02)\n\n7.80 / 0.00 (±3.27) / (±0.00)\n\nthe false positive edges in the temporal graph cannot be simply removed by increasing the sparsity regularizer in the causal discovery method, since otherwise, even more edges would be lost in the instantaneous graph. iVAE-AR, on the other hand, has a low precision and recall on the instantaneous graphs; showcasing that it predicts a very different graph with anticausal edges. Meanwhile, only iCITRIS-ENCO obtains a higher recall and precision across the different graph structures and sizes.\n\nNext, we look at ablation studies that investigate the applications and limitations of iCITRIS.\n\nG.1.1 ABLATION 1: NOISY INTERVENTION TARGETS\n\nIn the first ablation study, we focus on the dependency of iCITRIS on accurate intervention targets. In practice, performing perfect interventions is a difficult task, and is prone to noise. While we can easily observe whether we pushed a button or did external actions to influence a dynamical system, we do not know for sure whether the intervention succeeded or not. This corresponds to a case where the intervention targets, I t, are noisy and tend to have false positives, i.e., I t i = 1 although the intervention did not succeed. How sensitive is iCITRIS to such noise?\n\nTo investigate this question, we repeat the experiments of the Voronoi benchmark on the random graphs of size 6, but simulate that in 10% of the cases when I t i , we actually do not intervene on Ci and instead sample the value from its observational distribution. The results are summarized in Table 8 (left two columns), and clearly show that iCITRIS can yet work well in this setting. The variables are almost as well as before disentangled as before. The additional temporal variables are partially also because of noisy interventions in the post-processing causal discovery setting.\n\nG.1.2 ABLATION 2: EMPTY INSTANTANEOUS GRAPH\n\nThe main aspect of iCITRIS in contrast to the baselines is that supports instantaneous effects. However, in practice, we might not know whether instantaneous effects are in the data or not. Thus, this ablation study investigates, whether iCITRIS can yet be used as a replacement of the baselines like CITRIS, when perfect interventions are provided. For this, we repeat the experiments of the Voronoi benchmark on causal models with an empty instantaneous of size 6. As the results in Table 8 show, iCITRIS, CITRIS, and iVAE all are able to identify the causal variables and the graph. This show that iCITRIS can indeed be used as a replacement of CITRIS and iVAE, even in the setting that the variables are independent, conditioned on the previous time step.\n\n48\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Experimental results on the Instantaneous Temporal Causal3D dataset, with standard deviations across three seeds.\n\nModel\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nR2\n\nSpearman\n\nTriplets\n\nSHD (Instant)\n\nSHD (Temp)\n\n0.96 / 0.07 (±0.00) / (±0.01)\n\n0.96 / 0.12 (±0.00) / (±0.01)\n\n0.95 / 0.10 (±0.01) / (±0.02)\n\n0.95 / 0.14 (±0.01) / (±0.01)\n\n0.90 / 0.23 (±0.01) / (±0.02)\n\n0.89 / 0.26 (±0.01) / (±0.02)\n\n0.79 / 0.24 (±0.02) / (±0.04)\n\n0.76 / 0.24 (±0.03) / (±0.02)\n\n0.74 / 0.29 (±0.02) / (±0.03)\n\n0.72 / 0.36 (±0.04) / (±0.06)\n\n0.11 (±0.01)\n\n0.15 (±0.02)\n\n0.20 (±0.01)\n\n0.27 (±0.00)\n\n0.31 (±0.00)\n\n1.67 (±0.58)\n\n4.33 (±1.15)\n\n5.67 (±0.58)\n\n6.00 (±1.00)\n\n10.67 (±0.58)\n\n5.67 (±1.15)\n\n6.33 (±1.15)\n\n12.67 (±0.58)\n\n15.00 (±1.00)\n\n12.33 (±4.51)\n\nG.1.3 ABLATION 3: EMPTY TEMPORAL GRAPH\n\nAs a final ablation study, we consider the most difficult setup, namely having no temporal relations at all. In this case, all relations between causal variables are purely instantaneous, and we cannot use any information of the previous time step, i.e., zt, as an initial guidance for disentangling the variables. Once more, we repeat the experiments of the Voronoi benchmark on the random graphs of size 6, but with an empty instantaneous graph, and summarize the results in Table 8 (right columns). Due to the difficulty of the task, none of the methods was able to identify the causal variables. Since the probabilities of the edges in iCITRIS are initially around 0.5, the model focuses on finding K independent factors of variations instead of the causal variables. The balance that is crucial for the temporal setup is that the knowledge of the interventions and previous time step is more important than the instantaneous effects for some variables, which, in this case, does not hold. Hence, to overcome this problem, different optimization strategies than the ones discussed in Section 4.3 are needed. Interestingly, even the autoregressive iVAE fails at going beyond finding K independent factors, underlining the difficulty of the task.\n\nG.2\n\nINSTANTANEOUS TEMPORAL CAUSAL3DIDENT\n\nWe report the full experimental results for the Instantaneous Temporal Causal3DIdent, including standard deviations across three seeds for all models, are shown in Table 9. Next to the correlation and graph prediction metrics, we also list the results of the triplet evaluation, following Lippe et al. (2022b). The triplet distance measures how well we can perform combinations of causal factors in latent space without causing correlations among different factors.\n\nG.2.1 ABLATION STUDY 4: ORIGINAL TEMPORAL CAUSAL3DIDENT WITHOUT\n\nINSTANTANEOUS EFFECT\n\nTo verify that iCITRIS can be used as a replacement to CITRIS, even in environments where no instantaneous effects are present, we apply iCITRIS to the original Temporal Causal3DIdent dataset. This dataset has the same causal variables and relations, but instead of instantaneous effects, all effects are over temporal time steps. The results are shown in Table 10. iCITRIS achieves almost identical results to CITRIS, verifying that iCITRIS generalizes CITRIS. In terms of the causal graph prediction, iCITRIS occasionally predicted an instantaneous edge between the object shape and the object rotation; an edge that CITRIS incorrectly predicted over time. This is to be expected due to the visual complexity of the dataset.\n\nG.2.2 ABLATION STUDY 5: PERFECT INTERVENTIONS IN INSTANTANEOUS TEMPORAL\n\nCAUSAL3DIDENT\n\nAs an ablation to the shown dataset, we conduct an experiment on the Causal3DIdent dataset, where all interventions are perfect. The experimental results for this dataset, including standard deviations across three seeds for all models, are shown in Table 11. We additionally show the discriminator\n\n49\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: Experimental results on the original Temporal Causal3DIdent dataset. Results for CITRIS and iVAE are taken from Lippe et al. (2022b). iCITRIS performs on par with CITRIS, despite not assuming an empty instantaneous causal graph.\n\nModel\n\nR2\n\nSpearman\n\nTriplets\n\niCITRIS-ENCO 0.97 / 0.04 0.98 / 0.04 CITRIS 0.80 / 0.29 iVAE\n\n0.97 / 0.09 0.97 / 0.08 0.77 / 0.28\n\n0.08 0.07 0.27\n\nTable 11: Experimental results on the Instantaneous Temporal Causal3D dataset, with standard deviations across three seeds. The results for iCITRIS-ENCO are shown with and without using the Mutual Information Estimator.\n\nModel\n\niCITRIS-ENCO\n\niCITRIS-ENCO - No MI\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nR2\n\nSpearman\n\nTriplets\n\nSHD (Instant)\n\nSHD (Temp) Disc. Acc.\n\n0.96 / 0.05 (±0.00) / (±0.00)\n\n0.96 / 0.10 (±0.00) / (±0.00)\n\n0.96 / 0.15 (±0.00) / (±0.00)\n\n0.95 / 0.21 (±0.01) / (±0.01)\n\n0.95 / 0.09 (±0.00) / (±0.01)\n\n0.95 / 0.14 (±0.01) / (±0.01)\n\n0.92 / 0.19 (±0.01) / (±0.02)\n\n0.90 / 0.22 (±0.01) / (±0.01)\n\n0.82 / 0.20 (±0.02) / (±0.01)\n\n0.80 / 0.22 (±0.02) / (±0.01)\n\n0.79 / 0.29 (±0.01) / (±0.03)\n\n0.78 / 0.33 (±0.02) / (±0.00)\n\n0.09 (±0.00)\n\n0.13 (±0.00)\n\n0.14 (±0.00)\n\n0.19 (±0.01)\n\n0.27 (±0.01)\n\n0.27 (±0.01)\n\n1.33 (±1.15)\n\n3.00 (±1.00)\n\n4.00 (±1.00)\n\n4.67 (±0.58)\n\n6.67 (±2.52)\n\n11.00 (±1.00)\n\n5.00 (±1.73)\n\n8.33 (±2.00)\n\n5.00 (±1.73)\n\n10.00 (±2.00)\n\n15.33 (±1.53)\n\n12.67 (±1.53)\n\n55.41% (±0.87%)\n\n60.85% (±2.34%)\n\n61.05% (±4.43%)\n\n67.51% (±5.90%)\n\n86.87% (±2.71%)\n\n87.07% (±0.90%)\n\naccuracy of distinguishing between true and fake interventional samples. The results indicate that while this makes the task in general a bit easier, iCITRIS-ENCO still performs the best. The small differences in entanglement between iCITRIS-ENCO and iCITRIS-NOTEARS lead to considerable difference for generating new combinations of causal factors, highlighting the importance of strong disentanglement between causal factors. Similarly, the discriminator accuracy shows that iCITRISENCO can accurately model the distribution of the true causal model, while clear differences to the VAE-based baseline, iVAE and iVAE-AR, are visible.\n\nTo get an intuition on what graphs the different models identify, we have visualized on example of each model in Figure 18. In general, we see that iCITRIS-ENCO misses only one edge which is sparse anyway, since hue_b affects hue_o only for two shapes. Similarly to the results of Lippe et al. (2022b), we find that the object shape is a false positive parent of the rotation of the object. For CITRIS, we see that it starts to predict incorrect orientations due to correlations among factors. Finally, iVAE and iVAE-AR predict graphs that have little in common with the ground truth one.\n\nTo show the importance of the mutual information estimator in iCITRIS, we experiment with iCITRIS but without the MI estimator. The results in Table 11 show that the MI estimator is indeed a crucial performance to reach iCITRIS’s strong performance. Without the MI estimator, we experience higher correlations between different latent representations and causal variables. In the end, this also leads to a worsened graph estimation for both instantaneous and temporal effects.\n\nG.3 CAUSAL PINBALL\n\nFinally, the full experimental results for the Causal Pinball environment can be found in Table 12. Besides the correlation and graph metrics, we again report the triplet evaluation, which shows once more that iCITRIS and CITRIS both work well here.\n\nG.3.1 ABLATION STUDY 6: PERFECT INTERVENTIONS IN CAUSAL PINBALL\n\nAs an ablation study, we simulate a Causal Pinball environment where all partially-perfect interventions are replaced by perfect interventions. With this, we can make use of the Mutual Information es-\n\n50\n\nPublished as a conference paper at ICLR 2023\n\nhue_b\n\nhue_s\n\nhue_b\n\nhue_s\n\nrot_s\n\npos_o\n\nrot_o\n\nrot_s\n\npos_o\n\nrot_o\n\nhue_o\n\nobj_s\n\nhue_o\n\nobj_s\n\n(a) Original ground truth\n\n(b) iCITRIS-ENCO\n\nhue_b\n\nhue_s\n\nhue_b\n\nhue_s\n\nrot_s\n\npos_o\n\nrot_o\n\nrot_s\n\npos_o\n\nrot_o\n\nhue_o\n\nobj_s\n\nhue_o\n\nobj_s\n\n(c) iCITRIS-NOTEARS\n\n(d) CITRIS\n\nhue_b\n\nhue_s\n\nrot_s\n\npos_o\n\nrot_o\n\nrot_s\n\npos_o\n\nrot_o\n\nhue_o\n\nobj_s\n\nhue_b\n\nhue_s\n\nhue_o\n\nobj_s\n\n(e) iVAE\n\n(f) iVAE-AR\n\nFigure 18: Learned instantaneous graphs in the Instantaneous Temporal Causal3DIdent dataset for all five models for a single seed. Red arrows indicate false positive edges, and dashed red arrows false negatives. (a) The ground truth of the dataset. (b) iCITRIS-ENCO achieves for one score a perfect recovery of the graph, and for the other two graphs, we miss one edge to hue_o since hue_b only affects it for certain object shapes, and have an additional from the object shape to the rotation due to the complexity of the problem. (c) iCITRIS-NOTEARS had more false positive and negative edges than iCITRIS-ENCO. However, all orientations were correct. (d) CITRIS had in general a sparser graph than the true graph, but in contrast to iCITRIS, also obtained wrong orientations several times (e.g. between pos_o and rot_o). (e) The iVAE obtains very different graphs from the ground truth, with many incorrectly edges. (f) Due to the autoregressive prior in iVAE-AR, we observed a significant amount of false positive edges, with occasional incorrect orientation as well.\n\nTable 12: Experimental results on the Causal Pinball dataset over three seeds.\n\nModel\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nR2\n\nSpearman\n\nTriplets\n\nSHD (Instant)\n\nSHD (Temp)\n\n0.99 / 0.12 (±0.00) / (±0.01)\n\n0.99 / 0.25 (±0.00) / (±0.02)\n\n0.98 / 0.18 (±0.00) / (±0.01)\n\n0.99 / 0.38 (±0.00) / (±0.03)\n\n0.90 / 0.39 (±0.06) / (±0.07)\n\n0.95 / 0.50 (±0.01) / (±0.08)\n\n0.44 / 0.05 (±0.09) / (±0.00)\n\n0.47 / 0.05 (±0.10) / (±0.01)\n\n0.47 / 0.15 (±0.10) / (±0.04)\n\n0.52 / 0.40 (±0.11) / (±0.16)\n\n0.03 (±0.01)\n\n0.18 (±0.03)\n\n0.11 (±0.01)\n\n0.61 (±0.04)\n\n0.63 (±0.04)\n\n0.67 (±1.15)\n\n3.33 (±1.52)\n\n3.00 (±1.00)\n\n4.33 (±0.58)\n\n8.00 (±2.00)\n\n3.00 (±0.71)\n\n4.67 (±0.58)\n\n7.67 (±2.31)\n\n4.67 (±0.58)\n\n3.67 (±1.53)\n\ntimator in iCITRIS. The results on this dataset are shown in Table 13. Besides identifying the causal variables well, iCITRIS-ENCO identifies the instantaneous causal graph with minor errors. Interestingly, CITRIS obtains a good correlation score as well here. This is likely due to the instantaneous effects being very sparse, and perfect interventions giving a very strong preference towards independent variables in this case. Yet, there is still a gap between iCITRIS-ENCO and CITRIS in the instantaneous SHD, showing the benefit of learning the instantaneous graph jointly with the causal variables.\n\n51\n\nPublished as a conference paper at ICLR 2023\n\nTable 13: Perfect interventions in the Causal Pinball dataset. Experimental results are shown over three seeds.\n\nModel\n\niCITRIS-ENCO\n\niCITRIS-NOTEARS\n\nCITRIS\n\niVAE\n\niVAE-AR\n\nR2\n\nSpearman\n\nTriplets\n\nSHD (Instant)\n\nSHD (Temp)\n\n0.98 / 0.04 (±0.00) / (±0.01)\n\n0.99 / 0.17 (±0.00) / (±0.03)\n\n0.98 / 0.06 (±0.00) / (±0.04)\n\n0.99 / 0.19 (±0.00) / (±0.06)\n\n0.98 / 0.04 (±0.01) / (±0.01)\n\n0.99 / 0.18 (±0.00) / (±0.02)\n\n0.55 / 0.04 (±0.08) / (±0.03)\n\n0.58 / 0.14 (±0.09) / (±0.06)\n\n0.53 / 0.15 (±0.08) / (±0.09)\n\n0.55 / 0.30 (±0.09) / (±0.08)\n\n0.02 (±0.00)\n\n0.02 (±0.00)\n\n0.02 (±0.00)\n\n0.55 (±0.06)\n\n0.56 (±0.06)\n\n0.67 (±0.58)\n\n2.33 (±0.58)\n\n2.67 (±1.53)\n\n2.33 (±0.58)\n\n4.33 (±1.53)\n\n3.67 (±1.15)\n\n3.67 (±0.58)\n\n4.00 (±1.00)\n\n4.33 (±1.15)\n\n6.33 (±1.53)\n\npaddle_left\n\npaddle_right\n\npaddle_left\n\npaddle_right\n\npaddle_left\n\npaddle_right\n\npaddle_left\n\nball\n\nbumpers\n\nscore\n\nball\n\nbumpers\n\nscore\n\npaddle_right\n\n(a) Original ground truth\n\n(b) iCITRIS-ENCO\n\nball\n\nbumpers\n\nscore\n\nball\n\nbumpers\n\nscore\n\npaddle_left\n\n(c) iCITRIS-NOTEARS\n\npaddle_right\n\npaddle_left\n\n(d) CITRIS\n\nball\n\nbumpers\n\nscore\n\nball\n\nbumpers\n\nscore\n\n(e) iVAE\n\n(f) iVAE-AR\n\npaddle_right\n\nFigure 19: Learned instantaneous graphs in the Causal Pinball dataset for all five models for a single seed. Red arrows indicate false positive edges, and dashed red arrows false negatives. (a) The ground truth of the dataset. (b) iCITRIS-ENCO recovered the graph for one seed perfectly, and for the other two seeds, incorrectly oriented an edge between the ball and paddles. (c) iCITRIS-NOTEARS commonly has some incorrect orientations between the paddles and the ball. (d) CITRIS, similar to other experiments, tends to have a sparser instantaneous graph. (e) iVAE has a sparser graph, similar to CITRIS, but with additional false positive edges. (f) iVAE-AR predicts a causal graph that has no edge in common with the true graph.\n\nFurther, we visualize the predicted causal graphs of the different methods. In general, we found that the most difficult relations are between the paddles and the ball, in particular their orientation. This is due to the deterministic relations between the two factors, such that if the ball has been hit by the paddle, we can already predict it just from the ball position. Further, in many states, the ball and paddle do not affect each other, such that a state where the paddle would have hit the ball, but the ball was intervened upon in the same time step, is extremely rare. Overall, all models suffered from this problem, but iCITRIS showed to handle it.\n\n52",
  "translations": [
    "# Summary Of The Paper\n\nThe paper aims at learning the causal representation from time-series data with the access of interventions. The formulation is a state-space model, which the causal process is unobserved and the observation is a funtion of the unobserved causal process. It generalizes CITRIS (Lippe et al., 2022b) to include instantaneous effects in the underlying causal graph. To identify the unique representation, the paper requires the access to the intervensions on each latent variable. The causal graph and the causal representation (or variables) can be identified with the access of the interventions by maximizing likelihood, maximizing the information content, and minimizing the edges of a causal graph.\n\n# Strength And Weaknesses\n\nThe strength:\n\n(+) The work spends a lot of effort on the proof of identifiability.\n\n(+) The problem is important: the generalization of CITRIS (Lippe et al., 2022b) in the presence of instantaneous effects.\n\n(-) The identifiability relies on the access to the interventions on each causal variable, which can not be easy to get in practice. This in turn limits the contribution of the work by the setting of the solution.\n\n(-) The paper claims to identify the causal representation; however, the experiments didn't show what is the learned causal representation. Perhaps, besides showing the performance of causal discovery in the setting, it can be helpful to explain the causal representations. If there is any in the appendix, it would be good to include part of the analysis in the main content. Moreover, it would be more convincing to include a\n\n# Clarity, Quality, Novelty And Reproducibility\n\n(-) The paper is a bit difficult to read and it is not clear in the beginning what and why are causal variables and causal representation. \n\n(-) The problem is important; whereas, the proposed solution relies on access to the intervention of each causal variable. It becomes less significant and interesting with the proposal. (I do reckon the effort on the proof.) Maybe try to justify how reasonable the setting is.\n\n(-) It is not clear to me how the causal variables are determined. How the number of latent variables is determined and how the invertible function is constructed when the dimensions of the observation and the latent spaces are different?\n\n# Summary Of The Review\n\nMy main concern is the assumption of access to the interventions and the significance of the contributions can be limited. Moreover, it would be more convincing with real-world data experiments.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces iCITRIS, a novel causal representation learning framework designed to handle instantaneous effects in temporal sequences, which is a limitation of existing methods. The authors provide a comprehensive methodology that combines differentiable causal discovery techniques with a variational autoencoder architecture, extending the CITRIS framework to incorporate instantaneous causal relationships. Empirical evaluations across three diverse datasets demonstrate that iCITRIS consistently outperforms baseline models in accurately identifying causal variables and their causal graphs, showing robustness to different experimental conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing instantaneous causal effects, which are often overlooked in causal representation learning. The theoretical contributions, particularly the identifiability results and the novel definition of minimal causal variables, are well-articulated and add significant value to the field. However, the paper could benefit from a more extensive discussion on the limitations of the proposed method, especially regarding the assumptions made about interventions and the applicability of the model to more complex real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow from the introduction through to the experimental results. The quality of the writing is high, making it accessible to readers with varying levels of expertise in causal inference. The novelty of the proposed approach is significant, as it addresses a critical gap in existing literature. The reproducibility of the results is supported by detailed descriptions of the experiments and methods used; however, inclusion of code or supplementary materials would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to causal representation learning that effectively incorporates instantaneous effects. While the contributions are substantial and the experimental results are robust, a more thorough discussion on limitations and practical applicability would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel method for causal representation learning called iCITRIS, which focuses on identifying causal variables and their relationships from high-dimensional observations, particularly in contexts with instantaneous effects. The methodology includes theoretical advancements that generalize identifiability results, highlighting the necessity of interventions to accurately identify causal variables. The experimental results demonstrate that iCITRIS outperforms existing models across multiple datasets, including synthetic benchmarks and a real-world game scenario, confirming its effectiveness in identifying causal structures under varying complexities.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing a significant gap in causal representation learning, specifically the incorporation of instantaneous effects. The robust experimental design, which utilizes diverse datasets and comprehensive evaluation metrics, further validates the proposed method. However, the paper is limited by its reliance on certain assumptions regarding the availability of interventions and the challenges it faces in non-temporal settings. Additionally, while the results are promising, the generalizability of the method to more complex, real-world scenarios remains uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The quality of writing and presentation is high, facilitating understanding of complex concepts. The novelty of the approach is significant, particularly in the context of instantaneous effects, although some assumptions may hinder reproducibility in practical applications. The experiments are detailed and provide a strong foundation for future research, but the need for robust intervention data is a critical consideration for reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in causal representation learning through the introduction of iCITRIS, effectively addressing instantaneous effects. While the methodology is innovative and empirically validated across multiple datasets, the reliance on specific assumptions regarding interventions may limit its applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents **iCITRIS**, an advanced causal representation learning method that incorporates instantaneous effects, which have been overlooked in previous models. By extending the existing CITRIS framework, iCITRIS enables the identification of multidimensional causal variables and their causal graph using a differentiable causal discovery approach. The authors demonstrate the effectiveness of iCITRIS through experiments on three distinct datasets, showing that it outperforms baseline models in accurately identifying causal relationships and structures.\n\n# Strength And Weaknesses\nThe paper’s primary strength lies in its innovative approach to integrating instantaneous causal effects into causal representation learning, addressing a significant gap in the existing literature. The use of partially-perfect interventions to enhance identifiability is particularly noteworthy. Moreover, the empirical results are robust, demonstrating iCITRIS's superiority over existing methods like CITRIS and iVAE across various datasets. However, the reliance on specific types of interventions for identifiability may limit the generalizability of the findings. Additionally, while the methodology is theoretically sound, the complexity of the approach may pose challenges for practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical progression from theoretical foundations to empirical validation. The quality of the writing is high, making complex concepts accessible. However, the paper could benefit from more detailed explanations regarding the implementation of the proposed method and the experimental setup to enhance reproducibility. The novelty introduced by iCITRIS in capturing instantaneous effects is significant, setting a new direction for future research in causal representation learning.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of causal representation learning by addressing the critical issue of instantaneous causal effects. While the method shows strong empirical performance and innovative theoretical foundations, its reliance on specific interventions may restrict broader applicability. The clarity and quality of the presentation are commendable, though improvements in reproducibility are needed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents iCITRIS, a novel method for causal representation learning that specifically addresses the challenges associated with instantaneous effects in causal graphs. The authors propose a methodology that incorporates partially-perfect interventions to enhance causal variable identification. Extensive experiments are conducted on multiple datasets, including synthetic benchmarks, demonstrating the effectiveness of iCITRIS compared to existing models. The findings indicate that iCITRIS significantly improves the disentanglement of causal variables, with theoretical contributions that elucidate the conditions under which causal variables can be identified.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to causal representation learning and strong experimental validation across various datasets. iCITRIS is effectively compared against several baseline models, showcasing its performance improvements. However, the reliance on specific assumptions, such as the availability of partially-perfect interventions, may limit its real-world applicability. The focus on synthetic datasets raises concerns about the generalizability of the results, and while the theoretical contributions are valuable, they are contingent on assumptions that may not always hold in practice.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its concepts clearly, making it accessible for readers, though some sections could benefit from deeper explanations of underlying concepts, especially for those less familiar with causal inference. The methodology and experimental results are presented with sufficient detail to allow reproducibility, although the complexity introduced by the mutual information estimator might pose challenges in practical implementations. The novelty of the approach is noteworthy, contributing significantly to the field of causal learning.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the field of causal representation learning with its innovative method iCITRIS, supported by strong empirical results. However, the assumptions underlying the methodology and the focus on synthetic environments limit its applicability and generalizability to real-world scenarios.\n\n# Correctness\n4 \n\n# Technical Novelty And Significance\n4 \n\n# Empirical Novelty And Significance\n3",
    "## ICLR Review: Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems\n\n### Summary Of The Paper\nThis paper introduces **iCITRIS**, a novel approach to causal representation learning that effectively identifies causal variables and their causal graphs in the presence of both instantaneous and temporal causal effects. The methodology integrates a differentiable causal discovery framework, allowing for the simultaneous learning of causal graphs and variable representations. The findings are validated through experiments on interactive system datasets, demonstrating that iCITRIS outperforms existing state-of-the-art methods, particularly in scenarios where instantaneous effects are relevant.\n\n### Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to incorporating instantaneous causal relationships, which addresses a significant gap in the literature. The theoretical guarantees provided enhance the credibility of the proposed method. Additionally, the practical implications of this research are noteworthy, as the method holds promise for applications in reinforcement learning and dynamic systems. However, the reliance on known interventions may restrict applicability in scenarios lacking such interventions. Furthermore, the complexity of jointly learning graphs and causal variables could pose computational challenges in large-scale applications.\n\n### Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of writing is high, with a thorough review of related work that contextualizes the importance of the proposed method. The novelty of integrating instantaneous effects into causal representation learning is significant, setting it apart from existing approaches. While the theoretical and empirical results are compelling, the reproducibility could be hindered by the computational demands of the proposed framework.\n\n### Summary Of The Review\nOverall, the paper presents a substantial advancement in the field of causal representation learning with the introduction of iCITRIS. The combination of theoretical rigor and empirical validation provides a strong foundation for the claims made, although the method's reliance on known interventions and its computational complexity may limit its broader applicability.\n\n### Correctness\n4/5\n\n### Technical Novelty And Significance\n5/5\n\n### Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a novel adversarial training approach tailored for interactive systems, addressing the unique challenges posed by instantaneous and temporal effects. The methodology, termed iCITRIS, enhances existing adversarial training frameworks by incorporating mechanisms that effectively manage instantaneous effects—often overlooked in prior methods—allowing the model to learn more robustly from adversarial examples. Comprehensive experimental results demonstrate the superiority of iCITRIS in identifying causal relationships and maintaining robustness against adversarial attacks compared to traditional adversarial training approaches, validating its effectiveness across multiple datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the need to integrate instantaneous and temporal effects in adversarial training, which is a significant contribution to the field. The empirical evaluation is thorough, displaying the advantages of iCITRIS across various complex dynamic scenarios, and the inclusion of a graph learning component adds a valuable perspective on variable interactions. However, the paper lacks a discussion on the potential limitations of iCITRIS in highly stochastic environments and does not adequately address the computational complexity associated with its implementation. These gaps could limit the applicability of the proposed method in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents the concepts with clarity, making it accessible to readers. The quality of the experimental design is high, and the findings are reproducible, given the comprehensive nature of the experiments conducted. The novelty of integrating instantaneous effects into adversarial training is significant, contributing to the existing body of knowledge in the field. However, the lack of detailed discussions on potential limitations and computational complexities could hinder reproducibility in certain contexts.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful advancement in adversarial training by effectively addressing instantaneous effects in interactive systems. Its theoretical insights and empirical validation are commendable, though further exploration of limitations and computational aspects would enhance its practicality and applicability in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems\" introduces a framework called iCITRIS, aimed at advancing Causal Representation Learning (CRL) by addressing the identification of causal variables in real-time systems, especially in the presence of instantaneous effects. Key contributions include a methodology for identifying causal variables, a proof of identifiability for minimal causal variables under specific conditions, and experimental validation across three datasets demonstrating the framework's performance. However, the significance of these contributions appears modest, with the authors' claims often overstating the novelty and effectiveness of their approach.\n\n# Strength And Weaknesses\nStrengths of the paper include its systematic approach to causal variable identification and the theoretical insights provided regarding minimal causal variables. The introduction of the iCITRIS framework and its application to interactive systems is also a notable aspect. Conversely, the weaknesses lie in the incremental nature of the contributions, as many of the claims do not represent significant advancements over existing methodologies. The reliance on theoretical assumptions may limit the practical applicability of the findings, and the experimental results, while positive, are often marginal and could be attributed to noise rather than genuine improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its presentation of methodologies and results. However, the novelty of the contributions is questionable, as many elements appear to be extensions of existing work rather than entirely new concepts. The reproducibility of the results may be a concern, given the reliance on specific assumptions that may not hold in broader applications. The experimental setup and datasets used could have been described in greater detail to bolster reproducibility.\n\n# Summary Of The Review\nWhile the paper claims to make significant contributions to the field of causal representation learning, the actual impact of these contributions is limited. The framework iCITRIS provides incremental advancements rather than novel insights, and the empirical results do not convincingly demonstrate superiority over existing methods. Overall, the methodology and claims warrant cautious interpretation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems\" by Phillip Lippe et al. introduces a novel method called iCITRIS, which enhances causal representation learning by effectively handling instantaneous effects in temporal sequences. The methodology involves identifying causal variables and their corresponding causal graphs through temporal observations, addressing a significant gap in existing approaches that overlook instantaneous causal relations. The experimental results demonstrate iCITRIS's superior performance across multiple datasets, outperforming established methods like CITRIS and iVAE, particularly in terms of correlation and structural accuracy of causal graphs.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to incorporating instantaneous effects into causal representation learning, which is an important advancement for real-world applications. The experimental validation across diverse datasets exhibits robustness and strong performance, as evidenced by high R² scores and low Structural Hamming Distances (SHD). However, a potential weakness lies in the limited discussion of the theoretical implications of the findings, particularly regarding the necessity of interventions in identifying causal variables. Additionally, while the performance metrics are impressive, the paper could benefit from a more detailed exploration of the limitations of the iCITRIS method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, as it addresses a previously uncharted territory in causal learning. Reproducibility is supported through detailed descriptions of the datasets and methodologies used, although providing code or a more comprehensive experimental setup would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in causal representation learning through the introduction of iCITRIS, which effectively manages instantaneous effects in temporal sequences. The experimental results validate its effectiveness, though more exploration of theoretical implications and reproducibility could enhance the contribution further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents an extension of the causal inference method known as CITRIS, termed iCITRIS, which aims to identify causal variables in the presence of instantaneous effects. The authors propose a theoretical framework that builds upon the assumptions of conditionally independent causal variables and the availability of partially-perfect interventions. The findings suggest that iCITRIS can effectively identify causal relationships under certain conditions, though it faces limitations due to its reliance on specific assumptions and its performance in noisy environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address a gap in existing causal inference methods by incorporating instantaneous effects, which are critical in many real-world scenarios. However, the paper is hindered by several weaknesses, including a reliance on partially-perfect interventions that may not be feasible in practice, and assumptions such as independence of intervention targets and full support of distributions, which could limit its applicability. Additionally, the acyclic graph assumption restricts the modeling of feedback loops, and the lack of counterfactual analysis diminishes the depth of causal insights.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its methodology with a reasonable level of clarity, although some concepts could benefit from further elaboration. The quality of the empirical results is commendable, yet the generalizability to complex real-world scenarios remains unexamined. The novelty of introducing instantaneous effects into the causal identification framework is significant, but the assumptions made may impact the reproducibility of the results in varied contexts.\n\n# Summary Of The Review\nOverall, the paper provides an interesting contribution to causal inference by addressing instantaneous effects, but it is constrained by critical assumptions that limit its practical applicability. While the theoretical framework shows promise, further evaluation in diverse and complex environments is necessary to fully understand its robustness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces iCITRIS, a novel method for causal representation learning that addresses the challenge of instantaneous causal effects in temporal sequences. The authors propose an innovative framework that identifies multidimensional causal variables and learns the corresponding causal graph without needing counterfactuals or labeled data. The methodology combines differentiable causal discovery techniques to enhance the learning process, and empirical evaluations demonstrate that iCITRIS outperforms existing causal representation learning methods across various datasets, particularly in systems with instantaneous effects.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ability to handle instantaneous causal relationships, a limitation of many existing methods in the field. The proposed iCITRIS framework is theoretically sound and provides a clear advancement over previous approaches. However, the paper could benefit from a more extensive discussion of the limitations of the method, particularly regarding its scalability to larger datasets or more complex causal structures. Additionally, while the empirical results are promising, further validation on a wider variety of datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with causal inference and machine learning. The quality of writing is high, with detailed explanations of the methodology and background concepts. The novelty of the iCITRIS framework is significant, as it addresses a gap in the existing literature regarding instantaneous effects. Reproducibility is supported by the clarity in methodology, though the authors could include more details about the datasets and experimental setups to facilitate independent verification.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful contribution to causal representation learning by introducing iCITRIS, which effectively incorporates instantaneous effects. While the methodology is solid and the results are compelling, the paper would benefit from additional discussions regarding limitations and broader empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework aimed at enhancing the performance of neural networks in the context of [specific problem or application]. The authors propose a new algorithm that integrates [specific methodologies or techniques], addressing known limitations of existing approaches. The methodology is supported by theoretical derivations and extensive empirical evaluations across multiple datasets, demonstrating significant improvements in performance metrics compared to baseline models.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The proposed framework introduces unique mechanisms that could lead to advancements in the field, particularly concerning [specific aspects].\n2. **Strong Theoretical Support:** The authors provide a robust theoretical foundation that elucidates the rationale behind their approach, enhancing the credibility of their claims.\n3. **Comprehensive Empirical Results:** The experiments validate the proposed method's efficacy, showcasing superior performance across a variety of challenging benchmarks.\n4. **Clear Presentation:** The paper is well-organized and clearly articulated, making it accessible for a wide range of audiences, including those less familiar with the topic.\n\n**Weaknesses:**\n1. **Limited Benchmarking:** The comparison against existing methods is somewhat limited, as it primarily focuses on a narrow selection of algorithms, which may not fully represent the state-of-the-art.\n2. **Assumptions in the Model:** Certain assumptions made in the theoretical analysis could restrict the generalizability of the results to broader applications.\n3. **Absence of Ablation Studies:** The lack of detailed ablation studies makes it difficult to ascertain the influence of individual components of the proposed framework on the overall performance.\n4. **Scalability Issues:** The evaluation of the method's performance on larger datasets is insufficient, raising concerns regarding its scalability and real-world applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a logical flow that facilitates understanding. The quality of the writing and presentation is high, making complex ideas accessible. The novelty of the proposed approach is notable, offering a fresh perspective that could have significant implications. However, the reproducibility could be improved by providing clearer guidelines on implementation and additional experimental details.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to [specific problem], supported by strong theoretical foundations and empirical evidence. While it has notable strengths, particularly in clarity and innovation, addressing its limitations—such as broader benchmarking and scalability—would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework, **iCITRIS**, for **Causal Representation Learning** that extends the existing CITRIS method to account for instantaneous causal effects in temporal sequences. The authors address the common assumption in causal learning that interventions do not have immediate effects, which can result in inaccuracies in scenarios with slow measurement rates. The methodology involves identifying multidimensional causal variables and learning their causal graph through a differentiable causal discovery approach. Experimental evaluations demonstrate that iCITRIS effectively identifies causal variables and causal graphs across multiple datasets, showcasing its potential for real-world applications in interactive systems.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to incorporating instantaneous effects into causal representation learning, which is often overlooked in existing methodologies. The differentiation of causal variable identification and causal graph learning is another significant contribution, highlighting their interdependence. However, the paper could be criticized for not providing extensive exploration of scenarios where interventions are not directly observable, which limits its applicability in certain contexts. Additionally, while the experimental results are promising, further validation on a broader range of datasets could enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly conveys the proposed framework and its contributions. The methodology is explained adequately, allowing for understanding of how iCITRIS operates. However, some sections could benefit from more detailed explanations, particularly regarding the implementation of the differentiable causal discovery method. The novelty of addressing instantaneous causal effects is significant, setting this work apart from previous approaches. Reproducibility may be a concern if the authors do not provide sufficient details regarding the datasets used and the experimental setup in future versions.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful advancement in causal representation learning by introducing the iCITRIS framework, which accommodates instantaneous effects. While the contributions are substantial, further exploration of scenarios without direct interventions and a broader validation of the findings would strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces **iCITRIS**, a novel method for causal representation learning that effectively captures both instantaneous and temporal causal effects in interactive systems. By extending the existing **CITRIS** framework, the authors integrate differentiable causal discovery methods, allowing for the identification of minimal causal variables through partially-perfect interventions. Experimental results on three datasets demonstrate that **iCITRIS** outperforms baseline methods, particularly in scenarios where instantaneous causal relationships are crucial.\n\n# Strength And Weaknesses\nThe primary strength of the paper is its innovative approach to addressing the limitations of traditional causal representation learning methods, which often overlook instantaneous effects. The methodology is well-founded, building on established frameworks while introducing new concepts that enhance causal discovery. However, a potential weakness is the reliance on partially-perfect interventions, which may not be feasible in all real-world scenarios. Additionally, while the results are promising, further exploration in more complex settings, such as cyclic causal graphs or scenarios without direct interventions, could strengthen the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The writing quality is high, making complex concepts accessible to readers. The novelty of the approach is significant, as it addresses a previously unconsidered aspect of causal representation learning. However, the reproducibility of results may depend on the availability of datasets and the specifics of the implemented interventions, which are not fully detailed in the paper.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of causal representation learning by addressing instantaneous effects in causal modeling. While the methodology is robust and the empirical results are convincing, further exploration of more complex causal structures would enhance the impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems\" by Phillip Lippe et al. introduces a novel framework, iCITRIS, aimed at advancing causal representation learning (CRL) by effectively capturing both instantaneous and temporal causal effects from high-dimensional observations. The authors highlight the limitations of existing CRL methods, particularly their reliance on slower measurement rates and counterfactuals, and propose a new architecture that combines variational autoencoders and normalizing flows to learn causal graphs and identify causal variables. Experimental results demonstrate that iCITRIS outperforms baseline methods across several datasets in metrics such as R² scores and Structural Hamming Distance (SHD).\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear exposition of the limitations of current CRL methods and the introduction of iCITRIS as a robust alternative that addresses instantaneous causal effects. The methodology is well-founded in recent advancements in neural networks and offers a novel approach without relying on counterfactuals, which is a significant contribution to the field. However, the paper could improve by providing more detailed discussions on the assumptions made regarding interventions and the model's performance under non-ideal conditions. Additionally, a deeper exploration of the ethical implications of the proposed work would enhance the overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear presentation of its contributions, methodology, and experimental validation. The novelty of introducing a method that effectively handles instantaneous effects in CRL is evident, as is the technical quality of the proposed framework. The experimental validation is comprehensive, showcasing the effectiveness of iCITRIS against established baselines. Furthermore, the authors have made their code and datasets publicly available, which significantly enhances reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in causal representation learning by addressing instantaneous effects through the novel iCITRIS framework. While the methodology and results are compelling, the authors should consider expanding on certain assumptions and ethical implications to strengthen their contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents iCITRIS, a novel causal representation learning paradigm that extends causal analysis to account for instantaneous effects in temporal sequences. The methodology employs a modified variational autoencoder (VAE) architecture that integrates differentiable causal discovery techniques to map high-dimensional observations to a latent space while learning an instantaneous causal graph. The empirical findings demonstrate that iCITRIS outperforms existing models in accurately identifying causal variables and their causal graphs across various datasets, including both synthetic and real-world scenarios.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to addressing the limitations of prior causal representation learning methods, particularly the assumption of independence among causal variables. By introducing the iTRIS framework and the iCITRIS model, the authors successfully accommodate instantaneous causal effects, which are prevalent in practical applications. The empirical validation across diverse datasets further supports the robustness of the methodology. However, the paper could benefit from a more detailed discussion regarding the limitations of the proposed approach and potential challenges in real-world applications, such as the identification of intervention targets in unsupervised settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its contributions, with sufficient background on causal representation learning and definitions of key concepts. The methodology is described in detail, allowing for reproducibility. However, while the novel aspects of iCITRIS are highlighted, more explicit comparisons with existing methodologies could enhance the clarity of its contributions. The experimental setup is thorough, though additional details regarding the datasets and the specific metrics used for evaluation would strengthen the reproducibility of results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to causal representation learning by successfully integrating instantaneous effects into the analysis. iCITRIS demonstrates strong empirical performance across multiple datasets, although further elaboration on limitations and comparisons with existing methods would enhance the paper's clarity and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces iCITRIS, a novel method for causal representation learning that aims to improve the identifiability of causal structures from observational and interventional data. The approach relies on differentiable causal discovery methods, specifically ENCO and NOTEARS, to derive causal graphs under certain idealized assumptions regarding interventions. The authors report that iCITRIS demonstrates strong performance in controlled synthetic environments, although its effectiveness diminishes in more complex and realistic datasets, where the underlying assumptions about causal relationships often do not hold.\n\n# Strength And Weaknesses\nWhile the paper presents a novel contribution to the field of causal inference, it is heavily reliant on the assumptions of perfect or partially-perfect interventions, which may not be practically feasible. The identifiability results are constrained by several restrictive assumptions, such as the requirement for known intervention targets, limiting the applicability of the method across diverse scenarios. Furthermore, the reliance on differentiable methods presents a risk of overfitting, particularly in complex environments with noisy data. Empirical results suggest that iCITRIS may not consistently outperform simpler baselines, raising questions about the added complexity of the proposed method. The paper also lacks a thorough exploration of the method's robustness under violations of its assumptions and fails to address potential issues related to cycles in causal graphs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology and findings clearly. However, it could benefit from a more thorough discussion of the implications of its assumptions and the potential limitations in real-world applications. The novelty of the approach is somewhat tempered by the restrictive nature of its assumptions, which may hinder reproducibility and practical applicability. The evaluation metrics used, R2 and SHD, may not adequately reflect the complexities of causal relationships, further complicating the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, while iCITRIS presents interesting ideas in the realm of causal representation learning, its heavy reliance on restrictive assumptions and the lack of robustness under real-world conditions raise significant concerns regarding its practical applicability. The method's performance, particularly in noisy or complex environments, remains questionable, and its contributions may not justify the added complexity compared to simpler models.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents iCITRIS, a novel approach in causal representation learning aimed at effectively identifying causal variables and their relationships in interactive systems, particularly addressing the challenge of instantaneous effects. iCITRIS integrates these instantaneous causal effects into the causal discovery process, enhancing the modeling capabilities for real-world scenarios. The authors provide extensive experimental results demonstrating that iCITRIS significantly outperforms existing models across various datasets, including the Voronoi benchmark and the Causal Pinball environment, while simultaneously learning both causal variables and their graphs.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to causal representation learning, significantly improving the accuracy and applicability of causal learning methods in dynamic environments. The comprehensive evaluation across diverse datasets showcases the robustness and versatility of iCITRIS. However, the paper could benefit from a more detailed discussion on the computational efficiency of the proposed method, particularly in large-scale applications, as well as potential limitations or assumptions inherent in the model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, methodology, and findings. The quality of the presentation is high, with well-structured sections and a logical flow of ideas. The novelty of iCITRIS is evident in its integration of instantaneous effects and joint learning of causal variables and graphs, marking a significant advancement in the field. The authors have made their code and models publicly available, promoting reproducibility and collaboration within the research community.\n\n# Summary Of The Review\nOverall, iCITRIS represents a significant advancement in causal representation learning, offering robust performance and innovative methodologies that enhance the understanding of causal relationships in dynamic systems. The potential for real-world applications and future research directions makes this work a valuable contribution to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents iCITRIS, a novel framework for Causal Representation Learning (CRL) that extends existing methodologies to accommodate instantaneous causal effects. It proposes a differentiable approach to causal discovery, enabling the simultaneous learning of causal graphs and representation of high-dimensional data. The findings indicate that the identification of causal variables is achievable through partial interventions, enriching the theoretical landscape of CRL and suggesting avenues for future research in complex systems.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of differentiable causal discovery, allowing for adaptive learning of causal structures, and its theoretical contributions that address the challenges posed by instantaneous effects. However, the framework relies heavily on the assumption of the availability of interventions, which may limit its applicability in situations where such interventions are not feasible. Additionally, while the paper articulates theoretical implications well, empirical validation and testing in diverse settings remain areas for further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions. The methodology is presented with sufficient detail, enabling reproducibility, although additional empirical examples would strengthen the understanding of its practical applications. The novelty is significant, particularly in addressing instantaneous effects in causal inference, which is often overlooked in traditional frameworks. However, the complexity of the concepts may pose a challenge for readers less familiar with causal learning.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of causal representation learning by introducing a framework that addresses the complexities introduced by instantaneous effects. The theoretical advancements provided by iCITRIS offer valuable insights, though further empirical validation is needed to confirm its practical utility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces iCITRIS, an extension of the original CITRIS framework, specifically targeting causal representation learning with a focus on instantaneous effects. The methodology employs a variational autoencoder (VAE) or a normalizing flow built on a pretrained autoencoder, integrating a mapping function for encoding-decoding, an assignment function for causal variable assignments, and conditional Gaussian prior distributions modeled as multi-layer perceptrons (MLPs). Key contributions include detailed graph learning techniques using NOTEARS and ENCO, comprehensive training algorithms, and robust empirical evaluations on diverse datasets, demonstrating superior performance in identifying causal variables and relationships compared to baseline models.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its thorough implementation details, including the provision of pseudocode for the training algorithms (NOTEARS and ENCO), which enhances reproducibility. Additionally, the ablation studies highlight the model's robustness in the face of noisy interventions, suggesting practical applicability in real-world scenarios. However, the paper lacks a broader discussion on the implications of its findings and the innovative aspects of causal representation learning beyond technical specifics, which may limit its appeal to a wider audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates the methodologies employed, making it relatively easy to follow for readers familiar with the domain. The quality of the implementation, coupled with available code, facilitates reproducibility, an essential aspect for advancing research in this area. However, while the technical novelty is evident in the proposed methodologies, the novelty of the conceptual framework could be more explicitly stated to underscore its significance in the broader context of causal representation learning.\n\n# Summary Of The Review\niCITRIS represents a meaningful advancement in the field of causal representation learning, offering robust methodologies and impressive empirical results. While the paper excels in clarity and reproducibility, it could benefit from a more extensive discussion of its broader implications and contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces iCITRIS, a novel method for causal representation learning that incorporates instantaneous effects, representing an advance over its predecessor, CITRIS, which does not accommodate such dynamics. The authors claim that iCITRIS outperforms traditional causal representation methods, including Identifiable VAE (iVAE) and CITRIS, particularly in settings where instantaneous effects are present. Through experiments conducted on datasets such as the Voronoi benchmark and Causal Pinball dataset, the authors report high R² scores and low Structural Hamming Distance (SHD) values, signaling promising empirical results.\n\n# Strength And Weaknesses\nWhile iCITRIS demonstrates some advancements in causal representation learning, its contributions are somewhat derivative of previous works, with the novelty coming primarily from extending existing frameworks rather than introducing groundbreaking innovations. The performance improvements claimed by the authors may not be as substantial as suggested, especially given that baseline methods like iVAE and CITRIS have shown competitive performance even in the absence of instantaneous effects. Additionally, the reliance on partially-perfect interventions raises questions about the method's practicality in real-world applications, where such interventions may not be feasible. Furthermore, the paper does not adequately explore potential weaknesses or failure modes of iCITRIS, which could limit its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its exposition, allowing readers to understand the methodology and findings. However, the novelty of the proposed method is somewhat undermined by the lack of a thorough justification for the added complexities introduced by iCITRIS. The reproducibility of the results could be enhanced by providing more comprehensive details on experimental setups and potential limitations of the method, particularly in complex causal structures.\n\n# Summary Of The Review\nOverall, while iCITRIS presents an interesting approach to causal representation learning by addressing instantaneous effects, its contributions may not be as significant as claimed. The paper's reliance on previous works and the questionable necessity of its complexities raises skepticism about its purported advancements over existing methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to causal representation learning, particularly focusing on instantaneous and temporal effects in interactive systems. The authors introduce a framework that integrates causal inference techniques with representation learning to address challenges in identifying causal relationships from observational data. The methodology includes the development of a new algorithm that leverages temporal data to enhance causal identifiability. The findings demonstrate that the proposed approach outperforms existing methods in terms of accuracy and interpretability when applied to synthetic and real-world datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of causal inference and representation learning, which addresses a significant gap in the existing literature. The empirical results are robust, showcasing clear advantages over baseline methods. Additionally, the paper provides a comprehensive evaluation of the proposed algorithm across various scenarios, contributing to its credibility. However, one notable weakness is the limited discussion on the scalability of the proposed method, which may hinder its application to larger datasets. Furthermore, while the results are compelling, the paper could benefit from additional case studies to demonstrate real-world applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections and a logical flow of ideas. However, certain technical details in the methodology section could be elaborated to enhance reproducibility. The novelty of the approach is significant, as it brings fresh insights into causal representation learning, though some concepts may require further simplification for broader accessibility. The quality of the empirical results is commendable; however, the lack of detailed hyperparameter tuning and experimental setup descriptions limits reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of causal representation learning by proposing a new framework that effectively captures both instantaneous and temporal effects. While the findings are promising, the paper would benefit from a more thorough discussion of scalability and additional real-world applications to strengthen its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel causal inference framework named iCITRIS, which aims to improve the identification of causal relationships in complex systems. It employs advanced methodologies to address instantaneous and temporal causal effects while assuming acyclic graphs. The findings demonstrate the framework's effectiveness in identifying causal structures under certain conditions, although it acknowledges the limitations posed by imperfect interventions and the absence of latent confounders.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to causal representation learning and the introduction of a new framework that enhances the understanding of causal dynamics in various settings. However, it suffers from several weaknesses, such as the assumption of perfect interventions, limited exploration of temporal dependencies, and a lack of scalability considerations for high-dimensional systems. Additionally, the paper does not adequately address the implications of latent confounders or the robustness of causal structures across different data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections and a coherent presentation of methodologies. However, certain technical aspects could benefit from more detailed explanations, particularly regarding the assumptions made in the proposed framework. The novelty of the iCITRIS approach is notable, yet its reproducibility could be improved by providing more thorough experimental setups and validation processes. Overall, while the paper presents an interesting contribution to the field, certain areas require further elaboration to enhance understanding and applicability.\n\n# Summary Of The Review\nOverall, the paper presents a promising framework for causal inference, introducing significant new methodologies. However, it would benefit from addressing limitations related to imperfect interventions, latent confounders, and scalability, as well as providing more robust experimental validations. These improvements would bolster the framework's applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel causal representation learning model termed iCITRIS, designed to uncover causal variables and their relationships from high-dimensional observations, particularly focusing on instantaneous and temporal effects in interactive systems. The methodology employs likelihood maximization within a dynamic Bayesian network framework and integrates differentiable causal discovery methods (NOTEARS and ENCO) for graph learning, incorporating regularization techniques to ensure the structure adheres to properties of directed acyclic graphs (DAGs). Key findings include high R² scores indicating effective disentanglement of causal variables and favorable structural Hamming distances (SHD) demonstrating robust causal graph recovery, especially in comparison to traditional methods like CITRIS and iVAE.\n\n# Strength And Weaknesses\nStrengths of the paper include its well-structured methodology that effectively combines statistical principles with modern graph learning techniques, providing a comprehensive framework for causal representation learning. The empirical results are compelling, showcasing iCITRIS's resilience under varying assumptions and its superiority over existing methods in identifying causal relationships. However, a potential weakness lies in the assumptions around intervention targets, which may limit the model's applicability in scenarios where such targets are not known or are difficult to ascertain. Additionally, while the theoretical contributions are significant, the paper could benefit from a more detailed discussion on the practical implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical organization that guides the reader through the theoretical framework and empirical evaluations. The quality of the analysis is high, supported by rigorous mathematical formulations and comprehensive experimental results. In terms of novelty, the integration of differentiable causal discovery methods with likelihood maximization in a dynamic context is a significant advancement. The reproducibility of the results is bolstered by detailed descriptions of the methodology, although providing access to code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of causal representation learning through the introduction of the iCITRIS model, supported by both theoretical insights and empirical validation. While the methodological innovations and results are strong, further exploration of practical applications and broader assumptions could enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework, iCITRIS, for identifying causal variables and their relationships within acyclic causal graphs. It employs mutual information estimation to enhance performance, demonstrating its effectiveness through empirical evaluations on specific datasets, particularly in the context of video data. The main contributions include theoretical identifiability results under ideal conditions and an innovative approach to causal representation learning that emphasizes the use of interventions.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical contributions and the introduction of mutual information estimation, which appears to enhance performance. However, the paper has several weaknesses. It assumes the availability of perfect interventions, which limits its applicability in real-world scenarios. Additionally, the reliance on acyclic causal graphs raises concerns about its robustness in environments with cyclic relationships or noisy data. The identifiability results are constrained to datasets recorded by experts, which may hinder generalization. Furthermore, the lack of discussion on handling violations of model assumptions and the limited empirical testing on diverse datasets pose significant limitations to its practical utility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the methodology and findings. However, the complexity of the assumptions and limitations may lead to challenges in reproducibility, particularly for researchers aiming to apply the framework in less controlled environments or those involving non-observable interventions. The novelty of using mutual information estimation is commendable, but further exploration and discussion of its implications and limitations would enhance the paper's quality.\n\n# Summary Of The Review\nOverall, the paper makes valuable contributions to the field of causal inference through the introduction of iCITRIS and the use of mutual information estimation. However, significant limitations regarding assumptions, generalizability, and robustness raise concerns about its applicability in real-world scenarios. Future work is needed to address these issues and extend the framework's capabilities.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to causal representation learning, specifically addressing instantaneous effects through a new method called iCITRIS, which generalizes existing techniques. The authors argue that current methods overlook the complexities of combining instantaneous and temporal effects in causal inference. They validate their method using three datasets, demonstrating superior performance compared to existing approaches and acknowledging limitations in their assumptions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of iCITRIS, which aims to enhance causal identification in high-dimensional observations by incorporating instantaneous effects. However, the contribution feels somewhat incremental, as many of the challenges addressed have been previously acknowledged in the literature. The methodology is adequately described, but the novelty of the approach may not be as significant as claimed. Additionally, while the authors provide empirical validation, the comprehensiveness of the experiments could be questioned given the limited number of datasets used.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly written, making it accessible to readers familiar with causal inference. The methodology is presented in a logical manner, though it lacks depth in discussing the novelty of iCITRIS in comparison to existing methods. Reproducibility is addressed through the provision of code and hyperparameter settings, which is a positive aspect of the paper. However, the justification for certain choices may not be sufficiently robust, potentially limiting reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a commendable attempt to tackle an important aspect of causal representation learning; however, its contributions may not be as groundbreaking as suggested. While the introduction of iCITRIS is a step forward, the paper largely reiterates established challenges in the field without offering substantial new insights.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces iCITRIS, a novel approach to differentiable causal discovery that leverages differentiable optimization techniques for causal inference. The methodology emphasizes the identification of causal graphs with instantaneous and temporal edges, allowing for a deeper understanding of causal relationships. Key findings suggest that iCITRIS is effective in scenarios without counterfactuals or labeled data, showcasing its potential for unsupervised learning in causal inference. The authors highlight the robustness of their method in the presence of noisy intervention targets, making it applicable to real-world scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of differentiable methods into causal discovery, which could significantly enhance the field. The focus on instantaneous effects and the potential for unsupervised learning are particularly noteworthy, as they pave the way for future research directions. However, the paper could improve by providing a more systematic comparative analysis of model performance across different datasets. Additionally, while the exploration of causal graph complexities is promising, the implications for high-dimensional settings remain underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. The methodology and findings are articulated with sufficient detail, allowing for reproducibility. However, a more thorough discussion on ethical implications and the potential for multi-agent scenarios would enhance its quality. The novelty of the work is significant, especially in the context of differentiable optimization applied to causal inference.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to differentiable causal discovery through iCITRIS, with significant potential for advancing unsupervised learning techniques in this domain. While the contributions are noteworthy, further exploration of comparative performance and ethical considerations would strengthen the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents iCITRIS, a novel method for causal representation learning that effectively incorporates both instantaneous and temporal effects. The methodology is evaluated across three distinct datasets: the Voronoi benchmark, Instantaneous Temporal Causal3DIdent, and Causal Pinball. The findings reveal that iCITRIS consistently outperforms existing baseline models, such as CITRIS and iVAE, demonstrating superior performance in identifying causal variables and relationships, particularly in complex scenarios. Notably, the method achieved optimal R² scores and exhibited resilience against noise in data.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive evaluation and robust performance across various datasets, particularly in handling both instantaneous and temporal dependencies. The results from the Voronoi benchmark and Causal Pinball datasets demonstrate the method's efficacy in accurately identifying causal structures. However, the paper could benefit from a more detailed discussion of the theoretical foundations underlying the proposed method and a deeper exploration of its limitations. Additionally, while the ablation studies provide valuable insights, further exploration of the model's behavior in different noise conditions could enhance the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and results, making it accessible for readers. The quality of the experiments is high, with appropriate metrics (R² scores and SHD values) used to evaluate performance. The novelty of iCITRIS in addressing both instantaneous and temporal effects is significant, setting it apart from existing methods. However, reproducibility could be improved by providing more information on the implementation details and the datasets used, which would aid researchers in validating the results independently.\n\n# Summary Of The Review\nOverall, iCITRIS represents a significant advancement in causal representation learning, effectively addressing the challenges posed by both instantaneous and temporal effects. The paper's thorough evaluation and consistent performance across multiple datasets underscore its contributions to the field. However, greater clarity on the theoretical underpinnings and implementation details would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to differentiable causal discovery, aiming to enhance the understanding of causal relationships in complex systems. The methodology involves a new algorithm that integrates differentiable programming principles with causal inference techniques, allowing for end-to-end learning from data. The findings demonstrate improved accuracy in causal relationship identification compared to existing methods, validated on both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative methodology that combines differentiable programming with causal discovery, presenting a significant advancement in the field. The empirical results highlight the effectiveness of the proposed approach, showing superior performance over traditional methods. However, the paper has weaknesses, such as a lack of clarity in some sections, particularly in the experimental details, which may hinder reproducibility. Additionally, the dense presentation of the abstract and results could overwhelm readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to long paragraphs and excessive technical jargon, which may alienate less specialized readers. While the quality of the research is commendable, the organization of the supplementary material and references could be improved for better accessibility. The novelty of the proposed method is high, contributing significantly to the field of causal discovery. However, the reproducibility of the results may be impacted by the insufficient detail in the experimental setup.\n\n# Summary Of The Review\nOverall, the paper introduces a promising direction in differentiable causal discovery, showcasing significant empirical results. However, clarity and organization issues may hinder the paper's accessibility and reproducibility. Addressing these aspects would enhance the overall impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.319568219965581,
    -1.624516059741817,
    -1.785523022522698,
    -1.6050281144513083,
    -1.7929779014063032,
    -1.6669406127128845,
    -1.8057282980405314,
    -1.7721048962560755,
    -1.6689652567204518,
    -1.7294944671464796,
    -1.643048699169541,
    -1.4317048362881395,
    -1.7168840329209993,
    -1.6376215981999727,
    -1.7016707620309552,
    -1.7417077049753917,
    -1.7724437312124153,
    -1.6167112994320396,
    -1.7998498783176609,
    -1.9314283182445242,
    -1.8405333207096881,
    -1.4886409768157693,
    -1.7366695001271557,
    -1.7488277295398136,
    -1.9141684236641607,
    -1.762423368400116,
    -1.8348081814333388,
    -1.6624958840607797,
    -1.7047121715579974
  ],
  "logp_cond": [
    [
      0.0,
      -2.126192938358566,
      -2.1368251634863493,
      -2.1300659612831816,
      -2.123526675108353,
      -2.111600624388076,
      -2.1606299647557257,
      -2.1448752642522178,
      -2.1243163580584414,
      -2.1485292788284136,
      -2.148234341575254,
      -2.1586505283334927,
      -2.137992581992025,
      -2.14278703614816,
      -2.124512110017501,
      -2.143528612424911,
      -2.1734123576512463,
      -2.1533821656032104,
      -2.1376347348251135,
      -2.140907706857712,
      -2.1413061647985194,
      -2.1586658205776517,
      -2.153282145462005,
      -2.130978183342433,
      -2.144966855049438,
      -2.1470207612131342,
      -2.1626495423975065,
      -2.146968744268817,
      -2.19359403106364
    ],
    [
      -1.1798939523993113,
      0.0,
      -1.072470461326177,
      -1.030340535476628,
      -1.1430952779748524,
      -1.1517940138550509,
      -1.204984057832875,
      -1.1648985558785365,
      -1.032822334112399,
      -1.1965949504449702,
      -1.1161811488519435,
      -1.2750784745674504,
      -1.0863679437938174,
      -1.0524027183651157,
      -1.200309440166067,
      -1.0605011252002416,
      -1.200500672696247,
      -1.214098959722307,
      -1.1846324011172844,
      -1.151620369288792,
      -1.1515077479171678,
      -1.2276146797960872,
      -1.2276862671570064,
      -1.108143299662261,
      -1.1805530092981698,
      -1.2134478000754099,
      -1.2182016256127592,
      -1.1230778710622638,
      -1.2421667299998376
    ],
    [
      -1.413385996423374,
      -1.3833972659749978,
      0.0,
      -1.3819746062470901,
      -1.3345462816897855,
      -1.3779423447826313,
      -1.3792869349776888,
      -1.371931277388787,
      -1.3164256001846792,
      -1.3930377763981523,
      -1.4254910443696183,
      -1.4883325151030316,
      -1.3701739792575947,
      -1.3827042722187415,
      -1.3896838779572314,
      -1.338185429051952,
      -1.3893609640299722,
      -1.4071696289414182,
      -1.3612967583943303,
      -1.4430124209973292,
      -1.3383150311806087,
      -1.4464515145294023,
      -1.3998897416572493,
      -1.396573631736649,
      -1.3754694801682672,
      -1.411460275591101,
      -1.4101639704508864,
      -1.3484858182472894,
      -1.4851652816476113
    ],
    [
      -1.241072819344592,
      -1.1712851298429208,
      -1.1370966252807062,
      0.0,
      -1.1535020908137428,
      -1.144445122643617,
      -1.248708049689682,
      -1.2135200756945563,
      -1.1311397792094886,
      -1.1914435806794437,
      -1.1635788836644838,
      -1.2378944353843895,
      -1.101742107703636,
      -1.0579865772133301,
      -1.2227293443072291,
      -1.1624751170578598,
      -1.1999158203217637,
      -1.222490867693651,
      -1.1745489395721296,
      -1.2278803306262753,
      -1.1049036352573178,
      -1.2468502202756138,
      -1.2408854178708895,
      -1.1918524056766515,
      -1.2149423224514473,
      -1.211617445845192,
      -1.2526351881901363,
      -1.1406108916311801,
      -1.2659425863301508
    ],
    [
      -1.4274887582450204,
      -1.4035123111643908,
      -1.2774776655526354,
      -1.34104455134382,
      0.0,
      -1.4132570242510167,
      -1.3884780599179622,
      -1.3502049976288062,
      -1.3880405970445806,
      -1.3372526441905515,
      -1.413056328476808,
      -1.416112291962381,
      -1.4276707622753169,
      -1.337153158450064,
      -1.4159261983740068,
      -1.3941539321536223,
      -1.3073164489230868,
      -1.3934062767320632,
      -1.3541725597001337,
      -1.386554083431301,
      -1.3098482485166871,
      -1.4183639613807566,
      -1.3731621259619922,
      -1.3886215208467558,
      -1.3206793896732416,
      -1.3747973993869813,
      -1.414680674766788,
      -1.3885967515388762,
      -1.4468582416000548
    ],
    [
      -1.3962575054595772,
      -1.3115054204440564,
      -1.2391022984695372,
      -1.252970607392807,
      -1.2560315353673177,
      0.0,
      -1.2600794477932646,
      -1.2079174473577963,
      -1.2362138174505577,
      -1.2946741656239715,
      -1.2990714640434287,
      -1.3968975207610945,
      -1.2014011610878168,
      -1.1925831279515282,
      -1.2527469832800366,
      -1.3094550302542987,
      -1.3384134230097307,
      -1.291267178770616,
      -1.2890491683393859,
      -1.360830121689652,
      -1.3124831794452323,
      -1.362235626845119,
      -1.361351981645255,
      -1.2745774964753076,
      -1.290134351615862,
      -1.3026746340005182,
      -1.3050548959350743,
      -1.3549887234821996,
      -1.3943987263958575
    ],
    [
      -1.510093723474205,
      -1.47215124544428,
      -1.4014112215652406,
      -1.4532549817586542,
      -1.4201008922453562,
      -1.3641311401539375,
      0.0,
      -1.400112614922079,
      -1.395050988181431,
      -1.4257389951482051,
      -1.4774948901838254,
      -1.4998566818021994,
      -1.393006866841245,
      -1.4294868195994523,
      -1.3946318908145061,
      -1.452922017748155,
      -1.4733708194265724,
      -1.413369316021797,
      -1.4563768138831812,
      -1.4674626403105673,
      -1.4168862593987182,
      -1.4192742007244354,
      -1.4528951528584775,
      -1.4387762998640015,
      -1.473143656681954,
      -1.410465943770609,
      -1.412323370122962,
      -1.4396722156752777,
      -1.5021481748591472
    ],
    [
      -1.4889475372788008,
      -1.413451107263172,
      -1.3677473290383082,
      -1.4219575396362387,
      -1.3928865640631365,
      -1.3652938825401837,
      -1.4099167607566447,
      0.0,
      -1.3796124653419894,
      -1.3913097592929904,
      -1.4289377329015562,
      -1.5146503096559107,
      -1.386640888036688,
      -1.361301492583248,
      -1.3612814850056798,
      -1.4449641554078028,
      -1.451656030019741,
      -1.4084352365547708,
      -1.393875281276532,
      -1.4368006055183071,
      -1.3981012624093878,
      -1.4326766677976954,
      -1.4418937805111665,
      -1.41649893339179,
      -1.4411436380254705,
      -1.4004083156914557,
      -1.4062594439865312,
      -1.4328001542497095,
      -1.487471685443517
    ],
    [
      -1.3674569052987025,
      -1.2826540523899757,
      -1.209870330060628,
      -1.2642497578937997,
      -1.2802038107308047,
      -1.2803890206034183,
      -1.3236549825348596,
      -1.2547644688594595,
      0.0,
      -1.3523132795354682,
      -1.2856232677834973,
      -1.3832616540842217,
      -1.2317495198578108,
      -1.2860905222071026,
      -1.1670431634168374,
      -1.2639507184384393,
      -1.3150322443091602,
      -1.2692538066997492,
      -1.281772402711327,
      -1.3430962660178616,
      -1.2252471252835022,
      -1.3423576910666895,
      -1.3079044990860071,
      -1.2117655494068624,
      -1.3351153368442468,
      -1.3139106064067037,
      -1.3346113839359037,
      -1.2172905719040192,
      -1.3712683044514489
    ],
    [
      -1.3612400901573312,
      -1.3457931000130008,
      -1.2591086109639154,
      -1.2680793218248205,
      -1.2203323947384026,
      -1.3057606895325802,
      -1.3017906580811431,
      -1.2815823294512834,
      -1.3493790705513153,
      0.0,
      -1.3637327394447447,
      -1.4251366450791203,
      -1.3186451220663358,
      -1.2905926625383364,
      -1.3668963605647881,
      -1.3461773795222198,
      -1.2823156532782118,
      -1.3427016815784167,
      -1.3355400383373752,
      -1.3646149478318768,
      -1.2240857606421665,
      -1.3581318935126756,
      -1.3112392718735237,
      -1.338350100279535,
      -1.2619595964661527,
      -1.2864795539961595,
      -1.2946226605910311,
      -1.3454132420836786,
      -1.403582226470515
    ],
    [
      -1.308211853910423,
      -1.0872616169671048,
      -1.1581621787832377,
      -1.106356035305433,
      -1.1756668744683136,
      -1.1303998802729955,
      -1.2296279099212153,
      -1.2217276908879109,
      -1.1053850866922035,
      -1.2688338989657466,
      0.0,
      -1.3175682688081691,
      -1.0748364170200413,
      -1.095480578140756,
      -1.1768473457378508,
      -1.0968676687506864,
      -1.2093998962207275,
      -1.19918680827244,
      -1.1496394025391234,
      -1.2302861459581163,
      -1.2025437352561852,
      -1.246910710773161,
      -1.2333688017908282,
      -1.1994973579864963,
      -1.2176293876314312,
      -1.2200084501040644,
      -1.1666883872889826,
      -1.2090773569251443,
      -1.246635130218308
    ],
    [
      -1.226293969112354,
      -1.200150572545705,
      -1.1741650185317034,
      -1.1782827866853347,
      -1.1721009405671394,
      -1.1820395566275328,
      -1.1879918695140352,
      -1.2001672615644783,
      -1.1752260980702087,
      -1.2234296227912351,
      -1.1735747957327534,
      0.0,
      -1.2120672872651597,
      -1.1744808854662703,
      -1.1946865709291254,
      -1.2054837559499607,
      -1.2034404346438083,
      -1.2134741852701556,
      -1.1981439607703381,
      -1.1971579936690482,
      -1.1827565965655673,
      -1.1680984635429033,
      -1.1893521023496916,
      -1.2115285185381062,
      -1.2076793748643866,
      -1.1886044905094069,
      -1.1960821499747099,
      -1.187763809011405,
      -1.1929781753032973
    ],
    [
      -1.379413535113711,
      -1.3391891071039788,
      -1.293033217961969,
      -1.2471952691300934,
      -1.3528182588255215,
      -1.2218610046909122,
      -1.3247615261026433,
      -1.307993484992698,
      -1.2725216024030335,
      -1.3553600215553538,
      -1.2847786370594205,
      -1.4764048580054214,
      0.0,
      -1.2341299560469337,
      -1.348195894691299,
      -1.3072585789526867,
      -1.4023083657590374,
      -1.332335843036595,
      -1.3041831275504598,
      -1.413603689169245,
      -1.3351360476766434,
      -1.3493594139175962,
      -1.3905981530395866,
      -1.36204538372552,
      -1.3781279237557893,
      -1.3615680555422693,
      -1.3527320548060608,
      -1.3643864464493183,
      -1.4108972574819671
    ],
    [
      -1.2165181964936573,
      -1.1474498607402166,
      -1.108982084424282,
      -1.0794773589437674,
      -1.1349120764929064,
      -1.087293884335466,
      -1.1633711124267445,
      -1.1192312153111474,
      -1.111724816920033,
      -1.1704206206795542,
      -1.1437571644446758,
      -1.311013976536638,
      -1.0923135685972474,
      0.0,
      -1.1230745023673652,
      -1.1717041154702157,
      -1.1611685165623904,
      -1.1439354840186948,
      -1.1252419658415755,
      -1.1871434399267666,
      -1.173604037464606,
      -1.2009326704178973,
      -1.221900598309163,
      -1.1612046850367572,
      -1.2246928873345346,
      -1.188639526253973,
      -1.19028682924564,
      -1.2054633034338038,
      -1.2395338990581648
    ],
    [
      -1.3976354641312119,
      -1.3058019187181702,
      -1.264068707182697,
      -1.3209257279350506,
      -1.2918332206558556,
      -1.298938274882735,
      -1.332593468139923,
      -1.2699653341529398,
      -1.1892649402371547,
      -1.3376301112046665,
      -1.2696687416518464,
      -1.400765356143197,
      -1.2575485670860092,
      -1.2635386959958177,
      0.0,
      -1.3254126730004008,
      -1.336776116645977,
      -1.274236389344882,
      -1.3028878909962502,
      -1.3016151102689186,
      -1.30960810772523,
      -1.3437417342298295,
      -1.3254144518682816,
      -1.2768365174041059,
      -1.3393347632536827,
      -1.3056142170337142,
      -1.3080435944008089,
      -1.3292104042058814,
      -1.3890763644171547
    ],
    [
      -1.4059373532051784,
      -1.2717149922128026,
      -1.2629539720714702,
      -1.2872885770814058,
      -1.2888985215828517,
      -1.333183483479797,
      -1.410396974922404,
      -1.3940588343906943,
      -1.2773040761293808,
      -1.3913568470089577,
      -1.2940138650478155,
      -1.4795132458600382,
      -1.3071668156091703,
      -1.325660570190485,
      -1.3422864609575786,
      0.0,
      -1.360824439225616,
      -1.3303246878443347,
      -1.3064936826285127,
      -1.3520188072430015,
      -1.3892413992212689,
      -1.4173692191511216,
      -1.394528550001758,
      -1.3198933808510684,
      -1.3827759785523968,
      -1.393922953336176,
      -1.3856917046410508,
      -1.3571186898596017,
      -1.3980162041014883
    ],
    [
      -1.5117504648861524,
      -1.4419676547727314,
      -1.428423409607376,
      -1.4125494341955442,
      -1.4151121886297129,
      -1.4874661584143944,
      -1.5194999960186995,
      -1.4812973885106948,
      -1.4564434281120353,
      -1.4645279527150885,
      -1.4555223924645067,
      -1.498887282648664,
      -1.4917993654377406,
      -1.3816108310043602,
      -1.4300874089825772,
      -1.432712527993385,
      0.0,
      -1.446769135893004,
      -1.4332584307641991,
      -1.4133295503423902,
      -1.3537543508715746,
      -1.537119088907473,
      -1.4550897828754608,
      -1.4068343962084568,
      -1.3807515421042345,
      -1.500416418767916,
      -1.4592445221030597,
      -1.4173635167916232,
      -1.5030047594202054
    ],
    [
      -1.2877202322974628,
      -1.2017958918242744,
      -1.168728895122121,
      -1.200190438483396,
      -1.1914624737679025,
      -1.133795032301415,
      -1.2423973437872247,
      -1.2436565616483386,
      -1.174744745891043,
      -1.2813213624880593,
      -1.2194573300184122,
      -1.2984109230777852,
      -1.1595907163026373,
      -1.192893149319647,
      -1.1647239345151805,
      -1.1761630446989486,
      -1.2450742281935632,
      0.0,
      -1.211131104034216,
      -1.1913089268031083,
      -1.1711044611398393,
      -1.289923853260906,
      -1.2916514388857558,
      -1.2047812724534597,
      -1.2544084656592092,
      -1.258687364707469,
      -1.2511694457504605,
      -1.1669237280370865,
      -1.2696796107904258
    ],
    [
      -1.3724350973222463,
      -1.3189089930012317,
      -1.2579605865397865,
      -1.3030880081097058,
      -1.2524833020829542,
      -1.3390836079342228,
      -1.409542338568709,
      -1.3499247233489677,
      -1.3362068686356152,
      -1.32796963567677,
      -1.3208560391713051,
      -1.4663300402158241,
      -1.3093214772390036,
      -1.2760774123387009,
      -1.3942709791400587,
      -1.306862201689734,
      -1.3415440015982631,
      -1.365401861085346,
      0.0,
      -1.364022223594622,
      -1.3439038738024427,
      -1.3943980840586043,
      -1.3294595359704018,
      -1.3451120594895094,
      -1.3238558807915086,
      -1.3651896847943583,
      -1.338364495287047,
      -1.389129717781955,
      -1.400731580281244
    ],
    [
      -1.5962928313852298,
      -1.5475410140044263,
      -1.5154607896222125,
      -1.5338832508092926,
      -1.526078952731959,
      -1.5474061547778408,
      -1.58768346223124,
      -1.5667662864200873,
      -1.542311214359472,
      -1.6183513867278978,
      -1.5687655795791986,
      -1.605669255359617,
      -1.5667757419533643,
      -1.5306857575747415,
      -1.5308270280386382,
      -1.4912158159303892,
      -1.5285355848372693,
      -1.5076963214341135,
      -1.5294384208120448,
      0.0,
      -1.5516887985393393,
      -1.58593396109852,
      -1.5530665117787137,
      -1.4661140495789378,
      -1.5347803947775118,
      -1.5771282136385163,
      -1.5500621552632143,
      -1.486477748547222,
      -1.5766272437440336
    ],
    [
      -1.5468515253833148,
      -1.5061710038339289,
      -1.4953378937127164,
      -1.4555254723682256,
      -1.4327053061014317,
      -1.4712541265076993,
      -1.4853838634441958,
      -1.4899063373693997,
      -1.452556462104478,
      -1.460983652290491,
      -1.4995493313440413,
      -1.5897467419492943,
      -1.50061406476133,
      -1.4318181667880765,
      -1.4547717093573527,
      -1.4910609183697583,
      -1.4264594584408297,
      -1.4573225400196506,
      -1.4985682959125857,
      -1.5077390507517854,
      0.0,
      -1.509093619019718,
      -1.517183945610441,
      -1.476236135071791,
      -1.5263504375916177,
      -1.4612921147946274,
      -1.4811799186116952,
      -1.3560266805539234,
      -1.5781246729712997
    ],
    [
      -1.2223171676754607,
      -1.2218976102966266,
      -1.1304204997414697,
      -1.141446624239764,
      -1.1610855155396,
      -1.1067236501250117,
      -1.0898513960152982,
      -1.082837860659546,
      -1.1184753669166108,
      -1.1549112926726746,
      -1.1996323205593138,
      -1.1477051259412563,
      -1.1302450844636662,
      -1.131521228544851,
      -1.1448188617034072,
      -1.1872235723502302,
      -1.173472759713288,
      -1.2003766903923963,
      -1.1878282516617262,
      -1.18811214675709,
      -1.1342528364778475,
      0.0,
      -1.2056331535283025,
      -1.1487109986152826,
      -1.199056904449597,
      -1.1307928208316365,
      -1.1432445448976734,
      -1.1618341268004901,
      -1.1196825938825972
    ],
    [
      -1.4043984885305452,
      -1.3331624189557345,
      -1.2829687998940698,
      -1.3219206740153522,
      -1.3054378356745457,
      -1.337058243163321,
      -1.3333471404213642,
      -1.315643897876917,
      -1.3026489392994063,
      -1.2925685701056797,
      -1.3322051726116095,
      -1.3599626691613393,
      -1.311593051137819,
      -1.2882862278846454,
      -1.289907613182904,
      -1.3145899471248472,
      -1.2609447061324817,
      -1.29576618245088,
      -1.3007924477842536,
      -1.3208040877937903,
      -1.331145410221237,
      -1.3296021746906557,
      0.0,
      -1.3031948353836367,
      -1.2842612545983572,
      -1.2865825405771127,
      -1.30192191345087,
      -1.3011104772574238,
      -1.3631430789022516
    ],
    [
      -1.4260059932950115,
      -1.3809188079044799,
      -1.3785784675162294,
      -1.396011916134454,
      -1.4044727482121295,
      -1.35234753422612,
      -1.4170964262263677,
      -1.3665153976272315,
      -1.2716348925357328,
      -1.4567416060650447,
      -1.3846288159013795,
      -1.4481080673668816,
      -1.3974652845887454,
      -1.3684394345635373,
      -1.3418719947130116,
      -1.4083488874967884,
      -1.3613328785804557,
      -1.4043269766076114,
      -1.423173272899179,
      -1.3381197053216847,
      -1.3471788297120788,
      -1.400315913281649,
      -1.436396164413714,
      0.0,
      -1.430479605375494,
      -1.410340490814349,
      -1.433129017435566,
      -1.3076357608152054,
      -1.4733382516722104
    ],
    [
      -1.5569428356693906,
      -1.504730004483149,
      -1.47442778419942,
      -1.5237462125811692,
      -1.4732986781909074,
      -1.543525711482585,
      -1.5835109741715394,
      -1.5387372838085327,
      -1.5368339801499156,
      -1.5329898611693706,
      -1.5455079324638041,
      -1.603899757628051,
      -1.5437168752054797,
      -1.5292437032819255,
      -1.5355086294249038,
      -1.5549676431968331,
      -1.429132940293047,
      -1.518450484901999,
      -1.4878224506854456,
      -1.5498681416006854,
      -1.5462327160275355,
      -1.6145007713866926,
      -1.4822273037670026,
      -1.5288852666028243,
      0.0,
      -1.5692841841721397,
      -1.5701522762040707,
      -1.5630258964624475,
      -1.5800884474233747
    ],
    [
      -1.4372542111903786,
      -1.4598427761688106,
      -1.3172507782458367,
      -1.410895187452162,
      -1.3725599240583275,
      -1.3801767447924913,
      -1.3656616165302076,
      -1.308241932465398,
      -1.4042337733810568,
      -1.3965932211955394,
      -1.4249945830527497,
      -1.483615458368706,
      -1.388074337809225,
      -1.4221139898502788,
      -1.3861342317771235,
      -1.3951117876039447,
      -1.4180420671326677,
      -1.3758400684265333,
      -1.390891052332782,
      -1.4142279648473883,
      -1.3005926430919441,
      -1.3652186116067266,
      -1.3919574609764007,
      -1.3971482874671928,
      -1.4230049581547979,
      0.0,
      -1.3614616979141636,
      -1.3760478488052847,
      -1.4595881335409924
    ],
    [
      -1.5069623432185153,
      -1.4482625389018264,
      -1.3963939645048844,
      -1.4391274256056823,
      -1.4001167542145387,
      -1.3817297995928712,
      -1.3798295142791512,
      -1.40784220248124,
      -1.419403891575381,
      -1.4065731620907966,
      -1.4082998251879133,
      -1.5339224981308015,
      -1.4111148650007979,
      -1.4014726754327935,
      -1.4381312815687544,
      -1.4398040173384234,
      -1.4324140050140952,
      -1.4681835645564616,
      -1.4166827942761007,
      -1.4426576549223737,
      -1.3769739302957311,
      -1.4397242194690087,
      -1.4258232548824072,
      -1.4575523027887243,
      -1.4299385126126594,
      -1.4145781174021388,
      0.0,
      -1.467473125542437,
      -1.4727548075916224
    ],
    [
      -1.3666632561278733,
      -1.368086770299248,
      -1.276490514247297,
      -1.3071165388319221,
      -1.3014676750238878,
      -1.3198627861436856,
      -1.3090661655891573,
      -1.3245987558651147,
      -1.2514928517832198,
      -1.3553328202317676,
      -1.3765851732237784,
      -1.390574656764057,
      -1.3332143069790345,
      -1.3422480057507837,
      -1.3179227028679417,
      -1.3162648874289167,
      -1.3275370340686823,
      -1.2804514488687475,
      -1.325764115240006,
      -1.2651651408544158,
      -1.1886350774654562,
      -1.3563609928396088,
      -1.3647611680864726,
      -1.2732013816152492,
      -1.330686288352845,
      -1.3325014551028511,
      -1.3452260889587562,
      0.0,
      -1.361309791430158
    ],
    [
      -1.3531697799032691,
      -1.3185061557146505,
      -1.298920524935071,
      -1.2909819439175474,
      -1.2709191153563297,
      -1.2924086293222241,
      -1.318549294482625,
      -1.3381421688709865,
      -1.313569777960254,
      -1.3461218726335424,
      -1.3306999342284047,
      -1.27731884163853,
      -1.316062962047902,
      -1.31740502856235,
      -1.3235084456345252,
      -1.2758622435124685,
      -1.3009339312678345,
      -1.3043659376804166,
      -1.3140837511922234,
      -1.3032926621851517,
      -1.2939543646134652,
      -1.224477899541499,
      -1.3036476525270673,
      -1.2686168207587338,
      -1.2911097823875093,
      -1.3123730321452294,
      -1.2830710894678916,
      -1.3001692762164245,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.19337528160701511,
      0.18274305647923184,
      0.18950225868239956,
      0.19604154485722836,
      0.2079675955775051,
      0.1589382552098555,
      0.1746929557133634,
      0.19525186190713972,
      0.17103894113716755,
      0.17133387839032732,
      0.16091769163208847,
      0.18157563797355625,
      0.17678118381742136,
      0.1950561099480801,
      0.17603960754067005,
      0.14615586231433486,
      0.16618605436237077,
      0.1819334851404677,
      0.17866051310786935,
      0.17826205516706173,
      0.1609023993879295,
      0.16628607450357613,
      0.1885900366231481,
      0.1746013649161431,
      0.17254745875244692,
      0.15691867756807465,
      0.17259947569676415,
      0.125974188901941
    ],
    [
      0.4446221073425056,
      0.0,
      0.5520455984156398,
      0.5941755242651889,
      0.48142078176696446,
      0.47272204588676603,
      0.4195320019089419,
      0.4596175038632804,
      0.5916937256294179,
      0.4279211092968467,
      0.5083349108898734,
      0.3494375851743665,
      0.5381481159479995,
      0.5721133413767012,
      0.4242066195757499,
      0.5640149345415753,
      0.42401538704556985,
      0.4104171000195098,
      0.4398836586245325,
      0.4728956904530248,
      0.4730083118246491,
      0.3969013799457297,
      0.39682979258481055,
      0.5163727600795558,
      0.44396305044364714,
      0.41106825966640703,
      0.40631443412905766,
      0.5014381886795531,
      0.3823493297419793
    ],
    [
      0.372137026099324,
      0.4021257565477001,
      0.0,
      0.4035484162756078,
      0.4509767408329124,
      0.40758067774006657,
      0.40623608754500906,
      0.41359174513391084,
      0.46909742233801865,
      0.3924852461245456,
      0.36003197815307963,
      0.29719050741966635,
      0.41534904326510325,
      0.4028187503039564,
      0.39583914456546654,
      0.447337593470746,
      0.3961620584927257,
      0.37835339358127973,
      0.4242262641283676,
      0.3425106015253687,
      0.4472079913420892,
      0.33907150799329555,
      0.38563328086544857,
      0.38894939078604884,
      0.4100535423544307,
      0.3740627469315969,
      0.3753590520718115,
      0.4370372042754085,
      0.3003577408750866
    ],
    [
      0.3639552951067164,
      0.43374298460838756,
      0.4679314891706021,
      0.0,
      0.45152602363756555,
      0.46058299180769136,
      0.35632006476162625,
      0.391508038756752,
      0.4738883352418197,
      0.41358453377186466,
      0.4414492307868245,
      0.36713367906691885,
      0.5032860067476723,
      0.5470415372379782,
      0.3822987701440792,
      0.44255299739344856,
      0.40511229412954464,
      0.3825372467576573,
      0.43047917487917875,
      0.377147783825033,
      0.5001244791939905,
      0.35817789417569457,
      0.36414269658041887,
      0.4131757087746568,
      0.39008579199986104,
      0.3934106686061163,
      0.352392926261172,
      0.4644172228201282,
      0.33908552812115755
    ],
    [
      0.3654891431612828,
      0.3894655902419124,
      0.5155002358536678,
      0.45193335006248314,
      0.0,
      0.37972087715528646,
      0.40449984148834095,
      0.442772903777497,
      0.4049373043617226,
      0.45572525721575174,
      0.37992157292949513,
      0.3768656094439222,
      0.36530713913098634,
      0.4558247429562392,
      0.3770517030322964,
      0.39882396925268093,
      0.48566145248321635,
      0.39957162467424,
      0.43880534170616947,
      0.40642381797500216,
      0.48312965288961607,
      0.3746139400255466,
      0.41981577544431103,
      0.40435638055954737,
      0.4722985117330616,
      0.4181805020193219,
      0.37829722663951526,
      0.404381149867427,
      0.34611965980624837
    ],
    [
      0.27068310725330735,
      0.35543519226882814,
      0.4278383142433473,
      0.41397000532007744,
      0.4109090773455668,
      0.0,
      0.4068611649196199,
      0.45902316535508825,
      0.4307267952623268,
      0.372266447088913,
      0.36786914866945586,
      0.27004309195179,
      0.4655394516250677,
      0.4743574847613563,
      0.4141936294328479,
      0.35748558245858586,
      0.32852718970315387,
      0.3756734339422685,
      0.37789144437349864,
      0.3061104910232326,
      0.3544574332676522,
      0.3047049858677655,
      0.30558863106762946,
      0.3923631162375769,
      0.3768062610970224,
      0.3642659787123663,
      0.36188571677781023,
      0.31195188923068495,
      0.272541886317027
    ],
    [
      0.29563457456632647,
      0.33357705259625137,
      0.4043170764752908,
      0.35247331628187717,
      0.3856274057951752,
      0.4415971578865938,
      0.0,
      0.4056156831184523,
      0.41067730985910034,
      0.37998930289232624,
      0.32823340785670596,
      0.3058716162383319,
      0.4127214311992864,
      0.376241478441079,
      0.41109640722602525,
      0.3528062802923764,
      0.3323574786139589,
      0.3923589820187343,
      0.3493514841573502,
      0.3382656577299641,
      0.3888420386418132,
      0.3864540973160959,
      0.3528331451820539,
      0.3669519981765299,
      0.33258464135857735,
      0.3952623542699223,
      0.3934049279175693,
      0.36605608236525367,
      0.30358012318138416
    ],
    [
      0.2831573589772747,
      0.35865378899290357,
      0.4043575672177673,
      0.35014735661983676,
      0.379218332192939,
      0.4068110137158918,
      0.36218813549943074,
      0.0,
      0.39249243091408603,
      0.3807951369630851,
      0.34316716335451924,
      0.25745458660016474,
      0.3854640082193874,
      0.4108034036728274,
      0.4108234112503957,
      0.3271407408482727,
      0.3204488662363345,
      0.3636696597013047,
      0.37822961497954344,
      0.33530429073776835,
      0.37400363384668767,
      0.33942822845838005,
      0.330211115744909,
      0.3556059628642856,
      0.33096125823060496,
      0.37169658056461974,
      0.36584545226954424,
      0.33930474200636596,
      0.28463321081255843
    ],
    [
      0.30150835142174937,
      0.38631120433047617,
      0.4590949266598239,
      0.40471549882665214,
      0.3887614459896471,
      0.3885762361170335,
      0.34531027418559224,
      0.41420078786099235,
      0.0,
      0.3166519771849836,
      0.3833419889369545,
      0.28570360263623007,
      0.43721573686264104,
      0.3828747345133492,
      0.5019220933036144,
      0.4050145382820125,
      0.3539330124112916,
      0.39971145002070263,
      0.38719285400912473,
      0.32586899070259023,
      0.4437181314369496,
      0.32660756565376237,
      0.3610607576344447,
      0.4571997073135894,
      0.33384991987620505,
      0.35505465031374817,
      0.3343538727845481,
      0.4516746848164326,
      0.29769695226900295
    ],
    [
      0.3682543769891484,
      0.3837013671334788,
      0.47038585618256423,
      0.46141514532165906,
      0.509162072408077,
      0.4237337776138994,
      0.42770380906533645,
      0.4479121376951962,
      0.38011539659516425,
      0.0,
      0.3657617277017349,
      0.3043578220673593,
      0.4108493450801438,
      0.4389018046081432,
      0.36259810658169145,
      0.38331708762425976,
      0.44717881386826774,
      0.38679278556806285,
      0.39395442880910436,
      0.36487951931460283,
      0.5054087065043131,
      0.371362573633804,
      0.41825519527295585,
      0.39114436686694454,
      0.46753487068032684,
      0.44301491315032004,
      0.43487180655544844,
      0.38408122506280096,
      0.3259122406759647
    ],
    [
      0.33483684525911794,
      0.5557870822024362,
      0.4848865203863033,
      0.5366926638641081,
      0.46738182470122736,
      0.5126488188965455,
      0.4134207892483257,
      0.4213210082816301,
      0.5376636124773375,
      0.3742148002037944,
      0.0,
      0.32548043036137186,
      0.5682122821494997,
      0.547568121028785,
      0.4662013534316902,
      0.5461810304188546,
      0.43364880294881347,
      0.44386189089710104,
      0.49340929663041755,
      0.41276255321142474,
      0.4405049639133558,
      0.39613798839638004,
      0.4096798973787128,
      0.44355134118304473,
      0.42541931153810975,
      0.42304024906547655,
      0.4763603118805584,
      0.4339713422443967,
      0.3964135689512329
    ],
    [
      0.2054108671757855,
      0.2315542637424346,
      0.2575398177564361,
      0.2534220496028048,
      0.25960389572100007,
      0.2496652796606067,
      0.24371296677410426,
      0.2315375747236612,
      0.25647873821793077,
      0.20827521349690437,
      0.25813004055538613,
      0.0,
      0.21963754902297983,
      0.2572239508218692,
      0.23701826535901405,
      0.22622108033817878,
      0.22826440164433115,
      0.2182306510179839,
      0.23356087551780136,
      0.2345468426190913,
      0.24894823972257218,
      0.26360637274523624,
      0.24235273393844792,
      0.22017631775003332,
      0.2240254614237529,
      0.24310034577873263,
      0.23562268631342964,
      0.2439410272767344,
      0.2387266609848422
    ],
    [
      0.33747049780728844,
      0.3776949258170206,
      0.4238508149590303,
      0.46968876379090596,
      0.3640657740954778,
      0.49502302823008715,
      0.39212250681835603,
      0.4088905479283014,
      0.44436243051796587,
      0.3615240113656455,
      0.4321053958615788,
      0.2404791749155779,
      0.0,
      0.4827540768740657,
      0.36868813822970026,
      0.4096254539683126,
      0.31457566716196195,
      0.3845481898844043,
      0.4127009053705395,
      0.3032803437517544,
      0.38174798524435594,
      0.36752461900340316,
      0.3262858798814128,
      0.35483864919547936,
      0.33875610916521004,
      0.35531597737873,
      0.3641519781149385,
      0.35249758647168106,
      0.3059867754390322
    ],
    [
      0.42110340170631533,
      0.49017173745975606,
      0.5286395137756907,
      0.5581442392562053,
      0.5027095217070663,
      0.5503277138645066,
      0.47425048577322815,
      0.5183903828888252,
      0.5258967812799396,
      0.46720097752041845,
      0.49386443375529687,
      0.3266076216633347,
      0.5453080296027253,
      0.0,
      0.5145470958326075,
      0.46591748272975697,
      0.47645308163758227,
      0.49368611418127784,
      0.5123796323583971,
      0.45047815827320603,
      0.4640175607353667,
      0.43668892778207535,
      0.4157209998908096,
      0.47641691316321544,
      0.41292871086543803,
      0.4489820719459996,
      0.44733476895433255,
      0.43215829476616885,
      0.39808769914180786
    ],
    [
      0.3040352978997434,
      0.39586884331278505,
      0.4376020548482582,
      0.38074503409590466,
      0.40983754137509965,
      0.4027324871482203,
      0.3690772938910323,
      0.43170542787801547,
      0.5124058217938006,
      0.3640406508262888,
      0.43200202037910884,
      0.30090540588775827,
      0.4441221949449461,
      0.43813206603513755,
      0.0,
      0.37625808903055447,
      0.3648946453849782,
      0.4274343726860732,
      0.398782871034705,
      0.4000556517620366,
      0.3920626543057253,
      0.3579290278011258,
      0.3762563101626737,
      0.4248342446268494,
      0.3623359987772725,
      0.3960565449972411,
      0.3936271676301464,
      0.3724603578250738,
      0.3125943976138006
    ],
    [
      0.3357703517702133,
      0.46999271276258914,
      0.47875373290392154,
      0.454419127893986,
      0.45280918339254006,
      0.40852422149559464,
      0.33131073005298783,
      0.3476488705846974,
      0.46440362884601094,
      0.35035085796643406,
      0.4476938399275763,
      0.26219445911535355,
      0.4345408893662215,
      0.4160471347849066,
      0.3994212440178131,
      0.0,
      0.3808832657497758,
      0.41138301713105707,
      0.435214022346879,
      0.38968889773239024,
      0.35246630575412286,
      0.32433848582427016,
      0.34717915497363383,
      0.4218143241243233,
      0.358931726422995,
      0.34778475163921563,
      0.3560160003343409,
      0.38458901511579,
      0.34369150087390343
    ],
    [
      0.2606932663262629,
      0.3304760764396839,
      0.34402032160503926,
      0.35989429701687103,
      0.3573315425827024,
      0.28497757279802083,
      0.2529437351937158,
      0.29114634270172046,
      0.31600030310037996,
      0.30791577849732676,
      0.3169213387479086,
      0.27355644856375116,
      0.28064436577467466,
      0.39083290020805506,
      0.34235632222983803,
      0.3397312032190303,
      0.0,
      0.32567459531941134,
      0.33918530044821615,
      0.3591141808700251,
      0.4186893803408407,
      0.2353246423049422,
      0.3173539483369545,
      0.3656093350039584,
      0.3916921891081808,
      0.2720273124444992,
      0.3131992091093556,
      0.3550802144207921,
      0.2694389717922099
    ],
    [
      0.3289910671345768,
      0.41491540760776524,
      0.44798240430991854,
      0.4165208609486437,
      0.42524882566413713,
      0.48291626713062463,
      0.37431395564481496,
      0.373054737783701,
      0.4419665535409967,
      0.33538993694398034,
      0.39725396941362745,
      0.31830037635425446,
      0.4571205831294023,
      0.42381815011239254,
      0.4519873649168591,
      0.440548254733091,
      0.37163707123847645,
      0.0,
      0.40558019539782353,
      0.4254023726289313,
      0.44560683829220027,
      0.32678744617113353,
      0.3250598605462838,
      0.41193002697857994,
      0.36230283377283046,
      0.3580239347245706,
      0.3655418536815791,
      0.44978757139495307,
      0.3470316886416138
    ],
    [
      0.4274147809954145,
      0.4809408853164292,
      0.5418892917778744,
      0.49676187020795504,
      0.5473665762347066,
      0.4607662703834381,
      0.39030753974895194,
      0.4499251549686931,
      0.46364300968204564,
      0.4718802426408908,
      0.47899383914635574,
      0.3335198381018367,
      0.49052840107865725,
      0.52377246597896,
      0.40557889917760215,
      0.49298767662792686,
      0.4583058767193977,
      0.43444801723231485,
      0.0,
      0.43582765472303886,
      0.45594600451521816,
      0.40545179425905653,
      0.47039034234725907,
      0.45473781882815145,
      0.47599399752615223,
      0.43466019352330254,
      0.4614853830306138,
      0.4107201605357058,
      0.39911829803641696
    ],
    [
      0.3351354868592944,
      0.38388730424009787,
      0.4159675286223117,
      0.39754506743523166,
      0.4053493655125653,
      0.3840221634666834,
      0.3437448560132843,
      0.3646620318244369,
      0.3891171038850523,
      0.3130769315166264,
      0.3626627386653256,
      0.3257590628849072,
      0.3646525762911599,
      0.40074256066978275,
      0.400601290205886,
      0.440212502314135,
      0.40289273340725495,
      0.42373199681041074,
      0.40198989743247937,
      0.0,
      0.37973951970518494,
      0.3454943571460043,
      0.3783618064658105,
      0.46531426866558645,
      0.3966479234670124,
      0.3543001046060079,
      0.3813661629813099,
      0.4449505696973022,
      0.3548010745004906
    ],
    [
      0.2936817953263733,
      0.33436231687575924,
      0.3451954269969717,
      0.3850078483414625,
      0.40782801460825646,
      0.36927919420198885,
      0.3551494572654923,
      0.3506269833402884,
      0.38797685860521014,
      0.379549668419197,
      0.3409839893656468,
      0.2507865787603938,
      0.33991925594835815,
      0.4087151539216116,
      0.38576161135233544,
      0.3494724023399298,
      0.41407386226885845,
      0.38321078069003756,
      0.34196502479710245,
      0.33279426995790273,
      0.0,
      0.33143970168997017,
      0.3233493750992471,
      0.36429718563789715,
      0.3141828831180704,
      0.3792412059150607,
      0.35935340209799294,
      0.4845066401557647,
      0.26240864773838846
    ],
    [
      0.2663238091403086,
      0.26674336651914277,
      0.3582204770742996,
      0.3471943525760053,
      0.3275554612761693,
      0.3819173266907576,
      0.3987895808004711,
      0.40580311615622344,
      0.3701656098991586,
      0.3337296841430948,
      0.28900865625645555,
      0.34093585087451306,
      0.3583958923521031,
      0.3571197482709183,
      0.34382211511236216,
      0.3014174044655391,
      0.31516821710248144,
      0.2882642864233731,
      0.3008127251540431,
      0.30052883005867925,
      0.3543881403379219,
      0.0,
      0.2830078232874669,
      0.33992997820048676,
      0.28958407236617245,
      0.3578481559841329,
      0.345396431918096,
      0.3268068500152792,
      0.3689583829331722
    ],
    [
      0.33227101159661054,
      0.4035070811714212,
      0.45370070023308595,
      0.4147488261118035,
      0.43123166445260996,
      0.3996112569638348,
      0.40332235970579156,
      0.4210256022502388,
      0.4340205608277494,
      0.444100930021476,
      0.4044643275155462,
      0.37670683096581636,
      0.4250764489893366,
      0.4483832722425103,
      0.4467618869442518,
      0.4220795530023085,
      0.47572479399467404,
      0.4409033176762758,
      0.4358770523429021,
      0.4158654123333654,
      0.4055240899059187,
      0.40706732543650004,
      0.0,
      0.43347466474351903,
      0.45240824552879855,
      0.45008695955004296,
      0.43474758667628577,
      0.4355590228697319,
      0.3735264212249041
    ],
    [
      0.32282173624480204,
      0.36790892163533373,
      0.3702492620235842,
      0.3528158134053596,
      0.34435498132768405,
      0.3964801953136936,
      0.3317313033134459,
      0.38231233191258207,
      0.47719283700408077,
      0.2920861234747689,
      0.36419891363843404,
      0.300719662172932,
      0.35136244495106816,
      0.38038829497627624,
      0.40695573482680203,
      0.34047884204302514,
      0.3874948509593579,
      0.34450075293220217,
      0.3256544566406345,
      0.4107080242181289,
      0.4016488998277348,
      0.34851181625816463,
      0.3124315651260996,
      0.0,
      0.31834812416431957,
      0.3384872387254645,
      0.3156987121042476,
      0.4411919687246082,
      0.27548947786760314
    ],
    [
      0.3572255879947701,
      0.40943841918101165,
      0.43974063946474073,
      0.3904222110829916,
      0.4408697454732533,
      0.37064271218157563,
      0.3306574494926213,
      0.37543113985562804,
      0.37733444351424517,
      0.3811785624947901,
      0.3686604912003566,
      0.3102686660361098,
      0.370451548458681,
      0.38492472038223524,
      0.3786597942392569,
      0.3592007804673276,
      0.4850354833711137,
      0.39571793876216166,
      0.4263459729787151,
      0.3643002820634753,
      0.3679357076366252,
      0.29966765227746817,
      0.4319411198971581,
      0.3852831570613364,
      0.0,
      0.34488423949202107,
      0.34401614746009,
      0.35114252720171324,
      0.334079976240786
    ],
    [
      0.3251691572097375,
      0.3025805922313054,
      0.44517259015427935,
      0.351528180947954,
      0.38986344434178855,
      0.38224662360762474,
      0.3967617518699085,
      0.45418143593471805,
      0.3581895950190592,
      0.36583014720457663,
      0.3374287853473663,
      0.27880791003141003,
      0.374349030590891,
      0.3403093785498372,
      0.37628913662299257,
      0.3673115807961713,
      0.34438130126744837,
      0.38658329997358276,
      0.37153231606733406,
      0.3481954035527277,
      0.46183072530817193,
      0.3972047567933894,
      0.3704659074237153,
      0.36527508093292327,
      0.33941841024531816,
      0.0,
      0.40096167048595244,
      0.3863755195948313,
      0.30283523485912367
    ],
    [
      0.3278458382148235,
      0.38654564253151236,
      0.4384142169284544,
      0.3956807558276565,
      0.43469142721880005,
      0.45307838184046756,
      0.45497866715418755,
      0.42696597895209876,
      0.41540428985795774,
      0.42823501934254216,
      0.4265083562454255,
      0.3008856833025373,
      0.42369331643254093,
      0.43333550600054527,
      0.39667689986458443,
      0.39500416409491534,
      0.4023941764192436,
      0.3666246168768772,
      0.4181253871572381,
      0.39215052651096505,
      0.4578342511376077,
      0.39508396196433004,
      0.40898492655093155,
      0.3772558786446145,
      0.40486966882067943,
      0.4202300640312,
      0.0,
      0.3673350558909019,
      0.36205337384171643
    ],
    [
      0.29583262793290643,
      0.29440911376153167,
      0.38600536981348266,
      0.3553793452288576,
      0.3610282090368919,
      0.3426330979170942,
      0.3534297184716224,
      0.3378971281956651,
      0.41100303227755997,
      0.30716306382901215,
      0.28591071083700137,
      0.27192122729672286,
      0.3292815770817452,
      0.320247878309996,
      0.3445731811928381,
      0.3462309966318631,
      0.33495884999209746,
      0.38204443519203224,
      0.3367317688207738,
      0.39733074320636397,
      0.4738608065953236,
      0.306134891221171,
      0.2977347159743071,
      0.38929450244553054,
      0.3318095957079348,
      0.32999442895792863,
      0.3172697951020236,
      0.0,
      0.3011860926306218
    ],
    [
      0.3515423916547282,
      0.3862060158433469,
      0.4057916466229263,
      0.41373022764045,
      0.43379305620166764,
      0.41230354223577326,
      0.38616287707537245,
      0.3665700026870109,
      0.3911423935977434,
      0.35859029892445493,
      0.37401223732959266,
      0.42739332991946744,
      0.3886492095100953,
      0.3873071429956474,
      0.3812037259234722,
      0.4288499280455289,
      0.40377824029016285,
      0.4003462338775807,
      0.39062842036577394,
      0.40141950937284565,
      0.4107578069445321,
      0.48023427201649826,
      0.4010645190309301,
      0.43609535079926354,
      0.4136023891704881,
      0.392339139412768,
      0.42164108209010576,
      0.40454289534157284,
      0.0
    ]
  ],
  "row_avgs": [
    0.17503119667554204,
    0.4669808303257087,
    0.3941189611085025,
    0.4166818355130199,
    0.41412479556738535,
    0.36892750412763803,
    0.3676708039876573,
    0.35507203755327466,
    0.3796116409412194,
    0.4097343313796705,
    0.45433066789821613,
    0.2382333632036459,
    0.377519864544365,
    0.4731575840182626,
    0.39209980264122696,
    0.38956648046084086,
    0.32185111051801313,
    0.39732215745849153,
    0.45547722440515587,
    0.38416889233184415,
    0.3562542691012703,
    0.33277986947817245,
    0.42220632875990394,
    0.3572222602075871,
    0.3776948969986521,
    0.3686099631058622,
    0.40396021541626265,
    0.34076060370217487,
    0.4017749244614214
  ],
  "col_avgs": [
    0.3296077556845253,
    0.37569210453493984,
    0.42456558753409074,
    0.40543651453324786,
    0.4077344099910613,
    0.40461030455456687,
    0.3672777735318381,
    0.3917153828615239,
    0.4186874352161793,
    0.3626711018038467,
    0.3735803247091772,
    0.305007298194246,
    0.40290719684739035,
    0.41354932085187796,
    0.3900782162335768,
    0.3893664913203957,
    0.3797899191734431,
    0.37880236565036746,
    0.3850865668453255,
    0.3650898933300289,
    0.4093454397977715,
    0.3493875932696115,
    0.35493847682830054,
    0.3930299452238098,
    0.3694998879956649,
    0.3694631082556878,
    0.36762265867642674,
    0.38966280301299355,
    0.3187385394290721
  ],
  "combined_avgs": [
    0.2523194761800337,
    0.42133646743032427,
    0.4093422743212966,
    0.4110591750231339,
    0.4109296027792233,
    0.38676890434110245,
    0.3674742887597477,
    0.3733937102073993,
    0.39914953807869935,
    0.38620271659175864,
    0.41395549630369666,
    0.271620330698946,
    0.39021353069587766,
    0.4433534524350703,
    0.3910890094374019,
    0.3894664858906183,
    0.3508205148457281,
    0.38806226155442947,
    0.42028189562524065,
    0.37462939283093655,
    0.3827998544495209,
    0.341083731373892,
    0.3885724027941022,
    0.37512610271569846,
    0.3735973924971585,
    0.369036535680775,
    0.38579143704634467,
    0.36521170335758424,
    0.36025673194524677
  ],
  "gppm": [
    590.9129798720514,
    595.2161302838305,
    572.5188244096247,
    581.711938186097,
    578.8744293418943,
    581.3495375926008,
    594.9626646719926,
    584.3562474038273,
    574.6653668400903,
    599.4694917572211,
    595.74934426263,
    623.0160970865667,
    580.6918641956397,
    579.3561229811271,
    585.2845079134358,
    587.1793145153646,
    587.5112565803025,
    594.2611278085066,
    590.4113403129853,
    593.7070804129886,
    574.8070273027655,
    607.9357955553371,
    603.4222422851591,
    582.7400736330588,
    593.496223759301,
    596.5084663536372,
    596.5868059914707,
    586.4537273131479,
    620.9055996160594
  ],
  "gppm_normalized": [
    1.3647271991209953,
    1.3498654972705106,
    1.29762826358337,
    1.3146630021911563,
    1.3068925712043529,
    1.3182192929806607,
    1.3529115650896972,
    1.3215883279098348,
    1.2979257663670978,
    1.3574715210037387,
    1.3494304221549622,
    1.415115676889089,
    1.3148930961602878,
    1.3084381559320892,
    1.3233309233907828,
    1.331265026994792,
    1.3264172028228327,
    1.3388256054396253,
    1.3335393089255814,
    1.3483659161352717,
    1.2893152694109316,
    1.3754398937295425,
    1.3601214804355675,
    1.3188011599483551,
    1.3401907327911036,
    1.349677041005641,
    1.3498598141710656,
    1.3216306979894985,
    1.4057603506037706
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398,
    556,
    436,
    455,
    520,
    439,
    403,
    443,
    430,
    421,
    442,
    402,
    469,
    440,
    412,
    457,
    437,
    417,
    397,
    430,
    478,
    440,
    351,
    389,
    390,
    417,
    413,
    412,
    418,
    373,
    589,
    405,
    404,
    436,
    416,
    426,
    434,
    447,
    434,
    406,
    406,
    524,
    449,
    391,
    456,
    425,
    488,
    400,
    372,
    431,
    460,
    423,
    389,
    472,
    413,
    395,
    393,
    447,
    357
  ],
  "response_lengths": [
    2931,
    2372,
    2435,
    2500,
    2424,
    2506,
    2537,
    2545,
    2529,
    2257,
    2359,
    2946,
    2653,
    2250,
    2543,
    2505,
    2750,
    2295,
    2219,
    2426,
    2512,
    2496,
    2291,
    2787,
    2412,
    2186,
    2220,
    2487,
    2070
  ]
}