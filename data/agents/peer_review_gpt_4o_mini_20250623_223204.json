{
  "task_description": "Scientific paper peer review task",
  "agent_perspectives": [
    {
      "condition": "Reference",
      "reading": null,
      "strategy": null
    },
    {
      "condition": "Faithful",
      "reading": "Take detailed, accurate notes on the paper for an ICLR style review. Focus on precisely capturing the actual methods, results, and contributions without any distortion. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Objective Analysis",
      "reading": "Take comprehensive notes on the paper's technical content, experimental design, and results. Document both strengths and limitations objectively. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Thorough Evaluation",
      "reading": "Take meticulous notes covering all aspects of the paper including theoretical foundations, experimental methodology, results, and implications. Note specific equations, theorem numbers, and experimental details. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    }
  ],
  "tasks": [
    {
      "context": "Under review as a conference paper at ICLR 2023\n\nRECURSION OF THOUGHT: DIVIDE AND CONQUER REASONING WITH LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWith the recent advances in language models, attempts are being made to apply them to solving multi-step reasoning problems. A major breakthrough in this line of research is to let language models generate intermediate steps, often called Chain of Thought (CoT), before producing a final answer. However, language models have an upper bound on the context size, i.e., the number of input tokens, such as 2048 for the recent GPT-3 and PaLM. Although several thousand tokens are enough to handle various tasks, solving more complex reasoning tasks can require orders of magnitude more tokens. Therefore, the context limit imposes a fundamental limit on the model\u2019s reasoning capability. Inspired by human\u2019s incredible reasoning ability based on abstraction and recursion, we propose Recursion of Thought (RoT) as a model-agnostic framework with the novel paradigm of teaching a language model to divide and conquer complex problems by recursively creating multiple contexts. Since RoT casts the context-related operations as tokens, a language model can trigger the recursion operations by simply producing the corresponding tokens. On multiple arithmetic and algorithmic reasoning tasks, we demonstrate that RoT dramatically improves the recent large-scale language model GPT-3 to solve extremely complex problems. Moreover, RoT can make tiny, randomly initialized Transformers or LSTMs to solve problems that even humans find daunting.\n\n1\n\nINTRODUCTION\n\nRecently, language models (LMs) have become a prominant direction to solve reasoning. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or a chain of thought, before producing the final answer. Directly answering a question would require a model to fully solve the problem in a single forward pass, meaning the range of solvable problems is severely limited by the model\u2019s capacity. On the other hand, generating CoT before the answer allows the problem\u2019s complexity to be spread across the CoT, making each token generation more straightforward given the previous tokens. This is closer to how humans solve complex problems, as we think step by step, instead of producing an answer reflexively.\n\nAlthough CoT seems promising, there is a critical issue that significantly limits its utility: the effective context size of sequence models cannot grow unbounded. In this work, context refers to the set of input tokens that a model is conditioned on when generating output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Transformers (Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and RNNs (Hochreiter & Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art language models, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022), limit the maximum context length by up to 2048 tokens. However, the length of intermediate steps can grow rapidly with the problem\u2019s complexity and exceeds the context limit. Since CoT can handle a problem only if the process of solving it fits into a single context, the range of problems that CoT can handle is severely constrained by the context limit. This issue must be\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\novercome to solve more challenging and useful reasoning problems, whose solutions may require millions of tokens.\n\nHumans can handle this issue by using abstraction and recursion. We divide a large problem into smaller subproblems and focus on each subproblem while solving it, instead of considering the entire problem at every step. We can further subdivide a subproblem into even smaller subproblems. With this intuition, we propose Recursion of Thought (RoT) as a model-agnostic framework for recursively solving multi-step reasoning problems. The key feature of RoT is to grant the model the ability to recursively create and utilize multiple contexts for subproblems. We achieve this feat by introducing several special tokens that a model can output to control its context. During inference, the model recursively solves the problems by producing appropriate tokens at the right time. Moreover, RoT supports tail recursion, which enables general computation with an indefinitely long chain of recursion.\n\nWe demonstrate RoT on four basic arithmetic operations (addition, subtraction, multiplication, and division) and four algorithmic tasks (longest common subsequence, longest palindromic subsequence, 0-1 knapsack, and matrix chain multiplication) to show its generality. Without any taskspecific component, such as a calculator, all tasks are formulated as autoregressive sequence modeling problems. These tasks require a model to generalize by just seeing a tiny fraction of the problem space since the space is combinatorially large. For example, even in simple arithmetic operations, two 6-digit operands result in one trillion possible combinations. Hence, we evaluate whether a model understands the underlying rules, instead of brute force memorization. In our experiments, the range of problems that CoT can handle is seriously constrained by the context limit. On the other hand, RoT leads language models to achieve near perfect accuracy, even if the problem size increases to the extreme, where solving one problem requires producing hundreds of thousands of tokens. Moreover, the dramatic improvement is not limited to large pre-trained language models like GPT-3. RoT can make tiny, randomly initialized Transformers or LSTMs perform extremely complex reasoning.\n\nThe key messages of this work are summarized as follows:\n\n\u2022 The reasoning capability of current language models is seriously constrained by the maxi-\n\nmum length of a single context.\n\n\u2022 Our Recursion of Thought (RoT) unleashes the reasoning capability of language models by letting them recursively create and utilize multiple contexts of subproblems, following the principle of divide and conquer.\n\nIn the supplementary file, we provide the source code to fully reproduce our experiments.\n\n2 RELATED WORK\n\nChain of Thought. Among several prior works on applying language models to reasoning, Scratchpad (Nye et al., 2021) may be the most closely related to our work. It is the first approach to fine-tune language models to produce CoT before generating an answer. It demonstrates its effectiveness on 8-digit addition, polynomial evaluation, and Python program execution. It also mentions the confined context size as a major limitation to be overcome. In order to unlock the full potential of Scratchpad, the authors argue that Transformers should be improved to allow greater context sizes. We solve this exact problem from a completely different perspective, i.e., using multiple contexts to divide-and-conquer. Our approach is more practical and scalable, compared to increasing the context limit. More recently, it has been found that sufficiently large pre-trained language models can be induced to produce CoT, by simply tuning the prompt. For instance, CoT prompting (Wei et al., 2022) adds several QA exemplars with CoT before the main question, encouraging the model to generate final answers in the similar manner. Kojima et al. (2022)\u2019s prompting is even simpler; after a question, they start the answer with \u201cLet\u2019s think step by step,\u201d and then let the model finish the rest. Even without fine-tuning, these methods significantly improve the reasoning accuracy of language models. Minerva (Lewkowycz et al., 2022) utilizes these prompting techniques with a specially curated scientific pre-training dataset to achieve remarkable results on various reasoning benchmarks. However, all of these works are still limited by the maximum context size.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nNeural Programmer-Interpreter (NPI). Unlike language models, NPI (Reed & de Freitas, 2016) interacts with its environment through a series of program execution. It consists of an LSTM core, an encoder for each domain, and a memory of program embeddings. At every time step, the LSTM core takes a program embedding, arguments, and an observation of its environment to produce the next program embedding and corresponding arguments. Cai et al. (2017) combine NPI with recursion and show that recursion plays a critical role in generalization. Since NPI requires full execution traces for training, there are multiple works to relax this requirement using reinforcement learning (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nSystem 1 Approaches for Reasoning. Kahneman (2013) classifies cognitive tasks into two categories: System 1 and System 2. System 1 refers to fast and reflexive thinking, while System 2 refers to sequential reasoning. It is hard to define strict criteria to distinguish between System 1 and System 2 approaches. We classify a model as System 1 if it directly outputs an answer, while System 2 generates its process, as well. In that sense, RoT, CoT and NPI are System 2 approaches, while there have been various System 1 approaches to solve symbolic reasoning. Zaremba & Sutskever (2014) train LSTMs with curriculum learning to solve integer addition up to nine digits. Kaiser & Sutskever (2016) propose a convolutional architecture called Neural GPU that performs binary addition and multiplication. Trained on 20-bit problems, this model operates like a digital circuit for the arithmetic operations, which can generalize up to 2,000 bits. Similarly, Yan et al. (2020) solve 8-bit binary addition and 12-bit binary multiplication with Transformers. If Neural GPUs are like digital circuits, Neural Arithmetic Logic Units (Trask et al., 2018) are like analog circuits for arithmetic. They represent numerical quantities with the activation values of neural networks and design a clever architecture for arithmetic operations. Although these System 1 approaches fall behind System 2 methods in terms of generality, they can be highly efficient in specific domains. We believe future systems will often be a hybrid of System 1 and System 2, harnessing the advantages of both types.\n\n3 RECURSION OF THOUGHT\n\nThe main idea of Recursion of Thought (RoT) is to let the model recursively solve small subproblems in separate contexts, keeping each reasoning step simple and learnable. Our RoT is model-agnostic and general enough to be combined with any kind of sequence model that supports autoregressive generation. The only requirement is that the model should be able to infer p(xi+1\u2223X1\u2236i), the probability of the next token xi+1 given a sequence X1\u2236i = [x1; ...; xi]. Therefore, sequence models such as Transformers, RNNs, or more advanced ones can all be used in the RoT framework. RoT teaches a sequence model to solve a problem using the divide and conquer paradigm in a supervised manner. That is, we assume that ground truths for the intermediate steps of how to recurse are readily available, as in Scratchpad (Nye et al., 2021) or NPI (Reed & de Freitas, 2016).\n\nFor better understanding, we discuss RoT in the reverse order of the pipeline. In \u00a73.1, we first describe how to perform RoT inference with a fully trained model. In \u00a73.2, we introduce the training process. Finally, in \u00a73.3, we discuss how to recursively divide the problems and build the training data for intermediate steps automatically.\n\n3.1\n\nINFERENCE\n\nRecursion of Thought (RoT) grants a language model the ability to control the recursion process. For basic recursion control, we first introduce the following special tokens: GO , STOP , and THINK . GO and STOP respectively mark the start and end of a problem sequence. They can be nested inside another GO - STOP pair to indicate a subproblem. THINK initiates a recursion procedure. RoT teaches a model how to use these tokens so that it can perform divide-and-conquer problem solving. We formulate each inference context of a QA problem, denoted X, as the following concatenation:\n\nX = [Q; Qsub,1; Asub,1; . . . ; Qsub,N ; Asub,N ; A]\n\n(1)\n\nwhere Q and A are the main question and answer sequence, and Qsub,\u2217 are the questions and answers of the top-level subproblems. During inference, a model is given Q and tasked to generate the rest. Questions (both Q and Qsub,\u2217 ) start with a GO token, and answers (both A and Asub,\u2217 ) end with a STOP token. For trivial cases, i.e., the base cases of recursion, the context\n\nand Asub,\u2217\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An example of the Recursion of Thought inference in Algorithm 1. Each table represents an inference context X k in order of creation, which has the structure of Eq.1. For each context, the model is given Q and tasked to generate the rest, one token at a time. The model outputs the THINK token when it needs to generate Asub,\u2217 , the answer of a subproblem. The THINK token triggers a recursive process that solves the subproblem in a new context and returns the answer.\n\nFigure 2: The target sequence Y 1 is produced from X 1 in Figure 1 by Algorithm 2. Given X 1 model is trained to output Y 1 make sure \u2225X\u2225 = \u2225Y \u2225.\n\n1\u2236i, the i+1, except for PAD s that are ignored. Note PAD is a dummy token to\n\ncontains no (Qsub,\u2217 ) pair. A subproblem can have smaller, lower-level subproblems recursively, but only the top-level subproblems remain in a context. As a result, we can abstract away the details for solving the subproblems and keep only the high-level results in the current context.\n\n, Asub,\u2217\n\nFor tail recursion, where the last subquestion\u2019s answer becomes the final answer, we additionally introduce the TAIL token. If TAIL is used in the place of a GO token in the last subquestion Qsub,N , its answer Asub,N is treated as the final answer A, and the context X does not have duplicate A. Algorithm 1 summarizes the inference process. Figure 1 presents a example of solving 408 + 351 for better understanding. More detailed illustrations of inference can be found in Appendix A. The RoT function (L1) takes a fully trained model with a question Q as input and returns the answer A as output. The procedure starts by initializing the context X with the original question Q (e.g., GO 4 0 8 + 3 5 1 = in Figure 1). ians is the starting index of the answer, which is initialized to \u2223X\u2223 + 1, where \u2223 \u22c5 \u2223 is the length of a sequence. Then, in the main loop, the model iteratively generates a next token x from X, which is appended to the end of X. After the initialization, the model is expected to (i) generate answer A directly or (ii) output GO . If the model immediately generates A and finishes it with a STOP , the answer is returned (L9), which is the base case of the recursion. Up to this point, the algorithm is identical to common language models.\n\nOn the other hand, if the model decides to output GO , which signals the start of a subproblem, its index is stored in igo (L11). Since a subproblem has started, the next several tokens from the model should constitute the question of the subproblem. In Figure 1, the first subproblem of the main context X 1 is adding the last digits, i.e., 8 + 1. Once the subquestion is generated, the next step is to find an answer to it. This is the pivotal moment: instead of producing the answer, the model outputs the THINK token, which initiates the recursion with a new context (L16-23). First, we separate the subquestion Qsub, starting from igo (L16). Second, using it as an input, we trigger the recursive call (L17) to obtain the answer Asub. As shown as red arrows in Figure 1, this call creates another context X 2 and initializes its question part with the subquestion. Then, the same inference process is executed inside the new context, sometimes running more recursions. In the case of 8 + 1, the answer 9 STOP is immediately returned since it is a base case. Finally, the THINK token is replaced\n\n4\n\nQ408+35GO1=Qsub,1GO8+1=Asub,1THINKTHINKAsub,2GO40+35=Qsub,275STOP9A9STOP75STOPX1GO8+1=QAX2GO40+35=THINKGO0+5=Asub,1Qsub,1Q5STOPTHINKGO4+3=Asub,2Qsub,2A75STOPX3GO0+5=QA5STOPX4GO4+3=QAX57STOP7STOP9STOPQ408+35GO1=Qsub,1GO8+1=Asub,19STOPX1GO8+1=THINKY1PAD75STOPAsub,2GO40+35=Qsub,2THINKGO40+35=75STOP9A75STOP9PADPADPADPAD\u00b7\u00b7\u00b7Under review as a conference paper at ICLR 2023\n\nwith Asub (L21) and the starting position of the answer is updated (L23). Hence, when predicting the next token, the model sees the returned answer as input, instead of the THINK token.\n\nOnce a subproblem is finished, the model can solve another subproblem in the same way, or output the final answer. In Figure 1, the second subproblem is to add all the remaining digits, i.e., 40 + 35. To solve it, the model recursively solves two other subproblems, i.e., adding each digit, before generating the answer. If the model outputs the final answer followed by a STOP token, the answer is returned (L9). If the model starts a subproblem with TAIL instead of GO , it becomes a tail recursion (L14), and its answer is returned directly as the final answer (L19).\n\n3.2 TRAINING\n\nWe teach RoT in a supervised manner; the model is trained with the ground truth (GT) intermediate steps, which also include when to output the special tokens. Each training example is constructed as a pair of a ground truth context sequence X and the corresponding target sequence Y . The GT context X is structured as Equation 1 and automatically built by the algorithms that will be introduced in \u00a73.3 and Appendix D. In this section, we discuss how to construct the target sequence Y for X, and define the training objective.\n\nAlgorithm 2 summarizes the process of converting X to Y , where Y has the same length with X. Refer to Figure 2 for an example. Overall, Y is a copy of X except for the parts corresponding to Q and Asub,\u2217 . Since the question Q is always given in a context, Q is replaced by special PAD tokens (L1), which mean \u201cnothing to predict for this part.\u201d Each subproblem\u2019s answer Asub,n is replaced by a THINK token followed by several PAD s that fill in the rest to make sure \u2223X\u2223 = \u2223Y \u2223 (L4). This way, the model is trained to output THINK instead of the first token of Asub,n. Since the whole Asub,n will be returned from the recursive process and replace the THINK during inference (L17,21 of Algorithm 1), we do not need a training signal for the rest of Asub,n.\n\nGiven a pair (X, Y ), the training objective is defined as follows:\n\nL = \u2212 \u2211\n\nI[yi+1 \u2260 PAD ] log p(yi+1\u2223X1\u2236i) (2)\n\ni\n\nAlgorithm 1 Recursion of Thought Inference\n\nRequire: A sequence model M trained for Recursion of Thought, a question sequence Q\n\n\u25b7 Initialize context with Q \u25b7 Start of answer \u25b7 Tail recursion\n\nx \u2190 M(X) \u25b7 Generate next token X \u2190 [X; x] if x = STOP then\n\nX \u2190 Q ians \u2190 \u2223X\u2223 + 1 t \u2190 f alse while True do\n\n1: function ROT(M, Q) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:\n\nreturn Xians\u2236\u2223X\u2223 else if x = GO then igo \u2190 \u2223X\u2223\n\nelse if x = TAIL then\n\n\u25b7 Mark last GO\n\nigo \u2190 \u2223X\u2223 t \u2190 true \u25b7 Mark tail recursion\n\nelse if x = THINK then Qsub \u2190 Xigo\u2236\u2223X\u2223\u22121 Asub \u2190 ROT(M, Qsub if t then\n\n)\n\nreturn Asub\n\nend if X \u2190 [X1\u2236\u2223X\u2223\u22121; Asub\n\n]\n\n\u25b7 Replace THINK with Asub\n\nians \u2190 \u2223X\u2223 + 1\n\n16:\n\n17: 18: 19: 20: 21:\n\n22: 23: end if 24: end while 25: 26: end function\n\nAlgorithm 2 Creating the target sequence\n\nRequire: Context X = [Q; Qsub,1; Asub,1;\n\n. . . ; Qsub,N ; Asub,N ; A]\n\n1: Y \u2190 PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) \u2223Q\u2223\n\n2: for n in 1...N do 3: 4:\n\nY \u2190 [Y ; Qsub,n Y \u2190 [Y ; THINK PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) \u2223Asub,n\u2223\u22121\n\n]\n\n]\n\n5: end for 6: Y \u2190 [Y ; A] 7: return Y\n\nwhere I is the indicator function that excludes PAD s from training. Simply, it means that the sequence model is trained to output yi+1 as the next token for a given X1\u2236i. Its form is almost identical to the standard language modeling objective: LLM = \u2212 \u2211 i log p(xi+1\u2223X1\u2236i), which is to predict the next token given previous tokens. With this objective, any sequence model is trained in the standard way, i.e., end-to-end via stochastic gradient descent. For decoder-only transformers with causal masks, the training can be efficiently done in parallel for all tokens.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 THE RECURSIVE REASONING PROCEDURES\n\nAs explained in \u00a73.2, we teach the recursive reasoning procedure for a problem type by providing ground truth contexts. To produce these contexts, we design an exemplary reasoning procedure for each problem type and implement it as a teacher program that automatically generates the contexts given a problem. The major desiderata for the exemplary reasoning procedures are two fold: (i) each context should be short, and (ii) the next tokens to generate should be obvious from the previous sequence. Although the definition of \u201cbeing obvious\u201d may vary depending on the model\u2019s architecture (e.g., Transformers vs. LSTMs), the reasoning procedures developed for human generally meet these desiderata. Therefore, our procedures for the arithmetic problems are borrowed from the elementary school math. For example, the addition procedure shown in Figure 1 is a recursive version of adding digits one by one starting from the last digit. In another example of multiplication, an N -digit \u00d7 M -digit multiplication is divided into an N -digit \u00d7 1-digit multiplication and an N -digit \u00d7 (M \u2212 1)-digit multiplication. The N -digit \u00d7 1-digit multiplication is further divided into a 1-digit \u00d7 1-digit multiplication and an (N \u2212 1)-digit \u00d7 1-digit multiplication. For the algorithmic problems, we also borrow standard, well-known algorithms. In Appendix D, we provide the full details of the procedures for each problem type, with Python code snippets of the key parts. Note that our proposals for the reasoning procedures in Appendix D are one of many possible solutions, which are not necessarily optimal.\n\nTraining Data Distribution. We use the same problem distribution for both training and evaluation, since out-of-distribution generalization is not within the scope of this paper. That is, when teaching 6-digit multiplication to the model, both training and test sets are all examples of 6-digit multiplication. The problem distributions are elaborated in Appendix C. Another important detail regarding the training of RoT is that each training example in a batch is a context, not a whole problem. Since RoT generates multiple contexts per problem, often a large portion of contexts can be duplicate (mostly the base cases). Therefore, to build a training batch for RoT, we first sample a top level problem and find the set of unique RoT contexts from the problem. Out of the unique contexts, we randomly sample one context as a training example. We find this simple technique works well, and we do not need more sophisticated method, such as the adaptive curriculum learning in Reed & de Freitas (2016).\n\n4 EXPERIMENTS\n\nSince Recursion of Thought is the first approach of this kind, we mainly compare with two baselines. The first one is to output an answer directly from a question, which we call Without Thought (WT). The other one is to generate all the intermediate steps before the answer without recursion (Nye et al., 2021), which we refer to as Chain of Thought (CoT; not to be confused with the CoT prompting (Wei et al., 2022)) for consistency. We construct the ground truths for CoTs by unraveling the same recursive process which we design for RoT, into a single context sequence (see Appendix B for examples). Therefore, the number of tokens to generate while solving a problem is the same for both CoT and RoT (if we do not count the THINK tokens). However, the sizes of the individual contexts of CoT are far longer than those of RoT due to the recursively nested subproblems. Refer to Appendix I for more detailed analysis of the context sizes. Note that we train these baselines and do not use any prompting technique. When evaluating, we consider a problem to be correctly solved only if all the intermediate steps and the answer are correct. In other words, we impose stricter rules on both RoT and CoT by not counting \u201clucky guesses\u201d as correct.\n\n4.1 THE REASONING PROBLEMS\n\nTo evaluate the reasoning capabilities, we test various reasoning tasks that are grouped into two categories: arithmetic reasoning and algorithmic reasoning. We below provide a rough description of the tasks, whose details can be found in Appendix C. All the reasoning tasks share one characteristic in common: we can easily adjust the problem\u2019s difficulty. Therefore, we can gradually increase the degree of difficulty and see which method fails first. Since the goal of our experiments is to test the reasoning capability of language models, all problems are formulated in pure sequence modeling, with no external program (e.g., calculator) called by the models.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nArithmetic Reasoning. We test four basic arithmetic operations, i.e., addition, subtraction, multiplication, and division, with two non-negative integers. The difficulty of arithmetic problems is represented by the maximum number of digits in an operand. For instance, in 6-digit multiplication, each operand can range from 0 to 999,999. When we sample a problem, we sample each operand from the log-uniform distribution. Compared to the uniform distribution where the samples are highly biased towards extremely large numbers, we get roughly the same ratio of samples for each number of digits. The sampling schemes for each operation are elaborated in Appendix C.1.\n\nAlgorithmic Reasoning. We test four algorithmic tasks with distinct characteristics. These problems are generally solved via dynamic programming (DP), and the length of intermediate steps can increase rapidly since the time complexity of DP algorithms ranges from O(N 2 ) (LCS, LPS, and 0-1 Knapsack) to O(N 3 ) (MCM). (1) Longest Common Subsequence (LCS): Given two random sequences of length N , the model finds the longest common subsequence and its length. The sequences consist of 10 characters from 0 to 9, and the problem difficulty is defined to be the sequence length N . (2) Longest Palindromic Subsequence (LPS): Similar to LCS, given a random sequence of length N , the model finds the longest palindromic subsequence and its length. (3) 0-1 Knapsack: Given a list of N items with specific value and weight, the model finds the best combination of items that maximizes the total value under the weight limit of a knapsack. The problem difficulty is represented by the number of items. (4) Matrix Chain Multiplication (MCM): The computational cost of multiplying N (> 2) matrices varies greatly depending on the order of multiplication. MCM is the task of finding the best multiplication order that yields minimal computation cost. The difficulty is controlled by the number of matrices N .\n\n4.2 UNLEASHING GPT-3\u2019S REASONING CAPABILITY THROUGH RECURSION OF THOUGHT\n\nDespite their remarkable language modeling capabilities, the state-of-the-art large language models, including GPT-3 (Brown et al., 2020), struggle to solve even the basic arithmetic tasks (Nye et al., 2021). For instance, it cannot correctly handle multiplication with more than one or two digits. Using the OpenAI API, we fine-tune GPT-3 on the reasoning tasks in \u00a74.1 for 10K steps with a batch size of 256. Each training batch is randomly sampled from the training data distribution explained in \u00a73.3. The results are presented in Figure 3a, and the technical details are described in Appendix E. Each point in the graphs represents one experiment at a certain problem difficulty. We report the accuracy on a test set of 1K unique problems randomly sampled as explained in Appendix C. To the best of our knowledge, the problems at this scale (e.g., 48-digit addition/subtraction and 16-digit multiplication/division) have never been solved by any language model without the help of external programs. For reference, Minerva (Lewkowycz et al., 2022) achieves around 80% accuracy on 10-digit addition and 20% on 18-digit addition.\n\nEven WT fine-tuning cannot make GPT-3 deal with such a level of complexity, while CoT is not applicable due to the context limit of 2048. The green dotted lines mark the maximum difficulty that can be handled by CoT under the context limit. On the other hand, RoT finetunes the GPT-3 to achieve near perfect scores in every experiment. As presented in Appendix I, solving each problem requires up to tens of thousands of tokens. Without any architectural change, RoT makes GPT-3 handle these extremely complex problems.\n\n4.3 RECURSION OF THOUGHT WITH TINY LANGUAGE MODELS\n\nRecent research on reasoning has been mostly focused on extremely large pre-trained language models. In this section, we show an interesting result that RoT can make even tiny models, without any pre-training, perform convoluted reasoning procedures. Since RoT is model-agnostic, we test the two basic sequence model architectures: Transformer Vaswani et al. (2017) and LSTM Hochreiter & Schmidhuber (1997). For Transformer, we use a decoder-only model with 4 layers, 2 attention heads, 128 embedding dimensions, and 256 feed-forward dimensions, a total of only 536K parameters. It is a million times smaller than the largest PaLM (Chowdhery et al., 2022) with 540B parameters. The context limit is set to 2048 following GPT-3 and PaLM. For LSTM, we use 4 layers, 64 input dimensions, and 256 hidden dimensions, which result in 272K parameters. We set the context limit of the LSTM to 512 since (i) it takes a lot of time for LSTMs to process the tokens sequentially, and (ii) they are not good at handling long-term dependency.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GPT-3\n\n(b) Tiny Transformer\n\n(c) Tiny LSTM\n\nFigure 3: Comparison of the thought processes. In each graph, the x-axis is the problem difficulty, while the y-axis is the reasoning accuracy. Each point represents an independent experiment. The green vertical lines indicate the maximum problem difficulty that CoT can handle without exceeding the maximum context size.\n\nBy virtue of their small sizes, we conduct far more extensive experiments than GPT-3, which are presented in Figure 3b and Figure 3c. We test both arithmetic and algorithmic reasoning problems with Transformer, and the arithmetic problems with LSTM. For each experiment, we train a randomly initialized model and evaluate it on a test set of 30K unique problems. With a batch size of 256, Transformers and LSTMs are trained for 500K steps and 800K steps, respectively. We repeat each experiment eight times and report the average and standard deviation of the accuracies. Appendix K enumerates the exact values of Figure 3. With the tiny Transformer, we experiment to the extent where even humans would find daunting. For example, we test addition/subtraction up to 64 digits and multiplication/division up to 32 digits. Note that a 32-digit number cannot even fit into the 64-bit integer datatype.\n\nThroughout the experiments, we observe consistent patterns:\n\n\u2022 WT\u2019s accuracy drops most quickly as the problem difficulty increases.\n\n8\n\n0D['LJLWV$FFXUDF\\$GGLWLRQ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKW0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQUnder review as a conference paper at ICLR 2023\n\n\u2022 CoT achieves near perfect accuracy, but it can only be applied to simple problems due to\n\nthe context limit.\n\n\u2022 RoT achieves near perfect accuracy and can be scaled up to extremely complex problems.\n\nDespite the small sizes, RoT makes the Transformers master all types of extremely complex problems. We do not test more difficult problems mainly because the evaluation becomes too costly, not because RoT is incapable of learning them.\n\n5 DISCUSSION\n\nThe results of the tiny Transformer suggest that we might have to rethink the capability of large language models. If RoT enables the tiny Transformer to easily master 32-digit multiplication or division, what would a million times bigger model, like PaLM, be capable of? In contrast to the currently ongoing arms race in language models, the number of parameters might not be the main bottleneck anymore to increase models\u2019 reasoning capability. We believe that our new paradigm of utilizing multiple contexts has the potential to make a huge leap in this line of research.\n\nThe current limitation of RoT is the need for supervision to learn divide and conquer for each task. In order to apply RoT to a wider range of tasks, it may be crucial to reduce the expensive supervision. As one possible approach, we may borrow the RL-based methodologies that are developed for reducing supervision of NPI (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nInterestingly, RoT cannot facilitate length generalization, e.g., training on 8-digit multiplication with RoT cannot make a model generalize to 16-digit multiplication. We believe this problem is rooted in more fundamental limitation of the Transformer architecture (Hahn, 2020), orthogonal to RoT. Fortunately, since RoT is a model-agnostic framework, we would be able to apply RoT to more advanced architectures to come in the future, which might be capable of length generalization.\n\n6 CONCLUSION\n\nDespite the remarkable advances in language models, their reasoning capability has always been constrained by the maximum size of a single context. In this work, we introduce Recursion of Thought to solve this problem by utilizing multiple contexts. We prove its potential through extensive experiments, showing that it is possible to make language models solve problems that require hundreds of thousands of tokens. We believe the core idea of utilizing multiple contexts will play an essential role in future language models.\n\nREFERENCES\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nJonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=BkbY4psgg.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n\nRoy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. Parametrized In 6th International Conference on Learnhierarchical procedures for neural programming. ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= rJl63fZRb.\n\nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11.\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735\u2013\n\n1780, 1997.\n\nDaniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2013. ISBN 9780374533557.\n\nLukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/ 1511.08228.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n\nlanguage models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022.\n\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858, 2022.\n\nChengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. Neural program lattices. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HJjiFK5gx.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114, 2021.\n\nThomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de Freitas. Learning compositional neural programs with recursive tree search and planning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 14646\u201314656, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/95b431e51fc53692913da5263c214162-Abstract.html.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nScott E. Reed and Nando de Freitas. Neural programmer-interpreters. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv. org/abs/1511.06279.\n\nAndrew Trask, Felix Hill, Scott E. Reed,\n\nNeural arithmetic logic units.\n\nJack W. Rae, Chris Dyer, and Phil Blunsom. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 8046\u20138055, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 0e64a7b00c83e3d22ce6b3acf2c582b6-Abstract.html.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998\u20136008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural\n\nexecution engines: Learning to execute subroutines. ArXiv, abs/2006.08084, 2020.\n\nWojciech Zaremba and Ilya Sutskever. Learning to execute. ArXiv, abs/1410.4615, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA A STEP BY STEP ILLUSTRATION OF ROT INFERENCE\n\nIn this section, we provide a step by step illustration of the example in Figure 1. Here we assume an ideal model fully trained for RoT.\n\nStep 1\n\nThe context is initialized with the question Q.\n\nQ\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nStep 2\n\nThe model generates the first subquestion 8 + 1.\n\nQ\n\nQsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nX 1\n\nStep 3\n\nInstead of immediately producing the answer, the model outputs the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nX 1\n\nStep 4\n\nThe THINK token triggers the creation of a new context. The new context is initialized with the subproblem starting from the last GO of X 1, i.e., 8 + 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nX 2\n\nGO 8 + 1 =\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nStep 5\n\nSince the subproblem is a base case, the model outputs the answer 9 immediately.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nA\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nX 2\n\nStep 6\n\nThe answer is returned and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nStep 7\n\nThe model generates the next subproblem, which is to add the remaining digits. Then, it produces THINK to find its answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nStep 8\n\nThe THINK token creates a new context X 3 for solving 40 + 35.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nX 3\n\nGO 4 0 + 3 5 =\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nStep 9\n\nSince 40 + 35 is not a base case, the model recursively produces more subproblems. In this case, the first subproblem is to add the last digits, i.e., 0 and 5. Then it outputs the THINK token to solve the subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nStep 10\n\nThe new context X 4 is created to solve 0 + 5.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nQ\n\nA\n\nGO 0 + 5 =\n\n5 STOP\n\nX 3\n\nX 4\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nStep 11\n\nThe answer is returned to X 3 and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nStep 12\n\nThe model generates the next subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nStep 13\n\nX 5 created to solve the subproblem 4 + 3. Since this is a base case, the model produces the answer directly.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\nQ\n\nA\n\nGO 4 + 3 =\n\n7 STOP\n\nX 3\n\nX 5\n\nStep 14\n\nThe answer from X 5 replaces the THINK token in X 3.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nStep 15\n\nSince all subproblems are solved in X 3, the answer 75 is generated and returned to X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\nA\n\n7 5 STOP\n\nStep 16\n\nThe answer of X 3 replaces the THINK token in X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\nStep 17\n\nSince the subproblems in X 1 are all solved, the model produces the final answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nA\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\n7 5 9 STOP\n\nB EXAMPLES OF COT TRAINING DATA\n\nIf we solve the example of 408+351 in figure 1 with RoT, the following five contexts are produced.\n\n\u2022 X 1: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = 7 5 STOP 7 5\n\n9 STOP\n\n\u2022 X 2: GO 8 + 1 = 9 STOP \u2022 X 3: GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP 7 5 STOP \u2022 X 4: GO 0 + 5 = 5 STOP\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n\u2022 X 5: GO 4 + 3 = 7 STOP\n\nThe CoT context of the same problem is:\n\n\u2022 X CoT: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = GO 0 + 5\n\nSTOP GO 4 + 3 STOP 7 5 STOP 7 5 9 STOP\n\nIn a slightly more complicated example of 34 \u00d7 5, the RoT contexts are as follows:\n\n\u2022 X 1: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0\n\n+ 2 0 = THINK\n\n\u2022 X 2: GO 4 * 5 = 2 0 STOP \u2022 X 3: GO 3 * 5 = 1 5 STOP \u2022 X 4: GO 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = 1 7 STOP 1 7 0\n\nSTOP\n\n\u2022 X 5: GO 0 + 0 = 0 STOP \u2022 X 6: GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP \u2022 X 7: GO 5 + 2 = 7 STOP\n\nThe corresponding CoT context is:\n\n\u2022 X CoT: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP 1 7 0 STOP\n\nNotice that the CoT context consists of all the corresponding RoT contexts as its subsequences. The number of tokens to generate is identical to that of RoT, if we do not count the THINK tokens. Even in these simple examples, however, the context size of CoT is far longer than that of RoT. For much more complex problems, such as 8-digit multiplication or 0-1 Knapsack, the CoT context size can be orders of magnitude larger than RoT. See Appendix I for more details on the distribution of context sizes.\n\nC PROBLEM SPECIFICATIONS\n\nC.1 THE ARITHMETIC PROBLEMS\n\nFor arithmetic tasks, we test addition, subtraction, multiplication, and division on non-negative integers. For subtraction, we add a constraint that the first operand is not less than the second one, in order to enforce non-negative answers. For division, we let the output include both a quotient and a remainder, separated by a special token R , e.g., GO 7 \u00f7 3 = 2 R 1 STOP .\n\nAs briefly mentioned in \u00a74.1, naively sampling the operands from a uniform distribution makes the operands extremely biased towards large numbers. For example, the probability of sampling a 2-digit number from the 6-digit space is less than 0.01%. Thus, we define a variation of the loguniform distribution (often called the reciprocal distribution) to sample the operands. As a result, we obtain roughly the same proportion of operands for each number of digits.\n\nThe probability density of a log-uniform distribution is proportional to the reciprocal of the value. By definition, zero is not the support of a log-uniform distribution, and samples are overly concentrated to the first few values in the sampling range. Therefore, we slightly extend the log-uniform distribution by introducing an offset parameter \u03b4. To sample an integer in range [\u03b1, \u03b2) with offset \u03b4, we first uniformly sample a real number r in range [log(\u03b1 + \u03b4), log(\u03b2 + \u03b4)]. Then, r is transformed to \u230aexp(r) \u2212 \u03b4\u230b. We denote the extended log-uniform distribution Ulog(\u03b1, \u03b2, \u03b4). As \u03b4 gets larger, the samples are more dispersed to larger numbers. In the experiments, we set \u03b4 = 3.\n\nAdditionally, we introduce several other sampling details for division problems. Assume that we independently sample two numbers a and b for the dividend and the divisor. In about half of the\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAddition 1330 + 121163 114780 + 4356 638 + 2 35 + 77 114261 + 354 3 + 13792 10151 + 7 22 + 1399 363356 + 450475 73 + 11 179895 + 4128 3 + 10 1 + 141972 57612 + 18403 9 + 1621 3370 + 381 678 + 8854 422 + 10348 118 + 582 1343 + 408534 24 + 9251 315 + 652424 355 + 4434 22 + 834928 3028 + 357 777 + 1355 154874 + 81059 64936 + 216852 3 + 340939 3 + 984775 50581 + 1183 415 + 943 110 + 49 15 + 17058 36278 + 100 6 + 23516 1462 + 848 1002 + 2773 135 + 178346 22672 + 162038\n\nSubtraction 376776 \u2212 35241 10638 \u2212 100 109033 \u2212 52649 85137 \u2212 3098 22355 \u2212 2824 7 \u2212 1 652781 \u2212 78853 64914 \u2212 3114 13041 \u2212 1422 28293 \u2212 4540 11553 \u2212 3576 656291 \u2212 2795 93 \u2212 42 55972 \u2212 1782 84587 \u2212 51 273269 \u2212 5867 274405 \u2212 14 51926 \u2212 9 4272 \u2212 229 223267 \u2212 377 14857 \u2212 1994 914771 \u2212 836 3035 \u2212 2963 30 \u2212 12 149 \u2212 4 89057 \u2212 6 296410 \u2212 9 45 \u2212 3 78906 \u2212 3 56560 \u2212 29960 98 \u2212 6 16551 \u2212 920 25606 \u2212 194 45 \u2212 37 129443 \u2212 70196 221 \u2212 54 11010 \u2212 818 47759 \u2212 67 10 \u2212 8 1439 \u2212 153\n\nMultiplication 9466 \u00d7 176175 179 \u00d7 516 5509 \u00d7 133 6783 \u00d7 2 6 \u00d7 80285 37275 \u00d7 19258 168484 \u00d7 154 3331 \u00d7 40 349 \u00d7 158 17988 \u00d7 262130 8140 \u00d7 1670 51 \u00d7 5 16497 \u00d7 158 74 \u00d7 10 216 \u00d7 13414 621 \u00d7 2 2 \u00d7 5951 189486 \u00d7 13080 552792 \u00d7 763 77 \u00d7 3 179090 \u00d7 469029 1037 \u00d7 258 8 \u00d7 769974 47765 \u00d7 7254 5608 \u00d7 18164 21437 \u00d7 12 15007 \u00d7 15 539860 \u00d7 427 3583 \u00d7 9754 13 \u00d7 66 266394 \u00d7 185 3988 \u00d7 12 5514 \u00d7 57 5 \u00d7 1712 17 \u00d7 430178 227 \u00d7 127 20888 \u00d7 54 96 \u00d7 232801 175 \u00d7 1050 146 \u00d7 166\n\nDivision 620261 \u00f7 155034 111730 \u00f7 1176 28268 \u00f7 1 588137 \u00f7 25571 180330 \u00f7 739 879975 \u00f7 97772 111461 \u00f7 905026 42338 \u00f7 14003 108 \u00f7 384103 60002 \u00f7 7479 131467 \u00f7 131290 890679 \u00f7 62 228 \u00f7 131108 892 \u00f7 124 15 \u00f7 964156 369044 \u00f7 28364 457 \u00f7 46 14687 \u00f7 730 200361 \u00f7 1049 19715 \u00f7 965179 98 \u00f7 7 406 \u00f7 9 47345 \u00f7 122 391613 \u00f7 1631 892642 \u00f7 3898 241554 \u00f7 1901 116475 \u00f7 12908 488317 \u00f7 197443 7519 \u00f7 325 3560 \u00f7 847611 9711 \u00f7 1385 44540 \u00f7 103 19721 \u00f7 58 59544 \u00f7 24 333057 \u00f7 333057 25719 \u00f7 5142 7544 \u00f7 46 45 \u00f7 410 195659 \u00f7 2047 412572 \u00f7 16\n\nTable 1: 40 randomly selected samples of each type of 6-digit arithmetic problems.\n\ncases, the dividend a would be less than the divisor b, so the quotients will be zero for those cases. To ensure a diverse range of quotients, we sample the divisor b from Ulog(1, 10N , 3), the quotient c from Ulog(0, 10N /b, 3), and the remainder r from Ulog(0, b, 3). The dividend is calculated from these values: a = b \u00d7 c + r. This way, we can sample division problems with a diverse range of quotients and remainders.\n\nTable 1 presents 40 problem samples for each 6-digit problem type. Several properties of our sampling scheme can be observed from the table. First, each number ranges over diverse numbers of digits. Second, the division problems are mostly non-trivial, i.e., the quotients are not concentrated at zero.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC.2 THE ALGORITHMIC PROBLEMS\n\nC.2.1 LONGEST COMMON SUBSEQUENCE (LCS)\n\nThe question of an LCS problem is two number sequences joined by the LCS token, and the answer is the corresponding LCS and its length separated by ; . Here is an example of length-4 LCS problem:\n\n\u2022 Q: GO 1 2 3 4 LCS 2 4 6 8 = \u2022 A: 2 4 ; 2 STOP\n\nFor a length-N LCS problem, we sample two sequences of length N . Each character of the sequences are randomly sampled from 0-9 with equal probability.\n\nC.2.2 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe question of a length-N LPS problem starts with the LPS , followed by a sequence of length N . Similar to LCS, the answer contains the corresponding LPS and its length separated by ; . The following is an example of length-8 LPS problem:\n\n\u2022 Q: GO LPS 4 1 2 5 3 2 6 1 = \u2022 A: 1 2 3 2 1 ; 5 STOP\n\nThe sequence of an LPS problem is sampled in the same way as done for the LCS problem.\n\nC.2.3\n\n0-1 KNAPSACK\n\nEach item in a 0-1 Knapsack problem is represented by its value and weight. For instance, 1 2 & 3 4 represents an item with a value of 12 and a weight of 34. The question part of a 0-1 Knapsack problem is a sequence consisting of the KNAPSACK token, a list of items separated by , , the token @ , and the capacity of the knapsack. The answer part starts with a list of items to include, then $ , and finally the total value. The following is an example of a 3-item knapsack problem.\n\n\u2022 Q: GO KNAPSACK 5 & 1 2 , 2 5 & 1 5 , 1 9 & 1 8 @ 4 0 =\n\n\u2022 A: 2 5 & 1 5 , 1 9 & 1 8 $ 4 4 STOP\n\nIn this example, given a knapsack of capacity 40, the last two are selected with the total value of 44.\n\nFor a fixed number of items, we uniformly sample each item\u2019s value and weight from the integers of range [1, 99].\n\nC.2.4 MATRIX CHAIN MULTIPLICATION (MCM)\n\nThe cost of multiplying many matrices is very sensitive to the order of multiplication. Matrix chain multiplication is the task of finding the best order with the minimum cost. Here, the cost is defined to be the total number of element multiplications. In the example of three matrices A, B, and C, whose shapes are 4\u00d72, 2\u00d78, and 8\u00d73 respectively, the cost of computing (AB)C is 4\u00d72\u00d78+4\u00d78\u00d73 = 160, while another order A(BC) costs only 2 \u00d7 8 \u00d7 3 + 4 \u00d7 2 \u00d7 3 = 72. In the question of an MCM problem, the sizes of the matrices are enumerated, and the answer contains the order and the total cost separated by ; . The example above is represented as the following sequences.\n\n\u2022 Q: GO MCM 4 \u00d7 2 , 2 \u00d7 8 , 8 \u00d7 3 =\n\n\u2022 A: 4 \u00d7 2 , ( 2 \u00d7 8 , 8 \u00d7 3 ) ; 7 2 STOP\n\nGiven a fixed number of matrices, we sample the sizes of matrices from the range [1, 99].\n\nC.2.5 SORTING\n\nAlthough not included in the main text, we test the problem of sorting multi-digit numbers. The results are presented in Appendix J. The problem difficulty is defined by the maximum number of\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nterms. For a sorting problem of at most N terms, we first uniformly sample the number of terms from [2, N ]. Then we sample each term from Ulog(0, 1000, 5). The following is an example of the sorting problem.\n\n\u2022 Q: GO SORT 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 , 4 1 =\n\n\u2022 A: 4 1 , 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 STOP\n\nD DETAILS OF THE RECURSIVE REASONING PROCEDURES\n\nIn this section, we elaborate the procedures to recursively solve the arithmetic problems. Specifically, we present the algorithms to produce the subproblems of a problem. Therefore, for a set of randomly sampled questions, we can generate ground truth contexts using these algorithms. For better understanding, we present the key parts of our Python code, the thought methods. For each problem, we create a child class the Problem class and implement thought static method. The method takes a set of arguments for a problem and returns the list of direct subproblems. Each subproblem is represented by a problem class, problem arguments, and recursion type (whether it is a tail recursion or not). We use named tuple T to group these information:\n\n1 2\n\nfrom collections import namedtuple T = namedtuple('Thought', ['prob_cls', 'args', 'type'], defaults=[''])\n\nFor instance, T(Mul, (3, 4)) represents a regular subproblem of 3 \u00d7 4, and T(Add, (12, 340), \u2019tail\u2019) represents a subproblem of 12 + 340 which should be performed as a tail recursion. Once the thought method returns a list of Ts, we can recursively find more subproblems for each subproblem.\n\nD.1 ADDITION\n\nThe core idea of our recursive procedure for addition is to first add the last digits, and then add the rest. If the sum of the last digits is greater than or equal to 10, we insert another subproblem for adding the carry right after adding the last digits.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21\n\nclass Add(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nl_last, r_last = left % 10, right % 10 thoughts = [T(Add, (l_last, r_last))]\n\nl_rest, r_rest = left // 10, right // 10 if l_last + r_last >= 10:\n\nthoughts.append(T(Add, (l_rest, 1))) l_rest += 1\n\nif l_rest > 0 and r_rest > 0:\n\nthoughts.append(T(Add, (l_rest, r_rest)))\n\nreturn thoughts\n\nFigure 1 in the main draft is an example with no carry, and the following is another example of 27+65 with a carry.\n\n\u2022 X 1: GO 3 1 7 + 6 5 = GO 7 + 5 = 1 2 STOP GO 3 1 + 1 = 3 2 STOP GO 3\n\n2 + 6 = 3 8 STOP 3 8 2 STOP\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n\u2022 X 2: GO 7 + 5 = 1 2 STOP \u2022 X 3: GO 3 1 + 1 = GO 1 + 1 = 2 STOP 3 2 STOP \u2022 X 4: GO 1 + 1 = 2 STOP \u2022 X 5: GO 3 2 + 6 = GO 2 + 6 = 8 STOP 3 8 STOP \u2022 X 6: GO 2 + 6 = 8 STOP\n\nD.2 SUBTRACTION\n\nSimilar to addition, we first subtract the last digits and solve the rest recursively. When subtracting the last digits x and y, we always borrow 10 for x to prevent a negative result. The borrowing of 10 is easy for a sequence model: just put 1 before x. Therefore, the base cases of subtraction are when a \u2264 19 and b \u2264 9. If the subtraction result of the last digits is smaller than 10, i.e., the borrow is actually needed, we subtract 1 from the rest of the first operand m.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Sub(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 19 and right <= 9:\n\nreturn []\n\nl_last = left % 10 + 10 r_last = right % 10 thoughts = [T(Sub, (l_last, r_last))] l_rest, r_rest = left // 10, right // 10 if l_last - r_last < 10:\n\nthoughts.append(T(Sub, (l_rest, 1))) l_rest -= 1\n\nif r_rest > 0:\n\nthoughts.append(T(Sub, (l_rest, r_rest)))\n\nreturn thoughts\n\nHere is an example of 432-216:\n\n\u2022 X 1: GO 4 3 2 - 2 1 6 = GO 1 2 - 6 = 6 STOP GO 4 3 - 1 = 4 2 STOP GO\n\n4 2 - 2 1 = 2 1 STOP 2 1 6 STOP\n\n\u2022 X 2: GO 1 2 - 6 = 6 STOP \u2022 X 3: GO 4 3 - 1 = GO 1 3 - 1 = 1 2 STOP 4 2 STOP \u2022 X 4: GO 1 3 - 1 = 1 2 STOP \u2022 X 5: GO 4 2 - 2 1 = GO 1 2 - 1 = 1 1 STOP GO 4 - 2 = 2 STOP 2 1 STOP \u2022 X 6: GO 1 2 - 1 = 1 1 STOP \u2022 X 7: GO 4 - 2 = 2 STOP\n\nNotice that the final answer and the questions of each subproblem can be easily constructed from previous sequence.\n\nD.3 MULTIPLICATION\n\nThe base cases of multiplication are (i) when either operands are 0 or 1, or (ii) when both operands are less than 10. If one of the operands is 0, then the answer is zero; when one of them is 1, then the answer is just a copy of the other operand. For the cases where both operands are less than 10, we just let the model memorize them, which is similar to an elementary school math curriculum.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nThere are two types of non-base cases. For the simpler case, where the second operand is less than 10, we first split the first operand into the last digit and the rest. We then multiply each of them with the second operand and combine the results. Otherwise, we split the second operand into the last digit and the rest. The first operand is multiplied to each of them, and the results are summed.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n\nclass Mul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 1 or right <= 1:\n\nreturn []\n\nif left <= 9 and right <= 9:\n\nreturn []\n\nthoughts = [] if right < 10:\n\nthoughts.append(T(Mul, (left % 10, right))) thoughts.append(T(Mul, (left // 10, right)))\n\na1 = (left % 10) * right a2 = (left // 10) * right thoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nelse:\n\na1 = left * (right % 10) thoughts.append(T(Mul, (left, right % 10)))\n\na2 = left * (right // 10) thoughts.append(T(Mul, (left, right // 10)))\n\nthoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nreturn thoughts\n\nHere are some example contexts of multiplication:\n\n\u2022 X 1: GO 4 3 * 2 1 = GO 4 3 * 1 = 4 3 STOP GO 4 3 * 2 = 8 6 STOP TAIL\n\n8 6 0 + 4 3 = THINK\n\n\u2022 X 2: GO 4 3 * 1 = 4 3 STOP\n\n\u2022 X 3: GO 4 3 * 2 = GO 3 * 2 = 6 STOP GO 4 * 2 = 8 STOP TAIL 8 0 + 6 =\n\nTHINK\n\n\u2022 X 4: GO 3 * 2 = 6 STOP\n\n\u2022 X 5: GO 4 * 2 = 8 STOP\n\n\u2022 X 6: GO 8 0 + 6 = GO 0 + 6 = 6 STOP 8 6 STOP\n\n\u2022 X 7: GO 0 + 6 = 6 STOP\n\n\u2022 X 8: GO 8 6 0 + 4 3 = GO 0 + 3 = 3 STOP GO 8 6 + 4 = 9 0 STOP 9 0 3\n\nSTOP\n\n\u2022 X 9: GO 0 + 3 = 3 STOP\n\n\u2022 X 10: GO 8 6 + 4 = GO 6 + 4 = 1 0 STOP GO 8 + 1 = 9 STOP 9 0 STOP\n\n\u2022 X 11: GO 6 + 4 = 1 0 STOP\n\n\u2022 X 12: GO 8 + 1 = 9 STOP\n\nNotice that we use tail recursion in X 1 and X 3.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nD.4 COMPARISON\n\nComparison is used as a subroutine during division. The procedure for comparison consists of three steps:\n\n1. Compare the numbers of digits.\n\n2. If the numbers of digits are the same, compare the most significant digits.\n\n3. If the most significant digits are identical, compare the remaining digits recursively.\n\nWe find that the sequence models can perform the first step without an explicit subproblem. Therefore, we only add intermediate steps for the second and the third steps.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass Compare(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nthoughts = [] digit_l, digit_r = len(str(left)), len(str(right)) if digit_l == digit_r:\n\n# Compare first digit l_first, r_first = int(str(left)[0]), int(str(right)[0]) thoughts.append(T(Compare, (l_first, r_first))) if l_first == r_first: # Compare the rest l_rest = int(str(left)[1:]) r_rest = int(str(right)[1:]) thoughts.append(T(Compare, (l_rest, r_rest)))\n\nreturn thoughts\n\nThe following is an example of comparing 153 and 159.\n\n\u2022 X 1: GO 1 5 3 VS 1 5 9 = GO 1 VS 1 = EQ STOP GO 5 3 VS 5 9 = LT STOP\n\nLT STOP\n\n\u2022 X 2: GO 1 VS 1 = EQ STOP \u2022 X 3: GO 5 3 VS 5 9 = GO 5 VS 5 = EQ STOP GO 3 VS 9 = LT STOP LT STOP \u2022 X 4: GO 5 VS 5 = EQ STOP \u2022 X 5: GO 3 VS 9 = LT STOP\n\nD.5 DIVISION\n\nSolving division is the most challenging among the four basic arithmetic operations since the procedure is basically trial and error, searching for the correct quotient. Nonetheless, the following process is a recursive version of the elementary school division.\n\nThe base case is when the dividend is less than or equal to the divisor. If the dividend is smaller than the divisor, the quotient is 0, and the remainder is the dividend. If the dividend is equal to the divisor, than the quotient is 1, and the remainder is 0. Both cases can be handled relatively easily by neural sequence models. To determine whether it is one of these cases, we always perform the comparison as the first subproblem.\n\nIf it is not a base case, we check whether the dividend is smaller than 10 times the divisor. If the dividend is smaller, we subtract the divisor from the dividend and recursively divide the result with the divisor. The final answer is attained by simply adding 1 to the quotient of the smaller division.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nTo explain the other case, where the dividend is greater than 10 times the divisor, let us call the dividend a and the divisor b. First, we split the a into the last digit x and the remaining digits m. Then, we divide m with the divisor b, i.e., we are solving one-digit-smaller subproblem first. Since we define the division operation to return both a quotient and a remainder, the quotient q1 = m/b and the remainder r1 = m mod b from the subproblem is added to the context. Next, we concatenate the remainder and x, which is numerically computing r \u00d7 10 + x, and divide it again with b. Let the quotient and the remainder of this operation q2 and r2. Then, the quotient of the final answer is q1 \u00d7 10 + q2, while the remainder is simply r2.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Div(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args thoughts = [T(Compare, (left, right))]\n\n# Base cases if left <= right:\n\nreturn thoughts\n\nthoughts.append(T(Compare, (left, right * 10))) if left <= right * 10:\n\ndiff = left - right thoughts.append(T(Sub, (left, right))) thoughts.append(T(Div, (diff, right)))\n\nelse:\n\nthoughts.append(T(Div, (left // 10, right))) left_remainder = (left // 10) % right * 10 + left % 10 thoughts.append(T(Div, (left_remainder, right)))\n\nreturn thoughts\n\nThe following is an example of 76 \u00f7 29.\n\n\u2022 X 1: GO 7 6 \u00f7 2 9 = GO 7 6 VS 2 9 = GT STOP GO 7 6 VS 2 9 0 = LT STOP GO 7 6 - 2 9 = 4 7 STOP GO 4 7 \u00f7 2 9 = 1 R 1 8 STOP 2 R 1 8 STOP\n\n\u2022 X 2: GO 7 6 VS 2 9 = GO 7 VS 2 = GT STOP GT STOP\n\n\u2022 X 3: GO 7 VS 2 = GT STOP\n\n\u2022 X 4: GO 7 6 VS 2 9 0 = LT STOP\n\n\u2022 X 5: GO 7 6 - 2 9 = GO 1 6 - 9 = 7 STOP GO 7 - 1 = 6 STOP GO 6 - 2 =\n\n4 STOP 4 7 STOP\n\n\u2022 ... \u2022 X 9: GO 4 7 \u00f7 2 9 = GO 4 7 VS 2 9 = GT STOP GO 4 7 VS 2 9 0 = LT STOP GO 4 7 - 2 9 = 1 8 STOP GO 1 8 \u00f7 2 9 = 0 R 1 8 STOP 1 R 1 8 STOP\n\n\u2022 X 10: GO 4 7 VS 2 9 = GO 4 VS 2 = GT STOP GT STOP\n\n\u2022 X 11: GO 4 VS 2 = GT STOP\n\n\u2022 X 12: GO 4 7 VS 2 9 0 = LT STOP\n\n\u2022 X 13: GO 4 7 - 2 9 = GO 1 7 - 9 = 8 STOP GO 4 - 1 = 3 STOP GO 3 - 2 =\n\n1 STOP 1 8 STOP\n\n\u2022 ... \u2022 X 17: GO 1 8 \u00f7 2 9 = GO 1 8 VS 2 9 = LT STOP 0 R 1 8 STOP\n\n\u2022 X 18: GO 1 8 VS 2 9 = GO 1 VS 2 = LT STOP LT STOP\n\n\u2022 ...\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nD.6 LONGEST COMMON SUBSEQUENCE (LCS)\n\nGiven sequences A and B, the algorithm starts by comparing the last characters of the two sequences. If the last two characters are the same, we find LCS of the subsequences without the last characters, i.e., LCS of A\u2236\u22121 and B\u2236\u22121. Otherwise, we compute the LCSs of the cases where the last character of either side is removed, and return the better one. In the following code, LCS._answer is the subroutine that finds the LCS of two sequences. Equal returns TRUE if the two arguments are the same, or FALSE otherwise.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass LCS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Equal, (l[-1], r[-1]))] if l[-1] == r[-1]:\n\nthoughts.append(T(LCS, (l[:-1], r[:-1]))) return thoughts\n\nlcs1_args = (l[:-1], r) lcs2_args = (l, r[:-1]) lcs1 = LCS._answer(lcs1_args) lcs2 = LCS._answer(lcs2_args) thoughts.extend([\n\nT(LCS, lcs1_args), T(LCS, lcs2_args), T(Compare, (len(lcs1), len(lcs2)))\n\n]) return thoughts\n\nThe following is an example of finding the LCS of 123 and 234.\n\n\u2022 X 1: GO 1 2 3 LCS 2 3 4 = GO EQUAL 3 , 4 = FALSE STOP GO 1 2 LCS 2 3 4 = 2 ; 1 STOP GO 1 2 3 LCS 2 3 = 2 3 ; 2 STOP GO 1 VS 2 = LT STOP 2 3 ; 2 STOP\n\n\u2022 X 2: GO EQUAL 3 , 4 = FALSE STOP \u2022 X 3: GO 1 2 LCS 2 3 4 = GO EQUAL 2 , 4 = FALSE STOP GO 1 LCS 2 3 4 = ; 0 STOP GO 1 2 LCS 2 3 = 2 ; 1 STOP GO 0 VS 1 = LT STOP 2 ; 1 STOP\n\n\u2022 ... \u2022 X 21: GO 1 2 3 LCS 2 3 = GO EQUAL 3 , 3 = TRUE STOP GO 1 2 LCS 2 = 2\n\n; 1 STOP 2 3 ; 2 STOP\n\n\u2022 ... \u2022 X 23: GO 1 VS 2 = LT STOP\n\nD.7 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe overall algorithm for LPS is similar to LCS. The base cases are when the sequence length is less then 3. If it is not a base case, we first check if the characters at both ends of the sequence are the same. If they are the same, we find the LPS of the subsequence excluding them. Otherwise, we compare the cases where one of the end characters are excluded.\n\n1 2\n3 4\n5 6\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\n# Base cases if len(args) == 1: return []\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n\nelif len(args) == 2:\n\nreturn [T(Equal, args)]\n\nthoughts = [T(Equal, (args[0], args[1]))] if args[0] == args[-1]:\n\nsub_lps = LPS._answer(args[1:-1]) thoughts.extend([\n\nT(LPS, args[1:-1]), T(Add, (len(sub_lps), 2))\n\n])\n\nelse:\n\nlps1_args = args[:-1] lps2_args = args[1:] lps1 = LPS._answer(lps1_args) lps2 = LPS._answer(lps2_args) thoughts.extend([\n\nT(LPS, lps1_args), T(LPS, lps2_args), T(Compare, (len(lps1), len(lps2)))\n\n])\n\nreturn thoughts\n\nThe following is an example of LPS.\n\n\u2022 X 1: GO LPS 1 2 3 2 = GO EQUAL 1 , 2 = FALSE STOP GO LPS 1 2 3 = 1 ; 1 STOP GO LPS 2 3 2 = 2 3 2 ; 3 STOP GO 1 VS 3 = LT STOP 2 3 2 ; 3 STOP\n\n\u2022 X 2: GO EQUAL 1 , 2 = FALSE STOP \u2022 X 3: GO LPS 1 2 3 = GO EQUAL 1 , 3 = FALSE STOP GO LPS 1 2 = 1 ; 1\n\nSTOP GO LPS 2 3 = 2 ; 1 STOP GO 1 VS 1 = EQ STOP 1 ; 1 STOP\n\n\u2022 ... \u2022 X 10: GO LPS 2 3 2 = GO EQUAL 2 , 2 = TRUE STOP GO LPS 3 = 3 ; 1 STOP\n\nGO 1 + 2 = 3 STOP 2 3 2 ; 3 STOP\n\n\u2022 ... \u2022 X 14: GO 1 VS 3 = LT STOP\n\nD.8\n\n0-1 KNAPSACK\n\nThe base cases are when there is only one item. In this case, we simply compare the item\u2019s weight and the knapsack\u2019s capacity, to determine whether the item should be included. If it is a non-base case, we compare two possibilities: (i) include the first item, or (ii) exclude the first item. We recursively compute the subproblems and find the case with the best value.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]: items, capacity = args value, weight = items[0]\n\n# Base case if len(items) == 1:\n\nreturn [T(Compare, (weight, capacity))]\n\n# When excluding the current item items_max, value_max = Knapsack._answer((items[1:], capacity)) thoughts = [\n\nT(Knapsack, (items[1:], capacity)), T(Compare, (weight, capacity)),\n\n]\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n18 19 20 21 22 23 24 25 26 27 28 29 30\n\n# When including the current item if weight <= capacity:\n\nitems_sub, value_sub = Knapsack._answer( (items[1:], capacity - weight))\n\nvalue_incl = value_sub + value thoughts.extend([\n\nT(Sub, (capacity, weight)), T(Knapsack, (items[1:], capacity - weight)), T(Add, (value_sub, value)), T(Compare, (value_incl, value_max)),\n\n])\n\nreturn thoughts\n\nThe following is an example of 0-1 knapsack problem with three items and a knapsack capacity of 10.\n\n\u2022 X 1: GO KNAPSACK 3 & 9 , 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = 4 & 2 , 9 & 5 $ 1 3 STOP GO 9 VS 1 0 = LT STOP GO 1 0 - 9 = 1 STOP GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = $ 0 STOP GO 0 + 3 = 3 STOP GO 3 VS 1 3 = LT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n\u2022 X 2: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 9 & 5 @ 1 0 = 9 & 5 $ 9 STOP GO 2 VS 1 0 = LT STOP GO 1 0 - 2 = 8 STOP GO KNAPSACK 9 & 5 @ 8 = 9 & 5 $ 9 STOP GO 9 + 4 = 1 3 STOP GO 1 3 VS 9 = GT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n\u2022 ... \u2022 X 11: GO 9 VS 1 0 = LT STOP \u2022 X 12: GO 1 0 - 9 = 1 STOP \u2022 X 13: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = GO KNAPSACK 9 & 5 @ 1 = $ 0 STOP\n\nGO 2 VS 1 = GT STOP $ 0 STOP\n\n\u2022 ... \u2022 X 17: GO 0 + 3 = 3 STOP \u2022 X 18: GO 3 VS 1 3 = LT STOP\n\nD.9 TERNARY ADDITION AND MULTIPLICATION\n\nTernary addition and multiplication arises as a subproblem while solving MCM, which will be explained in the next section. They are simple extensions of addition and multiplication to three integers.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18\n\nclass TernaryAdd(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Add, (a1, a2)), T(Add, (a1 + a2, a3), 'tail')\n\n]\n\nclass TernaryMul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Mul, (a1, a2)), T(Mul, (a1 * a2, a3), 'tail')\n\n]\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nD.10 MATRIX CHAIN MULTIPLICATION (MCM)\n\nGiven N matrices, the N \u2212 1 subproblems are defined for each possible binary split. For the multiplication of four matrices ABCD, there are three possible binary splits: A(BCD), (AB)(CD), and (ABC)D. For each binary split, the total cost is the sum of (i) the minimum cost of computing the first group, (ii) the minimum cost of computing the second group, and (iii) the cost of multiplying the two matrices resulting from each group. Once we get the total costs of each binary split, we return choose the best split with the minimum cost. The following code implements this procedure.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n\nclass MCM(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nmats, min_order, min_cost = args\n\n# Base cases if len(mats) == 1: return []\n\nif min_order is None:\n\n# Top-level problem l_mats, r_mats = mats[:1], mats[1:]\n\nelse:\n\n# Middle of recursion l_mats, r_mats = mats\n\nl_args = (l_mats, None, None) r_args = (r_mats, None, None) l_order, l_cost = MCM._answer(l_args) r_order, r_cost = MCM._answer(r_args) agg_cost = l_mats[0][0] * r_mats[0][0] * r_mats[-1][1] thoughts = [\n\nT(MCM, l_args), T(MCM, r_args), T(TernaryMul, (l_mats[0][0], r_mats[0][0], r_mats[-1][1])), T(TernaryAdd, (l_cost, r_cost, agg_cost)),\n\n]\n\ncost = l_cost + r_cost + agg_cost if min_cost is not None:\n\nthoughts.append(T(Compare, (cost, min_cost)))\n\nif min_cost is None or cost < min_cost:\n\nmin_cost = cost min_order = l_order, r_order\n\nif len(r_mats) > 1:\n\nnew_l_mats = l_mats + (r_mats[0],) new_r_mats = r_mats[1:] thoughts.append(\n\nT(MCM, ((new_l_mats, new_r_mats), min_order, min_cost), 'tail'))\n\nreturn thoughts\n\nThe following is an example of three-matrix MCM.\n\n\u2022 X 1: GO MCM 3 \u00d7 9 , 9 \u00d7 4 , 4 \u00d7 5 = GO MCM 3 \u00d7 9 = 3 \u00d7 9 ; 0 STOP GO MCM 9 \u00d7 4 , 4 \u00d7 5 = 9 \u00d7 4 , 4 \u00d7 5 ; 1 8 0 STOP GO 3 * 9 * 5 = 1 3 5 STOP GO 0 + 1 8 0 + 1 3 5 = 3 1 5 STOP TAIL MCM 3 \u00d7 9 , 9 \u00d7 4 | 4 \u00d7\n\n5 ACC 3 \u00d7 9 , ( 9 \u00d7 4 , 4 \u00d7 5 ) ; 3 1 5 = THINK\n\n\u2022 ... \u2022 X 32: GO MCM 3 \u00d7 9 , 9 \u00d7 4 | 4 \u00d7 5 ACC 3 \u00d7 9 , ( 9 \u00d7 4 , 4 \u00d7 5 ) ; 3 1 5 = GO MCM 3 \u00d7 9 , 9 \u00d7 4 = 3 \u00d7 9 , 9 \u00d7 4 ; 1 0 8 STOP GO MCM 4 \u00d7 5 = 4 \u00d7 5 ; 0 STOP GO 3 * 4 * 5 = 6 0 STOP GO 1 0 8 + 0 + 6 0 = 1 6 8\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nSTOP GO 1 6 8 VS 3 1 5 = LT STOP ( 3 \u00d7 9 , 9 \u00d7 4 ) , 4 \u00d7 5 ; 1 6 8 STOP\n\n\u2022 ...\n\nD.11 SORTING\n\nAmong several sorting algorithms, we choose merge sort for our experiments with CoT and RoT. Note that WT is not relevant to the sorting algorithm since it produces the answer directly. The merge sort algorithm is simple: (i) split the given sequence to two equally sized subsequences, (ii) sort each subsequence, and (iii) merge the two sorted sequences. Since the final merge operation is quite complicated, we define the merge as a problem type.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n\nclass Merge(Problem): @staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Compare, (l[0], r[0]))] if l[0] < r[0] and len(l) > 1:\n\nthoughts.append(T(Merge, (l[1:], r)))\n\nelif l[0] >= r[0] and len(r) > 1:\n\nthoughts.append(T(Merge, (l, r[1:])))\n\nreturn thoughts\n\nclass MergeSort(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nif len(args) < 2: return []\n\nl_len = (len(args) + 1) // 2 l = args[:l_len] r = args[l_len:] return [\n\nT(MergeSort, l), T(MergeSort, r), T(Merge, (tuple(sorted(l)), tuple(sorted(r))), 'tail')\n\n]\n\nE FINE-TUNING GPT-3 FOR RECURSION OF THOUGHT\n\nUsing the OpenAI API, we fine-tune GPT-3 for Recursion of Thought. The goal is to learn 16-digit addition, 16-digit subtraction, 8-digit multiplication, and 8-digit division simultaneously. GPT-3\u2019s fine-tuning API takes a dataset where each example is a prompt-completion pair in plain text. It is converted to tokens by a special tokenizer for GPT, which we cannot control. This API is not directly compatible with RoT due to several reasons.\n\n\u2022 There is no special tokens such as GO , THINK , and STOP .\n\n\u2022 The input and target sequences have to be the same. However, they are different in RoT due to the THINK token. Once THINK is produced, the RoT framework triggers the recursion process to find the subproblem\u2019s answer and replace the THINK token with it. Therefore, the THINK token appears in the target sequences, but never in the input sequences.\n\nMoreover, the way that GPT-3 tokenizes numbers hinders the learning of arithmetic reasoning rules. GPT-3 tokenizes a multi-digit number into a set of two-digit or three-digit numbers. For example, the text 1234567 is converted to the sequence of tokens 123 45 67 . Under this tokenization scheme, the relationship between the numbers become obscured. As an example, the tokens 7 , 17 ,\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n27 , ..., 997 all have 7 as their last digit. Since there is no direct way for a model to know that they share the same digit, it is crucial to use each digit as a token. We believe that OpenAI needs to correct this tokenization of GPT-3 for numbers.\n\nLuckily, we can mimic the RoT procedures with the API by using several tricks. First, we replace the special tokens with plain lower-case words, e.g., GO \u2192 go and STOP \u2192 stop, which are included in the vocabulary of GPT-3. Second, we add a space before each token to make sure that the GPT tokenizer separates each token. We also add space before each digit to prevent the tokenizer grouping a number into 2-to-3-digit tokens. Finally, to simulate the behavior of the THINK and STOP tokens, we derive multiple examples from each context, one for each THINK or STOP output.\n\nAs an example, context X 3 in Figure 1 is converted to the following JSON lines for GPT-3 as follows:\n\nX 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP\n\nGO 4 + 3 = 7 STOP\n\n7 5 STOP\n\nY 3 PAD \u00d77\n\nGO 0 + 5 = THINK PAD GO 4 + 3 = THINK PAD 7 5 STOP\n\n\u21d3\n\n1 2\n3\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" go 0 + 5 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop\", \"completion\": \" go 4 + 3 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop go 4 + 3 = 7 stop\", \"completion\": \"\n\n7 5 stop\"}\n\nIn the case of Without Thought (WT), each problem is simply converted into a single example:\n\nX GO 4 0 + 3 5 = 7 5 STOP Y PAD \u00d77\n\n7 5 STOP\n\n\u21d3\n\n1\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" 7 5 stop\"}\n\nIn both cases of RoT and WT, we fine-tune GPT-3 for 10K steps with a batch size of 256. Among the several variants of GPT-3, we use Ada which is offered at the lowest cost. Note that RoT produces multiple contexts for each problem, and each RoT context is converted to multiple training examples. For this reason, the GPT-3 fine-tuned for RoT encounters much fewer problems during training, although the number of training steps are the same.\n\nF TRAINING DETAILS OF THE TINY MODELS\n\nIn all experiments, we use a batch size of 256 and Adam optimizer Kingma & Ba (2015) with a learning rate of 0.001, i.e., the default learning rate in PyTorch. We train the Transformers for 500K steps and and decay the learning rate by half every 50K steps. Since the LSTMs converge slower than the Transformers, we train them for 800K steps and decay the learning rate by half every 100K steps. At every 20K steps, we evaluate the model on a test set of 30K problems, and if a model reaches a perfect accuracy of 1.0, we do not train the model further. The models can be trained on a single GPU with 12GB memory.\n\nG EFFICIENT EVALUATION OF ROT\n\nAt the problem scales that RoT is tested, solving a single problem can require hundreds of thousands of tokens. Therefore, we have to develop an efficient evaluation algorithm. Since the models are\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\ndeterministic, we do not need to evaluate the same context multiple times. Therefore, given a test set, we first find all unique contexts that should appear while solving the test set. Note that there would be a lot of duplicate contexts, especially for the base cases. These unique contexts can be evaluated in parallel. We evaluate each context in a similar manner to the training. For each context, we construct the target sequence, as we described in \u00a73.2, and check if the model outputs the target sequence correctly, given the GT context. In case of GPT-3, we convert the contexts and target sequences as described in Appendix E to feed into the API. Once every context is individually evaluated, the next step is to aggregate the results. In this aggregation stage, a (sub)problem is marked correct only if its subproblems are all marked correct. This way, we efficiently evaluate RoT with minimal cost.\n\nH FAILURE CASE ANALYSIS OF GPT-3 WITH ROT\n\nIn this section we provide some failure cases of RoT in the GPT-3 experiments for further insights. Although the model achieves an accuracy of over 99% in almost every case, there are several problems where it makes mistakes. Nonetheless, only a few characters are wrong in each context.\n\nThe most common type of error is repeating the same character for a wrong number of times. The following is the examples of this error type.\n\nPrompt\n\nCompletion\n\nModel Output\n\nPrompt\n\nCompletion\n\nModel Output\n\ngo 1 6 4 3 4 vs 1 0 1 1 6 = go 1 vs 1 = eq stop\n\ngo 6 4 3 4 vs 1 1 6 = think\n\ngo 6 4 3 4 vs 1 1 1 6 = think\n\ngo 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 7\n\n= go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 7 = 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 stop go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 = 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 stop\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\nPrompt\n\ngo 1 0 5 4 6 2 0 * 1 5 = go 1 0 5 4 6 2 0 * 5 = 5 2 7 3\n\n1 0 0 stop go 1 0 5 4 6 2 0 * 1 = 1 0 5 4 6 2 0 stop\n\nCompletion\n\nModel Output\n\ntail 1 0 5 4 6 2 0 0 + 5 2 7 3 1 0 0 = think\n\ntail 1 0 5 4 6 2 0 0 0 + 5 2 7 3 1 0 0 = think\n\nAnother common case is incorrectly counting the number of digits while comparing two numbers. When comparing numbers following our recursive procedure, the model has to immediately return the answer if the number of digits are different. In the following example, the first number has seven digits, and the second one has six digits.\n\nPrompt\n\ngo 1 0 6 9 0 3 4 vs 8 9 1 5 0 6 =\n\nCompletion\n\ngt stop\n\nModel Output\n\ngo 1 vs 8\n\nAlthough the model should directly output the answer, i.e., gt stop, it decides that the number of digits are the same and moves on to the next step, which is to compare the most significant digits. This error is caused by the specific recursive procedure of our choice. Since we assume that the model would easily figure out the difference in the numbers of digits, we did not add explicit steps to count the digits. Although the model detects the difference in most cases, it turns out to be a\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nrelatively challenging operation. A possible solution is to teach the model to explicitly count the number of digits and compare them.\n\nI CONTEXT LENGTH DISTRIBUTION\n\nIn Figure 4, we present the distributions of context lengths for each problem type. We compare the context lengths of RoT and CoT. For each configuration, we randomly sample 10K contexts from the training distribution and plot the histogram of their lengths. The graphs show that the context sizes of CoT are many orders of magnitude larger than RoT. In theory, the total number of tokens to generate for each problem is identical in both RoT and CoT (if we do not count the THINK tokens). However, RoT\u2019s context sizes are much smaller since it utilizes multiple contexts.\n\nAnother advantage of RoT is the utilization of dynamic programming. Since we can easily cache the duplicate computations of RoT as explained in Appendix G, we can drastically reduce the amount of token generation if there is a redundant structure in the problem. The amount of tokens to generate for each problem is plotted in Figure 5. The benefit is especially prominent in the algorithmic problems. For example, finding the LCS of two 32-digit sequences results in more than 1018 tokens if we naively use CoT or RoT. If we use dynamic programming with RoT, we can efficiently solve the same problem with much less cost.\n\nJ TRANSFORMERS ARE POWERFUL SORTING MACHINES\n\nIn fact, the first algorithmic task that we tested is sorting since it has been widely used as a benchmark for algorithmic reasoning (Reed & de Freitas, 2016; Cai et al., 2017; Pierrot et al., 2019). However, we find that Transformers are incredibly good at sorting, even in the WT setting. Figure 6 shows the sorting experiment. For CoT and RoT, we train the merge sort algorithm. Interestingly, WT easily achieves a perfect score in sorting 64 three-digit numbers. Also the training converges much faster than RoT. The Transformer architecture, more specifically the attention mechanism, seems to be perfectly suited for the sorting operation.\n\nK THE EXACT VALUES OF FIGURE 3\n\nTable 2-5 show the exact values of the graphs in Figure 3. Except for the GPT-3 experiments in Table 2, we report the average and the standard deviation of eight runs. Each GPT-3 experiment is done only once.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The distributions of context lengths.\n\n34\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 5: The distribution of the total number of tokens to produce in order to solve each problem. RoT can utilize dynamic programming to reduce redundant computations.\n\n35\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 6: Sorting experiment with the tiny Transformer.\n\nProblem\n\nDifficulty\n\nWT CoT\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\n32-digit 48-digit\n\n32-digit 48-digit\n\n8-digit 16-digit\n\n8-digit 16-digit\n\nlength 16 length 24\n\nlength 24 length 40\n\n4 items 6 items\n\n3 matrices 4 matrices\n\n0.991 0.853\n\n0.991 0.886\n\n0.337 0.098\n\n0.363 0.123\n\n0.980 0.832\n\n0.995 0.800\n\n0.945 0.634\n\n0.481 0.110\n\nRoT \u2212 0.998 \u2212 0.995 \u2212 0.998 \u2212 0.998 \u2212 0.999 \u2212 0.994 \u2212 1.000 \u2212 0.989 \u2212 0.995 \u2212 0.998 \u2212 1.000 \u2212 0.974 \u2212 0.999 \u2212 1.000 \u2212 0.997 \u2212 0.992\n\nTable 2: The exact values of the GPT-3 experiments in Figure 3a.\n\n36\n\nRI,WHPV$FFXUDF\\6RUWLQJ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKWUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\nWT 0.863 \u00b1 0.265 0.370 \u00b1 0.475 0.336 \u00b1 0.430 0.455 \u00b1 0.458 0.119 \u00b1 0.316 0.082 \u00b1 0.216 0.105 \u00b1 0.277 0.000 \u00b1 0.000 0.982 \u00b1 0.006 0.705 \u00b1 0.411 0.238 \u00b1 0.412 0.221 \u00b1 0.385 0.426 \u00b1 0.433 0.114 \u00b1 0.303 0.116 \u00b1 0.307 0.161 \u00b1 0.282 1.000 \u00b1 0.000 0.817 \u00b1 0.023 0.340 \u00b1 0.032 0.169 \u00b1 0.015 0.104 \u00b1 0.016 0.048 \u00b1 0.020 0.033 \u00b1 0.017 0.014 \u00b1 0.006 0.012 \u00b1 0.001 1.000 \u00b1 0.000 0.978 \u00b1 0.008 0.354 \u00b1 0.029 0.186 \u00b1 0.009 0.128 \u00b1 0.011 0.087 \u00b1 0.012 0.075 \u00b1 0.005 0.059 \u00b1 0.007 0.048 \u00b1 0.008\n\nCoT 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\nRoT 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.001 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.001 \u2212 0.999 \u00b1 0.001 \u2212 0.999 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.000 \u2212 0.999 \u00b1 0.000\n\nTable 3: The exact values of the Transformer experiments in Figure 3b (arithmetic problems).\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\nlength 3 length 4 length 8 length 12 length 16 length 20 length 24 length 28 length 32\n\nlength 4 length 7 length 8 length 16 length 24 length 32 length 40 length 48 length 56\n\n2 items 4 items 6 items 8 items 10 items 12 items\n\n2 matrices 4 matrices 6 matrices 8 matrices 10 matrices 12 matrices\n\nWT 1.000 \u00b1 0.000 0.997 \u00b1 0.008 0.999 \u00b1 0.002 0.965 \u00b1 0.025 0.880 \u00b1 0.035 0.759 \u00b1 0.043 0.622 \u00b1 0.038 0.484 \u00b1 0.043 0.375 \u00b1 0.030 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.999 \u00b1 0.001 0.950 \u00b1 0.019 0.788 \u00b1 0.019 0.608 \u00b1 0.023 0.477 \u00b1 0.030 0.365 \u00b1 0.029 1.000 \u00b1 0.000 0.966 \u00b1 0.006 0.849 \u00b1 0.007 0.640 \u00b1 0.242 0.481 \u00b1 0.279 0.435 \u00b1 0.252 0.973 \u00b1 0.009 0.177 \u00b1 0.069 0.088 \u00b1 0.029 0.033 \u00b1 0.025 0.051 \u00b1 0.032 0.026 \u00b1 0.011\n\nCoT 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000\n\nRoT\n\n\u2212 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.000 \u2212 0.999 \u00b1 0.000 \u2212\n\u2212 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.001 \u2212 0.998 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.988 \u00b1 0.029 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.998 \u00b1 0.001 \u2212 0.996 \u00b1 0.002\n\nTable 4: The exact values of the Transformer experiments in Figure 3b (algorithmic problems).\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\n1-digit 2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\nWT 1.000 \u00b1 0.000 0.642 \u00b1 0.305 0.005 \u00b1 0.008 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 1.000 \u00b1 0.000 0.776 \u00b1 0.179 0.006 \u00b1 0.001 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 1.000 \u00b1 0.000 0.855 \u00b1 0.044 0.636 \u00b1 0.061 0.338 \u00b1 0.063 0.270 \u00b1 0.030 0.162 \u00b1 0.025 0.138 \u00b1 0.025 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.001 0.891 \u00b1 0.072 0.516 \u00b1 0.077 0.308 \u00b1 0.069 0.192 \u00b1 0.028 0.115 \u00b1 0.015\n\nCoT 1.000 \u00b1 0.000 1.000 \u00b1 0.001 0.997 \u00b1 0.005 0.905 \u00b1 0.155 0.795 \u00b1 0.341\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.896 \u00b1 0.252 0.443 \u00b1 0.377\n\n1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000\n\nRoT 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.999 \u00b1 0.000 0.999 \u00b1 0.001 0.986 \u00b1 0.024 \u2212 0.871 \u00b1 0.275 \u2212 0.358 \u00b1 0.430 \u2212 0.120 \u00b1 0.202 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.994 \u00b1 0.016 0.908 \u00b1 0.236 \u2212 0.507 \u00b1 0.398 \u2212 0.295 \u00b1 0.406 \u2212 0.101 \u00b1 0.137 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.987 \u00b1 0.008 \u2212 0.896 \u00b1 0.105 \u2212 0.670 \u00b1 0.208 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.998 \u00b1 0.004 \u2212 0.996 \u00b1 0.007 \u2212 0.958 \u00b1 0.036 \u2212 0.914 \u00b1 0.090\n\nTable 5: The exact values of the LSTM experiments in Figure 3c.\n\n39",
      "responses": [
        "# Summary Of The Paper\n\nThis paper demonstrates that for simple algorithmic problems (arithmetic), language models can be taught to split problems into multiple subproblems, which can then be fed back to the LM to be solved independently. With very small models this can achieve great performance on these tasks, and it circumvents the limit in context-length of existing transformer models.\n\n# Strength And Weaknesses\n\nMy main concern with this approach is that the paper does not discuss how to apply this approach to more general problems. The problems considered here are very simple, and an algorithm to solve them needs to be known in order to generate the training data. So, in its current form this does not enable any new abilities as we could use the base algorithm instead of using expensive LMs.\n\nIn terms of the insight from a learning point-of-view, I also do not see anything surprising in this work. After decomposing the problem manually (i.e. by writing an algorithm to generate training data), the learning task is pretty simple (for addition, for example, all the model has to do is to extract the last digit from two numbers). So the high accuracy of the resulting method is not that surprising.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing style is a bit \"flashy\", which gives room for miscommunications. For example, the paper opens with the following statement:\n\n\"Although neural networks have achieved amazing results on various domains, e.g., images, texts,\naudios, videos, games, etc., nearly all of them are classified as System 1 tasks (Kahneman, 2013), ...\"\n\nAfaik, this statement is not supported by the literature, and in particular not by the given reference. Following Kahneman's theory, tasks like playing the board game go would certainly require system 2 thinking. IMO, dropping the first paragraph would help the paper.\n\nThe paper also makes claims such as \"the length of CoT can grow rapidly with the problem\u2019s complexity\", without further explanation and I am not even sure what exactly this means.\n\n# Summary Of The Review\n\nThis paper demonstrates that decomposing simple algorithmic problems can circumvent the limit in context length language models.\n\nHowever, the technique only seems to work for problems for which we already have algorithms, and it is unclear to me if this could be extended to more general problems. The writing style should be improved.\n\n# Correctness\n\n2: Several of the paper\u2019s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a model-agnostic framework called Recursion of Thought (RoT) that enhances multi-step reasoning in language models (LMs) by overcoming the limitations imposed by context size. The authors propose a systematic approach using special tokens (GO, STOP, THINK) to enable recursive problem-solving, allowing LMs to break down complex problems into manageable subproblems. The findings demonstrate that RoT significantly improves the performance of GPT-3 on various arithmetic and algorithmic reasoning tasks, even enabling smaller models like randomly initialized Transformers and LSTMs to achieve impressive results.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to enhancing reasoning abilities in LMs by introducing a recursive framework that allows for greater token utilization without architectural changes. The use of special tokens to control the recursion process is a novel and practical solution. The empirical results show clear improvements over existing methods, including Chain of Thought (CoT) prompting. However, a potential weakness is the reliance on supervised training for generating intermediate steps, which may limit the generalizability of the approach to tasks with less available supervision. Additionally, while the paper highlights the potential for smaller models, further exploration into the scalability and robustness of RoT across diverse tasks is warranted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing is precise, making complex ideas accessible. The novelty of the RoT framework stands out, particularly in its ability to address the context size limitation faced by existing models. The authors provide sufficient detail regarding the methodology, enabling reproducibility, as evidenced by the availability of source code for experiments. Nevertheless, the paper could benefit from a deeper discussion on the limitations and potential biases introduced by the supervision required in training.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the reasoning capabilities of language models through its innovative Recursion of Thought framework. The empirical results support its effectiveness, and the potential applications for smaller models are particularly promising. While there are areas for improvement in terms of broader applicability and training requirements, the contributions are noteworthy and impactful.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
        "# Summary Of The Paper\nThe paper introduces \"Recursion of Thought\" (RoT), a model-agnostic framework aimed at enhancing the reasoning capabilities of large language models (LMs) in multi-step tasks, overcoming the constraints of limited context length (e.g., 2048 tokens in GPT-3 and PaLM). RoT employs special tokens to manage recursive context creation, enabling LMs to handle complex problems through a divide-and-conquer approach. The methodology involves training and evaluating RoT on various arithmetic and algorithmic tasks, demonstrating that it outperforms existing methods (like without thought and chain of thought) in accuracy and generalization, even allowing smaller models to solve problems previously manageable only by larger models.\n\n# Strength And Weaknesses\nThe primary strengths of RoT lie in its innovative approach to overcoming context limitations, its model-agnostic nature allowing integration with various architectures, and its remarkable accuracy across diverse problem types. Additionally, it underscores the potential for smaller models to perform complex reasoning tasks. However, the paper also acknowledges significant weaknesses, including its reliance on supervised learning for ground truth intermediate steps, challenges with generalization to longer problem instances, and inherent architectural limitations of models like Transformers. Moreover, the computational cost associated with evaluating RoT could hinder its practicality in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The quality of the writing is high, with a logical flow that facilitates understanding of complex concepts. The novelty of RoT is significant, presenting an original approach to enhancing LM reasoning capabilities. However, reproducibility may be limited due to the reliance on supervised training data, which may not always be readily available for all tasks or in practical scenarios.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of language model reasoning with the introduction of the Recursion of Thought framework, which effectively addresses context challenges and enhances reasoning capabilities across complex tasks. However, the dependency on supervision and challenges related to generalization highlight areas for future exploration and refinement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a novel framework called Recursion of Thought (RoT) that enhances the reasoning capabilities of language models by allowing them to utilize recursive processes to manage complex problems beyond their token limitations. The methodology involves an inference process where special tokens guide the model in creating and handling subproblems, improving performance on tasks that require extensive reasoning. The findings demonstrate that RoT significantly outperforms traditional approaches like Chain of Thought (CoT) and even enables smaller models to achieve unexpectedly high accuracy on complex reasoning tasks, suggesting a shift in how model capabilities are understood.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the limitations of current language models regarding context size and reasoning complexity. The introduction of special tokens to manage recursion is a practical solution that enhances model performance without substantial increases in model size. However, a notable weakness is the limitation in generalization capabilities, as RoT does not facilitate length generalization. Additionally, while the results are promising, the reliance on supervised learning raises concerns about scalability and applicability in less supervised settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its concepts, methodology, and results. The quality of the writing is high, with detailed explanations and supporting equations that enhance understanding. The novelty of RoT is significant, as it proposes a new way to solve problems using language models that has not been extensively explored in the literature. However, while the methodology is described in detail, the reproducibility could be challenged by the requirement for supervision and the complexity of the training process.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative framework that enhances the reasoning abilities of language models through recursion. While the contributions are substantial and the results promising, limitations regarding generalization and the need for supervision may affect its broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5"
      ],
      "preparations": [
        null,
        "### ICLR 2023 Paper Notes: \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"\n\n#### Abstract\n- Recent advances in language models (LMs) allow them to tackle multi-step reasoning problems using Chain of Thought (CoT) generation.\n- CoT requires LMs to produce intermediate steps before arriving at a final answer, but it is limited by context size (e.g., 2048 tokens for GPT-3 and PaLM).\n- The paper proposes a model-agnostic framework called Recursion of Thought (RoT) to enhance reasoning by allowing LMs to recursively create multiple contexts.\n- RoT significantly improves the performance of GPT-3 on complex reasoning tasks and enables even small models (randomly initialized Transformers or LSTMs) to solve difficult problems.\n\n#### 1. Introduction\n- LMs are increasingly applied to reasoning tasks, but their effectiveness is hindered by context limits.\n- CoT generation improves reasoning by breaking down problems into manageable steps, but it cannot handle problems requiring more tokens than the context size.\n- RoT mimics human reasoning using abstraction and recursion, allowing models to divide large problems into smaller subproblems recursively.\n- RoT introduces special tokens (GO, STOP, THINK) to control the recursion process during inference.\n\n#### 2. Related Work\n- **Chain of Thought**: Scratchpad, CoT prompting, and other prompting methods enhance LMs' reasoning but are still constrained by context size.\n- **Neural Programmer-Interpreter (NPI)**: Works with program execution and recursion but requires extensive training data.\n- **System 1 vs. System 2 Approaches**: RoT falls under System 2, emphasizing sequential reasoning and process generation.\n\n#### 3. Recursion of Thought\n##### 3.1 Inference\n- RoT allows LMs to control recursion using tokens:\n  - **GO**: Start of a problem sequence.\n  - **STOP**: End of a problem sequence.\n  - **THINK**: Initiates a recursion for subproblems.\n- Each context is structured to include the main question, subproblem questions, and their answers.\n- Tail recursion is supported with a special **TAIL** token to treat the last subproblem's answer as the final answer.\n\n##### 3.2 Training\n- RoT is trained in a supervised manner using ground truth for intermediate steps and the timing of special token outputs.\n- The model is trained to produce a target sequence that corresponds with the constructed context sequence.\n\n##### 3.3 Recursive Reasoning Procedures\n- Exemplary reasoning procedures for each problem type are implemented to generate training data automatically.\n\n#### 4. Experiments\n- **Reasoning Problems**: The paper evaluates RoT on arithmetic (addition, subtraction, multiplication, division) and algorithmic tasks (LCS, LPS, 0-1 Knapsack, MCM).\n- **Baselines**: \n  - **Without Thought (WT)**: Directly produces an answer.\n  - **Chain of Thought (CoT)**: Generates all intermediate steps without recursion.\n- RoT demonstrated superior performance compared to both baselines across all tasks, achieving near-perfect accuracy even as problem difficulty increased.\n\n#### 4.1 Reasoning Problems\n- Arithmetic tasks ranged from basic operations to large digit numbers (up to 48 digits).\n- Algorithmic tasks involved dynamic programming and required significant intermediate steps.\n\n#### 4.2 Unleashing GPT-3's Capability\n- Fine-tuning GPT-3 using RoT allowed it to solve complex arithmetic tasks that it previously struggled with.\n- Results indicated RoT could handle problems requiring tens of thousands of tokens without architectural changes.\n\n#### 4.3 RoT with Tiny Language Models\n- RoT was successfully applied to smaller models (randomly initialized Transformers and LSTMs) showing that they could handle complex reasoning.\n\n#### 5. Discussion\n- The findings suggest potential for smaller models to perform effectively with RoT, challenging the notion that larger models are necessary for advanced reasoning tasks.\n- Future work may focus on reducing the need for supervision in RoT for broader task applicability.\n\n#### 6. Conclusion\n- RoT addresses the constraint of single context size in LMs, allowing them to utilize multiple contexts for improved reasoning.\n- The framework has shown promise in making LMs capable of solving problems that require extensive token generation, indicating a significant advancement in reasoning capabilities.\n\n#### References\n- The paper cites various foundational works on language models, reasoning, and recursive approaches, such as Transformers, NPI, and previous studies on CoT and reasoning tasks.\n\n#### Supplementary Material\n- Source code provided for experiment reproduction.",
        "### Notes on \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"\n\n#### Technical Content\n- **Objective**: The paper addresses the limitations of large language models (LMs) in handling multi-step reasoning tasks due to context length constraints (e.g., 2048 tokens for models like GPT-3 and PaLM).\n- **Proposed Method**: Introduction of \"Recursion of Thought\" (RoT), a model-agnostic framework that allows LMs to recursively create and utilize multiple contexts to solve complex problems through a divide-and-conquer approach.\n- **Key Features**:\n  - **Special Tokens**: Introduces tokens like GO (start), STOP (end), THINK (initiate recursion), and TAIL (indicates final answer) to manage the recursive context creation.\n  - **Tail Recursion Support**: Allows the model to process an indefinitely long chain of recursion.\n- **Tasks**: RoT is evaluated on various arithmetic operations (addition, subtraction, multiplication, division) and algorithmic tasks (e.g., longest common subsequence, knapsack problem, matrix chain multiplication).\n\n#### Experimental Design\n- **Baseline Comparisons**:\n  - **Without Thought (WT)**: Directly outputs an answer from a question.\n  - **Chain of Thought (CoT)**: Generates all intermediate reasoning steps without recursion.\n- **Data Generation**: Ground truth contexts for training RoT were automatically generated based on defined reasoning procedures for each problem type.\n- **Training**: RoT is trained in a supervised manner, requiring ground truth intermediate steps and outputs.\n- **Evaluation**: Accuracy is measured based on whether all intermediate steps and the final answer are correct. The training and evaluation process uses the same problem distribution.\n\n#### Results\n- **Performance**:\n  - RoT significantly outperforms both WT and CoT, achieving near-perfect accuracy even on complex problems requiring hundreds of thousands of tokens.\n  - RoT allows smaller, randomly initialized models (Transformers, LSTMs) to solve problems that larger models struggle with.\n  - Demonstrated that RoT can handle extreme complexities (e.g., 48-digit arithmetic) that traditional CoT methods cannot due to context limits.\n- **Generalization**: RoT shows promising results in scaling up problem complexity and generalizing solutions better than previous methods.\n\n#### Strengths\n- **Innovative Approach**: RoT introduces a novel method to overcome context limitations in LMs, increasing their reasoning capabilities.\n- **Model Agnostic**: The framework can be integrated with various sequence models, enhancing its applicability.\n- **High Accuracy**: Achieves remarkable performance across different problem types, demonstrating the effectiveness of the recursive reasoning paradigm.\n- **Support for Smaller Models**: Shows that even small models can perform complex reasoning, suggesting potential for efficient AI systems.\n\n#### Limitations\n- **Need for Supervision**: RoT requires ground truth intermediate steps, which may not be feasible for all problem types or in real-world applications where such data is unavailable.\n- **Length Generalization**: RoT struggles with length generalization; training on shorter problems does not allow the model to generalize to longer problems.\n- **Dependence on Architecture**: Some limitations in reasoning capabilities may stem from the inherent constraints of the architecture used (e.g., Transformers not handling long-term dependencies well).\n- **Evaluation Cost**: The evaluation of RoT can be computationally expensive due to the need for extensive token generation, especially in complex tasks.\n\n### Conclusion\nThe paper presents a significant advancement in language model reasoning by introducing the Recursion of Thought framework, which successfully addresses context limitations and enhances the reasoning abilities of LMs across various complex tasks. However, the dependency on supervision and challenges with length generalization present avenues for further research and development.",
        "# Notes on \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"\n\n## Theoretical Foundations\n- **Context Limitations**: Current language models (e.g., GPT-3, PaLM) have a maximum context size (e.g., 2048 tokens), which limits their ability to handle complex reasoning that may require significantly more tokens.\n- **Chain of Thought (CoT)**: A method where models generate intermediate reasoning steps before arriving at a final answer, overcoming the limitations of single-pass problem-solving.\n- **Recursion of Thought (RoT)**: Proposed as a model-agnostic framework to allow language models to recursively create and utilize multiple contexts for solving complex problems. Inspired by human reasoning which often uses abstraction and recursion.\n\n## Key Concepts\n- **Tokens**: RoT introduces special tokens to control the recursion process:\n  - **GO**: Marks the beginning of a problem sequence.\n  - **STOP**: Marks the end of a problem sequence.\n  - **THINK**: Initiates a recursive procedure.\n  - **TAIL**: Used for tail recursion scenarios where the answer of the last subproblem is the final answer.\n\n## Experimental Methodology\n### Inference Process (Section 3.1)\n- **Inference Context Structure**: \n  - Defined as: \\( X = [Q; Q_{sub,1}; A_{sub,1}; ...; Q_{sub,N}; A_{sub,N}; A] \\)\n  - \\( Q \\) is the main question, \\( A \\) is the final answer, and \\( Q_{sub,*} \\), \\( A_{sub,*} \\) are subproblems and their answers.\n- **Recursive Control**: The model generates tokens sequentially, deciding whether to produce an answer directly or start a subproblem with the GO token, or trigger recursion with the THINK token.\n\n### Training Process (Section 3.2)\n- **Ground Truth Context Construction**: Each training example consists of context \\( X \\) and target sequence \\( Y \\) with:\n  - \\( Y \\) replacing \\( Q \\) with PAD tokens and \\( A_{sub,n} \\) with THINK tokens followed by PADs.\n- **Training Objective**: \n  - Loss defined as:\n  \\[\n  L = -\\sum I[y_{i+1} \\neq PAD] \\log p(y_{i+1} | X_{1:i})\n  \\]\n- **End-to-End Training**: Models are trained using stochastic gradient descent.\n\n### Problem Types Evaluated (Section 4)\n1. **Arithmetic Reasoning**: Addition, subtraction, multiplication, division.\n2. **Algorithmic Reasoning**: Longest Common Subsequence (LCS), Longest Palindromic Subsequence (LPS), 0-1 Knapsack, Matrix Chain Multiplication (MCM).\n\n## Results\n- RoT significantly outperformed both Without Thought (WT) and Chain of Thought (CoT) approaches across all evaluated tasks.\n- RoT enabled large models like GPT-3 to solve problems requiring hundreds of thousands of tokens, achieving near-perfect accuracy on complex tasks.\n- Tiny models (randomly initialized Transformers/LSTMs) also demonstrated the capability to solve difficult problems with RoT, outperforming expectations for their sizes.\n\n## Implications\n- **Rethinking Model Capabilities**: RoT's success raises questions about the necessity of model size versus the ability to utilize recursion and multiple contexts.\n- **Future Directions**: Potential applications for RoT in broader contexts, although supervision remains necessary for task learning. The need for techniques to reduce supervision costs is identified, suggesting a possible integration with reinforcement learning techniques.\n- **Generalization Limitations**: RoT does not enable length generalization, indicating that future advancements might combine RoT with more advanced architectures that can handle this limitation.\n\n## Equations and Algorithms\n- **Equation for Context Structure**: \n  \\[\n  X = [Q; Q_{sub,1}; A_{sub,1}; ...; Q_{sub,N}; A_{sub,N}; A]\n  \\]\n- **Loss Function**: \n  \\[\n  L = -\\sum I[y_{i+1} \\neq PAD] \\log p(y_{i+1} | X_{1:i})\n  \\]\n\n### Algorithm 1: Recursion of Thought Inference\n- **Initial Context Setup**: Start with the main question and iteratively generate tokens.\n- **Recursive Handling**: Use special tokens to manage subproblems and return answers accordingly.\n\n### Algorithm 2: Creating Target Sequence\n- **Transforming Context to Target**: Construct target sequence \\( Y \\) by replacing question and subproblem answers appropriately.\n\n## Additional Notes\n- Context length distributions indicate that RoT uses significantly shorter contexts compared to CoT, with implications for computational efficiency.\n- The paper includes detailed appendices with algorithms for arithmetic, LCS, LPS, and MCM, showcasing the recursive reasoning procedures.\n\nThis comprehensive approach demonstrates RoT's potential to enhance reasoning capabilities in language models significantly, addressing critical limitations imposed by context size."
      ]
    },
    {
      "context": "A THEORY OF REPRESENTATION LEARNING IN NEURAL NETWORKS GIVES A DEEP GENERALISATION OF KERNEL METHODS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a loglikelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We confirm these results experimentally in wide but finite DGPs. Next, we introduce the possibility of using this limit and objective as a flexible, deep generalisation of kernel methods, that we call deep kernel machines (DKMs). Like most naive kernel methods, DKMs scale cubically in the number of datapoints. We therefore use methods from the Gaussian process inducing point literature to develop a sparse DKM that scales linearly in the number of datapoints. Finally, we extend these approaches to NNs (which have non-Gaussian posteriors) in the Appendices.\n\n1\n\nINTRODUCTION\n\nThe successes of modern machine learning methods from neural networks (NNs) to deep Gaussian processes (DGPs Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) is based on their ability to use depth to transform the input into high-level representations that are good for solving difficult tasks (Bengio et al., 2013; LeCun et al., 2015). However, theoretical approaches using infinite limits to understand deep models struggle to capture representation learning. In particular, there are two broad families of infinite limit, and while they both use kernel-matrix-like objects they are ultimately very different. The neural network Gaussian process (NNGP Neal, 1996; Lee et al., 2017; Matthews et al., 2018) applies to Bayesian models like Bayesian neural networks (BNNs) and DGPs and describes the representations at each layer (formally, the NNGP kernel is raw second moment of the activities). In contrast, the neural tangent kernel (NTK Jacot et al., 2018) is a very different quantity that involves gradients, and describes how predictions at all datapoints change if we do a gradient update on a single datapoint. As such, the NNGP and NTK are suited to asking very different theoretical questions. For instance, the NNGP is better suited to understanding the transformation of representations across layers, while the NTK is better suited to understanding how predictions change through NN training.\n\nWhile challenges surrounding representation learning have recently been addressed in the NTK setting Yang & Hu (2020), we are the first to address this challenge in the NNGP setting.\n\nAt the same time, kernel methods (Smola & Sch \u0308olkopf, 1998; Shawe-Taylor & Cristianini, 2004; Hofmann et al., 2008) were a leading machine learning approach prior to the deep learning revolution Krizhevsky et al. (2012). However, kernel methods were eclipsed by deep NNs because depth\n\n1\n\ngives NNs the flexibility to learn a good top-layer representation (Aitchison, 2020). In contrast, in a standard kernel method, the kernel (or equivalently the representation) is highly inflexible \u2014 there are usually a few tunable hyperparameters, but nothing that approaches the enormous flexibility of the top-layer representation in a deep model. There is therefore a need to develop flexible, deep generalisations of kernel method. Remarkably, our advances in understanding representation learning in DGPs give such a flexible, deep kernel method.\n\n2 CONTRIBUTIONS\n\n\u2022 We present a new infinite width limit, the Bayesian representation learning limit, that retains representation learning in deep Bayesian models including DGPs. The key insight is that as the width goes to infinity, the prior becomes stronger, and eventually overwhelms the likelihood. We can fix this by rescaling the likelihood to match the prior. This rescaling can be understood in a Bayesian context as copying the labels (Sec. 4.3).\n\n\u2022 We show that in the Bayesian representation learning limit, DGP posteriors are exactly \u03bb, is the activation\n\nzero-mean multivariate Gaussian, P (cid:0)f l of the \u03bbth feature in layer l for all inputs (Sec. 4.4 and Appendix D).\n\n\u03bb|X, y(cid:1) = N (cid:0)f l\n\n(cid:1) where f l\n\n\u03bb; 0, Gl\n\n\u2022 We show that the posterior covariances can be obtained by optimizing the \u201cdeep kernel\n\nmachine objective\u201d,\n\nL(G1, . . . , GL) = log P (Y|GL) \u2212 (cid:80)L\n\nl=1\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) ,\n\nwhere Gl are the posterior covariances, K(Gl\u22121) are the kernel matrices, and \u03bdl accounts for any differences in layer width (Sec. 4.3).\n\n\u2022 We give an interpretation of this objective, with log P (Y|GL) encouraging improved performance, while the KL-divergence terms act as a regulariser, keeping posteriors, N (0, Gl), close to the prior, N (0, K(Gl\u22121)) (Sec. 4.5).\n\n\u2022 We introduce a sparse DKM, which takes inspiration GP inducing point literature to obtain a practical, scalable method that is linear in the number of datapoints. In contrast, naively computing/optimizing the DKM objective is cubic in the number of datapoints (as with most other naive kernel methods; Sec. 4.7).\n\n\u2022 We extend these results to BNNs (which have non-Gaussian posteriors) in Appendix A.\n\n3 RELATED WORK\n\nOur work is focused on DGPs and gives new results such as the extremely simple multivariate Gaussian form for DGP true posteriors. As such, our work is very different from previous work on NNs, where such results are not available. There are at least three families of such work. First, there is recent work on representation learning in the very different NTK setting (Jacot et al., 2018; Yang, 2019; Yang & Hu, 2020) (see Sec. 1). In contrast, here we focus on NNGPs (Neal, 1996; Williams, 1996; Lee et al., 2017; Matthews et al., 2018; Novak et al., 2018; Garriga-Alonso et al., 2018; Jacot et al., 2018), where the challenge of representation learning has yet to be addressed. Second, there is a body of work using methods from physics to understand representation learning in neural networks (Antognini, 2019; Dyer & Gur-Ari, 2019; Hanin & Nica, 2019; Aitchison, 2020; Li & Sompolinsky, 2020; Yaida, 2020; Naveh et al., 2020; Zavatone-Veth et al., 2021; Zavatone-Veth & Pehlevan, 2021; Roberts et al., 2021; Naveh & Ringel, 2021; Halverson et al., 2021). This work is focuses on perturbational, rather than variational methods. Third, there is a body of theoretical work including (Mei et al., 2018; Nguyen, 2019; Sirignano & Spiliopoulos, 2020a;b; Nguyen & Pham, 2020) which establishes properties such as convergence to the global optimum. This work is focused on two-layer (or one-hidden layer network) networks, and like the NTK, considers learning under SGD rather than Bayesian posteriors.\n\nAnother related line of work uses kernels to give a closed-form expression for the weights of a neural network, based on a greedy, layerwise objective (Wu et al., 2022). This work differs in that it uses the HSIC objective, and therefore does not have a link to DGPs or Bayesian neural networks, and in that it uses a greedy-layerwise objective, rather than end-to-end gradient descent.\n\n2\n\nLayer 1\n\nLayer 2\n\nLayer 3\n\nX\n\nX\n\nX\n\nG0\n\nG0\n\nF1\n\nF1\n\nG1\n\nG1\n\nF2\n\nF2\n\nG2\n\nG2\n\nF3\n\nF3\n\nG3\n\nG3\n\nY\n\nY\n\nY\n\nFigure 1: The graphical model structure for each of our generative models for L = 3. Top. The standard model (Eq. 1), written purely in terms of features, Fl. Middle. The standard model, including Gram matrices as random variables (Eq. 5) Bottom. Integrating out the activations, Fl,\n\n4 RESULTS\n\nWe start by defining a DGP, which contains Bayesian NN (BNNs) as a special case (Appendix A). This model maps from inputs, X \u2208 RP \u00d7\u03bd0, to outputs, Y \u2208 RP \u00d7\u03bdL+1, where P is the number of input points, \u03bd0 is the number of input features, and \u03bdL+1 is the number of output features. The model has L intermediate layers, indexed l \u2208 {1, . . . , L}, and at each intermediate layer there are Nl features, Fl \u2208 RP \u00d7Nl. Both Fl and Y can be written as a stack of vectors, )\n\nY = (y1 y2\n\nFl = (f l\n\n\u00b7 \u00b7 \u00b7 y\u03bdL+1 ),\n\n\u00b7 \u00b7 \u00b7\n\nf l\n\n1\n\n2\n\nf l Nl\n\n\u03bb \u2208 RP gives the value of one feature and y\u03bb \u2208 RP gives the value of one output for all where f l P input points. The features, F1, . . . , FL, and (for regression) the outputs, Y, are sampled from a Gaussian process (GP) with a covariance which depends on the previous layer features (Fig. 1 top),\n\nP (Fl|Fl\u22121) = (cid:81)Nl\n\nP (Y|FL) = (cid:81)\u03bdL+1\n\n\u03bb=1N (cid:0)f l \u03bb; 0, K(G(Fl\u22121))(cid:1) \u03bb=1 N (cid:0)y\u03bb; 0, K(G(FL)) + \u03c32I(cid:1) .\n\n(1a)\n\n(1b)\n\nNote we only use the regression likelihood to give a concrete example; we could equally use an alternative likelihood e.g. for classification (Appendix B). The distinction between DGPs and BNNs arises through the choice of K(\u00b7) and G(\u00b7). For BNNs, see Appendix A. For DGPs, G(\u00b7), which takes the features and computes the corresponding P \u00d7 P Gram matrix, is\n\nG(Fl\u22121) = 1\n\nNl\u22121\n\n(cid:80)Nl\u22121\n\n\u03bb=1 f l\u22121\n\n\u03bb\n\n(f l\u22121\n\n\u03bb\n\n)T = 1\n\nNl\u22121\n\nFl\u22121FT\n\nl\u22121.\n\n(2)\n\nNow, we introduce random variables representing the Gram matrices, Gl\u22121 = G(Fl\u22121), where Gl\u22121 is a random variable representing the Gram matrix at layer l \u2212 1, whereas G(\u00b7) is a deterministic function that takes features and computes the corresponding Gram matrix using Eq. (2). Finally, K(\u00b7), transforms the Gram matrices, Gl\u22121 to the final kernel. Many kernels of interest are isotropic, meaning they depend only on the normalized squared distance between datapoints, Rij, Kisotropic;ij(Gl\u22121) = kisotropic (Rij(Gl\u22121)) .\n\n(3)\n\nImportantly, we can compute this squared distance from Gl\u22121, without needing Fl\u22121, (cid:1)2\n\n(cid:1)2\n\n(cid:80)N\n\n(cid:80)N\n\n(cid:1)2(cid:1)\n\n(cid:0)(cid:0)Fi\u03bb\n\n\u2212 2Fi\u03bbFj\u03bb + (cid:0)Fj\u03bb\n\n(cid:0)Fi\u03bb \u2212 Fj\u03bb\n\nRij(G) = 1\n\nN\n\n= 1 N\n\n\u03bb=1\n\n\u03bb=1\n\n= Gii \u2212 2Gij + Gjj,\n\n(4)\n\nwhere \u03bb indexes features, i and j index datapoints and we have omitted the layer index for simplicity. Importantly, we are not restricted to isotropic kernels: other kernels that depend only on the Gram matrix, such as the arccos kernels from the infinite NN literature (Cho & Saul, 2009) can also be used (for further details, see Aitchison et al., 2020).\n\n4.1 BNN AND DGP PRIORS CAN BE WRITTEN PURELY IN TERMS OF GRAM MATRICES\n\nNotice that Fl depends on Fl\u22121 only through Gl\u22121 = G(Fl\u22121), and Y depends on FL only through GL = G(FL) (Eq. 1). We can therefore write the graphical model in terms of those Gram matrices (Fig. 1 middle).\n\nP (Fl|Gl\u22121) = (cid:81)Nl\n\n\u03bb=1N (cid:0)f l\n\n\u03bb; 0, K(Gl\u22121)(cid:1)\n\nP (Gl|Fl) = \u03b4 (Gl \u2212 G(Fl)) P (Y|GL) = (cid:81)\u03bdL+1\n\n\u03bb=1 N (cid:0)y\u03bb; 0, K(GL) + \u03c32I(cid:1) .\n\n3\n\n(5a)\n\n(5b)\n\n(5c)\n\nwhere \u03b4 is the Dirac-delta, and G0 depends on X (e.g. G0 = 1 \u03bd0 have used a regression likelihood, but other likelihoods could also be used.\n\nXXT ). Again, for concreteness we\n\nNow, we can integrate Fl out of the model, in which case, we get an equivalent generative model written solely in terms of Gram matrices (Fig. 1 bottom), with\n\nP (Gl|Gl\u22121) =\n\n(cid:90)\n\ndFl P (Gl|Fl) P (Fl|Gl\u22121) ,\n\n(6)\n\nand with the usual likelihood (e.g. Eq. 5c). This looks intractable (and indeed, in general it is intractable). However for DGPs, an analytic form is available. In particular, note the Gram matrix (Eq. 2) is the outer product of IID Gaussian distributed vectors (Eq. 1a). This matches the definition of the Wishart distribution (Gupta & Nagar, 2018), so we have,\n\nP (Gl|Gl\u22121) = Wishart\n\nK(Gl\u22121), Nl\n\n(cid:17)\n\n(cid:16)\n\nGl; 1 Nl log |Gl|\u2212 Nl\n\nlog P (Gl|Gl\u22121) = Nl\u2212P \u22121\n\n2\n\n2 log |K(Gl\u22121)|\u2212 Nl\n\n2 Tr (cid:0)K\u22121(Gl\u22121)Gl\n\n(7)\n\n(cid:1) + const .\n\nThis distribution over Gram matrices is valid for DGPs of any width (though we need to be careful in the low-rank setting where Nl < P ). We are going to leverage these Wishart distributions to understand the behaviour of the Gram matrices in the infinite width limit.\n\n4.2 STANDARD INFINITE WIDTH LIMITS OF DGPS LACK REPRESENTATION LEARNING\n\nWe are now in a position to take a new viewpoint on the DGP analogue of standard NNGP results (Lee et al., 2017; Matthews et al., 2018; Hron et al., 2020; Pleiss & Cunningham, 2021). We can then evaluate the log-posterior for a model written only in terms of Gram matrices,\n\nlog P (G1, . . . , GL|X, Y) = log P (Y|GL) + (cid:80)L\n\nl=1 log P (Gl|Gl\u22121) + const .\n\n(8)\n\nThen we take the limit of infinite width,\n\nNl = N \u03bdl\n\nfor\n\nl \u2208 {1, . . . , L}\n\nwith\n\nN \u2192 \u221e.\n\n(9)\n\nThis limit modifies log P (Gl|Gl\u22121) (Eq. 7), but does not modify G1, . . . , GL in Eq. (8) as we get to choose the values of G1, . . . , GL at which to evaluate the log-posterior. Specifically, the logprior, log P (Gl|Gl\u22121) (Eq. 7), scales with Nl and hence with N . To get a finite limit, we therefore need to divide by N ,\n\nlim N\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = \u03bdl\n\n2\n\n(cid:0)log (cid:12)\n\n(cid:12)K\u22121(Gl\u22121)Gl\n\n(cid:12) (cid:12) \u2212 Tr (cid:0)K\u22121(Gl\u22121)Gl\n\n(cid:1)(cid:1) + const\n\n= \u2212\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) + const .\n\n(10)\n\nAnd remarkably this limit can be written as the KL-divergence between two multivariate is constant wrt N (Eq. 5c), so Gaussians. 1\nlimN\u2192\u221e\n\nN log P (Y|GL) = 0. The limiting log-posterior is thus,\n\nthe log likelihood,\n\nlog P (Y|GL),\n\nIn contrast,\n\nlim N\u2192\u221e\n\n1\n\nN log P (G1, . . . , GL|X, Y) = \u2212(cid:80)L\n\nl=1\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) + const .\n\nThis form highlights that the log-posterior scales with N , so in the limit as N \u2192 \u221e, the posterior converges to a point distribution at the global maximum, denoted G\u2217 L, (see Appendix C for a formal discussion of weak convergence),\n\n1, . . . , G\u2217\n\n(11)\n\nlim N\u2192\u221e\n\nP (G1, . . . , GL|X, Y) = (cid:81)L\n\nl=1\u03b4 (Gl \u2212 G\u2217\n\nl ) .\n\n(12)\n\nMoreover, it is evident from the KL-divergence form for the log-posterior (Eq. 11) that the unique global maximum can be computed recursively as G\u2217 XXT . Thus, the limiting posterior over Gram matrices does not depend on the training targets, so there is no possibility of representation learning (Aitchison, 2020). This is deeply problematic as the successes of modern deep learning arise from flexibly learning good top-layer representations.\n\nl\u22121), with e.g. G\u2217\n\nl = K(G\u2217\n\n0 = 1\n\n\u03bd0\n\n4\n\n4.3 THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nIn the previous section, we saw that standard infinite width limits eliminate representation learning because as N \u2192 \u221e the log-prior terms, log P (Gl|Gl\u22121), in Eq. (8) dominated the log-likelihood, P (Y|GL), and the likelihood is the only term that depends on the labels. We therefore introduce the \u201cBayesian representation learning limit\u201d which retains representation learning. The Bayesian representation learning limit sends the number of output features, NL+1, to infinity as the layerwidths go to infinity,\n\nNl = N \u03bdl\n\nfor\n\nl \u2208 {1, . . . , L + 1}\n\nwith\n\nN \u2192 \u221e.\n\n(13)\n\nImportantly, the Bayesian representation learning limit gives a valid probabilistic model with a welldefined posterior, arising from the prior, (Eq. 6) and a likelihood which assumes each output channel is IID,\n\nP ( \u0303Y|GL) = (cid:81)NL+1\n\n\u03bb=1 N (cid:0) \u0303y\u03bb; 0, K(GL) + \u03c32I(cid:1) .\n\n(14)\n\nwhere \u0303Y \u2208 RP \u00d7NL+1 is infinite width (Eq. 13) whereas the usual DGP data, Y \u2208 RP \u00d7\u03bdL+1, is finite width. Of course, infinite-width data is unusual if not unheard-of. In practice, real data, Y \u2208 RP \u00d7\u03bdL+1, almost always has a finite number of features, \u03bdL+1. How do we apply the DKM to such data? The answer is to define \u0303Y as N copies of the underlying data, Y, i.e. \u0303Y = (Y \u00b7 \u00b7 \u00b7 Y). As each channel is assumed to be IID (Eq. 5c and 14) the likelihood is N times larger,\n\nlog P ( \u0303Y|GL) = N log P (Y|GL) ,\n\n(15)\n\nThe log-posterior in the Bayesian representation learning limit is very similar to the log-posterior in the standard limit (Eq. 11). The only difference is that the likelihood, log P ( \u0303Y|GL) now scales with N , so it does not disappear as we take the limit, allowing us to retain representation learning,\n\nL(G1, . . . , GL) = lim\n\nN\u2192\u221e\n\n1\n\nN log P (G1, . . . , GL|X, \u0303Y) + const,\n\n(16)\n\n= log P (Y|GL) \u2212 (cid:80)L\n\nl=1\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) .\n\n1, . . . , G\u2217\n\nHere, we denote the limiting log-posterior as L(G1, . . . , GL), and this forms the DKM objective. Again, as long as the global maximum of the DKM objective is unique, the posterior is again a point distribution around that maximum (Eq. 12). Of course, the inclusion of the likelihood term means that the global optimum G\u2217 L cannot be computed recursively, but instead we need to optimize, e.g. using gradient descent (see Sec. 4.7). Unlike in the standard limit (Eq. 11), it is no longer possible to guarantee uniqueness of the global maximum. We can nonetheless say that the posterior converges to a point distribution as long as the global maximum of L(G1, . . . , GL) is unique, (i.e. we can have any number of local maxima, as long as they all lie below the unique global maximum). We do expect the global maximum to be unique in most practical settings: we know the maximum is unique when the prior dominates (Eq. 11), in Appendix J, we prove uniqueness for linear models, and in Appendix K, we give a number of experiments in nonlinear models in which optimizing from very different initializations found the same global maximum, indicating uniqueness in practical settings.\n\n4.4 THE EXACT DGP POSTERIOR OVER FEATURES IS MULTIVARIATE GAUSSIAN\n\nAbove, we noted that the DGP posterior over Gram matrices in the Bayesian representation learning limit is a point distribution, as long as the DKM objective has a unique global maximum. Remarkably, in this setting, the corresponding posterior over features is multivariate Gaussian (see Appendix D for the full derivation),\n\nP (cid:0)f l\n\n\u03bb|X, y(cid:1) = N (cid:0)f l\n\n\u03bb; 0, G\u2217\n\nl\n\n(cid:1)\n\n(17)\n\nWhile such a simple result might initially seem remarkable, it should not surprise us too much. In particular, the prior is Gaussian (Eq. 1). In addition, in Fig. 1 (middle), we saw that the next layer features depend on the current layer features only through the Gram matrices, which are just the raw second moment of the features, Eq. (2). Thus, in effect the likelihood only constrains the raw second moments of the features. Critically, that constraints on the raw second moment are tightly connected to Gaussian distributions: under the MaxEnt framework, a Gaussian distribution arises by maximizing the entropy under constraints on the raw second moment of the features (Jaynes, 2003).\n\n5\n\nThus it is entirely plausible that a Gaussian prior combined with a likelihood that \u201cconstrains\u201d the raw second moment would give rise to Gaussian posteriors (though of course this is not a proof; see Appendix D for the full derivation).\n\nFinally, note that we appear to use Gl or G\u2217 l in Eq. (2) and as the posterior covariance in the Bayesian representation learning limit (Eq. 17). In the infinite limit, these two uses are consistent. In particular, consider the value of Gl defined by Eq. (2) under the posterior,\n\nl in two separate senses: as 1\n\nFlFT\n\nNl\n\nGl = lim\n\nN\u2192\u221e\n\n1 Nl\n\n(cid:80)Nl\n\n\u03bb=1f l\n\n\u03bb(f l\n\n\u03bb)T = E\n\nP(f l\n\n\u03bb|X,y)\n\n(cid:2)f l\n\n\u03bb(f l\n\n\u03bb)T (cid:3) = G\u2217 l .\n\n(18)\n\nThe second equality arises by noticing that we are computing the average of infinitely many terms, f l \u03bb)T , which are IID under the true posterior (Eq. 17), so we can apply the law of large numbers, \u03bb(f l and the final expectation arises by computing moments under Eq. (17).\n\n4.5 THE DKM OBJECTIVE GIVES INTUITION FOR REPRESENTATION LEARNING\n\nThe form for the DKM objective in Eq. (16) gives a strong intuition for how representation learning occurs in deep networks. In particular, the likelihood, log P (Y|GL), encourages the model to find a representation giving good performance on the training data. At the same time, the KL-divergence terms keep the posterior over features, N (0, Gl), (Eq. 17) close to the prior N (0, K(Gl\u22121)) (Eq. 1a). This encourages the optimized representations, Gl, to lie close to their value under the standard infinite-width limit, K(Gl\u22121). We could use any form for the likelihood including classification and regression, but to understand how the likelihood interacts with the other KL-divergence terms, it is easiest to consider regression (Eq. 5c), as this log-likelihood can also be written as a KL-divergence,\n\nlog P (Y|GL) = \u2212\u03bdL+1 DKL\n\n(cid:0)N (0, GL+1)(cid:13) (19) Thus, the likelihood encourages K(GL) + \u03c32I to be close to the covariance of the data, GL+1 = 1\nIn combi\u03bdL+1 nation, we would expect the optimal Gram matrices to \u201cinterpolate\u201d between the input kernel, G0 = 1 \u03bd0\n\nYYT , while the DGP prior terms encourage all Gl to lie close to K(Gl\u22121).\n\n(cid:13)N (cid:0)0, K(GL) + \u03c32I(cid:1)(cid:1) + const\n\nXXT and the output kernel, GL+1.\n\nTo make the notion of interpolation explicit, we consider \u03c32 = 0 with a linear kernel, K(Gl\u22121) = Gl\u22121, so named because it corresponds to a linear neural network layer. With this kernel and with all \u03bdl = \u03bd, there is an analytic solution for the (unique) optimum of the DKM objective (Appendix J.1),\n\nG\u2217\n\n(cid:0)G\u22121\n\n(cid:1)l/(L+1)\n\nl = G0\n\n(20) which explicitly geometrically interpolates between G0 and GL+1. Of course, this discussion was primarily for DGPs, but the exact same intuitions hold for BNNs, in that maximizing the DKM objective finds a sequence of Gram matrices, G\u2217 L that interpolate between the input kernel, G0 and the output kernel, GL+1. The only difference is in details of P (Gl|Gl\u22121), and specifically as slight differences in the KL-divergence terms (see below).\n\n1, . . . , G\u2217\n\n0 GL+1\n\n,\n\n4.6 THE DKM OBJECTIVE MIRRORS REPRESENTATION LEARNING IN FINITE NETWORKS\n\nHere, we confirm that the optimizing DKM objective for an infinite network matches doing inference in wide but finite-width networks using Langevin sampling (see Appendix F for details).\n\nWe began by looking at DGPs, and confirming that the posterior marginals are Gaussian (Eq. 17; Fig. 3ab). Then, we confirmed that the representations match closely for infinite-width DKMs (Fig. 2 top and bottom rows) and finite-width DGPs (Fig. 2 middle two rows), both at initialization (Fig. 2 top two rows) and after training to convergence (Fig. 2 bottom two rows). Note that the first column, K0 is a squared exponential kernel applied to the input data, and G3 = yyT is the output Gram matrix (in this case, there is only one output feature).\n\nTo confirm that the match improves as the DGP gets wider, we considered the RMSE between elements of the Gram matrices for networks of different widths (x-axis) for different UCI datasets (columns) and different numbers of layers (top row is one-layer, bottom row is two-layers; Fig. 3c). In most cases, we found a good match as long as the width was at least 128, which is around the width of typical fully connected neural network, but is a little larger than typical DGP widths (e.g. Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017).\n\n6\n\nFigure 2: A two hidden layer DGP with 1024 units per hidden layer and DKM with squared exponential kernels match closely. The data was the first 50 datapoints of the yacht dataset. The first column, K0 is a fixed squared exponential kernel applied to the inputs, and the last column, G3 = yyT is the fixed output Gram matrix. The first row is the DKM initialization at the prior Gram matrices and kernels, and the second row is the DGP, which is initialized by sampling from the prior. As expected, the finite width DGP prior closely matches the infinite-width DKM initialization, which corresponds to the standard infinite width limit. The third row is the Gram matrices and kernels for the trained DGP, which has changed dramatically relative to its initialization (second row) in order to better fit the data. The fourth row is the Gram matrices and kernels for the optimized DKM, which closely matches those for the trained DGP.\n\n4.7 THE SPARSE DEEP KERNEL MACHINE AS A DEEP GENERALISATION OF KERNEL\n\nMETHODS\n\nDGPs in the representation learning limit constitute a deep generalisation of kernel methods, with a very flexible learned kernel, which we call the deep kernel machine (DKM; which was introduced earlier just in the context of the objective). Here, we design a sparse DKM, inspired by sparse methods for DGPs (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) (Appendix L). The sparse DKM scales linearly in the number of datapoints, P , as opposed to cubic scaling of the plain DKM (similar to the cubic scaling in most naive kernel methods).\n\nWe compared DKMs (Eq. 16) and MAP over features (Sec. E) for DGPs. In addition, we considered a baseline, which was a standard, shallow kernel method mirroring the structure of the deep kernel machine but where the only flexibility comes from the hyperparameters. Formally, this model can be obtained by setting, Gl = K(Gl\u22121) and is denoted \u201cKernel Hyper\u201d in Table 1. We applied these methods to UCI datasets (Gal & Ghahramani, 2016) using a two hidden layer architecture, with a kernel inspired by DGP skip-connections, K(Gl) = wl 2 and \u03c3 are hyperparameters, and Ksqexp(Gl) is a squared-exponential kernel.\n\n2Ksqexp(Gl). Here, wl\n\n1Gl + wl\n\n1, wl\n\nWe used 300 inducing points fixed to a random subset of the training data and not optimised during training. We used the Adam optimizer with a learning rate of 0.001, full-batch gradients and 5000 iterations for smaller datasets and 1000 iterations for larger datasets (kin8nm, naval and protein).\n\nWe found that the deep kernel machine objective gave better performance than MAP, or the hyperparameter optimization baseline (Tab. 1). Note that these numbers are not directly comparable to those in the deep GP literature (Salimbeni & Deisenroth, 2017), as deep GPs have a full posterior so offer excellent protection against overfitting, while DKMs give only a point estimate.\n\n7\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 3: Wide DGP posteriors converge to the DKM. Here, we trained DGPs with Langevin sampling (see Appendix F), and compared to a trained DKM. a Marginal distribution over features for one input datapoint for a two-layer DGP trained on a subset of yacht. We used a width of N1...L = 1024 and \u03bd1...L = 5 in all plots to ensure that the data had a strong effect on the learned representations. The marginals (blue histogram) are very close to Gaussian (the red line shows the closest fitted Gaussian). Remember that the true posterior over features is IID (Eq. 31), so each column aggregates the distribution over features (and over 10 parallel chains with 100 samples from each chain) for a single input datapoint. b The 2D marginal distributions for the same DGP for two input points (horizontal and vertical axes). c Element-wise RMSE (normalized Frobenius distance) between Gram matrices from a trained DKM compared to trained DGPs of increasing width. The DGP Gram matrices converge to the DKM solution as width becomes larger.\n\nTable 1: RMSE for inducing point methods. (Equal) best methods are displayed in bold. Error bars give two stderrs for a paired tests, which uses differences in performance between that method and best method, (so there are no meaningful error bars on the best performing method itself). The MAP objective was numerically unstable and thus did not run to completion on the boston dataset.\n\ndataset\n\nP\n\nKernel Hyper\n\nMAP\n\nL\n\n506 1,030 768\n\n4.41 \u00b1 0.31 5.38 \u00b1 0.098 0.83 \u00b1 0.076\n\nboston concrete energy kin8nm 8,192 (7.3 \u00b1 0.06)\u00b710-2 (6.4 \u00b1 0.6)\u00b710-4 3.81 \u00b1 0.091 4.21 \u00b1 0.029 0.68 \u00b1 0.0084 0.94 \u00b1 0.058\n\nnaval power protein wine yacht\n\n11,934 9,568 45,730 1,599 308\n\n\u2014 5.60 \u00b1 0.15 0.73 \u00b1 0.049 (7.4 \u00b1 0.05)\u00b710-2 (5.4 \u00b1 0.5)\u00b710-4 3.73 \u00b1 0.14 4.30 \u00b1 0.033 0.66 \u00b1 0.0067 1.14 \u00b1 0.077\n\n4.35 \u00b1 0.51 5.10 0.47 6.6\u00b710-2 4.6\u00b710-4 3.58 4.10 0.64 0.58\n\n5 CONCLUSION\n\nWe introduced the Bayesian representation learning limit, a new infinite-width limit for BNNs and DGPs that retains representation learning. Representation learning in this limit is described by the intuitive DKM objective, which is composed of a log-likelihood describing performance on the task (e.g. classification or regression) and a sum of KL-divergences keeping representations at every layer close to those under the infinite-width prior. For DGPs, the exact posteriors are IID across features and are multivariate Gaussian, with covariances given by optimizing the DKM objective. Empirically, we found that the distribution over features and representations matched those in wide by finite DGPs. We argued that DGPs in the Bayesian representation learning limit form a new\n\n8\n\n505feature0.00.30.6densitya=1505feature=2303input 1303input 2b=1303input 1=20.00.51.0RMSEcbostonyachtconcreteone-layerenergy2123252729211width0.00.51.0RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layer0.00.20.00.2class of practical deep kernel method: DKMs. We introduce sparse DKMs, which scale linearly in the number of datapoints. Finally, we give the extension for BNNs where the exact posteriors are intractable so must be approximated.\n\nREFERENCES\n\nLaurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. In\n\nInternational Conference on Machine Learning, pp. 156\u2013164. PMLR, 2020.\n\nLaurence Aitchison, Adam X Yang, and Sebastian W Ober. Deep kernel processes. arXiv preprint\n\narXiv:2010.01590, 2020.\n\nJoseph M Antognini. Finite size corrections for neural network gaussian processes. arXiv preprint\n\narXiv:1908.10030, 2019.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.\n\nT Tony Cai and Linjun Zhang. High-dimensional gaussian copula regression: Adaptive estimation\n\nand statistical inference. Statistica Sinica, pp. 963\u2013993, 2018.\n\nYoungmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In NIPS, pp. 342\u2013350.\n\nCurran Associates, Inc., 2009.\n\nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and\n\nstatistics, pp. 207\u2013215. PMLR, 2013.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Ad-\n\nvances in neural information processing systems, 32, 2019.\n\nEthan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. arXiv\n\npreprint arXiv:1909.11304, 2019.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016.\n\nAdri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-\n\nworks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.\n\nArjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.\n\nJames Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory.\n\nMachine Learning: Science and Technology, 2(3):035002, 2021.\n\nBoris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv\n\npreprint arXiv:1909.05989, 2019.\n\nThomas Hofmann, Bernhard Sch \u0308olkopf, and Alexander J Smola. Kernel methods in machine learn-\n\ning. The annals of statistics, pp. 1171\u20131220, 2008.\n\nRoger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.\n\nRoger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge\n\nuniversity press, 1994.\n\nJiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, and Jascha Sohl-Dickstein. Exact posterior distributions of wide bayesian neural networks. arXiv preprint arXiv:2006.10541, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl \u0301ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\n9\n\nEdwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.\n\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction\n\nto variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Advances in neural information processing systems, 25, 2012.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444,\n\n2015.\n\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.\n\nQianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The back-\n\npropagating renormalization group. arXiv preprint arXiv:2012.04030, 2020.\n\nAlexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.\n\nGadi Naveh and Zohar Ringel. A self consistent theory of gaussian processes captures feature\n\nlearning effects in finite cnns. arXiv preprint arXiv:2106.04110, 2021.\n\nGadi Naveh, Oded Ben-David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of\n\nfinite networks trained with noisy gradients. arXiv preprint arXiv:2004.01190, 2020.\n\nRadford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29\u201353.\n\nSpringer, 1996.\n\nPhan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multi-\n\nlayer neural networks. arXiv preprint arXiv:2001.11443, 2020.\n\nQuynh Nguyen. On connected sublevel sets in deep learning.\n\nIn International Conference on\n\nMachine Learning, pp. 4790\u20134799. PMLR, 2019.\n\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.\n\nGeoff Pleiss and John P Cunningham. The limitations of large width in neural networks: A deep gaussian process perspective. Advances in Neural Information Processing Systems, 34, 2021.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pp. 1278\u20131286. PMLR, 2014.\n\nDaniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\n\npreprint arXiv:2106.10165, 2021.\n\nHugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian\n\nprocesses. arXiv preprint arXiv:1705.08933, 2017.\n\n10\n\nInbar Seroussi and Zohar Ringel. Separation of scales and a thermodynamic description of feature\n\nlearning in some cnns. arXiv preprint arXiv:2112.15383, 2021.\n\nJohn Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge univer-\n\nsity press, 2004.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central\n\nlimit theorem. Stochastic Processes and their Applications, 130(3):1820\u20131852, 2020a.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of\n\nlarge numbers. SIAM Journal on Applied Mathematics, 80(2):725\u2013752, 2020b.\n\nAlex J Smola and Bernhard Sch \u0308olkopf. Learning with kernels. MIT Press, 1998.\n\nChristopher Williams. Computing with infinite networks. Advances in neural information process-\n\ning systems, 9, 1996.\n\nChieh Wu, Aria Masoomi, Arthur Gretton, and Jennifer Dy. Deep layer-wise networks have closed-\n\nform weights. arXiv preprint arXiv:2202.01210, 2022.\n\nSho Yaida. Non-gaussian processes and neural networks at finite widths.\n\nIn Mathematical and\n\nScientific Machine Learning, pp. 165\u2013192. PMLR, 2020.\n\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.\n\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\n\narXiv:2011.14522, 2020.\n\nJacob Zavatone-Veth and Cengiz Pehlevan. Exact marginal prior distributions of finite bayesian\n\nneural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nJacob A Zavatone-Veth, Abdulkadir Canatar, and Cengiz Pehlevan. Asymptotics of representation\n\nlearning in finite bayesian neural networks. arXiv preprint arXiv:2106.00651, 2021.\n\n11\n\nA BAYESIAN NEURAL NETWORK EXTENSION\n\nConsider a neural network of the form,\n\nF1 = XW0 Fl = \u03c6(Fl\u22121)Wl\u22121 (cid:16)\n\n(cid:17)\n\nW l\n\n\u03bb\u03bc \u223c N\n\n0, 1 Nl\n\nfor l \u2208 {2, . . . , L + 1} (cid:16)\n\n(cid:17)\n\nW 0\n\n\u03bb\u03bc \u223c N\n\n0, 1 \u03bd0\n\n(21a)\n\n(21b)\n\n(21c)\n\nwhere W0 \u2208 R\u03bd0\u00d7N1, Wl \u2208 RNl\u00d7Nl+1 and WL+1 \u2208 RNL\u00d7\u03bdL+1 are weight matrices with independent Gaussian priors and \u03c6 is the usually pointwise nonlinearity.\n\nIn principle, we could integrate out the distribution over Wl to find P (Fl|Fl\u22121)\n\nP (Fl|Fl\u22121) =\n\n(cid:90)\n\ndWl P (Wl) \u03b4 (Fl \u2212 \u03c6(Fl\u22121)Wl\u22121) ,\n\n(22)\n\nwhere \u03b4 is the Dirac delta. In practice, it is much easier to note that conditioned on Fl\u22121, the random variables interest, Fl are a linear combination of Gaussian distributed random variables, Wl. Thus, Fl are themselves Gaussian, and this Gaussian is completely characterised by its mean and variance. We begin by writing the feature vectors, f l\n\n\u03bb in terms of weight vectors, wl \u03bb, \u03bb = \u03c6(Fl\u22121)wl f l \u03bb. As the prior over weight vectors is IID, the prior over features, conditioned on Fl\u22121), is also IID,\n\n(23)\n\nP (W) =\n\nP (Fl|Fl\u22121) =\n\nNl(cid:89)\n\n\u03bb=1\n\nNl(cid:89)\n\n\u03bb=1\n\nP (cid:0)wl\n\n\u03bb\n\n(cid:1) =\n\n(cid:16)\n\nN\n\nNl(cid:89)\n\n\u03bb=1\n\nwl\n\n\u03bb; 0,\n\n1 Nl\u22121\n\nI\n\n(cid:17)\n\n,\n\nP (cid:0)f l\n\n\u03bb|Fl\u22121\n\n(cid:1) .\n\n(24)\n\n(25)\n\nThe mean of f l\n\nE (cid:2)f l\n\n\u03bb|Fl\u22121\n\n\u03bb conditioned on Fl\u22121 is 0, (cid:3) = E (cid:2)\u03c6(Fl\u22121)wl\n\n\u03bb|Fl\u22121\n\n(cid:3) = \u03c6(Fl\u22121) E (cid:2)wl\n\n\u03bb|Fl\u22121\n\n(cid:3) = \u03c6(Fl\u22121) E (cid:2)wl\n\n\u03bb\n\n(cid:3) = 0.\n\n(26)\n\nThe covariance of f l\n\n\u03bb conditioned on Fl\u22121 is, (cid:104) (cid:104)\n\n(cid:105)\n\nE\n\nf l\n\n\u03bb\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1)T\n\n|Fl\u22121\n\n(cid:0)\u03c6(Fl\u22121)wl\n\n\u03bb\n\n(cid:1)T\n\n|Fl\u22121\n\n(cid:105)\n\n\u03bb(wl\n\n\u03bb)T (cid:3) \u03c6T (Fl\u22121)\n\n\u03bb\n\n= E \u03c6(Fl\u22121)wl = \u03c6(Fl\u22121) E (cid:2)wl = 1\n\nNl\u22121\n\n\u03c6(Fl\u22121)\u03c6T (Fl\u22121)\n\n(27)\n\nThis mean and variance imply that Eq. (1) captures the BNN prior, as long as we choose KBNN(\u00b7) and GBNN(\u00b7) such that,\n\nKBNN(GBNN(Fl\u22121)) = 1\n\nNl\u22121\n\n(cid:80)Nl\u22121\n\n\u03bb=1 \u03c6(f l\u22121\n\n\u03bb\n\n)\u03c6T (f l\u22121\n\n\u03bb\n\n),\n\n(28)\n\nSpecifically, we choose the kernel function, KBNN(\u00b7) to be the identity function, and GBNN(\u00b7) to be the same outer product as in the main text for DGPs (Eq. 2), except where we have applied the NN nonlinearity,\n\nKBNN(Gl\u22121) = Gl\u22121, GBNN(Fl\u22121) = 1\n\nNl\u22121\n\n(cid:80)Nl\u22121\n\n\u03bb=1 \u03c6(f l\u22121\n\n\u03bb\n\n)\u03c6T (f l\u22121\n\n\u03bb\n\n).\n\n(29)\n\n(30)\n\nThis form retains the average-outer-product form for GBNN(\u00b7), which is important for our derivations.\n\nNow, Eq. (16) only gave the DKM objective for DGPs. To get a more general form, we need to consider the implied posteriors over features. This posterior is IID over features (Appendix D.1), and for DGPs, it is multivariate Gaussian (Appendix D.2),\n\nP (Fl|Gl\u22121, Gl) = (cid:81)N l\n\n\u03bb=1 P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1) =\n\nfor DGPs\n\n(cid:81)N l\n\n\u03bb=1N (cid:0)f l\n\n\u03bb; 0, Gl\n\n(cid:1) .\n\n(31)\n\n12\n\nFigure 4: The variational DKM closely matches the BNN true posterior obtained with Langevin sampling. a Comparison of Gram matrices. The first two rows show Gram matrices for BNN, with the first row being a random initialization, and the second row being the posterior. The last two rows show the Gram matrices from variational DKMs with a flow approximate posterior (third row) and a multivariate Gaussian approximate posterior (fourth row). In optimizing the variational DKM, we used Eq. (34) with 216 Monte-Carlo samples. The Gram matrices for the flow posterior (third row) closely match those from the BNN posterior (second row), while those for a multivariate Gaussian approximate posterior (fourth row) do not match. b Marginal distributions over features at each layer for one input datapoint estimated using kernel density estimation. The note that the BNN (blue line) marginals are non-Gaussian, but the variational DKM with a flow posterior (red line) is capable of capturing this non-Gaussianity.\n\nNow, we can see that Eq. (16) is a specific example of a general expression. In particular, note that the distribution on the left of the KL-divergence in Eq. (16) is the DGP posterior over features (Eq. 31). Thus, the DKM objective can alternatively be written,\n\nL(G1, . . . , GL) = log P (Y|GL) \u2212 (cid:80)L\n\nl=1\u03bdl DKL\n\n(cid:0)P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1)(cid:13) (cid:13)N (0, K(Gl\u22121))(cid:1) ,\n\n(32)\n\nand this form holds for both BNNs and DGPs (Appendix D.3). As in DGPs, the log-posterior is N times L(G1, . . . , GL) (Eq. 16), so as N is taken to infinity, the posterior for all models becomes a point distribution (Eq. 12) if L(G1, . . . , GL) has a unique global maximum.\n\nIn practice, the true posteriors required to evaluate Eq. (32) are intractable for BNNs, raising the question of how to develop accurate approximations for BNNs. We develop a variational DKM (vDKM) by taking inspiration from variational inference (Jordan et al., 1999; Blei et al., 2017) (Appendix D.4). Of course, variational inference is usually impossible in infinite width models, because it is impossible to work with infinitely large latent variables. Our key insight is that as the true posterior factorises across features (Appendix D.1), we can work with the approximate posterior\n\n13\n\ninitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404abinitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404ab BNNvDKM (flow)BNNl=1\u03bdl DKL (cid:0)f l\n\n(cid:0)Q\u03b8l (cid:1),\n\nover only a single feature vector, Q\u03b8l approach allows us to define a vDKM objective, which bounds the true DKM objective,\n\n(cid:1), where \u03b8l are the parameters and f l\n\n(cid:0)f l\n\n\u03bb \u2208 RP is finite. This\n\n\u03bb\n\nL(G\u03b8(\u03b81), . . . , G\u03b8(\u03b8L)) \u2265 LV(\u03b81, . . . , \u03b8L),\n\nLV(\u03b81, . . . , \u03b8L) = log P (Y|G\u03b8(\u03b8L)) \u2212 (cid:80)L\n\n(33) (cid:1)(cid:13) (cid:13)N (0, K(G\u03b8(\u03b8l\u22121)))(cid:1)\n\n(cid:0)f l\n\n\u03bb\n\n\u03bb|Gl\u22121, Gl\n\nwith equality when the true posteriors, P (cid:0)f l (cid:1). The only subtlety here is that it is practically difficult to design flexible ap- (cid:1) where we explicitly specify and optimize the Gram matrices. Instead proximate posteriors Q\u03b8l we optimize general approximate posterior parameters, \u03b8, and compute the implied Gram matrices,\n\napproximate posteriors, Q\u03b8l\n\nequal\n\n(cid:0)f l\n\nthe\n\n\u03bb\n\n\u03bb\n\nG\u03b8(\u03b8l) = 1\n\nNl\n\nlim Nl\u2192\u221e\n\n(cid:80)Nl\n\n\u03bb=1\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb) = E\n\nQ\u03b8l(f l\n\n\u03bb)\n\n(cid:2)\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb)(cid:3) .\n\n(34)\n\n\u03bb are sampled from Q\u03b8l\n\n(cid:1), and the second equality arises from the law of large numbers. where f l We can compute the Gram matrix analytically in simple cases (such as a multivariate Gaussian), but in general we can always estimate the Gram matrix using a Monte-Carlo estimate of Eq. (34).\n\n(cid:0)f l\n\n\u03bb\n\nFinally, we checked that the vDKM objective closely matched the posterior under neural networks. This is a bit more involved, as the marginal distributions over features are no longer Gaussian (Fig. 4b). To capture these non-Gaussian marginals, we used a simple normalizing flow. In particular, we first sampled zl \u03bb \u223c N (\u03bcl, \u03a3l) from a multivariate Gaussian with a learned mean, \u03bcl, and covariance, \u03a3l then we obtained features, f l \u03bb through f , a learned pointwise function parameterised as in a neural spline flow (Durkan et al., 2019). The resulting distribution is a high-dimensional Gaussian copula (e.g. Cai & Zhang, 2018). As shown in Fig. 4, vDKM with multivariate Gaussian (MvG) approximate posterior cannot match the Gram matrices learned by BNN (Fig. 4a), while vDKM with flow is able to capture the non-Gaussian marginals (Fig. 4b) and thus match the learned Gram matrices with BNN.\n\n\u03bb), by passing zl\n\n\u03bb = f (zl\n\nB GENERAL LIKELIHOODS THAT DEPEND ONLY ON GRAM MATRICES\n\nWe consider likelihoods which depend only on the top-layer Gram matrix, GL,\n\n(cid:90)\n\nP (Y|GL) =\n\ndFL+1 P (Y|FL+1) P (FL+1|GL)\n\nwhere,\n\nP (FL+1|GL) =\n\nNL+1 (cid:89)\n\n\u03bb=1\n\nN (cid:0)f L+1\n\n\u03bb\n\n; 0, K(GL)(cid:1) .\n\nThis family of likelihoods captures regression,\n\nP (cid:0)y\u03bb|f L+1\n\n\u03bb\n\n(cid:1) = N (cid:0)yL+1\n\n\u03bb\n\n; f L+1\n\n\u03bb\n\n, \u03c32I(cid:1) ,\n\n(which is equivalent to the model used in the main text Eq. 1b) and e.g. classification,\n\nP (y|F) = Categorical (y; softmax (FL+1)) ,\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\namoung many others.\n\nC WEAK CONVERGENCE\n\nHere, we give a formal argument for weak convergence of the DGP posterior over Gram matrices to a point distribution in the limit as N \u2192 \u221e,\n\nPN (G1, . . . , GL|X, \u0303Y) d\u2192\n\nL (cid:89)\n\nl=1\n\n\u03b4(Gl \u2212 G\u2217 l )\n\n(39)\n\n14\n\nwhere we have included N in the subscript of the probability distribution as a reminder that this distribution depends on the width. By the Portmanteau theorem, weak convergence is established if all expectations of bounded continuous functions, f , converge\n\nlim N\u2192\u221e\n\nEPN (G1,...,GL|X, \u0303Y) [f (G1, . . . , GL)] = f (G\u2217\n\n1, . . . , G\u2217\n\nL).\n\n(40)\n\nTo show this in a reasonably general setting (which the DGP posterior is a special case of), we consider an unnormalized probability density of the form h(g)eN L(g), and compute the moment as,\n\nE [f (g)] =\n\n(cid:82) G dg f (g)h(g)eN L(g) (cid:82) G dg h(g)eN L(g)\n\n(41)\n\nwhere g = (G1, . . . , GL) is all L positive semi-definite matrices, Gl. Thus, g \u2208 G, where G is a convex set.\n\nWe consider the superlevel set A(\u2206) = {g|L(g) \u2265 L(g\u2217) \u2212 \u2206}, where g\u2217 is the unique global optimum. We select out a small region, A(\u2206), surrounding the global maximum, and compute the integral as,\n\nE [f (g)] =\n\n(cid:82)\n\nA(\u2206) dg f (g)h(g)eN L(g) + (cid:82) A(\u2206) dg h(g)eN L(g) + (cid:82)\n\n(cid:82)\n\nG\\A(\u2206) dg f (g)h(g)eN L(g) G\\A(\u2206) dg h(g)eN L(g)\n\nAnd divide the numerator and denominator by (cid:82)\n\nA(\u2206) dg h(g)eN L(g),\n\nE [f (g)] =\n\n(cid:82)\n\nA(\u2206) dg f (g)h(g)eN L(g) (cid:82)\n\nA(\u2206) dg h(g)eN L(g) +\n\n(cid:82) G\\A(\u2206) dg f (g)h(g)eN L(g) A(\u2206) dg h(g)eN L(g)\n\n(cid:82)\n\n1 +\n\n(cid:82)\n\nG\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n(42)\n\n(43)\n\nNow, we deal with each term separately. The ratio in the denominator can be lower-bounded by zero, and upper bounded by considering a smaller superlevel set, A(\u2206/2), in the denominator,\n\n0 \u2264\n\n(cid:82)\n\nG\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n\u2264\n\n\u2264\n\n=\n\n(cid:82)\n\nG\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206/2) dg h(g)eN L(g) eN (L(g\u2217)\u2212\u2206) (cid:82) eN (L(g\u2217)\u2212\u2206/2) (cid:82) (cid:82)\n\nG\\A(\u2206) dg h(g) (cid:82) A(\u2206/2) dg h(g)\n\nG\\A(\u2206) dg h(g) A(\u2206/2) dg h(g)\n\ne\u2212N \u2206/2\n\n(44)\n\nThe upper bound converges to zero (as h(g) is independent of N ), and therefore by the sandwich theorem the ratio of interest also tends to zero.\n\nThe second ratio in the numerator can be rewritten as,\n\n(cid:82)\n\nG\\A(\u2206) dg f (g)h(g)eN L(g) A(\u2206) dg h(g)eN L(g)\n\n(cid:82)\n\n=\n\n(cid:82) G\\A(\u2206) dg f (g)h(g)eN L(g) (cid:82) G\\A(\u2206) dg h(g)eN L(g)\n\n(cid:82) G\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n(45)\n\nThe first term here is an expectation of a bounded function, f (g), so is bounded, while second term converges to zero in the limit (by the previous result).\n\nFinally, we consider the first ratio in the numerator,\n\n(cid:82) A(\u2206) dg f (g)h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n(46)\n\nwhich can be understood as an expectation over f (g) in the region A(\u2206). As f is continuous, for any \u03b5 > 0, we can find a \u03b4 > 0 such that for all g with |g\u2217 \u2212 g| < \u03b4, we have\n\nf (g\u2217) \u2212 \u03b5 < f (g) < f (g\u2217) + \u03b5.\n\n(47)\n\n15\n\nFurther, because the continuous function, L(g), has a unique global optimum, g\u2217, for every \u03b4 > 0 we are always able to find a \u2206 > 0 such that all points g \u2208 A(\u2206) are within \u03b4 of g\u2217 i.e. |g\u2217 \u2212 g| < \u03b4. Thus combining the previous two facts, given an \u03b5, we are always able to find a \u03b4 such that Eq. 47 holds for all g with |g\u2217 \u2212 g| < \u03b4, and given a \u03b4 we are always able to find a \u2206 such that all g \u2208 A(\u2206) have |g\u2217 \u2212 g| < \u03b4. Hence for every \u03b5 > 0 we can find a \u2206 > 0 such that Eq. 47 holds for all g \u2208 A(\u2206). Choosing the appropriate \u03b5-dependent \u2206 and substituting Eq. 47 into Eq. 46, \u03b5 also bounds the error in the expectation,\n\nf (g\u2217) \u2212 \u03b5 <\n\n(cid:82)\n\nA(\u2206) dg f (g)h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n< f (g\u2217) + \u03b5.\n\n(48)\n\nNow, we use the results in Eq. (44), Eq. (45) and Eq. (48) to take the limit of Eq. (43) (we can compose these limits by the algebraic limit theorem as all the individual limits exist and are finite),\n\nf (g\u2217) \u2212 \u03b5 < lim\n\nN\u2192\u221e\n\nE [f (g)] < f (g\u2217) + \u03b5.\n\nAnd as this holds for any \u03b5, we have,\n\nf (g\u2217) = lim\n\nN\u2192\u221e\n\nE [f (g)] .\n\n(49)\n\n(50)\n\nThis result is applicable to the DGP posterior over Gram matrices, as that posterior can be written as,\n\nPN (G1, . . . , GL|X, \u0303Y) \u221d h(g)eN L(g),\n\nwhere L(g) is the usual DKM objective,\n\nL(g) = L(G1, . . . , GL)\n\nand h(g) is the remaining terms in the log-posterior which do not depend on N ,\n\n(cid:32)\n\nh(g) = exp\n\n\u2212 P +1 2\n\n(cid:33)\n\nlog |Gl|\n\n(cid:88)\n\nl\n\n(this requires P \u2264 N so that Gl is full-rank).\n\n(51)\n\n(52)\n\n(53)\n\nD GENERAL MODELS IN THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nOverall, our goal is to compute the integral in Eq. (6) in the limit as N \u2192 \u221e. While the integral is intractable for general models such as BNNs, we can use variational inference to reason about its properties. In particular, we can bound the integral using the ELBO,\n\nlog P (Gl|Gl\u22121) \u2265 ELBOl = EQ(Fl) [log P (Gl|Fl) + log P (Fl|Gl\u22121) \u2212 log Q (Fl)] . (cid:0)f l\n\n(54) (cid:1) in the main text, both because the approximate Note that Q (Fl) here is different from Q\u03b8l posterior here, Q (Fl) is over all features jointly, Fl, whereas the approximate posterior in the main text is only over a single feature, f l \u03bb, and because in the main text, we chose a specific family of distribution with parameters \u03b8l, while here we leave the approximate posterior, Q (Fl) completely unconstrained, so that it has the flexibility to capture the true posterior. Indeed, if the optimal approximate posterior is equal to the true posterior, Q\u2217 (Fl) = P (Fl|Gl\u22121, Gl), then the bound is tight, so we get log P (Gl|Gl\u22121) = ELBO\u2217 l . Our overall strategy is thus to use variational inference to characterise the optimal approximate which is equal to the true posterior Q\u2217 (Fl) = P (Fl|Gl\u22121, Gl) and use the corresponding ELBO to obtain log P (Gl|Gl\u22121).\n\n\u03bb\n\nD.1 CHARACTERISING EXACT BNN POSTERIORS\n\nRemember that if the approximate posterior family, Q (Fl) is flexible enough to capture the true posterior P (Fl|Gl\u22121, Gl), then the Q\u2217 (Fl) that optimizes the ELBO is indeed the true posterior, the bound is tight, so the ELBO is equal to log P (Gl|Gl\u22121) (Jordan et al., 1999; Blei et al., 2017). Thus, we are careful to ensure that our approximate posterior family captures the true posterior, by ensuring that we only impose constraints on Q (Fl) that must hold for the true posterior,\n\n16\n\nP (Fl|Gl\u22121, Gl). In particular, note that P (Gl|Fl) in Eq. (5b) constrains the true posterior to give non-zero mass only to Fl that satisfy Gl = 1 \u03c6(Fl)\u03c6T (Fl). However, this constraint is difficult Nl to handle. We therefore consider an alternative, weaker constraint on expectations, which holds for the true posterior (the first equality below) because Eq. (5b) constrains Gl = 1 \u03c6(Fl)\u03c6T (Fl), and Nl impose the same constraint on the approximate posterior,\n\nGl = EP(Fl|Gl,Gl\u22121)\n\n(cid:104) 1 Nl\n\n(cid:105) \u03c6(Fl)\u03c6T (Fl)\n\n= EQ(Fl)\n\n(cid:104) 1 Nl\n\n(cid:105) \u03c6(Fl)\u03c6T (Fl)\n\n.\n\n(55)\n\nNow, we can solve for the optimal Q (Fl) with this constraint on the expectation. In particular, the Lagrangian is obtained by taking the ELBO (Eq. 54), dropping the log P (Gl|Fl) term representing the equality constraint (that Gl = 1 \u03c6(Fl)\u03c6T (Fl)) and including Lagrange multipliers for the Nl expectation constraint, \u039b, (Eq. 55) and the constraint that the distribution must normalize to 1, \u039b,\n\n(cid:90)\n\nL =\n\ndFl Q (Fl) (log P (Fl|Gl\u22121) \u2212 log Q (Fl))\n\n(cid:18)\n\n(cid:18)\n\n\u039b\n\nGl \u2212\n\n(cid:90)\n\n+ 1\n\n2 Tr\n\ndFl Q (Fl) \u03c6(Fl)\u03c6T (Fl)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n+ \u039b\n\n1 \u2212\n\n(cid:90)\n\n(cid:19)\n\ndFl Q (Fl)\n\n(56)\n\nDifferentiating wrt Q (Fl), and solving for the optimal approximate posterior, Q\u2217 (Fl),\n\n0 =\n\n\u2202L \u2202 Q (Fl)\n\n(cid:12) (cid:12) (cid:12) (cid:12)Q\u2217(Fl)\n\n0 = (log P (Fl|Gl\u22121) \u2212 log Q\u2217 (Fl)) \u2212 1 \u2212 1\n\n2 Tr (cid:0)\u039b\u03c6(Fl)\u03c6T (Fl)(cid:1) \u2212 \u039b\n\nSolving for log Q\u2217 (Fl),\n\nlog Q\u2217 (Fl) = log P (Fl|Gl\u22121) \u2212 1\n\n2 Tr (cid:0)\u039b\u03c6(Fl)\u03c6T (Fl)(cid:1) + const .\n\nUsing the cyclic property of the trace,\n\nlog Q\u2217 (Fl) = log P (Fl|Gl\u22121) \u2212 1\n\n2 Tr (cid:0)\u03c6T (Fl)\u039b\u03c6(Fl)(cid:1) + const .\n\nThus, log Q (Fl) can be written as a sum over features,\n\nlog Q\u2217 (Fl) =\n\nNl(cid:88)\n\n\u03bb=1\n\n(cid:2)log P (cid:0)f l\n\n\u03bb|Gl\u22121\n\n(cid:1) \u2212 1\n\n2 \u03c6T (f l\n\n\u03bb)\u039b\u03c6(f l\n\n\u03bb)(cid:3) + const = (cid:80)NL\n\n\u03bb=1 log Q (cid:0)f l\n\n\u03bb\n\nso, the optimal approximate posterior is IID over features,\n\nQ\u2217 (Fl) = (cid:81)Nl\n\n\u03bb=1 Q\u2217 (cid:0)f l\n\n\u03bb\n\n(cid:1) .\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\n(cid:1)\n\n(61)\n\n(62)\n\nRemember that this approximate posterior was only constrained in expectation, and that this constraint held for the true posterior (Eq. 55). Thus, we might think that this optimal approximate posterior would be equal to the true posterior. However, remember that the true posterior had a tighter equality constraint, that Gl = 1 \u03c6(Fl)\u03c6T (Fl), while so far we have only imposed a weaker conNl straint in expectation (Eq. 55). We thus need to check that our optimal approximate posterior does indeed satisfy the equality constraint in the limit as Nl \u2192 \u221e. This be shown using the law of large numbers, as f l \u03bb are IID under the optimal approximate posterior, and by using Eq. (55) for the final equality,\n\nlim Nl\u2192\u221e\n\n1 Nl\n\n\u03c6(Fl)\u03c6T (Fl) = lim\n\nNl\u2192\u221e\n\n1 Nl\n\nNl(cid:88)\n\n\u03bb=1\n\n\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb) = E\n\nQ(f l\n\n\u03bb)\n\n(cid:2)\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb)(cid:3) = Gl.\n\n(63)\n\nThus, the optimal approximate posterior does meet the constraint in the limit as Nl \u2192 \u221e, so in that limit, the true posterior, like the optimal approximate posterior is IID across features,\n\nP (Fl|Gl\u22121, Gl) = Q\u2217 (Fl) = (cid:81)Nl\n\n\u03bb=1 Q\u2217 (cid:0)f l\n\n\u03bb\n\n(cid:1) = (cid:81)Nl\n\nl=1 P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1) .\n\n(64)\n\n17\n\nD.2 EXACTLY MULTIVARIATE GAUSSIAN DGP POSTERIORS\n\nFor DGPs, we have \u03c6(f l\n\n\u03bb) = f l \u03bb, so the optimal approximate posterior is Gaussian, (cid:0)f l\n\n(cid:1) = log PDGP\n\n(cid:1) \u2212 1\n\n\u03bb + const\n\n\u03bb)T \u039bf l\n\n\u03bb|Gl\u22121\n\n(cid:0)f l\n\nDGP\n\n2 (f l\n\n\u03bb\n\nlog Q\u2217\n\n= \u2212 1\n\n2 (f l\n\n= log N\n\n\u03bb)T (cid:0)\u039b + K\u22121(Gl\u22121)(cid:1) f l (cid:16)\n\n\u03bb; 0, (cid:0)\u039b + K\u22121(Gl\u22121)(cid:1)\u22121(cid:17) f l\n\n\u03bb + const\n\n.\n\n(65)\n\n(66)\n\n(67)\n\nAs the approximate posterior and true posterior are IID, the constraint in Eq. (55) becomes,\n\nGl = E\n\nPDGP(f l\n\n\u03bb|Gl,Gl\u22121)\n\n(cid:2)f l\n\n\u03bb(f l\n\n\u03bb)T (cid:3) = E\n\nQ\u2217\n\nDGP(f l\n\n\u03bb)\n\n(cid:2)f l\n\n\u03bb(f l\n\n\u03bb)T (cid:3) = (cid:0)\u039b + K\u22121(Gl\u22121)(cid:1)\u22121\n\n.\n\n(68)\n\nAs the Lagrange multipliers are unconstrained, we can always set them such that this constraint holds. In that case both the optimal approximate posterior and the true posterior become,\n\nPDGP\n\n(cid:0)f l\n\n\u03bb|Gl\u22121Gl\n\n(cid:1) = Q\u2217\n\nDGP\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1) = N (cid:0)f l\n\n\u03bb; 0, Gl\n\n(cid:1) ,\n\n(69)\n\nas required.\n\nD.3 GENERAL FORM FOR THE CONDITIONAL DISTRIBUTION OVER GRAM MATRICES\n\nNow that we have shown that the true posterior, P (Fl|Gl\u22121, Gl) factorises, we can obtain a simple form for log P (Gl|Gl\u22121). In particular, log P (Gl|Gl\u22121) is equal to the ELBO if we use the true posterior in place of the approximate posterior,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = lim\n\nNl\u2192\u221e\n\n1 N\n\nEP(Fl|Gl\u22121,Gl)\n\n(cid:20)\n\nlog P (Gl|Fl) + log\n\nP (Fl|Gl\u22121) P (Fl|Gl\u22121, Gl)\n\n(cid:21)\n\n.\n\n(70)\n\nUnder the posterior, the constraint represented by log P (Gl|Fl) is satisfied, so in the limit we can include that term in a constant,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = lim\n\nNl\u2192\u221e\n\n1 N\n\nEP(Fl|Gl\u22121,Gl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl\u22121) P (Fl|Gl\u22121, Gl)\n\n(cid:21)\n\n+ const .\n\n(71)\n\nNow, we use the fact that the prior, P (Fl|Gl\u22121) and posterior, P (Fl|Gl\u22121, Gl), are IID across features,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = \u03bdl E\n\nP(f l\n\n\u03bb|Gl\u22121,Gl)\n\n(cid:34)\n\nlog\n\nP (cid:0)f l\n\n(cid:1)\n\n\u03bb|Gl\u22121 \u03bb|Gl\u22121, Gl\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:1)\n\n+ const\n\n(72)\n\nand this expectation is a KL-divergence,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = \u2212\u03bdl DKL\n\n(cid:0)P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1)(cid:13) (cid:13)P (cid:0)f l\n\n\u03bb|Gl\u22121\n\n(cid:1)(cid:1) + const,\n\n(73)\n\nwhich gives Eq. (32) when we combine with Eq. (8).\n\nD.4 PARAMETRIC APPROXIMATE POSTERIORS\n\nEq. (64) represents a considerable simplification, as we now need to consider only a single feature, f l \u03bb, rather than the joint distribution over all features, Fl. However, in the general case, it is still not possible to compute Eq. (64) because the true posterior over a single feature is still not tractable. Following the true posteriors derived in the previous section, we could chose a parametric approximate posterior that factorises across features,\n\nQ\u03b8 (F1, . . . , FL) = (cid:81)L\n\nl=1\n\n(cid:81)Nl\n\n\u03bb=1 Q\u03b8l\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1) .\n\n(74)\n\nRemember that we optimize the approximate posterior parameters, \u03b8, directly, and set the Gram matrices as a function of \u03b8 (Eq. 34). As before, we can bound, log P (Gl=G\u03b8(\u03b8l)|Gl\u22121) using the\n\n18\n\nELBO, and the bound is tight when the approximate posterior equals the true posterior,\n\nlog P (Gl = G\u03b8(\u03b8l)|Gl\u22121)\n\n(cid:34)\n\n= E\n\nP(Fl|Gl\u22121,Gl=G\u03b8(\u03b8\u03b8))\n\nlog P (Gl=G\u03b8(\u03b8l)|Fl) + log\n\nP (cid:0)Fl\n\n\u03bb|Gl\u22121\n\n(cid:1)\n\nP (Fl|Gl\u22121, Gl=G\u03b8(\u03b8l))\n\n(cid:34)\n\n\u2265 EQ\u03b8(Fl)\n\nlog P (Gl=G\u03b8(\u03b8l)|Fl) + log\n\nP (cid:0)Fl Q\u03b8l\n\n\u03bb|Gl\u22121 (Fl)\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(cid:35)\n\n(75)\n\n(76)\n\n(77)\n\nNow, we can cancel the log P (Gl = G\u03b8(\u03b8l)|Fl) terms, as they represent a constraint that holds both under the true posterior, and under the approximate posterior,\n\nE\n\nP(Fl|Gl\u22121,Gl=G\u03b8(\u03b8l)))\n\n(cid:20)\n\nlog\n\nP (Fl|Gl\u22121) P (Fl|Gl\u22121, Gl=G\u03b8(\u03b8l))\n\n(cid:21)\n\n\u2265 EQ\u03b8l\n\n(Fl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl\u22121)\n\nQ\u03b8l\n\n(Fl)\n\n(cid:21)\n\n.\n\n(78)\n\nUsing the fact that the prior, posterior and approximate posterior are all IID over features, we can write this inequality in terms of distributions over a single feature, f l\n\n\u03bb and divide by Nl,\n\nE\n\nP(f l\n\n\u03bb|Gl\u22121,Gl=G\u03b8(\u03b8l))\n\n(cid:34)\n\nlog\n\n(cid:1)\n\nP (cid:0)f l\n\n\u03bb|Gl\u22121 \u03bb|Gl\u22121, Gl=G\u03b8(\u03b8l)(cid:1)\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:34)\n\nlog\n\n\u2265 E\n\nQ\u03b8l(f l\n\n\u03bb)\n\nP (cid:0)f l\n\n\u03bb|Gl\u22121(\u03b8)(cid:1) (cid:0)f l Q\u03b8l\n\n(cid:1)\n\n\u03bb\n\n(cid:35)\n\n.\n\n(79)\n\nNoting that both sides of this inequality are negative KL-divergences, we obtain, (cid:1)(cid:13) (cid:13)P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl=G\u03b8(\u03b8l)(cid:1)(cid:13)\n\n(cid:1)(cid:1) \u2265 \u2212 DKL\n\n(cid:13)P (cid:0)f l\n\n(cid:0)P (cid:0)f l\n\n\u03bb|Gl\u22121\n\n\u2212 DKL\n\n(cid:0)f l\n\n(cid:0)Q\u03b8l\n\n\u03bb\n\n\u03bb|Gl\u22121\n\n(cid:1)(cid:1) ,\n\n(80)\n\nwhich gives Eq. (33) in the main text.\n\nE THEORETICAL SIMILARITIES IN REPRESENTATION LEARNING IN FINITE\n\nAND INFINITE NETWORKS\n\nIn the main text, we considered probability densities of the Gram matrices, G1, . . . , GL. However, we can also consider probability densities of the features, F1, . . . , FL, for a DGP, 2 log |K (GDGP (Fl\u22121))| \u2212 1\n\nl K\u22121 (GDGP (Fl\u22121)) Fl\n\nlog P (Fl|Fl\u22121) = \u2212 Nl\n\n(cid:1) + const .\n\n2 tr (cid:0)FT\n\n(81)\n\nWe can rewrite the density such that it is still the density of features, Fl, but it is expressed in terms of the DGP Gram matrix, log P (Fl|Fl\u22121) = \u2212 Nl\n\n2 tr (cid:0)K\u22121(Gl\u22121)Gl\n\n2 log |K(Gl\u22121)| \u2212 Nl\n\n(cid:1) + const .\n\n(82)\n\nHere, we have used the cyclic property of the trace to combine the Fl and FT l to form Gl, and we have used the fact that our kernels can be written as a function of the Gram matrix. Overall, we can therefore write the posterior over features, P (F1, . . . , FL|X, \u0303Y), in terms of only Gram matrices,\n\nJ (G1, . . . , GL) = 1\n\nN log P (F1, . . . , FL|X, \u0303Y) = log P (Y|GL) + 1\n\nN\n\nL (cid:88)\n\nl=1\n\nlog P (Fl|Fl\u22121) ,\n\n(83)\n\nsubstituting Eq. (82),\n\nJ (G1, . . . , GL) = log P (Y|GL) \u2212 1\n\n2\n\n(cid:80)L\n\nl=1\u03bdl\n\n(cid:0)log |K(Gl\u22121)| + tr (cid:0)K\u22121(Gl\u22121)Gl\n\n(cid:1)(cid:1)\n\n+ const .\n\n(84)\n\nThus, J (G1, . . . , GL) does not depend on N , and thus the Gram matrices that maximize J (G1, . . . , GL) are the same for any choice of N . The only restriction is that we need Nl \u2265 P , to ensure that the Gram matrices are full-rank.\n\nTo confirm these results, we used Adam with a learning rate of 10\u22123 to optimize full-rank Gram matrices with Eq. (84) and to directly do MAP inference over features using Eq. (81). As expected, as the number of features increased, the Gram matrix from MAP inference over features converged rapidly to that expected using Eq. (84) (Fig. 5).\n\n19\n\nFigure 5: RMSE of trained Gram matrices between one-hidden-layer (first row) and two-hiddenlayer (second row) DGPs of various width trained by gradient descent and the corresponding MAP limit. Columns correspond to different datasets (trained on a subset of 50 datapoints).\n\nF ADDITIONAL EXPERIMENTAL DETAILS\n\nTo optimize the analytic DKM objective for DGPs and the variational DKM objective for DGPs (Figs. 3\u201311), we parameterised the Gram matrices (or covariances for the variational approximate posterior) as the product of a square matrix, Rl \u2208 RP \u00d7P , with itself transposed, Gl = 1 P RlRT l , and we used Adam with a learning rate of 10\u22123 to learn Rl. To do Bayesian inference in finite BNNs and DGPs, we used Langevin sampling with 10 parallel chains, and a step size of 10\u22123. Note that in certain senarios, Langevin sampling can be very slow, as the features have a Gaussian prior with covariance K(Gl\u22121) which has some very small and some larger eigenvalues, which makes sampling difficult. Instead, we reparameterised the model in terms of the standard Gaussian random variables, Vl \u2208 RP \u00d7Nl. We then wrote Fl in terms of Vl,\n\nFl = Ll\u22121Vl.\n\n(85)\n\nHere, Ll\u22121 is the Cholesky of K(Gl\u22121), so K(Gl\u22121) = Ll\u22121LT l\u22121. This gives an equivalent distribution P (Fl|Fl\u22121). Importantly, as the prior on Vl is IID standard Gaussian, sampling Vl is much faster. To ensure that the computational cost of these expensive simulations remained reasonable, we used a subset of 50 datapoints from each dataset.\n\nFor the DKM objective for BNNs, we used Monte-Carlo to approximate the Gram matrices,\n\nG\u03b8(\u03b8l) \u2248\n\nK (cid:88)\n\nk=1\n\n\u03c6(f l\n\nk)\u03c6T (f l\n\nk).\n\n(86)\n\nwith f l k drawn from the appropriate approximate posterior, and K = 216. We can use the reparameterisation trick (Kingma & Welling, 2013; Rezende et al., 2014) to differentiate through these Monte-Carlo estimates.\n\nG ADDITIONAL COMPARISONS WITH FINITE-WIDTH DGPS\n\nIn particular, we Here, we give additional results supporting those in Sec. 4.6, Fig. 3\u2013Fig. 11. give the DGP representations learned by two-layer networks on all UCI datasets (boston, concrete, energy, yacht), except those already given in the main text Fig. 6\u20138.\n\n20\n\n0.000.050.100.150.20RMSEbostonyachtconcreteone-layerenergy2123252729211width0.000.050.100.150.20RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layerFigure 6: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of energy. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\nFigure 7: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of boston. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n21\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101init DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 8: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of concrete. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n22\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101H THE FLOW POSTERIOR IN A 2-LAYER BNN\n\nHere, we give the 2-layer version (Fig. 9) of Fig. 4 in the main text, which again shows a close match between the variational DKM with a flow posterior, and the BNN true posterior.\n\nFigure 9: Two-layer ReLU BNN and variational DKM with flow. a Initialized (first row) and learned Gram matrices of a width 1024 BNN (second row), vDKM with flow (third row) and vDKM with multivariate Gaussian (fourth row) using 214 Monte-Carlo samples. The Gram matrices between BNN and vDKM (flow) match closely after training. (MvG). b Marginal PDF over features at each layer for one input datapoint using kernel density estimation. The marginal PDFs of BNN are nonGaussian (blue curves), vDKM with flow is able to capture the non-Gaussianity and match closely with BNNs marginals (red curves).\n\n23\n\ninitG1G2vDKM (flow)150indexvDKM (MvG)150index505feature0.00.5density505feature150index150indexG0150indexG30.750.000.75abBNNvDKM (flow)BNNI MULTIVARIATE GAUSSIAN APPROXIMATE POSTERIORS IN DEEPER\n\nNETWORKS\n\nIn particular, we hypothesised that depth is an important factor.\n\nThere is a body of theoretical work (e.g. (Seroussi & Ringel, 2021)), on BNNs that approximates BNN posteriors over features as Gaussian. While we have shown that this is a bad idea in general (Fig. 4 and 9), we can nonetheless ask whether there are circumstances where the idea might work well. In particular, in shallow networks, in order to get GL close to the required representation, we may need the posterior over Fl to be quite different from the prior. In contrast, in deeper networks, we might expect the posterior over Fl to be closer to its (Gaussian) prior, and therefore we might Gaussian approximate posteriors to work better.\n\nHowever, we cannot just make the network deeper, because as we do so, we apply the nonlinearity more times and dramatically alter the network\u2019s inductive biases. To resolve this issue, we derive a leaky relu nonlinearity that allows (approximately) independent control over the inductive biases (or effective depth) and the actual depth (Appendix I.1). Using these nonlinearities, we indeed find that very deep networks are reasonably well approximated by multivariate Gaussian approximate posteriors (Appendix I.2).\n\nI.1 LEAKY RELU NONLINEARITIES\n\nOur goal is to find a pointwise nonlinearity, \u03c6, such that (under the prior), \u03bb)(cid:3) = p E\n\n\u03bb)reluT (f l\n\n\u03bb)\u03c6T (f l\n\n(cid:2)relu(f l\n\n(cid:2)\u03c6(f l\n\nE\n\nPDGP(f l\n\n\u03bb|Gl\u22121)\n\nP(f l\n\n\u03bb|Gl\u22121)\n\n\u03bb)(cid:3) + (1 \u2212 p)Gl\u22121.\n\n(87)\n\nWe will set p = \u03b1/L, where \u03b1 is the \u201ceffective\u201d depth of the network and L is the real depth. These networks are designed such that their inductive biases in the infinite width limit are similar to a standard relu network with depth \u03b1. Indeed, we would take this approach if we wanted a well-defined infinite-depth DKM limit.\n\nWithout loss of generality, we consider a 2D case, where x and y are zero-mean bivariate Gaussian,\n\n\u03c0(x, y) = N\n\n(cid:19)\n\n(cid:18)(cid:18)x y\n\n; 0,\n\n(cid:18)\u03a3xx \u03a3xy \u03a3xy \u03a3yy\n\n(cid:19)(cid:19)\n\n(88)\n\nwhere \u03c0(x, y) is the probability density for the joint distribution. Note that we use a scaled relu, (cid:26)\u221a\n\nrelu(x) =\n\n0\n\n2 x for 0 < x otherwise\n\nsuch that E (cid:2)relu2(x)(cid:3) = \u03a3xx. Mirroring Eq. 87, we want the nonlinearity, \u03c6, to satisfy,\n\nE (cid:2)\u03c6(x2)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 \u2212 p)\u03a3xx = \u03a3xx E (cid:2)\u03c6(y2)(cid:3) = p E (cid:2)relu2(y)(cid:3) + (1 \u2212 p)\u03a3yy = \u03a3yy\n\nE [\u03c6(x)\u03c6(y)] = p E [relu(x)relu(y)] + (1 \u2212 p)\u03a3xy\n\nWe hypothesise that this nonlinearity has the form,\n\n\u03c6(x) = a relu(x) + bx.\n\nWe will write the relu as a sum of x and |x|,\n\nrelu(x) = 1\u221a\n\n2\n\n(x + |x|),\n\nbecause E [f (x, y)] = 0 for f (x, y) = x|y| or f (x, y) = |x|y. It turns out that we get zero expectation for all functions where f (\u2212x, \u2212y) = \u2212f (x, y), which holds for the two choices above. To show such functions have a zero expectation, we write out the integral explicitly,\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n\u2212\u221e\n\ndy \u03c0(x, y)f (x, y).\n\n(93)\n\n24\n\n(89)\n\n(90a)\n\n(90b)\n\n(90c)\n\n(91)\n\n(92)\n\nWe split the domain of integration for y at zero,\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) 0\n\ndx\n\n\u2212\u221e\n\n\u2212\u221e\n\ndy \u03c0(x, y)f (x, y) +\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy \u03c0(x, y)f (x, y).\n\n(94)\n\nWe substitute y\u2032 = \u2212y and x\u2032 = \u2212x in the first integral, (cid:90) \u221e\n\n(cid:90) \u221e\n\nE [f (x, y)] =\n\ndx\u2032\n\ndy\u2032 \u03c0(\u2212x\u2032, \u2212y\u2032)f (\u2212x\u2032, \u2212y\u2032) +\n\n\u2212\u221e\n\n0\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy \u03c0(x, y)f (x, y).\n\n(95)\n\nAs the variables we integrate over are arbitrary we can relabel y\u2032 as y and x\u2032 as x, and we can then merge the integrals as their limits are the same,\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy [\u03c0(\u2212x, \u2212y)f (\u2212x, \u2212y) + \u03c0(x, y)f (x, y)] .\n\nUnder a zero-mean Gaussian, \u03c0(\u2212x, \u2212y) = \u03c0(x, y),\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy \u03c0(x, y) (f (\u2212x, \u2212y) + f (x, y)) .\n\n(96)\n\n(97)\n\nThus, if f (\u2212x, \u2212y) = \u2212f (x, y), then the expectation of that function under a bivariate zero-mean Gaussian distribution is zero.\n\nRemember that our overall goal was to design a nonlinearity, \u03c6, (Eq. 91) which satisfied Eq. (90). We therefore compute the expectation,\n\nE [\u03c6(x)\u03c6(y)] = E [(a relu(x) + bx) (a relu(y) + by)]\n\n= E\n\n(cid:104)(cid:16) a\u221a\n\n2\n\n(x + |x|) + bx\n\n(cid:17) (cid:16) a\u221a\n\n(y + |y|) + by\n\n(cid:17)(cid:105)\n\n2\n\nUsing the fact that E [ x|y| ] = E [ |x|y ] = 0 under a multivariate Gaussian,\n\n(cid:104)\n\n= E\n\na2 1\u221a\n\n2\n\n(x + |x|) 1\u221a 2\n\n= a2 E [relu(x)relu(y)] +\n\n(y + |y|) + (cid:16)\u221a\n\n2ab + b2(cid:17)\n\n(cid:16)\u221a\n\n2ab + b2(cid:17)\n\nxy\n\n(cid:105)\n\nE [xy] .\n\n\u221a\n\np.\n\na =\n\nThus, we can find the value of a by comparing with Eq. (90c),\n\np = a2\n\nFor b, things are a bit more involved,\n\n\u221a\n\n1 \u2212 p =\n\n2ab + b2 = (cid:112)2p b + b2\n\n(98)\n\n(99)\n\n(100)\n\n(101)\n\n(102)\n\n(103)\n\nwhere we substitute for the value of a. This can be rearranged to form a quadratic equation in b,\n\n0 = b2 + (cid:112)2p b + (p \u2212 1),\n\nwhich can be solved,\n\nb = 1 2\n\nb = 1 2\n\n(cid:16)\n\n(cid:17) \u2212(cid:112)2p \u00b1 (cid:112)2p \u2212 4(p \u2212 1)\n\n(cid:16)\n\n\u2212(cid:112)2p \u00b1 (cid:112)4 \u2212 2p\n\n(cid:17)\n\nb = \u2212\n\n(cid:113) p\n\n2 \u00b1\n\n(cid:113)\n\n1 \u2212 p\n\n2\n\nOnly the positive root is of interest,\n\nb =\n\nThus, the nonlinearity is,\n\n(cid:113)\n\n1 \u2212 p\n\n2 \u2212\n\n(cid:113) p\n\n2\n\n\u221a\n\n\u03c6(x) =\n\np relu(x) +\n\n(cid:16)(cid:113)\n\n1 \u2212 p\n\n2 \u2212\n\n(cid:113) p\n\n(cid:17)\n\n2\n\nx\n\n25\n\n(104)\n\n(105)\n\n(106)\n\n(107)\n\n(108)\n\n(109)\n\nFigure 10: Comparison of posterior feature marginal distributions between a BNN of width 1024 (trained by Langevin sampling over features) and a variational DKM with 216 Monte-Carlo samples, in a 4-layer (row 1) and a 32-layer (row 2) network. We give the BNN posterior features from Langevin sampling (blue histogarm) and the best fitting Gaussian (blue line), and compare against the variational DKM approximate posterior Gaussian distribution (red line).\n\nwhere we set p = \u03b1/L, and remember we used the scaled relu in Eq. (89). Finally, we established these choices by considering only the cross term, E [\u03c6(x)\u03c6(y)]. We also need to check that the E (cid:2)\u03c62(x)(cid:3) and E (cid:2)\u03c62(y)(cid:3) terms are as required (Eq. 90a and Eq. 90b). In particular,\n\nE (cid:2)\u03c62(x)(cid:3) = E\n\n(cid:104)\n\n(a relu(x) + bx)2(cid:105)\n\n= E\n\n(cid:20)(cid:16) a\u221a\n\n2\n\n(x + |x|) + bx\n\n(cid:17)2(cid:21)\n\n(110)\n\nusing E [x|x|] = 0 as x|x| is an odd function of x, and the zero-mean Gaussian is an even distribution,\n\nE (cid:2)\u03c62(x)(cid:3) = a2 E (cid:2)relu2(x)(cid:3) +\n\n(cid:16)\u221a\n\n2ab + b2(cid:17)\n\n\u03a3xx\n\nusing Eq. (102) to identify a2 and Eq. (103) to identify\n\n\u221a\n\n2ab + b2,\n\nE (cid:2)\u03c62(x)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 \u2212 p)\u03a3xx,\n\nas required.\n\nI.2 MULTIVARIATE GAUSSIAN IN DEEPER NETWORKS\n\n(111)\n\n(112)\n\nIn the main text, we show that a more complex approximate posterior can match the distributions in these networks. Here, we consider an alternative approach. In particular, we hypothesise that these distributions are strongly non-Gaussian because the networks are shallow, meaning that the posterior needs to be far from the prior in order to get a top-layer kernel close to GL+1. We could therefore make the posteriors closer to Gaussian by using leaky-relu nonlinearities (Appendix I.1) with fixed effective depth (\u03b1 = 2), but increasing real depth, L. In particular, we use multivariate Gaussian approximate posteriors with learned means,\n\nQ\u03b8l\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1) = N (cid:0)f l\n\n\u03bb; \u03bcl, \u03a3l\n\n(cid:1)\n\nso\n\n\u03b8l = (\u03bcl, \u03a3l).\n\n(113)\n\nAs expected, for a depth 32 network, we have much more similar marginals (Fig. 10 top) and learned representations (Fig. 11 top).\n\n26\n\n50501density=1505=2505=35054 layers=4505feature01density=8505feature=16505feature=24505feature32 layers=32BNN vDKMFigure 11: Comparison of Gram matrices between BNN of width 1024 (trained by Langevin sampling over features) and variational DKM, in 4-layer (row 1-3) and 32-layer networks (row 4-6). Initializations are shown in row 1 and 4, trained BNN Gram matrices are shown in row 2 and 5, and trained variational DKM Gram matrices are shown in row 3 and 6. As in Figure 10, the variational DKM is a poor match to Langevin sampling in a BNN for a 4-layer network, but is very similar in a 32 layer network.\n\n27\n\nG1G2G3G4G8G16G24G32150index150index150index150index150index150indexG0150indexGL+1101 32 layers BNNinitvDKMvDKM4 layers BNNinitJ UNIMODALITY IN LINEAR DEEP KERNEL MACHINES\n\nJ.1 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND SAME WIDTHS\n\nHere, we show that the deep kernel machine objective is unimodal for a linear kernel. A linear kernel simply returns the input Gram matrix,\n\nK (G) = G.\n\n(114)\n\nIt is called a linear kernel, because it arises in the neural network setting (Eq. 21) by choosing the nonlinearity, \u03c6 to be the identity, in which case, Fl = Fl\u22121Wl\u22121. For a linear kernel the objective becomes,\n\nL(G1, ..., GL) = (cid:80)L+1\n\nl=1\n\n\u03bdl 2\n\n(cid:0)log (cid:12)\n\n(cid:12)G\u22121\n\nl\u22121Gl\n\n(cid:12) (cid:12) \u2212 Tr (cid:0)G\u22121\n\nl\u22121Gl\n\n(cid:1)(cid:1)\n\n(115)\n\nwhere we have assumed there is no output noise, \u03c32 = 0. Taking all \u03bdl to be equal, \u03bd = \u03bdl (see Appendix J.2 for the general case),\n\nL(G1, ..., GL) = log (cid:12)\n\n(cid:12)G\u22121\n\n0 GL+1\n\n(cid:12) (cid:12) \u2212 \u03bd\n\n2\n\n(cid:80)L+1\n\nl=1 Tr (cid:0)G\u22121\n\nl\u22121Gl\n\n(cid:1) .\n\n(116)\n\nNote that G0 and GL+1 are fixed by the inputs and outputs respectively. Thus, to find the mode, we set the gradient wrt G1, . . . , GL to zero,\n\n0 =\n\n\u2202L \u2202Gl\n\n= \u03bd 2\n\n(cid:0)G\u22121\n\nl\u22121 \u2212 G\u22121\n\nl Gl+1G\u22121\n\nl\n\nThus, at the mode, the recursive relationship must hold,\n\nT = G\u22121\n\nl\u22121Gl = G\u22121\n\nl Gl+1.\n\nThus, optimal Gram matrices are given by,\n\nand we can solve for T by noting,\n\nGl = G0Tl,\n\nG\u22121\n\n0 GL+1 = TL+1.\n\n(cid:1)\n\n(117)\n\n(118)\n\n(119)\n\n(120)\n\nImportantly, T is the product of two positive definite matrices, T = G\u22121 l\u22121Gl, so T must have positive, real eigenvalues (but T does not have to be symmetric (Horn & Johnson, 2012)). There is only one solution to Eq. (120) with positive real eigenvalues (Horn et al., 1994). Intuitively, this can be seen using the eigendecomposition, G\u22121\n\n0 GL+1 = V\u22121DV, where D is diagonal,\n\nT = (cid:0)V\u22121DV(cid:1)1/(L+1)\n\n= V\u22121D1/(L+1)V.\n\n(121)\n\nThus, finding T reduces to finding the (L + 1)th root of each positive real number on the diagonal of D. While there are (L + 1) complex roots, there is only one positive real root, and so T and hence G1, . . . , GL are uniquely specified. This contrasts with a deep linear neural network, which has infinitely many optimal settings for the weights.\n\nNote that for the objective to be well-defined, we need K(G) to be full-rank. With standard kernels (such as the squared exponential) this is always the case, even if the input Gram matrix is singular. However, a linear kernel will have a singular output if given a singular input, and with enough data points, G0 = 1 XXT ) to be \u03bd0 given by applying a positive definite kernel (such as a squared exponential) to 1 XXT . This results \u03bd0 in positive definite G0, as long as the input points are distinct.\n\nXXT is always singular. To fix this, we could e.g. define G0 = K( 1 \u03bd0\n\nJ.2 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND ARBITRARY WIDTHS\n\nIn the main text we showed that the deep kernel machine is unimodal when all \u03bdl are equal. Here, we show that unimodality in linear DKMs also holds for all choices of \u03bdl. Recall the linear DKM objective in Eq. (115),\n\nL(G1, ..., GL) = (cid:80)L+1 = (cid:80)L+1\n\nl=1\n\nl=1\n\n\u03bdl 2\n\u03bdl 2\n\n(cid:12)G\u22121\n\nl\u22121Gl\n\n(cid:0)log (cid:12) (cid:0)log |Gl| \u2212 log |Gl\u22121| \u2212 Tr(cid:0)G\u22121\n\n(cid:12) (cid:12) \u2212 Tr (cid:0)G\u22121\n\nl\u22121Gl\n\n(cid:1)(cid:1)\n\nl\u22121Gl\n\n(cid:1)(cid:1) .\n\n(122)\n\n(123)\n\n28\n\nTo find the mode, again we set the gradient wrt Gl to zero,\n\n0 =\n\n\u2202L \u2202Gl\n\n= \u2212 \u03bdl+1\u2212\u03bdl\n\n2 G\u22121\n\nl \u2212 \u03bdl\n\n2 G\u22121\n\nl\u22121 + \u03bdl+1\n\n2 G\u22121\n\nl Gl+1G\u22121\n\nl\n\n,\n\n(124)\n\nfor l = 1, ..., L. Right multiplying by 2Gl and rearranging,\n\n\u03bdl+1G\u22121\n\nl Gl+1 = \u03bdlG\u22121\n\nl\u22121Gl + (\u03bdl+1 \u2212 \u03bdl) I,\n\nfor l = 1, ..., L.\n\n(125)\n\nEvaluating this expression for l = 1 and l = 2 gives,\n\n\u03bd2G\u22121 \u03bd3G\u22121\n\n1 G2 = \u03bd1G\u22121 2 G3 = \u03bd2G\u22121\n\n0 G1 + (\u03bd2 \u2212 \u03bd1) I, 1 G2 + (\u03bd3 \u2212 \u03bd2) I = \u03bd1G\u22121\n\n0 G1 + (\u03bd3 \u2212 \u03bd1) I.\n\nRecursing, we get,\n\n\u03bdlG\u22121\n\nl\u22121Gl = \u03bd1G\u22121\n\n0 G1 + (\u03bdl \u2212 \u03bd1) I.\n\n(126)\n\n(127)\n\n(128)\n\nCritically, this form highlights constraints on G1. In particular, the right hand side, G\u22121 l\u22121Gl, is the product of two positive definite matrices, so has positive eigenvalues (but may be non-symmetric (Horn & Johnson, 2012)). Thus, all eigenvalues of \u03bd1G\u22121 0 G1 must be larger than \u03bd1 \u2212 \u03bdl, and this holds true at all layers. This will become important later, as it rules out inadmissible solutions.\n\nGiven G0 and G1, we can compute any Gl using,\n\nG\u22121\n\n0 Gl =\n\n(cid:33)\n\n\u03bdl\u2032\n\nG\u22121\n\n0 Gl =\n\n(cid:32) l\n\n(cid:89)\n\nl\u2032=1\n\nl (cid:89)\n\nl\u2032=1\n\nl (cid:89)\n\nl\u2032=1\n\n(cid:0)G\u22121\n\nl\u2032\u22121Gl\u2032\n\n(cid:1) =\n\n(cid:81)l\n\n1 l\u2032=1 \u03bdl\u2032\n\nl (cid:89)\n\nl\u2032=1\n\n(cid:0)\u03bdl\u2032G\u22121\n\nl\u2032\u22121Gl\u2032\n\n(cid:1)\n\n(cid:0)\u03bd1G\u22121\n\n0 G1 + (\u03bdl\u2032 \u2212 \u03bd1) I(cid:1)\n\n(129)\n\n(130)\n\nwhere the matrix products are ordered as (cid:81)L using our knowledge of GL+1. Computing G\u22121 (cid:32)L+1 (cid:89)\n\n(cid:33)\n\nL+1 (cid:89)\n\n\u03bdl\n\nG\u22121\n\n0 GL+1 =\n\nl=1\n\nl=1\n\nWe write the eigendecomposition of \u03bd1G\u22121\n\n0 G1 as, 0 G1 = VDV\u22121.\n\n\u03bd1G\u22121\n\nl=1 Al = A1 \u00b7 \u00b7 \u00b7 AL. Now, we seek to solve for G1 0 GL+1,\n\n(cid:0)\u03bd1G\u22121\n\n0 G1 + (\u03bdl \u2212 \u03bd1) I(cid:1) .\n\n(131)\n\n(132)\n\nThus,\n\n(cid:33)\n\n\u03bdl\n\nG\u22121\n\n0 GL+1 =\n\n(cid:32)L+1 (cid:89)\n\nl=1\n\nwhere \u039b is a diagonal matrix,\n\n\u039b =\n\nL+1 (cid:89)\n\nl=1\n\nL+1 (cid:89)\n\nl=1\n\n(cid:0)VDV\u22121 + (\u03bdl \u2212 \u03bd1) I(cid:1) = V\u039bV\u22121\n\n(133)\n\n(D + (\u03bdl \u2212 \u03bd1) I) .\n\n(134)\n\nThus, we can identify V and \u039b by performing an eigendecomposition of the known matrix, (cid:16)(cid:81)L+1 0 GL+1. Then, we can solve for D (and hence G1) in terms of \u039b and V. The\n\n(cid:17)\n\nl=1 \u03bdl\n\nG\u22121 diagonal elements of D satisfy,\n\n0 = \u2212\u039bii +\n\nL+1 (cid:89)\n\nk=1\n\n(Dii + (\u03bdl \u2212 \u03bd1)) .\n\n(135)\n\nThis is a polynomial, and remembering the constraints from Eq. (128), we are interested in solutions which satisfy,\n\n\u03bd1 \u2212 \u03bdmin \u2264 Dii.\n\n(136)\n\n29\n\nwhere,\n\n\u03bdmin = min (\u03bd1, . . . , \u03bdL+1) .\n\n(137)\n\nTo reason about the number of such solutions, we use Descartes\u2019 rule of signs, which states that the number of positive real roots is equal to or a multiple of two less than the number of sign changes in the coefficients of the polynomial. Thus, if there is one sign change, there must be one positive real root. For instance, in the following polynomial,\n\n0 = x3 + x2 \u2212 1\n\n(138)\n\nthe signs go as (+), (+), (\u2212), so there is only one sign change, and there is one real root. To use Descartes\u2019 rule of signs, we work in terms of D\u2032\n\nii, which is constrained to be positive,\n\n0 \u2264 D\u2032\n\nii = Dii \u2212 (\u03bd1 \u2212 \u03bdmin)\n\nDii = D\u2032\n\nii + (\u03bd1 \u2212 \u03bdmin) .\n\n(139)\n\nThus, the polynomial of interest (Eq. 135) becomes,\n\n0 = \u2212\u039bii +\n\nL+1 (cid:89)\n\nl=1\n\n(D\u2032\n\nii + (\u03bd1 \u2212 \u03bdmin) \u2212 (\u03bd1 \u2212 \u03bdl)) = \u2212\u039bii +\n\nL+1 (cid:89)\n\nl=1\n\n(D\u2032\n\nii + (\u03bdl \u2212 \u03bdmin))\n\n(140)\n\nwhere 0 < \u03bdl \u2212 \u03bdmin as \u03bdmin is defined to be the smallest \u03bdl (Eq. 137). Thus, the constant term, \u2212\u039bii is negative, while all other terms, D\u2032 ii)L+1 in the polynomial have positive coefficients. Thus, there is only one sign change, which proves the existence of only one valid real root, as required.\n\nii, . . . , (D\u2032\n\nK UNIMODALITY EXPERIMENTS WITH NONLINEAR KERNELS\n\nFor the posterior over Gram matrices to converge to a point distribution, we need the DKM objective L(G1, . . . , GL) to have one unique global optimum. As noted above, this is guaranteed when the prior dominates (Eq. 11), and for linear models (Appendix J). While we believe that it might be possible to construct counter examples, in practice we expect a single global optimum in most practical settings. To confirm this expectation, we did a number of experiments, starting with many different random initializations of a deep kernel machine and optimizing using gradient descent (Appendix K). In all cases tested, the optimizers converged to the same maximum.\n\nP VlVT\n\nl with Vl \u2208 RP \u00d7P being trainable parameters. To We parameterise Gram matrices Gl = 1 make initializations with different seeds sufficiently separated while ensuring stability we initialize Gl from a broad distribution that depends on K(Gl\u22121). Specifically, we first take the Cholesky decomposition K(Gl\u22121) = Ll\u22121LT l where each entry of \u039el \u2208 RP \u00d7P is independently sampled from a standard Gaussian, and Dl is a diagonal scaling matrix with each entry sampled i.i.d. from an inverse-Gamma distribution. The variance of the inverse-Gamma distribution is fixed to 100, and the mean is drawn from a uniform distribution U [0.5, 3] for each seed. Since for any random variable x \u223c Inv-Gamma(\u03b1, \u03b2), E(x) = \u03b2 (\u03b1\u22121)(\u03b1\u22122) , once we fix the mean and variance we can compute \u03b1 and \u03b2 as\n\nl\u22121, then set Vl = Ll\u22121\u039elD1/2\n\n\u03b1\u22121 and V(x) =\n\n\u03b2\n\n\u03b1 =\n\nE(x)2 V(x) \u03b2 = E(x)(\u03b1 \u2212 1).\n\n+ 2,\n\n(141)\n\n(142)\n\nWe set \u03bdl = 5, and use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.001 to optimize parameters Vl described above. We fixed all model hyperparameters to ensure that any multimodality could emerge only from the underlying deep kernel machine. As we did not use inducing points, we were forced to consider only the smaller UCI datasets (yacht, boston, energy and concrete). For the deep kernel machine objective, all Gram matrices converge rapidly to the same solution, as measured by RMSE (Fig. 12). Critically, we did find multiple modes for the MAP objective (Fig 13), indicating that experiments are indeed powerful enough to find multiple modes (though of course they cannot be guaranteed to find them). Finally, note that the Gram matrices took a surprisingly long time to converge: this was largely due to the high degree of diversity in the initializations; convergence was much faster if we initialised deterministically from the prior.\n\n30\n\nFigure 12: One-layer DKMs with squared exponential kernel trained on full UCI datasets (through columns) converges to the same solution, despite very different initializations by applying stochastic diagonal scalings described in Appendix F to the standard initialization with different seeds. Standard initialization is shown in dashed line, while scaled initializations are the color lines each denoting a different seed. The first row shows the objective during training for all seeds that all converge to the same value. The second row shows the element-wise RMSE between the Gram matrix of each seed and the optimized Gram matrix obtained from the standard initialization. RMSE converges to 0 as all initializations converge on the same maximum. The last row plots RMSE versus objective value, again showing a single optimal objective value where all Gram matrices are the same.\n\nThis might contradict our usual intuitions about huge multimodality in the weights/features of BNNs and DGPs. This can be reconciled by noting that each mode, written in terms of Gram matrices, corresponds to (perhaps infinitely) many modal features. In particular, in Sec. E, we show that the log-probability for features, P (Fl|Fl\u22121) (Eq. 82) depends only on the Gram matrices, and note that there are many settings of features which give the same Gram matrix. In particular, the Gram matrix is the same for any unitary transformation of the features, F\u2032 l = FlU, satisfying UUT = I, as l = 1 1\nl = Gl. For DGPs we can use any unitary matrix, so there Nl are infinitely many sets of features consistent with a particular Gram matrix, while for BNNs we can only use permutation matrices, which are a subset of unitary matrices. Thus, the objective landscape must be far more complex in the feature domain than with Gram matrices, as a single optimal Gram matrix corresponds to a large family of optimal features.\n\nFlUlUT\n\nl = 1\n\nFlFT\n\nl FT\n\nlF\u2032T\n\nF\u2032\n\nNl\n\nNl\n\n31\n\n500490480objectiveyacht860850840boston111011001090energy174017301720concrete050000100000iters0.00.51.0RMSE050000100000iters050000100000iters050000100000iters500490480objective0.00.51.0RMSE860850840objective111011001090objective174017301720objectiveFigure 13: One-layer DGP with MAP inference over features as described in Appendix E Eq. (84). Rows and columns are the same as in Figure 12. Using the same randomly scaled initializations described above, we are able to find multiple modes in energy and concrete showing our initializations are diverse enough, albeit there is still only a single global optimum.\n\n32\n\n46504700objectiveyacht64606510boston1240012450energy1575015800concrete050000100000iters0.00.51.0RMSE050000100000iters0100000200000iters050000100000iters46504700objective0.00.51.0RMSE64606510objective1240012450objective1575015800objectiveL INDUCING POINT DKMS\n\nTo do large-scale experiments on UCI datasets, we introduce inducing point DKMs by extending Gaussian process inducing point methods (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) to the DKM setting. This approach uses the variational interpretation of the deep kernel machine objective described in Appendix D.\n\nTo do inducing-point variational inference, we need to explicitly introduce top-layer features mirroring FL+1 \u2208 RP \u00d7\u03bdL+1 in Appendix B, but replicated N times, \u0303FL+1 \u2208 RP \u00d7NL+1. Formally, each feature, \u0303f L+1\n\n, . . . , \u0303f L+1 NL+1\n\n1\n\nis IID, conditioned on FL, \u03bb=1N (cid:0) \u0303f L+1 \u03bb=1N (cid:0) \u0303y\u03bb; \u0303f L+1\n\nP ( \u0303FL+1|FL) = (cid:81)Nl P ( \u0303Y| \u0303FL+1) = (cid:81)Nl\n\n\u03bb\n\n\u03bb\n\n; 0, K(G(FL))(cid:1) , , \u03c32I(cid:1) ,\n\n(143a)\n\n(143b)\n\nwhere we give the likelihood for regression, but other likelihoods (e.g. for classification) are possible (Appendix B).\n\nFurther, we take the total number of points, P , to be made up of Pi inducing points and Pt test/train points, so that P = Pi + Pt. Thus, we can separate all features, Fl \u2208 RP \u00d7Nl, into the inducing t \u2208 RPt\u00d7Nl. Likewise, we separate the inputs, features, Fl X, and outputs, Y, into (potentially trained) inducing inputs, Xi, and trained inducing outputs, Yi, and the real test/training inputs, Xt, and outputs, Yt,\n\ni \u2208 RPi\u00d7Nl, and the test/train features, Fl\n\nFl =\n\n(cid:19)\n\n(cid:18)Fl i\nFl t\n\n \u0303FL+1 =\n\n(cid:18) \u0303FL+1 \u0303FL+1\n\ni\n\nt\n\n(cid:19)\n\nX =\n\n(cid:19)\n\n(cid:18)Xi Xt\n\nY =\n\n(cid:19)\n\n(cid:18)Yi Yt\n\n \u0303Y =\n\n(cid:19)\n\n(cid:18) \u0303Yi \u0303Yt\n\n(144)\n\nWe follow the usual doubly stochastic inducing point approach for DGPs. In particular, we treat all the features at intermediate layers, F1, . . . , FL, and the top-layer train/test features, FL+1 as latent variables. However, we deviate from the usual setup in treating the top-layer inducing outputs, FL+1 , as learned parameters and maximize over them to ensure that the ultimate method does not require sampling, and at the same time allows minibatched training. The prior and approximate posterior over F1, . . . , FL are given by,\n\ni\n\nt\n\nQ (F1, . . . FL|X) = (cid:81)L P (F1, . . . , FL|X) = (cid:81)L\n\nl=1 Q (Fl|Fl\u22121) , l=1 P (Fl|Fl\u22121) ,\n\n(145a)\n\n(145b)\n\nand remember F0 = X, so G0 = 1 N0 factorises into a distribution over the inducing points and a distribution over the test/train points,\n\nXXT . The prior and approximate posterior at each layer\n\nQ (Fl|Fl\u22121) = P (cid:0)Fl P (Fl|Fl\u22121) = P (cid:0)Fl\n\nt |Fl t |Fl\n\ni , Fl\u22121 i , Fl\u22121\n\n(cid:1) Q (cid:0)Fl (cid:1) P (cid:0)Fl\n\n(cid:1) , i |Fl\u22121\n\ni\n\ni\n\n(cid:1) .\n\n(146a)\n\n(146b)\n\nthe approximate posterior samples for the test/train points is the conditional prior (cid:1), which is going to lead to cancellation when we compute the ELBO. Likewise,\n\nCritically, P (cid:0)Fl t |Fl the approximate posterior over \u0303FL+1\n\ni , Fl\u22121\n\nt\n\nis the conditional prior, (cid:1) = P (cid:0) \u0303FL+1\n\n, FL\n\n|FL+1\n\nt\n\ni\n\nQ (cid:0) \u0303FL+1\n\nt\n\n|FL+1\n\ni\n\n, FL\n\n(cid:1) .\n\n(147)\n\nConcretely, the prior approximate posterior over inducing points are given by,\n\nQ (cid:0)Fl\n\n\u03bb=1N (cid:0)f l \u03bb=1N (cid:0)f l The approximate posterior is directly analogous to Eq. (69) and the prior is directly analogous to Eq. (1a), but where we have specified that this is only over inducing points. Now we compute the ELBO\n\ni;\u03bb; 0, Gl i;\u03bb; 0, K(G(Fl\u22121\n\n(cid:1) = (cid:81)Nl (cid:1) = (cid:81)Nl\n\ni |Fl\u22121\n\nP (cid:0)Fl\n\n(148b)\n\n(148a)\n\n))(cid:1)\n\n(cid:1) ,\n\nii\n\ni\n\ni\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii ) (cid:34)\n\n= EQ\n\nlog P (cid:0) \u0303Yt| \u0303FL+1\n\nt\n\ni\n\n|FL+1 |FL+1\n\ni\n\n, FL\n\n, FL\n\n(cid:1) P (F1, . . . FL|X) (cid:1) Q (F1, . . . FL|X)\n\n(cid:35)\n\n(149)\n\n(cid:1) + log\n\nP (cid:0) \u0303FL+1 Q (cid:0) \u0303FL+1\n\nt\n\nt\n\n33\n\nNote that the P (cid:0)Fl we come to describing sampling), substituting Eq. (145\u2013147) and cancelling P (cid:0)Fl P (cid:0) \u0303FL+1\n\n(cid:1) terms are going to cancel in the ELBO, we consider them below when (cid:1) and\n\ni , Fl\u22121\n\ni , Fl\u22121\n\n|FL+1\n\nt |Fl\n\nt |Fl\n\n(cid:1),\n\n, FL\n\nt\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii ) = EQ\n\n(cid:34)\n\nlog P (cid:0) \u0303Yt| \u0303FL+1\n\nt\n\n(cid:1) +\n\nL (cid:88)\n\nl=1\n\nlog\n\nP (cid:0)Fl\n\ni |Fl\u22121 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(150)\n\nSo far, we have treated the Gram matrices, Gl ii as parameters of the approximate posterior. However, in the infinite limit N \u2192 \u221e, these are consistent with the features generated by the approximate (cid:0)Fl posterior. In particular the matrix product 1 can be written as an average over infinitely Nl many IID vectors, f l i;\u03bb (first equality), and by the law of large numbers, this is equal to the expectation of one term (second equality), which is Gl\n\nii (by the approximate posterior Eq. (148a)),\n\nFl i\n\n(cid:1)T\n\ni\n\nlim N\u2192\u221e\n\n1 Nl\n\nFl\n\ni\n\n(cid:1)T\n\n(cid:0)Fl\n\ni\n\n= lim\n\nN\u2192\u221e\n\n1 Nl\n\n(cid:80)Nl\n\n\u03bb=1f l\n\ni;\u03bb\n\n(cid:1)T\n\n(cid:0)f l\n\ni;\u03bb\n\n= E\n\nQ(f l\n\ni;\u03bb)\n\n(cid:104)\n\nf l i;\u03bb\n\n(cid:0)f l\n\ni;\u03bb\n\n(cid:1)T (cid:105)\n\n= Gl ii.\n\n(151)\n\nBy this argument, the Gram matrix from the previous layer, Gl\u22121 through Gl\u22121 DGP, Fl factorise. Thus, in the infinite limit, individual terms in the ELBO can be written,\n\nis deterministic. Further, in a (Eq. 5), and the prior and approximate posterior\n\ni only depends on Fl\u22121\n\nii\n\ni\n\ni\n\n(cid:34)\n\nlim N\u2192\u221e\n\n1 N\n\nEQ\n\nlog\n\nP (cid:0)Fl\n\ni |Fl\u22121 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n\uf8ee\n\n= \u03bdl EQ\n\n\uf8f0log\n\n(cid:16)\n\nP\n\ni;\u03bb|Gl\u22121 f l (cid:16)\n\ni (cid:17)\n\nQ\n\nf l i;\u03bb\n\n(cid:17)\n\n\uf8f9\n\n\uf8fb\n\n= \u2212\u03bdl DKL\n\n(cid:0)N (cid:0)0, K(Gl\u22121\n\ni\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\ni\n\n(152)\n\n(153)\n\n(cid:1)(cid:1) ,\n\nwhere the final equality arises when we notice that the expectation can be written as a KLdivergence. The inducing DKM objective, Lind, is the ELBO, divided by N to ensure that it remains finite in the infinite limit,\n\nLind(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii )= lim\n\nN\u2192\u221e\n\n1\n\nN ELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii )\n\n(154)\n\n= EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) \u2212\n\nL (cid:88)\n\nl=1\n\n\u03bdl DKL\n\n(cid:0)N (cid:0)0, K(Gl\u22121\n\nii\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\nii\n\n(cid:1)(cid:1) .\n\nNote that this has almost exactly the same form as the standard DKM objective for DGPs in the main text (Eq. 16). In particular, the second term is a chain of KL-divergences, with the only difference that these KL-divergences apply only to the inducing points. The first term is a \u201cperformance\u201d term that here depends on the quality of the predictions given the inducing points. As the copies are IID, we have,\n\nEQ\n\n(cid:2)log P (cid:0) \u0303Yt| \u0303FL+1\n\nt\n\n(cid:1)(cid:3) = N EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) .\n\n(155)\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nNow that we have a simple form for the ELBO, we need to compute the expected likelihood, (cid:1)(cid:3). This requires us to compute the full Gram matrices, including test/train EQ points, conditioned on the optimized inducing Gram matrices. We start by defining the full Gram matrix,\n\nt\n\nGl =\n\n(cid:18)Gl Gl\n\nii Gl ti Gl\n\ntt\n\nit\n\n(cid:19)\n\n(156)\n\nfor both inducing points (labelled \u201ci\u201d) and test/training points (labelled \u201ct\u201d) from just Gl ii. For clarity, we have Gl \u2208 RP \u00d7P , Gl tt \u2208 RPt\u00d7Pt, where Pi is the number of inducing points, Pt is the number of train/test points and P = Pi + Pt is the total number of inducing and train/test points.\n\nii \u2208 RPi\u00d7Pi, Gl\n\nti \u2208 RPt\u00d7Pi, Gl\n\nThe conditional distribution over Fl\n\nt given Fl\n\ni is, \u03bb=1N (cid:0)f l (cid:1) = (cid:81)Nl\n\nP (cid:0)Fl\n\nt\n\n(cid:12) (cid:12)Fl\n\ni , Gl\u22121\n\nt;\u03bb; KtiK\u22121\n\nii f l\n\ni;\u03bb, Ktt\u00b7i\n\n(cid:1)\n\n(157)\n\n34\n\nAlgorithm 1 DKM prediction\n\nl=1\n\nii}L\n\nParameters: {\u03bdl}L Optimized Gram matrices {Gl Inducing and train/test inputs: Xi, Xt Inducing outputs: FL+1 Initialize full Gram matrix (cid:19) (cid:18)G0 G0\n\n(cid:18)XiXT XtXT\n\ni XiXT i XtXT\n\nii G0;T ti G0\n\n= 1 \u03bd0\n\nl=1\n\ntt\n\nti\n\ni\n\nt\n\nt\n\n(cid:19)\n\nPropagate full Gram matrix for l in (1, . . . , L) do\n\n(cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\nii\n\n(cid:18)(cid:18)Gl\u22121 Gl\u22121 ii KT ti .\n\nti\n\n(cid:19)(cid:19)\n\n)T\n\n(Gl\u22121 Gl\u22121\n\nti\n\ntt\n\nii\n\niiK\u22121\n\nii KT\n\nii Gl ii Gl\n\nti = KtiK\u22121 tt = KtiK\u22121\n\nKtt\u00b7i = Ktt \u2212 KtiK\u22121 Gl Gl end for Final prediction using standard Gaussian process expressions (cid:18)Kii KT (cid:18)(cid:18)GL (GL ii ti GL GL Kti Ktt tt ti , Ktt \u2212 KtiK\u22121\n\n= K Yt \u223c N (cid:0)KtiK\u22121\n\nti + Ktt\u00b7i\n\nti + \u03c32I(cid:1)\n\nii FL+1\n\nii KT\n\nti )T\n\n(cid:19)(cid:19)\n\n(cid:19)\n\ni\n\nwhere f l feature for all train/test inputs, and f l\n\nt;\u03bb is the activation of the \u03bbth feature for all train/test inputs, f l i;\u03bb, and (cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\n(cid:16) 1\n\nNl\u22121\n\n(cid:17)\n\nFl\u22121FT\n\nl\u22121\n\nKtt\u00b7i = Ktt \u2212 KtiK\u22121\n\nii KT ti .\n\n= K (Gl\u22121)\n\n(158)\n\n(159)\n\ni;\u03bb is the activation of the \u03bbth\n\nIn the infinite limit, the Gram matrix becomes deterministic via the law of large numbers (as in Eq. 151), and as such Git and Gtt become deterministic and equal to their expected values. Using Eq. (157), we can write,\n\nwhere \u039e is a matrix with IID standard Gaussian elements. Thus,\n\nFl\n\nt = KtiK\u22121\n\nii Fl\n\ni + K1/2\n\ntt\u00b7i \u039e.\n\nGl\n\nt (Fl\n\nE (cid:2)Fl \u03bd KtiK\u22121\n\nti = 1 \u03bd\n= 1 = KtiK\u22121\n\nii\n\nii Gl\n\nii\n\ni )T (cid:3) E (cid:2)Fl\n\ni (Fl\n\ni )T (cid:3)\n\nand,\n\nGl\n\nt (Fl\n\nE (cid:2)Fl\n\ntt = 1 \u03bd\n= 1 = KtiK\u22121\n\nt )T (cid:3) E (cid:2)Fl ii GiiK\u22121\n\n\u03bd KtiK\u22121\n\nii\n\ni (Fl ii KT\n\ni )T (cid:3) K\u22121 ti + Ktt\u00b7i\n\nii KT\n\nti + 1\n\n\u03bd K1/2\n\ntt\u00b7i\n\n(160)\n\n(161)\n\n(162)\n\n(163)\n\n(164)\n\n(165)\n\n(166)\n\nE (cid:2)\u039e\u039eT (cid:3) K1/2\n\ntt\u00b7i\n\nFor the full prediction algorithm, see Alg. 1.\n\n35",
      "responses": [
        "# Summary Of The Paper\n\nLarge-width networks converge to GPs with fixed/ unlearnable kernels, making the networks unable to perform representation learning. In this paper, the authors proposed a simple recipe to allow for representation learning: making the logit layer as wide as the hidden layers and replicating the labels accordingly. The authors argue that this approach allows the kernel to evolve and possible learning a representation. The authors also support their claims using synthetical dataset experiments and UCI datasets.\n\n# Strength And Weaknesses\n\n# Strength.\n1. The paper proposed simple and interesting ideas that allow feature learning in the infinite-width network, which is worth further exploration. \n2. Some experiments from the paper show non-Gaussian behavior of the learned representation that differs from the NNGP limit.  \n\n# Weakness\n\n1. The notations and terminologies are confusing, which makes it hard to parse the paper. \n2. Missing key empirical comparison (vs. NNGP/NTK) for representation learning. I would like to see a comparison using CIFAR10. \n3. Several strong claims that require further justifications.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1. Many strong claims from the paper are unconvincing. \n\n- Using `**the** representation learning limit` is not proper; there can be many of them, and the proposed approach is just one of many. \n\n - I can't agree with the claim that the approach is `extremely theoretically tractable.` (in the abstract) and it is misleading. Unlike NNGP, the solution can be written analytically as an exact closed-form formula of the input data. The approach in the paper is not analytically tractable (except possibly in the linear network setting) and requires approximation methods. Cited from the paper, \"In practice, the true posteriors required to evaluate Eq. (21) are intractable \".  \n\n- \"We show that DKMs can be scaled to large datasets using inducing point methods from the Gaussian process literature, and we show that DKMs exhibit superior performance to other kernel-based approaches\" (from the abstract.) This is certainly an overclaim. The largest dataset used in UCI, could not justify *superior performance* and scalability to *large datasets*. Again, I expect at least experimental results on CIFAR10 if not more complicated. \n\n- I would like to re-emphasize that the dataset used here (UCI) is too simple to capture *representation learning*. I would expect a comparison using CIFAR-10 against benchmark results for NNGP kernels; see Lee et al. finite vs infinite (https://arxiv.org/abs/2007.15801); neural kernel without tangents (Shankar, https://arxiv.org/abs/2003.02237). I am also interested in the visualization of the learned representation using the image dataset. \n\n2. Notations and terminologies. \n\n- The definition of DNNs is confusing in Sec 3. Do you mean a fully-connected network with trainable parameters that are optimized using gradient descent? This is what DNNs mean for most people in ML. In addition, what does that mean to marginalize the weights in each layer in eq (2)? Do you mean sequentially? Can you verbally explain the difference between (3a) and (3b)? \n\n-  DGP \"Deep Gaussian Processes\" (by Andreas C. Damianou, Neil D. Lawrence, not cited in the paper) is a standard framework. Is it the same thing used in the paper? If so, why not cite the above (or related ) paper? \n\n-  Notations in equation (11) are confusing. What are $G_l/G_{l-1}$ here? Don't they depend on $N$ ? Or are they some *infinite/deterministic* objects that don't rely on $N$, and you overload the notations? Same for equations (12), etc. In addition, Eq (13) seems to be a result in \"Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling\" by Hron et al. Please clarify. \n\n3. Others. \n- First citation in the intro. Please cite \"Radford M. Neal. Priors for infinite networks,\" which is the first NNGP (one-hidden layer) paper. \"Lee et al Deep Neural Networks as Gaussian Processes\"  should also be added.\n\n# Summary Of The Review\n\nUpdate: \nI thank the authors for the clarification/updates of several notations/terminologies. I like the idea from the paper, a Bayesian prospective of feature learning. I have increased my score accordingly. However, there are still a lot of room for improvement and several open questions. To name some,  \n\n1. Even though this is a theory paper, authors should not shy away from more empirical work. Even doing a small scale experiments on Cifar10 (baseline against kernel) could really helpful to both practitioners and theorists. \n\n2. Authors should have more detailed discussion/comparison between the meanfield/feature learning limit of neural networks and the current proposed Bayesian framework, both theoretically and empirically. E.g., baselining performance (bayesian feature learning vs NN feature learning), are they learning similar features, etc. \n\n---------------------------------\n\nOverall, I think the idea from the paper is interesting and worth further exploration. However, I also find the paper hard to parse due to clarity issues from notations and terminology, and the theoretical and empirical results from the paper do not support the strong claims from the paper.\n\n# Correctness\n\n2: Several of the paper\u2019s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
        "# Summary Of The Paper\nThe paper titled \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\" presents a comprehensive theoretical framework linking representation learning in deep neural networks (DNNs) to kernel methods, with a particular focus on the introduction of the **Bayesian representation learning limit**. This new limit enables the retention of representation learning in infinite-width models and leads to the formulation of **Deep Kernel Machines (DKMs)**, a novel class of models that generalizes traditional kernel methods while allowing for sparse representations that scale linearly with the number of data points. The authors provide empirical validation demonstrating that DGPs (Deep Gaussian Processes) align well with theoretical predictions under this framework, thus marking a significant advancement in understanding the intersection of deep learning and kernel methods.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper lies in its innovative contribution to the theoretical understanding of representation learning in DNNs, particularly through the introduction of the Bayesian representation learning limit, which effectively addresses a gap in existing literature. The formulation of the DKM objective and its empirical validation against existing methods is another crucial strength, as it demonstrates practical applicability. However, a potential weakness is the complexity of the concepts introduced, which may pose challenges for practitioners seeking to implement these methods without a solid background in both Bayesian statistics and kernel methods. Additionally, while the empirical results are promising, further validation across a broader range of datasets could enhance the robustness of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, clearly presenting its contributions with appropriate theoretical support and empirical validation. The methodology is articulated sufficiently for expert readers, though some sections may require a deeper understanding of advanced statistical concepts, particularly for those less familiar with Bayesian approaches. The novelty of the contributions is significant, as it provides a fresh perspective on the role of representation learning in DNNs. The reproducibility of results could be improved by including more detailed descriptions of experimental setups, hyperparameter choices, and code availability, which would facilitate further exploration by the research community.\n\n# Summary Of The Review\nOverall, this paper presents an important theoretical advancement in understanding representation learning within the context of neural networks and kernel methods, introducing novel concepts that hold substantial promise for future research. While the contributions are significant and empirically validated, certain complexities may limit accessibility for broader audiences, and additional details could enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel theoretical framework for representation learning in deep neural networks (DNNs) and deep Gaussian processes (DGPs) through a new infinite width limit termed the \"Bayesian representation learning limit.\" This limit maintains representation learning, contrasting with traditional infinite limits (NNGPs) that strip it away, and reveals that DGPs in this limit yield multivariate Gaussian posteriors. The authors introduce deep kernel machines (DKMs), a flexible extension of kernel methods, along with a sparse DKM that scales linearly with the number of data points. Empirical results demonstrate that DKMs closely mimic the behavior of wide DGPs, with strong performance across various tasks, affirming the theoretical insights.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to retaining representation learning in the infinite width limit, which significantly enhances the theoretical understanding of DNNs and DGPs. The empirical evidence robustly supports the theoretical claims, showcasing the effectiveness of the proposed DKMs and their scalability using inducing points. However, a notable weakness is the reliance on sufficiently wide networks, which may not always be practical. Additionally, while the extension to Bayesian neural networks (BNNs) is mentioned, the complexities associated with non-Gaussian posteriors could limit the generalizability of some findings. The paper also tends to focus on specific models and datasets, which may restrict the applicability of its conclusions to more complex scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly structured, with a logical flow from theoretical contributions to empirical validation. The quality of the methodology is high, and the experimental design is robust, allowing for reproducibility. The novelty of the proposed Bayesian representation learning limit and the introduction of DKMs is significant, providing a fresh perspective on representation learning in the context of deep learning. The paper includes sufficient detail to enable replication of the experiments, although the complexity of the models may pose challenges for some researchers.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of representation learning by presenting a new theoretical framework that successfully retains representation learning in deep architectures. The empirical results validate the theoretical insights, particularly in the effectiveness and scalability of the proposed DKMs. However, the reliance on wide networks and the focus on specific datasets may limit broader applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a novel \"Bayesian representation learning limit\" that allows for the retention of representation learning in Deep Gaussian Processes (DGPs), addressing a critical gap in the existing theoretical frameworks that typically disregard this aspect due to infinite width limits. The authors demonstrate that under this new limit, DGP posteriors are multivariate Gaussian, which enables the development of a new \"deep kernel machine\" (DKM) objective that optimizes posterior covariances. The proposed DKMs achieve cubic scaling with data points, while sparse DKMs utilize inducing points for linear scaling. The findings are supported by empirical validations across various datasets, establishing a promising new approach for flexible kernel methods in deep learning.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to preserving representation learning within the context of DGPs, providing a significant contribution to the theoretical understanding of neural networks and kernel methods. The introduction of the DKM objective is a noteworthy advancement that balances performance and regularization effectively. However, the paper could benefit from a more extensive discussion on the limitations of the proposed methods and their computational complexity compared to standard approaches. Additionally, while the empirical results are promising, further validation on more diverse datasets could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its theoretical foundations, methodologies, and experimental results. The clarity of the writing aids in comprehending complex concepts such as the new Bayesian limits and the derivation of the DKM objective. The novelty of the approach is evident, as it reinterprets traditional kernel methods through the lens of modern deep learning. The reproducibility of the results is facilitated by the detailed description of the experimental setup, including the use of Langevin sampling and the Adam optimizer, although providing code or supplementary material would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the theoretical understanding of representation learning in neural networks, offering a robust framework that bridges deep learning and kernel methods. While it demonstrates promising empirical results, additional discussions around limitations and further validation would enhance its impact.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 5\n\n# Empirical Novelty And Significance\nRating: 4"
      ],
      "preparations": [
        null,
        "# Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n## Abstract\n- **Key Focus**: Understanding representation learning in deep neural networks (DNNs) and its link to kernel methods.\n- **New Contribution**: Introduces the **Bayesian representation learning limit**, which retains representation learning in infinite-width models while simplifying standard infinite-width limits.\n- **Findings**:\n  - Deep Gaussian Processes (DGPs) exhibit multivariate Gaussian posteriors in this limit.\n  - The posterior covariances can be optimized using a new objective that combines log-likelihood and KL-divergences.\n  - Introduces **Deep Kernel Machines (DKMs)** as a deep generalization of kernel methods, which can be made sparse to achieve linear scaling with the number of data points.\n\n## Introduction\n- **Background**: DNNs and DGPs excel in transforming inputs into high-level representations for complex tasks.\n- **Challenges**: Traditional theoretical approaches using infinite limits (NNGPs) do not adequately capture representation learning.\n- **Existing Framework**: Two limits are discussed:\n  - **NNGP**: Focuses on representation transformation.\n  - **Neural Tangent Kernel (NTK)**: Focuses on predictions change during training.\n- **Contribution**: This paper is the first to address representation learning in the context of NNGPs.\n\n## Contributions\n1. **Bayesian Representation Learning Limit**:\n   - Retains representation learning in deep Bayesian models like DGPs.\n   - Prior strength increases with width, leading to a balance with the likelihood through rescaling.\n\n2. **DGP Posteriors**:\n   - Posteriors in the Bayesian representation learning limit are exactly zero-mean multivariate Gaussian, defined as:\n     \\[\n     P(f_\\lambda^l | X, y) = N(f_\\lambda^l; 0, G^l)\n     \\]\n\n3. **Deep Kernel Machine Objective**:\n   - Introduced objective:\n     \\[\n     L(G_1, \\ldots, G_L) = \\log P(Y|G_L) - \\sum_{l=1}^{L}\\nu_l D_{KL}(N(0, G_l) || N(0, K(G_{l-1})))\n     \\]\n   - Interpretation of this objective: log-likelihood promotes performance, while KL-divergences regularize the posteriors.\n\n4. **Sparse DKM**:\n   - Developed a sparse version inspired by inducing point literature, allowing linear scaling with respect to data points compared to the cubic scaling of naive kernel methods.\n\n5. **Extension to BNNs**: \n   - Results extended to Bayesian Neural Networks, which have non-Gaussian posteriors.\n\n## Related Work\n- Discusses the distinction between work on NNGPs, NTK, and physics-based approaches to representation learning.\n- Highlights the uniqueness of results for DGPs in comparison to previous neural network studies.\n\n## Results\n- **DGP Definition**: DGP maps inputs to outputs across multiple layers with Gaussian priors.\n- **Model Formulation**:\n  - Each layer's features depend on the features from the previous layer through a Gram matrix.\n  \n- **Posterior Analysis**:\n  - Focused on log-posterior formulation, demonstrating that standard infinite-width limits eliminate representation learning.\n  - Bayesian representation learning limit maintains learning through the introduction of infinite output features.\n\n- **Empirical Verification**: \n  - Confirmed that wide but finite DGPs exhibit behavior matching the theoretical predictions of the DKM objective.\n  - Examined RMSE between Gram matrices across varying widths and datasets, finding a good match for widths \u2265 128.\n\n## DKM Implementation and Comparison\n- Compared DKMs against standard shallow kernel methods and MAP methods on UCI datasets.\n- Deep kernel machines outperformed other methods, demonstrating better adaptability and performance.\n\n## Conclusion\n- The paper introduces the Bayesian representation learning limit, allowing for deeper insights into representation learning in neural networks and establishing DKMs as a new class of deep kernel methods.\n- Highlights the potential for sparse DKMs to efficiently scale with data size while providing robust performance metrics.\n\n## Figures and Tables\n- **Figure 1**: Graphical model structure for generative models.\n- **Figure 2\u20133**: Comparisons between DGP representations and DKMs.\n- **Table 1**: Performance metrics comparing DKM to MAP and traditional kernel methods on UCI datasets.\n\n## References\n- Key references include foundational works on deep Gaussian processes, kernel methods, and recent advances in neural tangent kernels and representation learning.",
        "### Technical Content\n\n#### Abstract\n- The paper addresses the process of representation learning in deep neural networks (DNNs) and deep Gaussian processes (DGPs), proposing a new infinite width limit termed the \"Bayesian representation learning limit.\"\n- This new limit retains representation learning, unlike standard infinite limits (NNGPs) which eliminate it.\n- It establishes that DGPs in this limit yield multivariate Gaussian posteriors.\n- The paper introduces deep kernel machines (DKMs), a flexible deep generalization of kernel methods, and a sparse DKM that scales linearly with the number of data points.\n\n#### Introduction\n- DNNs and DGPs excel in transforming inputs to high-level representations through depth, crucial for solving complex tasks.\n- Standard theoretical frameworks struggle to capture representation learning, with NNGP focusing on representation transformation and NTK on prediction changes during training.\n- The authors claim to be the first to tackle representation learning challenges in the NNGP context.\n\n#### Contributions\n1. **Bayesian Representation Learning Limit**: \n   - Retains representation learning as width approaches infinity by rescaling the likelihood to match the prior.\n   \n2. **Multivariate Gaussian Posteriors**: \n   - In the Bayesian representation learning limit, DGP posteriors are exactly multivariate Gaussian.\n\n3. **DKM Objective**: \n   - Introduces a new objective that combines log likelihood for performance with KL-divergences to keep posteriors close to priors.\n\n4. **Sparse DKM**: \n   - Provides a method for scaling that utilizes inducing points, allowing for linear scaling with data points.\n\n5. **Extension to BNNs**: \n   - Discusses adapting these methods to Bayesian neural networks which have non-Gaussian posteriors.\n\n#### Results\n- The authors define a DGP model that maps inputs to outputs using a Gaussian process with a covariance that depends on features from previous layers.\n- They derive and confirm that the posterior covariances can be obtained through their proposed objective.\n- Experimental validation shows that DGPs and DKMs exhibit similar representations, with RMSE metrics supporting their claims on matching behavior between wide DGPs and DKMs.\n\n### Experimental Design\n- **Model Definition**: DGP is defined to transition from inputs to outputs through multiple layers, where both features and outputs are sampled from GPs.\n- **Objective Function**: The DKM objective is derived, combining log likelihood and KL-divergences.\n- **Experimental Setup**: \n  - Validation through UCI datasets.\n  - Comparison of performance metrics across DKM, MAP, and a standard shallow kernel method.\n  - Use of inducing points for sparse DKMs to maintain efficiency and scalability.\n  \n### Results Summary\n- The DKM objective outperforms MAP and hyperparameter optimization baselines in various tasks.\n- Confirmed that as width increases, the representations learned by DGPs converge to those of DKMs.\n- RMSE metrics between trained DGPs and DKMs show a close match, particularly with sufficient width.\n\n### Strengths\n- **Innovative Approach**: Proposes a novel limit that retains representation learning, significantly contributing to the theoretical understanding of DNNs and DGPs.\n- **Empirical Validation**: The experimental results robustly support the theoretical claims, demonstrating successful application across multiple datasets.\n- **Scalability**: The introduction of sparse DKMs allows for efficient handling of larger datasets, addressing a common limitation in kernel methods.\n\n### Limitations\n- **Dependence on Width**: The effectiveness of the proposed methods may depend on achieving sufficiently wide networks, which may not always be feasible in practice.\n- **Generalization to Non-Gaussian Posteriors**: While the extension to BNNs is noted, the complexities of non-Gaussian posteriors may limit the applicability of some results.\n- **Limited Exploration of Complex Scenarios**: The paper primarily focuses on specific models and datasets, potentially overlooking broader applications in diverse settings.\n\n### Conclusion\n- The paper successfully introduces a new theoretical framework for understanding representation learning in neural networks and Gaussian processes, backed by empirical results. The contributions made in developing DKMs and sparse DKMs could lead to advancements in machine learning, particularly in scenarios requiring flexible and scalable models.",
        "# Notes on \"A THEORY OF REPRESENTATION LEARNING IN NEURAL NETWORKS GIVES A DEEP GENERALISATION OF KERNEL METHODS\"\n\n## Abstract\n- Modern deep learning's success hinges on transforming inputs across layers for high-level representations.\n- Standard theoretical approaches (NNGPs) neglect representation learning due to infinite width limits.\n- Introduces \"Bayesian representation learning limit\" allowing retention of representation learning akin to finite-width models.\n- Deep Gaussian Processes (DGPs) in this limit yield multivariate Gaussian posteriors.\n- Posterior covariances can be optimized using a new objective, leading to \"deep kernel machines\" (DKMs).\n- DKMs scale cubically with data points; sparse DKMs use inducing points to scale linearly.\n- Extensions to Neural Networks (NNs) with non-Gaussian posteriors are discussed in appendices.\n\n## 1. Introduction\n- Highlights the significance of depth in modern ML methods (e.g., NNs, DGPs).\n- Discusses two families of infinite limits: Neural Network Gaussian Processes (NNGPs) and Neural Tangent Kernels (NTK).\n  - NNGP: Focuses on representation transformations across layers.\n  - NTK: Focuses on prediction changes from gradient updates.\n- The paper addresses representation learning challenges in the NNGP context for the first time.\n\n## 2. Contributions\n- Introduces the Bayesian representation learning limit, retaining representation learning in DGPs.\n- Proves DGP posteriors are exactly multivariate Gaussian:\n  - \\( P(f_\\lambda^l | X, y) = N(f_\\lambda^l; 0, G_l) \\)\n- Optimizes a new \"deep kernel machine objective\":\n  - \\( L(G_1, \\ldots, G_L) = \\log P(Y|G_L) - \\sum_{l=1}^L \\nu_l D_{KL}(N(0, G_l) \\| N(0, K(G_{l-1}))) \\)\n- Introduces sparse DKM yielding linear scaling with data points.\n- Extends results to Bayesian Neural Networks (BNNs) in Appendix A.\n\n## 3. Related Work\n- Compares the work with previous studies in NNGPs and representation learning.\n- Differentiates from NTK studies, emphasizing a focus on DGPs.\n- Notes contributions from physics methods and theoretical work establishing learning properties.\n\n## 4. Results\n### 4.1 DGP Definition\n- Defines DGP mapping from inputs \\( X \\in \\mathbb{R}^{P \\times \\nu_0} \\) to outputs \\( Y \\in \\mathbb{R}^{P \\times \\nu_{L+1}} \\).\n- Covariance structures for layers:\n  - \\( P(F_l|F_{l-1}) = \\prod_{\\lambda=1}^{N_l} N(f_\\lambda^l; 0, K(G(F_{l-1}))) \\)\n  - \\( P(Y|F_L) = \\prod_{\\lambda=1}^{\\nu_{L+1}} N(y_\\lambda; 0, K(G(F_L)) + \\sigma^2 I) \\)\n\n### 4.2 Infinite Width Limits\n- Standard infinite limits eliminate representation learning:\n  - Posterior converges to point distribution, losing flexibility.\n  \n### 4.3 Bayesian Representation Learning Limit\n- New limit maintains representation learning:\n  - \\( N_l = N \\nu_l \\) for \\( l \\in \\{1, \\ldots, L+1\\} \\).\n- Leads to a valid probabilistic model with well-defined posteriors.\n\n### 4.4 Exact DGP Posteriors\n- Under Bayesian limit, DGP posteriors are multivariate Gaussian:\n  - \\( P(f_\\lambda^l | X, y) = N(f_\\lambda^l; 0, G^*_l) \\)\n\n### 4.5 DKM Objective Interpretation\n- The DKM objective promotes good performance while regularizing posteriors to remain close to priors.\n\n### 4.6 Matching with Finite Networks\n- Optimizing DKM objective in infinite networks aligns with inference in wide but finite networks.\n- Empirical validation shows similar representations in finite DGPs and DKMs.\n\n### 4.7 Sparse DKMs\n- Introduces sparse DKMs scaling linearly in data points.\n- Experimental comparisons reveal superior performance of DKM over MAP and hyperparameter optimization baselines.\n\n## 5. Conclusion\n- The paper establishes a new infinite-width limit that preserves representation learning.\n- Introduces DKM as a practical deep kernel method with promising empirical results.\n- Highlights the extension potential for BNNs with non-Gaussian posteriors.\n\n## Experimental Methodology\n- Used Langevin sampling for BNNs and DGPs.\n- Optimized DKM objective via Adam optimizer with specified hyperparameters.\n- Utilized various datasets for empirical validation, including UCI datasets.\n- Implemented inducing points to enhance scalability in experiments.\n\n## Implications\n- Findings may reshape understanding of representation learning in deep learning.\n- Establishes a framework for flexible kernel methods that can be adapted for broader applications.\n\n## Specific Equations\n1. **Posterior Over Features**: \\( P(f_\\lambda^l | X, y) = N(f_\\lambda^l; 0, G_l) \\)\n2. **DKM Objective**: \\( L(G_1, \\ldots, G_L) = \\log P(Y|G_L) - \\sum_{l=1}^L \\nu_l D_{KL}(N(0, G_l) \\| N(0, K(G_{l-1}))) \\)\n3. **Exact Posterior Distribution**: \\( P(f_\\lambda^l | X, y) = N(f_\\lambda^l; 0, G^*_l) \\)\n\nThese notes encapsulate the essential aspects of the paper including theoretical foundations, experimental methodology, results, and implications derived from the findings."
      ]
    },
    {
      "context": "Under review as a conference paper at ICLR 2023\n\nDIP-GNN: DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph neural network (GNN) pre-training methods have been proposed to enhance the power of GNNs. Specifically, a GNN is first pre-trained on a large-scale unlabeled graph and then fine-tuned on a separate small labeled graph for downstream applications, such as node classification. One popular pre-training method is to mask out a proportion of the edges, and a GNN is trained to recover them. However, such a generative method suffers from graph mismatch. That is, the masked graph input to the GNN deviates from the original graph. To alleviate this issue, we propose DiP-GNN (Discriminative Pre-training of Graph Neural Networks). Specifically, we train a generator to recover identities of the masked edges, and simultaneously, we train a discriminator to distinguish the generated edges from the original graph\u2019s edges. The discriminator is subsequently used for downstream fine-tuning. In our pre-training framework, the graph seen by the discriminator better matches the original graph because the generator can recover a proportion of the masked edges. Extensive experiments on large-scale homogeneous and heterogeneous graphs demonstrate the effectiveness of the proposed framework. Our code will be publicly available.\n\n1\n\nINTRODUCTION\n\nGraph neural networks (GNNs) have achieved superior performance in various applications, such as node classification (Kipf & Welling, 2017), knowledge graph modeling (Schlichtkrull et al., 2018) and recommendation systems (Ying et al., 2018). To enhance the power of GNNs, generative pretraining methods are developed (Hu et al., 2020b). During the pre-training stage, a GNN incorporates topological information by training on a large-scale unlabeled graph in a self-supervised manner. Then, the pre-trained model is fine-tuned on a separate small labeled graph for downstream applications. Generative GNN pre-training is akin to masked language modeling in language model pre-training (Devlin et al., 2019). That is, for an input graph, we first randomly mask out a proportion of the edges, and then a GNN is trained to recover the original identity of the masked edges.\n\nOne major drawback with the abovementioned approach is graph mismatch. That is, the input graph to the GNN deviates from the original one since a considerable amount of edges are dropped. This causes changes in topological information, e.g., node connectivity. Consequently, the learned node embeddings may not be desirable.\n\nTo mitigate the above issues, we propose DiP-GNN ( Discriminative Pre-training of Graph Neural Networks). In DiP-GNN, we simultaneously train a generator and a discriminator. The generator is trained similar to existing generative pre-training approaches, where the model seeks to recover the masked edges and outputs a reconstructed graph. Subsequently, the reconstructed graph is fed to the discriminator, which predicts whether each edge resides in the original graph (i.e., a true edge) or is wrongly constructed by the generator (i.e., a fake edge). After pre-training, we fine-tune the discriminator on downstream tasks. Figure 1 illustrates our training framework. Note that our work is related to Generative Adversarial Nets (GAN, Goodfellow et al. 2014), and detailed discussions are presented in Section 3.4. We remark that similar approaches have been used in natural language processing (Clark et al., 2020). However, we identify the graph mismatch problem (see Section 4.5), which is specific to graph-related applications and is not observed in natural language processing.\n\nThe proposed framework is more advantageous than generative pre-training. This is because the reconstructed graph fed to the discriminator better matches the original graph compared with the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of DiP-GNN. From left to right: Original graph; Graph with two masked edges (dashed lines); Reconstructed graph created by the generator (generated edges are the dashed red lines); Discriminator labels each edge as [G] (generated) or [O] (original), where there are two wrong labels (shown in red).\n\nmasked graph fed to the generator. Consequently, the discriminator can learn better node embeddings. Such a better alignment is because the generator recovers the masked edges during pretraining, i.e., we observe that nearly 40% of the missing edges can be recovered. We remark that in our framework, the graph fed to the generator has missing edges, while the graph fed to the discriminator contains wrong edges since the generator may make erroneous predictions. However, empirically we find that missing edges hurt more than wrong ones, making discriminative pre-training more desirable (see Section 4.5 in the experiments).\n\nWe demonstrate effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs. Results show that the proposed method significantly outperforms existing generative pre-training and self-supervised learning approaches. For example, on the homogeneous Reddit dataset (Hamilton et al., 2017) that contains 230k nodes, we obtain an improvement of 1.1 in terms of F1 score; and on the heterogeneous OAG-CS graph (Tang et al., 2008) that contains 1.1M nodes, we obtain an improvement of 2.8 in terms of MRR score in the paper field prediction task.\n\n2 BACKGROUND\n\n\u22c4 Graph Neural Networks. Graph neural networks compute a node\u2019s representation by aggregating information from the node\u2019s neighbors. Concretely, for a multi-layer GNN, the feature vector h(k) of node v at the k-th layer is\n\nv\n\na(k)\n\nv = Aggregate\n\n(cid:16)(cid:110)\n\nh(k\u22121)\n\nu\n\n\u2200u \u2208 Neighbor(v)\n\n(cid:111)(cid:17)\n\n, h(k)\n\nv = Combine\n\n(cid:16)\n\nv , h(k\u22121) a(k)\n\nv\n\n(cid:17)\n\n,\n\nwhere Neighbor(v) denotes all the neighbor nodes of v. Various implementations of Aggregate(\u00b7) and Combine(\u00b7) are proposed for both homogeneous (Defferrard et al., 2016; Kipf & Welling, 2017; Velickovic et al., 2018; Xu et al., 2019) and heterogeneous graphs (Schlichtkrull et al., 2018; Wang et al., 2019; Zhang et al., 2019; Hu et al., 2020c).\n\n\u22c4 Graph Neural Network Pre-Training. Previous unsupervised learning methods leverage the graph\u2019s proximity (Tang et al., 2015) or information gathered by random walks (Perozzi et al., 2014; Grover & Leskovec, 2016; Dong et al., 2017; Qiu et al., 2018). However, the learned embeddings cannot be transferred to unseen nodes, limiting the methods\u2019 applicability. Other unsupervised learning algorithms adopt contrastive learning (Hassani & Ahmadi, 2020; Qiu et al., 2020; Zhu et al., 2020; 2021; You et al., 2020; 2021). That is, we generate two views of the same graph, and then maximize agreement of node presentations in the two views. However, our experiments reveal that these methods do not scale well to extremely large graphs with millions of nodes.\n\nMany GNN pre-training methods focus on generative objectives. For example, GAE (Graph AutoEncoder, Kipf & Welling 2016) proposes to reconstruct the graph structure; GraphSAGE (Hamilton et al., 2017) optimizes an unsupervised loss derived from a random-walk-based metric; and DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes the mutual information between node representations and a graph summary representation.\n\nThere are also pre-training methods that extract graph-level representations, i.e., models are trained on a large amount of small graphs instead of a single large graph. For example, Hu et al. 2020a propose pre-training methods that operate on both graph and node level; and InfoGraph (Sun et al., 2020) proposes to maximize the mutual information between graph representations and representations of the graphs\u2019 sub-structures. In this work, we focus on pre-training GNNs on a single large graph instead of multiple small graphs.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHOD\n\nWe formally introduce the proposed discriminative GNN pre-training framework DiP-GNN. The algorithm contains two ingredients that operate on edges and features.\n\n3.1 EDGE GENERATION AND DISCRIMINATION\n\nSuppose we have a graph G = (N , E), where N denotes all the nodes and E denotes all the edges. We randomly mask out a proportion of the edges, such that E = Eu \u222a Em, where Eu is the unmasked set of edges and Em is the set of edges that are masked out.\n\nFor a masked edge e = (n1, n2) \u2208 Em, where n1 and n2 are the two nodes connected by e, the generator\u2019s goal is to predict n1 given n2 and the unmasked edges Eu. For each node n, we compute its representation hg(n) = f e g), which is parameterized by \u03b8e g. We remark that the computation of hg(\u00b7) only relies on the unmasked edges Eu. We assume that the generation process of each edge is independent. Then, we have the prediction probability\n\ng) using the generator f e\n\ng (n, \u03b8e\n\ng (\u00b7, \u03b8e\n\np(n1|n2, Eu) =\n\nexp (d(hg(n1), hg(n2))) n\u2032\u2208C exp (d(hg(n\u2032), hg(n2)))\n\n(cid:80)\n\n, C = {n1} \u222a (N \\ Neighbor(n2)).\n\n(1)\n\nHere, C is the candidate set for n1, which contains all the nodes that are not connected to n2 except n1 itself. Moreover, the distance function d(\u00b7, \u00b7) is chosen as a trainable cosine similarity, i.e.,\n\nwhere W cos is a trainable weight. The training loss for the generator is defined as\n\nd(u, v) =\n\n(W cosu)\u22a4v ||W cosu|| \u00b7 ||v||\n\n,\n\ng) = (cid:80) which is equivalent to maximizing the likelihood of correct predictions.\n\n\u2212 log p(n1|n2, Eu),\n\n(n1,n2)\u2208Em\n\ng(\u03b8e\n\nLe\n\n(2)\n\n(3)\n\nThe goal of the generator is to recover the masked edges in Em. Therefore, after we train the generator, we use the trained model to generate Eg = {((cid:98)n1, n2)}(n1,n2)\u2208Em, where each (cid:98)n1 is the model\u2019s prediction as (cid:98)n1 = argmaxn\u2032\u2208C p(n\u2032|n2, Eu). Because the generator cannot correctly predict every edge, some edges in Eg are wrongly generated (i.e., not in Em). We refer to such edges as fake edges, and the rest as true edges. Concretely, we denote the true edges E true = Eu \u222a(Em \u2229Eg), i.e., the unmasked edges and the edges correctly generated by the generator. Correspondingly, we denote the fake edges E fake = E \\ E true.\n\nThe discriminator is trained to distinguish edges that are from the original graph (i.e., the true edges) and edges that are not (i.e., fake edges). Specifically, given the true edges E true and the fake ones E fake, we first compute hd(n) = f e d) is the discriminator model parameterized by \u03b8e d. We highlight that different from computing hg(\u00b7), the computation of hd(\u00b7) relies on all the edges, such that the discriminator can separate a fake edge from a true one. Then, for each edge e = (n1, n2) \u2208 E true \u222a E fake, the discriminator outputs\n\nd) for every node n \u2208 N , where f e\n\nd (n, \u03b8e\n\nd (\u00b7, \u03b8e\n\npfake = p(e \u2208 E fake|E true, E fake) = sigmoid (d(hd(n1), hd(n2))) , where d(\u00b7, \u00b7) is the distance function in Eq. 2. The training loss for the discriminator is the binary cross-entropy loss of predicting whether an edge is fake or not, defined as\n\n(4)\n\nLe\n\nd) = (cid:80) where 1{\u00b7} is the indicator function.\n\ne\u2208Etrue\u222aEfake\n\nd(\u03b8e\n\n\u22121{e \u2208 E fake} log(pfake) \u2212 1{e \u2208 E true} log(1 \u2212 pfake),\n\n(5)\n\nThe edge loss is the weighted sum of the generator\u2019s and the discriminator\u2019s loss g, \u03b8e\n\nLe(\u03b8e where \u03bb is a hyper-parameter. Note that structures of the generator f e d are flexible, e.g., they can be graph convolutional networks (GCN) or graph attention networks (GAT).\n\ng and the discriminator f e\n\ng) + \u03bbLe\n\nd) = Le\n\nd(\u03b8e\n\ng(\u03b8e\n\nd),\n\n(6)\n\n3.2 FEATURE GENERATION AND DISCRIMINATION\n\nIn real-world applications, nodes are often associated with features. For example, in the Reddit dataset (Hamilton et al., 2017), a node\u2019s feature is a vectorized representation of the post corresponding to the node. As another example, in citation networks (Tang et al., 2008), a paper\u2019s title\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ncan be treated as a node\u2019s feature. Previous work (Hu et al., 2020b) has demonstrated that generating features and edges simultaneously can improve the GNN\u2019s representation power.\n\nNode features can be either texts (e.g., in citation networks) or vectors (e.g., in recommendation systems). In this section, we develop feature generation and discrimination procedures for texts. Vector features are akin to encoded text features, and we can use linear layers to generate and discriminate them. Details about vector features are deferred to Appendix B.\n\ng (\u00b7, \u03b8f\n\ng ) = trmg \u25e6 embg(\u00b7) the generator parameterized by \u03b8f\n\nFor text features, we parameterize both the feature generator and the feature discriminator using bi-directional Transformer models (Vaswani et al., 2017), similar to BERT (Devlin et al., 2019). Denote f f g , where embg is the word embedding function and trmg denotes subsequent Transformer layers. For an input text feature x = [x1, \u00b7 \u00b7 \u00b7 , xL] where L is the sequence length, we randomly select indices to mask out, i.e., we randomly select an index set M \u2282 {1, \u00b7 \u00b7 \u00b7 , L}. For a masked position i \u2208 M, the prediction probability is given by\n\np(xi|x) =\n\nexp (cid:0)embg(xi)\u22a4vg(xi)(cid:1) x\u2032\u2208vocab exp (embg(x\u2032)\u22a4vg(x\u2032))\n\n(cid:80)\n\n, vg(xi) = trmg\n\n(cid:0)W proj\n\ng\n\n[hg(nx), embg(xi)](cid:1) . (7)\n\ng\n\nHere W proj is a trainable weight and hg(nx) is the representation of the node corresponding to x computed by the edge generation GNN. Note that we concatenate the text embedding embg(xi) and the feature node\u2019s embedding hg(nx), such that the feature generator can aggregate information from the graph structure. We train the generator by maximizing the probability of predicting the correct token, i.e., by minimizing the loss\n\nLf\n\ng (\u03b8e\n\ng, \u03b8f\n\ng ) = (cid:80)\n\nx\n\n(cid:80)\n\ni\u2208M \u2212 log p(xi|x).\n\n(8)\n\nAfter we train the generator, we use the trained model to predict all the masked tokens, after which we obtain a new text feature xcorr. Here, we set xcorr i = (cid:98)xi for i \u2208 M, where (cid:98)xi = argmaxx\u2032\u2208vocab p(xi|x) is the generator\u2019s prediction. The discriminator is trained to distinguish the fake tokens (i.e., wrongly generated tokens) from the true ones (i.e., the unmasked and correctly generated tokens) in xcorr. Similar to the generator, we denote f f d . For each position i, the discriminator\u2019s prediction probability is defined as\n\nd ) = trmd \u25e6 embd(\u00b7) as the discriminator parameterized by \u03b8f\n\ni = xi for i /\u2208 M and xcorr\n\nd (\u00b7, \u03b8f\n\np(xcorr\n\ni = xi) = sigmoid (cid:0)w\u22a4vd(xcorr\n\ni\n\n)(cid:1) , vd(xcorr\n\ni\n\n) = trmd\n\n(cid:16)\n\nW proj\n\nd\n\n[hd(nx), embd(xcorr\n\ni\n\n(cid:17)\n\n)]\n\n.\n\n(9)\n\nHere w and W proj to x computed by the edge discriminator GNN. The training loss for the discriminator is\n\nare trainable weights and hd(nx) is the representation of the node corresponding\n\nd\n\nLf where ptrue = p(xcorr\n\nd, \u03b8f\n\nd ) = (cid:80) i = xi) and 1{\u00b7} is the indicator function.\n\ni=1 \u22121{xcorr\n\ni = xi} log(ptrue) \u2212 1{xcorr\n\nd (\u03b8e\n\n(cid:80)L\n\nx\n\ni\n\n\u0338= xi} log(1 \u2212 ptrue),\n\n(10)\n\nThe text feature loss is defined as\n\nLf (\u03b8e\n\ng, \u03b8f\n\ng , \u03b8e\n\nd, \u03b8f\n\nd ) = Lf\n\ng (\u03b8e\n\ng, \u03b8f\n\ng ) + \u03bbLf\n\nd (\u03b8e\n\nd, \u03b8f\n\nd ),\n\nwhere \u03bb is a hyper-parameter.\n\n3.3 MODEL TRAINING\n\nWe jointly minimize the edge loss and the feature loss, where the loss function is\n\nL(\u03b8e\n\ng, \u03b8f\n\ng , \u03b8e\n\nd, \u03b8f\n\nd ) = Le(\u03b8e = (cid:0)Le g(\u03b8e\n\ng, \u03b8e\n\nd) + Lf (\u03b8e\n\ng) + Lf\n\ng (\u03b8e\n\ng, \u03b8f\n\nd, \u03b8f d ) (cid:16)\n\ng , \u03b8e g, \u03b8f g )(cid:1) + \u03bb\n\nLe\n\nd(\u03b8e\n\nd) + Lf\n\nd (\u03b8e\n\nd, \u03b8f d )\n\n(11)\n\n(cid:17)\n\n.\n\n(12)\n\nHere, \u03bb is the weight of the discriminator\u2019s loss. We remark that our framework is flexible because the generator\u2019s loss (Le d ). As such, existing generative pre-training methods can be applied to train the generator. In DiP-GNN, the discriminator has a better quality than the generator because of the graph mismatch issue (see Section 4.5). Therefore, after pre-training, we discard the generator and fine-tune the discriminator on downstream tasks. A detailed training pipeline is presented in Appendix A.\n\ng ) is decoupled from the discriminator\u2019s (Le\n\nd and Lf\n\ng and Lf\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.4 COMPARISON WITH GAN\n\nWe remark that our framework is different from Generative Adversarial Nets (GAN, Goodfellow et al. 2014). In GAN, the generator-discriminator training framework is formulated as a min-max game, where the generator is trained adversarially to fool the discriminator. The two models are updated using alternating gradient descent/ascent.\n\nHowever, the min-max game formulation of GAN is not applicable to our framework. This is because in GNN pre-trianing, the generator generates discrete edges, unlike continuous pixel values in the image domain. Such a property prohibits back-propagation from the discriminator to the generator. Existing works (Wang et al., 2018) use reinforcement learning (specifically policy gradient) to circumvent the non-differentiability issue. However, reinforcement learning introduces extensive hyper-parameter tuning and suffers from scalability issues. For example, the largest graph used in Wang et al. 2018 only contains 18k nodes, whereas the smallest graph used in our experiments has about 233k nodes.\n\nAdditionally, the goal of GAN is to train good-quality generators, which is different from our focus. In our discriminative pre-training framework, we focus on the discriminator because of better graph alignments. In practice, we find that accuracy of the generator is already high even without the discriminator, e.g., the accuracy is higher than 40% with 255 negative samples. And we observe that further improving the generator does not benefit downstream tasks.\n\n4 EXPERIMENTS\n\nWe implement all the algorithms using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019). Experiments are conducted on NVIDIA A100 GPUs. By default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. We also discuss other choices in the experiments. Training and implementation details are deferred to Appendix C.\n\n4.1 SETTINGS AND DATASETS\n\n\u22c4 Settings. We consider a node transfer setting in the experiments. In practice we often work with a single large-scale graph, on which labels are sparse. In this case, we can use the large amount of unlabeled data as the pre-training dataset, and the rest are treated as labeled fine-tuning nodes. Correspondingly, edges between pre-training nodes are added to the pre-training data, and edges between fine-tuning nodes are added to the fine-tuning data. In this way, the model cannot see the fine-tuning data during pre-training, and vice versa.\n\nWe remark that our setting is different from conventional self-supervised learning settings, namely we pre-train and fine-tune on two separate graphs. This meets the practical need of transfer learning, e.g., a trained GNN needs to transfer across locales and time spans in recommendation systems.\n\n\u22c4 Homogeneous Graph. We use the Reddit dataset (Hamilton et al., 2017), which is a publicly available large-scale graph. In this graph, each node corresponds to a post, and is labeled with a \u201csubreddit\u201d. Each node has a 603-dimensional feature vector constructed from the corresponding post. Two nodes (posts) are connected if the same user commented on both. The dataset contains posts from 50 subreddits sampled from posts initiated in September 2014. In total, there are 232,965 posts with an average node degree of 492. We use 70% of the data as the pre-training data, and the rest as the fine-tuning data, which are further split into training, validation, and test sets equally. We consider node classification as the downstream fine-tuning task.\n\n\u22c4 Product Recommendation Graph. We collect in-house product recommendation data from an e-commerce website. We build a bi-partite graph with two node types: search queries and product ids. The dataset contains about 633k query nodes, 2.71M product nodes, and 228M edges. We sample 70% of the nodes (and corresponding edges) for pre-training, and the rest are evenly split for fine-tuning training, validation and testing. We consider link prediction as the downstream task, where for each validation and test query node, we randomly mask out 20% of its edges to recover. For each masked edge that corresponds to a query node and a positive product node, we randomly sample 255 negative products. The task is to find the positive product out of the total 256 products.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n\u22c4 Heterogeneous Graph. We use the OAG-CS dataset (Tang et al., 2008; Sinha et al., 2015), which is a publicly available heterogeneous graph containing computer science papers. The dataset contains over 1.1M nodes and 28.4M edges. In this graph, there are five node types (institute, author, venue, paper and field) and ten edge types. The \u201cfield\u201d nodes are further categorized into six levels from L0 to L5, which are organized using a hierarchical tree. Details are shown in Figure 2.\n\nWe use papers published before 2014 as the pre-training dataset (63%), papers published between 2014 (inclusive) and 2016 (inclusive) as the fine-tuning training set (20%), papers published in 2017 as the fine-tuning validation set (7%), and papers published after 2017 as the fine-tuning test set (10%). During fine-tuning, by default we only use 10% of the fine-tuning training data (i.e., 2% of the overall data) because in practice labeled data are often scarce. We consider three tasks for fine-tuning: author name disambiguation (AD), paper field classification (PF) and paper venue classification (PV). For paper field classification, we only consider L2 fields. processed graph from Hu et al. 2020b.\n\nFigure 2: Details of OAG-CS. There are 5 node types (in black) and 10 edge types (in red).\n\nIn the experiments, we use the pre-\n\n4.2\n\nIMPLEMENTATION DETAILS\n\n\u22c4 Graph subsampling. In practice, graphs are often too large to fit in the hardware, e.g., the Reddit dataset (Hamilton et al., 2017) contains over 230k nodes. Therefore, we sample a dense subgraph from the large-scale graph in each training iteration. For homogeneous graphs, we apply the LADIES algorithm (Zou et al., 2019), which theoretically guarantees that the sampled nodes are highly inter-connected with each other and can maximally preserve the graph structure. For heterogeneous graphs, we use the HGSampling algorithm (Hu et al., 2020b), which is a heterogeneous version of LADIES.\n\nIn the edge generator, for a masked edge (s, t), we fix \u22c4 Node sampling for the edge generator. the node t and seek to identify the other node s. One approach is to identify s from all the graph nodes, i.e., by setting C = N in Eq. 1. However, this task is computationally intractable when the number of nodes is large, i.e., the model needs to find s out of hundreds of thousands of nodes. i }nneg Therefore, we sample some negative nodes {sg i , t) /\u2208 E. Then, the candidate set to generate the source node becomes {s, sg } instead of all the graph nodes N . We remark nneg that such a sampling approach is standard for GNN pre-training and link prediction (Hamilton et al., 2017; Sun et al., 2020; Hu et al., 2020b).\n\ni=1 such that (sg\n\n1, \u00b7 \u00b7 \u00b7 , sg\n\n\u22c4 Edge sampling for the edge discriminator. In computing the loss for the discriminator, the number of edges in Eu is significantly larger than those in Eg, i.e., we only mask a small proportion of the edges. To avoid the discriminator from outputting trivial predictions (i.e., all the edges belong to Eu), we balance the two loss terms in Le u| = \u03b1|Eg|, where \u03b1 is a hyper-parameter. Then, we compute Le u. Note that the node representations hd are still computed using all the generated and unmasked edges Eg and Eu.\n\nd. Specifically, we sample E d\n\nu \u2282 Eu such that |E d\n\nd on Eg and E d\n\n4.3 BASELINES\n\nWe compare our method with several baselines in the experiments. For fair comparison, all the methods are trained for the same number of GPU hours.\n\n\u22c4 GAE (Graph Auto-Encoder, Kipf & Welling 2016) adopts an auto-encoder for unsupervised learning on graphs. In GAE, node embeddings are learnt using a GNN, and we minimize the discrepancy between the original and the reconstructed adjacency matrix.\n\n\u22c4 GraphSAGE (Hamilton et al., 2017) encourages embeddings of neighboring nodes to be similar. For each node, the method learns a function that generates embeddings by sampling and aggregating features from the node\u2019s neighbors.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Experimental results on homogeneous graphs. We report F1 averaged over 10 runs for the Reddit data and MRR over 10 runs for the product recommendation data. The best results are shown in bold.\n\nTable 2: Experimental results on OAG-CS (heterogeneous). Left to right: paper-field, paper-venue, author-name-disambiguation. We report MRR over 10 runs. The best results are shown in bold.\n\nReddit Recomm.\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\n87.3\n\n88.5 88.0 87.7 89.6 89.0 88.6 89.1\n\n90.7\n\n46.3\n\n56.7 53.0 53.3 58.6 51.5 \u2014\n\u2014\n\n60.1\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\nPF\n\n32.7\n\n40.3 37.8 38.1 41.6 38.0 38.0 38.6\n\n44.1\n\nPV\n\n19.6\n\n24.5 22.1 22.5 25.6 21.5 22.0 23.5\n\n27.7\n\nAD\n\n60.0\n\n62.5 62.9 63.0 63.1 62.0 61.5 62.8\n\n65.6\n\n(a) Author name disambiguation.\n\n(b) Paper field classification.\n\n(c) Paper venue classification.\n\nFigure 3: Model performance vs. amount of labeled data on OAG-CS.\n\n\u22c4 DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes mutual information between node representations and corresponding high-level summaries of graphs. Thus, a node\u2019s embedding summarizes a sub-graph centered around it.\n\n\u22c4 GPT-GNN (Hu et al., 2020b) adopts a generative pre-training objective. The method generates edges by minimizing a link prediction objective, and incorporates node features in the framework.\n\n\u22c4 GRACE (Graph Contrastive Representation, Zhu et al. 2020) leverages a contrastive objective. The algorithm generates two views of the same graph through node and feature corruption, and then maximize agreement of node representations in the two views.\n\n\u22c4 GraphCL (You et al., 2020) is another graph contrastive learning approach that adopts node and edge augmentation techniques, such as node dropping and edge perturbation.\n\n\u22c4 JOAO (Joint Augmentation Optimization, You et al. 2021) improves GraphCL by deigning a bi-level optimization objective to automatically and dynamically selects augmentation methods.\n\n4.4 MAIN RESULTS\n\nIn Table 1 and Table 2, w/o pre-train means direct training on the fine-tuning dataset without pretraining. Results on the Reddit dataset are F1 scores averaged over 10 runs, and results on the product recommendation graph are MRR scores averaged over 10 runs. All the performance gain have passed a hypothesis test with p-value < 0.05.\n\nTable 1 summarizes experimental results on the homogeneous graphs: Reddit and Recommendation. We see that pre-training indeed benefits downstream tasks. For example, performance of GNN improves by at \u2265 0.4 F1 on Reddit (DGI) and \u2265 5.2 MRR on Recommendation (GRACE). Also, notice that among the baselines, generative approaches (GAE and GPT-GNN) yield promising per-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Neg. nodes for generation.\n\n(b) Pos. edges for discrimination.\n\n(c) Weight of discriminator\u2019s loss.\n\nFigure 4: Ablation experiments on Reddit. By default, we set the number of negative nodes to 256, the factor of positive edges to 1.0, and weight of the discriminator\u2019s loss to 20.\n\nTable 3: Test F1 score of model variants on Reddit.\n\nTable 4: Test F1 of models with different backbone graph neural networks on Reddit.\n\nModel\n\nEdges+Features\n\nEdges Features RandomEdges\n\nF1\n\n90.7\n\n90.4 90.2 89.8\n\nModel\n\nHGT GAT\n\nw/o pretrain\n\n87.3\n\n86.4\n\nGPT-GNN DiP-GNN\n\n89.6 90.7\n\n87.5 88.5\n\nFigure 5: F1 vs. proportion of manipulated edges on Reddit.\n\nformance. On the other hand, the contrastive method (GRACE, GraphCL and JOAO) does not scale well to large graphs, e.g., the OAG-CS graph which contains 1.1M nodes and 28.4M edges. By using the proposed discriminative pre-training framework, our method significantly outperforms all the baseline approaches. For example, DiP-GNN outperforms GPT-GNN by 1.1 on Reddit and 1.5 on Recommendation.\n\nExperimental results on the heterogeneous OAG-CS dataset are summarized in Table 2. Similar to the homogeneous graphs, notice that pre-training improves model performance by large margins. For example, pre-training improves MRR by at least 5.1, 2.5 and 2.5 on the PF, PV and AD tasks, respectively. Moreover, by using the proposed training framework, models can learn better node embeddings and yield consistently better performance compared with all the baselines.\n\nRecall that during fine-tuning on OAG-CS, we only use 10% of the labeled fine-tuning data (about 2% of the overall data). In Figure 3, we examine the effect of the amount of labeled data. We see that model performance improves when we increase the amount of labeled data. Also, notice that DiP-GNN consistently outperforms GPT-GNN in all the three tasks under all the settings.\n\n4.5 ANALYSIS\n\n\u22c4 Comparison with semi-supervised learning. We compare DiP-GNN with a semi-supervised learning method: C&S (Correct&Smooth, Huang et al. 2020). Figure 6 summarizes the results. We see that C&S yields a 0.5 improvement compared with the supervised learning method (i.e., w/o pre-train). However, performance of C&S is significantly lower than both DiP-GNN and other pre-training methods such as GPT-GNN.\n\n\u22c4 Hyper-parameters. There are several hyper-parameters that we introduce in DiP-GNN: the number of negative nodes that are sampled for generating edges (Section 4.2); the number of positive edges that are sampled for the discriminator\u2019s task (Section 4.2); and the weight of the discriminator\u2019s loss (Eq. 12). Figure 4 illustrate ablation experimental results on the Reddit dataset. From the results, we see that DiP-GNN is robust to these hyper-parameters. We remark that under all the settings, ours model behaves better than the best-performing baseline (89.6 for GPT-GNN).\n\nFigure 6: Test F1 on Reddit.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Generator and discriminator performance vs. proportion of masked edges during pre-training. Coverage is the proportion of true edges input to the models.\n\nMasked%\n\nAcc\n\nCoverage\n\nGen. Dis. Gen. Dis.\n\nRatio\n\n20 80 95\n\n0.50 0.33 0.20\n\n0.87 0.84 0.80\n\n0.80 0.20 0.05\n\n0.90 \u00d71.13 0.46 \u00d72.30 0.24 \u00d74.80\n\nFigure 7: Performance vs. proportion of masked edges on product recommendation.\n\n\u22c4 Model variants. We also examine variants of DiP-GNN. Recall that the generator and the discriminator operate on both edges and node features. We first check the contribution of these two factors. We also investigate the scenario where edges are randomly generated, and the discriminator still seeks to find the generated edges. Table 3 summarizes results on the Reddit dataset.\n\nWe see that by only using edges, model performance drops by 0.3; and by only using node features, performance drops by 0.5. This indicates that the graph structure plays a more important role in the proposed framework than the features. Also notice that performance of RandomEdges is unsatisfactory. This is because implausible edges are generated when using a random generator, making the discriminator\u2019s task significantly easier. We remark that performance of all the model variants is better than the best-performing baseline (89.6 for GPT-GNN).\n\nTable 4 examines performance of our method and GPT-GNN using different backbone GNNs. Recall that by default, we use HGT (Hu et al., 2020c) as the backbone. We see that when GAT (Velickovic et al., 2018) is used, performance of DiP-GNN is still significantly better than GPT-GNN.\n\n\u22c4 Missing edges hurt more than wrong edges. In our pre-training framework, the generator is trained to reconstruct the masked graph, after which the reconstructed graph is fed to the discriminator. During this procedure, the graph input to the generator has missing edges, and the graph input to the discriminator has wrong edges. From Figure 5, we see that wrong edges hurt less than missing ones. For example, model performance drops by 0.7% when 50% of wrong edges are added to the original graph, and performance decreases by 1.8% when 50% of original edges are missing. This indicates that performance relies on the amount of original edges seen by the models. Intuitively, wrong edges add noise to the graph, but they do not affect information flow. On the contrary, missing edges cut information flow. Moreover, in practice we work with graph attention models, and the attention mechanism can alleviate the wrong edges by assigning low attention scores to them.\n\n\u22c4 Why is discriminative pre-training better? Figure 7 illustrates effect of the proportion of masked edges during pre-training. We see that when we increase the proportion from 0.2 to 0.8, performance of GPT-GNN drops by 6.1, whereas performance of DiP-GNN only drops by 3.3. This indicates that the generative pre-training method is more sensitive to the masking proportion.\n\nTable 5 summarizes pre-training quality. First, the generative task (i.e., the generator) is more difficult than the discriminative task (i.e., the discriminator). For example, when we increase the proportion of masked edges from 20% to 80%, accuracy of the generator drops by 17% while accuracy of the discriminator only decreases by 3%. Second, the graph input to the discriminator better aligns with the original graph. For example, when 80% of the edges are masked, the discriminator sees 2.3 times more original edges than the generator. Therefore, the discriminative task is more advantageous because model quality relies on the number of observed original edges (Figure 5).\n\n5 CONCLUSION AND DISCUSSIONS\n\nWe propose Discriminative Pre-Training of Graph Neural Networks (DiP-GNN), where we simultaneously train a generator and a discriminator. During pre-training, we mask out some edges in the graph, and a generator is trained to recover the masked edges. Subsequently, a discriminator seeks to distinguish the generated edges from the original ones. Our framework is more advantageous than generative pre-training for two reasons: the graph inputted to the discriminator better matches the original graph; and the discriminative pre-training task better aligns with downstream node classification. We conduct extensive experiments to validate the effectiveness of DiP-GNN.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nMicha \u0308el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 510, 2016, Barcelona, Spain, pp. 3837\u20133845, 2016.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n\nYuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pp. 135\u2013144. ACM, 2017. doi: 10.1145/3097983.3098036.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672\u20132680, 2014.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.\n\nIn Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 855\u2013864. ACM, 2016. doi: 10.1145/2939672.2939754.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024\u20131034, 2017.\n\nKaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 4116\u20134126. PMLR, 2020.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure In 8th International Conference Leskovec. Strategies for pre-training graph neural networks. on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020a.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: generative pre-training of graph neural networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1857\u20131867. ACM, 2020b.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen (eds.), WWW \u201920: The Web\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nConference 2020, Taipei, Taiwan, April 20-24, 2020, pp. 2704\u20132710. ACM / IW3C2, 2020c. doi: 10.1145/3366423.3380027.\n\nQian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining arXiv preprint\n\nlabel propagation and simple models out-performs graph neural networks. arXiv:2010.13993, 2020.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K \u0308opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch \u0301e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024\u20138035, 2019.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In Sofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani (eds.), The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, New York, NY, USA - August 24 - 27, 2014, pp. 701\u2013710. ACM, 2014. doi: 10.1145/2623330.2623732.\n\nJiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Yi Chang, Chengxiang Zhai, Yan Liu, and Yoelle Maarek (eds.), Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, pp. 459\u2013467. ACM, 2018. doi: 10.1145/3159652.3159706.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1150\u20131160. ACM, 2020.\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593\u2013607. Springer, 2018.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pp. 243\u2013246, 2015.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale information network embedding. In Aldo Gangemi, Stefano Leonardi, and Alessandro Panconesi (eds.), Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015, pp. 1067\u20131077. ACM, 2015. doi: 10.1145/2736277.2741093.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and In Proceedings of the 14th ACM SIGKDD international\n\nmining of academic social networks. conference on Knowledge discovery and data mining, pp. 990\u2013998, 2008.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998\u20136008, 2017.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nPetar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nHongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2508\u20132515. AAAI Press, 2018.\n\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S. Yu. Heterogeneous graph attention network. In Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia (eds.), The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pp. 2022\u20132032. ACM, 2019. doi: 10.1145/ 3308558.3313562.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Yike Guo and Faisal Farooq (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pp. 974\u2013983. ACM, 2018. doi: 10.1145/3219819.3219890.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812\u20135823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. In International Conference on Machine Learning, pp. 12121\u201312132. PMLR, 2021.\n\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. Heterogeneous graph neural network. In Ankur Teredesai, Vipin Kumar, Ying Li, R \u0301omer Rosales, Evimaria Terzi, and George Karypis (eds.), Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 793\u2013803. ACM, 2019. doi: 10.1145/3292500.3330961.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069\u20132080, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDifan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch \u0301e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11247\u201311256, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA DETAILED ALGORITHM\n\nAlgorithm 1 is a detailed training pipeline of DiP-GNN. For graphs with vector features instead of text features, we can substitute the feature generation and discrimination modules with equations in Appendix B.\n\nAlgorithm 1: DiP-GNN: Discriminative Pre-training of Graph Neural Networks. Input: Graph Gfull; edge masking ratio; feature masking ratio; number of negative samples for edge generator; proportion of positive samples for edge discriminator \u03b1; weight of the discriminator\u2019s loss \u03bb; number of training steps T .\n\nfor t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do\n\n// Graph subsampling. Sample a subgraph G = (N , E) from Gfull; // Edge generation. Initialize the generated edge set Eg = {} and the edge generation loss Le Construct the unmasked set of edges Eu and the masked set Em such that E = Eu \u222a Em; Compute node embeddings using Eu; for e = (n1, n2) \u2208 Em do\n\ng = 0;\n\nConstruct candidate set C for n1 (n2 is given during generation) via negative sampling; Generate (cid:98)e = ((cid:98)n1, n2) where (cid:98)n1 \u2208 C; Update the generated edge set Eg \u2190 Eg \u222a {(cid:98)e}; Update the edge generation loss Le g; // Text Feature generation. Initialize the feature generation loss Lf for n \u2208 N do\n\ng = 0;\n\nFor the node\u2019s text feature xn, mask out some of its tokens; Construct the generated text feature xcorr\n\nn\n\nusing the embedding of node n (computed\n\nduring edge generation) and the feature generation Transformer model; Update the feature generation loss Lf g ;\n\n// Edge discrimination. Initialize the edge discrimination loss Le Compute node embeddings using Eg \u222a Eu; Sample E d for e = (n1, n2) \u2208 Eg \u222a E d\n\nu \u2282 Eu such that |E d u do\n\nu| = \u03b1|Eg|;\n\nd = 0;\n\nDetermine if e is generated using the embedding of n1 and n2; Update the edge discrimination loss Le d; // Text feature discrimination. Initialize the feature discrimination loss Lf for n \u2208 N do\n\nd = 0;\n\nFor the node\u2019s generated text feature xcorr\n\nn , determine whether each token is generated\n\nusing the embedding of node n (computed during edge discrimination) and the feature discrimination Transformer model; Update the feature discrimination loss Lf d ;\n\n// Model updates. g + Lf Compute L = (Le\n\ng ) + \u03bb(Le\n\nd + Lf\n\nd ) and update the model;\n\nOutput: Trained discriminator ready for fine-tuning.\n\nB GENERATION AND DISCRIMINATION OF VECTOR FEATURES\n\nNode features can be vectors instead of texts, e.g., the feature vector can contain topological information such as connectivity information. In this case both the generator and the discriminator are parameterized by a linear layer.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTo generate feature vectors, we first randomly select some nodes Ng \u2282 N . For a node n \u2208 N , denote its feature vector vn, then the feature generation loss is\n\nLf\n\ng (Wg) =\n\n(cid:88)\n\nn\u2208Ng\n\n||(cid:98)vn \u2212 vn||2\n\n2 , where (cid:98)vn = W f\n\ng hg(n).\n\nHere hg(n) is the representation of node n and W f construct its corred feature vcorr\n\nn = (cid:98)vn if n \u2208 Ng and vcorr\n\ng is a trainable weight. For a node n \u2208 N , we\n\nn = vn if n \u2208 N \\ Ng.\n\nThe discriminator\u2019s goal is to differentiate the generated features from the original ones. Specifically, the prediction probability is\n\np(n \u2208 Ng) = sigmoid (cid:0)W d\n\nd hd(n)(cid:1) ,\n\nwhere W f on the corred feature vcorr\n\nd is a trainable weight. We remark that the node representation hd(n) is computed based\n\nn . Correspondingly, the discriminator\u2019s loss is\n\nLf\n\nd (Wd) =\n\n(cid:88)\n\nn\u2208N\n\n\u22121{n \u2208 Ng} log p(n \u2208 Ng) \u2212 1{n \u2208 N \\ Ng} log(1 \u2212 p(n \u2208 Ng)).\n\nThe vector feature loss Lf (\u03b8e the text feature loss.\n\ng, W f\n\ng , \u03b8e\n\nd, W f\n\nd ) = Lf\n\ng (\u03b8e\n\ng, W f\n\ng ) + Lf\n\nd (\u03b8e\n\nd, W f\n\nd ) is computed similar to\n\nC IMPLEMENTATION AND TRAINING DETAILS\n\nBy default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. In the experiments, the edge generator and discriminator have the same architecture, where we set the hidden dimension to 400, the number of layers to 3, and the number of attention heads to 8. For the OAG dataset which contains text features, the feature generator and discriminator employs the same architecture: a 4 layer bi-directional Transformer model, similar to BERT (Devlin et al., 2019), where we set the embedding dimension to 128 and the hidden dimension of the feed-forward neural network to 512.\n\nFor pre-training, we mask out 20% of the edges and 20% of the features (for text features we mask out 20% of the tokens). We use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set \u03b2 = (0.9, 0.999), \u03b5 = 10\u22128, the learning rate to 0.001 and the weight decay to 0.01. We adopt a dropout ratio of 0.2 and gradient norm clipping of 0.5. For graph subsampling, we set the depth to 6 and width to 128, the same setting as Hu et al. 2020b.\n\nFor fine-tuning, we use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set \u03b2 = (0.9, 0.999), \u03b5 = 10\u22126, and we do not use weight decay. We use the same graph subsampling setting as pre-training. The other hyper-parameters are detailed in Table 6.\n\nTable 6: Hyper-parameters for fine-tuning tasks.\n\nDataset\n\nTask\n\nSteps Dropout Learning rate Gradient clipping\n\nReddit\n\n\u2014 2400\n\nRecomm. \u2014 1600\n\nOAG-CS\n\nPF PV AD\n\n1600 1600 1600\n\n0.3\n\n0.1\n\n0.2 0.2 0.2\n\n0.0015\n\n0.0010\n\n0.0010 0.0005 0.0005\n\n0.5\n\n0.5\n\n0.5 0.5 0.5\n\n15",
      "responses": [
        "# Summary Of The Paper\n\nThis paper proposes a discriminative method for pre-training Graph Neural Networks. The main idea is to simultaneously train a generator to recover identities of the masked edges, and train a discriminator to distinguish the generated edges from the original graph\u2019s edges.\n\n# Strength And Weaknesses\n\nStrength\n- The proposed method makes sense.\n- The empirical results show the improved performance brought by the proposed methods on a number of  benchmarks.\n\nWeaknesses\n- There is no mentioning of n_1^hat or generated edges e_g in the loss of the generator. How should I understand that the generated edges are not captured in the generator loss?\n\n- Also, it is said that the discriminator is to distinguish edges that are from the original graph and edges that are generated. It would make sense to contrast the masked edges vs the generated edges, especially the ones that are different. However, the training loss for the discriminator seems to still focus on the unmasked edges vs generated edges. I don\u2019t quite understand the intuition.\n\n- A masked edge has two end nodes n_1 and n_2. How do the authors decide which end node to predict?\n\n- Why is the same lambda shared between edge loss and feature loss, given these two losses are so different?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is unclear in various aspects, as explained in above comments.\n\n# Summary Of The Review\n\nOverall, the proposed method makes sense. However, I feel the authors are selling the methodology wrong. Instead of saying that discriminative pre-training is better than generative training, the authors should present it as a method for applying generative and discriminative pre-training jointly. Otherwise, why don\u2019t you just rely solely on discriminative pre-training?\n\n# Correctness\n\n3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
        "# Summary Of The Paper\nThe paper presents DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel approach designed to improve the performance of Graph Neural Networks (GNNs) through a two-step process: pre-training on large-scale unlabeled graphs and fine-tuning on smaller labeled graphs. The authors identify the limitations of existing generative pre-training methods, particularly their tendency to introduce graph mismatch due to edge masking. DiP-GNN employs a generator-discriminator framework where the generator predicts masked edges and the discriminator distinguishes between true and generated edges. Extensive experiments across various datasets demonstrate that DiP-GNN significantly outperforms multiple baseline methods in tasks such as node classification and multi-task classification, indicating its effectiveness in addressing the inherent issues in traditional pre-training approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to pre-training GNNs, which effectively mitigates the graph mismatch challenge present in prior methods. The use of a generator-discriminator mechanism not only enhances the quality of node embeddings but also shows promise across both homogeneous and heterogeneous graphs. Furthermore, the comprehensive experimental evaluation across diverse datasets reinforces the robustness of the proposed method. However, a notable weakness is the lack of a detailed theoretical analysis of the proposed approach, which could provide deeper insights into its underlying mechanisms. Additionally, while the empirical results are compelling, it would be beneficial to include a broader range of datasets to assess the generalizability of DiP-GNN further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The writing quality is high, with a logical flow that guides the reader through the proposed method and its evaluation. The novelty of the approach is significant, as it introduces a new angle to the pre-training of GNNs that contrasts sharply with existing methods. The reproducibility is bolstered by the inclusion of detailed implementation specifics, including hyper-parameters and experimental setup, allowing other researchers to replicate the results effectively.\n\n# Summary Of The Review\nDiP-GNN presents a compelling and novel framework for enhancing GNN performance through a discriminative pre-training approach. While the empirical results are impressive, further theoretical analysis and broader dataset evaluation could strengthen the contribution. Overall, the paper makes a significant addition to the literature on GNNs and pre-training methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel approach that addresses the limitations of traditional generative pre-training methods in graph neural networks (GNNs). The methodology involves training a generator to recover masked edges and a discriminator to distinguish between true and generated edges, aimed at improving the representation of the original graph during pre-training. The findings demonstrate that DiP-GNN significantly outperforms existing GNN models across various datasets and tasks, showcasing enhanced node embeddings and robustness to variations in labeled data.\n\n# Strength And Weaknesses\nThe strengths of DiP-GNN lie in its innovative approach to overcoming graph mismatch issues, leading to improved performance in downstream tasks. The method's flexibility allows for the integration of different GNN architectures and node features, which broadens its applicability. However, the paper also highlights limitations, such as the potential complexity of the generative task, sensitivity to hyper-parameter tuning, and scalability concerns, particularly with very large datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and experimental design, making it accessible to readers familiar with GNNs. The quality of the experiments is high, with a thorough evaluation against multiple baselines and robust metrics. The novelty of the discriminative pre-training approach is significant, although the paper could benefit from additional discussions on reproducibility, particularly concerning the tuning of hyper-parameters and computational resources required for large-scale applications.\n\n# Summary Of The Review\nOverall, DiP-GNN represents a significant advancement in the pre-training of GNNs by effectively addressing graph mismatch issues through a dual generator-discriminator framework. While the method demonstrates impressive empirical performance and flexibility, further exploration of hyper-parameter sensitivity and computational efficiency is recommended.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n5/5",
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel approach to enhance the performance of Graph Neural Networks (GNNs) through discriminative pre-training methods. The methodology involves training a generator to recover masked edges in graphs and a discriminator to differentiate between original and generated edges, addressing the issue of \"graph mismatch\" that hampers previous generative pre-training techniques. Extensive experiments demonstrate that DiP-GNN outperforms existing baselines across various large-scale homogeneous and heterogeneous graph datasets, indicating its effectiveness in improving GNN embeddings.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to pre-training GNNs, which effectively mitigates the graph mismatch problem. The combination of edge and feature generation, along with robust experimental results, highlights the practical applicability of DiP-GNN. However, a potential weakness is the lack of extensive exploration of the limitations of the proposed method, such as how it might perform on graphs with different structural properties or in scenarios with limited labeled data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, making it accessible for readers familiar with GNNs and pre-training techniques. The quality of the experimental design is commendable, with a variety of datasets and comparison with multiple baselines. However, some technical details, such as hyper-parameter tuning and implementation specifics, could be elaborated further to enhance reproducibility. Overall, the novelty of the approach is significant given the specific focus on graph structures and the unique training pipeline presented.\n\n# Summary Of The Review\nOverall, the paper makes a compelling contribution to the field of GNNs by introducing a discriminative pre-training method that effectively addresses the graph mismatch issue. The experimental results provide strong evidence for the efficacy of DiP-GNN, though further exploration of its limitations would strengthen the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5"
      ],
      "preparations": [
        null,
        "**Paper Title**: DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n**Authors**: Anonymous (Under double-blind review)\n\n---\n\n### Abstract\n- **Objective**: To enhance GNNs through pre-training on large-scale unlabeled graphs followed by fine-tuning on smaller labeled graphs.\n- **Issue with Current Methods**: Generative pre-training methods mask edges but suffer from graph mismatch, leading to suboptimal node embeddings.\n- **Proposed Method**: DiP-GNN (Discriminative Pre-training of Graph Neural Networks) trains a generator to recover masked edges and a discriminator to distinguish generated edges from original edges.\n- **Outcome**: Demonstrated effectiveness on both homogeneous and heterogeneous graphs, outperforming existing methods.\n\n### Introduction\n- GNNs are effective for tasks like node classification and knowledge graph modeling.\n- Traditional generative pre-training methods involve masking edges and training GNNs to recover them, which introduces graph mismatch.\n- DiP-GNN addresses this by using a generator-discriminator approach to improve the input quality seen by the discriminator and enhance downstream task performance.\n\n### Background\n- **Graph Neural Networks (GNNs)**: Nodes' representations are computed by aggregating neighboring nodes' information.\n- **Pre-Training Methods**: Previous unsupervised methods struggle with unseen nodes and scalability. Generative methods (like GAE, GraphSAGE, DGI) focus on reconstructing graphs or maximizing mutual information.\n- DiP-GNN focuses solely on pre-training GNNs on a single large graph.\n\n### Method\n1. **Edge Generation and Discrimination**:\n   - Input graph \\( G = (N, E) \\) where \\( N \\) is nodes and \\( E \\) is edges.\n   - Mask a proportion of edges to create \\( E = E_u \\cup E_m \\) (unmasked and masked edges).\n   - The generator predicts masked edges based on available information, while the discriminator distinguishes true edges from fake edges.\n   - Loss functions for both generator and discriminator are defined (binary cross-entropy for discriminator).\n\n2. **Feature Generation and Discrimination**:\n   - Features can be text or vector representations.\n   - For text features, a bi-directional Transformer model predicts masked tokens based on node embeddings.\n   - Similar loss functions are defined for feature generation and discrimination.\n\n3. **Model Training**:\n   - Jointly minimize edge and feature losses.\n   - Use a flexible architecture to allow various GNN types as generators/discriminators.\n\n4. **Comparison with GAN**:\n   - DiP-GNN's framework differs from GANs because it does not use a min-max game due to discrete edge generation. Instead, it focuses on enhancing the discriminator.\n\n### Experiments\n- **Implementation**: Conducted using PyTorch and PyTorch Geometric on NVIDIA A100 GPUs.\n  \n- **Settings and Datasets**:\n  - **Homogeneous Graph**: Reddit dataset (230k nodes, 492 avg. degree) for node classification.\n  - **Product Recommendation Graph**: Bi-partite graph from e-commerce data (633k query nodes, 2.71M product nodes).\n  - **Heterogeneous Graph**: OAG-CS dataset (1.1M nodes, 28.4M edges) for multi-task classification (paper field, venue, name disambiguation).\n  \n- **Graph Subsampling**: Techniques like LADIES and HGSampling are used to manage large graphs.\n\n- **Baselines**: Compared against GAE, GraphSAGE, DGI, GPT-GNN, GRACE, GraphCL, and JOAO.\n\n### Results\n- DiP-GNN significantly outperformed all baselines across datasets.\n  - **Reddit**: Achieved an F1 score of 90.7.\n  - **OAG-CS**: Improved MRR by at least 5.1 in paper field classification.\n  \n- **Ablation Studies**: \n  - The contribution of edges vs. features was evaluated, showing that both aspects are beneficial but edges have a more substantial impact.\n  - Performance degradation was more significant when edges were missing compared to when wrong edges were introduced.\n\n### Conclusion\n- DiP-GNN presents a novel discriminative pre-training approach for GNNs, effectively addressing the graph mismatch issue inherent in generative methods.\n- Extensive experiments validate that DiP-GNN improves performance on downstream tasks, highlighting the importance of the discriminator in achieving better alignment with original graph structures.\n\n### References\n- Key references are provided throughout the paper, detailing foundational and contemporary work in GNNs, pre-training methods, and comparisons with existing frameworks. \n\n### Appendix\n- Detailed algorithm and implementation specifics, including hyper-parameters for both pre-training and fine-tuning phases.\n\n--- \n\nThis summary captures the essential details of the paper's methodology, experiments, and findings, suitable for an ICLR-style review.",
        "### Technical Content\n\n#### Overview\n- **DiP-GNN** (Discriminative Pre-training of Graph Neural Networks) addresses the limitations of traditional generative pre-training methods in GNNs, particularly the issue of graph mismatch that occurs when edges are masked out.\n- The proposed method trains a generator to recover masked edges while simultaneously training a discriminator to distinguish between generated and true edges. This approach aims to create a more accurate representation of the original graph during the pre-training phase.\n\n#### Key Components\n1. **Generator**: \n   - Predicts the masked edges using the remaining unmasked edges. \n   - Uses a probability function based on node representations derived from the unmasked edges.\n   - The goal is to maximize the likelihood of correctly predicting the masked edges.\n\n2. **Discriminator**: \n   - Evaluates both true and generated edges, distinguishing between them.\n   - Utilizes embeddings from all edges to determine if an edge is true or fake, thereby learning better node representations.\n   - The discriminator is fine-tuned for downstream tasks after pre-training.\n\n3. **Loss Functions**:\n   - **Generator Loss**: Based on the likelihood of predicting masked edges.\n   - **Discriminator Loss**: Binary cross-entropy loss differentiating between true and fake edges.\n   - The overall training loss combines both losses, with a hyperparameter to balance their contributions.\n\n4. **Feature Generation and Discrimination**:\n   - Extends the model to handle node features, particularly for text, using a bi-directional Transformer model.\n   - Similar loss functions are used for feature generation and discrimination.\n\n5. **Training Pipeline**: \n   - Involves sub-sampling graphs to manage large datasets, applying negative sampling for edge generation, and balancing the discriminator's input to avoid trivial predictions.\n\n### Experimental Design\n\n#### Datasets\n- **Homogeneous Graph**: Reddit dataset with over 230k nodes, focusing on node classification.\n- **Product Recommendation Graph**: Bi-partite graph with 3.34M nodes and 228M edges, focusing on link prediction.\n- **Heterogeneous Graph**: OAG-CS dataset with over 1.1M nodes, used for multiple classification tasks (paper field, venue, and author name disambiguation).\n\n#### Experimental Setup\n- **Pre-training**: Pre-trains on large, unlabeled graphs, masked edges, and features.\n- **Fine-tuning**: Evaluates performance on downstream tasks with separate labeled graphs.\n- **Baselines**: Includes various GNN models like GAE, GraphSAGE, DGI, GPT-GNN, GRACE, GraphCL, and JOAOv2.\n\n### Results\n\n#### Performance Metrics\n- **F1 Score**: Used for evaluating node classification tasks.\n- **Mean Reciprocal Rank (MRR)**: Used for link prediction and classification tasks.\n\n#### Findings\n- DiP-GNN significantly outperforms all baseline methods in both homogeneous and heterogeneous graphs across multiple tasks.\n- Notable improvements include:\n  - **Reddit Dataset**: F1 score increased by 1.1 compared to GPT-GNN.\n  - **OAG-CS Dataset**: MRR improved by 5.1 for the paper field classification task.\n- The method demonstrates robustness to variations in labeled data and hyper-parameters.\n\n### Strengths\n- **Improved Graph Representation**: By addressing the graph mismatch issue, DiP-GNN provides better node embeddings, enhancing downstream task performance.\n- **Flexibility**: The framework can incorporate various graph neural network architectures and is adaptable to different types of node features.\n- **Robust Performance**: Extensive experiments show substantial performance gains over existing methods, validating the effectiveness of the framework.\n\n### Limitations\n- **Generative Task Complexity**: The generator may struggle with accuracy, especially as the proportion of masked edges increases, which could hinder performance if not properly managed.\n- **Dependency on Hyper-parameters**: The performance can be sensitive to the choice of hyper-parameters, requiring careful tuning.\n- **Scalability Concerns**: Although the method is designed for large graphs, the computational demands during training (especially the generation and discrimination steps) may become a bottleneck in extremely large datasets.\n\n### Conclusion\n- DiP-GNN offers a novel approach to enhancing GNNs through discriminative pre-training, effectively overcoming challenges associated with edge masking in traditional generative methods. The framework not only improves performance across various tasks but also provides a flexible platform for future research in graph neural networks.",
        "# Notes on \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\"\n\n## Abstract\n- **Objective**: Enhance Graph Neural Networks (GNNs) through pre-training methods.\n- **Methodology**: Introduces DiP-GNN, which trains a generator to recover masked edges and a discriminator to distinguish between original and generated edges.\n- **Problem Addressed**: Previous generative pre-training methods suffer from \"graph mismatch,\" where the masked input graph deviates from the original.\n- **Results**: Extensive experiments show the effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs.\n\n## 1. Introduction\n- **Background**: GNNs have shown superior performance in various applications such as node classification and knowledge graph modeling.\n- **Generative Pre-training Methods**: Involves masking edges and training GNNs to recover them, akin to masked language modeling. \n- **Issue Identified**: Graph mismatch leads to undesirable learned embeddings due to changes in topological information.\n- **Proposed Solution**: DiP-GNN employs a generator and a discriminator to improve edge recovery and reduce mismatch.\n- **Related Work**: Draws parallels with GANs but distinguishes based on the specific challenges in graph applications.\n\n## 2. Background\n### Graph Neural Networks\n- **Representation**: Node representation computed by aggregating information from neighbors.\n- **Mathematical Formulation**: \n  - Aggregation: \n    \\[\n    a^{(k)}_v = \\text{Aggregate}\\left(\\{h^{(k-1)}_u \\, | \\, \\forall u \\in \\text{Neighbor}(v)\\}\\right)\n    \\]\n  - Combination:\n    \\[\n    h^{(k)}_v = \\text{Combine}(v, a^{(k)}_v)\n    \\]\n\n### Graph Neural Network Pre-Training\n- Previous methods include proximity-based learning and random walk-based metrics.\n- Issues with scalability to large graphs noted, with generative methods like GAE and DGI identified.\n\n## 3. Method\n### 3.1 Edge Generation and Discrimination\n- **Graph Definition**: \\( G = (N, E) \\), where \\( N \\) is nodes and \\( E \\) is edges.\n- **Edge Masking**: Split edges into unmasked \\( E_u \\) and masked \\( E_m \\).\n- **Generator Objective**: Predict masked edges:\n  - Prediction probability:\n    \\[\n    p(n_1 | n_2, E_u) = \\frac{\\exp(d(h_g(n_1), h_g(n_2)))}{\\sum_{n' \\in C} \\exp(d(h_g(n'), h_g(n_2)))}\n    \\]\n- **Discriminator Objective**: Distinguish true edges from fake edges:\n  - Probability of fake edges:\n    \\[\n    p(e \\in E_{\\text{fake}} | E_{\\text{true}}, E_{\\text{fake}}) = \\text{sigmoid}(d(h_d(n_1), h_d(n_2)))\n    \\]\n- **Loss Functions**:\n  - Generator loss:\n    \\[\n    L_e^g = -\\sum_{(n_1, n_2) \\in E_m} \\log p(n_1 | n_2, E_u)\n    \\]\n  - Discriminator loss:\n    \\[\n    L_e^d = -\\sum_{e \\in E_{\\text{true}} \\cup E_{\\text{fake}}} \\left[ 1\\{e \\in E_{\\text{fake}}\\} \\log(p_{\\text{fake}}) + 1\\{e \\in E_{\\text{true}}\\} \\log(1 - p_{\\text{fake}}) \\right]\n    \\]\n\n### 3.2 Feature Generation and Discrimination\n- **Text Features**: Generated and discriminated using bi-directional Transformer models.\n- **Generator Loss for Features**:\n  \\[\n  L_f^g = -\\sum_{x} \\sum_{i \\in M} \\log p(x_i | x)\n  \\]\n- **Discriminator Loss for Features**:\n  \\[\n  L_f^d = -\\sum_{n \\in N} \\left[ 1\\{x_{\\text{corr}} \\neq x\\} \\log(p_{\\text{true}}) + 1\\{x_{\\text{corr}} = x\\} \\log(1 - p_{\\text{true}}) \\right]\n  \\]\n\n### 3.3 Model Training\n- **Combined Loss**:\n  \\[\n  L = L_e + \\lambda L_d\n  \\]\n- **Training Pipeline**: Detailed in Algorithm 1, includes edge generation, feature generation, and discrimination.\n\n### 3.4 Comparison with GAN\n- **Key Differences**: GAN's min-max game formulation not applicable due to discrete edge generation in GNN pre-training.\n\n## 4. Experiments\n### 4.1 Settings and Datasets\n- **Node Transfer Setting**: Utilizes a single large graph for pre-training and fine-tuning.\n- **Datasets**:\n  - **Reddit Dataset**: 232,965 posts, 70% for pre-training.\n  - **Product Recommendation Graph**: 633k query nodes, 2.71M product nodes.\n  - **OAG-CS Dataset**: 1.1M nodes, 28.4M edges, split by publication date.\n\n### 4.2 Implementation Details\n- **Graph Subsampling**: LADIES and HGSampling algorithms used for homogeneous and heterogeneous graphs respectively.\n- **Hyper-parameters**: Details provided in Table 6.\n\n### 4.3 Baselines\n- Compared with GAE, GraphSAGE, DGI, GPT-GNN, GRACE, GraphCL, JOAO.\n\n### 4.4 Main Results\n- **Performance Metrics**: F1 scores and MRR scores.\n- **Observations**: DiP-GNN consistently outperformed all baselines across datasets.\n\n### 4.5 Analysis\n- **Comparison with Semi-supervised Learning**: DiP-GNN outperforms C&S.\n- **Hyper-parameter Sensitivity**: DiP-GNN robust to variations in hyper-parameters.\n- **Model Variants**: Edge and feature contributions analyzed, showing the importance of both.\n- **Impact of Missing Edges vs. Wrong Edges**: Missing edges have a greater negative impact on performance.\n\n## 5. Conclusion and Discussions\n- **Summary of Contributions**: DiP-GNN shows that discriminative pre-training can better align with downstream tasks compared to generative methods.\n- **Future Directions**: Suggests potential for further exploration of graph neural network applications.\n\n## References\n- Key references include works by Kipf & Welling (2016), Hu et al. (2020), and Goodfellow et al. (2014) among others that are foundational in GNN and adversarial learning literature.\n\n## Appendix\n- **Algorithm 1**: Detailed training pipeline provided for implementing DiP-GNN.\n- **Vector Feature Generation/Discrimination**: Additional details on handling vector features instead of text features.\n- **Implementation Details**: Hyper-parameters and architecture specifics for the models used in experiments."
      ]
    },
    {
      "context": "Published as a conference paper at ICLR 2023\n\nGENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT\n\nSean Welleck1,3,* Ximing Lu1,* Peter West3,\u2020 Faeze Brahman1,3,\u2020\n\nTianxiao Shen3 Daniel Khashabi2 Yejin Choi1,3 1Allen Institute for Artificial Intelligence 2Center for Language and Speech Processing, Johns Hopkins University 3Paul G. Allen School of Computer Science & Engineering, University of Washington\n\nABSTRACT\n\nSequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present SELF-CORRECTION, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that SELFCORRECTION improves upon the base generator in three diverse generation tasks\u2013 mathematical program synthesis, lexically-constrained generation, and toxicity control\u2013 even when the corrector is much smaller than the base generator.\n\n1\n\nINTRODUCTION\n\nThe standard practice for natural language generation tasks is inherently single-pass: applying a decoding procedure to either a few-shot prompted language model or one tuned for a given task, then considering the generation as \u201cfinished\u201d (e.g. Radford et al. (2019); Brown et al. (2020); Chen et al. (2021)). Powerful generation models often meet most of the task requirements, yet miss a few (e.g., omitting a subset of keywords), or generate incorrect hypotheses that nevertheless provide useful structure (e.g., a correct problem solving strategy with a missing step). However, after generating even a slightly sub-optimal sequence, the single-pass paradigm requires models to \u201cstart from scratch\u201d, effectively discarding work already done. A more natural, intuitive approach is leveraging the generation as a useful starting point to refine into a higher quality output.\n\nTo formalize this intuition, we introduce Self-Correction for Sequence Generation. Figure 1 demonstrates its central principle: a generation model is re-framed as a base generator, which produces a reasonable initial hypothesis but does not need to solve the task in one pass, and a second module\u2013the corrector\u2013trained to make up the difference between the hypothesis and an optimal solution. Neither the generator nor the corrector must solve the full task in one pass, and the corrector can be applied multiple times to iteratively improve the output (\u00a73.6). We propose a simple, general procedure for training the corrector (Figure 2) by pairing generator outputs with carefully selected targets. The result is a system which self-corrects, producing outputs through multiple generation passes and breaking the task into steps that can be solved by dedicated and efficient sub-systems.\n\nSelf-Correction builds on past work for correction in the code and text (e.g. Yasunaga et al. (2021); Faltings et al. (2021)) domains, but provides a unified formalism with minimal assumptions about\n\n\u2217First authors, contributed equally. \u2020Second authors, contributed equally.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: SELF-CORRECTORs decompose generation into a base generator that proposes an initial hypothesis, and a corrector that iteratively improves its quality.\n\ndata and feedback, which applies generally to diverse tasks. A corrector model improves the base generator on 3 such tasks in our experiments: mathematical program synthesis (\u00a73.1), lexically constrained generation (\u00a73.2), and toxicity reduction (\u00a73.3). The trained corrector model even transfers to a larger generator with similar performance to training from scratch (\u00a73.4). Finally, we explore introducing a third module to the Self-Correction system (\u00a73.5)\u2013explicitly using natural language feedback to guide corrections\u2013with promising results. Self-Correction is an exciting path to build on the generations of strong models, with efficient, effective, and transferable corrector networks.\n\n2 SELF-CORRECTING SEQUENCE GENERATORS\n\nA typical autoregressive text generator (e.g. GPT-3 (Brown et al., 2020)) maps an input prompt to a distribution over outputs using a single parameterized module (e.g. a large transformer), p0(y|x). We explore an alternative that decomposes into two modules, a base generator, and a corrector,\n\np(y|x) =\n\n(cid:88)\n\ny0\n\np0(y0|x) (cid:124) (cid:123)(cid:122) (cid:125) generator\n\np\u03b8(y|y0, x) (cid:125) (cid:123)(cid:122) (cid:124) corrector\n\n(1)\n\nwhere the generator provides an initial hypothesis that is refined by the corrector. In practice, the corrector can be applied multiple times, p(yT |x) = (cid:80) t p\u03b8(yt+1|yt, x). Since a model of this form can both generate and correct its generations, we call it a Self-Corrector.\n\np0(y0|x) (cid:81)\n\n\u00b7 \u00b7 \u00b7 (cid:80)\n\nyT \u22121\n\n(cid:80)\n\ny1\n\ny0\n\nSelf-correctors have several unique properties compared to typical generators. First, a self-corrector decouples generation and correction, allowing us to freely parameterize each module \u2013 for instance, by prompting a single language model or using two different language models. In this paper, we develop a framework to train a separate corrector model (\u00a72.1). We find that the resulting selfcorrector improves upon the generator alone (\u00a73), even when the corrector is much smaller (\u00a73.4).\n\nSecond, since the generator and the corrector are separated, we can keep the generator as a generalpurpose language model and train the corrector with different objectives for different task requirements. In \u00a72.1, we propose a training algorithm for the corrector that is dedicated to improving generations, where the improvement can be in any aspect, measured by scalar values.\n\nThird, the corrector can receive explicit feedback about intermediate generations to guide subsequent generations. Formally, p(y|x) = (cid:80) p0(y0|x)p\u03b8(y|y0, x, f (y0)), where f is the feedback. The feedback can be of many forms, e.g. a sentence, a compiler trace, etc. In contrast, a typical generator that generates in a single pass does not leverage feedback on its own generation. In this paper, we show that the corrector can learn to exploit explicit natural language feedback to achieve better performance (\u00a73.5). Next, we describe our training framework of the corrector.\n\ny0\n\n2.1 LEARNING A CORRECTOR\n\nOur goal is to have the generator generate an initial hypothesis, then improve the hypothesis with the corrector (Eq. 1). We train the corrector to improve the quality of a hypothesis, while staying as close as possible to the original hypothesis. Here, quality is measured with a scalar value function v(y) which is accessible at training time (e.g. 0/1 indicator of program correctness, a toxicity score).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: SELF-CORRECTIVE LEARNING iteratively trains a corrector by generating hypotheses and corrections, forming value-improving pairs, and selecting those with high similarity for learning.\n\nAlgorithm 1 Self-corrective learning input Generator p0, corrector p\u03b8, prompts X, value v(\u00b7), feedback f (\u00b7)\n\nInitialize datapool D by sampling from p0 for iteration \u2208 {1, 2, . . .} do\n\nForm value-improving pairs P from D for step in 1, 2, . . . , M do\n\nSample a batch of value-improving pairs from P using Eq. 4 Compute the loss and update \u03b8 using gradient descent\n\nfor x \u2208 X do\n\nSample hypotheses y from datapool D Generate corrections y\u2032 \u223c p\u03b8(\u00b7|y, x, f (y)) Add all (x, y\u2032, v(y\u2032), f (y\u2032)) to the datapool D\n\n\u25b7 Initialization: Eq. 2\n\n\u25b7 Pairing: Eq. 3\n\n\u25b7 Learning\n\n\u25b7 Exploration: Eq. 5\n\nSince direct supervision on how to improve hypotheses is not available, we design a new algorithm to train the corrector, which we refer to as self-corrective learning. The algorithm collects a pool of generations, pairs them and selects pairs of generation that increase in value and are nearby, then updates the corrector on these pairs. As training progresses, more generations are added to the pool using the current corrector. Algorithm 1 summarizes self-corrective learning, detailed below.\n\nInitialization. Self-corrective learning begins with a generator p0(y0|x), a corrector p\u03b8(y\u2032|y, x) , a set of training prompts X, and a value function v : Y \u2192 R. Optionally, we can use additional feedback f : Y \u2192 F and learn p\u03b8(y\u2032|y, x, f (y)), where F is arbitrary.\n\nThe algorithm initializes a datapool of (input, output, value, feedback) examples by using the generator to generate multiple outputs for each input. Formally,\n\nDx = {(x, y, v(y), f (y)) | for all y \u2208 y1:N \u223c q(p0(\u00b7|x))}, D =\n\n(cid:91)\n\nx\u2208X\n\nDx,\n\n(2)\n\nwhere y1:N denotes N outputs generated with decoding algorithm q (e.g. temperature sampling). When available, (x, y, v(y), f (y)) examples from another source (e.g. a dataset) can also be added.\n\nPairing. Next, self-corrective learning forms value-improving pairs: examples of mapping a hypothesis to a higher-valued correction. We use the datapool D to form a set of (input, hypothesis, correction) pairs. A pair is formed when an output has a higher value than another \u2217:\n\nPx = {(x, y, y\u2032) | v(y) < v(y\u2032) for all y, y\u2032 \u2208 Dx \u00d7 Dx}, P =\n\n(cid:91)\n\nx\u2208X\n\nPx,\n\n(3)\n\nLearning. Next, self-corrective learning selects (input, hypothesis, correction) pairs to update the corrector with. We sample an input, x \u223c U(X), then sample a (x, y, y\u2032) pair proportional to its\n\n\u2217We also store the value and feedback for y and y\u2032 along with (x, y, y\u2032), which we omit to reduce clutter.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nimprovement in value as well as the proximity between the hypothesis y and the correction y\u2032:,\n\nP[(x, y, y\u2032)|x] \u221d exp (cid:0) \u03b1 \u00b7 (v(y\u2032) \u2212 v(y)) (cid:125)\n\n+ \u03b2 \u00b7 s(y, y\u2032) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) improvement where s(y, y\u2032) is a similarity function and Z(y) normalizes over the available corrections for y in Px. Increasing the hyperparameter \u03b1 \u2208 R\u22650 puts more weight on targets that add more value, while increasing \u03b2 \u2208 R\u22650 retains more similar targets. We update the corrector using the cross-entropy loss L(\u03b8) = \u2212 log p\u03b8(y\u2032|y, x, f (y)) on batches sampled in this way.\n\n(cid:123)(cid:122) proximity\n\n(cid:124)\n\n(cid:1)/Z(y),\n\n(4)\n\nExploration. During exploration, self-corrective learning adds new generations to the datapool by generating from the current corrector:\n\nD\u2032\n\nx = {(x, y\u2032, v(y\u2032), f (y\u2032)) | for all y\u2032 \u2208 y\u20321:N \u223c q(p\u03b8(\u00b7|y, x, f (y))}, D\u2032 =\n\n(cid:91)\n\nD\u2032\n\nx\n\nx\u2208X\n\n(5)\n\nand updating the datapool D \u2190 D \u222aD\u2032. The hypotheses y to correct can come from any source, e.g. newly sampled from the base generator, or from the datapool; we use the latter in our experiments.\n\nInference. We use the trained corrector along with a generator to generate a trajectory y0, y1, . . . , yT , and consider yT the final output. Since marginalizing over the intermediate generations in Eq. 1 is intractable, we approximate each summation with a single sequence generated with a decoding algorithm q(\u00b7). That is, we decode from the generator, then repeatedly from the corrector:\n\n\u2022 Generation: y0 \u223c q(p0(y0|x)); \u2022 Correction: yt+1 \u223c q(p\u03b8(yt+1|yt, x, f (yt))),\n\nt = 0, 1, . . . , T \u2212 1.\n\nThe stopping time T is either fixed, or when a target value is obtained (if v(y) is available).\n\n3 EXPERIMENTS\n\nWe evaluate SELF-CORRECTION on a diversity of tasks: mathematical program synthesis, in which generations are strictly correct or incorrect, and generators typically have low performance; lexically-constrained generation, which allows for partial credit, and generators usually give partially-correct solutions (e.g. matching 3 out of 5 constraints); and toxicity control, where \u2018correctness\u2019 is more loosely defined, and the output space is much more open-ended. Our experiments are organized to study three settings:\n\n1. Using self-correctors to improve upon generators (\u00a73.1,3.2,3.3). 2. Correcting generators that are much larger than the corrector (\u00a73.4). 3. Leveraging explicit feedback during training and inference (\u00a73.5).\n\nNext, we describe the self-correction setup and baselines for each task, along with their results. \u2217\n\n3.1 MATHEMATICAL PROGRAM SYNTHESIS\n\nFirst, we consider mathematical program synthesis (Austin et al., 2021; Mishra et al., 2022). Given a natural language problem specification x, the task is to generate a program y that upon execution returns the correct answer to x. The task is challenging as it draws on language understanding, multiple-step mathematical problem solving (e.g. identifying a solution strategy, decomposing a problem), and leveraging symbolic tools (e.g. built-in operations, variables). Furthermore, the task demands a high level of precision, e.g. a single misplaced operation makes the program incorrect.\n\nExperimental setup. As the corrector we use GPT-Neo 1.3B (Black et al., 2021), an open-source autoregressive language model. GPT-Neo is pre-trained on language and code (Gao et al., 2021), and hence is widely used for code-related generation (e.g. Chen et al. (2021); Ni et al. (2022); Mishra et al. (2022)). We consider two settings for the initial generator: (1) a separate fine-tuned instance of GPT-Neo 1.3B, and (2) few-shot prompted GPT-3 (Brown et al., 2020). For GPT-3, we evaluate the davinci and text-davinci-002 engines, representative of large (\u2248 175B\u2217) generators that are state-of-the-art in related tasks (Wei et al., 2022). See the Appendix for additional details.\n\n\u2217Code will be available at www.github.com/wellecks/self_correction. \u2217Estimated size of davinci (https://blog.eleuther.ai/gpt3-model-sizes). Further details not available.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nModel\n\nCorrect\n\nDataset Model\n\nParams Correct\n\nMultiarith GPT-NEO 1.3B\n\nMultitask\n\n+SELF-CORRECT +SELF-CORRECT\u2217\n\nGPT-NEO 1.3B +SELF-CORRECT +SELF-CORRECT\u2217\n\n60.00 98.33 99.17\n\n49.02 73.53 78.24\n\nGSM OpenAI 3B [6] OpenAI 6B [6] GPT-NEO [34] NEO FCP+PCP [34]\n\nGPT-NEO +SELF-CORRECT +SELF-CORRECT\u2217\n\n3B 6B 2.7B 2.7B\n\n1.3B 1.3B 1.3B\n\n15.50 20.00 18.80 19.50\n\n8.57 21.26 24.22\n\nTable 1: Evaluation results of mathematical program synthesis experiments. GPT-NEO (1.3B) is the initial generator for SELF-CORRECT. SELF-CORRECT\u2217 means only applying the corrector to incorrect outputs. Italicized: original non-program version of GSM.\n\nProblem: It takes Jennifer 20 minutes to groom each of her 2 long hair dachschunds. If she grooms her dogs every day, how many hours does she spend grooming her dogs in 30 days?\n\nProblem: Mrs. Wilsborough saved $500 to buy concert tickets for her family. She bought 2 VIP tickets at $100 each and 3 regular tickets at $50 each. How much of her savings does Mrs. Wilsborough have after she buys the tickets?\n\nGenerator:\n\nCorrector:\n\nGenerator:\n\nCorrector:\n\na=20*2 b=a*30 answer=b print(answer)\n\na=20*2 b=a*30 c=b/60 #fix answer=c print(answer)\n\na=2*100 b=3*50 c=a+b answer=c print(answer)\n\na=2*100 b=3*50 c=500-a-b #fix answer=c print(answer)\n\nFigure 3: Grade-school-math (GSM) self-corrections. On the left, the corrector fixes the units (from minutes to hours) in the generator\u2019s solution. On the right, the corrector revises the logic so that the program computes the total savings instead of the spent on tickets. We add #fix here to indicate the change. See Figure 7 and Figure 8 for additional examples.\n\nSelf-correction setup. As the value function we use correctness, which is 1 when the program y executes and outputs the ground-truth answer and 0 otherwise. Our main experiments do not use explicit feedback, i.e. f (y) = \u2205. At inference time, we study two settings for the corrector: (1) applying k corrections and selecting the final generation, (2) an oracle setting that only corrects a draft if the draft is incorrect. We use greedy decoding for the generator and corrector, and k = 1.\n\nDatasets. We evaluate on problems from 5 problem solving datasets: MultiArith (Roy et al., 2015), AddSub (Hosseini et al., 2014), SingleOp (Roy et al., 2015), SVAMP (Patel et al., 2021), and GSM8k (Cobbe et al., 2021). As in prior work (Austin et al., 2021; Ni et al., 2022; Mishra et al., 2022), we frame these as program synthesis by converting their solutions to Python programs. We separate our experiments into three increasingly difficult settings:\n\n1. MultiArith, using problems from the MultiArith arithmetic word problem dataset. 2. Multitask, using problems from 4 arithmetic datasets (MultiArith, AddSub, SingleOp, SVAMP). 3. GSM, using problems from the challenging GSM8k dataset.\n\nFor the MultiArith and Multitask settings, we make train/valid/test splits using 60/20/20% of the respective datasets. Similar to Ni et al. (2022), for the GSM setting we use the official GSM8k test split, and create a validation split using 20% of the training set. Note that the problems and answers in all datasets are the same as those from the original non-program datasets.\n\nBaselines. We compare SELF-CORRECT with its fine-tuned baseline generator (GPT-Neo 1.3B) in all three settings. For the GSM setting, we compare with existing work that uses models within the same magnitude of scale, including NEO FCP+PCP (Ni et al., 2022), which tunes GPT-NEO 2.7B with additional self-sampled programs, and their fine-tuned GPT-NEO 2.7B baseline. We also report 3B and 6B fine-tuned GPT3-like language models from Cobbe et al. (2021), which were trained on the non-program version of GSM8k. We evaluate larger models later in (\u00a73.4).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nRuntime CIDER Constraints\n\nMethod\n\nFluency Constraints\n\nNeuroLogic [28] NeuroLogic-A* [30]\n\n2.04s 19.24s\n\nGPT-2 SELF-CORRECT +NeuroLogic\n\n0.20s 0.80s 2.24s\n\n14.70 15.20\n\n14.97 15.30 15.28\n\n97.70 97.80\n\n91.38 94.58 97.80\n\nPrefix-Tuning [21] NeuroLogic [28] NeuroLogic-A* [30]\n\nGPT-2 SELF-CORRECT\n\n2.96 2.80 2.85\n\n2.94 2.98\n\n91.16 96.91 96.97\n\n91.50 98.77\n\nTable 2: Lexically-constrained generation. By training a corrector to optimize constraint satisfaction, SELF-CORRECT improves constraints while maintaining fluency, without modifying the underlying generator. Due to space, we show CIDER for COMMONGEN and human judgement for E2E as measures of fluency. Other metrics show similar trends and can be found in the Appendix.\n\nResults. As seen in Table 1, the self-corrector improves upon the generator in all three settings, using either inference strategy: always correcting (SELF-CORRECT), or only correcting incorrect solutions (SELF-CORRECT\u2217). The self-corrector\u2019s performance on Multiarith is very high after correction (9899%), a 38 point improvement over the generator, with a similar gain in the Multitask arithmetic setting. On the challenging GSM dataset, the self-corrector achieves 21%, and 24% with only correcting incorrect solutions, up from 8.57% for the generator. Notably, this is higher than the larger 2.7B GPT-Neo (also larger than generator+corrector), or larger models tuned on the language version of GSM. The results show that self-corrective learning can improve task performance via training a corrector. Qualitatively, the self-corrector can correct values in a correctly structured solution, fix the order of operations within a multistep solution, adjust unit conversions, and make larger multipart revisions (see Figures 3,7,8). Notably, these are learned automatically.\n\n3.2 LEXICALLY CONSTRAINED GENERATION\n\nNext, we consider lexically constrained generation. Given a set of constraint words x, the task is to generate a sentence y that includes all the given constraints. Faithful constraint satisfaction is crucial for many downstream tasks, e.g., those that require converting information to text (McKeown, 1985).\n\nDatasets and Metrics. We experiment on COMMONGEN (Lin et al., 2020) and E2E (Novikova et al., 2017). COMMONGEN is a benchmark for generative commonsense reasoning where the task is to generate a coherent sentence given a set of words (e.g., dog, catch). E2E involves converting structured inputs into natural language. For both tasks, we report standard metrics including human/automatic measures of fluency (BLEU, CIDER, etc.) as well as constraint coverage. We collect human measures of fluency on Amazon Mechanical Turk; see the Appendix for details.\n\nSetup. We parameterize the base generator with GPT-2 Radford et al. (2019) (large-size for COMMONGEN and medium-size for E2E). We fine-tuned the generator for each task. As the value function for self-corrective learning we use coverage, i.e. the percentage of constraints that are present in the output. For inference, we use beam search with the generator, then do up to 3 corrections using beam search, stopping early if all constraints are met. See the Appendix for additional details.\n\nResults. Table 2 shows the evaluation results. The self-corrector substantially improves constraint coverage over its GPT-2 generator for both tasks, while maintaining or improving its language quality. On the COMMONGEN benchmark, the self-corrector paired with the NeuroLogic constrained decoding algorithm (Lu et al., 2021) achieves the best results, outperforming the more sophisticated NeuroLogic-A* decoding algorithm, while being an order of magnitude faster. Notably, on E2E, self-correction outperforms Neurologic-A* decoding, despite only using standard beam search. This suggests that a corrector can be viewed as an alternative to using a more sophisticated decoding procedure (A*) for improving performance without modifying the underlying model. See Figure 9.\n\n3.3 TOXICITY REDUCTION\n\nNext, we consider the task of toxicity reduction (Gehman et al., 2020; Liu et al., 2021). Given a prompt x, the task is to generate a fluent continuation y while avoiding offensive content. This task is important for ensuring safe language model deployment, yet challenging: due to misaligned pretraining objectives (i.e. modeling internet text vs. non-toxic text), language models are suscepti-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nToxicity\n\nFluency\n\nDiversity\n\nAvg. Max. Prob. Perplexity dist-2 dist-3\n\nGPT-2\n\nPPLM [7] GeDi [17] DExpert [27] DAPT [15] PPO [29] Quark [29]\n\nSELF-CORRECT\n\n0.527\n\n0.520 0.363 0.314 0.428 0.218 0.196\n\n0.171\n\n0.520\n\n0.518 0.217 0.128 0.360 0.044 0.035\n\n0.026\n\n11.31\n\n32.58 43.44 25.21 31.22 14.27 12.47\n\n11.81\n\n0.85\n\n0.86 0.84 0.84 0.84 0.79 0.80\n\n0.80\n\n0.85\n\n0.86 0.83 0.84 0.84 0.82 0.84\n\n0.83\n\nTable 3: Toxicity reduction. GPT-2 is the base generator.\n\nFigure 4: Applying multiple corrections reduces toxicity.\n\nble to generating toxic completions, even when prompted with seemingly innocuous text (Gehman et al., 2020). Along with its practical importance, the task tests whether (self-)correctors can be an effective mechanism for controlling the outputs of language models in an open-ended setting.\n\nDatasets and Metrics. We use the REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) which contains 100k prompts designed to elicit toxic generations. Following the experimental setup of Liu et al. (2021), during training we use 85K prompts from the training set, and for evaluation we use the same 10K non-toxic prompts from test set as Liu et al. (2021). We use Perspective API to measure maximum toxicity, defined as the average maximum toxicity over 25 sampled generations, and the (empirical) toxicity probability of at least 1 out of 25 generations being toxic.\n\nBaselines. We compare SELF-CORRECT with its generator (GPT-2) and previously reported baselines from Lu et al. (2022a), including PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2021), DExpert (Liu et al., 2020), DAPT (Gururangan et al., 2020), PPO (Lu et al., 2022a), and Quark (Lu et al., 2022a). The latter two \u2013 Proximal Policy Optimization (PPO) and Quantized Reward Konditioning (Quark) \u2013 represent strong, state-of-the art approaches based on reinforcement learning.\n\nSetup. We use the off-the-shelf GPT-2 Large as the generator, and finetune another GPT-2 Large as the corrector. During inference, we use nucleus sampling with p = 0.9 to generate 25 samples for all baselines. As the value function, we use the Perspective API score, v(y) \u2208 [0, 1], which measures the toxicity of the completed sequence. We do up to three corrections with the corrector model.\n\nResults. Table 3 shows that SELF-CORRECT reduces the rate of toxic generations substantially, while also maintaining fluency and diversity. SELF-CORRECT outperforms all baselines. This includes inference-time algorithms (PPLM, GeDi, DExpert), which do not modify the generator but degrade fluency and yield higher toxicity compared to SELF-CORRECT, as well as reinforcement learning methods (PPO, Quark) that adjust the generator using toxicity as a (negative) reward. The strong baselines use equal or more parameters: PPO and Quark use 3 and 2 model copies. The results show that SELF-CORRECT is effective for detoxification, without modifying the generator.\n\n3.4 CHANGING MODULES \u2013 CORRECTING GPT-3\n\nNext, we show that a self-corrector can improve the outputs of a generator that is much larger than the corrector. We consider two cases: (1) training with a small generator, then swapping in the larger generator at test time; (2) training with the larger generator, i.e. using the large generator to initialize the datapool for self-corrective learning, then using the large generator at test time.\n\nToxicity. We evaluate case (1) for reducing the toxicity of a large generator (GPT-2 XL, GPT-3). We generate an initial sequence using the large generator, then refine it with our corrector trained in the previous experiments (\u00a73.3). Table 4 shows that the resulting self-corrector (large generator + corrector) has substantially reduced toxicity compared to the large generator. This shows the promise of using (self-)correctors for controlling the outputs of large language models.\n\nMath program synthesis. Table 4 shows results for math. Analogous to toxicity, the corrector is able to correct larger generators swapped in at test-time. For instance, the GPT-3 Instruct generator has quite high performance (84.90 Multitask, 36.80 GSM), which improves to 90.90 and 45.00,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTask\n\nDataset\n\nGenerator (train) Generator (test) Generator\n\nSelf-corrector\n\nMath Synthesis \u2191\n\nGSM\n\nNeo 1.3B Neo 1.3B GPT-3 Instruct\n\nDetoxification \u2193\n\nGPT2-L RTPrompts GPT2-L GPT2-L\n\nGPT-3 GPT-3 Instruct GPT-3 Instruct\n\nGPT2-XL GPT-3 GPT-3 Instruct\n\n6.96 36.80 36.80\n\n0.383 0.182 0.275\n\n24.30 45.00 45.92\n\n0.027 0.025 0.023\n\nTable 4: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.\n\nToxicity \u2193\n\nConstrained Gen. \u2191\n\nMath \u2191\n\nAvg. Max.\n\nProb.\n\nFluency\n\nFluency Constraints Correct Correct\u2217\n\nGenerator SELF-CORRECT + FEEDBACK\n\n0.527 0.171 0.156\n\n0.520 0.026 0.020\n\n11.31 11.81 11.86\n\n14.97 15.30 15.24\n\n91.38 94.58 95.88\n\n49.02 74.31 81.76\n\n49.02 79.80 82.35\n\nTable 5: Explicit natural language feedback. Correct\u2217 means only correcting incorrect outputs.\n\nrespectively, by adding in a corrector. The self-corrector (large generator + corrector) improves further by training with the GPT-3 Instruct generator, to 92.75 and 45.92, respectively.\n\n3.5 LEVERAGING EXPLICIT FEEDBACK\n\nNext, we demonstrate SELF-CORRECT\u2019s capacity to incorporate explicit natural language feedback. This amounts to defining a feedback function f , then using the same self-corrective learning and inference algorithms (\u00a72.1) as in our preceding experiments (in those experiments, f returned \u2205). We show that correctors learn to use the feedback, as evidenced by higher performance.\n\nToxicity. We use additional fine-grained information from the toxicity API as natural language feedback. Specifically, besides the overall toxicity score, Perspective API also provides scores for fine-grained attributes of toxicity (e.g. identity attack, profanity, flirtation, etc.). At training time, we compare the attribute scores from a hypothesis and its selected correction, and use the attribute with the largest decrease as natural language feedback (e.g. \"decrease toxicity in profanity\"). At inference time, we call the API on the current hypothesis and use the attribute with the highest score.\n\nLexical constraints. In training time, we generate natural language feedback for every example pair (x, y, y\u2032) by elaborating the extra lexical constraints satisfied by y\u2032 but not y. e.g. \u201cadding constraint word: read\u201d. At inference time, we elaborate all missing constraints in the current hypothesis.\n\nMath program synthesis. Math program synthesis contains a variety of problem types and errors, without an automated means for identifying the errors (e.g. an API). We explore obtaining natural language feedback about the current program by prompting a large language model. We prompt the model with a problem, hypothesis program, a gold solution, and few-shot demonstrations that show feedback on one part of the program; e.g. In the initial guess, 3 should be subtracted. When the program is correct, the feedback is Correct. At inference time, we also use feedback from the language model. We allow the feedback model access to a gold solution, which we expect makes the feedback higher quality, with the risk of solution leakage at inference-time. Our results in this task are thus used only to study the feasibility of explicit feedback for math program synthesis.\n\nSetup. For toxicity, lexical constraints, and math we use REALTOXICITYPROMPTS, COMMONGEN, and the MULTITASK arithmetic setting, respectively. We follow the setup of each task\u2019s previous experiments (\u00a73.3,\u00a73.2,\u00a73.1), except for math we use 5 correction iterations (previously 1). For math, we use GPT-3 (text-davinci-002) with 6 demonstrations as the feedback model.\n\nResults. Table 5 shows that explicit natural language feedback improves performance in all three tasks. For toxicity, this means that providing fine-grained attributes (e.g. identity attack, profanity,\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nAblation\n\nMath COMMONGEN\n\nSELF-CORRECT \u2717 proportional sampling \u2717 value pairing\n\n78.24 77.25 62.35\n\n94.55 93.49 91.76\n\nTable 6: Effect of pairing and proportional sampling.\n\nExploration Multiarith Multitask GSM8k\n\n\u2717 \u2713\n\n89.20 99.17\n\n73.49 78.24\n\n17.60 23.96\n\nTable 7: Effect of exploration on program synthesis.\n\nFigure 5: Math: multiple corrections.\n\netc.) during learning and inference improves upon using only the scalar toxicity score. Intuitively, feedback may help the model to focus on a useful correction; e.g., see Figure 6.\n\n3.6 ADDITIONAL ABLATIONS AND ANALYSIS\n\nEffect of multiple corrections. Previously, Figure 4 showed that multiple corrections led to better toxicity reduction. On math (Multitask setting), Figure 5 shows that performance improves with more than one correction, and that multiple corrections are more beneficial with feedback. Intuitively, in this math task, after 2-3 corrections the model needs additional guidance.\n\nEffect of pairing and proportional sampling. Self-corrective learning (i) samples pairs for learning proportional to Equation 4, (ii) only pairs sequences that improve value. We ablate these features by training on Multitask using a data pool that samples a pair for learning uniformly (rather than Equation 4), and a data pool without value pairing. Table 6 shows that both improve performance.\n\nEffect of exploration. To ablate the effect of exploration, we train a baseline only on correction pairs induced from the base generator. Table 7 shows results on the three math datasets, indicating that exploration improves performance.\n\n4 RELATED WORK\n\nSelf-Correction relates to work modeling text edits including supervised Wikipedia edits (Reid & Neubig, 2022; Faltings et al., 2021; Schick et al., 2022), unsupervised perturbations (Miao et al., 2019; Liu et al., 2020), training on human-written critiques (Saunders et al., 2022), or refining continuous variables (Lee et al., 2020; Li et al., 2022; Qin et al., 2022). In contrast, Self-Correction learns a text corrector online to improve a quality measure without supervised edits or critiques. Recently, Scheurer et al. (2022) use natural language feedback to improve generations. Denoising sequences is a common pretraining objective (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), while self-correction \u2018denoises\u2019 generations to improve a scalar quality measure. Reinforcement learning (RL) is often used to improve scalar measures in a generator (Ziegler et al., 2019; Stiennon et al., 2020; Lu et al., 2022a), yet is infeasible for many models (e.g. those accessed by API), and uses only scalar feedback. Moreover, RL-tuned generators can be used within Self-Correction. Self-Correction decomposes generation into multiple steps, similar to methods that generate rationales (Wei et al., 2022; Dohan et al., 2022), but Self-Correction produces intermediate steps of the same form as the output, allowing iterative application. Self-Correction relates to work on program synthesis (Fu et al., 2019; Balog et al., 2020; Gupta et al., 2020; Le et al., 2022) and repair (Gupta et al., 2020; Yasunaga & Liang, 2020). Yasunaga & Liang (2021) is closest in methodology, but Self-Correction uses a domain-agnostic formulation; see the Appendix for discussion.\n\n5 CONCLUSION\n\nWe introduced self-correctors, a class of models that decompose generation into initial generation and correction steps. We study self-correctors with a fixed base generator along with a corrector trained to improve outputs according to a scalar measure of quality. We presented a simple, general procedure for training the corrector, and find that self-correction is applicable and effective for improving performance, and controlling the outputs of both small and large generators. Moreover, we found that self-correction along with our learning framework provides a promising mechanism for using natural language feedback to improve generation, opening many avenues for future work.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021.\n\nMatej Balog, Rishabh Singh, Petros Maniatis, and Charles Sutton. Neural program synthesis with a\n\ndifferentiable program fixer, 2020. URL https://arxiv.org/abs/2006.10924.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168.\n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. ArXiv, abs/1912.02164, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.\n\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Narain Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. ArXiv, abs/2207.10342, 2022.\n\nFelix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5259\u20135274, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.414. URL https://aclanthology.org/2021.naacl-main.414.\n\nCheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, and Jishen In Advances in Neural Information\n\nZhao. Coda: An end-to-end neural program decompiler. Processing Systems, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL https://arxiv.org/ abs/2101.00027.\n\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3356\u20133369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\n\nKavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis. In H Larochelle, M Ranzato, R Hadsell, M F Balcan, and H Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17685\u201317695. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/cd0f74b5955dc87fd0605745c4b49ee8-Paper.pdf.\n\nSuchin Gururangan, Ana Marasovi \u0301c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342\u20138360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.\n\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523\u2013533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.\n\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929\u20134952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021. findings-emnlp.424.\n\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. arXiv preprint arXiv:2207.01780, 2022.\n\nJason Lee, Raphael Shu, and Kyunghyun Cho.\n\nIterative refinement in the continuous space for non-autoregressive neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1006\u20131015, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.73. URL https://aclanthology.org/2020.emnlp-main.73.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence preIn Proceedings of training for natural language generation, translation, and comprehension. the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582\u20134597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.\n\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-\n\nlm improves controllable text generation. ArXiv, abs/2205.14217, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJared Lichtarge, Christopher Alberti, Shankar Kumar, Noam M. Shazeer, Niki Parmar, and Simon\n\nTong. Corpora generation for grammatical error correction. ArXiv, abs/1904.05780, 2019.\n\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative comIn Findings of the Association for Computational Linguistics: EMNLP monsense reasoning. 2020, pp. 1823\u20131840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020. findings-emnlp.165.\n\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and antiexperts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6691\u20136706, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.522. URL https://aclanthology.org/2021. acl-long.522.\n\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3154\u20133169, 2022.\n\nXianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, and Sen Song. Unsupervised paraphrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302\u2013312, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020. acl-main.28.\n\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4288\u20134299, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339. URL https://aclanthology.org/2021.naacl-main.339.\n\nXiming Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. CoRR, abs/2205.13636, 2022a. doi: 10.48550/arXiv.2205.13636. URL https://doi.org/ 10.48550/arXiv.2205.13636.\n\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic In Proceedings of a*esque decoding: Constrained text generation with lookahead heuristics. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 780\u2013799, Seattle, United States, July 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https: //aclanthology.org/2022.naacl-main.57.\n\nKathleen McKeown. Text Generation. Studies in Natural Language Processing. Cambridge Univer-\n\nsity Press, 1985. doi: 10.1017/CBO9780511620751.\n\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01):6834\u20136842, Jul. 2019. doi: 10.1609/aaai.v33i01.33016834. URL https://ojs.aaai. org/index.php/AAAI/article/view/4659.\n\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. ArXiv, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning from self-sampled correct and partially-correct programs, 2022. URL https://arxiv.org/abs/2205.14318.\n\nJekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 201\u2013206, Saarbr\u00fccken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.\n\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080\u2013 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.\n\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based con-\n\nstrained text generation with langevin dynamics. arXiv preprint arXiv:2202.11705, 2022.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nMachel Reid and Graham Neubig. Learning to model editing processes, 2022. URL https:\n\n//openreview.net/forum?id=1bEaEzGwfhP.\n\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084.\n\nSubhro Roy, Tim Vieira, and Dan Roth. Reasoning about quantities in natural language. Transac-\n\ntions of the Association for Computational Linguistics, 3:1\u201313, 2015.\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv. org/abs/2206.05802.\n\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback, 2022. URL https://arxiv.org/ abs/2204.14146.\n\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collaborative language model, 2022. URL https://arxiv.org/abs/2208.11663.\n\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4615\u20134629, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373. URL https://aclanthology.org/2020.emnlp-main.373.\n\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Adback. vances in Neural Information Processing Systems, volume 33, pp. 3008\u20133021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nMichihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic ISBN\n\nIn 37th International Conference on Machine Learning, ICML 2020, 2020.\n\nfeedback. 9781713821120.\n\nMichihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In\n\nInternational Conference on Machine Learning (ICML), 2021.\n\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang. LM-critic: Language models for unsupervised grammatical error correction. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7752\u20137763, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 611. URL https://aclanthology.org/2021.emnlp-main.611.\n\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA RELATED WORK\n\nSelf-correction provides a flexible framework for improving the performance of off-the-shelf and fine-tuned language models on a wide range of tasks by decomposing generation into a base generator and a corrector. Our framework\u2019s minimal assumptions on the form of the corrector, value function, and data used to train the corrector, as well as its wide applicability differ from prior work.\n\nLearning to fix code. Our work relates to two streams of research in the code domain. One stream deals with program synthesis, in which a corrector model corrects code from a base synthesizer until it meets a given specification (Fu et al., 2019; Balog et al., 2020; Gupta et al., 2020; Le et al., 2022), while another stream deals with program repair: correcting code that is provided as input (Gupta et al., 2020; Yasunaga & Liang, 2020; 2021). Recently, Le et al. (2022) developed a modular program synthesis approach that involves a correction module trained on ground-truth outputs. In contrast, self-corrective learning supports cases without ground-truth outputs, e.g. toxicity.\n\nClosest to our methodology is Yasunaga & Liang (2021). Unlike Yasunaga & Liang (2021), selfcorrection does not assume a mechanism for generating synthetic negatives, a dataset of negatives, or a separate model that generates negatives. This is important because engineering these components for each new task can be prohibitive. Second, Yasunaga & Liang (2021) assume a 0/1 value function, while self-correction supports general scalar value functions. This is important for tasks such as toxicity that do not have a strict notion of correctness. Finally, we propose new pairing and proportional sampling mechanisms found to be important (Table 6).\n\nIterative text edits. Self-correction relates to recent works on editing text, including modeling Wikipedia edits (Reid & Neubig, 2022; Faltings et al., 2021; Schick et al., 2022), which relies on supervised edits, unsupervised methods (Miao et al., 2019; Liu et al., 2020) that perturb sequences with simple operations (e.g. insertion, deletion), editing with models trained on human-written critiques (Saunders et al., 2022), or iteratively updating continuous variables (Lee et al., 2020; Li et al., 2022; Qin et al., 2022). In contrast to these, self-correction learns an expressive text-to-text corrector that is trained online to improve a quality measure, without requiring a supervised dataset of edits or critiques. Recently, Scheurer et al. (2022) incorporate human feedback by fine-tuning on refinements that are similar to the feedback, rather than through an iterative corrector module. Finally, correcting text is inherent to the task of grammatical error correction (e.g. Lichtarge et al. (2019); Yasunaga et al. (2021); our work differs in that we correct a module within a generation system, and provide a framework for addressing a variety of tasks.\n\nDenoising and reinforcement learning. Separately, denoising ground-truth sequences is a common pretraining objective (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), while selfcorrection \u2018denoises\u2019 generations to improve a scalar quality measure. Scalar measures are often improved with reinforcement learning (RL) on a base generator (Ziegler et al., 2019; Stiennon et al., 2020; Lu et al., 2022a), which is infeasible for improving many language models (e.g. those accessed through an API), and uses only scalar feedback. Moreover, self-correction learns a delta between a generation and solution, and is complementary to RL-tuned generators, which can be used within a self-corrector. Finally, RL can be used as an alternative learning algorithm for training a corrector, which is an interesting direction for future work.\n\nModular generation. Self-correction decomposes generation into multiple steps, and is thus part of the general class of methods that decompose generation into a \u2018cascade\u2019 of modules (Dohan et al., 2022). Examples include using separate knowledge generation modules (Shwartz et al., 2020; Liu et al., 2022), or generating rationales before a response (Wei et al., 2022). Self-correction also produces a chain of intermediate steps, but each step is of the same form as the output, allowing for re-using previous generations.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB ADDITIONAL EXPERIMENTAL DETAILS\n\nB.1 CROSS-EXPERIMENT DETAILS\n\nIn all of our experiments we use an off-the-shelf embedding similarity function from SentenceTransformers (Reimers & Gurevych, 2019): sentence-transformers/all-MiniLM-L6-v2.\n\nB.2 MATHEMATICAL PROGRAM SYNTHESIS\n\nWe fine-tune a separate instance of GPT-Neo 1.3B as an initial generator, using the Huggingface library with default hyperparameters, except for evaluation steps, which we set to a small number to ensure a strong checkpoint is selected for each dataset. We use the finetuned initial generator as initialization for the corrector, and tune the corrector on sequences [SC]x[CURR]yi[START]yj[END], where x is a problem, yi and yj form a residual pair, and [\u00b7] are special tokens. The loss is on tokens after [START].\n\nFeedback. We write 6 demonstrations using training problems and generations from our GPTNeo base generator, and use GPT-3 (text-davinci-002) as a feedback model. We use the same training procedure and hyperparameters, except that the sequences now include feedback, [SC]x[CURR]yi[FEEDBACK]F(x,yi)[START]yj[END], where x is a problem, yi and yj form a residual pair, and F (x, yi) is feedback. We include loss on tokens after [FEEDBACK].\n\nB.3 LEXICALLY-CONSTRAINED GENERATION\n\nHyper-parameters. Table 8 and Table 9 show hyperparameters for CommonGen and E2E.\n\nHuman Evaluation. We evaluate fluency of generations in E2E task using human annotators on Amazon Mechanical Turk (AMT). We randomly sampled 100 instances, along with generations of different baselines and self-corrections. For each instance, we ask 3 annotators to evaluate the fluency of generations on a 3-point Likert scale. We aggregate annotations from 3 annotators using majority vote. We restricted the pool of annotators to those who are located in US or CA, and had 98% approval rate for at least 5,000 previous annotations.\n\nHyperparameter\n\nAssignment\n\nHyperparameter\n\nAssignment\n\nPredictor steps batch size optimizer learning rate decoding alg.\n\nGPT-2Large 6000 128 Adam 1.e\u22125 beam search (k=5)\n\nPredictor steps batch size optimizer learning rate decoding alg.\n\nGPT-2M edium 10000 100 Adam 1.e\u22125 beam search (k=5)\n\nTable 8: Hyperparameters for COMMONGEN.\n\nTable 9: Hyperparameters for E2E.\n\nC ADDITIONAL RESULTS\n\nToxicity\n\nFluency\n\nDiversity\n\nAvg. Max.\n\nProb.\n\nPerplexity\n\ndist-2\n\ndist-3\n\nGPT2-L SELF-CORRECT SELF-CORRECT + FEEDBACK\n\n0.527 0.171 0.156\n\n0.520 0.026 0.020\n\n11.31 11.81 11.86\n\n0.85 0.80 0.80\n\n0.85 0.83 0.83\n\nTable 10: Evaluation results of toxicity reduction experiments with natural language feedback.\n\nD QUALITATIVE EXAMPLES\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTask\n\nDataset\n\nGenerator (train) Generator (test) Generator\n\nSelf-corrector\n\nMath Synthesis \u2191\n\nMultitask\n\nGSM\n\nNeo 1.3B Neo 1.3B GPT-3 Instruct\n\nNeo 1.3B Neo 1.3B GPT-3 Instruct\n\nDetoxification \u2193\n\nGPT2-L RTPrompts GPT2-L GPT2-L\n\nGPT-3 GPT-3 Instruct GPT-3 Instruct\n\nGPT-3 GPT-3 Instruct GPT-3 Instruct\n\nGPT2-XL GPT-3 GPT-3 Instruct\n\n46.70 84.90 84.90\n\n6.96 36.80 36.80\n\n0.383 0.182 0.275\n\n80.00 90.90 92.75\n\n24.30 45.00 45.92\n\n0.027 0.025 0.023\n\nTable 11: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.\n\nBleu-4 CIDER Coverage\n\nRuntime\n\nNeuroLogic [28] NeuroLogic-A*esque [30]\n\nGPT-2 SELF-CORRECT SELF-CORRECT + feedback SELF-CORRECT+NeuroLogic\n\n26.70 28.20\n\n27.90 27.98 27.82 28.17\n\n14.70 15.20\n\n14.97 15.30 15.24 15.28\n\n97.70 97.80\n\n91.38 94.58 95.88 97.80\n\n2.04s/sent 19.24s/sent\n\n0.2s/sent 0.8s/sent 0.8s/sent 2.24s/sent\n\nTable 12: Evaluation rresults of lexically-constrained generation on COMMONGEN.\n\nCoverage BLEU-4 NIST R-L METEOR CIDER\n\nPREFIX-TUNING (Li & Liang, 2021)\n\n91.16\n\n70.30\n\n8.82 72.10\n\nGPT-2 SELF-CORRECT\n\n91.50 98.77\n\n67.12 68.81\n\n8.67 70.25 8.78 68.60\n\n46.30\n\n45.58 45.11\n\n2.46\n\n2.33 2.38\n\nTable 13: Evaluation results of lexically-constrained generation on E2E.\n\nProblem: Melanie had 19 dimes in her bank. Her dad gave her 39 dimes and her mother gave her 25 dimes. How many dimes does Melanie have now?\n\nProblem: Lana\u2019s favorite band was holding a concert where tickets were 6 dollars each. Lana bought 8 tickets for herself and her friends and 2 extra tickets in case anyone else wanted to go. How much did she spend?\n\nGenerator (GPT-Neo):\n\nanswer = 19 + 25 print(answer)\n\nFeedback (GPT-3):\n\n# In the initial guess, # 39 is not included.\n\nCorrector (GPT-Neo):\n\nanswer = 19 + 25 + 39 print(answer)\n\nGenerator (GPT-Neo):\n\nanswer=(6.0*8.0) print(answer)\n\nFeedback (GPT-3):\n\n# In the initial guess, # 2 tickets are not included.\n\nCorrector (GPT-Neo):\n\nanswer=(6.0*(8.0+2.0)) print(answer)\n\nFigure 6: Self-correction with natural language feedback.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nProblem: Ralph watches TV for 4 hours a day from Monday to Friday, and 6 hours a day on Saturday and Sunday. How many hours does Ralph spend watching TV in one week?\n\nGenerator:\n\nCorrector:\n\na=4*7 b=6*7 c=a+b answer=c print(answer)\n\na=4*5 b=6*2 c=a+b answer=c print(answer)\n\nProblem: The pirates plan to explore 4 islands. Two islands require walking 20 miles per day while the other two islands require 25 miles per day. How many miles will they have to walk if it takes 1.5 days to explore each island?\n\nGenerator:\n\nCorrector:\n\na=20*2 b=25*2 c=a+b d=c*1.5 e=d+b answer=e print(answer)\n\na=20*2 b=25*2 c=a+b d=c*1.5 answer=d print(answer)\n\nFigure 7: Grade school math (GSM) self-corrections. Left: the structure of the generator\u2019s solution is valid, but it incorrectly uses the total number of days in a week for both a and b; the corrector fixes the program to correctly account for the 5 weekdays and 2 weekend days. Right: the generator\u2019s solution contains an incorrect addition at the end; the corrector removes this line, resulting in a correct program.\n\nProblem: A spiral notebook costs 15, and a personal planner costs $10. How much would it cost in total to buy 4 spiral notebooks and 8 personal planners at a 20% discount?\n\nProblem: Julia has $40. She spends half of her money to buy a new game for her phone. She spends a quarter of what she has left on in-game purchases. How much money does she have left?\n\nGenerator:\n\nCorrector:\n\nGenerator:\n\nCorrector:\n\na=4*15 b=8*10 c=a+b answer=c print(answer)\n\na=4*15 b=8*10 c=a+b d=c*20/100 e=c-d answer=e print(answer)\n\na=40/2 b=40-a c=b/4 d=b-c e=d+40 answer=e print(answer)\n\na=40/2 b=a/4 c=a-b answer= c print(answer)\n\nFigure 8: Grade school math (GSM) self-corrections. Left: the generator\u2019s program doesn\u2019t include the discount; the corrector appends the discount to the program. Right: a more sophisticated multipart correction. The generator\u2019s assignment of b (line 2), and addition to e (line 5) are incorrect. The corrector removes these lines and adjusts the variable names accordingly.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nConstraints: \u2018table\u2019, \u2018paper\u2019, \u2018read\u2019 Generator: A man is reading book on a table. Corrector: A man is reading a book on a table and writing on a piece of paper.\n\nConstraints: \u2019stand\u2019, \u2019wait\u2019, \u2019look\u2019 Generator: He waits for someone to look at him. Corrector: He waits for someone to stand and look him in the eye.\n\nConstraints: \u2019walk\u2019, \u2019park\u2019, \u2019bench\u2019, \u2019dog\u2019, \u2019sit\u2019 Generator: A dog sitting on a bench in a park. Corrector: A dog sitting on a bench and a man walking a dog in a park.\n\nConstraints: name : The Mill | Type : restaurant | food : English | price : high | customer rating : average | area : riverside | family friendly : yes | near : Cafe Rouge Generator: The Mill is an expensive, three star restaurant located near Cafe Rouge. Corrector: The Mill is a children friendly English restaurant in the riverside area near Cafe Rouge.\n\nIt has an average customer rating and a high price range.\n\nConstraints: name : Blue Spice | Type : restaurant | food : Chinese | area : riverside | family friendly : yes | near : Rainbow Vegetarian Cafe Generator: Blue Spice is a restaurant providing Chinese food. It is near Rainbow Vegetarian Cafe. Corrector: Blue Spice is a family friendly Chinese restaurant in the riverside area near Rainbow Vegetarian Cafe.\n\nIt is located in the riverside.\n\nFigure 9: Constrained generation self-corrections. Top: COMMONGEN, and Bottom: E2E. Words in green indicate constraints that were previously missing.\n\nPrompt: Waters then went on to call the Trump Generator: administration a \"bunch of idiots\" and \"insolent clowns [who are] running\" Corrector: Administration\u2019s support for the Trans-Pacific Partnership on trade \"a total non-event.\"\n\nFigure 10: Toxicity control self-corrections. Words in red indicate toxic language.\n\n19",
      "responses": [
        "# Summary Of The Paper\n\nThis paper proposes a self-correction method which trains a corrector to iteratively correct imperfect generation results. The authors first train a generator on the downstream datasets (or directly prompt a large language model), and use it to construct a data pool. Then, the authors select value-improving pairs based on a task-specific value function to build the training set of the corrector. Finally, the corrector is trained based on these samples and generates samples to augment the original data pool. Experimental results show the effectiveness of self-correction in three generation tasks.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1) This paper is well organized and easy to follow.\n2) The proposed method can be applied to a wide range of text generation tasks. The experimental results show the superior performance of self-correction over some competitive baselines.\n\nWeaknesses:\n\n1) The name of the method \u201cself-correction\u201d is confusing for me, because the authors train a separate corrector to improve the base generator. The generator / corrector cannot consistently correct itself in this paper. They should cooperate with each other to achieve better generation performance.\n\n2) From the perspective of correctors, the proposed method seems to train a text editing model (corrector) via pseudo-labeled data generated by a pre-trained model (generator). Specifically, the fixed generator is used to construct the training dataset of the corrector via generating data and selecting value-improving pairs. Then, the corrector is trained on these \"pseudo-labeled\" data and augment the original data pool iteratively. Thus, I feel that the novelty of this method is somewhat limited because using pre-trained models to automatically generate training data is common in recent works. I don\u2019t find any specific design when training the corrector.\n\n3) The feedback has been mentioned for many times in this paper. But this part is individual compared with the whole design. I don\u2019t find any specific module to properly incorporate the feedback signal into the corrector.\n\n4) The experimental setting may be unfair because the corrector has a relatively large amount of model parameters. Thus, the total number of parameters in self-correction (including the generator and the corrector) is significantly larger than that of other baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe authors should further clarify the method design and the experimental settings. The overall quality of this paper is OK. But in my view, the novelty of the proposed method is somewhat limited from the perspective of correctors. The reproducibility of this paper is degraded due to the lack of codes.\n\n# Summary Of The Review\n\nThe proposed method can adapt to various text generation tasks and achieve good performance. However, as mentioned in my review, the authors should solve the concerns about the novelty, the method design, and the experimental settings to make this paper ready for publication.\n\n# Correctness\n\n3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
        "# Summary Of The Paper\nThe paper \"Generating Sequences by Learning to [Self]-Correct\" introduces a novel method called SELF-CORRECTION, which enhances sequence generation processes by incorporating a corrector that iteratively refines the outputs generated by a base model. The methodology divides the generation task into two modules: a base generator and a corrector that can leverage feedback, either scalar or natural language, to improve the generated output. The authors demonstrate the effectiveness of this approach across three tasks\u2014mathematical program synthesis, lexically-constrained generation, and toxicity control\u2014reporting significant improvements in output quality, correctness rates, and constraint satisfaction compared to baseline models.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to incorporating a separate correction mechanism that allows for iterative refinement of outputs, which addresses common limitations of traditional one-pass generation models. The empirical results across multiple tasks are compelling, showcasing the versatility and effectiveness of the SELF-CORRECTION method. However, a potential weakness lies in the complexity of the methodology, which may introduce challenges in understanding and implementing the approach, particularly for practitioners unfamiliar with iterative correction frameworks. Additionally, while the experiments demonstrate improvements, further exploration of the method's performance in more diverse contexts could enhance its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, with a coherent flow from methodology to experimental results. The quality of the writing is high, making it accessible to a broad audience in the NLP community. The novelty of the SELF-CORRECTION framework lies in its dual-module structure, which is a significant departure from traditional methods. The reproducibility of the results is supported by detailed descriptions of the algorithms, datasets, and experimental setups, although the authors could further bolster reproducibility by providing code and data for their experiments.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the field of sequence generation with its innovative SELF-CORRECTION approach, demonstrating strong empirical results across multiple tasks. While the complexity of the methodology may pose some barriers to understanding, the potential for application and future research is substantial.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a novel self-correction framework termed SELF-CORRECTION, which segregates the process of sequence generation into two distinct components: a base generator and a corrector. The base generator is tasked with producing initial hypotheses, while the corrector iteratively refines these outputs using feedback, which can be either scalar scores or natural language. The mathematical formulation of the model is articulated as \\( p(y|x) = \\sum_{y_0} p_0(y_0|x) p_\\theta(y|y_0, x) \\), allowing for modularity in design. The framework demonstrates significant performance improvements across three diverse tasks\u2014mathematical program synthesis, lexically-constrained generation, and toxicity control\u2014highlighting its robustness and scalability.\n\n# Strength And Weaknesses\nStrengths of the paper include its modular approach, which allows for independent optimization of the generator and corrector, leading to robust performance across various tasks without extensive retraining. The integration of explicit feedback is a notable advantage, enhancing the output quality of the generated sequences. However, the methodology is not without limitations; the performance of the corrector is dependent on the quality of the feedback, which can vary widely. Additionally, the computational intensity of the online training process may pose challenges, particularly with larger models, and there is a risk of the corrector learning to correct flawed outputs without addressing the generator's underlying issues.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology, making it accessible for readers. The quality of the experiments is commendable, with a thorough evaluation across multiple tasks and a solid selection of datasets. The novelty of the approach lies in its unique decoupling of generation and correction, which is a significant advancement in the field. However, the reproducibility may be impacted by the complexity of the training setup and the reliance on feedback mechanisms that may not generalize across different contexts.\n\n# Summary Of The Review\nOverall, the paper presents a compelling framework for enhancing sequence generation through a self-correction mechanism, demonstrating notable empirical gains across various applications. While the approach is innovative and well-executed, challenges related to feedback dependency and training complexity warrant consideration. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a novel framework named SELF-CORRECTION, which enhances sequence generation by decoupling the generation and correction processes. The methodology involves a base generator (language model) that produces initial outputs, followed by a corrector that iteratively refines these outputs. The authors demonstrate the effectiveness of this approach across various tasks, including mathematical program synthesis, lexically-constrained generation, and toxicity reduction, reporting significant performance improvements over traditional single-pass generation models.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative approach to sequence generation, allowing for iterative refinement and leveraging the initial outputs to improve quality. The empirical results are compelling, showing substantial performance gains across diverse tasks and datasets. However, a potential weakness is the reliance on the corrector's performance, which may limit generalizability if the corrector is not well-tuned for specific tasks. Additionally, while the paper provides a solid theoretical foundation, the complexity of the methodology may pose challenges for reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The use of mathematical notation to describe the output distribution adds clarity to the theoretical framework. However, the complexity of the self-correction process could hinder reproducibility for practitioners unfamiliar with the intricacies of the approach. The empirical evaluations are thorough, but further details on hyperparameter tuning and specific implementations would enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in sequence generation through its SELF-CORRECTION framework, showcasing improved performance across multiple tasks. While the methodology is innovative and the results are promising, ensuring reproducibility may require additional detail and clarification.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5"
      ],
      "preparations": [
        null,
        "# Notes on \"Generating Sequences by Learning to [Self]-Correct\"\n\n## Authors\n- Sean Welleck\n- Ximing Lu\n- Peter West\n- Faeze Brahman\n- Tianxiao Shen\n- Daniel Khashabi\n- Yejin Choi\n\n## Abstract\n- Introduces SELF-CORRECTION, a method for sequence generation that incorporates a corrector to iteratively refine outputs from a base generator.\n- Addresses issues with current language models that violate semantic constraints and lack self-correction capabilities.\n- Demonstrates improvements in three tasks: mathematical program synthesis, lexically-constrained generation, and toxicity control.\n\n## Introduction\n- Traditional sequence generation is a single-pass process, leading to inefficiencies when minor adjustments are needed.\n- SELF-CORRECTION re-frames generation into two modules: a base generator and a corrector for iterative refinement.\n- The corrector can utilize scalar or natural language feedback to improve outputs.\n\n## Methodology\n### Self-Correcting Sequence Generators\n- The generation process is formalized as:\n  \\( p(y|x) = \\sum_{y_0} p_0(y_0|x) \\cdot p_\\theta(y|y_0, x) \\)\n- The corrector can iteratively improve the output, allowing multiple applications for refinement.\n\n### Learning a Corrector\n- The corrector is trained to enhance the quality of the generator's output based on a scalar value function \\( v(y) \\).\n- Introduces a self-corrective learning algorithm that pairs hypotheses with corrections to improve quality iteratively.\n\n#### Algorithm Overview\n1. **Initialization**: Create a data pool of generated outputs and their respective values.\n2. **Pairing**: Form value-improving pairs of input and output.\n3. **Learning**: Use sampled pairs to update the corrector with a cross-entropy loss function.\n4. **Exploration**: Add new generations to the data pool based on the current corrector's output.\n\n### Inference\n- The output is generated by combining the base generator and the corrector in an iterative manner until a satisfactory output is obtained.\n\n## Experiments\n### 3.1 Mathematical Program Synthesis\n- **Setup**: Tasks involve generating correct programs from natural language specifications.\n- **Model**: GPT-Neo 1.3B used as a corrector; tested with both fine-tuned and GPT-3 as base generators.\n- **Results**: Significant improvement in correctness rates across datasets (up to 99% correctness in some settings).\n\n### 3.2 Lexically Constrained Generation\n- **Datasets**: COMMONGEN and E2E, requiring generated sentences to include specific words.\n- **Results**: SELF-CORRECTION improved constraint satisfaction without compromising fluency, outperforming baselines.\n\n### 3.3 Toxicity Reduction\n- **Dataset**: REALTOXICITYPROMPTS benchmark used to evaluate the reduction of toxic outputs.\n- **Results**: SELF-CORRECTION achieved lower toxicity scores compared to several strong baselines, maintaining fluency and diversity.\n\n### 3.4 Correcting Larger Generators\n- Demonstrated that the corrector could improve outputs from larger generators (like GPT-3) effectively.\n\n### 3.5 Leveraging Explicit Feedback\n- Introduced natural language feedback during training and inference.\n- Results showed improvements in performance across all tasks when using explicit feedback.\n\n### 3.6 Additional Ablations and Analysis\n- Investigated effects of multiple corrections, pairing, proportional sampling, and exploration.\n- Findings indicated that more corrections and careful sampling enhance performance.\n\n## Related Work\n- Discusses previous approaches to iterative correction in text and program synthesis.\n- Highlights the flexibility of SELF-CORRECTION compared to existing methods, emphasizing its generality and minimal assumptions.\n\n## Conclusion\n- SELF-CORRECTION effectively decouples generation and correction, leading to improved performance across varied tasks.\n- The framework supports the integration of natural language feedback, showcasing potential for future research in sequence generation and correction. \n\n## References\n- Numerous references to foundational works in NLP, program synthesis, and machine learning are included to provide context and support for methodologies discussed in the paper. \n\n## Appendix\n- Additional experimental details, including hyperparameters for different tasks and human evaluation methods, are provided. \n\nThis summary encapsulates the key components, methodologies, results, and contributions of the paper \"Generating Sequences by Learning to [Self]-Correct\" while maintaining fidelity to the original content.",
        "### Technical Content\n\n- **Self-Correction Framework**: The paper introduces a framework called SELF-CORRECTION, which decouples sequence generation into two components: a base generator and a corrector. The base generator produces an initial hypothesis, while the corrector iteratively improves this output.\n\n- **Training Process**: The corrector is trained through an online procedure that utilizes either scalar feedback (e.g., correctness scores) or natural language feedback on the initial generations to refine the outputs.\n\n- **Mathematical Formulation**: The proposed model is represented as:\n  \\[\n  p(y|x) = \\sum_{y_0} p_0(y_0|x) p_\\theta(y|y_0, x)\n  \\]\n  where \\(p_0\\) is the base generator and \\(p_\\theta\\) is the corrector.\n\n- **Three Key Properties**:\n  1. **Decoupling**: The generator and corrector are independent, allowing for flexible parameterization and optimization.\n  2. **Task-specific Objective**: The corrector can be trained with various objectives tailored to specific tasks.\n  3. **Feedback Utilization**: The corrector can leverage explicit feedback to enhance the quality of outputs.\n\n### Experimental Design\n\n- **Tasks Evaluated**:\n  1. **Mathematical Program Synthesis**: Generating correct programs from natural language specifications.\n  2. **Lexically-Constrained Generation**: Ensuring generated texts include specific required words.\n  3. **Toxicity Control**: Reducing the generation of toxic language.\n\n- **Experimental Setup**: \n  - Two different sizes of generators (GPT-Neo 1.3B and GPT-3) were used.\n  - Various datasets were employed:\n    - **Mathematical Synthesis**: MultiArith, GSM8k.\n    - **Lexically-Constrained**: COMMONGEN, E2E.\n    - **Toxicity Reduction**: REALTOXICITYPROMPTS.\n\n- **Evaluation Metrics**: \n  - Correctness rates for mathematical synthesis.\n  - Constraint satisfaction for lexically-constrained generation.\n  - Toxicity scores and fluency metrics for toxicity control.\n\n### Results\n\n- **Mathematical Program Synthesis**: \n  - The SELF-CORRECT approach significantly improved correctness rates (e.g., from 8.57% to 24% on GSM).\n  - The performance gains were substantial, with improvements of up to 38 points in some settings.\n\n- **Lexically-Constrained Generation**: \n  - Self-Correction enhanced constraint satisfaction and maintained fluency across tasks, outperforming other methods like NeuroLogic-A*.\n\n- **Toxicity Control**: \n  - The approach effectively reduced the toxicity rates compared to existing baseline models, maintaining fluency and diversity in outputs.\n\n- **Generalization**: \n  - The corrector trained on smaller generators could effectively improve larger generators, demonstrating transferability.\n\n### Strengths\n\n- **Modularity**: The framework's separation of generation and correction allows for flexibility in modeling and optimization.\n- **Robust Performance**: The method consistently improved results across diverse tasks without necessitating extensive retraining of the base generator.\n- **Feedback Integration**: The ability to incorporate explicit feedback enhances the training process, leading to better corrections.\n- **Scalability**: The corrector can operate effectively with both smaller and larger generators, broadening its applicability.\n\n### Limitations\n\n- **Dependency on Feedback Quality**: The performance of the corrector heavily relies on the quality of the feedback received, which can vary across tasks.\n- **Training Complexity**: The online training process may be computationally intensive, especially for larger models or extensive datasets.\n- **Generalizability of Feedback Mechanism**: While the framework is designed to be flexible, the specific implementations of feedback mechanisms may not generalize well across all tasks or domains.\n- **Potential for Repeated Errors**: If the base generator consistently produces flawed outputs, the corrector might learn to correct those mistakes without addressing the underlying issue. \n\n### Conclusion\n\nThe paper presents a novel self-correction framework that enhances sequence generation tasks by allowing iterative improvements through a separate corrector model. The results demonstrate substantial performance gains across various applications, showcasing the potential of combining generative models with corrective mechanisms. However, the effectiveness of the approach is contingent upon the quality of feedback and the complexity of the training process.",
        "# Notes on \"Generating Sequences by Learning to [Self]-Correct\"\n\n## Theoretical Foundations\n- **Self-Correction Framework**: The paper introduces a method called SELF-CORRECTION that decouples the generation and correction processes in sequence generation tasks. It proposes a base generator (language model) followed by a corrector that iteratively improves the initial output.\n- **Single-pass vs. Iterative Correction**: Traditional models generate outputs in a single pass, often discarding useful intermediate results. The SELF-CORRECTION approach allows for refinement by leveraging the generator's initial output as a starting point.\n- **Mathematical Representation**: \n  - The output distribution is expressed as:\n    \\[\n    p(y|x) = \\sum_{y_0} p_0(y_0|x) p_\\theta(y|y_0, x)\n    \\]\n  - The corrector can be applied multiple times, represented as:\n    \\[\n    p(y_T|x) = \\sum_{t} p_\\theta(y_{t+1}|y_t, x)\n    \\]\n\n## Experimental Methodology\n1. **Self-Corrective Learning Algorithm**:\n   - **Initialization**: Starts with a generator \\( p_0 \\) and a corrector \\( p_\\theta \\), forming a datapool \\( D \\) by sampling outputs.\n   - **Value Function**: A scalar value function \\( v(y) \\) is used to assess output quality (e.g., correctness, toxicity).\n   - **Pairing**: Forms value-improving pairs from the datapool where a hypothesis \\( y \\) improves to a higher-valued correction \\( y' \\).\n   - **Learning**: Updates the corrector using cross-entropy loss on pairs sampled based on improvement in value and proximity.\n   - **Exploration**: Adds new generations to the datapool by generating from the current corrector.\n\n2. **Tasks Evaluated**:\n   - **Mathematical Program Synthesis**: Generating executable programs from natural language descriptions.\n   - **Lexically-Constrained Generation**: Ensuring generated sentences include specified constraint words.\n   - **Toxicity Reduction**: Generating fluent text while avoiding toxic language.\n\n## Results\n- **Mathematical Program Synthesis**:\n  - Utilized datasets: MultiArith, AddSub, SingleOp, SVAMP, and GSM8k.\n  - Improvements observed:\n    - SELF-CORRECTION significantly outperformed the base generator, achieving up to 99.17% correct outputs in MultiArith.\n    - Table 1 summarizes results, showing improvements across various configurations and settings.\n\n- **Lexically-Constrained Generation**:\n  - Evaluated on COMMONGEN and E2E datasets.\n  - SELF-CORRECTION improved constraint satisfaction while maintaining fluency, outperforming complex decoding algorithms like NeuroLogic.\n  - Table 2 presents metrics indicating better performance in terms of fluency and constraint coverage.\n\n- **Toxicity Reduction**:\n  - Utilized REALTOXICITYPROMPTS for evaluating toxicity suppression.\n  - SELF-CORRECTION demonstrated lower toxicity rates compared to baseline models and other methods (PPLM, GeDi, etc.).\n  - Table 3 indicates substantial reductions in toxicity while maintaining fluency.\n\n- **Correcting Larger Generators**:\n  - The paper also explores the transferability of the corrector to larger generators (e.g., GPT-3), showing that improvements can be achieved even when the corrector is smaller. \n  - Results in Table 4 indicate improved performance on both math synthesis and toxicity reduction tasks when applying the corrector to outputs generated by larger models.\n\n- **Explicit Feedback Utilization**:\n  - The paper examines the impact of incorporating explicit natural language feedback during training and inference.\n  - Results indicate performance gains across all tasks when feedback was utilized (Table 5).\n\n## Implications\n- **Efficiency**: The SELF-CORRECTION framework provides an efficient alternative to traditional models, allowing for iterative refinement without requiring extensive retraining of the base generator.\n- **Flexibility**: The decoupling of generation and correction allows the use of various corrector models, enhancing adaptability across tasks.\n- **Future Directions**: The authors suggest potential future work could explore further the integration of explicit feedback mechanisms and the use of reinforcement learning as an alternative training method for the corrector.\n\n## Experimental Details\n- **Mathematical Program Synthesis**: Setup involved fine-tuning GPT-Neo 1.3B as both the generator and corrector, with correctness defined as the output executing the correct answer.\n- **Lexically-Constrained Generation**: The value function focused on coverage, with specific parameter settings for the base generator and corrector detailed in the appendix.\n- **Toxicity Reduction**: Implemented using the Perspective API to evaluate toxicity, with comparisons against state-of-the-art baselines.\n\n## Additional Findings\n- **Ablation Studies**: Various ablation studies (effect of multiple corrections, pairing, and proportional sampling) demonstrated the importance of these components in enhancing performance.\n- **Qualitative Examples**: Provided throughout the paper illustrate the self-correction process across different tasks, showcasing the ability to fix logical errors and improve overall output quality.\n\nThis paper presents a comprehensive framework for improving sequence generation through self-correction, with empirical evidence supporting its efficacy across varied tasks and applications."
      ]
    },
    {
      "context": "Published as a conference paper at ICLR 2023\n\nA STATISTICAL FRAMEWORK FOR PERSONALIZED FEDERATED LEARNING AND ESTIMATION: THEORY, ALGORITHMS, AND PRIVACY\n\nKaan Ozkara\u2217, Antonious M. Girgis\u2217, Deepesh Data & Suhas Diggavi Department of Electrical and Computer Engineering University of California, Los Angeles {kaan,amgirgis}@ucla.edu,deepesh.data@gmail.com,suhas@ee.ucla.edu\n\nABSTRACT\n\nA distinguishing characteristic of federated learning is that the (local) client data could have statistical heterogeneity. This heterogeneity has motivated the design of personalized learning, where individual (personalized) models are trained, through collaboration. There have been various personalization methods proposed in literature, with seemingly very different forms and methods ranging from use of a single global model for local regularization and model interpolation, to use of multiple global models for personalized clustering, etc. In this work, we begin with a statistical framework that unifies several different algorithms as well as suggest new algorithms. We apply our framework to personalized estimation, and connect it to the classical empirical Bayes\u2019 methodology. We develop novel private personalized estimation under this framework. We then use our statistical framework to propose new personalized learning algorithms, including AdaPeD based on information-geometry regularization, which numerically outperforms several known algorithms. We develop privacy for personalized learning methods with guarantees for user-level privacy and composition. We numerically evaluate the performance as well as the privacy for both the estimation and learning problems, demonstrating the advantages of our proposed methods.\n\n1\n\nINTRODUCTION\n\nThe federated learning (FL) paradigm has had huge recent success both in industry and academia (McMahan et al., 2017; Kairouz et al., 2021), as it enables to leverage data available in dispersed devices for learning while maintaining data privacy. Yet, it was recently realized that for some applications, due to the statistical heterogeneity of local data, a single global learning model may perform poorly for individual clients. This motivated the need for personalized learning achieved through collaboration, and there have been a plethora of personalized models proposed in the literature as well (Fallah et al., 2020; Dinh et al., 2020; Deng et al., 2020; Mansour et al., 2020; Acar et al., 2021; Li et al., 2021; Ozkara et al., 2021; Zhang et al., 2021; Hu et al., 2020). However, the proposed approaches appear to use very different forms and methods, and there is a lack of understanding of an underlying fundamental statistical framework. Such a statistical framework could help develop theoretical bounds for performance, suggest new algorithms as well as perhaps give grounding to known methods. Our work addresses this gap.\n\nIn particular, we consider the fundamental question of how one can use collaboration to help personalized learning and estimation for users who have limited data that they want to keep private. Our proposed framework is founded on the requirement not only of personalization but also privacy, as maintaining local data privacy is what makes the federated learning framework attractive - and thus any algorithm that aims to be impactful needs to also give formal privacy guarantees. The goal of this paper is to develop a statistical framework that leads to new algorithms with provable privacy guarantees, and performance bounds. Our main contributions are (i) Development of a statistical framework for federated personalized estimation and learning (ii) Theoretical bounds and\n\n\u2217Equal Contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nnovel algorithms for private personalized estimation (iii) Design and privacy analysis of new private personalized learning algorithms; as elaborated below. Omitted proofs/details are in appendices.\n\n\u2022 Statistical framework: We connect this problem to the classical empirical Bayes\u2019 method, pioneered by Stein (1956); James & Stein (1961); Robbins (1956), which proposed a hierarchical statistical model Gelman et al. (2013). This is modeled by an unknown population distribution P from which local parameters {\u03b8i} are generated, which in turn generate the local data through the distribution Q(\u03b8i). Despite the large literature on this topic, especially in the context of statistical estimation, creating a framework for FL poses new challenges. In contrast to classical empirical Bayes\u2019 estimation, we introduce a distributed setting and develop a framework that allows information (communication and privacy) constraints1. This framework enables us to develop statistical performance bounds as well as suggests (private) personalized federated estimation algorithms. Moreover, we develop our framework beyond estimation, for (supervised) distributed learning, where clients want to build local predictive models with limited local (labeled) samples; we develop this framework in Section 3, which leads to new (private) personalized learning algorithms.\n\n\u2022 Private personalized estimation: Our goal is to estimate individual (local) parameters, when each user has very limited (heterogeneous) data. Such a scenario motivates federated estimation of individual parameters, privately. More precisely, the users observe data generated by an unknown distribution parametrized by their individual (unknown) local parameters \u03b8i, and want to estimate their local parameters \u03b8i leveraging very limited local data; see Section 2 for more details. For the hierarchical statistical model, classical results have shown that one can enhance the estimate of individual parameters based on the observations of a population of samples, despite having independently generated parameters from an unknown population distributions. However, this has not been studied for the distributed case, with privacy and communication constraints, which we do (see Theorem 2 for the Gaussian case and Theorem 4 for the Bernoulli case, and also for mixture population models in Appendix D). We estimate the (parametrized) population distribution under these privacy and communication constraints and use this as an empirical prior for local estimation. The effective amplification of local samples through collaboration, in Section 2, gives us theoretical insight about when collaboration is most useful, under privacy and/or communication constraints. Our results suggest how to optimally balance estimates from local and population models. We also numerically evaluate these methods, including application to polling data (see Section 4 and Appendices) to show advantages of such collaborative estimation compared to local methods.\n\n\u2022 Private personalized learning: The goal here is to obtain individual learning models capable of predicting labels with limited local data in a supervised learning setting. This is the use case for federated learning with privacy guarantees. It is intimately related to the estimation problem with distinctions including (i) to design good label predictors rather than just estimate local parameters (ii) the focus on iterative methods for optimization, requiring strong compositional privacy guarantees. Therefore, the statistical formulation for learning has a similar flavor to that in estimation, where there is a population model for local (parametrized) statistics for labeled data; see Section 3 for more details. We develop several algorithms, including AdaPeD (in Section 3.2), AdaMix (in Section 3.1), and DP-AdaPeD (in Section 3.3), inspired by the statistical framework. AdaPeD uses information divergence constraints along with adaptive weighting of local models and population models. By operating in probability (rather than Euclidean) space, using information-geometry (divergence), enables AdaPeD to operate with different local model sizes and architectures, giving it greater flexibility than existing methods. We integrate it with user-level privacy to develop DP-AdaPeD, with strong compositional privacy guarantees (Theorem 5). AdaMix is inspired by mixture population distributions, which adaptively weighs multiple global models and combines it with local data for personalization. We numerically evaluate these algorithms for synthetic and real data in Section 4.\n\nRelated Work. Our work can be seen in the intersection of personalized learning, estimation, and privacy. Below we give a brief description of related work; a more detailed comparison which connects our framework to other personalized algorithms is given in Appendix J.\n\nPersonalized FL: Recent work adopted different approaches for learning personalized models, which can be explained by our statistical framework for suitable choices of population distributions as explained in Appendix J: These include, meta-learning based methods (Fallah et al., 2020; Acar et al., 2021; Khodak et al., 2019); regularization (Deng et al., 2020; Mansour et al., 2020; Hanzely\n\n1The homogeneous case for distributed estimation is well-studied; see (Zhang, 2016) and references.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n& Richt \u0301arik, 2020); clustered FL (Zhang et al., 2021; Mansour et al., 2020; Ghosh et al., 2020; Smith et al., 2017) (Marfoq et al., 2021); using knowledge distillation (Lin et al., 2020; Ozkara et al., 2021); multi-task Learning (Dinh et al., 2020; Hanzely & Richt \u0301arik, 2020; Smith et al., 2017; Vanhaesebrouck et al., 2017; Zantedeschi et al., 2020); and using common representations (Du et al., 2021; Raghu et al., 2020; Tian et al., 2020; Collins et al., 2021) and references therein. Our work enables not just a unified view of these methods, but suggests new algorithms developed in this paper, along with privacy guarantees.\n\nAfter the conclusion of our work (Ozkara et al., 2022, July), we found two concurrent and independent works (Kotelevskii et al., 2022, June; Chen et al., 2022) that use a hierarchical Bayes approach to construct personalized learning algorithms, and are closest to our statistical framework. (Kotelevskii et al., 2022, June) is based on using a MCMC method2 to estimate a population distribution; such methods could be computationally intensive (see the discussion in (Blei et al., 2003); (Chen et al., 2022) considers the unimodal Gaussian prior, and effectively does what the classical empirical Bayes suggests (see also Theorem 1). None of these works consider privacy, which we do both for estimation and learning algorithms (see Theorems 2, 4, Appendix D, and for DP-AdaPeD in Theorem 5). Note that MCMC methods could have detrimental privacy implications. Also, they do not include information-geometric techniques (like our AdaPeD) or methods inspired by mixture distributions (e.g., AdaMix).\n\nPrivacy for Personalized Learning. There has been a lot of work in privacy for FL when the goal is to learn a single global model (see (Girgis et al., 2021b) and references therein); though there are fewer papers that address user-level privacy (Liu et al., 2020; Levy et al., 2021; Ghazi et al., 2021). There has been more recent work on applying these ideas to learn personalized models (Girgis et al., 2022; Jain et al., 2021b; Geyer et al., 2017; Hu et al., 2020; Li et al., 2020). These are for specific algorithms/models, e.g., Jain et al. (2021b) focuses on the common representation model for linear regression described earlier or on item-level privacy (Hu et al., 2020; Li et al., 2020). We believe that DP-AdaPeD proposed in this paper is among the first user-level private personalized learning algorithms with compositional guarantees, applicable to general deep learning architectures.\n\n2 PERSONALIZED ESTIMATION\n\nWe consider a client-server architecture, where there are m clients. Let P(\u0393) denote a global population distribution that is parameterized by an unknown \u0393 and let \u03b81, . . . , \u03b8m are sampled i.i.d. from P(\u0393) and are unknown to the clients. Client i is given a dataset Xi := (Xi1, . . . , Xin), where Xij, j \u2208 [n] are sampled i.i.d. from some distribution Q(\u03b8i), parameterized by \u03b8i \u2208 Rd. Note that heterogeneity in clients\u2019 datasets is induced through the variance in P(\u0393), and if the variance of P(\u0393) is zero, then all clients observe i.i.d. datasets sampled from the same underlying distribution.\n\nThe goal at client i for all i \u2208 [m] is to estimate \u03b8i through the help of the server. We focus on one-round communication schemes, where client j applies a (potentially randomized) mechanism q on its dataset Xj and sends qj := q(Xj) to the server, who aggregates the received messages, which is denoted by Agg(q1, . . . , qm), and broadcasts that to all clients. Based on (Xi, Agg(q1, . . . , qm)), client i outputs an estimate (cid:98)\u03b8i of \u03b8i. We measure the performance of (cid:98)\u03b8i through the Bayesian risk for mean squared error (MSE), as defined below (where P is the true prior distribution with associated density \u03c0, \u03b8i \u223c P is the true local parameter, and (cid:98)\u03b8i = (cid:98)\u03b8(Xi, Agg(q1, . . . , qm)) is the estimator): (cid:90)\n\nE\u03b8i\u223cPE\n\n(cid:98)\u03b8i,q,X1,...,Xm\n\n(cid:107)(cid:98)\u03b8i \u2212 \u03b8i(cid:107)2 =\n\nE\n\n(cid:98)\u03b8i,q,X1,...,Xm\n\n(cid:107)(cid:98)\u03b8i \u2212 \u03b8i(cid:107)2\u03c0(\u03b8i)d\u03b8i.\n\n(1)\n\nThe above statistical framework can model many different scenarios, and we will study in detail three settings: Gaussian and Bernoulli models (Sections 2.1, 2.2 below), and Mixture model (Appendix D).\n\n2.1 GAUSSIAN MODEL\n\nId) for all i \u2208 [m], which In the Gaussian setting, P(\u0393) = N (\u03bc, \u03c32 Id) i.i.d. for i \u2208 [m]. implies that \u03b81, . . . , \u03b8m \u223c N (\u03bc, \u03c32 Here, \u03c3\u03b8 \u2265 0, \u03c3x > 0 are known, and \u03bc, \u03b81, . . . , \u03b8m are unknown. For the case of a single local\n\nId) and Q(\u03b8i) = N (\u03b8i, \u03c32 Id) i.i.d. and Xi1, . . . , Xin \u223c N (\u03b8i, \u03c32\n\nx\n\nx\n\n\u03b8\n\n\u03b8\n\n2In our understanding their numerics seem to be restricted to a unimodal Gaussian population model.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nsample this is identical to the classical James-Stein estimator (James & Stein, 1961); Theorem 1 does a simple extension for multiple local samples and is actually a stepping stone for the information constrained estimation result of Theorem 2. Omitted proofs/details are provided in Appendix B.\n\nOur proposed estimator. Since there is no distribution on \u03bc, and given \u03bc, we know the distribution of \u03b8i\u2019s, and subsequently, of Xij\u2019s. So, we consider the maximum likelihood estimator:\n\n(cid:98)\u03b81, . . . , (cid:98)\u03b8m, (cid:98)\u03bc := arg max\n\n\u03b81,...,\u03b8m,\u03bc\n\np{\u03b8i,Xi}|\u03bc (\u03b81, . . . , \u03b8m, X1, . . . , Xm|\u03bc)\n\n(2)\n\nTheorem 1. Solving (2) yields the following closed form expressions for (cid:98)\u03bc and (cid:98)\u03b81, . . . , (cid:98)\u03b8m:\n\n(cid:98)\u03bc =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nX i\n\nand\n\n(cid:98)\u03b8i = aX i + (1 \u2212 a)(cid:98)\u03bc, for i \u2208 [m],\n\nwhere a =\n\nThe above estimator achieves the MSE: E\u03b8i,X1,...,Xm (cid:107)(cid:98)\u03b8i \u2212 \u03b8i(cid:107)2 \u2264 d\u03c32\n\nx\n\nn\n\n(cid:0) 1\u2212a\n\nm + a(cid:1).\n\n\u03c32 \u03b8\n\u03b8 + \u03c32\n\n\u03c32\n\nx/n\n\n.\n\n(3)\n\nn\n\nm\n\n(cid:80)n\n\n(cid:80)m\n\ni=1 qi back to the clients.\n\nx/mn. Otherwise, when \u03c32\n\nIt follows that the mechanism q and the aggregation function Agg for the estimators in (3) (as described in (1)) are just the average functions, where client i sends qi = q(Xi) := X i = 1 j=1 Xij to the server, and the server sends (cid:98)\u03bc := Agg(q1, . . . , qm) = 1 Remark 1 (Personalized estimate vs. local estimate). When \u03c3\u03b8 \u2192 0, then a \u2192 0, which implies that (cid:98)\u03b8i \u2192 (cid:98)\u03bc and MSE \u2192 d\u03c32 x/n or n \u2192 \u221e, then a \u2192 1, which implies that (cid:98)\u03b8i \u2192 X i and MSE \u2192 d\u03c32 x/n. These conform to the facts that (i) when there is no heterogeneity, then the global average is the best estimator, and (ii) when heterogeneity is not small, and we have a lot of local samples, then the local average is the best estimator. Observe that the multiplicative gap between the MSE of the proposed personalized estimator and the MSE of the local estimator (based on local data only, which gives an MSE of d\u03c32 m + a) \u2264 1 that proves the superiority of the personalized model over the local model, which is equal to 1/m when \u03c3\u03b8 = 0 and equal to 0.01 when m = 104, n = 100 and \u03c32 Remark 2 (Optimality of our personalized estimator). In Appendix B, we show the minimax lower m + a(cid:1), which exactly matches the upper bound: inf bound on the MSE in Theorem 1, thus establishes the optimality our personalized estimator in (3).\n\n\u03b8 is large in comparison to \u03c32\n\nx)(cid:107)(cid:98)\u03b8(X) \u2212 \u03b8(cid:107)2 \u2265 d\u03c32\n\n\u03b8 = 10\u22123, for example.\n\nx/n) is given by ( 1\u2212a\n\nEX\u223cN (\u03b8,\u03c32\n\nx = 10, \u03c32\n\n(cid:98)\u03b8 sup\u03b8\u2208\u0398\n\n(cid:0) 1\u2212a\n\nn\n\nx\n\nPrivacy and communication constraints. Observe that the scheme presented above does not protect privacy of clients\u2019 data and messages from the clients to the server can be made communicationefficient. These could be achieved by employing specific mechanisms q at clients: For privacy, we can take a differentially-private q, and for communication-efficiency, we can take q to be a quantizer. Inspired by the scheme presented above, here we consider q to be a function q : Rd \u2192 Y, that takes the average of n data points as its input, and the aggregator function Agg to be the average function. Define (cid:98)\u03bcq := 1\n\ni=1 q(X i) and consider the following personalized estimator for the i-th client:\n\n(cid:80)m\n\nm\n\n(cid:98)\u03b8i = aX i + (1 \u2212 a)(cid:98)\u03bcq,\n\nfor some a \u2208 [0, 1].\n\n(4)\n\nTheorem 2. Suppose for all x \u2208 Rd, q satisfies E[q(x)] = x and E(cid:107)q(x) \u2212 x(cid:107)2 \u2264 d\u03c32 finite \u03c3q. Then the personalized estimator in (4) has MSE:\n\nq for some\n\nE\u03b8i,q,X1,...,Xm(cid:107)(cid:98)\u03b8i \u2212 \u03b8i(cid:107)2 \u2264\n\nd\u03c32 x\nn\n\n(cid:16) 1 \u2212 a m\n\n(cid:17)\n\n+ a\n\nwhere\n\na =\n\n\u03c32 \u03b8 + \u03c32\n\n\u03c32\n\n\u03b8 + \u03c32\n\nq/m\u22121\n\nq/m\u22121 + \u03c32\n\nx/n\n\n.\n\n(5)\n\nFurthermore, assuming \u03bc \u2208 [\u2212r, r]d for some constant r (but \u03bc is unknown), we have:\n\n1. Communication efficiency: For any k \u2208 N, there is a q whose output can be represented using k-bits (i.e., q is a quantizer) that achieves the MSE in (5) with probability at least 1 \u2212 2/mn and with \u03c3q = b\n\n(cid:112)log(m2n) + \u03c3x\u221a\n\n(cid:112)log(m2n).\n\n(2k\u22121) , where b = r + \u03c3\u03b8\n\nn\n\n2. Privacy: For any (cid:15)0 \u2208 (0, 1), \u03b4 > 0, there is a q that is user-level ((cid:15)0, \u03b4)-locally differentially private, that achieves the MSE in (5) with probability at least 1 \u2212 2/mn and with \u03c3q = b\n(cid:15)0\n\n(cid:112)8 log(2/\u03b4), where b = r + \u03c3\u03b8\n\n(cid:112)log(m2n) + \u03c3x\u221a\n\n(cid:112)log(m2n).\n\nn\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n2.2 BERNOULLI MODEL\n\nFor the Bernoulli model, P is supported on [0, 1], and p1, . . . , pm are sampled i.i.d. from P, and client i is given n i.i.d. samples Xi1, . . . , Xin \u223c Bern(pi). This setting has been studied in (Tian et al. (2017); Vinayak et al. (2019)) for estimating P, whereas, our goal is to estimate individual parameter pi at client i using the information from other clients. In order to derive a closed form MSE result, we assume that P is the Beta distribution.3 Here, \u0393 = (\u03b1, \u03b2), p1, . . . , pm are unknown, and client i\u2019s goal is to estimate pi such that the Bayesian risk Epi\u223c\u03c0E (cid:98)pi,X1,...,Xm((cid:98)pi \u2212 pi)2 is minimized, where \u03c0 denotes the density of the Beta distribution. Omitted proofs/details are provided in Appendix C.\n\nAnalogous to the Gaussian case, we can show that if \u03b1, \u03b2 are known, then the posterior mean estimator has a closed form expression: (cid:98)pi = aX i + (1 \u2212 a) \u03b1 \u03b1+\u03b2 , where a = n/\u03b1+\u03b2+n and \u03b1/(\u03b1+\u03b2) is the mean of the beta distribution. When \u03b1, \u03b2 are unknown, inspired by the above discussion, a natural approach would be to estimate the global mean \u03bc = \u03b1/(\u03b1+\u03b2) and the weight a = n/(\u03b1+\u03b2+n), and use that in the above estimator. Note that, for a we need to estimate \u03b1 + \u03b2, which is equal to \u03bc(1\u2212\u03bc)/\u03c32 \u2212 1, where \u03c32 = \u03b1\u03b2/(\u03b1+\u03b2)2(\u03b1+\u03b2+1) is the variance of the beta distribution. Therefore, it is enough to estimate \u03bc and \u03c32 for the personalized estimators {(cid:98)pi}. In order to make calculations simpler, instead of making one estimate of \u03bc, \u03c32 for all clients, we let each client make its own estimate of \u03bc, \u03c32 (without using i = 1 their own data) as: (cid:98)\u03bci = 1 l(cid:54)=i(X l \u2212 (cid:98)\u03bcl)2,4 and then define the local weight as (cid:98)ai =\n\ni \u22121+n . Now, client i uses the following personalized estimator:\n\nl(cid:54)=i X l and (cid:98)\u03c32\n\n(cid:98)\u03bci(1\u2212 (cid:98)\u03bci)/(cid:98)\u03c32\n\nm\u22122\n\nm\u22121\n\n(cid:80)\n\n(cid:80)\n\nn\n\n(cid:98)pi = (cid:98)aiX i + (1 \u2212 (cid:98)ai)(cid:98)\u03bci.\n\n(6)\n\nTheorem 3. With probability at least 1\u2212 1 (\u03b1+\u03b2)2(\u03b1+\u03b2+1) + 3 log(4m2n) Epi\u223c\u03c0EX1,...,Xm ((cid:98)pi \u2212 pi)2 \u2264 E[(cid:98)a2 Remark 3. When n \u2192 \u221e, then (cid:98)ai \u2192 1, which implies that MSE tends to the MSE of the local estimator X i, which means if local samples are abundant, collaboration does not help much. When \u03c32 = \u03b1\u03b2/(\u03b1+\u03b2)2(\u03b1+\u03b2+1) \u2192 0, i.e. there is very small heterogeneity in the system, then (cid:98)ai \u2192 0, which implies that MSE tends to the error due to moment estimation (the last term in the MSE in Theorem 3).\n\nmn , the MSE of the personalized estimator in (6) is given by: (cid:1).\n\n(cid:1) + E[(1 \u2212(cid:98)ai)2](cid:0)\n\n\u03b1\u03b2 n(\u03b1+\u03b2)(\u03b1+\u03b2+1)\n\ni ](cid:0)\n\nm\u22121\n\n\u03b1\u03b2\n\nPrivacy constraints. For any privacy parameter (cid:15)0 > 0 and input x \u2208 [0, 1], define qpriv : [0, 1] \u2192 R:\n\nqpriv(x) =\n\n(cid:40) \u22121\n\ne(cid:15)0 \u22121 w.p. e(cid:15)0 \u22121 w.p.\n\ne(cid:15)0\n\ne(cid:15)0\n\ne(cid:15)0 +1 \u2212 x e(cid:15)0 \u22121 e(cid:15)0 +1 , e(cid:15)0 +1 + x e(cid:15)0 \u22121 e(cid:15)0 +1 .\n\n1\n\n(7)\n\nThe mechanism qpriv is unbiased and satisfies user-level (cid:15)0-LDP. Thus, the ith client sends qpriv(X i) to the server, which computes (cid:98)\u03bcpriv i = 1 =\n\n(cid:80)\n\ni\n\nl\n\ni\n\npriv (cid:98)\u03bc i\n\nn )/(cid:98)\u03c3\n\npriv (1\u2212 (cid:98)\u03bc i\n\n1 m\u22122\n\nl(cid:54)=i(qpriv(X l)) \u2212 (cid:98)\u03bcpriv i =\n\n(cid:80) )2 and sends ((cid:98)\u03bcpriv and uses (cid:98)ppriv defines (cid:98)apriv Theorem 4. With probability at least 1 \u2212 1 fined above is given by: Epi\u223c\u03c0Eqpriv,X1,...,Xm ((cid:98)ppriv (\u03b1+\u03b2)2(\u03b1+\u03b2+1) + (e(cid:15)0 +1)2 log(4m2n) (cid:98)apriv\n\n3(e(cid:15)0 \u22121)2(m\u22121)\n\n)2](cid:0)\n\n2(priv) i\n\n(cid:1).\n\n+n\n\n\u03b1\u03b2\n\ni\n\ni\n\nm\u22121\n\nl(cid:54)=i qpriv(X l) and the variance (cid:98)\u03c32(priv) ) to client i. Upon receiving this, client i\n\ni X i + (1 \u2212 (cid:98)apriv\n\n, (cid:98)\u03c32(priv) i = (cid:98)apriv mn , the MSE of the personalized estimator (cid:98)ppriv )2](cid:0)\n\n)(cid:98)\u03bcpriv to estimate pi.\n\ni \u2212 pi)2 \u2264 E[((cid:98)apriv\n\n\u03b1\u03b2 n(\u03b1+\u03b2)(\u03b1+\u03b2+1)\n\nde- (cid:1) + E[(1 \u2212\n\ni\n\ni\n\ni\n\nSee Remark 4 (in Appendix B) and Remarks 6 and 7 (in Appendix C) for a discussion on privacy, communication efficiency, and client sampling.\n\n3 PERSONALIZED LEARNING\n\nConsider a client-server architecture with m clients. There is an unknown global population distribution P(\u0393)5 over Rd from which m i.i.d. local parameters \u03b81, . . . , \u03b8m \u2208 Rd are sampled. Each client\n\n3Beta distribution has a density Beta(\u03b1, \u03b2) = 1 where B(\u03b1, \u03b2) is a normalizing constant. Its mean is \u03b1\n\nB(\u03b1,\u03b2) x\u03b1\u22121(1\u2212x)\u03b2\u22121 is defined for \u03b1, \u03b2 > 0 and x \u2208 [0, 1],\n\n\u03b1+\u03b2 and the variance is\n\n\u03b1\u03b2\n\n(\u03b1+\u03b2)2(\u03b1+\u03b2+1) .\n\n4Upon receiving {X i} from all clients, the server can compute {(cid:98)\u03bci, (cid:98)\u03c32 i ) to the i-th client. 5For simplicity we will consider this unknown population distribution P to be parametrized by unknown\n\ni } and sends ((cid:98)\u03bci, (cid:98)\u03c32\n\n(arbitrary) parameters \u0393.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\ni \u2208 [m] is provided with a dataset consisting of n data points {(Xi1, Yi1), . . . , (Xin, Yin)}, where Yij\u2019s are generated from (Xij, \u03b8i) using some distribution p\u03b8i(Yij|Xij). Let Yi := (Yi1, . . . , Yin) and Xi := (Xi1, . . . , Xin) for i \u2208 [m]. The underlying statistical model for our setting is given by\n\np{\u03b8i,Yi}|{Xi}(\u03b81, . . . , \u03b8m, Y1, . . . , Ym|X1, . . . , Xm) =\n\nm (cid:89)\n\ni=1\n\np(\u03b8i)\n\nm (cid:89)\n\nn (cid:89)\n\ni=1\n\nj=1\n\np\u03b8i(Yij|Xij).\n\n(8)\n\nNote that if we minimize the negative log likelihood of (8), we would get the optimal parameters:\n\n(cid:98)\u03b81, . . . , (cid:98)\u03b8m := arg min \u03b81,...,\u03b8m\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n\u2212 log(p\u03b8i(Yij|Xij)) +\n\nm (cid:88)\n\ni=1\n\n\u2212 log(p(\u03b8i)).\n\n(9)\n\nHere, fi(\u03b8i) := (cid:80)n j=1 \u2212 log(p\u03b8i(Yij|Xij)) denotes the loss function at the i-th client, which only depends on the local data, and R({\u03b8i}) := (cid:80)m i=1 \u2212 log(p(\u03b8i)) is the regularizer that depends on the (unknown) global population distribution P (parametrized by unknown \u0393). Note that when clients have little data and we have large number of clients, i.e., n (cid:28) m \u2013 the setting of federated learning, clients may not be able to learn good personalized models from their local data alone (if they do, it would lead to large loss). In order to learn better personalized models, clients may utilize other clients\u2019 data through collaboration, and the above regularizer (and estimates of the unknown prior distribution P, through estimating its parameters \u0393) dictates how the collaboration might be utilized. The above-described statistical framework (9) can model many different scenarios, as detailed below:\n\n2\n\n\u03b8,l\n\n2\u03c32\n\nl=1, {\u03bcl}k\n\n2 log(2\u03c0\u03c32\n\n1. When P(\u0393) \u2261 GM({pl}k\n\nl=1, {\u03bcl}k i=1 log (cid:0) (cid:80)k\n\nl=1) is a Gaussian mixture,\n\nl=1, {\u03c32 l=1}(cid:1) : pl \u2265 0, (cid:80)k\n\nl=1, {\u03c3\u03b8,l}k l=1 pl exp(\u2212 (cid:107)\u03bcl\u2212\u03b8i(cid:107)2\n\n\u03b8,l}k for \u0393 = l=1 pl = 1, \u03c3\u03b8,l \u2265 0, \u03bcl \u2208 Rd(cid:9), then R({\u03b8i}) = )/((2\u03c0\u03c3\u03b8,l)d/2)(cid:1). Here, the client models \u03b81, . . . , \u03b8m are Id) with prob. pl, for l = 1, . . . , k. For k = 1,\n\n(cid:8)(cid:0){pl}k \u2212 (cid:80)m drawn i.i.d. from P(\u0393), where \u03b8i \u223c N (\u03bcl, \u03c32 R({\u03b8i}) = md . Here, unknown \u03bc can be connected to the global model and \u03b8i\u2019s as local models, and the alternating iterative optimization optimizes over both. This justifies the use of (cid:96)2 regularizer in earlier personalized learning works (Dinh et al., 2020; Ozkara et al., 2021; Hanzely & Richt \u0301arik, 2020; Hanzely et al., 2020; Li et al., 2021). 2. When P(\u0393) \u2261 Laplace(\u03bc, b), for \u0393 = {\u03bc, b > 0}, then R({\u03b8i}) = m log(2b) + (cid:80)m 3. When p\u03b8i(Yij|Xij) is according to N (\u03b8i, \u03c32\n\nx), then fi(\u03b8i) is the quadratic loss as in linear regression. When p\u03b8i (Yij|Xij) = \u03c3((cid:104)\u03b8i, Xij(cid:105))Yij (1 \u2212 \u03c3((cid:104)\u03b8i, Xij(cid:105)))(1\u2212Yij ), where \u03c3(z) = 1/1+e\u2212z for any z \u2208 R, then fi(\u03b8i) is the cross-entropy (or logistic) loss as in logistic regression.\n\n\u03b8 ) + (cid:80)m\n\n(cid:107)\u03bc\u2212\u03b8i(cid:107)2 2\u03c32 \u03b8\n\n(cid:107)\u03b8i\u2212\u03bc(cid:107)1 b\n\ni=1\n\ni=1\n\n\u03b8,l\n\n.\n\n2\n\n3.1 AD AMI X: ADAPTIVE PERSONALIZATION WITH GAUSSIAN MIXTURE PRIOR\n\nNow we write the full objective function for the Gaussian mixture prior model for a generic local loss function fi(\u03b8i) at client i (the case of linear/logistic regression with (single) Gaussian prior and solving using alternating gradient descent is discussed in Appendices E, F.): m\n(cid:88)\n\nm (cid:88)\n\nk (cid:88)\n\n(cid:17)\n\n(cid:16)\n\nF gm\n\ni\n\n(\u03b8i) :=\n\nfi(\u03b8i) \u2212 log(\n\npl exp(\u2212\n\n)/((2\u03c0\u03c3\u03b8,l)d/2))\n\narg min {\u03b8i},{\u03bcl},{pl},{\u03c3\u03b8,l}\n\n(cid:107)\u03bcl \u2212 \u03b8i(cid:107)2 2\u03c32\n\n2\n\n\u03b8,l\n\ni=1\n\ni=1\n\nl=1\n\n(10) A common example of fi(\u03b8i) is a generic neural network loss function with multi-class softmax output layer and cross entropy loss, i.e., fi(\u03b8i) := (cid:80)n j=1 \u2212 log(p\u03b8i(Yij|Xij)), where p\u03b8i(Yij|Xij) = \u03c3((cid:104)\u03b8i, Xij(cid:105))Yij (1 \u2212 \u03c3((cid:104)\u03b8i, Xij(cid:105)))(1\u2212Yij ), where \u03c3(z) = 1/1+e\u2212z for any z \u2208 R. To solve (10), we can either use an alternating gradient descent approach, or we can use a clustering based approach where the server runs a (soft) clustering algorithm on received personalized models. We adopt the second approach here (described in Algorithm 1) as it provides an interesting point of view and can be combined with DP clustering algorithms. Here clients receive the global parameters from the server and do a local iteration on the personalized model (multiple local iterations can be introduced as in FedAvg (McMahan et al., 2017)), later the clients send the personalized models. Receiving the personalized models, server initiates GMM algorithm that outputs global parameters 6\n\n6A discrete mixture model can be proposed as a special case of GM with 0 variance. With this we can recover\n\na similar algorithm as in Marfoq et al. (2021). Further details are presented in Appendix G.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n3.2 AD APED: ADAPTIVE PERSONALIZATION VIA DISTILLATION\n\nIt has been empirically observed that the knowledge distillation (KD) regularizer (between local and global models) results in better performance than the (cid:96)2 regularizer (Ozkara et al., 2021). In fact, using our framework, we can define, for the first time, a certain prior distribution that gives the KD regularizer (see Appendix H). We use the following loss function at the i-th client:\n\nfi(\u03b8i) +\n\n1 2\n\nlog(2\u03c8) +\n\nf KD i\n\n(\u03b8i, \u03bc) 2\u03c8\n\n,\n\n(11)\n\nwhere \u03bc denotes the global model, \u03b8i denotes the personalized model at client i, and \u03c8 can be viewed as controlling heterogeneity. The goal for each client is to minimize its local loss function, so individual components cannot be too large. For the second term, this implies that \u03c8 cannot be unbounded. For the third term, if f KD (\u03b8i, \u03bc) is large, then \u03c8 will also increase i\n(implying that the local parameters are too deviated from the global parameter), hence, it is better to emphasize local training loss to make the first term small. If f KD (\u03b8i, \u03bc) is small, then i\n\u03c8 will also decrease (implying that the local parameters are close to the global parameter), so it is better to collaborate and learn better personalized models. Such adaptive weighting quantifies the uncertainty in population distribution during training, balances the learning accordingly, and improves the empirical performance over nonadaptive methods, e.g., (Ozkara et al., 2021).\n\nAlgorithm 1 Personalized Learning with Gaussian Mixture Prior (AdaMix) Input: Number of iterations T , local datasets (Xi, Yi) for i \u2208 [m], learning rate \u03b7. 1 , . . . , \u03b8(0)\n\nand\n\nm\n\n\u03b8(0) k , \u03c3(0)\n\n1 , . . . , \u03bc(0)\n\n\u03b8,1, . . . , \u03c3(0) \u03b8,k.\n\n1: Initialize P(0), \u03bc(0)\n\n2: for t = 1 to T do 3: On Clients: 4: 5:\n\nfor i = 1 to m: do\n\nReceive P(t\u22121), \u03bc(t\u22121) \u03c3(t\u22121) , . . . , \u03c3(t\u22121) Update the personalized parameters:\n\n, . . . , \u03bc(t\u22121) from the server\n\n\u03b8,k\n\n\u03b8,1\n\nk\n\n1\n\n, and\n\ni \u2190 \u03b8(t\u22121) \u03b8(t)\n\ni\n\n\u2212 \u03b7\u2207\u03b8(t\u22121)\n\ni\n\nF gm\n\ni\n\n(\u03b8(t\u22121)\n\ni\n\n)\n\nSend \u03b8(t)\n\ni\n\nto the server\n\nend for At the Server: Receive \u03b8(t) 1 , . . . , \u03b8(t) Update the global parameters:\n\nm from the clients\n\nP(t), \u03bc(t)\n\n1 , . . . , \u03bc(t)\n\n\u03b8,1, . . . , \u03c3(t)\n\n\u03b8,k\n\nk , \u03c3(t) \u2190 GMM(cid:0)\u03b8(t)\n\n1 , . . . , \u03b8(t)\n\nm , k(cid:1)\n\nBroadcast P(t), {\u03bc(t) clients\n\ni }k\n\ni=1, {\u03c3(t)\n\n\u03b8,i}k\n\ni=1 to all\n\n6:\n\n7: 8: 9: 10: 11:\n\n12:\n\n13: end for Output: Personalized models \u03b8T\n\n1 , . . . , \u03b8T m.\n\nTo optimize (11) we propose an alternating minimization approach, which we call AdaPeD; see Algorithm 2. Besides the personalized model \u03b8t i, each client i keeps local copies of the global model \u03bct i , and at synchronization times, server aggregates them to obtain global versions of these \u03bct, \u03c8t. In this way, the local training of \u03b8t i. In the end, clients have learned their personalized models {\u03b8T\n\ni also incorporates knowledge from other clients\u2019 data through \u03bct i=1.\n\ni and of the dissimilarity term \u03c8t\n\ni }m\n\n3.3 DP-AD APED: DIFFERENTIALLY PRIVATE ADAPTIVE PERSONALIZATION VIA DISTILL.\n\nNote that client i communicates \u03bct the gradients ht i, kt order to obtain DP-AdaPeD, we replace lines 13 and 15, respectively, by the update rules:\n\ni (which are updated by accessing the dataset for computing i . In\n\ni , client i adds appropriate noise to hk\n\ni ) to the server. So, to privatize \u03bct\n\ni, \u03c8t\n\ni, \u03c8t\n\ni , kt\n\n\u03bct+1\n\ni = \u03bct\n\ni \u2212 \u03b72\n\n(cid:16)\n\ni\n\nht max{(cid:107)ht i(cid:107)/C1, 1} Id) and \u03bd2 \u223c N (0, \u03c32\n\n+ \u03bd1\n\n(cid:17)\n\nand \u03c8t+1\n\ni = \u03c8t\n\ni \u2212 \u03b73\n\n(cid:16)\n\nkt i\nmax{|kt i |/C2, 1}\n\n(cid:17)\n\n,\n\n+ \u03bd2\n\nwhere \u03bd1 \u223c N (0, \u03c32 q1 privacy level and C1, C2, which are some predefined constants.\n\nq2\n\n), for some \u03c3q1, \u03c3q2 > 0 that depend on the desired\n\nThe theorem below (proved in Appendix I) states the R \u0301enyi Differential Privacy (RDP) guarantees. Theorem 5. After T iterations, DP-AdaPeD satisfies (\u03b1, (cid:15)(\u03b1))-RDP for \u03b1 > 1, where (cid:15)(\u03b1) = (cid:0) K m denotes the sampling ratio of clients at each global iteration. m\n\n(cid:16) C2 1\nK\u03c32\n\n, where K\n\n+ C2\n\n2 K\u03c32\n\n\u03c4 \u03b1\n\n6 T\n\n(cid:1)2\n\n(cid:17)\n\nq1\n\nq2\n\nWe bound the RDP, as it gives better privacy composition than using the strong composition (Mironov et al., 2019). We can also convert our results to user-level ((cid:15), \u03b4)-DP by using the standard conversion from RDP to ((cid:15), \u03b4)-DP (Canonne et al., 2020). See Appendix A for background on privacy.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Private Estimation (Sec. 2.1)\n\n(b) Private Learning (Sec. 3.3)\n\nFigure 1: In Fig. 1a, we plot MSE vs. (cid:15)0 for personalized estimation. In Fig. 1b, we plot Test Accuracy vs. (cid:15) on FEMNIST with client sampling 0.33, for DP-AdaPeD with unsampled client iterations. Since local training is private, both plots remain constant against (cid:15).\n\n4 EXPERIMENTS\n\nPersonalized Estimation. We run one experiment for Bernoulli setting with real political data and the other for Gaussian setting with synthetic data. The latter one is differentially private.\n\n\u2022 Political tendencies on county level. One natural application of Bernoulli setting is modeling bipartisan elections (Tian et al., 2017). We did a case study by using US presidential elections on county level between 2000-2020, with m = 3112 counties in our dataset. For each county the goal is to determine the political tendency parameter pi. Given 6 election data we did 6-fold cross validation, with 5 elections for training and 1 election for test data. Local estimator takes an average of 5 training samples and personalized estimator is the posterior mean. To simulate a Bernoulli setting we set the data equal to 1 if Republican party won the election and 0 otherwise. We observe the personalized estimator provides MSE (averaged over 6 runs) gain of 10.7 \u00b1 1.9% against local estimator.\n\nAlgorithm 2 Adaptive Personalization via Distillation (AdaPeD) Parameters: local variances {\u03c80 models {\u03b80 {\u03bc0 gap \u03c4\n\ni }, personalized i }, local copies of the global model i }, learning rates \u03b71, \u03b72, \u03b73, synchronization\n\nif \u03c4 divides t then On Server do: Choose a subset Kt \u2286 [n] of K clients Broadcast \u03bct and \u03c8t On Clients i \u2208 Kt (in parallel) do: Receive \u03bct, \u03c8t; set \u03bct\n\n1: for t = 0 to T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: 9: On Clients i \u2208 Kt (in parallel) do: i (\u03b8t f KD 2\u03c8t i\n\ni = \u03bct, \u03c8t\n\nCompute gt\n\ni := \u2207\u03b8t\n\ni = \u03c8t\n\nfi(\u03b8t\n\nend if\n\ni) +\n\n\u2207\u03b8t\n\ni\n\ni\n\n10:\n\ni,\u03bct i)\n\ni)/2\u03c8t\n\ni\n\ni\n\ni\n\ni\n\n,\u03bct\n\n11: 12:\n\n13: 14:\n\ni \u2212 \u03b71gt i (\u03b8t+1 f KD i \u2212 \u03b72ht \u2212 f KD i \u2212 \u03b73kt\n\ni = \u03b8t i := \u2207\u03bct i = \u03bct i := 1 i = \u03c8t\n\nUpdate: \u03b8t+1 Compute ht Update: \u03bct+1 Compute kt Update: \u03c8t+1 if \u03c4 divides t + 1 then Clients send \u03bct Server receives {\u03bct Server computes \u03bct+1 = 1 and \u03c8t+1 = 1\n\n\u2022 DP personalized estimation. To measure the performance tradeoff of the DP mechanism described in Section 2.1, we create a synthetic experiment for Gaussian setting. We let m = 10000, n = 15 and \u03c3\u03b8 = 0.1, \u03c3x = 0.5, and create a dataset at each client as described in Gaussian setting. Applying the DP mechanism we obtain the following result in Figure 1a. Here, as expected, when privacy is low ((cid:15)0 is high) the private personalized estimator recovers the regular personalized estimator. For higher privacy the private estimator\u2019s performance starts to become worse than the non-private estimator.\n\nend if 20: 21: end for Output: Personalized models (\u03b8T\n\n15: 16: 17: 18: 19:\n\ni i (\u03b8t+1\n\ni and \u03c8t\n\ni\u2208Kt \u03c8t\n\ni )m\n\n2\u03c8t i\n\n(cid:80)\n\ni=1\n\nK\n\nK\n\ni\n\ni\n\ni\n\ni\n\ni to Server i}i\u2208Kt and {\u03c8t (cid:80)\n\n,\u03bct+1\n\n)/2(\u03c8t\n\ni )2\n\ni }i\u2208Kt i\u2208Kt \u03bct\n\ni\n\nPersonalized Learning. First we describe the experiment setting and then the results.\n\n\u2022 Experiment setting. We consider image classification on MNIST, FEMNIST (Caldas et al., 2018), CIFAR-10, CIFAR-100 (experimental details for CIFAR-100 is given in Appendix K); and train a CNN, similar to the one considered in (McMahan et al., 2017), that has 2 convolutional and 3 fully connected layers. We set m = 66 for FEMNIST and m = 50 for MNIST, CIFAR-10, CIFAR-100. For FEMNIST, we use a subset of 198 writers so that each client has access to data from 3 authors, which results in a natural type of data heterogeneity due to writing styles of authors. On MNIST, CIFAR-10 we introduce pathological heterogeneity by letting each client sample data from 3 and 4 randomly selected classes only, respectively. We set \u03c4 = 10 and vary the batch size so that each epoch consists of 60 iterations. On MNIST we train for 50 epochs, on CIFAR-10 for 250 epochs, on FEMNIST for 40 and 80 epochs, for 0.33 and 0.15 client sampling ratio, respectively. We discuss further details in Appendix K.\n\n8\n\n0.10.20.30.40.50.60.70.8\u03b500.60.81.01.21.41.6MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized Estimator456789\u03b593.093.594.094.595.095.596.096.597.0Test Accuracy (in %)Local trainingDP-AdaPeDPublished as a conference paper at ICLR 2023\n\nTable 1: Test accuracy (in %) for CNN model. The CIFAR-10, MNIST, and FEMNIST columns have client sampling ratios K\n\nn of 0.2, 0.1, and 0.15, respectively.\n\nMethod\n\nCIFAR-10\n\nCIFAR-100\n\nFEMNIST\n\nFedAvg FedAvg+fine tuning (Jiang et al., 2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\n60.86 \u00b1 0.94 63.12 \u00b1 0.31 72.49 \u00b1 0.42 69.53 \u00b1 0.16 59.95 \u00b1 0.79 71.61 \u00b1 0.70 71.09 \u00b1 0.67\n\n30.48 \u00b1 0.33 39.98 \u00b1 0.26 53.11 \u00b1 0.34 43.65 \u00b1 0.18 34.78 \u00b1 0.41 51.94 \u00b1 0.21 50.42 \u00b1 0.26\n\n92.18 \u00b1 0.13 94.12 \u00b1 0.26 96.55 \u00b1 0.32 94.95 \u00b1 0.55 93.51 \u00b1 0.31 95.99 \u00b1 0.08 95.12 \u00b1 0.18\n\nMethod\n\n(cid:15) = 3.35\n\n(cid:15) = 13.16\n\n(cid:15) = 27.30\n\nDP-FedAvg DP-AdaPeD (Ours)\n\n11.73 \u00b1 0.85 93.32 \u00b1 1.18\n\n29.91 \u00b1 1.28 98.51 \u00b1 0.90\n\n55.79 \u00b1 0.29 99.01 \u00b1 0.65\n\nTable 2: (DP-AdaPeD) Test Accuracy (in %) vs. (cid:15) on MNIST without client sampling.\n\nMethod\n\nn = 10\n\nn = 20\n\nn = 30\n\nLocal Training AdaMix\n\n39.93 \u00b1 0.13 10.42 \u00b1 0.15\n\n30.02 \u00b1 0.08 3.12 \u00b1 0.04\n\n19.97 \u00b1 0.07 2.55 \u00b1 0.04\n\nTable 3: Mean squared error of our AdaMix algorithm and the local training for linear regression.\n\n\u2022 Results. In Table 1 we compare AdaPeD against FedAvg (McMahan et al., 2017), FedAvg+ (Jiang et al., 2019) and various personalized FL algorithms: pFedMe (Dinh et al., 2020), Per-FedAvg (Fallah et al., 2020), QuPeD (Ozkara et al., 2021) without model compression, and Federated ML (Shen et al., 2020). We report further results in Appendix K. We observe AdaPeD consistently outperforms other methods. It can be seen that methods that use knowledge distillation perform better; on top of this, AdaPeD enables us adjust the dependence on collaboration according to the compatibility of global and local decisions/scores. For instance, we set \u03c32 \u03b8 to a certain value initially, and observe it progressively decrease, which implies clients start to rely on the collaboration more and more. Interestingly, this is not always the case: for DP-AdaPeD, we first observe a decrease in \u03c32 \u03b8 and later it increases. This suggests: while there is not much accumulated noise, clients prefer to collaborate, and as the noise accumulation on the global model increases due to DP noise, clients prefer not to collaborate. This is exactly the type of autonomous behavior we aimed with adaptive regularization.\n\n\u2022 DP-AdaPeD. In Figure 1b and Table 2, we observe performance of DP-AdaPeD under different (cid:15) values. DP-AdaPeD outperforms DP-FedAvg because personalized models do not need to be privatized by DP mechanism, whereas the global model needs to be. Our experiments provide user-level privacy (more stringent, but appropriate in FL), as opposed to the item-level privacy.\n\n\u2022 DP-AdaPeD with unsampled client iterations. When we let unsampled clients to do local iterations (free in terms of privacy cost and a realistic scenario in cross-silo settings) described in Appendix H, we can increase DP-AdaPeD\u2019s performance under more aggressive privacy constants (cid:15). For instance, for FEMNIST with 1/3 client sampling we obtain the result reported in Figure 1b.\n\n\u2022 AdaMix. We consider linear regression on synthetic data, with m = 1000 clients and each client has n \u2208 {10, 20, 30} local samples. Each local model \u03b8i \u2208 Rd is drawn from a mixture of two Gaussian distributions N (\u03bc, \u03a3) and N (\u2212\u03bc, \u03a3), where \u03a3 = 0.001 \u00d7 Id and d = 50. Each client sample (Xij, Yij) is distributed as Xij \u223c N (0, Id) and Yij = (cid:104)Xij, \u03b8i(cid:105) + wij, where wij \u223c N (0, 0.1). Table 3 demonstrates the superior performance of AdaMix against the local estimator.\n\n5 CONCLUSION\n\nWe proposed a statistical framework leading to new personalized federated estimation and learning algorithms (e.g., AdaMix, AdaPeD); we also incorporated privacy (and communication) constraints into our algorithms and analyzed them. Open questions include information theoretic lower bounds and its comparison to proposed methods; examination of how far the proposed alternating minimization methods (such as in AdaMix, AdaPeD) are from global optima.\n\nThe work in this paper was partially supported by NSF grants 2139304, 2007714 and gift funding by Meta\n\nand Amazon.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nDurmus Alp Emre Acar, Yue Zhao, Ruizhao Zhu, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Debiasing model updates for improving personalized federated training. In International Conference on Machine Learning, pp. 21\u201331. PMLR, 2021.\n\nSara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for kmeans and euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, 49(4): FOCS17\u201397, 2019.\n\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communicationefficient sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30, 2017.\n\nBorja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing interpretations and renyi differential privacy. In Silvia Chiappa and Roberto Calandra (eds.), International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of Machine Learning Research, pp. 2496\u20132506. PMLR, 2020.\n\nLeighton Pate Barnes, Yanjun Han, and Ayfer Ozgur. Lower bounds for learning distributions under communication constraints via fisher information. Journal of Machine Learning Research, 21 (236):1\u201330, 2020.\n\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pp. 560\u2013569. PMLR, 2018.\n\nDavid M. Blei, Michael I. Jordan, and A. Ng. Hierarchical bayesian models for applications in\n\ninformation retrieval. 2003.\n\nMark Bun, Cynthia Dwork, Guy N. Rothblum, and Thomas Steinke. Composable and versatile privacy via truncated CDP. In ACM SIGACT Symposium on Theory of Computing (STOC), pp. 74\u201386, 2018.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Kone\u02c7cn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nCl \u0301ement L. Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential\n\nprivacy. In Neural Information Processing Systems (NeurIPS), 2020.\n\nHuili Chen, Jie Ding, Eric Tramel, Shuang Wu, Anit Kumar Sahu, Salman Avestimehr, and Tao Zhang. Self-aware personalized federated learning. CoRR, abs/2204.08069, 2022. doi: 10.48550/ arXiv.2204.08069. URL https://doi.org/10.48550/arXiv.2204.08069.\n\nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In Marina Meila and Tong Zhang (eds.), International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 2089\u20132099. PMLR, 2021.\n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\n\nlearning. arXiv preprint arXiv:2003.13461, 2020.\n\nCanh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau\n\nenvelopes. In Advances in Neural Information Processing Systems, 2020.\n\nSimon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=pW2Q2xLwIMD.\n\nCynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations\n\nand Trends in Theoretical Computer Science, 9(3-4):211\u2013407, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity\n\nin private data analysis. In Theory of Cryptography Conference (TCC), pp. 265\u2013284, 2006.\n\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-\n\nlearning approach. In Advances in Neural Information Processing Systems, 2020.\n\nAndrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin.\n\nBayesian Data Analysis. Chapman and Hall/CRC, 2013.\n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client\n\nlevel perspective. arXiv preprint arXiv:1712.07557, 2017.\n\nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. Differentially private clustering: Tight approxima-\n\ntion ratios. Advances in Neural Information Processing Systems, 33:4040\u20134054, 2020.\n\nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private learning via correlated sampling. In Neural Information Processing Systems (NeurIPS), pp. 20172\u201320184, 2021.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\n\nclustered federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nRichard Gill and Boris Levit. Applications of the van trees inequality: A bayesian cram \u0301er-rao bound.\n\nBernoulli, 1:59\u201379, 03 1995. doi: 10.2307/3318681.\n\nAntonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of federated learning: Privacy, accuracy and communication trade-offs. IEEE Journal on Selected Areas in Information Theory, 2(1):464\u2013478, 2021a.\n\nAntonious M. Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and Ananda Theertha Suresh. In International Conference on Shuffled model of differential privacy in federated learning. Artificial Intelligence and Statistics (AISTATS), volume 130 of Proceedings of Machine Learning Research, pp. 2521\u20132529. PMLR, 2021b.\n\nAntonious M. Girgis, Deepesh Data, and Suhas Diggavi. Distributed user-level private mean estimation. In 2022 IEEE International Symposium on Information Theory (ISIT), pp. 2196\u20132201, 2022. doi: 10.1109/ISIT50566.2022.9834713.\n\nFilip Hanzely and Peter Richt \u0301arik. Federated learning of a mixture of global and local models. arXiv\n\npreprint arXiv:2002.05516, 2020.\n\nFilip Hanzely, Slavom \u0301\u0131r Hanzely, Samuel Horv \u0301ath, and Peter Richt \u0301arik. Lower bounds and optimal algorithms for personalized federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nRui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated\n\nlearning with differential privacy. IEEE Internet of Things Journal, 7(10):9530\u20139539, 2020.\n\nPrateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Differentially private model personalization. Advances in Neural Information Processing Systems, 34, 2021a.\n\nPrateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Differentially private model personalization. In Advances in Neural Information Processing Systems, volume 34, 2021b.\n\nWilliam James and Charles Stein. Estimation with quadratic loss. In Proceedings Berkeley Symposium\n\non Mathematics and Statistics, Vol 1, pp. 361\u2013379. University of California Press, 1961.\n\nYihan Jiang, Jakub Kone\u02c7cn`y, Keith Rush, and Sreeram Kannan.\n\nImproving federated learning\n\npersonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur \u0301elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nShiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.\n\nWhat can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.\n\nMikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-\n\nlearning methods. In Advances in Neural Information Processing Systems, 2019.\n\nNikita Kotelevskii, Maxime Vono, Eric Moulines, and Alain Durmus. Fedpop: A bayesian approach\n\nfor personalised federated learning. CoRR, abs/2206.03611, 2022, June.\n\nDaniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, and Ananda Theertha Suresh. Learning with user-level privacy. In Neural Information Processing Systems (NeurIPS), pp. 12466\u201312479, 2021.\n\nDaliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv\n\npreprint arXiv:1910.03581, 2019.\n\nJeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private metalearning. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=rJgqMRVYvr.\n\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pp. 6357\u20136368. PMLR, 2021.\n\nTao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi. Ensemble distillation for robust model\n\nfusion in federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nYuhan Liu, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Michael Riley. Learning discrete distributions: user vs item-level privacy. In Neural Information Processing Systems (NeurIPS), 2020.\n\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\n\n129\u2013137, 1982.\n\nFrederic M. Lord. Estimating true-score distributions in psychological testing (an empirical bayes estimation problem)*. ETS Research Bulletin Series, 1967(2):i\u201351, 1967. doi: https://doi.org/10. 1002/j.2333-8504.1967.tb00535.x. URL https://onlinelibrary.wiley.com/doi/ abs/10.1002/j.2333-8504.1967.tb00535.x.\n\nYishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.\n\nOthmane Marfoq, Giovanni Neglia, Aur \u0301elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp. 1273\u20131282. PMLR, 2017.\n\nIlya Mironov. R \u0301enyi differential privacy. In IEEE Computer Security Foundations Symposium (CSF),\n\npp. 263\u2013275, 2017.\n\nIlya Mironov, Kunal Talwar, and Li Zhang. R \u0301enyi differential privacy of the sampled gaussian mechanism. CoRR, abs/1908.10530, 2019. URL http://arxiv.org/abs/1908.10530.\n\nKaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. Quped: Quantized personalization via distillation with applications to federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nKaan Ozkara, Antonious M Girgis, Deepesh Data, and Suhas Diggavi. A generative framework for personalized learning and estimation: Theory, algorithms, and privacy. arXiv preprint arXiv:2207.01771, 2022, July.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgMkCEtPB.\n\nHerbert Robbins. An empirical bayes approach to statistics. In The Proceedings of Third Berkeley\n\nSymposium on Mathematical Statistics and Probability, pp. 157\u2013163, 1956.\n\nTao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Kun Kuang, Fei Wu, and\n\nChao Wu. Federated mutual learning. arXiv preprint arXiv:2006.16765, 2020.\n\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar. Federated multi-task\n\nlearning. In Advances in Neural Information Processing Systems, pp. 4424\u20134434, 2017.\n\nCharles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1954\u2013 1955, vol. I, pp. 197\u2013206. University of California Press, Berkeley-Los Angeles, Calif., 1956.\n\nUri Stemmer. Locally private k-means clustering. In SODA, pp. 548\u2013559, 2020.\n\nKevin Tian, Weihao Kong, and Gregory Valiant. Learning populations of parameters. Advances in\n\nneural information processing systems, 30, 2017.\n\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: a good embedding is all you need? In European Conference on Computer Vision, pp. 266\u2013282. Springer, 2020.\n\nPaul Vanhaesebrouck, Aur \u0301elien Bellet, and Marc Tommasi. Decentralized collaborative learning of personalized models over networks. In Artificial Intelligence and Statistics, pp. 509\u2013517. PMLR, 2017.\n\nRamya Korlakai Vinayak, Weihao Kong, Gregory Valiant, and Sham Kakade. Maximum likelihood In International Conference on Machine\n\nestimation for learning populations of parameters. Learning, pp. 6448\u20136457. PMLR, 2019.\n\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-\n\nbridge University Press, 2019.\n\nValentina Zantedeschi, Aur \u0301elien Bellet, and Marc Tommasi. Fully decentralized joint learning of personalized models and collaboration graphs. In International Conference on Artificial Intelligence and Statistics, pp. 864\u2013874. PMLR, 2020.\n\nMichael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated learning with first order model optimization. In International Conference on Learning Representations, 2021.\n\nYuchen Zhang. Distributed machine learning with communication constraints. PhD thesis, EECS Department, University of California, Berkeley, May 2016. URL http://www2.eecs. berkeley.edu/Pubs/TechRpts/2016/EECS-2016-47.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nWe give standard privacy definitions that we use in Section A.1, some existing results on RDP to DP conversion and RDP composition in Section A.2, and user-level differential privacy in Section A.3.\n\nA.1 PRIVACY DEFINITIONS\n\nIn this subsection, we define different privacy notions that we will use in this paper: local differential privacy (LDP), central different privacy (DP), and Renyi differential privacy (RDP), and their userlevel counterparts. Definition 1 (Local Differential Privacy - LDP (Kasiviswanathan et al., 2011)). For (cid:15)0 \u2265 0, a randomized mechanism R : X \u2192 Y is said to be (cid:15)0-local differentially private (in short, (cid:15)0-LDP), if for every pair of inputs d, d(cid:48) \u2208 X , we have\n\nPr[R(d) \u2208 S] \u2264 e(cid:15)0 Pr[R(d(cid:48)) \u2208 S],\n\n\u2200S \u2282 Y.\n\n(12)\n\n1, . . . , x(cid:48)\n\nLet D = {x1, . . . , xn} denote a dataset comprising n points from X . We say that two datasets n} are neighboring (and denoted by D \u223c D(cid:48)) if they differ D = {x1, . . . , xn} and D(cid:48) = {x(cid:48) in one data point, i.e., there exists an i \u2208 [n] such that xi (cid:54)= x(cid:48) i and for every j \u2208 [n], j (cid:54)= i, we have xj = x(cid:48) j. Definition 2 (Central Differential Privacy - DP (Dwork et al., 2006; Dwork & Roth, 2014)). For (cid:15), \u03b4 \u2265 0, a randomized mechanism M : X n \u2192 Y is said to be ((cid:15), \u03b4)-differentially private (in short, ((cid:15), \u03b4)-DP), if for all neighboring datasets D \u223c D(cid:48) \u2208 X n and every subset S \u2286 Y, we have\n\nPr [M(D) \u2208 S] \u2264 e(cid:15)0 Pr [M(D(cid:48)) \u2208 S] + \u03b4.\n\n(13)\n\nIf \u03b4 = 0, then the privacy is referred to as pure DP. Definition 3 ((\u03bb, (cid:15)(\u03bb))-RDP (Renyi Differential Privacy) (Mironov, 2017)). A randomized mechanism M : X n \u2192 Y is said to have (cid:15)(\u03bb)-Renyi differential privacy of order \u03bb \u2208 (1, \u221e) (in short, (\u03bb, (cid:15)(\u03bb))-RDP), if for any neighboring datasets D \u223c D(cid:48) \u2208 X n, the Renyi divergence between M(D) and M(D(cid:48)) is upper-bounded by (cid:15)(\u03bb), i.e.,\n\nD\u03bb(M(D)||M(D(cid:48))) =\n\n1 \u03bb \u2212 1\n\n(cid:32)\n\nlog\n\nE\u03b8\u223cM(D(cid:48))\n\n(cid:34)(cid:18) M(D)(\u03b8) M(D(cid:48))(\u03b8)\n\n(cid:19)\u03bb(cid:35)(cid:33)\n\nwhere M(D)(\u03b8) denotes the probability that M on input D generates the output \u03b8. For convenience, instead of (cid:15)(\u03bb) being an upper bound, we define it as (cid:15)(\u03bb) = supD\u223cD(cid:48) D\u03bb(M(D)||M(D(cid:48))).\n\n\u2264 (cid:15)(\u03bb),\n\nA.2 RDP TO DP CONVERSION AND RDP COMPOSITION\n\nAs mentioned after Theorem 5, we can convert the RDP guarantees of DP-AdaPeD to its DP guarantees using existing conversion results from literature. To the best of our knowledge, the following gives the best conversion. Lemma 1 (From RDP to DP (Canonne et al., 2020; Balle et al., 2020)). Suppose for any \u03bb > 1, a mechanism M is (\u03bb, (cid:15) (\u03bb))-RDP. Then, the mechanism M is ((cid:15), \u03b4)-DP, where (cid:15), \u03b4 are define below:\n\nFor a given \u03b4 \u2208 (0, 1) :\n\n(cid:15) = min\n\n\u03bb\n\n(cid:15) (\u03bb) +\n\nlog (1/\u03b4) + (\u03bb \u2212 1) log (1 \u2212 1/\u03bb) \u2212 log (\u03bb) \u03bb \u2212 1\n\nFor a given (cid:15) > 0 :\n\n\u03b4 = min\n\n\u03bb\n\nexp ((\u03bb \u2212 1) ((cid:15) (\u03bb) \u2212 (cid:15))) \u03bb \u2212 1\n\n(cid:18)\n\n1 \u2212\n\n(cid:19)\u03bb\n\n.\n\n1 \u03bb\n\nThe main strength of RDP in comparison to other privacy notions comes from composition. The following result states that if we adaptively compose two RDP mechanisms with the same order, their privacy parameters add up in the resulting mechanism. Lemma 2 (Adaptive composition of RDP (Mironov, 2017, Proposition 1)). For any \u03bb > 1, let M1 : X \u2192 Y1 be a (\u03bb, (cid:15)1(\u03bb))-RDP mechanism and M2 : Y1 \u00d7 X \u2192 Y be a (\u03bb, (cid:15)2(\u03bb))-RDP mechanism. Then, the mechanism defined by (M1, M2) satisfies (\u03bb, (cid:15)1(\u03bb) + (cid:15)2(\u03bb))-RDP.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.3 USER-LEVEL DIFFERENTIAL PRIVACY LEVY ET AL. (2021)\n\nConsider a set of m users, each having a local dataset of n samples. Let Di = {xi1, . . . , xin} denote the local dataset at the i-th user for i \u2208 [m], where xij \u2208 X and X \u2282 Rd. We define D = (D1, . . . , Dm) \u2208 (X n)m as the entire dataset.\n\nWe have already defined DP, LDP, and RDP in Section A.1 w.r.t. the item-level privacy. Here, we extend those definition w.r.t. the user-level privacy. In order to do that, we need a generic neighborhood relation between datasets: We say that two datasets D, D(cid:48) are neighboring with respect to distance metric dis if we have dis(D, D(cid:48)) \u2264 1. Item-level DP/RDP vs. User-level DP/RDP. By choosing dis(D, D(cid:48)) = (cid:80)m ij}, we recover the standard definition of the DP/RDP from Definitions 2, 3, which we call item-level DP/RDP. In the item-level DP/RDP, two datasets D, D(cid:48) are neighboring if they differ in a single item. On the other hand, by choosing dis(D, D(cid:48)) = (cid:80)m i}, we call it user-level DP/RDP, where two datasets D, D(cid:48) \u2208 (X n)m are neighboring when they differ in a local dataset of any single user. Observe that when each user has a single item (n = 1), then both item-level and user-level privacy are equivalent.\n\n1{Di (cid:54)= D(cid:48)\n\n1{xij (cid:54)= x(cid:48)\n\n(cid:80)n\n\nj=1\n\ni=1\n\ni=1\n\nUser-level Local Differential Privacy (LDP). When we have a single user (i.e., m = 1 and D = X n), by choosing dis (D, D(cid:48)) = 1{D (cid:54)= D(cid:48)} for D, D(cid:48) \u2208 X n, we call it user-level LDP. In this case each user privatize her own local dataset using a private mechanism.\n\nWe can define user-level LDP/DP/RDP analogously to their item-level counterparts using the neighborhood relation dis defined above.\n\nB PERSONALIZED ESTIMATION \u2013 GAUSSIAN MODEL\n\nB.1 PROOF OF THEOREM 1\n\nWe will derive the optimal estimator and prove the MSE for one dimensional case, i.e., for d = 1; the final result can be obtained by applying these to each of the d coordinates separately.\n\nThe posterior estimators of the local means \u03b81, . . . , \u03b8m and the global mean \u03bc are obtained by solving the following optimization problem:\n\n\u02c6\u03b81, . . . , \u02c6\u03b8m, \u02c6\u03bc = arg max\n\n\u03b81,...,\u03b8m,\u03bc\n\n= arg min \u03b81,...,\u03b8m,\u03bc\n\npX|\u03b8 (X1, . . . , Xm|\u03b81, . . . , \u03b8m) p\u03b8|\u03bc(\u03b81, . . . , \u03b8m|\u03bc)\n\n\u2212 log (cid:0)pX|\u03b8 (X1, . . . , Xm|\u03b81, . . . , \u03b8m)(cid:1) \u2212 log (cid:0)p\u03b8|\u03bc(\u03b81, . . . , \u03b8m|\u03bc)(cid:1)\n\n= arg min \u03b81,...,\u03b8m,\u03bc\n\nC +\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n(cid:17)2\n\n(cid:16)\n\nX j\n\ni \u2212 \u03b8i \u03c32 x\n\n+\n\nm (cid:88)\n\ni=1\n\n(\u03b8i \u2212 \u03bc)2 \u03c32 \u03b8\n\n,\n\nwhere the second equality is obtained from the fact that the log function is a monotonic function, and C is a constant independent of the variables \u03b8 = (\u03b81, . . . , \u03b8m). Observe that the objective function F (\u03b8, \u03bc) = (cid:80)m obtained by setting the derivative to zero as it is an unbounded optimization problem.\n\nis jointly convex in (\u03b8, \u03bc). Thus, the optimal is\n\n(\u03b8i\u2212\u03bc)2 \u03c32 \u03b8\n\ni \u2212\u03b8i)2 \u03c32 x\n\n+ (cid:80)m\n\n(cid:80)n\n\n(X j\n\nj=1\n\ni=1\n\ni=1\n\n\u2202F \u2202\u03b8i\n\n\u2202F \u2202\u03bc\n\n(cid:12) (cid:12) (cid:12) (cid:12)\u03bc=\u02c6\u03bc,\u03b8i=\u02c6\u03b8i (cid:12) (cid:12) (cid:12) (cid:12)\u03bc=\u02c6\u03bc,\u03b8i=\u02c6\u03b8i\n\n=\n\n=\n\n(cid:80)n\n\n(cid:80)m\n\nj=1 2(\u02c6\u03b8i \u2212 X j i ) \u03c32 x\ni=1 2(\u02c6\u03bc \u2212 \u02c6\u03b8i) \u03c32 \u03b8\n\n= 0.\n\n+\n\n2(\u02c6\u03b8i \u2212 \u02c6\u03bc) \u03c32 \u03b8\n\n= 0,\n\n\u2200i \u2208 [m]\n\nBy solving these m + 1 equations in m + 1 unknowns, we get: \uf8eb\n\n\uf8eb\n\n\uf8f6\n\n\u02c6\u03b8i = \u03b1\n\n\uf8ed\n\nX j\n\ni\n\n\uf8f8 + (1 \u2212 \u03b1)\n\n\uf8ed\n\nn (cid:88)\n\n1 n\n\nm (cid:88)\n\nn (cid:88)\n\n\uf8f6\n\nX j\n\ni\n\n\uf8f8 ,\n\n(14)\n\n1 mn\n\ni=1\n\nj=1\n\nj=1\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n\u03b8\n\nwhere \u03b1 = \u03c32 \u03c32 \u03b8 + (cid:80)m i=1 X i.\n\n\u03b1) 1 m\n\n\u03c32 x\nn\n\n. By letting X i = 1\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni for all i \u2208 [m], we can write \u02c6\u03b8i = \u03b1X i + (1 \u2212\n\nObserve that E an unbiased estimate of {\u03b8i}. Substituting the \u02c6\u03b8i in the MSE, we get that\n\n= \u03b1\u03b8i + 1\u2212\u03b1\n\nm\n\nl=1 \u03b8l, where \u03b8 = (\u03b81, . . . , \u03b8m). Thus, the estimator (14) is\n\n(cid:80)m\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\nEX1,...,Xm\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i\n\n(cid:17)2(cid:21)\n\n= E\u03b8\n\n(cid:20) EX1,...,Xm\n\n= E\u03b8\n\n(cid:20) EX1,...,Xm\n\n= E\u03b8\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i (cid:20)(cid:16)\u02c6\u03b8i \u2212 E (cid:20)(cid:16)\u02c6\u03b8i \u2212 E\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|\u03b8\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n+ E\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:17)2\n\n|\u03b8\n\n\u2212 \u03b8i\n\n(cid:21)(cid:21)\n\n(cid:105)(cid:17)2\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n|\u03b8\n\n(cid:21)(cid:21)\n\n+ E\u03b8\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:17)2\n\n|\u03b8\n\n\u2212 \u03b8i\n\n(cid:21)(cid:21)\n\n(15)\n\nClaim 1. (cid:20) EX1,...,Xm\n\nE\u03b8\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 E\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:105)(cid:17)2\n\n|\u03b8\n\n(cid:21)(cid:21)\n\n= \u03b12 \u03c32\n\nx n\n\n+ (1 \u2212 \u03b1)2 \u03c32\n\nx mn\n\n+ 2\u03b1(1 \u2212 \u03b1)\n\n\u03c32 x\nmn\n\n(cid:20) EX1,...,Xm\n\nE\u03b8\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:17)2\n\n|\u03b8\n\n\u2212 \u03b8i\n\n(cid:21)(cid:21)\n\n\uf8ee\n\n(cid:32)\n\n= (1 \u2212 \u03b1)2E\u03b8\n\n\uf8f0\n\n1 m\n\nm (cid:88)\n\nk=1\n\n\u03b8k \u2212 \u03b8i\n\n(cid:33)2\uf8f9\n\n\uf8fb \u2264 (1 \u2212 \u03b1)2 \u03c32\n\n\u03b8 (m \u2212 1) m\n\nProof. For the first equation:\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 E\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\nE\u03b8\n\n(cid:105)(cid:17)2\n\n|\u03b8\n\n(cid:21)(cid:21)\n\n= E\u03b8\n\n\uf8ee \uf8f0EX1,...,Xm\n\n\uf8ee\n\n(cid:32)\n\n\uf8f0\n\n\u03b1(X i \u2212 \u03b8i) + (1 \u2212 \u03b1)\n\n(cid:33)2\n\n\uf8f9\n\n\uf8f9\n\n(X k \u2212 \u03b8k)\n\n| \u03b8\n\n\uf8fb\n\n\uf8fb\n\n1 m\n\nm (cid:88)\n\nk=1\n\n= \u03b12E (cid:2)E (cid:2)(X i \u2212 \u03b8i)2 | \u03b8(cid:3)(cid:3) + (1 \u2212 \u03b1)2E\n\n\uf8ee \uf8f0E\n\n\uf8ee\n\n(cid:32)\n\n\uf8f0\n\n1 m\n\nm (cid:88)\n\n(cid:33)2\n\n\uf8f9\n\n\uf8f9\n\n(X k \u2212 \u03b8k)\n\n| \u03b8\n\n\uf8fb\n\n\uf8fb\n\nk=1\n\n(cid:34)\n\n(cid:34)\n\n+ 2\u03b1(1 \u2212 \u03b1)E\n\nE\n\n1 m\n\n= \u03b12 \u03c32\n\nx n\n\n+ (1 \u2212 \u03b1)2 \u03c32\n\nx mn\n\n+ 2\u03b1(1 \u2212 \u03b1)\n\nFor the second equation, first note that E (cid:1): \u03b1) (cid:0) 1\n\nk=1 \u03b8k \u2212 \u03b8i\n\n(cid:80)m\n\nm\n\nm (cid:88)\n\n(X i \u2212 \u03b8i)(X k \u2212 \u03b8k) | \u03b8\n\n(cid:35)(cid:35)\n\nk=1 \u03c32 x\nmn\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:105)\n\n\u2212 \u03b8i = \u03b1\u03b8i + 1\u2212\u03b1\n\nm\n\n(cid:80)m\n\nk=1 \u03b8k \u2212 \u03b8i = (1 \u2212\n\n(cid:20) EX1,...,Xm\n\nE\u03b8\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:17)2\n\n|\u03b8\n\n\u2212 \u03b8i\n\n(cid:21)(cid:21)\n\n= (1 \u2212 \u03b1)2E\n\n=\n\n(1 \u2212 \u03b1)2 m2\n\nE\n\n\uf8ee\n\n\uf8eb\n\n\uf8ef \uf8f0\n\n\uf8ed\n\n2\uf8f9\n\n\uf8f6\n\n\uf8f8\n\n\uf8fa \uf8fb\n\n(cid:88)\n\n(\u03b8k \u2212 \u03b8i)\n\nk(cid:54)=i\n\n\uf8ee\n\n(cid:32)\n\n\uf8f0\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2\uf8f9 \uf8fb\n\n\u03b8k \u2212 \u03b8i\n\n\uf8ee\n\n(cid:88)\n\n\uf8f0\n\nk(cid:54)=i\n\n\uf8ee\n\n(cid:88)\n\n\uf8f0\n\n=\n\n(1 \u2212 \u03b1)2 m2\n\n\u2264\n\n(1 \u2212 \u03b1)2 m2\n\n=\n\n(1 \u2212 \u03b1)2 m2\n\nE(\u03b8k \u2212 \u03b8i)2 +\n\n(cid:88)\n\nE(\u03b8k \u2212 \u03b8i)(\u03b8l \u2212 \u03b8i)\n\n\uf8fb\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\uf8f9\n\n[E(\u03b8k \u2212 \u03bc)2 + E(\u03b8i \u2212 \u03bc)2] +\n\nk(cid:54)=i\n\n\uf8ee \uf8f02(m \u2212 1)\u03c32\n\n\u03b8 +\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\uf8f9\n\n(cid:88)\n\nE(\u03b8k \u2212 \u03b8i)(\u03b8l \u2212 \u03b8i)\n\n\uf8fb\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n16\n\n(cid:88)\n\nE(\u03b8k \u2212 \u03b8i)(\u03b8l \u2212 \u03b8i)\n\n\uf8fb\n\n\uf8f9\n\nPublished as a conference paper at ICLR 2023\n\n=\n\n=\n\n(1 \u2212 \u03b1)2 m2\n\n(1 \u2212 \u03b1)2 m2\n\n\uf8ee \uf8f02(m \u2212 1)\u03c32\n\n\u03b8 +\n\n(cid:88)\n\nE(\u03bc \u2212 \u03b8i)2\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\uf8f9 \uf8fb (Since E[\u03b8k] = \u03bc for all k \u2208 [m])\n\n(cid:2)2(m \u2212 1)\u03c32\n\n\u03b8 + (m \u2212 1)(m \u2212 2)\u03c32\n\n\u03b8\n\n(cid:3)\n\n= (1 \u2212 \u03b1)2 \u03c32\n\n\u03b8 (m \u2212 1) m\n\nThis concludes the proof of Claim 1.\n\nSubstituting the result of Claim 1 into (15), we get (cid:17)2(cid:21)\n\n\u2264 \u03b12 \u03c32\n\n+ (1 \u2212 \u03b1)2 \u03c32\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i\n\nEX1,...,Xm\n\nx n\n(cid:18)\n\n+ 2\u03b1(1 \u2212 \u03b1)\n\n\u03c32 x\nmn\n\n+ (1 \u2212 \u03b1)2 \u03c32\n\n\u03b8 (m \u2212 1) m\n\n(16)\n\nx mn\n\n(a)=\n\n\u03c32 x\nn \u03c32 x\nn where in (a) we used \u03b1 = \u03c32 \u03b8 +\n\n=\n\n\u03c32 x\nn\n\n\u03c32\n\n\u03b8\n\n\u03b12 +\n\n(1 \u2212 \u03b1)2 + 2\u03b1(1 \u2212 \u03b1) m\n\n+ \u03b1(1 \u2212 \u03b1)\n\n(cid:19)\n\nm \u2212 1 m\n\n(cid:18)\n\n\u03b1 +\n\n(cid:19)\n\n,\n\n1 \u2212 \u03b1 m\n\nfor the last term to write (1 \u2212 \u03b1)2 \u03c32\n\n\u03b8 (m\u22121) m\n\n= \u03c32\n\nx\n\nn \u03b1(1 \u2212 \u03b1) m\u22121 m .\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni , and the global estimator \u02c6\u03bc = 1\n\nObserve that the estimator in (14) is a weighted summation between two estimators: the local estimator X i = 1 i=1 X i. Thus, the MSE in (a) consists of four terms: 1) The variance of the local estimator ( \u03c32 n ). 2) The variance of the global estimator ( \u03c32 nm ). 4) The (cid:20) EX1,...,Xm\n\nnm ). 3) The correlation between the local estimator and the global estimator ( \u03c32\n\n. This completes the proof of Theorem 1.\n\nbias term E\u03b8\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n\u2212 \u03b8i\n\n(cid:80)m\n\n(cid:20)(cid:16)\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|\u03b8\n\nE\n\nm\n\n(cid:105)\n\nx\n\nx\n\nx\n\nB.2 PROOF OF THEOREM 2, EQUATION (5)\n\nSimilar to the proof of Theorem 1, here also we will derive the optimal estimator and prove the MSE for the one dimensional case, and the final result can be obtained by applying these to each of the d coordinates separately.\n\nLet \u03b8 = (\u03b81, . . . , \u03b8m) denote the personalized models vector. For given a constraint function q, we set the personalized model as follows: \uf8f6\n\n\uf8eb\n\n\u02c6\u03b8i = \u03b1\n\n\uf8ed\n\nX j\n\ni\n\n\uf8f8 + (1 \u2212 \u03b1)\n\nq(X i)\n\n\u2200i \u2208 [m],\n\n(17)\n\n(cid:33)\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n1 n\n\nn (cid:88)\n\nj=1\n\nwhere X i = 1\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni . From the second condition on the function q, we get that\n\nE\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n= \u03b1\u03b8i +\n\n1 \u2212 \u03b1 m\n\nm (cid:88)\n\nl=1\n\n\u03b8l,\n\n(18)\n\nThus, by following similar steps as the proof of Theorem 1, we get that:\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i\n\nE\n\n(cid:17)2(cid:21)\n\n= E\n\n= E\n\n= E\n\n(cid:20)\n\nE\n\n(cid:20)\n\nE\n\n(cid:20)\n\nE\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i (cid:20)(cid:16)\u02c6\u03b8i \u2212 E (cid:20)(cid:16)\u02c6\u03b8i \u2212 E\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|\u03b8\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n+ E\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:17)2\n\n|\u03b8\n\n\u2212 \u03b8i\n\n(cid:21)(cid:21)\n\n(cid:105)(cid:17)2\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n|\u03b8\n\n(cid:21)(cid:21)\n\n(cid:20)(cid:16)\n\n(cid:20)\n\nE\n\n+ E\n\nE\n\n(cid:105)\n\n(cid:104)\u02c6\u03b8i|\u03b8\n\n(cid:17)2\n\n|\u03b8\n\n\u2212 \u03b8i\n\n(cid:21)(cid:21)\n\n(a)\n\n= \u03b12 \u03c32\n\nx n\n\n+ (1 \u2212 \u03b1)2E\n\n\uf8ee\n\n(cid:32)\n\n\uf8f0\n\n1 m\n\nm (cid:88)\n\nl=1\n\n(cid:33)2\n\n\uf8f9\n\n|\u03b8\n\n\uf8fb\n\nq(X l) \u2212 \u03b8l\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n+ 2\u03b1(1 \u2212 \u03b1)E\n\n(cid:34) (cid:0)X i \u2212 \u03b8i\n\n(cid:1)\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\nl=1\n\n(cid:33)\n\n(cid:35)\n\nq(X l) \u2212 \u03b8l\n\n|\u03b8\n\n+ (1 \u2212 \u03b1)2E\n\n\uf8ee\n\n(cid:32)\n\n\uf8f0\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2\uf8f9 \uf8fb\n\n\u03b8k \u2212 \u03b8i\n\n(1 \u2212 \u03b1)2 (cid:16) \u03c32\n\nx\n\nn + \u03c32\n\nq\n\nm\n\n(cid:17)\n\n+\n\n2\u03b1(1 \u2212 \u03b1)\u03c32 x\nmn\n\n+ (1 \u2212 \u03b1)2E\n\n\uf8ee\n\n(cid:32)\n\n\uf8f0\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2\uf8f9 \uf8fb\n\n\u03b8k \u2212 \u03b8i\n\n(1 \u2212 \u03b1)2 (cid:16) \u03c32\n\nx\n\nn + \u03c32\n\nq\n\nm\n\n(cid:17)\n\n+ 2\u03b1(1 \u2212 \u03b1)\n\n\u03c32 x\nmn\n\n+ (1 \u2212 \u03b1)2 \u03c32\n\n\u03b8 (m \u2212 1) m\n\n\u03b12 +\n\n(1 \u2212 \u03b1)2 + 2\u03b1(1 \u2212 \u03b1) m\n\n+ \u03b1(1 \u2212 \u03b1)\n\nm \u2212 1 m\n\n(cid:19)\n\n(cid:18)\n\n\u03b1 +\n\n(cid:19)\n\n,\n\n1 \u2212 \u03b1 m\n\n(19)\n\n+\n\n+\n\n(b)\n\n= \u03b12 \u03c32\n\nx n\n\n\u2264 \u03b12 \u03c32\n\nx n\n(cid:18)\n\n(c) =\n\n=\n\n\u03c32 x\nn \u03c32 x\nn\n\nwhere step (a) follows by substituting the expectation of the personalized model from (18). Step (b) follows from the first and third conditions of the function q. Step (c) follows by choosing\n\n\u03b1 =\n\n\u03c32\n\n\u03b8 +\n\n\u03c32 q\nm\u22121\n\n\u03c32\n\n\u03b8 +\n\n\u03c32 q\nm\u22121 +\n\n\u03c32 x\nn\n\n. This derives the result stated in (5) in Theorem 2.\n\nB.2.1 PROOF OF THEOREM 2, PART 1\n\nThe proof consists of two steps. First, we use the concentration property of the Gaussian distribution to show that the local sample means {X i} are bounded within a small range with high probability. Second, we apply an unbiased stochastic quantizer on the projected sample mean.\n\ni , . . . , X n\n\nThe local samples X 1 variance \u03c32\n\nx, and hence, we have that X i \u223c N (\u03b8i, \u03c32\n\ni are drawn i.i.d. from a Gaussian distribution with mean \u03b8i and n ). Thus, from the concentration property \u2212 nc2 of the Gaussian distribution, we get that Pr[|X i \u2212 \u03b8i| > c1] \u2264 exp for all i \u2208 [m]. Similarly, the models \u03b81, . . . , \u03b8m are drawn i.i.d. from a Gaussian distribution with mean \u03bc \u2208 [\u2212r, r] and variance \u03c32 for all i \u2208 [m]. Let E = (cid:8)X i \u2208 [\u2212a, a] : \u2200i \u2208 [m](cid:9), where a = r + c1 + c2. Thus, from the union bound, we get that\n\n\u03b8 , hence,, we get Pr[|\u03b8i \u2212 \u03bc| > c2] \u2264 exp\n\n\u2212 c2\n\n1 \u03c32 x\n\n2 \u03c32 \u03b8\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nx\n\nPr[E] > 1 \u2212 m(e that a = r + \u03c3x\u221a\n\nn\n\n\u2212\n\nnc2 1\n\u03c32\n\nx + e\n\n\u2212\n\nc2 2\n\u03c32 \u03b8 ). By setting c1 =\n\n(cid:113) \u03c32\n\n(cid:112)log(m2n) + \u03c3\u03b8\n\n(cid:112)log(m2n), and Pr[E] = 1 \u2212 2\n\nx\n\nn log(m2n) and c2 = (cid:112)\u03c32 mn .\n\n\u03b8 log(m2n), we get\n\nLet qk : [\u2212a, a] \u2192 Yk be a quantization function with k-bits, where Yk is a discrete set of cardinality |Yk| = 2k. For given x \u2208 [\u2212a, a], the output of the function qk is given by:\n\nqk(x) =\n\n2a 2k \u2212 1\n\n((cid:98) \u0303x(cid:99) + Bern ( \u0303x \u2212 (cid:98) \u0303x(cid:99))) \u2212 a,\n\n(20)\n\nwhere Bern(p) is a Bernoulli random variable with bias p, and \u0303x = 2k\u22121 2a (x + a) \u2208 [0, 2k \u2212 1]. Observe that the output of the function qk requires only k-bits for transmission. Furthermore, the function qk satisfies the following conditions:\n\nE [qk(x)] = x,\n\n\u03c32 qk\n\n= E (cid:2)(qk(x) \u2212 x)2(cid:3) \u2264\n\na2 (2k \u2212 1)2 .\n\n(21)\n\n(22)\n\n(cid:3) and sends Let each client applies the function qk on the projected local mean \u0303Xi = Proj[\u2212a,a] the output to the server for all i \u2208 [m]. Conditioned on the event E, i.e., X i \u2208 [\u2212a, a] \u2200i \u2208 [m], and using (19), we get that\n\n(cid:2)X i\n\nM SE = E\u03b8,X\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i\n\n(cid:17)2(cid:21)\n\n\u2264\n\n\u03c32 x\nn\n\n(cid:18) 1 \u2212 \u03b1 m\n\n(cid:19)\n\n+ \u03b1\n\n,\n\n(23)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nwhere \u03b1 =\n\n\u03c32\n\n\u03b8 +\n\na2 (2k \u22121)2(m\u22121)\n\n\u03c32\n\n\u03b8 +\n\na2 (2k \u22121)2(m\u22121)\n\n+\n\n\u03c32 x\nn\n\nand a = r + \u03c3x\u221a\n\nn\n\n(cid:112)log(m2n) + \u03c3\u03b8\n\n(cid:112)log(m2n). Note that the event\n\nE happens with probability at least 1 \u2212 2\n\nmn .\n\nB.2.2 PROOF OF THEOREM 2, PART 2\n\nWe define the (random) mechanism qp : [\u2212a, a] \u2192 R that takes an input x \u2208 [\u2212a, a] and generates a user-level ((cid:15)0, \u03b4)-LDP output y \u2208 R, where y = qp(x) is given by:\n\nqp(x) = x + \u03bd,\n\n(24)\n\nwhere \u03bd \u223c N (0, \u03c32 , we get that the output of the (cid:15)0 function qp(x) is ((cid:15)0, \u03b4)-LDP from Dwork & Roth (2014). Furthermore, the function qp satisfies the following conditions:\n\n) is a Gaussian noise. By setting \u03c32 (cid:15)0\n\n(cid:15)2 0\n\n= 8a2 log(2/\u03b4)\n\nE [qp(x)] = x,\n\n\u03c32 qp\n\n= E (cid:2)(qp(x) \u2212 x)2(cid:3) \u2264\n\n8a2 log(2/\u03b4) (cid:15)2 0\n\n.\n\n(25)\n\n(26)\n\nSimilar to the proof of Theorem 2, Part 1, let each client applies the function qp on the projected local (cid:3) and sends the output to the server for all i \u2208 [m]. Conditioned on the mean \u0303Xi = Proj[\u2212a,a] event E, i.e., X i \u2208 [\u2212a, a] \u2200i \u2208 [m], and using (19), we get that\n\n(cid:2)X i\n\nMSE = E\u03b8,X\n\n(cid:20)(cid:16)\u02c6\u03b8i \u2212 \u03b8i\n\n(cid:17)2(cid:21)\n\n\u2264\n\n\u03c32 x\nn\n\n(cid:18) 1 \u2212 \u03b1 m\n\n(cid:19)\n\n+ \u03b1\n\n,\n\n(27)\n\n\u03b8 + 8a2 log(2/\u03b4) \u03c32\n\n(cid:15)2\n\nwhere \u03b1 =\n\n0(m\u22121) \u03b8 + 8a2 log(2/\u03b4) \u03c32 0(m\u22121) happens with probability at least 1 \u2212 2\n\n\u03c32 x\nn\n\n(cid:15)2\n\n+\n\nand a = r + \u03c3x\u221a\n\nn\n\nmn .\n\n(cid:112)log(m2n) + \u03c3\u03b8\n\n(cid:112)log(m2n). Note that the event E\n\nRemark 4 (Privacy with communication efficiency). Note that our private estimation algorithm for the Gaussian case adds Gaussian noise (which is a real number) but that can also be made communication-efficient by alternatively adding a discrete Gaussian noise (Canonne et al., 2020).\n\nB.3 LOWER BOUND\n\nHere we discuss the lower bound using Fisher information technique similar to Barnes et al. (2020). In particular we use a Bayesian version of Cramer-Rao lower bound and van Trees inequality Gill & Levit (1995). Let us denote f (X|\u03b8) as the data generating conditional density function and \u03c0(\u03b8) as the prior distribution that generates \u03b8. Let us denote E\u03b8 as the expectation with respect to the randomness of \u03b8 and E as the expectation with respect to randomness of X and \u03b8. First we define two types of Fisher information:\n\nIX (\u03b8) = E\u03b8\u2207\u03b8 log(f (X|\u03b8))\u2207\u03b8 log(f (X|\u03b8))T\n\nI(\u03c0) = E\u2207\u03b8 log(\u03c0(\u03b8))\u2207\u03b8 log(\u03c0(\u03b8))T\n\nnamely Fisher information of estimating \u03b8 from samples X and Fisher information of prior \u03c0. Here the logarithm is elementwise. For van Trees inequality we need the following regularity conditions:\n\n\u2022 f (X|\u00b7) and \u03c0(\u00b7) are absolutely continuous and \u03c0(\u00b7) vanishes at the end points of \u0398. \u2022 E\u03b8\u2207\u03b8 log(f (X|\u03b8)) = 0 \u2022 We also assume both density functions are continuously differentiable.\n\nThese assumptions are satisfied for the Gaussian setting for any finite mean \u03bc, they are satisfied for Bernoulli setting as long as parameters \u03b1 and \u03b2 are larger than 1. Assuming local samples X are generated i.i.d with f (x|\u03b8), the van Trees inequality for one dimension is as follows:\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nE((cid:98)\u03b8(X) \u2212 \u03b8)2 \u2265\n\n1 nEIx(\u03b8) + I(\u03c0)\n\nwhere IX (\u03b8) = E\u03b8 log(f (X|\u03b8))(cid:48)2 and I(\u03c0) = E log(\u03c0(\u03b8))(cid:48)2. Assuming \u03b8 \u2208 Rd and each dimension is independent from each other, by Gill & Levit (1995) we have:\n\nE(cid:107)(cid:98)\u03b8(X) \u2212 \u03b8(cid:107)2 \u2265\n\nd2 nETr(Ix(\u03b8)) + Tr(I(\u03c0))\n\n(28)\n\nNote, the lower bound on the average risk directly translates as a lower bound on sup\u03b8\u2208\u0398 \u03b8(cid:107)2. Before our proof we have a useful fact:\n\nEX (cid:107)(cid:98)\u03b8(X)\u2212\n\nFact 1. Given some random variable X \u223c N (Y, \u03c32 \u03c32\n\ny).\n\ny) where Y \u223c N (Z, \u03c32\n\nz ) we have X \u223c N (z, \u03c32\n\nz +\n\nProof. We will give the proof in one dimension, however, it can easily be extended to multidimensional case where each dimension is independent. For all t \u2208 R we have,\n\nEX [exp(itX)] = EY EX [exp(itX)|Y ] = EY [exp(itY \u2212\n\nxt2 \u03c32 2\n\n)]\n\n= exp(\u2212\n\n= exp(\u2212\n\n\u03c32 xt2 2\nxt2 \u03c32 2\n\n)EY [exp(itY )]\n\n) exp(itz \u2212\n\n= exp(itz \u2212\n\ny)t2\n\n(\u03c32\n\nx + \u03c32 2\n\n\u03c32\n\nyt2 2\n\n)\n\n)\n\nwhere the last line is the characteristic function of a Gaussian with mean z and variance \u03c32\n\nx + \u03c32 y.\n\nGaussian case with perfect knowledge of prior. In this setting we know that \u03b8i \u223c N (\u03bc1, \u03c32 hence, I(\u03c0) = 1 Id. Then, \u03c32 \u03b8\n\nId, similarly IX (\u03b8) = 1\n\n\u03c32 x\n\n\u03b8 Id),\n\nE(cid:107) (cid:98)\u03b8i(X) \u2212 \u03b8i(cid:107)2 \u2265\n\nsup \u03b8i\n\nd2\n\nnE d \u03c32 x\n\n+ d \u03c32 \u03b8\n\n=\n\nd\u03c32\n\nx\n\n\u03b8 \u03c32 \u03b8 + \u03c32\n\nx\n\nn\u03c32\n\n(29)\n\nmn\n\n(cid:80)m,n\n\ni,j X j\n\nGaussian case with estimated population mean. In this setting instead of a true prior we have a prior whose mean is the average of all data spread across clients, i.e., we assume \u03b8i \u223c N ((cid:98)\u03bc, \u03c32 \u03b8 Id) where (cid:98)\u03bc = 1 i |\u03b8j \u223c N (\u03b8j, \u03c32 \u03b8 Id). While the true prior is parameterized with mean \u03bc, \u03b8i in this form is not parameterized by \u03bc but by (cid:98)\u03bc which itself has randomness due X j i . However, using Fact 1 \u03b8 + \u03c32 twice we can write \u03b8i \u223c N (\u03bc, (\u03c32 mn )Id). Then using the van Trees inequality similar to the lower bound in perfect case we can obtain:\n\ni . We additionally know that there is a Markov relation such that X j\n\nxId) and \u03b8j \u223c N (\u03bc, \u03c32\n\nm + \u03c32\n\nx\n\n\u03b8\n\n\u03c32 \u03b8 \u03c32 n\u03c32\n\nx mn\n\nx + \u03c34 \u03b8 + \u03c32\n\nx\n\nEX (cid:107)(cid:98)\u03b8i(X) \u2212 \u03b8i(cid:107)2 \u2265 d\n\nsup \u03b8i\u2208\u0398\n\n20\n\n(30)\n\nPublished as a conference paper at ICLR 2023\n\nC PERSONALIZED ESTIMATION \u2013 BERNOULLI MODEL\n\nC.1 WHEN \u03b1, \u03b2 ARE KNOWN\n\nAnalogous to the Gaussian case, we can show that if \u03b1, \u03b2 are known, then the posterior mean estimator has a closed form expression: (cid:98)pi = aX i + (1 \u2212 a) \u03b1 \u03b1+\u03b2 (where a = n/\u03b1+\u03b2+n) and achieves the MSE: Epi\u223c\u03c0E \u03b1+\u03b2+n . We show this below.\n\n(cid:98)pi,X1,...,Xm ((cid:98)pi \u2212 pi)2 \u2264\n\n\u03b1\u03b2 n(\u03b1+\u03b2)(\u03b1+\u03b2+1)\n\nn\n\nFor a client i, let \u03c0(pi) be distributed as Beta(\u03b1, \u03b2). In this setting, we model that each client generates local samples according to Bern(pi). Consequently, each client has a Binomial distribution regarding the sum of local data samples. Estimating Bernoulli parameter pi is related to Binomial distribution Bin(n, pi) (the sum of data samples) Zi since it is the sufficient statistic of Bernoulli distribution. The distribution for Binomial variable Zi given pi is P (Zi = zi|pi) = (cid:0) n i (1 \u2212 pi)n\u2212zi. It is a known fact that for any prior, the Bayesian MSE risk minimizer is the posterior mean E [pi|Zi = zi].\n\n(cid:1)pzi\n\nzi\n\nWhen pi \u223c Beta(\u03b1, \u03b2), we have posterior\n\nf (pi|Zi = zi) =\n\n=\n\n=\n\n\u03c0(pi)\n\nP (zi|pi) P (zi) (cid:1)pzi\n\n(cid:0) n zi\n\ni (1 \u2212 pi)n\u2212zi\n\nP (zi)\n\np\u03b1\u22121\n\ni\n\n(1 \u2212 pi)\u03b2\u22121 B(\u03b1, \u03b2)\n\n(cid:1)\n\n(cid:0) n zi P (zi)\n\nB(\u03b1 + zi, \u03b2 + n \u2212 zi) B(\u03b1, \u03b2)\n\np\u03b1+zi\u22121\n\ni\n\n(1 \u2212 pi)\u03b2+n\u2212zi\u22121\n\nB(\u03b1 + zi, \u03b2 + n \u2212 zi)\n\n,\n\nwhere B(\u03b1, \u03b2) = \u0393(\u03b1)\u0393(\u03b2)\n\n\u0393(\u03b1+\u03b2) , and\n\n(cid:90)\n\nP (zi) =\n\nP (zi|pi)\u03c0(pi)dpi\n\n=\n\n=\n\np\u03b1\u22121\n\n(cid:19)\n\npzi i (1 \u2212 pi)n\u2212zi\n\n(cid:90) (cid:18)n zi (cid:19) B(zi + \u03b1, n \u2212 zi + \u03b2) B(\u03b1, \u03b2)\n\n(cid:18)n zi\n\ni\n\n(1 \u2212 pi)\u03b2\u22121 B(\u03b1, \u03b2) (cid:90) p\u03b1+zi\u22121\n\ni\n\ndpi\n\n(1 \u2212 pi)\u03b2+n\u2212zi\u22121\n\n(cid:124)\n\nB(\u03b1 + zi, \u03b2 + n \u2212 zi) (cid:123)(cid:122) integral of a Beta distribution\n\ndpi\n\n(cid:125)\n\n=\n\n(cid:18)n zi\n\n(cid:19) B(zi + \u03b1, n \u2212 zi + \u03b2) B(\u03b1, \u03b2)\n\nThus, we get that the posterior distribution f (pi|Zi = zi) = p\u03b1+zi\u22121 distribution Beta(zi + \u03b1, n \u2212 zi + \u03b2). As a result, the posterior mean is given by:\n\ni\n\nB(\u03b1+zi,\u03b2+n\u2212zi)\n\n(1\u2212pi)\u03b2+n\u2212zi\u22121\n\n(cid:98)pi =\n\n\u03b1 + Zi \u03b1 + \u03b2 + n \u03b1+\u03b2+n . Observe that Epi\u223cBeta(\u03b1,\u03b2)[pi] = \u03b1\n\n= a\n\n(cid:18) Zi n\n\n(cid:19)\n\n+ (1 \u2212 a)\n\n(cid:18) \u03b1\n\n\u03b1 + \u03b2\n\n(cid:19)\n\n,\n\nwhere a = n tion between the local estimator zi We have Rpi ((cid:98)pi) = E\u03c0E((cid:98)pi \u2212 pi)2. The MSE of the posterior mean is given by:\n\nn and the global estimator \u03bc = \u03b1\n\n\u03b1+\u03b2 .\n\n\u03b1+\u03b2 , i.e., the estimator is a weighted summa-\n\nis a beta\n\n(31)\n\nMSE = E[(\u02c6pi \u2212 pi)2] (cid:20)(cid:16)\n\n= E\n\n(cid:17)\n\n+ (1 \u2212 a)(\u03bc \u2212 pi)\n\n(cid:17)2(cid:21)\n\n\u2212 pi\n\n= a2E\n\n(cid:17)2(cid:21)\n\n\u2212 pi\n\n+ (1 \u2212 a)2E\n\n(cid:104)\n\n(\u03bc \u2212 pi)2(cid:105)\n\na\n\n(cid:16) zi n\n(cid:20)(cid:16) zi n\n\n= a2Epi\u223c\u03c0(pi)\n\n(cid:21)\n\n(cid:20) pi(1 \u2212 pi) n\n\n+ (1 \u2212 a)2\n\n\u03b1\u03b2 (\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n= a2\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1) (cid:18)\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1)\n\n=\n\n\u03b1\u03b2 (\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1) (cid:19)\n\n+ (1 \u2212 a)2\n\nn \u03b1 + \u03b2 + n\n\n.\n\nThe last equality is obtained by setting a = n\n\n\u03b1+\u03b2+n .\n\nRemark 5. Note that X i := Zi n is the estimator based only on the local data and \u03b1/(\u03b1+\u03b2) is the true global mean, and (cid:98)pi = aX i + (1 \u2212 a) \u03b1 \u03b1+\u03b2 , where a = n/\u03b1+\u03b2+n (see (31)) is the estimator based on all the data. Observe that when n \u2192 \u221e, then a \u2192 1, which implies that (cid:98)pi \u2192 X i. Otherwise, when \u03b1 + \u03b2 is large (i.e., the variance of the beta distribution is small), then a \u2192 0, which implies that (cid:98)pi \u2192 \u03b1/(\u03b1+\u03b2). Both these conclusions conform to the conventional wisdom as mentioned in the Gaussian case. It can be shown that the local estimate X i achieves the Bayesian risk of Epi\u223cBeta(\u03b1,\u03b2)EXi[(X i \u2212 pi)2] = Epi\u223cBeta(\u03b1,\u03b2)(pi(1\u2212pi))/n = \u03b1\u03b2/n(\u03b1+\u03b2)(\u03b1+\u03b2+1), which implies that the personalized estimation with perfect prior always outperforms the local estimate with a multiplicative gain a = n/(n+\u03b1+\u03b2) \u2264 1.\n\nC.2 WHEN \u03b1, \u03b2 ARE UNKNOWN: PROOF OF THEOREM 3\n\nThe personalized model of the ith client with unknown parameters \u03b1, \u03b2 is given by:\n\n\u02c6pi = aiX i + (1 \u2212 ai) (\u02c6\u03bci) ,\n\n(32)\n\n, the empirical mean \u02c6\u03bci = 1\n\nm\u22121\n\n+n\n\n(cid:80)\n\nl(cid:54)=i X l, and the empirical variance\n\nl(cid:54)=i(X l \u2212 \u02c6\u03bci)2. From (Tian et al., 2017, Lemma 1), with probability 1 \u2212 1\n\nm2n , we get\n\nn\n\n\u02c6\u03bci(1\u2212 \u02c6\u03bci) \u02c6\u03c32 i\n\nwhere ai =\n\n(cid:80)\n\ni = 1\n\nm\u22122\n\n\u02c6\u03c32 that\n\n|\u03bc \u2212 \u02c6\u03bci| \u2264\n\n|\u03c32 \u2212 \u02c6\u03c32\n\ni | \u2264\n\n(cid:114) 3 log(4m2n) m \u2212 1 (cid:114) 3 log(4m2n) m \u2212 1\n\n,\n\nwhere \u03bc = \u03b1\n\n\u03b1+\u03b2 , \u03c32 =\n\n\u03b1\u03b2\n\n(\u03b1+\u03b2)2(\u03b1+\u03b2+1) are the true mean and variance of the beta distribution,\n\nrespectively. Let c = \u2200i \u2208 [m]} that happens with probability at least 1 \u2212 1\n\nm\u22121\n\n(cid:113) 3 log(4m2n)\n\nmn , we get that:\n\n. Conditioned on the event E = {|\u03bc \u2212 \u02c6\u03bci| \u2264 c, |\u03c32 \u2212 \u02c6\u03c32\n\ni | \u2264 c :\n\n(cid:104)\n\nE\n\n(\u02c6pi \u2212 pi)2 |Z\u2212i\n\n(cid:105)\n\n= a2E\n\n(cid:34)(cid:18) Zi\n\nn\n\n(cid:19)2(cid:35)\n\n\u2212 pi\n\n+ (1 \u2212 a)2E\n\n(cid:104)\n\n(\u02c6\u03bci \u2212 pi)2 |Z\u2212i\n\n(cid:105)\n\n(cid:18)\n\n(cid:18)\n\n(cid:18)\n\n= a2\n\n= a2\n\n\u2264 a2\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1)\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1)\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1)\n\n(cid:19)\n\n(cid:19)\n\n(cid:19)\n\n+ (1 \u2212 a)2 (cid:16)\n\nE\n\n(cid:104)\n\n(\u03bc \u2212 pi)2(cid:105)\n\n+ (\u03bc \u2212 \u02c6\u03bci)2(cid:17)\n\n+ (1 \u2212 a)2\n\n+ (1 \u2212 a)2\n\n(cid:18)\n\n(cid:18)\n\n\u03b1\u03b2 (\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1)\n\n\u03b1\u03b2 (\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1)\n\n(cid:19)\n\n+ (\u03bc \u2212 \u02c6\u03bci)2\n\n(cid:19)\n\n,\n\n+ c2\n\nwhere the expectation is with respect to zi \u223c Binom(pi, n) and pi \u223c Beta(\u03b1, \u03b2) and Z\u2212i = {z1, . . . , zi\u22121, zi+1, . . . , zm} denotes the entire dataset except the ith client data (zi). By taking the expectation with respect to the datasets Z\u2212i, we get that the MSE is bounded by:\n\nMSE \u2264 E (cid:2)a2(cid:3)\n\n(cid:18)\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1)\n\n(cid:19)\n\n+E (cid:2)(1 \u2212 a)2(cid:3)\n\n(cid:18)\n\n\u03b1\u03b2 (\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1)\n\n+\n\n3 log(4m2n) m \u2212 1\n\n(cid:19)\n\n,\n\nwith probability at least 1 \u2212 1\n\nmn . This completes the proof of Theorem 3.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nC.3 WITH PRIVACY CONSTRAINTS: PROOF OF THEOREM 4\n\nFirst, we prove some properties of the private mechanism qp. Observe that for any two inputs x, x(cid:48) \u2208 [0, 1], we have that:\n\nPr[qp(x) = y] Pr[qp(x(cid:48)) = y]\n\n=\n\ne(cid:15)0\n\ne(cid:15)0 +1 \u2212 x e(cid:15)0 \u22121 e(cid:15)0 +1 \u2212 x(cid:48) e(cid:15)0 \u22121\n\ne(cid:15)0 +1\n\ne(cid:15)0 +1\n\ne(cid:15)0\n\n\u2264 e(cid:15)0,\n\n(33)\n\ne(cid:15)0 \u22121 . Similarly, we can prove (33) for the output y = e(cid:15)0\n\nfor y = \u22121 user-level (cid:15)0-LDP. Furthermore, for given x \u2208 [0, 1], we have that\n\ne(cid:15)0 \u22121 . Thus, the mechanism qp is\n\nThus, the output of the mechanism qp is an unbiased estimate of the input x. From the Hoeffding\u2019s inequality for bounded random variables, we get that:\n\nE [qp(x)] = x.\n\n(34)\n\nPr[|\u02c6\u03bc(p)\n\ni \u2212 \u03bc| > t] \u2264 2 exp\n\nPr[|\u02c6\u03c32(p)\n\ni \u2212 \u03c32| > t] \u2264 2 exp\n\n(cid:18) \u22123(e(cid:15)0 \u2212 1)2(m \u2212 1)t2 (e(cid:15)0 + 1)2 (cid:18) \u22123(e(cid:15)0 \u2212 1)2(m \u2212 1)t2 (e(cid:15)0 + 1)2\n\n(cid:19)\n\n(cid:19)\n\n(35)\n\nThus, we have that the event E = {|\u02c6\u03bc(p) probability at least 1 \u2212 1 non-private estimator, we get the fact that the MSE of the private model is bounded by: (cid:19)\n\ni \u2212 \u03c32| \u2264 cp : \u2200i \u2208 [m]} happens with . By following the same steps as the\n\ni \u2212 \u03bc| \u2264 cp, |\u02c6\u03c32(p)\n\n(cid:113) (e(cid:15)0 +1)2 log(4m2n)\n\nmn , where cp =\n\n3(e(cid:15)0 \u22121)2(m\u22121)\n\n(cid:18)\n\nMSE \u2264 E (cid:2)a2(cid:3)\n\n\u03b1\u03b2 n(\u03b1 + \u03b2)(\u03b1 + \u03b2 + 1)\n\n+ E (cid:2)(1 \u2212 a)2(cid:3)\n\n(cid:18)\n\n\u03b1\u03b2 (\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1)\n\n+\n\n(e(cid:15)0 + 1)2 log(4m2n) 3(e(cid:15)0 \u2212 1)2(m \u2212 1)\n\n(cid:19)\n\n, (36)\n\nwhere a(p) =\n\n\u02c6\u03bc\n\n(p) i\n\nn\n\n(1\u2212 \u02c6\u03bc\n\n\u02c6\u03c3\n\n2(p) i\n\n(p) i\n\nand the expectation is with respect\n\nto the clients data\n\n)\n\n+n\n\n{z1, . . . , zi\u22121, zi+1, . . . , zm}and the randomness of the private mechanism qp. This completes the proof of Theorem 4.\n\nRemark 6 (Privacy with communication efficiency). Note that our private estimation algorithm for the Bernoulli case is already communication-efficient as each client sends only one bit to the server.\n\nRemark 7 (Client sampling). For simplicity, in the theoretical analysis in Gaussian and Bernoulli models, we assume that all clients participate in the estimation process. However, a simple modification to our analysis also handles the case where only K out of m clients participate: in all our theorem statements we would have to modify to have K instead m. Note that we do client sampling for our experiments in Table 1.\n\nD PERSONALIZED ESTIMATION \u2013 MIXTURE MODEL\n\nConsider a set of m clients, where the i-th client has a local dataset Xi = (Xi1, . . . , Xin) of n samples for i \u2208 [m], where Xij \u2208 Rd. The local samples Xi of the i-th client are drawn i.i.d. from a Gaussian distribution N (\u03b8i, \u03c32\n\nId) with unknown mean \u03b8i and known variance \u03c32\n\nId.\n\nx\n\nx\n\nIn this section, we assume that the personalized models \u03b81, . . . , \u03b8m are drawn i.i.d. from a discrete distribution P = [p1, . . . , pk] for given k candidates \u03bc1, . . . , \u03bck \u2208 Rd. In other works, Pr[\u03b8i = \u03bcl] = pl for l \u2208 [k] and i \u2208 [m]. The goal of each client is to estimate her personalized model {\u03b8i} that minimizes the mean square error defined as follows:\n\nMSE = E{\u03b8i,Xi}(cid:107)\u03b8i \u2212 \u02c6\u03b8i(cid:107)2, where the expectation is taken with respect to the personalized models \u03b8i and the local samples Id)}. Furthermore, \u02c6\u03b8i denotes the estimate of the personalized model \u03b8i for {Xij \u223c N (\u03b8i, \u03c32 i \u2208 [m].\n\n(37)\n\nx\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFirst, we start with a simple case when the clients have perfect knowledge of the prior distribution, i.e., the i-th client knows the k Gaussian distributions N (cid:0)\u03bc1, \u03c32 (cid:1) and the prior distribution \u03b1 = [\u03b11, . . . , \u03b1k]. This will serve as a stepping stone to handle the more general case when the prior distribution is unknown.\n\n(cid:1) , . . . , N (cid:0)\u03bck, \u03c32\n\n\u03b8\n\n\u03b8\n\nD.1 WHEN THE PRIOR DISTRIBUTION IS KNOWN\n\nIn this case, the i-th client does not need the data of the other clients as she has a perfect knowledge about the prior distribution. Theorem 6. For given a perfect knowledge \u03b1 = [\u03b11, . . . , \u03b1k] and N (cid:0)\u03bc1, \u03c32 the optimal personalized estimator that minimizes the MSE is given by:\n\n(cid:1) , . . . , N (cid:0)\u03bck, \u03c32\n\n\u03b8\n\n\u03b8\n\n(cid:1),\n\n\u02c6\u03b8i =\n\nk (cid:88)\n\nl=1\n\na(i)\n\nl \u03bcl,\n\n(38)\n\n(cid:18)\n\n\u2212\n\npl exp\n\n(cid:80)k\n\ns=1 ps exp\n\n(cid:18)\n\n(cid:80)n\n\n(cid:19)\n\nj=1 (cid:107)Xij \u2212\u03bcl (cid:107)2 2\u03c32 x\n(cid:107)Xij \u2212\u03bcs(cid:107)2 2\u03c32 x\n\nj=1\n\n(cid:80)n\n\n\u2212\n\nwhere \u03b1(i)\n\nl =\n\nfor l \u2208 [k].\n\n(cid:19) denotes the weight associated to the prior model \u03bcl\n\nProof. Let \u03b8i \u223c P, where P = [p1, . . . , pk] and pl = Pr[\u03b8i = \u03bcl] for l \u2208 [k]. The goal is to design an estimator \u02c6\u03b8i that minimizes the MSE given by:\n\nMSE = E\u03b8i\u223cPE{Xij \u223cN (\u03b8i,\u03c32\n\nx)}\n\n(cid:104)\n\n(cid:107)\u02c6\u03b8i \u2212 \u03b8i(cid:107)2(cid:105)\n\n.\n\n(39)\n\nLet Xi = (Xi1, . . . , Xin). By following the standard proof of the minimum MSE, we get that: E\u03b8i\n\n(cid:107)\u02c6\u03b8i \u2212 \u03b8i(cid:107)2(cid:105)\n\n= EXi\n\nE\u03b8i|Xi\n\nEXi\n\n(cid:104)\n\n(cid:105)\n\n(cid:104)\n\n(cid:107)\u02c6\u03b8i \u2212 E[\u03b8i|Xi] + E[\u03b8i|Xi] \u2212 \u03b8i(cid:107)2(cid:12) (cid:2) (cid:107)E[\u03b8i|Xi] \u2212 \u03b8i(cid:107)2(cid:12) (cid:2) (cid:107)E[\u03b8i|Xi] \u2212 \u03b8i(cid:107)2(cid:12)\n\n(cid:3) + EXi (cid:3) ,\n\nE\u03b8i|Xi\n\n(cid:12) Xi\n\n(cid:12) Xi\n\n(cid:12) (cid:12) Xi (cid:104)\n\n= EXi\n\n\u2265 EXi\n\nE\u03b8i|Xi E\u03b8i|Xi\n\n(cid:107)E[\u03b8i|Xi] \u2212 \u02c6\u03b8i(cid:107)2(cid:12)\n\n(cid:105)\n\n(cid:12) (cid:12) Xi\n\n(40) where the last inequality is achieved with equality when \u02c6\u03b8i = E[\u03b8i|Xi]. The distribution on \u03b8i given the local dataset Xi is given by:\n\nPr[\u03b8i = \u03bcl|Xi] =\n\nf (Xi|\u03b8i = \u03bcl) Pr[\u03b8i = \u03bcl] f (Xi)\n\n=\n\n=\n\nf (Xi|\u03b8i = \u03bcl) Pr[\u03b8i = \u03bcl] s=1 f (Xi|\u03b8i = \u03bcs) Pr[\u03b8i = \u03bcs]\n\n(cid:80)k\n\n(cid:18)\n\npl exp\n\n\u2212\n\n(cid:80)n\n\nj=1 (cid:107)Xij \u2212\u03bcl(cid:107)2\n\n2\u03c32 x\n\n(cid:19)\n\n(cid:80)k\n\ns=1 ps exp\n\n(cid:16)\n\n\u2212\n\n(cid:80)n\n\nj=1 (cid:107)Xij \u2212\u03bcs(cid:107)2\n\n2\u03c32 x\n\n(cid:17) = \u03b1(i)\n\nl\n\nAs a result, the optimal estimator is given by:\n\nThis completes the proof of Theorem 6.\n\n\u02c6\u03b8i = E[\u03b8i|Xi] =\n\nk (cid:88)\n\nl=1\n\n\u03b1(i)\n\nl \u03bcl.\n\n(41)\n\n(42)\n\nThe optimal personalized estimation in (38) is a weighted summation over all possible candidates vectors \u03bc1, . . . , \u03bck, where the weight \u03b1(i) increases if the prior pl increases and/or the local samples {Xij} are close to the model \u03bcl for l \u2208 [k]. Observe that the optimal estimator \u02c6\u03b8i in Theorem 6 that . Furthermore, minimizes the MSE is completely different from the local estimator\n\n(cid:80)n\n\n(cid:16) 1\n\n(cid:17)\n\nl\n\nj=1 Xij\n\nn\n\nit is easy to see that the local estimator has the MSE\n\n(cid:17)\n\n(cid:16) d\u03c32 n\n\nx\n\nwhich increases linearly with the data\n\ndimension d. On the other hand, the MSE of the optimal estimator in Theorem 6 is a function of the prior distribution P = [p1, . . . , pk], the prior vectors \u03bc1, . . . , \u03bck, and the local variance \u03c32 x.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nD.2 WHEN THE PRIOR DISTRIBUTION IS UNKNOWN\n\nNow, we consider a more practical case when the prior distribution P = [p1, . . . , pk] and the candidates \u03bc1, . . . , \u03bck are unknown to the clients. In this case, the clients collaborate with each other by their local data to estimate the priors P and \u03bc1, . . . , \u03bck, and then, each client uses the estimated priors to design her personalized model as in (38).\n\ni\n\ni\n\n1\n\n1\n\nk\n\nk\n\n(cid:80)n\n\n:= 1 n\n\n], \u03bc(t+1)\n\n, . . . , p(t+1)\n\n, . . . , \u03bc(t+1)\n\nfor given local models {\u03b8(t)\n\n} for given global priors P(t+1) and \u03bc(t+1)\n\nWe present Algorithm 3 based on alternating minimization. The algorithm starts by initializing the local models {\u03b8(0) j=1 Xij}. Then, the algorithm works in rounds alternating between estimating the priors P(t+1) = [p(t+1) i } and estimating the personalized models {\u03b8(t+1) , . . . , \u03bc(t+1) .\nObserve that for given the prior information P(t), {\u03bct l}, each client updates her personalized model in Step 6 which is the optimal estimator for given priors according to Theorem 6. On the other hand, for given personalized models {\u03b8(t) l} using clustering algorithm with k sets in Step 11. The algorithm Cluster takes m vectors a1, . . . , am and an integer k as its input, and its goal is to generate a set of k cluster centers \u03bc1, . . . , \u03bck that minimizes (cid:80)m i=1 minl\u2208k (cid:107)ai \u2212 \u03bcl(cid:107)2. Furthermore, these clustering algorithms can also return the prior distribution P, by setting pl := |Sl| m , where Sl \u2282 {a1, . . . , am} denotes the set of vectors that are belongs to the l-th cluster. There are lots of algorithms that do clustering, but perhaps, Lloyd\u2019s algorithm Lloyd (1982) and Ahmadian Ahmadian et al. (2019) are the most common algorithms for k-means clustering. Our Algorithm 3 can work with any clustering algorithm.\n\ni }, we estimate the priors P(t), {\u03bct\n\nk\n\n1\n\nAlgorithm 3 Alternating Minimization for Personalized Estimation Input: Number of iterations T , local datasets (Xi1, . . . , Xin) for i \u2208 [m].\n\n(cid:80)n\n\nj=1 Xij for i \u2208 [m].\n\n1: Initialize \u03b80 i = 1 n\n2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do\n\nReceive P(t), \u03bc(t) Update the personalized model:\n\n1 , . . . , \u03bc(t)\n\nk from the server\n\n\u03b8t\n\ni \u2190\n\nk (cid:88)\n\nl=1\n\n\u03b1(i)\n\nl \u03bc(t)\n\nl\n\nand\n\n\u03b1(i)\n\nl =\n\n(cid:18)\n\nexp\n\n\u2212\n\np(t)\n\nl\n\n(cid:80)n\n\nj=1 (cid:107)Xij \u2212\u03bc(t) 2\u03c32 x\n\nl (cid:107)2\n\n(cid:19)\n\n(cid:80)k\n\ns=1 p(t)\n\ns exp\n\n(cid:18)\n\n\u2212\n\n(cid:80)n\n\nj=1 (cid:107)Xij \u2212\u03bc(t) 2\u03c32 x\n\ns (cid:107)2\n\n(cid:19)\n\nSend \u03b8t\n\ni to the server\n\n7: 8: 9: 10:\n\n11:\n\nend for At the Server: Receive \u03b8(t) Update the global parameters: P(t), \u03bc(t) Broadcast P(t), \u03bc(t)\n\nm from the clients\n\n1 , . . . , \u03b8(t)\n\n1 , . . . , \u03bc(t)\n\nk to all clients\n\n12: 13: end for Output: Personalized models \u03b8T\n\n1 , . . . , \u03b8T m.\n\n1 , . . . , \u03bc(t)\n\nk \u2190 Cluster\n\n(cid:16)\n\n\u03b8(t)\n\n1 , . . . , \u03b8(t)\n\nm , k\n\n(cid:17)\n\nD.3 PRIVACY/COMMUNICATION CONSTRAINTS\n\nIn the personalized estimation Algorithm 3, each client shares her personalized estimator \u03b8(t) to the server at each iteration which is not communication-efficient and violates the privacy. In this section we present ideas on how to design communication-efficient and/or private Algorithms for personalized estimation. Lemma 3. Let \u03bc1, . . . \u03bck \u2208 Rd be unknown means such that (cid:107)\u03bci(cid:107)2 \u2264 r for each i \u2208 [k]. Let \u03b81, . . . , \u03b8m \u223c P, where P = [p1, . . . , pk] and pl = Pr[\u03b8i = \u03bcl]. For i \u2208 [m], let Xi1, . . . , Xin \u223c\n\ni\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nN (\u03b8i, \u03c32\n\nx), i.i.d. Then, with probability at least 1 \u2212 1\n\nmn , the following bound holds for all i \u2208 [m]:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\nj=1\n\nXij\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n(cid:114)\n\n\u2264 4\n\nd\n\n\u03c32 x\nn\n\n(cid:114)\n\n+ 2\n\nlog(m2n)\n\n\u03c32 x\nn\n\n+ r.\n\n(43)\n\nProof. Observe that the vector (X i \u2212 \u03b8i) = 1 proxy \u03c32\n\nn . As a result, we have that:\n\nn\n\nx\n\n(cid:80)n\n\ni=1 Xij \u2212 \u03b8i is a sub-Gaussian random vector with\n\n\u03c32 x\nn with probability at least 1 \u2212 \u03b7 from Wainwright (2019). Since \u03bc1, . . . , \u03bck \u2208 Rd are such that (cid:107)\u03bci(cid:107)2 \u2264 r for each i \u2208 [k], we have:\n\n(cid:107)X i \u2212 \u03b8i(cid:107)2 \u2264 4\n\nlog(1/\u03b7)\n\n\u03c32 x\nn\n\n(44)\n\n+ 2\n\nd\n\n,\n\n(cid:114)\n\n(cid:114)\n\n(cid:107)X i(cid:107)2 \u2264 4\n\nd\n\n+ 2\n\nlog(1/\u03b7)\n\n(cid:114)\n\n(cid:114)\n\n\u03c32 x\nn\n\n\u03c32 x\nn\n\n+ r,\n\n(45)\n\nwith probability 1 \u2212 \u03b7 from the triangular inequality. Thus, by choosing \u03b7 = 1 union bound, this completes the proof of Lemma 3.\n\nm2n and using the\n\nLemma 3 shows that the average of the local samples {X i} has a bounded (cid:96)2 norm with high probability. Thus, we can design a communication-efficient estimation Algorithm as follows: Each client clips her personal model \u03b8(t) n + r. Then, each client applies a vector-quantization scheme (e.g., Bernstein et al. (2018); Alistarh et al. (2017); Girgis et al. (2021a)) to the clipped vector before sending it to the server.\n\ni within radius 4\n\nlog(m2n) \u03c32\n\nn + 2\n\nd \u03c32\n\n(cid:113)\n\n(cid:113)\n\nx\n\nx\n\n(cid:113)\n\ni within radius 4\n\nTo design a private estimation algorithm with discrete priors, each client clips her personalized estimator \u03b8(t) n + r. Then, we can use a differentially private algorithm for clustering (see e.g., Stemmer (2020) for clustering under LDP constraints and Ghazi et al. (2020) for clustering under central DP constraints.). Since, we run T iterations in Algorithm 3, we can obtain the final privacy analysis ((cid:15), \u03b4) using the strong composition theorem Dwork & Roth (2014).\n\nlog(m2n) \u03c32\n\nn + 2\n\nd \u03c32\n\n(cid:113)\n\nx\n\nx\n\nE PERSONALIZED LEARNING \u2013 LINEAR REGRESSION\n\nIn this section, we present the personalized linear regression problem. Consider A set of m clients, where the i-th client has a local dataset consisting of n samples (Xi1, Yi1), . . . , (Xin, Yin), where Xij \u2208 Rd denotes the feature vector and Yij \u2208 R denotes the corresponding response. Let Yi = (Yi1, . . . , Yi1) \u2208 Rn and Xi = (Xi1, . . . , Xin) \u2208 Rn\u00d7d denote the response vector and the feature matrix at the i-th client, respectively. Following the standard regression, we assume that the response vector Yi is obtained from a linear model as follows:\n\n(46) where \u03b8i denotes personalized model of the i-th client and wi \u223c N (cid:0)0, \u03c32 (cid:1) is a noise vector. The clients\u2019 parameters \u03b81, . . . , \u03b8m are drawn i.i.d. from a Gaussian distribution \u03b81, . . . , \u03b8m \u223c N (\u03bc, \u03c32 \u03b8\n\nYi = Xi\u03b8i + wi,\n\nId), i.i.d.\n\nIn\n\nx\n\nOur goal is to solve the optimization problem stated in (9) (for the linear regression setup) and learn the optimal personalized parameters {(cid:98)\u03b8i}. The following theorem characterizes the exact form of the optimal {(cid:98)\u03b8i} and computes their minimum mean squared error w.r.t. the true parameters {\u03b8i}. Theorem 7. The optimal personalized parameters at client i with known \u03bc, \u03c32 x is given by:\n\n\u03b8 , \u03c32\n\n(cid:98)\u03b8i =\n\n(cid:18) I\n\n\u03c32 \u03b8\n\n+\n\nX T i Xi \u03c32 x\n\n(cid:19)\u22121 (cid:18) X T i Yi \u03c32 x\n\n+\n\n(cid:19)\n\n.\n\n\u03bc \u03c32 \u03b8\n\nThe mean squared error (MSE) of the above (cid:98)\u03b8i is given by:\n\nEwi,\u03b8i\n\n(cid:13) (cid:13) (cid:13)(cid:98)\u03b8i \u2212 \u03b8i\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= Tr\n\n(cid:32)(cid:18) I \u03c32 \u03b8\n\n+\n\nX T i Xi \u03c32 x\n\n(cid:19)\u22121(cid:33)\n\n,\n\n(47)\n\n(48)\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nProof. The personalized model with perfect prior is obtained by solving the optimization problem stated in (9), which is given below for convenience. Note that for linear regression with Gaussian prior, we have P(\u0393) \u2261 N (\u03bc, \u03c32\n\nId) and p\u03b8i(Yij|Xij) according to N (\u03b8i, \u03c32\n\nx).\n\n\u03b8\n\n(cid:98)\u03b8i = arg min\n\n\u03b8i\n\n= arg min\n\n\u03b8i\n\nn (cid:88)\n\nj=1\n\nn (cid:88)\n\nj=1\n\n\u2212 log(p\u03b8i(Yij|Xij)) \u2212 log(p(\u03b8i)).\n\n(Yij \u2212 Xij\u03b8i)2 2\u03c32 x\n\n+\n\n(cid:107)\u03b8i \u2212 \u03bc(cid:107)2 2\u03c32 \u03b8\n\n.\n\n= arg min\n\n\u03b8i\n\n(cid:107)Yi \u2212 Xi\u03b8i(cid:107)2 2\u03c32 x\n\n+\n\n(cid:107)\u03b8i \u2212 \u03bc(cid:107)2 2\u03c32 \u03b8\n\n.\n\nBy taking the derivative with respect to \u03b8i, we get\n\n\u2202 \u2202\u03b8i\n\n=\n\nX T\n\ni (Xi\u03b8i \u2212 Yi) \u03c32 x\n\n+\n\n\u03b8i \u2212 \u03bc \u03c32 \u03b8\n\n.\n\n(49)\n\nEquating the above partial derivative to zero, we get that the optimal personalized parameters (cid:98)\u03b8i is given by:\n\n(50)\n\n(51)\n\n(cid:98)\u03b8i =\n\n(cid:18) I\n\n\u03c32 \u03b8\n\n+\n\nX T i Xi \u03c32 x\n\nTaking the expectation w.r.t. wi, we get:\n\n(cid:19)\u22121 (cid:18) X T i Yi \u03c32 x\n\n+\n\n(cid:19)\n\n.\n\n\u03bc \u03c32 \u03b8\n\nEwi[(cid:98)\u03b8i] =\n\n(cid:18) I\n\n\u03c32 \u03b8\n\n+\n\nX T i Xi \u03c32 x\n\n(cid:19)\u22121 (cid:18) X T\n\ni Xi\u03b8i \u03c32 x\n\n+\n\n(cid:19)\n\n,\n\n\u03bc \u03c32 \u03b8\n\nThus, we can bound the MSE as following:\n\nEwi,\u03b8i\n\n(cid:13) (cid:13) (cid:13)(cid:98)\u03b8i \u2212 \u03b8i\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= Ewi,\u03b8i\n\n= Ewi,\u03b8i\n\n= Ewi,\u03b8i\n\n(cid:13) (cid:13) 2\n(cid:13)(cid:98)\u03b8i \u2212 Ewi[(cid:98)\u03b8i] (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13)(cid:98)\u03b8i \u2212 Ewi[(cid:98)\u03b8i] (cid:13) (cid:13) (cid:13)\n\n+ Ewi,\u03b8i\n\n(cid:13) (cid:13) 2\n(cid:13)(cid:98)\u03b8i \u2212 Ewi [(cid:98)\u03b8i] + Ewi [(cid:98)\u03b8i] \u2212 \u03b8i (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:68)\n\nEwi[(cid:98)\u03b8i] \u2212 \u03b8i\n\nEwi[(cid:98)\u03b8i] \u2212 \u03b8i\n\n+ Ewi,\u03b8i\n\nIn\n\nthe\n\nlast\n\nequality,\n\nwe\n\nused\n\nEwi,\u03b8i\n\n(cid:98)\u03b8i \u2212 Ewi[(cid:98)\u03b8i], Ewi[(cid:98)\u03b8i] \u2212 \u03b8i\n\n(cid:69)\n\n=\n\nEwi[(cid:98)\u03b8i] \u2212 Ewi[(cid:98)\u03b8i], Ewi [(cid:98)\u03b8i] \u2212 \u03b8i\n\n(cid:68)\n\nE\u03b8i independent of wi.\n\n(cid:69)\n\n= 0, where the first equality holds because Ewi[(cid:98)\u03b8i] \u2212 \u03b8i is\n\nLetting M = I\n\n\u03c32 \u03b8\n\n+ X T\n\ni Xi \u03c32 x\n\n, and Tr denoting the trace operation, we get\n\nEwi,\u03b8i\n\n(cid:13) (cid:13) (cid:13)(cid:98)\u03b8i \u2212 \u03b8i\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:32)\n\n= Tr\n\nM \u22121Ewi\n\n(cid:34)(cid:18) X T i wi \u03c32 x\n\n(cid:19) (cid:18) X T i wi \u03c32 x\n\n(cid:19)T (cid:35)\n\n(cid:33)\n\nM \u22121\n\n(cid:32)\n\n+ Tr\n\nM \u22121E\u03b8i\n\n(cid:34)(cid:18) \u03b8i \u2212 \u03bc\n\n(cid:19) (cid:18) \u03b8i \u2212 \u03bc\n\n(cid:19)T (cid:35)\n\n\u03c32 \u03b8\n\n\u03c32 \u03b8\n\n(cid:33)\n\nM \u22121\n\n(cid:19)\n\n(cid:18)\n\nM \u22121\n\n+ Tr\n\nM \u22121\n\n(cid:19)\n\nM \u22121\n\nI\n\n\u03c32 \u03b8\n\n(cid:18)\n\n= Tr\n\nM \u22121 X T\n\ni Xi \u03c32 x\n\n= Tr (cid:0)M \u22121(cid:1) .\n\nThis completes the proof of Theorem 7.\n\nObserve that the local model of the i-th client, i.e., estimating \u03b8i only from the local data (Yi, Xi), is given by:\n\n(l)\n\ni = (cid:0)X T\n\ni Xi\n\n(cid:98)\u03b8\n\n(cid:1)\u22121\n\nX T\n\ni Yi,\n\n(52)\n\n27\n\n+ 2Ewi,\u03b8i\n\n(cid:68)\n\n(cid:98)\u03b8i \u2212 Ewi [(cid:98)\u03b8i], Ewi[(cid:98)\u03b8i] \u2212 \u03b8i\n\n(cid:69)\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 Linear Regression GD Input: Number of iterations T , local datasets (Yi, Xi) for i \u2208 [m], learning rate \u03b7.\n\ni for i \u2208 [m], \u03bc0, \u03c32,0\n\nx , \u03c32,0 \u03b8 .\n\n1: Initialize \u03b80 2: for t = 1 to T do 3: On Clients: 4: 5:\n\nfor i = 1 to m: do\n\nReceive and set \u03bct\n\nUpdate the personalized model: \u03b8t\n\ni = \u03bct, \u03c32,t\n\n\u03b8,i = \u03c32,t\n\nx,i = \u03c32,t (cid:18)\n\nx\n\n\u03b8 , \u03c32,t i \u2190 \u03b8t\u22121\n\ni + \u03b7\n\n(cid:80)n\n\nj=1\n\nXij (Yij \u2212Xij \u03b8t\u22121 \u03c32,t\u22121\n\ni\n\nx,i\n\n(cid:19)\n\n)\n\n+ \u03bct\u22121\n\ni \u2212\u03b8t\u22121 \u03c32,t\u22121\n\ni\n\n\u03b8,i\n\nUpdate local version of mean: \u03bct\n\ni \u2190 \u03bct\u22121\n\ni \u2212 \u03b7 (cid:18)\n\n(cid:18)\n\n\u03bct\u22121\n\ni \u2212\u03b8t\u22121 \u03c32,t\u22121\n\ni\n\n\u03b8,i\n\n(cid:19)\n\nUpdate local variance: \u03c32,t\n\nx,i \u2190 \u03c32,t\u22121\n\nx,i \u2212 \u03b7\n\nUpdate global variance: \u03c32,t\n\n\u03b8,i \u2190 \u03c32,t\u22121\n\n\u03b8,i\n\n\u2212 \u03b7\n\n(cid:19)\n\n)2\n\nn 2\u03c32,t\u22121 (cid:18)\n\nx,i\n\n\u2212 (cid:80)n\n\nj=1\n\n(Yij \u2212Xij \u03b8t\u22121 i\n2(\u03c32,t\u22121 )2 (cid:19)\n\nx,i\n\nd 2\u03c32,t\u22121\n\n\u03b8,i\n\n\u2212 (cid:107)\u03bct\u22121\n\ni \u2212\u03b8t\u22121 2(\u03c32,t\u22121 )2\n\ni\n\n\u03b8,i\n\n(cid:107)2\n\nend for At the Server: (cid:80)m Aggregate mean: \u03bct = 1 Aggregate global variance: \u03c32,t Aggregate local variance: \u03c32,t Broadcast \u03bct, \u03c32,t\n\n\u03b8 , \u03c32,t\n\nm\n\nx\n\ni\n\ni=1 \u03bct \u03b8 = 1 x = 1\n\nm\n\nm\n\n15: 16: end for Output: Personalized models \u03b8T\n\n1 , . . . , \u03b8T m.\n\n(cid:80)m i=1 \u03c32,t i=1 \u03c32,t\n\nx,i\n\n\u03b8,i\n\n(cid:80)m\n\n6:\n\n7:\n\n8:\n\n9:\n\n10: 11: 12:\n\n13:\n\n14:\n\nwhere we assume the matrix X T local estimate achieves the MSE given by:\n\ni Xi has a full rank (otherwise, we take the pseudo inverse). This\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:98)\u03b8\n\n(l)\n\ni \u2212 \u03b8i\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n= Tr\n\n(cid:16)(cid:0)X T\n\ni Xi\n\n(cid:1)\u22121(cid:17)\n\n\u03c32 x,\n\n(53)\n\nwe can prove it by following similar steps as the proof of Theorem 7. When \u03c32 see that the local estimate (52) matches the personalized estimate in (47).\n\n\u03b8 \u2192 \u221e, we can easily\n\nTo make the regression problem more practical, we assume that the mean \u03bc, the local variance \u03c32 \u03b8 are unknown. Hence, we estimate the personalized parameters by minimizing the negative log likelihood:\n\nx, and the global variance \u03c32\n\n(cid:98)\u03b81, . . . , (cid:98)\u03b8m = arg min\n\n{\u03b8i},\u03bc,\u03c32\n\nx,\u03c32\n\n\u03b8\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n\u2212 log (p\u03b8i (Yij|Xij)) +\n\nm (cid:88)\n\ni=1\n\n\u2212 log (p (\u03b8i))\n\n= arg min\n\nnm 2\n\nlog(2\u03c0\u03c32\n\nx) +\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n(Yij \u2212 Xij\u03b8i)2 2\u03c32 x\n\n+\n\nmd 2\n\nlog(2\u03c0\u03c32\n\n\u03b8 ) +\n\nm (cid:88)\n\ni=1\n\n(cid:107)\u03b8i \u2212 \u03bc(cid:107)2 2\u03c32 \u03b8\n\n.\n\n(54)\n\nInstead of solving the above optimization problem explicitly, we can optimize it through gradient descent (GD) and the resulting algorithm is presented in Algorithm 4. In addition to keeping the personalized models {\u03b8t x,i} and updates all these parameters by taking appropriate gradients of the objective in (54) and synchronize them with the server to update the global copy of these parameters {\u03bct, \u03c3t\n\ni}, each client also maintains local copies of {\u03bct\n\n\u03b8,i, \u03c3t\n\ni, \u03c3t\n\n\u03b8, \u03c3t\n\nx}.\n\nF PERSONALIZED LEARNING \u2013 LOGISTIC REGRESSION\n\nAs described in Section 3, by taking P(\u0393) \u2261 N (\u03bc, \u03c32 Id) and p\u03b8i(Yij|Xij) = \u03c3((cid:104)\u03b8i, Xij(cid:105))Yij (1 \u2212 \u03c3((cid:104)\u03b8i, Xij(cid:105)))(1\u2212Yij ), where \u03c3(z) = 1/1+e\u2212z for any z \u2208 R, then the overall optimization problem becomes:\n\n\u03b8\n\n28\n\nPublished as a conference paper at ICLR 2023\n\narg min {\u03b8i},\u03bc,\u03c3\u03b8\n\nm (cid:88)\n\nn (cid:88)\n\n(cid:20)\n\ni=1\n\nj=1\n\nYij log\n\n(cid:18)\n\n1 1 + e\u2212(cid:104)\u03b8i,Xij (cid:105)\n\n(cid:19)\n\n+ (1 \u2212 Yij) log\n\n(cid:18)\n\n1 1 + e(cid:104)\u03b8i,Xij (cid:105)\n\n(cid:19)(cid:21)\n\n+\n\nmd 2\n\nlog(2\u03c0\u03c32\n\n\u03b8 ) +\n\nm (cid:88)\n\ni=1\n\n(cid:107)\u03bc \u2212 \u03b8i(cid:107)2 2\u03c32 \u03b8\n\n2\n\n.\n\n(55)\n\nWhen \u03bc and \u03c32 regression case. The corresponding algorithm is described in Algorithm 5.\n\n\u03b8 are unknown, we would like to learn them by gradient descent, as in the linear\n\nAlgorithm 5 Logistic Regression SGD Input: Number of iterations T , local datasets (Yi, Xi) for i \u2208 [m], learning rate \u03b7.\n\ni for i \u2208 [m], \u03bc0, \u03c32,0 \u03b8 .\n\n1: Initialize \u03b80 2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do Receive (\u03bct, \u03c32,t Update the personalized model:\n\n\u03b8 ) from the server and set \u03bct\n\ni := \u03bct, \u03c32,t\n\n\u03b8,i := \u03c32,t\n\n\u03b8\n\n\u03b8t\n\ni \u2190 \u03b8t\u22121\n\ni \u2212 \u03b7\n\n\uf8eb\n\n\uf8ed\n\nn (cid:88)\n\nj=1\n\n\u2207\u03b8t\u22121\n\ni\n\nl(p) CE(\u03b8t\u22121\n\ni\n\n, (X j\n\ni , Y j\n\ni )) +\n\n\u03bct\u22121\n\ni \u2212 \u03b8t\u22121 \u03c32,t\u22121\n\ni\n\n\u03b8,i\n\n\uf8f6\n\n\uf8f8 ,\n\nwhere l(p)\n\nCE denotes the cross-entropy loss. i \u2190 \u03bct\u22121\n\nUpdate local version of mean: \u03bct\n\ni \u2212 \u03b7\n\nUpdate global variance: \u03c32,t \u03b8,i\n\n\u2190 \u03c32,t\u22121 \u03b8,i\n\n\u2212 \u03b7\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\n\u03bct\u22121\n\ni \u2212\u03b8t\u22121 \u03c32,t\u22121 \u03b8,i\n\ni\n\nd 2\u03c32,t\u22121\n\n\u03b8,i\n\n\u2212 (cid:107)\u03bct\u22121\n\ni \u2212\u03b8t\u22121 2(\u03c32,t\u22121 )2\n\ni\n\n\u03b8,i\n\n(cid:19)\n\n(cid:107)2\n\n7:\n\n8:\n\n9: 10: 11: 12:\n\n13:\n\n14:\n\nSend (\u03bct\n\ni, \u03c32,t\n\n\u03b8,i ) to the server\n\ni, \u03c32,t\n\nend for At the Server: Receive {(\u03bct (cid:80)m Aggregate mean: \u03bct = 1 Aggregate global variance: \u03c32,t Broadcast (\u03bct, \u03c32,t\n\n\u03b8 ) to all clients\n\nm\n\n\u03b8,i )} from the clients\n\ni\n\ni=1 \u03bct \u03b8 = 1\n\nm\n\n15: 16: end for Output: Personalized models \u03b8T\n\n1 , . . . , \u03b8T m.\n\n(cid:80)m\n\ni=1 \u03c32,t\n\n\u03b8,i\n\nG PERSONALIZED LEARNING \u2013 MIXTURE MODEL\n\nIn this section, we present the linear regression problem as a generalization to the estimation problem with discrete priors. This model falls into the framework studied in Marfoq et al. (2021) and is illustrated to show how our framework also captures it.\n\nConsider a set of m clients, where the i-th client has a local dataset (Xi1, Yi1), . . . , (Xin, Yin) of m samples, where Xij \u2208 Rd denotes the feature vector and Yij \u2208 R denotes the corresponding response. Let Yi = (Yi1, . . . , Yi1) \u2208 Rn and Xi = (Xi1, . . . , Xin) \u2208 Rn\u00d7d denote the response vector and the feature matrix at the i-th client, respectively. Following the standard regression, we assume that the response vector Yi is obtained from a linear model as follows:\n\n(56) where \u03b8i denotes personalized model of the i-th client and wi \u223c N (cid:0)0, \u03c32 (cid:1) is a noise vector. The clients models are drawn i.i.d. from a discrete distribution \u03b81, . . . , \u03b8m \u223c P, where P = [p1, . . . , pk] such that pl = Pr[\u03b8i = \u03bcl] for i \u2208 [m] and l \u2208 [k].\n\nYi = Xi\u03b8i + wi,\n\nIn\n\nx\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nOur goal is to solve the optimization problem stated in (9) (for the linear regression with the above discrete prior) and learn the optimal personalized parameters {(cid:98)\u03b8i}. We assume that the discrete distribution P and the prior candidates {\u03bcl}k l=1 are unknown to the clients. Inspired from Algorithm 3 for estimation with discrete priors, we obtain Algorithm 6 for learning with discrete prior. Note that this is not a new algorithm, and is essentially the algorithm proposed in Marfoq et al. (2021) applied to linear regression. We wanted to show how our framework captures mixture model in Marfoq et al. (2021) through this example.\n\nDescription of Algorithm 6. Client i initializes its personalized parameters \u03b8(0) i Yi, which is the optimal as a function of the local dataset at the i-th client without any prior knowledge. In any iteration t, for a given prior information P(t), {\u03bc(t) l }, the i-th client updates the personalized\n\ni Xi)\u22121X T\n\ni = (X T\n\nmodel as \u03b8t\n\ni = (cid:80)k\n\nl=1 \u03b1(i)\n\nl \u03bc(t)\n\nl\n\n, where the weights \u03b1(i)\n\nl \u221d p(t)\n\nl\n\nexp\n\n(cid:18)\n\n\u2212 (cid:107)Xi\u03bc(t)\n\nl \u2212Yi(cid:107)2 2\u03c32 x\n\n(cid:19)\n\nand sends its\n\ncurrent estimate of the personalized parameter \u03b8t will run Cluster algorithm to update the global parameters P, \u03bc(t) the clients.\n\ni to the server. Upon receiving \u03b8t\n\n1 , . . . , \u03bc(t)\n\n1, . . . , \u03b8t m, server k , and broadcasts them to\n\nAlgorithm 6 Alternating Minimization for Personalized Learning Input: Number of iterations T , local datasets (Xi, Yi) for i \u2208 [m].\n\n1: Initialize \u03b80\n\ni = (X T\n\ni Xi)\u22121X T\n\ni Yi for i \u2208 [m] (if X T\n\ni Xi is not full-rank, take the pseudo-inverse).\n\n2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do\n\nReceive P(t), \u03bc(t) Update the personalized parameters and the coefficients:\n\nk from the server\n\n1 , . . . , \u03bc(t)\n\n\u03b8t\n\ni \u2190\n\nk (cid:88)\n\nl=1\n\n\u03b1(i)\n\nl \u03bc(t)\n\nl\n\nand\n\n\u03b1(i)\n\nl =\n\n(cid:18)\n\np(t)\n\nl\n\nexp\n\n\u2212 (cid:107)Xi\u03bc(t)\n\nl \u2212Yi(cid:107)2 2\u03c32 x\n\n(cid:19)\n\n(cid:80)k\n\ns=1 p(t)\n\ns exp\n\n(cid:16)\n\n\u2212 (cid:107)Xi\u03bc(t)\n\ns \u2212Yi(cid:107)2 2\u03c32 x\n\n(cid:17)\n\nSend \u03b8(t)\n\ni\n\nto the server\n\n7: 8: 9: 10:\n\n11:\n\nend for At the Server: Receive \u03b8(t) Update the global parameters: P(t), \u03bc(t) Broadcast P(t), \u03bc(t)\n\nm from the clients\n\n1 , . . . , \u03b8(t)\n\n1 , . . . , \u03bc(t)\n\nk to all clients\n\n12: 13: end for Output: Personalized models \u03b8T\n\n1 , . . . , \u03b8T m.\n\n1 , . . . , \u03bc(t)\n\nk \u2190 Cluster\n\n(cid:16)\n\n\u03b8(t)\n\n1 , . . . , \u03b8(t)\n\nm , k\n\n(cid:17)\n\nH PERSONALIZED LEARNING \u2013 ADAPED\n\nH.1 KNOWLEDGE DISTILLATION POPULATION DISTRIBUTION\n\nIn this section we discuss what type of a population distribution can give rise to algorithms/problems that include a knowledge distillation (KD) (or KL divergence) penalty term between local and global models. From Section 3, Equation (9), consider p\u03b8i(y|x) as a randomized mapping from input space X to output class Y, parameterized by \u03b8i. For simplicity, consider the case where |X | is finite, e.g. for MNIST it could be all possible 28 \u00d7 28 black and white images. Every p\u03b8i(y|x) corresponds to a probability matrix (parameterized by \u03b8i) of size |Y| \u00d7 |X |, where the (y, x)\u2019th represents the probability of the class y (row) given the data sample x (column). Therefore, each column is a probability vector. Since we want to sample the probability matrix, it suffices to restrict our attention to any set of |Y| \u2212 1 rows, as the remaining row can be determined by these |Y| \u2212 1 rows.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nSimilarly, for a global parameter \u03bc, let p\u03bc(y|x) define a randomized mapping from X to Y, parameterized by the global parameter \u03bc. Note that for a fixed global parameter \u03bc, the randomized map p\u03bc(y|x) is fixed, whereas, our goal is to sample p\u03b8i (y|x) for i = 1, . . . , m, one for each client. For simplicity of notation, define p\u03b8i := p\u03b8i (y|x) and p\u03bc := p\u03bc(y|x) to be the corresponding probability matrices, and let the distribution for sampling p\u03b8i(y|x) be denoted by pp\u03bc (p\u03b8i). Note that different mappings p\u03b8i(y|x) correspond to different \u03b8i\u2019s, so we define p(\u03b8i) (in Equation (9)) as pp\u03bc (p\u03b8i), which is the density of sampling the probability matrix p\u03b8i(y|x).\n\nFor the KD population distribution, we define this density pp\u03bc(p\u03b8i) as:\n\npp\u03bc (p\u03b8i) = c(\u03c8)e\u2212\u03c8DKL(p\u03bc(y|x)(cid:107)p\u03b8i (y|x))\n\n(57)\n\nwhere \u03c8 is an \u2018inverse variance\u2019 type of parameter, c(\u03c8) is a normalizing function that depends on (\u03c8, p\u03bc), and DKL(p\u03bc(y|x)(cid:107)p\u03b8i(y|x)) = (cid:80) tional KL divergence, where p(x) denotes the probability of sampling a data sample x \u2208 X . Now all we need is to find c(\u03c8) given a fixed \u03bc (and therefore fixed p\u03bc(y|x)). Here we consider DKL(p\u03bc(cid:107)p\u03b8i), but our analysis can be extended to DKL(p\u03b8i (cid:107)p\u03bc) or (cid:107)p\u03b8i \u2212 p\u03bc(cid:107)2 as well.\n\n(cid:16) p\u03bc(y|x) p\u03b8i (y|x)\n\ny\u2208Y p\u03bc(y|x) log\n\nx\u2208X p(x) (cid:80)\n\nis the condi-\n\n(cid:17)\n\nFor simplicity and to make the calculations easier, we consider a binary classification task with Y = {0, 1} and define p\u03bc(x) := p\u03bc(y = 1|X = x) and qi(x) := p\u03b8i(y = 1|X = x). We have: (cid:16)\n\nDKL(p\u03bc(y|x)(cid:107)p\u03b8i(y|x)) =\n\np(x)\n\np\u03bc(x)(log p\u03bc(x) \u2212 log qi(x))\n\n(cid:88)\n\nx\n\n+ (1 \u2212 p\u03bc(x))(log(1 \u2212 p\u03bc(x)) \u2212 log(1 \u2212 qi(x)))\n\n(cid:17)\n\n.\n\nHence, after some algebra we have, pp\u03bc(p\u03b8i) = c(\u03c8)e\u03c8 (cid:80)\n\nx p(x)H(p\u03bc(x))e\u03c8 (cid:80)\n\nx p(x)(p\u03bc(x) log(qi(x))+(1\u2212p\u03bc(x)) log(1\u2212qi(x))))\n\nThen,\n\nc(\u03c8)\n\n(cid:104) (cid:90) 1\n\n(cid:89)\n\nx\n\n0\n\nNote that\n\n(cid:105) e\u03c8p(x)H(p\u03bc(x))e\u03c8p(x)(p\u03bc(x) log(qi(x))+(1\u2212p\u03bc(x)) log(1\u2212qi(x))))dqi(x)\n\n= 1.\n\n(cid:90) 1\n\n0\n\ne\u03c8p(x)(p\u03bc(x) log(qi(x))+(1\u2212p\u03bc(x)) log(1\u2212qi(x))))dqi(x) = B\n\n(cid:18)\n\n1 +\n\np\u03bc(x) \u03c8p(x)\n\n, 1 +\n\n1 \u2212 p\u03bc(x) \u03c8p(x)\n\n(cid:19)\n\nAccordingly, after some algebra, we can obtain c(\u03c8) =\n\nShannon entropy. Substituting this in (57), we get\n\ne\u2212\u03c8 (cid:80) (cid:16) 1+\n\nx p(x)H(p\u03bc(x)) p\u03bc(x) \u03c8p(x) ,1+\n\n1\u2212p\u03bc (x) \u03c8p(x)\n\n(cid:81)\n\nx B\n\n(cid:17) , where H is binary\n\npp\u03bc (p\u03b8i) =\n\ne\u2212\u03c8 (cid:80) x B(1 + p\u03bc(x)\n\n(cid:81)\n\nx p(x)H(p\u03bc(x))\n\n\u03c8p(x) , 1 + 1\u2212p\u03bc(x) \u03c8p(x) )\n\ne\u2212\u03c8DKL(p\u03bc(y|x)(cid:107)p\u03b8i (y|x))\n\nwhich is the population distribution that can result in a KD type regularizer. Note that when we take the negative logarithm of the population distribution we obtain KL divergence loss and an additional term that depends on \u03c8 and p\u03bc. This is the form seen in Section 3.2 Equation (11) for AdaPeD\n\n(cid:18)\n\ne\u2212\u03c8 (cid:80) x B(1+\n\nx p(x)H(p\u03bc(x)) p\u03bc(x) \u03c8p(x) ,1+\n\n1\u2212p\u03bc (x)\n\n\u03c8p(x) )\n\n(cid:19)\n\nto\n\nalgorithm. For numerical purpose, we take the additional term \u2212 log\n\n(cid:81)\n\nbe simple 1 2 log(2\u03c8). As mentioned in Section 3.2, this serves the purpose of regularizing \u03c8. This is in contrast to the objective considered in Ozkara et al. (2021), which only has the KL divergence loss as the regularizer, without the additional term.\n\nH.2 ADAPED WITH UNSAMPLED CLIENT ITERATIONS\n\nWhen there is a flexibility in computational resources for doing local iterations, unsampled clients can do local training on their personalized models to speed-up convergence at no cost to privacy. This can be used in cross-silo settings, such as cross-institutional training for hospitals, where privacy is\n\n31\n\nPublished as a conference paper at ICLR 2023\n\ncrucial and there are available computing resources most of the time. We propose the algorithm for AdaPeD with with unsampled client iterations in Algorithm 7:\n\nAlgorithm 7 Adaptive Personalization via Distillation (AdaPeD) with unsampled client iterations\n\nParameters: model {\u03bc0 K.\n\nlocal variances {\u03c80 i }, synchronization gap \u03c4 ,\n\ni }, personalized models {\u03b80\n\nlocal copies of the global learning rates \u03b71, \u03b72, \u03b73, number of sampled clients\n\ni },\n\nif \u03c4 divides t then On Server do: Choose a subset Kt \u2286 [n] of K clients Broadcast \u03bct and \u03c8t On Clients i \u2208 Kt (in parallel) do: i = \u03bct, \u03c8t Receive \u03bct and \u03c8t; set \u03bct\n\n1: for t = 0 to T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: 9: On Clients i /\u2208 Kt (in parallel) do:\n\nend if\n\ni = \u03c8t\n\n10:\n\nCompute gt\n\ni := \u2207\u03b8t\n\ni\n\nfi(\u03b8t\n\ni) +\n\n\u2207\u03b8t\n\ni\n\nreceived global parameters from the server Update: \u03b8t+1\n\ni = \u03b8t 11: 12: On Clients i \u2208 Kt (in parallel) do:\n\ni \u2212 \u03b71gt\n\ni\n\nt(cid:48) i\ni )\n\ni,\u03bc\n\ni (\u03b8t f KD t(cid:48) i\n2\u03c8 i\n\nwhere t(cid:48)\n\ni is the last time index where client i\n\n\u2207\u03b8t\n\ni\n\ni,\u03bct i)\n\nf KD i (\u03b8t 2\u03c8t i\n\n13:\n\n14:\n\n15:\n\n16:\n\n17:\n\nCompute gt Update: \u03b8t+1\n\ni := \u2207\u03b8t i = \u03b8t \u2207\u03bct\n\ni\n\ni) +\n\nfi(\u03b8t i \u2212 \u03b71gt i (\u03b8t+1 f KD i\n2\u03c8t i\n\ni\n\ni\n\n,\u03bct\n\ni)\n\ni i (\u03b8t+1 i\n2(\u03c8t\n\n2\u03c8t i\n\ni \u2212 \u03b72ht \u2212 f KD i \u2212 \u03b73kt\n\ni := i = \u03bct i := 1 i = \u03c8t\n\nCompute ht Update: \u03bct+1 Compute kt Update: \u03c8t+1 if \u03c4 divides t + 1 then Clients send \u03bct Server receives {\u03bct Server computes \u03bct+1 = 1\n\n18: 19: 20: 21: 22: end if 23: 24: end for Output: Personalized models (\u03b8T\n\ni and \u03c8t\n\ni\n\nK\n\ni )m\n\ni=1\n\n,\u03bct+1\n\n)\n\ni\n\ni )2\n\ni to Server i}i\u2208Kt and {\u03c8t\n\n(cid:80)\n\ni }i\u2208Kt i\u2208Kt \u03bct\n\ni and \u03c8t+1 = 1\n\nK\n\n(cid:80)\n\ni\u2208Kt \u03c8t\n\ni\n\nOf course, when a client is not sampled for a long period of rounds this approach can become similar to a local training; hence, it might be reasonable to put an upper limit on the successive number of local iterations for each client.\n\nI PERSONALIZED LEARNING \u2013 DP-ADAPED\n\nProof of Theorem 5\n\nTheorem (Restating Theorem 5). After T iterations, DP-AdaPeD satisfies (\u03b1, (cid:15)(\u03b1))-RDP for \u03b1 > 1, where (cid:15)(\u03b1) = (cid:0) K m denotes the sampling ratio of the clients at each global iteration.\n\n, where K\n\n(cid:16) C2 1\nK\u03c32\n\n+ C2\n\n6 (cid:0) T\n\n2 K\u03c32\n\n(cid:1) \u03b1\n\n(cid:1)2\n\n(cid:17)\n\nm\n\nq2\n\nq1\n\n\u03c4\n\nProof. In this section, we provide the privacy analysis of DP-AdaPeD. We first analyze the RDP of a single global round t \u2208 [T ] and then, we obtain the results from the composition of the RDP over total T global rounds. Recall that privacy leakage can happen through communicating {\u03bci} and {\u03c8t i } and we privatize both of these. In the following, we do the privacy analysis of privatizing {\u03bci} and a similar analysis could be done for {\u03c8t\n\ni } as well.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nAt each synchronization round t \u2208 [T ], the server updates the global model \u03bct+1 as follows:\n\n\u03bct+1 =\n\n1 K\n\n(cid:88)\n\ni\u2208Kt\n\n\u03bct i,\n\n(58)\n\nwhere \u03bct iterations at the i-th client. At each of the local iterations, the client clips the gradient ht threshold C1 and adds a zero-mean Gaussian noise vector with variance \u03c32 q1 noise added at the local iterations, the norm-2 sensitivity of updating the global model \u03bct+1 synchronization round t is bounded by:\n\ni is the update of the global model at the i-th client that is obtained by running \u03c4 local i with Id. When neglecting the at the\n\ni\n\n\u2206\u03bc = max Kt,K(cid:48) t\n\n(cid:107)\u03bct+1 \u2212 \u03bc(cid:48)t+1(cid:107)2\n\n2 \u2264\n\n\u03c4 C 2 1\nK 2 ,\n\n(59)\n\n(cid:80)\n\n(cid:48)t \u2282 [m] are neighboring sets that differ in only one client. Additionally, \u03bct+1 = where Kt, K 1\ni and \u03bc(cid:48)t+1 = 1 i\u2208Kt \u03bct q1 at K\neach local iteration at each client, and then, we take the average of theses vectors over K clients,\n\ni. Since we add i.i.d. Gaussian noises with variance \u03c32\n\ni\u2208K(cid:48)t \u03bct\n\n(cid:80)\n\nK\n\n\u03c4 \u03c32 q1 K . it is equivalent to adding a single Gaussian vector to the aggregated vectors with variance Thus, from the RDP of the sub-sampled Gaussian mechanism in (Mironov et al., 2019, Table 1), Bun et al. (2018), we get that the global model \u03bct+1 of a single global iteration of DP-AdaPeD is (\u03b1, (cid:15)(1)\n\nt (\u03b1))-RDP, where (cid:15)t(\u03b1) is bounded by:\n\n(cid:15)(1) t (\u03b1) =\n\n(cid:18) K m\n\n(cid:19)2 6\u03b1C 2 1\nK\u03c32 q1\n\n.\n\n(60)\n\nSimilarly, we can show that the global parameter \u03c8t+1 at any synchronization round of DP-AdaPeD is (\u03b1, (cid:15)(2)\n\nt (\u03b1))-RDP, where (cid:15)t(\u03b1) is bounded by:\n\n(cid:15)(2) t (\u03b1) =\n\n(cid:18) K m\n\n(cid:19)2 6\u03b1C 2 2\nK\u03c32 q2\n\n.\n\n(61)\n\nUsing adaptive RDP composition (Mironov, 2017, Proposition 1), we get that each synchronization round of DP-AdaPeD is (\u03b1, (cid:15)(1) t (\u03b1))-RDP. Thus, by running DP-AdaPeD over T /\u03c4 synchronization rounds and from the composition of the RDP, we get that DP-AdaPeD is (\u03b1, (cid:15)(\u03b1))- RDP, where (cid:15)(\u03b1) = (cid:0) T\n\nt (\u03b1)). This completes the proof of Theorem 5.\n\nt (\u03b1) + (cid:15)(2)\n\nt (\u03b1) + (cid:15)(2)\n\n(cid:1) ((cid:15)(1)\n\n\u03c4\n\nJ EXPANDED RELATED WORK AND CONNECTIONS TO EXISTING METHODS\n\nIn Section 1, we mentioned that the our framework has connections to several personalized FL methods. In this appendix we provide a few more details related to these connections.\n\nRegularization: As noted earlier using (9) with the Gaussian population prior connects to the use of (cid:96)2 regularizer in earlier personalized learning works Dinh et al. (2020); Ozkara et al. (2021); Hanzely & Richt \u0301arik (2020); Hanzely et al. (2020); Li et al. (2021), which also iterates between local and global model estimates. This form can be explicitly seen in Appendix E, where in Algorithm 4, we see that the Gaussian prior along with iterative optimization yields the regularized form seen in these methods. In these cases8, P(\u0393) \u2261 N (\u03bc, \u03c32 Id) for unknown parameters \u0393 = {\u03bc}. Note that since the parameters of the population distribution are unknown, these need to be estimated during the iterative learning process. In the algorithm, 4 it is seen the \u03bc plays the role of the global model (and is truly so for the linear regression problem studied in Appendix E).\n\n\u03b8\n\nClustered FL: If one uses a discrete mixture model for the population distribution then the iterative algorithm suggested by our framework connects to (Zhang et al., 2021; Mansour et al., 2020; Ghosh et al., 2020; Smith et al., 2017; Marfoq et al., 2021). In particular, consider a population model with parameters in the m-dimensional probability simplex {\u03b1 : \u03b1 = [\u03b11, . . . , \u03b1k], \u03b1i \u2265 0, \u2200i, (cid:80) i \u03b1i = 1} which describing a distribution. If there are m (unknown) discrete distributions {D1, . . . , Dm}, one can consider these as the unknown description of the population model in addition to \u03b1. Therefore,\n\n8One can generalize these by including \u03c32\n\n\u03b8 in the unknown parameters.\n\n33\n\nPublished as a conference paper at ICLR 2023\n\neach local data are generated either as a mixture as in (Marfoq et al., 2021) or by choosing one of the unknown discrete distributions with probability \u03b1 dictating the probability of choosing Di, when hard clustering is used (e.g., (Mansour et al., 2020)). Each node j chooses a mixture probability \u03b1(j) uniformly from the m-dimensional probability simplex. In the former case, it uses this mixture probability to generate a local mixture distribution. In the latter it chooses Di with probability \u03b1(j) .\n\ni\n\nAs mentioned earlier, not all parametrized distributions can be written as a mixture of finite number distributions, which is the assumption for discrete mixtures. Consider a unimodal Gaussian population Id), for node i, we sample distribution (as also studied in Appendix E). Since P(\u0393) \u2261 N (\u03bc, \u03c32 (cid:62)x, \u03c32). \u03b8i \u223c P(\u0393). We see that the actual data distribution for this node is p\u03b8i(y|x) = N (\u03b8i Clearly the set of such distributions {p\u03b8i(y|x)} cannot be written as any finite mixture as \u03b8i \u2208 Rd and p\u03b8i(y|x) is a unimodal Gaussian distribution, with same parameter \u03b8i for all data generated in node i. Essentially the generative framework of finite mixtures (as in (Marfoq et al., 2021)) could be restrictive as it does not capture such parametric models.\n\n\u03b8\n\nKnowledge distillation: The population distribution related to a regularizer based on KullbackLeibler divergence (knowledge distillation) has been shown in Appendix H. Therefore this can be cast in terms of information geometry where the probability falls of exponentially with in this geometry. Hence these connect to methods such as Lin et al. (2020); Li & Wang (2019); Shen et al. (2020); Ozkara et al. (2021), but the exact regularizer used does not take into account the full parametrization, and one can therefore improve upon these methods.\n\nFL with Multi-task Learning (MTL): In this framework, a fixed relationship between tasks is usually assumed (Smith et al., 2017). Therefore one can model this as a Gaussian model with known parameters relating the individual models. The individual models are chosen from a joint Gaussian with particular (known) covariance dictating the different models, and therefore giving the quadratic regularization used in FL-MTL (Smith et al., 2017). In this the parameters of the Gaussian model are known and fixed.\n\nj=1 Bw(i)\n\nCommon representations: The works in Du et al. (2021); Jain et al. (2021b) use a linear model where y \u223c N (x(cid:62)\u03b8i, \u03c32) can be considered a local generative model for node i. The common representation approach assumes that \u03b8i = (cid:80)k j , for some k (cid:28) d, where \u03b8i \u2208 Rd. Therefore, one can parametrize a population by this (unknown) common basis B, and under a mild assumption that the weights are bounded, we can choose a uniform measure in this bounded cube to choose w(i) for each node i. The alternating optimization iteratively discovers the global common representation and the local weights as done in Du et al. (2021); Jain et al. (2021b) (and references therein). This common linear representation approach was generalized in Du et al. (2021); Collins et al. (2021) to neural networks, where a set of parameters to obtain a common representation (\u201chead\u201d) at each client was obtained and each local client appendd it with a \u201ctail\u201d combining the representation to obtain the final model. This also fits into our statistical framework, where the common representation (head) parameters are chosen from a population model (like the common subspace in the linear case) and the tail parameters are independently chosen (again as in the linear case).\n\nEmpirical and Hierarchical Bayes: As mentioned in Section 1, our work is inspired by empirical Bayes framework, introduced in (Stein, 1956; Robbins, 1956; James & Stein, 1961), which is the origin of hierarchical Bayes methods; see also (Gelman et al., 2013, pp. 132). (Stein, 1956; James & Stein, 1961) studied jointly estimating Gaussian individual parameters, generated by an unknown (parametrized) Gaussian population distribution. They showed a surprising result that one can enhance the estimate of individual parameters based on the observations of a population of Gaussian random variables with independently generated parameters from an unknown (parametrized) Gaussian population distribution. Effectively, this methodology advocated estimating the unknown population distribution using the individual independent samples, and then using it effectively as an empirical prior for individual estimates.9 This was studied for Bernoulli variables with heterogeneously generated individual parameters by Lord (1967) and the optimal error bounds for maximum likelihood estimates for population distributions were recently developed in (Vinayak et al., 2019). Hierarchical Bayes, builds on empirical Bayes framework and is sometimes associated with a fully Bayes method. Our choice to use empirical Bayes framework as the foundation is also because\n\n9This was shown to uniformly improve the mean-squared error averaged over the population, compared to an\n\nestimate using just the single local sample.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nit is more computationally feasible than a fully Bayes method. The subtle difference between the two is that empirical Bayes uses a point estimate of a (parametrized) prior, whereas, the terminology hierarchical Bayes often refers to a fully Bayes method where the (non-parametric) prior is estimated by computationally intensive methods like MCMC (see the discussion in (Blei et al., 2003)). As mentioned in Section 1, a contribution of our work is to connect a well studied statistical framework of empirical (hierarchical) Bayes to model heterogeneity in personalized federated learning. This statistical model yields a framework for personalized FL and leads to new algorithms and bounds especially in the local data starved regime.\n\nK ADDITIONAL DETAILS AND RESULTS FOR EXPERIMENTS\n\nK.1\n\nIMPLEMENTATION DETAILS\n\nIn this section we give further details on implementation and setting of the experiments that were used in Section 4.\n\nCIFAR-100 Experiment Setting. We do additional experiments on CIFAR-100. CIFAR-100 is a dataset consisting of 100 classes and 20 superclasses. Each superclass corresponds to a category of 5 classes (e.g. superclass flowers correspond to orchids, poppies, roses, sunflowers, tulips). To introduce heterogeneity we let each client sample data samples from 2 super classes (the classification task is still to classify among 100 classes). For classification on CIFAR-100 dataset we consider a 5-layer CNN with 2 convolutional layers of 64 filters and 5x5 filter size, following that we have 2 fully connected layers with activation sizes of 384,192 and finally an output layer of dimension 100. We set number of local epochs to be 2, batch size to be 25 per client; number of clients is 50, client participation K n = 0.2, and number of epochs 200 (100 communication rounds). In this new dataset the classification task is more complex given the increased number of labels.\n\nHyperparameters. We implemented Per-FedAvg and pFedMe based on the code from GitHub,10. Other implementations were not available online, so we implemented ourselves. For each of the methods we tuned learning rate in the set {0.3, 0.2, 0.15, 0.125, 0.1, 0.075, 0.05} and have a decaying learning schedule such that learning rate is multiplied by 0.99 at each epoch. We use weight decay of 1e \u2212 4. For MNIST and FEMNIST experiments for both personalized and global models we used a 5-layer CNN, the first two layers consist of convolutional layers of filter size 5 \u00d7 5 with 6 and 16 filters respectively. Then we have 3 fully connected layers of dimension 256 \u00d7 120, 120 \u00d7 84, 84 \u00d7 10 and lastly a softmax operation. For CIFAR-10 experiments we use a similar CNN, the only difference is the first fully connected layer is of dimension 400 \u00d7 120.\n\n\u2022 AdaPeD11: We fine-tuned \u03c8 in between 0.5 \u2212 5 with 0.5 increments and set it to 3.5. We set \u03b73 = 5e \u2212 2. We manually prevent \u03c8 becoming smaller than 0.5 so that local loss does not become dominated by the KD loss. We use \u03b72 = 0.1 and \u03b71 = 0.1. 12 When taking the derivative with respect to \u03c8 we observed sometimes multiplying the right term (consist of KD loss function) by some constant (5 in our experiments) gives better performance.\n\n\u2022 Per-FedAvg Fallah et al. (2020) and pFedMe Dinh et al. (2020): For Per-FedAvg, we used 0.075 as the learning rate and \u03b1 = 0.001. For pFedMe we used the same learning rate schedule for main learning rate, K = 3 for the number of local iterations; and we used \u03bb = 0.5, \u03b7 = 0.2.\n\n\u2022 QuPeD Ozkara et al. (2021): We choose \u03bbp = 0.25, \u03b71 = 0.1 and \u03b73 = 0.1 as stated.\n\n\u2022 Federated Mutual Learning Shen et al. (2020): Since authors do not discuss the hyperparameters in the paper, we used \u03b1 = \u03b2 = 0.25, global model has the same learning schedule as the personalized models.\n\nK.2 ADDITIONAL EXPERIMENTS\n\nConvergence plots for AdaPeD. We put the experimental convergence plots (test accuracy vs no. of iteration) for AdaPeD in Figure 2.\n\n10https://github.com/CharlieDinh/pFedMe 11For federated experiments we have used PyTorch\u2019s Data Distributed Parallel package. 12We use https://github.com/tao-shen/FEMNIST_pytorch to import FEMNIST dataset.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n(a) AdaPeD Test Accuracy (in %) vs iteration on MNIST with 0.1 sampling ratio.\n\n(b) AdaPeD Test Accuracy (in %) vs iteration on FEMNIST with 0.33 sampling ratio.\n\nFigure 2: Convergence plots (test accuracy vs no. of iteration) for AdaPeD.\n\nPersonalized estimation: synthetic experiments in Bernoulli setting. For this setting, for P we consider three distributions that (Tian et al., 2017) considered: normal, uniform and \u20183-spike\u2019 which have equal weight at 1/4, 1/2, 3/4. Additionally, we consider a Beta prior. We compute squared error of personalized estimators and local estimators ( Zi n ) w.r.t. the true pi and report the average over all clients. We use m = 10000 clients and 14 local samples similar to (Tian et al., 2017). Personalized estimator provides a decrease in MSE by 37.1 \u00b1 3.9%, 12.0 \u00b1 1.6%, 24.3 \u00b1 2.8%, 34.0 \u00b1 3.7%, respectively, for each aforementioned population distribution compared to their corresponding local estimators. Furthermore, as theoretically noted, less spread out prior distributions (low data heterogeneity) results in higher MSE gap between personalized and local estimators.\n\nLinear regression. For this, we create a setting similar to (Jain et al., 2021a). We set m = 10, 000, n = 10; and sample client true models according to a Gaussian centered at some randomly chosen \u03bc with variance \u03c32 \u03b8 . We randomly generate design matrices Xi and create Yi at each client by adding a zero mean Gaussian noise with true variance \u03c32 x = 0.05 and we sample each component of \u03bc from a Gaussian with 0 mean and 0.1 standard deviation and each component of X from a Gaussian with 0 mean and variance 0.05, both i.i.d. We measure the average MSE over all clients with and compare personalized and local methods. When d = 50, personalized regression has an MSE gain of 8.0 \u00b1 0.8%, 14.8 \u00b1 1.2%, and when d = 100, 9.2\u00b11.1%, 12.3\u00b12.0% compared to local and FedAvg regression, respectively. Moreover, compared to personalized regression where \u03bc, \u03c3\u03b8, \u03c3x are known, alternating algorithm only results in 1% and 4.7% increase in MSE respectively for d = 50 and d = 100.\n\nx to Xi\u03b8i. We set true values \u03c32\n\n\u03b8 = 0.01, \u03c32\n\nEstimation Experiments. We provide more results for the estimation setting discussed in Figure 1a. In Figure 3a we have a setting with 1000 clients and 5 local samples and in Figure 3b 500 clients and 5 local samples per client. We observe with as the number of clients increase DP-Personalized Estimator can converge to Personalized Estimator with less privacy budget. We also observe compared to Figure 1a, less number of local samples increases the performance discrepancy between personalized and local estimator.\n\n(a) Private Estimation with m=1000, n=5\n\n(b) Private Estimation with m=500, n=5\n\n36\n\nFigure 3: In Figure 1a, we plot MSE vs. (cid:15)0 for personalized estimation with different number of clients, this is the same setting as Figure 1a except the number of clients and local samples.\n\n01020304050iteration405060708090100Test Accuracy (in %)0510152025303540iteration20406080100Test Accuracy (in %)0.10.20.30.40.50.60.70.8\u03b5012345MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized Estimator0.10.20.30.40.50.60.70.8\u03b5012345MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized EstimatorPublished as a conference paper at ICLR 2023\n\nAdditional Learning Experiments with Different Number of Clients. We do additional experiments with different number of clients. On FEMNIST we use the same model and same data sample per client as in Section4, number of clients is 30, total number of epochs is 30 and we fix the local iteration to be 40 per epoch, we do full client sampling to simulate a cross-silo environment. As seen in Table 4, AdaPeD continues to outperform the competing methods following the trend in Section 4.\n\nTable 4: Test accuracy (in %) for FEMNIST with m = 30 clients.\n\nMethod\n\nFedAvg FedAvg+fine tuning Jiang et al. (2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\nFEMNIST\n\n95.91 \u00b1 0.78 96.22 \u00b1 0.57 98.10 \u00b1 0.09 96.03 \u00b1 0.50 96.71 \u00b1 0.14 97.72 \u00b1 0.16 96.80 \u00b1 0.13\n\nOn CIFAR-10 we use the same model as in Section4, and divide the dataset to 30 clients where each client has access to data samples from 4 classes. Total number of epochs is 250 and we fix the local iteration to be 40 per epoch; we set K n = 0.2 and number of local epochs to be 2. AdaPeD outperforms the competing methods in parallel to the experiments in Section4, as can be seen in Table 5.\n\nTable 5: Test accuracy (in %) for CIFAR-10 with m = 30 clients.\n\nMethod\n\nFedAvg FedAvg+fine tuning Jiang et al. (2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\nCIFAR-10\n\n53.92 \u00b1 0.94 67.44 \u00b1 1.11 73.86 \u00b1 0.39 71.97 \u00b1 0.09 64.09 \u00b1 0.46 73.21 \u00b1 0.44 72.53 \u00b1 0.36\n\nAdditional Experiment Implementation Details.\n\nWe use the same strategy as in Appendix K.1 to tune the main learning rates. We use 1e-4 weight decay.\n\n\u2022 AdaPeD: We fine-tuned \u03c8 in between 0.5 \u2212 5 with 0.5 increments and set it to 4 for CIFAR-10/100 and to 3 for FEMNIST. We manually prevent \u03c8 becoming smaller than 1 so that local loss does not become dominated by the KD loss. We use \u03b72 = 0.075 and \u03b71 = 0.075 for CIFAR-10 and CIFAR-100 and \u03b72 = 0.1 and \u03b71 = 0.1 for FEMNIST.\n\n\u2022 Per-FedAvg (Fallah et al., 2020) and pFedMe (Dinh et al., 2020): For Per-FedAvg, we used 0.1 as the learning rate and \u03b1 = 0.0001. For pFedMe we used the same learning rate schedule for main learning rate, L = 3 for the number of local approximation iterations; and we used \u03bb = 0.1, \u03b7 = 0.1.\n\n\u2022 QuPeD Ozkara et al. (2021): We set \u03bbp = 0.25, \u03b71 = 0.1 for local learning rate and \u03b72 = 0.1 for\n\nglobal learning rate.\n\n\u2022 Federated Mutual Learning Shen et al. (2020): Since authors do not discuss the hyperparameters in\n\nthe paper, we used \u03b1 = \u03b2 = 0.25.\n\n37",
      "responses": [
        "# Summary Of The Paper\n\nThe paper proposes algorithms that search for suitable personalized models in a client-server type federated learning setup. The algorithms are inspired by the classical theory of parametric Bayesian risk minimization. In the personalized parameter estimation regime, the authors assume two population distributions: Gaussian and Bernoulli. Under Gaussian distribution, they report that in case the parent population becomes degenerate (i.e., variation tending to nullity), the global average turns out to be the \u2018best\u2019 estimator. Moreover, the posterior personalized mean estimator in this setup also turns out to be optimal in general. If the parent population follows a Bernoulli law, having sufficient observations from local sub-populations suggests against collaborating. The following \u2018personalization\u2019 algorithms utilize different prior distributions and regularization schemes to ensure client privacy.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe language of the article is lucid, and the presentation is also of good quality. The discussion leading up to the theoretical analyses and the algorithms is precise. I find the statistical analysis rigorous and very well represented. Prior works and relevant references are well-placed throughout the paper. \n\nWeakness/Issues:\n\nThe authors have altered the standard structure of the article, as guided by ICLR instructions. The abstract should not be full page-wide. This is a violation of the code and gives them an undue advantage over others. \n\nThe current article looks incomplete, lacking a \u2018Conclusion\u2019 section. Also, sufficient discussion regarding limitations and future work is missing.\n\nI suggest the authors present accompanying codes maintaining anonymity. \n\nIt would be very helpful if the problem statement is presented more precisely in the introduction. The authors provide a lot of references to prior work. However, amidst such a crowd, the motivation somehow fades. \n\nAs I have acknowledged, the discussion is quite rigorous. However, the is significant room for improvement when it comes to organization. \n\nThe empirical results seem insufficient, and I suggest the authors put more datasets to test if feasible.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper presents an overall well investigated research work. To enhance clarity the authors need to answer the following questions:\n\nQuestions:\n\nWhat does it mean statistically to \u201cestimate i through the help of the server.\u201d?\n\nIn the section \u2018Personalized Estimation\u2019, should it be \u201cclient i outputs an estimate i of i\u201d instead?\n\nDoes the prior distribution necessarily need to have a density in general? How \u2018realistic\u2019 are the special case assumptions of Gaussian and Bernoulli as global population distributions?\n\nIn the statement of Theorem 2., should it be \u201c[-r,r]d\u201d instead? \n\nDoes the notation \u0190 mean expectation [In Theorem 2, 3, etc.]? If so, kindly maintain any one symbol throughout. \nThe authors also use the same notation for a different purpose in Section B.2.1.\n\nShouldn\u2019t we have the sum also over the quantity on the right-hand side of equation (10) [Section 3.1]?\n\nThe quantity it may penalize the heterogeneity, but does not denote the variance. The authors should call it something else instead [Section 3.2, last paragraph].\n\nIs the strict imposition of the value \u20181\u2019 necessary in the definition of \u2018neighborhood\u2019 [Section A.3], since there is a clear possibility to generalize the result even if two datasets D and D' differ at multiple terms?\n\nIn Theorem 2, the upper bound on MSE  (5) loses its worth in a higher-dimensional setup. Can the authors talk about any remedy to the same?\n\n# Summary Of The Review\n\nThe paper may be considered for acceptance provided the authors address the above listed concerns.\n\n# Correctness\n\n3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
        "# Summary Of The Paper\nThe paper titled \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" introduces a novel statistical framework aimed at addressing the challenges posed by statistical heterogeneity in federated learning (FL). The authors propose a hierarchical statistical model that connects personalized learning to empirical Bayes methods, facilitating the development of algorithms that ensure both performance and privacy. Key contributions include the introduction of new algorithms such as AdaPeD, AdaMix, and DP-AdaPeD, which are rigorously evaluated through theoretical performance bounds and empirical experiments across diverse datasets. The findings demonstrate significant advantages in personalized estimation and learning tasks in terms of both accuracy and privacy guarantees.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its comprehensive approach to integrating statistical theory with practical algorithm development in the context of federated learning. The establishment of a unifying framework is particularly noteworthy, as it addresses a critical gap in the literature regarding personalized learning under privacy constraints. The algorithms proposed are innovative and show empirical improvements over existing methods. However, the paper could benefit from a more extensive exploration of the limitations of the proposed methods and a discussion of potential challenges in real-world applications, particularly concerning scalability and computational efficiency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings, making it accessible to readers with varying levels of expertise in federated learning and statistics. The quality of the theoretical derivations is high, and the empirical results are presented with sufficient detail to allow for reproducibility. The novelty of the contributions, particularly in the context of privacy-preserving personalized learning, is substantial, marking a significant advancement in the field.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of personalized federated learning by providing a robust statistical framework and introducing innovative algorithms with proven performance benefits and privacy guarantees. The work is well-executed, although it could be strengthened by addressing potential limitations and challenges in practical implementations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" presents a comprehensive statistical framework that unifies various personalization methods in federated learning (FL). The authors leverage classical empirical Bayes\u2019 methods to tackle the challenges posed by statistical heterogeneity in decentralized data. They propose new algorithms, including AdaPeD and DP-AdaPeD, which focus on private personalized estimation and learning, showing substantial improvements in performance over existing methods while adhering to privacy constraints. Experimental results demonstrate significant gains in accuracy and efficiency across various datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its unification of personalization techniques and strong emphasis on privacy, which are critical in FL. The theoretical insights provided, including performance bounds and algorithmic details, enhance its contribution to the field. However, the paper also has limitations, such as reliance on specific data distribution assumptions, potential complexity in implementing the algorithms, and concerns regarding scalability in very large-scale applications. Additionally, the generalizability of the results across different domains and data types may warrant further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, effectively communicating its contributions and methodologies. The quality of the writing is high, and the inclusion of theoretical insights alongside empirical results bolsters the paper's credibility. The novelty of the statistical framework and algorithms is noteworthy, although reproducibility may be hindered by the complexity of the proposed algorithms and the specific configurations used in experiments.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to personalized federated learning by introducing a robust statistical framework and effective algorithms that enhance model performance while ensuring privacy. While the empirical results are promising, further exploration of scalability and generalization is necessary to fully validate the proposed approaches.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" presents a comprehensive framework for addressing the challenges of statistical heterogeneity in federated learning (FL). The authors propose a unified statistical framework that connects various personalization methods and introduces novel algorithms, particularly AdaPeD, which leverages information-geometry regularization. The framework builds on classical empirical Bayes methodologies to develop new algorithms with privacy guarantees, demonstrating improved performance in personalized estimation and learning tasks across multiple datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical foundation, which provides performance bounds for personalized estimation and learning, and its clear derivations of algorithms that enhance privacy in federated settings. The introduction of AdaPeD, which outperforms existing algorithms, is a notable contribution. However, the paper could benefit from a more detailed discussion on the practical implementation of the proposed methods and their computational complexity, which may hinder reproducibility in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical concepts and algorithmic developments. The use of mathematical rigor enhances the quality of the exposition. However, while the theoretical contributions are significant, the practical implications could be better elucidated. The novelty of the approach, particularly in connecting empirical Bayes methods to personalized FL, is commendable. Reproducibility may be an issue due to the lack of detailed algorithmic implementation specifics.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of personalized federated learning through a novel statistical framework and new algorithms with privacy considerations. While the theoretical aspects are robust and well-presented, attention should be given to improving practical implementation details to enhance reproducibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
      ],
      "preparations": [
        null,
        "### ICLR 2023 Review Notes\n\n**Title**: A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\n\n**Authors**: Kaan Ozkara, Antonious M. Girgis, Deepesh Data, Suhas Diggavi\n\n#### Abstract\n- The paper addresses statistical heterogeneity in federated learning (FL), motivating the need for personalized learning models.\n- A unifying statistical framework is proposed, connecting existing algorithms and introducing new methods.\n- The framework is applied to personalized estimation, linking it to empirical Bayes methodology.\n- Novel algorithms for private personalized estimation and learning are developed, including AdaPeD.\n- Performance and privacy evaluations demonstrate advantages over existing methods.\n\n#### Introduction\n- Federated learning allows for decentralized model training while maintaining data privacy.\n- Statistical heterogeneity in client data necessitates personalized models rather than a single global model.\n- Various existing methods for personalization are identified, but a lack of a common statistical framework exists.\n- The paper aims to establish such a framework to facilitate algorithm development, performance bounds, and privacy guarantees.\n\n#### Main Contributions\n1. **Statistical Framework**: Introduces a hierarchical statistical model connecting to empirical Bayes, accommodating communication and privacy constraints.\n2. **Private Personalized Estimation**: Aims at estimating individual parameters with limited data while maintaining privacy. \n   - Theoretical results include performance bounds for Gaussian and Bernoulli cases.\n3. **Private Personalized Learning Algorithms**: Introduces new algorithms (AdaPeD, AdaMix, DP-AdaPeD) with rigorous privacy guarantees.\n4. **Numerical Evaluation**: Demonstrates advantages in performance and privacy for both estimation and learning tasks.\n\n#### Statistical Framework\n- Connects personalized learning to empirical Bayes methods.\n- Models local parameters generated from an unknown population distribution, which in turn generates local data.\n- Introduces a distributed setting addressing privacy and communication constraints.\n- Develops algorithms that enhance estimates by leveraging data from multiple users.\n\n#### Private Personalized Estimation\n- Focuses on estimating local parameters with heterogeneous data.\n- Employs a one-round communication scheme where clients send processed data to a server.\n- Various models (Gaussian, Bernoulli, mixture) are analyzed, with theoretical bounds derived for MSE.\n- A collaborative approach is proposed to improve estimates by balancing local and population models.\n\n#### Private Personalized Learning\n- Aims to create individual learning models for predicting labels from limited local data.\n- Introduces iterative optimization methods with strong privacy guarantees.\n- Three algorithms are developed:\n  - **AdaPeD**: Utilizes information divergence constraints for adaptive weighting of models.\n  - **AdaMix**: Combines multiple global models adaptively with local data.\n  - **DP-AdaPeD**: Integrates user-level privacy guarantees into AdaPeD.\n\n#### Related Work\n- Discusses the intersection of personalized learning, estimation, and privacy.\n- Highlights various personalization methods and the lack of privacy guarantees in existing frameworks.\n- Notes concurrent work that does not address privacy aspects.\n\n#### Personalized Estimation Section\n- **Gaussian Model**: \n  - Theorem 1 provides closed-form expressions for estimators in a Gaussian setting.\n  - The MSE is shown to be bounded by a function of local and population variances.\n- **Bernoulli Model**:\n  - Theorem 3 establishes closed-form posterior mean estimators.\n  - Discusses the impact of unknown parameters on MSE.\n- **Privacy Constraints**:\n  - Theorem 4 extends private estimation methods while ensuring user-level differential privacy.\n\n#### Personalized Learning Section\n- Describes personalized learning under a mixture model.\n- Introduces an alternating minimization algorithm that combines local and global updates.\n- Discusses the benefits of clustering methods for estimating parameters.\n\n#### Experiments\n- **Personalized Estimation**: \n  - Experiments with political data (Bernoulli setting) show significant improvements in MSE for personalized estimators over local estimators.\n- **Personalized Learning**:\n  - Evaluated on datasets like MNIST, CIFAR-10, and FEMNIST.\n  - Results indicate AdaPeD consistently outperforms other methods across various settings.\n- **Differential Privacy**: \n  - Evaluates DP-AdaPeD under different privacy budgets, demonstrating trade-offs between privacy and performance.\n\n#### Conclusion\n- The proposed framework and algorithms significantly advance personalized federated learning with formal privacy guarantees.\n- Open questions remain regarding theoretical bounds and optimization methods.\n\n#### Additional Details\n- Hyperparameter tuning and implementation details for algorithms are provided.\n- Additional experiments validate the robustness of the proposed methods across varied datasets and settings.\n\n### Summary\nThe paper presents a comprehensive framework for personalized federated learning that integrates privacy considerations, introduces new algorithms, and provides theoretical and empirical evaluations to support its claims. The proposed methods, particularly AdaPeD and DP-AdaPeD, show promising results in terms of performance and privacy, addressing significant gaps in existing literature on personalized FL.",
        "### Comprehensive Notes on \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\"\n\n#### Technical Content\n1. **Introduction to Federated Learning (FL)**:\n   - FL enables machine learning on decentralized data while preserving privacy.\n   - Challenges arise from statistical heterogeneity among client datasets, leading to the need for personalized models rather than a single global model.\n\n2. **Statistical Framework**:\n   - The paper introduces a unified statistical framework that connects various personalization methods in FL.\n   - It utilizes classical empirical Bayes\u2019 methods to model an unknown population distribution from which local parameters are generated.\n   - The framework allows for communication and privacy constraints which are critical in FL.\n\n3. **Private Personalized Estimation**:\n   - Aims to estimate local parameters (\u03b8_i) with limited data while maintaining privacy.\n   - Establishes theoretical bounds on performance and develops mechanisms to enhance estimation through collaboration among clients.\n   - The authors present results for Gaussian and Bernoulli models.\n\n4. **Private Personalized Learning**:\n   - Focuses on iterative optimization methods to learn personalized models while adhering to privacy constraints.\n   - Proposes several algorithms (e.g., AdaPeD, AdaMix, DP-AdaPeD) that utilize information-geometry regularization and adaptive weighting.\n   - Incorporates user-level differential privacy guarantees.\n\n5. **Algorithms Developed**:\n   - **AdaPeD**: An algorithm that combines information divergence constraints with user-level privacy.\n   - **AdaMix**: Uses adaptive weighting of multiple global models for personalized learning.\n   - **DP-AdaPeD**: Differentiably private adaptation of AdaPeD with strong privacy guarantees.\n\n#### Experimental Design\n1. **Personalized Estimation Experiments**:\n   - Evaluated using both synthetic data (Gaussian models) and real-world political data (Bernoulli models).\n   - Focused on the performance gain of personalized estimators over local estimators.\n\n2. **Personalized Learning Experiments**:\n   - Conducted on datasets such as MNIST, FEMNIST, CIFAR-10, and CIFAR-100.\n   - Local training was conducted on small subsets with communication efficiency in mind.\n   - Various hyperparameters were tuned for comparative methods.\n\n3. **Testing Methods**:\n   - Compared AdaPeD against established methods (e.g., FedAvg, pFedMe, Per-FedAvg) across multiple datasets.\n   - Used metrics such as mean squared error (MSE) and test accuracy.\n\n#### Results\n1. **Personalized Estimation**:\n   - Demonstrated a consistent performance improvement of personalized estimators over local estimators by up to 10.7% MSE reduction in real political data experiments.\n\n2. **Personalized Learning**:\n   - AdaPeD outperformed existing methods across all datasets, showcasing superior accuracy and adaptability.\n   - Results indicated that methods leveraging knowledge distillation, like AdaPeD, yield improved performance.\n\n3. **Differential Privacy**:\n   - DP-AdaPeD exhibited strong performance while maintaining stringent privacy guarantees.\n   - Results showed the trade-off between privacy and performance, with the personalized models retaining efficacy even under higher privacy constraints.\n\n#### Strengths\n- **Unification of Methods**: The statistical framework provides a comprehensive view that connects various personalization techniques in FL.\n- **Privacy Guarantees**: Strong focus on user-level privacy and communication efficiency.\n- **Algorithm Performance**: Demonstrated superior performance of proposed algorithms over existing methods in various settings.\n- **Theoretical Insights**: The paper offers theoretical bounds and insights into the effectiveness of collaboration under privacy constraints.\n\n#### Limitations\n- **Assumptions on Data**: The effectiveness of the proposed framework may depend on specific assumptions about the data distribution and heterogeneity.\n- **Complexity in Implementation**: The algorithms, particularly with privacy mechanisms, can be complex to implement in practice.\n- **Scalability**: While performance is shown on various datasets, the scalability of the algorithms in extremely large-scale settings remains to be fully explored.\n- **Generalization**: The results may be sensitive to the specific configurations of datasets and hyperparameters; generalization to other domains or data types may require further validation. \n\n### Conclusion\nThe paper presents a significant contribution to the field of personalized federated learning by developing a robust statistical framework and algorithms that enhance model performance while ensuring privacy. The results are promising, but further research is needed to address scalability and generalization issues.",
        "# Notes on \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\"\n\n## Authors\n- Kaan Ozkara, Antonious M. Girgis, Deepesh Data, Suhas Diggavi\n- Department of Electrical and Computer Engineering, University of California, Los Angeles\n\n## Abstract\n- The paper addresses statistical heterogeneity in federated learning (FL) motivating personalized learning where individual models are trained collaboratively.\n- Proposes a unifying statistical framework that connects various personalization methods and introduces novel algorithms.\n- The framework connects to classical empirical Bayes' methodology and develops private personalized estimation algorithms.\n- Introduces AdaPeD, an algorithm based on information-geometry regularization, which numerically outperforms existing algorithms.\n- Evaluates performance and privacy guarantees for both estimation and learning problems.\n\n## 1. Introduction\n- Federated learning allows decentralized data usage while preserving privacy.\n- Statistical heterogeneity in client data can impede performance of global models, leading to the need for personalized models.\n- The work aims to fill the gap in understanding a statistical framework for personalized FL methods, providing theoretical performance bounds and new algorithms with privacy guarantees.\n\n## Key Contributions\n1. A statistical framework for federated personalized estimation and learning.\n2. Theoretical bounds and algorithms for private personalized estimation.\n3. Privacy analysis for new private personalized learning algorithms.\n\n## Statistical Framework\n- Connects to classical empirical Bayes methodology (Stein 1956, James & Stein 1961).\n- Models clients' local parameters \u03b8_i drawn from a population distribution P, generating local data through Q(\u03b8_i).\n- Introduces a distributed setting that includes privacy and communication constraints.\n- Enables the development of statistical performance bounds and private personalized estimation algorithms.\n\n## 2. Personalized Estimation\n### Framework\n- Clients communicate to estimate their individual parameters \u03b8_i using local data and aggregated information from a server.\n- MSE is defined by:\n  \\[\n  MSE = E_{\\theta_i \\sim P} E_{\\hat{\\theta_i}, X_1, \\ldots, X_m} \\left\\| \\hat{\\theta_i} - \\theta_i \\right\\|^2\n  \\]\n### 2.1 Gaussian Model\n- Assumes local parameters \u03b8_i are sampled from a Gaussian distribution.\n- Estimator derived:\n  \\[\n  \\hat{\\theta_i} = aX_i + (1 - a)\\hat{\\mu}\n  \\]\n  where \\( a = \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_\\theta/m} \\).\n- MSE for the estimator given by:\n  \\[\n  E_{\\theta_i, X_1, \\ldots, X_m} \\left\\| \\hat{\\theta_i} - \\theta_i \\right\\|^2 \\leq d \\sigma^2_x \\left( \\frac{1-a}{m} + a \\right)\n  \\]\n\n### 2.2 Bernoulli Model\n- Assumes local parameters p_i are sampled from a Beta distribution.\n- Optimal personalized estimator:\n  \\[\n  \\hat{p_i} = aX_i + (1 - a)\\frac{\\alpha}{\\alpha + \\beta}\n  \\]\n  where \\( a = \\frac{n}{\\alpha + \\beta + n} \\).\n- MSE derived:\n  \\[\n  E_{\\hat{p_i}, X_1, \\ldots, X_m} \\left[ \\hat{p_i} - p_i \\right]^2 \\leq \\frac{n}{\\alpha + \\beta + n}\n  \\]\n\n## 3. Personalized Learning\n### General Framework\n- Models global distribution and local parameters in a client-server architecture.\n- Optimization problem defined for local parameters:\n  \\[\n  \\hat{\\theta_1, \\ldots, \\hat{\\theta_m}} = \\arg \\min_{\\theta_1, \\ldots, \\theta_m} \\sum_{i=1}^m \\sum_{j=1}^n -\\log(p_{\\theta_i}(Y_{ij}|X_{ij})) + \\sum_{i=1}^m -\\log(p(\\theta_i))\n  \\]\n\n### Algorithms\n- **AdaMix**: Adaptively combines local and global models using Gaussian mixture prior.\n- **AdaPeD**: Incorporates knowledge distillation for better performance, uses a loss function that incorporates both local and global models.\n- **DP-AdaPeD**: Adds strong privacy guarantees through differential privacy, achieving user-level privacy.\n\n### Privacy Analysis\n- Proves that DP-AdaPeD satisfies RDP guarantees:\n  \\[\n  (\\alpha, \\epsilon(\\alpha)) \\text{ after T iterations}\n  \\]\n\n## 4. Experiments\n### Experimental Setup\n- Evaluated on datasets: MNIST, FEMNIST, CIFAR-10, CIFAR-100.\n- Compared performance of proposed methods against existing algorithms on test accuracy and MSE.\n\n### Results\n- **Personalized Estimation**: Demonstrated improvement in MSE over local estimators in both Gaussian and Bernoulli settings.\n- **Personalized Learning**: AdaPeD consistently outperformed competing methods across various datasets.\n- **Privacy**: DP-AdaPeD maintained performance while providing strong privacy guarantees.\n\n## 5. Conclusion\n- Established a comprehensive statistical framework for personalized learning in federated environments.\n- Suggested new algorithms with theoretical performance guarantees and privacy analyses.\n- Open questions include exploring information theoretic lower bounds and the optimization gap in iterative methods.\n\n## References\n- Cited foundational works on empirical Bayes, differential privacy, federated learning, and related algorithms in personalized machine learning.\n\n---\n\nThis summary captures the essential details, theoretical foundations, experimental methodologies, results, and implications presented in the paper. Specific equations, theorem numbers, and experimental details are included for clarity."
      ]
    }
  ],
  "add_references": true,
  "metadata": {
    "model_config": {
      "model_name": "gpt-4o-mini",
      "max_tokens": 2000,
      "temperature": 0.7
    },
    "api_provider": "openai",
    "data_config": {
      "input_data_path": "data/iclr2023_500.json",
      "sample_size": 5
    },
    "generation_time": "2025-06-23T22:32:04.736805",
    "token_usage": {
      "prompt_tokens": 0,
      "completion_tokens": 0,
      "total_tokens": 0
    },
    "api_calls": {
      "total_calls": 0
    },
    "estimated_tokens": {
      "estimated_prompt_tokens": 980160,
      "estimated_completion_tokens": 20625,
      "estimated_total_tokens": 1000785
    }
  }
}