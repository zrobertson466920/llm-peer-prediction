{
  "example_idx": 73,
  "reference": "Published as a conference paper at ICLR 2023\n\nTOEPLITZ NEURAL NETWORK FOR SEQUENCE MODELING\n\n2Zhen Qin 2Xiaodong Han 3Weixuan Sun 2Bowen He 1Dong Li 4Yuchao Dai 5Lingpeng Kong 1Yiran Zhong∗ 1Shanghai AI Laboratory 4Northwestern Polytechnical University\n\n2SenseTime Research\n\n5The University of Hong Kong\n\n3Australian National University\n\n3Dongxu Li\n\nABSTRACT\n\nSequence modeling has important applications in natural language processing and computer vision. Recently, the transformer-based models have shown strong performance on various sequence modeling tasks, which rely on attention to capture pairwise token relations, and position embedding to inject positional information. While showing good performance, the transformer models are inefficient to scale to long input sequences, mainly due to the quadratic space-time complexity of attention. To overcome this inefficiency, we propose to model sequences with a relative position encoded Toeplitz matrix and use a Toeplitz matrix-vector production trick to reduce the space-time complexity of the sequence modeling to log linear. A lightweight sub-network called relative position encoder is proposed to generate relative position coefficients with a fixed budget of parameters, enabling the proposed Toeplitz neural network to deal with varying sequence lengths. In addition, despite being trained on 512-token sequences, our model can extrapolate input sequence length up to 14K tokens in inference with consistent performance. Extensive experiments on autoregressive and bidirectional language modeling, image modeling, and the challenging Long-Range Arena benchmark show that our method achieves better performance than its competitors in most downstream tasks while being significantly faster. The code is available at https://github.com/OpenNLPLab/Tnn.\n\n1\n\nINTRODUCTION\n\nFigure 1: The left figure shows the training speed (x-axis), performances (y-axis), and GPU memory footprints (circle sizes) of the TNN and competing methods on Long-Range Arena benchmark. The TNN beats the competitors with a clear margin. The right figure plots the extrapolation results with different sequence lengths, where the x-axis denotes sequence lengths, and the y-axis denotes log PPL. It demonstrates that regardless of the sequence length, the PPL of the TNN remains constant.\n\n∗Indicates the corresponding author. Email: zhongyiran@gmail.com\n\n1\n\n246810121416Speed (steps per sec)505560657075Long-Range Arena ScorecosFormerFlashFnetTransformer-LSTransformerNystormPerformerReformerS4TNN2000400060008000100001200014000Inference length4681012Log PPLTrans1+eluPerformercosFormerSyn(D)Syn(R)S4GSSTNNPublished as a conference paper at ICLR 2023\n\nSequence modeling is a fundamental problem in natural language processing, speech processing, and computer vision. Various sequence modeling methods have been proposed in the literature, including recurrent (Hochreiter & Schmidhuber, 1997), convolutional architectures (LeCun et al., 1989), and transformers (Vaswani et al., 2017). These models utilize various properties of sequential data for their modeling. For example, recurrent models (Hochreiter & Schmidhuber, 1997) mimic the sequential property by sequentially processing the input while maintaining hidden states through steps. Convolutional models (LeCun et al., 1989) enforce the locality bias sequentially and only interact elements within local patches. Transformers use attention matrices to model pairwise relations regardless of the distance between them. Recently, Transformers (Vaswani et al., 2017; Dosovitskiy et al., 2021) show strong performance on a wide range of applications across domains and become arguably one of the most successful architectures for sequence modeling in general.\n\nThere are two main components in transformers: the attention mechanism that learns pairwise correlations of tokens from data, and the position embedding to introduce positional inductive biases. The vanilla attention mechanism requires quadratic space-time complexity, which precludes Transformers from handling long sequences. Numerous attention variants have been proposed recently to reduce the complexity, including linear transformers (Katharopoulos et al., 2020), and Performer (Choromanski et al., 2021). Although the types of attention vary, the position embedding remains in every method, which indicates the importance of position information in sequence modeling. This motivates us to ask the following question: since position information is important, can we design a model that relies entirely on the position information of its elements regardless of their content, thus alleviating the quadratic computation cost of the vanilla attention mechanism?\n\nIn this paper, we give an affirmative answer to this question by introducing Toeplitz neural network, a new efficient architecture that solely exploits relative position relations for sequence modeling. In specific, instead of attention matrices, the Toeplitz neural network uses Toeplitz matrices to capture relations between each token pair. There are two motivations for selecting the Toeplitz matrix. One is that it compactly represents relative positional relations between tokens with much fewer parameters, i.e., 2n − 1 parameters for an n × n Toeplitz matrix. The other is that the Toeplitz matrix-vector production can be efficiently processed in O(n log n) complexity, which is exactly what we used in our token mixing operation. In this way, we avoid computing content similarities between tokens and effectively reduce the quadratic computation complexity of transformers to log linear, rendering a more efficient sequence modeling architecture.\n\nWe further propose relative position encoder, a lightweight module that generates relative position parameters to assemble the Toeplitz matrices, so that the number of the TNN’s parameters will no longer depend on the sequence length. Moreover, it allows TNN to deal with varying sequence lengths without retraining. In addition, the input sequence length extrapolation becomes an important ability in sequence modeling as training on longer sequences can be prohibitively expensive (Press et al., 2022). We propose an exponential decay bias that directly applies to the Toeplitz matrix. Our model achieves a consistent performance to a sequence length of 14K tokens in inference when training on sequences of 512 tokens. We also show analytically that the Toeplitz neural network represents a general form of sequence modeling methods, and derives transformers, CNNs, and the recently proposed State-space-based methods (Gu et al., 2022) as its special forms.\n\nWe validate our model on a wide range of sequence modeling tasks and benchmarks. These include auto-regressive language modeling, text classification, image classification, and the Long-Range Arena benchmark. As illustrated in Fig. 1, our model achieves state-of-the-art performance on most tasks at a favorable log linear space-time complexity. It also demonstrates superior extrapolation capabilities when training on shorter sequences and evaluating on longer ones off-the-shelf.\n\n2 PRELIMINARY\n\nIn this section, we introduce concepts used throughout the paper, including positional embedding, token and channel mixing, and the Toeplitz matrix. Notations used can be found in Appendix A.\n\nPositional embedding is introduced in transformers (Vaswani et al., 2017) to inject positional inductive bias. It often uses fixed or learned parameters to encode position-specific information, thus making the model position-aware. There are mainly two types of positional embeddings: the absolute positional embedding (Vaswani et al., 2017) and the relative position embedding (Shaw et al., In this work, we focus on the relative position embedding to emphasize pair-wise token 2018).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nrelations. A typical relative positional embedding (Raffel et al., 2020) is formulated as:\n\neij = q⊤\n\ni kj/\n\n√\n\nd + wi−j,\n\n(1)\n\nwhere j, i are two positional indices, eij denotes the attention score before softmax. The qi, kj represents the queries and keys in the attention. The wi−j is a positional coefficient. In this case, the relative position information is added to the attention as a bias.\n\nToken and channel mixing are used by (Yu et al., 2022) to refer to the two main procedures in sequence modeling. The token mixing refers to the process of mixing information between token pairs and the channel mixing for those between feature channels. In the Transformers, given the attention matrix A ∈ Rn×n and token matrix X ∈ Rn×d, the attention operation AX can be regarded as a token mixing process and the FFN module is used for channel mixing.\n\nResearchers often classify various sequence modeling techniques based on the token mixing techniques used. MLP-based methods (Liu et al., 2021; Tolstikhin et al., 2021) use matrix multiplication on the sequence dimension for token mixing. FFT-based methods (Lee-Thorp et al., 2022) utilize the FFT on the sequence dimension to mix token-wise information. The State-space-based methods (Gu et al., 2022) leverage the state equations and hidden states to model sequences, as well as perform interactions between tokens.\n\nToeplitz matrix is a special form of a matrix that has constant values along each diagonal running from left to right, i.e.,\n\nTij = Ti+1,j+1 = ti−j, T ∈ Rn×n.\n\n(2)\n\nThere are two nice properties of a Toeplitz matrix: 1). For an n×n Toeplitz matrix, we can efficiently describe it with 2n − 1 parameters. 2). The Toeplitz matrix-vector production is faster than standard matrix-vector production. In particular, we have:\n\nTheorem 2.1. For a Toeplitz matrix T ∈ Rn×n and any vector x ∈ Rn, the time complexity of Tx is O(n log n).\n\nWe provide detailed proof in Appendix B. This property enables us to use the Toeplitz matrices to perform efficient token mixing.\n\n3 TOEPLITZ NEURAL NETWORK\n\nIn this section, we provide a detailed design and analysis of our proposed Toeplitz Neural Network (TNN) by giving a glance at the overall structure of our model first and then describing each of its components. We also discuss the connection between the TNN and other sequence modeling methods at the end of this section.\n\n3.1 THE OVERALL ARCHITECTURE\n\nOur model consists of a stack of Gated Toeplitz Units (GTU) and GLU (Shazeer, 2020). GTU is a modified GLU layer injected with the proposed Toeplitz Neural Operator (TNO), as illustrated in Fig. 2. A TNO is used to perform token mixing with a Toeplitz matrix. To generate relative position coefficients for the Toeplitz matrix, we propose a Relative Position Encoder (RPE), a lightweight fully-connected sub-network to encode the relative position information. An exponential decay bias is also added to the Toeplitz matrix to enable extrapolation on longer inputs.\n\n3.2 TOEPLITZ NEURAL OPERATOR\n\nHere, we will show how to use a Toeplitz matrix to represent relative positional information. Let us consider i, j to be two positions in a 1D sequence, by using the relative position embedding in Eq. 1, we can define a Toeplitz matrix T ∈ Rn×n, where Tij = ti−j. Specifically, given a sequence x of n tokens, x = [x0, x1, . . . , xn−1]⊤ ∈ Rn, we use a scalar ti−j to represent the relative position coefficients between xi and xj. Then a Toeplitz matrix T ∈ Rn×n can be formed by gathering ti−j\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Network structure overview of the proposed Toeplitz Neural Network. The proposed sequence modeling block is composed of a Gated Toeplitz Unit and a GLU Shazeer (2020) and. We propose the TNO to perform token mixing with only relative position information. We use a small fully-connected network named RPE to encode relative position information.\n\nfor every token pair:\n\n\n\n \n \n\nT =\n\nt0\n\nt−1\n\n· · ·\n\nt1 ... tn−1\n\nt0\n\n. . .\n\nt−n+1 ...\n\nt0 t1\n\nt−1 t0\n\nLet us define a token mixing operation as:\n\ny = Tx ∈ Rn,\n\n\n\n \n \n\n∈ Rn×n.\n\n(3)\n\n(4)\n\nwhere y is the token mixing result. For any d-dimensional sequences, the token mixing is performed on each dimension individually.\n\nAs aforementioned in Theorem 2.1, the computation complexity of Eq. 4 is O(n log n). As we need to perform token mixing on d dimensions, our TNO has a computation complexity of O(nd log n). One following question is how to calculate the relative position coefficients in T. A naive solution is to make the coefficients learnable parameters, such that the model can directly learn them from training data. However, this solution has some drawbacks: 1). Parameter explosion. For a ddimensional sequence of n tokens, there are a total of (2n − 1)d learnable parameters, which can be prohibitively large as n increases. It also shows an unsatisfactory performance in our ablation studies in Sec. 4.3. 2). Fixed input sequence length. Since the sequence length n is fixed in training, we are unable to adjust the sequence length during inference, i.e., it will cause a crucial performance drop when the sequence length changes. To address these drawbacks, we propose a relative position encoder to generate the relative position coefficients.\n\n3.3 RELATIVE POSITION ENCODER\n\nWe illustrate the network structure of our RPE in Fig. 2, which is a fully connected network with K layers. The input of the network is a 1-dimensional scalar, i.e., the value of −(n − 1), . . . , (n − 1), ∀n ∈ N+, and output a d dimension vector, which is used to assemble the Toeplitz matrix. In this case, the number of the TNN’s parameters will no longer depend on the input sequence length and the TNN will have the flexibility to deal with various sequence lengths in the inference stage.\n\nNote that recent literature (Mildenhall et al., 2021) claims that projecting the scalar input to a higher dimensional space with high frequency functions, i.e., sin and cos functions, before passing a network can lead to better performance. However, in our ablations, we find that using the original integer achieves better performance.\n\nExponential decay bias Previous models (Vaswani et al., 2017; Qin et al., 2022) often use a fixed sequence length in both training and inference. If we need to infer a longer sequence, the model\n\n4\n\nOutputRPE2n-1×dTNOInput×dToeplitz matricesNormLinearNormLinearK ×2n-1RPELinear×d2n-1Input EmbeddingsLinearLinearTNO*Output EmbeddingsGLUN ×LinearGTUTNNPublished as a conference paper at ICLR 2023\n\nneeds to be retrained on the longer sequence length to maintain the performance, which can be prohibitively expensive in the application.\n\nALiBi (Press et al., 2022) shows that by applying a simple penalty to the query-key attention scores, the Transformer can handle longer sequence length in inference without compromising the performance. The penalty is a linear bias that is proportional to the distance between tokens. Inspired by this technique, we propose an exponential decay bias that directly applies to the Toeplitz matrix to achieve the same goal. In specific, let us define a decay rate of λ ∈ [0, 1], and the new relative position coefficients ̄ti−j in T can be expressed as:\n\nALiBi can be seen as a special case of our method. Given the equation of ALiBi:\n\n ̄ti−j = λ|i−j|ti−j.\n\n√\n\n ̄sij = q⊤\n\ni kj/\n\nd + m|i − j|,\n\nexp( ̄sij) = exp(q⊤\n\ni kj/\n\n√\n\nd) exp(m|i − j|),\n\nand\n\n√\n\nsij = q⊤\n\ni kj/\n\nd,\n\nλ ≜ exp(m),\n\n(5)\n\n(6)\n\n(7)\n\nwe have:\n\nexp( ̄sij) = exp(sij)λ|i−j|. It means the ALiBi applies an exponential decay on the softmax attention matrices whereas ours applies it on the Toeplitz matrices.\n\n(8)\n\n3.4 RELATION TO OTHER SEQUENCE MODELING MODELS\n\nIn this section, we will show the relationship between our model and other sequence modeling models such as the Transformers (Vaswani et al., 2017), CNNs (LeCun et al., 1989), and the State space (Gu et al., 2022). We also compare the theoretical space-time complexity of our model with previous sequence modeling models in Table. 1.\n\nTransformers A Transformer with relative position embedding can be expressed as:\n\nO = Softmax(QK⊤/\n\n√\n\nd + T)V.\n\n(9)\n\nComparing it with Eq. 4, the TNN can be regarded as an attention-free transformer, i.e., removing the Q, K, and the Softmax, while only keeping the relative position matrices T.\n\nCNNs A convolutional layer can be viewed as a Toeplitz matrix of a special structure. Considering a 1D convolution:\n\ny = h ∗ x, yi =\n\ni (cid:88)\n\nj=0\n\nhi−jxj, h ∈ Rm, x ∈ Rn, y ∈ Rn+m−1.\n\n(10)\n\nLet’s define a Toeplitz matrix T ∈ R(n+m−1)×(n+m−1):\n\nTst =\n\n(cid:26)ht−s 0\n\n0 ≤ t − s ≤ m − 1, 0 ≤ t ≤ n − 1 others,\n\n(cid:20)\n\n, z =\n\n(cid:21)\n\nx 0m−1\n\n∈ Rn+m−1.\n\n(11)\n\nThen:\n\ny = Tz ∈ Rn+m−1.\n\n(12)\n\nTherefore, a 1D CNN can be viewed as a special case of the TNN with a zero-padded input. For better illustration, we provide a matrix form of CNN operation in Appendix C.1.\n\nState space The equation of the State space can be expressed as:\n\nui = Aui−1 + Bxi, yi = Cui, A ∈ Rh×h, B ∈ Rh×1, C ∈ R1×h, i = 1, . . . , n\n\n(13)\n\nwhere xi is the input, yi is the output,ui is the intermediate state. According to (Gu et al., 2022), the output of the State space is:\n\nyi =\n\ni (cid:88)\n\nj=0\n\nki−jxj, k = (cid:0)CB, CAB, . . . , CAn−1B(cid:1)⊤\n\n∈ Rn.\n\n(14)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nLet’s define the Toeplitz matrix T ∈ Rn×n:\n\nThen:\n\nTi−j =\n\n(cid:26)ki−j, i ≥ j 0, i < j\n\n.\n\ny = Tx, x ∈ Rn, y ∈ Rn.\n\n(15)\n\n(16)\n\nIn this case, the State space can be regarded as a special form of TNN with the coefficients that are calculated by the State space. We also provide the matrix form in Appendix C.2 for better illustration.\n\nTable 1: Comparison of theoretical space-time complexity of several models. Parallel indicates whether parallel training is possible, n indicates the sequence length, and d indicates the feature dimension, e indicates the CNN kernel size. Here we only list about 1D CNN.\n\nMethod\n\nCNN RNN\n\nTime complexity Space complexity Parallel\n\nned\n\nnd2\n\nnd\n\nnd\n\nTrue\n\nFalse\n\nVanilla Attention\n\nLinear Attention\n\nMLP\n\nFFT\n\nState space\n\nTNN\n\nn2d\n\nn2d\n\nTrue\n\nnd2\n\nnd\n\nTrue\n\nn2d\n\nn2d\n\nTrue\n\nnd log n\n\nnd log n\n\nnd log n\n\nnd\n\nTrue\n\nnd\n\nTrue\n\nnd\n\nTrue\n\n4 EXPERIMENT We compare our method to four kinds of sequential modeling methods including attention-based methods, MLP-based methods, FFT-based methods, and State-space-based methods. In particular, we select the following methods:\n\n• Attention-based: Vanilla transformer(Vaswani et al., 2017), Transformer-LS(Zhu et al., 2021), FLASH, (Hua et al., 2022), 1+elu (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020), cosFormer (Qin et al., 2022).\n\n• MLP-based: gMLP(Liu et al., 2021), Synthesizer (Random), Synthesizer (Dense) (Tay\n\net al., 2021).\n\n• FFT-based: FNet(Lee-Thorp et al., 2022), GFNet (Rao et al., 2021), AFNO(Guibas et al.,\n\n2021).\n\n• State-space-based: S4(Gu et al., 2022), DSS (Gupta et al., 2022), GSS(Mehta et al., 2022).\n\nWe evaluate our methods on the WikiText-103 (Merity et al., 2017) for autoregressive language modeling and the input length extrapolation ability, and the GLUE benchmark (Wang et al., 2018) for bidirectional language modeling. We also validate the accuracy and efficiency of our methods in handling long-range dependencies on the Long-Range Arena benchmark (Tay et al., 2020). To demonstrate the robustness of our model, we implement our model in DeiT (Touvron et al., 2021) structure and compare its performance with the vanilla DeiT (Touvron et al., 2021) on the ImageNet1K (Deng et al., 2009) for image classification.\n\n4.1 SETTING\n\nWe implement our models in Pytorch (Paszke et al., 2019) and train them on 8 V100 GPUs. We adopt the same training configuration for all competitors, including batch size, learning rate, training epochs/updates, etc. More detailed hyper-parameters are listed in Appendix D.\n\nFor the autoregressive language modeling, all models are trained on the WikiText-103 dataset (Merity et al., 2017) for 50K steps with a learning rate of 0.005. We use perplexity (PPL) as the evaluation metric.\n\nFor the bidirectional language modeling, we choose the Roberta (Liu et al., 2019) model as the base model structure for all methods. All models are pre-trained on the WikiText-103 (Merity et al., 2017) for 50K steps with lr=0.005 and fine-tuned on the GLUE dataset (Wang et al., 2018). We use different learning rates among 1e-5, 3e-5, 6e-5, 1e-4 and choose the best result after fine-tuning for 3 epochs.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFor the Long-Range Arena benchmark, we adopt the same experimental configurations from the Skyformer Chen et al. (2021). We ensure that performances and efficiencies of all methods are obtained with a similar parameter size and the same training hyperparameters.\n\nFor the image classification on the ImageNet-1k dataset, we adopt the Deit (Touvron et al., 2021) network structure and replace the transformer layers with our model.\n\n4.2 RESULTS\n\nAutoregressive language modeling Autoregressive language modeling is a crucial task that requires the models to estimate causal probability distribution given the previously seen tokens. In Table 2, we compare the proposed TNN with competing sequence modeling models. First, compared to existing Mlp-based methods, TNN shows better performances with a clear margin on both val set and test set. Transformer-based methods are currently dominant sequence modeling methods. As a strong baseline, Transformer adopts a standard self-attention module with quadratic complexity, TNN still outperforms it on both val and test sets. in addition, TNN achieves better results than most efficient transformers including FLASH, 1+elu, Performer, and cosFormer. Finally, compared with recent emerging State-space-based sequence modeling methods, TNN achieves superior performance to all competing methods. it proves the effectiveness of our method in causal models.\n\nFurther, we also compared the extrapolation capabilities of each method. In Figure 1, we show that our method outperforms all other methods and is comparable to ALiBi (Press et al., 2022). Complete results can be found in Appendix 15.\n\nBidirectional language modeling We benchmark bidirectional modeling methods on the GLUE datasets in Table. 3. TNN achieves competitive results across all tasks. Further, it is worth noting that TNN boosts the results of CoLA by a significant margin, showing the ability of reasoning logistic information from sequences. It demonstrates the effectiveness of TNN in bidirectional language modeling.\n\nLong-Range Arena benchmark As shown in Table 4, we compare TNN with competing methods across five tasks of the LRA benchmark. The results before the Transformer-LS are taken from Skyformer (Chen et al., 2021). As demonstrated, TNN achieves the best scores on three tasks and the second places on the left two tasks. In terms of overall results, TNN outperforms all other competing methods including S4 (Gu et al., 2022) 1\n\nTable 2: Performances comparison of autoregressive language modeling on the Wikitext103 dataset. The best result is highlighted in bold and the second in underline. ↓ means lower is better. Attn stands for Attention, Ss stands for State space, Trans stands for Transformer, LS stands for Transformer-LS.\n\nMethod\n\nAttn-based Trans LS FLASH 1+elu Performer cosFormer MLP-based Syn(D) Syn(R) gMLP Ss-based S4 DSS GSS Ours TNN\n\nPPL (val)\n\n24.40 23.56 25.92 27.44 62.50 26.53\n\n31.31 33.68 28.08\n\n38.34 39.39 29.61\n\nPPL (test)\n\n24.78 24.05 26.70 28.05 63.16 27.06\n\n32.43 34.78 29.13\n\n39.66 41.07 30.74\n\nParams (m)\n\n44.65 47.89 42.17 44.65 44.65 44.65\n\n46.75 44.65 47.83\n\n45.69 45.73 43.84\n\n23.98\n\n24.67\n\n48.68\n\nFor speed comparison, we compare the training speed of the TNN with other methods in Table 5. For a fair and comprehensive comparison, we follow exactly the same configurations of the Skyformer Chen et al. (2021) and report step per second under different sequence lengths. Timing is conducted on an Nvidia A6000 GPU with 48G GPU memory.\n\nImage modeling We report classification results on the ImageNet-1k dataset in Table 6. As shown, under similar parameter sizes, TNN achieves better results than Deit-Tiny and comparable results with Deit-Small. It demonstrates the capability of our method in encoding visual signals.\n\n1We re-run the S4 experiments with the new configuration to match the number of parameters. For the sake of completeness, we also compare TNN with S4 in the original size of S4 using the suffix ”-Large” in Table14, which validates our ability to encode long sequences.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Performances comparison of bidirectional sequence modeling on the GLUE benchmark. MNLI is reported by the match/mismatch splits. MRPC is reported by F1 score. CoLA is reported by Matthews correlation coefficient. All the other tasks are measured by accuracy. The best result is highlighted in bold and the second in underline. The larger the better for all metrics. ”-” means unconverted. Attn stands for Attention, Ss stands for State space, Trans stands for Transformer, LS stands for Transformer-LS.\n\nMethod Attn-based Trans LS FLASH 1+elu Performer cosFormer MLP-based Syn(D) Syn(R) gMLP FFT-based FNet GFNet AFNO Ss-based S4 DSS GSS Ours TNN\n\nMNLI\n\nQNLI\n\nQQP\n\nSST-2 MRPC\n\nCoLA\n\nAVG\n\nParams(m)\n\n79.37/79.07 77.01/76.78 79.45/80.08 74.87/75.37 58.85/59.52 75.10/75.95\n\n50.93/51.02 52.82/52.13 73.30/73.60\n\n62.45/64.71 66.75/67.45 68.79/69.28\n\n68.45/68.42 35.46/35.22 50.53/51.58\n\n87.79 84.86 87.10 82.59 63.44 82.61\n\n62.80 62.29 80.56\n\n73.31 65.42 73.20\n\n72.14 50.80 62.58\n\n88.04 86.85 88.83 86.90 79.10 86.12\n\n81.33 78.11 86.48\n\n79.43 80.25 85.12\n\n84.61 65.18 80.98\n\n90.25 90.25 90.71 87.27 81.42 89.45\n\n82.34 82.22 90.25\n\n81.88 84.40 88.88\n\n87.04 65.37 85.67\n\n88.35 82.65 82.50 83.03 82.11 81.93\n\n81.79 81.38 82.30\n\n82.91 82.44 82.35\n\n83.36 80.95 82.11\n\n38.63 40.65 29.40 -\n19.41 33.03\n\n- 4.63 36.06\n\n- 9.62 36.19\n\n23.01 6.14 6.56\n\n78.79 77.01 76.87 70.00 63.41 74.88\n\n58.60 59.08 74.65\n\n63.53 65.19 71.97\n\n69.58 48.45 60.00\n\n124.70 128.28 127.12 124.70 124.70 124.70\n\n131.00 129.42 131.08\n\n124.70 130.06 121.57\n\n131.79 123.76 122.80\n\n76.72/76.06\n\n85.06\n\n88.30\n\n90.60\n\n82.96\n\n49.85\n\n78.51\n\n126.40\n\nTable 4: Performances Comparison on the Long Range Arena benchmark. We use bold and underline to highlight the best and the second result of each task respectively. The proposed TNN achieves the best performances and outperforms all competing methods.\n\nModel Transformer Kernelized Attention Nystromformer Linformer Informer Performer Reformer BigBird Skyformer LS cosFormer FLASH S4 TNN\n\nText 61.95 60.22 64.83 58.93 62.64 64.19 62.93 63.86 64.70 66.62 67.70 64.10 85.92 86.39\n\nListOps 38.37 38.78 38.51 37.45 32.53 38.02 37.68 39.25 38.69 40.30 36.50 38.70 50.60 47.33\n\nRetrieval 80.69 81.77 80.52 78.19 77.57 80.04 78.99 80.28 82.06 81.68 83.15 86.10 67.30 89.40\n\nPathfinder 65.26 70.73 69.48 60.93 57.83 66.30 66.49 68.72 70.73 69.98 71.96 70.25 72.44 73.89\n\nImage 40.57 41.29 41.30 37.96 38.10 41.43 48.87 43.16 40.77 47.60 51.23 47.40 78.07 77.84\n\nAVG. 57.37 58.56 58.93 54.69 53.73 58.00 58.99 59.05 59.39 61.24 62.11 61.31 70.87 74.97\n\n4.3 ABLATION STUDY\n\nNetwork structure configuration We ablate different structure configurations on the autoregressive language modeling task in Table 7. We consider three options of configuration: the GTU+GLU, GTU only, and attention+GLU. We empirically find that the GTU+GLU one achieves better performance than other options and choose it as our structure in TNN.\n\nInput of relative position encoder In Table 8, we ablate different RPE inputs on language modeling. (-(n-1),...,(n-1)) denotes that we feed 2n − 1 constants into the RPE. (-(n-1),...,(n-1))/n denotes normalized constants. The sin, cos denotes the absolute position embedding method used in (Vaswani et al., 2017). We empirically find that using the original integers as the input for the RPE leads to better performance.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nRelative position encoder There are two ways to generate relative position coefficients for the Toeplitz matrix. One is to set these coefficients as learnable parameters and allow TNN to learn them from data. The other is to use our proposed RPE network to generate these coefficients. We compare these two strategies in Table 9. The TNN with our RPE network achieves an improvement of 2.47 PPL in language modeling.\n\nTable 5: Speed comparison on Long-Range Arena benchmark. We mark it with a dash if a method exhausts GPU memory. The higher the better for all metrics. The 1K,...,5K represent the input sequence length.\n\nPerformances comparison Table 7: with different structure configurations. GTU+GLU achieves better performance in language modeling.\n\nmodel Transformer FLASH LS Performer cosFormer Linformer Reformer Nystorm State space FNet TNN\n\nSpeed(steps per sec) 4K 3K -\n- 7.23 8.47 5.24 6.28 10.04 12.02 7.79 9.19 8.77 11.26 5.69 7.46 6.11 7.46 6.66 8.35 8.39 9.18 8.07 9.90\n\n2K 3.05 11.06 8.68 16.23 12.82 15.63 10.87 9.62 14.88 14.37 15.35 Table 6: Performances comparison of image classification on the ImageNet-1k dataset.\n\n1K 15.34 20.49 15.43 28.41 22.94 27.17 20.16 14.12 25.99 24.61 25.72\n\n5K -\n6.93 4.76 9.06 7.14 7.42 4.70 5.26 5.40 7.44 7.00\n\nMethod GTU+GLU GTU only Attention+GLU\n\nPPL(val) 23.98 25.19 27.40\n\nTable 8: Results comparison with different RPE inputs.\n\nMethod (-(n-1),...,(n-1)) (-(n-1),...,(n-1))/n sin, cos\n\nPPL(val) 23.98 24.11 24.04\n\nTable 9: Performances comparison of TNN with and without RPE. RPE brings an improvement in language modeling.\n\nModel Transformer TNN\n\nDeiT-Tiny Acc 72.20 72.29\n\nDeiT-Small\n\nParam Acc 5.7M 79.90 6.4M 79.20\n\nParam 22.0M 23.4M\n\nMethod TNN TNN w/o RPE\n\nPPL(val) 23.98 26.45\n\nExponential decay rate We ablate different exponential decay rates in Table 10 on the language modeling. We train these model variants with a fixed sequence length of 512 and test them on a series of sequence lengths from 512 to 14336 and compute the average PPL. When there is no exponential decay, the model fails to extrapolate to a longer sequence length. We also test our model with a learnable decay rate, but it does not show better performance. We empirically select 0.99 as the exponential decay rate in our method.\n\nTable 10: Ablation of exponential decay rates in input length extrapolation. The model variants are trained on a fixed sequence length of 512 and tested on a series of sequence lengths ranging from 512 to 14336. We compute the average PPL for all sequence lengths.\n\nDecay rate\n\n0.99 (ours) 0.90 0.95 0.999 1 (no decay) learnable\n\nPPL (val) 23.98 25.28 24.56 23.98 24.03 27.65\n\nAvg PPL (extrapolation) 23.70 25.22 24.63 24.56 672.72 24.39\n\n5 CONCLUSION\n\nIn this paper, we propose Toeplitz neural network, a new efficient architecture that relies entirely on relative positional information for sequence modeling. The proposed model enjoys a favorable log linear space-time complexity. Thanks to the proposed relative position encoder and exponential decay techniques, Toeplitz neural network generalizes to long sequences with a fixed budget of parameters while obtaining consistently superior performance than competing methods across multiple challenging tasks, including language modeling, image modeling, and sequence modeling on long inputs, i.e., the Long-Range Arena benchmark. Toeplitz neural network is also a generic sequence modeling approach, which renders various popular architectures, such as Transformers, CNNs, and State-space-based methods, as its special forms, offering a unified view for sequence modeling.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRonald Newbold Bracewell and Ronald N Bracewell. The Fourier transform and its applications,\n\nvolume 31999. McGraw-hill New York, 1986.\n\nYifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr ̈om method. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021.\n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.\n\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with perIn International Conference on Learning Representations, 2021. URL https: formers. //openreview.net/forum?id=Ua6zuk0WRH.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy.\n\nRobert M Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends® in\n\nCommunications and Information Theory, 2(3):155–239, 2006.\n\nAlbert Gu, Karan Goel, and Christopher R ́e. Efficiently modeling long sequences with structured\n\nstate spaces. In The International Conference on Learning Representations (ICLR), 2022.\n\nJohn Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. In International\n\nEfficient token mixing for transformers via adaptive fourier neural operators. Conference on Learning Representations, 2021.\n\nAnkit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured\n\nstate spaces, 2022.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv\n\npreprint arXiv:2202.10447, 2022.\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ̧ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020.\n\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.\n\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4296–4313, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.naacl-main.319. URL https://aclanthology.org/2022.naacl-main.319.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nHanxiao Liu, Zihang Dai, David So, and Quoc V Le. Pay attention to mlps. Advances in Neural\n\nInformation Processing Systems, 34:9204–9215, 2021.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language model-\n\ning via gated state spaces. arXiv preprint arXiv:2206.13947, 2022.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. 5th International Conference on Learning Representations, ICLR, Toulon, France, 2017.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99–106, 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.\n\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= Bl8CQrx2Up4.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nYongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for image classification. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464–468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://aclanthology.org/N18-2074.\n\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020.\n\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning, pp. 10183–10192. PMLR, 2021.\n\nIlya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34:24261– 24272, 2021.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers &amp; distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347–10357, July 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, 2018.\n\nWeihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10819–10829, 2022.\n\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=M_ lkFOwVdYc.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA MATHEMATICAL NOTATIONS\n\nAppendix\n\nNotation\n\nX Q, K, V O\nd m⊤ s\n1d Id\n\nMeaning\n\nHidden state. Query, key, value. Attention output. Feature dimension. s-th row of matrix M . All-ones vector with dimension d. Identity matrix with dimension d.\n\nTable 11: Mathematical notations used in the paper.\n\nB PROOF OF THEOREM\n\nIn this section, we will prove Theorem 2.1. Before doing that, let’s first introduce the circulant matrix and Toeplitz matrix: Definition B.1. A matrix C ∈ Rn×n is a circulant matrix if and only if Cij = c(i−j+n) mod n , i.e.,\n\nC =\n\n\n\n \n \n \n \n \n\n\nc0\n\nc1\n\nc2 ... ... cn−1\n\ncn−1\n\ncn−2\n\nc0\n\nc1 . . .\n\n. . .\n\ncn−1 . . . . . . . . . . . .\n\n· · · . . . . . . . . .\n\nc1 c2\n\n· · ·\n\n. . .\n\nc1 ... ...\n\ncn−1\n\ncn−2\n\nc0 c1\n\ncn−1 c0\n\n\n\n \n \n \n \n \n\n\n∈ Rn×n.\n\n(17)\n\nDefinition B.2. A matrix T ∈ Rn×n is a Toeplitz matrix if and only if Tij = ti−j , i.e.,\n\nT =\n\n\n\n \n \n \n \n \n\n\nt0\n\nt1\n\nt2 ... ... tn−1\n\nt−1\n\nt0\n\nt1 . . .\n\n. . .\n\nt−2\n\nt−1 . . . . . . . . . . . .\n\n· · · . . . . . . . . .\n\nt1 t2\n\n· · ·\n\n. . .\n\nt−n+1 ... ...\n\nt−1\n\ntn−2\n\nt0 t1\n\nt−1 t0\n\n\n\n \n \n \n \n \n\n\n∈ Rn×n.\n\n(18)\n\nBased on the definition, we can give a key lemma: Lemma B.3. A circulant matrix C ∈ Rn×n is orthogonally equivalent to the diagonal matrix Λ, in particular, the orthogonal matrix F is a n × n DFT matrix:\n\nC = F⊤ΛF,\n\nΛ = diag{F[a0, a1, . . . , an−1]⊤} ∈ Rn×n, Fst = exp\n\n(cid:19)\n\n(cid:18) 2πsti n\n\n, i2 = −1.\n\n(19)\n\nThe proof can be found in (Gray et al., 2006). Based on this, we can prove a key lemma: Lemma B.4. For a vector x ∈ Rn and a circulant matrix C ∈ Rn×n, matrix multiplication Cx can be done in O(n log n) time.\n\nProof of Lemma B. Because F, F⊤ is a DFT matrix, so Fx and F⊤x can be done O(n log n) time (Bracewell & Bracewell, 1986). Since Λ is a diagonal matrix, so Λx can be done in O(n) time,\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nnote that its diagonal elements F[a0, a1, . . . , an−1]⊤ can also be computed in O(n log n) time complexity, therefore,\n\nCx = F⊤ΛFx = F⊤ (Λ(Fx)) ,\n\n(20)\n\ncan be done in O(n log n).\n\nBased on this, we can prove Theorem 2.1:\n\nProof of Theorem 2.1. We first fill the Toeplitz matrix T ∈ Rn× into a circulant matrix C ∈ R2n×2n:\n\ni.e.,\n\nC =\n\n\n\n \n \n \n \n \n \n \n \n \n \n\nt0\n\nt1\n\nt2 ... tn−1 t0\n\nt−n+1 ...\n\nt−2 t−1\n\nt−1\n\nt0 . . .\n\n. . . tn−1 . . . . . .\n\nt−2\n\nck =\n\n. . . . . . . . . . . . . . . . . . . . .\n\n. . . . . .\n\n. . .\n\n. . .\n\nt0 t1 . . .\n\n. . . . . . . . .\n\n \n\n\n\ntk, 0 ≤ k ≤ n − 1 t0, k = n tk−2n, n + 1 ≤ k ≤ 2n − 1\n\nt−n+1 ... ...\n\nt0\n\nt−n+1 ...\n\ntn−1 . . . . . .\n\nt−1 t0 t1\n\nt2 ...\n\ntn−1 t0\n\nt−2 t−1 t0\n\nt1\n\nt2 ... tn−1\n\nt−2 t−1\n\nt0 . . .\n\n. . .\n\n. . . . . .\n\n. . . . . . . . . . . . . . . . . . . . .\n\n(21)\n\n∈ R2n×2n.\n\n,\n\nt2\n\n. . . . . . t−n+1 . . .\n\n. . .\n\nt0 t1\n\n\n\n \n \n \n \n \n \n \n \n \n \n\nt1\n\nt2 ...\n\ntn−1 t0 t−n+1 ... ...\n\nt−1 t0\n\nUsing the notation of block matrix, we can define:\n\nC =\n\n(cid:20) C1 C2 C3 C4\n\n(cid:21)\n\nFor the vector x ∈ Rn, let’s define:\n\n∈ R2n×2n, Cs ∈ Rn×n, s = 1, 2, 3, 4, C1 = T.\n\nx1 =\n\n(cid:21)\n\n(cid:20) x 0n\n\n∈ R2n,\n\nCx1 =\n\n(cid:20) C1 C2 C3 C4\n\n(cid:21) (cid:20) x 0n\n\n(cid:21)\n\n=\n\n(cid:20) C1x C3x\n\n(cid:21)\n\n=\n\n(cid:20) Tx C3x\n\n(cid:21)\n\n∈ R2n,\n\n[In 0n×n] Cx1 = [In 0n×n]\n\n(cid:21)\n\n(cid:20) Tx C3x\n\n= Tx.\n\nso:\n\ntherefore:\n\nNote that:\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\n• Computing Cx1 has a time complexity of O(2n log(2n)) = O(n log n).\n\n• [ In 0n×n ] Cx1 is equivalent to selecting the first n rows of Cx1, the time complexity\n\nis O(n).\n\nSo the total time complexity is O(n log n).\n\nC MATRIX FORM OF SEQUENTIAL MODELS\n\nIn this section, we give the matrix form of some sequence models mentioned in section 3.4.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nC.1 CNN\n\nThe matrix form of CNN mentioned in Eq. 10 is:\n\n\n\n \n \n\n\ny0 y1 y2 ... yn+m−1\n\n\n\n \n \n\n\n=\n\n\n\n \n \n \n \n \n \n \n \n \n \n\nh0\n\nh1 h2 ...\n\nhm−2\n\n0\n\nh0 h1\n\nh2 ...\n\nhm−1 hm−2\n\n0 0\n\n...\n\n0\n\nhm−1 0\n\n...\n\n0\n\n. . .\n\n. . . . . .\n\n. . .\n\n. . .\n\n...\n\n0\n\n...\n\n0\n\nh0\n\nh1 ...\n\n0\n\n...\n\n0\n\n0\n\nh0\n\nh1 ...\n\n. . . hm−3 . . . hm−2 hm−3 ...\n\nhm−1 hm−2 hm−1\n\n· · ·\n\n0\n\nC.2 STATE SPACE\n\nThe Toeplitz matrix mentioned in Eq. 15 is:\n\n\n\n \n \n \n \n \n\n\nk0\n\nk1\n\nk2 ... ... ks−1\n\n0\n\nk0\n\nk1 . . .\n\n· · ·\n\n0\n\n0\n\n... ...\n\n· · ·\n\n. . .\n\n· · · . . . 0\n. . . . . . . . . . . . . . . k1 k0 0\n· · · k2 k1 k0\n\n0\n\n0\n\nT =\n\nD CONFIGURATIONS\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n \n \n \n\n\n\n\n \n \n\n\nx0 x1 x2 ... xn−1\n\n\n\n \n \n\n\n∈ Rn+m−1.\n\n(27)\n\n∈ Rn×n.\n\n(28)\n\nTable 12: Detailed training configurations used in our experiments. “Total batch size” means batch per gpu × update freq × num gpus. “ALM” stands for Autoregressive Language Model. “BLM” stands for Bidirectional Language Model. “IM” stands for Image Modeling.\n\nData Tokenizer method Src Vocab size Sequence length Total batch size Number of updates/epochs Warmup steps/epochs Peak learning rate Learning rate scheduler Optimizer Adam ε Adam (β1, β2)\n\nWeight decay\n\nGradient clipping\n\nAML\n\nWikiText-103 BPE 50265 512 128 50k updates 4k steps 5e-4 Inverse sqrt Adam 1e-8 (0.9, 0.98) 0.2 for TNN, 0.1 for others -\n\n15\n\nBLM\n\nWikiText-103 BPE 50265 512 512 50k updates 3k steps 5e-4 Polynomial decay Adam 1e-6 (0.9, 0.98)\n\n0.2 for TNN, 0.1 for others\n\n-\n\nIM\n\nImageNet-1k -\n- -\n2048 300 epochs 5 epochs 2.5e-4 cosine Adamw 1e-8 (0.9, 0.98)\n\n0.1\n\n1.0\n\nPublished as a conference paper at ICLR 2023\n\nTable 13: Detailed model configurations used in our experiments.\n\nModel TNN Layer Feature dim GTU GTU dim GTU act GLU GLU dim GLU act RPE RPE layer RPE dim RPE act Exponential decay bias\n\nLM\n\n6 512\n\n1536 SiLU\n\n512 SiLU\n\n6 64 ReLU\n\n0.99\n\nRoberta\n\nDeit-tiny\n\nDeit-small\n\n12 768\n\n2304 SiLU\n\n768 SiLU\n\n6 64 ReLU\n\n0.99\n\n12 192\n\n576 SiLU\n\n192 SiLU\n\n1 48 ReLU\n\n0.95\n\n12 384\n\n1152 SiLU\n\n384 SiLU\n\n1 48 ReLU\n\n0.9\n\nTable 14: Performances Comparison on the Long Range Arena benchmark. We use bold and underline to highlight the best and the second result of each task respectively. The proposed TNN achieves the best performances and outperforms all competing methods.\n\nModel S4-Large TNN-Large\n\nText 86.82 87.90\n\nListOps 59.60 61.04\n\nRetrieval 90.90 90.97\n\nPathfinder 94.20 93.00\n\nPath-X Image AVG. 88.65 96.35 86.09 86.21 88.24 96.10\n\nE EXPERIMENTS\n\nF EXTRAPOLATION\n\nG VISUALIZATION\n\nIn this section, we visualize Tnn, in particular, we choose the Toeplitz matrix used in Roberta for visualization.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Visualization of the Toeplitz matrix used by each layer in Roberta, each element of the matrix represents the interaction between tokens. The Toeplitz matrices show similar behaviors to conventional transformer attention matrices where the diagonal concentrates the most attention.\n\n(i) Layer 1.\n\n(ii) Layer 2.\n\n(iii) Layer 3.\n\n(iv) Layer 4.\n\n(v) Layer 5.\n\n(vi) Layer 6.\n\n(vii) Layer 7.\n\n(viii) Layer 8.\n\n(ix) Layer 9.\n\n(x) Layer 10.\n\n(xi) Layer 11.\n\n(xii) Layer 12.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n. r\ne t\nt e\nb\n\ns i\n\nr e\nw o\n\nl\n\ns n\na e\n\nm ↓\n\n.\n\ne n\n\ni l\nr e\nd n\nu\n\nn\n\ni\n\nd n\no c\ne s\n\ne h\n\nt\n\nd n\na\n\nd\n\nl\n\no b\n\nn\n\ni\n\nd e\nt\n\nh g\n\ni l\n\nh g\n\ni\n\nh\n\ns i\n\nt l\n\nu s\ne r\n\nt s\ne b\n\ne h\nT\n\n.\n\nd o\nh\n\nt e\n\nm h\nc a\ne\n\nf o\n\ne c\nn a\n\nm\n\nr o\nf r\ne p\n\nn o\n\ni t\na l\n\no p\na r\nt\n\nx e\n\ne h\nT\n\n:\n\n5 1\n\ne l\n\nb a\nT\n\nN N\nT\n\n↓ L\nP P\n\n7 6\n\n.\n\n4 2\n\n5 2\n\n.\n\n4 2\n\n5 0\n\n.\n\n4 2\n\n1 9\n\n.\n\n3 2\n\n3 8\n\n.\n\n3 2\n\n9 7\n\n.\n\n3 2\n\n3 7\n\n.\n\n3 2\n\n3 6\n\n.\n\n3 2\n\n8 5\n\n.\n\n3 2\n\n4 5\n\n.\n\n3 2\n\n3 5\n\n.\n\n3 2\n\n1 5\n\n.\n\n3 2\n\n1 5\n\n.\n\n3 2\n\n9 4\n\n.\n\n3 2\n\n8 4\n\n.\n\n3 2\n\n8 4\n\n.\n\n3 2\n\n8 4\n\n.\n\n3 2\n\n7 4\n\n.\n\n3 2\n\n6 4\n\n.\n\n3 2\n\n0 7\n\n.\n\n3 2\n\ni\n\nB\n\ni\n\nL A\n\n↓ L\nP P\n\nS S\nG\n\n↓ L\nP P\n\n5 1\n\n.\n\n4 2\n\n8 3\n\n.\n\n3 2\n\n8 9\n\n.\n\n2 2\n\n4 7\n\n.\n\n2 2\n\n7 5\n\n.\n\n2 2\n\n2 5\n\n.\n\n2 2\n\n3 4\n\n.\n\n2 2\n\n4 2\n\n.\n\n2 2\n\n7 1\n\n.\n\n2 2\n\n1 1\n\n.\n\n2 2\n\n8 0\n\n.\n\n2 2\n\n7 0\n\n.\n\n2 2\n\n5 0\n\n.\n\n2 2\n\n3 0\n\n.\n\n2 2\n\n2 0\n\n.\n\n2 2\n\n0 0\n\n.\n\n2 2\n\n0 0\n\n.\n\n2 2\n\n0 0\n\n.\n\n2 2\n\n9 9\n\n.\n\n1 2\n\n0 4\n\n.\n\n2 2\n\n6 6\n\n.\n\n9 3\n\n6 7\n\n.\n\n9 3\n\n1 9\n\n.\n\n9 3\n\n2 8\n\n.\n\n0 4\n\n4 0\n\n.\n\n1 4\n\n8 0\n\n.\n\n1 4\n\n3 5\n\n.\n\n1 4\n\n8 0\n\n.\n\n4 4\n\n7 2\n\n.\n\n8 4\n\n2 3\n\n.\n\n3 5\n\n3 7\n\n.\n\n7 5\n\n5 2\n\n.\n\n0 6\n\n6 3\n\n.\n\n3 6\n\n2 9\n\n.\n\n4 7\n\n7 8\n\n.\n\n1 8\n\n7 6\n\n.\n\n7 8\n\n1 1\n\n.\n\n2 9\n\n0 0\n\n.\n\n6 9\n\n7 4\n\n.\n\n1 0\n1\n\n6 2\n\n.\n\n0 6\n\nS S\nD\n\n↓ L\nP P\n\n7 0\n\n.\n\n1 4\n\n0 5\n\n.\n\n0 4\n\n2 2\n\n.\n\n0 4\n\n3 0\n\n.\n\n0 4\n\n4 9\n\n.\n\n9 3\n\n5 8\n\n.\n\n9 3\n\n9 7\n\n.\n\n9 3\n\n4 6\n\n.\n\n9 3\n\n9 5\n\n.\n\n9 3\n\n4 5\n\n.\n\n9 3\n\n1 5\n\n.\n\n9 3\n\n9 4\n\n.\n\n9 3\n\n9 4\n\n.\n\n9 3\n\n6 4\n\n.\n\n9 3\n\n5 4\n\n.\n\n9 3\n\n6 4\n\n.\n\n9 3\n\n4 4\n\n.\n\n9 3\n\n3 4\n\n.\n\n9 3\n\n3 4\n\n.\n\n9 3\n\n5 7\n\n.\n\n9 3\n\n4 S\n\n↓ L\nP P\n\n4 7\n\n.\n\n0 3\n\n1 4\n\n.\n\n0 3\n\n4 2\n\n.\n\n0 3\n\n5 1\n\n.\n\n0 3\n\n8 0\n\n.\n\n0 3\n\n4 0\n\n.\n\n0 3\n\n0 0\n\n.\n\n0 3\n\n1 9\n\n.\n\n9 2\n\n8 8\n\n.\n\n9 2\n\n5 8\n\n.\n\n9 2\n\n3 8\n\n.\n\n9 2\n\n2 8\n\n.\n\n9 2\n\n2 8\n\n.\n\n9 2\n\n0 8\n\n.\n\n9 2\n\n9 7\n\n.\n\n9 2\n\n9 7\n\n.\n\n9 2\n\n9 7\n\n.\n\n9 2\n\n8 7\n\n.\n\n9 2\n\n8 7\n\n.\n\n9 2\n\n7 9\n\n.\n\n9 2\n\nP L\nM\n\ng\n\n↓ L\nP P\n\n2 1\n+ E\n3 9\n\n.\n\n8\n\n5 1\n+ E\n8 5\n\n.\n\n1\n\n6 1\n+ E\n6 9\n\n.\n\n4\n\n7 1\n+ E\n7 6\n\n.\n\n5\n\n8 1\n+ E\n9 5\n\n.\n\n3\n\n0 2\n+ E\n9 1\n\n.\n\n2\n\n1 2\n+ E\n1 6\n\n.\n\n1\n\n1 2\n+ E\n8 0\n\n.\n\n5\n\n2 2\n+ E\n6 1\n\n.\n\n1\n\n2 2\n+ E\n8 9\n\n.\n\n1\n\n2 2\n+ E\n8 7\n\n.\n\n2\n\n2 2\n+ E\n3 9\n\n.\n\n3\n\n2 2\n+ E\n3 9\n\n.\n\n4\n\n2 2\n+ E\n0 7\n\n.\n\n5\n\n2 2\n+ E\n8 1\n\n.\n\n7\n\n2 2\n+ E\n4 0\n\n.\n\n8\n\n2 2\n+ E\n1 4\n\n.\n\n9\n\n2 2\n+ E\n1 4\n\n.\n\n2\n\n9 +\nE 4\n3\n\n.\n\n1\n\n3 1\n\n.\n\n9 2\n\n)\n\nR\n\n( n\ny S\n\n↓ L\nP P\n\n)\n\nD\n\n( n\ny S\n\n↓ L\nP P\n\n6 3\n\n.\n\n7 0\n1\n\n7 5\n\n.\n\n4 8\n1\n\n7 5\n\n.\n\n0 5\n2\n\n8 4\n\n.\n\n2 0\n3\n\n0 8\n\n.\n\n5 4\n3\n\n3 1\n\n.\n\n0 9\n3\n\n5 3\n\n.\n\n5 1\n5\n\n0 3\n\n.\n\n9 8\n5\n\n9 4\n\n.\n\n1 6\n6\n\n1 6\n\n.\n\n6 1\n7\n\n8 9\n\n.\n\n9 3\n7\n\n3 6\n\n.\n\n5 7\n7\n\n7 6\n\n.\n\n9 9\n7\n\n8 3\n\n.\n\n2 0\n8\n\n1 7\n\n.\n\n0 1\n8\n\n1 1\n\n.\n\n7 3\n8\n\n2 6\n\n.\n\n4 4\n8\n\n2 7\n\n.\n\n1 6\n8\n\n3 3\n\n.\n\n6 5\n5\n\n8 7\n\n.\n\n4 3\n\n3 4\n\n.\n\n2 3\n\n6\n\n.\n\n1 0\n1\n\n8 4\n\n.\n\n9 6\n1\n\n4 4\n\n.\n\n4 2\n2\n\n4 4\n\n.\n\n5 6\n2\n\n5 5\n\n.\n\n8 9\n2\n\n6 8\n\n.\n\n2 2\n3\n\n3 6\n\n.\n\n9 9\n3\n\n5 8\n\n.\n\n4 5\n4\n\n7\n\n.\n\n5 9\n4\n\n2\n\n.\n\n7 2\n5\n\n9 6\n\n.\n\n1 5\n5\n\n8 7\n\n.\n\n5 6\n5\n\n5 1\n\n.\n\n6 7\n5\n\n4 7\n\n.\n\n8 8\n5\n\n3 3\n\n.\n\n8 9\n5\n\n5 2\n\n.\n\n0 1\n6\n\n2 4\n\n.\n\n8 1\n6\n\n3 0\n\n.\n\n7 2\n6\n\n6 5\n\n.\n\n2 2\n4\n\nr e\n\nm\n\nr o\nF s\no c\n\nr e\n\nm\n\nr o\nf r\ne P\n\n↓ L\nP P\n\n6 0\n\n.\n\n7 2\n\n0 9\n\n.\n\n2 3\n\n8 2\n\n.\n\n5 5\n\n8 8\n\n.\n\n2 0\n1\n\n6 2\n\n.\n\n5 7\n1\n\n5 6\n\n.\n\n7 6\n2\n\n2 0\n\n.\n\n8 6\n3\n\n7 7\n\n.\n\n0 2\n8\n\n1 5\n\n.\n\n5 3\n3 1\n\n0 5\n\n.\n\n5 3\n7 1\n\n9 1\n\n.\n\n6 4\n1 2\n\n9 7\n\n.\n\n4 9\n4 2\n\n4 2\n\n.\n\n2 0\n9 2\n\n2 7\n\n.\n\n8 2\n0 3\n\n3 8\n\n.\n\n7 4\n2 3\n\n1 9\n\n.\n\n1 4\n3 3\n\n1 8\n\n.\n\n4 4\n6 3\n\n0 7\n\n.\n\n1 5\n8 3\n\n6 2\n\n.\n\n1 5\n9 3\n\n5 7\n\n.\n\n4 6\n7 1\n\n↓ L\nP P\n\n4 7\n\n.\n\n9 5\n1\n\n0 3\n\n.\n\n4 0\n5\n\n6 1\n\n.\n\n3 6\n\n8 2\n\n.\n\n0 2\n0 1\n\n3 8\n\n.\n\n8 6\n5 1\n\n0 5\n\n.\n\n8 3\n1 2\n\n9 8\n\n.\n\n3 9\n6 2\n\n2 8\n\n.\n\n5 4\n9 4\n\n1 9\n\n.\n\n0 7\n1 7\n\n5 1\n\n.\n\n3 4\n4 8\n\n7 0\n\n.\n\n4 3\n2 0\n1\n\n6 5\n\n.\n\n0 2\n4 1\n1\n\n9 0\n\n.\n\n7 5\n5 2\n1\n\n6 6\n\n.\n\n7 4\n8 4\n1\n\n3 8\n\n.\n\n3 2\n6 3\n1\n\n7 7\n\n.\n\n1 6\n6 4\n1\n\n5 8\n\n.\n\n9 5\n9 7\n1\n\n5 3\n\n.\n\n6 2\n0 0\n2\n\n1 3\n\n.\n\n1 7\n9 0\n2\n\n9 7\n\n.\n\n4 8\n6 8\n\nu\n\nl e\n+ 1\n\n↓ L\nP P\n\n5 0\n\n.\n\n8 2\n\n5 3\n\n.\n\n7 4\n\n7 4\n\n.\n\n0 7\n\n8 8\n\n.\n\n1 9\n\n6 5\n\n.\n\n1 1\n1\n\n2 9\n\n.\n\n9 2\n1\n\n9 0\n\n.\n\n7 4\n1\n\n8 8\n\n.\n\n6 0\n2\n\n7 8\n\n.\n\n7 6\n2\n\n1 3\n\n.\n\n9 9\n2\n\n2 6\n\n.\n\n2 5\n3\n\n2 0\n\n.\n\n9 8\n3\n\n0 5\n\n.\n\n1 1\n4\n\n7 2\n\n.\n\n3 5\n4\n\n6 0\n\n.\n\n7 5\n4\n\n9 1\n\n.\n\n4 0\n5\n\n8 3\n\n.\n\n5 5\n5\n\n1 0\n\n.\n\n4 8\n5\n\n3 8\n\n.\n\n9 8\n5\n\n6 8\n\n.\n\n9 9\n2\n\nH S\nA L\nF\n\n↓ L\nP P\n\n7 4\n\n.\n\n5 6\n1 4\n7 1\n\n8 8\n\n.\n\n2 0\n5 6\n4 3\n\n2 1\n\n.\n\n8 8\n7 7\n4 6\n\n5\n\n.\n\n3 7\n8 9\n1 7\n1\n\n5 4\n\n.\n\n0 5\n9 6\n1\n\n9 6\n\n.\n\n4 2\n\n0 1\n+ E\n7 1\n\n.\n\n4\n\n3 1\n+ E\n7 6\n\n.\n\n2\n\n5 1\n+ E\n6 2\n\n.\n\n1\n\n6 1\n+ E\n8 5\n\n.\n\n1\n\n6 1\n+ E\n1 1\n\n.\n\n8\n\n7 1\n+ E\n0 4\n\n.\n\n3\n\n8 1\n+ E\n2 2\n\n.\n\n1\n\n8 1\n+ E\n3 0\n\n.\n\n4\n\n9 1\n+ E\n7 0\n\n.\n\n1\n\n9 1\n+ E\n2 5\n\n.\n\n2\n\n9 1\n+ E\n6 9\n\n.\n\n4\n\n0 2\n+ E\n8 2\n\n.\n\n1\n\n9 1\n+ E\n6 1\n\n.\n\n1\n\n6 +\nE 5\n2\n\n.\n\n6\n\nS L\n\n↓ L\nP P\n\n5 0\n\n.\n\n4 2\n\n9 4\n\n.\n\n3 2\n\n1 2\n\n.\n\n3 2\n\n7 0\n\n.\n\n3 2\n\n7 9\n\n.\n\n2 2\n\n7 9\n\n.\n\n2 2\n\n9 9\n\n.\n\n2 2\n\n5 2\n\n.\n\n3 2\n\n3 8\n\n.\n\n3 2\n\n6 5\n\n.\n\n4 2\n\n5 4\n\n.\n\n5 2\n\n2 4\n\n.\n\n6 2\n\n1 1\n\n.\n\n7 2\n\n8 7\n\n.\n\n8 2\n\n3 1\n\n.\n\n0 3\n\n4 1\n\n.\n\n1 3\n\n1 2\n\n.\n\n3 3\n\n2 7\n\n.\n\n4 3\n\n5 0\n\n.\n\n6 3\n\n1 7\n\n.\n\n6 2\n\nr e\n\nm\n\nr o\nf s\nn a\nr\n\nT\n\n↓ L\nP P\n\nn e\nl\n\nq e\nS\n\n8 7\n\n.\n\n4 2\n\n6 3\n\n.\n\n1 4\n\n5 3\n\n.\n\n2 6\n\n2 5\n\n.\n\n2 8\n\n7 1\n\n.\n\n0 0\n1\n\n2 4\n\n.\n\n8 1\n1\n\n4 4\n\n.\n\n3 3\n1\n\n5 9\n\n.\n\n8 8\n1\n\n6 0\n\n.\n\n6 4\n2\n\n3 9\n\n.\n\n0 7\n2\n\n5 6\n\n.\n\n1 1\n3\n\n8 5\n\n.\n\n6 4\n3\n\n8 1\n\n.\n\n2 7\n3\n\n9 2\n\n.\n\n7 8\n3\n\n4 9\n\n.\n\n5 9\n3\n\n4 5\n\n.\n\n6 2\n4\n\n0 5\n\n.\n\n3 6\n4\n\n5 3\n\n.\n\n6 0\n5\n\n6 8\n\n.\n\n6 8\n4\n\n6 3\n\n.\n\n1 6\n2\n\n2 1\n5\n\n8 6\n7\n\n4 2\n0 1\n\n0 8\n2 1\n\n6 3\n5 1\n\n2 9\n7 1\n\n8 4\n0 2\n\n2 7\n0 3\n\n6 9\n0 4\n\n0 2\n1 5\n\n4 4\n1 6\n\n8 6\n1 7\n\n2 9\n1 8\n\n6 1\n2 9\n\n0 4\n2 0\n1\n\n4 6\n2 1\n1\n\n8 8\n2 2\n1\n\n2 1\n3 3\n1\n\n6 3\n3 4\n1\n\ng v\nA\n\n18",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a new attention mechanism that depends only on the relative positions of tokens and is independent of the features themselves. Since the attention coefficient depend only on the relative positions, the resulting attention matrix is Toeplitz. This results in a reduction in the number of parameters and in time complexity of feature updates, since Toeplitz matrices can be applied in O(n log n) time, where n is the sequence length. The authors demonstrate that the proposed method closely matches the performance of conventional attention models, while being faster in terms of training time.\n\n# Strength And Weaknesses\n\nStrengths: \n\nThe proposed idea is very interesting and novel. It is quite surprising to see that attention coefficients that depend on just relative positions perform well. The paper is well-written and the the flow if ideas is great. The provided experimental evidence is sufficient to demonstrate the merit of the proposed ideas. \n\nWeaknesses:\n\nThe paper is quite good as is and does not have many weaknesses. I do have the following suggestions/ questions to the authors:\n\n1. In designing the RPE, the authors claim that learning the weights of Toeplitz matrix performs worse compared to the RPE network. This is a bit unintuitive, since the former should be able to degenerate to the latter. I am surprised to see that learning the weights directly does not perform well. Maybe the authors could shed more light on this. \n\n2. It would be great to have visualizations of the learnt attention. I am curious to see the kind of associations that are learnt on the datasets considered. Does the model identify semantic relationships in the language? What is the intuition behind using only relative position based attention to capture associations between words?\n\n3. Are there any extensions possible to incorporate multi-headed attention?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nQuality: The paper is well-written and is an interesting read. \n\nClarity: The paper is clearly written.\n\nOriginality: The presented ideas are quite novel.\n\n# Summary Of The Review\n\nThe paper presents a very interesting and novel idea. The  proposed ideas are backed up with good experimental evidence. Overall, this paper is quite refreshing to read and I recommend acceptance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper presents the Toeplitz Neural Network (TNN), addressing the inefficiencies of transformer-based sequence modeling due to the quadratic space-time complexity of attention mechanisms. The authors introduce a relative position encoded Toeplitz matrix that reduces this complexity to log-linear, enabling the model to generalize across varying sequence lengths. The TNN demonstrates superior performance across multiple benchmarks including autoregressive and bidirectional language modeling, image classification, and the Long-Range Arena benchmark, achieving faster computation times and better extrapolation capabilities.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to reducing complexity in sequence modeling, which could have significant implications for various applications in NLP and computer vision. The extensive experimental validation on diverse benchmarks highlights the model's robustness and effectiveness. However, the paper could benefit from deeper insights into the theoretical underpinnings of the Toeplitz matrix in the context of neural networks, as well as a more thorough discussion of potential limitations or scenarios where TNN may not excel.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers with a background in deep learning and sequence modeling. The quality of the experiments and results is high, with appropriate comparisons to existing methods, enhancing its credibility. The implementation is made available on GitHub, promoting reproducibility. However, further elaboration on the training process and hyperparameter tuning could improve clarity regarding the setup.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of sequence modeling by introducing a novel architecture that leverages Toeplitz matrices to improve efficiency and performance. While the findings are compelling, the paper could provide more theoretical insights and discussions on limitations to strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for efficient sequence modeling by utilizing relative positional information, thereby addressing the quadratic complexity typical of traditional transformer models. The methodology involves the use of Toeplitz matrices to represent sequences, combined with a Relative Position Encoder (RPE) and a unique architecture composed of Gated Toeplitz Units (GTU) and Gated Linear Units (GLU). The findings indicate that TNN outperforms traditional transformer models in various tasks, including autoregressive and bidirectional language modeling, long-range dependencies, and image classification, demonstrating superior extrapolation capabilities for longer sequences.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its significant contributions to efficiency, parameter economy, and generalizability. By reducing the complexity of sequence processing to O(n log n) and maintaining performance across varying input lengths, TNN addresses critical limitations of existing models. The architecture's ability to generalize to longer sequences without retraining is particularly noteworthy. However, the reliance on relative positional information may hinder the model's effectiveness in capturing complex content-based relationships, and concerns regarding overfitting and substantial training resource requirements could limit its applicability. Additionally, the lack of exploration into alternative architectures or designs limits the depth of the analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, findings, and implications. The experiments are rigorously designed, with consistent configurations ensuring fair comparisons across models. The novelty of the approach is substantial, as it proposes a new way of handling sequence modeling that diverges from traditional attention mechanisms. However, while the paper provides sufficient detail for reproducibility, the reliance on specific computational resources may pose challenges for broader adoption.\n\n# Summary Of The Review\nOverall, the TNN presents a promising advancement in efficient sequence modeling, showing strong empirical results across multiple tasks while addressing critical inefficiencies in existing transformer architectures. However, its limitations regarding reliance on positional information and high resource demands warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Toeplitz Neural Network (TNN), a novel architecture designed for sequence modeling that seeks to alleviate the quadratic time complexity associated with traditional transformer models. By employing a relative position encoded Toeplitz matrix, TNN reduces the computational burden to log-linear time, which enables it to handle sequences of up to 14,000 tokens while being trained on much shorter sequences (512 tokens). The methodology incorporates Gated Toeplitz Units (GTUs) and Gated Linear Units (GLUs) along with a lightweight Relative Position Encoder (RPE) to enhance flexibility and performance across various benchmarks, demonstrating superior results compared to existing methods.\n\n# Strength And Weaknesses\nThe key strengths of the paper are its innovative approach to reducing complexity in sequence modeling and its thorough empirical evaluation across multiple tasks, showing outstanding performance against state-of-the-art models. The introduction of the RPE and the effective use of Toeplitz matrices are significant contributions that enhance the model's capability. However, the paper could benefit from more detailed explanations of the implications of the exponential decay bias and its impact on the model's performance. Additionally, while the ablation studies provide insight into the architectural choices, further exploration of the underlying principles of the proposed methods would strengthen the theoretical foundation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, presenting a clear progression from theoretical foundations to experimental results. The equations and theorems are articulated effectively, making the complex concepts more accessible. The novelty of the TNN architecture is evident, although the paper could improve reproducibility by providing more detailed hyperparameter settings and code availability for the implemented models. Overall, the quality of the writing and the presentation of the findings are commendable.\n\n# Summary Of The Review\nThe Toeplitz Neural Network introduces a compelling approach to sequence modeling, achieving significant reductions in computational complexity while maintaining competitive performance across various benchmarks. The clarity of presentation and the thoroughness of the empirical evaluations highlight the potential impact of this work, although further elaboration on certain theoretical aspects could enhance its overall contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), which innovatively employs relative position encoding to reduce the space-time complexity of sequence modeling from quadratic to log-linear. The methodology demonstrates superior performance across various benchmarks, including autoregressive and bidirectional language modeling, image modeling, and the Long-Range Arena benchmark. Notably, TNN allows for input sequence lengths to be extrapolated up to 14K tokens without retraining, enhancing efficiency for long sequences while maintaining parameter efficiency through its novel encoding mechanism.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach**: The TNN architecture represents a significant advancement in reducing computational complexity, which is crucial for scaling up sequence models.\n2. **Performance on Benchmarks**: The results across multiple benchmarks indicate that TNN performs exceptionally well, suggesting strong applicability in various contexts.\n3. **Efficiency in Scaling**: The ability to handle longer input sequences without retraining is a major advantage for practical applications.\n4. **Parameter Efficiency**: The design allows for effective handling of varying sequence lengths while keeping the parameter count stable.\n5. **Unified Framework**: TNN’s capacity to unify different sequence modeling methods enhances its theoretical significance in the field.\n\n**Weaknesses:**\n1. **Limited Empirical Validation**: The applicability of the TNN approach might not be fully established across diverse tasks and datasets, which raises questions about its generalizability.\n2. **Context-Specific Performance**: While benchmarks show strong performance, real-world scenarios may present challenges that are not captured in the paper's evaluations.\n3. **Extrapolation Limitations**: The model's performance at lengths beyond the tested range remains uncertain, which could affect its usability in extreme cases.\n4. **Fixed Parameter Budget**: The reliance on a fixed budget could hinder the model’s ability to capture complex patterns that require more parameters.\n5. **Overlooked Model Nuances**: The unification of various methods may neglect specific advantages that distinct models offer for particular tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with sufficient detail on the methodology to allow for reproducibility. The novelty of the TNN approach is compelling, although the empirical validation could benefit from broader and more diverse evaluations. The theoretical aspects are presented adequately, but a deeper analysis of the model's performance under varied conditions would enhance the quality of the contribution.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of sequence modeling through the introduction of the Toeplitz Neural Network, showcasing impressive performance and efficiency. However, further empirical validation and robustness testing across diverse datasets are necessary to fully establish its generalizability and reliability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for efficient sequence modeling. This method employs Toeplitz matrices to manage pairwise token interactions efficiently, achieving a computational complexity of O(n log n), significantly lower than the O(n²) complexity typical of transformer models. The TNN incorporates a relative position encoder that maintains a consistent parameter count regardless of input length, demonstrating impressive extrapolation capabilities and performance improvements across various benchmarks in language modeling and image classification.\n\n# Strength And Weaknesses\nThe paper's major strengths lie in its innovative use of Toeplitz matrices for sequence modeling, which provides a fresh perspective on capturing positional relationships without the overhead of traditional attention mechanisms. The reduction in computational complexity while maintaining strong empirical performance makes a compelling case for its practical applicability. However, a potential weakness is the lack of comprehensive comparison with a broader range of sequence models, particularly those optimized for specific tasks. Additionally, while the architecture showcases impressive extrapolation capabilities, the authors could elaborate more on the theoretical implications of this feature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methodology and its advantages. The quality of the experimental validation is commendable, providing sufficient evidence of the TNN's performance across various tasks. The novelty of the approach is significant, as it challenges the prevailing transformer-based paradigm. However, reproducibility could be enhanced by providing more detailed implementation guidelines and hyperparameter settings, as well as access to code or data.\n\n# Summary Of The Review\nOverall, the Toeplitz Neural Network presents a novel and efficient approach to sequence modeling, demonstrating significant improvements in computational efficiency and performance over existing models. While the paper is clear and well-structured, some aspects of reproducibility could be strengthened. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Efficient Adversarial Training via Structured Neural Networks\" introduces a novel structured neural network architecture, specifically a Toeplitz neural network (TNN), designed to enhance the efficiency of adversarial training. By leveraging the properties of Toeplitz matrices, the proposed architecture significantly reduces the computational complexity of adversarial training from quadratic to log-linear time. The findings indicate that the TNN not only outperforms existing adversarial training methods in terms of robustness and accuracy but also maintains a manageable number of parameters across varying input sizes.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to adversarial training using a structured neural network, which is both novel and promising for real-world applications. The substantial reduction in computational complexity is a noteworthy contribution that can potentially facilitate the training of larger models, making the method highly applicable in practice. Furthermore, the empirical results presented are robust, demonstrating the TNN's superior performance over several state-of-the-art methods. However, the theoretical justification for utilizing Toeplitz matrices specifically in this context is not extensively discussed, leaving room for further exploration. Additionally, the paper could benefit from a broader comparison with a wider array of adversarial training strategies to reinforce its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its methodology and findings, making it accessible to the reader. The quality of the experiments is high, with thorough benchmarking against existing methods. The novelty of using structured neural networks for adversarial training is significant, offering new insights into efficient training techniques. However, reproducibility could be enhanced by providing more detailed descriptions of experimental setups and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to adversarial training through the use of Toeplitz neural networks. It showcases significant improvements in efficiency and performance compared to traditional methods, though further theoretical analysis and broader comparisons could enhance its contribution to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Toeplitz Neural Network for Sequence Modeling\" introduces a novel architecture known as the Toeplitz Neural Network (TNN), which aims to enhance the efficiency of sequence modeling, particularly in comparison to traditional transformer models. The authors claim that TNN significantly reduces space-time complexity to log-linear levels, thereby addressing long-standing computational inefficiencies. Additionally, the paper presents a lightweight encoder for integrating relative positional information, positing that this innovation could influence a broad spectrum of sequence modeling tasks. The experimental results purportedly showcase TNN's ability to manage input sequences up to 14K tokens, outperforming existing models across various benchmarks, although the actual performance gains appear more modest upon closer inspection.\n\n# Strength And Weaknesses\nThe strengths of the paper include its ambitious approach to improving efficiency in sequence models and the broad applicability of the proposed architecture. The introduction of the lightweight encoder is a notable contribution, potentially aiding in the processing of long sequences. However, the paper exhibits weaknesses in its overly optimistic framing of TNN's capabilities, particularly regarding performance metrics and speed comparisons. While TNN shows competitive results, the claims of state-of-the-art performance and transformative potential may be overstated, as some results demonstrate variability and modest improvements over existing models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, making it accessible to readers. However, some of the claims, particularly those regarding the revolutionary impact of TNN, could benefit from more nuanced explanations to avoid misleading interpretations. The novelty of the proposed architecture is notable, yet the actual significance may not be as groundbreaking as suggested. Reproducibility is partially addressed through extensive testing across various tasks; however, the variability in results raises questions about the consistency of performance across different applications.\n\n# Summary Of The Review\nOverall, the paper presents an interesting advancement in sequence modeling with the Toeplitz Neural Network, claiming efficiency improvements and broad applicability. However, the actual contributions and performance enhancements may not be as transformative as portrayed, and the framing of TNN as a universal solution could oversimplify the complexities involved in adapting different models.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for sequence modeling that addresses the complexity limitations of traditional transformer models. By leveraging a relative position encoded Toeplitz matrix, the TNN achieves logarithmic space-time complexity, allowing it to effectively handle long input sequences of up to 20K tokens during inference. Experimental results demonstrate that the TNN outperforms several existing models in autoregressive and bidirectional language modeling tasks, as well as on the Long-Range Arena benchmark, while also showcasing superior training speeds and competitive performance in image classification tasks.\n\n# Strength And Weaknesses\nThe TNN's primary strength lies in its ability to reduce computational complexity while maintaining high performance across a variety of tasks, making it a promising alternative to traditional transformers and efficient models. The empirical results indicate significant improvements in perplexity and accuracy metrics compared to several baselines, particularly in long-range context tasks. However, the paper could benefit from further exploration of the limitations of TNN in certain contexts or tasks not covered in the current evaluation, as well as a more in-depth analysis of the impact of the relative position encoder.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible for readers familiar with sequence modeling. The quality of the experiments is high, with comprehensive validation across multiple benchmarks. The novelty of the Toeplitz matrix approach is significant, offering a fresh perspective on reducing complexity in sequence models. The reproducibility of the results is likely supported by the detailed reporting of experimental settings and metrics, although providing code or models would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents the TNN as an innovative and effective solution for sequence modeling that addresses key challenges faced by existing transformer architectures. Its strong empirical performance across various tasks underscores its potential significance in the field. However, additional exploration of its limitations would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture for sequence modeling that emphasizes the importance of relative positional information while claiming to operate without traditional attention mechanisms. The authors argue that TNN can effectively manage varying sequence lengths, extrapolate to significantly longer sequences without retraining, and maintain a fixed budget of parameters for its relative position encoder. The experimental results presented suggest that TNN outperforms existing models across multiple benchmarks, although the paper does not thoroughly evaluate the limitations of these benchmarks or the generalizability of the findings.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to sequence modeling through the use of relative positional information, which could simplify models and reduce computational complexity. However, there are notable weaknesses, including the assumption that positional relationships alone can encapsulate the necessary contextual information, potentially ignoring content-based relationships that are critical in various tasks. The reliance on a fixed parameter budget raises concerns about expressiveness and performance trade-offs, while the lack of a comprehensive comparison with existing architectures may mislead readers regarding the universal applicability of TNN.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas with a reasonable level of clarity, though some concepts, such as the exponential decay bias and the implications of a \"fixed budget of parameters,\" are not sufficiently detailed or justified. The novelty of the TNN architecture is evident, but the assumptions underpinning its effectiveness warrant further scrutiny. Reproducibility is a concern, as the paper does not provide extensive details regarding the experimental setup or the diversity of datasets used, which could affect the reliability of the findings.\n\n# Summary Of The Review\nOverall, the paper proposes a promising new approach to sequence modeling with the Toeplitz Neural Network, but it is built on several assumptions that require more rigorous examination. The findings, while potentially impactful, may not generalize well across different tasks or datasets due to the limitations discussed.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for sequence modeling that aims to address the inefficiencies of transformer models, particularly their quadratic attention complexity. The TNN employs a relative position encoded Toeplitz matrix to reduce computational complexity to log-linear, allowing it to effectively manage longer sequences. The authors validate the model through extensive experimentation, demonstrating that TNN outperforms traditional approaches in various tasks such as autoregressive language modeling and image classification, while also showcasing superior extrapolation capabilities on longer sequences.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to sequence modeling, leveraging Toeplitz matrices and relative positional relationships to optimize performance and efficiency. This novel methodology simplifies the model's parameterization, independent of sequence length, which is a significant advantage over conventional transformer architectures. However, the paper could benefit from a more in-depth discussion of the limitations of the TNN, particularly in terms of potential trade-offs in model complexity or applicability to other domains beyond those tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and the underlying concepts of Toeplitz matrices and relative position embeddings. The quality of the experiments is high, and the results are presented in a manner that facilitates understanding. However, the reproducibility of the findings could be enhanced by providing more detailed information on the experimental setup and hyperparameter tuning. While the novelty of the approach is significant, further comparative analysis with existing state-of-the-art methods would strengthen the paper's claims.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to sequence modeling with the Toeplitz Neural Network, demonstrating significant improvements in efficiency and performance over traditional models. Although it exhibits strengths in methodology and results, the discussion of limitations and reproducibility aspects could be improved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at improving the efficiency and performance of machine learning algorithms in resource-constrained environments. The proposed method leverages a hybrid approach that combines traditional optimization techniques with modern deep learning architectures. The authors conduct extensive experiments across a range of benchmarks, demonstrating that their approach consistently outperforms existing methods in terms of speed and accuracy, particularly in scenarios with limited computational resources.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach**: The integration of classical optimization methods with deep learning represents a significant advancement in the field, potentially opening new avenues for research.\n2. **Thorough Experimental Validation**: The paper includes a comprehensive set of experiments that validate the effectiveness of the proposed method across various datasets and tasks, thereby establishing its credibility.\n3. **Real-World Applicability**: The focus on resource-constrained environments highlights the practical relevance of the research, making it valuable for applications in mobile and edge computing.\n\n**Weaknesses:**\n1. **Limited Baseline Comparisons**: While the results are impressive, the paper would benefit from including a wider variety of baseline methods to better contextualize the performance gains.\n2. **Potential Scalability Issues**: The scalability of the proposed method to larger datasets or more complex tasks is not adequately addressed, leaving questions about its practicality in diverse settings.\n3. **Insufficient Discussion on Limitations**: The authors do not sufficiently explore the limitations of their approach, which could provide a more balanced view of its applicability and potential challenges.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized, with a clear structure that effectively communicates the problem, methodology, and results. The writing is generally accessible, although some sections could benefit from additional explanations to enhance understanding for readers less familiar with the underlying concepts. The novelty of the approach is significant, and the methodology is described in enough detail to allow for reproducibility, although additional information on hyperparameter tuning and implementation specifics would further improve this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of machine learning, offering an innovative approach that demonstrates clear empirical advantages. Addressing the identified weaknesses, particularly regarding baseline comparisons and scalability, would further enhance the impact and applicability of the research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for efficient sequence modeling in natural language processing and computer vision. The TNN employs relative position encoded Toeplitz matrices, which significantly reduce the computational complexity associated with traditional transformer models from quadratic to log-linear. The authors present several key innovations, including a lightweight relative position encoder and an extrapolation capability that allows the model to handle sequences of varying lengths, demonstrating consistent performance on tasks such as autoregressive and bidirectional language modeling, image modeling, and the Long-Range Arena benchmark.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing the inefficiencies of existing models, particularly in handling long input sequences. The introduction of the relative position encoder is a significant advancement, allowing for flexibility and efficiency. Additionally, the performance results across multiple benchmarks indicate that the TNN not only meets but often exceeds the capabilities of contemporary models. However, a potential weakness is the lack of extensive empirical comparisons with a wider range of existing architectures beyond those mentioned, which could provide a more comprehensive understanding of its relative performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The innovations presented are novel and contribute significantly to the field of sequence modeling. The reproducibility of the results could be enhanced by providing more detailed implementation guidelines and sharing code or datasets used in the experiments, which would help others validate the findings.\n\n# Summary Of The Review\nOverall, the Toeplitz Neural Network presents a compelling and efficient solution to the challenges of sequence modeling, demonstrating notable improvements in performance and computational efficiency. While its contributions are significant and innovative, further empirical validation against a broader spectrum of existing models would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for sequence modeling that addresses the inefficiencies of traditional models, particularly transformers, when handling long sequences. By utilizing Toeplitz matrices, the TNN achieves a complexity reduction to O(nd log n) for token mixing operations. The proposed architecture incorporates a Relative Position Encoder (RPE) to efficiently manage parameters across varying sequence lengths and demonstrates the ability to extrapolate to 14K tokens during inference. Extensive experiments show that TNN outperforms existing models on various benchmarks while maintaining superior training efficiency and speed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to sequence modeling, which tackles the critical issue of computational complexity associated with long sequences. The introduction of the Gated Toeplitz Units (GTUs) and the RPE is a significant contribution that enhances the architecture's efficiency. The paper also presents a thorough empirical evaluation across multiple tasks, demonstrating the practical applicability of the proposed model. However, a potential weakness is the limited discussion on the theoretical underpinnings of the Toeplitz matrices in the context of neural networks, which could leave some readers questioning the foundational rationale behind certain design choices.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, with a clear structure that facilitates understanding of the proposed methodology and results. The authors provide sufficient details regarding their experiments, which enhances reproducibility. However, more explicit comparisons with existing models in terms of theoretical frameworks could improve clarity. The novelty of the approach is evident, particularly in how it leverages mathematical structures to enhance performance, making it a noteworthy contribution to the field.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in sequence modeling through the introduction of the Toeplitz Neural Network and the Relative Position Encoder. While the contributions are significant and well-supported by empirical results, the paper could benefit from a deeper theoretical exploration of its methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces Toeplitz Neural Networks (TNN) as a novel approach for sequence modeling, aiming to address inefficiencies associated with traditional transformer models. By leveraging Toeplitz matrices, the authors claim to enhance parameter efficiency and computational speed, achieving log-linear complexity. The methodology incorporates innovative techniques like a relative position encoder and exponential decay bias. Experimental results demonstrate that TNN outperforms existing models across various tasks, including autoregressive and bidirectional language modeling, as well as image modeling, achieving state-of-the-art results on multiple benchmarks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its significant contribution to improving the efficiency of sequence modeling through the use of Toeplitz matrices, which is a novel approach in this context. The experiments are comprehensive, covering a diverse range of tasks and providing thorough comparisons with established baselines. However, a potential weakness is the limited discussion on the practical implications of using TNN in real-world applications and the scalability of the model for extremely large datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it easy to follow the arguments and methodologies presented. The novelty of leveraging Toeplitz matrices for neural networks is a strong point, marking an original contribution to the field. The reproducibility of the results is supported by detailed mathematical notations and proofs included in the appendices, allowing other researchers to replicate the experiments effectively.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in sequence modeling by introducing Toeplitz Neural Networks, which improve computational efficiency and accuracy. While the methodology and results are robust, further exploration of real-world applicability would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the Toeplitz Neural Network (TNN) for sequence modeling, addressing the quadratic complexity of traditional transformer architectures in attention mechanisms. By employing Toeplitz matrices to encode relative positional information, the TNN reduces this complexity to \\(O(n \\log n)\\). The model features a lightweight Relative Position Encoder (RPE) that allows for flexibility in sequence lengths and demonstrates strong extrapolation capabilities, achieving competitive performance across various tasks, including autoregressive and bidirectional language models, image modeling, and the Long-Range Arena benchmark.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to reducing computational complexity through the use of Toeplitz matrices, which not only improve efficiency but also maintain performance on large sequence lengths. The empirical results are robust, showing that TNN outperforms existing models in several benchmarks. However, a potential weakness is the reliance on intricate mathematical formulations that may limit accessibility for practitioners unfamiliar with these concepts. Additionally, while the paper emphasizes efficiency, it does not extensively discuss the implications of the introduced complexity reduction on model interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents a coherent argument for the proposed methodology, with clear definitions of key concepts such as positional embeddings and Toeplitz matrices. The mathematical formulations are rigorous, although they may pose a barrier to understanding for some readers. The novelty is significant, as the TNN offers a fresh perspective on sequence modeling, particularly in its approach to positional encoding. The reproducibility of the results is bolstered by detailed descriptions of the model architecture and experimental setup, although access to code and datasets would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in sequence modeling through the introduction of TNN, effectively combining efficiency with competitive performance. While the mathematical rigor is commendable, it may hinder broader comprehension among practitioners. The contributions are noteworthy for both theoretical and empirical domains.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel architecture called the Toeplitz Neural Network (TNN), which aims to improve performance in sequence modeling tasks by leveraging the properties of Toeplitz matrices. The authors claim that TNN achieves reduced space-time complexity to log-linear and enhances extrapolation capabilities across various datasets. However, the paper lacks rigorous validation of these claims and fails to provide a comprehensive analysis of the architecture's limitations compared to existing methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical foundation and the introduction of the Toeplitz matrix, which has potential implications for efficient computation in sequence modeling. However, significant weaknesses include a lack of thorough examination of the architecture's limitations, superficial comparisons with other models, and an overemphasis on performance metrics without adequate discussion on edge cases or noisy data handling. Additionally, the architecture's complexity and reliance on relative position encoders introduce new challenges that are not clearly justified.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper includes an abundance of figures and tables, the clarity of the presentation suffers due to occasional ambiguities and potential misinterpretations of the data. The novelty of the TNN architecture is notable; however, the lack of detailed ablation studies and the limited scope of experiments raise concerns about reproducibility and generalizability. The authors do not sufficiently critique their assumptions regarding position information, further complicating the assessment of their contributions.\n\n# Summary Of The Review\nThe paper introduces an intriguing theoretical framework with the Toeplitz Neural Network but falls short in providing a balanced discussion of its limitations and practical implications. The lack of rigorous validation and detailed comparisons diminishes the overall impact of the work, resulting in an overly optimistic portrayal of the TNN's applicability in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents the Toeplitz Neural Network (TNN), which introduces an innovative approach to sequence modeling through the use of a relative position encoded Toeplitz matrix. This method significantly reduces the computational complexity from quadratic to log-linear, enabling the model to process longer input sequences efficiently, up to 14K tokens during inference. The TNN showcases state-of-the-art performance across various tasks, including autoregressive and bidirectional language modeling and image modeling, while maintaining a lightweight architecture and exceptional training speed. Furthermore, the TNN serves as a generalized framework that integrates concepts from transformers, CNNs, and state-space methods, making it applicable across multiple domains.\n\n# Strength And Weaknesses\nStrengths of the paper include its groundbreaking efficiency and scalability, which allow for effective processing of long sequences without compromising performance. The unified framework of the TNN enhances its applicability across different domains, and the open-source availability encourages community collaboration. However, a potential weakness lies in the need for thorough validation of the model's performance across diverse datasets and tasks to ensure robustness, as the paper primarily highlights its successes without extensive discussion of limitations or failure cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers. The novelty of the TNN is evident in its innovative use of Toeplitz matrices for sequence modeling, and the empirical results demonstrate its significant advantages over existing models. The open-source code provided enhances reproducibility and allows other researchers to build upon this work. However, additional details on implementation specifics and parameter tuning could further enhance reproducibility.\n\n# Summary Of The Review\nThe Toeplitz Neural Network represents a significant advancement in sequence modeling, delivering remarkable efficiency and scalability while achieving state-of-the-art performance across various applications. Its innovative framework and open-source availability position it as a valuable contribution to the field, although further validation across diverse datasets would strengthen its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the Toeplitz Neural Network (TNN), a novel framework for sequence modeling that addresses the computational inefficiencies associated with traditional transformer architectures. By employing Toeplitz matrices, the TNN offers a compact representation of positional relations, enabling a significant reduction in computational complexity from O(n²) to O(nd log n). The framework also introduces a relative position encoder that enhances the model's ability to handle varying input lengths without retraining, demonstrating impressive extrapolation capabilities on longer sequences.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to reducing the complexity of sequence modeling, as well as its theoretical rigor in deriving the benefits of using Toeplitz matrices. The ability of TNN to generalize across drastically increased sequence lengths is particularly noteworthy. However, weaknesses include a lack of extensive empirical validation compared to existing models and potential limitations in demonstrating practical applicability across diverse tasks beyond those explored in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical foundations and contributions. The quality of the writing is high, with sufficient detail provided to understand the methodology. The novelty of the approach is significant, as it offers an alternative to the widely used transformer architectures without the associated computational bottlenecks. Reproducibility is supported through the clear exposition of the TNN framework, though empirical results could enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling new framework for sequence modeling that theoretically improves upon existing methods. While the TNN shows promise in its computational efficiency and extrapolation capabilities, further empirical validation is necessary to fully establish its effectiveness across a broader range of applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Toeplitz Neural Network for Sequence Modeling\" introduces a novel architecture known as the Toeplitz Neural Network (TNN), which aims to enhance sequence modeling efficiency. The TNN utilizes a relative position encoded Toeplitz matrix, significantly reducing the space-time complexity from quadratic to log-linear. Key components include Gated Toeplitz Units (GTU) and a lightweight sub-network called the relative position encoder (RPE), which generates relative position coefficients. Experiments demonstrate the model's capability to handle input sequences of up to 14K tokens during inference, despite being trained on 512-token sequences, showcasing its performance across various tasks including autoregressive and bidirectional language modeling.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to reducing complexity in sequence modeling through the use of Toeplitz matrices, which offers a promising alternative to existing models like Transformers. The empirical results are robust, showing superior performance across multiple benchmarks, indicating the effectiveness of the proposed architecture. However, the paper lacks a comprehensive discussion on the broader implications of the TNN beyond its implementation, which limits its contextual significance. Additionally, while the experimental setup is well-structured, the reliance on specific hyper-parameter settings may affect the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with a clear presentation of its methodology and findings. The use of visualizations to illustrate the interaction between tokens within the Toeplitz matrices adds to the clarity and understanding of the proposed model. Code availability on GitHub enhances reproducibility. However, the novelty, while significant in the context of sequence modeling, may not be fully articulated in terms of its broader impact on existing methodologies and future research directions.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in sequence modeling through the introduction of the Toeplitz Neural Network, demonstrating both methodological innovation and strong empirical performance. While it excels in clarity and reproducibility, it could benefit from a deeper exploration of the implications of its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Toeplitz Neural Network (TNN), which claims to outperform transformer-based models, particularly in tasks involving long-range dependencies and image classification. The authors argue that TNN is more efficient than traditional attention mechanisms and can generalize various architectures, including CNNs and state-space models. Significant findings include TNN's superior performance on the Long-Range Arena benchmark and its ability to handle longer sequences without retraining. Additionally, the paper introduces relative position encoders as a novel aspect of TNN.\n\n# Strength And Weaknesses\nWhile the paper claims TNN surpasses existing models, competitive performance against well-established benchmarks raises questions about the significance of its improvements. TNN’s efficiency claims are challenged by the advancements made in linear attention models, which have demonstrated reduced complexity without loss of performance. The comparison of TNN with MLP-based methods appears less relevant given MLPs' limitations in capturing long-range dependencies. Furthermore, although TNN aims to generalize various architectures, it risks diluting the unique strengths of these models. The introduction of relative position encoders lacks novelty, as similar approaches have been explored in prior works.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, though it tends to emphasize TNN's strengths while downplaying the achievements of existing models, which may lead to a biased interpretation of its contributions. The quality of the methodology is sound; however, the claims of novelty do not hold up against established models that have successfully tackled similar challenges. Reproducibility could be a concern, given the lack of comprehensive benchmarks against which TNN is compared.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach with the Toeplitz Neural Network, but its contributions to the field appear limited when considering existing models that address similar challenges more effectively. The claims of superiority and novelty are undermined by prior work, making it difficult to justify TNN's significance in the current landscape of neural architectures.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to sequence modeling using Toeplitz Neural Networks (TNNs), which significantly reduce the quadratic space-time complexity associated with traditional attention mechanisms. The authors propose a method that leverages Toeplitz matrix-vector products to enhance computational efficiency while maintaining performance. Empirical results demonstrate that TNNs achieve competitive performance on benchmark tasks, indicating their potential as an effective alternative to existing models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative methodology that addresses a critical limitation of current models by reducing computational complexity. The empirical results validate the proposed approach, showcasing its effectiveness in various sequence modeling tasks. However, the paper has weaknesses, particularly in clarity and organization. Several sections contain inconsistent notation and terminology, which may hinder reader comprehension. Additionally, the lack of detailed explanations for certain terms and concepts could limit accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept with potential significance, its clarity suffers from inconsistent formatting, notation, and terminology. A more coherent structure and clearer definitions would enhance reader understanding. The quality of the empirical results is strong, but reproducibility could be affected due to insufficient detail in methodological descriptions. Overall, while the novelty of the approach is commendable, the presentation requires refinement for better clarity and reproducibility.\n\n# Summary Of The Review\nThe paper offers a promising contribution to sequence modeling through the introduction of Toeplitz Neural Networks, effectively addressing computational challenges faced by traditional models. However, issues with clarity and consistency in presentation detract from its overall impact. With revisions focused on enhancing readability and reproducibility, this work could significantly influence future research in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to sequence modeling through the introduction of Toeplitz neural networks. It primarily emphasizes improvements in efficiency and performance on the Long-Range Arena benchmark, showcasing the architecture's ability to enhance training and inference speed. The authors also introduce a relative position encoder to capture positional relationships within sequences, but the paper does not extensively explore its implications beyond the benchmark context.\n\n# Strength And Weaknesses\nStrengths of the paper include its demonstrated efficiency gains on specific benchmarks, as well as the introduction of the relative position encoder, which shows promise for improving model performance. However, the paper exhibits notable weaknesses, such as a lack of exploration into real-world applications, multimodal task adaptation, and potential limitations of the Toeplitz structure in dynamic environments. Additionally, the paper does not address ethical implications associated with deploying such models in sensitive applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly. However, the novelty is somewhat limited by the narrow focus on benchmark comparisons without a broader exploration of practical applications. The reproducibility of the results may be hindered by the lack of comprehensive comparisons with other emerging architectures, particularly those suited for low-resource environments or edge devices.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of sequence modeling through the introduction of Toeplitz neural networks. While it presents promising results on benchmark tasks, it would benefit from a more extensive discussion of real-world applications, limitations, and ethical considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture designed for efficient sequence modeling that addresses the high space-time complexity of traditional transformer models. The authors validate the performance of TNN through extensive experiments against various baseline models using metrics such as Perplexity (PPL) for language modeling and accuracy for classification tasks. The findings indicate that TNN achieves superior performance, particularly in long-sequence tasks, though the paper lacks detailed statistical significance metrics such as p-values or confidence intervals.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to sequence modeling, offering a potentially more efficient alternative to existing models. The use of ablation studies effectively highlights the impact of different architectural choices on performance. However, a significant weakness is the lack of rigorous statistical testing to substantiate the claimed performance improvements, which could raise concerns regarding the reliability of the results. Additionally, while the experimental setup is well-controlled, the absence of explicit statistical significance metrics diminishes the robustness of the reported findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow from introduction to conclusion. The quality of the experiments appears high, and the authors provide sufficient detail for reproducibility regarding training configurations. However, the novelty of the TNN architecture, while promising, is somewhat undermined by the absence of comprehensive statistical validation. The potential for reproduction is good, but the lack of explicit significance metrics could hinder confidence in the results.\n\n# Summary Of The Review\nThe Toeplitz Neural Network presents an intriguing advancement in sequence modeling, demonstrating superior performance over traditional models. However, the lack of rigorous statistical significance testing raises questions about the reliability of the reported improvements. Overall, while the contributions are notable, further validation is necessary to enhance the robustness of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel model leveraging a Toeplitz matrix structure for encoding positional information in sequence data. The methodology emphasizes a relative position encoder that aims to enhance the model's extrapolation capabilities for long sequences. Key findings indicate that the model demonstrates promising performance in tasks such as language and image modeling, particularly for sequences up to 14K tokens.\n\n# Strength And Weaknesses\nThe paper's primary strengths lie in its innovative approach to handling positional information and its demonstrated capabilities for extrapolation in long sequences. However, it has notable weaknesses, including a lack of investigation into the limitations of the Toeplitz matrix structure regarding complex token interactions. Furthermore, the reliance on relative positional encoding may hinder its adaptability to tasks needing more nuanced, content-based relationships. The absence of performance validation on sequences longer than 14K tokens raises concerns about scalability. Additionally, the lack of comprehensive comparisons with recent state-of-the-art methods limits the paper's contextual relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of its methodology and findings. However, it lacks thorough discussions on critical aspects such as the impact of decay rates and hyperparameter tuning on model performance, which could affect reproducibility. The novelty of the approach is commendable, but the limited exploration of its generalizability and performance degradation with longer sequences detracts from its overall quality.\n\n# Summary Of The Review\nOverall, while the paper presents a novel approach to positional encoding with encouraging results, it is hampered by significant limitations regarding its adaptability, scalability, and failure to provide comprehensive comparisons with existing models. Future work should clearly outline directions to address these shortcomings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Toeplitz Neural Network for Sequence Modeling\" proposes a novel approach for sequence modeling by utilizing a Toeplitz matrix to capture relative positional information, thus addressing the quadratic complexity associated with transformer models. The authors claim that their method reduces computational complexity to log-linear while demonstrating the ability to handle longer sequences during inference, up to 14K tokens. Experimental results showcase state-of-the-art performance on various benchmarks, with comparisons to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its efficiency claims, particularly the reduction of complexity for long sequences, which could be significant for practical applications. However, the methodology appears to lack originality, relying heavily on established concepts in sequence modeling without demonstrating substantial innovation. The experimental validation, while showing competitive results, seems to overly emphasize certain benchmarks, raising concerns about the robustness of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is inconsistent, with some sections using unnecessarily convoluted terminology that detracts from understanding the core ideas. The overall quality of the paper suffers from a lack of depth in discussing the novelty of the Toeplitz matrix application, making it challenging to ascertain its significance in advancing the field. Reproducibility is not explicitly addressed, and without a clear roadmap for implementing their approach, it may hinder other researchers from validating the findings.\n\n# Summary Of The Review\nOverall, the paper presents a method that reduces computational complexity for sequence modeling, which is commendable; however, it lacks substantial novelty and depth in its contributions. The work appears more like a rebranding of existing ideas rather than a breakthrough in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), a novel architecture for sequence modeling that leverages relative positional information to enhance performance while maintaining a reduced computational complexity of O(n log n). The authors demonstrate the TNN's capabilities through experiments focused on language modeling and image classification, highlighting its parameter efficiency achieved via Toeplitz matrices. Additionally, the paper presents the innovative concept of exponential decay bias to improve extrapolation to longer sequences.\n\n# Strength And Weaknesses\nThe TNN offers significant contributions, particularly in its parameter efficiency and computational complexity, which represent advancements over traditional sequence models. However, the paper lacks a comprehensive comparative analysis with other relevant architectures, such as graph neural networks and convolutional recurrent networks, which could provide insights into the relative strengths and weaknesses of TNN. The absence of a detailed ablation study and exploration of applications beyond language and vision diminishes the potential impact of the findings. Future work could benefit from integrating TNN with existing frameworks and investigating hybrid models to enhance its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology and findings clearly. However, the novelty of the TNN could be further contextualized through comparisons with state-of-the-art methods in sequence modeling. The reproducibility of the results would likely be strengthened by providing more details on the implementation and experimental setup, including datasets and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the TNN presents a promising approach to sequence modeling with notable strengths in efficiency and parameter reduction. However, the lack of comprehensive comparisons with other architectures and limited exploration of applications may impede its broader acceptance within the research community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Toeplitz Neural Network (TNN), which demonstrates superior performance in autoregressive language modeling compared to traditional transformer models and efficient alternatives. TNN achieves a perplexity of 23.98 on the WikiText-103 validation set, outperforming strong baselines like the Transformer and other efficient models. Additionally, TNN excels in bidirectional language modeling on the GLUE benchmark, particularly enhancing performance on the CoLA task, and achieves remarkable scores on the Long-Range Arena benchmark. The model showcases robust extrapolation capabilities with consistent performance at input sequence lengths of up to 14K tokens and exhibits competitive training speeds. Beyond language tasks, TNN also performs comparably to DeiT models in image classification on the ImageNet-1K dataset, indicating its versatility and efficiency.\n\n# Strength And Weaknesses\nThe TNN presents several strengths, including its state-of-the-art performance across multiple benchmarks, efficiency in training, and versatility in handling both language and image classification tasks. Its ability to outperform established methods like S4 and other attention-based models in long-range tasks is particularly noteworthy. However, the paper could benefit from a more detailed explanation of the underlying methodology and the specific architectural innovations that enable TNN's performance. Additionally, comparisons with other state-of-the-art methods in broader contexts could enhance the evaluation of its significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its findings clearly, with appropriate metrics and benchmarks for evaluation. The quality of the results is high, demonstrating comprehensive testing across various tasks. However, while the performance metrics are compelling, the novelty of the underlying architecture could be better articulated, as the paper does not delve deeply into the theoretical implications of the Toeplitz structure. Reproducibility is supported by the reported performance metrics, but additional details on implementation specifics would strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field with the introduction of the Toeplitz Neural Network, showcasing impressive results across multiple benchmarks. While the findings are compelling and demonstrate both performance and efficiency, the paper could improve its clarity regarding the architectural innovations and methodological details.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Toeplitz Neural Network for Sequence Modeling\" presents a novel neural network architecture that leverages Toeplitz matrices to enhance sequence modeling tasks. The authors propose a methodology that integrates relative position encoding with Toeplitz structures, aiming to improve both efficiency and performance in sequential data processing. The findings indicate that the proposed Toeplitz Neural Network (TNN) outperforms traditional sequence models in terms of training speed and accuracy across various benchmarks.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative approach to utilizing Toeplitz matrices, which can significantly reduce computational complexity while maintaining high performance. The integration of relative position encoders is a notable contribution that enhances the model's ability to capture dependencies in sequential data. However, the paper suffers from clarity issues, particularly in the presentation of its methodology and results. The abstract is overly lengthy, and the introduction is dense, making it challenging to grasp the key contributions quickly. Furthermore, several figures lack detailed captions, which diminishes their interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the technical content is sound, the paper's clarity is compromised by inconsistent terminology and lengthy paragraphs. The mathematical notation could be presented more clearly, and the use of acronyms should be defined upon first mention. The overall quality of the writing is affected by typographical errors and awkward phrasing, which detract from the professionalism of the document. Although the proposed ideas are novel, the reproducibility of the results may be hindered by the insufficient clarity in methodology and data presentation.\n\n# Summary Of The Review\nThe paper introduces a promising approach to sequence modeling through the use of Toeplitz matrices, showcasing potential improvements in efficiency and accuracy. However, issues related to clarity and presentation hinder the overall impact of the work. A more concise and organized structure would significantly enhance readability and comprehension.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.070339856463892,
    -1.645284485887619,
    -1.8922661000360135,
    -1.8351576096119488,
    -1.7740771841366292,
    -1.7408523381075387,
    -1.4458461436468877,
    -1.9361300833846171,
    -1.7032837193511277,
    -1.8430792075712974,
    -1.5730692815349931,
    -1.2678013020512129,
    -1.6410697431887775,
    -1.6591210435686554,
    -1.6335346647590234,
    -1.6925772506341221,
    -1.9150479084501695,
    -1.6768891996904671,
    -1.6741023851613623,
    -1.7491954000111263,
    -1.9155967680129313,
    -1.604227454994655,
    -1.875561140488509,
    -1.6554600353851836,
    -1.935842530884154,
    -1.9561032788955788,
    -1.8927841632120694,
    -1.7844385651794548,
    -1.6694196962565817
  ],
  "logp_cond": [
    [
      0.0,
      -1.8492504106921657,
      -1.855899297394896,
      -1.85621369308147,
      -1.8601048001272509,
      -1.8505884607095788,
      -1.8789811961704652,
      -1.8613996102991552,
      -1.8750702250798341,
      -1.8609958806407605,
      -1.86921161844379,
      -1.9029941674639865,
      -1.847331557442664,
      -1.8459576758608391,
      -1.8577849481353,
      -1.8556250948791637,
      -1.8650274099726056,
      -1.8703983310380832,
      -1.8839920940896375,
      -1.819690976675974,
      -1.8633812140443906,
      -1.8731773951150785,
      -1.853286109974614,
      -1.8730404689514137,
      -1.8711492118452642,
      -1.8719736228356227,
      -1.857567997000223,
      -1.8643122021610892,
      -1.8634829763039222
    ],
    [
      -1.358033800297574,
      0.0,
      -1.1869244706989313,
      -1.2225820389409323,
      -1.182465562017906,
      -1.212463470784297,
      -1.3327285014234027,
      -1.2336101120065577,
      -1.1698681153879953,
      -1.2477227208502204,
      -1.1500839235305973,
      -1.3687099110241894,
      -1.1617367797472806,
      -1.2084005010403243,
      -1.2306208525904776,
      -1.2120253972493478,
      -1.1822265902441709,
      -1.1721972958182454,
      -1.2786926123094209,
      -1.0992040476676848,
      -1.2668405211967886,
      -1.2909086075359946,
      -1.2503905281920011,
      -1.2452562157180476,
      -1.2406312895937723,
      -1.2068382348146716,
      -1.231177005850369,
      -1.163125235646619,
      -1.2662119774006901
    ],
    [
      -1.5708584616494448,
      -1.4769294586108745,
      0.0,
      -1.4711001187253234,
      -1.4339408361896628,
      -1.4418358495824382,
      -1.5652807144837246,
      -1.5147688409501028,
      -1.4675849687786466,
      -1.406169108250957,
      -1.4546553013347172,
      -1.6316097224852784,
      -1.4905485961152318,
      -1.4493226881292185,
      -1.5049103713922631,
      -1.4724117509591916,
      -1.513638288159343,
      -1.5187757311156558,
      -1.5106094202225,
      -1.4220171880976966,
      -1.4829679442876933,
      -1.5825217275784946,
      -1.5382337822374106,
      -1.472173037910835,
      -1.4932100808952735,
      -1.4937007345908153,
      -1.4774352818594625,
      -1.5059236772019,
      -1.553449883038412
    ],
    [
      -1.508822957297021,
      -1.42175605736461,
      -1.3541639737267155,
      0.0,
      -1.4109064425017261,
      -1.4124381036253464,
      -1.502452535206225,
      -1.4023463749410792,
      -1.4118171436823175,
      -1.4059977960458736,
      -1.3935574693598423,
      -1.5617213746547898,
      -1.3750333852382275,
      -1.3285607109245292,
      -1.3997024314201918,
      -1.4153231490862344,
      -1.4072486628706706,
      -1.404359548296105,
      -1.4779890535658895,
      -1.3141534685844904,
      -1.4703955441260892,
      -1.475457566734909,
      -1.479801461350207,
      -1.4581399606438616,
      -1.4846941052700284,
      -1.4274456672747693,
      -1.439471122334165,
      -1.4678530362885318,
      -1.4104414898556372
    ],
    [
      -1.5589817570438878,
      -1.4461380686210024,
      -1.487144032357519,
      -1.48712853476746,
      0.0,
      -1.4850413138819794,
      -1.5203833611188542,
      -1.443400131533901,
      -1.4887243891632096,
      -1.3943042077350447,
      -1.4893719175813838,
      -1.547582171904921,
      -1.4535579509866754,
      -1.4580727826299362,
      -1.4770517501918516,
      -1.4743861320037335,
      -1.4821778512464254,
      -1.4225371503334572,
      -1.5125073261548543,
      -1.4451593873842496,
      -1.4655231077437396,
      -1.522843790619124,
      -1.4971283767637493,
      -1.4903357677670321,
      -1.4823074871259942,
      -1.4606976634566196,
      -1.486811046619662,
      -1.4884789918732508,
      -1.520686045228735
    ],
    [
      -1.3702450099997472,
      -1.2742068583751058,
      -1.2162198044878827,
      -1.2947172195956256,
      -1.2834345108719123,
      0.0,
      -1.408641942676015,
      -1.3083874599499592,
      -1.3088936119285088,
      -1.2626826828098807,
      -1.2401914213394414,
      -1.4354273721831277,
      -1.276807273724807,
      -1.2664541965453928,
      -1.3127979754853158,
      -1.2917435523367602,
      -1.2825173757716821,
      -1.3170799628258927,
      -1.228957929850057,
      -1.2793220763696818,
      -1.3072558484616557,
      -1.3746879645623096,
      -1.3629424085685233,
      -1.3112517760087485,
      -1.33991780011753,
      -1.2929291564222012,
      -1.2747236923711973,
      -1.3043321609057712,
      -1.3015469347099378
    ],
    [
      -1.1663318991456009,
      -1.1123990159432464,
      -1.0909300838580729,
      -1.1168398286426928,
      -1.1159393198023821,
      -1.0856028538203795,
      0.0,
      -1.0709410720304613,
      -1.1275027815817147,
      -1.1394611989320311,
      -1.0933722498011988,
      -1.1786415900917566,
      -1.129019693876245,
      -1.0707087278700185,
      -1.1233106131152222,
      -1.1456253787195196,
      -1.1303316529972685,
      -1.1304995704767593,
      -1.1183444123847461,
      -1.0960658005977957,
      -1.1354934348192345,
      -1.1808690808525808,
      -1.156308392112793,
      -1.146157847936988,
      -1.128191440320815,
      -1.1236324202850319,
      -1.130898161138889,
      -1.1353590731259098,
      -1.1633485629767477
    ],
    [
      -1.716344736945749,
      -1.5829423458579042,
      -1.5959427013640108,
      -1.561231019329884,
      -1.562591539687904,
      -1.6265879235003602,
      -1.6439483878648824,
      0.0,
      -1.5547142239646112,
      -1.6198020570223184,
      -1.6167543958734114,
      -1.676810924320318,
      -1.552444333669354,
      -1.5242851274412959,
      -1.579167605971352,
      -1.6022467064026917,
      -1.5792895361369246,
      -1.5998654681206181,
      -1.604274047625393,
      -1.4817277223407845,
      -1.5884729029307267,
      -1.638259747773408,
      -1.6010614967567152,
      -1.5892280525775109,
      -1.58740683145505,
      -1.5659237064483547,
      -1.642334602885143,
      -1.562527965155664,
      -1.6298985218893018
    ],
    [
      -1.4057682918504013,
      -1.2328637337257735,
      -1.2508517020116587,
      -1.2415514869578335,
      -1.2632471894517312,
      -1.265183912538737,
      -1.390734926553939,
      -1.2817026078504932,
      0.0,
      -1.3214230393776767,
      -1.222945627871539,
      -1.4608403489617197,
      -1.2271415984460015,
      -1.2283479727566826,
      -1.2850314469916064,
      -1.2587703611515413,
      -1.2976539257961763,
      -1.2525690735472137,
      -1.3187886223536742,
      -1.1741768923729463,
      -1.2979097229396643,
      -1.3275588579923214,
      -1.3390880285810547,
      -1.2849475011787537,
      -1.355629609795858,
      -1.2784245127561211,
      -1.3072760083226906,
      -1.1784172087775822,
      -1.3128043990017768
    ],
    [
      -1.5453535317598504,
      -1.5045617321942324,
      -1.4096423209903137,
      -1.4724230306074346,
      -1.43736470647832,
      -1.4788418549893083,
      -1.6198780161825816,
      -1.5022151197063447,
      -1.521166749053819,
      0.0,
      -1.4992215723111226,
      -1.641390625350618,
      -1.5015537814604287,
      -1.5090393635059172,
      -1.4774387541760114,
      -1.4837730802877305,
      -1.497124825264792,
      -1.5353523965203693,
      -1.5544461989097418,
      -1.4888644024686062,
      -1.4850387277976824,
      -1.5377854884573146,
      -1.5269713716846423,
      -1.5262233664546214,
      -1.5293893446334121,
      -1.5281449783246737,
      -1.4247827938074307,
      -1.5214541468957254,
      -1.5517993999142385
    ],
    [
      -1.2567308398506012,
      -1.0922161041231209,
      -1.1310138727107326,
      -1.1459238349896952,
      -1.152367722124614,
      -1.1227171757028527,
      -1.244408517956854,
      -1.1718958278484535,
      -1.1497432188256849,
      -1.1671943573393482,
      0.0,
      -1.3127321110791237,
      -1.1374842124246705,
      -1.167389164090866,
      -1.160988385740701,
      -1.1422489857380536,
      -1.141021895706164,
      -1.1443956603018803,
      -1.2007730340276637,
      -1.1472273256896606,
      -1.1760532659122638,
      -1.2284411806185853,
      -1.221506121433166,
      -1.1758630558691667,
      -1.2379029430594934,
      -1.1380428591876037,
      -1.1342286285185454,
      -1.1800789054685192,
      -1.1701752678034478
    ],
    [
      -1.072743258143703,
      -1.0201161717445344,
      -1.0601429497519383,
      -1.0460248015260345,
      -1.0144068835127906,
      -1.0315399928820421,
      -1.0353700727017845,
      -1.0097992988497475,
      -1.0532048902032511,
      -1.043485343251116,
      -1.0379742618430488,
      0.0,
      -1.0316107227999394,
      -1.0341409587351444,
      -1.0413444222171466,
      -1.0534469746401787,
      -1.0394173322533033,
      -1.067804210017648,
      -1.0426001210683145,
      -1.0399184555059204,
      -1.056922258304672,
      -1.0463759365607006,
      -1.0213242195884262,
      -1.031406059385111,
      -0.9881374477563429,
      -1.018072915706712,
      -1.0230628918824825,
      -1.044355866558225,
      -1.0135122824373293
    ],
    [
      -1.3296761114512392,
      -1.0936768248659945,
      -1.1547083586049882,
      -1.112873101110105,
      -1.0990661392894492,
      -1.1471167789782826,
      -1.298248034979828,
      -1.155266197221241,
      -1.1228930404726254,
      -1.2369824781779275,
      -1.1394888574759816,
      -1.3687822891606058,
      0.0,
      -1.1874029775879404,
      -1.1788704092499855,
      -1.0582775473691193,
      -1.228689775775612,
      -1.0982395893653252,
      -1.193451548141445,
      -1.1030175743703532,
      -1.2157062839560504,
      -1.2950781863526417,
      -1.2511884975198004,
      -1.2604541675992336,
      -1.254485644922956,
      -1.1773856451139282,
      -1.2264468895172045,
      -1.1737224890568898,
      -1.2351977013498467
    ],
    [
      -1.3688752474112271,
      -1.3177454381177343,
      -1.2414398156668716,
      -1.2262906702186862,
      -1.2883143526913152,
      -1.2888398851325895,
      -1.3527159085156848,
      -1.2537731527842004,
      -1.3158454269359983,
      -1.2919454314920498,
      -1.3221925890024786,
      -1.3656095072252272,
      -1.3034699750569778,
      0.0,
      -1.3333901411221576,
      -1.3058333431931226,
      -1.325426596296328,
      -1.3075240130427987,
      -1.2643973014906429,
      -1.209435513908814,
      -1.3583200275122929,
      -1.3965607863989458,
      -1.3312482294215704,
      -1.3395987580614899,
      -1.286191869676488,
      -1.2741869896036013,
      -1.3418984641788936,
      -1.3447664434760815,
      -1.3171494144737916
    ],
    [
      -1.312255339676284,
      -1.2017055349693735,
      -1.1843854379686867,
      -1.156240885734421,
      -1.1725793278976941,
      -1.200654034805428,
      -1.2599824228313536,
      -1.1830266979515318,
      -1.1920949947736479,
      -1.2214513234021167,
      -1.1857448513593603,
      -1.3227412939505068,
      -1.1316063962017577,
      -1.2043751581908222,
      0.0,
      -1.1977887431881575,
      -1.1986657590179404,
      -1.1886534587130897,
      -1.2345678162103326,
      -1.188861983797614,
      -1.2560039400321883,
      -1.277365686152718,
      -1.2168443247960397,
      -1.2262361158970154,
      -1.2599055511873622,
      -1.1620153978229104,
      -1.1696725385896585,
      -1.234156362672429,
      -1.2028497094075943
    ],
    [
      -1.3870968296413324,
      -1.2905091244233047,
      -1.268207106831113,
      -1.271294097427532,
      -1.2428438184009254,
      -1.2630794428257068,
      -1.3749298056229775,
      -1.3294060903284415,
      -1.2999130956704272,
      -1.257032763473854,
      -1.30366698029273,
      -1.4115465149141926,
      -1.2347210533989323,
      -1.30027924618923,
      -1.292923854733293,
      0.0,
      -1.3008578870078666,
      -1.3592489663383776,
      -1.2740664717883075,
      -1.2863028030238137,
      -1.3466733891152673,
      -1.3320903358049947,
      -1.3364649123464831,
      -1.373696355203323,
      -1.3160748167106726,
      -1.303513133782381,
      -1.3265874492445804,
      -1.2927980616781796,
      -1.3250864364558628
    ],
    [
      -1.6433714228068739,
      -1.4750009249597602,
      -1.4632238208451334,
      -1.4796404541914605,
      -1.4975523534723887,
      -1.4980144163068347,
      -1.6241361312294782,
      -1.4789948658015633,
      -1.492819637069579,
      -1.47456527983426,
      -1.5195868675093083,
      -1.6659094290476815,
      -1.4962748612786796,
      -1.5357753568629822,
      -1.5075073150881042,
      -1.518618297433041,
      0.0,
      -1.5201948608652718,
      -1.5692547614103496,
      -1.4699725471209193,
      -1.514720060589754,
      -1.55708468087131,
      -1.4815384324657626,
      -1.5458570520331487,
      -1.5450036619641363,
      -1.5304731954299855,
      -1.5128635698194366,
      -1.5614202251172784,
      -1.5104035655444572
    ],
    [
      -1.3770552172835904,
      -1.1924835816820993,
      -1.2224098721400654,
      -1.1912290454511487,
      -1.18642852690185,
      -1.2714249124938595,
      -1.3335221985738444,
      -1.2209064428051222,
      -1.181792226428788,
      -1.2935115408804445,
      -1.2239478105362402,
      -1.41483397563011,
      -1.164848843184156,
      -1.2223488481613527,
      -1.2401331411878074,
      -1.2636188452161359,
      -1.2927940316788067,
      0.0,
      -1.2997317894626106,
      -1.127085505513063,
      -1.2655936060598016,
      -1.3288851486920243,
      -1.310872020823723,
      -1.2880512978045466,
      -1.2860866768004875,
      -1.1855883190716832,
      -1.280908823183491,
      -1.2708820739126012,
      -1.31180421902395
    ],
    [
      -1.4004213171760977,
      -1.3425870969575255,
      -1.3253786818302826,
      -1.3979932965011317,
      -1.3258080440784077,
      -1.3060938974938556,
      -1.3882051491295546,
      -1.268735387106771,
      -1.378499269386753,
      -1.3190600090089608,
      -1.3325171707225663,
      -1.4326262540423003,
      -1.3483694252492628,
      -1.2543331632580714,
      -1.367063399379404,
      -1.322500571484418,
      -1.3137546183757196,
      -1.3966461273142203,
      0.0,
      -1.3501560733081652,
      -1.3611965006856803,
      -1.4076734552523287,
      -1.400405372365963,
      -1.4177455351648893,
      -1.2923444389710872,
      -1.3193027204792953,
      -1.3446275700921526,
      -1.3793244391690802,
      -1.3788941014871328
    ],
    [
      -1.4256436328293196,
      -1.2320414337033083,
      -1.279681111732478,
      -1.2405384577818566,
      -1.2726224401597845,
      -1.35264359221028,
      -1.4327038397557585,
      -1.2661790505161594,
      -1.2644143382781146,
      -1.360701321725501,
      -1.3366857646124493,
      -1.4907885702486274,
      -1.3034899174512993,
      -1.2802524944765956,
      -1.3554262353159219,
      -1.349667017257604,
      -1.355029539094128,
      -1.2637783723580245,
      -1.4266958702682817,
      0.0,
      -1.4044402269574434,
      -1.3757539687390774,
      -1.4043161158884396,
      -1.3473730243975077,
      -1.3858463331120812,
      -1.2985384688886228,
      -1.3857248990445772,
      -1.325830640227114,
      -1.3352629557767342
    ],
    [
      -1.6418655618226767,
      -1.5681888494677838,
      -1.544930409397504,
      -1.5873682772021103,
      -1.5337459694305235,
      -1.5764166705861413,
      -1.7009650296499976,
      -1.5862353889897358,
      -1.568173849554693,
      -1.565507824929779,
      -1.5891855048479573,
      -1.7054295225871812,
      -1.5612669136604678,
      -1.6293282777160494,
      -1.5925612449651962,
      -1.5798236571448439,
      -1.5806614427280472,
      -1.5824288502232118,
      -1.6183647970204507,
      -1.5861832142813652,
      0.0,
      -1.6224655647549302,
      -1.547163790729034,
      -1.6040474367743913,
      -1.6376176503067497,
      -1.5942865295957473,
      -1.5811573116142643,
      -1.572875340152006,
      -1.6065599526050223
    ],
    [
      -1.3259803224388782,
      -1.2309678829743713,
      -1.2656547670093001,
      -1.2489907391968273,
      -1.2601602900708775,
      -1.255443931593074,
      -1.2731236416968565,
      -1.2265999008177713,
      -1.2376786543830605,
      -1.2811768957260776,
      -1.2581879055387692,
      -1.3283808974465556,
      -1.2272275111504816,
      -1.2789114796684187,
      -1.2632242061356902,
      -1.2678847252594208,
      -1.259141254601713,
      -1.2318964911580679,
      -1.298099288458406,
      -1.193155318903915,
      -1.2325542566062835,
      0.0,
      -1.2514116115067706,
      -1.200602575061885,
      -1.3078983476318111,
      -1.2589801925531152,
      -1.2774057793320617,
      -1.2337832092508878,
      -1.163430542879073
    ],
    [
      -1.4730143178041686,
      -1.4374924656142296,
      -1.411007975385774,
      -1.4624864867265295,
      -1.4319724578544482,
      -1.4538535128477206,
      -1.5261500094151055,
      -1.392416986930139,
      -1.4207505409993677,
      -1.4018896452337977,
      -1.4685291485357015,
      -1.5367363146852138,
      -1.4060814240033934,
      -1.4455506475074837,
      -1.4029418488001764,
      -1.418959412410545,
      -1.3991822775973755,
      -1.4412751229216678,
      -1.4707168915310451,
      -1.431119456812688,
      -1.408300762783051,
      -1.4766749251463365,
      0.0,
      -1.4596886867640377,
      -1.4413496646568418,
      -1.4671879396865959,
      -1.4576459536396023,
      -1.4526937917564036,
      -1.4264346925978575
    ],
    [
      -1.3822558450558264,
      -1.2756237567845206,
      -1.2472085130924064,
      -1.27099822722456,
      -1.2875130724614574,
      -1.2977372750041045,
      -1.373250218786992,
      -1.2466739357509553,
      -1.2652006629394579,
      -1.3173804568766836,
      -1.286422722344691,
      -1.4133086883581722,
      -1.3088260190259013,
      -1.3051378534990261,
      -1.3400128322062395,
      -1.315772403938208,
      -1.292699757539299,
      -1.3356638418716276,
      -1.3795025363865716,
      -1.2911470562367455,
      -1.3187836342219468,
      -1.2824218764617947,
      -1.3225237295847885,
      0.0,
      -1.3729727613055234,
      -1.3331906640574758,
      -1.3381701971214368,
      -1.2823466995507566,
      -1.3189003158416563
    ],
    [
      -1.5881123813627112,
      -1.59235418884319,
      -1.4816657485783509,
      -1.5899898791877762,
      -1.5420801694280104,
      -1.5563859091559658,
      -1.6079951966749984,
      -1.5347148866803482,
      -1.5914773538781433,
      -1.4766684499888283,
      -1.5724085145787077,
      -1.628733708898316,
      -1.5765771046497021,
      -1.5089867045565941,
      -1.5635458625674636,
      -1.5766482575237315,
      -1.575798670117437,
      -1.559702044215105,
      -1.5589148888421376,
      -1.5560919899732886,
      -1.6049469615954204,
      -1.6248395150231996,
      -1.5720649973247305,
      -1.6080231569758539,
      0.0,
      -1.5685677519767984,
      -1.5435064536470275,
      -1.574860685700012,
      -1.6165506432969652
    ],
    [
      -1.6161751835307079,
      -1.497214445415129,
      -1.5538664442305048,
      -1.4523787589007786,
      -1.497032296215024,
      -1.5445822804438962,
      -1.636955083239294,
      -1.460779366935287,
      -1.5152241653761982,
      -1.5310938883029692,
      -1.510600322170296,
      -1.6519473012464057,
      -1.5256288331687204,
      -1.5053002417987749,
      -1.508817893246146,
      -1.526251914990687,
      -1.5064251755846978,
      -1.4432125318519289,
      -1.5753351132663629,
      -1.4289313001022927,
      -1.5350161325985452,
      -1.5925108387296207,
      -1.570722055583681,
      -1.5933312550778973,
      -1.5080340719612297,
      0.0,
      -1.518998155086849,
      -1.5494344619261435,
      -1.5317461728089246
    ],
    [
      -1.498758582287431,
      -1.4390393448228604,
      -1.3781244955928904,
      -1.4725288293525025,
      -1.4428801170439638,
      -1.4215599430180745,
      -1.5559861295724056,
      -1.4477056827867916,
      -1.4476851441826537,
      -1.357677700949111,
      -1.4131552214867544,
      -1.5882899931480894,
      -1.4377307635998784,
      -1.5133150082753448,
      -1.4545809168685546,
      -1.4605125593616608,
      -1.4533631981695643,
      -1.4770723594689998,
      -1.4851823592758684,
      -1.48605053777303,
      -1.4598635887105975,
      -1.5409492984428361,
      -1.4726730275457838,
      -1.5160771304381366,
      -1.4601709320462182,
      -1.4512950350605014,
      0.0,
      -1.4836874326019887,
      -1.478394796755112
    ],
    [
      -1.5324647774184494,
      -1.3957526142258547,
      -1.435163959109626,
      -1.4787419486540443,
      -1.3767849068906914,
      -1.449416998138072,
      -1.5163397585007707,
      -1.4161899783166023,
      -1.3941554969357652,
      -1.470580606447902,
      -1.4620744643535608,
      -1.5485906763625186,
      -1.4107352508874864,
      -1.4816003548256897,
      -1.4821737274578208,
      -1.4459997786596945,
      -1.5055314062476381,
      -1.4440629642164236,
      -1.4904179335961902,
      -1.4434008834928596,
      -1.4601338824292933,
      -1.4864027594989944,
      -1.5042116495775693,
      -1.488142819874857,
      -1.4614781096951512,
      -1.4474844398808866,
      -1.4863474913592132,
      0.0,
      -1.4665601698897643
    ],
    [
      -1.3357261319974638,
      -1.2750073623375286,
      -1.2666828033235011,
      -1.2182644921797428,
      -1.2882624764904786,
      -1.274571337476182,
      -1.3508313517603938,
      -1.2135156840855863,
      -1.256317225721567,
      -1.2961296126124406,
      -1.292411599101997,
      -1.3753397421345366,
      -1.261402740202262,
      -1.2605286854825235,
      -1.2207469793402597,
      -1.2836285706536361,
      -1.216411224009641,
      -1.2882021669182002,
      -1.304741512281785,
      -1.2376076216118947,
      -1.2570783623033428,
      -1.2555766627286968,
      -1.2354995240471929,
      -1.279898651860594,
      -1.3192731852531594,
      -1.2710307698813248,
      -1.2670858073064435,
      -1.2868444363956064,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2210894457717263,
      0.21444055906899595,
      0.21412616338242185,
      0.21023505633664108,
      0.21975139575431313,
      0.19135866029342674,
      0.20894024616473672,
      0.19526963138405784,
      0.20934397582313147,
      0.20112823802010205,
      0.16734568899990543,
      0.22300829902122787,
      0.22438218060305282,
      0.21255490832859203,
      0.21471476158472824,
      0.20531244649128633,
      0.19994152542580879,
      0.18634776237425443,
      0.25064887978791806,
      0.20695864241950135,
      0.19716246134881343,
      0.2170537464892779,
      0.19729938751247822,
      0.1991906446186278,
      0.19836623362826922,
      0.21277185946366894,
      0.2060276543028028,
      0.20685688015996972
    ],
    [
      0.28725068559004496,
      0.0,
      0.45836001518868774,
      0.4227024469466867,
      0.4628189238697129,
      0.43282101510332205,
      0.3125559844642163,
      0.4116743738810613,
      0.4754163704996237,
      0.3975617650373986,
      0.4952005623570217,
      0.2765745748634296,
      0.4835477061403384,
      0.43688398484729474,
      0.4146636332971414,
      0.43325908863827123,
      0.4630578956434481,
      0.4730871900693736,
      0.3665918735781981,
      0.5460804382199342,
      0.3784439646908304,
      0.35437587835162443,
      0.39489395769561786,
      0.40002827016957143,
      0.40465319629384666,
      0.43844625107294743,
      0.41410748003724995,
      0.4821592502409999,
      0.37907250848692886
    ],
    [
      0.3214076383865687,
      0.41533664142513893,
      0.0,
      0.42116598131069005,
      0.45832526384635064,
      0.4504302504535753,
      0.32698538555228884,
      0.3774972590859107,
      0.42468113125736684,
      0.4860969917850564,
      0.43761079870129627,
      0.2606563775507351,
      0.40171750392078165,
      0.442943411906795,
      0.38735572864375034,
      0.41985434907682184,
      0.3786278118766704,
      0.3734903689203577,
      0.3816566798135135,
      0.47024891193831686,
      0.4092981557483202,
      0.30974437245751885,
      0.3540323177986029,
      0.4200930621251784,
      0.39905601914073996,
      0.39856536544519816,
      0.414830818176551,
      0.38634242283411346,
      0.33881621699760145
    ],
    [
      0.32633465231492775,
      0.4134015522473389,
      0.48099363588523336,
      0.0,
      0.4242511671102227,
      0.42271950598660246,
      0.33270507440572383,
      0.4328112346708697,
      0.42334046592963137,
      0.42915981356607524,
      0.4416001402521066,
      0.2734362349571591,
      0.46012422437372136,
      0.5065968986874196,
      0.43545517819175705,
      0.4198344605257145,
      0.4279089467412782,
      0.43079806131584375,
      0.35716855604605935,
      0.5210041410274584,
      0.36476206548585965,
      0.3597000428770398,
      0.3553561482617418,
      0.3770176489680872,
      0.3504635043419204,
      0.40771194233717956,
      0.3956864872777839,
      0.36730457332341704,
      0.4247161197563116
    ],
    [
      0.21509542709274143,
      0.3279391155156268,
      0.2869331517791103,
      0.2869486493691691,
      0.0,
      0.28903587025464983,
      0.253693823017775,
      0.3306770526027283,
      0.2853527949734196,
      0.37977297640158447,
      0.2847052665552454,
      0.2264950122317082,
      0.3205192331499538,
      0.31600440150669296,
      0.29702543394477754,
      0.2996910521328957,
      0.29189933289020376,
      0.351540033803172,
      0.2615698579817749,
      0.32891779675237953,
      0.30855407639288956,
      0.2512333935175053,
      0.27694880737287986,
      0.28374141636959704,
      0.29176969701063493,
      0.3133795206800096,
      0.28726613751696717,
      0.28559819226337835,
      0.2533911389078942
    ],
    [
      0.3706073281077915,
      0.46664547973243287,
      0.524632533619656,
      0.4461351185119131,
      0.4574178272356264,
      0.0,
      0.33221039543152364,
      0.4324648781575795,
      0.43195872617902986,
      0.478169655297658,
      0.5006609167680973,
      0.30542496592441104,
      0.46404506438273163,
      0.4743981415621459,
      0.42805436262222285,
      0.4491087857707785,
      0.4583349623358566,
      0.423772375281646,
      0.5118944082574817,
      0.46153026173785694,
      0.43359648964588304,
      0.3661643735452291,
      0.37790992953901537,
      0.42960056209879016,
      0.40093453799000867,
      0.4479231816853375,
      0.4661286457363414,
      0.4365201772017675,
      0.43930540339760094
    ],
    [
      0.2795142445012868,
      0.33344712770364127,
      0.3549160597888148,
      0.32900631500419486,
      0.32990682384450554,
      0.3602432898265082,
      0.0,
      0.3749050716164264,
      0.318343362065173,
      0.30638494471485656,
      0.35247389384568883,
      0.26720455355513106,
      0.3168264497706428,
      0.3751374157768692,
      0.3225355305316655,
      0.30022076492736804,
      0.31551449064961923,
      0.3153465731701284,
      0.32750173126214155,
      0.349780343049092,
      0.3103527088276532,
      0.2649770627943069,
      0.28953775153409467,
      0.29968829570989963,
      0.3176547033260726,
      0.32221372336185583,
      0.3149479825079986,
      0.3104870705209779,
      0.28249758067013997
    ],
    [
      0.21978534643886816,
      0.35318773752671295,
      0.3401873820206063,
      0.37489906405473317,
      0.37353854369671313,
      0.30954215988425693,
      0.2921816955197347,
      0.0,
      0.3814158594200059,
      0.31632802636229873,
      0.3193756875112057,
      0.25931915906429914,
      0.38368574971526304,
      0.41184495594332127,
      0.35696247741326514,
      0.3338833769819254,
      0.35684054724769254,
      0.336264615263999,
      0.3318560357592242,
      0.4544023610438326,
      0.3476571804538904,
      0.2978703356112091,
      0.33506858662790195,
      0.34690203080710624,
      0.3487232519295671,
      0.37020637693626246,
      0.293795480499474,
      0.37360211822895306,
      0.3062315614953153
    ],
    [
      0.29751542750072635,
      0.47041998562535414,
      0.4524320173394689,
      0.4617322323932942,
      0.44003652989939646,
      0.4380998068123907,
      0.3125487927971886,
      0.4215811115006345,
      0.0,
      0.38186067997345097,
      0.4803380914795887,
      0.242443370389408,
      0.4761421209051262,
      0.47493574659444504,
      0.4182522723595212,
      0.4445133581995864,
      0.4056297935549513,
      0.45071464580391396,
      0.38449509699745343,
      0.5291068269781813,
      0.4053739964114633,
      0.37572486135880623,
      0.36419569077007297,
      0.418336218172374,
      0.34765410955526965,
      0.42485920659500653,
      0.396007711028437,
      0.5248665105735455,
      0.39047932034935084
    ],
    [
      0.29772567581144704,
      0.338517475377065,
      0.43343688658098367,
      0.37065617696386277,
      0.40571450109297746,
      0.3642373525819891,
      0.22320119138871575,
      0.3408640878649527,
      0.3219124585174784,
      0.0,
      0.34385763526017477,
      0.2016885822206793,
      0.34152542611086867,
      0.33403984406538023,
      0.36564045339528595,
      0.3593061272835669,
      0.34595438230650544,
      0.30772681105092814,
      0.2886330086615556,
      0.3542148051026912,
      0.358040479773615,
      0.30529371911398284,
      0.31610783588665514,
      0.316855841116676,
      0.31368986293788526,
      0.31493422924662373,
      0.4182964137638667,
      0.32162506067557195,
      0.29127980765705885
    ],
    [
      0.31633844168439196,
      0.4808531774118723,
      0.44205540882426053,
      0.42714544654529796,
      0.4207015594103791,
      0.45035210583214047,
      0.3286607635781391,
      0.40117345368653967,
      0.4233260627093083,
      0.40587492419564497,
      0.0,
      0.26033717045586946,
      0.4355850691103227,
      0.40568011744412713,
      0.4120808957942921,
      0.4308202957969396,
      0.4320473858288292,
      0.4286736212331128,
      0.3722962475073295,
      0.4258419558453326,
      0.39701601562272937,
      0.34462810091640783,
      0.3515631601018272,
      0.3972062256658264,
      0.3351663384754997,
      0.4350264223473894,
      0.4388406530164477,
      0.3929903760664739,
      0.4028940137315453
    ],
    [
      0.1950580439075098,
      0.2476851303066785,
      0.20765835229927454,
      0.2217765005251784,
      0.25339441853842226,
      0.23626130916917076,
      0.2324312293494284,
      0.2580020032014654,
      0.21459641184796174,
      0.22431595880009692,
      0.22982704020816414,
      0.0,
      0.2361905792512735,
      0.23366034331606844,
      0.22645687983406626,
      0.21435432741103422,
      0.22838396979790954,
      0.19999709203356497,
      0.22520118098289843,
      0.22788284654529245,
      0.21087904374654087,
      0.22142536549051228,
      0.24647708246278666,
      0.2363952426661018,
      0.27966385429486995,
      0.24972838634450079,
      0.24473841016873044,
      0.2234454354929878,
      0.2542890196138836
    ],
    [
      0.3113936317375383,
      0.547392918322783,
      0.4863613845837893,
      0.5281966420786726,
      0.5420036038993283,
      0.49395296421049495,
      0.3428217082089495,
      0.4858035459675365,
      0.5181767027161521,
      0.40408726501085,
      0.5015808857127959,
      0.27228745402817167,
      0.0,
      0.4536667656008371,
      0.462199333938792,
      0.5827921958196582,
      0.4123799674131656,
      0.5428301538234523,
      0.4476181950473326,
      0.5380521688184243,
      0.4253634592327271,
      0.3459915568361358,
      0.3898812456689771,
      0.38061557558954395,
      0.3865840982658215,
      0.46368409807484934,
      0.41462285367157303,
      0.4673472541318877,
      0.4058720418389308
    ],
    [
      0.2902457961574283,
      0.34137560545092116,
      0.41768122790178386,
      0.4328303733499692,
      0.3708066908773402,
      0.3702811584360659,
      0.3064051350529706,
      0.405347890784455,
      0.34327561663265715,
      0.36717561207660565,
      0.3369284545661768,
      0.2935115363434282,
      0.3556510685116776,
      0.0,
      0.3257309024464978,
      0.3532877003755328,
      0.33369444727232733,
      0.35159703052585667,
      0.39472374207801253,
      0.44968552965984143,
      0.30080101605636256,
      0.26256025716970965,
      0.327872814147085,
      0.31952228550716555,
      0.3729291738921674,
      0.3849340539650541,
      0.31722257938976184,
      0.3143546000925739,
      0.3419716290948638
    ],
    [
      0.3212793250827395,
      0.43182912978964993,
      0.44914922679033675,
      0.47729377902460235,
      0.4609553368613293,
      0.43288062995359544,
      0.3735522419276698,
      0.4505079668074916,
      0.44143966998537554,
      0.4120833413569067,
      0.4477898133996632,
      0.31079337080851666,
      0.5019282685572657,
      0.42915950656820123,
      0.0,
      0.4357459215708659,
      0.43486890574108306,
      0.44488120604593373,
      0.3989668485486908,
      0.44467268096140944,
      0.37753072472683513,
      0.35616897860630536,
      0.41669033996298377,
      0.407298548862008,
      0.3736291135716612,
      0.471519266936113,
      0.46386212616936495,
      0.3993783020865944,
      0.43068495535142914
    ],
    [
      0.3054804209927897,
      0.40206812621081744,
      0.4243701438030092,
      0.42128315320659016,
      0.44973343223319673,
      0.4294978078084153,
      0.3176474450111446,
      0.36317116030568064,
      0.3926641549636949,
      0.43554448716026806,
      0.38891027034139203,
      0.28103073571992954,
      0.4578561972351898,
      0.39229800444489205,
      0.39965339590082904,
      0.0,
      0.3917193636262555,
      0.33332828429574457,
      0.41851077884581467,
      0.4062744476103084,
      0.34590386151885477,
      0.3604869148291274,
      0.356112338287639,
      0.3188808954307991,
      0.3765024339234495,
      0.389064116851741,
      0.36598980138954174,
      0.39977918895594255,
      0.36749081417825935
    ],
    [
      0.2716764856432956,
      0.44004698349040927,
      0.4518240876050361,
      0.43540745425870897,
      0.4174955549777808,
      0.41703349214333474,
      0.2909117772206913,
      0.4360530426486062,
      0.4222282713805905,
      0.4404826286159096,
      0.3954610409408612,
      0.249138479402488,
      0.4187730471714899,
      0.3792725515871873,
      0.4075405933620653,
      0.39642961101712837,
      0.0,
      0.3948530475848977,
      0.3457931470398199,
      0.4450753613292502,
      0.40032784786041553,
      0.35796322757885957,
      0.4335094759844069,
      0.36919085641702076,
      0.3700442464860332,
      0.384574713020184,
      0.4021843386307329,
      0.3536276833328911,
      0.4046443429057123
    ],
    [
      0.29983398240687675,
      0.4844056180083678,
      0.45447932755040177,
      0.4856601542393184,
      0.49046067278861716,
      0.40546428719660765,
      0.34336700111662277,
      0.45598275688534495,
      0.49509697326167923,
      0.3833776588100226,
      0.4529413891542269,
      0.2620552240603571,
      0.5120403565063112,
      0.45454035152911443,
      0.43675605850265975,
      0.41327035447433125,
      0.38409516801166044,
      0.0,
      0.37715741022785654,
      0.549803694177404,
      0.4112955936306655,
      0.3480040509984428,
      0.3660171788667441,
      0.3888379018859205,
      0.39080252288997963,
      0.49130088061878396,
      0.39598037650697604,
      0.4060071257778659,
      0.36508498066651707
    ],
    [
      0.2736810679852646,
      0.3315152882038368,
      0.3487237033310797,
      0.27610908866023065,
      0.34829434108295465,
      0.3680084876675067,
      0.2858972360318077,
      0.4053669980545913,
      0.2956031157746093,
      0.35504237615240153,
      0.3415852144387961,
      0.24147613111906208,
      0.3257329599120995,
      0.4197692219032909,
      0.30703898578195843,
      0.3516018136769443,
      0.36034776678564273,
      0.277456257847142,
      0.0,
      0.3239463118531971,
      0.312905884475682,
      0.26642892990903366,
      0.27369701279539926,
      0.256356849996473,
      0.38175794619027514,
      0.35479966468206703,
      0.3294748150692097,
      0.29477794599228213,
      0.2952082836742296
    ],
    [
      0.32355176718180667,
      0.517153966307818,
      0.4695142882786483,
      0.5086569422292697,
      0.47657295985134174,
      0.3965518078008463,
      0.31649156025536773,
      0.48301634949496686,
      0.4847810617330117,
      0.3884940782856252,
      0.412509635398677,
      0.25840682976249885,
      0.445705482559827,
      0.46894290553453066,
      0.3937691646952044,
      0.3995283827535223,
      0.39416586091699823,
      0.4854170276531018,
      0.32249952974284457,
      0.0,
      0.34475517305368286,
      0.3734414312720489,
      0.34487928412268665,
      0.40182237561361855,
      0.3633490668990451,
      0.4506569311225035,
      0.3634705009665491,
      0.4233647597840122,
      0.4139324442343921
    ],
    [
      0.2737312061902546,
      0.3474079185451475,
      0.3706663586154273,
      0.32822849081082106,
      0.3818507985824078,
      0.33918009742679,
      0.2146317383629337,
      0.3293613790231955,
      0.34742291845823825,
      0.3500889430831524,
      0.32641126316497404,
      0.21016724542575016,
      0.35432985435246356,
      0.28626849029688195,
      0.3230355230477351,
      0.3357731108680875,
      0.3349353252848841,
      0.33316791778971955,
      0.29723197099248067,
      0.3294135537315661,
      0.0,
      0.2931312032580011,
      0.36843297728389723,
      0.31154933123854,
      0.27797911770618167,
      0.321310238417184,
      0.334439456398667,
      0.3427214278609254,
      0.30903681540790906
    ],
    [
      0.2782471325557767,
      0.3732595720202836,
      0.3385726879853548,
      0.35523671579782756,
      0.3440671649237774,
      0.3487835234015808,
      0.33110381329779837,
      0.37762755417688365,
      0.36654880061159445,
      0.3230505592685773,
      0.34603954945588566,
      0.2758465575480993,
      0.37699994384417335,
      0.32531597532623624,
      0.3410032488589647,
      0.33634272973523416,
      0.345086200392942,
      0.37233096383658704,
      0.30612816653624897,
      0.4110721360907399,
      0.37167319838837143,
      0.0,
      0.3528158434878843,
      0.40362487993276996,
      0.2963291073628438,
      0.3452472624415397,
      0.32682167566259324,
      0.3704442457437671,
      0.4407969121155819
    ],
    [
      0.4025468226843405,
      0.4380686748742795,
      0.46455316510273503,
      0.4130746537619796,
      0.4435886826340609,
      0.4217076276407885,
      0.3494111310734036,
      0.48314415355837004,
      0.4548105994891414,
      0.4736714952547114,
      0.4070319919528076,
      0.3388248258032953,
      0.46947971648511566,
      0.43001049298102534,
      0.4726192916883327,
      0.4566017280779642,
      0.4763788628911336,
      0.43428601756684126,
      0.40484424895746396,
      0.44444168367582115,
      0.46726037770545803,
      0.3988862153421726,
      0.0,
      0.41587245372447135,
      0.4342114758316673,
      0.4083732008019132,
      0.41791518684890683,
      0.4228673487321055,
      0.4491264478906516
    ],
    [
      0.27320419032935717,
      0.37983627860066305,
      0.40825152229277717,
      0.3844618081606237,
      0.3679469629237262,
      0.3577227603810791,
      0.28220981659819167,
      0.4087860996342283,
      0.3902593724457257,
      0.3380795785085,
      0.3690373130404927,
      0.24215134702701135,
      0.34663401635928226,
      0.3503221818861575,
      0.31544720317894415,
      0.33968763144697567,
      0.3627602778458845,
      0.31979619351355604,
      0.275957498998612,
      0.3643129791484381,
      0.3366764011632368,
      0.3730381589233889,
      0.3329363058003951,
      0.0,
      0.28248727407966023,
      0.32226937132770783,
      0.3172898382637468,
      0.37311333583442696,
      0.33655971954352726
    ],
    [
      0.3477301495214429,
      0.343488342040964,
      0.45417678230580316,
      0.3458526516963778,
      0.39376236145614363,
      0.3794566217281883,
      0.3278473342091557,
      0.40112764420380587,
      0.34436517700601077,
      0.45917408089532574,
      0.36343401630544636,
      0.307108821985838,
      0.3592654262344519,
      0.4268558263275599,
      0.3722966683166904,
      0.35919427336042253,
      0.3600438607667171,
      0.376140486669049,
      0.37692764204201645,
      0.37975054091086546,
      0.33089556928873365,
      0.31100301586095447,
      0.36377753355942355,
      0.3278193739083002,
      0.0,
      0.3672747789073556,
      0.3923360772371265,
      0.360981845184142,
      0.31929188758718885
    ],
    [
      0.3399280953648709,
      0.45888883348044973,
      0.402236834665074,
      0.5037245199948002,
      0.4590709826805548,
      0.41152099845168255,
      0.3191481956562847,
      0.4953239119602917,
      0.4408791135193806,
      0.42500939059260956,
      0.44550295672528284,
      0.3041559776491731,
      0.43047444572685833,
      0.4508030370968039,
      0.44728538564943277,
      0.42985136390489176,
      0.449678103310881,
      0.5128907470436499,
      0.3807681656292159,
      0.527171978793286,
      0.4210871462970336,
      0.36359244016595804,
      0.38538122331189784,
      0.3627720238176815,
      0.44806920693434904,
      0.0,
      0.4371051238087298,
      0.40666881696943524,
      0.4243571060866542
    ],
    [
      0.39402558092463846,
      0.453744818389209,
      0.514659667619179,
      0.4202553338595669,
      0.44990404616810564,
      0.4712242201939949,
      0.3367980336396639,
      0.44507848042527787,
      0.44509901902941573,
      0.5351064622629584,
      0.479628941725315,
      0.30449417006398005,
      0.45505339961219105,
      0.3794691549367246,
      0.4382032463435148,
      0.43227160385040864,
      0.43942096504250516,
      0.4157118037430696,
      0.40760180393620105,
      0.40673362543903946,
      0.4329205745014719,
      0.3518348647692333,
      0.42011113566628566,
      0.3767070327739328,
      0.43261323116585126,
      0.44148912815156804,
      0.0,
      0.4090967306100808,
      0.41438936645695734
    ],
    [
      0.2519737877610053,
      0.38868595095360003,
      0.34927460606982885,
      0.30569661652541047,
      0.4076536582887633,
      0.33502156704138275,
      0.268098806678684,
      0.36824858686285245,
      0.3902830682436895,
      0.31385795873155287,
      0.32236410082589395,
      0.2358478888169362,
      0.37370331429196835,
      0.302838210353765,
      0.3022648377216339,
      0.3384387865197602,
      0.2789071589318166,
      0.3403756009630312,
      0.2940206315832645,
      0.3410376816865952,
      0.32430468275016144,
      0.2980358056804604,
      0.2802269156018855,
      0.2962957453045978,
      0.3229604554843035,
      0.33695412529856816,
      0.2980910738202416,
      0.0,
      0.3178783952896904
    ],
    [
      0.3336935642591179,
      0.39441233391905306,
      0.40273689293308057,
      0.45115520407683896,
      0.38115721976610306,
      0.3948483587803997,
      0.3185883444961879,
      0.4559040121709954,
      0.4131024705350148,
      0.37329008364414107,
      0.3770080971545846,
      0.29407995412204513,
      0.40801695605431965,
      0.4088910107740582,
      0.44867271691632205,
      0.3857911256029456,
      0.45300847224694074,
      0.3812175293383815,
      0.3646781839747968,
      0.43181207464468696,
      0.4123413339532389,
      0.4138430335278849,
      0.43392017220938883,
      0.3895210443959878,
      0.3501465110034223,
      0.3983889263752569,
      0.4023338889501382,
      0.3825752598609753,
      0.0
    ]
  ],
  "row_avgs": [
    0.20755811909141916,
    0.41772461733124366,
    0.39524525843485037,
    0.4057986598880173,
    0.29234638078526304,
    0.43626962456272916,
    0.31934163803059834,
    0.34019848940920133,
    0.4153676975685145,
    0.3319634332788945,
    0.3998276931727955,
    0.2314348377716562,
    0.4483414167946133,
    0.34937156885051046,
    0.4213049830733793,
    0.3818304348243328,
    0.39041297820127874,
    0.4180042518124884,
    0.32152156068025245,
    0.4080500559823017,
    0.3204251668437221,
    0.34930057574285417,
    0.4319145918939627,
    0.34111555133058274,
    0.366120671054125,
    0.42440521876025766,
    0.42513023004644074,
    0.3208335720743337,
    0.3948262419887967
  ],
  "col_avgs": [
    0.3006734256469588,
    0.3979322581161372,
    0.40725992535101563,
    0.3946220598835376,
    0.40791661017416,
    0.38380823114005974,
    0.30226658266556033,
    0.40144436804990996,
    0.3886303350203229,
    0.38544591827383473,
    0.3857479716164987,
    0.26508222285354704,
    0.3976629242595335,
    0.39267611176431716,
    0.37394836823949695,
    0.38093461004943935,
    0.3757500954230389,
    0.37884404220042384,
    0.3503085858358056,
    0.4183184291628272,
    0.3623919879936467,
    0.3258110732896668,
    0.34733595754633756,
    0.3517804062778756,
    0.3517505250572019,
    0.3804714855954631,
    0.36701992114204923,
    0.37278838259553204,
    0.35736270405521775
  ],
  "combined_avgs": [
    0.254115772369189,
    0.40782843772369043,
    0.401252591892933,
    0.40021035988577747,
    0.3501314954797115,
    0.41003892785139445,
    0.3108041103480793,
    0.37082142872955565,
    0.4019990162944187,
    0.3587046757763646,
    0.3927878323946471,
    0.24825853031260162,
    0.4230021705270734,
    0.3710238403074138,
    0.39762667565643817,
    0.3813825224368861,
    0.38308153681215884,
    0.3984241470064561,
    0.33591507325802905,
    0.4131842425725645,
    0.34140857741868436,
    0.3375558245162605,
    0.3896252747201501,
    0.3464479788042292,
    0.35893559805566344,
    0.4024383521778604,
    0.396075075594245,
    0.3468109773349329,
    0.3760944730220072
  ],
  "gppm": [
    607.891016726162,
    580.1150702370031,
    570.6958132184558,
    575.9891512782517,
    564.0373580914351,
    584.899768868863,
    621.4625352859011,
    570.418254898869,
    581.9359193177347,
    581.1557172886286,
    585.9765702183141,
    637.7260400643697,
    580.2723212201556,
    579.8565638916845,
    591.4234513324371,
    585.2641975819455,
    586.6093451253008,
    586.3111930085628,
    600.9404097722175,
    567.0261169664468,
    589.7003913503477,
    612.1185329748308,
    601.3322347844841,
    598.9976037974644,
    596.6231593153431,
    584.4439432455307,
    591.3947954662453,
    584.7430347392515,
    596.7599623014684
  ],
  "gppm_normalized": [
    1.4246488795606977,
    1.3102149181632814,
    1.2855545643894692,
    1.3073572105195503,
    1.2644723111091403,
    1.317288877814994,
    1.4125841868986342,
    1.2818777474578689,
    1.3088061318410327,
    1.3028963552500399,
    1.31314543472758,
    1.449192263192941,
    1.3056374612725001,
    1.306263383035611,
    1.3327331389132748,
    1.3246814353023626,
    1.3159437807860441,
    1.324469217571524,
    1.3517230105662572,
    1.274631840513107,
    1.324685930074787,
    1.3803156628340933,
    1.3529074385634436,
    1.3454668096765074,
    1.3384229870206483,
    1.3220877246258997,
    1.3257100605809131,
    1.3154175029863377,
    1.337114565323679
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364,
    515,
    394,
    445,
    456,
    590,
    399,
    441,
    475,
    430,
    427,
    416,
    502,
    397,
    443,
    381,
    435,
    391,
    431,
    382,
    457,
    431,
    379,
    342,
    407,
    378,
    371,
    372,
    478,
    412
  ],
  "response_lengths": [
    2635,
    2241,
    2613,
    2628,
    3250,
    2350,
    2527,
    2753,
    2462,
    2458,
    2410,
    2906,
    2365,
    2491,
    2147,
    2516,
    2227,
    2508,
    2190,
    2539,
    2401,
    2272,
    1982,
    2381,
    2149,
    2101,
    2194,
    2700,
    2327
  ]
}