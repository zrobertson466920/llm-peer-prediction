{
  "example_idx": 53,
  "reference": "Published as a conference paper at ICLR 2023\n\nEVERYBODY NEEDS GOOD NEIGHBOURS: AN UNSUPERVISED LOCALITY-BASED METHOD FOR BIAS MITIGATION\n\nTimothy Baldwin1,2\n\nXudong Han1,2 1The University of Melbourne 2Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) xudongh1@student.unimelb.edu.au, {tbaldwin,t.cohn}@unimelb.edu.au\n\nTrevor Cohn1\n\nABSTRACT\n\nLearning models from human behavioural data often leads to outputs that are biased with respect to user demographics, such as gender or race. This effect can be controlled by explicit mitigation methods, but this typically presupposes access to demographically-labelled training data. Such data is often not available, motivating the need for unsupervised debiasing methods. To this end, we propose a new meta-algorithm for debiasing representation learning models, which combines the notions of data locality and accuracy of model fit, such that a supervised debiasing method can optimise fairness between neighbourhoods of poorly vs. well modelled instances as identified by our method. Results over five datasets, spanning natural language processing and structured data classification tasks, show that our technique recovers proxy labels that correlate with unknown demographic data, and that our method outperforms all unsupervised baselines, while also achieving competitive performance with state-of-the-art supervised methods which are given access to demographic labels.\n\n1\n\nINTRODUCTION\n\nIt is well known that naively-trained models potentially make biased predictions even if demographic information (such as gender, age, or race) is not explicitly observed in training, leading to discrimination such as opportunity inequality (Hovy & Søgaard, 2015; Hardt et al., 2016). Although a range of fairness metrics (Hardt et al., 2016; Blodgett et al., 2016) and debiasing methods (Elazar & Goldberg, 2018; Wang et al., 2019; Ravfogel et al., 2020) have been proposed to measure and improve fairness in model predictions, they generally require access to protected attributes during training. However, protected labels are often not available (e.g., due to privacy or security concerns), motivating the need for unsupervised debiasing methods, i.e., debiasing without access to demographic variables. Previous unsupervised debiasing work has mainly focused on improving the worst-performing groups, which does not generalize well to ensuring performance parity across all protected groups (Hashimoto et al., 2018; Lahoti et al., 2020).\n\nIn Section 3, we propose a new meta-algorithm for debiasing representation learning models, named Unsupervised Locality-based Proxy Label assignment (ULPL). As shown in Figure 1, to minimize performance disparities, ULPL derives binary proxy labels based on model predictions, indicating poorly- vs. well-modelled instances. These proxy labels can then be combined with any supervised debiasing method to optimize fairness without access to actual protected labels. The method is based on the key observation that hidden representations are correlated with protected groups even if protected labels are not observed in model training, enabling the modelling of unobserved protected labels from hidden representations. We additionally introduce the notion of data locality to proxy label assignment, representing neighbourhoods of poorly- vs. well-modelled instances in a nearestneighbour framework.\n\nIn Section 4, we compare the combination of ULPL with state-of-the-art supervised debiasing methods on five benchmark datasets, spanning natural language processing and structured data classification. Experimental results show that ULPL outperforms unsupervised and semi-supervised\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: An overview of ULPL. Given a model trained to predict label y from x by optimizing a particular loss, we derive binary proxy labels over- vs. under-performing within each target class based on training losses. These proxy labels are then smoothed according to the neighbourhood in latent space. Finally, the group-unlabeled data is augmented with z′, enabling the application of supervised bias mitigation methods.\n\nbaselines, while also achieving performance competitive with state-of-the-art supervised techniques which have access to protected attributes at training time.\n\nIn Section 5, we show that the proxy labels inferred by our method correlate with known demographic data, and that it is effective over multi-class intersectional groups and different notions of group-wise fairness. Moreover, we test our hypothesis of locality smoothing by studying the predictability of protected attributes and robustness to hyperparameters in finding neighbours.\n\n2 RELATED WORK\n\nRepresentational fairness One line of work in the fairness literature is on protected information leakage, i.e., bias in the hidden representations. For example, it has been shown that protected information influences the geometry of the embedding space learned by models (Caliskan et al., 2017; May et al., 2019). Previous work has also shown that downstream models learn protected information such as authorship that is unintentionally encoded in hidden representations, even if the model does not have access to protected information during training (Li et al., 2018; Wang et al., 2019; Zhao et al., 2019; Han et al., 2021b). Rather than reduce leakage, in this work, we make use of leakage as a robust and reliable signal of unobserved protected labels and derive proxy information from biased hidden representations for bias mitigation.\n\nEmpirical fairness Another line of work focuses on empirical fairness by measuring model performance disparities across protected groups, e.g., via demographic parity (Dwork et al., 2012), equalized odds and equal opportunity (Hardt et al., 2016), or predictive parity (Chouldechova, 2017). Based on aggregation across groups, empirical fairness notions can be further broken down into group-wise fairness, which measures relative dispersion across protected groups (Li et al., 2018; Ravfogel et al., 2020; Han et al., 2022a; Lum et al., 2022), and per-group fairness, which reflects extremum values of bias (Zafar et al., 2017; Feldman et al., 2015; Lahoti et al., 2020). We follow previous work (Ravfogel et al., 2020; Han et al., 2021b; Shen et al., 2022) in focusing primarily on improving group-wise equal opportunity fairness.\n\nUnsupervised bias mitigation Recent work has considered semi-supervised bias mitigation, such as debiasing with partially-labelled protected attributes (Han et al., 2021a), noised protected labels(Awasthi et al., 2020; Wang et al., 2021; Awasthi et al., 2021), or domain adaptation of protected attributes (Coston et al., 2019; Han et al., 2021a). However, these approaches are semi-supervised, as true protected labels are still required for optimizing fairness objectives.\n\nAlthough Gupta et al. (2018) has proposed to use observed features as proxies for unobserved protected labels, the selection of proxy features is handcrafted and does not generalize to unstructured\n\n2\n\nxyN/Aŷ xLossyLossLossUnder-performingOver-performingLossUnder-performingOver-performinghz’z’z’Published as a conference paper at ICLR 2023\n\ninputs (e.g., text or images). Therefore, there is no guarantee of correlation between proxy labels and unobserved protected labels (Chen et al., 2019).\n\nThe most relevant line of work focuses on the notion of Max-Min fairness (Rawls, 2001), which aims to maximize the minimum performance across protected groups. Hashimoto et al. (2018) optimize worst-performing distributions without access to actual protected labels, but suffer from the risk of focusing on outliers, reducing the effectiveness of bias mitigation. Adversarially reweighted learning (ARL) (Lahoti et al., 2020) extends the idea by employing an additional adversary in training to prevent the optimization from focusing on noisy outliers, based on the notion of computational identifiability (H ́ebert-Johnson et al., 2017). However, adversarial training is notoriously non-convex, and there is no guarantee that the adversary will learn contiguous regions rather than identifying outliers. In contrast, our proposed neighbourhood smoothing method is memory-based, does not require adversarial training, and one can explicitly adjust the smoothness of neighbourhood search.\n\nUnsupervised fairness evaluation To access fairness without demographics, recent work (Kallus et al., 2020) has proposed to measure fairness w.r.t. auxiliary variables such as surname and geolocation in different datasets, which is a different research topic and beyond the scope of this paper. In this paper, we use protected labels for tuning and evaluation, and in practice, one can employ our unsupervised debiasing methods together with unsupervised fairness evaluation approaches to perform hyperparameter tuning for better fairness.\n\nDataset cartography Training instances are also grouped based on predictability in the literature on dataset cartography, which is similar to the assignment of proxy labels in this paper. Swayamdipta et al. (2020) propose to visualize training instances according to variability and confidence, where a higher-confidence indicates the instance label can be predicted more easily. Le Bras et al. (2020) also group training instances by their predictability, measured by training simple linear discriminators. Such methods focus on improving in- and out-of-distribution performance without taking fairness into consideration. In comparison, our proposed method aims to mitigate bias by assigning proxy protected group labels to training instances based on their losses within a particular class.\n\n3 METHODS\n\n3.1 PROBLEM FORMULATION\n\nConsider a dataset consisting of n instances D = {(xi, yi, zi)}n i=1, where xi is an input vector to the classifier, yi ∈ [1, . . . , C] represents target class label, and zi ∈ [1, . . . , G] is the group label, such as gender. For unsupervised bias mitigation, protected labels are assumed to be unobserved at training and inference time. nc,g denotes the number of instances in a subset with target label c and protected label g, i.e., Dc,g = {(xi, yi, zi)|yi = c, zi = g}n i=1. A vanilla model (m = f ◦ e) consists of two connected parts: the encoder e is trained to compute the hidden representation from an input, h = e(x), and the classifier makes prediction, ˆy = f (h).\n\n(cid:80)\n\nLet Lc,g = 1 l(m(xi), yi) be the average empirical risk for Dc,g, where l is a loss nc,g function such as cross-entropy. Similarly, let Lc denote the average for instances with target label c (Dc), and L denote the overall empirical risk.\n\n(xi,yi,zi)∈Dc,g\n\nFairness measurement We follow previous work in measuring group-wise performance disparities (Ravfogel et al., 2020; Roh et al., 2021; Shen et al., 2022). Specifically, for a particular utility metric U , e.g., the true positive rate, the results for each protected group are C-dimensional vectors, one dimension for each class. For the subset of instances Dc,g, we denote the corresponding evaluation results as Uc,g. Let Uc denote the overall utilities of class c, then group-wise fairness is achieved if the utilities of all groups are identical, Uc,g = Uc, ∀c, g ⇔ |Uc,g − Uc| = 0, ∀c, g. In addition to the overall performance metric U , we denote the fairness metric F , as a measurement of group-wise utility disparities.\n\n3.2 UNSUPERVISED LOCALITY-BASED PROXY LABEL ASSIGNMENT\n\nProxy label assignment To mitigate bias, the first question is how to minimize disparities of non-differentiable metrics in model training. Previous work has shown that empirical risk-based\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nobjectives can form a practical approximation of expected fairness, as measured by various metrics including AUC-ROC (Lahoti et al., 2020), positive rate (Roh et al., 2021), and true positive rate (Shen et al., 2022). Without loss of generality, we illustrate with the equal opportunity fairness. Note that our method generalizes to other fairness criteria, see Appendix D for detailss.\n\nc=1\n\nc=1\n\n(cid:80)G\n\n(cid:80)G\n\nBy replacing the utility metrics U with empirical risks w.r.t. an appropriate loss function L, the group-wise fairness metrics are reformulated as (cid:80)C g=1 |Lc,g − Lc| = 0, which is an approximation of the desired fairness measurement (cid:80)C g=1 |Uc,g − Uc| = 0. However, protected labels (z) are not observed in unsupervised debiasing settings, which raises the question: how can we optimize fairness objectives with unobserved protected labels? Based on the fairness objective (cid:80)C g=1 |Lc,g − Lc| = 0, we propose to focus on groups within each target class that are systematically poorly modelled. To this end, we binarize the numerous , where z′ unobserved group labels into two types based on their training losses: z′ i\ndenotes the augmented proxy group label. The two types of protected labels indicate that the loss of an instance is either greater than the mean (an ‘under-represented’ group) or ≤ the mean (an ‘over-represented’ group). Each instance can now be assigned with a binary proxy label and used with existing debiasing methods, resulting in augmented datasets D′ = {(xi, yi, z′\n\ni = 1Li>Lyi\n\ni)}n\n\ni=1.\n\n(cid:80)G\n\nc=1\n\ni\n\nNeighbourhood smoothing Simply focusing on worse-performing instances can force the classifier to memorize noisy outliers (Arpit et al., 2017), reducing the effectiveness of bias mitigation. To address this problem, we find the neighbourhood that is likely to be from the same demographic based on the observation of protected information leakage (introduced in Section 2, and justified in Section 5.2), and smooth the proxy label of each instance based on its neighbours.\n\nSpecifically, we use the notion of data locality and adopt a k-Nearest-Neighbour classifier (k-NN) to smooth the proxy label. Given hidden representations {h1, h2, . . . , hn}, where hi = e(xi), and a query point hj, k-NN searches for the k points {hj1 , hj2, . . . , hjk }, s.t. yj = yjk is closest in distance to hj, and then makes predictions through majority vote among proxy labels {z′ }. Unlike the standard setting of k-NN, where the query point is excluded from j1 consideration, we include the query instance. As a result, the smoothing process degrades to using naive proxy labels when k = 1, where the discriminator prediction is the original proxy label without smoothing. For k > 1, on the other hand, neighbourhood smoothing comes into effect.\n\n, . . . , z′\n\n, z′ j2\n\njk\n\nProxy label assignment and neighbour smoothing can be applied at different granularities, such as calculating the loss at steps vs. iterations; see Appendix C.1 for details.\n\n3.3 THEORETICAL JUSTIFICATION\n\nApproximating Fairness Criteria In multi-class classification settings, the equal opportunity fairness is achieved if ˆy ⊥ z|y, ∀y, i.e., the true positive rates (TPR) of each target class are equal for all partitions of the dataset, where partitioning is based on z. Using the definition of cross-entropy of the i-th instance, − (cid:80)C is a function of xi, the loss for the subset of instances with target label c can be simplified as:\n\n1{yi}(c) log(p(ˆyi = c)) where ˆyi\n\nc=1\n\nLc =\n\n1 nc\n\n(cid:88)\n\n(xi,yi,zi)∈Dc\n\n− log(p(ˆyi = c)) =\n\n1 nc\n\n(cid:88)\n\n(xi,yi,zi)∈D\n\n− log(p(ˆy = c|yi = c))\n\n(1)\n\nNotice that Lc is calculated on the subset Dc = {(xi, yi, zi)|yi = c}n i=1, making Lc an unbiased estimator of − log p(ˆy = c|yi = c), which approximates − log TPR of the c-th class. As such, it can be seen that TPR can be empirically replaced by cross-entropy loss when measuring EO fairness.\n\nFairness Lower Bound Consider the worst case in fairness, e.g. p(ˆy = 1, y = 1|z = 1) ≈ 0 and p(ˆy = 1, y = 1|z = 2) ≈ 1, where the TPR gap between the two groups is 1. Such unfairness in training is shown as the minimum training loss of instances in group 1 being larger than the maximum loss of instances in group 0. Taking the proxy label assignment into consideration, this example results in the strong correlation between the gold-class group label and proxy group labels, i.e., p(z′ = 1|z = 1) = p(z′ = 0|z = 2) ≈ 1. Therefore, the correlation between proxy labels\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nand gold-class group labels is positively correlated with unfairness, and the optimization w.r.t. proxy labels increases the lower bound of fairness.\n\n4 EXPERIMENTAL RESULTS\n\nThis section demonstrates the effectiveness of our proposed method through experiments against various competitive baselines and across five widely-used datasets. We report evaluation results of all models based on average values over five runs with different random seeds for each dataset.\n\n4.1 EXPERIMENT SETUP\n\nDatasets We consider the following benchmark datasets1 from the fairness literature: (1) Moji (Blodgett et al., 2016; Elazar & Goldberg, 2018), sentiment analysis with protected attribute race; (2) Bios (De-Arteaga et al., 2019; Subramanian et al., 2021), biography classification with protected attributes gender and economy; (3) TrustPilot (Hovy et al., 2015), product rating prediction with protected attributes age, gender, and country; (4) COMPAS (Flores et al., 2016), recidivism prediction with protected attributes gender and race; and (5) Adult (Kohavi, 1996), income prediction with protected attributes gender and race.\n\nTo enable thorough comparison and explore correlation with unobserved protected attributes, for datasets with more than one protected attribute, we treat each protected attribute as a distinct task, e.g., Bios–gender, and Bios–economy are treated as two different tasks. As a result, there are ten different tasks in total.\n\nBaselines We employ the following baselines: (1) Vanilla, which trains the classifier without explicit bias mitigation; (2) FairBatch (Roh et al., 2021), which adjusts the resampling probabilities of each protected group for each minibatch to minimize loss disparities; (3) GDCLA (Shen et al., 2022), which adjusts the weights of each protected group to minimize loss disparities; (4) GDGLB (Shen et al., 2022), which is a variant of GDCLA that additionally minimizes loss differences across target classes; (5) Adv (Li et al., 2018), which trains the adversary to identify protected information from hidden representations, and removes protected information through unlearning adversaries; (6) SemiAdv (Han et al., 2021a), which trains the adversary with partially-observed protected labels; and (7) ARL (Lahoti et al., 2020), which employs an adversary to assign larger weights to computationally-identifiable underrepresented instances. Besides the Vanilla model, methods (2)- (5) are supervised debiasing baselines, SemiAdv is a semi-supervised debiasing baseline method, and ARL is the baseline for unsupervised bias mitigation.\n\nIn terms of our proposed method, we examine its effectiveness in combination with several supervised debiasing methods, GDCLA, GDGLB, and Adv, denoted ULPL+GDCLA, ULPL+GDGLB, and ULPL+Adv, respectively. To be clear, the supervision in each case is based on the proxy labels z′ i\nlearned in an unsupervised manner by ULPL.\n\nEvaluation Metrics This paper is generalizable to different metrics by varying the objectives of the debiasing methods. For illustration purposes, we follow Ravfogel et al. (2020); Shen et al. (2022) and Han et al. (2021a) in measuring the overall accuracy and equal opportunity fairness, which measures true positive rate (TPR) disparities across groups. Consistent with Section 3.1, we measure the sum of TPR gap across subgroups to capture absolute disparities. We focus on less fair classes by using root mean square aggregation for class-wise aggregation. Overall, the fairness\n\nmetric is F = 1 −\n\n1 C\n\n(cid:80)C\n\nc=1\n\n(cid:16) 1\n\nG\n\n(cid:80)G\n\ng=1 |TPRc,g − TPRc|\n\n(cid:17)2\n\n. For both metrics, larger is better.\n\n(cid:114)\n\nModel comparison Previous work has shown that debiasing methods suffer from performance– fairness trade-offs in bias mitigation (Shen et al., 2022). Most debiasing methods involve a trade-off hyperparameter to control the extent to which the model sacrifices performance for fairness, such as λGDCLA , the strength of additional regularization objectives of GDCLA and our proposed method ULPL+GDCLA. As shown in Figure 2a, given the performance–fairness trade-offs, selecting the\n\n1Key characteristics of the datasets, including dataset statistics, are provided in Appendix A.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) Tuning ULPL+GDCLA trade-off hyperparameter. Shaded areas = 95% CI estimated over 5 runs.\n\n(b) Performance–fairness curve of GDCLA (red dashed line), and ULPL+GDCLA (black solid line).\n\nFigure 2: ULPL+GDCLA degrades to Vanilla performance (dotted lines) when setting λ = 0. As increase the weight of the fairness objective, fairness (orange dashed line) improves at the cost of performance (blue solid line). Figure 2b focuses on the Pareto frontier, and presents AUC-PFC as shaded area over the Bios-gender dataset.\n\nDataset\n\nAttribute\n\nVanilla\n\nGDCLA GDGLB FairBatch Adv SemiAdv\n\nARL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nMoji\n\nR\n\nBios\n\nTrustPilot\n\nAdult\n\nCOMPAS\n\nG\n\nE\n\nG\n\nA\n\nC\n\nG\n\nR\n\nG\n\nR\n\n0.172\n\n0.471\n\n0.490\n\n0.130\n\n0.128\n\n0.125\n\n0.092\n\n0.082\n\n0.111\n\n0.101\n\n0.249 0.230 0.245 0.247 0.247\n\n0.193 0.209 0.190 0.185\n\n0.498 0.480 0.482 0.484 0.483\n\n0.461 0.485 0.474 0.472\n\n0.507 0.495 0.495 0.494 0.495\n\n0.484 0.503 0.495 0.493\n\n0.133 0.132 0.130 0.132 0.131\n\n0.129 0.133 0.133 0.132\n\n0.131 0.131 0.131 0.132 0.132\n\n0.132 0.132 0.132 0.131\n\n0.132 0.131 0.130 0.129 0.131\n\n0.129 0.131 0.131 0.131\n\n0.095 0.096 0.094 0.094 0.094\n\n0.097 0.093 0.094 0.094\n\n0.084 0.085 0.083 0.084 0.082\n\n0.086 0.082 0.085 0.084\n\n0.124 0.124 0.119 0.120 0.121\n\n0.123 0.126 0.127 0.125\n\n0.100 0.102 0.106 0.102 0.097\n\n0.096 0.105 0.105 0.101\n\nTable 1: AUC-PFC on five datasets w.r.t. different protected attributes: G: gender; E: economy; A: age; C: country; R:race. Debiasing methods are introduced in Section 4. AUC-PFC scores are calculated based on trade-off curves averaged over 5 repeated runs with different random seeds.\n\nbest performance degrades to vanilla training and choosing the best fairness results in random predictions. As such, performance and fairness must be considered simultaneously in model comparisons, such as early stopping and hyperparameter tuning. We use protected attributes for early stopping over a validation set and report results on the test set. In practice, model selection should be made in a domain-specific manner, where the best method varies. To make quantitative comparisons based on the performance–fairness trade-offs, we follow Han et al. (2023) in reporting the area under the performance–fairness trade-off curves (AUC-PFC) of each method. As shown in Figure 2b, the performance–fairness trade-off curve (PFC) of a particular method consists of a Pareto frontier, which represents the best results that can be achieved in different scenarios, and the area under the curve based on PFC (AUC-PFC) reflects the overall goodness of a method. In particular, the AUC-PFC score of GDCLA (red) and ULPL+GDCLA (blue) are 0.498 and 0.485, respectively, and their difference (i.e., area between the two curves) is 0.013. See Appendix B for further details.\n\n4.2 CAN WE MITIGATE BIAS WITHOUT ACCESS TO PROTECTED ATTRIBUTES?\n\nTable 1 compares our proposed ULPL based methods against baselines.\n\nVanilla and supervised debiasing baselines: Compared with the Vanilla model, supervised debiasing baselines (GDCLA, GDGLB, FairBatch, and Adv) substantially improve fairness with relatively little performance cost, resulting in larger AUC-PFC scores. Among the four supervised de-\n\n6\n\n0.00.20.40.60.81.00.00.20.40.60.8PerformanceVanilla FairnessVanilla Performancez'+GDCla Fairnessz'+GDCla Performance0.850.900.951.00Fairness0.30.40.50.60.70.8Performance0.850.900.951.00Fairnessz'+GDClaGDClaPublished as a conference paper at ICLR 2023\n\nDataset Moji\n\nBios\n\nTrustPilot\n\nAdult\n\nCOMPAS\n\nAttribute\n\nR\n\nG\n\nE\n\nG\n\nA\n\nC\n\nG\n\nR\n\nG\n\nR\n\nF rz′\n\n0.636 0.996\n\n0.837 0.656\n\n0.915 0.214\n\n0.963 0.056\n\n0.971 0.045\n\n0.960 0.034\n\n0.951 0.600\n\n0.751 0.691\n\n0.894 0.983\n\n0.672 0.855\n\nTable 2: Fairness (F ) and Pearson’s rz′ between proxy labels and signed gaps for Vanilla.\n\nbiasing methods, GDCLA outperforms other methods, which is consistent with previous work (Shen et al., 2022).\n\nSimilar to the baseline debiasing methods, debiasing w.r.t. proxy labels (ULPL+∗) also improves fairness over Vanilla, and achieves results that are competitive with supervised debiasing methods.\n\nSemi-supervised debiasing baselines: We examine the effectiveness of SemiAdv by removing 50% of the protected labels, i.e., the adversary is trained over a subset of training instances. Observe that SemiAdv achieves almost identical results to Adv, consistent with Han et al. (2021a). Although SemiAdv uses protected labels in training, it is substantially outperformed by the proposed unsupervised method ULPL+GDCLA.\n\nUnsupervised debiasing baselines: ARL (Lahoti et al., 2020) is also an unsupervised debiasing method that trains an adversary to predict the weights of each instance such that the weighted empirical risk is maximized. The training objective of ARL does not match with the definition of group-wise fairness, and as such, it results in lower AUC-PFC scores than our proposed methods, that explicitly optimize for performance parity across protected groups.\n\nIn terms of excluding outliers, the adversary of ARL is intended to predict smooth weights of instances from (x, y), such that the trained model focuses more on worse-performing instances and is discouraged from memorizing noisy outliers. However, our results show that our ULPL method is more robust and effective in implementing these notions.\n\nDifferent ULPL methods: Among the proxy label-based methods, ULPL+GDCLA consistently outperforms other methods. ULPL+GDCLA calculates the loss differences separately before aggregation, which eliminates the influence of group size in debiasing and treats each group and class equally in optimization, which is better aligned with the evaluation metric.\n\nAdv is a popular method for achieving representational fairness, and differs from ULPL+GDCLA and ULPL+GDGLB in that it directly optimizes for empirical fairness. Removing protected information from hidden representations requires accurate perceptions of the global geometry of particular protected groups. However, proxy labels are based on local loss differences within each class, meaning that the same proxy label in different classes may conflate different protected groups. As such, the combination of ULPL with Adv is less effective than the two other combinations.2\n\n5 ANALYSIS\n\n5.1 PROXY LABEL ASSIGNMENT\n\nWe first investigate if the ULPL labels are meaningful through the lens of Pearson’s correlation (rz′) between proxy labels and signed performance gaps. Given that F is optimized if (cid:80) c,g |TPRc,g − TPRc| = 0, an instance (xi, yi) should be assigned z′ i = 0 if TPRyi,zi − TPRzi < 0, i.e., the unobserved group z is under-performing in class yi, and z′ i = 1 otherwise. We calculate rz′ between P (z′ = 0|y, z) and TPRy,z − TPRz, and presents the results of Vanilla over each dataset in Table 2.\n\nIt can be seen that there exists a strong correlation rz′ for all datasets other than TrustPilot, indicating that the unsupervised proxy label recovers demographic data and provides a strong signal for bias mitigation. We also observe that better fairness results in smaller rz′, for example, for TrustPilot and Bios-E, which is not surprising as the gaps (|TPRc,g − TPRc|) are close to 0.\n\n2See Appendix E.6 for further discussion.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nAttribute\n\n(a)\n\nLeakage\n\n(b)\n\nULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nMoji\n\nBios\n\nTrustPilot\n\nAdult\n\nCOMPAS\n\nR\n\n83\n\n9 7\n3\n\nG\n\n79\n\n1 1\n1\n\nE\n\n84\n\n1 1\n1\n\nG\n\n40\n\n1 1\n1\n\nA\n\n52\n\n1 1\n1\n\nC\n\n26\n\n1 1\n1\n\nG\n\n80\n\n13 13 9\n\nR\n\n25\n\n13 7\n13\n\nG\n\n83\n\n3 9\n3\n\nR\n\n43\n\n13 13 5\n\nTable 3: (a) Leakage (%) of protected attributes. (b) Best k assignments of each method.\n\n5.2 EFFECTIVENESS OF THE NEIGHBOURHOOD-SMOOTHING\n\nIn smoothing ULPL labels, we hypothesise that an instance’s neighbours are likely from the same protected group. Except for instance itself in smoothing, the remaining nearest neighbours are essentially the results of a standard k-nearest-neighbour (KNN) model. Therefore, we perform analysis based on standard KNN models and investigate if the remaining nearest neighbours are helpful for label smoothing, i.e., are they from the same protected group as the target instance.\n\nProtected information predictability: Proxy label smoothing is based on the hypothesis that there is a strong correlation between the hidden representations and the protected labels, even if protected labels are not observed during training. To test this hypothesis, we employ 1-NN for protected label prediction based on Vanilla hidden representations within each target class, using leave-one-out cross-validation over each batch to evaluate the predictability of protected attributes.\n\nTable 3a presents the results (macro F1 score) for each protected attribute, from which we can see a strong correlation between unobserved protected labels and hidden representations.\n\nFurthermore, in Appendix E.2, we show that although debiasing methods successfully reduce performance disparities in downstream tasks, leakage of protected attributes in debiased hidden representations is still high, consistent with previous work (Han et al., 2021b).\n\nincluding:\n\nSensitivity to k-NN hyperparameters: Estimations of protected labels can be affected by k-NN’s hyperparameters, (1) p, the norm of distance; (2) whether nearestneighbours must match the target class versus all classes; and (3) k, the number of nearest neighbours. We explore the sensitivity to these hyperparameters for the Moji dataset in Figure 3.\n\nFirst, we can see that k-NN is highly robust to varying values of p. In terms of whether label smoothing should be class-specific or inspecific, there is a slight empirical advantage to performing it in a class-specific manner.\n\nFigure 3: Hyperparameter sensitivity analysis over the Moji dataset.\n\nIn terms of k, for Moji, higher values result in better estimations of protected labels, although there is a clear plateau. If we explore this effect over the other datasets in terms of the k value that achieves the highest AUC–PFC score, as can be seen in Table 3b, there is no clear trend, with neighbourhood-smoothing (k > 1) improving results for Moji, Adult, and COMPAS and the best results being achieved for values from 3 to 13, whereas for Bios and TrustPilot, no neighbourhood smoothing (k = 1) performs best. Although the optimal value of k varies greatly across datasets and debiasing methods, it is possible to perform fine-grained tuning to reduce computational cost. In Appendix C.4, we discuss situations where label smoothing succeeds or fails, and an effective tuning strategy for the value of k.\n\n5.3 DEBIASING FOR INTERSECTIONAL GROUPS\n\n8\n\n51015k0.820.840.86fscoreclass_wise = False51015kclass_wise = Truep2468Published as a conference paper at ICLR 2023\n\nDataset\n\nAttribute\n\nVanilla\n\nGDCLA GDGLB Adv\n\nARL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nMoji\n\nR\n\nAdult\n\nCOMPAS\n\nG\n\nR\n\nG×R\n\nG\n\nR\n\nG×R\n\n0.173\n\n0.087\n\n0.076\n\n0.073\n\n0.109\n\n0.097\n\n0.094\n\n0.253 0.233 0.254\n\n0.197 0.214 0.192 0.186\n\n0.091 0.091 0.091\n\n0.092 0.090 0.090 0.091\n\n0.081 0.081 0.077\n\n0.081 0.082 0.081 0.083\n\n0.079 0.077 0.074\n\n0.078 0.080 0.078 0.080\n\n0.121 0.121 0.117\n\n0.120 0.123 0.124 0.122\n\n0.094 0.098 0.094\n\n0.092 0.101 0.100 0.099\n\n0.085 0.094 0.945\n\n0.092 0.095 0.097 0.092\n\nTable 5: AUC-PFC based on demographic parity fairness.\n\nWe also investigate the robustness of binary proxy labels to non-binary intersectional groups (i.e. the cross product of values across different protected attributes).\n\nDataset Attribute\n\nVanilla\n\nBios G×E\n\n0.455\n\nTrustPilot G×A×C\n\n0.115\n\nAdult G×R\n\n0.072\n\nCOMPAS G×R\n\n0.098\n\nGDCLA GDGLB Adv\n\n0.487 0.453 0.456\n\n0.126 0.122 0.121\n\n0.072 0.075 0.072\n\n0.091 0.100 0.098\n\nTable 4 presents debiasing results for intersectional groups over those datasets that are labelled with more than one protected attribute. Compared to the single protected attribute results, the AUC-PFC scores of Vanilla are consistently smaller, indicating greater bias across intersectional groups, consistent with the findings of Subramanian et al. (2021). For ULPL models, on the other hand, the results are competitive with supervised debiasing methods, consistent with the single attribute setting.\n\nARL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nTable 4: AUC-PFC w.r.t. intersectional groups.\n\n0.120 0.105 0.101 0.122\n\n0.075 0.076 0.079 0.071\n\n0.097 0.098 0.101 0.099\n\n0.426 0.470 0.459 0.456\n\n5.4 OTHER FAIRNESS METRICS: DEMOGRAPHIC PARITY\n\nFinally, we investigate the robustness of ULPL to other notions of fairness. For illustration purposes, we focus on demographic parity fairness (DP) (Blodgett et al., 2016), which requires model predictions to be independent of protected attributes. Again, we aggregate accuracy and DP fairness trade-offs as AUC-PFC scores. Since DP is sensitive to class imbalance and there is no standard way of generalizing DP to multi-class classification tasks, we only conduct experiments over binary classification tasks, namely Moji, Adult, and COMPAS.\n\nTable 5 shows the results w.r.t. demographic parity fairness. The overall trend is similar to our original results for equal opportunity fairness, indicating that ULPL is robust to different fairness metrics when combined with a range of debasing methods.\n\n6 CONCLUSION\n\nMuch of previous work in the fairness literature has the critical limitation that it assumes access to training instances labelled with protected attributes. To remove this restriction, we present a novel way of deriving proxy labels, enabling the adaptation of existing methods to unsupervised bias mitigation. We conducted experiments over five widely-used NLP and ML benchmark datasets, and showed that, when combined with different debiasing strategies, our proposed method consistently outperforms naively-trained models and unsupervised debiasing baselines, achieving results which are competitive with supervised debiasing methods. Furthermore, we showed our proposed method to be generalizable to multi-class intersectional groups and different notions of fairness.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe thank the anonymous reviewers for their helpful feedback and suggestions. This work was funded by the Australian Research Council, Discovery grant DP200102519. This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.\n\nETHICS STATEMENT\n\nThis work focuses on learning fair models without observation of protected labels at training time. Demographics are assumed to be available only for evaluation purposes, and not used for model training or inference. We only use attributes that the user has self-identified in our experiments. All data and models in this study are publicly available and used under strict ethical guidelines.\n\nREPRODUCIBILITY STATEMENT\n\nAll baseline experiments are conducted with the FairLib library (Han et al., 2022b). Source code is available at https://github.com/HanXudong/An_Unsupervised_ Locality-based_Method_for_Bias_Mitigation Appendix A reports relevant statistics, details of train/test/dev splits, etc. for the five benchmark datasets that are used in this paper. Appendix B reports implementation details of evaluation metrics and corresponding aggregation approaches. Appendix C presents details of computing infrastructure used in our experiments, computational budget, hyperparameter search, etc. Appendix F reports PFC and full dis-aggregated results, i.e., mean ± std over 5 random runs with different random seeds.\n\nREFERENCES\n\nDevansh Arpit, Stanisław Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon In Proceedings of the 34th Lacoste-Julien. A closer look at memorization in deep networks. International Conference on Machine Learning - Volume 70, ICML’17, pp. 233–242. JMLR.org, 2017.\n\nPranjal Awasthi, Matth ̈aus Kleindessner, and Jamie H. Morgenstern. Equalized odds postprocessing\n\nunder imperfect group information. In AISTATS, 2020.\n\nPranjal Awasthi, Alex Beutel, Matthaeus Kleindessner, Jamie H. Morgenstern, and Xuezhi Wang. Evaluating fairness of machine learning models under uncertain and incomplete information. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021.\n\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social media: A case study of African-American English. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1119–1130, November 2016. doi: 10. 18653/v1/D16-1120.\n\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. Semantics derived automatically from\n\nlanguage corpora contain human-like biases. Science, 356:183 – 186, 2017.\n\nJiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, and Madeleine Udell. Fairness under unawareness: Assessing disparity when protected class is unobserved. Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nAlexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism\n\nprediction instruments. Big data, 5(2):153–163, 2017.\n\nAmanda Coston, Karthikeyan Natesan Ramamurthy, Dennis Wei, Kush R. Varshney, Skyler Speakman, Zairah Mustahsan, and Supriyo Chakraborty. Fair transfer learning with missing protected attributes. Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMaria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: In Proceedings of the A case study of semantic representation bias in a high-stakes setting. Conference on Fairness, Accountability, and Transparency, pp. 120–128, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pp. 214–226, 2012.\n\nYanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 11–21, 2018.\n\nBjarke Felbo, Alan Mislove, Anders Søgaard, Iyad Rahwan, and Sune Lehmann. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017.\n\nMichael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 259–268, 2015.\n\nAnthony W. Flores, Kristin A. Bechtel, and Christopher T. Lowenkamp. False positives, false negatives, and false analyses: A rejoinder to ”machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks”. Federal Probation, 80:38, 2016.\n\nMaya R. Gupta, Andrew Cotter, Mahdi Milani Fard, and Serena Wang. Proxy fairness. ArXiv,\n\nabs/1806.11212, 2018.\n\nXudong Han, Timothy Baldwin, and Trevor Cohn. Decoupling adversarial training for fair NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 471–477, August 2021a. doi: 10.18653/v1/2021.findings-acl.41.\n\nXudong Han, Timothy Baldwin, and Trevor Cohn. Diverse adversaries for mitigating bias in training. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 2760–2765, April 2021b.\n\nXudong Han, Timothy Baldwin, and Trevor Cohn. Balancing out bias: Achieving fairness through In Proceedings of the 2022 Conference on Empirical Methods in Natural balanced training. Language Processing, pp. 11335–11350, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics.\n\nXudong Han, Aili Shen, Yitong Li, Lea Frermann, Timothy Baldwin, and Trevor Cohn. FairLib: A unified framework for assessing and improving fairness. In Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 60–71, Abu Dhabi, UAE, December 2022b. Association for Computational Linguistics.\n\nXudong Han, Timothy Baldwin, and Trevor Cohn. Fair enough: Standardizing evaluation and model\n\nselection for fairness research in nlp. arXiv preprint arXiv:2302.05711, 2023.\n\nMoritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances\n\nin Neural Information Processing Systems, 29:3315–3323, 2016.\n\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pp. 1929–1938. PMLR, 2018.\n\n ́Ursula H ́ebert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Calibration for the\n\n(computationally-identifiable) masses. ArXiv, abs/1711.08513, 2017.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDirk Hovy and Anders Søgaard. Tagging performance correlates with author age. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 483–488, July 2015. doi: 10.3115/v1/P15-2079.\n\nDirk Hovy, Anders Johannsen, and Anders Søgaard. User review sites as a resource for large-scale sociolinguistic studies. Proceedings of the 24th International Conference on World Wide Web, 2015.\n\nNathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved\n\nprotected class using data combination. In FAT*, 2020.\n\nDiederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\n\nConference on Learning Representations (ICLR), 2015.\n\nRon Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In KDD,\n\n1996.\n\nPreethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural Information Processing Systems, volume 33, pp. 728–740, 2020.\n\nRonan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International Conference on Machine Learning, pp. 1078–1088. PMLR, 2020.\n\nYitong Li, Timothy Baldwin, and Trevor Cohn. Towards robust and privacy-preserving text representations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 25–30, July 2018. doi: 10.18653/v1/P18-2005.\n\nMarco Lui and Timothy Baldwin.\n\nlangid. py: An off-the-shelf language identification tool.\n\nIn\n\nProceedings of the ACL 2012 system demonstrations, pp. 25–30, 2012.\n\nKristian Lum, Yunfeng Zhang, and Amanda Bower. De-biasing “bias” measurement.\n\nIn 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22), Seoul, Republic of Korea, June 2022. ACM. doi: 10.1145/3531146.3533105.\n\nR Timothy Marler and Jasbir S Arora. Survey of multi-objective optimization methods for engineer-\n\ning. Structural and Multidisciplinary Optimization, 26(6):369–395, 2004.\n\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. On measuring social biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 622–628, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1063.\n\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7237–7256, July 2020. doi: 10.18653/v1/2020.acl-main.647.\n\nJohn Rawls. Justice as Fairness: A Restatement. Harvard University Press, 2001.\n\nYuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection for model fairness. In Proceedings of the 9th International Conference on Learning Representations, 2021.\n\nAili Shen, Xudong Han, Trevor Cohn, Timothy Baldwin, and Lea Frermann. Optimising equal opportunity fairness in model training. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4073–4084, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.299.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nShivashankar Subramanian, Xudong Han, Timothy Baldwin, Trevor Cohn, and Lea Frermann. Evaluating debiasing techniques for intersectional biases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 2492–2498, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with In Proceedings of the 2020 Conference on Empirical Methods in Natural training dynamics. Language Processing (EMNLP), pp. 9275–9293, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.746.\n\nJialu Wang, Yang Liu, and Caleb C. Levy. Fair classification with group-dependent label noise. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021.\n\nTianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5310–5319, 2019.\n\nForest Yang, Mouhamadou Cisse, and Sanmi Koyejo. Fairness with overlapping groups; a probabilistic perspective. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4067–4078. Curran Associates, Inc., 2020.\n\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp. 962–970. PMLR, 2017.\n\nHan Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair\n\nrepresentations. In International Conference on Learning Representations, 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA DATASETS AND PRE-PROCESSING\n\nA.1 MOJI\n\nFollowing previous studies (Ravfogel et al., 2020; Han et al., 2021b), the original training dataset is balanced with respect to both sentiment and ethnicity but skewed in terms of sentiment–ethnicity combinations (40% happy-AAE, 10% happy-SAE, 10% sad-AAE, and 40% sad-SAE, respectively). The dev and test sets are balanced in terms of sentiment–ethnicity combinations. The dataset contains 100K/8K/8K train/dev/test instances.\n\nWhen varying training set distributions, we keep the 8k test instances unchanged. We use DeepMoji (Caliskan et al., 2017) to obtain twitter representations, where DeepMoji is a model pretrained over 1.2 billion English tweets and DeepMoji is fixed during model training.\n\nA.2 BIOS\n\nBios experiments are based on a biography classification dataset (De-Arteaga et al., 2019; Ravfogel et al., 2020), where biographies were scraped from the web, and annotated for the protected attribute of binary gender and target label of 28 profession classes.\n\nBesides the binary gender attribute, we additionally consider economic status as a second protected attribute. Subramanian et al. (2021) semi-automatically labelled economic status based on the individual’s home country (wealthy vs. rest of world), as geotagged from the first sentence of the biography. For bias evaluation and mitigation, we consider the intersectional groups, i.e., the Cartesian product of the two protected attributes, leading to 4 intersectional classes: female–wealthy, female–rest, male–wealthy, and male–rest.\n\nSince the data is not directly available, in order to construct the dataset, we use the scraping scripts of Ravfogel et al. (2020), leading to a dataset with 396k biographies.3 Following Ravfogel et al. (2020), we randomly split the dataset into train (65%), dev (10%), and test (25%).\n\nThe augmentation for economic attributes follows previous work (Subramanian et al., 2021), which results in approximate 30% instances that are labelled with both protected attributes.\n\nA.3 TRUSTPILOT\n\nProfession\n\nTotal\n\nMale\n\nFemale\n\nprofessor physician attorney photographer journalist nurse psychologist teacher dentist surgeon architect painter model poet software engineer filmmaker composer accountant dietitian comedian chiropractor pastor paralegal yoga teacher interior designer personal trainer DJ rapper\n\nTotal\n\n21715 7581 6011 4398 3676 3510 3280 2946 2682 2465 1891 1408 1362 1295 1289 1225 1045 1012 730 499 474 453 330 305 267 264 244 221\n\n72578\n\n$\n\n46 42 51 53 41 8\n31 35 52 73 64 47 15 46 70 56 70 55 5\n69 62 59 12 13 16 41 71 75\n\n9\n\n$\n\n9 8\n10 11 9\n1 6\n6 11 12 12 9\n2 7\n14 10 14 9\n1 9\n14 15 3\n3 4\n10 16 15\n\n45\n\n$\n\n37 41 33 30 41 76 52 49 30 13 21 36 70 39 14 29 14 29 82 19 21 23 70 71 67 42 11 9\n\n7\n\n$\n\n7 8\n6 6\n9 15 11 10 6\n2 3\n8 13 8\n2 6\n2 6\n12 3\n3 4\n15 12 12 7\n2 1\n\n39\n\nTable 6: For each profession in Bios, the table shows the number of individuals and the breakdown across demographics as a percentage. $ and $ denote the economic status (high vs. low, respectively).\n\nWe fellow previous work (Li et al., 2018) in using the TrustPilot dataset derived from Hovy et al. (2015), where each review is an-\n\n3There are slight discrepancies in the dataset composition due to data attrition: the original dataset (De-\n\nArteaga et al., 2019) had 399k instances, while 393k were collected by Ravfogel et al. (2020).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nScore\n\nTotal\n\nUK\n\nUS\n\nUK\n\nUS\n\nFemale\n\nMale\n\nOver45 Under35 Over45 Under35 Over45 Under35 Over45 Under35\n\n1 2\n3 4\n5\n\n5051 1783 2001 4877 29489\n\n13 15 14 13 17\n\n17 15 15 14 11\n\n3 2\n3 4\n5\n\n3 5\n4 7\n4\n\n23 24 25 23 26\n\n31 29 26 25 22\n\n4 4\n4 5\n5\n\n6 6\n8 10 9\n\nTable 7: For each score in TrustPilot, the table shows the number of instances and the breakdown across demographics as a percentage.\n\nnotated with the target rating variable and associated with three protected labels gender (male vs. female), age (under-35-year-old vs. over-45-year-old), and, location (UK vs. the US). The original dataset contains 5 different countries (US, UK, Germany, Denmark, and France), and Li et al. (2018) discard non-English reviews after automatic language classification (Lui & Baldwin, 2012). Despite this, there are some non-English reviews in the filtered dataset, and there, we further drop instances from Germany, Denmark and France, resulting in a dataset with 54k instances in total.\n\nA.4 ADULT AND COMPAS\n\nExcept for race features, we use the same pre-processing as in Lahoti et al. (2020) for COMPAS (Flores et al., 2016) and Adult (Kohavi, 1996) datasets with 5,278 and 43,131 examples, respectively. Lahoti et al. (2020) considers binary race groups (white vs. black). However, there are more than two protected groups in the original dataset. Specifically, there are 3 race groups in COMPAS: African-American, Caucasian, and Other; and 5 race groups in Adult: White, AsianPac-Islander, Amer-Indian-Eskimo, Other, Black.\n\nB EVALUATION METRICS\n\n(cid:80)\n\nBesides the absolute gap metric (|Uc,g − Uc| = 0), a broad range of formats of metrics have been introduced in previous studies to capture different assumptions about the nature of fairness. For example, Lum et al. (2022) propose to measure the variability of performance across demographic groups ( 1 g |Uc,g−Uc|2), Yang et al. (2020) only focus on the largest gap (maxg|Uc,g−Uc|), and Feldman et al. (2015) measure performance ratio rather than gap in measuring fairness ( maxgUc,g ). Although, different aggregation methods have been applied to measure group-wise fairness, the optimization of any one of them is a sufficient condition for the optimization of other methods, as the optimization conditions of these metrics are identical, Uc,g = Uc∀c, g.\n\nmingUc,g\n\nG−1\n\nFor fair comparison across different debiasing approaches, we should select the hyperparameters consistently. Previous work has used different criteria for model selection, including: (1) minimum loss (Hashimoto et al., 2018; Li et al., 2018); (2) maximum utility (Lahoti et al., 2020), e.g., based on accuracy or F-measure; (3) manual selection based on visual inspection of the trade-off curve (Elazar & Goldberg, 2018; Ravfogel et al., 2020); and (4) constrained selection (Han et al., 2021b; Subramanian et al., 2021), by selecting the best fairness constrained to a particular level of performance, and vice versa. Each selection criterion reflects the performance at a particular situation, making it very hard to rigorously compare methods.\n\nInstead, the AUC-PFC score is the integral of performance–fairness curves with respect to performance on an interval [0, 1]. For a particular dataset, by the definition of fairness metrics, a random classifier achieves the best fairness. Therefore, the integration from 0 to the random prediction accuracy is dataset-specific and is identical to different methods. In this paper, we normalize AUC-PFC scores for each dataset by ignoring the performance worse than random guess. Table 8 summaries the lowest accuracy scores w.r.t. each dataset.\n\nWhen calculating AUC-PFC scores, for these methods that are not flexible to achieve best fairness, we manually add the random model to the calculation. Taking the Vanilla model on Moji as an\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nMoji Bios TrustPilot Adult COMPAS 50% 30%\n\n68%\n\n76%\n\n56%\n\nTable 8: Majority label proportion, i.e., lowest accuracy of each dataset.\n\nexample, the performance and fairness are 0.7109 ± 0.0110 and 0.6358 ± 0.1331, respectively. The random model corresponds to 0.5 accuracy and 1.0 fairness. Given these two points, the PFC is the line form (0.7109, 0.6358) to (0.5, 1), and the AUC-PFC score is\n\n(0.7109 − 0.5) × 0.6358 + 0.5 × (0.7109 − 0.5) × (1 − 0.6358) = 0.172,\n\nwhich is consistent with Table 1.\n\nInstead of However, we still need to select a model for early stopping before model selection. considering performance and fairness metrics separately, we use the distance to the optimal point (“DTO”), which quantifies the accuracy–fairness tradeoff (Marler & Arora, 2004; Han et al., 2022a). DTO measures the normalized Euclidean distance for a given combination of accuracy and fairness to the optimal point which denotes the ideal result, e.g., accuracy and fairness of 1.0. It is typically unachievable in practice.\n\nB.1\n\nINTERPRETATION OF AUC–PFC RESULTS\n\nThe main motivation for using AUC–PFC is for ease of comparison between approaches, as it aggregates the performance–fairness trade–off curve (PFC) of each model to a single number, enabling systematic comparison across different tasks. The two common questions related to AUC–PFC are:\n\n• The magnitude of AUC–PFC differs from a single metric, and a 0.0001 improvement in the AUC–PFC score is equivalent to a 1 percentage point (pp) improvement in both performance and fairness (0.01 × 0.01). In the paper, numbers are rounded to 3 decimals, and a minimum difference in AUC–PFC (0.001) is roughly equivalent to a 3 pp improvement in both performance and fairness in a PFC plot.\n\n• The calculation of AUC–PFC scores is normalized by the worst performance, which is the majority label proportion when using the accuracy metric. Therefore, AUC–PFC scores represent to what extent a model improves the performance or fairness over the random model.\n\nThere is no doubt that using AUC–PFC comes with certain limitations. To address the major concerns related to AUC–PFC scores, we present additional results in Appendix E, including disaggregated results for each dataset.\n\nIn particular, we provide the PFC of each method (e.g., Figure 6 in Appendix F), representing the best fairness that can be achieved at different performance levels, and vice versa.\n\nOne limitation of a PFC plot is that it is hard to make quantitative conclusions based on the plot itself, and we cannot conclude that one method is better than another if any intersection exists between their PFCs.\n\nTo address this problem, we additionally conduct quantitative comparisons across different debiasing methods by model selection w.r.t. two different criteria, and then compare both the performance and fairness of the selected models (e.g., Table 14 in Appendix E). For each method, we report the evaluation results averaged over 5 random runs with standard deviation for both the development set and test set.\n\nAs stated in Appendix F, we present disaggregated results (including a PFC plot and a table) for all 15 settings on GitHub.\n\nC EXPERIMENTAL DETAILS\n\nWe conduct our experiments on an HPC cluster instance with 4 CPU cores, 32GB RAM, and one NVIDIA V100 GPU.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC.1 ASSIGNING AND SMOOTHING PROXY LABELS\n\nAssigning proxy labels In the current experiments, proxy labels are assigned based on the losses of each minibatch, i.e., the loss per instance is taken from a particular training iteration. We acknowledge that there are other ways of extracting training losses, e.g. taking losses from the final model or averaged over multiple iterations as the reviewer suggested, and we leave it as a future work.\n\nSmoothing proxy labels The proxy label assignment and smoothing happen simultaneously at each iteration. By doing so, our method can be incorporated into existing systems with only a few lines of changes to replace the actual protected labels with our proxy labels.\n\nAt each minibatch, the actual protected labels are replaced with smoothed proxy labels. All debiasing methods will be on the proxy labels in the later process.\n\nDuring label smoothing, unsmoothed labels are used for voting to avoid inconsistency in smoothing decisions for other examples. We first collect the nearest neighbours of each instance and then do the voting for all of them.\n\nC.2 MODELS AND PARAMETER TUNING\n\nAll approaches presented in this paper share the same dataset-specific hyperparameters as the standard model. Hyperparameters are tuned using grid-search, in order to minimize distance to the optimal.\n\nHyperparameter\n\nnumber of epochs\n\npatience\n\nencoder\n\nembedding size\n\nhidden size\n\nSearch space\n\nMoji\n\nBios\n\nTrustPilot\n\nAdult\n\nCOMPAS\n\nBest assignment\n\n-\n\n-\n\n-\n\n-\n\n-\n\n100\n\n10\n\nDeepMoji (Felbo et al., 2017)\n\nBERT (Devlin et al., 2019)\n\n2304\n\n768\n\n768\n\n-\n\n101\n\n-\n\n447\n\n300\n\n2\n\nnumber of hidden layers\n\nchoice-integer[1, 3]\n\nbatch size\n\noptimizer\n\nloguniform-integer[64, 2048]\n\n1024\n\n1024\n\n1024\n\n512\n\n1024\n\n-\n\nAdam (Kingma & Ba, 2015)\n\nlearning rate\n\nloguniform-float[10−6, 10−1]\n\n3 × 10−5\n\n10−5\n\n3 × 10−5\n\n3 × 10−4\n\n10−4\n\nlearning rate scheduler\n\nLRS patience\n\nLRS reduction factor\n\nTrainable Parameter\n\n-\n\n-\n\n-\n\n-\n\nreduce on plateau\n\n2 epochs\n\n0.5\n\n782k\n\n329k\n\n323k\n\n122k\n\n225k\n\nTable 9: Search space of dataset-specific hyperparameters.\n\nAll debiasing methods in this paper does not introduce extra parameter to the main task model, and will not need to considered at the inference time. As such, we provide method-specific hyperparameters separately, and the search space for method-specific hyperparameters are shared across difference datasets.\n\n• GDCLA tunes the strength of the additional loss for minimizing absolute loss difference\n\nwithin each class. loguniform-float[10−6, 10−1], 40 times.\n\n• GDGLB tunes the strength of the additional loss for minimizing absolute loss difference.\n\nloguniform-float[10−5, 10−0], 40 times.\n\n• FairBatch tunes the adjustment rate for resampling probabilities. loguniform-float[10−4,\n\n10−0], 40 times.\n\n• Adv tunes the weights of unlearning adversaries in training. loguniform-float[10−2, 10+2],\n\n40 times.\n\n• SemiAdv tunes the weights of unlearning adversaries in training. loguniform-float[10−2,\n\n10+2], 40 times.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nAttribute\n\nVanilla GDCLA GDGLB FairBatch Adv SemiAdv ARL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nMoji\n\nBios\n\nTrustPilot\n\nAdult\n\nCOMPAS\n\nR\n\n58 55 54 54 47 51 47 148 148 179\n\nG\n\n47 131 121 46 54 57 71 35 35 49\n\nE\n\n37 131 116 48 48 53 73 37 37 50\n\nG×E\n\nG\n\nA\n\nC\n\nG×A×C\n\nG\n\nR\n\nG×R\n\nG\n\nR\n\nG×R\n\n39 149 157 47 41 42 40 40 38 41\n\n20 43 34 26 29 30 27 31 32 47\n\n20 36 33 26 28 29 25 33 33 46\n\n20 36 33 27 28 30 27 34 34 46\n\n18 42 45 24 23 25 20 33 32 36\n\n25 26 27 24 23 24 22 51 49 62\n\n17 26 28 21 29 20 21 55 54 61\n\n22 27 28 21 29 29 23 54 54 68\n\n9 10 10 11 11 11 11 15 15 15\n\n10 11 12 10 9\n11 11 16 16 16\n\n10 12 11 11 9\n9 12 18 17 17\n\nTable 10: Average computational budget, measured in seconds.\n\n• ARL tunes the learning rate of learning adversaries in training.\n\n10+2], 40 times.\n\nloguniform-float[10−4,\n\n• ULPL methods tunes the k from 1 to 15, and p-norm from 2 to 6.\n\nNotice that, this paper report the AUC-PFC, which eliminate the requirement for model selection, i.e., there is no best-found trade-off hyperparameters w.r.t. bias mitigation.\n\nC.3 COMPUTATIONAL BUDGET\n\nTable 10 shows average GPU time of model training. Noticing that debiasing components will not be used for inference, i.e., different methods have identical inference cost.\n\nC.4 PARAMETER TUNING FOR LABEL SMOOTHING\n\nFor Bios, class-specific neighbourhood smoothing degrades to naive proxy labels when there is only a small number of instances in a particular class. For example, there are 28 distinct target classes in the Bios dataset, with a highly skewed distribution. As such, there can be only one instance per target class in a minibatch, and the neighbourhood smoothing does not work in this case.\n\nFor TrustPilot, we hypothesise that it is due to the leakage of protected information being very low, and accordingly the neighbourhoods of instances being noisy.\n\nThe selection of k for label smoothing. As observed in Section 5.2, the optimal value of k varies greatly across datasets and debiasing methods, and in the context of this paper, we deal with this through a simple grid search over different values of k, which is computationally expensive.\n\nAlthough we do not currently have an algorithm for efficiently optimizing k at this time, we have observed that the value of k is positively correlated with model leakage and unfairness. Therefore, we could start tuning the value of k from a large value if the model is significantly biased, as the instances from the same protected group are likely to be close to each other. Otherwise, we can use the proxy labels without smoothing if the results are reasonably fair.\n\nD THEORETICAL JUSTIFICATION\n\nD.1 FROM EMPIRICAL LOSSES TO UTILITY METRICS\n\nFor illustration purposes, we assume binary settings for both target class and protected attribute labels. In Section 3.3, we have shown that the proposed method can be used to improve the equal opportunity fairness.\n\nDemographic parity (DP) For DP fairness, the predictions are expected to be independent from protected attributes (ˆy ⊥ z), and the fairness is satisfied if the differences in positive prediction rate\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nbetween demographic groups are zero: p(ˆy = 1|z = 0) = p(ˆy = 1|z = 1). Thus,\n\np(ˆy = 1, y = 0|z = 0) + p(ˆy = 1, y = 1|z = 0) =p(ˆy = 1, y = 0|z = 1) + p(ˆy = 1, y = 1|z = 1).\n\nSince p(ˆy = 0, y|z) + p(ˆy = 1, y|z) = 1, ∀y, z, by replacing p(ˆy = 1, y = 0|z) with 1 − p(ˆy = 0, y = 0|z),\n\n(1 − p(ˆy = 0, y = 0|z = 0)) + p(ˆy = 1, y = 1|z = 0) =(1 − p(ˆy = 0, y = 0|z = 1)) + p(ˆy = 1, y = 1|z = 1).\n\nAn equivalent condition to the DP fairness is that\n\np(ˆy = 1, y = 1|z = 0) − p(ˆy = 0, y = 0|z = 0) =p(ˆy = 1, y = 1|z = 1) − p(ˆy = 0, y = 0|z = 1).\n\nA sufficient condition for DP is, both p(ˆy = 1, y = 1|z = 0) = p(ˆy = 1, y = 1|z = 1) and p(ˆy = 0, y = 0|z = 0) = p(ˆy = 0, y = 0|z = 1) are satisfied. Next, we show how to map the conditional joint probability to training losses. As for the y = 1, recall that, L1 is an unbiased estimator of − log(p(ˆy = 1|yi = 1) (Equation (1)),\n\nL1 = − log(p(ˆy = 1|yi = 1) L1 = − log(p(ˆy = 1|yi = 1) − log(p(yi = 1)) + log(p(yi = 1)) L1 = − log(p(ˆy = 1|yi = 1)p(yi = 1)) + log(p(yi = 1)) L1 = − log(p(ˆy = 1, yi = 1)) + log(p(yi = 1))\n\nBy substituting the joint probability with losses,\n\np(ˆy = 1, y = 1|z = 0) = p(ˆy = 1, y = 1|z = 1) − log(p(ˆy = 1, y = 1|z = 0)) = − log(p(ˆy = 1, y = 1|z = 1)) L1,0 − log(p(yi = 1|zi = 0)) = L1,1 − log(p(yi = 1|zi = 1))\n\nL1,0 − L1,1 = log(\n\np(yi = 1|zi = 0) p(yi = 1|zi = 1)\n\n)\n\nSimilarly, L0 is an unbiased estimator of − log(p(ˆy = 0, yi = 0) + log(p(yi = 0)), and the DP condition for y = 0, p(ˆy = 1, y = 1|z = 0) = p(ˆy = 1, y = 1|z = 1), is equivalent to L0,0 − L0,1 = log( p(yi=0|zi=0)\n\np(yi=0|zi=1) ).\n\nNotice that p(y|z), ∀y, z are dataset-specific constant numbers, and if p(y|z = 0) = p(y|z = 1), ∀y, the DP conditions are identical to Equalized Odds fairness (Hardt et al., 2016), and can be approximated by L1,0 = L1,1 and L0,0 = L0,1. Last but not least, recall that the nature of DP assumes y and z are independent, therefore, p(y|z = 0) − p(y|z = 1) ≈ 0, ∀y generally holds when the DP fairness is desired.\n\nConfusion-matrix based metrics So far, we have shown that minimizing loss differences can approximate the optimization of the two most wildly used notions of fairness: EO and DP fairness. Since model predictions and target labels are observed during training, such approximation can also be applied to other confusion-matrix-based metrics. For example, the cross-entropy loss of instances w.r.t. predictions as 0 and 1 are approximations of the positive predictive value (p(y = 1|ˆy = 1)) and negative predictive value (p(y = 0|ˆy = 0)), respectively.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nD.2 OPTIMIZING FAIRNESS WITH PROXY LABELS\n\nBias mitigation for different fairness criteria Overall, ULPL only assigns proxy labels to training instances, and the optimization for fairness is achieved using existing supervised debiasing methods, by learning uniform representations across proxy groups (such as Adv) or minimizing loss disparities in training (such as GDCLA and GDGLB). Based on ULPL, different fairness criteria can be optimized by employing different variants of a particular debiasing method. Taking Adv as an example, a discriminator is trained to recover the protected information from hidden representations, and the main task model is optimized to remove protected information from hidden representations through unlearning the discriminator. By doing so, the hidden representations and corresponding predictions are expected to be independent of the protected attribute, ensuring DP fairness. To adopt Adv for EO fairness, the discriminator takes target labels into consideration, e.g. training a specific discriminator for instances with positive target class only, and the removal of protected information is then class-dependent, aligning with the definition of EO fairness.\n\nE ADDITIONAL RESULTS\n\nE.1 PROXY LABEL ASSIGNMENT – MOJI\n\nSentiment Race\n\nP (z′ = 1)\n\nPPR↑\n\nTPR↑\n\nFPR↓\n\nPPV↑\n\nNPV↓\n\nsad\n\nhappy\n\nSAE AAE\n\nSAE AAE\n\n17.0 ± 1.2 68.5 ± 1.7\n\n79.9 ± 0.8 14.3 ± 0.7\n\n91.2 ± 0.7 45.7 ± 2.6\n\n35.0 ± 3.1 6.4 ± 0.8\n\n91.3 ± 0.7 64.2 ± 3.2\n\n64.8 ± 2.2 87.3 ± 0.6\n\n55.3 ± 3.0 15.8 ± 2.3\n\n20.1 ± 0.8 85.7 ± 0.7\n\n65.0 ± 3.1 93.6 ± 0.8\n\n8.8 ± 0.7 54.3 ± 2.6\n\n64.8 ± 2.2 87.3 ± 0.6\n\n91.3 ± 0.7 64.2 ± 3.2\n\nTable 11: Proxy label assignment without smoothing and evaluations for the Vanilla model over Moji. Evaluation results ± standard deviation (%) are averaged over 5 runs with different random seeds. ± P (z′ = 1) refers to the proportion of instances being assigned with 1, indicating worseperformed groups. Evaluating metrics include: (1) positive predictive rate (PPR), corresponding to the demographic parity fairness (Blodgett et al., 2016), (2) true positive rate (TPR) and false positive rate (FPR), corresponding to equalized odds and equal opportunity fairness (Hardt et al., 2016), and (3) positive predictive value (PPV) and negative predictive value (NPV), corresponding to test fairness (Chouldechova, 2017).\n\nWe first investigate if the ULPL labels are meaningful through the lens of training examples in the Moji dataset. Table 11 presents the results of the Vanilla model.\n\nIt can be seen that, AAE tweets are more likely to be classified as happy, while SAE tweets are more likely to be classified as sad, resulting in consistent trend in gaps with respect to PPR, TPR, FPR, PPV, and NPV. Based on loss-disparities, AAE instances with sad target labels are more possible to be assigned with z′ = 1 (68.5% vs. 17% for SAE and AAE, respectively), encouraging debiasing methods to focus more on sad-AAE instances in training. Similarly, happy-SAE instances are more likely to be assigned with z′ = 1, indicating that happy-SAE are upweighted in training.\n\nFor the dataset distribution perspective of view, as introduced in Appendix A.1, Moji is balanced with respect to both sentiment and ethnicity but skewed in terms of sentiment–ethnicity combinations (40% happy-AAE, 10% happy-SAE, 10% sad-AAE, and 40% sad-SAE, respectively), which is closely related to the DL assignments that minority groups within each target class are assigned with z′ = 1. I.e., our proposed proxy label differentiates minority groups with majority groups within each target class without observing demographic labels.\n\nE.2 PROTECTED LABEL PREDICTABILITY AFTER DEBASING\n\nNeighbour smoothing requires protected information in hidden representations during the whole training, requiring encoded protected information in hidden representations. Han et al. (2021b) show that although supervised debiasing methods have shown success in reducing performance disparities in downstream tasks, the predictability of protected attributes in debiased hidden representations is\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Predictability after debiasing.\n\nstill well above the ideal value. We take ULPL+GDCLA as an example and explore the protected label predictability across different debiased models (i.e., different trade-offs).\n\nAs seen from Figure 4, fairness scores (the blue line) improve at the cost of performance. However, the predictability of protected labels is quite stable at a high level, indicating that protected information is still encoded in debiased models, and our proposed neighbourhood smoothing method is robust to bias mitigation.\n\nE.3 EFFECTIVENESS OF THE NEIGHBOUR SMOOTHING\n\nIn smoothing z′ labels, we hypothesis that the nearest neighbours of an instance are likely to from the same protected group. Except the instance itself, the remaining nearest neighbours are essentially the results of a standard k-nearest-neighbour (k-NN) model. Therefore, we perform analysis based on standard k-NN models, and investigate if the remaining nearest neighbours are helpful for label smoothing, i.e., from the same protected group as the target instance.\n\nE.4 OTHER FAIRNESS METRICS, DP\n\nWe investigate the robustness of ULPL methods to other notions of fairness. For illustration purposes, we focus on demographic parity fairness (DP) (Blodgett et al., 2016) in this experiment, which requires model predictions to be independent with protected attributes. Again, we aggregate accuracy and DP fairness trade-offs as the AUC-PFC scores. Since DP is sensitive to class imbalance and there is no standard way of generalizing DP to multi-class classification tasks, we only conduct experiments over binary classification tasks, including Moji, Adult, and COMPAS. Table 12 shows results w.r.t. demographic parity fairness.\n\nTrends for different methods are similar to the results of equal opportunity fairness, indicating that debasing methods are robust to different fairness metrics.\n\nDataset\n\nAttribute\n\nVanilla\n\nGDCLA GDGLB FairBatch Adv SemiAdv\n\nARL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nMoji\n\nR\n\nAdult\n\nCOMPAS\n\nG\n\nR\n\nG×R\n\nG\n\nR\n\nG×R\n\n0.173\n\n0.087\n\n0.076\n\n0.073\n\n0.109\n\n0.097\n\n0.094\n\n0.253 0.233 0.254 0.254 0.255\n\n0.197 0.214 0.192 0.186\n\n0.091 0.091 0.091 0.091 0.090\n\n0.092 0.090 0.090 0.091\n\n0.081 0.081 0.079 0.077 0.077\n\n0.081 0.082 0.081 0.083\n\n0.079 0.077 0.073 0.074 0.074\n\n0.078 0.080 0.078 0.080\n\n0.121 0.121 0.117 0.117 0.119\n\n0.120 0.123 0.124 0.122\n\n0.094 0.098 0.103 0.094 0.093\n\n0.092 0.101 0.100 0.099\n\n0.085 0.094 0.094 0.945 0.945\n\n0.092 0.095 0.097 0.092\n\nTable 12: AUC-PFC based on demographic parity fairness.\n\n21\n\n3HUIRUPDQFH3UHGLFWDELOLW\\RI]N)DLUQHVVPublished as a conference paper at ICLR 2023\n\nDataset\n\nS\n\nT\n\nVanilla\n\nGDCLA\n\nFairBatch SemiAdv\n\nARL ULPL+GDCLA\n\nBios\n\nG E −0.0011 E\n\n0.0003 G −0.0041 −0.0131\n\nTrustPilot\n\n0.0025 0.0047\n\nG A 0.0021 G C 0.0012 G −0.0017 −0.0032 A\nA C\n0.0018 −0.0009 C G −0.0041 −0.0023 C\n0.0011\n\nA −0.0027\n\nAdult\n\nCOMPAS\n\nG R R\n\n0.0053 −0.0098 G −0.0023 −0.0010\n\nG R −0.0012 −0.0036 R\n0.0058 −0.0026\n\nG\n\n−0.0054 −0.0166\n\n−0.0007 −0.0010 −0.0006 −0.0001 −0.0001 0.0011\n\n−0.0045 −0.0040\n\n−0.0036 0.0025\n\n−0.0021 0.0018 −0.0188 −0.0028\n\n−0.0018 −0.0018 −0.0015 0.0004 0.0009 0.0001 −0.0000 0.0012 −0.0003 −0.0007 −0.0008 −0.0023\n\n0.0037 −0.0080 −0.0033 −0.0012\n\n0.0002 −0.0014 −0.0064 −0.0083\n\nAverage\n\n0.0002 −0.0026\n\n−0.0027\n\n−0.0026 −0.0019\n\n0.0002 −0.0120\n\n0.0006 0.0008 −0.0020 −0.0006 −0.0023 −0.0010\n\n−0.0060 −0.0020\n\n0.0011 −0.0035\n\n−0.0022\n\nTable 13: AUC-PFC score differences between debiasing w.r.t. target protected attributes (T), and source protected attributes (S). Larger numbers indicate better generalizations to unobserved protected groups.\n\nFigure 5: PFC of in-domain and cross-domain debiasing over the Bios–gender dataset.\n\nE.5 HOW DOES BIAS MITIGATION AFFECT FAIRNESS FOR UNOBSERVED GROUPS?\n\nProxy group labels (z′) are dynamically adjusted at each minibatch during training, which differs from fixed protected labels in supervised debiasing. As a result, supervised debiasing methods based on the observed protected attributes z mitigate biases for particular protected groups.\n\nWhile the proposed proxy label approaches focus on the group of instances that are underrepresented during training, which is expected to be more general than debiasing to a particular protected attribute. Figure 5 demonstrates the difference in AUC-PFC scores between in-domain debiasing and cross-domain debiasing. For each debiasing method, we train the debiased model and conduct model selections based on the source protected attribute (Economy). The trained models are then evaluated w.r.t. the target unobserved protected attribute (Gender). The ability to generalize to unobserved protected attributes is measured as the difference between in-domain and cross-domain AUC-PFC.\n\nTable 13 summaries AUC-PFC differences across difference datasets w.r.t. a subset of debiasing methods. Overall, Vanilla shows the best generalization to unobserved protected attributes, and unsupervised debiasing methods are better than supervised and semi-supervised debiasing methods.\n\nIntuitively, the Vanilla model and unsupervised debiasing methods (ARL and ULPL+GDCLA) are agnostic to cross-attribute generalization, and their difference should be close to 0 which is clearly not the case. The AUC-PFC decrease of these methods is caused by the model selection, i.e., although the training process is identical, different models may be selection based on their own pro-\n\n22\n\n0.30.40.50.60.70.8Performance0.8250.8500.8750.9000.9250.9500.9751.000FairnessModelsGDCLAz'+GDCLAAttributeEGPublished as a conference paper at ICLR 2023\n\ntected attributes. As shown in Figure 5, the selected model of ULPL+GDCLA based on economy labels at accuracy around 0.6 is not a Pareto point for gender, confirming that decreases in AUC-PFC of Vanilla and unsupervised debiasing methods are caused by cross-domain model selection.\n\nE.6 THE APPLICATION OF ULPL TO DIFFERENT DEBIASING APPROACHES\n\nGDCLA are representatives of debiasing methods that directly optimize loss parity. In particular, the training objective of GDCLA is:\n\nLGDCLA = L + λGDCLA\n\n(cid:88)\n\n(cid:88)\n\nc\n\ng\n\n|Lc,g − Lc|,\n\nwhere |Lc,g − Lc| is optimized to achieve better fairness. Since Lc,g, Lc, and L are average losses, their magnitude are irrelevant to subset sizes (nc,g, nc, and n, respectively), which in turn applies the same strength of fairness regularization to all subset of instances Dc,g, ∀c, g. In other words, GDCLA ignores the influence of group size in bias mitigation, resulting in robustness to imbalanced class distributions.\n\nThere is a perfect alignment between ULPL and GDCLA in the sense that the proxy group z′ = 1 will always be upweighted during the optimization of GDCLA. I.e., ULPL+GDCLA reduces the loss disparities across instances within each target class, which in turn improves the lower bound of group-wise fairness, especially for the EO fairness. As a result of the consistency, we observe that ULPL+GDCLA outperforms other unsupervised methods.\n\nGDGLB is a variant of GDCLA, and they can only be differentiated by the way of incorporating fairness regularization:\n\nLGDGLB = L + λGDGLB\n\n(cid:88)\n\n(cid:88)\n\n|Lc,g − L|,\n\nc\n\ng\n\nwhere the average loss in the regularization term is based on all instances (L), differing from the average loss within each target class (Lc) for GDCLA. As a result, GDGLB additionally encourages the performance parity across target class, which is typically known as long-tail learning. However, ULPL+GDGLB could potentially lead to worse results for better-performed target classes. For example, for ith target class, assuming that loss differences have been minimized (i.e., Li,z′=0 ≈ Li,z′=1), it is possible that all instances with target label y = i will be under-fitted if Li > L.\n\nAdv represents a different family of debiasing methods which aims at learning fair hidden representations. The training objective of Adv includes the mutual information (M I) to the training objective in addition to standard loss:\n\nLAdv = L + M I(z, h), where the h = e(x) is the hidden representation of input x extracted from the encoder e. By minimizing the M I objective, we expected the learned hidden representations h are orthogonal to protected attributes z.\n\nIn practice, M I(z, h) is expressed by the combination of marginal entropy (H(z)) and conditional entropy (H(z|h)): M I(z, h) = H(z) − H(z|h), where H(z) is a constant number and can be ignored in the optimization, and H(z|h) is estimated by an adversary (d) that is trained to identify the protected attributes (ˆz = d(h)).\n\nThe key step of Adv is the training of d, i.e., if d can effectively recover z from h. One problem associated with ULPL is that the mapping from proxy labels to ground truth labels is class-specific, making it harder for the recovering. Therefore, although the adversaries are non-linear classifiers, the effectiveness of ULPL+Adv is not as good as other ULPL +∗ methods as shown in Table 1.\n\nF FULL RESULTS\n\nIn addition to AUC-PFC scores, we present PFC and evaluation results for each dataset. We investigate two different selection criteria and report the evaluation results over the development and test sets. Specifically, we conduct model selection over the development set based on: (1) maximum fairness within a performance trade-off threshold of 5% (F@P−5%); and (2) maximum fairness within a performance trade-off threshold of 10% (F@P−10%).\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nSelection\n\nMethod\n\nPerformance\n\nFairness\n\nPerformance\n\nFairness\n\nTest Set\n\nDevelopment Set\n\nF@P-5%\n\nF@P-10%\n\nVanilla GDCLA GDGLB FairBatch Adv SemiAdv ARL ULPL+RL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\nVanilla GDCLA GDGLB FairBatch Adv SemiAdv ARL ULPL+GDCLA ULPL+GDGLB ULPL+Adv\n\n71.1 ± 1.1 74.2 ± 0.3 75.9 ± 0.4 75.4 ± 0.4 75.3 ± 0.4 75.7 ± 0.2 70.7 ± 1.0 66.3 ± 2.0 66.7 ± 2.6 72.6 ± 0.4 67.8 ± 5.6\n\n71.1 ± 1.1 74.2 ± 0.3 75.9 ± 0.4 75.4 ± 0.4 75.3 ± 0.4 75.7 ± 0.2 63.7 ± 3.8 64.6 ± 3.3 72.6 ± 0.4 67.8 ± 5.6\n\n63.6 ± 1.3 92.9 ± 1.4 77.6 ± 1.3 90.3 ± 0.8 89.7 ± 1.6 90.1 ± 0.5 78.5 ± 5.5 85.7 ± 4.9 88.0 ± 7.0 65.1 ± 1.6 69.4 ± 9.2\n\n63.6 ± 1.3 92.9 ± 1.4 77.6 ± 1.3 90.3 ± 0.8 89.7 ± 1.6 90.1 ± 0.5 84.6 ± 4.2 92.5 ± 3.0 65.1 ± 1.6 69.4 ± 9.2\n\n71.4 ± 1.2 73.7 ± 0.3 75.2 ± 0.2 74.8 ± 0.2 74.9 ± 0.3 74.9 ± 0.4 69.2 ± 1.0 66.1 ± 2.1 66.2 ± 2.4 72.5 ± 0.6 68.2 ± 5.5\n\n71.4 ± 1.2 73.7 ± 0.3 75.2 ± 0.2 74.8 ± 0.2 74.9 ± 0.3 74.9 ± 0.4 63.0 ± 3.9 64.8 ± 2.2 72.5 ± 0.6 68.2 ± 5.5\n\n65.9 ± 1.2 94.5 ± 1.0 79.7 ± 1.6 90.9 ± 0.7 91.0 ± 0.8 91.1 ± 0.5 78.1 ± 5.7 86.7 ± 4.6 88.8 ± 6.8 66.9 ± 1.3 71.2 ± 8.9\n\n65.9 ± 1.2 94.5 ± 1.0 79.7 ± 1.6 90.9 ± 0.7 91.0 ± 0.8 91.1 ± 0.5 85.9 ± 3.5 92.6 ± 2.4 66.9 ± 1.3 71.2 ± 8.9\n\nTable 14: Evaluation results ± standard deviation (%) of selected models over the Moji dataset.\n\nFor the demonstration purpose, here we present the results for Moji. The full disaggregated results of 15 settings can also be seen at https://github.com/HanXudong/An_Unsupervised_ Locality-based_Method_for_Bias_Mitigation/blob/main/unsupervised_ bias_mitigation/NB_Appendix_indomain_tradeoffs_dispaly.ipynb.\n\nFigure 6: PFC over the Moji dataset.\n\nMoji Figure 6 shows performance–fairness trade-off curves (PFC) of each debiasing method. Table 14 summarises performance and equal opportunity fairness results w.r.t. two different selection criteria over the test set and dev set.\n\nThe areas under each PFC in Figure 6 correspond to a number in Table 1. Consistent with the Table 1, it can been from Figure 6 that GDCLA results in the best PFC. In addition, PFCs of Adv and SemiAdv are highly overlapped with each other, and their AUC–PFC scores are also identical in Table 1. Last but not least, ULPL+GDCLA is better than ARL most of the time in Figure 6, which is summarized as a 0.016 improvement in terms of AUC-PFC score in Table 1.\n\n24\n\n0.500.550.600.650.700.75Performance0.600.650.700.750.800.850.900.951.00FairnessModelsVanillaGDCLAGDGLBFairBatchAdvSemiAdvARLz'+RLz'+GDCLAz'+GDGLBz'+Adv",
  "translations": [
    "# Summary Of The Paper\n\nThis paper focuses on the problem of debiasing representations learned by encoder-decoder models when demographic information is missing. The authors propose Unsupervised Locality-based Proxy Label assignment (ULPL), which assigns a binary “proxy” label to each example based on whether the model incurs a larger or smaller than average loss on that example. Once examples have been assigned proxy labels, it is possible to apply existing debiasing methods. In order to be robust to noisy outliers, the paper also proposes smoothing the proxy label for each example via a majority vote among that example's neighbors in the encoded space. In experiments on 5 datasets (with various protected attributes in focus for each dataset) they show that ULPL leads to larger areas under the performance fairness curve than ARL and a baseline that does not employ fairness interventions; ULPL is claimed to lead to similar performance as some supervised methods. Analysis showing that proxy labels generated by ULPL are generally correlated with protected labels is provided, as well as some analysis of kNN smoothing and an experiment where groups are identities by cross-products of protected attributes.\n\n# Strength And Weaknesses\n\nStrengths:\n- The crux of the approach in this paper is to try to equalize training loss among all examples. The proposed method, ULPL, simply groups the examples into examples that experience lower than average loss, and those that experience higher.  Once labeled, other debiasing methods are used to equalize the loss. This seems reasonable as a way to improve fairness. The argument made in the paper is that examples with lower loss are likely to be of the same (under-represented) demographic group. While there is some empirical evidence that supports this claim in previous work, I find it weaker and somewhat inconsequential: if training loss is more evenly divided across examples (and training loss is a good proxy for performance) then fairness should improve across all groups.\n- This paper is clearly written and easy to follow.\n- Experiments are performed with 5 datasets. Significant analysis is also presented.\n\nWeaknesses:\n- The proposed label smoothing is not consistently effective. Table 3 shows that in 50% of experimental conditions, no label smoothing is better than label smoothing.\n- Label smoothing seems to be impossible to tune in realistic settings (in terms of the number of neighbors k). A standard approach of tuning k would be selecting k that yields the best results on the validation set. Since the metric being optimized is AUC-PFC, demographic group information is required to compute the metric. But the motivation of this paper includes the assumption that no demographic information is available.\n- The results are difficult to interpret. Table 1, which includes the main results, reports AUC-PFC, which characterizes the tradeoff between performance and fairness. I assume that the proposed method helps to improve fairness, but does it also improve performance? A more detailed characterization of how ULPL affects performance and fairness would help. Moreover, the difference between the proposed approach and other competing methods is often a few thousandths of a unit of area, which seems small, but as a reader it is difficult to be certain.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: while the paper is easy to follow, after reading I’m unsure of the following:\n- For each dataset, are the protected attributes in question always binary? How many values can each take on? This could help shed more light on the results.\n- For ULPL, is the loss per instance taken from the final model or summed over all training iterations?\n- What is the precise way to apply this? Train a model, then label the examples, assign new labels with knn, then debias? Or does this whole process happen at each iteration?\n- During neighborhood smoothing, are points relabeled with their smoothed label or do they retain their unsmoothed labeled? I imagine that an example’s smoothed label can be different from its unsmoothed label, leading it to vote differently in smoothing decisions for other examples. \n- In the sensitivity to k-NN hyperparamters section, what is the variable p?\n- In the same section: what is class-specific vs. ins-pecific smoothing?\n\nQuality:\n- ULPL seems like reasonable approach to debiasing without group information\n- The experiments seem to be technically sound, however the results do not seem very significant.\n- The writing is clear, but some of the details about the method and experimental methodology have been left out.\n\nNovelty:\n- The primary novelty is the idea to group examples into those that experience more/less than average loss (ULPL) as a proxy for protected groups when none are provided. As previously mentioned, while I don’t believe that examples with more loss will always (or even most of the time) correspond to specific protected groups, using existing debiasing methods to more evenly distribute the loss seems like a reasonable way to encourage parity across groups, when groups are unknown.\n- The idea to use k-NN smoothing is not particularly novel and moreover, as previously mentioned, it seems problematic from a hyperparameter-tuning perspective. Moreover, it only helps half of the time. \n- The problem of debiasing without group information is interesting, relevant, and has not received much attention.\n\nReproducibility:\n- A link to the code used to run the experiments is provided, so I believe the results could be replicated. However, I have not tried to replicate them myself.\n\nOther comments:\n- Please bold the best results in the table to make them easier to read.\n- Consider adding a citation to work on dataset cartography, which also groups points based on how difficult they are to predict correctly\n\n# Summary Of The Review\n\nThe paper is written clearly but some important details are missing. One of the methods proposed in this paper (label smoothing) does not consistently provide benefit and seems impossible to tune. The primary method proposed (ULPL) seems reasonable but doesn’t lead to significant gains.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces Unsupervised Locality-based Proxy Label assignment (ULPL), a novel method for mitigating demographic bias in machine learning models without requiring demographic labels. The methodology involves deriving binary proxy labels from model predictions based on training losses and employing a k-Nearest Neighbour (k-NN) approach for label smoothing to enhance robustness against outliers. Experimental results across five diverse datasets demonstrate that ULPL outperforms unsupervised baselines and achieves competitive results compared to state-of-the-art supervised methods, showcasing its effectiveness in ensuring fairness across various protected attribute combinations.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to bias mitigation in the absence of demographic labels, which is a significant contribution to the field of fairness in machine learning. The use of locality-based techniques for proxy label assignment is particularly noteworthy, as it addresses the shortcomings of previous unsupervised methods that often fail to ensure fair treatment across all groups. The experimental validation across multiple datasets adds credibility to the findings. However, a potential weakness lies in the reliance on the assumption that empirical losses can serve as effective proxies for utility metrics, which may not hold in all scenarios. Additionally, the paper could benefit from a more detailed discussion on the limitations and potential biases introduced by the proxy label assignment process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing quality is high, making complex ideas accessible. The novelty of the ULPL method is evident, and the approach of using unsupervised techniques to derive proxy labels is innovative. The reproducibility of the experiments is supported by the availability of code on GitHub and detailed documentation of the datasets and evaluation metrics used, which enhances the paper's reliability.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the domain of bias mitigation in machine learning by introducing an unsupervised method that does not rely on demographic labels. The experimental results affirm the method's efficacy and competitiveness, marking it as a valuable contribution to the field. However, the assumptions underlying the proxy label assignment process warrant further scrutiny to ensure broad applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper addresses the challenge of biased predictions in machine learning models trained on human behavioral data, particularly concerning demographic factors like gender and race. It introduces an unsupervised debiasing method called Unsupervised Locality-based Proxy Label assignment (ULPL), which uses data locality and model fit to mitigate bias without requiring access to demographically labeled training data. The authors provide a theoretical framework linking empirical risk minimization with fairness metrics, and extensive experiments show that ULPL outperforms existing unsupervised methods and achieves competitive results against state-of-the-art supervised approaches across multiple datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative use of unsupervised techniques to tackle bias mitigation, particularly in scenarios where labeled demographic data is unavailable. The robust experimental validation across five benchmark datasets showcases the method's effectiveness and the solid theoretical foundation provided enhances the credibility of the approach. However, the paper acknowledges limitations such as sensitivity to hyperparameters, potential predictability of protected attributes, and a lack of generalization to more complex intersectional demographic groups, which could affect the method's applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow from problem statement to methodology and results. The quality of the writing is high, making complex concepts accessible. The novelty of the proposed method is significant as it offers a new unsupervised approach to bias mitigation, contributing to ongoing discussions in fairness in machine learning. However, the reproducibility could be improved by providing more detailed guidelines on hyperparameter selection and the specific conditions under which the method performs best.\n\n# Summary Of The Review\nOverall, this paper presents a novel and effective approach to mitigating bias in machine learning models without relying on labeled demographic data. While the proposed ULPL method shows promising results across various datasets, attention to hyperparameter sensitivity and the implications of predictability in protected attributes are important considerations for future research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel unsupervised bias mitigation method called Unsupervised Locality-based Proxy Label assignment (ULPL), addressing the challenge of bias in machine learning models trained on human behavioral data, which often lack demographic labels. The methodology leverages data locality and model accuracy to generate binary proxy labels, facilitating supervised debiasing strategies that optimize fairness. Empirical results demonstrate that ULPL significantly improves fairness across five datasets in NLP and structured data classification, outperforming unsupervised baselines and achieving competitive performance compared to supervised methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to bias mitigation without the need for demographic labels, which are often unavailable. The use of k-NN for smoothing proxy labels effectively addresses the issue of noisy outliers and enhances the robustness of the method. However, a potential weakness is the assumption that local neighborhoods are representative of demographic groups, which may not always hold true in diverse datasets. Additionally, while the paper demonstrates improvements in fairness metrics, further exploration into the trade-offs between performance and fairness across a wider range of datasets could provide deeper insights.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The theoretical justification for the ULPL method is sound, providing a solid foundation for the claims made. The novelty of the approach is evident, particularly in its capacity to derive proxy labels without demographic data. The availability of source code enhances reproducibility, which is crucial for validating the findings and enabling other researchers to build upon this work.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in unsupervised bias mitigation methods by introducing the ULPL technique, which effectively balances fairness and performance without relying on demographic information. The results are promising, indicating that ULPL could be a valuable tool in the quest for fair machine learning practices.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel unsupervised debiasing method named Unsupervised Local Proxy Labels (ULPL) aimed at mitigating bias in machine learning models. The methodology employs data locality and proxy label assignment to achieve its objectives. Experimental results demonstrate that ULPL outperforms existing unsupervised baselines and achieves competitive performance compared to state-of-the-art supervised methods across five diverse datasets, showcasing its robustness across multiple fairness metrics. The authors provide a solid theoretical justification for their approach, linking empirical losses to fairness criteria effectively.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to debiasing using unsupervised techniques, which represents a significant advancement in the field of bias mitigation. The competitive performance of ULPL relative to supervised methods highlights its practical applicability. However, the method's effectiveness may be context-dependent, with potential limitations concerning the reliance on proxy labels that may not capture the complexities of demographic data fully. Additionally, while the method shows robustness across fairness metrics, it may inadvertently neglect certain dimensions of fairness, such as demographic parity. The complexity of implementation and potential misalignment of proxy labels also raise concerns regarding practicality and generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, providing clarity in presenting its methodology and findings. The theoretical foundation is robust, although the practical implications and complexities of implementation could be articulated more thoroughly. The novelty of the approach is clear, but the reproducibility may be hindered by the method's complexity and reliance on hyperparameter tuning. A more streamlined implementation or detailed guidelines for practitioners would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a novel and promising approach to unsupervised bias mitigation in machine learning, demonstrating competitive performance and robustness across various metrics. However, concerns regarding implementation complexity, generalizability, and the ethical implications of using proxy labels suggest areas for improvement.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "## ICLR Review\n\n### Summary Of The Paper\nThe paper titled \"EVERYBODY NEEDS GOOD NEIGHBOURS: A NOVEL UNSUPERVISED METHOD FOR BIAS MITIGATION THROUGH LOCALITY-BASED ENHANCEMENTS\" introduces an innovative methodology for bias mitigation in machine learning known as Localized Proxy Label (LPL). This unsupervised approach leverages local neighborhood information to derive proxy labels, avoiding the reliance on demographic data that characterizes many existing methods. The authors present a theoretical foundation for their method and validate its performance through extensive experiments on multiple datasets, demonstrating that LPL outperforms both unsupervised and semi-supervised baselines while achieving comparable results to state-of-the-art supervised techniques.\n\n### Strength And Weaknesses\nThe paper's strengths lie in its introduction of a locality-based strategy for bias mitigation, which significantly advances the field by addressing the limitations of previous methods that depend on demographic attributes or global statistics. The combination of rigorous theoretical justification and comprehensive empirical validation adds credibility to the proposed methodology. However, a notable weakness is the sensitivity of the LPL method to the choice of neighborhood size (k), which may complicate its application across different datasets and tasks. Additionally, while the empirical results are promising, the exploration of LPL's robustness in the context of highly imbalanced datasets remains limited.\n\n### Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to the reader. The quality of the writing and the presentation of results are high, which aids in understanding the significance of the contributions. The novelty of the LPL method is clear, as it offers a unique approach to bias mitigation that diverges from traditional reliance on demographic data. However, reproducibility could be enhanced by providing more detailed information regarding the hyperparameter tuning process, particularly concerning the neighborhood size.\n\n### Summary Of The Review\nOverall, this paper presents a substantial contribution to the field of bias mitigation in machine learning with its novel Localized Proxy Label method. By focusing on local context rather than demographic information, the authors address significant limitations in existing techniques and demonstrate strong empirical performance across various datasets. Future work should address the sensitivity to neighborhood size and explore the method's effectiveness in diverse contexts.\n\n### Correctness\n4\n\n### Technical Novelty And Significance\n5\n\n### Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel unsupervised locality-based method for adversarial training, termed Unsupervised Locality-based Adversarial Training (ULAT). This approach aims to enhance model robustness against adversarial attacks by identifying vulnerable regions in the data space without relying on labeled adversarial examples. The authors propose a meta-algorithm that optimizes the training process by focusing on the locality of data points, thereby achieving significant improvements in model performance across multiple benchmark datasets. The experimental results demonstrate that ULAT outperforms existing adversarial training methods, confirming its efficacy in enhancing model robustness.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach that merges locality-based techniques with adversarial training, contributing to the ongoing discourse on model robustness. The experimental validation across diverse datasets solidifies the claims made, showcasing the method's versatility. Additionally, the strong theoretical foundation provided offers a robust justification for the proposed methodology. However, weaknesses include the complexity associated with hyperparameter tuning, which may hinder practical implementations, as well as concerns regarding the scalability of the method to larger datasets and more complex models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings. The quality of the writing is high, with thorough explanations of the proposed algorithm and its implications. The novelty of the approach is noteworthy, as it introduces a fresh perspective to adversarial training by leveraging data locality. However, the reproducibility of the results may be challenged by the need for careful tuning of hyperparameters, which could vary across different implementations and datasets.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of adversarial training through the introduction of an unsupervised locality-based method. The robust experimental results and theoretical grounding support its effectiveness, although challenges related to implementation complexity and scalability warrant further investigation.\n\n# Correctness\n4 \n\n# Technical Novelty And Significance\n5 \n\n# Empirical Novelty And Significance\n4 ",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" introduces a novel meta-algorithm called Unsupervised Locality-based Proxy Label Assignment (ULPL) designed to mitigate bias in representation learning models. The methodology leverages locality to assign proxy labels without relying on demographic data, claiming to achieve unprecedented fairness across various machine learning applications. The authors present experimental results indicating that ULPL outperforms existing unsupervised methods and poses a challenge to state-of-the-art supervised techniques on multiple benchmark datasets, suggesting a paradigm shift in the approach to bias mitigation in AI.\n\n# Strength And Weaknesses\nThe key strength of the paper lies in its innovative introduction of ULPL, which has the potential to redefine the landscape of fairness in machine learning by eliminating the need for demographic data. The experimental validation indicates significant performance improvements over existing methods, which is compelling. However, the paper may overstate the generalizability and effectiveness of ULPL, especially the assertion that it could completely eradicate bias in AI systems. The claims regarding its superiority over all existing methods should be substantiated with comprehensive comparative analyses, considering potential limitations and edge cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting its findings in a clear and structured manner. The novelty of the ULPL approach is significant, particularly due to its unsupervised nature and the innovative use of locality for bias mitigation. However, the reproducibility of the results could be a concern, as the paper might benefit from more detailed methodological descriptions and access to code or datasets used for validation. The ambitious claims regarding performance improvements necessitate careful scrutiny and replication to establish credibility.\n\n# Summary Of The Review\nOverall, the paper presents a potentially transformative approach to bias mitigation in machine learning through the introduction of the ULPL algorithm. While the methodology is innovative and the findings appear promising, the claims of complete bias eradication and unparalleled performance should be cautiously interpreted. Further validation and clarity on reproducibility are necessary to fully assess the impact of this work on the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" by Baldwin, Han, and Cohn proposes a novel unsupervised debiasing method called Unsupervised Locality-based Proxy Label assignment (ULPL). This method aims to mitigate bias in machine learning models trained on human behavioral data, particularly in scenarios where demographic labels are unavailable. The authors demonstrate that ULPL outperforms various unsupervised baselines and achieves competitive performance compared to state-of-the-art supervised methods across multiple datasets, including Moji, Bios, TrustPilot, COMPAS, and Adult.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing the critical issue of bias in machine learning without relying on demographic information, which is often scarce. The introduction of neighborhood smoothing to enhance proxy label assignment is a notable innovation that improves the robustness of the method. However, the reliance on proxy labels derived from model predictions raises questions about potential biases inherent in those predictions. Additionally, while the results are promising, the paper would benefit from a more in-depth analysis of the limitations and potential failure cases of ULPL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents its methodology and findings. The experimental design is sound, and the results are convincingly reported, including revised AUC-PFC scores that strengthen the authors' claims. The proposed method demonstrates technical novelty in its approach to unsupervised bias mitigation. However, reproducibility could be enhanced by providing more details on the implementation specifics and datasets used, which would facilitate independent validation of the results.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of bias mitigation in machine learning through its innovative unsupervised approach. While the methodology and results are promising, further exploration of the limitations and reproducibility aspects would strengthen the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" introduces a novel approach to addressing bias in machine learning models by deriving proxy labels from hidden representations that correlate with protected attributes. The methodology leverages a k-nearest neighbors (k-NN) technique to assign these proxy labels based on locality, aiming to improve fairness without requiring explicit demographic information. The findings suggest that the proposed method can achieve a balance between model performance and fairness metrics, although it raises critical questions about the underlying assumptions and risks associated with proxy labeling.\n\n# Strength And Weaknesses\nThe paper's key strength lies in its innovative attempt to tackle bias mitigation using locality-based methods, which may provide a useful alternative to traditional supervised approaches. However, several weaknesses emerge upon closer examination. The reliance on the assumption that hidden representations correlate with protected attributes may not hold universally, risking the potential reinforcement of biases. The binary conversion of continuous loss values into proxy labels oversimplifies complex bias dynamics, and the validity of these proxy labels depends heavily on the representativeness of the chosen k-NN instances. Additionally, the method's generalizability to various demographic intersections and its ability to maintain a fair-performance trade-off remain questionable, particularly in diverse datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that outlines its contributions and methodology. However, the clarity of the assumptions made could be improved, as some foundational claims lack sufficient empirical support or discussion of their limitations. The novelty of the proposed method is notable, but its practical implementation and generalizability could benefit from more rigorous testing across different contexts and datasets. Reproducibility may be hindered by the reliance on specific choices of k and distance metrics, which could lead to varying results under different conditions.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting method for bias mitigation using unsupervised techniques, it is built on several questionable assumptions that may limit its effectiveness and generalizability. Addressing these assumptions is essential for enhancing the robustness and applicability of the proposed approach in real-world scenarios.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the Unsupervised Locality-based Proxy Label assignment (ULPL), an innovative meta-algorithm aimed at mitigating bias in machine learning models without relying on demographic labels. By leveraging data locality and model fit accuracy, ULPL assigns binary proxy labels to enhance fairness during model training. Experimental results across five benchmark datasets demonstrate that ULPL outperforms existing unsupervised methods and achieves competitive results compared to supervised approaches, confirming its efficacy in addressing bias in diverse demographic contexts.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to bias mitigation, circumventing the need for sensitive demographic information, which is often unavailable or unethical to use. The methodology is well-structured, utilizing a k-nearest-neighbor framework for neighborhood smoothing, which adds robustness to the label assignment process. However, a potential weakness is the reliance on proxy labels, which may not fully capture the underlying complexities of demographic biases. Additionally, while the experimental results are promising, further exploration of edge cases and a broader range of datasets could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and logically organized, making it accessible to readers with varying levels of familiarity with the topic. The quality of the experiments conducted is commendable, with a thorough analysis of results and a solid discussion of the implications. The novelty of the proposed method is significant, as it addresses a critical gap in the existing literature on bias mitigation. The authors also ensure reproducibility by providing access to code and datasets, which is essential for validating their results.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of bias mitigation in machine learning by introducing a novel unsupervised method that avoids the use of demographic labels. While the approach shows promise and is well-documented, further validation through additional datasets and scenarios would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses an important issue in the field of machine learning, specifically focusing on bias mitigation in predictive models. The authors propose a novel approach that aims to enhance model fairness by integrating adversarial training mechanisms with traditional debiasing techniques. The methodology involves a two-stage training process where an adversarial network is employed to identify and minimize biases in the main predictive model. The findings demonstrate that the proposed method significantly reduces bias in predictions across multiple datasets while maintaining competitive accuracy.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance of the Problem:** The issue of bias in machine learning models is increasingly critical, making this research highly relevant to current discussions in the field.\n2. **Innovation:** The integration of adversarial training with debiasing techniques presents a novel approach that could advance bias mitigation strategies.\n3. **Comprehensive Evaluation:** The authors conduct extensive experiments across diverse datasets, showcasing the robustness of their method under various conditions.\n4. **Clarity:** The paper is well-structured and articulates the methodology clearly, making it accessible to a broad audience.\n\n**Weaknesses:**\n1. **Lack of Detail:** The paper could benefit from a more in-depth discussion of the underlying mechanisms of the proposed adversarial training approach.\n2. **Comparative Analysis:** While results against several baselines are presented, a more thorough discussion of the implications of these comparisons could enhance understanding.\n3. **Generalizability:** The paper lacks a discussion on the generalizability of the approach to other tasks or domains not covered in the experiments.\n4. **Parameter Sensitivity:** There is limited exploration of how varying hyperparameters affect performance, which is crucial for practitioners aiming to apply the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with clear exposition of its methodology. The quality of the experiments and the logical structure of the paper contribute to its overall clarity. However, the reproducibility of the findings could be improved by providing more details on hyperparameter tuning and the effects of different configurations on model performance. The novelty lies in the combination of adversarial training with debiasing techniques, which has not been extensively explored in existing literature.\n\n# Summary Of The Review\nThis paper presents a significant contribution to the field of bias mitigation in machine learning by proposing an innovative method that combines adversarial training with traditional debiasing techniques. While it demonstrates strong empirical results and clarity in presentation, addressing the identified weaknesses could further enhance its impact and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" introduces a novel unsupervised method for debiasing representation learning models without relying on demographic labels. The proposed Unsupervised Locality-based Proxy Label assignment (ULPL) generates proxy labels based on model predictions to distinguish between well-modeled and poorly modeled instances. The authors demonstrate that this method enhances fairness in model predictions and outperforms existing unsupervised baselines, achieving competitive results compared to supervised debiasing techniques across various datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to bias mitigation in the absence of demographic data, addressing a significant challenge in fairness research. The use of proxy labels to improve model accuracy and fairness is a notable contribution that could have wide-ranging implications for the field. However, the paper could benefit from a more detailed analysis of potential limitations, such as the robustness of the proxy label generation process and its applicability to different types of models or datasets. Additionally, empirical results, while promising, would be strengthened by more extensive experiments and comparisons with a wider array of state-of-the-art methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally strong, with a well-structured presentation of the methodology and results. The authors effectively communicate the significance of their contributions to the field of bias mitigation. The quality of the writing is high, though some sections may benefit from additional clarification regarding the theoretical underpinnings of the ULPL method. In terms of novelty, the paper presents a unique approach that is likely to stimulate further research in unsupervised debiasing techniques. Reproducibility is supported by the description of the methodology, although the inclusion of code or supplementary material would enhance the ability of others to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of bias mitigation through its innovative use of unsupervised proxy labels. While the methodology and results are compelling, additional empirical validation and a deeper exploration of limitations would strengthen the paper's contributions. The approach has the potential to influence future research directions in fairness without necessitating demographic data.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" by Timothy Baldwin, Xudong Han, and Trevor Cohn addresses the issue of biased outputs in machine learning models trained on human behavioral data, which can be influenced by demographic factors. The authors propose an innovative unsupervised debiasing method called Unsupervised Locality-based Proxy Label assignment (ULPL), which assigns proxy labels based on model predictions and utilizes nearest-neighbor techniques to enhance fairness without relying on demographic labels. Experimental results demonstrate that ULPL outperforms existing unsupervised baselines and achieves competitive performance with supervised methods across five benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to bias mitigation without the need for demographic labels, making it applicable in scenarios where such data is unavailable. The methodology is well-defined, leveraging locality-based techniques to assign proxy labels, which appears to be a significant advancement over previous methods that often focused on specific demographic groups. However, potential weaknesses include the need for a thorough evaluation of the method’s generalizability across diverse datasets and tasks beyond those tested. Additionally, while the results are promising, the paper could benefit from a more detailed exploration of the limitations and potential biases introduced by the proxy label assignment process itself.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear structure that effectively communicates the problem, methodology, and findings. The quality of the experiments is high, and the comparative analysis provides a robust evaluation of the proposed method against various baselines. The novelty of the approach is significant, as it presents a new way to tackle bias in machine learning models without the reliance on sensitive demographic data. Reproducibility is somewhat supported by the clear delineation of the methodology, although additional details on the implementation and specific experimental setups would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of machine learning fairness by offering an innovative unsupervised method for bias mitigation that does not depend on demographic labels. The methodology is well-defined and the results are impressive, although further exploration of its limitations and reproducibility would strengthen future applications of this approach.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" addresses the critical issue of bias in machine learning models, proposing an innovative unsupervised debiasing approach known as Unsupervised Locality-Based Proxy Labels (ULPL). The authors detail a methodology that leverages locality and proxy labels to mitigate bias without the need for demographic information. Through extensive experiments on multiple datasets, the paper demonstrates that ULPL effectively reduces bias while maintaining model performance, presenting quantitative results that include metrics such as AUC-PFC and fairness evaluations.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear motivation and significance in addressing bias in a label-free manner, which is a notable gap in the existing literature. The methodology is well-articulated, providing a solid framework for understanding how proxy labels and neighborhood smoothing contribute to bias mitigation. The empirical validation across various datasets strengthens the findings and demonstrates robustness against different conditions. However, a potential weakness is the reliance on proxy labels, which may introduce some uncertainty regarding their effectiveness across diverse applications. Additionally, while the results are promising, further investigation into the interplay of hyperparameters and their influence on the method's performance could enhance the study.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with each section logically building upon the previous one. The quality of writing is high, making complex concepts accessible to readers in the field. The novelty of the approach lies in the unsupervised nature of the method and the innovative use of proxy labels in the context of locality-based bias mitigation. The authors have made efforts towards reproducibility by providing source code and detailed descriptions of the datasets used, which enhances the credibility and usability of their findings.\n\n# Summary Of The Review\nOverall, the paper presents a comprehensive and well-structured study on unsupervised bias mitigation, addressing a significant gap in the literature. The proposed method is novel and empirically validated, with clear contributions to the field of machine learning fairness. The clarity and reproducibility of the work further enhance its value to both researchers and practitioners.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the Unsupervised Locality-based Proxy Label assignment (ULPL) algorithm, which aims to mitigate bias in machine learning models without requiring demographic labels in training data. The methodology involves generating binary proxy labels based on training loss comparisons and employing a k-Nearest Neighbour (k-NN) approach to smooth these labels, ultimately enhancing fairness in model predictions. Empirical evaluations across five datasets demonstrate that ULPL generates proxy labels that align with latent demographic data while outpacing unsupervised benchmarks and maintaining competitive performance with state-of-the-art supervised methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the innovative approach of using locality-based proxy label assignment for bias mitigation, which addresses a significant gap in existing methodologies that require demographic information. The empirical results are compelling, showing consistent improvement over various datasets and robust performance against established techniques. However, a potential weakness lies in the reliance on k-NN for label smoothing, which may introduce sensitivity to the choice of k and could affect generalizability across different data distributions. Furthermore, the paper could benefit from a more thorough exploration of the limitations of the ULPL approach in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and written clearly, allowing for easy comprehension of the proposed method and its implications. The theoretical justification provided for the ULPL algorithm is sound, and the experimental results are presented in a manner that supports the claims made. The novelty of incorporating unsupervised debiasing through locality-based proxy labels is significant, contributing to the field of fairness in machine learning. However, details regarding the implementation and reproducibility of the experiments could be enhanced to facilitate further validation by other researchers.\n\n# Summary Of The Review\nOverall, this paper presents a substantial contribution to the field of bias mitigation in machine learning by introducing the ULPL algorithm that operates without demographic labels. The empirical results are strong and suggest that the proposed method is both effective and innovative. However, attention to the sensitivity of the k-NN component and additional information on reproducibility could strengthen the paper.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a novel methodology called Unsupervised Locality-based Proxy Label assignment (ULPL) aimed at mitigating bias in machine learning models without access to protected attributes. The methodology relies on the correlation between hidden representations and unobserved protected labels to assign proxy labels, which are then used to train models. The authors claim that ULPL outperforms unsupervised baselines and is competitive with supervised methods, demonstrating marginal improvements in performance across five benchmark datasets.\n\n# Strength And Weaknesses\nThe paper presents an interesting approach to bias mitigation, particularly the attempt to operate without protected attributes, which is a relevant concern in the field. However, the method's reliance on assumptions regarding hidden representations raises significant concerns about the potential introduction of bias in the proxy labels produced. The marginal improvements over existing methods may not justify the increased complexity of ULPL, especially given the sensitivity of k-nearest neighbor (k-NN) parameters and the limited generalizability of results due to the small number of datasets used. Furthermore, the lack of empirical validation with true demographic labels undermines the reliability of the method's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly. However, the novelty of the proposed approach is questionable, as it may not represent a substantial advancement over existing bias mitigation strategies. The reproducibility of results is also a concern, given the inherent uncertainties tied to parameter tuning in k-NN and the limited dataset variety. Additionally, the ethical implications associated with using proxy labels derived from potentially biased data are not adequately addressed.\n\n# Summary Of The Review\nWhile the paper introduces a novel methodology for bias mitigation, its reliance on questionable assumptions and the marginal improvements reported raise doubts about its practical applicability. The approach requires further refinement and validation to ensure its reliability and effectiveness in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel unsupervised method for bias mitigation called Unsupervised Locality-based Proxy Label assignment (ULPL). This innovative framework leverages data locality and model fit to assign proxy labels without the need for demographically-labeled training data, addressing a critical gap in bias mitigation efforts. The findings indicate that ULPL outperforms existing unsupervised baselines and achieves competitive results compared to state-of-the-art supervised methods, demonstrating its versatility across various datasets in natural language processing and structured data classification. The proxy labels generated correlate well with unknown demographic data, highlighting the method's potential for enhancing fairness in AI applications.\n\n# Strength And Weaknesses\nThe primary strength of this paper is its groundbreaking approach to unsupervised debiasing, which significantly advances the field by enabling bias mitigation without reliance on labeled demographic data. The method's performance across multiple tasks and datasets showcases its robustness and applicability. However, a potential weakness lies in the need for further exploration of the limits of the method, particularly regarding its scalability and effectiveness in real-world applications where data distribution may vary significantly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the proposed methodology and findings. The quality of the experiments is high, with a comprehensive evaluation against both unsupervised and supervised benchmarks. The novelty of the ULPL framework is significant, offering a fresh perspective on bias mitigation in machine learning. Reproducibility appears feasible, as the methodology is well-detailed; however, the paper would benefit from additional implementation details or code availability to aid in this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of bias mitigation in AI by introducing a novel, unsupervised approach that performs competitively with more traditional methods. Its emphasis on fairness without the need for demographic data is particularly impactful, potentially transforming how practitioners approach model development.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework called Unsupervised Locality-Based Proxy Label Assignment (ULPL) aimed at mitigating bias in machine learning models, particularly in scenarios where demographic labels are not available due to privacy concerns. The methodology is grounded in the idea that hidden representations in data can implicitly encode information about protected attributes, which can be leveraged through neighborhood-based approaches. Key findings indicate that ULPL successfully assigns proxy labels based on data locality, allowing for effective bias mitigation while optimizing fairness metrics without direct access to sensitive demographic information.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contribution to unsupervised bias mitigation, which is particularly relevant in today's context of data privacy. The ULPL framework is well-justified, utilizing the concept of neighborhood smoothing to derive proxy labels effectively. However, the paper could benefit from empirical validation across diverse datasets and models to strengthen its claims. The theoretical insights are promising, but the lack of extensive empirical results may limit the practical applicability of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a structured approach that logically presents its theoretical foundations and contributions. The quality of the theoretical analysis is high, showcasing a deep understanding of bias in machine learning and the role of proxy labels. The novelty of the ULPL framework is significant, as it presents a new avenue for addressing bias without demographic labels. However, the reproducibility could be enhanced by providing more details on implementation and experiments, which are currently lacking.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful theoretical contribution to the field of unsupervised bias mitigation in machine learning. While the ULPL framework is promising and well-motivated, further empirical validation is needed to assess its practical effectiveness in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces an unsupervised bias mitigation method termed Unsupervised Locality-based Proxy Label assignment (ULPL). The methodology employs proxy label assignment based on training losses to distinguish poorly modeled instances from well-modeled ones. A key feature of ULPL is the smoothing of proxy labels using k-Nearest-Neighbors (k-NN), which adjusts based on data locality with hyperparameter tuning conducted through grid search. The experiments are conducted across five different datasets—Moji, Bios, TrustPilot, Adult, and COMPAS—with performance assessed using clearly defined metrics such as AUC-PFC scores. The findings indicate that ULPL outperforms existing methods in both model performance and hyperparameter selection.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to unsupervised bias mitigation and the thorough experimental validation across multiple datasets. The use of k-NN for smoothing proxy labels is a noteworthy contribution that enhances the locality consideration in the bias mitigation process. However, the paper could benefit from a more extensive discussion on the broader implications of the proposed method for bias mitigation strategies. While the empirical results are solid, the technical details may overwhelm readers who are less familiar with the underlying concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with clear descriptions of methodologies and implementation details. The availability of code further enhances reproducibility and allows for validation of results. The novelty of the approach is evident; however, the focus on technical aspects might hinder accessibility for a broader audience. Overall, the quality of the writing is good, but the paper could be improved with a more comprehensive discussion on the implications of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in unsupervised bias mitigation with the introduction of ULPL. While the methodology is innovative and the results are promising, the paper could enhance its impact by addressing broader implications and ensuring accessibility for readers of varying expertise.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method called Unsupervised Locality-based Proxy Label assignment (ULPL), aimed at mitigating bias in unsupervised learning without relying on demographic labels. The authors claim that ULPL outperforms existing unsupervised methods and achieves competitive results compared to state-of-the-art supervised approaches. The findings are based on various performance metrics, highlighting ULPL's robustness across different scenarios. However, the comparisons drawn with other methods are questionable due to differences in assumptions and conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address bias in unsupervised learning without requiring demographic information, which is a relevant and important area of research. However, the paper's weaknesses stem from its selective comparisons with other methods, particularly those that operate under different conditions, such as supervised techniques. This leads to a potential misrepresentation of ULPL's effectiveness. The authors also fail to sufficiently credit foundational works in the area of bias mitigation, which undermines the novelty of their contributions. Furthermore, the lack of granularity in performance metrics makes it difficult to evaluate the true advantages of ULPL over existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the selective nature of its comparisons and the insufficient detail regarding the methodologies of previous works. While the proposed method is novel in its approach to unsupervised bias mitigation, the paper does not convincingly establish its significance relative to prior research. The reproducibility of the results may also be called into question due to a lack of detailed methodological descriptions that would allow other researchers to replicate the findings effectively.\n\n# Summary Of The Review\nIn summary, while the paper introduces an interesting approach to unsupervised bias mitigation with ULPL, the analysis presented is flawed due to selective comparisons and insufficient acknowledgment of prior work. This may mislead readers regarding the method's true contributions and effectiveness. Overall, the paper requires more rigorous evaluation and clearer contextualization of its claims.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" presents a novel approach to mitigating bias in machine learning models through an unsupervised locality-based method. The authors propose the Unsupervised Locality-Based Learning (ULBL) framework, which aims to identify and adjust for bias in data without requiring labeled instances. The methodology involves leveraging local neighborhoods in the feature space to re-weight samples, thereby improving model fairness. The findings demonstrate a significant reduction in bias across multiple datasets, showcasing the effectiveness of the proposed method compared to existing bias mitigation techniques.\n\n# Strength And Weaknesses\nThe key strength of this paper lies in its innovative approach to bias mitigation without the reliance on labeled data, which is a significant limitation in many existing methodologies. The empirical results are compelling, showing improved performance across various metrics related to fairness and accuracy. However, the paper could benefit from a more thorough analysis of the computational complexity of the proposed method. Additionally, while the results are promising, they are primarily based on synthetic datasets, and further validation on real-world datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is generally good, although there are some typographical and formatting inconsistencies that detract from its overall readability. The novelty of the proposed ULBL framework is significant, as it introduces a fresh perspective on bias mitigation that is not reliant on supervision. However, some sections could be more clearly articulated, particularly in the presentation of mathematical formulations and results, which would enhance reproducibility. The use of inconsistent variable definitions and lack of clear headers in results tables make it difficult to fully assess the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, this paper presents a novel and significant contribution to the field of bias mitigation in machine learning through an unsupervised approach. While the methodology and findings are promising, there is room for improvement in presentation and clarity. Further validation on real-world datasets would be beneficial to solidify the claims made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to unsupervised debiasing methods in machine learning. It employs a methodology that emphasizes the use of local neighborhoods, specifically k-nearest neighbors (k-NN), to mitigate bias in model predictions. The findings reveal that while the proposed method shows promise in improving fairness metrics such as equal opportunity and demographic parity, it is constrained by several limitations, including the reliance on proxy labels and a narrow evaluation scope limited to five benchmark datasets.\n\n# Strength And Weaknesses\nThe paper makes a significant contribution to the field of bias mitigation by highlighting the potential of unsupervised methods. However, it falls short in several areas. Firstly, the exploration of hybrid debiasing techniques that combine unsupervised and supervised methods is notably absent, which could enhance the efficacy of bias mitigation strategies. Additionally, the reliance on k-NN raises questions about the robustness of the methodology, as alternative proximity measures or clustering techniques could yield better performance. The limited dataset evaluation also restricts the generalizability of the findings, and the treatment of intersectional identities is insufficiently addressed. Furthermore, the authors do not adequately consider the noise and inaccuracies inherent in proxy labels, which may undermine the proposed debiasing approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, providing a clear overview of the proposed methodology and its intended contributions. However, the novelty of the approach is somewhat diminished by the lack of comprehensive evaluation and discussion on limitations. The reproducibility of the results could be improved by providing more detailed descriptions of the datasets, methodologies, and evaluation metrics used. Additionally, the paper would benefit from a comparative analysis with state-of-the-art baselines that incorporate demographic information, offering insights into the trade-offs involved in choosing unsupervised methods.\n\n# Summary Of The Review\nOverall, the paper offers valuable insights into unsupervised debiasing techniques, but it is hindered by several limitations, including a narrow focus on evaluation datasets, reliance on proxy labels, and insufficient exploration of hybrid methods. Addressing these issues could significantly enhance the paper's contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the Unsupervised Locality-based Proxy Label assignment (ULPL) method for mitigating bias in machine learning models. It employs a comprehensive experimental design utilizing five benchmark datasets (Moji, Bios, TrustPilot, Adult, and COMPAS) to evaluate ULPL against various baselines. The findings demonstrate that ULPL effectively balances performance and fairness, as measured by the Area Under Performance-Fairness Curve (AUC-PFC) and group-wise True Positive Rate (TPR) disparities. The authors emphasize the importance of statistical significance testing, hyperparameter sensitivity analysis, and the control of confounding variables to validate their results.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous statistical methodology and thorough evaluation framework, which includes multiple datasets and a well-defined set of fairness metrics. The use of AUC-PFC for assessing the trade-off between performance and fairness is particularly noteworthy, as it provides a robust means of comparison across methods. However, the paper acknowledges limitations related to the AUC-PFC calculations and potential misinterpretations of overlapping performance-fairness curves, which could affect the conclusions drawn from the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, findings, and implications. The quality of the statistical analyses is high, with adequate detail provided for reproducibility, including specific hyperparameter settings and dataset characteristics. The novelty of the ULPL method is significant, as it offers a new approach to proxy label assignment that addresses fairness issues in machine learning. The reproducibility statement reinforces the commitment to transparency, although the paper could benefit from clearer discussions on the limitations and assumptions of the proposed methods.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of bias mitigation in machine learning through the introduction of the ULPL method. The rigorous statistical approach and comprehensive evaluation strengthen its findings, though some limitations regarding the interpretation of results should be addressed.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper presents a novel approach for bias mitigation in machine learning models through the use of proxy labels. The authors propose a k-nearest neighbor (k-NN) based method that assigns proxy labels to instances in order to smooth out biases correlated with demographic groups. The findings suggest that the proposed method can improve fairness metrics like AUC-PFC, but the paper lacks a thorough exploration of the methodology's limitations and real-world applicability.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative approach to using proxy labels for bias mitigation, which is a relevant and important topic in the field of machine learning fairness. However, several weaknesses undermine its overall contribution. Notably, the paper does not address the limitations of proxy labels, such as their correlation with actual demographic labels under varying conditions. Additionally, the assumptions made regarding the stability of relationships between hidden representations and protected groups, as well as the impact of k-NN hyperparameters, are not sufficiently justified. The lack of empirical validation across diverse datasets and potential biases introduced by class imbalance further weaken the findings. Furthermore, the discussion on the ethical implications and practical applicability of the method is notably absent.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, but the quality of the discussion around limitations and assumptions could be improved. While the novelty of using proxy labels for bias mitigation is commendable, the lack of comprehensive comparisons with other unsupervised methods diminishes the significance of the findings. Reproducibility is a concern due to the insufficient exploration of different granularities for proxy label assignment and the absence of a detailed methodology for empirical validation.\n\n# Summary Of The Review\nOverall, while the paper introduces a promising method for bias mitigation using proxy labels, it suffers from several critical limitations that were not adequately addressed. The assumptions made by the authors and the lack of empirical validation in diverse scenarios raise questions about the robustness and applicability of the proposed method in real-world settings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Everybody Needs Good Neighbours: An Unsupervised Locality-Based Method for Bias Mitigation\" introduces a meta-algorithm called Unsupervised Locality-based Proxy Label assignment (ULPL) aimed at addressing bias in machine learning models without the need for demographic data. The authors argue that traditional methods require protected attributes, which are often unavailable, and propose an unsupervised approach that assigns binary proxy labels based on model predictions. Experimental results demonstrate that ULPL outperforms existing unsupervised baselines and is competitive with supervised methods, suggesting its effectiveness in mitigating bias.\n\n# Strength And Weaknesses\nWhile the paper presents a methodology that claims to improve upon existing bias mitigation techniques, it lacks significant novelty. The proposed ULPL method can be viewed as a rebranding of established clustering techniques applied to the context of bias mitigation. The findings, while empirically valid, do not introduce groundbreaking insights into the problem. The theoretical justifications provided by the authors appear to be superficial, and the reliance on common benchmark datasets suggests a missed opportunity for original contributions. However, the empirical results do provide a useful benchmark for future work in this area.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, making it accessible to readers familiar with bias mitigation in machine learning. The quality of the experiments is reasonable, and while the methodology is reproducible, it relies heavily on existing frameworks and datasets that do not offer new perspectives. The novelty of the contributions seems overstated, as the core concepts have been explored previously in the literature. The theoretical underpinnings do not provide compelling evidence for the claims made, leading to questions about the robustness of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a method for bias mitigation that lacks significant novelty and relies on established techniques. While it offers empirical results that suggest some effectiveness, the contributions do not sufficiently advance the field or provide deep theoretical insights. The authors may have overstated the significance of their findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents an unsupervised debiasing method, referred to as Unsupervised Locality-based Proxy Labels (ULPL), aimed at mitigating biases in machine learning models. The methodology leverages locality-based approaches to assign proxy labels dynamically during training, promoting fairness across various demographic groups. The findings demonstrate that ULPL effectively reduces bias as measured by traditional fairness metrics such as equal opportunity and demographic parity, while also suggesting potential for generalization to unobserved protected attributes.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to debiasing through unsupervised means, enabling broader applicability without the need for labeled data. However, the research could benefit from a more in-depth exploration of supervised techniques that might enhance the outcomes. Additionally, while the empirical analysis provides valuable insights, it relies heavily on conventional fairness metrics, which may not fully capture the complexities of intersectional identities. The theoretical framework could also be expanded to include multi-objective optimization strategies that balance fairness with performance more explicitly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with logical organization and sufficient detail regarding the methodology. However, the novelty of the approach could be strengthened by a more comprehensive theoretical justification. The reproducibility of the results is supported by the use of standard datasets and metrics, although further explanation on parameter sensitivity and hyperparameter tuning would enhance the understanding of model behavior and reliability.\n\n# Summary Of The Review\nOverall, the paper presents a promising and novel approach to debiasing in machine learning through an unsupervised method. While it achieves significant contributions in bias reduction, there remain areas for improvement, particularly in incorporating supervised techniques and enhancing the theoretical framework to address the multifaceted nature of bias.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel method called Unsupervised Locality-based Proxy Label assignment (ULPL) aimed at mitigating bias in machine learning models without the need for demographic labels. The methodology is extensively evaluated across five benchmark datasets: Moji, Bios, TrustPilot, Adult, and COMPAS. The findings demonstrate that ULPL substantially outperforms various unsupervised baselines and achieves competitive performance, particularly in terms of fairness, when compared to state-of-the-art supervised methods. Notably, the combination of ULPL with existing supervised techniques (e.g., ULPL+GDCLA) yields impressive results, indicating the method's effectiveness in addressing fairness concerns while maintaining accuracy across diverse datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to unsupervised bias mitigation, which addresses a significant problem in machine learning. The results consistently show that ULPL improves fairness metrics, such as the Area Under the Performance-Fairness Curve (AUC-PFC), while maintaining competitive performance levels, showcasing a successful balance between these two critical aspects. However, the paper could benefit from a more detailed exploration of the underlying mechanisms that contribute to the method's effectiveness, particularly regarding how ULPL recovers proxy labels that correlate with demographic data. Additionally, further analysis on the limitations of ULPL and potential failure cases in diverse scenarios would strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and results. The quality of the experiments is high, with thorough evaluations across multiple datasets. The novelty of the ULPL method is commendable, as it introduces a fresh perspective on unsupervised bias mitigation. Reproducibility is supported by the detailed explanation of the experimental setup; however, providing access to code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of bias mitigation in machine learning through the introduction of the ULPL method. The results indicate that ULPL is a robust solution that achieves a commendable balance between performance and fairness, making it a valuable contribution to the literature. Areas for improvement include deeper insights into the method's workings and greater emphasis on reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to [specific problem or area of research], proposing a [brief description of the methodology, e.g., new algorithm, model, or framework]. The authors conduct a series of experiments to validate their approach, demonstrating that it outperforms existing methods in terms of [specific metrics or outcomes]. Key findings indicate [summarize primary results, e.g., improvements in accuracy, efficiency, or robustness], highlighting the effectiveness of the proposed solution.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its comprehensive evaluation of the proposed method against several benchmarks, providing strong empirical evidence for its claims. Additionally, the innovative approach to [specific aspect] is a notable contribution to the field. However, the paper has weaknesses, including excessive jargon and acronyms that may hinder comprehension for a broader audience. Furthermore, some sections are overly lengthy, which can detract from the clarity and engagement of the narrative.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel contribution, the clarity of the writing could be improved. The abstract and introduction are dense and could benefit from simplification. The inconsistency in the formatting of figures, tables, and citations detracts from the overall quality. The reproducibility statement is adequate but could be more concise to allow readers to quickly grasp the essential points. Overall, the paper demonstrates sufficient novelty in its approach, but the clarity and presentation need attention.\n\n# Summary Of The Review\nOverall, the paper introduces a promising new methodology with significant empirical support. However, improvements in clarity, structure, and consistency would enhance its accessibility and impact. Addressing these issues will ensure that the contributions are more effectively communicated to a wider audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.223946821864487,
    -1.5511711149737608,
    -1.685842565897751,
    -1.7135806391672364,
    -1.877026188644962,
    -1.5684872560077523,
    -1.5670149662899147,
    -1.7577856242572236,
    -1.7415279565223716,
    -1.8038382724825355,
    -1.7091645763015,
    -1.3117799132429993,
    -1.4991570587006053,
    -1.6081957944135126,
    -1.697504205869822,
    -1.7205826561448,
    -1.8435813642617218,
    -1.7603063481176418,
    -1.691282458548807,
    -2.002825416512437,
    -1.7277384688941944,
    -1.5826254388925307,
    -1.6513264736823188,
    -1.8189577645281525,
    -1.8014469930303572,
    -1.7994575930956984,
    -1.8490990566730596,
    -1.690107959201481,
    -1.6122290173422331
  ],
  "logp_cond": [
    [
      0.0,
      -2.083925373342036,
      -2.123102672897608,
      -2.0857077043566274,
      -2.1299004839786595,
      -2.117459687671038,
      -2.1608315737230437,
      -2.123741907768121,
      -2.087991762178065,
      -2.1253633479810286,
      -2.102493390160794,
      -2.1552338810921743,
      -2.104191076896412,
      -2.1157926717811892,
      -2.1012270645130164,
      -2.0946364910718187,
      -2.1026514680843684,
      -2.114217456621322,
      -2.131143359480517,
      -2.085620918477512,
      -2.104508937322997,
      -2.15341111097198,
      -2.132609709326434,
      -2.1138419738885603,
      -2.1185662783322967,
      -2.109830665990832,
      -2.121509205837811,
      -2.1028510122717647,
      -2.154860369564896
    ],
    [
      -1.1555102977694318,
      0.0,
      -1.12752674149656,
      -1.1081533431651196,
      -1.1042621644556891,
      -1.2260959558128504,
      -1.2542856548016785,
      -1.1549812109977964,
      -1.0891937072651188,
      -1.1454446554608468,
      -1.1081238139778589,
      -1.284303921548693,
      -1.1590591053448527,
      -1.1028043374175385,
      -1.159871747249509,
      -1.0773110716325693,
      -1.1435090403361123,
      -1.1705951698754995,
      -1.1765334997675239,
      -1.1566450515111082,
      -1.1452467758596923,
      -1.2178583842099544,
      -1.2045570266759704,
      -1.142695042102619,
      -1.2154076113480103,
      -1.1298333933358242,
      -1.1878722232094752,
      -1.1516336923816752,
      -1.2547603656004969
    ],
    [
      -1.2945659531102334,
      -1.2259640369503961,
      0.0,
      -1.2316533529546683,
      -1.2041026862179371,
      -1.3232133153876042,
      -1.364583985770583,
      -1.2880913223828523,
      -1.1719124278027704,
      -1.286485524151472,
      -1.2229352671614275,
      -1.4105490473905875,
      -1.2519033066441814,
      -1.1692681511715424,
      -1.3245104613431378,
      -1.2072095464884691,
      -1.2555441414722535,
      -1.2088308171531743,
      -1.3182440356249037,
      -1.302017761864975,
      -1.2611314950519004,
      -1.359409022708837,
      -1.3274830199392336,
      -1.2322781530021103,
      -1.3405081623306965,
      -1.272336142037447,
      -1.2987697371643196,
      -1.2668991383072927,
      -1.3885796839107158
    ],
    [
      -1.2357266369854298,
      -1.147359275176928,
      -1.219162767343373,
      0.0,
      -1.2799337204841617,
      -1.2903018481208244,
      -1.4056470872700053,
      -1.310307074385605,
      -1.1860888548054371,
      -1.3329485207269711,
      -1.1282266009331954,
      -1.4145831976873666,
      -1.2379627589215434,
      -1.2314994016616525,
      -1.2513900869365908,
      -1.1872011327143759,
      -1.2259563370514408,
      -1.1674802525656227,
      -1.2599221762899409,
      -1.2152849273640107,
      -1.2525863696992514,
      -1.3999717225610524,
      -1.3427926759643658,
      -1.2952141506922796,
      -1.359858101328309,
      -1.2710588276658663,
      -1.3273947169156726,
      -1.2251095599277793,
      -1.4494993406814536
    ],
    [
      -1.505281247050734,
      -1.3873822168058279,
      -1.3889122206306055,
      -1.467478319620464,
      0.0,
      -1.5227051750297744,
      -1.5607429406030913,
      -1.477464711922672,
      -1.4199292642927985,
      -1.451623043368188,
      -1.4644224613423464,
      -1.599675793885687,
      -1.4711227140309289,
      -1.4682063695484007,
      -1.470871750951686,
      -1.4322610221655045,
      -1.4118124344914236,
      -1.477650899947192,
      -1.47888369620914,
      -1.4910808711373709,
      -1.4598433589008226,
      -1.5279990007074817,
      -1.4745411781008009,
      -1.4541803747995574,
      -1.4846162421847355,
      -1.4678609326407024,
      -1.4859366546832888,
      -1.4191451807703623,
      -1.5480282845606286
    ],
    [
      -1.351491247876109,
      -1.2383064179260157,
      -1.2704461072387643,
      -1.2619565325425326,
      -1.2432433067527104,
      0.0,
      -1.2762642934673047,
      -1.2331296241763086,
      -1.2414414149596777,
      -1.2737221869070043,
      -1.2501521690177677,
      -1.317673845127513,
      -1.237401574545332,
      -1.2203665368905796,
      -1.227971665134843,
      -1.2467180923626222,
      -1.2277070200043305,
      -1.2473265886723224,
      -1.2933140137321415,
      -1.2683874994730027,
      -1.2549997146249805,
      -1.286121429308856,
      -1.290768937464714,
      -1.3021305702881338,
      -1.2989318800401117,
      -1.2452778363355452,
      -1.2846412888751548,
      -1.252112935633651,
      -1.3509438851531919
    ],
    [
      -1.227862952266614,
      -1.1833574348685456,
      -1.1670428278276228,
      -1.1954939361449899,
      -1.163173580311997,
      -1.2058138631029154,
      0.0,
      -1.186807756435565,
      -1.2105021222488475,
      -1.1888131019087596,
      -1.1501751747868363,
      -1.210482726946286,
      -1.1918252267918887,
      -1.2154963758180521,
      -1.1909721830575781,
      -1.2232274042218738,
      -1.145278297844891,
      -1.198382149776626,
      -1.2106781443228125,
      -1.2093575015960498,
      -1.2007986204871406,
      -1.2279815061101698,
      -1.1990428355122613,
      -1.1902180570246887,
      -1.1905143442429578,
      -1.214679293392232,
      -1.1896059969680595,
      -1.2079741400891062,
      -1.230545704095751
    ],
    [
      -1.4578722350115634,
      -1.3487811627094435,
      -1.321734544181287,
      -1.3764317317463584,
      -1.3409282286028181,
      -1.3367078035895543,
      -1.4526867722042611,
      0.0,
      -1.2644739937626752,
      -1.297200826762514,
      -1.3394245256243886,
      -1.4997966987658127,
      -1.2814453984209553,
      -1.2472804414682537,
      -1.3164265964382846,
      -1.3420165010912044,
      -1.3673528379510687,
      -1.3419778999949934,
      -1.386233198102631,
      -1.4038157910121978,
      -1.3383367425582007,
      -1.350677119365997,
      -1.4609083191965404,
      -1.3771036821610227,
      -1.4395932430110745,
      -1.2351950498722102,
      -1.4370842840006757,
      -1.3697973318003207,
      -1.4812463237952453
    ],
    [
      -1.3214013762708008,
      -1.2520809644538673,
      -1.2537966492613304,
      -1.2196286075482123,
      -1.2800976893812601,
      -1.3786273625990166,
      -1.4790073667174128,
      -1.2559247176576938,
      0.0,
      -1.2887009449113231,
      -1.2720607244187445,
      -1.4528523462536542,
      -1.2162490021755745,
      -1.075506207718815,
      -1.2155665007766647,
      -1.2546017744523685,
      -1.3295802488580115,
      -1.2952926917681817,
      -1.367411349634119,
      -1.2047368111502914,
      -1.2832459383172905,
      -1.3177803393994807,
      -1.3988521517523624,
      -1.2150380800221197,
      -1.3943545788356477,
      -1.2036199120768638,
      -1.3956777823487383,
      -1.1515333597409492,
      -1.4401048481655856
    ],
    [
      -1.5221451822625645,
      -1.4450664830890796,
      -1.4675064993504618,
      -1.5015231873696533,
      -1.4770946437801091,
      -1.5116317704609215,
      -1.555922320380405,
      -1.4083907365644692,
      -1.4304565181564342,
      0.0,
      -1.5131738397404573,
      -1.5834673541621302,
      -1.461203881223527,
      -1.4327779525082078,
      -1.4574776334066997,
      -1.488142790962979,
      -1.4459899449775333,
      -1.522077239090874,
      -1.5060320299196945,
      -1.543949245621914,
      -1.4970658769378953,
      -1.4471351560559649,
      -1.4972416317259714,
      -1.5121881665808405,
      -1.4776901712758677,
      -1.4199271789481842,
      -1.5226236990401785,
      -1.5029883280779834,
      -1.5861036779232827
    ],
    [
      -1.144914390615426,
      -1.110473789730375,
      -1.196114978994749,
      -1.1113068517440694,
      -1.2245869868736852,
      -1.3357100444694747,
      -1.3665486511053553,
      -1.2169136751976046,
      -1.1550081949864015,
      -1.3063456513729865,
      0.0,
      -1.3553112925799313,
      -1.2070017057358924,
      -1.1815483140243066,
      -1.2071029284128707,
      -1.1963581031810053,
      -1.1748633507933348,
      -1.2038066149279687,
      -1.2099743447487383,
      -1.1949360400658455,
      -1.1814363086846529,
      -1.3458649541087364,
      -1.2752847359844321,
      -1.2321970487328904,
      -1.2968218026713267,
      -1.1624244276590534,
      -1.2422915979374713,
      -1.1949830699928177,
      -1.4074546568880517
    ],
    [
      -1.0951880889228356,
      -1.0764596659344483,
      -1.0736221149968486,
      -1.0837682694856814,
      -1.0592149134535718,
      -1.0803932168524308,
      -1.0806059447075176,
      -1.087568326678602,
      -1.0766102668132727,
      -1.0789072096232777,
      -1.0789426060850105,
      0.0,
      -1.0784794491120786,
      -1.089770676159459,
      -1.0595434339355794,
      -1.0887161826953773,
      -1.0565642035271159,
      -1.0673485125647229,
      -1.088501559618653,
      -1.040177765284151,
      -1.0561945343401917,
      -1.0749582546516512,
      -1.0442607646949982,
      -1.0534426463141293,
      -1.0518998325020457,
      -1.0882446498821503,
      -1.043145637063565,
      -1.0665946201904026,
      -1.0510174805269057
    ],
    [
      -1.160941738219203,
      -1.1050045221025602,
      -1.1342692052624526,
      -1.108004316097583,
      -1.1473893768363912,
      -1.1459928095141636,
      -1.2099059504831646,
      -1.0523933942519959,
      -1.036115571494734,
      -1.131837353842981,
      -1.0974544419280057,
      -1.2471100464348877,
      0.0,
      -1.0496174620863152,
      -1.0773944183746766,
      -1.1221866323839937,
      -1.1104154630036769,
      -1.0839131800494086,
      -1.1709518113230386,
      -1.0914969974987256,
      -1.0808930808408213,
      -1.1379227086755286,
      -1.1950287326380382,
      -1.1703877373734317,
      -1.1906997698691606,
      -1.0583872454567562,
      -1.157986305503368,
      -1.1253628706716519,
      -1.2306133135828279
    ],
    [
      -1.2679263815664275,
      -1.1470130032985986,
      -1.1254919316519827,
      -1.1881680394780803,
      -1.2023250859325845,
      -1.2539578519621182,
      -1.3471872928808233,
      -1.148139637066425,
      -0.9909529182370086,
      -1.1948908512095584,
      -1.189205150107134,
      -1.3312011900014813,
      -1.102936177213927,
      0.0,
      -1.1769351643746533,
      -1.1503527034069827,
      -1.212814610694425,
      -1.1960849758778682,
      -1.2646802605209135,
      -1.2658378006453328,
      -1.2416071281453263,
      -1.219035129905163,
      -1.3004114337406674,
      -1.230185371336538,
      -1.285067149343142,
      -1.1439180195307166,
      -1.2634389736988407,
      -1.2236690130808197,
      -1.3511930323377903
    ],
    [
      -1.2751623564427648,
      -1.2884144070244388,
      -1.3138682082011153,
      -1.2646896632528877,
      -1.2904227920391396,
      -1.3133632599516165,
      -1.3759849965453663,
      -1.2903318021835952,
      -1.1325603572806424,
      -1.291241373363783,
      -1.242916663806787,
      -1.3426881923149752,
      -1.2034849636489335,
      -1.2421351792800535,
      0.0,
      -1.304643876054544,
      -1.314581542392541,
      -1.313892269237912,
      -1.3228831021112497,
      -1.199134182089205,
      -1.2825469385925796,
      -1.2713170730311538,
      -1.3211529666637902,
      -1.2681785311796823,
      -1.2705407828906228,
      -1.2633459625268915,
      -1.2860131892918958,
      -1.2753018262112101,
      -1.363682669853027
    ],
    [
      -1.2817739086828308,
      -1.1996868283729674,
      -1.2675768176640603,
      -1.2419421059675202,
      -1.2933807119302119,
      -1.357977432181705,
      -1.4589812285692538,
      -1.2951339855178523,
      -1.2453917452060503,
      -1.3551078648991919,
      -1.265032317717913,
      -1.4494265223254919,
      -1.2731810501496383,
      -1.2351755162764744,
      -1.322585279092667,
      0.0,
      -1.3026009604284532,
      -1.277910261926528,
      -1.3451552498935826,
      -1.314756652912616,
      -1.3122423174923408,
      -1.4020790470930709,
      -1.3836907127420224,
      -1.333690686418959,
      -1.395371088699248,
      -1.2901261112539035,
      -1.3555965143740532,
      -1.2958257102477388,
      -1.4444960560049598
    ],
    [
      -1.41881455969275,
      -1.434310878662393,
      -1.4478416608342406,
      -1.3730722472412031,
      -1.4645031748777413,
      -1.4780081310007351,
      -1.558587876626819,
      -1.466995287138032,
      -1.4089529908071894,
      -1.447037167574189,
      -1.3884886895859228,
      -1.5605576910482,
      -1.430199758279791,
      -1.4475995839348101,
      -1.4708379792575106,
      -1.4212747710184377,
      0.0,
      -1.4126409424128286,
      -1.4637594111999055,
      -1.4337324417683743,
      -1.3751625183513532,
      -1.5739127366467374,
      -1.423475566096011,
      -1.4828052425644054,
      -1.4307464469138498,
      -1.4039393094195043,
      -1.4756520150916164,
      -1.3381193301000736,
      -1.6020380439262847
    ],
    [
      -1.353276243051459,
      -1.3536955464593259,
      -1.274743840742941,
      -1.2498764781318878,
      -1.3439820190711094,
      -1.4072629796206015,
      -1.428273454598861,
      -1.3590843020321242,
      -1.2890578120443914,
      -1.3808888872227008,
      -1.2767901165107816,
      -1.4386747775066946,
      -1.2900940901533462,
      -1.3006707409986034,
      -1.3582257288702464,
      -1.2761082212711974,
      -1.2938880358407636,
      0.0,
      -1.3585269741496857,
      -1.3441099782874366,
      -1.2789073828492583,
      -1.4170836858946045,
      -1.39589672534677,
      -1.3320711681391353,
      -1.4086315751865113,
      -1.3307413572954798,
      -1.3637838621822447,
      -1.2569864348501045,
      -1.4674751772071888
    ],
    [
      -1.2861797490598548,
      -1.2795735891431663,
      -1.2974008071145358,
      -1.2191153165236595,
      -1.2958914926963632,
      -1.3168342752246636,
      -1.3695729742826634,
      -1.2460785772626535,
      -1.2620921554498286,
      -1.2432326740507782,
      -1.194539732307932,
      -1.3533788466080676,
      -1.2614848751311118,
      -1.2713655906393801,
      -1.2307126811286042,
      -1.332733974464308,
      -1.203128269685091,
      -1.2455575194262953,
      0.0,
      -1.2797642546539492,
      -1.218170817363368,
      -1.3514621362377646,
      -1.3144408900970574,
      -1.2799154742124086,
      -1.244459537278468,
      -1.328889391319728,
      -1.3029339097661412,
      -1.1971802839769874,
      -1.4142711272185389
    ],
    [
      -1.5220028353978694,
      -1.5424089976041067,
      -1.6022139335867673,
      -1.5111895492810385,
      -1.597397719440239,
      -1.6226609395042704,
      -1.665436628837658,
      -1.6189590971193235,
      -1.4442391199449998,
      -1.635121167352722,
      -1.5279298338272564,
      -1.6894820768578558,
      -1.5301220230465455,
      -1.612203165216368,
      -1.5138764659191328,
      -1.5596851667104616,
      -1.5596734622012023,
      -1.5707013088422646,
      -1.6086285970189902,
      0.0,
      -1.553909676511891,
      -1.6778855382632372,
      -1.6374096731370198,
      -1.4659297032182763,
      -1.5797195105539228,
      -1.6351175675330656,
      -1.6018587167719904,
      -1.4650790783116956,
      -1.691399968279702
    ],
    [
      -1.3253402189349393,
      -1.3324770692337833,
      -1.3270506298320002,
      -1.2973705417519934,
      -1.3285600456214652,
      -1.3650301424154245,
      -1.4410536844644157,
      -1.3592020826258904,
      -1.314945186124382,
      -1.3742832975749588,
      -1.2830275170811754,
      -1.4379623568704756,
      -1.2831711585679202,
      -1.3282115634787932,
      -1.327758262997385,
      -1.3418140912893337,
      -1.265706461621008,
      -1.2694671616747482,
      -1.321294328818829,
      -1.2931324986486625,
      0.0,
      -1.4354748165744893,
      -1.3683246386635417,
      -1.3545689240987613,
      -1.341683218359454,
      -1.33521437351775,
      -1.3353597255449183,
      -1.2527729375097805,
      -1.4767384233515888
    ],
    [
      -1.2946306165126693,
      -1.2338892117990734,
      -1.2831911162759495,
      -1.272515067131347,
      -1.233816441076368,
      -1.2116931440065644,
      -1.262125461230687,
      -1.1369977885005262,
      -1.1613029140409283,
      -1.146125286913195,
      -1.2501397312530995,
      -1.2856748259153332,
      -1.201802215886381,
      -1.2008377880477885,
      -1.149877187641846,
      -1.2091077540693558,
      -1.266079313253101,
      -1.2494215912810926,
      -1.280498896966019,
      -1.262540558830165,
      -1.2485495608018702,
      0.0,
      -1.2967175362750287,
      -1.2180525409283611,
      -1.3101833106804956,
      -1.153615610259242,
      -1.2505904033597404,
      -1.237893307388098,
      -1.2387258568057165
    ],
    [
      -1.3527928833826477,
      -1.3416295188710148,
      -1.3629143894083626,
      -1.324963370103047,
      -1.3209249865440575,
      -1.3581807301744906,
      -1.3982411701614448,
      -1.3690664706429696,
      -1.3297605191983939,
      -1.3415326909360452,
      -1.3207352754137702,
      -1.424168568124959,
      -1.3200908527754358,
      -1.3382850844742202,
      -1.33889691256314,
      -1.3324670762569555,
      -1.280859180589427,
      -1.337992228813218,
      -1.3694569329018198,
      -1.340855628307126,
      -1.334915004750683,
      -1.4014934752572283,
      0.0,
      -1.3505837587337968,
      -1.2968813213504828,
      -1.3479670205821899,
      -1.337509515993855,
      -1.329210828480195,
      -1.4245673503472098
    ],
    [
      -1.4551605857327337,
      -1.419154804377872,
      -1.4159969324152073,
      -1.4714040731273657,
      -1.4425582543737134,
      -1.5164632601413177,
      -1.5411121235221679,
      -1.455939225926203,
      -1.3226362365733364,
      -1.4395034893373169,
      -1.4326990322602333,
      -1.5506453573817138,
      -1.491015123926549,
      -1.4465561977735284,
      -1.4589158434887233,
      -1.451290493338348,
      -1.4417319517515452,
      -1.4564062659323374,
      -1.469927529699131,
      -1.3331636101914925,
      -1.4550355942010664,
      -1.516397918968012,
      -1.4845634677257806,
      0.0,
      -1.471120894770319,
      -1.4351202032639458,
      -1.4811071059563716,
      -1.2942633355018187,
      -1.5097652863774431
    ],
    [
      -1.4482877762821902,
      -1.500802469181431,
      -1.5355643381857778,
      -1.4657203690350775,
      -1.5164856501453197,
      -1.503164434566327,
      -1.536876628973031,
      -1.5088703986526106,
      -1.4439058165322054,
      -1.4043112866333618,
      -1.4726465813785337,
      -1.5560271623390798,
      -1.481814470361449,
      -1.519605031768246,
      -1.407502488038182,
      -1.5129037800373253,
      -1.390649931609838,
      -1.4750276560984745,
      -1.4488692526790237,
      -1.419954195601975,
      -1.450168436317615,
      -1.5634244315031058,
      -1.4500584065757027,
      -1.5066947858656183,
      0.0,
      -1.523287707192328,
      -1.5070035368494554,
      -1.4182223032164565,
      -1.6040229345649246
    ],
    [
      -1.4190153570058075,
      -1.3374629842065586,
      -1.371201685124548,
      -1.366597141698036,
      -1.3787367885239892,
      -1.4071647085305332,
      -1.493746813068307,
      -1.2834567210188077,
      -1.2706593686754344,
      -1.3229664636709837,
      -1.321423514350738,
      -1.5387041462386604,
      -1.2983890139916658,
      -1.2692716293816952,
      -1.3648830342834066,
      -1.35828527075778,
      -1.3863826214659203,
      -1.3706101183449226,
      -1.4251825019367022,
      -1.427259787447231,
      -1.3875445411279093,
      -1.4075272670136207,
      -1.4748271317572241,
      -1.3901014453244154,
      -1.4825107856117956,
      0.0,
      -1.4244866937006562,
      -1.3669676178022272,
      -1.5207331168877316
    ],
    [
      -1.4624561965940581,
      -1.4794442501176008,
      -1.454129115072131,
      -1.4863856163284814,
      -1.42644206250457,
      -1.5173125307450965,
      -1.5339271614781567,
      -1.5155424525587518,
      -1.4794635500375368,
      -1.4782681682046273,
      -1.4719543496702954,
      -1.5150904258920843,
      -1.4498159927080068,
      -1.5048008261845707,
      -1.4291467774025883,
      -1.4977606810342774,
      -1.430394355162847,
      -1.4787686250800005,
      -1.4795329445100271,
      -1.4375076807894838,
      -1.4152179942709953,
      -1.5147151644892445,
      -1.3839718595931767,
      -1.463324597167492,
      -1.4550687946876157,
      -1.4642662160300246,
      0.0,
      -1.4636426484885745,
      -1.557311494952261
    ],
    [
      -1.2736873119392758,
      -1.2902634252060692,
      -1.3117326674697016,
      -1.2595086229827317,
      -1.269552834809687,
      -1.3337417090466746,
      -1.4230176052064805,
      -1.3086675731999338,
      -1.1161650982528555,
      -1.3181892193087628,
      -1.2694814119571698,
      -1.404963109712994,
      -1.281064662794768,
      -1.2709607793313669,
      -1.2834094986780535,
      -1.2741134994601284,
      -1.251145106669681,
      -1.2174510783149566,
      -1.3255461291113932,
      -1.1885828508741132,
      -1.2613934619932743,
      -1.3963175942633235,
      -1.35050677661896,
      -1.0854716529843456,
      -1.3156563355784088,
      -1.30762294082607,
      -1.3166484316690286,
      0.0,
      -1.429083480888911
    ],
    [
      -1.3584156924456716,
      -1.2926923851159564,
      -1.3244374954756069,
      -1.3430597332537573,
      -1.2846156666461586,
      -1.330475130051521,
      -1.3172131498758348,
      -1.2838920056679335,
      -1.300761672057552,
      -1.2905568195854762,
      -1.3408214129076563,
      -1.3138117650164165,
      -1.3264634096375418,
      -1.3114277561019598,
      -1.3097323583794316,
      -1.3085887303560713,
      -1.3158875301781643,
      -1.340173441161915,
      -1.3217474753875826,
      -1.3136931655752846,
      -1.3023241942688841,
      -1.2833895363406882,
      -1.3019006786662466,
      -1.2796042396561214,
      -1.3068559904459185,
      -1.2836647624927668,
      -1.3264567241352312,
      -1.3237602649610314,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.1400214485224507,
      0.10084414896687877,
      0.13823911750785944,
      0.09404633788582739,
      0.1064871341934488,
      0.0631152481414432,
      0.1002049140963659,
      0.13595505968642208,
      0.09858347388345834,
      0.12145343170369305,
      0.0687129407723126,
      0.11975574496807484,
      0.10815415008329765,
      0.12271975735147045,
      0.12931033079266818,
      0.12129535378011846,
      0.10972936524316479,
      0.09280346238396975,
      0.13832590338697504,
      0.11943788454148985,
      0.0705357108925071,
      0.09133711253805288,
      0.11010484797592657,
      0.10538054353219017,
      0.11411615587365498,
      0.102437616026676,
      0.12109580959272215,
      0.0690864522995911
    ],
    [
      0.395660817204329,
      0.0,
      0.42364437347720085,
      0.4430177718086412,
      0.44690895051807167,
      0.32507515916091045,
      0.2968854601720823,
      0.39618990397596443,
      0.46197740770864204,
      0.405726459512914,
      0.44304730099590195,
      0.2668671934250677,
      0.39211200962890813,
      0.44836677755622234,
      0.3912993677242518,
      0.4738600433411915,
      0.4076620746376485,
      0.3805759450982613,
      0.37463761520623695,
      0.3945260634626526,
      0.40592433911406856,
      0.3333127307638064,
      0.3466140882977904,
      0.40847607287114185,
      0.33576350362575047,
      0.4213377216379366,
      0.36329889176428565,
      0.3995374225920856,
      0.29641074937326395
    ],
    [
      0.3912766127875176,
      0.4598785289473548,
      0.0,
      0.4541892129430827,
      0.4817398796798138,
      0.3626292505101467,
      0.3212585801271679,
      0.39775124351489866,
      0.5139301380949806,
      0.39935704174627884,
      0.4629072987363234,
      0.27529351850716344,
      0.4339392592535696,
      0.5165744147262086,
      0.3613321045546132,
      0.4786330194092818,
      0.4302984244254975,
      0.47701174874457664,
      0.3675985302728473,
      0.38382480403277586,
      0.4247110708458506,
      0.32643354318891404,
      0.3583595459585174,
      0.45356441289564065,
      0.34533440356705447,
      0.4135064238603039,
      0.3870728287334313,
      0.4189434275904582,
      0.2972628819870351
    ],
    [
      0.4778540021818065,
      0.5662213639903084,
      0.49441787182386343,
      0.0,
      0.4336469186830747,
      0.423278791046412,
      0.30793355189723104,
      0.40327356478163146,
      0.5274917843617992,
      0.38063211844026523,
      0.585354038234041,
      0.29899744147986973,
      0.47561788024569296,
      0.48208123750558385,
      0.46219055223064553,
      0.5263795064528605,
      0.48762430211579555,
      0.5461003866016136,
      0.4536584628772955,
      0.4982957118032256,
      0.46099426946798494,
      0.31360891660618395,
      0.37078796320287055,
      0.4183664884749567,
      0.35372253783892726,
      0.44252181150137004,
      0.3861859222515638,
      0.4884710792394571,
      0.26408129848578277
    ],
    [
      0.371744941594228,
      0.4896439718391341,
      0.48811396801435647,
      0.409547869024498,
      0.0,
      0.35432101361518753,
      0.3162832480418707,
      0.3995614767222899,
      0.4570969243521634,
      0.4254031452767739,
      0.4126037273026155,
      0.27735039475927503,
      0.4059034746140331,
      0.4088198190965613,
      0.40615443769327597,
      0.4447651664794574,
      0.4652137541535384,
      0.39937528869776995,
      0.3981424924358219,
      0.3859453175075911,
      0.41718282974413934,
      0.3490271879374802,
      0.4024850105441611,
      0.42284581384540454,
      0.3924099464602264,
      0.40916525600425957,
      0.3910895339616731,
      0.4578810078745996,
      0.3289979040843334
    ],
    [
      0.21699600813164333,
      0.3301808380817366,
      0.298041148768988,
      0.3065307234652197,
      0.32524394925504185,
      0.0,
      0.29222296254044755,
      0.33535763183144374,
      0.32704584104807455,
      0.29476506910074796,
      0.3183350869899846,
      0.25081341088023934,
      0.33108568146242034,
      0.3481207191171727,
      0.3405155908729094,
      0.3217691636451301,
      0.3407802360034218,
      0.3211606673354299,
      0.2751732422756108,
      0.30009975653474963,
      0.3134875413827718,
      0.2823658266988962,
      0.2777183185430383,
      0.26635668571961846,
      0.26955537596764056,
      0.32320941967220707,
      0.28384596713259747,
      0.3163743203741014,
      0.21754337085456044
    ],
    [
      0.33915201402330064,
      0.38365753142136905,
      0.39997213846229185,
      0.37152103014492477,
      0.40384138597791774,
      0.3612011031869993,
      0.0,
      0.38020720985434964,
      0.3565128440410672,
      0.3782018643811551,
      0.41683979150307837,
      0.35653223934362877,
      0.3751897394980259,
      0.3515185904718625,
      0.37604278323233653,
      0.34378756206804084,
      0.42173666844502367,
      0.3686328165132886,
      0.35633682196710215,
      0.35765746469386483,
      0.36621634580277407,
      0.3390334601797449,
      0.36797213077765334,
      0.3767969092652259,
      0.3765006220469569,
      0.35233567289768275,
      0.3774089693218552,
      0.35904082620080846,
      0.3364692621941636
    ],
    [
      0.29991338924566024,
      0.40900446154778014,
      0.4360510800759365,
      0.38135389251086527,
      0.4168573956544055,
      0.4210778206676693,
      0.3050988520529625,
      0.0,
      0.4933116304945484,
      0.4605847974947097,
      0.418361098632835,
      0.25798892549141095,
      0.47634022583626834,
      0.51050518278897,
      0.441359027818939,
      0.4157691231660192,
      0.39043278630615497,
      0.4158077242622302,
      0.3715524261545926,
      0.35396983324502584,
      0.41944888169902295,
      0.40710850489122663,
      0.2968773050606832,
      0.3806819420962009,
      0.3181923812461491,
      0.5225905743850134,
      0.32070134025654795,
      0.3879882924569029,
      0.2765393004619783
    ],
    [
      0.4201265802515708,
      0.4894469920685043,
      0.4877313072610412,
      0.5218993489741592,
      0.46143026714111146,
      0.36290059392335494,
      0.26252058980495874,
      0.4856032388646778,
      0.0,
      0.45282701161104844,
      0.46946723210362706,
      0.28867561026871735,
      0.5252789543467971,
      0.6660217488035565,
      0.5259614557457069,
      0.48692618207000304,
      0.41194770766436006,
      0.44623526475418984,
      0.37411660688825266,
      0.5367911453720802,
      0.4582820182050811,
      0.4237476171228909,
      0.34267580477000914,
      0.5264898765002519,
      0.34717337768672385,
      0.5379080444455078,
      0.3458501741736333,
      0.5899945967814224,
      0.301423108356786
    ],
    [
      0.281693090219971,
      0.3587717893934559,
      0.33633177313207363,
      0.3023150851128822,
      0.32674362870242635,
      0.292206502021614,
      0.24791595210213058,
      0.39544753591806625,
      0.3733817543261013,
      0.0,
      0.2906644327420782,
      0.22037091832040523,
      0.3426343912590084,
      0.37106031997432765,
      0.3463606390758358,
      0.3156954815195565,
      0.3578483275050022,
      0.2817610333916616,
      0.29780624256284094,
      0.25988902686062154,
      0.3067723955446402,
      0.3567031164265706,
      0.3065966407565641,
      0.29165010590169493,
      0.3261481012066678,
      0.38391109353435127,
      0.281214573442357,
      0.30084994440455204,
      0.21773459455925281
    ],
    [
      0.564250185686074,
      0.5986907865711251,
      0.513049597306751,
      0.5978577245574306,
      0.4845775894278148,
      0.37345453183202526,
      0.3426159251961447,
      0.49225090110389536,
      0.5541563813150985,
      0.40281892492851346,
      0.0,
      0.35385328372156866,
      0.5021628705656076,
      0.5276162622771934,
      0.5020616478886293,
      0.5128064731204947,
      0.5343012255081652,
      0.5053579613735313,
      0.49919023155276165,
      0.5142285362356545,
      0.5277282676168471,
      0.36329962219276357,
      0.43387984031706783,
      0.4769675275686096,
      0.41234277363017324,
      0.5467401486424466,
      0.46687297836402863,
      0.5141815063086823,
      0.30170991941344827
    ],
    [
      0.21659182432016366,
      0.23532024730855094,
      0.2381577982461507,
      0.22801164375731786,
      0.2525649997894275,
      0.2313866963905684,
      0.23117396853548167,
      0.22421158656439721,
      0.2351696464297266,
      0.23287270361972157,
      0.23283730715798878,
      0.0,
      0.23330046413092065,
      0.22200923708354026,
      0.2522364793074199,
      0.22306373054762196,
      0.25521570971588337,
      0.2444314006782764,
      0.22327835362434634,
      0.2716021479588482,
      0.2555853789028075,
      0.23682165859134807,
      0.2675191485480011,
      0.25833726692886994,
      0.2598800807409536,
      0.22353526336084895,
      0.26863427617943425,
      0.24518529305259662,
      0.26076243271609356
    ],
    [
      0.33821532048140224,
      0.39415253659804517,
      0.3648878534381528,
      0.39115274260302235,
      0.35176768186421414,
      0.35316424918644174,
      0.2892511082174407,
      0.4467636644486095,
      0.46304148720587124,
      0.36731970485762444,
      0.40170261677259966,
      0.2520470122657177,
      0.0,
      0.4495395966142901,
      0.4217626403259287,
      0.3769704263166116,
      0.38874159569692845,
      0.4152438786511967,
      0.32820524737756673,
      0.4076600612018797,
      0.4182639778597841,
      0.36123435002507676,
      0.3041283260625671,
      0.3287693213271736,
      0.30845728883144474,
      0.4407698132438491,
      0.34117075319723744,
      0.3737941880289535,
      0.26854374511777745
    ],
    [
      0.3402694128470851,
      0.461182791114914,
      0.4827038627615299,
      0.4200277549354323,
      0.4058707084809281,
      0.3542379424513944,
      0.2610085015326893,
      0.4600561573470876,
      0.617242876176504,
      0.41330494320395417,
      0.41899064430637867,
      0.2769946044120313,
      0.5052596171995856,
      0.0,
      0.4312606300388593,
      0.45784309100652987,
      0.3953811837190875,
      0.41211081853564435,
      0.34351553389259903,
      0.34235799376817977,
      0.3665886662681863,
      0.38916066450834963,
      0.3077843606728452,
      0.3780104230769745,
      0.32312864507037053,
      0.464277774882796,
      0.34475682071467184,
      0.3845267813326929,
      0.25700276207572226
    ],
    [
      0.4223418494270572,
      0.4090897988453832,
      0.3836359976687067,
      0.4328145426169343,
      0.4070814138306824,
      0.38414094591820547,
      0.32151920932445566,
      0.40717240368622676,
      0.5649438485891796,
      0.4062628325060389,
      0.454587542063035,
      0.3548160135548468,
      0.49401924222088844,
      0.4553690265897685,
      0.0,
      0.39286032981527796,
      0.3829226634772809,
      0.38361193663190996,
      0.37462110375857227,
      0.498370023780617,
      0.41495726727724236,
      0.42618713283866816,
      0.37635123920603175,
      0.4293256746901397,
      0.4269634229791992,
      0.4341582433429305,
      0.4114910165779262,
      0.42220237965861185,
      0.3338215360167949
    ],
    [
      0.4388087474619693,
      0.5208958277718327,
      0.45300583848073983,
      0.4786405501772799,
      0.42720194421458824,
      0.362605223963095,
      0.2616014275755463,
      0.4254486706269478,
      0.47519091093874977,
      0.3654747912456082,
      0.4555503384268871,
      0.27115613381930825,
      0.44740160599516177,
      0.48540713986832573,
      0.397997377052133,
      0.0,
      0.4179816957163469,
      0.4426723942182722,
      0.3754274062512175,
      0.4058260032321841,
      0.40834033865245933,
      0.31850360905172925,
      0.3368919434027777,
      0.38689196972584106,
      0.325211567445552,
      0.4304565448908966,
      0.36498614177074695,
      0.42475694589706126,
      0.2760866001398403
    ],
    [
      0.42476680456897187,
      0.40927048559932877,
      0.3957397034274812,
      0.4705091170205187,
      0.3790781893839805,
      0.3655732332609867,
      0.28499348763490273,
      0.37658607712368974,
      0.4346283734545324,
      0.3965441966875327,
      0.45509267467579906,
      0.28302367321352184,
      0.41338160598193086,
      0.39598178032691167,
      0.3727433850042112,
      0.4223065932432841,
      0.0,
      0.43094042184889325,
      0.3798219530618163,
      0.4098489224933475,
      0.46841884591036864,
      0.26966862761498445,
      0.4201057981657108,
      0.36077612169731643,
      0.41283491734787203,
      0.4396420548422175,
      0.36792934917010545,
      0.5054620341616483,
      0.24154332033543713
    ],
    [
      0.40703010506618265,
      0.40661080165831587,
      0.48556250737470075,
      0.5104298699857539,
      0.4163243290465324,
      0.3530433684970402,
      0.33203289351878085,
      0.4012220460855176,
      0.47124853607325035,
      0.379417460894941,
      0.4835162316068602,
      0.32163157061094716,
      0.47021225796429555,
      0.4596356071190384,
      0.40208061924739535,
      0.48419812684644437,
      0.46641831227687813,
      0.0,
      0.40177937396795604,
      0.41619636983020514,
      0.48139896526838344,
      0.3432226622230372,
      0.36440962277087174,
      0.42823517997850646,
      0.3516747729311305,
      0.42956499082216193,
      0.3965224859353971,
      0.5033199132675372,
      0.29283117091045296
    ],
    [
      0.40510270948895233,
      0.4117088694056408,
      0.39388165143427134,
      0.47216714202514765,
      0.39539096585244393,
      0.37444818332414354,
      0.32170948426614365,
      0.4452038812861536,
      0.42919030309897854,
      0.4480497844980289,
      0.496742726240875,
      0.3379036119407395,
      0.42979758341769525,
      0.41991686790942695,
      0.4605697774202029,
      0.358548484084499,
      0.4881541888637162,
      0.44572493912251177,
      0.0,
      0.41151820389485794,
      0.473111641185439,
      0.33982032231104253,
      0.37684156845174965,
      0.4113669843363985,
      0.4468229212703392,
      0.36239306722907916,
      0.3883485487826659,
      0.4941021745718197,
      0.27701133133026823
    ],
    [
      0.48082258111456766,
      0.4604164189083304,
      0.4006114829256697,
      0.4916358672313985,
      0.40542769707219817,
      0.3801644770081667,
      0.337388787674779,
      0.38386631939311355,
      0.5585862965674373,
      0.3677042491597151,
      0.4748955826851806,
      0.3133433396545813,
      0.4727033934658915,
      0.3906222512960691,
      0.48894895059330423,
      0.4431402498019754,
      0.4431519543112348,
      0.4321241076701725,
      0.39419681949344687,
      0.0,
      0.4489157400005461,
      0.3249398782491999,
      0.36541574337541727,
      0.5368957132941607,
      0.4231059059585143,
      0.3677078489793715,
      0.40096669974044663,
      0.5377463382007415,
      0.311425448232735
    ],
    [
      0.4023982499592551,
      0.3952613996604111,
      0.4006878390621942,
      0.43036792714220096,
      0.3991784232727291,
      0.3627083264787698,
      0.2866847844297786,
      0.368536386268304,
      0.4127932827698124,
      0.35345517131923554,
      0.444710951813019,
      0.2897761120237188,
      0.44456731032627417,
      0.39952690541540115,
      0.3999802058968094,
      0.38592437760486065,
      0.4620320072731863,
      0.4582713072194462,
      0.40644414007536533,
      0.4346059702455318,
      0.0,
      0.2922636523197051,
      0.35941383023065265,
      0.3731695447954331,
      0.3860552505347403,
      0.3925240953764444,
      0.392378743349276,
      0.4749655313844139,
      0.2510000455426056
    ],
    [
      0.2879948223798614,
      0.3487362270934573,
      0.2994343226165812,
      0.3101103717611837,
      0.3488089978161628,
      0.37093229488596635,
      0.3204999776618438,
      0.44562765039200447,
      0.4213225248516024,
      0.43650015197933567,
      0.3324857076394312,
      0.2969506129771975,
      0.3808232230061497,
      0.3817876508447422,
      0.4327482512506846,
      0.37351768482317493,
      0.31654612563942974,
      0.33320384761143806,
      0.3021265419265118,
      0.3200848800623657,
      0.3340758780906605,
      0.0,
      0.285907902617502,
      0.3645728979641696,
      0.27244212821203506,
      0.4290098286332886,
      0.33203503553279035,
      0.3447321315044327,
      0.34389958208681426
    ],
    [
      0.2985335902996711,
      0.309696954811304,
      0.28841208427395615,
      0.3263631035792718,
      0.33040148713826123,
      0.2931457435078282,
      0.253085303520874,
      0.28226000303934917,
      0.3215659544839249,
      0.30979378274627356,
      0.33059119826854855,
      0.22715790555735982,
      0.33123562090688297,
      0.31304138920809854,
      0.31242956111917874,
      0.3188593974253633,
      0.37046729309289184,
      0.3133342448691008,
      0.281869540780499,
      0.31047084537519276,
      0.31641146893163574,
      0.24983299842509044,
      0.0,
      0.300742714948522,
      0.35444515233183593,
      0.3033594531001289,
      0.31381695768846374,
      0.3221156452021239,
      0.22675912333510895
    ],
    [
      0.36379717879541884,
      0.3998029601502806,
      0.4029608321129452,
      0.34755369140078685,
      0.37639951015443907,
      0.3024945043868348,
      0.27784564100598463,
      0.36301853860194955,
      0.4963215279548161,
      0.37945427519083563,
      0.3862587322679192,
      0.26831240714643867,
      0.3279426406016035,
      0.3724015667546241,
      0.3600419210394292,
      0.3676672711898046,
      0.37722581277660727,
      0.36255149859581515,
      0.3490302348290215,
      0.48579415433665996,
      0.36392217032708607,
      0.3025598455601406,
      0.33439429680237187,
      0.0,
      0.34783686975783357,
      0.38383756126420665,
      0.33785065857178087,
      0.5246944290263338,
      0.3091924781507094
    ],
    [
      0.35315921674816697,
      0.30064452384892615,
      0.2658826548445794,
      0.33572662399527964,
      0.2849613428850375,
      0.29828255846403007,
      0.26457036405732626,
      0.2925765943777465,
      0.35754117649815176,
      0.39713570639699536,
      0.3288004116518235,
      0.24541983069127737,
      0.31963252266890807,
      0.28184196126211125,
      0.39394450499217526,
      0.2885432129930319,
      0.4107970614205192,
      0.32641933693188263,
      0.3525777403513335,
      0.38149279742838216,
      0.3512785567127421,
      0.23802256152725132,
      0.35138858645465443,
      0.2947522071647388,
      0.0,
      0.27815928583802907,
      0.29444345618090173,
      0.3832246898139007,
      0.19742405846543254
    ],
    [
      0.3804422360898909,
      0.4619946088891398,
      0.4282559079711503,
      0.4328604513976624,
      0.42072080457170924,
      0.3922928845651652,
      0.3057107800273915,
      0.5160008720768907,
      0.528798224420264,
      0.4764911294247147,
      0.4780340787449604,
      0.26075344685703805,
      0.5010685791040326,
      0.5301859637140032,
      0.4345745588122918,
      0.4411723223379185,
      0.41307497162977813,
      0.4288474747507758,
      0.37427509115899626,
      0.3721978056484674,
      0.41191305196778916,
      0.3919303260820777,
      0.3246304613384743,
      0.40935614777128304,
      0.3169468074839028,
      0.0,
      0.37497089939504225,
      0.4324899752934712,
      0.2787244762079668
    ],
    [
      0.3866428600790015,
      0.3696548065554588,
      0.3949699416009287,
      0.3627134403445782,
      0.4226569941684897,
      0.3317865259279631,
      0.31517189519490296,
      0.3335566041143079,
      0.3696355066355228,
      0.3708308884684324,
      0.37714470700276426,
      0.3340086307809753,
      0.39928306396505286,
      0.34429823048848895,
      0.4199522792704713,
      0.35133837563878223,
      0.41870470151021255,
      0.3703304315930591,
      0.3695661121630325,
      0.41159137588357586,
      0.43388106240206437,
      0.33438389218381515,
      0.4651271970798829,
      0.38577445950556766,
      0.3940302619854439,
      0.384832840643035,
      0.0,
      0.3854564081844851,
      0.29178756172079856
    ],
    [
      0.4164206472622052,
      0.39984453399541176,
      0.3783752917317793,
      0.43059933621874924,
      0.420555124391794,
      0.3563662501548064,
      0.2670903539950005,
      0.38144038600154717,
      0.5739428609486255,
      0.3719187398927182,
      0.42062654724431114,
      0.28514484948848695,
      0.409043296406713,
      0.4191471798701141,
      0.4066984605234274,
      0.4159944597413525,
      0.4389628525317999,
      0.47265688088652436,
      0.3645618300900877,
      0.5015251083273677,
      0.42871449720820665,
      0.29379036493815747,
      0.3396011825825209,
      0.6046363062171354,
      0.37445162362307216,
      0.38248501837541093,
      0.3734595275324524,
      0.0,
      0.2610244783125699
    ],
    [
      0.2538133248965615,
      0.3195366322262767,
      0.28779152186662627,
      0.26916928408847585,
      0.32761335069607456,
      0.28175388729071216,
      0.2950158674663983,
      0.32833701167429963,
      0.311467345284681,
      0.32167219775675693,
      0.2714076044345768,
      0.2984172523258166,
      0.2857656077046913,
      0.3008012612402733,
      0.30249665896280153,
      0.30364028698616186,
      0.29634148716406883,
      0.27205557618031806,
      0.2904815419546505,
      0.2985358517669485,
      0.309904823073349,
      0.3288394810015449,
      0.3103283386759865,
      0.33262477768611176,
      0.3053730268963146,
      0.3285642548494663,
      0.2857722932070019,
      0.28846875238120173,
      0.0
    ]
  ],
  "row_avgs": [
    0.10761748059366823,
    0.38852557909482954,
    0.4069504339157608,
    0.4403496347793611,
    0.4031098186313114,
    0.300881948345923,
    0.36965413563987476,
    0.3930524355714539,
    0.4481947305700009,
    0.31323137463988254,
    0.4792508437222338,
    0.24141774086383233,
    0.3695257567077646,
    0.39338771308332215,
    0.4144870941747362,
    0.39930070314332483,
    0.3924004195449037,
    0.4164203625635184,
    0.4112695684658297,
    0.42274536221637016,
    0.3878457775639133,
    0.3488184732807435,
    0.30393566135595496,
    0.3668272574554528,
    0.31673726945233344,
    0.4113826549190089,
    0.3760396805389677,
    0.39960992816044094,
    0.3002139035620766
  ],
  "col_avgs": [
    0.3705649686647317,
    0.4014049334580081,
    0.3829698035402687,
    0.39526161558323414,
    0.3802514381269713,
    0.3403985427078532,
    0.2850787216327199,
    0.3809904454918474,
    0.43726751599323316,
    0.37475366486515466,
    0.39939318006954055,
    0.27972546015320215,
    0.4016234952408958,
    0.4057268870716495,
    0.3916594151801907,
    0.3876889454452642,
    0.3968307313343062,
    0.3855813820718198,
    0.3490283821190839,
    0.3858297170846367,
    0.38949529192869364,
    0.32165565229829296,
    0.3387694038287295,
    0.38273351407939343,
    0.3432922932217505,
    0.39080786650460336,
    0.3462683021055354,
    0.4072000658706364,
    0.2709321068845473
  ],
  "combined_avgs": [
    0.23909122462919996,
    0.3949652562764188,
    0.39496011872801473,
    0.4178056251812976,
    0.39168062837914136,
    0.3206402455268881,
    0.32736642863629734,
    0.38702144053165066,
    0.442731123281617,
    0.3439925197525186,
    0.43932201189588715,
    0.26057160050851724,
    0.38557462597433023,
    0.3995573000774858,
    0.40307325467746347,
    0.3934948242942945,
    0.3946155754396049,
    0.4010008723176691,
    0.3801489752924568,
    0.40428753965050346,
    0.3886705347463035,
    0.33523706278951826,
    0.3213525325923422,
    0.3747803857674231,
    0.330014781337042,
    0.4010952607118061,
    0.36115399132225157,
    0.40340499701553867,
    0.28557300522331197
  ],
  "gppm": [
    593.3497192114429,
    651.5597612906092,
    660.4543493059676,
    654.4396199727291,
    658.9267063352288,
    678.697437832948,
    708.1146492220421,
    658.7142492236086,
    634.1203058061158,
    660.4512325840658,
    653.3459942948939,
    709.4834064107881,
    652.5529405025535,
    648.6756708101112,
    654.3371812769493,
    655.6746121198483,
    652.915530293123,
    658.7665548094637,
    677.8048083375721,
    653.4435192530345,
    656.7747042913498,
    689.2759668042997,
    680.196055825755,
    657.986748060487,
    676.5808642833857,
    654.0081765521672,
    676.557368949015,
    647.3182581916409,
    714.5308444799873
  ],
  "gppm_normalized": [
    1.3716002421242919,
    1.450498721942872,
    1.469183204472847,
    1.4516409652397313,
    1.460373543192278,
    1.5117273387353032,
    1.5815526556796777,
    1.4621847198972608,
    1.4041140691766907,
    1.4680079141896487,
    1.4513765971400852,
    1.5831062428170217,
    1.4489173280407528,
    1.4368822775645513,
    1.4522574040217735,
    1.4587036951751113,
    1.447793579883559,
    1.4559926468459674,
    1.50358244187744,
    1.4560740914094896,
    1.4495580240747583,
    1.53161117817267,
    1.505059801887813,
    1.4613392173615303,
    1.5006920587445447,
    1.4518479368003987,
    1.5035526008705682,
    1.4312768191433298,
    1.5916111678454232
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390
  ],
  "response_lengths": [
    6534,
    2694,
    2534,
    2385,
    2515,
    2737,
    2414,
    2617,
    2296,
    2712,
    2333,
    3031,
    2712,
    2772,
    2619,
    2643,
    2354,
    2404,
    2270,
    2359,
    2457,
    2522,
    2590,
    2410,
    2435,
    2473,
    2277,
    2678,
    2081
  ]
}