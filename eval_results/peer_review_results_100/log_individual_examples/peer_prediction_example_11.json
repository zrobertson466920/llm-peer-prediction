{
  "example_idx": 11,
  "reference": "Published as a conference paper at ICLR 2023\n\nNOISE INJECTION NODE REGULARIZATION FOR ROBUST LEARNING\n\nNoam Levi§ & Tomer Volansky Department of Physics Tel Aviv University Tel Aviv, Israel {noam,tomerv}@mail.tau.ac.il\n\nItay M. Bloch§ Berkeley Center for Theoretical Physics, University of California and Theoretical Physics Group, Lawrence Berkeley National Laboratory, Berkeley, CA, U.S.A. itayblochm@berkeley.edu\n\nMarat Freytsis NHETC, Department of Physics and Astronomy Rutgers University Piscataway, NJ, U.S.A. marat.freytsis@rutgers.edu\n\nABSTRACT\n\nWe introduce Noise Injection Node Regularization (NINR), a method of injecting structured noise into Deep Neural Networks (DNN) during the training stage, resulting in an emergent regularizing effect. We present theoretical and empirical evidence for substantial improvement in robustness against various test data perturbations for feed-forward DNNs when trained under NINR. The novelty in our approach comes from the interplay of adaptive noise injection and initialization conditions such that noise is the dominant driver of dynamics at the start of training. As it simply requires the addition of external nodes without altering the existing network structure or optimization algorithms, this method can be easily incorporated into many standard architectures. We find improved stability against a number of data perturbations, including domain shifts, with the most dramatic improvement obtained for unstructured noise, where our technique outperforms existing methods such as Dropout or L2 regularization, in some cases. Further, desirable generalization properties on clean data are generally maintained.\n\n1\n\nINTRODUCTION\n\nNonlinear systems often display dynamical instabilities which enhance small initial perturbations and lead to cumulative behavior that deviates dramatically from a steady-state solution. Such instabilities are prevalent across physical systems, from hydrodynamic turbulence to atomic bombs (see Jeans & Darwin (1902); Parker (1958); Chandrasekhar (1961); Drazin & Reid (2004); Strogatz (2018) for just a few examples). In the context of deep learning (DL), DNNs, once optimized via stochastic gradient descent (SGD), suffer from similar instabilities as a function of their inputs. While remarkably successful in a multitude of real world tasks, DNNs are often surprisingly vulnerable to perturbations in their input data as a result (Szegedy et al., 2014). Concretely, after training, even small changes to the inputs at deployment can result in total predictive breakdown.\n\nOne may classify such perturbations with respect to the distribution from which training data is implicitly drawn. This data is typically assumed to have support over (the vicinity of) some lowdimensional submanifold of potential inputs, which is only learned approximately due to the discrete nature of the training set. To perform well during training, a network need only have well-defined behavior on the data manifold, accomplished through training on a given data distribution. However, data seen on deployment can display other differences with respect to the training set, as illustrated\n\n§Equal contribution\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of perturbations to data inputs with respect to the joint probability distribution manifold of features and labels. Points indicate {sample, label} pairs {x, y}, where different colored points correspond to samples drawn from different marginal distributions. Black points represent pairs from a training dataset {xi, yi}N i=1, with the red spheres indicating corrupted inputs, determined by shifted distribution functions fcorrupted(x + ε, y). The gray arrow represents an adversarial attack, performed by ascending up the gradient of the network output to reach the closest decision boundary, while generalization from training to test data is depicted as interpolation from black to blue points. Finally, domain shift is a shift in the underlying distribution on the same manifold, depicted by the green arrow and points.\n\nin Fig. 1. These distortions introduce vulnerabilities that are a crucial drawback of trained DNNs, making them susceptible to commonly occurring noise which is ubiquitous in real-world tasks. By studying how networks dynamically act to mitigate the negative effects of input noise, we identify a novel dynamical regularization method starting in a noise-dominated regime, leading to more robust behavior for a range of data perturbations. This is the central contribution of this work.\n\nBackground: Regularization involves introducing additional constraints in order to solve an illposed problem or to prevent over-fitting. In the context of DL problems, different regularization schemes have been proposed (for a review, see Kukaˇcka et al. (2018) and references therein). These methods are designed to constrain the network parameters during training, thereby reducing sensitivity to irrelevant features in the input data, as well as avoiding overfitting. For instance, weight norm regularization (L2, L1, etc.) (Cortes & Vapnik, 1995; Zheng et al., 2003) can be used to reduce overfitting to the training data, and is often found to improve generalization performance (Hinton, 1987; Krogh & Hertz, 1991; Zhang et al., 2018). Alternatively, introducing stochasticity during training (e.g., Dropout (Srivastava et al., 2014)), has become a standard addition to many DNN architectures, for similar reasons. These methods are mostly optimized to reduce the generalization error from training to test data, under the assumption that both are sampled from the same underlying distribution (Srivastava et al., 2014). Here, we propose a new method which is instead tailored for robustness. Our method relies on noise-injection, that actively reduces the sensitivity to uncorrelated input perturbations.\n\nOur contribution: In this paper, we employ Noise Injection Nodes (NINs), which feed random noise through designated optimizable weights, forcing the network to adapt to layer inputs which contain no useful information. Since the amount of injected noise is a free parameter, at initialization we can set it to be anything from a minor perturbation to the dominant effect, leading to a system breakdown for extreme values. The general behavior of NINs and how they probe the network is the main goal of Levi et al. (2022), while we focus here on their regularizing properties in different noise injection regimes. The results of Levi et al. (2022) are explicitly recast in the context of regularization in App. C for a linear model, which captures the main insights.\n\nOur study suggests that within a certain range of noise injection parameter values, this procedure can substantially improve robustness against subsequent input corruption and partially against other forms of distributional shifts, where the maximal improvement occurs for large noise injection magnitudes approaching the boundary of this window, above which the training accuracy degrades to random guessing. To the best of our knowledge, this regime has not been previously explored.\n\nIn the following, we analyze how the addition of NINs produces a regularization scheme which we call Noise Injection Node Regularization (NINR). The main features of NINR are enhanced stability, simplicity, and flexibility, without drastically compromising generalization performance. In order to demonstrate these features, we consider two types of feed-forward architectures: Fully Connected Networks (FCs) and Convolutional Neural Networks (CNNs), and use various datasets to train the systems. We compare NINR robustness improvement with standard regularization methods, as well as performance of these systems when using input corruption during training (CDT). Our results\n\n2\n\nfdata(x,y)fimages(x,y)DomainShiftfcorrupted(x+ε,y)Generalizationfadv(x+εadv,y)Published as a conference paper at ICLR 2023\n\nFigure 2: Robustness against random input perturbations tested on FC network (left) and CNN (right). Test accuracy vs. the scale of input noise corruption defined in Eq. (7) is shown for L2, Dropout, in-NINR, fullNINR and CDT with σnoise = 0.4. Shades indicate 2 standard deviations estimated over 10 distinct runs. For the input-NINR (full-NINR) fully-connected implementations we take σε = 51.8 (16.4) in the decay phase and σε = 231.6 (51.8) in the catapult phase. Similarly, for the convolutional implementations we take σε = 2.8 (0.9) in the decay phase and σε = 87.5 (62) in the catapult phase. This key result illustrates that NINR significantly increases the robustness of generic architectures trained on the FMNIST dataset, while marginally affecting generalization (σnoise = 0). Comparing the CDT and input-NINR curves demonstrates the advantage of our regularization method. While both techniques perform similarly well on data corruption of σnoise = 0.4, CDT is significantly worse on clean data. This is a result of the CDT network being forced to fit both noise and data, without the ability to suppress the latter, a crucial attribute of NINR. Here, the learning rate is fixed to η = 0.05 with mini-batch size B = 128. Each training run is performed for 500 SGD training epochs in total, or until 98% training accuracy has been achieved. For further details, see Sec. 3 and App. A.\n\ncan easily be generalized to other architectures and more complex NINR topologies. In Fig. 2, we present our main results, comparing networks trained on the FMNIST dataset and demonstrating improved robustness against input perturbations without compromising generalization on clean data.\n\nThe paper is organized as follows. In Sec. 2 we briefly review important analytical and empirical results that are explored in depth in the work of Levi et al. (2022), demonstrating in this work how NINs implicitly generate adaptive regularization terms in the loss function. In Sec. 3, we empirically study the effectiveness of NINR. We begin by evaluating its effect on robustness against perturbations, including domain shifts and those adversarially designed, demonstrating the enhanced performance of NINR. We then verify that generalization performance on clean data is not hindered by training with NINR. We discuss related work in Sec. 4, finally concluding in Sec. 5.\n\n2 NOISE INJECTION NODES REGULARIZATION\n\nIn the following sections we explain how an effective regularization scheme against input corruption naturally emerges as a consequence of adding a NIN to a DNN. First, we discuss how the NIN generates implicit regularization terms directly from computing the effective loss function. Then, we review the adaptive nature of these terms as they relate to Noise Injection Weight (NIW) dynamics during training, and discuss the expected robustness gains depending on the evolution of the NIWs.\n\n2.1 EMERGENT REGULARIZATION TERMS\n\nIn order to see how NINs generate implicit regularization terms, we study a vanilla feed-forward DNN setup. Consider a supervised learning problem modeled by a neural network optimized under SGD, with an associated single sample loss function, L : Rdin → R. The loss depends on the model parameters θ = {W (l), b(l)|l = 0, ..., NL − 1}, where NL is the number of layers, and the weights and biases associated with a given layer are W (l) ∈ Rdl×dl+1, b(l) ∈ Rdl+1 . At each SGD iteration, a mini-batch B consists of a set of labeled examples, {(xi, yi)}|B| i=1 ∈ Rdin × Rdlabel. The addition of a NIN in a given layer, lNI, corresponds to a random scalar input, ε ∈ R, sampled repeatedly for each SGD training epoch from a chosen distribution1, connected via NIWs WNI ∈ R1×dlNI+1. We define for a given layer l, the preactivation z(l) = W (l)x(l) + b(l), therefore the addition of a NIN to a dense layer results in a translation to the preactivation at lNI, as z(lNI) → z(lNI) + εWNI.\n\n1One may also generate ε only once, before training. We empirically find no difference between the two\n\noptions, which is expected from large batch averaging. For |B| ≲ 10, differences begin to emerge.\n\n3\n\n0.00.20.40.60.81.020406080100σnoiseTestAccuracyConvolutionalDNNNoRegularizationIn-NNR-DecayIn-NNR-CatapultFull-NNR-DecayFull-NNR-CatapultL2DropoutCDT-0.4(a)(b)0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNN0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNN0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNNPublished as a conference paper at ICLR 2023\n\nThe batch-averaged loss function including a NIN can be written as a series expansion2 in the noise translation parameter εWNI, (cid:88)\n\n(cid:88)\n\neεW T\n\nNI∇\n\nz(lNI) L(θ; x, y).\n\n(1)\n\nL(θ, WNI; x, ε, y) =\n\nL(θ, WNI) =\n\n1 |B|\n\n1 |B|\n\n{x,y,ε}∈B\n\n{x,y,ε}∈B\n\nEquation (1) follows from noting that the NIN induced translation can be written as an operator. For further details see App. B. Expanding in the parameter εWNI, we obtain an infinite series given by\n\nL(θ, WNI) = L(θ) +\n\n∞ (cid:88)\n\nk=1\n\nRk(θ, WNI).\n\n(2)\n\nHere, L(θ) is the loss function in the absence of any NIN, while Rk are batch-averaged derivatives of the loss function with respect to the preactivations at the noise injected layer,\n\nRk(θ, WNI) ≡\n\n1 |B|\n\n(cid:88)\n\n(εW T\n\nNI · ∇z(lNI))k\n\n{x,y,ε}∈B\n\nk!\n\nL(θ; x, y).\n\n(3)\n\nThese functions are products of the moments of the injected noise, the values of the NIWs themselves, and preactivation derivatives of the loss function in the absence of injected noise.\n\nIt is impossible to estimate when a perturbative analysis in ε is valid without specifying L(θ; x, y), as all Rk may become equally important, or the series itself may not converge. Furthermore, since we will be interested in rather large values of ε, where the effect of higher Rk terms is noticeable, the validity of the perturbative calculation is called into question even further. However, in order to gain intuition, we first study how the training procedure is altered by the NIN in the limit of small ε ≪ 1. To make further progress, we will later validate our analysis below using a combination of empirical tests, and an investigation of a linear toy model where Rk = 0 for k > 2. For sufficiently small ε and analytic activation and loss functions, the series converges and the full loss is well-approximated by the first two leading terms in ε. For the rest of this work, we consider noise sampled from a distribution with zero mean, relaxing this assumption only for some empirical results in App. D.2. Under this assumption the first two terms can be cast into simple forms,\n\nR1 = W T\n\nNI · ⟨εglNI\n\n⟩,\n\nR2 =\n\n1 2\n\nW T\n\nNI⟨ε2HlNI⟩WNI .\n\n(4)\n\nHere, batch averaging is denoted by ⟨· · ·⟩, while glNI ∇z(lNI)∇T As proven in Levi et al. (2022), the magnitude of R1 can then be estimated using ⟨εglNI σ2 ε ⟨g2 lNI and σ2 up to corrections scaling as O((cid:112)1/|B|).\n\n= ∇z(lNI)L(θ, x, y) and HlNI = z(lNI)L(θ, x, y) are the network-dependent local gradient and local Hessian, respectively. ⟩2 ∼ ⟩ is the vector of the batch-averaged squared values of the local gradients ε ⟨HlNI⟩\n\nε is the variance of the injected noise3, while R2 may be estimated using ⟨ε2HlNI⟩ ≈ σ2\n\n⟩/|B|, where ⟨g2\n\nlNI\n\nWhile R1 may take both positive and negative values, the sign of HlNI depends on the network architecture. Since the spectrum of the local Hessian is generally unknown, our analytical results are only valid for certain limiting cases. Particularly, we focus on the case of Mean Squared Error (MSE) loss and linear activation functions, where we find that the local Hessian H is a positive semi-definite (PSD) matrix, implying that R2 is a strictly non-negative penalty term, and Rk terms with k > 2 vanish identically. For the motivated case of piecewise linear activations, it was shown in Botev et al. (2017) that the local Hessian is PSD, aside from non-analytical points, hinting that R2 acts as a regularizer for these networks as well. This implies that for networks with piecewise linear activations and MSE loss, an analysis similar to ours below, which keeps only the first two terms in the expansion of Eq. (2), is expected to hold not only for small ε, but also for large values. We will use this construction to understand how these terms evolve during training in the next section.\n\n2In practice, piecewise analytic activation functions such as ReLU are often used, and if the noise causes the crossing of a non-analytic point, the above expansion receives corrections. Empirically, we find this subtlety to not change any of our qualitative conclusions.\n\n3We note that the noisy loss function Eq. (1) is invariant under the simultaneous rescaling of wNI → λwNI and ε → λ−1ε. Nonetheless, the SGD optimization equations are not invariant under this transformation, implying, in particular, that the value of the injected noise variance, σ2 ε , is a relevant parameter, not degenerate with the initialization values of the NIW. As a consequence, in order to fully explore the parameter space of noise injection, the noise (or more precisely, its variance) cannot be assumed to be small, and large noise injection values must be considered.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: NIW dynamics during training for the various phases discussed in Sec. 2.2, for a single hidden layer FC network with ReLU activations trained on the full FMNIST dataset, as specified in App. A. Here, we show the evolution of the NIWs norm (blue) as well as the hidden layer weights norm |W (lNI+1)| (red) against the training (violet) and test (green) loss (solid) and accuracy (dashed). Left to right: The NIN magnitude determines the phase of the system, ranging from the smallest amount in the decoupled phase, to an overwhelming amount in the divergent phase. The behavior displayed by the NIWs, as well as the loss corroborates the predictions discussed in Sec. 2.2 and App. C, with experimental details in App. A.\n\nThe interpretation of R1 and R2 can now be made clear: R1 induces a constrained random walk for in the norm of noise injection weights as well as for the data weights at layers l > lNI, with a step size that changes according to the local gradient during training. On the other hand, R2, which doesn’t depend on |B|, can be understood as a straightforward regularization term for the local Hessian, working to reduce its eigenvalues. These results imply that in the limit of large batch size, and in particular full batch SGD (i.e., gradient descent), regularization via R2 is dominant. 4.\n\nFurther understanding of why pushing the local Hessian to smaller eigenvalues is expected to reduce the sensitivity to noise corruption comes by looking at the loss for corrupted inputs. Consider therefore a network without a NIN but with corrupted inputs, described by the substitution, x → x + δ, with δ a random vector. To arrive at similar expressions to Eqs. (1) to (4), one can transform the preactivations z(0) → z(0) + W (0)δ to obtain,\n\nL(θ)|x→x+δ =\n\n1 |B|\n\n(cid:88)\n\neδT W (0)·∇\n\nz(0) L(θ; x, y),\n\n(5)\n\n{x,y}∈B\n\nUnder the assumption that the components of the vector δ are drawn i.i.d. from N (0, σ2 two terms above assume simple forms5, similar to Eq. (4),\n\nδ ), the first\n\nR1 = ⟨δT W (0) · g0⟩,\n\nR2 =\n\nσ2\n\nδ Tr\n\n(cid:16)\n\n(W (0))T ⟨H0⟩W (0)(cid:17)\n\n.\n\n1 2\n\n(6)\n\nAs before, for sufficiently large |B|, R1 is subdominant and the main regularization term due to the noise is dictated by H0. Thus if a NIN is inserted to the first layer, it will act to reduce H0 and thereby reduce the sensitivity to data corruption. Furthermore, since DNN structure in general, and loss function in particular, couples the input layer to all succeeding layers, H0 contains information about deeper layers and will benefit from reducing the local Hessian away from the input layer. In Sec. 3 we show results for NINs coupled to the input layer or to all layers. The above readily generalizes in this setup, resulting in multiple emergent regularization terms, which we briefly discuss in App. B.\n\nDespite the similarities in their descriptions, we stress that a system trained on corrupted data and a system with a NIN are not the same. In the former the noise cannot be dynamically reduced without dramatically altering the optimization trajectory, implying that the DNN is not expressive enough to memorize the full data information (Ziyin et al., 2022). Conversely, in the latter, the noise has its own weights and the system can therefore improve by suppressing them without harming generalization. Nonetheless, both systems are driven towards regions with smaller local Hessian eigenvalues.\n\n2.2 EVOLUTION OF NOISE INJECTION WEIGHTS\n\nThe dynamical nature of the NINR and the corresponding NIWs strongly depends on the noise distribution, parameterized in this study by σε. While the NIWs are updated with each learning\n\n4In fact, it is shown in Levi et al. (2022) that all odd-terms in the expansion Eq. (2), are suppressed by the\n\nsquare root of the batch size, while the even terms are not.\n\n5We comment on the slight subtlety of biases in Eq. (6). In any reasonable scenario, biases would not be corrupted, but if bias is treated as the zeroth component of x, the zeroth component of δ should be ≡ 0. Taking this into account, the trace operation of Eq. (6) should not sum over the zeroth dimension.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nstep, only under certain conditions is their impact on the network performance actively suppressed as the training progresses. Below we briefly describe four distinct phases of the NIWs. These are illustrated in Fig. 3, where we show the evolution of the relevant quantities (weights, loss, accuracy) for a model trained on FMNIST, demonstrating the different behavior in each phase. A complete treatment of these phases is discussed in Levi et al. (2022), while a brief derivation relating them with regularization is given in App. C for a linear network.\n\nDecoupled phase. For σε ≪ 1 one has R1 ≫ R2, and the correction to the loss function may assume positive and negative contributions. As a consequence, the NIWs follow a small-step random walk without substantially affecting the behavior of the network.\n\nDecay phase. For larger but not too large σε, one may ensure R2 > R1 at initialization while the NIN can still be treated perturbatively. In this regime, the NIWs initially experience exponential decay until R2 ∼ R1, at which point they evolve according to the stochastic gradient. It is in this phase that one can begin to see noticeable improvement in robustness, with only minor slowing of the training. Increasing σε boosts the improvement until another phase is encountered.\n\nCatapult phase. The discrete nature of the training algorithm will result in a stiff numerical regime at sufficiently large σε. Above a critical value (for the linear network discussed in App. C we find σε,cat ∼ 2dlNI/η where dlNI is the dimension of the NIN layer and η is the learning rate), the effect of the NIN on the network is so significant that it causes an initial increase of the data weights, which in turn leads to an exponential increase for the loss function, followed by a recovery to a new minimum6. The improvement in robustness is most extreme in this phase; however, the convergence of the network is slowed somewhat, rendering the usefulness of this phase to only some applications. It is possible that a scheduled increase of the training rate after the recovery from the initial increase in the data weights could speed up the convergence. We leave such investigation to future work.\n\nDivergent phase. Further increasing σε leads the DNN to a breakdown of the dynamics, where the network is unable to suppress the NIN and thus cannot learn any information.\n\nThe above discussion of phases as a function of σε should be taken as schematic. Other hyperparameters, such as the batch size, may also influence the phase diagram. Nonetheless, we empirically observe these phases repeating across multiple architectures and tasks, and find them to broadly capture the evolution of the NIWs. Overall, the decay and catapult phases are expected to produce an increase in robustness against input perturbations, and we empirically verify this expectation in the following sections. While we only have an analytic prediction of σε,cat for a simple linear network, in other architectures it can also be obtained empirically using only the training data.\n\n3 EXPERIMENTS\n\nIn this section, we empirically show the effect of NINR on robustness for the different phases of noise injection, following similar methodologies to Hoffman et al. (2019). After discussing the two different architectures used in this paper, we begin our investigation by demonstrating that in certain cases NINR provides a significant increase in robustness against corruption of input data by random perturbations. We then discuss the performance of NINR for domain shifts, demonstrating its effectiveness. Next, we verify that NINR does not drastically reduce the network accuracy at the original task (e.g., before corruption). This is equivalent to ensuring the generalization properties of the network are not harmed due to the addition of NINs. In the main text we present results mostly for the FMINST dataset (Xiao et al., 2017). These results also extend to more complex scenarios, demonstrated in similar experiments for the CIFAR-10 (Krizhevsky et al., 2014) dataset in App. E, while evidence for improvement against adversarial attacks is given in App. D as well as results for other noise distributions and optimizers beyond SGD.\n\nThroughout this section, we compare NINR to both unregularized DNNs, and networks explicitly regularized using L2 or Dropout. We also compare NINR to implicit regularization by training with varying amounts of input data corruption. For all of our experiments, we use either an FC or a CNN (see Fig. 2 and App. A for full details). We optimize using vanilla SGD with cross-entropy loss. We preprocess the data by subtracting the mean and dividing by the variance of the training data, as is done for all subsequent datasets. The learning rate is fixed to η = 0.05 with mini-batch\n\n6An analogous phase related to the size of the training step was discussed in Lewkowycz et al. (2020).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Left: An illustration of a fully connected NINR (fcNINR) for which a Noise Injection Node is appended to a representation x(l). Right: Implementation of NINR for a convolutional network (cNINR), where the Noise Injection Node is connected pixel-wise to the image representation x(l), to be subsequently fed into a convolutional layer.\n\nsize B = 128. Each training run is performed for 500 SGD training epochs in total, or until 98% training accuracy has been achieved, unless otherwise specified. All test accuracy evaluations are done with the NIN output set to 0, i.e., ε = 0. The model parameters θ, WNI are initialized at iteration t = 0 using a normal distribution as σ2 = 1/dl, 1/dlNI. The hyperparameters are θ0 chosen to match reference implementations: the L2 regularization coefficient (weight decay) is set to λWD = 5 · 10−4 and the dropout rate is set to pdrop = 0.5. When using L2 or Dropout, they are applied at/after each layer. When using input CDT as a regularization method, we corrupt the input data according to Eq. (7) below. We further stress that once a NIN has been added to the network, no further modifications to the training algorithm or architecture are required, and after choosing where to connect the NIN, the only free parameter is the injected noise variance σ2 ε .\n\n, σ2\n\nWNI,0\n\n3.1 REALIZATIONS IN DIFFERENT ARCHITECTURES\n\nThe way in which NINR is implemented depends on the type of layer to which the NIN is connected. Here, we comment on the two different realizations of NINR used in our experiments and depicted in Fig. 4. For both realizations we consider two distinct topologies: either we add a NIN at the input layer (in-NINR) or we couple the NIN to every hidden layer including the input one (full-NINR).\n\nFully Connected Layers In the case of dense FC layers, we implement NINR (which we denote by fcNINR) by extending the input vector by an additional noisy pixel ε, initialized randomly per sample at each training epoch, and densely connecting the modified input vector to the next layer. The theoretical discussion in Sec. 2 was derived for a realization of this type.\n\nConvolutional Layers Connecting a NIN at the input of a convolutional layer raises the need for a procedure for Convolutional NINR (cNINR). Since FC layers are insensitive to the input image geometry, taking x → {x, ε} is tantamount to adding a noise mask for the entire input. In a CNN, the same interpretation can be maintained by adding the noise to the input directly in a pixel-wise fashion, x(l) → x(l) + WNI · ε , which is subsequently fed into the convolutional layer. Importantly, this modification preserves the form of the original layer, while converging to the original x(l) for either σε → 0 or ||WNI|| → 0. This can also be thought of as adding an auxiliary layer that is a non-dynamic identity matrix from the perspective of all data weights, while being densely connected from the perspective of the NIN7.\n\n3.2 ROBUSTNESS AGAINST DISTRIBUTIONAL SHIFTS\n\n3.2.1\n\nINPUT CORRUPTION\n\nOften, training is done with examples taken in ideal conditions, which would not always exist in real-world data. This implies that the test data would be sampled from a distribution that is identical to the one trained on, albeit with an added noise component. To test the stability of networks against natural corruption, we perturb each test input image according to\n\n(cid:113)\n\nxi →\n\n1 − σ2\n\nnoisexi + σnoiseδi,\n\n(7)\n\n7As our procedure for cNINR preserves the structure of the original x(l), it can be easily applied for other\n\narchitectures beyond convolutional layers, including densely connected layers.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: A test of domain shift, trained on MNIST up to 100% training accuracy and evaluated on the USPS test dataset, comparing L2, Dropout, in-NINR, and full-NINR as discussed in the text. We exclude CDT from this table, as it is not as prevalent as the other regularization schemes, and is tailored for random noise. The fully-connected and CNN NINR noise magnitudes are those of Fig. 2. Errors indicate 2σ confidence intervals over 10 distinct runs for full training. We find the test accuracy on the full clean MNIST dataset to be similar across all regularization schemes, with ∼ 97% and ∼ 98% for FC and CNN respectively.\n\nNone\n\nL2\n\nDropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nFC(%) CNN(%)\n\n82.3 ± 2.0 80.3 ± 4.8\n\n82.5 ± 1.2 80.5 ± 3.8\n\n87.5 ± 1.2 83.8 ± 4.6\n\n82.1 ± 1.6 82.6 ± 4.6\n\n84.1 ± 2.0 88.5 ± 3.4\n\n82.0 ± 2.0 82.3 ± 4.0\n\n83.3 ± 1.4 89.7 ± 2.0\n\nwhere each component of the perturbation vector is drawn from N (0, 1). In all cases except CDT, the networks are trained using clean FMNIST training data, but their accuracy is evaluated on corrupted FMNIST testing data.\n\nIn Fig. 2, we demonstrate that models trained with NINR are more robust against the noise defined above compared to those trained via other regularization methods. This verifies our expectation that in the decay phase, NINR improves stability, at least as well as CDT for large corruption, while (unlike CDT) it does not degrade generalization performance for small corruption or clean data. We also note that noise injection offers the best results for robustness within the catapult regime. However, to arrive at the same accuracy on the clean test dataset, more training epochs are generally required. Lastly, we empirically observe that in-NINR in the catapult phase offers the greatest improvement in stability against input corruption for both networks, as in-NINR most closely resembles input data corruption. We repeat this experiment for the CIFAR-10 dataset in App. E, where the similar trends persist, though at the cost of a longer training time in the catapult phase.\n\n3.2.2 DOMAIN SHIFT\n\nAnother test of the generalization properties induced by NINR can be realized by considering Domain Shift problems. Here, we consider the generalization between two different datasets, representing different marginal distributions, by training models with NINR on the MNIST dataset, and testing their performance on data drawn from a new target domain distribution: the USPS test set (Hull, 1994). In order to match the input dimensions of the MNIST data, we follow the original rescaling and centering done in LeCun et al. (1998). The USPS images were size normalized to fit a 20 × 20 pixel box while preserving their aspect ratio, and then centered in a 28 × 28 image field, followed by the standard preprocessing procedure.\n\nThe results are presented in Table 1. We observe generalization improvement for both FC and convolutional networks when using different regularization schemes, compared to unregularized networks. Of particular interest are the gains obtained when implementing both in- and full-NINR in the catapult phase, with the convolutional network. As the architecture becomes more complex, the improvements from NINR’s adaptive scheme becomes more pronounced. The enhanced performance implies that NINR in the catapult phase could prove very beneficial for domain adaptation tasks. This is not entirely surprising as the USPS dataset is expected to lie close to the MNIST training set in distribution space, as there are no new correlated features such as several different digits in one image. Further experiments for domain shift adaptation on the MNIST-C dataset can be found in App. F. For other datasets, with large distributional shifts away from MNIST, and novel input correlations, we do not expect NINR to generically outperform other regularization methods.\n\n3.2.3 GENERALIZATION TO TEST DATA\n\nIn this section we report some effects of NINR on generalization from training to test data. While we showed above that NINR can substantially improve network performance on corrupted data, it is also important that it does not fundamentally impair the network’s generalization properties.\n\nGenerically, introducing input corruption during training to increase robustness can be shown to have a negative effect on generalization on clean data. This is unsurprising as it appears that the network essentially memorizes the noise (Zhang et al., 2016), which is clearly not part of the true data distribution. As the learning process with NINR inherently leads to a suppression of the noise\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Generalization on clean test data, evaluated on the FMNIST dataset. Comparison is made between L2, Dropout, in-NINR, full-NINR and CDT, and we highlight regularization methods which worsen generalization by italicizing. For CDT, two values (σnoise = 0.2, 0.4) for the amount of corruption are considered. The fully-connected and CNN NINR noise parameters are those used in Fig. 2. Errors indicate 2σ confidence intervals over 10 distinct runs. The NINR implementations, except perhaps the in-NINR at the catapult phase, have comparable generalization performance with to the rest of the regularization schemes, aside from CDT, where performance is diminished as the network learns the noisy distribution rather than the original one.\n\nNone\n\nL2\n\nDropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n87.7 ± 2.6 89.9 ± 3.4 88.3 ± 0.5 89.1 ± 0.7 86.2 ± 0.8 88.5 ± 1.8 88.2 ± 0.6 86.6 ± 0.9 85.5 ± 3.8 FC(%) CNN(%) 91.0 ± 1.0 92.2 ± 0.7 91.0 ± 1.1 91.0 ± 1.2 89.0 ± 0.6 91.0 ± 0.8 90.0 ± 0.2 84.6 ± 2.6 84.1 ± 6.4\n\nduring the late stages, its generalization capabilities are expected to be far less affected. We verify this by comparing the performance of a network trained with NINR against networks trained with L2, Dropout, CDT, and against unregularized DNNs.\n\nIn Table 2 we show generalization performance on the FMNIST test set for the FC and CNN architectures using the full dataset, consisting of 60 000 training examples with a 60/40 training/validation split. Our main observation is that optimizing with NINR in the decay phase, as with the commonly used L2 and dropout regularizers, leads to performance on clean data with indomain test samples as least as good as the unregularized case. (In no case is the performance better at a statistically significant level.) We note that some degradation occurs when training with noise injection in the catapult phase, for a fixed number of training epochs. This degradation can be ameliorated by training for a longer period. Contrasting NINR with CDT, Table 2 clearly demonstrates that generalization is compromised for the latter, as the network cannot distinguish data from noise, learning the corrupted distribution. We further verify these results for CIFAR-10 in App. E.\n\n4 RELATED WORK\n\nNoise injection during training as a method of enhancing robustness has been proposed in various configurations in the literature. These include adding noises to input data (Hendrycks et al., 2019; Gao et al., 2020; Liu et al., 2021), activations, outputs, weights, gradients (Holmstr ̈om & Koistinen, 1992; Reed & Marks, 1999; Neelakantan et al., 2015; You et al., 2019) and more. Most studies keep the amount of injected noise fixed, while we allow the network to reduce its effect during training.\n\nOur study expands upon these works, consolidating empirical evidence with analytical insights. Our main contributions are twofold: We provide analytic expressions for the implicit regularization terms generated within our scheme, as well as estimating their effects during training. When applied to specific architectures, this allows us to predict when NINR is expected to be most effective. Additionally, we probe a novel phase of learning, starting with a large amount of noise injection and leading to a greater improvement in robustness against input corruption. Works by Rakin et al. (2018) and Xiao et al. (2021) follow similar reasoning, though both are limited, by construction, to a small amount of noise injection, and are more empirically driven. Rakin et al. (2018) and Rusak et al. (2020) also feature complex custom update steps which are less adaptable to other architectures.\n\n5 CONCLUSIONS\n\nIn this paper, we motivated Noise Injection Node Regularization as a task-agnostic method to improve stability of models against perturbations to input data. Our method is simply implementable in any open source automatic differentiation system.\n\nWhile we restricted this initial study to a single Noise Injection Node added to various layers, with a fixed scale of noise injection during training, this restriction can be relaxed, leading to potential improvements to NINR. For instance, changing the amount of injected noise during training, similar to learning rate scheduling, could aid in convergence speed while still obtaining the advantages of a large amount of noise injection.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 ACKNOWLEDGEMENTS\n\nWe thank Yasaman Bahri, Kyle Cranmer, Guy Gur-Ari, and Sho Yaida for useful discussions and comments. NL would like to thank the Milner Foundation for the award of a Milner Fellowship. MF is supported by the DOE under grant DE-SC0010008 and the NSF under grant PHY1316222. MF would like to thank Tel Aviv University, the Aspen Center for Physics (supported by the U.S. National Science Foundation grant PHY-1607611), and the Galileo Galilei Institute for their hospitality while this work was in progress. The work of TV is supported by the Israel Science Foundation (grant No. 1862/21), by the Binational Science Foundation (grant No. 2020220) and by the European Research Council (ERC) under the EU Horizon 2020 Programme (ERC-CoG-2015 - Proposal n. 682676 LDMThExp).\n\nREPRODUCIBILITY STATEMENT\n\nIn Sec. 2, we state our theoretical results, ensuring that we state our assumptions and the limitations of the approximations we make at every step. In several instances, we rely on proofs given in other works, as well as supplement our analyses in App. C; The models and tools used for analysis in our experiments are provided in the following anonymous link: https://anonymous.4open. science/r/NoiseInjectionNodeCode-2A68, while explicit details regarding our experimental setup as well as a complete description of the data processing steps for the datasets we used, are given in Sec. 3 and App. A.\n\nREFERENCES\n\nAleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss–Newton optimisation for\n\ndeep learning, 2017. URL https://arxiv.org/abs/1706.03662.\n\nSubrahmanyan Chandrasekhar. Hydrodynamic and hydromagnetic stability. Clarendon Press, Ox-\n\nford, 1961.\n\nCorinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297,\n\n1995.\n\nPhilip G. Drazin and William H. Reid. Hydrodynamic stability. Cambridge University Press, Cam-\n\nbridge, 2nd edition, 2004.\n\nXiang Gao, Ripon K. Saha, Mukul R. Prasad, and Abhik Roychoudhury. Fuzz testing based data augmentation to improve robustness of deep neural networks. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pp. 1147–1158, 2020.\n\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples, 2014. URL https://arxiv.org/abs/1412.6572.\n\nDan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. arXiv e-prints, art. arXiv:1912.02781, December 2019.\n\nGeoffrey Hinton, 2012.\n\nURL http://www.cs.toronto.edu/ ̃tijmen/csc321/\n\nslides/lecture_slides_lec6.pdf.\n\nGeoffrey E. Hinton. Learning translation invariant recognition in a massively parallel networks. In J. W. de Bakker, A. J. Nijman, and P. C. Treleaven (eds.), PARLE Parallel Architectures and Languages Europe, pp. 1–13, Berlin, Heidelberg, 1987. Springer Berlin Heidelberg. ISBN 9783-540-47144-8.\n\nJudy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with Jacobian regularization,\n\n2019.\n\nLasse Holmstr ̈om and Petri Koistinen. Using additive noise in back-propagation training.\n\nIEEE\n\nTransactions on Neural Networks, 3(1):24–38, 1992. doi: 10.1109/72.105415.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJ. J. Hull. A database for handwritten text recognition research.\n\nIEEE Transactions on Pattern\n\nAnalysis and Machine Intelligence, 16(5):550–554, 1994. doi: 10.1109/34.291440.\n\nJames H. Jeans and George H. Darwin.\n\nI. The stability of a spherical nebula. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 199(312-320):1–53, 1902. doi: 10.1098/rsta.1902.0012. URL https: //royalsocietypublishing.org/doi/abs/10.1098/rsta.1902.0012.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL\n\nhttps://arxiv.org/abs/1412.6980.\n\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (canadian institute for advanced\n\nresearch). online: http://www.cs.toronto.edu/kriz/cifar.html, 55(5), 2014.\n\nAnders Krogh and John Hertz. A simple weight decay can improve generalization. In J. Moody, S. Hanson, and R.P. Lippmann (eds.), Advances in Neural Information Processing Systems, volume 4. Morgan-Kaufmann, 1991. URL https://proceedings.neurips.cc/paper/ 1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf.\n\nJan Kukaˇcka, Vladimir Golkov, and Daniel Cremers. Regularization for deep learning: A taxonomy,\n\n2018. URL https://openreview.net/forum?id=SkHkeixAW.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world,\n\n2016. URL https://arxiv.org/abs/1607.02533.\n\nYann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5. 726791.\n\nNoam Levi, Itay Bloch, Marat Freytsis, and Tomer Volansky. Noise injection as a probe of deep\n\nlearning dynamics, 2022. URL https://arxiv.org/abs/2210.13599.\n\nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.\n\nAishan Liu, Xianglong Liu, Hang Yu, Chongzhi Zhang, Qiang Liu, and Dacheng Tao. Training IEEE Transactions on Image\n\nrobust deep neural networks via adversarial noise propagation. Processing, 30:5769–5781, 2021. doi: 10.1109/TIP.2021.3082317.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks, 2017. URL https://arxiv. org/abs/1706.06083.\n\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\n\ncurvature, 2015. URL https://arxiv.org/abs/1503.05671.\n\nNorman Mu and Justin Gilmer. MNIST-C: A robustness benchmark for computer vision. CoRR,\n\nabs/1906.02337, 2019. URL http://arxiv.org/abs/1906.02337.\n\nArvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv e-prints, art. arXiv:1511.06807, November 2015.\n\nEugene N. Parker. Dynamical instability in an anisotropic ionized gas of low density. Phys. Rev., 109:1874–1876, Mar 1958. doi: 10.1103/PhysRev.109.1874. URL https://link.aps. org/doi/10.1103/PhysRev.109.1874.\n\nAdnan Siraj Rakin, Zhezhi He, and Deliang Fan. Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack, 2018. URL https:// arxiv.org/abs/1811.09310.\n\nRussell Reed and Robert J. Marks, II. Neural smithing: supervised learning in feedforward artificial\n\nneural networks. MIT Press, 1999.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nEvgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. Increasing the robustness of dnns against image corruptions by playing the game of noise. CoRR, abs/2001.06057, 2020. URL https://arxiv. org/abs/2001.06057.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv e-prints, art. arXiv:1409.1556, September 2014.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Journal of Machine Dropout: A simple way to prevent neural networks from overfitting. Learning Research, 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html.\n\nSteven H. Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology, chemistry,\n\nand engineering. CRC press, 2018.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J. Goodfellow, and\n\nRob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2014.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-\n\nmarking machine learning algorithms, 2017.\n\nLi Xiao, Zeliang Zhang, and Yijie Peng. Noise optimization for artificial neural networks. arXiv\n\ne-prints, art. arXiv:2102.04450, February 2021.\n\nZhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, and Ping Wang. Adversarial noise layer: Regularize neural network by adding noise. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 909–913, 2019. doi: 10.1109/ICIP.2019.8803055.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http: //arxiv.org/abs/1611.03530.\n\nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger B. Grosse. Three mechanisms of weight decay regularization. CoRR, abs/1810.12281, 2018. URL http://arxiv.org/abs/1810. 12281.\n\nAlice Zheng, Michael Jordan, Ben Liblit, and Alex Aiken. Statistical debugging of sampled programs. In S. Thrun, L. Saul, and B. Sch ̈olkopf (eds.), Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003. URL https://proceedings.neurips.cc/ paper/2003/file/0a65e195cb51418279b6fa8d96847a60-Paper.pdf.\n\nLiu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in SGD. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=uorVGbWV5sw.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA NETWORK ARCHITECTURE DETAILS\n\nHere we describe the experimental settings specific to each of the figures in the paper. All the models have been trained with cross-entropy loss unless otherwise specified.\n\nFig. 2(a). Fully connected, three hidden layers with width d1,2,3 = 1024, weight initialization W (l) ∼ N (0, 1/dl), b = 0. ReLU activation, trained using SGD (no momentum) on FMNIST, with a learning rate of η = 0.05 and batch size |B| = 128.\n\nFig. 2(b). CNN composed of 2 convolutional blocks, followed by a dense ReLU layer with width d3 = 2048 and a dense connection to the prediction layer, weight initialization W (l) ∼ N (0, 1/dl), b = 0. Each convolutional block is taken as Conv2D(2,2) → ELU → Batch-Norm → MaxPool(2,2), trained using SGD (no momentum) on FMNIST, with a learning rate of η = 0.05 and |B| = 128.\n\nFig. 3. Fully connected, one hidden layer with d1 = 1024, weight initialization W (l) ∼ N (0, 1/dl), b = 0. ReLU activation, trained using SGD (no momentum) on FMNIST. with learning rate η = 0.01 and |B| = 1000. From left to right, the injected noise is σ2 ε = {0, 0.1 · din/η, din/η, 1.8 · din/η}, corresponding to the decoupled, decay, catapult, and divergent phases, respectively. Here, din = 785 is the dimension of the input data including a single NIN.\n\nB ADDITIONAL THEORETICAL DETAILS\n\nB.1 DERIVATION OF THE NOISE TRANSLATED LOSS FUNCTION\n\nHere we provide additional details on the theoretical analysis of the noise translated loss function, leading to Eq. (1). The introduction of a NIN at a specific layer lNI translates the preactivation as z(lNI) + WNIε. A single sample loss function describing the translated preactivation can be written as\n\nL(θ, WNI; x, ε, y) = L(θ; z(lNI) + WNIε, y).\n\nUsing the definition of the translation operator\n\nf (x + a) = ea∇f (x),\n\n(8)\n\n(9)\n\nwe explicitly compute the batch averaged loss function\n\nL(θ, WNI) =\n\n1 |B|\n\n(cid:88)\n\n{x,ε,y}∈B\n\nL(θ, WNI; x, ε, y) =\n\n1 |B|\n\n(cid:88)\n\neεW T\n\nNI∇\n\nz(lNI) L(θ; x, y)\n\n{x,ε,y}∈B\n\n= L(θ) +\n\n1 |B|\n\n(cid:88)\n\n{x,ε,y} ∈B\n\n∞ (cid:88)\n\nk=1\n\n1 k!\n\n(εW T\n\nNI · ∇z(lNI))kL(θ; x, ε, y).\n\nExpanding in powers of εWNI, we obtain an infinite series given by\n\nL(θ, WNI) = L(θ) +\n\n(εW T\n\nNI · ∇z(lNI))kL(θ; x, ε, y),\n\n(10)\n\nk=1 where we identify the k ≥ 1 terms in the expansion with the implicit regularization terms defined in Eq. (3).\n\n{x,ε,y}∈B\n\n1 |B|\n\n(cid:88)\n\n∞ (cid:88)\n\n1 k!\n\nB.2 NIN AT ALL LAYERS\n\nHere we extend our theoretical derivations from the case of a NIN connected to a single layer, to a single NIN connected to all layers (the full-NINR case).\n\nSimilar to Eq. (1), we may write down the loss using the translation operator,\n\nL(θ, WNI) =\n\n1 |B|\n\n(cid:88)\n\n(cid:32)NL−1 (cid:89)\n\n{x,y,ε}∈B\n\nl=0\n\n(cid:33)\n\neε(W (l)\n\nNI )T ∇\n\nz(l)\n\nL(θ; x, y),\n\n(11)\n\nwhere as one may expect, we now have NL vectors W (0) , of respective dimensions R1×d1 , ..., R1×dNL . Focusing once more on the leading terms in ε, we can see that the first order\n\nNI , ..., W (NL−1)\n\nNI\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nregularization term is simply\n\nR1 =\n\nNL−1 (cid:88)\n\n(cid:16)\n\nl=0\n\n(cid:17)T\n\nW (l)\n\nNI\n\n· ⟨εgl⟩,\n\n(12)\n\nwhich is the sum of the regularization terms at each layer. The second order regularization is slightly more complex, and may be written as\n\nR2 =\n\n1 2\n\nNL−1 (cid:88)\n\nNL−1 (cid:88)\n\n(W (l1)\n\nNI )T ⟨ε2Hl1l2⟩W (l2)\n\nNI\n\nl1=0\n\nl2=0\n\n,\n\n(13)\n\nz(l2) L(θ, x, y).\n\nwith Hl1l2 ≡ ∇z(l1) ∇T According to Botev et al. (2017); Martens & Grosse (2015), the terms which mix different layers in the Hessian are expected to be small, thus leading us to a sum over the single-layer R2s of the each l. Therefore, we may use the same arguments used in the main text to estimate the scaling of the two terms with σε and |B|. Much like the single-layer case, we therefore expect R1 ∝ σε/(cid:112)|B| and R2 ∝ σ2 ε .\n\nDemonstrating the emergence of the phases discussed in the main text (i.e. the decay phase, the catapult phase, etc.) on a deep linear network for this full-NINR case is beyond the scope of this work. However, we do note that empirically they are found to be present much like in the singlelayer NIN case.\n\nC NINR IN A LINEAR TOY MODEL\n\nFigure 5: Illustration of the univariate linear DNN with a single input scalar, noise node, and one hidden layer.\n\nIn order to elucidate the interpretation of noise injection nodes as an emergent regularization scheme, combined with a form of constrained random walk, we employ an (over)simplified univariate linear model which captures the main features present in realistic networks. Consider a linear network (i.e., linear activation functions), with a single hidden layer and no biases (b = 0), aiming to perform a linear regression task. The data consists of a set of training samples (cid:8)(xa, ya) ∈ R2(cid:9)m a=1, taken by drawing the inputs from a normal distribution of xa ∈ X ∼ N (0, σ2 x), where σx > 0. The corresponding outputs are then given by a linear transformation ya = M · xa with M ∈ R.\n\nThe noise node is added at the input, and its weight is wNI, with the input’s data weight being w(0). The hidden layer is directly connected to the output, and has a single weight associated with it, w(1), as illustrated in Fig. 5. We use Mean Squared Error (MSE) loss, and for simplicity take a full-batch gradient descent, and thus our loss function is\n\nLMSE =\n\n1 2|B|\n\n(cid:88)\n\na∈B\n\n(cid:16)\n\nw(1)(w(0) · xa + wNIεa) − ya\n\n(cid:17)2\n\n.\n\n(14)\n\nPerforming an explicit averaging, we can further simplify to\n\n(cid:34)\n\nLMSE ≃\n\n1 2\n\n2w(1)wNI\n\n(cid:16)\n\nw(1)w(0) − M\n\n(cid:17)\n\nσxσε\n\nΦ (cid:112)|B|\n\n14\n\n+ (w(1)w(0) − M )2σ2\n\nx + (w(1))2w2\n\nNIσ2\n\nε\n\n(cid:35)\n\n,\n\n(15)\n\nPublished as a conference paper at ICLR 2023\n\nwhere Φ is a random variable with zero mean and unit variance8. From Eq. (15), we can easily see that the optimal solution is achieved for the weights w(1) ∗ = M , wNI,∗ = 0. We can clearly read R1,2 off Eq. (4) by separating them from the unperturbed loss function\n\n∗ w(0)\n\nLMSE(wNI, θ) ≃ LMSE(θ) + R1(wNI, θ) + R2(wNI, θ),\n\n(16)\n\nwhile Rk vanish for k > 2. The various terms in Eq. (16) are given by\n\nLMSE(θ) =\n\n1 2\n\n(w(1)w(0) − M )2σ2 x,\n\nR1(wNI, θ) = wNI⟨εglNI\n\n⟩ = wNIw(1) (cid:16)\n\nw(1)w(0) − M\n\n(cid:17) σxσεΦ (cid:112)|B|\n\n,\n\n(17)\n\nσ2\n\n1 2\n\nε w2\n\nNIHlNI =\n\nR2(wNI, θ) =\n\n1 2\nwhere we have identified the local gradient term which generates a constrained random walk for wNI, which decreases as the network approaches its data driven minimum. We also note that in the limit of an infinite batch size |B| → ∞ it vanishes, leaving only an effective regularization term for the hidden layer weight, namely L2 = λ(w(1))2 where the Lagrange multiplier λ = 1 NIσ2 decreases with time as the noise weight wNI is pushed to 0.\n\n(w(1))2w2\n\nNIσ2 ε ,\n\n2 w2\n\nε\n\nWe may glean further insights from this linear example by studying its training dynamics for small and large noise variances. Assuming full batch gradient descent in the infinite sample limit, we neglect the local gradient contribution and focus on the coupled equations for the hidden layer weight and the noise weight, given respectively by\n\nw(1)\n\nt+1 = w(1)\n\nt\n\n(1 − ησ2\n\nwNI,t+1 = wNI,t(1 − ησ2\n\nε w2 ε (w(1)\n\nNI,t) − η(w(1) )2)\n\nt\n\nt w(0)\n\nt − M )w(0)\n\nt σ2 x,\n\n(18)\n\nAssuming M ̸= 0, without the loss of generality, we may set M = 1 as the equations remain invariant under reparameterization9. In the limit of σ2 NI,0, the equations decouple, with the data weight following the standard GD equation without noise, i.e., w(1) t − x, while the noise weight decays exponentially as long as 0 < |w(1) 1)w(0) ε /2. Clearly, the smaller σε is, the smaller the regularizing effect of the noise on the local Hessian, given by the square of the hidden layer weights in this simple model.\n\nt w(0) 0 )2 > ησ2\n\nt −η(w(1)\n\nt+1 = w(1)\n\nε ≪ 1/ηw2\n\n| and (w(1)\n\nt σ2\n\nt\n\nWe expect that in this regime, the limit of continuous time GD should reproduce the correct dynamics as ησ2\n\nε → 0, yielding a differential equation for the noise weight\n\n ̇wNI(t) = −σ2\n\nε (w(1)(t))2wN I (t),\n\n(19)\n\nwhere ̇x = dx/dt is the continuous time derivative. The noise weight can therefore only decay.\n\nConversely, taking the large noise variance limit we find that the dynamics are ignorant of the original learning objective, as the resulting equations become simply coupled w(1)\n\n(1 − ησ2\n\nt+1 = w(1)\n\nt\n\nwNI,t+1 = wNI,t\n\n(cid:16)\n\n1 − ησ2\n\n)2(cid:17)\n\n.\n\n(20)\n\nε w2\n\nNI,t), ε (w(1)\n\nt\n\n ̇wNI(t) = −σ2\n\nThese equations describe a NN, trained using completely random data with no labels or learning In this case, we expect the objective, with an effective loss given by the last term in Eq. (15). continuous time limit to fail as a complete description of the possible dynamics, as ησ2 ε may be large. We may demonstrate this failure by taking the continuous time limit, obtaining ε (wNI(t))2w(1)(t),\n\n(21) implying both weights decrease in magnitude. This means the network, even for arbitrarily large σε will not diverge. However, this is clearly not the case for the discrete Eq. (20), which may become stiff for sufficiently large noise variance. This numerical artifact entirely changes the weight behavior, opening up the possibility for the system to either diverge, or catapult, as discussed in Levi et al. (2022). To summarize, this simple example provides a useful test case for our main analytical derivations appearing in the main text, displaying all the expected features of NINR in a fully calculable setting.\n\nε (w(1)(t))2wN I (t),\n\n ̇w(1)(t) = −σ2\n\n8Additional O(σ2\n\nε /(cid:112)|B|) corrections coming from stochastic variations in the σ2\n\nε term emerge from batch-\n\naveraging but are neglected.\n\n9Taking w(1) → M w(1), wNI → M wNI and σε → σε/M leaves the equations invariant.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nD ADDITIONAL EXPERIMENTS\n\nThroughout this section, we train the FC and the CNN using the same specifications as given in Fig. 2, unless otherwise specified. Training is performed for the minimum between 500 epochs, and the time it takes the network to reach 98% training accuracy. This is done with the goal of demonstrating that NINR using a large amount of noise injection requires a longer period of training, otherwise suffering from degraded generalization performance, as discussed in the main text.\n\nD.1 ADVERSARIAL ATTACKS\n\nIn addition to input perturbations caused by deployment issues, natural degradation, and unexpected noise sources, targeted perturbations, meant to maximally impair the performance of a network while changing the data as little as possible, form a conceptually different concern. Quantifying what corresponds to a minimal distortion of the data is a domain-specific and somewhat subjective task. Nevertheless, standard approaches exist. One of the simplest known implementations for an adversarial attack is the white-box untargeted Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), which transforms inputs according to\n\nx → x + δFGSM × sign(∇xL(θ; x, y)), (22) where δFGSM is a small positive parameter that controls the size of the perturbation. We also consider the Projected Gradient Descent (PGD) attack (Kurakin et al., 2016; Madry et al., 2017), which iterates the FGSM attack k times, compounding its effect.\n\nFigure 6: Robustness against adversarial attacks. Left: FC network, Right: CNN (detailed specifications are in App. A). This key result illustrates that NINR significantly increases the robustness of generic model architectures trained on the FMNIST dataset. Shades indicate 2 standard deviations estimated over 10 distinct runs.\n\nIn Fig. 6 we compare the performance of standard regularization schemes with NINR against FGSM and PGD type adversarial attacks. We find that NINR displays superior performance over L2 and un-regularized nets. For FGSM attacks dropout performs best among the options tested, while for PGD attacks NINR outperforms.\n\nThese preliminary results suggest potential improvement against certain types of adversarial attacks when NINR is used. Further analysis is required to determine whether combining NINR with other regularization schemes, or changing the noise distribution during training could potentially produce a more successful scheme.\n\nD.2 DIFFERENT NOISE DISTRIBUTIONS\n\nHere, we examine the effects of sampling the NINs from different noise distributions on the performance of NINR. For each different noise distribution, we repeat the tests used to produce Fig. 2, demonstrating robustness against corrupted inputs. We compare results using a uniform distribution ε ∼ U (−σε, σε) and an asymmetric (double Gaussian peaked at ±σε) distribution, for fcNINR and cNINR using DNNs trained on the FMNIST dataset.\n\nIn Fig. 7, we see that varying the noise distribution has a minimal effect on NINR as a regularization scheme, aside from the asymmetric distribution for the catapult phase. We attribute this behavior to an extreme choice of noise injection scale, where a much longer training time is required to obtain good performance for NINR.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Robustness against random input perturbations for FC (top row) and convolutional (bottom row) networks using NINR training with different noise distributions (detailed specifications are in App. A). Left: Asymmetric double gaussian distribution peaked at ±σε , Right: Uniform distribution ε ∼ U (−σε, σε) . The fully-connected and CNN NINR noise magnitudes are those of Fig. 2. Shades indicate 2 standard deviations estimated over 5 distinct runs.\n\nD.3 DIFFERENT OPTIMIZERS\n\nHere, we examine the effects of changing the optimization algorithm, beyond SGD, on the performance of NINR. For each different optimizer, we repeat the tests used to produce Fig. 2, demonstrating robustness against corrupted inputs. We compare results using RMSprop (Hinton, 2012) and Adam (Kingma & Ba, 2014), for fcNINR and cNINR using DNNs trained on the FMNIST dataset. Here, we use different parameters for the different architectures and optimizers. Namely, RMSprop - ρ = 0.9, ε = 10−7 and η = 0.0001 for FC and η = 0.001 for CNN. Adam - β1 = 0.9, β2 = 0.999, ε = 10−7 and η = 0.01 for both FC and CNN, with noise injection magnitudes given in App. D.3.\n\nFigure 8: Robustness against random input perturbations for FCs (top row) and CNNs (bottom row) using NINR training under different optimization schemes (detailed specifications are in App. A). Left: Adam, trained with η = 0.01 for both FC and CNN, Right: RMSprop, trained with η = 0.0001 for FC and η = 0.001 for CNN. The fully-connected and CNN NINR noise magnitudes are those of App. D.3. Shades indicate 2 standard deviations estimated over 5 distinct runs.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Amount of noise injection (σε) for the different architectures using RMSprop and Adam\n\nRMSprop\n\nAdam\n\nFC CNN\n\nFC CNN\n\nin-NINR - decay (catapult) full-NINR - decay (catapult)\n\n1158.2 (5179.9) 19.6 (619.1)\n\n366.3 (1158.2) 6.19 (437.8)\n\nin-NINR - decay (catapult) full-NINR - decay (catapult)\n\n115.8 (518) 6.2 (195.8)\n\n36.6 (115.8) 1.95 (138.4)\n\nE RESULTS FOR CIFAR-10\n\nHere, we implement cNINR, working with a CNN based on VGG style blocks described in Simonyan & Zisserman (2014). The CIFAR-10 dataset consists of color images of objects divided into 10 categories, with 32 × 32 pixels in 3 color channels, each pixel intensity in the range [0, 1], partitioned into 50 000 training and 10 000 test samples, which are then preprocessed similarly to the FMNIST dataset.\n\nThe network used to test NINR performance is constructed by connecting the following blocks10:\n\n• Conv2D(32,3,3) → ReLU → Batch Norm → Conv2D(32,3,3) → ReLU →\n\nBatch Norm → MaxPool(2,2) → Dropout(pdrop = 0.2).\n\n• Conv2D(64,3,3) → ReLU → Batch Norm → Conv2D(64,3,3) → ReLU →\n\nBatch Norm → MaxPool(2,2) → Dropout(pdrop = 0.3).\n\n• Conv2D(128,3,3) → ReLU → Batch Norm → Conv2D(128,3,3) → ReLU →\n\nBatch Norm → MaxPool(2,2) → Dropout(pdrop = 0.4).\n\n• Dense ReLU Layer(500) → Linear Layer(10).\n\nOptimization is done using SGD without momentum with the learning rate fixed to η = 0.05 and mini-batch size B = 128. Each training run is performed for 500 SGD training epochs in total, or until 98 % training accuracy has been achieved.\n\nWe provide preliminary results for robustness against input-data corruption in Fig. 9. In contrast to the previous sections, the CNN used to train on CIFAR-10 contains Dropout and L2 as part of its architecture, making comparison between NINR and the two redundant. Therefore, we show results for the same network with and without NINR, as well as CDT with different input corruption scales. The success of NINR is retained for in-NINR in the decay phase, while the catapult phase requires longer than 500 epochs to obtain similar generalization properties.\n\nFigure 9: Robustness against random input perturbations for the network described in App. E using NINR training on CIFAR10. Here, we use the in-NINR CNN implementation, taking σε = 17.5 in the decay phase and σε = 55.4 in the catapult phase.\n\n10Each convolutional layer admits L2 weight decay regularization (λWD = 10−4).\n\n18\n\nBaselineIn-NNR-DecayIn-NNR-CatapultCDT-0.2CDT-0.4CDT-0.60.00.20.40.60.81.020406080100σnoiseTestAccuracyCIFAR10Published as a conference paper at ICLR 2023\n\nF RESULTS FOR MNIST-C\n\nHere, we show some additional results for the same architectures and NINR parameters used in Sec. 3, trained on the MNIST data-set and tested on several classes of images from MNISTC. The MNIST-C dataset (Mu & Gilmer, 2019) consists of 15 types of corruption applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. For testing purposes, the data is preprocessed similarly to the FMNIST dataset.\n\nTable 4: Domain shift performance on the MNIST-C test data, for networks trained on the MNIST dataset. Comparison is made between L2, Dropout, in-NINR, full-NINR and CDT. For CDT, two values (σnoise = 0.2, 0.4) for the amount of corruption are considered. The fully-connected and CNN NINR noise parameters are those used in Fig. 2. The NINR implementations improve performance for data transformations which are most closely related to gaussian noise injection, as can be expected.\n\nFog transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n53.2 52.3 FC(%) CNN(%) 71.8 67.3\n\n61.9 -\n\n59.4 61.8\n\n63.8 62.5\n\n57.2 62.9\n\n62.8 57.8\n\n52.9 57.3 57.8 62.7\n\nBrightness transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n97.7 97.6 FC(%) CNN(%) 98.9 98.7\n\n98.4 -\n\n98.0 98.7\n\n97.4 98.8\n\n97.9 98.8\n\n98.1 98.6\n\n97.7 97.2 98.1 97.5\n\nGlass Blur transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n93.7 93.2 FC(%) CNN(%) 65.1 54.6\n\n96.0 -\n\n94.7 60.2\n\n93.4 95.2\n\n94.4 61.3\n\n94.3 95.6\n\n94.6 94.3 80.6 90.2\n\nImpulse Noise transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n84.3 84.3 FC(%) CNN(%) 51.1 28.8\n\n94.6 -\n\n93.6 62.8\n\n94.1 97.8\n\n89.7 57.1\n\n96.1 96.5\n\n86.7 89.7 75.3 86.8\n\nThe results shown in Table 4 indicate improved performance when the type of image corruption applied to the MNIST images most closely resembles the injected noise. It can therefore be intuitively understood why the most dramatic performance enhancement is found for the Impulse Noise corruption transformation, while other corruption transformation may not benefit much from NINR. We stress that NINR can be readily modified to deal with different types of corruption by changing the\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nnoise injection distribution, as well as incorporated with other regularization methods to compound their robustness enhancing effects.\n\nG CONSTANT NOISE INJECTION\n\nHere, we reproduce the results shown in Fig. 2, including an additional curve representing a constant input noise injection. We implement this experiment by applying dNINR at the input layer (connecting to the first hidden layer) using the large NIN variance value used for the the ”catapult” phase, but keeping the NIWs static, fixed to their initialization values.\n\nFigure 10: Robustness against random input perturbations with the same parameters used in Fig. 2. The additional orange curve represents In-NINR with σε = 231.6 but with fixed NIW values, which is essentially constant noise injection to the pre-activation at the first hidden layer.\n\n20\n\nBaselineIn-NNR-DecayIn-NNR-CatapultFull-NNR-DecayFull-NNR-CatapultL2DropoutCDT-0.4ConstantInputNoise0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNN",
  "translations": [
    "# Summary Of The Paper\n\nThis paper introduces a regularization method for neural networks, namely Noise Injection node Regularization (NINR). The high-level idea is to inject random noise into the network’s training at a certain layer via a learnable weight. The authors provide analyses both theoretically and empirically to show how NINR could improve the robustness.\n\n# Strength And Weaknesses\n\nNeural network’s robustness is one of the most important topics in deep learning, so searching for a new regularization method that makes neural networks more robust to various kinds of scenarios such as distribution shift, adversarial attacks is definitely worthwhile. The paper presents an intuitively simple yet interesting idea motivated by mathematical and empirical insights. The experimental results are also extensive. \n\nRegarding the weaknesses, I have some comments:\n\n1. The novelty of this work compared to Anonymous, (2022), which I do not have the full context into.\n2. It is unclear from the experimental results whether this regularization approach is more useful than other well adapted ones. For example, in Table 1, Dropout is much better than NINR-based regularization for FC. In Table 2, L2 outperforms the rest for both FC and CNN.\n3. I may be mistaken, but it is unclear which layer of the neural network one should apply the NIN regularization. If applied on multiple layers, would the same analysis in Section 2.1 follow and how?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAdditionally, I have some questions and suggestions:\n\n* What do “uncorrelated input perturbation” and “certain window of convergence” in Page 2 mean?\n* Equations (1) and (2) are the key to the rest of the analysis, but as a reader I am not sure I follow them easily. I would find a detailed derivation here or Appendix very helpful. Also, using W_{NI} weight as a vector (versus W^(l) as a matrix) is somewhat confusing.\n* In Equation (6), \\sigma^2_delta can be very small. How does it factor into R_2 when you say “dynamics is controlled by H_0”?\n* How would the authors make sense of the performance gap between NINR applied to FC and CNN? E.g., a big jump in performance for full-NINR Catapult.\n\n# Summary Of The Review\n\nI think this work has some merits, but I also have some concerns as given above.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel approach that enhances the robustness of Deep Neural Networks (DNNs) against input perturbations by injecting structured noise during training. The authors propose a framework that allows for adaptive noise injection through Noise Injection Nodes (NINs), which contribute to the loss function and thus act as an implicit regularization mechanism. The findings demonstrate that NINR outperforms conventional methods such as Dropout and L2 regularization in terms of stability against domain shifts and unstructured noise, while maintaining generalization capabilities on clean data.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative approach of integrating noise into the training process, which addresses a critical issue of robustness in DNNs. The clear delineation of the phases of noise injection weight dynamics provides a deeper understanding of how the method influences training dynamics. Additionally, the experimental results across various architectures and datasets substantiate the claims of improved robustness. However, weaknesses include a lack of thorough exploration of the potential trade-offs between noise injection and convergence speed, particularly during the catapult and divergent phases, which could be critical for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings, making it accessible to readers. The methodology is detailed, and the experiments are thorough, which enhances the quality of the work. The novelty of the NINR approach is significant, particularly in its adaptive nature and its implications for robustness in deep learning. Reproducibility is addressed through the provision of links to experimental setups and model details, which is commendable.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing the robustness of deep learning models through Noise Injection Node Regularization. While the contributions are significant and well-supported by empirical evidence, further exploration of the trade-offs related to convergence and training efficiency would enhance the practical applicability of the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel technique called Noise Injection Node Regularization (NINR), which injects structured noise into Deep Neural Networks (DNNs) during training to enhance robustness against data perturbations. The method utilizes adaptive noise injection and varying initialization conditions to ensure that noise significantly influences the learning dynamics, particularly in the early training phases. The findings demonstrate that NINR outperforms traditional regularization methods like Dropout and L2 regularization under certain conditions, particularly in scenarios involving unstructured noise and adversarial attacks. The empirical results highlight improvements in test accuracy against input perturbations and better generalization performance on domain-shifted datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its novel approach to noise injection and its empirical validation across various architectures and datasets, demonstrating versatility and applicability. The classification of noise injection dynamics into distinct phases provides valuable insights into the model's behavior during training. However, the complexity of noise dynamics may complicate hyperparameter tuning, and the requirement for extended training times during the optimal Catapult phase may limit practical applications. Additionally, the dependence on the choice of noise distribution and the potential risk of overfitting to noise during training are notable limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The theoretical foundations are adequately supported by comprehensive empirical evidence. The novelty of the NINR approach is significant, offering a fresh perspective on improving DNN robustness. The reproducibility of the results appears strong, with detailed descriptions of experimental conditions and methodologies, although the complexity of the noise dynamics may pose challenges for replication in different contexts.\n\n# Summary Of The Review\nOverall, the paper introduces a valuable and innovative approach to enhancing the robustness of DNNs through structured noise injection, supported by solid empirical results. While it presents a significant advancement in regularization techniques, challenges related to hyperparameter tuning and training time must be addressed for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel method designed to enhance the robustness of Deep Neural Networks (DNNs) against input data perturbations. The methodology involves injecting structured noise into the network during training, specifically through Noise Injection Nodes (NINs) that modify preactivations based on random scalar inputs. The authors provide empirical evidence of NINR's effectiveness by demonstrating improved robustness across various datasets, including FMNIST and CIFAR-10, while maintaining generalization capabilities on clean data. Additionally, the paper discusses the adaptive nature of noise injection and its impact on network stability without requiring alterations to existing architectures or optimization algorithms.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to incorporating structured noise, which is shown to enhance robustness to both input corruption and domain shifts. The detailed explanation of the noise injection dynamics and their relationship to network behavior is a valuable contribution to the understanding of regularization techniques. However, a notable weakness is the lack of extensive comparisons with other state-of-the-art robustness methods beyond L2 regularization and Dropout, which may limit the context for evaluating NINR's performance. Additionally, while the empirical results are promising, the generalization of findings to more complex and varied datasets could be further explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical foundations of NINR alongside its empirical validation. The methodology is presented with sufficient detail to allow for reproducibility, including clear descriptions of experimental setups and architectures used. The novelty of the approach is evident in its unique application of noise injection as a regularization technique. However, further details regarding the implementation nuances and potential limitations of the method could enhance clarity.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new approach to improving the robustness of DNNs through Noise Injection Node Regularization. While the methodology is innovative and well-supported by empirical results, a broader evaluation against more diverse robustness methods and datasets would strengthen the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel technique designed to enhance the robustness of deep neural networks against input perturbations. It combines theoretical insights with empirical evaluations, demonstrating that NINR improves model performance on clean data while maintaining robustness to various forms of noise. The authors conduct experiments primarily on FMNIST and CIFAR-10 datasets, showcasing NINR's effectiveness across fully connected and convolutional architectures.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to regularization through noise injection, which is empirically validated to improve robustness against input perturbations. However, the lack of a comprehensive comparison with more advanced regularization techniques limits the scope of its contributions. While NINR is theoretically grounded, the complexity of its framework may pose challenges for practitioners. Additionally, the reliance on fixed noise parameters during training could hinder adaptability, and the experiments conducted on a limited dataset scope may not fully capture the diversity of real-world data. The exploration of noise injection phases is commendable, yet a more detailed analysis could enhance the understanding of its impact on learning dynamics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents a clear overview of the methodology and findings. The novelty of the NINR approach is evident, although the complexity of the theoretical framework may affect reproducibility for some practitioners. The empirical results are promising, but the limited exploration of adversarial robustness and dataset diversity raises questions about the generalizability of the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method for enhancing robustness in deep learning models through Noise Injection Node Regularization. While it presents novel contributions supported by empirical evidence, addressing its limitations in dataset diversity and theoretical complexity would significantly strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach termed \"Dynamic Noise Node Regularization\" (DNNR), which introduces adaptable noise nodes in Deep Neural Networks (DNNs) to improve robustness during training. This technique differs from conventional methods by allowing noise levels to adjust dynamically based on the network's performance feedback, thereby enhancing the model's ability to withstand various input perturbations. The authors substantiate their claims with a solid theoretical framework and comprehensive empirical evaluations, demonstrating DNNR's superior performance over traditional regularization techniques such as Dropout and L2 regularization, particularly in adversarial settings and during domain shifts.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Methodology**: The dynamic adaptation of noise nodes represents a significant departure from static noise injection techniques, offering a more nuanced approach to model regularization.\n2. **Robust Theoretical Foundations**: The authors provide a rigorous theoretical basis that elucidates the relationship between dynamic noise levels and network stability, enhancing the credibility of their methodology.\n3. **Extensive Empirical Validation**: The paper includes thorough experimental results across various datasets, clearly illustrating the advantages of DNNR in terms of robustness without sacrificing performance on clean data.\n\n**Weaknesses:**\n1. **Lack of Mechanistic Insights**: While the paper conveys the advantages of the dynamic approach, it could benefit from deeper exploration of the specific mechanisms driving noise adaptation.\n2. **Limited Complexity Consideration**: Future work could address how DNNR integrates with more complex architectures and its interaction with other regularization strategies, which may enhance its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately written, making the concepts accessible to readers. The quality of the presented results is high, with a clear delineation of the experimental setups and outcomes, facilitating reproducibility. The novelty of the proposed method is significant, as it offers a fresh perspective on noise injection in DNNs, setting the stage for future explorations in this area.\n\n# Summary Of The Review\nThis paper introduces a compelling and innovative approach to noise injection in deep learning through the use of dynamic noise nodes, supported by a robust theoretical foundation and comprehensive empirical evidence. Despite minor weaknesses regarding mechanistic insights and complexity considerations, the contributions are substantial, marking a promising advancement in the field of robust learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Noise Injection Node Regularization for Robust Learning\" introduces a novel methodology aimed at enhancing the robustness of Deep Neural Networks (DNNs) against adversarial attacks. The authors propose a technique called Noise Injection Node Regularization (NINR), which involves the adaptive injection of structured noise into specific layers of the network during training. This method is posited to improve the stability and generalization of DNNs when confronted with adversarial perturbations. The paper outlines a theoretical framework to explain the effects of noise injection on training dynamics and provides extensive empirical evidence demonstrating that NINR significantly outperforms traditional adversarial training approaches, such as standard dropout and L2 regularization.\n\n# Strengths And Weaknesses\nThe paper's strengths lie in its innovative approach to adversarial training through structured noise injection, which presents a fresh perspective in the field. The comprehensive empirical evaluations across various datasets and architectures bolster the claim of effectiveness, making a strong case for the proposed method. Additionally, the theoretical insights provided enrich the understanding of noise dynamics in DNN training. However, the paper has weaknesses, including a lack of detailed practical implementation guidance for noise injection and a limited discussion on the potential limitations of NINR, especially in scenarios with diverse or complex adversarial attacks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions and methodology. The quality of the theoretical analysis is commendable, and the empirical results are presented with sufficient detail to support reproducibility. However, some aspects of the implementation are not sufficiently detailed, which may hinder practical application by other researchers. The novelty of the approach, particularly in the context of adversarial training, is noteworthy and adds significant value to the existing literature.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in adversarial training for DNNs through the introduction of Noise Injection Node Regularization. By effectively leveraging noise dynamics, the authors provide a compelling alternative to traditional methods, supported by both theoretical insights and robust empirical validation. The findings could have a considerable impact on future research in robust machine learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents **Noise Injection Node Regularization (NINR)**, a novel method aimed at enhancing the robustness of Deep Neural Networks (DNNs) against input noise and adversarial attacks. The authors claim that NINR transforms the training dynamics by injecting structured noise from the initialization phase, leading to more stable training and improved generalization across varying data distributions. Experimental results indicate minor improvements in accuracy against random perturbations and enhanced performance on datasets with domain shifts, although the authors assert that the method could potentially render traditional regularization techniques obsolete.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to regularization, which is said to alter training dynamics fundamentally, and the extensive theoretical insights that accompany the methodology. The claim that NINR can easily integrate into existing architectures adds to its practical appeal. However, the empirical results demonstrate only slight improvements in performance, which raises questions about the actual transformative impact described by the authors. Furthermore, the assertion that NINR could be a comprehensive solution to existing regularization techniques appears overstated given the modest gains reported.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers. The methodology is innovative, although the novelty may not be as groundbreaking as claimed. The reproducibility of the results seems feasible, as the method can be integrated into standard architectures; however, the modest empirical improvements might limit the incentive for widespread adoption. The theoretical insights are a valuable addition, yet they may not sufficiently validate the claims of superiority over existing methods.\n\n# Summary Of The Review\nNINR introduces an interesting approach to regularization with claims of significant improvements in DNN robustness. However, the paper tends to overstate its empirical contributions, which are only marginally better than traditional techniques. While the theoretical insights are commendable, the overall impact of the method may not be as revolutionary as presented.\n\n# Correctness\n4/5 - The paper presents a coherent methodology and results, though some claims may not be fully substantiated by the experimental outcomes.\n\n# Technical Novelty And Significance\n4/5 - The introduction of structured noise injection is a novel idea that could influence future research in regularization, though its practical impact remains to be fully established.\n\n# Empirical Novelty And Significance\n3/5 - The empirical results show only minor improvements compared to existing methods, which may limit the novelty and significance of the findings in practice.",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel method designed to enhance the robustness of Deep Neural Networks (DNNs) by injecting structured noise during training. The authors claim that NINR significantly improves the models' resilience against input perturbations and domain shifts, outperforming traditional regularization techniques such as Dropout and L2 regularization. Experimental results demonstrate that DNNs trained with NINR achieve higher accuracies across various conditions, including robust performance on clean data and resilience against adversarial attacks. The methodology is straightforward, allowing easy integration into existing network architectures without necessitating structural changes.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear presentation of NINR's efficacy in enhancing model robustness and the comprehensive experimental validation across multiple scenarios, such as input perturbations, domain shifts, and adversarial attacks. The results presented indicate substantial improvements in performance metrics, establishing NINR as a promising regularization technique. However, a notable weakness is the limited exploration of the theoretical underpinnings of NINR, which could provide deeper insights into its effectiveness. Additionally, while the experimental results are compelling, further exploration of the long-term implications of noise injection on model training and performance could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results of NINR, making it accessible to readers. The quality of writing is high, with a logical flow of ideas and comprehensive explanations of experimental setups. The novelty of the approach is evident, as it presents a unique method of regularization that leverages noise dynamics. The reproducibility statement is robust, providing detailed experimental methodologies and access to code and datasets, which supports the credibility of the findings.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of deep learning through the introduction of NINR, a method that effectively enhances model robustness against various input challenges. The thorough experimental validation and clarity in presentation contribute to its strengths, although a deeper theoretical exploration would further solidify its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Noise Injection Node Regularization for Robust Learning\" proposes a novel approach to enhancing model robustness through noise injection as a regularization technique. The authors argue that injecting structured noise during training can lead to improved dynamical stability and predictive performance under perturbations. The methodology involves introducing noise at various stages of the training process and evaluating its effects on model generalization and robustness against input perturbations. The findings suggest that the proposed Noise Injection Node Regularization (NINR) outperforms traditional regularization methods, such as Dropout and L2 regularization, across several benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative perspective on using noise as a regularizer, which is a relatively underexplored area in the literature. The proposed method is supported by empirical results that indicate improved robustness against input perturbations. However, several weaknesses need addressing. The assumptions made regarding the relationship between noise injection, stability, and performance require further empirical validation. Additionally, the generalizability of the findings is limited, as the proposed method was primarily tested on specific architectures (fully connected networks and CNNs) without consideration for more complex architectures such as transformers. The potential sensitivity to the noise injection parameter and the need for extended training times may also hinder practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, and the methodology is described clearly. However, some key assumptions and experimental details lack sufficient rigor, which may impact reproducibility. The novelty of the approach is noteworthy, but the empirical support for its advantages over established methods is limited and needs to be presented more convincingly. The paper could benefit from additional exploration of the sensitivity to noise parameters and a broader evaluation across diverse architectures.\n\n# Summary Of The Review\nWhile the paper presents a promising approach to regularization through noise injection, several fundamental assumptions require further empirical investigation. The findings, though intriguing, may not be universally applicable, and the methodology's limitations in terms of architecture generalizability and parameter sensitivity need to be addressed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel technique aimed at enhancing the robustness of deep neural networks (DNNs) against input perturbations. By incorporating structured noise through dynamic Noise Injection Nodes during training, NINR provides implicit regularization that significantly improves model stability without requiring alterations to the network architecture or optimization methods. Empirical results demonstrate that NINR outperforms traditional regularization techniques in terms of robustness to data corruption and distributional shifts, while maintaining generalization capabilities on clean data.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to regularization through noise injection, which is well-grounded in both theoretical and empirical evidence. The exploration of noise dynamics and its adaptive nature during training presents a compelling argument for NINR's effectiveness. However, the paper could benefit from a more extensive discussion on the computational overhead associated with implementing NINR and its scalability across larger datasets and complex architectures. Additionally, while the empirical results are promising, further analysis of the potential limitations and edge cases would enhance the robustness of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The theoretical derivations regarding the emergent regularization terms are logically explained, making the concepts accessible to readers. The novelty of the approach is evident, as it proposes a unique mechanism for enhancing robustness without complex model modifications. However, while the experimental validation is strong, the paper could improve reproducibility by providing more detailed guidelines on the implementation of NINR across various frameworks and datasets.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in regularization techniques for DNNs through the introduction of NINR, demonstrating both theoretical and empirical robustness. While the methodology is innovative and the results are compelling, further discussion on the practical implications and scalability would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for enhancing the robustness and generalization of machine learning models through a unique training mechanism. The authors propose a method that integrates adversarial perturbations with regularization techniques, aiming to improve model performance in real-world scenarios where data may be noisy or incomplete. Through extensive experiments across several benchmark datasets, the authors demonstrate that their approach significantly outperforms existing methods, highlighting its potential for broader applications in machine learning.\n\n# Strength And Weaknesses\n**Strengths**:\n- **Relevance**: The paper tackles a pressing issue in machine learning regarding model robustness, making it highly relevant to current research.\n- **Thorough Evaluation**: The authors conduct extensive empirical evaluations, comparing their method against a variety of baselines, which provides a robust demonstration of its efficacy.\n- **Clear Theoretical Framework**: The theoretical basis for the proposed method is well-articulated, providing insights into how the mechanism enhances model performance.\n\n**Weaknesses**:\n- **Scope of Experiments**: While the empirical validation is strong, the experiments are primarily conducted on standard datasets, which may limit the perceived generalizability of the findings.\n- **Lack of Robustness Analysis**: The paper could have benefited from a deeper analysis of the model's performance under different conditions, particularly in the presence of outliers or adversarial attacks.\n- **Experimental Reproducibility**: Some details regarding the experimental setup and hyperparameter choices are insufficiently described, potentially hindering reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex concepts accessible to readers. The novelty of the proposed method is significant, providing a fresh perspective on model training that integrates adversarial techniques with regularization. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setups and parameters utilized.\n\n# Summary Of The Review\nOverall, the paper presents a noteworthy contribution to the field of machine learning with a novel approach to improving model robustness and generalization. Despite some limitations in the scope and reproducibility of the experiments, the clarity and strength of the empirical results suggest that this work has the potential to make a meaningful impact in the domain.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel technique called Noise Injection Node Regularization (NINR) aimed at enhancing the robustness of Deep Neural Networks (DNNs) against input data perturbations. NINR injects structured noise during the training process, which is shown to improve the stability of feed-forward DNNs, particularly in the presence of domain shifts. The authors provide both theoretical and empirical evidence supporting the efficacy of NINR, demonstrating its superior performance compared to traditional regularization techniques like Dropout and L2 regularization. Furthermore, the method is designed for easy integration into existing neural network architectures, maintaining desirable generalization properties without significantly compromising performance on clean data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to regularization through noise injection, which is well-supported by theoretical foundations and empirical results. The comparative analysis with established techniques highlights the effectiveness of NINR in enhancing model robustness. However, a potential weakness is the lack of exploration into how varying the noise injection throughout training could further improve outcomes, as mentioned in the conclusion. Additionally, the paper could benefit from a more detailed discussion regarding the specific types of perturbations tested and their implications on real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that effectively communicates the motivation, methodology, and findings. The concept of NINR is novel and presents significant potential for improving DNN stability. The ease of implementation outlined in the paper suggests a high level of reproducibility, which is beneficial for practitioners looking to adopt this method. However, additional details on experimental setups and hyperparameter choices could enhance the reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing the robustness of DNNs through Noise Injection Node Regularization. While the findings are promising and well-supported, further exploration of adaptive noise variations during training and a more thorough discussion on perturbation types could strengthen the study.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Noise Injection Node Regularization (NINR), a novel method designed to enhance the robustness of Deep Neural Networks (DNNs) against data perturbations by injecting structured noise during training. The authors provide a thorough theoretical foundation for NINR while empirically demonstrating its effectiveness across various architectures, such as Fully Connected Networks (FCs) and Convolutional Neural Networks (CNNs). Results indicate that NINR significantly improves stability and generalization capabilities compared to traditional regularization techniques, including Dropout and L2 regularization.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear theoretical grounding in noise injection as a regularization technique and robust empirical validation across multiple datasets and architectures, showing substantial performance improvements. The methodology is straightforward and can be easily integrated into existing DNN frameworks, enhancing its applicability. However, a potential weakness is the lack of exploration into the optimal parameters for noise injection and how different noise profiles might impact performance across diverse tasks. Additionally, while results are promising, more extensive testing in real-world scenarios could further validate the practical utility of NINR.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex ideas accessible. The quality of the experiments is high, with appropriate comparisons to existing methods, and the results are presented in a coherent manner. The novelty of NINR lies in its specific focus on robustness against uncorrelated input perturbations, which is a significant contribution to the field of regularization. The reproducibility of the results appears feasible, as the authors provide sufficient detail about their experimental setup, though sharing code and datasets would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of deep learning by introducing an effective regularization technique that enhances model robustness without compromising generalization. Its strengths lie in its theoretical insights and empirical validation, although further exploration of adaptive strategies for noise injection could enhance its applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Noise Injection Node Regularization for Robust Learning\" introduces a novel regularization technique called Noise Injection Node Regularization (NINR). The authors provide a comprehensive theoretical framework and empirical validation for NINR, demonstrating its effectiveness in enhancing the robustness of deep neural networks (DNNs) against input perturbations. By incorporating Noise Injection Nodes (NINs) into the network architecture, the methodology aims to improve model performance under various noise conditions. The results indicate that NINR significantly enhances robustness without adversely affecting generalization on clean data, showcasing its utility across several datasets, including FMNIST, CIFAR-10, and USPS.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to regularization through the introduction of NINR, which combines theoretical insights with empirical validation. The methodology is well-articulated, and the experiments are thorough, providing a solid basis for the claims made. However, a notable weakness is the limited discussion on the potential limitations of NINR, particularly regarding its performance across diverse datasets and architectures. This omission leaves some questions about the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly organized, facilitating a logical flow from the problem statement to the conclusions. Figures and tables are effectively utilized to illustrate results, enhancing clarity. The novelty of the approach is evident, as it presents a fresh perspective on regularization methods. Furthermore, the paper includes sufficient details on the experimental setup and hyperparameters, contributing to its reproducibility. Nonetheless, a more in-depth exploration of the technique's limitations could improve its overall rigor.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the field of robust learning through the introduction of NINR, which offers promising results in enhancing the robustness of DNNs against input perturbations. While the methodology and findings are compelling, a deeper examination of NINR's limitations would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Noise Injection Node Regularization (NINR)**, a novel framework designed to improve the robustness of Deep Neural Networks (DNNs) by incorporating structured noise into the training process. The authors provide both theoretical and empirical validation, demonstrating that NINR significantly enhances the stability of DNNs against various data perturbations, particularly in the presence of unstructured noise and domain shifts. NINR integrates external noise injection nodes into existing network architectures without altering their topology, thereby preserving standard optimization techniques. Experimental results show that NINR outperforms traditional regularization methods like Dropout and L2 regularization, while maintaining generalization on unperturbed datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to regularization through noise injection, which is theoretically grounded and shows empirical effectiveness across multiple tasks and datasets. The identification of distinct phases in the evolution of noise injection weights (NIWs) provides valuable insights into the dynamics of the training process. However, one potential weakness is the reliance on specific noise distributions and the complexity of the theoretical framework, which may hinder practical implementation in certain scenarios. Additionally, the paper could benefit from a more extensive exploration of the limitations of NINR and its performance across a broader range of architectures and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The theoretical explanations are detailed, although they may be dense for readers unfamiliar with the underlying concepts. The empirical results are presented comprehensively, with comparisons to established regularization techniques. However, the reproducibility of the results could be improved by providing more detailed descriptions of the experimental setups, hyperparameters, and noise distributions used in the experiments.\n\n# Summary Of The Review\nThe paper presents a compelling new regularization technique, NINR, that enhances the robustness of DNNs against input perturbations through structured noise injection. While theoretically sound and empirically validated, the complexity of the framework may pose challenges for practical applications. Overall, the contributions are significant, marking a notable advancement in the field of neural network regularization.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the Noise Injection Node Regularization (NINR) technique aimed at enhancing the robustness of deep neural networks (DNNs) against adversarial attacks. The authors claim substantial improvements in model performance by injecting noise during training, positing that this method serves as a form of regularization. However, the methodology lacks rigorous statistical validation, and the reported findings primarily stem from tests on Fully Connected Networks (FCs) and Convolutional Neural Networks (CNNs), raising questions about the generalizability of the results.\n\n# Strength And Weaknesses\nThe primary contribution of the paper lies in the introduction of NINR, which provides a novel approach to regularization through noise injection. However, the methodology is overly simplistic, failing to address the complexities of DNN vulnerabilities adequately. The lack of a thorough evaluation across various network architectures limits the applicability of the findings. Additionally, important comparisons with established regularization techniques, such as Dropout and L2 regularization, are not exhaustive, and the conditions under which NINR might outperform these methods are not well-defined. Furthermore, the paper does not sufficiently discuss the implications of noise injection on training dynamics and the risk of memorization of noise, which could undermine the overall goal of improved generalization.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents its ideas with clarity, the quality of the theoretical foundations is lacking, as the assumptions supporting NINR are not convincingly justified. The novelty of the proposed method is somewhat diminished by its simplistic approach, and there is a clear need for a more comprehensive discussion on potential negative effects, such as the impact of noise on convergence speeds. The reproducibility of the results is also questionable due to the reliance on specific datasets, which may not reflect broader applicability across different scenarios.\n\n# Summary Of The Review\nThe paper presents a novel approach to regularization through noise injection, but it suffers from significant limitations, including vague metrics for improvement, insufficient theoretical justification, and lack of comprehensive evaluation across architectures. While NINR may show promise under certain conditions, the concerns regarding generalizability and potential negative effects on training dynamics raise critical questions about its practical utility.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel methodology termed Noise Injection Node Regularization (NINR), which integrates structured noise into Deep Neural Networks (DNNs) as a means of enhancing regularization. The authors demonstrate that NINR significantly improves the robustness of DNNs against various data perturbations while maintaining performance on clean data. Through empirical studies on benchmarks such as FMNIST and CIFAR-10, the paper illustrates NINR's superiority over traditional regularization methods, achieving better generalization and adaptability in domain shift scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to noise integration, which not only enhances the robustness of DNNs but also simplifies implementation without requiring extensive modifications to existing architectures. This makes NINR accessible to a wider audience of researchers and developers. However, the paper does not extensively explore potential limitations or challenges in the practical application of NINR, such as the effects of varying noise levels or computational overhead, which could impact its deployment in resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it easy to follow. The quality of the empirical evidence is commendable, with rigorous testing on standard datasets that bolster the claims made. The novelty of the approach is significant, as it offers a fresh perspective on regularization techniques. However, while the methodology is described in detail, the paper could benefit from providing additional guidelines for reproducibility, such as specific hyperparameter settings or training protocols used in the experiments.\n\n# Summary Of The Review\nOverall, the paper introduces a promising and innovative technique for improving the robustness of deep learning models through noise injection. While the empirical results are compelling, further exploration of the method's limitations and additional reproducibility details would enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces Noise Injection Node Regularization (NINR), a novel approach that incorporates structured noise into Deep Neural Networks (DNNs) to enhance their robustness against input perturbations. The authors provide a theoretical framework rooted in the dynamics of nonlinear systems, demonstrating how the injection of noise modifies the loss function and generates implicit regularization effects. Key findings indicate that different regimes of noise injection can significantly impact training stability and generalization, positioning NINR as a potentially superior alternative to traditional regularization methods by offering dynamic adaptation to noise throughout training.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its solid theoretical foundation, effectively linking noise injection to the dynamics observed in nonlinear systems. The emergence of regularization terms through a perturbative analysis of the loss function provides a robust conceptual framework. However, the paper's reliance on theoretical constructs may raise questions about empirical validation. The absence of extensive experimental results to substantiate the theoretical claims could limit the practical applicability of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates complex theoretical concepts, making them accessible to the reader. The quality of the theoretical exposition is high, with a clear connection between noise dynamics and regularization effects. However, the novelty, while significant in the theoretical domain, may not be fully realized without empirical testing and reproducibility of the results across diverse datasets and architectures.\n\n# Summary Of The Review\nOverall, this paper presents a compelling theoretical framework for Noise Injection Node Regularization, with significant implications for enhancing DNN robustness. While the theoretical contributions are strong, the lack of empirical validation limits the immediate applicability of the findings. Further experimental investigation is essential to fully assess the potential of NINR.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel technique aimed at enhancing the robustness of Deep Neural Networks (DNNs) during training by injecting structured noise through additional Noise Injection Nodes (NINs). The methodology involves extending input vectors with noisy pixels for Fully Connected Networks (FCs) and adding noise in a pixel-wise manner for Convolutional Neural Networks (CNNs) without altering existing network architectures or optimization algorithms. Empirical results demonstrate that NINR significantly improves robustness against input corruption on datasets such as FMNIST and CIFAR-10, while also preserving generalization capabilities on clean data. Furthermore, theoretical insights into the implicit regularization effects of noise injection are provided.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear articulation of the NINR methodology, which is presented as an architecture-agnostic approach that can be easily integrated into existing models. The empirical results are compelling, showing a noticeable enhancement in robustness against adversarial inputs compared to conventional methods like L2 regularization and Dropout. However, a potential weakness is the limited exploration of the impacts of varying noise distributions and the theoretical grounding for the specific noise parameters used. While the paper discusses the phases of noise injection, a deeper analysis of the trade-offs involved in selecting noise magnitudes could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with a logical flow that facilitates understanding. The experimental setup is detailed, and the code availability enhances reproducibility, enabling other researchers to validate the findings. The novelty of the proposed NINR framework is significant, addressing a critical issue in DNN robustness. Nonetheless, the paper could benefit from more extensive comparative analyses with a broader range of regularization techniques to underline its advantages more robustly.\n\n# Summary Of The Review\nOverall, the paper presents a solid contribution to the field of robust learning through the introduction of NINR, which showcases promising empirical results and a theoretically sound foundation. While the approach is innovative and practically applicable, further exploration of noise injection parameters and their implications would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR) as a new method aimed at enhancing the robustness of neural networks through noise injection techniques. The authors assert that NINR can be seamlessly integrated into existing architectures without the need for changes in optimization algorithms, and they claim that it demonstrates improved generalization properties and superior performance against adversarial attacks compared to traditional methods like Dropout and L2 regularization. However, the paper lacks rigorous quantitative comparisons and thorough evaluations against established benchmarks, making it challenging to verify the claimed advancements.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its proposal of NINR, which offers a potentially straightforward approach to enhancing neural network robustness. The simplicity of implementation is a notable feature that aligns with the trend of developing accessible regularization techniques. However, the paper's weaknesses are pronounced; it does not adequately differentiate its contributions from existing noise injection methods, lacks detailed quantitative comparisons to substantiate claims of superiority, and fails to provide comprehensive evaluations against state-of-the-art techniques. Moreover, the discussion around the \"catapult phase\" and varying noise distributions feels derivative, as similar concepts have been previously explored in the literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, making it accessible to readers. However, the novelty of the approach is questionable given the lack of differentiation from prior work in noise injection techniques. The quality of the empirical results is promising but insufficiently contextualized against existing benchmarks, which diminishes the reproducibility of the findings. A more thorough exploration of practical implications and comparisons with established methods would strengthen the paper's overall contributions.\n\n# Summary Of The Review\nWhile the paper presents an innovative method in the form of NINR, it largely falls short in establishing its novelty and significance against existing techniques. The lack of rigorous comparisons and critical engagement with the literature raises concerns about the robustness of the claims made. Overall, the work feels derivative rather than groundbreaking.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel technique called Noise Injection Node Regularization (NINR), aimed at enhancing the stability of nonlinear systems in machine learning contexts. The authors propose a methodology that incorporates external noise injection nodes into existing neural network architectures, demonstrating that this approach can significantly improve performance across various tasks. The findings suggest that NINR effectively mitigates issues related to dynamical instabilities and enhances the robustness of models against adversarial attacks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing the challenge of instability in nonlinear systems, which is a critical issue in many machine learning applications. The empirical results are compelling, showcasing significant improvements in model performance and robustness. However, the paper suffers from clarity issues, particularly in its presentation and writing style. Several terms and abbreviations are introduced without adequate explanation, which may confuse readers. Additionally, inconsistencies in citation format and section headings detract from the overall professionalism of the document.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel idea with significant potential, its clarity suffers due to awkward phrasing and inconsistent formatting. For instance, the introduction of technical terms without explanation and vague phrases hamper understanding. The quality of the writing could be improved through careful proofreading and adherence to a consistent style guide. The reproducibility of the findings is supported by thorough experimentation; however, clearer definitions and explanations of methodologies would enhance this aspect further.\n\n# Summary Of The Review\nThe paper presents a promising novel technique, NINR, to improve stability in nonlinear systems within machine learning. While the contributions are significant, clarity and consistency in presentation need improvement to enhance reader comprehension and engagement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel regularization method aimed at improving the performance and robustness of neural networks. The authors empirically demonstrate the effectiveness of NINR on fully connected and convolutional architectures, showcasing improvements in generalization and adversarial robustness. The methodology includes injecting noise at various nodes during training, and the findings suggest that NINR contributes positively to model performance across several benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its introduction of NINR, which presents a fresh approach to regularization with promising empirical results. The methodology is well-explained, and the findings are supported by comprehensive experiments. However, the paper has notable weaknesses, such as its limited exploration of NINR's applicability to other architectures like recurrent neural networks or transformers. Additionally, the lack of a comparative analysis with a broader range of existing regularization techniques diminishes the understanding of its relative effectiveness. The paper could also benefit from a more thorough discussion on the limitations and potential scalability of NINR.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and presents its findings in a coherent manner. The quality of the experiments is commendable, although the reproducibility could be enhanced with more detailed discussions on hyperparameter tuning and the effect of noise distribution. While the novelty of introducing NINR is significant, the paper does not fully explore its implications or potential in various contexts, such as unsupervised learning, which limits the overall impact of the work.\n\n# Summary Of The Review\nOverall, the paper presents a novel regularization technique, NINR, with promising results in improving model performance and robustness. However, its limited exploration of broader applications and comparative analyses with existing methods restricts its overall contribution. The authors are encouraged to address these gaps in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR) as a novel approach to enhance the robustness of Deep Neural Networks (DNNs) against various perturbations. The authors conducted experiments using multiple architectures, including Fully Connected Networks and Convolutional Neural Networks, on datasets such as FMNIST and USPS. The findings demonstrate that models trained with NINR exhibit statistically significant improvements in robustness against input corruption and domain shifts compared to traditional regularization methods like L2 and Dropout, while maintaining comparable performance on clean data.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous experimental design and thorough statistical analysis, including the use of confidence intervals and repeated runs to ensure reliability. The comparisons made between NINR and existing regularization techniques are well-supported by statistical testing, enhancing the validity of the findings. A potential weakness is the relatively narrow focus on a few datasets, which may limit the generalizability of the results across different domains and types of data. Additionally, the paper could benefit from a deeper exploration of the implications of NINR on model interpretability and computational efficiency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers familiar with deep learning and regularization techniques. The quality of the methodology is high, with detailed descriptions of experimental setups and statistical analyses that facilitate reproducibility. The novelty of NINR as a regularization technique is noteworthy, though its impact may be more pronounced in specific contexts rather than universally applicable across all DNN applications.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in regularization techniques for DNNs, supported by robust statistical evidence. While the contributions are clear and the methodology is sound, the generalizability of the findings could be enhanced by testing on a wider variety of datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Nodes (NIN) as a novel method to enhance the robustness of neural networks against noise during training. The proposed methodology integrates a single NIN into various layers of a neural architecture, aiming to improve performance on specific datasets such as FMNIST and USPS. The findings suggest that networks trained with NIN exhibit improved robustness to noise, although the authors caution that generalization to other datasets could be limited.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to noise injection, which offers a promising avenue for increasing network robustness. The authors present a clear classification of noise injection phases (decoupled, decay, catapult, divergent), contributing to the theoretical understanding of the training dynamics. However, the study has notable weaknesses, including a limited scope as it focuses on a single NIN configuration and does not explore the effects of multiple NINs or dynamic noise scales. Furthermore, the paper lacks comprehensive evaluations across diverse datasets, raising concerns about the generalizability of the results. There is also a risk of overfitting to noise, and the dependence on hyperparameters could hinder reproducibility and optimal application in different contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its contributions. However, the theoretical aspects regarding noise injection phases require more empirical validation, which could enhance the quality of the findings. While the novelty of the approach is noteworthy, the reproducibility is limited due to the sensitivity of performance to hyperparameter choices and the lack of comprehensive guidance on parameter tuning. Additionally, the absence of real-world application validation diminishes the practical significance of the findings.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to enhancing neural network robustness through noise injection. While it demonstrates potential benefits, the limitations in scope, generalization, and empirical validation suggest that further research is necessary to fully understand and leverage the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a method aimed at improving the robustness of deep neural networks (DNNs) by injecting structured noise during training. The authors argue that NINR, which employs adaptive noise and initialization, outperforms traditional regularization techniques such as Dropout and L2 regularization while maintaining generalization on clean data. The methodology involves comparing NINR against standard techniques using datasets like FMNIST and CIFAR-10, demonstrating claimed enhancements in robustness.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its clear demonstration of improved robustness compared to established methods, supported by experimental results. However, the weaknesses are significant; the methodology lacks originality, as the concepts presented are largely derivative of existing approaches in regularization. Additionally, the novelty of the proposed technique feels overstated, with many claims appearing as rehashes of known principles. The paper does not sufficiently push the boundaries of current knowledge in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, but the clarity of the contributions is undermined by a lack of novelty. While the authors present their findings in a coherent manner, the absence of innovative concepts limits the overall impact of the work. The reproducibility of the results is not explicitly addressed, and the reliance on standard datasets does not provide a robust foundation for broader applicability.\n\n# Summary Of The Review\nOverall, the paper presents a method that claims to improve robustness in DNNs through noise injection; however, the contributions lack originality and do not significantly advance the field. The claims of novelty are overstated, and the work feels more like a reiteration of existing techniques than a groundbreaking study.\n\n# Correctness\n4/5 - The methodology and claims are generally sound, but the lack of genuine novelty affects the perceived correctness of the contributions.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions are not sufficiently innovative, as they largely repackage established concepts in regularization without introducing new insights.\n\n# Empirical Novelty And Significance\n2/5 - The empirical results, while showing improvements, do not represent a significant advancement in understanding or methodology, as they rely on conventional datasets and established metrics.",
    "# Summary Of The Paper\nThe paper introduces a novel regularization technique called Noise Injection Node Regularization (NINR), aimed at enhancing the robustness of neural networks against noise and other perturbations. The methodology involves injecting noise into the nodes of a neural network during training, which purportedly improves generalization and stability. The findings, based on experiments primarily conducted on standard datasets like FMNIST and CIFAR-10, indicate that NINR provides significant improvements in model robustness, although the paper does not explore its application in unsupervised learning contexts or with varying types of noise.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to regularization through noise injection, which is supported by empirical evidence demonstrating improved performance on benchmark datasets. However, the paper has notable weaknesses, including a limited exploration of NINR's integration with existing regularization techniques, lack of evaluation on diverse datasets, and insufficient investigation into the long-term effects of noise injection on model adaptability. Additionally, the study could benefit from a more comprehensive quantitative assessment of robustness metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to readers familiar with deep learning concepts. The methodology is described sufficiently, allowing for reproducibility; however, the lack of detailed experimental setup might hinder replication efforts. The novelty of the proposed technique is commendable, yet further validation across diverse learning scenarios and with different noise types is warranted to fully establish its significance.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to enhancing neural network robustness through Noise Injection Node Regularization. While the contributions are significant, particularly in the context of supervised learning, the paper would benefit from broader evaluations and deeper investigations into its applicability and long-term effects in various contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Noise Injection Node Regularization (NINR), a novel regularization technique aimed at enhancing model robustness against input data perturbations. The methodology involves injecting noise into the training process, which is shown to significantly improve stability and performance across various noise levels on the FMNIST dataset. Key findings include NINR's superior performance in maintaining accuracy under random input perturbations, improved generalization during domain shifts (notably from MNIST to USPS), and competitive results on clean datasets compared to traditional methods like L2 regularization and Dropout. Preliminary results also suggest that NINR increases adversarial robustness against common attack methods.\n\n# Strength And Weaknesses\nStrengths of the paper lie in its comprehensive evaluation of NINR against established regularization techniques, showcasing clear improvements in robustness and performance metrics. The use of diverse datasets and the assessment of domain shifts highlight the generalizability of the proposed method. However, a notable weakness is the indication that NINR may require longer training times to achieve optimal performance, which could limit its practical applicability in certain scenarios. Furthermore, while the paper reports promising results, a more thorough investigation into the specific training dynamics and potential computational overhead would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and findings, allowing for easy comprehension of the proposed NINR technique. The quality of the experiments is high, with robust comparative analyses against traditional methods. The novelty of NINR is apparent in its unique approach to noise injection for regularization. However, the reproducibility of the results could be enhanced by providing more detailed information on experimental setups and hyperparameter choices, which are critical for replicating the findings.\n\n# Summary Of The Review\nOverall, the paper presents a promising new regularization technique, NINR, which demonstrates significant improvements in robustness against input perturbations while maintaining generalization performance. Although the paper's empirical results are compelling, attention to training dynamics and reproducibility would enhance its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper, titled \"Noise Injection Node Regularization for Robust Learning,\" introduces a novel approach to enhance the robustness of neural networks through a method called Noise Injection Node Regularization (NINR). The authors propose a methodology that incorporates noise directly into the nodes of a neural network, aiming to improve generalization performance under varying input conditions. The findings demonstrate that NINR significantly outperforms traditional regularization techniques in several benchmark datasets, indicating its potential for robust learning applications.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to regularization, which combines principles from both noise injection and node-based manipulation. This hybrid method presents a fresh perspective in the field of robust learning. However, the paper has notable weaknesses, including a lack of clarity in some sections, particularly in the presentation of methodology and results. The abstract could be better structured, and the writing style, while formal, sometimes veers into complexity that could alienate readers not deeply familiar with the terminology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe overall clarity of the paper is hindered by abrupt transitions between concepts and some overly complex sentence structures. While the methodology is sound, clearer explanations of technical terms and more descriptive figure captions are necessary for enhancing reader comprehension. The novelty of the proposed NINR method is significant, contributing a new dimension to regularization techniques. However, reproducibility could be improved with more detailed descriptions of experimental setups and clearer definitions of acronyms and technical terms.\n\n# Summary Of The Review\nThis paper presents a promising new method for improving the robustness of neural networks through Noise Injection Node Regularization. While the contributions are noteworthy, the clarity and organization of the paper could be enhanced to better convey the significance of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.5479857979366956,
    -1.7210065695633867,
    -1.7628399916321709,
    -1.602982993334136,
    -1.6976969996203797,
    -1.5677401836266223,
    -1.5824226637268335,
    -1.7799655955607299,
    -1.5012616890322068,
    -1.798175695837931,
    -1.6648323386238943,
    -1.3995559147838215,
    -1.6070539271811257,
    -1.5512482577611983,
    -1.4916782822138,
    -1.553313438883767,
    -1.8114444820466475,
    -1.444520458222679,
    -1.7719749530328577,
    -1.7710777772268746,
    -1.883449363445715,
    -1.7855771686049795,
    -1.499405347461659,
    -1.4743100180987327,
    -1.85321388386622,
    -1.776201748935497,
    -1.6195511412033834,
    -1.642130715642843,
    -1.631115555690495
  ],
  "logp_cond": [
    [
      0.0,
      -2.299031333812998,
      -2.317281925215309,
      -2.3055677614467074,
      -2.322229993970426,
      -2.3461209250032904,
      -2.3281159239703126,
      -2.3290358143741376,
      -2.3363308440073975,
      -2.3118101225108547,
      -2.325892386917889,
      -2.3805124583128965,
      -2.325018344694514,
      -2.307085430404709,
      -2.322634266236589,
      -2.3157198423498597,
      -2.3101465478988206,
      -2.3444325922481792,
      -2.342266879087433,
      -2.301504611159802,
      -2.3063577905688213,
      -2.327656539539275,
      -2.333532240633345,
      -2.3278627636084925,
      -2.33665536446214,
      -2.339001374580254,
      -2.3434530975348653,
      -2.3260259333228617,
      -2.338165195891643
    ],
    [
      -1.3597180273806206,
      0.0,
      -1.1961978098992736,
      -1.2527974381418292,
      -1.304843216351767,
      -1.3632943126329118,
      -1.344067461080493,
      -1.3604641520163283,
      -1.279805119622685,
      -1.2986214067656536,
      -1.2227645252003898,
      -1.4372935402495157,
      -1.2535612357043384,
      -1.298430562903482,
      -1.3204160634026447,
      -1.1880179404647861,
      -1.3380337331636507,
      -1.3337394741666957,
      -1.2890635929315046,
      -1.2390722031280579,
      -1.3055704955686074,
      -1.3536589383982023,
      -1.3460136096091952,
      -1.3154986149698495,
      -1.2669051984020203,
      -1.3247245574219084,
      -1.3637192590138563,
      -1.3002767651829212,
      -1.415352375664568
    ],
    [
      -1.433246440042541,
      -1.2590010582396016,
      0.0,
      -1.3411370910463396,
      -1.3738475998257413,
      -1.4225899303917215,
      -1.403238037928973,
      -1.3655437024796795,
      -1.3685353766930106,
      -1.3297690901923598,
      -1.3716659756860583,
      -1.510197092576951,
      -1.3193147508246752,
      -1.3603192916101294,
      -1.4181249433459686,
      -1.2785294927783284,
      -1.3797093393215696,
      -1.414158755578838,
      -1.3535600092175795,
      -1.3792015261914476,
      -1.3730007577875765,
      -1.3897369171793879,
      -1.4190949988469321,
      -1.3653471573473792,
      -1.3340418500782298,
      -1.425937351136267,
      -1.428506535032846,
      -1.3929895305751452,
      -1.4637314471621807
    ],
    [
      -1.2582479867352903,
      -1.0895143355575503,
      -1.1578111587914257,
      0.0,
      -1.1897982660877975,
      -1.2562568362103608,
      -1.223680945056248,
      -1.264602954046096,
      -1.1829311183351467,
      -1.1954918166483146,
      -1.1160797610741837,
      -1.32856704898482,
      -1.1462207128422937,
      -1.184036398762415,
      -1.1613401598958422,
      -1.1183198475156226,
      -1.2698432797815216,
      -1.2663657145973957,
      -1.2346886339447698,
      -1.1148013490662994,
      -1.2251287407199145,
      -1.2908670274361402,
      -1.2670690569421685,
      -1.144664811813464,
      -1.2452660928538821,
      -1.1940839834194323,
      -1.2381701327450918,
      -1.1589423008147384,
      -1.3170261754293557
    ],
    [
      -1.351126987399297,
      -1.2821833384892773,
      -1.3003062965026961,
      -1.252732024643345,
      0.0,
      -1.3493831057075505,
      -1.3627569053245805,
      -1.3639817848875766,
      -1.3010994974859762,
      -1.266569744929819,
      -1.2916809066208883,
      -1.4053197568740499,
      -1.2879114905845892,
      -1.319585602823987,
      -1.256721900951917,
      -1.2509112211966855,
      -1.2903273495715353,
      -1.365470258206274,
      -1.3325331653616768,
      -1.2491533654368097,
      -1.3260893739713744,
      -1.3787335652397679,
      -1.2988948937044258,
      -1.297197746956153,
      -1.3036740318902844,
      -1.3434855220248982,
      -1.2598982782186265,
      -1.3261709891541025,
      -1.4212433657991972
    ],
    [
      -1.2688443299631065,
      -1.2323980135061812,
      -1.2649006099244817,
      -1.2206113297000383,
      -1.258518509684783,
      0.0,
      -1.2467343083586635,
      -1.3062623687922412,
      -1.2628076824072323,
      -1.216715858838754,
      -1.239922480179819,
      -1.2895290550642233,
      -1.2021196837589396,
      -1.2483597810725822,
      -1.2421415089534549,
      -1.2147034421694674,
      -1.2751977121570126,
      -1.2762065108483203,
      -1.2610750077075925,
      -1.2065908536354848,
      -1.2415655046849456,
      -1.2845290143098027,
      -1.2943743616014778,
      -1.2715905112764094,
      -1.2832389573953396,
      -1.2776894403021057,
      -1.2887100727694911,
      -1.2866334805635764,
      -1.2812218948115264
    ],
    [
      -1.2804869405419865,
      -1.2110480881389802,
      -1.2239235360774925,
      -1.222673621840617,
      -1.2816355469870289,
      -1.2506999044200384,
      0.0,
      -1.2590988880491982,
      -1.2282540167451157,
      -1.1736090922731146,
      -1.219378711106204,
      -1.3048221766669033,
      -1.2134978423467013,
      -1.2422440617152857,
      -1.247806134984962,
      -1.2149457245625128,
      -1.2342166236825047,
      -1.2892221227187508,
      -1.2346699460448738,
      -1.2317003130451496,
      -1.2288452143083404,
      -1.2624126182295665,
      -1.277519613622106,
      -1.2757794282332708,
      -1.281561805565038,
      -1.2744804266788294,
      -1.2494895602197564,
      -1.275369763538601,
      -1.2799363867685325
    ],
    [
      -1.5231200520694275,
      -1.4533683951782506,
      -1.4359828638688696,
      -1.46501246097358,
      -1.4965003729142938,
      -1.4949814720655714,
      -1.4939771391353753,
      0.0,
      -1.436796988722966,
      -1.5041113980121632,
      -1.4643720773383415,
      -1.5379104028677781,
      -1.4395590578682964,
      -1.4525026661073956,
      -1.495698624565302,
      -1.426136128005385,
      -1.4873619030182337,
      -1.4816807796054157,
      -1.4886323371722627,
      -1.4558140499285417,
      -1.4757789374073176,
      -1.5027488763807382,
      -1.5166678248700416,
      -1.466690111503522,
      -1.5135751349551652,
      -1.4555515193153896,
      -1.5095025886923088,
      -1.4811085402891748,
      -1.5200399937143672
    ],
    [
      -1.181240007401619,
      -1.0625401282961497,
      -1.0494003899464655,
      -1.090494811084998,
      -1.142881061740438,
      -1.1457665789595206,
      -1.1637584982105733,
      -1.1389641793534524,
      0.0,
      -1.1151390420428302,
      -1.096928948847905,
      -1.2417375453288615,
      -1.0895308729200717,
      -1.0969622907150944,
      -1.1218537450640302,
      -1.0692640262417898,
      -1.1238865029200034,
      -1.1405903168609628,
      -1.112815365407373,
      -1.1011964547282058,
      -1.1478411501085204,
      -1.14441150472409,
      -1.1670626913978182,
      -1.128018275824189,
      -1.1716607281874372,
      -1.140762944227713,
      -1.152053686664004,
      -1.0996858722272627,
      -1.1824975322827804
    ],
    [
      -1.4080053012752765,
      -1.4082656544283694,
      -1.3609964979685123,
      -1.4214504796941925,
      -1.4088421531973496,
      -1.4564069807189566,
      -1.4400879939226432,
      -1.437947620319123,
      -1.4242972464213142,
      0.0,
      -1.429654932864943,
      -1.5120553163547743,
      -1.4013323733439864,
      -1.3742081849625056,
      -1.4142881151191045,
      -1.4246190327359365,
      -1.3911491156189677,
      -1.4569820074167366,
      -1.4179971564691334,
      -1.4266698200080856,
      -1.4419532185299608,
      -1.4673407766861226,
      -1.4366781356820748,
      -1.4407559033422306,
      -1.4764056156268164,
      -1.452814613853975,
      -1.4398023682936945,
      -1.4179825966866118,
      -1.4725794649185342
    ],
    [
      -1.3376769693410233,
      -1.1667762439340115,
      -1.260947888179156,
      -1.2166382362761452,
      -1.2589747227622967,
      -1.3018341195125505,
      -1.2843033921886993,
      -1.2975586946321134,
      -1.2627342451495296,
      -1.257692603625205,
      0.0,
      -1.3963631573567843,
      -1.237734764823529,
      -1.2381136233145495,
      -1.1953175393449924,
      -1.184801671536857,
      -1.2952208460580652,
      -1.2901491448529168,
      -1.1399593496639493,
      -1.1976261166529543,
      -1.286085169595909,
      -1.2904161619539727,
      -1.2818827778493929,
      -1.2851647036421558,
      -1.3093887595942457,
      -1.2911338667166607,
      -1.3130115276848389,
      -1.29119014064279,
      -1.3412813897274218
    ],
    [
      -1.1964990917311356,
      -1.171231661838212,
      -1.1668907808483886,
      -1.1539580229555708,
      -1.146526977946982,
      -1.1315628514516174,
      -1.1568649651438427,
      -1.166243765086723,
      -1.1716515343739424,
      -1.1267811393225975,
      -1.1433320952355959,
      0.0,
      -1.1555906661976807,
      -1.1254979666454341,
      -1.1406475913471854,
      -1.1425067872378318,
      -1.1421769654276621,
      -1.1477308908199588,
      -1.158710047501155,
      -1.1577194135704387,
      -1.1546352400991156,
      -1.1410844851557274,
      -1.1493998623852193,
      -1.1624562525143427,
      -1.1491076576742563,
      -1.1535331251269663,
      -1.1466527516453195,
      -1.1731680023005235,
      -1.1300938928965
    ],
    [
      -1.2740956218506532,
      -1.1885732398399615,
      -1.2113768582275188,
      -1.1826067877097481,
      -1.2215981533243347,
      -1.284858370108545,
      -1.2774001630975942,
      -1.2590354117901317,
      -1.241251806903813,
      -1.2171425521473591,
      -1.1958270578762584,
      -1.3437094131014087,
      0.0,
      -1.2115652672992865,
      -1.2558659671438697,
      -1.2036466517291249,
      -1.286588008332541,
      -1.2592419425332373,
      -1.2576769153599967,
      -1.250495520452741,
      -1.2590830345454675,
      -1.270603835358112,
      -1.2758517382236259,
      -1.2423551329135438,
      -1.3054198190671258,
      -1.2433038808285561,
      -1.2616792376770527,
      -1.2267859295754566,
      -1.3232067599858806
    ],
    [
      -1.1765426627727127,
      -1.1412158140339848,
      -1.1811927735723038,
      -1.1606441059362718,
      -1.1859573305541107,
      -1.214537321742185,
      -1.2283131132390988,
      -1.2387568183029376,
      -1.1444266818182502,
      -1.1302930267976032,
      -1.140120829920145,
      -1.2808180658893855,
      -1.081791850101049,
      0.0,
      -1.1721326019642972,
      -1.1353212440134008,
      -1.1381244482980009,
      -1.178420750856719,
      -1.2156237452792087,
      -1.1447255802663847,
      -1.170349835667859,
      -1.189760866604245,
      -1.1870630398414892,
      -1.1539032331432257,
      -1.2292247523756314,
      -1.147951817606995,
      -1.1853521425086602,
      -1.1605957142780996,
      -1.1992132039026715
    ],
    [
      -1.156783406604439,
      -1.0805877732439642,
      -1.119340628211296,
      -1.040972966609249,
      -1.0803005684408191,
      -1.140309381518093,
      -1.1257104108714517,
      -1.1802057172292053,
      -1.088955946397882,
      -1.0928917836337415,
      -1.009909754860375,
      -1.2025221170220737,
      -1.0990689170791785,
      -1.0978727464986666,
      0.0,
      -1.0709350017122916,
      -1.1350508410386484,
      -1.0799579323307074,
      -1.1109920684519639,
      -1.0253353445499225,
      -1.1324314292016997,
      -1.1004967953570526,
      -1.1044231909556146,
      -1.0808599605111209,
      -1.134730857271597,
      -1.1371154617622088,
      -1.126894095245878,
      -1.041973684111537,
      -1.0826197580436863
    ],
    [
      -1.2488758486077747,
      -1.109701956327983,
      -1.1643285423564043,
      -1.1436125867242277,
      -1.1899105220141055,
      -1.2185400843269678,
      -1.2455468488134804,
      -1.2116260398871912,
      -1.179034691876463,
      -1.2049917412868703,
      -1.1187171063816077,
      -1.3035585508137448,
      -1.1567016320486263,
      -1.1830079729609506,
      -1.1975837158449705,
      0.0,
      -1.2436619302420904,
      -1.1960346874783563,
      -1.1963371001748513,
      -1.153158384864062,
      -1.2244061468895675,
      -1.2226405904434836,
      -1.2600026024388165,
      -1.2149840230874158,
      -1.2442625375316365,
      -1.2247521786626825,
      -1.2722483763256647,
      -1.1920316431048215,
      -1.270735702502125
    ],
    [
      -1.4713874632479433,
      -1.4555215098078143,
      -1.442813889247691,
      -1.4640341298939556,
      -1.439950381969154,
      -1.4902626619849135,
      -1.4667681071402856,
      -1.5051559498145468,
      -1.468542256453958,
      -1.4212467507840716,
      -1.4673006288819352,
      -1.555245984810038,
      -1.4424111387523335,
      -1.3942126582299799,
      -1.4475952266743581,
      -1.460811493222239,
      0.0,
      -1.5273369212240187,
      -1.4686312527314982,
      -1.4337082895775521,
      -1.4429360296687337,
      -1.5033922886442652,
      -1.4183306782252296,
      -1.4410427579530036,
      -1.4782052275147084,
      -1.4970479185637224,
      -1.4537691009502987,
      -1.4609359827622208,
      -1.5362612743941988
    ],
    [
      -1.178662109238241,
      -1.0847143176563638,
      -1.0973872333866481,
      -1.0566775958728745,
      -1.1064968872645835,
      -1.1403717300609995,
      -1.1473786346350516,
      -1.0737706197348245,
      -1.1276413615278866,
      -1.104452990857774,
      -1.0838300554624205,
      -1.175440970983777,
      -1.060315803464526,
      -1.1265334485121172,
      -1.1291797316368501,
      -1.0816699277625077,
      -1.1419074667001192,
      0.0,
      -1.1149240121573316,
      -1.1004607330896479,
      -1.1126151598795204,
      -1.129753462969605,
      -1.1455091086385862,
      -1.1056022471073534,
      -1.1799313948272956,
      -1.0777804704211926,
      -1.1386618134653823,
      -1.101427328433139,
      -1.12041456978364
    ],
    [
      -1.4650771163211227,
      -1.3828467549572538,
      -1.3896669812152005,
      -1.3899699614532628,
      -1.4444997651836435,
      -1.3986204346995512,
      -1.4340046950615637,
      -1.4553895200351976,
      -1.3760563403906132,
      -1.4195218541215566,
      -1.356185643495963,
      -1.4857382583534309,
      -1.4032810246603526,
      -1.3880926001181577,
      -1.4356378836116497,
      -1.3861119488739089,
      -1.4052507544546498,
      -1.4534302845684437,
      0.0,
      -1.3773987225096318,
      -1.4033518657134971,
      -1.4636465392219342,
      -1.4471961538863054,
      -1.4126755115410472,
      -1.4097272676223227,
      -1.467464276872631,
      -1.4541886885704416,
      -1.42936976159778,
      -1.4999146097540046
    ],
    [
      -1.4373071568128024,
      -1.3187053225488845,
      -1.3731067915597608,
      -1.269848091395669,
      -1.368837659062566,
      -1.4363079379537997,
      -1.3982860620996522,
      -1.4278510170003753,
      -1.3708886556908355,
      -1.3466087636709545,
      -1.3111917195845504,
      -1.5086357337058212,
      -1.3608157876651024,
      -1.3349010036951972,
      -1.331933684694694,
      -1.3107457999649568,
      -1.394737405999663,
      -1.3854472777255766,
      -1.3953945649871529,
      0.0,
      -1.3746157391368181,
      -1.4275571908464464,
      -1.3873772135467648,
      -1.3407853133060983,
      -1.4351149522091116,
      -1.3811450904989917,
      -1.3847665987914923,
      -1.3813126905059967,
      -1.431785918903077
    ],
    [
      -1.493220477453591,
      -1.4733463973237666,
      -1.4370572329027063,
      -1.4635024951518347,
      -1.4698551303469367,
      -1.5427400955930028,
      -1.5335494690064557,
      -1.5086755602174242,
      -1.4978033255780876,
      -1.495127043506379,
      -1.489560332383378,
      -1.6233847096313752,
      -1.452699583870039,
      -1.4899782951435168,
      -1.4945663044125364,
      -1.4601993045266752,
      -1.479970004909599,
      -1.5634260557526152,
      -1.5583910107600978,
      -1.4141298535471911,
      0.0,
      -1.488971629359522,
      -1.4805527141001675,
      -1.5036481586287018,
      -1.5006902859415885,
      -1.494976060345705,
      -1.5285264819041944,
      -1.4938907830495363,
      -1.5644435471016236
    ],
    [
      -1.3971669104288833,
      -1.4215446317397733,
      -1.4151822372663467,
      -1.4014016862639391,
      -1.402615442636815,
      -1.422564602624694,
      -1.410088700265927,
      -1.4485418886479302,
      -1.4154815023943754,
      -1.387036713170215,
      -1.3946580670510038,
      -1.4520613880679898,
      -1.38402774321753,
      -1.4230367167721572,
      -1.386083010812778,
      -1.3689872840488038,
      -1.3974989058075435,
      -1.4323931917160648,
      -1.447626565447911,
      -1.3737769628750205,
      -1.3890785094978575,
      0.0,
      -1.375238693704582,
      -1.4287771464571069,
      -1.447368683176531,
      -1.4528299750889233,
      -1.3746867901905857,
      -1.3813969294036563,
      -1.3879138976363772
    ],
    [
      -1.1528120665052288,
      -1.18710917063131,
      -1.162156818236859,
      -1.1655167888168647,
      -1.1151688715098873,
      -1.2052293361381277,
      -1.1739540461499467,
      -1.190830675673693,
      -1.1793180320153274,
      -1.08374971756809,
      -1.1328581819175363,
      -1.2154741258272437,
      -1.149112508128999,
      -1.1499607161353427,
      -1.1600457047559862,
      -1.1620497914103283,
      -1.1093453273961313,
      -1.19798868634816,
      -1.1562132063104575,
      -1.1417893702466224,
      -1.109228543248217,
      -1.132994016681676,
      0.0,
      -1.1733008456606058,
      -1.1519072038904405,
      -1.1802694556909825,
      -1.0968569540771587,
      -1.166662977568187,
      -1.2142114239271573
    ],
    [
      -1.1227005817825384,
      -1.074047342017633,
      -1.04902117392285,
      -1.0165136069306528,
      -1.0793339826328407,
      -1.113438966844574,
      -1.1427898771327831,
      -1.107099103587671,
      -1.0621293794679354,
      -1.0812997488056602,
      -1.1004585134970726,
      -1.2095126999980934,
      -1.0546020950036294,
      -1.0638227158170115,
      -1.0512647670973205,
      -1.0856298455395972,
      -1.083808699429019,
      -1.0965866073816344,
      -1.0913172209432977,
      -1.0680835798964332,
      -1.1004596217172138,
      -1.124562476662755,
      -1.1165745971497212,
      0.0,
      -1.1017608144741524,
      -1.1044664031784486,
      -1.093405297351361,
      -1.0356094999520238,
      -1.1624925592222124
    ],
    [
      -1.5927957446083583,
      -1.4322140199670619,
      -1.4567662925283684,
      -1.504891537716376,
      -1.4789897989398804,
      -1.541477351390196,
      -1.5146802766354273,
      -1.5675276299645529,
      -1.5195770168162208,
      -1.507300841588692,
      -1.5018724748632108,
      -1.5759635712340483,
      -1.5188601346247375,
      -1.5365430302050707,
      -1.493567436156795,
      -1.5108967163192493,
      -1.518272419529898,
      -1.5404267024078946,
      -1.5284777073436822,
      -1.4916812370864847,
      -1.4662207508352731,
      -1.5226319213050425,
      -1.4991041029974896,
      -1.4901947829426228,
      0.0,
      -1.5446097354161612,
      -1.5015947083619017,
      -1.5035302718409442,
      -1.5627739601589594
    ],
    [
      -1.4267193426080227,
      -1.3943178351206602,
      -1.3929911369634411,
      -1.3609688996314442,
      -1.3729866481650281,
      -1.4181268473777973,
      -1.452580972271824,
      -1.370981838477251,
      -1.3921299445646194,
      -1.3926130498482823,
      -1.3644830459184902,
      -1.4642622735030155,
      -1.3654763890070394,
      -1.3839590508088633,
      -1.380158902432493,
      -1.395904664992801,
      -1.4276672302686726,
      -1.3854026621291093,
      -1.4237860941568525,
      -1.3490484125846907,
      -1.3872603890269413,
      -1.4189733462767986,
      -1.4281938652447996,
      -1.3987520945794722,
      -1.4391263970316552,
      0.0,
      -1.3929691143564724,
      -1.386833134336759,
      -1.453495342748935
    ],
    [
      -1.2878043636988394,
      -1.2373022144640324,
      -1.2456869629834977,
      -1.1981277393067895,
      -1.161998067267531,
      -1.296755044214026,
      -1.2577402458192424,
      -1.2641966232083999,
      -1.271839606298856,
      -1.2207166517484516,
      -1.2255591585508994,
      -1.3222405035467257,
      -1.2081594270647658,
      -1.2298517716154607,
      -1.204884450069596,
      -1.259471130625242,
      -1.2274286499654667,
      -1.2311612174156867,
      -1.2440503007695345,
      -1.2319831259444987,
      -1.2410928967631307,
      -1.2222677241601534,
      -1.1892734788291484,
      -1.1990348327012934,
      -1.190496280533923,
      -1.2324430450338415,
      0.0,
      -1.2239550832654835,
      -1.2549396042097003
    ],
    [
      -1.340177991081264,
      -1.2233377263201437,
      -1.2466998975858885,
      -1.2295829654976531,
      -1.2758270491387602,
      -1.3188210040786996,
      -1.3484175097653393,
      -1.3256918841687837,
      -1.235265087316613,
      -1.2494081173861586,
      -1.2462939682365108,
      -1.3773035158048919,
      -1.2322868406018548,
      -1.245756697687051,
      -1.2661428797768746,
      -1.242984892340489,
      -1.2718717332601597,
      -1.2751815440446568,
      -1.30069854306791,
      -1.2435031389383122,
      -1.284766354125545,
      -1.3105564477137428,
      -1.2892402421270657,
      -1.2247275131978062,
      -1.326964909454308,
      -1.2614147760008951,
      -1.309691935735663,
      0.0,
      -1.358740592504184
    ],
    [
      -1.2671331258652423,
      -1.2579265344545456,
      -1.2452141478087497,
      -1.2274223981826573,
      -1.258615168672847,
      -1.2804967917448224,
      -1.2521159538043989,
      -1.2504554967889265,
      -1.2468906136247953,
      -1.1891449384512967,
      -1.2378395793321133,
      -1.2546455988492136,
      -1.2442486656634046,
      -1.2610272242875875,
      -1.2295225429002254,
      -1.2462411996748672,
      -1.2553811029276785,
      -1.2124626298002936,
      -1.2676984022024367,
      -1.245795148343604,
      -1.2114323531662006,
      -1.1481532262386394,
      -1.2168157800294417,
      -1.2276159321651858,
      -1.260173437145659,
      -1.212971009955576,
      -1.2002215750324599,
      -1.2133646100580064,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2489544641236976,
      0.23070387272138637,
      0.24241803648998816,
      0.22575580396626949,
      0.20186487293340516,
      0.2198698739663829,
      0.2189499835625579,
      0.211654953929298,
      0.23617567542584084,
      0.22209341101880664,
      0.16747333962379907,
      0.2229674532421817,
      0.24090036753198651,
      0.22535153170010647,
      0.23226595558683583,
      0.23783925003787498,
      0.20355320568851631,
      0.20571891884926252,
      0.24648118677689368,
      0.24162800736787426,
      0.22032925839742035,
      0.21445355730335036,
      0.22012303432820302,
      0.2113304334745556,
      0.20898442335644152,
      0.20453270040183025,
      0.22195986461383388,
      0.2098206020450526
    ],
    [
      0.36128854218276607,
      0.0,
      0.5248087596641131,
      0.4682091314215575,
      0.4161633532116198,
      0.3577122569304749,
      0.3769391084828937,
      0.3605424175470584,
      0.44120144994070176,
      0.4223851627977331,
      0.4982420443629969,
      0.283713029313871,
      0.4674453338590483,
      0.4225760066599047,
      0.400590506160742,
      0.5329886290986006,
      0.382972836399736,
      0.387267095396691,
      0.4319429766318821,
      0.48193436643532883,
      0.41543607399477933,
      0.36734763116518443,
      0.37499295995419146,
      0.40550795459353717,
      0.4541013711613664,
      0.3962820121414783,
      0.35728731054953045,
      0.4207298043804655,
      0.30565419389881865
    ],
    [
      0.3295935515896298,
      0.5038389333925692,
      0.0,
      0.42170290058583126,
      0.3889923918064295,
      0.3402500612404493,
      0.3596019537031978,
      0.3972962891524914,
      0.39430461493916025,
      0.4330709014398111,
      0.39117401594611256,
      0.25264289905521986,
      0.44352524080749567,
      0.40252070002204143,
      0.34471504828620225,
      0.4843104988538425,
      0.3831306523106013,
      0.3486812360533329,
      0.4092799824145914,
      0.3836384654407232,
      0.3898392338445944,
      0.373103074452783,
      0.34374499278523873,
      0.39749283428479165,
      0.428798141553941,
      0.3369026404959039,
      0.3343334565993248,
      0.3698504610570257,
      0.29910854446999013
    ],
    [
      0.3447350065988457,
      0.5134686577765857,
      0.44517183454271025,
      0.0,
      0.4131847272463385,
      0.3467261571237752,
      0.37930204827788794,
      0.33838003928804006,
      0.4200518749989892,
      0.40749117668582135,
      0.48690323225995225,
      0.27441594434931593,
      0.4567622804918423,
      0.418946594571721,
      0.4416428334382938,
      0.48466314581851333,
      0.3331397135526144,
      0.3366172787367403,
      0.36829435938936617,
      0.4881816442678366,
      0.37785425261422145,
      0.31211596589799573,
      0.3359139363919674,
      0.45831818152067205,
      0.35771690048025384,
      0.4088990099147036,
      0.36481286058904416,
      0.4440406925193976,
      0.2859568179047802
    ],
    [
      0.34657001222108264,
      0.41551366113110233,
      0.39739070311768354,
      0.4449649749770346,
      0.0,
      0.3483138939128292,
      0.3349400942957992,
      0.3337152147328031,
      0.3965975021344035,
      0.4311272546905607,
      0.4060160929994914,
      0.2923772427463298,
      0.4097855090357905,
      0.3781113967963927,
      0.44097509866846263,
      0.44678577842369416,
      0.4073696500488444,
      0.3322267414141056,
      0.3651638342587029,
      0.44854363418357,
      0.3716076256490053,
      0.3189634343806118,
      0.3988021059159539,
      0.4004992526642266,
      0.39402296773009526,
      0.35421147759548144,
      0.43779872140175313,
      0.3715260104662772,
      0.2764536338211825
    ],
    [
      0.2988958536635158,
      0.3353421701204411,
      0.30283957370214054,
      0.347128853926584,
      0.3092216739418392,
      0.0,
      0.32100587526795876,
      0.26147781483438104,
      0.30493250121939,
      0.35102432478786816,
      0.3278177034468033,
      0.27821112856239893,
      0.36562049986768264,
      0.3193804025540401,
      0.3255986746731674,
      0.3530367414571549,
      0.29254247146960966,
      0.291533672778302,
      0.30666517591902975,
      0.36114932999113747,
      0.3261746789416766,
      0.28321116931681956,
      0.2733658220251445,
      0.29614967235021283,
      0.2845012262312827,
      0.2900507433245165,
      0.27903011085713114,
      0.28110670306304586,
      0.2865182888150959
    ],
    [
      0.30193572318484696,
      0.37137457558785325,
      0.358499127649341,
      0.3597490418862166,
      0.3007871167398046,
      0.33172275930679507,
      0.0,
      0.32332377567763526,
      0.3541686469817178,
      0.4088135714537189,
      0.3630439526206295,
      0.27760048705993023,
      0.36892482138013216,
      0.3401786020115478,
      0.3346165287418714,
      0.36747693916432067,
      0.34820604004432876,
      0.2932005410080827,
      0.3477527176819597,
      0.3507223506816839,
      0.3535774494184931,
      0.32001004549726697,
      0.3049030501047274,
      0.30664323549356265,
      0.30086085816179553,
      0.30794223704800405,
      0.33293310350707706,
      0.3070529001882325,
      0.302486276958301
    ],
    [
      0.2568455434913024,
      0.3265972003824793,
      0.3439827316918602,
      0.3149531345871499,
      0.28346522264643603,
      0.28498412349515845,
      0.28598845642535453,
      0.0,
      0.34316860683776396,
      0.27585419754856666,
      0.31559351822238835,
      0.24205519269295173,
      0.34040653769243345,
      0.3274629294533342,
      0.28426697099542797,
      0.35382946755534483,
      0.2926036925424962,
      0.2982848159553142,
      0.29133325838846713,
      0.3241515456321882,
      0.3041866581534123,
      0.2772167191799917,
      0.26329777069068827,
      0.31327548405720784,
      0.2663904606055647,
      0.32441407624534024,
      0.27046300686842106,
      0.29885705527155504,
      0.2599256018463627
    ],
    [
      0.32002168163058786,
      0.43872156073605706,
      0.45186129908574135,
      0.4107668779472089,
      0.3583806272917689,
      0.3554951100726862,
      0.3375031908216335,
      0.3622975096787544,
      0.0,
      0.3861226469893766,
      0.40433274018430176,
      0.2595241437033453,
      0.41173081611213513,
      0.40429939831711237,
      0.37940794396817656,
      0.43199766279041696,
      0.37737518611220344,
      0.360671372171244,
      0.3884463236248339,
      0.400065234304001,
      0.3534205389236864,
      0.3568501843081169,
      0.33419899763438865,
      0.3732434132080178,
      0.32960096084476964,
      0.36049874480449384,
      0.3492080023682027,
      0.40157581680494414,
      0.3187641567494264
    ],
    [
      0.39017039456265445,
      0.38991004140956154,
      0.4371791978694186,
      0.37672521614373844,
      0.3893335426405813,
      0.34176871511897433,
      0.3580877019152877,
      0.36022807551880787,
      0.37387844941661674,
      0.0,
      0.36852076297298786,
      0.2861203794831566,
      0.3968433224939445,
      0.4239675108754253,
      0.38388758071882645,
      0.3735566631019944,
      0.4070265802189632,
      0.34119368842119435,
      0.38017853936879753,
      0.3715058758298453,
      0.3562224773079701,
      0.3308349191518083,
      0.3614975601558561,
      0.35741979249570033,
      0.32177008021111453,
      0.3453610819839559,
      0.35837332754423645,
      0.38019309915131916,
      0.3255962309193967
    ],
    [
      0.32715536928287103,
      0.4980560946898829,
      0.4038844504447383,
      0.44819410234774915,
      0.40585761586159763,
      0.3629982191113439,
      0.3805289464351951,
      0.367273643991781,
      0.40209809347436476,
      0.4071397349986894,
      0.0,
      0.26846918126711006,
      0.4270975738003653,
      0.4267187153093448,
      0.46951479927890194,
      0.48003066708703734,
      0.36961149256582915,
      0.3746831937709776,
      0.524872988959945,
      0.4672062219709401,
      0.37874716902798533,
      0.37441617666992166,
      0.3829495607745015,
      0.3796676349817385,
      0.3554435790296486,
      0.3736984719072336,
      0.3518208109390555,
      0.37364219798110443,
      0.3235509488964725
    ],
    [
      0.2030568230526859,
      0.22832425294560954,
      0.23266513393543287,
      0.24559789182825065,
      0.25302893683683947,
      0.2679930633322041,
      0.24269094963997873,
      0.23331214969709846,
      0.22790438040987904,
      0.272774775461224,
      0.2562238195482256,
      0.0,
      0.24396524858614077,
      0.27405794813838735,
      0.25890832343663606,
      0.25704912754598963,
      0.25737894935615935,
      0.25182502396386264,
      0.24084586728266655,
      0.24183650121338274,
      0.2449206746847059,
      0.2584714296280941,
      0.25015605239860217,
      0.23709966226947876,
      0.2504482571095652,
      0.2460227896568552,
      0.2529031631385019,
      0.22638791248329793,
      0.2694620218873214
    ],
    [
      0.33295830533047255,
      0.41848068734116417,
      0.3956770689536069,
      0.4244471394713776,
      0.385455773856791,
      0.32219555707258074,
      0.32965376408353153,
      0.34801851539099404,
      0.36580212027731274,
      0.3899113750337666,
      0.4112268693048673,
      0.26334451407971704,
      0.0,
      0.39548865988183923,
      0.35118796003725605,
      0.40340727545200084,
      0.3204659188485848,
      0.34781198464788843,
      0.349377011821129,
      0.3565584067283847,
      0.34797089263565817,
      0.3364500918230138,
      0.33120218895749987,
      0.3646987942675819,
      0.30163410811399993,
      0.3637500463525696,
      0.345374689504073,
      0.3802679976056691,
      0.28384716719524516
    ],
    [
      0.37470559498848566,
      0.4100324437272136,
      0.37005548418889456,
      0.3906041518249266,
      0.36529092720708767,
      0.33671093601901325,
      0.3229351445220996,
      0.31249143945826074,
      0.4068215759429481,
      0.4209552309635951,
      0.4111274278410533,
      0.27043019187181283,
      0.4694564076601493,
      0.0,
      0.3791156557969011,
      0.4159270137477975,
      0.4131238094631975,
      0.3728275069044793,
      0.33562451248198966,
      0.40652267749481363,
      0.38089842209333935,
      0.3614873911569534,
      0.36418521791970915,
      0.3973450246179726,
      0.3220235053855669,
      0.4032964401542034,
      0.3658961152525382,
      0.39065254348309875,
      0.35203505385852685
    ],
    [
      0.33489487560936104,
      0.4110905089698358,
      0.37233765400250385,
      0.45070531560455085,
      0.4113777137729808,
      0.35136890069570703,
      0.3659678713423482,
      0.31147256498459464,
      0.402722335815918,
      0.39878649858005843,
      0.4817685273534249,
      0.28915616519172627,
      0.3926093651346214,
      0.3938055357151333,
      0.0,
      0.42074328050150833,
      0.35662744117515155,
      0.4117203498830926,
      0.3806862137618361,
      0.46634293766387747,
      0.35924685301210024,
      0.39118148685674736,
      0.38725509125818536,
      0.41081832170267907,
      0.3569474249422029,
      0.35456282045159115,
      0.36478418696792203,
      0.44970459810226293,
      0.4090585241701137
    ],
    [
      0.3044375902759924,
      0.443611482555784,
      0.3889848965273628,
      0.40970085215953933,
      0.36340291686966153,
      0.33477335455679924,
      0.3077665900702866,
      0.34168739899657585,
      0.3742787470073041,
      0.3483216975968968,
      0.43459633250215934,
      0.24975488807002222,
      0.39661180683514075,
      0.37030546592281643,
      0.3557297230387966,
      0.0,
      0.3096515086416767,
      0.35727875140541077,
      0.35697633870891576,
      0.40015505401970497,
      0.32890729199419955,
      0.33067284844028344,
      0.2933108364449506,
      0.33832941579635123,
      0.30905090135213054,
      0.32856126022108456,
      0.2810650625581024,
      0.3612817957789456,
      0.28257773638164196
    ],
    [
      0.3400570187987042,
      0.3559229722388333,
      0.3686305927989566,
      0.3474103521526919,
      0.3714941000774936,
      0.321181820061734,
      0.34467637490636194,
      0.30628853223210073,
      0.34290222559268946,
      0.39019773126257595,
      0.3441438531647123,
      0.2561984972366096,
      0.36903334329431403,
      0.4172318238166677,
      0.3638492553722894,
      0.35063298882440863,
      0.0,
      0.2841075608226289,
      0.34281322931514935,
      0.3777361924690954,
      0.36850845237791385,
      0.3080521934023823,
      0.3931138038214179,
      0.3704017240936439,
      0.33323925453193914,
      0.3143965634829251,
      0.3576753810963489,
      0.35050849928442673,
      0.27518320765244875
    ],
    [
      0.26585834898443794,
      0.3598061405663151,
      0.3471332248360308,
      0.3878428623498045,
      0.33802357095809543,
      0.3041487281616795,
      0.2971418235876273,
      0.3707498384878545,
      0.3168790966947923,
      0.3400674673649049,
      0.36069040276025843,
      0.26907948723890196,
      0.384204654758153,
      0.3179870097105617,
      0.3153407265858288,
      0.36285053046017124,
      0.3026129915225597,
      0.0,
      0.3295964460653473,
      0.34405972513303107,
      0.3319052983431585,
      0.31476699525307383,
      0.2990113495840927,
      0.3389182111153255,
      0.26458906339538335,
      0.3667399878014863,
      0.3058586447572966,
      0.3430931297895399,
      0.32410588843903887
    ],
    [
      0.306897836711735,
      0.38912819807560384,
      0.3823079718176572,
      0.3820049915795949,
      0.32747518784921414,
      0.37335451833330646,
      0.337970257971294,
      0.31658543299766007,
      0.3959186126422445,
      0.35245309891130105,
      0.41578930953689475,
      0.2862366946794268,
      0.36869392837250503,
      0.38388235291469996,
      0.336337069421208,
      0.3858630041589488,
      0.3667241985782079,
      0.318544668464414,
      0.0,
      0.3945762305232259,
      0.36862308731936055,
      0.30832841381092346,
      0.3247787991465523,
      0.3592994414918105,
      0.362247685410535,
      0.30451067616022676,
      0.3177862644624161,
      0.3426051914350776,
      0.2720603432788531
    ],
    [
      0.33377062041407224,
      0.45237245467799014,
      0.39797098566711386,
      0.5012296858312058,
      0.4022401181643087,
      0.3347698392730749,
      0.3727917151272224,
      0.34322676022649934,
      0.40018912153603914,
      0.42446901355592015,
      0.45988605764232426,
      0.2624420435210535,
      0.4102619895617723,
      0.4361767735316775,
      0.43914409253218056,
      0.4603319772619179,
      0.37634037122721176,
      0.385630499501298,
      0.3756832122397218,
      0.0,
      0.39646203809005653,
      0.34352058638042826,
      0.38370056368010985,
      0.43029246392077636,
      0.3359628250177631,
      0.38993268672788295,
      0.3863111784353823,
      0.38976508672087795,
      0.3392918583237976
    ],
    [
      0.3902288859921239,
      0.4101029661219484,
      0.44639213054300875,
      0.4199468682938803,
      0.4135942330987783,
      0.34070926785271216,
      0.3498998944392593,
      0.3747738032282908,
      0.3856460378676274,
      0.38832231993933597,
      0.3938890310623371,
      0.26006465381433985,
      0.43074977957567606,
      0.39347106830219825,
      0.38888305903317866,
      0.4232500589190398,
      0.40347935853611605,
      0.32002330769309983,
      0.3250583526856172,
      0.4693195098985239,
      0.0,
      0.3944777340861929,
      0.4028966493455475,
      0.3798012048170132,
      0.3827590775041265,
      0.38847330310001005,
      0.35492288154152063,
      0.38955858039617874,
      0.3190058163440914
    ],
    [
      0.3884102581760962,
      0.3640325368652062,
      0.37039493133863277,
      0.3841754823410404,
      0.3829617259681646,
      0.3630125659802854,
      0.3754884683390525,
      0.3370352799570493,
      0.3700956662106041,
      0.39854045543476446,
      0.3909191015539757,
      0.3335157805369897,
      0.4015494253874494,
      0.36254045183282235,
      0.39949415779220154,
      0.41658988455617574,
      0.38807826279743596,
      0.3531839768889147,
      0.3379506031570685,
      0.411800205729959,
      0.396498659107122,
      0.0,
      0.41033847490039754,
      0.35680002214787265,
      0.3382084854284486,
      0.3327471935160562,
      0.4108903784143938,
      0.4041802392013232,
      0.3976632709686023
    ],
    [
      0.34659328095643027,
      0.31229617683034916,
      0.3372485292248002,
      0.3338885586447944,
      0.3842364759517718,
      0.29417601132353144,
      0.3254513013117124,
      0.30857467178796605,
      0.32008731544633173,
      0.4156556298935692,
      0.3665471655441228,
      0.2839312216344154,
      0.3502928393326601,
      0.3494446313263164,
      0.33935964270567287,
      0.33735555605133083,
      0.39006002006552776,
      0.3014166611134992,
      0.34319214115120156,
      0.3576159772150367,
      0.39017680421344214,
      0.366411330779983,
      0.0,
      0.3261045018010533,
      0.34749814357121855,
      0.3191358917706766,
      0.40254839338450044,
      0.33274236989347217,
      0.2851939235345018
    ],
    [
      0.35160943631619435,
      0.40026267608109967,
      0.42528884417588264,
      0.4577964111680799,
      0.394976035465892,
      0.36087105125415864,
      0.3315201409659496,
      0.3672109145110618,
      0.41218063863079735,
      0.3930102692930726,
      0.37385150460166017,
      0.26479731810063933,
      0.4197079230951033,
      0.41048730228172126,
      0.4230452510014122,
      0.38868017255913556,
      0.3905013186697137,
      0.37772341071709836,
      0.382992797155435,
      0.40622643820229953,
      0.37385039638151896,
      0.34974754143597764,
      0.35773542094901156,
      0.0,
      0.3725492036245803,
      0.36984361492028417,
      0.3809047207473717,
      0.43870051814670896,
      0.31181745887652035
    ],
    [
      0.2604181392578617,
      0.4209998638991581,
      0.39644759133785157,
      0.3483223461498439,
      0.37422408492633963,
      0.31173653247602395,
      0.33853360723079273,
      0.28568625390166713,
      0.3336368670499992,
      0.34591304227752806,
      0.35134140900300914,
      0.2772503126321717,
      0.33435374924148253,
      0.31667085366114933,
      0.359646447709425,
      0.3423171675469707,
      0.334941464336322,
      0.3127871814583254,
      0.32473617652253783,
      0.3615326467797353,
      0.38699313303094685,
      0.33058196256117744,
      0.35410978086873035,
      0.36301910092359724,
      0.0,
      0.3086041484500588,
      0.3516191755043183,
      0.34968361202527576,
      0.29043992370726057
    ],
    [
      0.3494824063274744,
      0.3818839138148369,
      0.38321061197205597,
      0.4152328493040529,
      0.40321510077046896,
      0.35807490155769983,
      0.32362077666367317,
      0.4052199104582461,
      0.38407180437087773,
      0.3835886990872148,
      0.41171870301700686,
      0.31193947543248157,
      0.4107253599284577,
      0.3922426981266338,
      0.39604284650300414,
      0.380297083942696,
      0.3485345186668245,
      0.39079908680638775,
      0.35241565477864456,
      0.42715333635080643,
      0.38894135990855583,
      0.3572284026586985,
      0.34800788369069746,
      0.37744965435602484,
      0.33707535190384186,
      0.0,
      0.3832326345790247,
      0.38936861459873806,
      0.3227064061865621
    ],
    [
      0.33174677750454395,
      0.38224892673935096,
      0.3738641782198857,
      0.4214234018965939,
      0.45755307393585243,
      0.3227960969893573,
      0.361810895384141,
      0.3553545179949835,
      0.34771153490452744,
      0.39883448945493183,
      0.39399198265248403,
      0.29731063765665766,
      0.41139171413861764,
      0.38969936958792273,
      0.4146666911337873,
      0.36008001057814143,
      0.39212249123791665,
      0.38838992378769666,
      0.3755008404338489,
      0.38756801525888473,
      0.37845824444025267,
      0.39728341704323,
      0.43027766237423504,
      0.42051630850209,
      0.4290548606694604,
      0.3871080961695419,
      0.0,
      0.39559605793789987,
      0.3646115369936831
    ],
    [
      0.3019527245615792,
      0.41879298932269937,
      0.39543081805695457,
      0.41254775014518996,
      0.36630366650408286,
      0.32330971156414345,
      0.2937132058775038,
      0.3164388314740594,
      0.40686562832622997,
      0.39272259825668443,
      0.39583674740633223,
      0.2648271998379512,
      0.4098438750409883,
      0.39637401795579197,
      0.3759878358659685,
      0.39914582330235415,
      0.3702589823826834,
      0.3669491715981863,
      0.34143217257493297,
      0.39862757670453086,
      0.3573643615172981,
      0.3315742679291003,
      0.3528904735157774,
      0.4174032024450369,
      0.3151658061885352,
      0.38071593964194794,
      0.3324387799071802,
      0.0,
      0.2833901231386591
    ],
    [
      0.36398242982525275,
      0.37318902123594944,
      0.3859014078817453,
      0.40369315750783774,
      0.372500387017648,
      0.3506187639456726,
      0.37899960188609616,
      0.3806600589015685,
      0.3842249420656998,
      0.4419706172391984,
      0.39327597635838174,
      0.3764699568412815,
      0.3868668900270904,
      0.37008833140290753,
      0.4015930127902696,
      0.3848743560156278,
      0.3757344527628166,
      0.4186529258902014,
      0.36341715348805836,
      0.385320407346891,
      0.4196832025242945,
      0.4829623294518557,
      0.41429977566105336,
      0.4034996235253092,
      0.37094211854483605,
      0.4181445457349191,
      0.4308939806580352,
      0.4177509456324886,
      0.0
    ]
  ],
  "row_avgs": [
    0.22121978708798756,
    0.4112950827977526,
    0.3816229898779759,
    0.3944181131160081,
    0.3821561971219025,
    0.3090547448967275,
    0.3335180884014242,
    0.29870907068410574,
    0.37201364789955826,
    0.36740538596436184,
    0.39661884481629733,
    0.24733254033810984,
    0.35595231728816346,
    0.3758063516438081,
    0.38884797725792986,
    0.34649223374030486,
    0.345199555149338,
    0.3286807730251697,
    0.34932083807338926,
    0.39157737924248853,
    0.3835607083582776,
    0.3777534265902895,
    0.34168697023071026,
    0.3817103117617279,
    0.33809094908819853,
    0.37548143020577457,
    0.384534705486447,
    0.36136801003722796,
    0.3946503704343924
  ],
  "col_avgs": [
    0.32708117969612877,
    0.391941270405685,
    0.38093798592741107,
    0.39183508352022545,
    0.3663748608780056,
    0.3301299210605562,
    0.3340857011764472,
    0.3335097013811015,
    0.36499983373800815,
    0.3805607020151617,
    0.3868057748174178,
    0.27497685736548666,
    0.3893367031697633,
    0.3741077470790035,
    0.368889259549507,
    0.39394062358435616,
    0.35458762941325744,
    0.3402708868907495,
    0.3527838499682478,
    0.38987613193736903,
    0.3602894048899151,
    0.33919989289700136,
    0.345906797794735,
    0.36429061313469596,
    0.33692618043494715,
    0.3458496758260669,
    0.3452035372333762,
    0.3650850820718743,
    0.3062959127593481
  ],
  "combined_avgs": [
    0.27415048339205816,
    0.40161817660171883,
    0.3812804879026935,
    0.3931265983181168,
    0.374265528999954,
    0.31959233297864187,
    0.33380189478893574,
    0.31610938603260363,
    0.3685067408187832,
    0.3739830439897618,
    0.39171230981685756,
    0.26115469885179826,
    0.3726445102289634,
    0.37495704936140584,
    0.37886861840371844,
    0.3702164286623305,
    0.3498935922812977,
    0.3344758299579596,
    0.35105234402081853,
    0.39072675558992875,
    0.37192505662409636,
    0.3584766597436454,
    0.3437968840127226,
    0.37300046244821194,
    0.33750856476157287,
    0.36066555301592074,
    0.3648691213599116,
    0.3632265460545511,
    0.35047314159687026
  ],
  "gppm": [
    584.6137880225549,
    581.2950346289147,
    585.6544210906585,
    582.6710066505473,
    595.0034397101366,
    607.9526877311499,
    608.0256103918156,
    602.6862814863396,
    595.7370927474801,
    584.650525209206,
    585.3389282016831,
    636.3757040684596,
    584.3257472628044,
    591.7757772982286,
    595.1094392840614,
    581.8698311607517,
    594.9564883711388,
    609.5445845169589,
    600.650423085779,
    580.2314436771014,
    593.1595704978168,
    607.2221723639614,
    606.8981481477648,
    598.8579046847764,
    604.3513182093492,
    599.4895120580398,
    605.9011812568973,
    594.3545139331949,
    622.667433332758
  ],
  "gppm_normalized": [
    1.376928352947113,
    1.2976697482318278,
    1.3128154460537267,
    1.2996261091917127,
    1.3265066393610456,
    1.3547552345997775,
    1.3603403619306167,
    1.3466624237457834,
    1.3328011592953997,
    1.305053233640263,
    1.3012146945413667,
    1.429880689993899,
    1.3052810574258253,
    1.3241696382735175,
    1.3228227128426606,
    1.3000204847188894,
    1.3281309965480508,
    1.36130819999506,
    1.3435318457964534,
    1.2979058960595509,
    1.3201371452018773,
    1.3423295346687643,
    1.3483105971865748,
    1.3383409822886436,
    1.3411241184586011,
    1.3345347483440333,
    1.3449400292404707,
    1.3233777075163222,
    1.3886602113852853
  ],
  "token_counts": [
    1207,
    434,
    493,
    429,
    428,
    406,
    454,
    432,
    465,
    440,
    395,
    511,
    454,
    459,
    388,
    451,
    431,
    433,
    459,
    475,
    406,
    332,
    377,
    453,
    375,
    404,
    371,
    411,
    404,
    540,
    455,
    437,
    430,
    521,
    409,
    389,
    478,
    408,
    402,
    425,
    502,
    482,
    414,
    404,
    440,
    399,
    395,
    422,
    465,
    395,
    363,
    418,
    397,
    461,
    389,
    405,
    485,
    416,
    591,
    436,
    437,
    455,
    402,
    510,
    464,
    508,
    466,
    453,
    422,
    488,
    439,
    436,
    446,
    468,
    473,
    418,
    388,
    465,
    435,
    376,
    411,
    407,
    434,
    468,
    401,
    449,
    396
  ],
  "response_lengths": [
    2703,
    2431,
    2571,
    2581,
    2273,
    2842,
    2661,
    2917,
    2628,
    2653,
    2442,
    2708,
    2507,
    2504,
    2426,
    2704,
    2665,
    2270,
    2275,
    2607,
    2553,
    2221,
    2282,
    2274,
    2408,
    2568,
    2311,
    2547,
    2236
  ]
}