{
  "example_idx": 58,
  "reference": "EQUIVARIANT DISENTANGLED TRANSFORMATION FOR DOMAIN GENERALIZATION UNDER COMBINATION SHIFT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMachine learning systems may encounter unexpected problems when the data distribution changes in the deployment environment. A major reason is that certain combinations of domains and labels are not observed during training but appear in the test environment. Although various invariance-based algorithms can be applied, we find that the performance gain is often marginal. To formally analyze this issue, we provide a unique algebraic formulation of the combination shift problem based on the concepts of homomorphism, equivariance, and a refined definition of disentanglement. The algebraic requirements naturally derive a simple yet effective method, referred to as equivariant disentangled transformation (EDT), which augments the data based on the algebraic structures of labels and makes the transformation satisfy the equivariance and disentanglement requirements. Experimental results demonstrate that invariance may be insufficient, and it is important to exploit the equivariance structure in the combination shift problem.\n\n1\n\nINTRODUCTION\n\nThe way we humans perceive the world is combinatorial — we tend to cognize a complex object or phenomenon as a combination of simpler factors of variation. Further, we have the ability to recognize, imagine, and process novel combinations of factors that we have never observed so that we can survive in this rapidly changing world. Such ability is usually referred to as generalization. However, despite recent super-human performance on certain tasks, machine learning systems still lack this generalization ability, especially when only a limited subset of all combinations of factors are observable (Sagawa et al., 2020; Tr ̈auble et al., 2021; Goel et al., 2021; Wiles et al., 2022). In risk-sensitive applications such as driver-assistance systems (Alcorn et al., 2019; Volk et al., 2019) and computer-aided medical diagnosis (Castro et al., 2020; Bissoto et al., 2020), performing well only on a given subset of combinations but not on unobserved combinations may cause unexpected and catastrophic failures in a deployment environment.\n\nDomain generalization (Wang et al., 2021a) is a problem where we need to deal with combinations of two factors: domains and labels. Recently, Gulrajani & Lopez-Paz (2021) questioned the progress of the domain generalization research, claiming that several algorithms are not significantly superior to an empirical risk minimization (ERM) baseline. In addition to the model selection issue raised by Gulrajani & Lopez-Paz (2021), we conjecture that this is due to the ambitious goal of the usual domain generalization setting: generalizing to a completely unknown domain. Is it really possible to understand art if we have only seen photographs (Li et al., 2017)? Besides, those datasets used for evaluation usually have almost uniformly distributed domains and classes for training, which may be unrealistic to expect in real-world applications.\n\nA more practical but still challenging learning problem is to learn all domains and labels, but only given a limited subset of the domain-label combinations for training. We refer to the usual setting of domain generalization as domain shift and this new setting as combination shift. An illustration is given in Fig. 1. Combination shift is more feasible because all domains are at least partially observable during training but is also more challenging because the distribution of labels can vary significantly across domains. The learning goal is to improve generalization with as few combinations as possible.\n\n1\n\n(a) Domain shift: Y train Y train\n\n= Y test\n\n1\n\n2 = {0, 1, 2, 3, 4}.\n\n2\n\n= {A, B}, Y test\n\n1\n\n= {C},\n\n(b) Combination shift: Y train Y train = {0, 1, 2, 3, 4}, Y test\n\n1\n\n2\n\n= Y test\n\n1 = {A, B, C},\n\n2 = {0, 1, 2, 3}.\n\nFigure 1: Domain generalization under domain shift (an unseen domain) and combination shift (unseen combinations of domains and labels). Domain: color, label: digit, training:\n\n, test:\n\n.\n\nTo solve the combination shift problem, a straightforward way is to apply the methods designed for domain shift. One approach is based on the idea that the prediction of labels should be invariant to the change of domains (Ganin et al., 2016; Sun & Saenko, 2016; Arjovsky et al., 2019; Creager et al., 2021). However, we find that the performance improvement is often marginal. Recent works (Wiles et al., 2022; Schott et al., 2022) also provided empirical evidence showing that invariance-based domain generalization methods offer limited improvement. On the other hand, they also showed that data augmentation and pre-training could be more effective. To analyze this phenomenon, a unified perspective on different methods is desired.\n\nIn this work, we provide an algebraic formulation for both invariance-based methods and data augmentation methods to investigate why invariance may be insufficient and how we should learn data augmentations. We also derive a simple yet effective method from the algebraic requirements, referred to as equivariant disentangled transformation (EDT), to demonstrate its usefulness.\n\nOur main contributions are as follows:\n\nWe provide an algebraic formulation for the combination shift problem. We show that invariance is only half the story and it is important to exploit the equivariance structure. We present a refined definition of disentanglement beyond the one based on group action (Higgins et al., 2018), which may be interesting in its own right.\n\nBased on this algebraic formulation, we derive (a) what combinations are needed to effectively learn augmentations; (b) what augmentations are useful for improving generalization; and (c) what regularization can be derived from the algebraic constraints, which can serve as a guidance for designing data augmentation methods.\n\nAs a proof of concept, we demonstrate that learning data augmentations based on the algebraic structures of labels is a promising approach for the combination shift problem.\n\n2 PROBLEM: DOMAIN GENERALIZATION UNDER COMBINATION SHIFT\n\nThroughout the following sections, we study the problem of transforming a set of features X to a set of targets Y via a function f : X → Y . Here, X can be a set of images, texts, audios, or more structured data, while Y is the space of outputs. Further, the target Y may have multiple components. For example, Y1 is the set of domain indices and Y2 is the set of target labels.\n\nIdeally, all combinations of domains and target labels would be uniformly observable. However, in reality, it may not be the case because of selection bias, uncontrolled variables, or changing environments (Sagawa et al., 2020; Tr ̈auble et al., 2021). Let Y train denote the sets of i-th components (the support of the marginal distributions) observed in the training and test data. In the usual domain generalization setting (Wang et al., 2021a; Gulrajani & Lopez-Paz, 2021), the goal is to generalize to a completely unseen domain, i.e., domain shift. We have Y train but\n\nand Y test\n\n= Y test\n\ni\n\ni\n\n2\n\n2\n\n2\n\nAB0C1234AB0C12341\n\n∩ Y test\n\nY train can generalize without the knowledge of the unknown domain (Wiles et al., 2022).\n\n1 = ∅. However, it is unclear how different domains should relate and why a model\n\nIn this work, we focus on a more practical condition, called combination shift and illustrated in Fig. 1, where all test domains and labels can be observed separately during training, i.e., Y test (i = 1, 2), but not all their combinations. An example is the spurious relationship problem (Torralba & Efros, 2011), such as the co-occurrence of the objects and their background (Sagawa et al., 2020). In an extreme case, the combinations in the training and test sets could be disjoint, which requires completely out-of-distribution generalization. We survey related problems and approaches in more detail in Appendix D.\n\n⊆ Y train\n\ni\n\ni\n\n3 FORMULATION: EQUIVARIANCE TO PRODUCT ALGEBRA ACTIONS\n\nThis section outlines the concepts needed to formally describe the problem and our proposed method. See Appendices B and C for a more detailed review and concrete examples. Those who are interested in the proposed method itself may skip this section and directly jump to Section 4.\n\nBecause in the domain generalization problem, we have at least two sets, domains and labels, it is natural to study their product structure, which is manifested as statistical independence or operational disentanglement. We focus on the latter and use the following definition: Definition 1. Let {Ai = (Ai, {f j )}i∈I be algebras indexed by i ∈ I, each of which consists of the underlying set Ai and a collection of operations f j i of arity nj indexed by j ∈ Ji. Let A = (cid:81) i∈I Ai be the product algebra whose underlying set is the product set A = (cid:81) i∈I Ai. Let A act on sets X and Y via actions actX : A × X → X and actY : A × Y → Y . A transformation f : X → Y is disentangled if it is equivariant to actX and actY .\n\nnj i → Ai}j∈Ji\n\n: A\n\ni\n\nIn short, a disentangled transformation is a function equivariant to actions by a product algebra. Note that a definition of disentangled representations based on product group action has been given in Higgins et al. (2018), which is a special case when {Ai}i∈I are all groups. We emphasize that the concept of disentanglement is rooted in product, not group nor action. We will unwind this definition and discuss the reasons for this extension as well as its limitations below.\n\n3.1 HOMOMORPHISM AND EQUIVARIANCE\n\nAn algebra consists of one or more sets, a collection of operations on these sets, and a collection of universally quantified equational axioms that these operations need to satisfy. A homomorphism between algebras is a function between the underlying sets that preserves the algebraic structure.\n\nA (left) action of a set A on another set X is simply a binary function act : A × X → X. An action is equivalent to its exponential transpose or currying, a function (cid:99)act : A → X X from A to the set of endofunctions X X , also known as a representation of A on X. An action is faithful if all endofunctions are distinct, and trivial if all elements are mapped to the identity function idX .\n\nLet actX and actY be actions of A on X and Y , respectively. A function f : X → Y is equivariant to actX and actY if\n\nSpecifically, if actY is trivial, f is called invariant to actX :\n\n∀a ∈ A, f ◦ (cid:99)actX (a) = (cid:99)actY (a) ◦ f.\n\n∀a ∈ A, f ◦ (cid:99)actX (a) = f.\n\n(1)\n\n(2)\n\nIn summary, for an underlying set X, an algebra over X describes the structure of the set X itself, while an action or a representation of another algebraic structure A on X describes the structure of a subset of the endofunctions X X . Homomorphisms and equivariant functions describe how the structures of the set and endofunctions are preserved, respectively. An equivariant map can be also considered as a homomorphism between two algebras whose operations are all unary and indexed by elements in the set A. Note that only the equivariance — the structure of endofunctions — may not fully characterizes a learning problem, because not all operations are unary operations. In some problems, it would be necessary to consider the preservation of the structure of other operations with the concept of algebra homomorphism. See also Appendices A to C.\n\n3\n\n3.2 MONOID AND GROUP\n\nLet us focus on the endofunctions X X for now. A way to describe the structure of a subset of endofunctions X X is to specify an algebra A and an action of A on X preserving the algebraic structure. For example, an important operation is the function composition ◦ : X X × X X → X X , which can be described by how an action preserves a binary operation · : A × A → A:\n\n∀a1, a2 ∈ A, (cid:99)act(a1 · a2) = (cid:99)act(a1) ◦ (cid:99)act(a2).\n\n(3)\n\nSince the function composition is associative, (A, ·) should be a semigroup. If we also want to include the identity function idX , then there should exist an identity element e ∈ A (a nullary operation), which makes (A, ·, e) a monoid.\n\nRemark 1 (Group). If we only consider invertible endofunctions, then A becomes a group (Higgins et al., 2018). However, only considering groups could be too restrictive. For example, periodic boundary conditions are required (Higgins et al., 2018; Caselles-Dupr ́e et al., 2019; Quessard et al., 2020; Painter et al., 2020) for two-dimensional environments (e.g., dSprites (Matthey et al., 2017)), so that all the movements are invertible and have a cyclic group structure. This is only possible in synthetic environments such as games, not in the real world. Another example is the 3D Shapes dataset (Burgess & Kim, 2018), which consists of images of three-dimensional objects with different shapes, colors, orientations, and sizes. It is acceptable to model the shape, color, and orientation with permutation groups or cyclic groups. However, it is unreasonable if we increase the size of the largest object, then it becomes the smallest. This is because we only consider the set of natural numbers, representing size, count, or price, and of which addition only has a monoid structure. Therefore, it is important to consider endofunctions in general, not only the invertible ones. In this work, we mainly focus on monoid actions that only describe the function composition and identity function.\n\n3.3 PRODUCT AND DISENTANGLEMENT\n\nFinally, we are in a position to introduce the concept of disentanglement used in Definition 1. For two objects Y1 and Y2, we can consider their product Y = Y1 × Y2, which is defined via a pair of canonical projections p1 : Y1 × Y2 → Y1 and p2 : Y1 × Y2 → Y2. This means that we can divide the product into parts and process each part separately without losing information. We reiterate that:\n\nProduct structure is the core of disentanglement.\n\nSpecifically, (a) if Y1 and Y2 are just sets, Y is their Cartesian product; (b) if Y1 and Y2 have algebraic structures, Y is the product algebra and the operations are defined componentwise; and (c) if A1 and A2 act on Y1 and Y2, respectively, then the product algebra A = A1 × A2 can act on Y = Y1 × Y2 componentwise.\n\nAdditionally, if we let P Y be the set of all measures on Y , then P Y1 × P Y2 is the set of joint distributions where two components are statistically independent, while P Y = P (Y1 × Y2) is the set of all possible joint distributions. Product is the common denominator for all the definitions of disentanglement. In Definition 1, we only considered the product structure of endofunctions.\n\nWe can use this definition to formulate the domain generalization problem as follows. We assume that Y = Y1 × Y2 has two components, where Y1 is the set of domain indices and Y2 is the set of other target labels. we choose a structure of a subset of the endofunctions Y Y , described by two algebras A1 and A2 and two actions actY1 and actY2 . Then, we let the product algebra A = A1 × A2 act on Y = Y1 × Y2 componentwise via an action actY . We also assume that there is an action actX of A on X that manipulates the features. After properly choosing the algebras and actions, the problem can be then formulated as finding a function equivariant to actX and actY .\n\nNote that it is usually unnecessary and sometimes impossible to decompose X into a product, i.e., X = X1 ×X2 may not exist. For example, when X is a set of objects with different shapes and colors, there does not exist an object without color. In this case, we could only equip the endofunctions X X with a product structure.\n\n4\n\n4 METHOD: EQUIVARIANT DISENTANGLED TRANSFORMATION\n\nIn this section, we present our proposed method based on an algebraic formulation of the combination shift problem. The basic idea is that if we choose the algebra properly, the algebraic requirements of the transformation naturally lead to useful architectures and regularization.\n\nIn the following discussion, we assume that actYi i = 1, 2. We denote an instance whose labels are y1 and y2 by xy1,y2.\n\n(ai, yi) = y′\n\ni for some ai ∈ Ai and yi, y′\n\ni ∈ Yi,\n\n4.1 MONOID STRUCTURE\n\nFirst, we discuss how to choose the algebra that is suitable for our problem and derive the algebraic requirements. As discussed in Section 3.2, we only require that algebras A1 and A2 are monoids, which means that there exist associative binary operations ·i : Ai × Ai → Ai and identity elements ei ∈ Ai for i = 1, 2. Then, according to Eq. (3) (action commutes with composition), we can derive that a product action (cid:99)act(a1, a2) on X or Y can be decomposed in two ways:\n\n(cid:99)act(a1, a2) = (cid:99)act(a1, e2) ◦ (cid:99)act(e1, a2) = (cid:99)act(e1, a2) ◦ (cid:99)act(a1, e2).\n\nOr equivalently, the following diagram commutes (when the action is on Y = Y1 × Y2):\n\n(y1, y2)\n\n(cid:99)act(e1,a2)\n\n(y1, y′\n\n2)\n\n(cid:99)act(a1,e2)\n\n(cid:99)act(a1,a2)\n\n(cid:99)act(a1,e2)\n\n(4)\n\n(5)\n\n(y′\n\n1, y2)\n\n(cid:99)act(e1,a2)\n\n(y′\n\n1, y′\n\n2)\n\nThus, we can focus on the endofunctions of the form (cid:99)act(a1, e2) and (cid:99)act(e1, a2), whose compositions constitute all endofunctions of interest. Remark 2 (Size). Denoting the cardinality of a set A by |A| and the image of a function f on a set X by f [X], i.e., a set defined by {f (x) | x ∈ X}, we can prove that | (cid:99)act([A1], e2)| ≤ |A1|, | (cid:99)act(e1, [A2])| ≤ |A2|, and | (cid:99)act([A1], [A2])| = | (cid:99)act([A1], e2)| × | (cid:99)act(e1, [A2])| ≤ |A1| × |A2|. The equality holds when the actions are faithful. Thanks to the monoid structure and the product structure, we can reduce the number of endofunctions that we need to deal with from |A1| × |A2| to at most |A1| + |A2|. We can further reduce the number if A1 or A2 has a smaller generator. For example, although the monoid (N, +) of natural numbers under addition has infinite elements, it can be generated from a singleton {1}. In this case, we can focus on a single endofunction that increases the value by a unit, and all other endofunctions are compositions of this special endofunction.\n\n4.2 EQUIVARIANCE REQUIREMENT\n\nThen, consider a function f : X → Y that extracts only necessary information and preserves the algebraic structure of interest. We require it to be equivariant to two actions actX and actY . Recall that we can consider endofunctions only of the form (cid:99)act(a1, e2) and (cid:99)act(e1, a2). Based on Eq. (1) (action commutes with transformation), we can derive the algebraic requirement shown in the following commutative diagram:\n\nxy1,y2\n\n(cid:99)actX (a1,e2)\n\nxy′\n\n1,y2\n\nf\n\nf\n\n(y1, y2)\n\np1\n\ny1\n\n(cid:99)actY (a1,e2)\n\np2\n\np2\n\ny2\n\nidY2\n\n(cid:99)actY1\n\n(a1)\n\n(6)\n\n(y′\n\n1, y2)\n\np1\n\ny′\n\n1\n\nWith the projections p1 and p2, we can see that this requirement results in the following four conditions: (a) f1 = p1 ◦f is equivariant to (cid:99)actX (−, e2) and (cid:99)actY1; (b) f1 is invariant to (cid:99)actX (e1, −); and dually, (c) f2 = p2 ◦ f is equivariant to (cid:99)actX (e1, −) and (cid:99)actY2 ; (d) f2 is invariant to (cid:99)actX (−, e2). The symbol − is a placeholder, into which arguments can be inserted.\n\n5\n\n(R, 1)\n\nNotation:\n\ncomponent 1\n\ncomponent 2\n\naugmentation prediction\n\nTraining:\n\n(cid:99)act(a1, e2) (cid:99)act(e1, a2) (cid:99)act(a1, a2) f : X → Y1 × Y2\n\n(B, 1)\n\nSelect suitable data pairs and learn component augmentations separately (Eq. (7));\n\nRegularize augmentations (Eqs. (8) and (9)), simultaneously or alternatively;\n\nTrain a prediction model (Eq. (10)).\n\n(R, 0)\n\n(B, 0)\n\nFigure 2: Equivariant Disentangled Transformation (EDT). All diagrams commute.\n\n4.3 ALGORITHM\n\nFinally, we present a method directly derived from the algebraic requirements of the transformation, referred to as equivariant disentangled transformation (EDT) and illustrated in Fig. 2. Since the formulation above naturally generalizes to the case of multiple factors Y = Y1 × · · · × Yn, we present the method in the general form.\n\nArchitecture Since the output space Y and the selected endofunctions on it are manually designed, the action actY on Y is known and fixed. However, the action actX on X is usually not available. So our first goal is to learn a set of endofunctions αj i , . . . , en) indexed by aj i ∈ Ai, i = 1, . . . , n. These endofunctions can be considered as learned augmentations of data that only modify a single factor while keeping other factors fixed. Second, we need to approximate the equivariant function f using a trainable function φ : X → Y . Due to the property of product, any function to a product arises from component functions φi : X → Yi, i = 1, . . . , n. Therefore, we can train a model for each component and make these models satisfy the algebraic requirements specified bellow.\n\ni : X → X representing (cid:99)actX (e1, . . . , aj\n\nData selection and augmentation To train an augmentation αj x and x′ such that actX ((e1, . . . , aj (aj and xy1,...,y′ we can learn the augmentations by minimizing a statistical distance d : P X × P X → R≥0:\n\ni , we need to collect pairs of instances i , . . . , en), x) = x′, in other words, pairs of the form xy1,...,yi,...,yn i , yi) = y′ i. Then, denoting the set of all measures on X by P X,\n\n, where actYi\n\ni,...,yn\n\nl0(αj\n\ni ) = d(αj\n\ni (x), x′).\n\n(7)\n\nWith a slight abuse of notation, here x and x′ also represent the empirical distribution. Choices of the statistical distance d include the expected pairwise distance (Kingma & Welling, 2014), maximum mean discrepancy (Li et al., 2015; Dziugaite et al., 2015; Muandet et al., 2017), Jensen–Shannon divergence (Goodfellow et al., 2014), and Wasserstein metric (Arjovsky et al., 2017; Gulrajani et al., 2017; Miyato et al., 2018).\n\nn\n\ni,...,y′\n\n1,...,y′\n\nRemark 3 (Cycle consistency). It is possible to use all pairs of the form xy1,...,yi,...,yn and xy′ , i.e., pairs of instances whose i-th labels correspond to the action, but other labels could be different. For example, if Ai is a group, we can simultaneously train two models that are the inverse of each other with a cycle consistency constraint (Zhu et al., 2017; Goel et al., 2021). With this constraint, the learned augmentation is likely an approximation of (cid:99)actX (e1, . . . , aj i , . . . , en). However, it is still possible to obtain approximations of (cid:99)actX (q1, . . . , aj i , . . . , qn) and its inverse where q1, . . . , qn are not necessarily the identity elements. This happens especially when there are more than two factors and not all combinations are available, which is demonstrated in Section 5.\n\n6\n\n(a) Compositionality of multi-scale augmentations\n\n(b) Commutativity of two disentangled augmentations\n\nFigure 3: Regularizing compositionality and commutativity (and other algebraic structures) of augmentations is a way to introduce inductive biases and exploit the relationships between training examples, which is useful especially when the combinations of factors are scarce in the training data.\n\nThe rich algebraic structure yields various constraints, which can be used as regularization for augmentations. Next, we present three regularization techniques derived from the basic product monoid structure. Note that we can introduce more constraints if we choose a richer algebra.\n\nRegularization 1 (Compositionality of augmentations) According to Eq. (3), if αj·k i , we can simply define it as αj·k approximated action of aj it directly, the algebraic requirement leads to the following regularization:\n\nis the i . If we need to approximate\n\ni = αj\n\ni ◦ αk\n\ni ·i ak\n\ni\n\ni\n\ni (αk\n\ni , αk\n\nl1(αj\n\n) = d(αj\n\ni (x)), αj·k\n\ni , αj·k A special case is when we know the composition is the identity function αj·k i is the inverse of ak i . This regularization is then equivalent to the “cycle consistency loss” in the CycleGAN model (Zhu et al., 2017) or the “isomorphism loss” in the GroupifiedVAE model (Yang et al., 2022). Another example is for modifying instances with real-valued targets. We could use multi-scale augmentations (e.g., α1 increases the value by 1 unit and α5 increases the value by 5 units) to reduce the cumulative error and gradient computation, and this regularization ensures that these augmentations are consistent with each other (e.g., (α1)5 ≈ α5).\n\ni = idX , i.e., aj\n\n(x)).\n\n(8)\n\ni\n\nRegularization 2 (Commutativity of augmentations) According to the diagram in Eq. (5), we can derive the following regularization, which means that the order of augmentations for different factors should not matter:\n\ni , αl\n\nl2(αk\n\nj(αk This can be interpreted as a commutativity requirement: the augmentations are grouped by the factors they modify, and augmentations from different groups should commute, but augmentations within the same group are usually not commutative. Again, we point out that this is only based on the product monoid structure and is nothing group-specific.\n\nj) = d(αl\n\ni (x)), αk\n\nj(x))).\n\ni (αl\n\n(9)\n\nIn Fig. 4, we illustrate a concrete example of compositionality and commutativity regularization based on the dSprites dataset (Matthey et al., 2017). The movement of position can be modeled via the additive monoid of natural numbers; while the change of shape can be formulated by a permutation/cyclic group. Suitable training example pairs can be used for learning augmentations directly (l0), but such pairs may be limited. Algebraic regularization terms (e.g., l1 and l2) introduce inductive biases so that more relationships between training examples can be used as supervision.\n\nRegularization 3 (Equivariance of transformation) According to the diagram in Eq. (6), we can derive the following equivariance and invariance regularization:\n\nl3(αj\n\ni , φk) =\n\n(cid:40)\n\nd(φi(αj d(φk(αj\n\n(aj i (x)), actYi i (x)), φk(x))\n\ni , φi(x)))\n\ni = k, i ̸= k.\n\n(10)\n\nIt is a good strategy to learn the augmentations first and then use them to improve the transformation (Goel et al., 2021). However, we can see from this regularization that if the transformation is well trained, it can be used for improving the augmentations too.\n\n7\n\nTable 1: The classification accuracy (%, “mean (standard deviation)” of 5 trials) on the colored MNIST data. For each setting (column), the method with the highest mean accuracy and those methods that are not statistically significantly different from the best one (via one-tailed t-tests with a significance level of 0.05), if any, are highlighted in boldface.\n\n(train/test)\n\nERM IRM CORAL DANN Fish Mixup MixStyle EDT\n\nAXIS (14/36)\n\nSTEP (15/35)\n\nRAND-0.5 (25/25)\n\nRAND-0.7 (35/15)\n\nRAND-0.9 (45/5)\n\n56.74(12.40) 55.40(7.23) 72.47(17.33) 82.33(12.76) 69.06(14.50) 63.59(11.98) 97.10(1.36) 97.58(0.17)\n\n47.09(8.29) 39.54(7.78) 49.72(10.53) 45.02(3.79) 45.18(3.36) 36.30(4.42) 95.73(1.83) 98.13(0.15)\n\n91.13(3.70) 87.61(5.37) 83.48(5.83) 91.76(2.67) 79.93(5.12) 92.56(1.81) 95.25(1.83) 96.70(1.36)\n\n97.18(0.98) 96.89(2.44) 94.61(3.62) 97.99(0.32) 96.29(1.33) 97.62(0.99) 97.57(1.11) 98.55(0.13)\n\n98.61(0.29) 98.20(0.57) 98.21(0.75) 98.54(0.20) 98.13(0.43) 98.20(0.65) 98.13(0.76) 98.21(0.42)\n\n5 EXPERIMENTS\n\nAs a proof of concept, we conduct experiments to support the following claims:\n\nLearning data augmentation is a promising approach for the combination shift problem. Cycle consistency may be insufficient, and additional constraints need to be considered. We should regularize the data augmentations so that they satisfy the algebraic requirements.\n\n5.1 COMBINATION SHIFT\n\nFirst, we experimentally demonstrate the insufficiency of the invariance-based approach and the potential of the augmentation-based approach for the combination shift problem.\n\nData We colored the grayscale images from the MNIST dataset (LeCun et al., 1998) with 5 colors to create a semi-synthetic setting. Therefore, there are 5 domains (colors) and 10 classes (digits). We tested the methods in the most extreme case where the combinations of domains and classes of the training and test sets are disjoint. We selected five types of combinations as the training set: AXIS: all red digits and zeros of all colors; STEP: three digits for each color (shown in Fig. 6 in Appendix E); RAND-0.5/-0.7/-0.9: combinations randomly selected with a fixed ratio.\n\nMethod In addition to an ERM baseline, we evaluated four invariance-based methods: IRM (Arjovsky et al., 2019), CORAL (Sun & Saenko, 2016), DANN (Ganin et al., 2016), and Fish (Shi et al., 2022); and two augmentation-based methods: Mixup (Zhang et al., 2018) and MixStyle (Zhou et al., 2021). Model architectures and hyperparameters are given in Appendix E.\n\nResults We can see from Table 1 that the ERM baseline and invariance-based methods perform poorly if only limited combinations of domains and classes are observable. The high variance indicates that the learned representation may still depend on the domains. As more combinations become observable in training, the differences in performance of all methods become less statistically significant. On the other hand, the augmentation-based methods usually provide higher performance improvements, although the mixup method may deteriorate performance depending on the setting. MixStyle performs consistently well, partially because it is specifically designed for image styles and thus lends itself well to this setting. With the algebraic constraints, EDT may capture the underlying distribution better and offer larger improvements.\n\n5.2 DATA AUGMENTATION\n\nNext, we discuss potential issues of the augmentation-based method (Goel et al., 2021) based on CycleGAN (Zhu et al., 2017), which matches the bidirectionally transformed distributions and regularizes the composition to be the identity functions. There are two major issues of this approach. Firstly, it is designed only for two domains (e.g., female and male). Secondly and more importantly,\n\n8\n\nTable 2: The misclassification rate (%) of shape and mean squared errors (×100) of scale, orientation, and positions on the dSprites dataset (“mean (standard deviation)” of 5 trials).\n\nShape\n\nScale\n\nOrientation\n\nPosition X\n\nPosition Y\n\nERM MixStyle EDT (l0, l3) EDT (l0, l1, l2, l3)\n\n60.84(2.24) 59.92(2.00) 14.36(0.75) 4.55(0.21)\n\n3.76(0.24) 6.73(1.07) 1.30(0.06) 0.59(0.01)\n\n13.13(0.72) 13.04(0.54) 2.09(0.07) 2.01(0.07)\n\n1.97(0.69) 0.20(0.10) 0.04(0.01) 0.02(0.00)\n\n1.87(0.25) 0.21(0.06) 0.04(0.01) 0.02(0.00)\n\n(a) Without compositionality regularization, the error may accumulate after a few compositions.\n\n(b) Without commutativity regularization, pairs for learning augmentations may be insufficient.\n\nFigure 4: Randomly selected 5 images (top row) in the dSprites dataset (Matthey et al., 2017) and augmented images (bottom 4 rows) of position (Fig. 4a, left ⇝ right) and shape (Fig. 4b, square ⇝ ellipse ⇝ heart ⇝ square), without (left) and with (right) regularization.\n\nas discussed in Remark 3, when there are more than two factors, cycle consistency alone may not guarantee the identity of non-transformed factors. The comparison on the 3D Shapes dataset (Burgess & Kim, 2018) is shown in Fig. 8 in Appendix E. We can observe that although the floor hue is transformed as desired and the reconstructed images are almost identical to the original ones, other factors such as the object/wall hues are also changed. In contrast, the algebraic requirements of EDT ensure the approximated augmentations are consistent with the desired actions.\n\n5.3 ALGEBRAIC REGULARIZATION\n\nFinally, we further compare heuristic and learned data augmentations and demonstrate the usefulness of algebraic regularization. We used the dSprites dataset (Matthey et al., 2017) and considered one factor as target label and the others as domains. Some methods are no longer applicable because of the continuous or even periodic values of factors and the multiplicatively increasing number of combinations. In Table 2, we can see that MixStyle provides no significant performance gain in this setting because the heuristic augmentation does not match the underlying mechanism anymore (See also Fig. 10 in Appendix E). In Fig. 4, we provide the results of an ablation study of the compositionality (l1) and commutativity (l2) regularization, showing that these regularization terms can reduce errors accumulated by compositions of augmentations and increase the number of supervision signals for learning augmentations, as illustrated in Fig. 3.\n\n6 CONCLUSION\n\nUnlike the usual goal of generalizing to an unseen domain, we formulated the problem of combination shift as learning the knowledge of each factor (domains and labels) and generalizing to unseen combinations of factors, which makes deployment more feasible but training more challenging. We found that invariance-based methods may not work well in this setting, but augmentationbased methods usually excel. To formally analyze data augmentations and provide a guideline on augmentation design, we presented an algebraic formulation of the problem, which also leads to a refined definition of disentanglement. We demonstrated the usefulness of constraints derived from algebraic requirements, discussed potential issues of the existing augmentation method based on cycle consistency, and showed the importance of algebraic regularization. We then pointed out several promising research directions, such as incorporating algebra homomorphism and multi-sorted algebra to discuss a wider range of data augmentation operations. We hope that our algebraic formulation can be used to derive practical algorithms in applications and inspire further studies in this direction.\n\n9\n\nREFERENCES\n\nIsabela Albuquerque, Jo ̃ao Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas. Generalizing to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804, 2019. D\n\nMichael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Computer Vision and Pattern Recognition, 2019. 1\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International Conference on Machine Learning, 2017. 4.3\n\nMartin Arjovsky, L ́eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\narXiv preprint arXiv:1907.02893, 2019. 1, 5.1, D\n\nSteve Awodey. Category theory. Oxford university press, 2010. B\n\nSolon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairml-\n\nbook.org, 2019. D\n\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151–175, 2010. D\n\nAharon Ben-Tal and Arkadi Nemirovski. Robust optimization–methodology and applications. Math-\n\nematical programming, 92(3):453–480, 2002. D\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013. D\n\nGeorge M Bergman. An invitation to general algebra and universal constructions. Springer, 2015. B\n\nAlceu Bissoto, Eduardo Valle, and Sandra Avila. Debiasing skin lesion datasets and models? not so\n\nfast. In Computer Vision and Pattern Recognition Workshops, 2020. 1\n\nGilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification\n\ntasks to a new unlabeled sample. In Neural Information Processing Systems, 2011. D\n\nChris Burgess and Hyunjik Kim. 3D shapes dataset. https://github.com/deepmind/3d-shapes, 2018.\n\n1, 5.2, 8, 2\n\nHugo Caselles-Dupr ́e, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled representation learning requires interaction with environments. In Neural Information Processing Systems, 2019. 1, D\n\nDaniel C Castro, Ian Walker, and Ben Glocker. Causality matters in medical imaging. Nature\n\nCommunications, 11(1):1–10, 2020. 1\n\nShuxiao Chen, Edgar Dobriban, and Jane Lee. A group-theoretic framework for data augmentation.\n\nIn Neural Information Processing Systems, 2020. D\n\nElliot Creager, J ̈orn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant\n\nlearning. In International Conference on Machine Learning, 2021. 1, D\n\nPim de Haan, Taco S Cohen, and Max Welling. Natural graph networks. In Neural Information\n\nProcessing Systems, 2020. A.3\n\nDavid S Dummit and Richard M Foote. Abstract algebra. Prentice Hall Englewood Cliffs, NJ, 1991.\n\nB\n\nGintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Conference on Uncertainty in Artificial Intelligence, 2015. 4.3\n\n10\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ̧ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. 1, 5.1, D\n\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional\n\nneural networks. In Computer Vision and Pattern Recognition, 2016. A.1\n\nKaran Goel, Albert Gu, Yixuan Li, and Christopher R ́e. Model patching: Closing the subgroup performance gap with data augmentation. In International Conference on Learning Representations, 2021. 1, 3, 4.3, 5.2, D, 8\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Processing Systems, 2014. 4.3, E.1\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International\n\nConference on Learning Representations, 2021. 1, 2\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein GANs. In Neural Information Processing Systems, 2017. 4.3\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. C.4, D\n\nIrina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018. 1, 3, 1, A.2, C.4, D\n\nJustin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal\n\nof Big Data, 6(1):1–54, 2019. D\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International\n\nConference on Learning Representations, 2015. E.1, E.3\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference\n\non Learning Representations, 2014. 4.3, D\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition, 1998. 5.1, 1\n\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain\n\ngeneralization. In International Conference on Computer Vision, 2017. 1\n\nYujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International\n\nConference on Machine Learning, 2015. 4.3\n\nXialei Liu, Joost Van De Weijer, and Andrew D Bagdanov. Leveraging unlabeled data for crowd\n\ncounting by learning to rank. In Computer Vision and Pattern Recognition, 2018. A.1\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch ̈olkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In International Conference on Machine Learning, 2019. C.4, D\n\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement\n\ntesting sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 1, 4.3, 4, 5.3, 3\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. 4.3, E.1\n\nKrikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Sch ̈olkopf, et al. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends® in Machine Learning, 10(1-2):1–141, 2017. 4.3\n\n11\n\nMatthew Painter, Adam Prugel-Bennett, and Jonathon Hare. Linear disentangled representations and\n\nunsupervised action estimation. Neural Information Processing Systems, 2020. 1, D\n\nRobin Quessard, Thomas Barrett, and William Clements. Learning disentangled representations and group structure of dynamical environments. In Neural Information Processing Systems, 2020. 1, D\n\nJoaquin Qui ̃nonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset\n\nshift in machine learning. MIT Press, 2008. D\n\nAlexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R ́e. Learning to compose domain-specific transformations for data augmentation. Neural Information Processing Systems, 30, 2017. D\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015. E.1\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust\n\nneural networks. In International Conference on Learning Representations, 2020. 1, 2\n\nLukas Schott, Julius Von K ̈ugelgen, Frederik Tr ̈auble, Peter Vincent Gehler, Chris Russell, Matthias Bethge, Bernhard Sch ̈olkopf, Francesco Locatello, and Wieland Brendel. Visual representation learning does not generalize strongly within the same domain. In International Conference on Learning Representations, 2022. 1\n\nYuge Shi, Jeffrey Seely, Philip Torr, Siddharth N, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In International Conference on Learning Representations, 2022. 5.1, D\n\nConnor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.\n\nJournal of big data, 6(1):1–48, 2019. D\n\nRui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentanglement with guarantees. In International Conference on Learning Representations, 2020. C.4, D\n\nHwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022. D\n\nMasashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments:\n\nIntroduction to covariate shift adaptation. MIT press, 2012. D\n\nMasashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. Neural Information Processing Systems, 20, 2007. D\n\nBaochen Sun and Kate Saenko. Deep CORAL: Correlation alignment for deep domain adaptation.\n\nIn European conference on computer vision, pp. 443–450. Springer, 2016. 1, 5.1, D\n\nRaphael Suter, Djordje Miladinovic, Bernhard Sch ̈olkopf, and Stefan Bauer. Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness. In International Conference on Machine Learning, 2019. C.4, D\n\nSeiya Tokui and Issei Sato. Disentanglement analysis with partial information decomposition. In\n\nInternational Conference on Learning Representations, 2022. C.4, D\n\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In Computer Vision and Pattern\n\nRecognition, 2011. 2\n\nFrederik Tr ̈auble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Sch ̈olkopf, and Stefan Bauer. On disentangled representations learned from correlated data. In International Conference on Machine Learning, 2021. 1, 2\n\n12\n\nGeorg Volk, Stefan M ̈uller, Alexander Von Bernuth, Dennis Hospach, and Oliver Bringmann. Towards robust cnn-based object detection through augmentation with synthetic rain variations. In Intelligent Transportation Systems Conference (ITSC), 2019. 1\n\nRiccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In Neural Information Processing Systems, 2018. D\n\nHao Wang, Hao He, and Dina Katabi. Continuously indexed domain adaptation. In International\n\nConference on Machine Learning, 2020. D\n\nJindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Wenjun Zeng, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In International Joint Conference on Artificial Intelligence, 2021a. 1, 2, D\n\nTan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang. Self-supervised learning disentangled group representation as feature. In Neural Information Processing Systems, 2021b. D\n\nYulin Wang, Gao Huang, Shiji Song, Xuran Pan, Yitong Xia, and Cheng Wu. Regularizing deep networks with semantic data augmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021c. D\n\nOlivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In International Conference on Learning Representations, 2022. 1, 1, 2, E.4\n\nTao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a group-based unsupervised representation disentanglement framework. In International Conference on Learning Representations, 2022. 4.3, D\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. 5.1, D, 10, E.4\n\nYifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning:\n\nA survey. arXiv preprint arXiv:2110.04596, 2021. D\n\nKaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with MixStyle. In\n\nInternational Conference on Learning Representations, 2021. 5.1, D, 10, E.4\n\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In International Conference on Computer Vision, 2017. 3, 4.3, 5.2, 8\n\n13\n\n(cid:32)\n\n,\n\n⊕\n\n(cid:33)\n\nf ×f\n\n((R, 0), (B, 1))\n\np1×p2\n\nf\n\n(R, 1)\n\nFigure 5: A homomorphism preserving binary operations ⊕ and p1 × p2.\n\nA LIMITATIONS AND FUTURE WORK\n\nIn this section, we discuss the limitations of this work and potential future work directions.\n\nA.1 ALGEBRA HOMOMORPHISM\n\nIn this work, we only formulated data augmentations of the endofunction form α : X → X, i.e., modifications of only one input. However, there are other operations that do not fall into this form. We suggest using algebra homomorphisms to capture their relations. Here we give three examples:\n\nComponent combination If the instance can be divided into multiple components, then we can recombine the components from multiple instances to generate a new instance: α : X n → X. This is especially useful when there are many factors and the combinations in the training set are sparse.\n\nStyle transfer Another example is when we cannot divide the instances but can combine their characteristics, such as style transfer (Gatys et al., 2016). An example is given in Fig. 5, where ⊕ : X × X → X is the binary operation that takes the “style” of the first image and the “content” of the second image, and p1 × p2 : Y × Y → Y is the corresponding operation in the label space Y . Then, we need to ensure that this binary operation is compatible with other augmentations. For example, if the object in the content image changes, the object in the generated image should change accordingly; while the generated image should not change regardless of the object in the style image.\n\nCrowd counting Counting the number of objects or people in an image is an example where we can exploit the structure of natural numbers N. In addition to the monotone function requirement induced by the total order of natural numbers N (Liu et al., 2018), the free monoid structure (N, +) may induce other useful constraints. For example, the count of two parts should be the sum of the counts of each part. This requirement can be formulated as an algebra homomorphism.\n\nA.2 STATISTICS AND APPROXIMATION\n\nSimilarly to previous work (Higgins et al., 2018), we focused more on the algebraic aspect. We admit that there is still a gap between formulation and practice, because algebra only describes exact equality (=), but sometimes we are more interested in approximate equality (≈). It would be useful to define concepts such as commutativity over a metric space, so that we can analyze errors and introduce statistical tools, to get the best of both worlds.\n\nA.3 STATE AND MULTI-SORTED ALGEBRA\n\nAnother issue is that we only considered endofunctions X → X so all data augmentations are applicable to all instances in a “stateless” way, which may not hold true in more complex situations. As a future work, we may consider general functions Xi → Xj and define which functions are composable and which are not.\n\nAlso, it could be useful to discuss operations on multiple sets based on multi-sorted algebra, such as graphs (de Haan et al., 2020).\n\n14\n\nB A BRIEF REVIEW OF ALGEBRA\n\nIn this section, we review the algebraic concepts used in this work. We refer the readers to Dummit & Foote (1991) (abstract algebra), Bergman (2015) (universal algebra), and Awodey (2010) (category theory) for further readings.\n\nB.1 ALGEBRA\n\nDefinition 2 (Algebra). A (single-sorted) algebra consists of\n\na set A, called the underlying set of the algebra,\n\na collection of operations {f i : Ani → A}i∈I , and\n\na collection of universally quantified equational axioms that those operations satisfy.\n\nFor example, elementary algebra is the study of the set of numbers with arithmetic operations such as addition, subtraction, multiplication, division, and exponentiation. Linear algebra is the study of the set of vectors with operations of vector addition and scalar multiplication.\n\nSome algebras with only one binary operation are listed below.\n\nDefinition 3 (Magma). A magma is a set A equipped with a binary operation · : A × A → A.\n\nDefinition 4 (Semigroup). A semigroup is a magma (S, ·) whose binary operation is associative:\n\n∀s1, s2, s3 ∈ S, (s1 · s2) · s3 = s1 · (s2 · s3).\n\n(11)\n\nDefinition 5 (Monoid). A monoid is a semigroup (M, ·) that has an identity element e ∈ M (a nullary operation e : 1 → M ):\n\n∀m ∈ M, e · m = m · e = m.\n\n(12)\n\nDefinition 6 (Group). A group is a monoid (G, ·, e), and every element has an inverse (a unary operation (−)−1 : G → G):\n\n∀g ∈ G, g · g−1 = g−1 · g = e.\n\n(13)\n\nDefinition 7 (Abelian group). An Abelian group is a group (G, ·, e, (−)−1) whose binary operation is commutative:\n\n∀g1, g2 ∈ G, g1 · g2 = g2 · g1.\n\n(14)\n\nB.2 HOMOMORPHISM\n\nDefinition 8 (Homomorphism). A homomorphism between two algebras (A, {f i (B, {f i\n\nB}i∈I ) of the same type is a function between the underlying sets h : A → B such that\n\nA}i∈I ) and\n\n∀a1, . . . , ani\n\n∈ A, h(f i\n\nA(a1, . . . , ani\n\n)) = f i\n\nB(h(a1), . . . , h(ani\n\n))\n\n(15)\n\nholds for all corresponding operations f i\n\nA : Ani → A and f i\n\nB : Bni → B.\n\nIn other words, the following diagram commutes for all i ∈ I:\n\nAni\n\nf i\n\nA\n\nA\n\nhni\n\nBni\n\nf i\n\nB\n\nB\n\nh\n\n(16)\n\nAn invertible homomorphism is called an isomorphism. For example, exp and log functions form a pair of isomorphisms between (R, +) and (R+, ×) because exp(x + y) = exp(x) × exp(y) and log(x × y) = log(x) + log(y).\n\n15\n\nB.3 EXPONENTIAL\n\nDefinition 9 (Exponential). Given sets A and B, the function set BA is the set of all functions from A to B. Given a set A and a function set BA, there exists an evaluation map ε : BA × A → B that sends a function f : A → B and a value a ∈ A to the evaluation ε(f, a) = f (a) ∈ B.\n\nDefinition 10 (Exponential transpose). For a binary function f : A × B → C, its exponential transpose (also known as currying) is a function (cid:98)f : A → C B such that\n\n∀a ∈ A, ∀b ∈ B, f (a, b) = (cid:98)f (a)(b).\n\n(17)\n\nB.4 ACTION\n\nDefinition 11 (Action). A (left) action of a set A on a set X is a binary function act : A × X → X. Definition 12 (Representation). A representation of a set A on a set X is a function (cid:99)act : A → X X . Definition 13 (Algebra preservation). A representation (cid:99)act : A → X X preserves an algebra over A if it is a homomorphism from A to X X .\n\nA magma/semigroup action preserves composition (a binary operation):\n\n∀a1, a2 ∈ A, ∀x ∈ X, act(a1 · a2, x) = act(a1, act(a2, x)).\n\nA × A × X\n\nidA × act\n\nA × X\n\n·×idX\n\nA × X\n\nact\n\nX\n\nact\n\nOr equivalently,\n\n∀a1, a2 ∈ A, (cid:99)act(a1 · a2) = (cid:99)act(a1) ◦ (cid:99)act(a2).\n\nA × A\n\n(cid:99)act× (cid:99)act\n\nX X × X X\n\n·\n\nA\n\n(cid:99)act\n\n◦\n\nX X\n\nA monoid action preserves identity (a nullary operation):\n\n∀x ∈ X, act(e, x) = x.\n\n(cid:99)act(e) = idX .\n\n1\n\ne\n\nA\n\n(cid:99)act\n\n1\n\nidX\n\nX X\n\nA group action preserves inverse (a unary operation):\n\n∀a ∈ A, ∀x ∈ X, act(a−1, act(a, x)) = x.\n\n∀a ∈ A, (cid:99)act(a−1) = (cid:99)act(a)−1.\n\nA\n\n(cid:99)act\n\nX X\n\n(−)−1\n\n(−)−1\n\nA\n\n(cid:99)act\n\nX X\n\n16\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\n(27)\n\nB.5 EQUIVARIANCE\n\nDefinition 14 (Equivariance). A function f : X → Y is equivariant to two actions actX : A × X → X and actY : A × Y → Y if\n\nOr equivalently,\n\n∀a ∈ A, ∀x ∈ X, f (actX (a, x)) = actY (a, f (x)).\n\nA × X\n\nidA ×f\n\nA × Y\n\nactX\n\nX\n\nactY\n\nY\n\nf\n\n∀a ∈ A, f ◦ (cid:99)actX (a) = (cid:99)actY (a) ◦ f.\n\nX\n\n(cid:99)actX (a)\n\nX\n\nf\n\nf\n\nY\n\nY\n\n(cid:99)actY (a)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\ncommutes for all a ∈ A. This justifies that an equivariant map is a homomorphism between two algebras whose operations are all unary and indexed by elements in the set A.\n\nB.6 PRODUCT\n\nDefinition 15 (Product). A product A×B of two objects A and B and the corresponding projections p1 : A × B → A and p2 : A × B → B satisfy that for any object C and morphisms f1 : C → A and f2 : C → B, there is a unique morphism f : C → A × B, such that f1 = p1 ◦ f and f2 = p2 ◦ f , as indicated in\n\nC\n\nf\n\nf1\n\nf2\n\nA\n\np1\n\nA × B\n\nB\n\np2\n\n(32)\n\nConsider two morphisms f : C → A and g : D → B. Based on the universal property of A × B, there exists a unique morphism f × g : C × D → A × B such that the following diagram commutes:\n\np1\n\nC\n\nC × D\n\np2\n\nf\n\nf ×g\n\nD\n\ng\n\nA\n\np1\n\nA × B\n\nB\n\np2\n\n(33)\n\nFor example, let both C and D be Y1 × Y2, f = p1, and g = p2. Then, the following diagram represents “recombination of components”:\n\nY1 × Y2\n\np1\n\n(Y1 × Y2) × (Y1 × Y2)\n\np2\n\nY1 × Y2\n\np1\n\nY1\n\np1×p2\n\np1\n\nY1 × Y2\n\np2\n\np2\n\nY2\n\n(34)\n\n17\n\nC ALGEBRA IN SUPERVISED LEARNING\n\nIn this section, we look ahead to the application of algebraic theory to supervised learning.\n\nC.1 SUPERVISED LEARNING\n\nLet X be the set of inputs and Y the set of outputs. In supervised learning, we want to find a function f : X → Y that satisfies some properties. Generally, this is achieved by collecting a set of pairs {(xi, yi) ∈ X × Y }i∈I as training examples and defining a measure of “goodness” of functions. For example, for a pair (xi, yi), we expect f to map xi to yi.\n\nLet us consider this procedure from an algebraic perspective.\n\nNullary operation First, we point out that identifying an element x from a set X can be considered as a nullary operation x : 1 → X, and evaluating a function f : X → Y at an element x is simply function composition f ◦ x : 1 → Y . Then, requiring\n\nis equivalent to say that f should be an algebra homomorphism:\n\nf (x) = y\n\n1\n\nx\n\nX\n\n1\n\ny\n\nY\n\nf\n\n(35)\n\n(36)\n\nTherefore, a function that can predict all training examples perfectly is simply a homomorphism from algebra (X, {xi : 1 → X}i∈I ) to algebra (Y, {yi : 1 → Y }i∈I ) where all operations are nullary.\n\nThis perspective frames direct supervision as an algebraic requirement. However, it is still not practically useful, because the training examples are usually finite and cannot enumerate the set of inputs, but we need machine learning only when the inputs in a test environment are not exactly the same as the inputs for training. Two things are missing: first, we need an assumption to relate training and test data; second, we need not only “yes or no” but also “how much”. As discussed in Appendix A, pure algebra only deals with exact equality, so integrating algebra and statistical learning is an important research direction.\n\nUnary operation Many works introducing algebraic theory, especially group theory, into machine learning, including this work, have focused on unary operations and their relations. A unary operation or an endofunction αX : X → X transforms a set of states to itself. A homomorphism between (X, αX ) and (Y, αY ) just relates these unary operations:\n\nX\n\nαX\n\nX\n\nf\n\nf\n\nαY\n\nY\n\nY\n\n(37)\n\nUsually, there are multiple unary operations, which themselves form an algebra. Magma/semigroup describes composition, monoid describes identity, and group describes invertibility. An invertible unary operation/endofunction is also called a symmetry. The structure of these unary operations can be described by an action preserving the algebraic structure, which was extensively used in this work.\n\nBinary and n-ary operations As also covered in Appendix A, not all operations are unary operations. It would be useful to include n-ary operations and their relations as algebraic requirements for f :\n\nX n\n\nαX\n\nX\n\nY n\n\nαY\n\nY\n\nf n\n\nf\n\n18\n\n(38)\n\nSpecifically, operad theory could be useful for analyzing a collection of finitary operations obeying equational axioms.\n\nMoreover, future research could continue to explore n-ary functions from an algebraic perspective. For example, f : X → Y and g : A → B may relate two binary functions αX : X × X → A and αY : Y × Y → B in the following sense:\n\nX × X\n\nf ×f\n\nY × Y\n\nαX\n\nA\n\ng\n\nαY\n\nB\n\n(39)\n\nwhich could be used for formulating relation-preserving functions, such as equality (learning from similarity) and order (learning to rank), or metrics, such as isometry, contraction, and Lipschitz continuous function.\n\nC.2 BINARY CLASSIFICATION\n\nNow, let us consider a concrete example, binary classification. Let ̄n be a set whose cardinality is n, ̄1 a singleton (a set of a single element), + the disjoint union of sets (union of labeled/indexed elements), ∼= the isomorphism between two sets (a bijective function). In binary classification, Y is simply a set of two elements ̄2 ∼= ̄1 + ̄1.\n\nIn other words, we only have a space with the concept of sameness or equality and no other operations. The learning process is to find a function f : X → ̄2, which decomposes into a pair of functions f = f1 + f2, where fi : Xi → ̄1(i = 1, 2). This results in a decomposition of X into two sets X ∼= X1 + X2, i.e., classification of elements in X. Let us examine the unary operations (endofunctions) on ̄2. There are in total four endofunctions on ̄2, which forms a monoid. There are only two invertible ones: the identity and the one that swaps two elements, which constitute a representation of the symmetric group S2 on ̄2.\n\nC.3 REGRESSION\n\nTo formulate regression, we usually let Y be the set of real numbers R. However, from an algebraic perspective, many operations of real numbers are not needed in the learning process. For example, we rarely consider the product or ratio of two target values. On the other hand, the order, scale, and zero point are of our central interest. Thus, if there exist a minimal value and a unit interval of targets, we can isomorphically transform the target and let Y be the set of natural numbers N. If we cannot determine a minimal value but we are still able to quantize the target values, we can take a step further and consider the algebra of integers Z and the negation operation. There are two important operations of natural numbers: 0 : 1 → N as a nullary operation that identifies the number zero and the successor function S : N → N as a unary operation that maps a number n to the next number S(n).\n\nLet xn ∈ X be an instance whose label is n. If X also has the structure of natural numbers, then there exist an element x0 that has the minimal value and a unary operation T : X → X that takes an instance as input and outputs another instance whose label is one unit higher. The requirement of f being a homomorphism means that the instance with the minimal value is mapped to 0, i.e., f (x0) = 0, and the operation T corresponds to the successor function S in the following way:\n\nxn\n\nT\n\nxS(n)\n\nf\n\nf\n\nn\n\nS\n\nS(n)\n\n(40)\n\nGiven the number zero 0 and the successor function S of natural numbers N, we can define a commutative monoid with 0 as the identity element and a monoid operation + defined recursively: a + S(b) := S(a + b). This is the free monoid (N, +) generated from a generator {1 := S(0)}.\n\n19\n\nThen, we can consider the case when the free monoid (N, +) acts on X and N itself. A function equivariant to free monoid actions is a function f : X → N such that the following diagram commutes:\n\n(m, xn)\n\nidN ×f\n\n(m, n)\n\nactX\n\nxn+m\n\nf\n\n+\n\nn + m\n\nNote that when m is the generator 1, this diagram can be reduced to Eq. (40).\n\nThe crowd counting example in Appendix A can be illustrated in the following diagram:\n\n(xm, xn)\n\nf ×f\n\n(m, n)\n\n⊕\n\nxm+n\n\nf\n\n+\n\nm + n\n\n(41)\n\n(42)\n\nwhich means that the count of two parts should be the sum of the counts of each part. This requirement is formulated as a homomorphism of binary operations ⊕ : X × X → X and + : N × N → N.\n\nC.4 DISCUSSION\n\nAs discussed in Section 3.1, the equivariance alone may not fully characterizes a learning problem. For example, in binary classification, if we only require the transformation f : X → Y to be equivariant to actions by the symmetric group S2, then f is only unique up to permutation; Similarly, in regression, f is only unique up to shift by a natural number or an integer. This may not cause a problem, but we still need some information to determine the optimal solution, for example, the zero point (a nullary operation) in regression.\n\nSimilarly to Higgins et al. (2018), we focused on the algebraic aspect of disentanglement. It is worth noting that this formulation is not yet compatible with some definitions of disentanglement based on statistical independence, probability metric, or causal mechanisms (Higgins et al., 2017; Suter et al., 2019; Locatello et al., 2019; Shu et al., 2020; Tokui & Sato, 2022). In statistical learning, we usually want to find a conditional distribution ̄f : X → P Y , where P Y denotes all probability measures on Y , instead of merely a deterministic transformation f : X → Y . To extend this framework and fully capture the statistical aspect of disentanglement, we need to further incorporate the structure of probability measures, which is left for future work.\n\n20\n\nD LITERATURE REVIEW: DISTRIBUTION SHIFT\n\nIn this section, we review related work in distribution shift in a broader sense.\n\nThe difference between the training and test data in supervised learning is an important problem and has been studied for years. The distribution shift problem (Qui ̃nonero-Candela et al., 2008) refers to the general case where the training and test data are drawn from related but different distributions:\n\nptrain(X, Y ) ̸= ptest(X, Y )\n\nThe difference can be measured by some distribution divergence (Ben-David et al., 2010; Albuquerque et al., 2019). Distribution shift can be subcategorized by the distribution assumptions:\n\nCovariate shift: ptrain(Y | X) = ptest(Y | X) (Sugiyama et al., 2007; Sugiyama & Kawanabe, 2012) Label shift: ptrain(X | Y ) = ptest(X | Y ), e.g., class imbalance (Johnson & Khoshgoftaar, 2019) and long-tailed class distribution (Zhang et al., 2021) Concept shift: ptrain(X) = ptest(X), e.g., noisy labels (Song et al., 2022)\n\nDistribution shift is also closely related to robust optimization (Ben-Tal & Nemirovski, 2002) and fairness in machine learning (Barocas et al., 2019).\n\nDomain adaptation/generalization (Wang et al., 2021a) is a special distribution shift problem (Qui ̃nonero-Candela et al., 2008), implying that the tasks are indexed by a categorical (Blanchard et al., 2011) or continuous (Wang et al., 2020) domain variable. All three types of distribution shift mentioned above may happen when there are multiple domains. To solve this problem, domaininvariant representation learning (Ganin et al., 2016; Sun & Saenko, 2016; Arjovsky et al., 2019; Creager et al., 2021; Shi et al., 2022) has been widely used, which aims to extract features invariant to domain change. In this work, we showed the limitations of invariance-based methods in the combination shift problem.\n\nA closely related concept is disentanglement (Bengio et al., 2013), which can be defined via statistical independence (Suter et al., 2019; Locatello et al., 2019; Shu et al., 2020; Tokui & Sato, 2022) or product group action (Higgins et al., 2018; Caselles-Dupr ́e et al., 2019; Quessard et al., 2020; Painter et al., 2020; Wang et al., 2021b; Yang et al., 2022). Our work follows the latter direction. We provided a refined definition of disentanglement based on algebra in Definition 1, which can be seen as an extension of Higgins et al. (2018). We also discussed potential directions for further extension in Appendix A, including algebra homomorphism, statistics, non-endofunctions, and multi-sorted algebra.\n\nVarious methods have been developed based on the concept of disentanglement. On approach is based on variants of the variational autoencoder (VAE) (Kingma & Welling, 2014; Higgins et al., 2017). Another promising approach is based on either heuristic (Zhang et al., 2018; Shorten & Khoshgoftaar, 2019; Chen et al., 2020; Zhou et al., 2021) or learned (Ratner et al., 2017; Volpi et al., 2018; Wang et al., 2021c; Goel et al., 2021) data augmentation. Learning data augmentation is the central interest of our work.\n\n21\n\nFigure 6: A set of combinations of the colored MNIST data with only 15/50 = 30% data for training. Shaded combinations are used for testing.\n\nE EXPERIMENTS\n\nE.1 MNIST\n\nData The MNIST1 dataset contains grayscale hand-written digit images of size 28 × 28 in 10 classes. The size of the training set is 60 000 and the size of the test set is 10 000. We only used the images in the training set and colored them with five colors (red, yellow, green, blue, and purple) with equal probabilities. The images were resized to 32 × 32 to fit the model. No manual data augmentation was used.\n\nData split We selected five types of combinations as the training set:\n\nAXIS: all red digits and zeros of all colors\n\nSTEP: three digits for each color, shown in Fig. 6\n\nRAND-0.5/-0.7/-0.9: combinations randomly selected with a fixed ratio 0.5, 0.7, or 0.9. All domains and classes were ensured to appear at least once.\n\nThe remaining combinations were used as the test set.\n\nModel We used U-Net (Ronneberger et al., 2015) for the image-to-image data augmentations with 3 layers of downscale/upscale modules and a sigmoid as the last layer. We used a convolutional neural network with spectral norm (Miyato et al., 2018) as the discriminator for distribution matching (Goodfellow et al., 2014) between images (l0, l1, and l2). To reduce the number of models, the discriminator was conditioned on the factors via additive embedding. We use the same architecture of the discriminator for the classifier except the dimension of output was set to 10. The learning objective for the classifier (l3) is the cross-entropy/negative log-likelihood.\n\nOptimization We used an Adam optimizer (Kingma & Ba, 2015) with batch size of 32, learning rate of 1 × 10−3 for the augmentations and 1 × 10−4 for the discriminator and the classifier. The model was trained for 10 000 iterations.\n\nInfrastructure The experiments were conducted on an NVIDIA Tesla V100 GPU.\n\n1MNIST (LeCun et al., 1998) http://yann.lecun.com/exdb/mnist/\n\n22\n\nFigure 7: A path of transformations of data (left to right, top to bottom) of the 3D Shapes dataset.\n\nFigure 8: (top row) 10 images from the 3D Shapes dataset (Burgess & Kim, 2018) with red floor; (middle rows) augmented data (red to orange) and reconstructed data (orange to red) transformed by a CycleGAN model (Zhu et al., 2017; Goel et al., 2021); (bottom row) augmented data transformed by EDT, which satisfies the algebraic constraints.\n\n23\n\nfloor hueorientationorientationorientationfloor huewall hueshapefloor huefloor hueorientationscaleorientationfloor huewall huewall hueshapewall hueorientationfloor huewall hueobject hueobject hueobject hueobject huefloor hueobject huewall huefloor huescaleorientationobject hueorientationshapewall hueshapeorientationscaleobject huefloor hueorientationobject huescaleobject hueorientationorientationorientationwall huescalescaleobject huewall hueorientationscalewall hueorientationscaleoriginalcycle-forwardcycle-backwardoursE.2\n\n3D SHAPES\n\nData The 3D Shapes2 dataset contains images of three-dimensional objects with 6 factors (floor hue, wall hue, object hue, scale, shape, and orientation), whose dimensions are 10, 10, 10, 8, 4, and 15. The size of the dataset is 480 000.\n\nData selection Since the goal is to improve generalization using as few combinations as possible, we used a set of properly selected combinations of factors. Concretely, we first randomly select an instance, and then randomly change a factor at a time. An example of a path of transformations is shown in Fig. 7. We used 10 random paths so there are at most 570 training examples (only around 0.1% of all data).\n\nModel and optimization Because there is only one image for each combination of factors, there is no need to use distribution matching. We used pixel-wise binary cross-entropy as the learning objective for l0, l1, and l2. Other hyperparameters are the same as those used above.\n\nE.3 DSPRITES\n\nData The dSprites3 dataset contains images of 2D shapes generated from 6 ground truth independent latent factors: color, shape, scale, rotation, x and y positions of a sprite, whose dimensions are 1, 3, 6, 40, 32, and 32. The size of the dataset is 737 280.\n\nData selection Note that there is no bijection between the factors and the images because of the intrinsic symmetries of the shapes, e.g., C4 of the square and C2 of the ellipse. To this end, we only considered a subset of the original dataset where the orientation only ranges from 0◦ to 90◦, which resulted in a dataset of size 184 320. The split of training and test data was similar to the above. Thus, we used only 830/184 320 ≈ 0.5% data for learning augmentations.\n\nModel and optimization We used a simple 3-layer MLP (64 × 64 → 256 → 64 → output) with ReLU activation as the prediction model, cross-entropy (classification) or mean squared error (regression) as the learning objectives, and an Adam optimizer (Kingma & Ba, 2015) with batch size of 32 and learning rate of 1 × 10−4.\n\nResults Additionally, we show the augmented images in Fig. 9. We can see that these augmentations are not equally easy to learn: the shape and position augmentations perform relatively well, but modifying the scale and orientation may cause shape distortion.\n\n23D Shapes (Burgess & Kim, 2018) https://github.com/deepmind/3d-shapes Apache License\n\n2.0\n\n3dSprites (Matthey et al., 2017) https://github.com/deepmind/dsprites-dataset Apache\n\nLicense 2.0\n\n24\n\n(a) Shape: square, ellipse, heart\n\n(b) Scale: 6 values linearly spaced in [0.5, 1]\n\n(c) Orientation: 10 values in [0◦, 90◦]\n\n25\n\n(d) Position X: 32 values in [0, 1]\n\n(e) Position Y: 32 values in [0, 1]\n\nFigure 9: Augmented training examples of the dSprites dataset\n\n26\n\nFigure 10: Mixup (Zhang et al., 2018) and MixStyle (Zhou et al., 2021) augmentations on the colored MNIST, 3D Shapes, and dSprites datasets.\n\nE.4 HEURISTIC AUGMENTATION\n\nFig. 10 shows the images from the colored MNIST, 3D Shapes, and dSprites datasets augmented by Mixup (Zhang et al., 2018) and MixStyle (Zhou et al., 2021). We can observe that MixStyle actually modifies the colors of the images in the colored MNIST dataset, which may explain why its performance is good in Table 1. Thus, our results also support the claim “heuristic augmentation improves generalization if the augmentation describes an attribute” from the empirical study of Wiles et al. (2022). When it is hard to design augmentations by hand, learning augmentations from data and regularizing these augmentations based on the algebraic constraints is a promising way to improve generalization, which is the main claim of our paper.\n\n27\n\noriginalmixupstyleoriginalmixupstyleoriginalmixupstyle",
  "translations": [
    "# Summary Of The Paper\n\nThe authors introduce the combination shift problem towards generalization - especially when the goal is to learn with as few combinations are observed in data. Towards this goal, the author extend the definition of disentaglement beyond of invariances to group actions - highlighting the necessity to exploit the equivariance structure. The authors then provide a strategy to effectively learn data augmentations towards the combination shift problem and improve generalization and subsequently provide proof of concept experiments.\n\n# Strength And Weaknesses\n\n**Strengths:**\n1. The combination shift problem is very well motivated, appropriately formulated and most importantly is an important problem in real world settings \n2. Most of the paper (and appendix) is very well written and clear\n3. The proposed algorithm (sub section 4.3) is simple, without losing the majorly desired properties\n4.  The proof of concept experiments are well designed and show the use case of the proposed approach\n\n**Weaknesses**:\n1. While in definition 1, the authors assume that the product algebra in not necessarily finite (via an indexing set), it is unclear  - (the ramifications) of what happens when product decomposition is not countable. The algebraic regularization subsection - maybe is indicative of this - but currently the clarity of that is on the lower side.\n2. It is unclear if there is a strategy to learn the product decomposition (without underlying knowledge about the data - and especially in relation to 1 above). Please include synthetic proof of concept experiments to showcase the same.\n\n**Minor:**\n1. Is there a citation for the 1st line of the introduction? From cognitive studies/ neuroscience ?\n2.  Work [1] which assumes invariance to all groups initially (unless evidence shows otherwise) and then narrows over the subspace lattice  to identify the groups to be not invariant to is relevant work\n\n**References**\n1. Mouli, S. Chandra, and Bruno Ribeiro. \"Neural Networks for Learning Counterfactual G-Invariances from Single Environments.\" ICLR 2021\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity - Please see strength 2\n\nReproducibility - Currently code is not provided\n\nNovelty and Quality - Please see strengths 1,3, 4\n\n# Summary Of The Review\n\nIn my opinion, the strengths of the paper out weigh the weaknesses - and therefore recommend border line accept (initial review)\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents a novel approach to address the combination shift problem in domain generalization through a framework called Equivariant Disentangled Transformation (EDT). It introduces an algebraic formulation that characterizes this problem, highlighting the limitations of traditional invariance-based methods. The proposed methodology leverages algebraic structures of labels to ensure that data transformations satisfy both equivariance and disentanglement criteria. Experimental results demonstrate that EDT significantly outperforms existing methods in scenarios with limited combinations of domains and labels, underscoring the importance of exploiting equivariance in achieving robust generalization.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative algebraic formulation of the combination shift problem and the introduction of EDT, which effectively bridges the gap between theory and practical application in domain generalization. The experiments are comprehensive, providing clear evidence of the proposed method's superiority compared to traditional invariance-based approaches. However, the paper could strengthen its impact by including more diverse datasets and real-world applications to validate the generalizability of the findings. Additionally, while the theoretical underpinnings are well-explained, some aspects of the methodology could benefit from further elaboration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with the field. The quality of writing is high, with a logical flow of ideas from problem definition to experimental validation. The novelty of the approach is significant, as it introduces a new perspective on domain generalization through algebraic structures. Reproducibility is supported by a detailed description of the experiments and the methods used, although the inclusion of code or supplementary materials would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of domain generalization by addressing the combination shift problem through a novel algebraic framework. The proposed EDT method shows promising results, demonstrating the importance of equivariance in achieving effective generalization. While the paper is well-written and provides solid empirical evidence, further exploration of diverse applications would strengthen its findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the challenge of domain generalization under a \"combination shift,\" where certain combinations of domains and labels are not observed during training but appear in the testing phase. The authors introduce an innovative algebraic framework for this problem, encapsulated in their proposed method, Equivariant Disentangled Transformation (EDT). This method leverages algebraic structures to generate data augmentations that are disentangled and equivariant, thereby improving generalization performance. Experimental results demonstrate that EDT significantly outperforms traditional heuristics on several datasets, including Colored MNIST, dSprites, and 3D Shapes, revealing the effectiveness of algebraic regularization in enhancing data augmentation strategies.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel approach to a pertinent problem in the field of domain generalization, particularly through the introduction of an algebraic framework that provides new insights into data augmentation. The comprehensive experimental design across diverse datasets lends credibility to the findings and illustrates the method's robustness. However, the paper has limitations; notably, the generalizability of the results to more complex or varied datasets is not fully established. Additionally, the complexity of the algebraic formulation may pose challenges in practical implementations, and the focus on equivariance could overlook other essential properties relevant to specific learning tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents the methodology, although some sections may benefit from additional detail regarding the implications of the algebraic framework. The novelty of the proposed method is significant, offering a fresh perspective in a well-established area of research. The quality of the experimental evaluation is high, with robust statistical tests supporting the findings. However, reproducibility may be hindered by the complexity of the algebraic transformations and the lack of detailed implementation guidelines provided in the paper.\n\n# Summary Of The Review\nOverall, the paper provides a compelling and innovative approach to domain generalization under combination shifts, supported by solid experimental evidence. While the methodology is novel and the findings promising, the complexity and potential limitations in generalizability warrant further investigation to enhance practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to address the challenges of domain generalization under a setting termed \"combination shift,\" where not all combinations of domains and labels are available during training. The authors introduce the Equivariant Disentangled Transformation (EDT), which leverages algebraic structures to ensure both equivariance and disentanglement during data augmentation. Through comprehensive experiments on the Colored MNIST dataset, the paper demonstrates that EDT outperforms traditional invariance-based methods, highlighting the importance of algebraic constraints in handling unseen combinations effectively.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative formulation of the combination shift problem and the introduction of EDT, which offers a fresh perspective on data augmentation in machine learning. The algebraic approach provides a solid theoretical foundation and is well-supported by empirical results, which convincingly show the advantages of the proposed method over existing techniques. However, the paper could benefit from a more extensive discussion on the limitations of the approach and potential edge cases where EDT may underperform. Furthermore, while the experiments are promising, the generalizability of the findings to more complex datasets or real-world scenarios remains to be fully established.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow from the introduction to the formulation and methodology. The use of equations and definitions is precise, aiding in the understanding of complex concepts. The novelty of the approach is significant, as it combines ideas from algebra with machine learning in a unique way. However, the reproducibility of results may be limited by the specific dataset used and the experimental setups, which should be clearly detailed for others to replicate.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the domain generalization literature by addressing the combination shift problem through an innovative algebraic framework. The proposed EDT method shows strong empirical performance, although further exploration of its limitations and broader applicability would enhance the study.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel algebraic formulation of the combination shift problem, introducing the equivariant disentangled transformation (EDT) method. The authors argue that this method represents a significant advancement in the field of domain generalization by addressing limitations in existing invariance-based approaches. The empirical results show that the EDT method outperforms traditional methods, particularly through the application of tailored data augmentation strategies. The paper also offers a rigorous analysis of existing methods, introduces algebraic regularization techniques, and suggests future research directions.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach and rigorous analysis. The introduction of the EDT method is a notable contribution to the field, supported by empirical evidence demonstrating its effectiveness. However, the complexity of the algebraic formulation may limit its accessibility to practitioners without a strong mathematical background. Additionally, while the experiments provide valuable insights, the reliance on synthetic datasets raises concerns about the real-world applicability of the findings. The analysis could also benefit from more extensive quantitative comparisons with a wider array of existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the proposed methods and their implications. However, the dense mathematical content may be challenging for readers who are not familiar with algebraic principles, potentially hindering broader understanding and engagement with the work. The novelty of the proposed methods is significant, but reproducibility may be impacted by the complex nature of the techniques and the necessity for careful tuning of the regularization methods.\n\n# Summary Of The Review\nIn summary, the paper makes a noteworthy contribution to the field of domain generalization through its innovative EDT method and comprehensive analysis of existing approaches. While the findings are promising, the complexity of the proposed methods and the limited scope of the empirical evaluations may restrict their practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to address the combination shift problem in domain generalization through the introduction of the **Combinatorial Augmentation Framework (CAF)**. The authors shift the focus from traditional invariance-based methods to an algebraic framework that emphasizes combinatorial structures between domains and labels. The methodology includes defining disentangled representations in a new light, deriving algorithms for combinatorial augmentation, and providing empirical validation across several datasets (colored MNIST, 3D Shapes, dSprites). The findings indicate that CAF achieves significant improvements in generalization performance compared to standard methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to framing the combination shift problem through combinatorial relationships rather than invariance, which is a significant departure from established methodologies. This fresh perspective is backed by comprehensive experimental validation, showcasing the effectiveness of the proposed framework. However, a notable weakness is the lack of clarity in distinguishing the new combinatorial structures from traditional invariance concepts, which may confuse readers. Additionally, the theoretical justifications for why the combinatorial approach outperforms traditional methods could be further developed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with a clear presentation of the methodology and experimental results. However, some terminological clarifications would enhance understanding, especially for readers familiar with existing invariant-based approaches. The quality of writing is high, though the novelty of combining algebraic structures with domain generalization is not fully articulated in terms of theoretical implications. Reproducibility appears strong as the authors provide detailed algorithms and experimental setups, allowing other researchers to replicate their work.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of domain generalization by proposing the Combinatorial Augmentation Framework, which effectively addresses the combination shift problem. While the methodology and experimental results are robust, the paper would benefit from clearer terminology and deeper theoretical insights.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to adversarial training termed **Equivariant Adversarial Transformation (EAT)**, aimed at improving the robustness of machine learning models against adversarial attacks. The authors propose an algebraic framework that leverages concepts like homomorphism and equivariance to analyze and generate adversarial examples that maintain their adversarial properties under various transformations. Through comprehensive experimental evaluations on standard datasets, the authors demonstrate that models trained using EAT consistently outperform those trained with traditional methods, particularly in scenarios characterized by input transformations.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative algebraic framework that offers a structured understanding of adversarial transformations, which is a significant contribution to the field. The introduction of equivariant transformations provides a new perspective on generating adversarial examples, potentially leading to improved robustness in machine learning models. However, the paper does face limitations, particularly concerning the computational overhead introduced by the algebraic formulations and the complexity of the proposed transformations. Additionally, while the experimental results are compelling, the authors could further explore the implications of their method on a wider range of adversarial attacks and real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the proposed methodology and findings. The novel approach is grounded in solid theoretical foundations, which enhances its credibility. However, the complexity of the algebraic framework may pose challenges for some readers, potentially hindering reproducibility for those less familiar with the underlying mathematical concepts. The experimental setup is robust, utilizing well-known benchmarks, which aids in validating the results presented.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the area of adversarial training by introducing the concept of equivariant transformations. While the methodology is innovative and the results are promising, the paper could benefit from further exploration of practical implications and a more accessible presentation of its theoretical framework.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents an innovative approach to domain generalization under the combination shift problem, introducing the Equivariant Disentangled Transformation (EDT) framework. It claims to offer a novel algebraic formulation that redefines traditional methods, emphasizing the importance of equivariance in machine learning methodologies. The methodology proposes three new regularization techniques and showcases experimental results demonstrating significant performance improvements over existing approaches, positioning EDT as a potential cornerstone of future research.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious claim to provide a groundbreaking algebraic framework that addresses the combination shift problem, which is a critical issue in domain generalization. The introduction of three regularization techniques derived from this framework is an interesting contribution that could enhance the robustness of machine learning models. However, the paper's weaknesses include overstatements regarding the efficacy and universality of the EDT methodology, as well as a lack of comprehensive comparison with a broader range of existing methods. The claims of revolutionary change may not be sufficiently substantiated by the experimental results presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is written with a clear and assertive tone, promoting its novel contributions, the clarity of the methodology could be improved. The algebraic formulation and its implications require more detailed exposition to ensure that readers can fully grasp the significance of the claims. Additionally, reproducibility may be a concern as the paper does not provide sufficient information about the experimental setup or the datasets used, which are critical for validating the results.\n\n# Summary Of The Review\nOverall, the paper proposes a potentially impactful approach to domain generalization through the EDT framework, claiming substantial advancements over existing methods. However, the evidence presented does not entirely support the sweeping claims made, and further clarification and detail are needed to ascertain the true novelty and significance of the contributions.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the Equivariant Disentangled Transformation (EDT) method, which aims to tackle the challenges posed by combination shifts in machine learning, where not all combinations of domains and labels are observed during training. The authors provide an algebraic formulation to exploit the equivariance structure of the data, leading to improved data augmentation and regularization techniques. Experimental results demonstrate that EDT significantly outperforms traditional invariance-based methods and existing data augmentation techniques, achieving classification accuracies above 99% on specific tasks while also reducing misclassification rates effectively.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its novel approach to addressing combination shift through algebraic structures, which is theoretically grounded and practically validated through comprehensive experiments. The experimental results are compelling, showing clear superiority over previous methods. However, a potential weakness lies in the lack of detailed discussion on the limitations of the proposed method, and the results may raise concerns about reproducibility due to the altered values presented in the tables. Additionally, the paper could benefit from further exploration of the theoretical implications of the EDT approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and the writing is clear, making the complex concepts accessible to readers. The quality of the experimental design is commendable, yet the altered values in the results may hinder reproducibility. The novelty of the approach is significant, as it proposes a fresh perspective on an important problem in domain generalization; however, the implications of the algebraic formulation could be elaborated further to enhance understanding.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in addressing combination shifts in machine learning through the novel EDT method, which is both theoretically and empirically validated. Despite concerns about reproducibility due to altered data values, the findings indicate a promising direction for future research in leveraging algebraic structures for improved generalization.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel approach to address the combination shift problem by leveraging the equivariance structure inherent in the data. The authors introduce a refined definition of disentanglement and argue that their method, based on algebraic constraints, can generalize to unseen combinations of domains and labels. Empirical results indicate that their augmentation-based techniques outperform existing invariance-based methods on specific datasets, although the generalizability of these findings to other datasets remains questionable.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to the combination shift problem and the introduction of a new definition of disentanglement. However, several weaknesses are evident. The assumption that all transformations can be captured by equivariant mappings is overly simplistic and may not hold in real-world scenarios where data distributions are complex. Furthermore, the reliance on algebraic relations raises concerns about the method's ability to generalize across varying contexts. The lack of robust empirical validation and the limited exploration of potential causal structures also detracts from the overall contributions of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but some areas lack clarity, particularly concerning the assumptions made about data observability and the implications of cycle consistency. The novelty of the method is notable; however, its significance is diminished by the lack of comprehensive reproducibility details, such as hyperparameters and data preprocessing steps. This limits the ability of other researchers to replicate the experiments and validate the claims made by the authors.\n\n# Summary Of The Review\nThe paper presents a potentially valuable approach to the combination shift problem through the use of equivariance and algebraic constraints. However, the assumptions made may oversimplify complex data interactions, and the empirical findings require further validation across diverse datasets. Overall, while the paper contributes interesting ideas, its practical applicability remains uncertain due to the identified limitations.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper addresses the combination shift problem in domain generalization, where traditional methods based on invariance have shown limited performance improvements. The authors propose an innovative algebraic framework and introduce the Equivariant Disentangled Transformation (EDT) method to effectively address this issue. Their approach emphasizes the role of equivariance in conjunction with invariance, and they validate the effectiveness of EDT through experiments on various datasets, demonstrating significant performance gains under combination shift conditions.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its novel approach to the combination shift problem, providing a clear theoretical foundation through an algebraic framework. The introduction of EDT as a method that incorporates both invariance and equivariance is a significant contribution to the field. However, a potential weakness is the paper's reliance on complex algebraic concepts, which may limit accessibility for practitioners less familiar with these ideas. Additionally, while the experimental results are promising, further exploration of the method's robustness across more diverse datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas with clarity. The methodology is detailed, allowing for reproducibility; however, the algebraic framework might present a barrier to understanding for some readers. The novelty of the approach is notable, particularly in the context of integrating algebraic concepts with machine learning. Overall, the quality of writing and presentation is high, enhancing comprehension of complex topics.\n\n# Summary Of The Review\nThis paper offers a significant contribution to the domain generalization literature by proposing an algebraic framework and the EDT method to tackle the combination shift problem. While the theoretical underpinnings are strong, the complexity of the concepts may hinder broader accessibility. The empirical results indicate promise, but further validation across varied datasets would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses a critical issue in machine learning related to model generalization under distributional shifts. The authors propose a novel framework that enhances model performance by leveraging theoretical insights to adapt to varying data distributions. The methodology includes rigorous theoretical analysis combined with empirical validation, showcasing the effectiveness of the proposed approach on benchmark datasets. The findings suggest that the new method can significantly improve robustness in real-world applications where data contexts change unexpectedly.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance**: The paper tackles a significant and timely problem in machine learning, emphasizing the need for models that can adapt to shifts in data distribution.\n2. **Theoretical Contribution**: The authors provide a solid theoretical foundation that enhances the understanding of model behavior under distributional changes.\n3. **Innovative Approach**: The proposed methodology is unique and offers fresh perspectives on improving model generalization.\n4. **Practical Implications**: The findings have the potential to be applied across various domains, thereby enhancing the robustness of machine learning models in practical scenarios.\n\n**Weaknesses:**\n1. **Clarity**: Certain sections of the paper are convoluted, making it difficult for readers to follow the authors' arguments and conclusions.\n2. **Empirical Evidence**: Although the theoretical groundwork is strong, the empirical results could be more extensive to substantiate all claims effectively. Additional experiments on diverse datasets would be beneficial.\n3. **Comparative Analysis**: The paper lacks a comprehensive comparison with existing methodologies, which would help in illustrating the advantages and limitations of the proposed approach.\n4. **Generalization Concerns**: There is a need for further discussion on the generalizability of the findings across different contexts and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper presents a high-quality theoretical contribution, but the clarity of presentation could be improved significantly. The novelty of the approach is commendable, as it introduces innovative ideas that could reshape current understanding in the field. However, the reproducibility of the results is somewhat hindered by the limited scope of experiments and insufficient details on the experimental setup.\n\n# Summary Of The Review\nThe paper presents a significant contribution to understanding model generalization under distributional shifts, supported by a solid theoretical framework. While the innovative approach holds promise for practical applications, improvements in clarity, empirical validation, and comparative analysis are necessary to enhance the paper's impact and comprehensibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces a novel approach called Equivariant Disentangled Transformation (EDT) to address the challenges associated with domain generalization under combination shifts. The authors critique existing domain generalization methods for their inadequacy in handling unseen combinations of factors. They propose an algebraic formulation of the combination shift problem, focusing on concepts such as homomorphism, equivariance, and disentanglement, which are leveraged in the EDT method to enhance data augmentation strategies. Experimental results demonstrate that this approach significantly improves generalization performance in comparison to traditional invariance-based methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous theoretical foundation, which establishes an algebraic framework for understanding combination shifts and their implications for domain generalization. The introduction of EDT as a practical solution for data augmentation is innovative and timely, given the limitations of existing methods. However, a potential weakness is the lack of extensive empirical validation across diverse datasets and real-world scenarios, which may limit the generalizability of the findings. Additionally, the paper could benefit from a more thorough exploration of the computational complexity associated with the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its ideas, making it accessible to readers with a background in machine learning and algebra. The methodology is articulated with sufficient detail to allow for reproducibility, although some additional clarifications could enhance understanding, particularly regarding the implementation of the EDT method. The novelty of framing the combination shift problem through an algebraic lens is a significant contribution, marking a meaningful advancement in the field.\n\n# Summary Of The Review\nOverall, this paper presents a novel and theoretically sound approach to tackle the combination shift problem in domain generalization through the Equivariant Disentangled Transformation method. While the theoretical contributions are robust, further empirical validation is necessary to fully establish the practical significance of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper addresses the challenges of machine learning performance degradation in scenarios of data distribution change by introducing a novel problem setting termed \"combination shift,\" where not all combinations of domain-label pairs are observed during training. The authors propose a method called Equivariant Disentangled Transformation (EDT), which employs algebraic structures to enhance generalization capabilities. The findings demonstrate that EDT significantly outperforms traditional methods in scenarios of combination shift, highlighting the effectiveness of algebraic regularization in enhancing data augmentation techniques.\n\n# Strength And Weaknesses\nThe paper presents a strong theoretical foundation through its algebraic formulation, which provides a rigorous approach to the combination shift problem. The introduction of EDT and its demonstrated superiority over existing methods is a notable contribution to the field of domain generalization. However, the paper could benefit from a broader range of experiments across different datasets and tasks, which would further validate the proposed method's robustness and generalizability. Additionally, while the mathematical framework is compelling, it may present challenges in understanding for practitioners who are less familiar with algebraic concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear progression from the problem statement to the proposed methodology and experimental validation. The quality of writing is high, although some sections may require additional clarification for readers who are not experts in algebraic structures. The novelty of the approach is significant, as it offers a fresh perspective on domain generalization. Reproducibility is supported through detailed descriptions of the methodology and experimental setup, though providing access to code or datasets would enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of domain generalization by introducing a novel problem setting and a corresponding method that leverages algebraic principles. While the theoretical foundation and experimental results are strong, enhancements in clarity and additional empirical validation could improve the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Equivariant Disentangled Transformation for Domain Generalization Under Combination Shift\" introduces a novel method, Equivariant Disentangled Transformation (EDT), aimed at addressing the challenges posed by combination shift in domain generalization. The authors present an algebraic formulation of the problem, defining key concepts such as homomorphism, equivariance, and disentanglement, which serve as the foundation for their method. Through extensive experiments on datasets like colored MNIST, 3D Shapes, and dSprites, the authors demonstrate that EDT outperforms existing methods, including empirical risk minimization (ERM) and various invariance-based strategies, in handling combination shifts effectively.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear delineation of the problem of combination shift, which is a significant advancement over traditional domain shift in machine learning. The theoretical contributions, particularly the refined definitions and emphasis on algebraic structures, provide a solid foundation for the proposed method. The experimental results are compelling, showing that EDT achieves superior performance metrics compared to baseline approaches. However, a potential weakness is the limited diversity of datasets used in the experiments, which may not fully capture the generalizability of the proposed method across different domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical flow that guides the reader through the problem statement, methodology, and results. The quality of the theoretical explanations is high, making complex concepts accessible. The novelty of the approach is significant, introducing a new perspective on domain generalization. However, while the experiments are thorough, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter settings used.\n\n# Summary Of The Review\nOverall, this paper presents a strong theoretical framework and a novel method for addressing combination shift in domain generalization, supported by robust empirical validation. While the contributions are significant and the clarity is commendable, there are some limitations regarding dataset diversity and reproducibility details that could be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Equivariant Disentangled Transformation (EDT) methodology aimed at addressing the challenges posed by combination shifts in domain generalization. It formalizes the problem using an algebraic framework that incorporates homomorphic representations and principles of equivariance, creating a more nuanced understanding of feature-label relationships. The authors demonstrate that their approach significantly enhances the performance of domain generalization tasks compared to traditional methods, particularly in scenarios characterized by limited observable combinations of domains and labels.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous formulation of the combination shift problem using algebraic structures, which provides a theoretically sound basis for the proposed EDT methodology. The experimental validation effectively showcases the practicality of the approach, yielding substantial improvements in performance metrics over existing techniques. However, the paper could benefit from a more comprehensive discussion of the limitations of the proposed methods and potential scenarios where they may not perform as expected. Additionally, the complexity of the algebraic formalism may pose challenges for practitioners seeking to implement the methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections that logically build on each other; however, the technical nature of the algebraic concepts might hinder understanding for readers not well-versed in advanced mathematics. The quality of the writing is high, with a clear focus on the problem and the proposed solution. In terms of novelty, the introduction of an algebraic framework to tackle the combination shift is a fresh perspective in the domain generalization literature. The reproducibility of the results could be improved by providing more details on the experimental setup and hyperparameters used in the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in domain generalization through the introduction of the Equivariant Disentangled Transformation methodology. While the theoretical foundations and empirical results are compelling, the paper would benefit from clearer explanations of complex concepts and additional details to enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel method called Equivariant Disentangled Transformation (EDT) aimed at improving domain generalization and data augmentation techniques. The authors present an algebraic framework to support their methodology and conduct experiments primarily on colored MNIST and 3D Shapes datasets. While they claim that learning data augmentations leads to enhanced performance, the empirical results show only marginal improvements, and the complexity of the proposed approach raises concerns regarding its practical applicability.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its attempt to formalize the learning of data augmentations through a theoretical framework. However, it heavily relies on existing concepts without introducing significant innovations. The proposed algebraic formulation is criticized for its complexity, which could hinder practical implementation. Additionally, the limited evaluation across diverse datasets raises doubts about the generalizability of the findings. The authors acknowledge limitations but fail to provide convincing evidence of the superiority of their method over simpler, established techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is inconsistent, with some sections lacking clear explanations of key contributions and implications. The quality of the writing may pose challenges for readers, particularly those seeking actionable insights. In terms of novelty, the approach does not significantly advance the field, as it does not introduce truly innovative methodologies. Reproducibility is also a concern, as many claims are based on limited experiments and the lack of comprehensive evaluation suggests that further validation is needed.\n\n# Summary Of The Review\nOverall, the paper attempts to address important aspects of domain generalization but falls short in terms of novelty and practical applicability. The complex theoretical framework and limited empirical validation weaken its contributions to the field. The authors need to provide clearer insights and more robust evaluations to strengthen their claims.\n\n# Correctness\nRating: 3\n\n# Technical Novelty And Significance\nRating: 2\n\n# Empirical Novelty And Significance\nRating: 2",
    "# Summary Of The Paper\nThe paper introduces the Equivariant Disentangled Transformation (EDT), a novel methodology designed to address the combination shift problem prevalent in domain generalization. Through a unique algebraic formulation, the authors effectively link theoretical concepts in algebra to practical machine learning applications, showcasing improved generalization capabilities when learning from limited combinations of factors. Experimental results demonstrate that EDT significantly outperforms traditional invariance-based methods, achieving higher classification accuracy across diverse datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach, which offers a fresh perspective on domain generalization, and the robust experimental validation that highlights the effectiveness of the proposed method. The algebraic foundation provides a solid theoretical grounding that could inspire future research. However, the paper may benefit from a more thorough exploration of potential limitations or specific scenarios where the EDT might underperform compared to existing methods. Additionally, while the applications discussed are promising, more concrete examples or case studies could strengthen the practical implications of the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presented, allowing for easy comprehension of complex concepts. The quality of writing is high, with a logical flow that guides the reader through the methodology and results. The novelty of the approach is significant, introducing a unique intersection of algebra and machine learning that has not been extensively explored. The reproducibility of the results is bolstered by the detailed methodology and the availability of datasets used in experiments, although sharing code and implementation details would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a groundbreaking approach to domain generalization through the Equivariant Disentangled Transformation, demonstrating both theoretical innovation and empirical success. While the contributions are strong and well-articulated, additional exploration of limitations and practical examples would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for addressing domain generalization under combination shifts using algebraic structures, specifically through the concepts of homomorphism and equivariance. The proposed methodology, called Equivariant Disentangled Transformation (EDT), leverages these algebraic principles to enhance model robustness against unseen combinations of domains and labels. The findings suggest that traditional invariance-based approaches are inadequate, and further theoretical exploration of algebraic constructs can significantly improve the generalization capabilities of machine learning systems.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical formulation of the problem, grounded in algebraic concepts, which provides a solid foundation for the proposed EDT methodology. The innovation of redefining disentanglement through product structures adds depth to the discussion and opens avenues for future research. However, the paper may lack empirical validation of the proposed methods, as the theoretical insights do not appear to be fully backed by extensive experimental results, limiting the immediate applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents complex theoretical concepts with a commendable level of clarity. The quality of the writing is high, making advanced topics accessible. The novelty is significant, as the integration of algebraic structures into machine learning frameworks is relatively underexplored. Reproducibility may be a concern due to the theoretical nature of the work, as it does not provide comprehensive experimental results or detailed methodologies that other researchers could readily replicate.\n\n# Summary Of The Review\nOverall, the paper delivers a valuable theoretical contribution to the understanding of domain generalization through an algebraic lens, proposing an innovative methodology with significant implications for future research. However, the lack of empirical validation raises questions about the practicality of the proposed approach in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper addresses the combination shift problem prevalent in domain generalization and introduces the Equivariant Disentangled Transformation (EDT) method, which leverages algebraic structures for data augmentation. The methodology emphasizes maintaining equivariance through the use of monoid structures and provides a framework for learning transformations that respect the underlying algebraic properties. Experimental results demonstrate that EDT significantly outperforms traditional invariance-based methods across several datasets, including colored MNIST, dSprites, and 3D Shapes, highlighting the advantages of algebraic regularization in managing unseen combinations of domains and labels.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel approach to leveraging algebraic structures for data augmentation, which represents a significant advancement in handling the combination shift problem. The methodology is theoretically sound, with a well-defined algorithm and regularization techniques that enhance the learning process. However, the paper lacks clarity in conveying some of the more intricate details of the algebraic formulations, and there is no code repository provided, which hinders reproducibility and makes it challenging for others to validate the results or build upon the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the complexity of the algebraic concepts presented, which may be challenging for readers without a strong background in algebraic structures. The quality of the writing is generally good, but the dense presentation of theoretical content may benefit from additional explanations or illustrations. In terms of novelty, the approach is quite innovative, introducing a fresh perspective on data augmentation in the context of domain generalization. However, the lack of a publicly available code repository significantly impacts the reproducibility of the findings, which is a critical aspect of empirical research.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of domain generalization through the introduction of the EDT method, which effectively addresses the combination shift problem using algebraic structures. While the theoretical foundation and experimental results are compelling, the paper's clarity and reproducibility could be improved to enhance its accessibility and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the combination shift problem in domain generalization by proposing an Equivariant Disentangled Transformation (EDT) method. The authors claim that their approach refines the existing definitions of disentanglement and emphasizes the importance of exploiting equivariance in representation learning. Experimental results demonstrate that EDT outperforms traditional methods in specific scenarios. However, the paper appears to draw heavily from existing literature and critiques, raising questions about the originality of its contributions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle a relevant problem in domain generalization and its experimental validation showing improved performance of the proposed method over traditional approaches. However, significant weaknesses include a lack of substantial novelty, as many of its claims echo previous works. The paper reiterates known limitations of existing methods without presenting groundbreaking insights or a clear distinction from earlier research. Additionally, its reliance on a specific algebraic framework may limit broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, allowing readers to follow the authors' arguments and methodology. However, the novelty of the contributions is questionable, as many ideas presented seem to be reiterations of established concepts in the field. The reproducibility of results is not extensively discussed, which could be a concern for researchers looking to build upon this work.\n\n# Summary Of The Review\nOverall, the paper presents a method aimed at addressing the combination shift problem but does so in a manner that lacks significant novelty and fails to provide substantial advancements over existing literature. While the experimental results are promising, the contributions may not sufficiently differentiate from prior works to warrant a strong impact on the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach called \"Equivariant Disentangled Transformation\" (EDT) aimed at improving domain generalization under combination shifts. The authors propose a methodology that leverages equivariant transformations to disentangle relevant features from the data, facilitating better generalization across varying domains. Through extensive experiments, the findings indicate that the EDT significantly outperforms existing methods in terms of accuracy and robustness on benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing domain generalization, which is a critical area in machine learning. The use of equivariant transformations to disentangle features is a promising direction that could inspire further research. However, the paper exhibits several weaknesses, including inconsistent formatting, typographical errors, and a lack of clarity in some mathematical notations, which may hinder comprehension among readers. Additionally, the experimental setup could benefit from more detailed explanations regarding the selection of datasets and hyperparameters.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the core ideas presented in the paper are novel and hold significant potential, the clarity of presentation is undermined by formatting inconsistencies and typographical errors throughout the text. The methodology is explained reasonably well, but certain mathematical notations could be better defined for a broader audience. Reproducibility is somewhat compromised due to the lack of detailed descriptions of experimental setups, such as the specific configurations used for training and validation.\n\n# Summary Of The Review\nOverall, the paper introduces a promising new method for domain generalization using equivariant transformations. However, the clarity of presentation and attention to formatting details need improvement to enhance reader comprehension and reproducibility of results.\n\n# Correctness\n4/5 - The methodology and findings appear to be sound, but certain mathematical expressions have formatting inconsistencies that could lead to misinterpretation.\n\n# Technical Novelty And Significance\n4/5 - The proposed approach of using equivariant transformations for disentangling features is innovative and contributes to the ongoing research in domain generalization.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results demonstrate improved performance, the novelty is somewhat diminished by the reliance on standard benchmark datasets. More extensive testing on diverse datasets could strengthen the claims.",
    "# Summary Of The Paper\nThe paper addresses the combination shift problem in domain generalization, proposing an algebraic formulation that aims to capture the dynamics of data relationships through the lens of monoid actions. The authors conduct experiments on synthetic datasets to evaluate their method, presenting initial findings that demonstrate the potential effectiveness of their approach. However, the study does not explore related shifts such as covariate and label shifts, which could enrich the context of the problem.\n\n# Strength And Weaknesses\nThe paper presents a novel algebraic framework that provides a fresh perspective on the combination shift problem, which is commendable. However, the narrow focus on monoid actions limits the applicability of the findings, and a broader exploration of algebraic structures could yield deeper insights. The experiments conducted on synthetic datasets lack validation in real-world scenarios, which diminishes the generalizability of the results. Furthermore, the evaluation metrics used are limited, and the absence of a thorough discussion on the paper's limitations leaves significant gaps in understanding the robustness of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is satisfactory, although some concepts could benefit from further elaboration, particularly the implications of cycle consistency and the integration of transfer learning techniques. The quality of the methodology is reasonable, but the novelty, while present, could have been enhanced by considering diverse regularization techniques and a more comprehensive array of performance metrics. The reproducibility of the results is uncertain due to the limited experimental setup and the reliance on synthetic datasets.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to understanding the combination shift problem through an algebraic lens, but it falls short in exploring related shifts and providing robust empirical validation. Future work should address these limitations and consider a broader range of methodologies and datasets to strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach for domain generalization under combination shift, termed \"equivariant disentangled transformation\" (EDT). The authors emphasize the statistical significance of their method by employing an algebraic framework to analyze performance against the baseline of empirical risk minimization (ERM). Through rigorous statistical testing, including one-tailed t-tests, the authors demonstrate that EDT significantly outperforms existing methods in various experimental setups using a modified colored MNIST dataset.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its clear formulation of the problem, distinguishing between domain shift and combination shift, and its robust statistical methodology that reinforces the validity of the findings. The use of statistical testing to substantiate claims is commendable, as it addresses the limitations of previous works in the field. However, a potential weakness is the reliance on a specific dataset (colored MNIST), which may limit the generalizability of the results to other domains or datasets. Additionally, while the statistical analysis is thorough, the paper could benefit from a more expansive discussion on the implications of the findings for future research directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it easy to follow the authors' arguments and methodology. The quality of the analysis is high, with a clear explanation of the statistical methods used. The novelty of the proposed EDT method is significant, as it introduces a new perspective on handling combination shifts in domain generalization. Reproducibility is supported by the detailed description of the experimental setup and the statistical metrics employed, although providing access to code or datasets would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of domain generalization by introducing a statistically rigorous approach to handling combination shifts. The findings are well-supported by empirical evidence, although the scope of the experiments could be broadened to enhance generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper formulates data augmentations using endofunctions of the form \\( \\alpha: X \\rightarrow X \\), primarily focusing on algebraic principles. It discusses the limitations of existing augmentation methods, particularly in their applicability and effectiveness in real-world scenarios. The findings suggest that while the algebraic framework provides a theoretical basis for data augmentation, there is a significant gap in bridging theory with practical applications. The paper also proposes future directions for research, including the exploration of statistical learning aspects and multi-sorted algebra.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel approach to data augmentation through algebraic formulations and its identification of limitations in current methods. However, the weaknesses are notable: the focus on stateless data augmentations may not capture the complexities inherent in real-world data, and the exploration of combining components from multiple instances is insufficient. Additionally, while the theoretical framework is promising, its practical implications remain underexplored, and the lack of comprehensive empirical validation raises concerns about the robustness and applicability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation of ideas, although the theoretical focus may create a disconnect for practitioners. The novelty lies in the algebraic approach to data augmentation, yet the quality is undermined by limited exploration of its empirical validation. The reproducibility of results may be challenging due to the lack of detailed methodologies and comparisons with existing techniques, which are essential for establishing the proposed method's efficacy.\n\n# Summary Of The Review\nOverall, the paper presents a novel theoretical framework for data augmentation rooted in algebraic concepts, but it falls short in practical applicability and empirical validation. Future work should address the identified limitations to enhance the robustness and relevance of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Equivariant Disentangled Transformation for Domain Generalization under Combination Shift\" attempts to address the well-known issue of data distribution shifts in machine learning, introducing a methodology they term Equivariant Disentangled Transformation (EDT). The authors assert that their approach provides a fresh perspective on domain generalization, particularly in scenarios characterized by combination shifts, which they define and differentiate from more traditional domain shifts. Their experiments purportedly demonstrate that EDT outperforms static methods, although these findings are largely predictable given the existing literature on data augmentation.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its structured approach to a problem that many in the field are familiar with, potentially providing a framework for further exploration. However, the weaknesses are pronounced: the paper relies heavily on established concepts without offering substantial novelty. The claims of new theoretical insights seem overstated, as they largely reiterate existing knowledge in machine learning practice. The experimental results, while confirming the effectiveness of augmentation, lack the groundbreaking impact that the authors suggest.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is written in a technically proficient manner, it suffers from a lack of clarity regarding its contributions, as many concepts presented are not as novel as claimed. The quality of the methodology is undermined by an overcomplicated algebraic formulation that does not add significant value to the discourse on domain generalization. In terms of reproducibility, the experimental setup and results are adequately described, but the lack of theoretical novelty may discourage further exploration by the community.\n\n# Summary Of The Review\nOverall, this paper revisits a well-established problem in machine learning with a convoluted approach that does not yield new insights or significant advancements. The authors have a reasonable grasp on the fundamentals, but their attempt to introduce complexity detracts from practical relevance and clarity.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to domain generalization through the lens of equivariance, introducing the equivariant disentangled transformation (EDT) method. It emphasizes the limitations of traditional invariance-based methods and proposes that leveraging equivariance can lead to improved model robustness against combination shifts in unseen data. The findings suggest that learning data augmentations grounded in algebraic structures enhances generalization capabilities, with potential implications for structured learning and data augmentation strategies.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to integrating algebraic principles with machine learning, specifically in the context of domain generalization. The critique of existing methods and the proposal of EDT is compelling, as it opens avenues for further research into hybrid models that encompass both invariance and equivariance. However, the paper could benefit from exploring more diverse datasets beyond MNIST and dSprites to validate the robustness of the proposed method. Additionally, the potential for incorporating probabilistic models into the framework remains unexplored and could enhance the methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and articulates its contributions clearly, with a solid methodology that is easy to follow. The novelty of the approach is significant, particularly in its exploration of algebraic structures for data augmentation. However, reproducibility may be a concern due to the limited experimental validation on diverse datasets, which could hinder the generalizability of the proposed method across various applications.\n\n# Summary Of The Review\nOverall, the paper offers a fresh perspective on domain generalization by harnessing equivariance principles and introducing the EDT method. While the contributions are notable and the methodology sound, further exploration of hybrid approaches and validation on a wider range of datasets would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Equivariant Disentangled Transformation (EDT) method, specifically designed to address the combination shift problem, where training data and labels are only partially observed. It presents extensive benchmark results across multiple datasets, including Colored MNIST, dSprites, and 3D Shapes, demonstrating that EDT consistently outperforms traditional empirical risk minimization (ERM) methods and other invariance-based techniques (e.g., IRM, CORAL, DANN). The findings indicate significant improvements in accuracy and reductions in misclassification rates, particularly in complex domain generalization scenarios, supported by comprehensive ablation studies that assess the impact of various regularization techniques.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough empirical evaluation and the establishment of EDT as a superior method for handling combination shifts in domain generalization tasks. The reported results are statistically significant and demonstrate clear performance gains over existing approaches. However, a potential weakness is the absence of a detailed theoretical framework or justification for why EDT outperforms other methods, which may limit the understanding of its underlying principles. Additionally, while the experiments are extensive, the paper could benefit from a more diverse range of datasets to further validate the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The clarity of the writing and the logical flow of the experiments enhance the reader's understanding. The quality of the experimental design is high, with appropriate comparisons and statistical analyses. The novelty of the EDT method is notable, as it presents a fresh approach to a well-established problem in domain generalization. However, reproducibility may be a concern if the implementation details or hyperparameter settings are not adequately described, which could hinder other researchers from replicating the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and significant advancement in the field of domain generalization through the introduction of the EDT method. The empirical results robustly support its effectiveness, although a more theoretical exploration of the method's fundamentals could enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing the challenges of domain generalization in machine learning, specifically focusing on the \"combination shift\" phenomenon. The authors propose a new methodology that leverages a combination of data augmentation techniques and a unique learning framework to enhance model robustness across varying domains. Through extensive empirical evaluations, the findings indicate significant improvements in performance on benchmark datasets, demonstrating that the proposed method outperforms existing state-of-the-art approaches.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to a well-known problem and the thorough empirical evaluation that supports its claims. The methodology is well-structured, and the results are compelling, showcasing a clear advantage over prior techniques. However, the paper suffers from some weaknesses, including a lack of clarity in the presentation, particularly in the introduction and conclusion sections. Additionally, the excessive use of jargon and inconsistent terminology may hinder accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper demonstrates a high level of technical quality and presents novel contributions, its clarity could be significantly improved. The abstract is dense, and the introduction lacks clear subheadings, which can make it challenging for readers to navigate the argument. The methodology is reproducible, as the authors provide sufficient details on their experimental setup; however, some mathematical notations and definitions could benefit from clearer explanations to enhance understanding.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of domain generalization by proposing a novel methodology that shows promising results. However, improvements in clarity and presentation are necessary to make the work more accessible and impactful.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.7724050932380533,
    -1.5344204337059058,
    -1.8258924981310485,
    -1.6160381871327703,
    -1.791139472283009,
    -1.7583492629404598,
    -1.5792447492922717,
    -1.8308014649040238,
    -1.8781916105280778,
    -1.8553393082176823,
    -1.5561717470473941,
    -1.4668492021454507,
    -1.5909227949731692,
    -1.5622770278840963,
    -1.4889766870725922,
    -1.6678064431760666,
    -1.833075193281992,
    -1.6962252527310302,
    -1.7723639861042477,
    -1.5622591999567996,
    -1.6251416968238912,
    -1.5832282277782836,
    -1.7876211753763604,
    -1.5552037521193516,
    -1.9409875793566669,
    -2.0176879525683926,
    -1.8665488549983786,
    -1.6171774312447456,
    -1.6202476993137809
  ],
  "logp_cond": [
    [
      0.0,
      -2.5675813861095342,
      -2.562711119761884,
      -2.562609876568843,
      -2.557680198263144,
      -2.58413306136046,
      -2.6129129350569302,
      -2.5893031877006525,
      -2.551861965476954,
      -2.5567678919728984,
      -2.5853370131943603,
      -2.5989911180290823,
      -2.5771563322469193,
      -2.5771797253907636,
      -2.595504002660976,
      -2.5811315426261436,
      -2.5820328283115606,
      -2.5784170313866097,
      -2.5707106341274,
      -2.5597074265366406,
      -2.5670524782113877,
      -2.604418421790806,
      -2.58448117762072,
      -2.5962385922461477,
      -2.6013083040131084,
      -2.590638974101113,
      -2.5614239318735037,
      -2.5989247288449637,
      -2.592513890126424
    ],
    [
      -1.1326325308612868,
      0.0,
      -1.0277128432261722,
      -0.9531755867982956,
      -1.1056118351597266,
      -1.0893023955692975,
      -1.209936948993438,
      -1.1418454605980661,
      -1.0497298523182939,
      -1.1024387973280059,
      -1.1247323756872893,
      -1.2290968667783726,
      -1.072015093864772,
      -1.0459651374934908,
      -1.0918035057076552,
      -1.053241293536041,
      -1.153351646826517,
      -1.050727437215607,
      -1.062472715843431,
      -1.0467360320122745,
      -1.116074935612841,
      -1.1776863503714443,
      -1.1431788509358798,
      -1.0904556267592158,
      -1.191582817006022,
      -1.1367251417045945,
      -1.0595691149037898,
      -1.1105772534506542,
      -1.2031027783309252
    ],
    [
      -1.4806887315504749,
      -1.3800980863937107,
      0.0,
      -1.3490533963720246,
      -1.4021552771808972,
      -1.3926114734205997,
      -1.5043343615321245,
      -1.4430776176224724,
      -1.3528307355345384,
      -1.4290544212544103,
      -1.432034469850001,
      -1.5176974305185038,
      -1.4082592231516018,
      -1.3572498236745023,
      -1.3826274492089698,
      -1.4083990531505022,
      -1.4169860975285118,
      -1.4153756692803503,
      -1.41933089748299,
      -1.34929071689333,
      -1.4635256263821623,
      -1.4972455501625248,
      -1.4221184144962442,
      -1.422482862564377,
      -1.4954940938741579,
      -1.4306220138271353,
      -1.407309128749861,
      -1.390731317618599,
      -1.4979381280814592
    ],
    [
      -1.2621370482852463,
      -1.1189990100921043,
      -1.1380778317687836,
      0.0,
      -1.1935126851119524,
      -1.207805902016271,
      -1.2798585373550662,
      -1.259377580715998,
      -1.1312638964561468,
      -1.1937216814705416,
      -1.1984968655555233,
      -1.3159995519496779,
      -1.1319919412527832,
      -1.1433659093483604,
      -1.1597658644287974,
      -1.1793819074041034,
      -1.2640215786840512,
      -1.1873707654282954,
      -1.1950564227905751,
      -1.1654088000201503,
      -1.2678853326978912,
      -1.2743643731720953,
      -1.2632224468793884,
      -1.2205233758544156,
      -1.2965030566932323,
      -1.2411763478310267,
      -1.169575740217617,
      -1.2069583744099708,
      -1.245094508561575
    ],
    [
      -1.4175236036389,
      -1.2927330413653237,
      -1.2812140505443557,
      -1.2597588280127698,
      0.0,
      -1.363553100997106,
      -1.4114558897032068,
      -1.2964533725722047,
      -1.2791353417293592,
      -1.3260302057419018,
      -1.3004868569069314,
      -1.4782116143475066,
      -1.2958532138034102,
      -1.3047067882110284,
      -1.3271292872315126,
      -1.3399855455709908,
      -1.377143943385545,
      -1.3120591050731165,
      -1.3188885596504394,
      -1.2902095720229894,
      -1.3560595664406447,
      -1.397590788226972,
      -1.346725872797892,
      -1.3561918791840777,
      -1.4165139838218834,
      -1.349718866925482,
      -1.2580758581501577,
      -1.3323025931507746,
      -1.3990354644090504
    ],
    [
      -1.4275596025104922,
      -1.3413190300773048,
      -1.3225274354951884,
      -1.3364098603544043,
      -1.3781394957808581,
      0.0,
      -1.4372576316870262,
      -1.368543398274064,
      -1.3590077481089191,
      -1.3600067561935545,
      -1.3747039404327457,
      -1.4606664945874188,
      -1.3559590695063866,
      -1.3750289232676565,
      -1.322111878323731,
      -1.337099747654139,
      -1.4422923419957507,
      -1.349944511810176,
      -1.3591953131938352,
      -1.3069173263086407,
      -1.3773814586786242,
      -1.4443258080129675,
      -1.4163166613855271,
      -1.391388786523683,
      -1.4469391127252762,
      -1.4120060526749472,
      -1.3660951121928977,
      -1.3441351419490866,
      -1.4078761119999235
    ],
    [
      -1.3130732715232887,
      -1.2594401568142126,
      -1.205066642002713,
      -1.2149353097088347,
      -1.2557261581176142,
      -1.2439346690295354,
      0.0,
      -1.2367453702883415,
      -1.2157535669755586,
      -1.2825168037291632,
      -1.2323452900752683,
      -1.2802481392363423,
      -1.221241525703962,
      -1.230598403887297,
      -1.2049656507176634,
      -1.2247805812257844,
      -1.2568125081507406,
      -1.2228905447945844,
      -1.2512947265129375,
      -1.2522317860893575,
      -1.2602221132799476,
      -1.2850520642444911,
      -1.2712391341912843,
      -1.237938888055822,
      -1.2816074715264496,
      -1.2172684340984234,
      -1.2475878875608728,
      -1.2417451036872913,
      -1.2990200203687292
    ],
    [
      -1.5080188516994566,
      -1.3882072996383408,
      -1.4206827483201723,
      -1.4126929779287742,
      -1.4245551115761141,
      -1.455280456030253,
      -1.4952019560129477,
      0.0,
      -1.39329426149106,
      -1.446764335086488,
      -1.433176498644648,
      -1.5388232128700265,
      -1.4096755872281004,
      -1.4628096398780082,
      -1.4499798270556854,
      -1.4437587895048496,
      -1.47322811155061,
      -1.4311614921575284,
      -1.3816090760894755,
      -1.4112603333209686,
      -1.407641977772876,
      -1.4073560277191404,
      -1.4774627096364992,
      -1.458551492942369,
      -1.5299608837937548,
      -1.383648848633307,
      -1.4226509527012283,
      -1.4325350030327646,
      -1.4872671321785025
    ],
    [
      -1.5103403118775647,
      -1.36717507179338,
      -1.360666493617115,
      -1.345589953841766,
      -1.4087373356849189,
      -1.4271083500684991,
      -1.5241736406971393,
      -1.4587864576392702,
      0.0,
      -1.4506356232768958,
      -1.4378345660569427,
      -1.5446469901761255,
      -1.3996348005264396,
      -1.3568047682845474,
      -1.4023659909931423,
      -1.3967561986838064,
      -1.4805414262272483,
      -1.3845974240171408,
      -1.4159372611271026,
      -1.363380843130514,
      -1.4778030820416135,
      -1.5310363706065833,
      -1.4660231337027794,
      -1.479632291783057,
      -1.5422778542116131,
      -1.4397172056931222,
      -1.417899209334584,
      -1.3954518601259231,
      -1.5162192286664467
    ],
    [
      -1.5095364835395073,
      -1.4858437996725329,
      -1.496091207486271,
      -1.4481558930573863,
      -1.4738844614048852,
      -1.5078771659896941,
      -1.594830165988638,
      -1.482585258810397,
      -1.4590856489114066,
      0.0,
      -1.524734031652282,
      -1.5787714803927706,
      -1.5037319807083398,
      -1.529807662853809,
      -1.488024013232224,
      -1.523867683750321,
      -1.5550020465683658,
      -1.526318494030646,
      -1.4416370812715256,
      -1.4818462500247591,
      -1.475391940302114,
      -1.501841447524301,
      -1.4820492803578875,
      -1.5446899164386407,
      -1.5564544302844492,
      -1.5678526197945721,
      -1.4965188114944137,
      -1.508758771950391,
      -1.5436995358736192
    ],
    [
      -1.20682225665569,
      -1.1053303949682722,
      -1.096871771670905,
      -1.0871697684795678,
      -1.08445950639918,
      -1.1252584091128846,
      -1.2469708620174071,
      -1.146204992597261,
      -1.1073230658791782,
      -1.1638981795555456,
      0.0,
      -1.266546957467437,
      -1.0729605546099448,
      -1.0559236798456662,
      -1.1115248554176136,
      -1.0477891694365573,
      -1.1998318704395698,
      -1.102684660046176,
      -1.1206692263513962,
      -1.1272095392318564,
      -1.144399814697931,
      -1.2258017095615874,
      -1.155887770247339,
      -1.1280621599899472,
      -1.2544834286331747,
      -1.1670216805763518,
      -1.0797498585252863,
      -1.1510964922438502,
      -1.1885096014773486
    ],
    [
      -1.2341437096921746,
      -1.2421255414626737,
      -1.2382478996971833,
      -1.2342613997307204,
      -1.2352133975134045,
      -1.224493602205556,
      -1.2331954831099832,
      -1.234124432957506,
      -1.2247278745854129,
      -1.2375884509172306,
      -1.2553664635832933,
      0.0,
      -1.2722430477017093,
      -1.2520267733280255,
      -1.2494086146811525,
      -1.257776147030714,
      -1.215441405561138,
      -1.2481097169047062,
      -1.2382826295489016,
      -1.2380460906480408,
      -1.2595126228562619,
      -1.2259301698669776,
      -1.2220282205887525,
      -1.2335546118973488,
      -1.2503459441136773,
      -1.2132566007809853,
      -1.2263753022017905,
      -1.2337546724274429,
      -1.2123711989529546
    ],
    [
      -1.188680520234041,
      -1.093243128372588,
      -1.0857972415356927,
      -1.0615802697436851,
      -1.0977964425893405,
      -1.1404469248750784,
      -1.2125797322122402,
      -1.1734753240316542,
      -1.1221820197948338,
      -1.1649960193049966,
      -1.0903189370333326,
      -1.2444293224274714,
      0.0,
      -1.121869840697898,
      -1.0998111735117029,
      -1.115799087244326,
      -1.1921358485039508,
      -1.1170886524248305,
      -1.1217869347068632,
      -1.1380006011262582,
      -1.1941508586460512,
      -1.2394372626092751,
      -1.1850018062476617,
      -1.1717681624550904,
      -1.235497008802087,
      -1.1600714484682992,
      -1.0825000962517342,
      -1.1693355507755738,
      -1.2477891451884564
    ],
    [
      -1.177616285131749,
      -1.0605160108070704,
      -1.010621217700617,
      -1.0128608941039683,
      -1.108519398769046,
      -1.1242993631126539,
      -1.2304712672881464,
      -1.1297146794848911,
      -1.0879255430112662,
      -1.1673434763752422,
      -1.122491906502213,
      -1.2246524795883975,
      -1.0985007905069117,
      0.0,
      -1.1327817953999288,
      -1.0794777466777594,
      -1.1462451043604174,
      -1.0969113878448122,
      -1.130305797297656,
      -1.0798201850613116,
      -1.1670051453580386,
      -1.182860410297171,
      -1.1563574832550367,
      -1.1171178224534462,
      -1.2314455385115468,
      -1.1128728702941264,
      -1.109699288361903,
      -1.1452558318111128,
      -1.1994242182630457
    ],
    [
      -1.211485847164803,
      -1.078570502747053,
      -1.0780819586870896,
      -1.0652510226620675,
      -1.096996651111667,
      -1.097966307970027,
      -1.1442477177887647,
      -1.1143158207692363,
      -1.072452390990768,
      -1.1489973529544917,
      -1.108520748243808,
      -1.1804770495895938,
      -1.0678959238284094,
      -1.05955887237383,
      0.0,
      -1.0900066875744394,
      -1.1199362869004899,
      -1.060356796429597,
      -1.1279722741069296,
      -1.085775253549288,
      -1.113316222332019,
      -1.1976841738899737,
      -1.1702109661027582,
      -1.069800640846977,
      -1.185763791564469,
      -1.0812497488668857,
      -1.1056234993769114,
      -1.0498293793398197,
      -1.1735324291267604
    ],
    [
      -1.3115416061954375,
      -1.177695377303643,
      -1.218433150056859,
      -1.1935738194976564,
      -1.2431337567755039,
      -1.226924085339195,
      -1.2791796624236103,
      -1.2582127146748654,
      -1.1960219448703486,
      -1.238175882607956,
      -1.246733083062599,
      -1.314830217937686,
      -1.253224778660379,
      -1.1624983344879984,
      -1.2157146573108277,
      0.0,
      -1.2992992579848577,
      -1.22132325460429,
      -1.2462755733038535,
      -1.164962791244465,
      -1.2903462265178482,
      -1.316262860180918,
      -1.2909371225357622,
      -1.2955661361266484,
      -1.356071013978337,
      -1.223290471370552,
      -1.2827303485806463,
      -1.2506531330284572,
      -1.30113009148214
    ],
    [
      -1.5427030085005393,
      -1.4759231171298086,
      -1.4463376968346462,
      -1.4994178869768375,
      -1.4528481504679456,
      -1.4811190146918036,
      -1.4911105790477666,
      -1.4909772758078388,
      -1.468565334272824,
      -1.4970319242749521,
      -1.4996431098612073,
      -1.5330155469700095,
      -1.4838747378496804,
      -1.5022567642330162,
      -1.4438617023230156,
      -1.450790706802497,
      0.0,
      -1.4768811435047766,
      -1.4874742974740993,
      -1.4941251142596668,
      -1.4665174913598054,
      -1.5106000120451177,
      -1.4903859990479154,
      -1.490104545556893,
      -1.5157025097091037,
      -1.439543055879791,
      -1.4494458576367202,
      -1.4950171980611682,
      -1.509331669974954
    ],
    [
      -1.3321296588081573,
      -1.2252672565958052,
      -1.2712695439287145,
      -1.2163166691913379,
      -1.260099002620246,
      -1.2689947852956687,
      -1.3668834873878999,
      -1.281509849722991,
      -1.2604085379427463,
      -1.3241085281324871,
      -1.2500342960034687,
      -1.3652290432705163,
      -1.2102530743560733,
      -1.2504216449249672,
      -1.2347029819546151,
      -1.2675889989617874,
      -1.3401173639207344,
      0.0,
      -1.2668520693262229,
      -1.229773221494405,
      -1.2845760837158957,
      -1.3413240433251201,
      -1.2958925901101608,
      -1.285398966427147,
      -1.3892892520204738,
      -1.2648249266641785,
      -1.2298712830045018,
      -1.2607500944189012,
      -1.3292410446429903
    ],
    [
      -1.4614647865576629,
      -1.3575909186728505,
      -1.3479189493081862,
      -1.3385101622708917,
      -1.4140368898425733,
      -1.3932967564652274,
      -1.4498579988753173,
      -1.334887990703085,
      -1.3581729984108422,
      -1.353835763074775,
      -1.4177620484371618,
      -1.4876155470952488,
      -1.380043978027777,
      -1.4053135455136454,
      -1.358050570558276,
      -1.355824152857418,
      -1.4491998698172117,
      -1.3744927773312614,
      0.0,
      -1.365804918564431,
      -1.411719616425084,
      -1.3973328315135847,
      -1.4316218600644282,
      -1.407455737880523,
      -1.4207793253917493,
      -1.4275523250226867,
      -1.3323388755859087,
      -1.3580394774923665,
      -1.457300327855517
    ],
    [
      -1.198600906268907,
      -1.1002026946750636,
      -1.0514372844707724,
      -1.0877319693257066,
      -1.122205931881762,
      -1.1297772418424163,
      -1.2447169602694823,
      -1.198158906186525,
      -1.0744017186804957,
      -1.162105365468715,
      -1.1600417285323732,
      -1.258225123844323,
      -1.1427550994511755,
      -1.0989203824377343,
      -1.1144817713600654,
      -1.1162393752888335,
      -1.2010935296142413,
      -1.1152221128267623,
      -1.1324751343316581,
      0.0,
      -1.191604173056634,
      -1.2278824857163648,
      -1.2034372797742086,
      -1.1819078266526002,
      -1.2413854454334505,
      -1.2056610246194757,
      -1.1368457836150436,
      -1.1046525061208878,
      -1.225428444767304
    ],
    [
      -1.2625383898341758,
      -1.1724486703650425,
      -1.2151827152371064,
      -1.1797482577834209,
      -1.2009621035811726,
      -1.271289422962551,
      -1.3290770595958759,
      -1.1939130547631849,
      -1.1824532147567168,
      -1.2108498294292251,
      -1.1796241697271779,
      -1.3620313843631717,
      -1.1729969720615647,
      -1.2088489303668737,
      -1.2099733210926968,
      -1.2061456876292236,
      -1.2809392296583648,
      -1.1916897380878415,
      -1.202381594421561,
      -1.2349823784019136,
      0.0,
      -1.2501769774413811,
      -1.26284820592567,
      -1.2402287629933688,
      -1.335626729262199,
      -1.2264231887963941,
      -1.2234175592158163,
      -1.2275038435424315,
      -1.3358493765483779
    ],
    [
      -1.2828496609004023,
      -1.2483021781307464,
      -1.2244595911088754,
      -1.2430357637476241,
      -1.230971752717166,
      -1.234626809538303,
      -1.290356320026406,
      -1.2033481574641354,
      -1.2417902128678198,
      -1.2520367411951139,
      -1.2474975953313077,
      -1.2746428966113432,
      -1.2598312378886094,
      -1.2687909633482273,
      -1.2226171176884308,
      -1.2485666252468834,
      -1.2500302074019618,
      -1.2430256825811357,
      -1.2333639528248106,
      -1.245717809101063,
      -1.2048872215211528,
      0.0,
      -1.2833624271379205,
      -1.2230383181474525,
      -1.3241408643692336,
      -1.1711435001794008,
      -1.2336445420194624,
      -1.2248508960152393,
      -1.203632254751417
    ],
    [
      -1.435753454200744,
      -1.3935851221057336,
      -1.388591468572318,
      -1.3858673674497364,
      -1.3815284697601073,
      -1.424235957328737,
      -1.5006321137335636,
      -1.4052173405692918,
      -1.3853663429161769,
      -1.3942254004441739,
      -1.4283592670756062,
      -1.485942649838317,
      -1.3896051463215187,
      -1.3969210351023273,
      -1.4002820726735243,
      -1.4116195487989454,
      -1.458451937703539,
      -1.4096240520803238,
      -1.4264197719396232,
      -1.4075023393094368,
      -1.4469129174386233,
      -1.5234765549763867,
      0.0,
      -1.4353659727093497,
      -1.4589230152139914,
      -1.4261194002048152,
      -1.4057745796614396,
      -1.4458782398178542,
      -1.4493993214404264
    ],
    [
      -1.2492050769055336,
      -1.1251830111682397,
      -1.0942145557014122,
      -1.0739217377337305,
      -1.1424070368762416,
      -1.1648175840460673,
      -1.1945959941416977,
      -1.1640297916599736,
      -1.1454055969422685,
      -1.217112945513788,
      -1.1578399906549646,
      -1.2357968442745773,
      -1.139055467157608,
      -1.089813808352828,
      -1.041764414371807,
      -1.1323275319664996,
      -1.1602910048395083,
      -1.1273156119074668,
      -1.1700404456578604,
      -1.1460182565899737,
      -1.1729055289577666,
      -1.1729977428246225,
      -1.174815065788828,
      0.0,
      -1.2286393607312245,
      -1.109207991728569,
      -1.138856593658786,
      -1.1193682079751512,
      -1.1875222006373076
    ],
    [
      -1.6423881540541685,
      -1.6156616616425779,
      -1.5447348995899455,
      -1.5680579486135262,
      -1.5502422764369121,
      -1.6040927433833065,
      -1.5834665353570623,
      -1.6302812184727764,
      -1.5460552467564594,
      -1.584832585353669,
      -1.5946545415146764,
      -1.6135584561549214,
      -1.5870855171384908,
      -1.6113061330055374,
      -1.6067796730766832,
      -1.6119129026683163,
      -1.5782504423112533,
      -1.5928224357158247,
      -1.6333317752141054,
      -1.610400111584879,
      -1.6331555248284888,
      -1.6355542083940624,
      -1.5680804775019388,
      -1.6480326472945346,
      0.0,
      -1.573818208081113,
      -1.533686918824472,
      -1.6054802047724923,
      -1.613306625208518
    ],
    [
      -1.649796208929823,
      -1.5417926880763226,
      -1.505843984837125,
      -1.530933052363103,
      -1.5510347733338155,
      -1.6032710580447547,
      -1.6151546120964053,
      -1.5895103029879531,
      -1.5375858667117017,
      -1.6523943638068868,
      -1.5669894785589245,
      -1.6923884064961123,
      -1.5293788226392593,
      -1.5408133846164997,
      -1.5196674871752855,
      -1.546003636133775,
      -1.5870665526504437,
      -1.539090785658335,
      -1.6248723326724912,
      -1.5667383125928422,
      -1.6063115589647012,
      -1.648353592074262,
      -1.6662879963014499,
      -1.552708444191035,
      -1.6612268169121276,
      0.0,
      -1.581360036293663,
      -1.5831735475625206,
      -1.6338049200587537
    ],
    [
      -1.4720632888065133,
      -1.4286499629396745,
      -1.3937758669545501,
      -1.3901522274149736,
      -1.3893455194810933,
      -1.409334368919219,
      -1.4834283795789318,
      -1.462781172540821,
      -1.386394556766765,
      -1.4744371647920524,
      -1.4098292991487091,
      -1.5276109938865003,
      -1.4052509614156032,
      -1.4748862038158588,
      -1.4248027537280619,
      -1.466618615665836,
      -1.4674461559921532,
      -1.4271912209775992,
      -1.4136912038899025,
      -1.4000623567057324,
      -1.4774533519315867,
      -1.5716306428275864,
      -1.420357428903885,
      -1.4851503582660028,
      -1.421924260150824,
      -1.4691632315160965,
      0.0,
      -1.4084245678449099,
      -1.5041303311706387
    ],
    [
      -1.3336622183752065,
      -1.2148895843289083,
      -1.142555516187601,
      -1.1601246840610582,
      -1.180248629486613,
      -1.227938059200444,
      -1.3143286294511982,
      -1.2394430757724904,
      -1.1419679003199066,
      -1.2830098667213583,
      -1.2271282699488946,
      -1.3471961446836744,
      -1.2026445369852514,
      -1.2327480870005987,
      -1.1799372428399415,
      -1.1953290312483946,
      -1.2952190432566961,
      -1.2174669593381102,
      -1.1893524172339747,
      -1.1763563251202476,
      -1.2349742448151477,
      -1.2851186153456668,
      -1.280334984023174,
      -1.2050861674585278,
      -1.3131757695707065,
      -1.2168691441213004,
      -1.1889053341522524,
      0.0,
      -1.2731649222982964
    ],
    [
      -1.2424206262006319,
      -1.2002831286024402,
      -1.1957554528790901,
      -1.1701884257437658,
      -1.1714495064304553,
      -1.1655866649722375,
      -1.2005343053716748,
      -1.1811909202471678,
      -1.1631262055123044,
      -1.1978739872825734,
      -1.1911316586516443,
      -1.2247442326078948,
      -1.1993242587096233,
      -1.19572971278385,
      -1.1984354464080749,
      -1.1637585990127344,
      -1.226106532315732,
      -1.1920826251221281,
      -1.2101980739086877,
      -1.1904373281798626,
      -1.1851885999187535,
      -1.1412468285985145,
      -1.2034267969497259,
      -1.1763226377627427,
      -1.2283085321588332,
      -1.1316566402281096,
      -1.1930761484505144,
      -1.161242508469133,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.20482370712851905,
      0.20969397347616914,
      0.2097952166692103,
      0.2147248949749092,
      0.18827203187759345,
      0.15949215818112306,
      0.1831019055374008,
      0.2205431277610992,
      0.21563720126515484,
      0.18706808004369302,
      0.17341397520897095,
      0.19524876099113397,
      0.19522536784728972,
      0.17690109057707737,
      0.19127355061190965,
      0.19037226492649273,
      0.19398806185144357,
      0.2016944591106533,
      0.21269766670141266,
      0.20535261502666557,
      0.1679866714472471,
      0.18792391561733313,
      0.17616650099190556,
      0.17109678922494487,
      0.1817661191369404,
      0.21098116136454959,
      0.17348036439308956,
      0.17989120311162932
    ],
    [
      0.40178790284461896,
      0.0,
      0.5067075904797336,
      0.5812448469076101,
      0.42880859854617914,
      0.4451180381366082,
      0.3244834847124678,
      0.3925749731078396,
      0.4846905813876119,
      0.4319816363778999,
      0.4096880580186164,
      0.30532356692753315,
      0.46240533984113386,
      0.4884552962124149,
      0.4426169279982506,
      0.48117914016986485,
      0.3810687868793887,
      0.48369299649029873,
      0.4719477178624747,
      0.48768440169363125,
      0.41834549809306476,
      0.35673408333446144,
      0.39124158277002596,
      0.44396480694668994,
      0.34283761669988366,
      0.39769529200131126,
      0.47485131880211595,
      0.4238431802552516,
      0.3313176553749806
    ],
    [
      0.3452037665805736,
      0.4457944117373378,
      0.0,
      0.4768391017590239,
      0.4237372209501513,
      0.4332810247104488,
      0.32155813659892396,
      0.38281488050857604,
      0.4730617625965101,
      0.3968380768766382,
      0.39385802828104755,
      0.30819506761254467,
      0.4176332749794467,
      0.46864267445654617,
      0.44326504892207863,
      0.4174934449805463,
      0.4089064006025367,
      0.4105168288506982,
      0.4065616006480586,
      0.47660178123771857,
      0.36236687174888615,
      0.3286469479685237,
      0.40377408363480427,
      0.4034096355666714,
      0.3303984042568906,
      0.3952704843039132,
      0.41858336938118756,
      0.43516118051244956,
      0.32795437004958927
    ],
    [
      0.35390113884752394,
      0.4970391770406659,
      0.4779603553639866,
      0.0,
      0.42252550202081784,
      0.4082322851164992,
      0.3361796497777041,
      0.35666060641677233,
      0.4847742906766235,
      0.42231650566222867,
      0.417541321577247,
      0.3000386351830924,
      0.484046245879987,
      0.4726722777844099,
      0.4562723227039729,
      0.43665627972866683,
      0.35201660844871907,
      0.4286674217044748,
      0.42098176434219514,
      0.4506293871126199,
      0.3481528544348791,
      0.341673813960675,
      0.3528157402533818,
      0.39551481127835464,
      0.31953513043953796,
      0.37486183930174355,
      0.44646244691515324,
      0.4090798127227995,
      0.37094367857119526
    ],
    [
      0.3736158686441089,
      0.4984064309176852,
      0.5099254217386533,
      0.5313806442702391,
      0.0,
      0.42758637128590293,
      0.37968358257980217,
      0.49468609971080424,
      0.5120041305536498,
      0.4651092665411072,
      0.4906526153760775,
      0.3129278579355024,
      0.49528625847959873,
      0.4864326840719806,
      0.4640101850514964,
      0.45115392671201815,
      0.41399552889746394,
      0.47908036720989244,
      0.4722509126325696,
      0.5009299002600196,
      0.4350799058423642,
      0.39354868405603693,
      0.44441359948511705,
      0.4349475930989313,
      0.37462548846112553,
      0.4414206053575269,
      0.5330636141328513,
      0.45883687913223437,
      0.3921040078739586
    ],
    [
      0.3307896604299676,
      0.417030232863155,
      0.4358218274452714,
      0.42193940258605545,
      0.38020976715960164,
      0.0,
      0.32109163125343354,
      0.3898058646663958,
      0.3993415148315407,
      0.3983425067469053,
      0.3836453225077141,
      0.29768276835304097,
      0.40239019343407323,
      0.3833203396728033,
      0.4362373846167289,
      0.4212495152863207,
      0.31605692094470905,
      0.40840475113028374,
      0.39915394974662455,
      0.45143193663181913,
      0.38096780426183563,
      0.3140234549274923,
      0.34203260155493265,
      0.36696047641677687,
      0.3114101502151836,
      0.3463432102655126,
      0.3922541507475621,
      0.4142141209913732,
      0.3504731509405363
    ],
    [
      0.266171477768983,
      0.31980459247805904,
      0.3741781072895587,
      0.364309439583437,
      0.3235185911746574,
      0.33531008026273623,
      0.0,
      0.3424993790039301,
      0.36349118231671307,
      0.29672794556310844,
      0.3468994592170034,
      0.29899661005592937,
      0.3580032235883097,
      0.34864634540497463,
      0.3742790985746083,
      0.3544641680664873,
      0.322432241141531,
      0.3563542044976873,
      0.32795002277933416,
      0.3270129632029142,
      0.31902263601232406,
      0.2941926850477805,
      0.30800561510098734,
      0.3413058612364497,
      0.29763727776582205,
      0.3619763151938482,
      0.33165686173139886,
      0.33749964560498036,
      0.28022472892354244
    ],
    [
      0.32278261320456725,
      0.44259416526568307,
      0.4101187165838516,
      0.4181084869752496,
      0.4062463533279097,
      0.3755210088737708,
      0.33559950889107615,
      0.0,
      0.4375072034129639,
      0.3840371298175358,
      0.3976249662593758,
      0.2919782520339973,
      0.42112587767592347,
      0.3679918250260157,
      0.38082163784833845,
      0.38704267539917425,
      0.3575733533534138,
      0.39963997274649543,
      0.44919238881454837,
      0.4195411315830553,
      0.4231594871311479,
      0.4234454371848835,
      0.3533387552675247,
      0.37224997196165477,
      0.300840581110269,
      0.4471526162707169,
      0.4081505122027955,
      0.39826646187125925,
      0.34353433272552136
    ],
    [
      0.36785129865051314,
      0.5110165387346979,
      0.5175251169109629,
      0.5326016566863119,
      0.46945427484315894,
      0.4510832604595787,
      0.3540179698309385,
      0.41940515288880764,
      0.0,
      0.42755598725118205,
      0.4403570444711351,
      0.3335446203519523,
      0.4785568100016382,
      0.5213868422435304,
      0.4758256195349355,
      0.4814354118442714,
      0.39765018430082955,
      0.493594186510937,
      0.4622543494009752,
      0.5148107673975637,
      0.40038852848646433,
      0.34715523992149455,
      0.41216847682529845,
      0.3985593187450207,
      0.3359137563164647,
      0.4384744048349556,
      0.4602924011934939,
      0.48273975040215467,
      0.36197238186163117
    ],
    [
      0.345802824678175,
      0.36949550854514945,
      0.35924810073141145,
      0.407183415160296,
      0.3814548468127972,
      0.3474621422279882,
      0.2605091422290444,
      0.3727540494072854,
      0.39625365930627576,
      0.0,
      0.3306052765654004,
      0.27656782782491174,
      0.35160732750934254,
      0.32553164536387325,
      0.36731529498545834,
      0.33147162446736145,
      0.3003372616493165,
      0.3290208141870363,
      0.41370222694615677,
      0.3734930581929232,
      0.37994736791556827,
      0.3534978606933814,
      0.3732900278597948,
      0.3106493917790416,
      0.29888487793323315,
      0.2874866884231102,
      0.35882049672326866,
      0.34658053626729135,
      0.31163977234406315
    ],
    [
      0.34934949039170404,
      0.4508413520791219,
      0.45929997537648903,
      0.4690019785678263,
      0.47171224064821415,
      0.4309133379345096,
      0.309200885029987,
      0.4099667544501331,
      0.44884868116821597,
      0.3922735674918485,
      0.0,
      0.2896247895799571,
      0.48321119243744937,
      0.500248067201728,
      0.44464689162978055,
      0.5083825776108368,
      0.3563398766078243,
      0.4534870870012182,
      0.43550252069599793,
      0.42896220781553773,
      0.4117719323494631,
      0.33037003748580673,
      0.40028397680005523,
      0.42810958705744695,
      0.3016883184142194,
      0.38915006647104233,
      0.4764218885221079,
      0.4050752548035439,
      0.3676621455700455
    ],
    [
      0.2327054924532761,
      0.22472366068277694,
      0.22860130244826737,
      0.23258780241473032,
      0.23163580463204614,
      0.24235559993989475,
      0.2336537190354675,
      0.2327247691879446,
      0.2421213275600378,
      0.2292607512282201,
      0.21148273856215738,
      0.0,
      0.1946061544437414,
      0.2148224288174252,
      0.21744058746429817,
      0.20907305511473662,
      0.25140779658431267,
      0.2187394852407445,
      0.2285665725965491,
      0.22880311149740984,
      0.2073365792891888,
      0.2409190322784731,
      0.24482098155669818,
      0.23329459024810184,
      0.2165032580317734,
      0.2535926013644654,
      0.2404738999436602,
      0.23309452971800781,
      0.254478003192496
    ],
    [
      0.4022422747391283,
      0.49767966660058116,
      0.5051255534374766,
      0.5293425252294841,
      0.49312635238382874,
      0.45047587009809087,
      0.378343062760929,
      0.417447470941515,
      0.46874077517833546,
      0.4259267756681726,
      0.5006038579398366,
      0.34649347254569784,
      0.0,
      0.46905295427527127,
      0.49111162146146636,
      0.47512370772884327,
      0.3987869464692184,
      0.4738341425483388,
      0.46913586026630605,
      0.452922193846911,
      0.39677193632711805,
      0.3514855323638941,
      0.4059209887255075,
      0.41915463251807883,
      0.35542578617108234,
      0.43085134650487,
      0.5084226987214351,
      0.4215872441975954,
      0.34313364978471284
    ],
    [
      0.3846607427523472,
      0.5017610170770259,
      0.5516558101834792,
      0.549416133780128,
      0.45375762911505024,
      0.4379776647714424,
      0.3318057605959499,
      0.43256234839920515,
      0.4743514848728301,
      0.3949335515088541,
      0.4397851213818833,
      0.33762454829569877,
      0.46377623737718454,
      0.0,
      0.42949523248416743,
      0.48279928120633686,
      0.4160319235236789,
      0.46536564003928405,
      0.43197123058644027,
      0.48245684282278467,
      0.3952718825260577,
      0.37941661758692535,
      0.40591954462905955,
      0.44515920543065013,
      0.33083148937254947,
      0.4494041575899699,
      0.45257773952219327,
      0.4170211960729835,
      0.3628528096210506
    ],
    [
      0.27749083990778933,
      0.4104061843255393,
      0.41089472838550267,
      0.4237256644105247,
      0.3919800359609251,
      0.3910103791025652,
      0.34472896928382757,
      0.37466086630335593,
      0.4165242960818243,
      0.33997933411810055,
      0.38045593882878426,
      0.30849963748299847,
      0.42108076324418287,
      0.42941781469876217,
      0.0,
      0.3989699994981528,
      0.3690404001721024,
      0.4286198906429952,
      0.36100441296566266,
      0.4032014335233043,
      0.37566046474057324,
      0.2912925131826185,
      0.318765720969834,
      0.41917604622561533,
      0.30321289550812325,
      0.4077269382057065,
      0.38335318769568083,
      0.43914730773277255,
      0.31544425794583186
    ],
    [
      0.35626483698062916,
      0.4901110658724237,
      0.4493732931192076,
      0.47423262367841024,
      0.42467268640056277,
      0.4408823578368717,
      0.3886267807524564,
      0.40959372850120124,
      0.471784498305718,
      0.42963056056811055,
      0.42107336011346774,
      0.35297622523838057,
      0.4145816645156877,
      0.5053081086880682,
      0.4520917858652389,
      0.0,
      0.36850718519120895,
      0.4464831885717766,
      0.4215308698722131,
      0.5028436519316017,
      0.3774602166582184,
      0.35154358299514854,
      0.37686932064030443,
      0.37224030704941824,
      0.3117354291977297,
      0.4445159718055147,
      0.38507609459542036,
      0.4171533101476095,
      0.36667635169392665
    ],
    [
      0.29037218478145266,
      0.35715207615218336,
      0.3867374964473458,
      0.33365730630515444,
      0.3802270428140464,
      0.35195617859018835,
      0.3419646142342254,
      0.3420979174741532,
      0.364509859009168,
      0.33604326900703985,
      0.3334320834207847,
      0.30005964631198245,
      0.3492004554323116,
      0.33081842904897574,
      0.3892134909589764,
      0.382284486479495,
      0.0,
      0.35619404977721536,
      0.34560089580789266,
      0.3389500790223252,
      0.3665577019221866,
      0.32247518123687424,
      0.3426891942340766,
      0.3429706477250989,
      0.3173726835728883,
      0.39353213740220094,
      0.3836293356452718,
      0.3380579952208238,
      0.32374352330703804
    ],
    [
      0.36409559392287294,
      0.47095799613522504,
      0.4249557088023157,
      0.47990858353969235,
      0.4361262501107841,
      0.4272304674353615,
      0.3293417653431303,
      0.4147154030080391,
      0.4358167147882839,
      0.37211672459854306,
      0.4461909567275615,
      0.3309962094605139,
      0.4859721783749569,
      0.445803607806063,
      0.46152227077641506,
      0.42863625376924275,
      0.35610788881029576,
      0.0,
      0.42937318340480735,
      0.46645203123662515,
      0.4116491690151345,
      0.3549012094059101,
      0.4003326626208694,
      0.4108262863038832,
      0.3069360007105564,
      0.43140032606685175,
      0.46635396972652843,
      0.435475158312129,
      0.3669842080880399
    ],
    [
      0.31089919954658485,
      0.4147730674313972,
      0.4244450367960615,
      0.433853823833356,
      0.35832709626167447,
      0.3790672296390203,
      0.3225059872289304,
      0.43747599540116267,
      0.41419098769340557,
      0.4185282230294727,
      0.3546019376670859,
      0.28474843900899893,
      0.3923200080764708,
      0.3670504405906023,
      0.4143134155459718,
      0.41653983324682975,
      0.323164116287036,
      0.3978712087729863,
      0.0,
      0.40655906753981674,
      0.3606443696791637,
      0.37503115459066305,
      0.34074212603981957,
      0.3649082482237247,
      0.3515846607124984,
      0.344811661081561,
      0.44002511051833904,
      0.4143245086118812,
      0.3150636582487307
    ],
    [
      0.3636582936878927,
      0.46205650528173603,
      0.5108219154860272,
      0.47452723063109303,
      0.44005326807503753,
      0.4324819581143833,
      0.3175422396873173,
      0.36410029377027464,
      0.4878574812763039,
      0.4001538344880846,
      0.40221747142442643,
      0.3040340761124767,
      0.41950410050562414,
      0.46333881751906536,
      0.4477774285967342,
      0.4460198246679661,
      0.3611656703425583,
      0.4470370871300373,
      0.4297840656251415,
      0.0,
      0.3706550269001656,
      0.3343767142404348,
      0.35882192018259107,
      0.38035137330419944,
      0.3208737545233491,
      0.35659817533732396,
      0.425413416341756,
      0.4576066938359118,
      0.33683075518949557
    ],
    [
      0.36260330698971543,
      0.4526930264588487,
      0.4099589815867848,
      0.44539343904047035,
      0.4241795932427186,
      0.3538522738613401,
      0.29606463722801535,
      0.4312286420607063,
      0.44268848206717437,
      0.41429186739466606,
      0.44551752709671333,
      0.2631103124607195,
      0.4521447247623265,
      0.4162927664570175,
      0.41516837573119436,
      0.4189960091946676,
      0.3442024671655264,
      0.4334519587360497,
      0.4227601024023302,
      0.3901593184219776,
      0.0,
      0.3749647193825101,
      0.36229349089822116,
      0.3849129338305224,
      0.28951496756169215,
      0.3987185080274971,
      0.4017241376080749,
      0.39763785328145973,
      0.2892923202755133
    ],
    [
      0.30037856687788134,
      0.33492604964753725,
      0.3587686366694083,
      0.3401924640306595,
      0.3522564750611177,
      0.3486014182399806,
      0.29287190775187755,
      0.3798800703141483,
      0.3414380149104639,
      0.33119148658316977,
      0.335730632446976,
      0.3085853311669404,
      0.32339698988967425,
      0.3144372644300564,
      0.3606111100898528,
      0.3346616025314002,
      0.33319802037632185,
      0.340202545197148,
      0.34986427495347305,
      0.33751041867722065,
      0.37834100625713085,
      0.0,
      0.29986580064036317,
      0.3601899096308312,
      0.25908736340905003,
      0.4120847275988828,
      0.34958368575882126,
      0.3583773317630443,
      0.37959597302686654
    ],
    [
      0.3518677211756165,
      0.39403605327062685,
      0.3990297068040425,
      0.4017538079266241,
      0.4060927056162531,
      0.36338521804762336,
      0.2869890616427968,
      0.38240383480706863,
      0.40225483246018356,
      0.3933957749321866,
      0.3592619083007542,
      0.3016785255380434,
      0.3980160290548418,
      0.3907001402740331,
      0.3873391027028361,
      0.3760016265774151,
      0.32916923767282147,
      0.37799712329603663,
      0.36120140343673723,
      0.38011883606692365,
      0.3407082579377372,
      0.2641446203999738,
      0.0,
      0.3522552026670107,
      0.3286981601623691,
      0.36150177517154525,
      0.38184659571492086,
      0.3417429355585062,
      0.33822185393593407
    ],
    [
      0.30599867521381796,
      0.43002074095111187,
      0.46098919641793934,
      0.4812820143856211,
      0.41279671524311,
      0.3903861680732843,
      0.3606077579776539,
      0.391173960459378,
      0.40979815517708307,
      0.3380908066055637,
      0.397363761464387,
      0.3194069078447743,
      0.4161482849617435,
      0.46538994376652365,
      0.5134393377475446,
      0.422876220152852,
      0.39491274727984327,
      0.4278881402118848,
      0.3851633064614912,
      0.4091854955293779,
      0.382298223161585,
      0.38220600929472903,
      0.3803886863305235,
      0.0,
      0.3265643913881271,
      0.4459957603907825,
      0.4163471584605656,
      0.43583554414420034,
      0.36768155148204396
    ],
    [
      0.2985994253024984,
      0.32532591771408903,
      0.3962526797667214,
      0.37292963074314067,
      0.39074530291975473,
      0.33689483597336034,
      0.3575210439996046,
      0.3107063608838905,
      0.39493233260020744,
      0.35615499400299777,
      0.34633303784199043,
      0.32742912320174544,
      0.3539020622181761,
      0.3296814463511295,
      0.33420790627998365,
      0.32907467668835055,
      0.36273713704541355,
      0.34816514364084217,
      0.30765580414256144,
      0.33058746777178794,
      0.3078320545281781,
      0.3054333709626045,
      0.3729071018547281,
      0.2929549320621323,
      0.0,
      0.36716937127555394,
      0.40730066053219494,
      0.33550737458417457,
      0.3276809541481489
    ],
    [
      0.3678917436385696,
      0.47589526449207,
      0.5118439677312676,
      0.48675490020528955,
      0.46665317923457716,
      0.41441689452363795,
      0.4025333404719873,
      0.42817764958043947,
      0.4801020858566909,
      0.36529358876150586,
      0.45069847400946816,
      0.3252995460722803,
      0.48830912992913333,
      0.47687456795189287,
      0.49802046539310707,
      0.4716843164346176,
      0.4306213999179489,
      0.4785971669100577,
      0.3928156198959014,
      0.4509496399755504,
      0.4113763936036914,
      0.3693343604941306,
      0.35139995626694276,
      0.4649795083773576,
      0.356461135656265,
      0.0,
      0.43632791627472955,
      0.434514405005872,
      0.38388303250963896
    ],
    [
      0.39448556619186537,
      0.43789889205870414,
      0.4727729880438285,
      0.47639662758340506,
      0.47720333551728533,
      0.4572144860791596,
      0.3831204754194468,
      0.40376768245755756,
      0.4801542982316136,
      0.3921116902063262,
      0.4567195558496695,
      0.3389378611118783,
      0.4612978935827754,
      0.39166265118251986,
      0.44174610127031677,
      0.39993023933254257,
      0.3991026990062254,
      0.43935763402077943,
      0.4528576511084761,
      0.4664864982926462,
      0.38909550306679197,
      0.2949182121707923,
      0.4461914260944937,
      0.3813984967323758,
      0.4446245948475547,
      0.3973856234822821,
      0.0,
      0.45812428715346876,
      0.36241852382774
    ],
    [
      0.28351521286953907,
      0.4022878469158373,
      0.47462191505714446,
      0.4570527471836874,
      0.43692880175813253,
      0.3892393720443015,
      0.30284880179354734,
      0.3777343554722552,
      0.475209530924839,
      0.33416756452338725,
      0.390049161295851,
      0.2699812865610711,
      0.41453289425949413,
      0.3844293442441469,
      0.43724018840480405,
      0.42184839999635093,
      0.32195838798804943,
      0.39971047190663533,
      0.4278250140107709,
      0.440821106124498,
      0.38220318642959783,
      0.3320588158990787,
      0.33684244722157164,
      0.41209126378621774,
      0.3040016616740391,
      0.40030828712344513,
      0.42827209709249314,
      0.0,
      0.3440125089464492
    ],
    [
      0.377827073113149,
      0.41996457071134063,
      0.42449224643469075,
      0.4500592735700151,
      0.44879819288332556,
      0.45466103434154337,
      0.41971339394210605,
      0.4390567790666131,
      0.4571214938014765,
      0.42237371203120744,
      0.4291160406621366,
      0.3955034667058861,
      0.42092344060415754,
      0.4245179865299309,
      0.421812252905706,
      0.4564891003010465,
      0.3941411669980488,
      0.42816507419165273,
      0.41004962540509315,
      0.4298103711339183,
      0.4350590993950274,
      0.4790008707152664,
      0.416820902364055,
      0.44392506155103817,
      0.3919391671549477,
      0.48859105908567124,
      0.4271715508632665,
      0.45900519084464797,
      0.0
    ]
  ],
  "row_avgs": [
    0.19209331553769862,
    0.42829610424542724,
    0.40201313858258286,
    0.4049339965452117,
    0.45239851536816855,
    0.379022307522416,
    0.33116324137811054,
    0.38839947938638275,
    0.4388425482464607,
    0.34502189524031984,
    0.4143695243282896,
    0.2296366298402465,
    0.43850960355120455,
    0.4321658158259004,
    0.3762668186122734,
    0.4151371091709474,
    0.3479107129050491,
    0.413970670653594,
    0.37779895040368744,
    0.404130692581336,
    0.390493455115159,
    0.3398510384975107,
    0.36256471611255225,
    0.40250841644919794,
    0.34380793389414155,
    0.43113248747052213,
    0.42133505335437577,
    0.3850640239824013,
    0.43093247133239165
  ],
  "col_avgs": [
    0.33867188543519144,
    0.41641127566318314,
    0.43077929817905714,
    0.43769538527345525,
    0.4052840270603081,
    0.3894596066999173,
    0.3283071238654918,
    0.38263492120667336,
    0.4242897423145302,
    0.3758737261731863,
    0.389234776333973,
    0.3072735209352685,
    0.40782955414466143,
    0.40636257421132316,
    0.41338364914719067,
    0.4047432481356537,
    0.35539302209224416,
    0.4051473801076761,
    0.39601256449719413,
    0.4129147416158534,
    0.3704813061692932,
    0.3376706226152789,
    0.36181716594424695,
    0.3768795214551857,
    0.317865563946149,
    0.3873495024669552,
    0.40876562416899415,
    0.39567821618362553,
    0.33556112009179817
  ],
  "combined_avgs": [
    0.265382600486445,
    0.4223536899543052,
    0.41639621838081997,
    0.4213146909093335,
    0.4288412712142383,
    0.3842409571111667,
    0.3297351826218012,
    0.38551720029652803,
    0.4315661452804954,
    0.3604478107067531,
    0.4018021503311313,
    0.2684550753877575,
    0.423169578847933,
    0.4192641950186118,
    0.394825233879732,
    0.40994017865330057,
    0.35165186749864663,
    0.40955902538063504,
    0.38690575745044076,
    0.40852271709859467,
    0.3804873806422261,
    0.33876083055639483,
    0.3621909410283996,
    0.3896939689521918,
    0.33083674892014525,
    0.40924099496873867,
    0.41505033876168496,
    0.39037112008301345,
    0.3832467957120949
  ],
  "gppm": [
    570.5928442667006,
    571.3985457112227,
    560.2485439181044,
    561.8361827609062,
    574.6720913113047,
    580.0361707225676,
    609.5575849259313,
    583.6104251722571,
    564.4099000968679,
    585.3117228662294,
    584.9543007439979,
    615.2914222593151,
    575.9335403809171,
    576.7483247105748,
    573.479443725528,
    575.453306522624,
    595.0314148317856,
    576.0022021363671,
    578.6467560328967,
    572.4395611117798,
    592.5561504319222,
    603.7870551408853,
    593.4525735032304,
    589.0880019968716,
    610.0084636999378,
    578.684192316494,
    572.6703367894796,
    578.7020983259243,
    607.8855657007878
  ],
  "gppm_normalized": [
    1.3173029638908065,
    1.2877213578908544,
    1.2633692327341717,
    1.2620487116558798,
    1.2904667422233769,
    1.3086105376505128,
    1.3790511894543769,
    1.3122860925001707,
    1.2671721809180094,
    1.3192059491994768,
    1.317076811148933,
    1.391186374970608,
    1.2957929147651193,
    1.2960260676724695,
    1.2895287886307976,
    1.297076982409992,
    1.339109661836296,
    1.2903228222620005,
    1.2996377425005083,
    1.2921123629291775,
    1.327134835895428,
    1.3585704889980241,
    1.3299276935666293,
    1.3262535131321762,
    1.3716090261130383,
    1.3017155787705004,
    1.2878098585588877,
    1.298184985658773,
    1.3694740826730165
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364
  ],
  "response_lengths": [
    2866,
    2630,
    2638,
    2460,
    2346,
    2519,
    2499,
    2367,
    2390,
    2355,
    2281,
    2973,
    2439,
    2440,
    2527,
    2518,
    2280,
    2392,
    2268,
    2580,
    2133,
    2676,
    2264,
    2325,
    2242,
    2324,
    2188,
    2566,
    2104
  ]
}