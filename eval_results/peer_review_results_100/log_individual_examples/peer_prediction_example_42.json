{
  "example_idx": 42,
  "reference": "Under review as a conference paper at ICLR 2023\n\nINCREMENTAL PREDICTIVE CODING: A PARALLEL AND FULLY AUTOMATIC LEARNING ALGORITHM\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNeuroscience-inspired models, such as predictive coding, have the potential to play an important role in the future of machine intelligence. However, they are not yet used in industrial applications due to some limitations, one of them being the lack of efficiency. In this work, we address this by proposing incremental predictive coding (iPC), a variation of the original framework derived from the incremental expectation maximization algorithm, where every operation can be performed in parallel without external control. We show both theoretically and empirically that iPC is more efficient than the original algorithm by Rao and Ballard 1999, while maintaining performance comparable to backpropagation in image classification tasks. This work impacts several areas, has general applications in computational neuroscience and machine learning, and specific applications in scenarios where automatization and parallelization are important, such as distributed computing and implementations of deep learning models on analog and neuromorphic chips.\n\n1\n\nINTRODUCTION\n\nIn recent years, deep learning has reached and surpassed human-level performance in a multitude of tasks, such as game playing (Silver et al., 2017; 2016), image recognition (Krizhevsky et al., 2012; He et al., 2016), natural language processing (Chen et al., 2020), and image generation (Ramesh et al., 2022). These successes are achieved entirely using deep artificial neural networks trained via backpropagation (BP), which is a learning algorithm that is often criticized for its biological implausibilities (Grossberg, 1987; Crick, 1989; Abdelghani et al., 2008; Lillicrap et al., 2016; Roelfsema & Holtmaat, 2018; Whittington & Bogacz, 2019), such as lacking local plasticity and autonomy. In fact, backpropagation requires a global control signal required to trigger computations, since gradients must be sequentially computed backwards through the computation graph. These properties are not only important for biological plausibility: parallelization, locality, and automation are key to build efficient models that can be trained end-to-end on non Von-Neumann machines, such as analog chips (Kendall et al., 2020). A learning algorithm with most of the above properties is predictive coding (PC).\n\nPC is an influential theory of information processing in the brain (Mumford, 1992; Friston, 2005), where learning happens by minimizing the prediction error of every neuron. PC can be shown to approximate backpropagation in layered networks (Whittington & Bogacz, 2017), as well as on any other model (Millidge et al., 2020), and can exactly replicate its weight update if some external control is added (Salvatori et al., 2022b). Also the differences with BP are interesting, as PC allows for a much more flexible training and testing (Salvatori et al., 2022a), has a rich mathematical formulation (Friston, 2005; Millidge et al., 2022), and is an energy-based model (Bogacz, 2017). This makes PC unique, as it is the only model that jointly allows training on neuromorphic chips, is an implementation of influential models of cortical functioning in the brain, and can match the performance of backpropagation in different tasks. Its main drawback, however, is the efficiency, as it is slower than BP. In this work, we address this problem by proposing a variation of PC that is much more efficient than the original one.\n\nSimply put, PC is based on the assumption that brains implement an internal generative model of the world, needed to predict incoming stimuli (or data) (Friston et al., 2006; Friston, 2010; Friston et al., 2016). When presented with a stimulus that differs from the prediction, learning happens by\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nupdating internal neural activities and synapses to minimize the prediction error. In computational models, this is done via multiple expectation-maximization (EM) (Dempster et al., 1977) steps on the variational free energy, in this case a function of the total error of the generative model. During the Estep, internal neural activities are updated in parallel until convergence; during the M-step, a weight update to further minimize the same energy function is performed. This approach results in two limitations: first, the E-step is slow, as it can require dozens of iterations before convergence; second, an external control signal is needed to switch from the E to M step. In this paper, we show how to address both of these problems by considering a variation of the EM algorithm, called incremental expectation-maximization (iEM), which performs both E and M steps in parallel (Neal & Hinton, 1998). This algorithm is provably faster, does not require a control signal to switch between the two steps, and has solid convergence guarantees (Neal & Hinton, 1998; Karimi et al., 2019). What results is a training algorithm that we call incremental predictive coding (iPC) that is a simple variation of PC that addresses the main drawback of PC (namely, efficiency), with no drawbacks from the learning perspective, as it has been formally proven to have equivalent convergence properties to standard PC. Furthermore, we provide initial evidence that iPC is also potentially more efficient than BP in the specific case of full-batch training. In fact, we theoretically show that, on an ideal parallel machine, to complete one update of all weights on a network with L layers, the time complexity of iPC is O(1), while that of BP is O(L). However, additional engineering efforts are needed to reach this goal, which are beyond the focus of this work: our experiments are performed using PyTorch (Paszke et al., 2017), which is not designed to parallelize computations across layers on GPUs. We partially address this limitation by performing some experiments on CPUs, which empirically confirm our claims about efficiency, as shown in Fig. 3.Our contributions are briefly as follows:\n\n1. We first develop the update rule of iPC from the variational free energy of a hierarchical generative model using the incremental EM approach. We then discuss the implications of this change in terms of autonomy and convergence guarantees: it has in fact been proven that iEM converges to a minimum of the loss function (Neal & Hinton, 1998; Karimi et al., 2019), and hence this result naturally extends to iPC. We conclude by analyzing similarities and differences between iPC, standard PC, and BP.\n\n2. We empirically compare the efficiency of PC and iPC on generation tasks, by replicating some experiments performed in (Salvatori et al., 2021), and classification tasks, by replicating experiments similar to those presented in (Whittington & Bogacz, 2017). In both cases, iPC is by far more efficient than the original counterpart. Furthermore, we present initial evidence that iPC can decrease the training loss faster than BP, assuming that a proper parallelization is done.\n\n3. We then test our model on a large number of image classification benchmarks, showing that that iPC performs better than PC, on average, and similarly to BP. Then, we show that iPC requires less parameters than BP to perform well on convolutional neural networks (CNNs). Finally, we show that iPC follows the trends of energy-based models on training robust classifiers (Grathwohl et al., 2019), and yields better calibrated outputs than BP on the best performing models.\n\n2 PRELIMINARIES\n\nIn this section, we introduce the original formulation of predictive coding (PC) as a generative model proposed by Rao and Ballard 1999. Let us consider a generative model g : Rd × RD −→ Ro, where x ∈ Rd is a vector of latent variables called causes, y ∈ Ro is the generated vector, and θ ∈ RD is a set of parameters. We are interested in the following inverse problem: given a vector y and a generative model g, we need the parameters θ that maximize the marginal likelihood\n\np(y, θ) =\n\n(cid:90)\n\nx\n\np(y | x, θ)p(x, θ)dx.\n\n(1)\n\nHere, the first term inside the integral is the likelihood of the data given the causes, and the second is a prior distribution over the causes. Solving the above problem is intractably expensive. Hence, we need an algorithm that is divided in two phases: inference, where we infer the best causes x given both θ and y, and learning, where we update the parameters θ based on the newly computed causes. This algorithm is expectation-maximization (EM) (Dempster et al., 1977). The first step, which we call inference or E-step, computes p(x | y, θ), that is the posterior distribution of the causes given\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) An example of an hierarchical Gaussian generative model with three layers. (b) Comparison of the temporal training dynamics of PC, Z-IL, and iPC, where Z-IL is a variation of predictive coding that is equivalent to BP, originally introduced in Song et al. (2020). We assume that we train the networks on a dataset for supervised learning for a period of time T . Here, t is the time axis during inference, which always starts at t = 0. The squares represent nodes in one layer, and pink rounded rectangles indicate when the connection weights are modified: PC (1st row) first conducts inference on the hidden layers, according to Eq. equation 6, until convergence, and then it updates the weights via Eq. 7. Z-IL (2nd row) only updates the weights at specific inference moments depending on which layer the weights belong to. To conclude, iPC updates the weights at every time step t, while performing inference in parallel.\n\na generated vector y. Computing the posterior is, however, intractable (Friston, 2003). To this end, we approximate the intractable posterior with a tractable probability distribution q(x, θ). To make the approximation as good as possible, we want to minimize the KL-divergence between the two probability distributions. Summarizing, to solve our learning problem, we need to (i) minimize a KL-divergence, and (ii) maximize a likelihood. We do it by defining the following energy function, also known as variational free-energy:\n\nF (x, y, θ) = KL(q(x, θ) ∥ p(x | y, θ)) − ln(p(y, θ)),\n\n(2)\n\nwhere we have used the log-likelihood. This function is minimized by multiple iterations of the EM algorithm as follows:\n\n(cid:26)Inference (E-step): x∗ = argmaxxF (x, y, θ), Learning (M-step): θ∗ = argmaxθF (x, y, θ).\n\n(3)\n\n2.1 PREDICTIVE CODING\n\nSo far, we have only presented the general problem. To actually derive proper equations for learning causes and update the parameters, and use them to train neural architectures, we need to specify the generative function g(x, θ). Following the general literature, (Rao & Ballard, 1999; Friston, 2005), we define the generative model as a hierarchical Gaussian generative model, where the causes x and parameters θ are defined by a concatenation of the causes and weight matrices of all the layers, i.e., x = (x(0), . . . , x(L)), and θ = (θ(0), . . . , θ(L−1)). Hence, we have a multilayer generative model, where layer 0 is the one corresponding to the generated image y, and layer L the highest in the hierarchy. The marginal probability of the causes is as follows:\n\np(x(0), . . . , x(L)) = p(x(L))\n\nL−1 (cid:89)\n\nl\n\np(x(l−1) | x(l)) =\n\nL (cid:89)\n\nl\n\nN (μ(l), Σ(l)),\n\n(4)\n\nwhere μ(l) is the prediction of layer l according to the layer above, given by μ(l) = θ(l) · f (x(l+1)), with f being a non-linear function and μ(L) = x(L). For simplicity, from now on, we consider Gaussians with identity variance, i.e., Σ(l) = 1 for every layer l. With the above assumptions, the\n\n3\n\nWeights updateInput layer (fixed)Output layer (fixed)Inference stepNumber of iterationsLayer indexHidden layer (running inference)PCZ-IL(BP)iPC(a)Generative Model(b) Differences between PC, Z-IL, and iPCUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Learning a dataset D = {yi} with iPC. 1: Require: For every i, x(0) 2: for t = 0 to T do 3: 4: 5: end for\n\nFor every i and l, update x(l) to minimize F via Eq.(6) For every l, update each θ(l) to minimize F via Eq.(7)\n\nis fixed to yi,\n\ni\n\ni\n\nfree-energy becomes\n\n(cid:88)\n\nF =\n\n∥x(l) − μ(l)∥2.\n\nl\n\n(5)\n\nFor a detailed formulation on how this energy function is derived from the variational free-energy of Eq. 2, we refer to (Friston, 2005; Bogacz, 2017; Buckley et al., 2017), or to the supplementary material. Note that this energy function is equivalent to the one proposed in the original formulation of predictive coding (Rao & Ballard, 1999). A key aspect of this model is that both inference and learning are achieved by optimizing the same energy function, which aims to minimize the prediction error of the network. The prediction error of every layer is given by the difference between its real value x(l) and its prediction μ(l). We denote the prediction error ε(l) = x(l)−μ(l). Thus, the problem of learning the parameters that maximize the marginal likelihood given a data point y reduces to an alternation of inference and weight update. During both phases, the values of the last layer are fixed to the data point, i.e., x(0) = y for every t ≤ T .\n\nInference: During this phase, that corresponds to the E-step, the weight parameters θ(l) are fixed, while the values x(l) are continuously updated via gradient descent. ∂F ∂x(l)\n\n= γ · (−ε(l) + f ′(x(l)) ∗ θ(l−1) T · ε(l−1)),\n\n∆x(l) = −γ\n\n(6)\n\nwhere ∗ denotes element-wise multiplication, and l > 0. This process either runs until convergence, or for a fixed number of iterations T .\n\nLearning: During this phase, which corresponds to the M-step, the values x are fixed, and the weights are updated once via gradient descent according to the following equation:\n\n∆θ(l) = −α\n\n∂F ∂θ(l)\n\n= α · x(l+1)ε(l).\n\n(7)\n\nNote that the above algorithm is not limited to generative tasks, but can also be used to solve supervised learning problems (Whittington & Bogacz, 2017). Assume that we are provided with a data point yin with label yout. In this case, we treat the label as the vector y we need to generate, and the data point as the prior on x(L). The inference and learning phases are identical, with the only difference that now we have two vectors fixed during the whole duration of the process: x(0) = yout, and x(L) = yin. While this algorithm is able to obtain good results on small image image classification tasks, it is much slower than BP due to the large number of inference steps T needed to let the causes x converge.\n\n3\n\nINCREMENTAL PREDICTIVE CODING\n\nOne of the main drawbacks of energy based models such as PC and equilibrium propagation (Scellier & Bengio, 2017), is their efficiency. In fact, these algorithms are much slower than BP due to the inference phase, which requires multiple iterations to converge. The goal of this paper is to address this problem for predictive coding, by developing a variation based from the incremental EM (iEM) algorithm (Neal & Hinton, 1998), which was developed to address the lack of efficiency of the original EM. This algorithm excels when dealing with multiple data points at the same time (Neal & Hinton, 1998), a scenario that is almost always present in standard machine learning.\n\nLet D = {yi}i<N be a dataset of cardinality N , and g(x, θ) be a generative model. Our goal is now to minimize the global marginal likelihood, defined on the whole dataset, i.e.,\n\np(yi, θ).\n\n(8)\n\np(D, θ) =\n\n(cid:88)\n\ni\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Left and centre: Decrease of the energy of generative models as a function of the number of iterations performed from the beginning of the training process. Right: Training loss of different classifiers trained using iPC, BP, and multiple parameterizations of PC as a function of the number of non-parallel matrix multiplications performed from the beginning of the training process.\n\nThe same reasoning also applies to the global variational free energy, which will be the sum of the free energies of every single data point. In this case, the iEM algorithm performs the E-step and M-step in parallel, with no external control needed to switch between the two phases. In detail, both the values x and the parameters θ are updated simultaneously at every time step t, until convergence (or for a fixed number of iterations T ), according to the same update rule defined in Eqs. 6 and 7, on all the points of the dataset. No explicit forward and backward passes are necessary as each layer is updated in parallel. To our knowledge, this is the first learning algorithm for deep neural networks where every single operation is performed in parallel. Note that this increased speed does not harm the final performance, as it has been formally proven that minimizing a free-energy function such as ours (i.e., equivalent to the sum of independent free-energy functions) using iEM, also finds a minimum of the global marginal likelihood of Eq.8 (Neal & Hinton, 1998; Karimi et al., 2019). We actually provide empirical evidence that the model converges to better minima using iPC rather than the original formulation of PC in Fig. 2 and Table 1. The pseudocode of iPC is given in Alg. 1.\n\nConnections to BP: PC in general shares multiple similarities with BP in supervised learning tasks: when the output error is small, the parameter update of PC is an approximation of that of BP (Millidge et al., 2020); when controlling which parameters have to be updated at which time step, the two updates can even be made equivalent (Salvatori et al., 2022b). To make PC perform exactly the same weight update of BP, every weight matrix θl must be updated only at t = l, which corresponds to its position in the hierarchy (Song et al., 2020). That is, as soon as the output error reaches a specific layer. This is different from the standard formulation of PC, which updates the parameters only when the energy representing the total error has converged. Unlike PC, iPC updates the parameters at every time step t. Intuitively, it can hence be seen as a “continuous shift” between Z-IL and PC, where Z-IL is a variation of PC that is equivalent to BP, originally introduced in Song et al. (2020).. A graphical representation of the differences of all three algorithms is given in Fig. 1 (right), with the pseudo-codes provided in the first section of the supplementary material.\n\nAutonomy: Both PC and Z-IL lack full autonomy, as an external control signal is always needed to switch between inference and learning: PC waits for the inference to converge (or, for T iterations), while Z-IL updates the weights of specific layers at specific inference moments t = l. BP is considered to be less autonomous than PC and Z-IL: a control signal is required to forward signals as well as backward errors, and additional places to store the backward errors are required. All of those drawbacks are removed in iPC, which is able to learn a dataset without the control signals required by the other algorithms: given a dataset D, iPC runs inference and weight updates simultaneously until the energy F is minimized. As soon as the energy minimization has converged, training ends.\n\n3.1 EFFICIENCY\n\nIn this section, we analyze the efficiency of iPC with respect to both the original formulation of PC and BP. We only provide partial evidence of the increased efficiency against BP, as standard deep learning frameworks, such as Pytorch, do not allow to parallelize operations in different layers. While we leave the development of a framework able to perform every operation in parallel to future work, we provide evidence that the speed up against BP in full batch training is theoretically possible using iPC.\n\n5\n\nTrain LossNon-parallel MultiplicationsEnergyEnergyIterationsIterationsTiny ImagenetCIFAR10MNISTFashionMNIST(a)Generation(b) ClassificationTest ErrorIterationsUnder review as a conference paper at ICLR 2023\n\nComparison with PC: We now show how iPC is more efficient than the original formulation. To do that, we have trained multiple models with iPC and PC on different tasks and datasets. First, we have trained a generative model with 4 layers and 256 hidden neurons on a subset of 100 images of the Tiny ImageNet and CIFAR10 datasets, as done in (Salvatori et al., 2021). A plot with the energies as a function of the number of iterations is presented in Fig. 2 (left and centre). In both cases, the network trained with iPC converges much faster than the networks trained with PC with different values of T . Many more plots with different parameterizations are given in Fig. 7 in the supplementary material.\n\nTo show that the above results hold in different set-ups as well, we have trained a classifier with 4 layers on a subset of 250 images of the FashionMNIST dataset, following the framework proposed in (Whittington & Bogacz, 2017), and studied the training loss. As it is possible to train an equivalent model using BP, we have done it using the same set-up and learning rate, and included it in the plot. This, however, prevents us from using the number of iterations as an efficiency measure, as one iteration of BP is more complex than one iteration of PC, and are hence not comparable. As a metric, we have hence used the number of non-parallel matrix multiplications needed to perform a weight update. This is a fair metric, as matrix multiplications are by far the most expensive operation performed when training neural networks, and the ones with largest impact on the training speed. One iteration of PC and iPC have the same speed, and consist of 2 non-parallel matrix multiplications. One epoch of BP, consists of 2L non-parallel matrix multiplications. The results are given in Fig. 2 (right). In all cases, iPC converges much faster than all the other methods. In the supplementary material, we provide other plots obtained with different datasets, models, and parameterizations, as well as a study on how the test error decreases during training. Again, many more plots with different parameterizations are given in Fig. 8 in the supplementary material.\n\nComparison with BP: While the main goal of this work is simply to overcome the core limitation of original PC — the slow inference phase — there is one scenario where iPC is potentially more efficient than BP, which is full batch training. Particularly, we first prove this formally using the number of non-parallel matrix multiplications needed to perform a weight update as a metric. To complete one weight update, iPC requires two sets of non-parallel multiplications: the first uses the values and weight parameters of every layer to compute the prediction of the layer below; the second uses the error and transpose of the weights to propagate the error back to the layer above, needed to update the values. BP, on the other hand, requires 2L sets of non-parallel multiplications for a complete update of the parameters: L for a forward pass, and L for a backward one. These operations cannot be parallelized. More formally, we prove a theorem that holds when training on the whole dataset D in a full-batch regime. For details about the proof, and an extensive discussion about time complexity of BP, PC, and iPC, we refer to the supplementary material.\n\nTheorem 1 Let M and M ′ be two equivalent networks with L layers trained on the same dataset. Let M be trained using BP, and M ′ be trained using iPC. Then, the time complexity needed to perform one full update of the weights is O(1) for iPC and O(L) for BP.\n\n3.2 CPU IMPLEMENTATION\n\nTo further provide evidence of the efficiency of iPC with respect to BP, we have implmented the parallelization of iPC on a CPU, and compared it to BP, also implemented on CPU. We compute the time in milliseconds (ms) needed to perform one weight update of both on a randomly generated datapoint. In Fig. 3, we have plotted the ratio\n\nms of iPC / ms of BP\n\nfor architectures with different depths and widths. The results show that our naive implementation adds a computational overhead given by communication and synchronization across threads that makes iPC slower than BP on small\n\nFigure 3: Ratio of the actual running time needed to perform a single weight update between BP and iPC on a CPU. Every dot represents a model, if the model lies below the horizontal line with label 100, its weight update performed using iPC is faster than one performed using BP.\n\n6\n\n Hidden Dim. 81632641282565121024iPC / BPUnder review as a conference paper at ICLR 2023\n\nTable 1: Final accuracy of BP, PC, and iPC on different architectures trained with different datasets.\n\nBP/Z-IL\n\nPC\n\niPC\n\n98.26% ± 0.12% 98.55% ± 0.14% 98.54% ± 0.86% MLP on MNIST MLP on FashionMNIST 88.54% ± 0.64% 85.12% ± 0.75% 89.13% ± 0.86% 95.35% ± 1.53% 94.53% ± 1.54% 96.45% ± 1.04% CNN on SVHN 69.34% ± 0.54% 70.84% ± 0.64% 72.54% ± 0.93% CNN on CIFAR-10 75.64% ± 0.64% 64.63% ± 1.55% 72.42% ± 0.53% AlexNet on CIFAR-10\n\narchitectures (hidden dimension ≤ 64). However, this difference is inverted in large networks: in the most extreme case, one weight update on a network with 32 hidden layers and 1024 parameters per layer using iPC is 10 times faster than that using BP. This is still below the result of Theorem 1 due to the large overhead introduced in our implementation.\n\n4 CLASSIFICATION EXPERIMENTS\n\nWe now demonstrate that iPC shows a similar level of generalization quality compared to BP. We test the performance of iPC on different benchmarks. Since we focus on generalization quality in this section, all methods are run until convergence, and we have used early stopping to pick the best performing model. These experiments were performed using multi-batch training. In this case, we lose our advantage in efficiency over BP, as we need to recompute the error every time a new batch is presented. However, the proposed algorithm is still much faster than the original formulation of PC, and yields a better classification performance.\n\nSetup of experiments: We investigate image classification benchmarks using PC, iPC, and BP. We first trained a fully connected network with 2 hidden layers and 64 hidden neurons per layer on the MNIST dataset (LeCun & Cortes, 2010). Then, we trained a mid-size CNN with three convolutional layers with 64 − 128 − 64 kernels followed by two fully connected layers on FashionMNIST, the Street View House Number (SVHN) dataset (Netzer et al., 2011), and CIFAR10 (Krizhevsky et al., 2012) with no data augmentation. Finally, we trained AlexNet (Krizhevsky et al., 2012), a large-scale CNN, on CIFAR10. To make sure that our results are not the consequence of a specific choice of hyperparameters, we performed a comprehensive grid-search on hyperparameters, and reported the highest accuracy obtained. The search is further made robust by averaging over 5 seeds. Particularly, we tested over 8 learning rates (from 0.000001 to 0.01), 4 values of weight decay (0.0001, 0.001, 0.01, 0.1), and 3 values of the integration step γ (0.1, 0.5, 1.0), and each combination of hyperparameters are evaluated with 5 seeds with mean and standard error reported. To conclude, we have used no data augmentation in the experiments.\n\nResults: In Table 1, iPC outperforms BP in all the small- and medium-size architectures. For the simplest framework (MNIST on a small MLP), PC outperforms all the other training methods, with iPC following by a tiny margin (0.01%). However, PC fails to scale to more complex problems, where it gets outperformed by all the other training methods. The performance of iPC, on the other hand, is stable under changes in size, architecture, and dataset. In fact, iPC reaches a slightly better accuracy than BP on most of the considered tasks.\n\nTable 2: Change of final accuracy when increasing the width.\n\nC\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n67.92 BP iPC 70.61\n\n71.23 74.12\n\n71.65 74.91\n\n72.64 75.88\n\n73.35 76.61\n\n73.71 77.04\n\n74.19 77.48\n\n74.51 77.41\n\n10\n\n74.62 76.51\n\n15\n\n75.08 76.55\n\n20\n\n75.51 76.12\n\nChange of width: Table 1 shows that iPC performs better on a standard CNN than on AlexNet, which has many more parameters and maxpooling layers. To investigate how iPC behaves when adding max-pooling layers and increasing the width, we trained a CNN with three convolutional layers (8, 16, 8) and maxpools, followed by a fully connected layer (128 hidden neurons) on CIFAR10. We have also replicated the experiment by increasing the width of the network by multi-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Robustness of BP and iPC under distribution shift (AlexNet on CIFAR10 under five different intensities of the corruptions rotation, Gaussian blur, Gaussian noise, hue, brightness, and contrast). Left: Comparable decline of model accuracy between BP and iPC. Right: iPC maintains model calibration significantly better than BP under distribution shift.\n\nplying every hidden dimension by a constant C, (e.g., C = 3 means a network with 3 convolutional layers (24, 48, 24), each followed by a maxpool, and a fully connected one (384 hidden neurons)). The results in Table 2 show that iPC (i) outperforms BP under each parametrization, (ii) needs less parameters to obtain good results, but (iii) sees its performance decrease, once it has reached a specific parametrization. This is in contrast to BP, which is able to generalize well even when extremely overparametrized. This suggests that iPC is more efficient than BP in terms of the number of parameters, but that finding the best parameters for iPC may need some extra tuning.\n\n4.1 ROBUSTNESS AND CALIBRATION\n\nRobustness and uncertainty quantification in deep learning have become a topic of increasing interest in recent years. While neural networks trained via BP reach a strong model performance, their lack of explainability and robustness has been widely studied (Abdar et al., 2021; Ovadia et al., 2019). Recently, it has been noted that treating classifiers as generative energy-based models benefits the robustness of the model (Grathwohl et al., 2019). As PC is precisely an energy-based classifier, originally developed for generation tasks, we postulate that iPC possesses better robustness and calibration characteristics than BP. Calibration describes the degree to which predicted logits matches the empirical distribution of observations given the prediction confidence. One may use a calibrated model’s output to quantify the uncertainty in its predictions and interpret it as probability—not just model confidence. Let ˆP be our random prediction vector indicating the model’s confidence that the prediction ˆY is correct. We say ˆP is well-calibrated, if the model confidence matches the model performance, i.e., P( ˆY = Y | ˆP = p) = p (Guo et al., 2017). We measure the deviation from calibration using the adaptive expected calibration error (AdaECE), which estimates E[|P( ˆY = Y | ˆP = p) − p|] (Nguyen & O’Connor, 2015). In recent years, it has become well-known that neural networks trained with BP tend to be overconfident in their predictions (Guo et al., 2017) and that miscalibration increases dramatically under distribution shift (Ovadia et al., 2019). More details on the experiments are in the supplementary material.\n\nResults: Our results are shown in Fig. 4. The boxplots indicate the distributions of accuracy (left) and calibration error (right) over various forms of data corruption with equal levels of intensity. We find that the discriminative performance of the BP and iPC models are comparable under distribution shift. Both models keep a reasonable classification performance for mild corruptions, but show accuracies going down to chance performance under extreme corruptions. The calibration of model output, however, differs strongly: The iPC-trained model yields better calibrated outputs and is able to signal its confidence a lot better. This is essential for using the model output as indication of uncertainty. On in-distribution data, we observe that iPC yields an average calibration error of 0.05, whereas BP yields 0.12. Moreover, we observe that the increase in calibration error is a lot weaker for iPC: The median calibration error of the iPC model is lower across all levels of shift intensities compared to that of BP for the mildest corruption. Furthermore, iPC displays better calibration up to level 3 shifts than BP does on in-distribution data. This has potentially a strong impact of applying either method in safety-critical applications.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5 RELATED WORKS\n\nSeveral previous research efforts aim to achieve supervised learning in a biologically plausible way. One is to explore how the error can be encoded differently than in BP where the error is not encoded locally. One of the earliest works was to use a second set of “error” neurons that can act as the feedback variables (encoding error in BP) (Stork, 1989; Schwartz, 1993). Another promising assumption is that the error can be represented in neurons’ dendrites (K ̈ording & K ̈onig, 2001; 2000; Richards & Lillicrap, 2019; Sacramento et al., 2018). Such efforts are unified in (Lillicrap et al., 2020), with a broad range of works (Pineda, 1987; 1988; O’Reilly, 1996; Ackley et al., 1985; Hinton et al., 1995; Bengio, 2014; Lee et al., 2015) encoding the error term in activity differences.\n\nNeuroscience-inspired algorithms have recently gained the attention of the machine learning community, due to interesting properties such as locality, autonomy and their energy-based formulation. To this end, multiple works have used PC to tackle machine learning problems, from generation tasks (Ororbia & Kifer, 2020), to image classification on complex datasets such as ImageNet (He et al., 2016), associative memories (Salvatori et al., 2021), continual learning (Ororbia et al., 2020), and NLP (Pinchetti et al., 2022). There is a more theoretical line of work that is related to the free energy principle and active inference (Friston, 2008; 2010; Friston et al., 2006; 2016), which aims to model learning, perception, and behavior as an imperative to minimize a free energy. While being initially a theoretical framework, it has been used in multiple applications in fields such as control theory (Baltieri & Buckley, 2019; Friston, 2011) and reinforcement learning (Friston et al., 2009). To conclude, it is important to note that iEM is not the only formulation that improves the efficiency of the original EM, as some other variations have been proposed, such as an online version (Capp ́e & Moulines, 2009), a stochastic one (Chen et al., 2018), or a newer incremental version (Karimi et al., 2019) inspired by the SAGA algorithm (Defazio et al., 2014).\n\n6 DISCUSSION\n\nIn this paper, we have proposed a biologically inspired learning rule, called incremental predictive coding (iPC) motivated by the incremental EM algorithm. iPC enables all the computations to be executed simultaneously, locally, and autonomously, and has theoretical convergence guarantees in non-asymptotic time (Karimi et al., 2019). This allows a solid gain in efficiency compared to the original formulation of PC as well as BP in the full-batch case, as shown with extensive experiments, with no drawbacks in the converging to a minimum of the loss. This is confirmed by the good experimental results in terms of accuracy and robustness in classification tasks.\n\nAn interesting aspect worth discussing, is the time step that triggers the weight update in the three variations of PC: the original formulation, Z-IL, and, now, iPC. The first method updates the parameters only in the last step of the inference, when the neural activities have converged. This has interesting theoretical properties, as it has been shown to simulate how learning is performed in multiple models of cortical circuits, as its credit assignment converges to an equilibrium called prospective configuration (Song et al., 2022). The second, Z-IL, shows that it suffices to time the updates at different levels of the hierarchy in different moments of the inference, to exactly replicate the update given by BP on any possible neural network (Song et al., 2020; Salvatori et al., 2022b). This is interesting, as it connects PC, a theory developed to model credit assignment in the brain, to BP, a method developed to train deep learning models. Our newly proposed iPC, on the other hand, updates the parameters continuously, resulting in great gains in terms of efficiency, and no apparent loss in terms of performance. Future work will investigate whether there are better variations of iPC, or whether the optimal update rule can be learned with respect to specific tasks and datasets. Again, the answer may lie in some variations of the EM algorithm, such as dynamical EM (Anil Meera & Wisse, 2021; Friston et al., 2008), or in an implementation of precision-weighted prediction errors, as in (Jiang & Rao, 2022).\n\nOn a broader level, this work shrinks the gap between computational neuroscience and machine intelligence by tackling the problem of the computational efficiency of neuroscience-inspired training algorithms. Advances in this direction are also interesting from the perspective of hardware implementations of deep learning on energy-based chips, such as analog and quantum computers. In this case, iPC is an interesting improvement, as it is still not known how external control can be implemented on these chips, and hence algorithms able to train neural networks in a fully automatic fashion may play an important role in the future.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243–297, 2021.\n\nMohammed. Abdelghani, Timothy. Lillicrap, and Douglas Tweed. Sensitivity derivatives for flexible\n\nsensorimotor learning. Neural Computation, 20(8):2085–2111, 2008.\n\nDavid Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann\n\nmachines. Cognitive Science, 9(1):147–169, 1985.\n\nAjith Anil Meera and Martijn Wisse. Dynamic expectation maximization algorithm for estimation\n\nof linear systems with colored noise. Entropy, 23(10):1306, 2021.\n\nManuel Baltieri and Chris Buckley. PID control as a process of active inference with linear genera-\n\ntive models. Entropy, 2019.\n\nYoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target\n\npropagation. arXiv:1407.7906, 2014.\n\nRafal Bogacz. A tutorial on the free-energy framework for modelling perception and learning.\n\nJournal of Mathematical Psychology, 76:198–211, 2017.\n\nChris Buckley, Chang Kim, Simon McGregor, and Anil Seth. The free energy principle for action\n\nand perception: A mathematical review. Journal of Mathematical Psychology, 2017.\n\nOlivier Capp ́e and Eric Moulines. On-line expectation–maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593–613, 2009.\n\nJianfei Chen, Jun Zhu, Yee Whye Teh, and Tong Zhang. Stochastic expectation maximization with\n\nvariance reduction. Advances in Neural Information Processing Systems, 31, 2018.\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big selfsupervised models are strong semi-supervised learners. 34th Conference on Neural Information Processing Systems, NeurIPS, 2020.\n\nFrancis Crick. The recent excitement about neural networks. Nature, 337(6203):129–132, 1989.\n\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. Advances in Neural Information Processing Systems, 27, 2014.\n\nArthur Dempster, Nan Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1): 1–22, 1977.\n\nKarl Friston. Learning and inference in the brain. Neural Networks, 16(9):1325–1352, 2003.\n\nKarl Friston. A theory of cortical responses. Philosophical Transactions of the Royal Society B:\n\nBiological Sciences, 360(1456), 2005.\n\nKarl. Friston. Hierarchical models in the brain. PLoS Computational Biology, 2008.\n\nKarl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11\n\n(2):127–138, 2010.\n\nKarl Friston. What is optimal about motor control? Neuron, 2011.\n\nKarl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of\n\nPhysiology, 2006.\n\nKarl Friston, Jean. Daunizeau, and Stephan. Kiebel. Reinforcement learning or active inference?\n\nPloS One, 2009.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKarl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni Pezzulo, et al.\n\nActive inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879, 2016.\n\nKarl J. Friston, N. Trujillo-Barreto, and Jean Daunizeau. DEM: A variational treatment of dynamic\n\nsystems. Neuroimage, 41(3):849–885, 2008.\n\nWill Grathwohl, Kuan-Chieh Wang, J ̈orn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.\n\nStephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cog-\n\nnitive Science, 11(1):23–63, 1987.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\n\nnetworks. ICML 2017, 3:2130–2143, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n\nnition. 2016.\n\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and Radford M. Neal. The” wake-sleep” algo-\n\nrithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995.\n\nLinxing Preston Jiang and Rajesh P. N. Rao. Dynamic predictive coding: A new model of hierar-\n\nchical sequence learning and prediction in the cortex. bioRxiv, 2022.\n\nBelhal Karimi, Hoi-To Wai, Eric Moulines, and Marc Lavielle. On the global convergence of (fast) incremental expectation maximization methods. Advances in Neural Information Processing Systems, 32, 2019.\n\nJack Kendall, Ross Pantone, Kalpana Manickavasagam, Yoshua Bengio, and Benjamin Scellier. Training end-to-end analog neural networks with equilibrium propagation. arXiv preprint arXiv:2006.01981, 2020.\n\nKonrad P. K ̈ording and Peter K ̈onig. Learning with two sites of synaptic integration. Network:\n\nComputation in Neural Systems, 11, 2000.\n\nKonrad P. K ̈ording and Peter K ̈onig. Supervised and unsupervised learning with two sites of synaptic\n\nintegration. Journal of Computational Neuroscience, 11(3):207–215, 2001.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In 26th Annual Conference on Neural Information Processing Systems (NIPS) 2012, 2012.\n\nYann LeCun and Corinna Cortes. MNIST handwritten digit database. The MNIST Database, 2010.\n\nURL http://yann.lecun.com/exdb/mnist/.\n\nDong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.\n\nIn Proc. ECMLPKDD, 2015.\n\nTimothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7 (1):1–10, 2016.\n\nTimothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Back-\n\npropagation and the brain. Nature Reviews Neuroscience, 2020.\n\nBeren Millidge, Alexander Tschantz, and Christopher L Buckley. Predictive coding approximates\n\nbackprop along arbitrary computation graphs. arXiv:2006.04182, 2020.\n\nBeren Millidge, Tommaso Salvatori, Yuhang Song, Rafal Bogacz, and Thomas Lukasiewicz. PrearXiv preprint\n\ndictive coding: Towards a future of deep learning beyond backpropagation? arXiv:2202.09467, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Mumford. On the computational architecture of the neocortex. Biological Cybernetics, 66\n\n(3):241–251, 1992.\n\nRadford M. Neal and Geoffrey E. Hinton. A view of the EM algorithm that justifies incremental,\n\nsparse, and other variants. In Learning in graphical models, pp. 355–368. Springer, 1998.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nKhanh Nguyen and Brendan T. O’Connor. Posterior calibration and exploratory analysis for natural\n\nlanguage processing models. In EMNLP, 2015.\n\nRandall C O’Reilly. Biologically plausible error-driven learning using local activation differences:\n\nThe generalized recirculation algorithm. Neural Computation, 8(5):895–938, 1996.\n\nAlex Ororbia and Daniel Kifer. The neural coding framework for learning generative models.\n\narXiv:2012.03405, 2020.\n\nAlexander Ororbia, Ankur Mali, C. Lee Giles, and Daniel Kifer. Continual learning of recurrent IEEE Transactions on Neural\n\nneural networks by locally aligning distributed representations. Networks and Learning Systems, 31(10):4267–4278, 2020.\n\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nLuca Pinchetti, Tommaso Salvatori, Beren Millidge, Yuhang Song, Yordan Yordanov, and Thomas Lukasiewicz. Predictive coding beyond gaussian assumptions. 36th Conference on Neural Information Processing Systems, 2022.\n\nFernando J. Pineda. Generalization of back-propagation to recurrent neural networks. Physical\n\nReview Letters, 59(19):2229, 1987.\n\nFernando J. Pineda. Dynamics and architecture for neural computation. Journal of Complexity, 4\n\n(3):216–245, 1988.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nRajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1):79–87, 1999.\n\nBlake A. Richards and Timothy P. Lillicrap. Dendritic solutions to the credit assignment problem.\n\nCurrent Opinion in Neurobiology, 54:28–36, 2019.\n\nPieter R. Roelfsema and Anthony Holtmaat. Control of synaptic plasticity in deep cortical networks.\n\nNature Reviews Neuroscience, 19(3):166, 2018.\n\nJo ̃ao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits approximate the backpropagation algorithm. In Advances in Neural Information Processing Systems, pp. 8721–8732, 2018.\n\nTommaso Salvatori, Yuhang Song, Yujian Hong, Lei Sha, Simon Frieder, Zhenghua Xu, Rafal BoIn Advances in\n\ngacz, and Thomas Lukasiewicz. Associative memories via predictive coding. Neural Information Processing Systems, volume 34, 2021.\n\nTommaso Salvatori, Luca Pinchetti, Beren Millidge, Yuhang Song, Tianyi Bao, Rafal Bogacz, Learning on arbitrary graph topologies via predictive coding.\n\nand Thomas Lukasiewicz. arXiv:2201.13180, 2022a.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTommaso Salvatori, Yuhang Song, Zhenghua Xu, Thomas Lukasiewicz, and Rafal Bogacz. Reverse differentiation via predictive coding. In Proceedings of the 36th AAAI Conference on Artificial Intelligence. AAAI Press, 2022b.\n\nBenjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-\n\nbased models and backpropagation. Frontiers in Computational Neuroscience, 11:24, 2017.\n\nEric L. Schwartz. Computational Neuroscience. Mit Press, 1993.\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529, 2016.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550, 2017.\n\nYuhang Song, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Can the brain do backpropagation? — Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\nYuhang Song, Beren Gray Millidge, Tommaso Salvatori, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Inferring neural activity before plasticity: A foundation for learning beyond backpropagation. bioRxiv, 2022.\n\nDavid G. Stork. Is backpropagation biologically plausible. In International Joint Conference on\n\nNeural Networks, volume 2, pp. 241–246. IEEE Washington, DC, 1989.\n\nJames C. R. Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity. Neural Computation, 29(5), 2017.\n\nJames C. R. Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends\n\nin Cognitive Sciences, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Standard and dendritic neural implementation of predictive coding. The dendritic implementation makes use of interneurons il = Wlxl (according to the notation used in the figure). Both implementations have the same equations for all the updates, and are thus equivalent; however, dendrites allow a neural implementation that does not take error nodes into account, improving the biological plausibility of the model. Figure taken and adapted from (Whittington & Bogacz, 2019).\n\nA A DISCUSSION ON BIOLOGICAL PLAUSIBILITY\n\nIn this section, we discuss the biological plausibility of the proposed algorithm, a topic overlooked in the main body of this paper. In the literature, there is often a disagreement on whether a specific algorithm is biologically plausible or not. Generally, it is assumed that an algorithm is biologically plausible when it satisfies a list of properties that are also satisfied in the brain. Different works consider different properties. In our case, we consider as list of minimal properties that include local computations and lack of a global control signals to trigger the operations. Normally, predictive coding networks take error nodes into account, often considered implausible from the biological perspective (Sacramento et al., 2018). Even so, the biological plausibility of our model is not affected by this: it is in fact possible to map PC on a different neural architecture, in which errors are encoded in apical dendrites rather than separate neurons (Sacramento et al., 2018; Whittington & Bogacz, 2019). Graphical representations of the differences between the two implementations can be found in Fig. 5, taken (and adapted) from (Whittington & Bogacz, 2019). Furthermore, our formulation is more plausible than the original formulation of PC, as it is able to learn without the need of external control signals that trigger the weight update.\n\n14\n\nArtificial Neural NetworkBiological Neural Network(b)Dendritic lError NodesInterneuronsSynaptic WeightsStandardUnder review as a conference paper at ICLR 2023\n\nB PSEUDOCODES OF Z-IL AND PC\n\nAlgorithm 2 Learning a dataset D = {yi} with PC. 1: Require: For every i, x(0) 2: for t = 0 to T do 3: 4:\n\nFor every i and l, update x(l) to minimize F via Eq.(7) if t = T then\n\nis fixed to yi,\n\ni\n\nFor every l update each θ(l) to minimize F via Eq. (8)\n\nend if\n\n5: 6: end for\n\nAlgorithm 3 Learning one training pair (sin, sout) with Z-IL\n\nfor each level l do\n\n0 is fixed to sout.\n\n0 is fixed to sin, x0\n\n1: Require: xL 2: Require: x(l) = μ(l) for l ∈ {1, ..., L−1}, and t = 0. 3: for t = 0 to T do 4: 5: 6: 7: 8: 9:\n\nUpdate x(l) to minimize F via Eq.(7) if t = l then\n\nUpdate θ(l) to minimize F via Eq.(8).\n\nend for\n\nend if\n\n10: end for\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Theoretical Efficiency of PC, Z-IL, BP, and iPC.\n\nOne inference step\n\nPC\n\nZ-IL\n\nBP\n\niPC\n\nNumber of MMs per weight update Number of SMMs per weight update\n\n(2L − 1) 2\n\n(2L − 1)T 2T\n\n(2L − 1)(L − 1) 2(L − 1)\n\n(2L − 1) (2L − 1)\n\n(2L − 1) 2\n\nC ON THE EFFICIENCY OF PC, BP, AND IPC\n\nIn this section, we discuss the time complexity and efficiency of PC, BP, Z-IL, and iPC. We now start with the first three, and introduce a metric that we use to compute such complexity. This metric is the number of simultaneous matrix multiplications (SMMs), i.e., the number of non-parallelizable matrix multiplications needed to perform a single weight update. It is a reasonable approximation of running time, as multiplications are by far the most complex operation (≈ O(N 3)) performed by the algorithm.\n\nC.1 COMPLEXITY OF PC, BP, AND Z-IL\n\nSerial Complexity: To complete a single update of all weights, PC and Z-IL run for T and (L − 1) inference steps, respectively. To study the complexity of the inference steps we consider the number of matrix multiplications (MMs) required for each algorithm: One inference step requires (2L − 1) MMs: L for updating all the errors, and (L − 1) for updating all the value nodes (Eq. equation 6). Thus, to complete one weight update, PC and Z-IL require (2L − 1)T and (2L − 1)(L − 1) MMs, respectively. Note also that BP requires (2L − 1) MMs to complete a single weight update: L for the forward, and (L − 1) for the backward pass. These numbers are summarized in the first row of Table 3. According to this measure, BP is the most efficient algorithm, Z-IL ranks second, and PC third, as in practice T is much larger than L. However, this measure only considers the total number of matrix multiplications needed, without considering whether some of them can be performed in parallel, which could significantly reduce the time complexity. We now address this problem.\n\nParallel complexity: The MMs performed during inference can be parallelized across layers. In fact, computations in Eq. equation 6 are layer-wise independent, thus L MMs that update all the error nodes take the time of only one MM if properly parallelized. Similarly, in Eq. equation 6, (L − 1) MMs that update all the value nodes take the time of only one MM if properly parallelized. As a result, one inference step only takes the time of 2 MMs if properly parallelized (since, as stated, it consists of updating all errors and values via Eq. equation 6). Thus, one inference step takes 2 SMMs; one weight update with PC and Z-IL takes 2T and 2(L − 1) SMMs, respectively. Since no MM can be parallelized in BP (the forward pass in the network and the backward pass of error are both layer-dependent), before performing a single weight update, (2L − 1) SMMs are required. These numbers are summarized in the second row of Table 3. Overall, measured over SMMs, BP and Z-IL are equally efficient (up to a constant factor), and faster than PC.\n\nC.2 COMPLEXITY OF IPC\n\nTo complete one weight update, iPC requires one inference step, thus (2L − 1) MMs or 2 SMMs, as also demonstrated in the last column of Table 3. Compared to BP, iPC takes around L times less SMMs per weight update, and should hence be significantly faster in deep networks. Intuitively, this is because matrix multiplications in BP have to be done sequentially along layers, while the ones in iPC can all be done in parallel across layers (Fig. 6). More formally, we have the following theorem, which holds when performing full-batch training:\n\nTheorem 1. Let M and M ′ be two equivalent networks with L layers trained on the same dataset. Let M (resp., M ′) be trained using BP (resp., iPC). Then, the time complexity measured by SMMs needed to perform one full update of the weights is O(1) and O(L) for iPC and BP, respectively.\n\nProof. Consider training on an MLP with L layers, and update weights for multiple times on a single datapoint. Generalizations to multiple datapoints and multiple mini-batches are similar and will be provided after. We first write the equations needed to be computed for iPC to produce one weight\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nupdate:\n\nx(L)\n\ni\n\ni,t = sin nl−1 (cid:88)\n\nˆx(l)\n\ni,t =\n\nand x(0)\n\ni,t = sout\n\ni\n\nθ(l+1)\n\ni,j\n\nf (x(l+1)\n\nj,t\n\n)\n\nfor l ∈ {1, . . . , L}\n\n(9)\n\nj=1\n\ni,t = x(l) ε(l)\n\ni,t − ˆx(l)\n\ni,t\n\ni,t+1 = x(l) x(l)\n\ni,t + γ ·\n\n −ε(l)\n\ni,t + x(l)\n\ni,t\n\nfor l ∈ {1, . . . , L}\n\n\n\nε(l+1) k,t θ(l)\n\nk,i\n\n for l ∈ {1, . . . , L}\n\nn(l+1) (cid:88)\n\nk=1\n\ni,j,t+1 = θ(l) θ(l)\n\ni,j,t − α · ε(l+1)\n\ni,t\n\nf (x(l) j,t)\n\nfor l ∈ {1, . . . , L}.\n\n(10)\n\n(11)\n\nWe then write the three equations needed to be computed for BP to produce one weight update:\n\nx0\n\ni\n\ni,t = sin nl−1 (cid:88)\n\nx(l)\n\ni,t =\n\nθ(l+1)\n\ni,j\n\nf (x(l+1)\n\nj,t\n\n) for l ∈ {1, . . . , L}\n\nj=1\n\nε(L) i,t = sout\n\ni − x(L)\n\ni,t\n\ni,t = f ′ (cid:16) ε(l)\n\nx(l)\n\ni,t\n\n(cid:17) n(l+1) (cid:88)\n\nk=1\n\nk,t θ(l) ε(l+1)\n\nk,i for l ∈ {L, . . . , 1}\n\ni,j,t+1 = θ(l) θ(l)\n\ni,j,t − α · ε(l+1)\n\ni,t\n\nf (x(l)\n\nj,t) for l ∈ {1, . . . , L}.\n\n(12)\n\n(13)\n\nFirst, we notice that the matrix multiplication (MM) is the most complex operation. Specifically, for two adjacent layers with the size of nl and nl, the complexity of MM is O(nlnl), but the maximal complexity of the other operations is O(max nl, nl). In the above equations, only equations with MM are numbered, which are the equations that we investigate in our complexity analysis.\n\nEq. equation 9 for iPC takes L MMs, but one SMM, since the the for-loop for l ∈ {1, . . . , L} can run in parallel for different l. This is further because the variables on the right side of Eq. equation 9 are immediately available. Differently, Eq. equation 12 for iPC takes L MMs, and also L SMMs, since the for-loop for l ∈ {1, . . . , L} has to be executed one after another, following the specified order {2, . . . , L}. This is further because the qualities on the right side of Eq. equation 12 are immediately available, but require to solve Eq. equation 12 again for another layer. That is, to get x(L)\n\ni,t , Eq. equation 12 has to be solved recursively from l = 1 to l = L.\n\nSimilar sense applies to the comparison between Eqs. equation 10 and equation 13. Eq. equation 10 for iPC takes L − 1 MMs but 1 SMMs; Eq. equation 13 for BP takes L − 1 MMs and also L − 1 SMMs.\n\nOverall, Eqs. equation 9 and equation 10 for iPC take 2L − 1 MMs but 2 SMMs; Eqs. equation 12 and equation 13 for BP take 2L − 1 MMs and also 2L − 1 SMMs. Then, the time complexity measured by SMMs needed to perform one full update of the weights is O(1) and O(L) for iPC and BP, respectively.\n\nC.3 EFFICIENCY ON ONE DATA POINT\n\nTo make the difference more visible and provide more insights, we explain this in detail with a sketch of this process on a small network in Fig. 6, where the horizontal axis of m is the time step measured by simultaneous matrix multiplications (SMMs), i.e., within a single m, one can perform one matrix multiplication or multiple ones in parallel; if two matrix multiplications have to be executed in order (e.g., the second needs results from the first), they will need to be put into\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Graphical PClustration of the efficiency over backward SMMs of BP and iPC on a 3-layer network. iPC never clears the error (red neurons), while BP clears it after every update. This allows iPC to perform 5 full and 2 partial updates of the weights in the first 6 SMMs. In the same time frame, BP only performs 3 full updates. Note that the SMMs of forward passes are excluded for simplicity, w.l.o.g., as the insight from this example generalizes to the SMMs of the forward pass.\n\ntwo steps of m. Note that we only consider the matrix multiplications for the backward pass, i.e., the matrix multiplications that backpropagate the error of a layer from an adjacent layer for BP and the inference of Eq. equation 6 for iPC, thus the horizontal axis m is strictly speaking “Backward SMM”. The insight for the forward pass is similar as that of the backward pass. As it has been said, for BP, backpropagating the error from one layer to an adjacent layer requires one matrix multiplication; for iPC, one step of inference on one layer via Eq. equation 6 requires one matrix multiplication. BP and iPC are presented in the first and second rows, respectively. Before both methods are able to update weights in all layers, they need two matrix multiplications for spreading the error through the network, i.e., a weights update of all layers occurs for the first time at m = 2 for both methods. After m = 2, BP cleared all errors on all neurons, so at m = 3, BP backpropagates the error from l = 0 to l = 1, and at m = 4, BP backpropagates the error from l = 1 to l = 2 after which it can make an update of weights at all layers again for the second time. Note that the matrix multiplication that backpropagates errors from l = 1 to l = 2 at m = 4 cannot be put at m = 3, as it requires the results of the matrix multiplication at m = 3, i.e., it requires the error to be backpropagated to l = 1 from l = 0 at m = 3. However, this is different for iPC. After m = 2, iPC does not reset xl i,t. At m = 3, iPC performs two matrix multiplications in parallel, corresponding to two inferences steps at two layers, l = 1 and l = 2, updating xl i,t of these two layers. Note that the above two matrix multiplications of two inference steps can run in parallel and be put into a single m, as inference requires only locally and immediately available information. In this way, a weight update in iPC is able to be performed at every m ever since the very first few steps of m.\n\ni,t, and hence the error signals are held in εl\n\ni,t, i.e., the error signals are still held in εl\n\ni,t to μl\n\nD TRAINING DETAILS\n\nWe now list some additional details to reproduce our results.\n\nD.1 EXPERIMENTS OF EFFICIENCY\n\nThe experiments for the efficiency of generative models were run on fully connected networks with 128, 256 or 512 hidden neurons, and L ∈ {4, 5}. Every network was trained on CIFAR10 or Tiny Imagenet with learning rates α = 0.00005 and γ = 0.5, and T ∈ {8, 12, 16}. The experiments on discriminative models are performed using networks with 64 hidden neurons, depth L ∈ {3, 4, 6}, and learning rates α = 0.0001 and γ = 0.5. The networks trained with BP have the same learning rate α. All the plots for every combination of hyperparameters can be found in Figures 8 and 7.\n\n18\n\nInput neuronHidden neuron (error not updated)Hidden neuron (error updated)Weights (not updated)Weights (updated)Output neuronm = 1Backward SMMmm = 2m = 3m = 4m = 5m = 6m = 0BPiPCUnder review as a conference paper at ICLR 2023\n\nD.2 EXPERIMENTS OF GENERALIZATION QUALITY\n\nAs already stated in the paper body, to make sure that our results are not the consequence of a specific choice of hyperparameters, we performed a comprehensive grid search on hyperparameters, and reported the highest accuracy obtained, and the search is further made robust by averaging over 5 seeds. Particularly, we tested over 8 learning rates (from 0.000001 to 0.01), 4 values of weight decay (0.0001, 0.001, 0.01, 0.1), and 3 values of the integration step γ (0.1, 0.5, 1.0). We additionally verified that the optimized value of each hyperparameter lies within the searched range of that hyperparameter. As for additional details, we used standard Pytorch initialization for the parameters. For the hardware, we used a single Nvidia GeForce RTX 2080 GPU on an internal cluster. Despite the large search, most of of the best results were obtained using the following hyperparameters: γ = 0.5 (γ = 1 for Alexnet), α = 0.00005.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Efficiency of multiple generative networks trained with PC.\n\n20\n\nHD = 128HD = 256HD = 512L = 4L = 5CIFAR10HD = 128HD = 256HD = 512L = 4L = 5Tiny ImagenetIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsUnder review as a conference paper at ICLR 2023\n\nFigure 8: Efficiency of multiple discriminative networks trained with PC and BP.\n\n21\n\nL = 3L = 4L = 6Train LossTest AccuracyMNISTHD = 128HD = 256HD = 512FashionMNISTSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsTrain LossTest Accuracy",
  "translations": [
    "# Summary Of The Paper\n\nThis paper describes a variant of predictive coding, named incremental predictive coding (iPC), based on incremental EM, which it is argued should be considered a biologically plausible approach to learning in the brain.  The complexity of iPC is considered in relation to back-propagation (BP), and a CPU implementation is provided.  Further, the generalization performance is investigated on a number of datasets, and the algorithm is shown to perform well in comparison to BP and PC.\n\n# Strength And Weaknesses\n\nStrengths:\n\n-  The biological plausibility argument is interesting, and in general the argument is convincing that some form of 'localized EM' algorithm is more plausible than BP or PC alternatives, while retaining convergence and generalization properties.\n-  The experimentation convincingly demonstrates that iPC should be considered a viable alternative to BP generally, at least for simple architectures and specialized hardware.\n\nWeaknesses:\n\n- I'm mainly concerned about the paper's novelty - essentially, iPC is equivalent to iEM applied to a hierarchical Gaussian model.  The theoretical properties are described elsewhere (e.g. Karimi 2019) and the biological plausibility argument is hard to evaluate, although likely to be worth pursuing further.\n- There is little theoretical novelty, since the time-complexity analysis (Theorem 1) essentially follows simply by definition.  As discussed by the authors, the comparison of training-loss convergence rates in terms of 'non-parallel matrix multiplications' is an interesting result, but this is investigated solely empirically (Fig. 2 right).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity, quality and reproducibility are mainly good (I spotted a few typos - for instance, in Eq. 4, the conditional in the second expression should read 'p(x^(l-1) | x^(l))', and the Gaussian formulation in the third expression should include the prior and the x's).  As noted above, the novelty is an issue for me.\n\n# Summary Of The Review\n\nAn interesting investigation of an algorithm that may have relevance in neuroscience, and deserves further attention.  Potentially, the paper may be of interest to those working in neuroscience and optimization.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNone.",
    "# Summary Of The Paper\nThe paper presents Incremental Predictive Coding (iPC), an innovative algorithm that enhances the efficiency of predictive coding (PC) by employing the incremental expectation-maximization (iEM) technique. By allowing parallel processing of the expectation and maximization steps without external control, iPC achieves remarkable efficiency improvements over traditional PC and backpropagation (BP) methods. The authors validate iPC's performance through extensive experiments on classification tasks like MNIST and CIFAR-10, demonstrating faster convergence, lower training loss, and better calibration of outputs compared to existing frameworks.\n\n# Strength And Weaknesses\nThe main strengths of this paper are its clear contributions to algorithmic efficiency in predictive coding and its empirical validation across multiple benchmarks. The introduction of iPC as a parallel and fully automatic learning algorithm is a significant advancement, addressing the inefficiencies associated with traditional PC. However, the paper could benefit from a more detailed discussion of the limitations of iPC, particularly in terms of its scalability to more complex models and datasets beyond those tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, findings, and contributions. The quality of the writing is high, with appropriate technical depth and clarity. Novelty is evident in the introduction of iPC and its theoretical underpinnings; however, while the empirical results are promising, further reproducibility tests on a wider range of datasets and applications would strengthen the claims made.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field by presenting iPC, which combines the biological plausibility of predictive coding with the efficiency of modern machine learning techniques. The experimental validation supports its claims, although further exploration of its limitations and broader applicability could enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel learning algorithm inspired by predictive coding principles. iPC enhances the efficiency of predictive coding by employing an Incremental Expectation-Maximization approach, enabling parallel execution of inference and learning without requiring external control signals. The findings demonstrate that iPC achieves faster convergence and comparable performance to backpropagation (BP) across various datasets, while also exhibiting superior calibration under distribution shifts.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its significant efficiency improvements, achieving \\(O(1)\\) time complexity for weight updates compared to \\(O(L)\\) for BP. This parallelized approach not only reduces computational overhead but also aligns closely with biological learning mechanisms. Additionally, iPC exhibits robust performance across different architectures, particularly in terms of parameter efficiency and calibration. However, the limitations include potential implementation challenges in existing frameworks, the complexity of hyperparameter tuning, and a need for more extensive empirical comparisons against state-of-the-art methods to validate its claimed advantages fully. Furthermore, the practical benefits of iPC may depend on specific hardware capabilities, which introduces variability in its application.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its methodology and findings clearly, making it accessible for both neuroscientists and machine learning practitioners. The quality of the experimental design is commendable, featuring comprehensive comparative analyses across multiple datasets and architectures. The novelty of the iPC algorithm is significant, as it addresses crucial inefficiencies in traditional approaches while enhancing biological plausibility. However, the reproducibility of results may be affected by the current implementation challenges and the complexity involved in optimizing hyperparameters for various tasks.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in predictive coding through the introduction of Incremental Predictive Coding. While it showcases promising efficiency and performance metrics, further validation and more robust implementations are necessary to fully leverage its potential in practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Incremental Predictive Coding (iPC), a novel algorithm that enhances the efficiency of predictive coding (PC) by enabling parallel operations without the need for external control. The authors introduce a theoretical framework based on incremental expectation-maximization (iEM) that allows simultaneous execution of the E- and M-steps, leading to a significant reduction in time complexity from \\(O(L)\\) for backpropagation (BP) to \\(O(1)\\) for iPC. Empirical results demonstrate that iPC not only matches or exceeds BP performance on various image classification tasks but also exhibits superior convergence, efficiency, and calibration under distribution shifts. The findings suggest promising applications in computational neuroscience, machine learning, and neuromorphic computing.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous theoretical foundations and empirical validation, showcasing iPC's efficiency and performance advantages over traditional methods. The introduction of iEM provides a compelling mechanism for addressing the criticisms of PC related to speed and biological plausibility. However, one weakness is the limited exploration of the generalizability of iPC across different architectures and more complex tasks beyond the evaluated benchmarks. While the experiments cover a range of datasets, further investigation into real-world applications and extended neural network architectures would enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical foundations and experimental methodologies. The introduction effectively sets up the problem space, and the subsequent sections logically build upon this foundation. The novelty of iPC lies in its integration of parallel operations within the predictive coding framework, which is a significant deviation from traditional BP methodologies. The reproducibility is supported by the inclusion of pseudocode and detailed descriptions of the experimental setups, allowing other researchers to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the efficiency of predictive coding through the introduction of iPC, which demonstrates promising results against established methods like BP. The theoretical and empirical contributions are well-articulated, although further exploration into its applicability across diverse tasks would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces an incremental predictive coding (iPC) framework that enhances the efficiency of predictive coding by enabling parallel operations without external control. It seeks to address the biological implausibilities associated with traditional backpropagation methods by mimicking neural processes more closely. The findings indicate that iPC achieves performance comparable to backpropagation on image classification tasks while requiring fewer parameters, exhibits improved robustness and calibration under distribution shifts, and presents solid theoretical convergence guarantees. Comprehensive experiments across various datasets support the claims regarding its efficiency and performance.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its potential to improve efficiency in machine learning through iPC, which aligns more closely with biological neural processes. The performance equivalence to backpropagation combined with fewer parameters suggests that iPC could lead to more efficient model designs. Additionally, the improved robustness and calibration of outputs are significant for safety-critical applications. However, limitations include the challenges in realizing theoretical efficiency gains in practical implementations due to existing framework constraints and the need for further validation of robustness in diverse conditions. The generalizability of results is also in question, as the findings are primarily based on specific datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and findings clearly, making it accessible for readers. The quality of the empirical results is high, with comprehensive experiments supporting the claims made. The novelty of the iPC approach is notable, particularly in its alignment with biological plausibility and efficiency. However, reproducibility may be hindered by limitations in existing frameworks, which could affect the implementation of iPC in practical settings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in learning algorithms inspired by neuroscience, showcasing a promising framework that enhances efficiency while addressing biological plausibility. However, further exploration is needed to validate its practical implementation, robustness, and generalizability across various tasks and architectures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Learning (iPL), an innovative framework designed to enhance computational efficiency while retaining the effectiveness of traditional learning methods like backpropagation in neural networks. The authors present a unique methodology based on the Incremental Learning Algorithm (ILA) that enables simultaneous updates of weights and internal states across layers without requiring external control signals. The findings demonstrate that iPL achieves O(1) time complexity for weight updates, significantly improving training efficiency while maintaining or surpassing accuracy in image classification tasks compared to traditional methods.\n\n# Strength And Weaknesses\nThe paper’s primary strengths lie in its methodological innovation and empirical validation. The autonomous learning mechanism of iPL represents a significant advancement over existing predictive coding models, removing the need for iterative convergence and thereby streamlining the training process. Additionally, the experimental results across multiple datasets convincingly illustrate iPL's efficiency and robustness. However, a potential weakness is the lack of extensive comparative analysis with a broader range of state-of-the-art methods, which could further contextualize the significance of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to a broad audience. The quality of the theoretical and experimental analyses is high, with a solid mathematical foundation supporting the claims. The novelty of the framework is apparent, as it bridges theoretical neuroscience with practical machine learning. However, the reproducibility of the results could be better supported by providing additional details regarding experimental setups and hyperparameter choices, which would facilitate independent verification of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of machine learning through the introduction of Incremental Predictive Learning, which enhances both efficiency and performance in neural network training. While the findings are promising, further comparative analyses and additional reproducibility details would strengthen the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents Incremental Adversarial Training (iAT), a novel adversarial training method that leverages principles from the incremental expectation-maximization (EM) framework to improve training efficiency. The authors propose that iAT allows for simultaneous updates of model parameters and adversarial examples, thereby eliminating the need for sequential updates and control signals typical of traditional methods. Empirical experiments demonstrate that iAT not only converges faster but also requires fewer computational resources while achieving robustness comparable to or exceeding that of existing adversarial training techniques.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adversarial training, offering a significant advancement by enabling fully parallel training processes. The strong theoretical foundation based on the incremental EM algorithm enhances the credibility of the proposed method. Additionally, the experiments conducted are comprehensive, effectively showcasing the advantages of iAT. However, the paper has notable weaknesses, including the complexity of practical implementation, which may hinder adoption. Furthermore, a limited scalability analysis raises concerns about how well iAT performs with larger models and datasets, and the paper could benefit from a broader range of experimental validation across various adversarial scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that clearly present the theoretical framework and empirical findings. The quality of the writing is high, making complex concepts accessible. In terms of novelty, iAT represents a significant advancement in adversarial training methodologies. However, the reproducibility of the results could be improved with a more detailed discussion of practical implementation challenges and the specific configurations used in experiments.\n\n# Summary Of The Review\nOverall, this paper offers a substantial contribution to adversarial training through the introduction of iAT, an efficient and parallelized method for generating adversarial examples. While the theoretical and empirical results are compelling, the complexities of implementation and limited scalability analysis suggest areas for further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces **Incremental Predictive Coding (iPC)**, a novel learning algorithm that claims to significantly enhance efficiency in machine learning by eliminating the need for external control mechanisms. iPC is argued to be exponentially more efficient than traditional backpropagation (BP) and original predictive coding (PC), with the authors providing empirical evidence demonstrating that iPC surpasses BP in both training speed and accuracy across various tasks. The theoretical foundation of iPC suggests a time complexity of **O(1)** for weight updates, a substantial improvement over BP's **O(L)**, and the authors claim that iPC's efficiency could lead to broader applications in computational neuroscience and machine learning, particularly in safety-critical domains.\n\n# Strength And Weaknesses\nStrengths of the paper include its ambitious claims regarding the efficiency and robustness of iPC compared to existing methods, as well as its potential for real-time learning in complex applications. The empirical results presented are compelling and indicate significant advantages over traditional methods. However, the weaknesses lie in the dramatic framing of the contributions, which may overstate the actual impact of iPC. The paper does not adequately address potential limitations or drawbacks of the proposed method, and the sweeping claims about its universal applicability may require more rigorous validation in diverse contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly, with a logical flow from the introduction of iPC to its theoretical analysis and empirical validation. However, the novelty of the claims, while intriguing, could benefit from additional empirical data across a wider range of tasks and benchmarks to support the assertions made. Reproducibility is a concern, as the paper does not provide detailed methodology or code, which would be necessary for other researchers to validate the findings independently. \n\n# Summary Of The Review\nIn summary, the paper presents an intriguing new algorithm, iPC, which claims to redefine efficiency in machine learning. While the empirical results are promising and the theoretical foundation appears solid, the dramatic framing of its contributions may overshadow the need for thorough validation and reproducibility in various contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel algorithm called Incremental Predictive Coding (iPC), which enhances the efficiency of the original predictive coding framework proposed by Rao and Ballard in 1999. iPC operates fully automatically and in parallel, achieving performance comparable to backpropagation (BP) in image classification tasks while demonstrating superior theoretical efficiency with O(1) time complexity for weight updates compared to BP's O(L). Empirical evaluations show that iPC converges faster and achieves higher accuracy on benchmark datasets like MNIST and CIFAR-10, while also maintaining better robustness and calibration under distribution shifts.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear theoretical contributions that establish iPC as more efficient than BP, and strong empirical results demonstrating its effectiveness across various datasets and tasks. The maintained calibration and robustness of iPC compared to BP is particularly noteworthy, as it suggests practical advantages in real-world applications. However, the paper could benefit from a more in-depth exploration of the limitations of iPC, including potential scenarios where it may not outperform BP or where its assumptions may not hold. Additionally, the discussion of future work could be expanded to provide clearer pathways for further research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind iPC, its theoretical foundations, and the empirical results supporting its claims. The quality of the writing is high, with a logical flow that guides the reader through the concepts. The novelty of the approach is significant, presenting a biologically plausible alternative to backpropagation. However, while the experiments are well-designed, the reproducibility could be enhanced by providing more details about the datasets, hyperparameters, and implementation specifics, which would allow other researchers to replicate the results more easily.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative framework in the form of Incremental Predictive Coding, demonstrating both theoretical and empirical advantages over traditional backpropagation methods. While the contributions are compelling and well-supported by data, further exploration of limitations and enhanced reproducibility would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), positing it as a more efficient alternative to traditional backpropagation (BP) and standard predictive coding (PC) methods. The authors claim that iPC enhances biological plausibility by eliminating global control signals and enabling simultaneous updates, thus aligning more closely with neural processes. The methodology involves comparing iPC's performance against BP in image classification tasks, suggesting that iPC converges effectively while requiring fewer parameters. However, the authors' assumptions regarding efficiency, convergence, and the generative model framework raise questions about the model's broader applicability and empirical validation.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to learning mechanisms, particularly in terms of biological relevance. The claims regarding iPC's efficiency and convergence are compelling, yet they depend heavily on specific assumptions, such as the ability to leverage ideal parallel processing and the foundational premise of the brain as a generative model. The paper's limitations include its narrow focus on image classification, lack of robustness in addressing convergence in complex environments, and potential oversimplifications in measuring performance metrics. Additionally, the reliance on hyperparameter tuning for reproducibility may undermine the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas in a clear manner, though some technical details could benefit from further elaboration. The innovative aspects of iPC are noteworthy, contributing to the ongoing discourse around biologically inspired learning mechanisms. However, the assumptions made regarding convergence, efficiency, and the nature of learning tasks may detract from the reproducibility and reliability of the findings, especially given the inherent variability in neural network training.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing proposal in the realm of predictive coding, with notable contributions to understanding efficiency and biological plausibility. However, significant assumptions and limitations regarding convergence, generalizability, and reproducibility warrant caution in accepting the findings as definitive.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel variant of predictive coding aimed at improving efficiency in neural network training by allowing parallel operations without the need for external control signals. The authors argue that iPC retains the core advantages of predictive coding while overcoming its limitations, particularly in inference speed. Through extensive image classification experiments, the paper demonstrates that iPC not only matches the performance of backpropagation in various tasks but also exhibits superior calibration and robustness, making it a promising approach for both computational neuroscience and machine learning applications.\n\n# Strength And Weaknesses\nOne of the key strengths of the paper is its innovative approach to enhancing predictive coding by allowing parallel processing, which significantly increases efficiency. The empirical results presented are compelling, showcasing that iPC can achieve performance levels comparable to and often exceeding those of backpropagation on multiple benchmarks. However, a notable weakness is the lack of extensive theoretical analysis to support the claims made about iPC's efficiency advantages. Additionally, while the experimental results are encouraging, further testing across a broader range of tasks and datasets would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with a clear structure that facilitates understanding of the proposed methodology and results. The quality of the experiments and the analyses presented is high, contributing to the credibility of the findings. The novelty of the iPC approach is significant, as it bridges the gap between traditional predictive coding and the efficiency of backpropagation. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter choices used in the classification tasks.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field by introducing iPC, which enhances the efficiency of predictive coding while maintaining comparable performance to backpropagation. The empirical results are promising but would benefit from additional theoretical backing and broader testing. The clarity and organization of the paper are commendable.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel algorithm designed to enhance the efficiency and performance of deep learning models in resource-constrained environments. It introduces a new optimization framework that integrates adaptive learning rates and model pruning techniques to achieve better training speed and reduced model size without significantly compromising accuracy. The authors validate their approach through extensive experiments on standard benchmark datasets, demonstrating that their method outperforms several existing algorithms in terms of both computational efficiency and predictive performance.\n\n# Strengths And Weaknesses\n**Strengths**:\n- **Innovation**: The integration of adaptive learning rates with model pruning is a novel approach that addresses key limitations in current deep learning practices, particularly in scenarios where computational resources are limited.\n- **Theoretical Foundations**: The paper is well-grounded in theory, providing rigorous mathematical proofs that support the efficacy of the proposed algorithm.\n- **Empirical Validation**: The results are compelling, showcasing significant improvements over baseline models, which adds credibility to the claims made by the authors.\n\n**Weaknesses**:\n- **Empirical Validation**: Although the experiments are thorough, they primarily focus on a limited set of datasets. Including a more diverse range of benchmarks would strengthen the generalizability of the findings.\n- **Comparative Analysis**: The paper lacks a comprehensive comparison with the latest state-of-the-art methods. A broader evaluation would provide more context for the performance claims.\n- **Clarity and Presentation**: Certain sections are dense and may benefit from clearer explanations or visual aids to aid reader comprehension, particularly for complex concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is mixed; while the theoretical contributions are well-articulated, some technical sections are overly complex, which may hinder understanding. The methodology is presented in a systematic manner, but the lack of visual aids could detract from the overall quality. The novelty of the approach is high, as it combines established ideas in innovative ways. However, reproducibility could be improved by providing clearer details on the implementation and hyperparameter settings used in experiments.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in optimizing deep learning models for efficiency without sacrificing performance. While the theoretical underpinnings are strong and empirical results are promising, the paper would benefit from broader experimental validation and improved clarity in presentation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Incremental Predictive Coding (iPC), a novel variation of predictive coding (PC) that aims to enhance the efficiency of the original PC algorithm while maintaining its performance in machine learning tasks, specifically in image classification. The methodology involves parallel execution of the estimation (E) and maximization (M) steps of the learning process, eliminating the need for a global control signal, which is a limitation of backpropagation (BP) and traditional PC models. The findings demonstrate that iPC converges faster than both the original PC and BP, showcasing its potential for future applications in computational neuroscience, machine learning, and neuromorphic computing.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to addressing the inefficiencies of traditional predictive coding by introducing a parallel processing mechanism. This not only enhances the speed and autonomy of the learning algorithm but also keeps it aligned with biological plausibility, which is a significant consideration in the field. Additionally, the empirical results support the theoretical claims of improved convergence rates. However, a potential weakness is the limited exploration of the algorithm's performance across various datasets and tasks, which may restrict the generalizability of the findings. Further validation on diverse benchmarks could strengthen the paper’s contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The methodology is presented in a coherent manner, making it accessible for readers with a background in machine learning and computational neuroscience. The novelty of proposing a parallelized version of predictive coding is significant, as it bridges a crucial gap between biological models and practical machine learning applications. However, the reproducibility of the results could be improved by providing more detailed descriptions of the experimental setup and the datasets used for evaluation.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of predictive coding by introducing iPC, which effectively enhances efficiency while retaining performance. While the methodology is promising and well-supported by empirical results, further validation across a broader range of tasks would bolster its significance.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel learning algorithm that enhances the efficiency of traditional predictive coding models while maintaining competitive performance with backpropagation in image classification tasks. The methodology involves the development of an update rule based on incremental expectation-maximization, allowing for continuous updates of neural activities and parameters. The results indicate that iPC outperforms existing predictive coding methods and shows robust accuracy across several datasets, including MNIST, CIFAR10, and FashionMNIST.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to improving the efficiency of predictive coding, addressing a significant limitation in the field. The empirical results convincingly demonstrate that iPC not only enhances convergence speed but also requires fewer parameters than backpropagation, which is particularly valuable for applications in computational neuroscience and neuromorphic hardware. However, a potential weakness is the lack of extensive exploration of the theoretical implications of iPC beyond time complexity, which could provide deeper insights into its broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the motivations, methodology, and findings. The quality of the experiments is high, with appropriate datasets and metrics used for evaluation. The novelty of iPC is significant, particularly in its potential applications in non-Von Neumann architectures. Reproducibility is supported by the detailed description of the methodology; however, the lack of code or supplementary materials limits verification of the results by external researchers.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to predictive coding that effectively enhances efficiency while maintaining competitive performance with backpropagation. The results are promising, with significant implications for both machine learning and computational neuroscience, although further exploration of theoretical aspects and provision of reproducibility materials would strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel learning algorithm designed to address the efficiency limitations of traditional predictive coding models. It presents a parallel and fully automatic approach to learning, enhancing autonomy and convergence in the context of image classification tasks. The authors demonstrate both theoretically and empirically that iPC outperforms the original predictive coding framework and backpropagation in terms of efficiency and lower parameter requirements, showcasing competitive performance across various datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to improving predictive coding through the development of iPC, which is well-justified by theoretical foundations and empirical evidence. The empirical results show that iPC exhibits faster convergence and efficiency compared to conventional methods, addressing a significant gap in the literature regarding the automation of learning algorithms. However, a notable weakness is the limited exploration of practical implications and potential real-world applications of iPC, which may hinder its immediate applicability in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, making complex concepts accessible to the reader. The quality of writing is high, with thorough explanations of the methodology and results. The novelty of iPC is significant, as it presents a fresh perspective on predictive coding and its efficiency. The reproducibility of the results is supported by the inclusion of pseudocode and detailed experimental setups, although additional implementation details could further enhance reproducibility in practical applications.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of machine learning by presenting a novel algorithm that enhances the efficiency of predictive coding. While the theoretical and empirical findings are compelling, further exploration of the practical applications of iPC is needed to fully realize its potential impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel algorithm inspired by predictive coding principles and the incremental expectation maximization (iEM) paradigm. iPC is designed to operate without external supervisory signals, achieving parallelizable learning processes. The authors demonstrate through theoretical and empirical evaluations that iPC outperforms the original predictive coding model and achieves performance metrics comparable to backpropagation in image classification tasks, such as MNIST and CIFAR10, while exhibiting enhanced computational efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to learning without the biological implausibilities associated with backpropagation, such as global control signals and sequential computations. The proposed iPC algorithm efficiently synchronizes inference and learning steps, leading to reduced training times and improved performance metrics. However, a notable weakness is the limited exploration of the algorithm's applicability to more complex tasks beyond image classification, which could provide a broader understanding of its generalizability and robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical foundations of iPC, the methodology employed, and the empirical results achieved. The quality of writing is high, making it accessible to a broad audience. The novelty is significant, as iPC represents a departure from traditional backpropagation methods and aligns more closely with biological learning processes. Reproducibility is facilitated through detailed descriptions of the experimental setup and evaluation metrics, although some specifics regarding implementation details could be elaborated upon.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative alternative to traditional deep learning methods with its introduction of iPC. The empirical results support its claims of efficiency and performance, although further exploration of its applicability to diverse domains would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes an incremental predictive coding (iPC) algorithm as an enhancement over traditional predictive coding (PC). The authors assert that iPC offers improved efficiency compared to backpropagation (BP) in full-batch training scenarios. However, the methodology relies heavily on theoretical assumptions that may not hold in practical applications, particularly with current limitations in frameworks like PyTorch. The findings present iPC as requiring fewer parameters than BP while claiming better-calibrated outputs; however, the empirical validation of these claims is limited and conducted under conditions not fully representative of real-world scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical exploration of an alternative learning algorithm that aims to improve upon existing methods. However, significant weaknesses include a lack of rigorous empirical validation, limited generalizability of results across diverse tasks, and insufficient addressing of practical implementation challenges. The paper's reliance on theoretical constructs without comprehensive testing raises considerable doubts about the claims made regarding the efficiency and advantages of iPC over established algorithms like BP.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper lacks clarity in presenting how iPC can be effectively implemented within existing frameworks, leading to questions regarding its reproducibility. While the concept of iPC possesses some novelty, its practical implications remain untested across varied datasets, which undermines its overall quality. The discussion around the algorithm's advantages, particularly regarding calibration and robustness, does not convincingly establish significant improvements over BP, further complicating the assessment of its novelty.\n\n# Summary Of The Review\nOverall, while the iPC algorithm presents a theoretically intriguing approach to predictive coding, the paper falls short in empirical validation and practical applicability. The claims made regarding efficiency and performance are not sufficiently backed by robust experiments or clear implementation strategies, raising doubts about the algorithm's viability in real-world applications.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents Incremental Predictive Coding (iPC), a novel learning algorithm inspired by neuroscience, aimed at improving the efficiency and effectiveness of machine learning models. The authors propose a fully parallel architecture that enhances training speed and performance compared to traditional backpropagation methods. The empirical results demonstrate that iPC achieves high accuracy in image classification tasks while utilizing fewer parameters, thereby offering a more biologically plausible approach to machine learning. Additionally, iPC supports continuous learning and produces better-calibrated outputs, making it suitable for applications in safety-critical domains.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to learning, which aligns machine learning with biological processes. This could lead to significant advancements in various fields, including computational neuroscience and neuromorphic computing. The empirical validation showcases iPC's efficiency and effectiveness, providing robust evidence of its advantages over traditional methods. However, the paper could benefit from a more detailed discussion of potential limitations and challenges in implementation. Furthermore, while the theoretical foundations are solid, the practical applicability of iPC in real-world scenarios could be explored more thoroughly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, findings, and implications of iPC. The writing quality is high, making it accessible to both experts and non-experts in the field. The novelty of the algorithm is significant, as it combines insights from neuroscience with advanced machine learning techniques. However, reproducibility may be a concern if the implementation details are not fully disclosed or if the empirical results are not easily replicable by other researchers.\n\n# Summary Of The Review\nOverall, the paper introduces an exciting and innovative algorithm, Incremental Predictive Coding, that could reshape our understanding of machine learning by drawing parallels with biological processes. Its efficiency, empirical performance, and potential applications make it a noteworthy contribution to the field, though further exploration of practical challenges and implementation details would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces Incremental Predictive Coding (iPC), a novel learning framework inspired by biological neural processes and grounded in predictive coding theory. The authors present iPC as a theoretically robust model that emphasizes autonomy and locality in learning by eliminating the need for external control signals typically required by backpropagation. The paper details the formulation of iPC as an energy-based model, highlights its efficiency through O(1) weight updates in parallel processing environments, and discusses convergence properties akin to traditional predictive coding. The findings suggest that iPC could enhance robustness and generalization in machine learning tasks, aligning more closely with biological systems.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative theoretical contributions, such as the establishment of iPC as a biologically plausible alternative to conventional learning algorithms. The emphasis on autonomous learning and the detailed theoretical guarantees regarding convergence are particularly noteworthy. However, the paper may benefit from empirical validation of the proposed methods, as the theoretical discussions, while compelling, lack practical demonstrations of effectiveness across diverse scenarios. Furthermore, the implications for real-world applications could be elaborated to provide clearer pathways for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections that present complex theoretical concepts in an accessible manner. The quality of the writing is commendable, although the depth of theoretical discussions may challenge readers less familiar with predictive coding and energy-based models. The novelty of the iPC framework is significant, proposing a fresh approach to learning that is both theoretically sound and biologically inspired. However, reproducibility remains a concern, as the paper does not provide implementation details or empirical results to facilitate verification of the claims.\n\n# Summary Of The Review\nIn summary, this paper presents Incremental Predictive Coding (iPC) as a promising theoretical advancement in machine learning that emphasizes biological plausibility and efficiency. While the theoretical contributions are strong and novel, the lack of empirical validation may limit the immediate applicability and reproducibility of the proposed framework.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel variation of the predictive coding framework that aims to enhance computational efficiency through parallel processing. iPC operates without external control signals for managing computation phases, addressing a key limitation of traditional methods such as backpropagation (BP). The authors derive the update rule for iPC from the variational free energy of a hierarchical generative model using an incremental expectation-maximization (iEM) approach. Experimental results demonstrate that iPC significantly improves convergence speed and training loss reduction compared to traditional predictive coding methods, validated across multiple image classification tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to predictive coding, offering a solution to the inefficiencies associated with the sequential nature of backpropagation. The theoretical claims regarding the time complexity reduction to O(1) for weight updates are compelling and well-supported by empirical evidence. However, a notable weakness is the reliance on CPU for initial experiments, which may not fully capture the potential performance benefits on larger architectures. Additionally, while the paper discusses future work on enhancing parallelization capabilities, it lacks a comprehensive analysis of the practical challenges in implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the iPC algorithm, including algorithmic pseudocode and detailed derivations of the update rules. This clarity enhances the reproducibility of the work, as the authors provide sufficient information for others to replicate the implementation using PyTorch. The novelty of the approach is significant, as it offers a fresh perspective on predictive coding by introducing a parallel processing framework that could potentially transform how these models are utilized in practice.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the predictive coding framework through the introduction of Incremental Predictive Coding. Its methodological innovations and empirical validations showcase its potential impact on efficiency in neural network training. However, the reliance on CPU experiments and the need for further engineering efforts to optimize performance on various hardware setups are points that warrant consideration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces incremental predictive coding (iPC), a novel approach that aims to enhance the efficiency of traditional predictive coding (PC) methods. The authors claim that iPC can achieve comparable performance to backpropagation (BP) in image classification tasks while requiring fewer parameters and enabling better parallelization without external control signals. However, the paper's comparative analysis often presents BP in a biased light, failing to acknowledge its extensive historical optimizations and successes across numerous applications.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical contributions to the understanding of iPC and its potential advantages in terms of efficiency and parameter reduction. The claim that iPC can handle parallel processing more effectively is an interesting point that warrants further exploration. However, the paper exhibits weaknesses in its comparisons to BP, which are not sufficiently nuanced. The authors tend to downplay BP's longstanding effectiveness and optimization, potentially leading to a skewed understanding of the landscape of predictive coding methods. Additionally, the empirical evidence provided does not adequately address scenarios where BP might still outperform iPC, which limits the robustness of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas in a generally clear manner, although the framing of comparisons to BP could benefit from greater depth and fairness. The quality of the theoretical foundations is commendable, but the empirical results lack sufficient context and may not translate well to real-world applications. The novelty of iPC is notable, yet the paper could have benefited from a more balanced discussion of existing methodologies and their contributions in the field. Reproducibility concerns arise as the empirical results do not fully explore the practical implications of iPC's advantages over BP.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach with incremental predictive coding, but its comparisons to backpropagation lack depth and fairness. While the theoretical contributions are valuable, the empirical evidence does not fully support the claims of superiority over existing methods, necessitating a more balanced perspective on the strengths and weaknesses of both iPC and BP.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel learning algorithm titled \"Incremental Predictive Coding (iPC),\" which aims to improve the efficiency of predictive coding frameworks in neural networks. The methodology involves a parallel and fully automatic learning approach that allows for continuous adaptation of models to incoming data streams without the need for extensive retraining. The findings demonstrate that the iPC algorithm outperforms existing methods in terms of predictive accuracy and computational efficiency on various benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to incremental learning, effectively addressing the limitations of traditional predictive coding methods. The comprehensive experiments conducted showcase the robustness and adaptability of the proposed algorithm. However, the paper contains several formatting and stylistic inconsistencies that may hinder readability and understanding, such as unclear terminology and inconsistent citation formats.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a significant advancement in the field, clarity issues regarding terminology and inconsistent formatting detract from its overall quality. The novelty of the approach is commendable, as it presents a unique solution to the challenges of incremental learning. However, the reproducibility of the results could be improved by providing clearer descriptions of the experimental setup and ensuring that all mathematical notations and pseudocode are uniformly presented.\n\n# Summary Of The Review\nOverall, the paper contributes valuable insights into incremental predictive coding through a novel algorithm. However, clarity and formatting issues need to be addressed to enhance the reader's understanding and the reproducibility of the results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the efficiency of incremental predictive coding (iPC) in comparison to traditional predictive coding (PC) and backpropagation (BP) within the context of image classification tasks. The authors present empirical evidence suggesting that iPC yields better performance and efficiency; however, the study primarily focuses on benchmarks without delving into broader applications or adaptability of iPC to diverse neural architectures, such as recurrent networks or transformers. Additionally, the paper lacks a thorough exploration of practical implications, scalability, and robustness in real-world scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical evidence supporting the efficacy of iPC in image classification tasks, which may offer a promising alternative to existing methods. However, several weaknesses undermine the overall contribution. The discussion of biological plausibility is limited, and the potential for iPC's application in real-time systems, broader tasks beyond image classification, and its interaction with other learning paradigms is not adequately addressed. Furthermore, the lack of a comprehensive analysis regarding the implications of noise, data distribution shifts, and ethical considerations represents significant gaps in the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents its findings. However, the novelty of the approach could be better articulated, particularly in relation to existing models in computational neuroscience. The reproducibility of the results is not sufficiently supported by detailed methodologies or a roadmap for future research that could validate and expand upon the findings. The authors should provide clearer guidance on how their work can be built upon in subsequent studies.\n\n# Summary Of The Review\nOverall, the paper presents a compelling exploration of iPC's advantages in image classification but falls short in addressing its broader applicability and implications. The limitations in discussing scalability, robustness, and potential ethical issues diminish its impact, suggesting that further research is necessary to fully realize the potential of iPC.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel approach termed **incremental predictive coding (iPC)**, which is derived from the **incremental expectation-maximization (iEM)** algorithm. The authors claim that iPC enhances computational efficiency while maintaining performance levels comparable to traditional backpropagation (BP). Key contributions include a theoretical analysis demonstrating that iPC can achieve weight updates in constant time (O(1)), as opposed to the linear time complexity (O(L)) of BP. Empirical evaluations across various architectures and datasets, including MNIST and FashionMNIST, reveal that iPC outperforms BP in training loss reduction, especially in smaller models, and exhibits improved robustness against distribution shifts.\n\n# Strength And Weaknesses\nStrengths of the paper include its solid theoretical foundation, with formal guarantees for the efficiency of iPC, and rigorous empirical testing that supports its claims. The paper effectively utilizes statistical methodologies to analyze performance metrics, including mean and standard error reporting, which enhances the credibility of the results. However, a notable weakness is the lack of explicit p-value reporting, which would strengthen the claims of statistical significance. Furthermore, while the paper explores different hyperparameter configurations, the generalizability of findings could be further validated through additional datasets or real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured and clear, with a logical flow from theoretical foundations to empirical results. The quality of the writing is high, making complex concepts accessible. The novelty of the iPC approach is significant, particularly given its efficiency advantages over existing methods. Reproducibility is addressed through a comprehensive experimental design, including grid searches for hyperparameter tuning, although the lack of detailed statistical significance reporting may hinder complete reproducibility.\n\n# Summary Of The Review\nThis paper presents a promising advancement in predictive coding through the introduction of incremental predictive coding (iPC), demonstrating theoretical and empirical advantages over traditional backpropagation methods. While the contributions are substantial, the authors should enhance their statistical rigor to strengthen claims of significance and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes an innovative framework known as iterative Parallel Computation (iPC) aimed at improving the efficiency of deep learning models during training. The authors present theoretical guarantees for the convergence of iPC, supported by empirical results that demonstrate its potential benefits when applied to certain neural network architectures. However, the methodology relies heavily on PyTorch, which may not fully optimize the parallelization capabilities of iPC on GPUs. Key findings suggest that while iPC could enhance computational efficiency, its practical application remains questionable due to limited experiments and untested scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical contributions, particularly the guarantees for convergence, which add a significant layer of credibility to the proposed method. Nonetheless, the weaknesses are pronounced, including a lack of empirical validation across diverse datasets and architectures, and insufficient exploration of dynamic learning strategies that could enhance the model's performance. Additionally, the practical implementation of iPC is not adequately addressed, limiting its applicability in real-world scenarios. The future work section is vague and does not provide clear directions for further research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that outlines the theoretical background and proposed methodology. However, the novelty of the approach may be overshadowed by its limitations in experimental validation and practical implementation. Reproducibility could be an issue, as the reliance on PyTorch without exploring optimizations for parallel computations raises concerns about the scalability of the findings in real-world applications. Overall, while the core idea is novel, the execution lacks sufficient clarity and depth to fully support its claims.\n\n# Summary Of The Review\nWhile the paper introduces a theoretically promising framework in iPC with convergence guarantees, it falls short in empirical validation and practical applicability, particularly in real-world scenarios. The contributions, though significant, require further exploration and testing to establish their effectiveness and relevance across diverse contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents \"Incremental Predictive Coding\" (iPC), an algorithm that aims to improve efficiency in machine learning by building upon predictive coding theory. The authors propose that their method offers a parallel and fully automatic approach to learning, claiming enhanced performance compared to traditional backpropagation (BP) methods. The study includes empirical results demonstrating iPC's advantages in terms of training time and calibration, despite relying on long-established concepts and methodologies.\n\n# Strength And Weaknesses\nThe paper outlines a structured approach to developing the iPC algorithm, attempting to provide theoretical guarantees and empirical evidence for its effectiveness. However, it largely revisits existing ideas in predictive coding and machine learning without offering substantial new insights. While the results indicate some improvements over BP, they do not present groundbreaking advancements or novel methodologies, making the contributions seem less significant in the broader context of the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably well-structured, but the novelty of the contributions is questionable. The writing is clear, though it occasionally overstates the significance of the findings. Reproducibility is not sufficiently addressed, as the experiments rely heavily on parameter tuning without comprehensive validation across diverse scenarios. The authors' claims regarding the advantages of iPC over BP would benefit from a more rigorous evaluation of the underlying assumptions.\n\n# Summary Of The Review\nOverall, the paper presents an incremental advancement in the realm of predictive coding and machine learning, but it lacks substantial novelty and robust empirical validation. The contributions, while framed as innovative, are largely iterative and do not represent a significant leap forward in the field.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces Incremental Predictive Coding (iPC), a novel learning algorithm designed to enhance efficiency in training deep neural networks by minimizing the reliance on backpropagation. The authors present a comprehensive methodology that integrates iPC with standard architectures, including Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs). Key findings indicate that iPC offers a compelling alternative to traditional learning methods, demonstrating improved parameter efficiency and robustness, particularly in calibration tasks, while also hinting at its biological plausibility.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to learning, presenting iPC as a potential solution to the limitations of backpropagation, particularly in terms of parameter efficiency and robustness. However, the paper lacks an extensive comparison with a wider range of state-of-the-art optimization algorithms, limiting the contextual understanding of iPC's performance. Additionally, while the methodology is solid, the experiments could benefit from exploring a broader array of architectures and real-world applications, which would further substantiate the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its contributions, methodology, and results. The quality of the writing is high, making complex concepts accessible. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of experimental setups and hyperparameter tuning specific to iPC. The novelty of the proposed approach is notable, yet it could be strengthened by addressing the lack of comparisons with other contemporary learning algorithms.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to incremental learning through iPC, showcasing its potential benefits in parameter efficiency and robustness. However, the lack of comprehensive comparisons and exploration of diverse architectures and applications may limit its impact. Further development in these areas could significantly enhance the significance of the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces incremental predictive coding (iPC), a novel learning framework that claims to outperform traditional predictive coding (PC) and backpropagation (BP) across various benchmark tasks. The authors provide empirical evidence demonstrating that iPC achieves competitive accuracy in image classification tasks, such as MNIST, FashionMNIST, SVHN, and CIFAR-10, while requiring fewer parameters than BP. Additionally, the study highlights iPC's faster convergence and efficiency in training, as well as its robustness under distribution shifts, positioning it as a viable alternative to established methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive evaluation of iPC against established methods, showcasing its competitive performance across multiple datasets and architectures. The efficiency and parameter requirements are also significant advantages, suggesting that iPC could be more practical in real-world applications. However, a notable weakness is that iPC does not always surpass BP, particularly in more complex models like AlexNet on CIFAR-10, which may limit its applicability in scenarios where maximum performance is critical. Additionally, the paper could benefit from a deeper exploration of the theoretical underpinnings of iPC.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, making it accessible to a broad audience. The methodology is detailed, allowing for reproducibility of the experiments. The novelty of iPC is evident, as it introduces a new paradigm for predictive coding that addresses some limitations of existing methods. However, the empirical results could be strengthened with more extensive comparison against a wider array of architectures and datasets to fully establish the robustness of the claims.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for incremental predictive coding as a competitive alternative to traditional learning methods, particularly in simpler models. While the empirical results are promising, the limitations observed in more complex architectures suggest the need for further investigation. The contributions are noteworthy, though the paper would benefit from addressing theoretical foundations and broader empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework that enhances predictive coding methodologies in neural networks. It introduces a structured approach to integrate theoretical concepts with empirical validations, demonstrating improved performance on standard benchmark datasets. The findings indicate significant advancements in model accuracy and efficiency while maintaining computational feasibility, particularly in complex environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its clear articulation of contributions, which advance the existing understanding of predictive coding. The methodology is robust, incorporating a variety of experiments that substantiate the claims made. However, the paper suffers from a lack of clarity in some sections, particularly in the introduction and contributions, where excessive jargon and redundancy can confuse readers. Additionally, figures and tables are underutilized in supporting the narrative, which could detract from the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a significant advance in the field, clarity is hindered by convoluted sentences and inconsistent terminology. The quality of the writing, although generally solid, could be improved by simplifying complex expressions and ensuring that technical jargon is well-defined. Novelty is present in the proposed framework, but the execution could benefit from a more engaging writing style. Reproducibility is a concern due to insufficient explanation of the methodology and lack of explicit references to supplementary materials.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to predictive coding methodologies with a promising framework. However, improvements in clarity and structure are necessary to enhance reader comprehension and engagement. Addressing these weaknesses would significantly strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4257483669114754,
    -1.663988242071978,
    -1.842522927519154,
    -1.6631319018061799,
    -1.9486638914008672,
    -1.7600598752334293,
    -1.689690757607348,
    -1.7620686024271839,
    -1.5566865785903286,
    -1.9512133508228984,
    -1.5438143081359181,
    -1.47910415256317,
    -1.616796310525413,
    -1.700832916682332,
    -1.5730138749660094,
    -1.6561231414751059,
    -1.8888985887468988,
    -1.5339119775888288,
    -1.7465885331941717,
    -1.8624269918203624,
    -1.8084172051582925,
    -1.712640769132567,
    -1.7672955469479668,
    -1.7981313815951452,
    -1.8960399229588158,
    -1.901834891633399,
    -1.629880579006073,
    -1.5592586954207786,
    -1.8905379217092673
  ],
  "logp_cond": [
    [
      0.0,
      -2.1550113275886273,
      -2.176646400732747,
      -2.161955506010172,
      -2.173220815298607,
      -2.1986243686940172,
      -2.240771680778168,
      -2.1803233267557016,
      -2.1784876026299616,
      -2.1750856992908463,
      -2.1845811163791735,
      -2.2613049724587055,
      -2.164060918493407,
      -2.1790356379527105,
      -2.1927038326719184,
      -2.174880688107566,
      -2.194491494215328,
      -2.2061236643161526,
      -2.2141752862716726,
      -2.170843369041315,
      -2.191756191421763,
      -2.2287145639353834,
      -2.1777677570879446,
      -2.1624871526410083,
      -2.2489349521764272,
      -2.1980259524716046,
      -2.207629946190346,
      -2.1751750061752975,
      -2.24945907940867
    ],
    [
      -1.2789860974255065,
      0.0,
      -1.1536590423726503,
      -1.1444650517896773,
      -1.1789031888910433,
      -1.2228319134658678,
      -1.3039487756897474,
      -1.2042900943853414,
      -1.1769042808053187,
      -1.2442248391903332,
      -1.2033968637169836,
      -1.374298844081193,
      -1.162072040492697,
      -1.2032663795879273,
      -1.1692069576950068,
      -1.178016465885034,
      -1.2556313047439815,
      -1.179259197410015,
      -1.2322461529663622,
      -1.1683060480159415,
      -1.2273723388204365,
      -1.2599400333272552,
      -1.2424606166612489,
      -1.201247425241812,
      -1.3245226591353818,
      -1.2314383162400984,
      -1.2251571824400718,
      -1.2477550944735367,
      -1.326811509242407
    ],
    [
      -1.466611299188534,
      -1.4637245898165887,
      0.0,
      -1.3919226353644338,
      -1.2915535016551702,
      -1.4709770651097596,
      -1.5276242236794446,
      -1.452859688571343,
      -1.4242975421501214,
      -1.4173244074774431,
      -1.452136446881302,
      -1.591773878236247,
      -1.4348420262224453,
      -1.4779757902360144,
      -1.4774728267298265,
      -1.4654570088130665,
      -1.4505978562710005,
      -1.4386111103280068,
      -1.4609515911935087,
      -1.4182727947311364,
      -1.4438342949082812,
      -1.522676509388402,
      -1.5072078544221508,
      -1.419476725352269,
      -1.5478615319170062,
      -1.4938685827895701,
      -1.4996242367151775,
      -1.455218321697605,
      -1.5884291496631504
    ],
    [
      -1.3355488713250254,
      -1.2505967445779749,
      -1.2052735276912698,
      0.0,
      -1.2057503643009744,
      -1.324225775902582,
      -1.3730662614177787,
      -1.2417127981620806,
      -1.203117239012613,
      -1.3245254297179392,
      -1.2179101936111782,
      -1.414037560599674,
      -1.2357676836178613,
      -1.289066651738009,
      -1.2683253971150583,
      -1.2946547995803943,
      -1.3141091306094883,
      -1.3185488866081554,
      -1.3339246452173694,
      -1.2647873680877129,
      -1.2884450217809555,
      -1.3656352910909468,
      -1.324531337044427,
      -1.2444069454612765,
      -1.3629796051679803,
      -1.2986040019514218,
      -1.328139842123196,
      -1.295089487521559,
      -1.3717435286894575
    ],
    [
      -1.5695761745676104,
      -1.5834233194317757,
      -1.4319149389450723,
      -1.4848556220410907,
      0.0,
      -1.6259902875530654,
      -1.6588214715798133,
      -1.5376345494647734,
      -1.4999238876956007,
      -1.5213637678134344,
      -1.4958092987381193,
      -1.654389307879389,
      -1.560458136257343,
      -1.5992074293133565,
      -1.5730961386478468,
      -1.5641803918278534,
      -1.494497694539678,
      -1.5175946154640994,
      -1.5301325765523475,
      -1.6037275610718493,
      -1.4816936621837224,
      -1.6524144401459273,
      -1.570246536181967,
      -1.5302251997035534,
      -1.5866449642404052,
      -1.5775318231497633,
      -1.5818982102129024,
      -1.5523218937607641,
      -1.6665264148597458
    ],
    [
      -1.4129442902588962,
      -1.3286454368971208,
      -1.3178739011560416,
      -1.3127229768239022,
      -1.3115701264189112,
      0.0,
      -1.3917875569781462,
      -1.2984043968532712,
      -1.275665950339069,
      -1.3739516577940492,
      -1.2696042029527097,
      -1.4462707426204402,
      -1.310966739779993,
      -1.3135183200781022,
      -1.3366371520616078,
      -1.3369331409095366,
      -1.4023616548466729,
      -1.3555715439795664,
      -1.3127361016889847,
      -1.284959452760071,
      -1.3344495651960195,
      -1.3967954477020232,
      -1.3918424597704748,
      -1.3041606641536192,
      -1.419967103674296,
      -1.4005377783816029,
      -1.3463212789491081,
      -1.3770078635662533,
      -1.4397684804196123
    ],
    [
      -1.405160871723102,
      -1.2881688193269254,
      -1.3063087930186135,
      -1.3218384251069877,
      -1.3183034113056413,
      -1.3436666201203675,
      0.0,
      -1.3437330923022681,
      -1.356276861569615,
      -1.2741282269480336,
      -1.2913629549284797,
      -1.3753812802452807,
      -1.3269264602081885,
      -1.365079246204483,
      -1.3569202739826887,
      -1.3110766357332917,
      -1.3876624455608677,
      -1.3484316763283448,
      -1.3420937814227538,
      -1.3106442935037383,
      -1.3231614750401555,
      -1.396538014211047,
      -1.3812827775519412,
      -1.3162998983880163,
      -1.3466515987440355,
      -1.3168825117664342,
      -1.354763669104065,
      -1.3718996349086763,
      -1.3819298745712478
    ],
    [
      -1.458038080433728,
      -1.444517355628112,
      -1.3973259492799206,
      -1.3920900323583656,
      -1.3862305184416281,
      -1.4105342672787786,
      -1.4821615708845925,
      0.0,
      -1.3731218749675724,
      -1.4086584238403177,
      -1.4105460656466626,
      -1.509838482089664,
      -1.4082361191571895,
      -1.4390906186404844,
      -1.4285379797065105,
      -1.4359218898070867,
      -1.4580172411756622,
      -1.4086439337710333,
      -1.3992705206970886,
      -1.428571852369775,
      -1.3748145592311836,
      -1.4725834680446774,
      -1.4441293902197747,
      -1.371355408645715,
      -1.452023637315478,
      -1.3882067843627643,
      -1.4228996293909968,
      -1.4372101945929887,
      -1.5045078348715566
    ],
    [
      -1.2310378199579566,
      -1.197441603673797,
      -1.1151139714995097,
      -1.1064939643132132,
      -1.1056836474058889,
      -1.1723327681033104,
      -1.2870011772828358,
      -1.1187688901082065,
      0.0,
      -1.1828052809157086,
      -1.1688124215349278,
      -1.3050828503395009,
      -1.1572705560393293,
      -1.1846335597059279,
      -1.1730074180996921,
      -1.161517008976822,
      -1.2180211541310164,
      -1.1560367320826728,
      -1.1448857438702413,
      -1.1865966921357787,
      -1.1805071087982923,
      -1.2109393555344394,
      -1.2131432861842852,
      -1.131709857097204,
      -1.2482984671565558,
      -1.1648266173884037,
      -1.2031600119244825,
      -1.2289946203714122,
      -1.2683966155248205
    ],
    [
      -1.620576368145012,
      -1.6151713588941632,
      -1.569133107553497,
      -1.577556480828122,
      -1.516964793050124,
      -1.5875620577104927,
      -1.5947648778468222,
      -1.6118260783796916,
      -1.5856159411990354,
      0.0,
      -1.5733774017511526,
      -1.6949969651687,
      -1.5569021550673094,
      -1.5896243601865225,
      -1.5842523696149737,
      -1.5706570866094929,
      -1.6054961972108428,
      -1.6015145931608372,
      -1.5938075879069382,
      -1.594834389839709,
      -1.5376213476698695,
      -1.6358202259438568,
      -1.5903340880351444,
      -1.611297032268006,
      -1.6440582222226334,
      -1.5923066687568483,
      -1.6079852723539751,
      -1.582125291255885,
      -1.6249275511540997
    ],
    [
      -1.1778949187908203,
      -1.1238328306483845,
      -1.1095915404909684,
      -1.0602490936837983,
      -1.0534587485181857,
      -1.1305814751068068,
      -1.219952057241785,
      -1.1316193350001542,
      -1.0589946560971724,
      -1.1497028662286415,
      0.0,
      -1.2331374069384573,
      -1.1069234628621014,
      -1.1406838492050917,
      -1.1006111819024387,
      -1.1308937041434735,
      -1.1755093592540917,
      -1.152810272768292,
      -1.1469899984255045,
      -1.1227708679247297,
      -1.091282265348052,
      -1.1868773993275967,
      -1.1422881529584439,
      -1.1290243811478946,
      -1.1834975409675395,
      -1.168408298440794,
      -1.1444424682596523,
      -1.1513625358750232,
      -1.2327527280118327
    ],
    [
      -1.2706643940272055,
      -1.2500700147197883,
      -1.247197560479311,
      -1.2563684196849785,
      -1.2378843608774597,
      -1.2162470518494164,
      -1.2402607862334942,
      -1.196816360434314,
      -1.2289631600430309,
      -1.243558643414715,
      -1.2444993633165848,
      0.0,
      -1.217012033383733,
      -1.265607748393488,
      -1.2366074511190492,
      -1.2516482277504042,
      -1.2416842167156876,
      -1.2399486831424116,
      -1.224812248275956,
      -1.2310422282642934,
      -1.2225249349409166,
      -1.2216417950992675,
      -1.2392100940964699,
      -1.2046821560324112,
      -1.1945694420433062,
      -1.2492814807762287,
      -1.2075635873482697,
      -1.2181610926030404,
      -1.1993106024304372
    ],
    [
      -1.2638329354942652,
      -1.1882790310513127,
      -1.2213070977872222,
      -1.1559904864878052,
      -1.2068357531693037,
      -1.2287142762911578,
      -1.3127751115344224,
      -1.24775533341378,
      -1.2324466079464929,
      -1.240859134250922,
      -1.1899591558756648,
      -1.3451686688011761,
      0.0,
      -1.1878481836332346,
      -1.208974408271998,
      -1.2349434665441477,
      -1.279638184202026,
      -1.2302406148900293,
      -1.2170818276083004,
      -1.2163734713218022,
      -1.2189693943544802,
      -1.3013751859334908,
      -1.2908323542934186,
      -1.2463651073295277,
      -1.317060572560405,
      -1.2686327683701561,
      -1.270271204149794,
      -1.300564591555296,
      -1.3135720941674922
    ],
    [
      -1.267522907596699,
      -1.2738726239569107,
      -1.234303816968267,
      -1.2653930607506314,
      -1.227959188647489,
      -1.2712274037234115,
      -1.3567024301300654,
      -1.2279892339053347,
      -1.2924187405545304,
      -1.2526133659797356,
      -1.2482019354559784,
      -1.4037418808279107,
      -1.2245091053289265,
      0.0,
      -1.2236584171291462,
      -1.2793530804690445,
      -1.302327496543299,
      -1.2867868121317596,
      -1.2977532570164518,
      -1.264315053699722,
      -1.247507855235463,
      -1.3456271077341682,
      -1.305614940414905,
      -1.2592627106782464,
      -1.3548414866441243,
      -1.3366850685537448,
      -1.309384738012713,
      -1.278318761547173,
      -1.3729962961288933
    ],
    [
      -1.2032988835052192,
      -1.1666370899582477,
      -1.1806989588002141,
      -1.153760192199377,
      -1.1156357714449727,
      -1.2198340174876818,
      -1.2521826788639145,
      -1.167315673911266,
      -1.1395782872438254,
      -1.1610537867076345,
      -1.1637875919610277,
      -1.2861125809880658,
      -1.1278798843466178,
      -1.1615670166583352,
      0.0,
      -1.1407961239617614,
      -1.1607391050523046,
      -1.1670763747482211,
      -1.1597713663470985,
      -1.1770697295152386,
      -1.151274216518689,
      -1.2024702478879752,
      -1.1753167740989967,
      -1.177595995328446,
      -1.2174088372713283,
      -1.1461080938812602,
      -1.1736721115171769,
      -1.1699677168236415,
      -1.2635629925545675
    ],
    [
      -1.2945271116240442,
      -1.1757187552976593,
      -1.1701953909799876,
      -1.1802806145439184,
      -1.1620226653829198,
      -1.2212892953930328,
      -1.265298329780468,
      -1.2227610652733525,
      -1.1800957815576474,
      -1.199092261085718,
      -1.1788036677070493,
      -1.3272837125799888,
      -1.187720203232678,
      -1.1992619076897275,
      -1.1955274884878653,
      0.0,
      -1.2502534799016793,
      -1.2046795228209128,
      -1.2129906598203137,
      -1.1674114456622238,
      -1.1798612717062293,
      -1.292124486171808,
      -1.2562247516764042,
      -1.1805588362421509,
      -1.3014671680373768,
      -1.2857516106340339,
      -1.23586509106966,
      -1.22515089803815,
      -1.3146807780439642
    ],
    [
      -1.5463419660109736,
      -1.4585544778985753,
      -1.4682653016187148,
      -1.4795372999272263,
      -1.415003814381035,
      -1.5508377108578688,
      -1.6141430749592234,
      -1.4646093291546713,
      -1.4505138227795564,
      -1.4903734616085453,
      -1.4620164699532332,
      -1.6035000706398657,
      -1.4890119901235688,
      -1.5097189321275526,
      -1.5160418336314783,
      -1.5471726142138527,
      0.0,
      -1.4670960893548077,
      -1.53085779265176,
      -1.5072882047270886,
      -1.463805611571433,
      -1.5282003215139335,
      -1.5176807713876248,
      -1.4885882332903442,
      -1.523594559298456,
      -1.5227427148481927,
      -1.477934238233313,
      -1.4850811035946097,
      -1.6263684778316565
    ],
    [
      -1.217119347936327,
      -1.14628637323249,
      -1.1472841362672725,
      -1.1415807332305727,
      -1.0963175839745258,
      -1.1839232005881102,
      -1.1925736810679055,
      -1.1093489847831155,
      -1.150268400726352,
      -1.1352974434177237,
      -1.18177814002363,
      -1.252465964814884,
      -1.1217918129787354,
      -1.1513816256586196,
      -1.1314139147159865,
      -1.1656040288742937,
      -1.1278350770018992,
      0.0,
      -1.1244747615738777,
      -1.1779178478511567,
      -1.1117918873635282,
      -1.1965787973448003,
      -1.1643774512934175,
      -1.170397588320387,
      -1.1676003265484625,
      -1.1521861562101567,
      -1.1696918609517362,
      -1.192516099630226,
      -1.1961260661884263
    ],
    [
      -1.4308227995121374,
      -1.417989341097029,
      -1.3581498552801063,
      -1.422180669790948,
      -1.3127753337695103,
      -1.3576264119627457,
      -1.4101413387166608,
      -1.3359289511092622,
      -1.3888499442145095,
      -1.3105505578940084,
      -1.3880772706137958,
      -1.4714716197787423,
      -1.3712082786038333,
      -1.4052528176434689,
      -1.4107541555670993,
      -1.4219902511186224,
      -1.3814077454552796,
      -1.3964597766560891,
      0.0,
      -1.350376820938261,
      -1.3538726376905204,
      -1.4403113147753237,
      -1.4331737525874146,
      -1.4074513415225727,
      -1.4343077346900859,
      -1.422036053276765,
      -1.39099644992011,
      -1.4130240391299627,
      -1.465719532468382
    ],
    [
      -1.5016866617529565,
      -1.4319250102472014,
      -1.4272947868681258,
      -1.4401084371069428,
      -1.4524355752336866,
      -1.4513152814038168,
      -1.5382935510719022,
      -1.4314181319093422,
      -1.4436968977094866,
      -1.4894610897343987,
      -1.4428618105288478,
      -1.6107280040005034,
      -1.4653066957073364,
      -1.4951102127037412,
      -1.4527575969348714,
      -1.4879926372492651,
      -1.5091266910444003,
      -1.5212781290944402,
      -1.444726527207494,
      0.0,
      -1.4562645700487777,
      -1.5302860867059989,
      -1.5011387019620466,
      -1.4309233344210621,
      -1.535420001286275,
      -1.5289107382062375,
      -1.5106658178617631,
      -1.51934618275466,
      -1.5865526533108605
    ],
    [
      -1.4922446740931317,
      -1.4619629120405961,
      -1.4411562282378074,
      -1.4489611935333104,
      -1.3408930379508992,
      -1.4762906428552178,
      -1.508907674567403,
      -1.431809054573024,
      -1.4193094514743472,
      -1.3875409254133109,
      -1.4366601888407944,
      -1.5715905707656308,
      -1.4605227429991856,
      -1.4571496559828794,
      -1.414663252384977,
      -1.464932066219057,
      -1.4555142305795876,
      -1.4541014579866272,
      -1.4299442417158432,
      -1.4316446667326097,
      0.0,
      -1.5033523230590817,
      -1.4530562035776222,
      -1.437398045598126,
      -1.4527450643384727,
      -1.462295044277974,
      -1.4893273889358505,
      -1.4329659382263105,
      -1.5418863648675307
    ],
    [
      -1.3574448356029902,
      -1.2695380837403034,
      -1.3226679547033946,
      -1.300884551651094,
      -1.2352939740189728,
      -1.294881444999608,
      -1.3158104103638364,
      -1.2963024525048212,
      -1.255010120680654,
      -1.297663877811092,
      -1.2732827483647886,
      -1.3230720361512849,
      -1.2539266687690271,
      -1.297562399773639,
      -1.2179554035694895,
      -1.2952707124869094,
      -1.2866205064752798,
      -1.2512467258679585,
      -1.3131562759141242,
      -1.2797078071543024,
      -1.2709222096703297,
      0.0,
      -1.2992076649849897,
      -1.2740224855981297,
      -1.2926988596011522,
      -1.2676458169666525,
      -1.2327982146128664,
      -1.3022840987144086,
      -1.2883636842377335
    ],
    [
      -1.4331419252202096,
      -1.412356805408108,
      -1.4302314800338438,
      -1.3990792255208049,
      -1.359698792293443,
      -1.4252774352864739,
      -1.455427838349544,
      -1.374805809869086,
      -1.3778012437032334,
      -1.380482425805479,
      -1.3880725090631074,
      -1.5117913677395265,
      -1.3746658270213843,
      -1.4102524863633252,
      -1.3981461446832626,
      -1.4063802923770414,
      -1.4159954843932958,
      -1.3891862204977057,
      -1.4541445020307036,
      -1.413980419468309,
      -1.3539272351307996,
      -1.435757589823068,
      0.0,
      -1.3978465460437979,
      -1.4292888692293229,
      -1.4566246494999686,
      -1.3996294444380994,
      -1.4093668020445427,
      -1.4821666530869515
    ],
    [
      -1.449811599575365,
      -1.4698312826008928,
      -1.3873186871847516,
      -1.411563714878327,
      -1.3966130033970896,
      -1.4677806132469966,
      -1.5081362109519407,
      -1.396666807339106,
      -1.4028296875316788,
      -1.4158813744227263,
      -1.4471460607652744,
      -1.5396599208066468,
      -1.4709499454386317,
      -1.479126667163559,
      -1.4660922455715664,
      -1.47555605301137,
      -1.4354997057855086,
      -1.4787971273875868,
      -1.4611488701941893,
      -1.4163138163168087,
      -1.4200233360928696,
      -1.5041571406507956,
      -1.4282158970654582,
      0.0,
      -1.4535773176679205,
      -1.460760662966727,
      -1.4651768535205483,
      -1.4168539236184876,
      -1.5060617382027321
    ],
    [
      -1.6075823675790226,
      -1.6035199833638747,
      -1.5995295817848723,
      -1.6001592050193574,
      -1.5339039463852127,
      -1.5856109967782892,
      -1.6090628051466007,
      -1.6003064732889185,
      -1.577306503985742,
      -1.5612373614381332,
      -1.6003802420819548,
      -1.610337317821871,
      -1.59502065988062,
      -1.6064106595320629,
      -1.579038840959448,
      -1.6014167130235313,
      -1.5418379688047317,
      -1.5872711225233962,
      -1.579396078583339,
      -1.59242179951735,
      -1.5417129001956833,
      -1.6068460953194654,
      -1.5820290314117904,
      -1.5508348045687612,
      0.0,
      -1.6010708435762437,
      -1.5596438189003072,
      -1.5871972625478026,
      -1.598662218828853
    ],
    [
      -1.4850228322534142,
      -1.4055964620242731,
      -1.455917045440617,
      -1.4220002117479187,
      -1.4191202521327155,
      -1.5019801312566778,
      -1.4693754669552326,
      -1.4361685357533642,
      -1.4100388301724753,
      -1.3750287445758012,
      -1.458208338562697,
      -1.5663398658477075,
      -1.4828158221270316,
      -1.5035269534591664,
      -1.4059482177015807,
      -1.4839011265908495,
      -1.476361159568608,
      -1.4388735557274297,
      -1.4775252333015585,
      -1.484419207628033,
      -1.4569065191488026,
      -1.4779505532612591,
      -1.4862535174705034,
      -1.4335971020309284,
      -1.5109430548596994,
      0.0,
      -1.4508810289787195,
      -1.487891380125096,
      -1.5117223163773563
    ],
    [
      -1.2943256523252877,
      -1.2634503478528443,
      -1.2387901404738888,
      -1.2617348981579164,
      -1.2074390641440782,
      -1.2882074720579728,
      -1.3237866051476077,
      -1.2580332738949378,
      -1.2809366681825993,
      -1.2352948956584942,
      -1.2891602732667877,
      -1.3424109452648407,
      -1.231958248342225,
      -1.2792985955210174,
      -1.2416625450336858,
      -1.2642132217835926,
      -1.2316202533563243,
      -1.2626487518240739,
      -1.261173856158145,
      -1.2720071846828243,
      -1.2287430929421599,
      -1.3305578135203717,
      -1.2348087435630302,
      -1.2864101073335843,
      -1.25996571151176,
      -1.2919070383523261,
      0.0,
      -1.2648581393807024,
      -1.3534246664943943
    ],
    [
      -1.2352247155956442,
      -1.2497144909260633,
      -1.188246333557523,
      -1.1916268048770289,
      -1.1632643270778984,
      -1.2094664542298559,
      -1.2647292860948005,
      -1.176324101867863,
      -1.1929399585953282,
      -1.175163911178147,
      -1.2057371696107553,
      -1.2954918799604043,
      -1.211568047465277,
      -1.1966989797662535,
      -1.1708939606597397,
      -1.2180009518596737,
      -1.1849365444090154,
      -1.2029658446047886,
      -1.2101946630105977,
      -1.2070933288279753,
      -1.1301112107988547,
      -1.2481067257232406,
      -1.178291867927436,
      -1.1527770272326208,
      -1.2376846952291825,
      -1.2306657337510754,
      -1.1883212375717451,
      0.0,
      -1.2674562834878305
    ],
    [
      -1.5560115759033155,
      -1.5491442022199637,
      -1.5442047524163423,
      -1.5219279976398314,
      -1.4962853237447484,
      -1.5559009009305562,
      -1.5109274892114968,
      -1.5092965800452132,
      -1.5308951602664314,
      -1.4707617315553743,
      -1.524199720049039,
      -1.5470847107477403,
      -1.5040956208436815,
      -1.5352909210482064,
      -1.5213048979218051,
      -1.5439548108249164,
      -1.564358710642883,
      -1.5142899593235497,
      -1.536543413596705,
      -1.5266500126080573,
      -1.4980269311175505,
      -1.4620164638015245,
      -1.5153040522403538,
      -1.5122291423196879,
      -1.553014585945234,
      -1.480980708190999,
      -1.5043769426202498,
      -1.5124467677178053,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2707370393228481,
      0.2491019661787286,
      0.2637928609013036,
      0.25252755161286844,
      0.2271239982174582,
      0.1849766861333073,
      0.24542504015577382,
      0.2472607642815139,
      0.25066266762062916,
      0.24116725053230192,
      0.1644433944527699,
      0.26168744841806824,
      0.24671272895876495,
      0.23304453423955707,
      0.2508676788039095,
      0.23125687269614748,
      0.21962470259532285,
      0.21157308063980285,
      0.2549049978701605,
      0.2339921754897123,
      0.19703380297609208,
      0.24798060982353087,
      0.2632612142704671,
      0.1768134147350482,
      0.22772241443987085,
      0.21811842072112952,
      0.250573360736178,
      0.17628928750280526
    ],
    [
      0.38500214464647153,
      0.0,
      0.5103291996993278,
      0.5195231902823008,
      0.48508505318093476,
      0.4411563286061102,
      0.3600394663822306,
      0.45969814768663664,
      0.4870839612666593,
      0.4197634028816448,
      0.4605913783549944,
      0.2896893979907851,
      0.5019162015792811,
      0.4607218624840508,
      0.4947812843769712,
      0.485971776186944,
      0.4083569373279965,
      0.484729044661963,
      0.43174208910561584,
      0.49568219405603653,
      0.43661590325154154,
      0.4040482087447228,
      0.4215276254107292,
      0.4627408168301661,
      0.33946558293659623,
      0.4325499258318797,
      0.4388310596319063,
      0.4162331475984413,
      0.33717673282957095
    ],
    [
      0.37591162833061986,
      0.3787983377025652,
      0.0,
      0.4506002921547201,
      0.5509694258639837,
      0.37154586240939436,
      0.3148987038397093,
      0.38966323894781096,
      0.41822538536903253,
      0.42519852004171077,
      0.39038648063785186,
      0.250749049282907,
      0.40768090129670864,
      0.3645471372831395,
      0.3650501007893274,
      0.3770659187060874,
      0.39192507124815346,
      0.4039118171911471,
      0.3815713363256452,
      0.4242501327880175,
      0.39868863261087273,
      0.3198464181307519,
      0.33531507309700315,
      0.4230462021668848,
      0.29466139560214777,
      0.3486543447295838,
      0.3428986908039764,
      0.387304605821549,
      0.2540937778560035
    ],
    [
      0.32758303048115445,
      0.412535157228205,
      0.45785837411491004,
      0.0,
      0.4573815375052055,
      0.3389061259035979,
      0.2900656403884012,
      0.4214191036440993,
      0.46001466279356684,
      0.3386064720882407,
      0.4452217081950016,
      0.24909434120650586,
      0.4273642181883186,
      0.37406525006817093,
      0.39480650469112155,
      0.36847710222578556,
      0.34902277119669156,
      0.3445830151980245,
      0.32920725658881045,
      0.398344533718467,
      0.3746868800252243,
      0.29749661071523303,
      0.33860056476175293,
      0.41872495634490337,
      0.30015229663819953,
      0.3645278998547581,
      0.33499205968298384,
      0.3680424142846208,
      0.2913883731167224
    ],
    [
      0.3790877168332567,
      0.36524057196909143,
      0.5167489524557949,
      0.4638082693597765,
      0.0,
      0.32267360384780175,
      0.28984241982105385,
      0.41102934193609375,
      0.44874000370526645,
      0.42730012358743275,
      0.45285459266274786,
      0.2942745835214782,
      0.38820575514352407,
      0.3494564620875107,
      0.37556775275302035,
      0.38448349957301375,
      0.4541661968611892,
      0.4310692759367678,
      0.4185313148485197,
      0.3449363303290178,
      0.4669702292171447,
      0.29624945125493984,
      0.3784173552189001,
      0.41843869169731374,
      0.3620189271604619,
      0.3711320682511039,
      0.3667656811879647,
      0.396341997640103,
      0.2821374765411213
    ],
    [
      0.3471155849745331,
      0.43141443833630855,
      0.4421859740773877,
      0.4473368984095272,
      0.4484897488145181,
      0.0,
      0.36827231825528317,
      0.4616554783801581,
      0.4843939248943603,
      0.3861082174393802,
      0.4904556722807196,
      0.31378913261298913,
      0.44909313545343643,
      0.4465415551553271,
      0.4234227231718215,
      0.4231267343238927,
      0.35769822038675647,
      0.40448833125386296,
      0.44732377354444464,
      0.4751004224733584,
      0.4256103100374098,
      0.3632644275314061,
      0.3682174154629545,
      0.45589921107981013,
      0.3400927715591333,
      0.35952209685182646,
      0.4137385962843212,
      0.38305201166717606,
      0.3202913948138171
    ],
    [
      0.2845298858842462,
      0.4015219382804227,
      0.38338196458873464,
      0.3678523325003604,
      0.3713873463017068,
      0.3460241374869806,
      0.0,
      0.34595766530508,
      0.3334138960377331,
      0.4155625306593145,
      0.39832780267886836,
      0.3143094773620674,
      0.3627642973991596,
      0.32461151140286515,
      0.33277048362465944,
      0.3786141218740564,
      0.3020283120464804,
      0.3412590812790033,
      0.3475969761845943,
      0.37904646410360976,
      0.36652928256719264,
      0.29315274339630104,
      0.3084079800554069,
      0.3733908592193318,
      0.3430391588633126,
      0.3728082458409139,
      0.3349270885032831,
      0.3177911226986718,
      0.3077608830361003
    ],
    [
      0.3040305219934558,
      0.317551246799072,
      0.3647426531472633,
      0.3699785700688183,
      0.37583808398555574,
      0.35153433514840526,
      0.27990703154259133,
      0.0,
      0.38894672745961145,
      0.3534101785868662,
      0.35152253678052126,
      0.2522301203375199,
      0.3538324832699944,
      0.3229779837866995,
      0.33353062272067335,
      0.3261467126200972,
      0.30405136125152166,
      0.35342466865615063,
      0.3627980817300953,
      0.33349675005740886,
      0.3872540431960003,
      0.28948513438250645,
      0.31793921220740917,
      0.39071319378146896,
      0.3100449651117059,
      0.37386181806441954,
      0.3391689730361871,
      0.3248584078341952,
      0.2575607675556273
    ],
    [
      0.32564875863237197,
      0.3592449749165316,
      0.4415726070908188,
      0.4501926142771153,
      0.4510029311844397,
      0.3843538104870181,
      0.26968540130749274,
      0.4379176884821221,
      0.0,
      0.37388129767462,
      0.38787415705540074,
      0.25160372825082766,
      0.3994160225509993,
      0.3720530188844007,
      0.38367916049063644,
      0.39516956961350647,
      0.33866542445931214,
      0.4006498465076558,
      0.4118008347200872,
      0.3700898864545499,
      0.3761794697920362,
      0.3457472230558891,
      0.3435432924060433,
      0.4249767214931246,
      0.30838811143377276,
      0.3918599612019249,
      0.35352656666584603,
      0.3276919582189164,
      0.288289963065508
    ],
    [
      0.3306369826778863,
      0.3360419919287352,
      0.38208024326940127,
      0.37365686999477643,
      0.4342485577727744,
      0.3636512931124056,
      0.3564484729760762,
      0.33938727244320677,
      0.365597409623863,
      0.0,
      0.3778359490717458,
      0.25621638565419835,
      0.39431119575558893,
      0.36158899063637584,
      0.36696098120792464,
      0.3805562642134055,
      0.34571715361205557,
      0.34969875766206115,
      0.35740576291596016,
      0.35637896098318933,
      0.4135920031530289,
      0.3153931248790416,
      0.36087926278775395,
      0.33991631855489235,
      0.30715512860026495,
      0.35890668206605003,
      0.3432280784689232,
      0.3690880595670134,
      0.32628579966879867
    ],
    [
      0.3659193893450978,
      0.41998147748753367,
      0.4342227676449497,
      0.48356521445211986,
      0.4903555596177325,
      0.4132328330291113,
      0.3238622508941331,
      0.4121949731357639,
      0.4848196520387458,
      0.39411144190727665,
      0.0,
      0.31067690119746083,
      0.4368908452738167,
      0.40313045893082644,
      0.44320312623347946,
      0.4129206039924447,
      0.3683049488818264,
      0.39100403536762607,
      0.3968243097104136,
      0.42104344021118845,
      0.4525320427878661,
      0.3569369088083214,
      0.40152615517747425,
      0.4147899269880235,
      0.3603167671683787,
      0.37540600969512417,
      0.3993718398762658,
      0.3924517722608949,
      0.31106158012408547
    ],
    [
      0.20843975853596453,
      0.22903413784338178,
      0.231906592083859,
      0.2227357328781916,
      0.2412197916857104,
      0.2628571007137537,
      0.23884336632967584,
      0.282287792128856,
      0.2501409925201392,
      0.23554550914845507,
      0.23460478924658523,
      0.0,
      0.262092119179437,
      0.21349640416968207,
      0.24249670144412083,
      0.2274559248127659,
      0.23741993584748244,
      0.2391554694207585,
      0.25429190428721404,
      0.24806192429887663,
      0.25657921762225344,
      0.25746235746390256,
      0.2398940584667002,
      0.2744219965307588,
      0.2845347105198639,
      0.22982267178694138,
      0.27154056521490033,
      0.2609430599601297,
      0.2797935501327329
    ],
    [
      0.35296337503114783,
      0.42851727947410034,
      0.3954892127381908,
      0.46080582403760784,
      0.40996055735610937,
      0.38808203423425525,
      0.30402119899099067,
      0.369040977111633,
      0.3843497025789202,
      0.37593717627449097,
      0.42683715464974825,
      0.2716276417242369,
      0.0,
      0.4289481268921784,
      0.4078219022534151,
      0.3818528439812654,
      0.33715812632338715,
      0.3865556956353837,
      0.39971448291711265,
      0.4004228392036109,
      0.3978269161709329,
      0.3154211245919223,
      0.3259639562319945,
      0.3704312031958854,
      0.2997357379650081,
      0.3481635421552569,
      0.34652510637561895,
      0.31623171897011715,
      0.30322421635792085
    ],
    [
      0.433310009085633,
      0.42696029272542124,
      0.466529099714065,
      0.43543985593170054,
      0.47287372803484296,
      0.42960551295892047,
      0.3441304865522665,
      0.4728436827769973,
      0.4084141761278015,
      0.44821955070259634,
      0.4526309812263536,
      0.2970910358544212,
      0.4763238113534054,
      0.0,
      0.4771744995531857,
      0.42147983621328744,
      0.398505420139033,
      0.41404610455057234,
      0.4030796596658801,
      0.43651786298260986,
      0.4533250614468689,
      0.3552058089481638,
      0.39521797626742705,
      0.4415702060040856,
      0.34599143003820765,
      0.36414784812858714,
      0.3914481786696189,
      0.422514155135159,
      0.32783662055343865
    ],
    [
      0.36971499146079023,
      0.40637678500776175,
      0.3923149161657953,
      0.41925368276663244,
      0.4573781035210367,
      0.3531798574783276,
      0.3208311961020949,
      0.4056982010547434,
      0.43343558772218405,
      0.41196008825837493,
      0.40922628300498176,
      0.2869012939779436,
      0.44513399061939163,
      0.41144685830767425,
      0.0,
      0.432217751004248,
      0.4122747699137048,
      0.4059375002177883,
      0.4132425086189109,
      0.39594414545077083,
      0.4217396584473203,
      0.37054362707803423,
      0.3976971008670127,
      0.39541787963756336,
      0.3556050376946811,
      0.4269057810847492,
      0.39934176344883254,
      0.4030461581423679,
      0.3094508824114419
    ],
    [
      0.36159602985106165,
      0.4804043861774465,
      0.48592775049511827,
      0.47584252693118745,
      0.4941004760921861,
      0.4348338460820731,
      0.3908248116946378,
      0.4333620762017534,
      0.4760273599174585,
      0.4570308803893879,
      0.4773194737680566,
      0.3288394288951171,
      0.46840293824242796,
      0.45686123378537835,
      0.4605956529872406,
      0.0,
      0.4058696615734265,
      0.451443618654193,
      0.44313248165479213,
      0.4887116958128821,
      0.4762618697688765,
      0.3639986553032979,
      0.3998983897987016,
      0.475564305232955,
      0.35465597343772903,
      0.370371530841072,
      0.42025805040544584,
      0.43097224343695584,
      0.34144236343114165
    ],
    [
      0.34255662273592513,
      0.43034411084832347,
      0.42063328712818393,
      0.4093612888196725,
      0.4738947743658637,
      0.33806087788902994,
      0.2747555137876754,
      0.4242892595922274,
      0.4383847659673423,
      0.3985251271383534,
      0.42688211879366555,
      0.2853985181070331,
      0.39988659862333,
      0.37917965661934616,
      0.3728567551154205,
      0.3417259745330461,
      0.0,
      0.421802499392091,
      0.35804079609513884,
      0.3816103840198102,
      0.4250929771754657,
      0.3606982672329653,
      0.37121781735927395,
      0.4003103554565546,
      0.36530402944844265,
      0.3661558738987061,
      0.4109643505135858,
      0.4038174851522891,
      0.26253011091524225
    ],
    [
      0.3167926296525019,
      0.38762560435633886,
      0.38662784132155625,
      0.39233124435825606,
      0.43759439361430297,
      0.3499887770007186,
      0.3413382965209233,
      0.4245629928057133,
      0.38364357686247685,
      0.39861453417110515,
      0.3521338375651988,
      0.2814460127739449,
      0.4121201646100934,
      0.38253035193020923,
      0.4024980628728423,
      0.3683079487145351,
      0.4060769005869296,
      0.0,
      0.4094372160149511,
      0.3559941297376721,
      0.4221200902253006,
      0.3373331802440285,
      0.3695345262954113,
      0.3635143892684418,
      0.36631165104036634,
      0.38172582137867206,
      0.36422011663709264,
      0.3413958779586028,
      0.3377859114004025
    ],
    [
      0.3157657336820343,
      0.3285991920971427,
      0.3884386779140654,
      0.3244078634032237,
      0.43381319942466146,
      0.38896212123142604,
      0.3364471944775109,
      0.4106595820849095,
      0.35773858897966226,
      0.4360379753001633,
      0.3585112625803759,
      0.27511691341542943,
      0.3753802545903384,
      0.34133571555070286,
      0.33583437762707247,
      0.3245982820755493,
      0.3651807877388922,
      0.3501287565380826,
      0.0,
      0.3962117122559108,
      0.3927158955036514,
      0.30627721841884803,
      0.3134147806067571,
      0.33913719167159906,
      0.31228079850408585,
      0.32455247991740666,
      0.3555920832740618,
      0.33356449406420907,
      0.2808690007257897
    ],
    [
      0.3607403300674059,
      0.430501981573161,
      0.4351322049522366,
      0.4223185547134196,
      0.4099914165866758,
      0.4111117104165456,
      0.3241334407484602,
      0.4310088599110202,
      0.4187300941108758,
      0.3729659020859637,
      0.41956518129151466,
      0.251698987819859,
      0.397120296113026,
      0.3673167791166212,
      0.40966939488549103,
      0.3744343545710973,
      0.3533003007759621,
      0.34114886272592226,
      0.41770046461286836,
      0.0,
      0.4061624217715847,
      0.33214090511436356,
      0.36128828985831585,
      0.4315036573993003,
      0.3270069905340873,
      0.33351625361412496,
      0.3517611739585993,
      0.3430808090657025,
      0.27587433850950194
    ],
    [
      0.31617253106516086,
      0.3464542931176964,
      0.3672609769204851,
      0.3594560116249821,
      0.4675241672073933,
      0.3321265623030747,
      0.2995095305908895,
      0.37660815058526853,
      0.38910775368394535,
      0.42087627974498165,
      0.37175701631749813,
      0.23682663439266172,
      0.3478944621591069,
      0.35126754917541314,
      0.3937539527733156,
      0.3434851389392355,
      0.3529029745787049,
      0.35431574717166536,
      0.3784729634424493,
      0.3767725384256828,
      0.0,
      0.3050648820992108,
      0.3553610015806703,
      0.3710191595601666,
      0.3556721408198198,
      0.34612216088031844,
      0.31908981622244204,
      0.37545126693198205,
      0.2665308402907618
    ],
    [
      0.3551959335295767,
      0.4431026853922635,
      0.3899728144291723,
      0.41175621748147284,
      0.4773467951135941,
      0.4177593241329589,
      0.3968303587687305,
      0.4163383166277457,
      0.4576306484519128,
      0.41497689132147486,
      0.4393580207677783,
      0.38956873298128203,
      0.45871410036353977,
      0.41507836935892795,
      0.4946853655630774,
      0.4173700566456575,
      0.4260202626572871,
      0.4613940432646084,
      0.3994844932184427,
      0.4329329619782645,
      0.44171855946223726,
      0.0,
      0.41343310414757717,
      0.4386182835344372,
      0.41994190953141475,
      0.44499495216591445,
      0.4798425545197005,
      0.41035667041815826,
      0.4242770848948334
    ],
    [
      0.3341536217277572,
      0.3549387415398588,
      0.33706406691412294,
      0.3682163214271619,
      0.40759675465452383,
      0.3420181116614929,
      0.3118677085984227,
      0.3924897370788807,
      0.38949430324473333,
      0.38681312114248767,
      0.37922303788485934,
      0.25550417920844026,
      0.39262971992658247,
      0.3570430605846415,
      0.36914940226470416,
      0.3609152545709253,
      0.35130006255467094,
      0.3781093264502611,
      0.3131510449172632,
      0.35331512747965776,
      0.41336831181716716,
      0.33153795712489886,
      0.0,
      0.3694490009041689,
      0.3380066777186439,
      0.31067089744799814,
      0.3676661025098673,
      0.3579287449034241,
      0.28512889386101525
    ],
    [
      0.3483197820197801,
      0.3283000989942524,
      0.4108126944103936,
      0.3865676667168181,
      0.4015183781980556,
      0.3303507683481486,
      0.28999517064320446,
      0.4014645742560392,
      0.3953016940634664,
      0.38225000717241886,
      0.3509853208298708,
      0.25847146078849836,
      0.3271814361565135,
      0.3190047144315862,
      0.3320391360235788,
      0.32257532858377513,
      0.3626316758096366,
      0.31933425420755834,
      0.3369825114009559,
      0.38181756527833643,
      0.3781080455022756,
      0.29397424094434954,
      0.36991548452968703,
      0.0,
      0.34455406392722465,
      0.3373707186284183,
      0.3329545280745969,
      0.3812774579766576,
      0.29206964339241304
    ],
    [
      0.28845755537979323,
      0.29251993959494116,
      0.2965103411739436,
      0.29588071793945847,
      0.3621359765736032,
      0.31042892618052664,
      0.28697711781221513,
      0.29573344966989734,
      0.31873341897307395,
      0.33480256152068266,
      0.295659680876861,
      0.2857026051369449,
      0.3010192630781958,
      0.289629263426753,
      0.31700108199936783,
      0.29462320993528457,
      0.3542019541540842,
      0.30876880043541965,
      0.3166438443754769,
      0.3036181234414659,
      0.35432702276313255,
      0.2891938276393504,
      0.31401089154702544,
      0.34520511839005463,
      0.0,
      0.2949690793825721,
      0.3363961040585086,
      0.30884266041101327,
      0.2973777041299628
    ],
    [
      0.41681205937998467,
      0.4962384296091258,
      0.44591784619278196,
      0.47983467988548023,
      0.4827146395006834,
      0.3998547603767211,
      0.43245942467816634,
      0.4656663558800347,
      0.4917960614609236,
      0.5268061470575978,
      0.443626553070702,
      0.33549502578569146,
      0.41901906950636736,
      0.3983079381742325,
      0.4958866739318182,
      0.4179337650425494,
      0.425473732064791,
      0.4629613359059692,
      0.4243096583318404,
      0.417415684005366,
      0.44492837248459627,
      0.4238843383721398,
      0.41558137416289553,
      0.46823778960247053,
      0.39089183677369954,
      0.0,
      0.4509538626546794,
      0.413943511508303,
      0.3901125752560426
    ],
    [
      0.33555492668078535,
      0.3664302311532288,
      0.3910904385321843,
      0.3681456808481567,
      0.4224415148619949,
      0.34167310694810027,
      0.3060939738584654,
      0.3718473051111353,
      0.34894391082347376,
      0.39458568334757893,
      0.34072030573928536,
      0.28746963374123236,
      0.3979223306638482,
      0.35058198348505565,
      0.3882180339723873,
      0.36566735722248045,
      0.3982603256497488,
      0.3672318271819992,
      0.36870672284792816,
      0.35787339432324883,
      0.4011374860639132,
      0.29932276548570136,
      0.3950718354430429,
      0.3434704716724888,
      0.3699148674943131,
      0.33797354065374696,
      0.0,
      0.36502243962537073,
      0.2764559125116788
    ],
    [
      0.3240339798251344,
      0.3095442044947152,
      0.3710123618632555,
      0.3676318905437497,
      0.39599436834288015,
      0.34979224119092267,
      0.29452940932597804,
      0.3829345935529156,
      0.36631873682545035,
      0.38409478424263166,
      0.3535215258100233,
      0.2637668154603743,
      0.34769064795550153,
      0.36255971565452505,
      0.38836473476103883,
      0.34125774356110483,
      0.37432215101176314,
      0.35629285081598994,
      0.3490640324101808,
      0.3521653665928033,
      0.42914748462192387,
      0.311151969697538,
      0.38096682749334265,
      0.4064816681881578,
      0.3215740001915961,
      0.3285929616697032,
      0.3709374578490334,
      0.0,
      0.2918024119329481
    ],
    [
      0.33452634580595175,
      0.3413937194893035,
      0.346333169292925,
      0.3686099240694358,
      0.39425259796451884,
      0.33463702077871105,
      0.3796104324977705,
      0.3812413416640541,
      0.3596427614428359,
      0.419776190153893,
      0.36633820166022835,
      0.3434532109615269,
      0.38644230086558573,
      0.35524700066106085,
      0.36923302378746214,
      0.3465831108843509,
      0.32617921106638437,
      0.37624796238571756,
      0.35399450811256217,
      0.36388790910120994,
      0.3925109905917168,
      0.42852145790774276,
      0.37523386946891346,
      0.3783087793895794,
      0.33752333576403326,
      0.40955721351826835,
      0.3861609790890175,
      0.378091153991462,
      0.0
    ]
  ],
  "row_avgs": [
    0.23209557015450252,
    0.43825185942223255,
    0.3763378028938323,
    0.36689888788745273,
    0.38773173733576466,
    0.40991787569735433,
    0.3481702710421593,
    0.33538704232542293,
    0.370168035727606,
    0.3558165697591929,
    0.4025234725799279,
    0.24703850479546757,
    0.36905820262223005,
    0.41472974611930535,
    0.3950791571237557,
    0.43230534681650007,
    0.38158143559728586,
    0.3740573599970924,
    0.3500561476304843,
    0.37539014131798953,
    0.35274487509303515,
    0.42816783966864425,
    0.35388390321852253,
    0.34700458647530397,
    0.31033465142855743,
    0.4384665535948447,
    0.3592081430693776,
    0.35269810485304215,
    0.3690549186559365
  ],
  "col_avgs": [
    0.3407347092612673,
    0.37572690312356344,
    0.3978999640896304,
    0.3985322427420509,
    0.42712990817636987,
    0.3598401781490711,
    0.31825703655422705,
    0.3950866142218061,
    0.3995118043295367,
    0.3923008307735626,
    0.3891120811904908,
    0.28148052296050524,
    0.39279057172627097,
    0.3645086314643614,
    0.3858605723612404,
    0.36663870940801085,
    0.36208112580050067,
    0.3756899011165559,
    0.3727080039013556,
    0.38201955276547084,
    0.4014186197702588,
    0.327156665626488,
    0.35771627966191805,
    0.3928056988594663,
    0.33327334682900844,
    0.35473445692785394,
    0.36575785172637104,
    0.36356817021355947,
    0.299817789172051
  ],
  "combined_avgs": [
    0.2864151397078849,
    0.40698938127289797,
    0.38711888349173135,
    0.3827155653147518,
    0.40743082275606723,
    0.38487902692321274,
    0.3332136537981932,
    0.3652368282736145,
    0.38483992002857137,
    0.3740587002663778,
    0.39581777688520936,
    0.2642595138779864,
    0.3809243871742505,
    0.3896191887918334,
    0.390469864742498,
    0.39947202811225546,
    0.37183128069889326,
    0.37487363055682416,
    0.36138207576591996,
    0.3787048470417302,
    0.37708174743164696,
    0.37766225264756614,
    0.3558000914402203,
    0.36990514266738517,
    0.32180399912878294,
    0.3966005052613493,
    0.3624829973978743,
    0.3581331375333008,
    0.33443635391399373
  ],
  "gppm": [
    586.8461369944463,
    594.264403668896,
    580.0065961059446,
    581.1121839164264,
    566.4513769122417,
    598.3576738603269,
    617.1015243616855,
    581.3123794660013,
    582.9006226986812,
    580.1374816578613,
    588.8387631573332,
    632.8121049954227,
    584.9938367137863,
    597.8948599185267,
    591.6570663582786,
    597.8509403665405,
    596.2770513014275,
    594.8845994976525,
    591.5610077250649,
    585.7377046803442,
    578.5869678331668,
    617.1810047497826,
    599.0874797917822,
    582.1236444660824,
    607.5689821812286,
    601.4465847631616,
    598.3129278247615,
    598.7765065154579,
    625.2713057825715
  ],
  "gppm_normalized": [
    1.3562873881154787,
    1.3514902590047264,
    1.318596390984683,
    1.3166244473783313,
    1.2803567388180999,
    1.3603255443456832,
    1.406421616212882,
    1.3170344215253438,
    1.319500330279738,
    1.3166641457928634,
    1.3368377395012676,
    1.4403994127391464,
    1.3277941443536534,
    1.3540005288899055,
    1.3409146982586686,
    1.3587449763766628,
    1.3503241273573483,
    1.343227485189921,
    1.3386127405000348,
    1.3343223485223312,
    1.301639878030785,
    1.3993030635663535,
    1.3520715170800046,
    1.3208873444877636,
    1.37435420882329,
    1.3641722825937665,
    1.3566760403973348,
    1.3543078044771584,
    1.4189375130495254
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360
  ],
  "response_lengths": [
    2643,
    2173,
    2553,
    2652,
    2536,
    2450,
    2471,
    2489,
    2534,
    2524,
    2487,
    2855,
    2578,
    2366,
    2263,
    2278,
    2378,
    2511,
    2595,
    2618,
    2516,
    1970,
    2376,
    2553,
    2454,
    2041,
    2323,
    2444,
    2070
  ]
}