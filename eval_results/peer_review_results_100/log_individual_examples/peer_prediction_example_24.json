{
  "example_idx": 24,
  "reference": "SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS\n\nTianlong Chen1∗, Zhenyu Zhang1∗, Ajay Jaiswal1, Shiwei Liu1, Zhangyang Wang1 1VITA Group, University of Texas at Austin {tianlong.chen,zhenyu.zhang,ajayjaiswal,shiwei.liu,atlaswang}@utexas.edu\n\nABSTRACT\n\nDespite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and downstream fine-tuning, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on exploring the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers. To this end, we propose a new plug-and-play training framework, SMoEDropout, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a randomly initialized and fixed router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a “self-slimmable” property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments across diverse transformer architectures on a variety of tasks demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks {ASDiv-A, MAWPS, SVAMP}, respectively. Codes and models are available in https://github.com/VITA-Group/Random-MoE-as-Dropout.\n\nINTRODUCTION\n\n1 Scaling neural networks, historically with the blessing of modern hardware, have dramatically improved the state-of-the-art on a wide array of real-world machine learning applications and leaderboards, conforming to the empirical scaling laws (Kaplan et al., 2020), where the final model quality has been found to have a power-law relationship with the amount of data, model size, and compute time. Transformers (Vaswani et al., 2017), swiftly after their introduction, have become de facto choice for many natural language processing (NLP) (Yang et al., 2019c; Liu et al., 2019b; Talmor et al., 2018; Jaiswal et al., 2021; Yang et al., 2019b; Wang et al., 2018; Ding et al., 2019; Chowdhery et al., 2022; Wei et al., 2022) and computer vision (Dosovitskiy et al., 2020; Han et al., 2020; Touvron et al., 2021; Mao et al., 2022; Zheng et al., 2021; Parmar et al., 2018) applications and now their parameter counts are typically measured in billions rather than millions. Unfortunately, this exploitation of parameters actuates a roughly quadratic blow-up in training costs, as both the model size and the number of training examples increase especially for dense advanced transformer-based models (e.g., BERT (Devlin et al., 2018) and GPT (Brown et al., 2020)) and require thousands of GPU days for training. Additionally, these gigantic transformers suffer from the representation collapse issue during vanilla training, which is affirmed by a high degree of parameter redundancy (Guo et al., 2019; Ganesh et al., 2020; McCarley et al., 2019) and observed ineffective usage of the transformer expressiveness (Michel et al., 2019; Chen et al., 2022a).\n\n*Equal Contribution.\n\n1\n\nSparse Mixture-of-Experts (SMoEs) enable efficient scaling of model capacity at a fixed computational cost by performing input-dependent conditional computing. Such property facilitates training transformers with significantly high parameter counts at moderately increased cost, compared to their dense counterparts, resulting in improved training efficiency. For instance, with similar training FLOPS, Switch-Large (Fedus et al., 2021) (a kind of SMoE) is 35× larger than a T5-Large dense model (Raffel et al., 2020). Despite their advantages in mitigating computational and energy footprints, SMoEs have many critical limitations. Firstly, the current learning-based routing mechanisms in SMoEs tend to push hidden representations clustering around expert centroids (Chi et al., 2022), implying a trend toward representation collapse, which in turn leads to redundant experts, inferior expert specialization, thereby substandard performance (Mittal et al., 2022; Chen et al., 2022b). Secondly, SMoEs suffer from poor scalability during inference and downstream fine-tuning prominently due to overfitting of the learned routing policy to the number of activated experts during training. Naive solutions to mitigate such sparsity immutability often lead to performance degradation. As recent research efforts for SMoEs are predominantly focused on improving routing policies to encourage expert specializations, we explore the overlooked scalability bottleneck of SMoEs and ask: Does there exist a principled and pluggable approach to modify SMoE training that can enhance scalability at inference and downstream fine-tuning of large-scale transformers, by dynamically adapting the number of activated experts subject to resource availability?\n\nFigure 1: Bits-Per-Character (↓) on enwik8’s test-set with a 4-layer Transformer-XL. SMoEDropout demonstrates a “self-slimmable” property where inference performance is smoothly boosted along with the increase of activated parameters. Learnable SMoEs tend to overfit certain levels of network capacity. Note that only gray curve is produced by (5) different dense models.\n\nTo this end, this paper proposes a novel plug-and-play training framework, named SMoE-Dropout, to enable scaling transformers to better accuracy in the full capacity setting without collapse. More specifically, SMoE-Dropout employs a fixed router network that is randomly initialized to activate experts and progressively increases their number as training progresses over time. Our simple, yet highly effective strategy has a multi-fold win-win for trained transformers, specifically: ❶ obtaining a “self-slimmable” property during inference and downstream fine-tuning subject to resource availability, which delivers a once-for-all in-situ trade-off between efficiency and performance; ❷ mitigating representational collapse and effectively utilizing the full model capacity, where activating more experts produces superior performance (Figure 1 (blue)); ❸ eliminating the overhead of learning routing policies for SMoE. Note that SMoE-Dropout can be swiftly adapted for training any deep learning network (e.g. CNNs), given some splitting techniques (Zhang et al., 2021), but this work primarily focuses on transformers considering their exploding computational footprints. Our innovative contributions can be summarized as:\n\n⋆ We propose a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers in the full capacity setting without collapse. SMoE-Dropout facilitates the randomly and sparsely activated structure of network modules, playing an implicit regularization role similar to dropout. Our new framework leads to enhanced generalization and reduced training costs (e.g., up to 37% running time savings) compared to the vanilla training of large dense transformers at equivalent parameter counts.\n\n⋆ Transformers trained by SMoE-Dropout naturally exhibit a “self-slimmable” property that displays smooth and consistent performance boosts when increasing activated experts during inference or fine-tuning (Figure 1 (blue)). This property enjoys an “in-situ” trade-off between efficiency and performance at deployment, subject to resource availability.\n\n⋆ Our extensive experiments across representative architectures on a variety of tasks validate the effectiveness of our proposed SMoE-Dropout. Specifically, during pre-training, our approach has {1.37, 4.10}, {2.53, 12.44} and {154.12, 188.00} (×10−2) lower BPC than {vanilla dense training (with the same parameter counts), learned SMoE} for Transformer-XL, BERT, and RoBERTa, respectively; after transferring, SMoEDropout obtains {0.07%, 1.03%, 0.78%, 1.09%} performance improvements for BERT and {−, 5.88%, 0.07%, 5.04%} for RoBERTa, on {CSQA, ASDiv-A, MAWPS, SVAMP} reasoning tasks compared to its dense training counterpart.\n\n2\n\n0.81.01.21.41.61.82.0Parameter Count (×107)1.161.181.201.221.241.261.28Bits-Per-Character (BPC) Densely Training w. DropoutTraining w. Learnable SMoETraining w. SMoE-DropoutFigure 2: Overview of our proposed SMoE-Dropout. Left describes the standard transformer layer, consisting of multi-head attention and multi-layer perceptron (MLP) components. Middle Left shows the process of modulization. It splits the original MLP evenly and constructs a series of experts which are smaller MLPs with a reduced hidden dimension. Middle Right presents the overall procedure of SMoE-Dropout. The random router selects the top-k experts given a token embedding and then reweights the features from activated experts. In the end, a summation is conducted to aggregate all features. Right displays the gradually increased number of chosen experts, along with the training procedure.\n\n2 RELATED WORKS\n\nMixture of Experts (MoE). MoE is a special kind of neural network, where its parameters are partitioned into a series of sub-modules (commonly referred to as experts), and conditional computation is then performed in an input-dependent fashion (Jacobs et al., 1991; Jordan & Jacobs, 1994; Chen et al., 1999; Yuksel et al., 2012). The traditional dense MoEs are computationally intensive, as they adopt all experts for each input (Eigen et al., 2013). Fortunately, recent investigations (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021) have proved the effectiveness of MoEs with sparsely activated experts (i.e., SMoE) at both training and inference stages, which greatly trim down the cost and scale language models to enormous sizes like trillions of parameters (Fedus et al., 2021). This efficient fashion of SMoEs gains increasing popularity in various NLP (Shazeer et al., 2017; Lepikhin et al., 2020; Zhou et al., 2022; Zhang et al., 2021; Zuo et al., 2022; Jiang et al., 2021) and vision (Riquelme et al., 2021; Eigen et al., 2013; Ahmed et al., 2016; Gross et al., 2017; Wang et al., 2020; Yang et al., 2019a; Abbas & Andreopoulos, 2020; Pavlitskaya et al., 2020) tasks.\n\nHowever, its sparse-gated manner incurs several downsides, including: (1) Unstable training. Zoph et al. (2022) pointed out that while techniques like gradient clipping can stabilize SMoE training, they often result in lower quality. The router z-loss (Zoph et al., 2022) is a preferred solution for achieving both improved performance and stability. (2) Poor specialization. One of the intriguing goals of SMoE is to divide-and-conquer the learning task by solving each piece of the task with adaptively selected experts (Aoki et al., 2021; Hazimeh et al., 2021; Ma et al., 2018; Mittal et al., 2022). To encourage specialization and decrease redundancy among experts (Chen et al., 2022b), Dai et al. (2022) pre-defined the expert assignment for different input categories, while Hazimeh et al. (2021) advocated multiple, diverse router policies. (3) Representation collapse and load imbalance among experts. As the primary issue of learning-based SMoEs, various approaches have been proposed to mitigate their negative effects. Shazeer et al. (2017) injected Gaussian noises into gating networks to promote the routing balance. Later, Lepikhin et al. (2020); Fedus et al. (2021) applied an auxiliary loss of load balancing regularizers; Lewis et al. (2021) performed the routing by dealing with a linear assignment problem; Clark et al. (2022) utilized reinforcement learners; Zhou et al. (2022) routed top-k inputs per expert instead of selecting top experts per input sample. Beyond learned routing policies, Roller et al. (2021) and Zuo et al. (2022) designed deterministic hashing and stochastic assignments, respectively, which eliminate the necessity for router networks.\n\nZuo et al. (2022), one closely related prior work, also endorsed the advantage of stochastic expert assignment. They randomly activate experts for each input during training and inference, which leads\n\n3\n\n......NormalizationMulti-Head AttentionNormalizationMulti-Layer Perceptron...Multi-Layer PerceptronExpertExpertExpertTransformer LayerExpertExpertExpertExpertExpert......SplittingModulization......Token Embedding...Random RouterExpertExpertExpertExpert...Point-wise AdditionPoint-wise AdditionPoint-wise MultiplicationSummationSMoE-Dropout......Gradually Increased kAlong withTrainingto inconsistent inference results. To address the prediction randomness, Zuo et al. (2022) employed a consistent regularized loss to penalize the discrepancy among different experts. However, such regularization is prone to redundancy in SMoEs and sacrifices the network capacity. In our proposal, the fixed router with random weights generates deterministic inferences. Meanwhile, the presented “self-slimmable” attribute suggests the full models’ expressiveness is adequately exploited.\n\nDropout and Other Training Techniques for Transformers in NLP. Dropout (Srivastava et al., 2014) was developed to prevent overfitting in over-parameterized networks during training, by randomly omitting neurons and their corresponding connections. Follow-up studies develop plenty of dropout variants (Zhang & He, 2020; Wan et al., 2013; Ba & Frey, 2013; Kingma et al., 2015; Gal et al., 2017; Wu & Gu, 2015; Tompson et al., 2015; DeVries & Taylor, 2017; Park & Kwak, 2016; Semeniuta et al., 2016). In parallel, McAllester (2013); Mou et al. (2018); Mianjy & Arora (2020); Zhang & Xu (2022); Neklyudov et al. (2017); Gal & Ghahramani (2016) have devoted themselves in deriving the theoretical foundation for dropout and explaining its implicit regularization impacts.\n\nOther notorious bottlenecks of transformer training primarily stem from overfitting and instability caused by poor optimization (Zhang et al., 2020; Liu et al., 2019a; 2020a), insufficient or heterogeneous downstream data (Variˇs & Bojar, 2021; Zhang & Vaidya, 2021), etc.. Accordingly, numerous remedies are developed to address the issues. For example, data augmentations (Sun et al., 2020), improved initialization (Liu et al., 2020b;a; Xu et al., 2020; Zhu et al., 2021), upgraded normalization (Wang et al., 2022; Yang et al., 2022), enhanced optimizers (Cohen et al., 2022), weight decay (Loshchilov & Hutter, 2017), and early stopping.\n\n3 METHODOLOGY\n\n3.1 PRELIMINARY\n\nSparse Mixture of Experts (SMoEs). SMoE models leverage conditional computation to activate different subsets of a network for different inputs. A building block of SMoEs is the expert layer including a multi-head attention block and multiple experts in parallel. In this work, we consider SMoE for Transformers, where SMoE layers are incorporated into contiguous Transformer blocks. SMoE expert can be normally constructed by either splitting the vanilla MLP of transformers into smaller pieces (Zhang et al., 2021) or replicating the MLP (Fedus et al., 2021). Most existing SMoE works mainly concentrate on the MLP component in transformers since MLPs constitute roughly 2/3 of total model parameters counts storing substantial amounts of learned knowledge as memory networks (Geva et al., 2020; Dai et al., 2021).\n\nLet {Ei}N i=1 denote the experts, where i is the index of expert and N is the total number of experts. A gating network or router R is inserted to choose the top-k experts with the largest scores R(x)i, and x represents the input embedding. Usually, k ≪ N, which implies a sparsely activated setting. Specifically, the resultant output of the expert layer can be depicted as follows:\n\ny =\n\nk (cid:88)\n\nj=1\n\nR(x)j · Ej(x); R(x) = TopK(softmax(G(x)), k); TopK(v, k) =\n\n(cid:26) v if v is the top k otherwise\n\n0\n\n(1)\n\nwhere G is the critical part of a router R. For a learnable routing, G is a neural network that can be one or a few layers MLP (Shazeer et al., 2017; Fedus et al., 2021). Ej(x) stands for features from the expert Ej. It will be further summed with a scaling coefficient R(x)j to form the final output y. The TopK function maintains the largest k values and sets the reset elements to zero. In practice, a load or important balancing loss (Shazeer et al., 2017) is employed to avoid the representation collapse issue, i.e., always picking the same experts for different inputs and ignoring others.\n\nDropout and its variants. Dropout is a conventional training technique employed to alleviate the risk of overfitting. The vanilla dropout is typically applied to fully connected layers with a dropping probability p. During each training iteration, neurons will be disabled with the probability p. In other words, the omission of neurons follows a Bernoulli(p) distribution. As for the inference phase, there is no dropout and all neurons are activated. To counterbalance the surplus information during training, the output logits are reweighted by 1 − p. In this paper, we selected two representatives among diverse proposed dropout variants, concrete dropout (Gal et al., 2017) and dropblock (Ghiasi et al., 2018) as our comparison baselines. ▷ Concrete Dropout. It replaces the discrete Bernoulli(p) distribution of dropout with a continuous relaxation, i.e., Concrete distribution, and allows an automatic tuning of the dropping probability p. For example, considering the one-dimensional case, as shown in Gal et al. (2017), a\n\n4\n\nConcrete random variable z is described as z = sigmoid(cid:0) 1 t × (cid:0)log(p) − log(1 − p) + log(u) − log(1 − u)(cid:1)(cid:1), where u ∼ Unif(0, 1) is a uniform random variable and t denotes a temperature hyperparameter. Note that parameter p is optimized in a data-driven way. ▷ DropBlock. Instead of performing Bernoulli dropping per feature map, Ghiasi et al. (2018) applies it in areas within feature maps. They claim that DropBlock improves the generalization and limits overfitting by hiding certain areas of features or input samples.\n\n3.2 A NEW TRAINING PIPELINE: SMOE-DROPOUT\n\nModulization. The first step in our SMoE-Dropout, turns a large densely connected MLP into multiple smaller MLPs with the same size, as demonstrated in Figure 2. Without loss of generality, in Figure 2, we use a single-layer MLP f with a dimension d for illustrations. After the modulization, it is divided into a set of MLPs {E1, E2 · · · , EN}, where they have the same hidden dimension d N . Random Routing Policy. Few prior works have investigated some form of random routing policies, such as Roller et al. (2021) utilizes a hash table to enforce a pre-defined deterministic random mapping from inputs to experts and Zuo et al. (2022) adopts a fully random assignment in each training iteration. Although they have shown some benefits from random policies, both methods suffer from inconsistent inference predictions, and can not outperform the densely trained models with equivalent parameter counts. In contrast, our proposed framework, SMoE-Dropout considers a randomly initialized and fixed router network to guide token assignment. Different from previous works, our proposal’s assignment is (1) implicitly optimized during training, since feature embeddings remain updated for the same input sample; (2) deterministic during inference thanks to the fixed weights in R. Extensive results in Section 4 verify the superiority of our proposal, compared to existing random policies and the dense baseline with the same model parameters. Additionally, another crucial design in SMoE-Dropout’s routing is the progressively enlarged number of activated experts (k). Riquelme et al. (2021); Jiang et al. (2021) reveal that altering k in the inference phase incurs significant performance degradation if the SMoE is learned with a fixed k. For example, (Riquelme et al., 2021)’s SMoE trained with k = 1 has 20% ∼ 30% accuracy drops on ImageNet, when activating k ≥ 7 experts during the evaluation. This drawback substantially restricts the practical use of SMoEs because diverse real-world scenarios require different resource budgets, necessitating flexible and effective network capacity during inference. To tackle this limitation, we adopt a training strategy that gradually enriches the active network capacity by linearly increasing the number of selected experts k during training. This approach coincides with the principle of curriculum learning and provides the attractive “self-slimmable” ability, which consistently boosts performance for transformers as the number of activated experts increases during inference and downstream fine-tuning, as shown in Figure 1. SMoE-Dropout. Our effective proposal comprises three simple and highly effective steps, as described in Figure 2. First, it divides the MLP into a series of MLPs with a reduced size for modulization (Middle Left of Figure 2). Then, a random policy parameterized by fixed weights is introduced to route token embeddings to k experts with the largest response (Middle Right of Figure 2). Finally, it progressively actives more experts, preventing the overfitting to the amounts of used network capacity during training. (Right of Figure 2).\n\n4 EXPERIMENT\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nNetwork Architectures and Comparison Baselines. In our experiments, we have adopted three representative transformer-based networks, including BERT (Devlin et al., 2018), TransformerXL (Dai et al., 2019), and RoBERTa (Liu et al., 2019b). Specifically, we use double-size BERTbase / RoBERTabase that have 12 transformer layers, 768-dimensional encoder layers, 6144-/3072dimensional feed-forward networks (MLPs), and 12 attention heads. For both Transformer-XL, we choose a reduced size due to limited resources, which has 4 layers, 256-dimensional encoder layers, 8192-dimensional feed-forward networks, and 8 attention heads with a head size of 64.\n\nFor sufficient comparisons with our proposal, Training w. SMoE-Dropout, we consider five baselines: (i) Densely Training w. Dropout, where the vanilla dropout is applied to feed-forward networks (MLPs); (ii) Densely Training w. Concrete Dropout (Gal et al., 2017); (iii) Densely Training w. DropBlock (Ghiasi et al., 2018). Note that both Concrete Dropout and DropBlock are inserted in feed-forward networks, replacing the vanilla dropout; (iv) Training w. Learnable SMoE (Fedus et al., 2021); (v) Training w. THOR (Zuo et al., 2022), where THOR is another random SMoE\n\n5\n\nthat randomly activates a pair of experts for each input sample and adopts an auxiliary consistency regularization based on Kullback-Leibler (KL) divergence. To compute the regularization term, two forward processes are needed in each training iteration. Pre-Training. ▷ Datasets. Transformer-XL is pre-trained on enwik8 (Mahoney, 2011) dataset, while we use BooksCorpus (Zhu et al., 2015) for BERT and RoBERTa. ▷ Training Configurations. For Transformer-XL, we follow the official training setups, using Adam optimizer and the learning rate starts from 2.5 × 10−4 and decreases according to a cosine annealing scheduler. We use a batch size of 22 and optimize the network for 4 × 105 iterations. As for BERT pre-training, we adopt an AdamW optimizer with an initial learning rate of 5 × 10−5 that linearly decays to 0. The batch size and total training steps are 64 and 1 × 105, respectively. RoBERTa’s pre-training configurations strictly follow the default from HuggingFace1, but with reduced training steps of 1 × 105. Moreover, we conduct a grid search and set the coefficient of THOR’s regularization term as 2. Similarly, the temperature in Concrete dropout is t = 0.1. ▷ Evaluation Metrics. Since both performance and efficiency are essential, we assess the pre-training performance via Bits-Per-Character (BPC) on the hold-out validation set, where a smaller BPC value indicates a better pre-training; and we report training time per iteration & the number of floating point operations (FLOPs) of singlesample inference, for evaluating the efficiency. {1 RTX A6000, batch size 22} and {8 V100, batch size 64} are adopted for time measurements of Transformer-XL and BERT/RoBERTa, respectively. Downstream Fine-Tuning. ▷ Datasets. Five benchmarks across three downstream tasks are examined in this paper, including text classification (SST-2 (Socher et al., 2013)), arithmetic reasoning (ASDiv-A (Miao et al., 2020), MAWPS (Koncel-Kedziorski et al., 2016), SVAMP Patel et al. (2021)), and commonsense reasoning (CSQA (Talmor et al., 2018)). ▷ Training Configurations. We perform dense fine-tuning for all approaches. Given a downstream parameter budget, SMoE-based methods will select the most voted experts based on their routing policies. Detailed training setups are listed as follows. We fine-tune the pre-trained Transformer-XL with a smaller learning rate of 1 × 10−4 and a batch size of 64 on SST-2 benchmark. And for BERT and RoBERTa, we fine-tune the models on the aforementioned four reasoning datasets. The learning rate is fixed at 2 × 10−5 and the batch size is 64. In each downstream task, the fine-tuning continues for 3 epochs, while other configurations are kept the same as the ones in pre-training. ▷ Evaluation Metrics. At the evaluation phase, accuracy (%) and the problem solving rate (%) (Wei et al., 2022) are reported on the test set of SST-2 and other reasoning tasks, respectively.\n\n4.2 SUPERIOR PERFORMANCE OF SMOE-DROPOUT\n\nWe adopt classical transformer-based models, i.e., {Transformer-XL, BERT, RoBERTa}, and train them in a dense or SMoE-based manner on {enwik8, BookCorpus, BookCorpus}. Evaluation results are summarized in Table 1, where all models are compared under the same number of parameter counts. The following observations can be drawn: ❶ Our SMoE-Dropout demonstrates superior performance compared to all other training algorithms. Specifically, SMoE-Dropout with all experts selected obtains 1.37 ∼ 18.49, 0.56 ∼ 12.44, and 152.82 ∼ 188.00 (×10−2) lower BPC for Transformer-XL, BERT, and RoBERTa, respectively. This validates the effectiveness of our proposals. ❷ Appropriate random routing policies show consistent performance benefits across all three network backbones. Moreover, our randomly weighted router surpasses the completely random allocation in THOR, which is within expectation since our assignment is implicitly “optimized” using evolved feature embeddings. ❸ In terms of training efficiency, SMoE-Dropout has up to 21%, 37%, and 25% training time savings compared to the dense training of three backbones. If only half of the experts (k = N 2 ) are activated, our approach enjoys extra 23% ∼ 34% inference FLOPs reduction with a comparable BPC. Although the learnable SMoE reaches the best efficiency, it results in inferior performance.\n\nBesides, we report another group of experiments varying the expert numbers (parameter counts) during evaluation. As shown in Figure 3, for SMoE-based approaches, we directly change the number of activated experts at the inference stage, which is an in-situ fashion from the single trained transformer. While for dense training baselines, each dot in their curve requires a separately trained model since it does not allow modifications of network capacity without further fine-tuning. Our findings are as follows: ❶ The performance of SMoE-Dropout is stably improved along with more parameters used, and it outperforms the others after 1.0, 10, and 8 (×107) parameter counts for three backbones. Such “slimmable” property enables scaling transformers to the full capacity without\n\n1https://huggingface.co/docs/transformers/model_doc/roberta.\n\n6\n\nTable 1: Testing performance of {Transformer-XL, BERT, RoBERTa} network backbones on {enwik8, BookCorpus, BookCorpus} datasets, respectively. All models are compared under the same number of parameter counts. Training time (s) and inference FLOPs (×1010) are reported. For THOR (Zuo et al., 2022), SMoE, and SMoE-Dropout, evaluations are performed with half (k = N\n\n2 ) or all (k = N) experts activated.\n\nMethods\n\nTransformer-XL\n\nBERT\n\nRoBERTa\n\nBPC (↓)\n\nTime\n\nInfer. FLOPs BPC (↓)\n\nTime\n\nInfer. FLOPs BPC (↓)\n\nTime\n\nInfer. FLOPs\n\nDense w. Dropout Dense w. Concrete Dropout Dense w. DropBlock THOR (k = N) SMoE (k = N)\n\nSMoE-Dropout (k = N 2 ) SMoE-Dropout (k = N)\n\n1.1623 1.3335 1.2468 1.3110 1.1896\n\n1.1776 1.1486\n\n5.1298 6.3519 5.3902 4.8830 4.7982\n\n5.0220 5.0220\n\n7.7579 7.7579 7.7579 7.7620 7.7620\n\n5.6145 7.7620\n\n7.6546 7.6419 7.6349 7.6434 7.7537\n\n7.6372 7.6293\n\n0.2088 0.3031 0.2119 0.1439 0.1387\n\n0.1905 0.1905\n\n135.72 135.72 135.72 135.73 135.73\n\n89.330 135.73\n\n8.0903 8.0820 8.0773 8.0778 8.4291\n\n6.7693 6.5491\n\n0.1898 0.2410 0.1934 0.1607 0.1538\n\n0.1799 0.1799\n\n101.75 101.75 101.75 101.76 101.76\n\n78.558 101.76\n\nFigure 3: Testing performance over # parameter counts of {Transformer-XL, BERT, RoBERTa} networks on {enwik8, BookCorpus, BookCorpus} datasets, respectively. A smaller BPC suggests a better model.\n\ncollapse, bringing a once-for-all trade-off respected to inference resource availability. ❷ In contrast, learnable SMoE’s and THOR’s BPC are quickly saturated and deteriorated when adopting more experts, which implies the existence of expert redundancy (or representation collapse). The potential reasons for their substandard results are (i) the overfitting to fixed # experts utilized during training for learnable SMoE, (ii), and the consistency regularization between experts’ predictions for THOR.\n\n4.3 TRANSFER STUDY OF SMOE-DROPOUT: SELF-SLIMMABLE\n\nWe further investigate SMoE-Dropout and its intriguing “self-slimmable” property in a transfer learning scenario. Pre-trained models from Section 4.2 are densely fine-tuned on various downstream tasks, including text classification {SST-2} and challenging arithmetic & commonsense reasoning {CSQA, ASDiv-A, MAWPS, SVAMP}. The performance2 is collected in Table 2. We find: equipped with SMoE-Dropout, Transformer-XL achieves 0.47% ∼ 2.43% accuracy improvements on SST-2, BERT / RoBERTa obtain {0.07% ∼ 9.72%, 0.42% ∼ 3.78%, 0.26% ∼ 1.30%, 1.09% ∼ 4.90%} and {−, 2.10% ∼ 5.88%, 0.07% ∼ 0.27%, 5.04% ∼ 5.93%} performance boosts on {CSQA, ASDiv-A, MAWPS, SVAMP} respectively, suggesting an enhanced transferability.\n\nSimilarly, we alter the model capacity during downstream fine-tuning. Starting from one pretraining, the SMoE-based method first calculates the selected times of each expert based on one feedforward pass with downstream data, then chooses the top activated experts to meet certain parameter budgets, and performs the subsequent dense fine-tuning. As displayed in Figure 4, our SMoE-Dropout has a continually increased accuracy or problem-solving rate when involving more parameters, and clearly surpasses the rest of approaches at parameter counts beyond 0.8, 8, and 10.5 (×107) for Transformer-XL, BERT, and RoBERTa respectively. It shows a flexible capacity adjustment, i.e., “self-slimmable”, according to the downstream resource constraint.\n\n4.4 EXTRA INVESTIGATION AND ABLATION STUDY\n\nQ1: When does SMoE-Dropout outperform other baselines? A1: Sufficient Model Capacity.\n\nTo answer Q1 and understand SMoE-Dropout’s superiority in diverse scenarios, we investigate our proposal with different model capacities by varying model depth (e.g., layers) & width (e.g., experts). 2Due to limited computation resources, {our, official} pre-trained BERT/RoBERTa models are produced with {105, 106} training iterations, {128, 256} batch size, {MLM, MLM and NSP} tasks, on {BookCorpus (800M words), BookCorpus (800M words) and English Wikipedia (2, 500M words)} dataset, respectively. The huge gap of pre-training outlays justifies the difference between our and official performance.\n\n7\n\nRoBERTaTransformer-XLBERTTable 2: Transfer performance {Accuracy (% ↑), Problem Solving Rate (% ↑)} of {Transformer-XL, BERT, RoBERTa} networks on {SST-2, CSQA, ASDiv-A, MAWPS, SVAMP} datasets. All models are compared under the same number of parameter counts. The same densely fine-tuning is adopted for all approaches, while THOR, SMoE, and SMoE-Dropout are tuned with half (k = N\n\n2 ) or all (k = N) experts activated.\n\nMethods\n\nTransformer-XL\n\nBERT\n\nRoBERTa\n\nSST-2\n\nCSQA\n\nASDiv-A\n\nMAWPS\n\nSVAMP\n\nASDiv-A\n\nMAWPS\n\nSVAMP\n\nDense w. Dropout THOR (k = N) SMoE (k = N)\n\nSMoE-Dropout (k = N 2 ) SMoE-Dropout (k = N)\n\n81.94 81.13 79.98\n\n81.60 82.41\n\n30.44 20.79 29.27\n\n30.32 30.51\n\n55.27 52.52 55.88\n\n54.97 56.30\n\n80.47 79.95 80.73\n\n80.99 81.25\n\n34.24 30.43 33.15\n\n33.65 35.33\n\n49.58 53.36 52.10\n\n52.94 55.46\n\n78.06 77.86 77.86\n\n76.30 78.13\n\n28.90 28.44 27.98\n\n31.19 33.94\n\nFigure 4: Transfer performance over # parameter counts of {Transformer-XL, BERT, RoBERTa} networks on downstream {SST-2, CSQA, ASDiv-A, MAWPS, SVAMP} datasets, respectively. Only the fine-tuning of Dense w. Dropout needs multiple pre-trained models with different amounts of network capacity.\n\nModel Depth - Different Number of Layers. We conduct experiments on enwik8 dataset with Transformer-XL that has 2, 4, 8, 12 layers and each layer is turned into the SMoE layer through a modularization. The comparison results of Densely Training w. Dropout and Training w. Learnable SMoE are reported in Figure 5 (a). We find that densely trained transformer performs the best when the network capacity is small like 2 layers, while with sufficiently large model capacity (≥ 4 layers), SMoE-Dropout demonstrates a consistent advantage compared to the others. Meantime, along with the increase of layers, the performance gap of SMoEs between the learned policy and our random policy keeps enlarging, signifying SMoE-Dropout’s better scalability.\n\nModel Width - Different Number of Experts. Similarly, we study the influence of model capacity by examining Transformer-XL with different widths of 2, 4, 8, 16 experts. Results are summarized in Figure 5 (b). Consistent observations can be drawn that: (i) Densely Training w. Dropout outperforms SMoE-based training under small network widths such as ≤ 8 experts; (ii) SMoEDropout presents enhanced performance when applied to large models with 16 experts; (iii) Learnable routing policies are effective with a small number of experts like ≤ 8 experts, while it gets worse results than our random routing with a sufficient number of experts, e.g., 16 experts. Q2: What is a better SMoE-Dropout design? A2: Random Weight Router; Later-layer SMoE. To answer Q2, we focus on the main constituents of SMoE-Dropout: Modularization, Random Routing Policies, and Gradually Increased k. Comprehensive ablations are depicted below.\n\nAblation on Diverse Random Routing Policies. An appropriate design of random routing policies determines the achievable performance of SMoE-Dropout. We compare our random initialized and fixed router to SMoE with fully random assignments (Zuo et al., 2022) and random hash SMoE with a pre-defined deterministic random assignment (Roller et al., 2021). Transformer-XL results on enwik8 are collected in Fig. 5 (c), where our proposed random routing obtains substantially lower BPC of 2.96 ∼ 170.11 (×10−2) than the other two under different amounts of model parameters. Ablation on w./w.o. Gradually Increased k. Figure 5 (d) investigates SMoE-Dropout variants with and without gradually increased k. We see that disabling the progressive manner of enlarg-\n\n8\n\nBERT on MAWPSBERT on ASDiv-ABERT on CSQATransformer-XL on SST-2RoBERTa on ASDiv-ARoBERTa on MAWPSRoBERTa on SVAMP BERT on SVAMP Figure 5: Extra studies about SMoE-Dropout. Testing BPC of Transformer-XL is collected on enwik8. (a) and (b) investigate diverse training mechanisms under different model depths and widths, respectively. (c) is the ablation of random routing policies. (d) examines the effects of gradually increased k. (e) studies the appropriate locations to insert SMoE expert layers.\n\ning the number of activated experts causes unsatisfied performance. Also, as a result, the “selfslimmable” property completely disappears, e.g., adopting all model parameters leads to worse BPC. Ablation on Different Positions for Modularization. It remains mysterious where is the best position to insert SMoE layers. To address this question, we perform modularization to different transformer layers and record their performance in Figure 5 (e). Specifically, given a 4-layer Transformer-XL, we compare four options: (i) Early, the first two layers are SMoE layers; (ii) Middle, the 2nd and 3rd layers are SMoE layers; (iii) Later, the last two layers are SMoE layers; (iv) Every-2, there is one SMoE layer every two transformer layers, i.e., the 2nd and 4th layers. From the results, introducing SMoEs to later layers is in general more beneficial than modulizing earlier transformer layers. One possible reason is that shallow layers might capture common features that need to be shared across input samples. More dissections are left for future works. Q3: Extra benefits from SMoE-Dropout? A3: Improved Distillation and Less Overfitting. Distilling into Single Expert on Downstream Tasks. Besides all the benefits in pre-training inference and downstream transfer, we explore additional advantages of SMoE-Dropout under the distillation scheme that is usually preferred in resource-limited applications. As shown in Table 3, we distill all pre-trained Transformer-XLs into the same smaller variant with a single expert on the SST-2 downstream task. Our algorithm produces the most distillable models among all four methods by a clear accuracy margin of 0.76% ∼ 1.89%. Overfitting. We investigate the potential for overfitting to the training data distribution as model parameters increase in SMoEDropout, SMoE, and densely trained transformers. As shown in Figure 5 (a) and (b), experiments are conducted on enwik8 with Transformer-XL, and three approaches are compared under the same parameter counts. We observe both SMoE-Dropout and Densely Training w. Dropout do not exhibit any indication of overfitting. That is, the performance is consistently improved as we increase the layers from 2 to 12 or experts from 2 to 16. In contrast, Training w. Learnable SMoE incurs BPC deterioration owing to overfitting when we expend the transformer to 12 layers, similar to the findings in Zoph et al. (2022). We attribute the reduced overfitting to the implicit regularization effect of SMoE-Dropout and Dropout.\n\nTable 3: Distillation results of Transformer-XL on SST-2.\n\nDense w. Dropout THOR SMoE\n\n81.25 80.76 80.12\n\nSMoE-Dropout\n\nAccuracy (↑)\n\nMethod\n\n82.01\n\n5 CONCLUSION\n\nIn this paper, we present a novel plug-and-play SMoE-Dropout strategy for training overparameterized transformers in full-capacity settings without collapse. We design a fixed and randomly initialized router to assign experts and gradually increase their number along with the training. As a result, our proposal provides an appealing “self-slimmable” property to large transformers during inference and downstream fine-tuning, depending on available resources. It implies alleviated representation collapse and delivers an in-situ trade-off between efficiency and performance. Extensive experiments across various combinations of network backbone and dataset, consistently demonstrate the significantly improved performance and training time savings from our algorithm. Future work includes the extension of other network architectures and tasks like vision recognition.\n\nACKNOWLEDGEMENT\n\nThe research of ZW is in part supported by the US Army Research Office Young Investigator Award (W911NF2010240).\n\n9\n\n(a)(b)(c)(d)(e)REFERENCES\n\nAlhabib Abbas and Yiannis Andreopoulos. Biased mixtures of experts: Enabling computer vision inference under data transfer limitations. IEEE Transactions on Image Processing, 29:7656–7667, 2020.\n\nKarim Ahmed, Mohammad Haris Baig, and Lorenzo Torresani. Network of experts for large-scale image categorization. In European Conference on Computer Vision, pp. 516–532. Springer, 2016.\n\nRaquel Aoki, Frederick Tung, and Gabriel L Oliveira. Heterogeneous multi-task learning with expert\n\ndiversity. arXiv preprint arXiv:2106.10595, 2021.\n\nJimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. Advances in\n\nneural information processing systems, 26, 2013.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nKe Chen, Lei Xu, and Huisheng Chi. Improved learning algorithms for mixture of experts in multi-\n\nclass classification. Neural networks, 12(9):1229–1252, 1999.\n\nTianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, and Zhangyang Wang. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12020–12030, 2022a.\n\nTianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, arXiv preprint\n\nand Furu Wei. Task-specific expert pruning for sparse mixture-of-experts. arXiv:2206.00277, 2022b.\n\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, and Furu Wei. On the representation collapse of sparse mixture of experts. arXiv preprint arXiv:2204.09179, 2022.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. arXiv preprint arXiv:2202.01169, 2022.\n\nJeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Knowledge neurons in pretrained\n\ntransformers. arXiv preprint arXiv:2104.08696, 2021.\n\nYong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan, Cong Zhou, Jingquan Wang, Zhangyin Feng, Fan Zhang, Xueyu Hu, and Shuming Shi. One model, multiple modalities: A sparsely activated approach for text, sound, image, video and code. arXiv preprint arXiv:2205.06126, 2022.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\n\nwith cutout. arXiv preprint arXiv:1708.04552, 2017.\n\n10\n\nMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop\n\nreading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\n\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\n\nmodels with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n\nYarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent\n\nneural networks. Advances in neural information processing systems, 29, 2016.\n\nYarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. Advances in neural information process-\n\ning systems, 30, 2017.\n\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming Chen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. Compressing large-scale transformer-based models: A case study on bert. arXiv preprint arXiv:2002.11985, 2020.\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\n\nkey-value memories. arXiv preprint arXiv:2012.14913, 2020.\n\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock: A regularization method for convolu-\n\ntional networks. Advances in neural information processing systems, 31, 2018.\n\nSam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale weakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6865–6873, 2017.\n\nFu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi Wang. Reweighted proximal\n\npruning for large-scale language representation. arXiv preprint arXiv:1909.12486, 2019.\n\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao. A survey on visual transformer. ArXiv, abs/2012.12556, 2020.\n\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\n\nlocal experts. Neural computation, 3(1):79–87, 1991.\n\nAjay Jaiswal, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, and Ying Ding. Radbert-cl: Factually-aware contrastive learning for radiology report classification. Proceedings of machine learning research, 158:196–208, 2021.\n\nHao Jiang, Ke Zhan, Jianwei Qu, Yongkang Wu, Zhaoye Fei, Xinyu Zhang, Lei Chen, Zhicheng Dou, Xipeng Qiu, Zikai Guo, et al. Towards more effective and economic sparsely-activated model. arXiv preprint arXiv:2110.07431, 2021.\n\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\n\nNeural computation, 6(2):181–214, 1994.\n\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.\n\n11\n\nDurk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-\n\nzation trick. Advances in neural information processing systems, 28, 2015.\n\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152–1157, 2016.\n\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pp. 6265–6274. PMLR, 2021.\n\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019a.\n\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the diffi-\n\nculty of training transformers. arXiv preprint arXiv:2004.08249, 2020a.\n\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural ma-\n\nchine translation. arXiv preprint arXiv:2008.07772, 2020b.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1930–1939, 2018.\n\nMatt Mahoney. Large text compression benchmark, 2011.\n\nZhiyuan Mao, Ajay Jaiswal, Zhangyang Wang, and Stanley H. Chan. Single frame atmospheric turbulence mitigation: A benchmark study and a new physics-inspired transformer model. ArXiv, abs/2207.10040, 2022.\n\nDavid McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307.2118,\n\n2013.\n\nJ. S. McCarley, Rishav Chakravarti, and Avirup Sil. Structured pruning of a bert-based question\n\nanswering model. arXiv preprint arXiv:1910.06360, 2019.\n\nPoorya Mianjy and Raman Arora. On convergence and generalization of dropout training. Advances\n\nin Neural Information Processing Systems, 33:21151–21161, 2020.\n\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975–984, 2020.\n\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances\n\nin neural information processing systems, 32, 2019.\n\nSarthak Mittal, Yoshua Bengio, and Guillaume Lajoie. Is a modular architecture enough? arXiv\n\npreprint arXiv:2206.02713, 2022.\n\n12\n\nWenlong Mou, Yuchen Zhou, Jun Gao, and Liwei Wang. Dropout training, data-dependent regIn International conference on machine learning, pp.\n\nularization, and generalization bounds. 3645–3653. PMLR, 2018.\n\nKirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. Advances in Neural Information Processing Systems, 30, 2017.\n\nSungheon Park and Nojun Kwak. Analysis on the dropout effect in convolutional neural networks.\n\nIn Asian conference on computer vision, pp. 189–204. Springer, 2016.\n\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam M. Shazeer, Alexander Ku,\n\nand Dustin Tran. Image transformer. In ICML, 2018.\n\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\n\nword problems? arXiv preprint arXiv:2103.07191, 2021.\n\nSvetlana Pavlitskaya, Christian Hubschneider, Michael Weber, Ruby Moritz, Fabian Huger, Peter Schlicht, and Marius Zollner. Using mixture of expert models to gain insights into semantic In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern segmentation. Recognition Workshops, pp. 342–343, 2020.\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ́e Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34, 2021.\n\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. Hash layers for large sparse models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=lMgDDWb1ULW.\n\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. Recurrent dropout without memory loss.\n\narXiv preprint arXiv:1603.05118, 2016.\n\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\nLichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Philip S Yu, and Lifang He. Mixuptransformer: dynamic data augmentation for nlp tasks. arXiv preprint arXiv:2010.02394, 2020.\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.\n\nYixuan Tang, Hwee Tou Ng, and Anthony KH Tung. Do multi-hop question answering systems\n\nknow how to answer the single-hop sub-questions? arXiv preprint arXiv:2002.09919, 2020.\n\n13\n\nJonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efficient object localization using convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 648–656, 2015.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv’e J’egou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\n\nDuˇsan Variˇs and Ondˇrej Bojar. Sequence length is a domain: Length-based overfitting in transformer\n\nmodels. arXiv preprint arXiv:2109.07276, 2021.\n\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International conference on machine learning, pp. 1058–1066. PMLR, 2013.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:\n\nScaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022.\n\nXin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell, and Joseph E Gonzalez. Deep mixture of experts via shallow embedding. In Uncertainty in artificial intelligence, pp. 552–562. PMLR, 2020.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nHaibing Wu and Xiaodong Gu. Towards dropout training for convolutional neural networks. Neural\n\nNetworks, 71:1–10, 2015.\n\nPeng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit Cheung, Simon JD Prince, and Yanshuai Cao. Optimizing deeper transformers on small datasets. arXiv preprint arXiv:2012.15355, 2020.\n\nBrandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019a.\n\nQiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. arXiv preprint arXiv:2208.01313, 2022.\n\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718, 2019b.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019c.\n\nSeniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts. IEEE Transactions on Neural Networks and Learning Systems, 23(8):1177–1193, 2012. doi: 10.1109/TNNLS.2012.2200299.\n\n14\n\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. Why {adam} beats {sgd} for attention models, 2020. URL https: //openreview.net/forum?id=SJx37TEtDH.\n\nMinjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. Advances in Neural Information Processing Systems, 33:14011– 14023, 2020.\n\nWancong Zhang and Ieshan Vaidya. Mixup training leads to reduced overfitting and improved\n\ncalibration for the transformer architecture. arXiv preprint arXiv:2102.11402, 2021.\n\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Conditional computation of transformer models for efficient inference. arXiv preprint arXiv:2110.01786, 2021.\n\nZhongwang Zhang and Zhi-Qin John Xu.\n\nImplicit regularization of dropout. arXiv preprint\n\narXiv:2207.05952, 2022.\n\nMinghang Zheng, Peng Gao, Renrui Zhang, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-\n\nto-end object detection with adaptive clustering transformer. ArXiv, abs/2011.09315, 2021.\n\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. arXiv preprint arXiv:2202.09368, 2022.\n\nChen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit: Learning to initialize neural networks for stable and efficient training. Advances in Neural Information Processing Systems, 34:16410–16422, 2021.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching In Proceedings of the IEEE international conference on computer movies and reading books. vision, pp. 19–27, 2015.\n\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022.\n\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao. Taming sparsely activated transformer with stochastic experts. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=B72HXs80q4.\n\n15\n\nA1 MORE IMPLEMENTATION DETAILS\n\nAlgorithm 1: Concrete Dropout in a PyTorch-like style\n\nclass ConcreteDropout(nn.Module):\n\ndef __init__(self, weight_regularizer=1e-6,\n\ndropout_regularizer=1e-5, init_min=0.1, init_max=0.1):\n\nsuper(ConcreteDropout, self).__init__()\n\nself.weight_regularizer = weight_regularizer self.dropout_regularizer = dropout_regularizer init_min = np.log(init_min) - np.log(1. - init_min) init_max = np.log(init_max) - np.log(1. - init_max) self.p_logit = nn.Parameter(torch.empty(1).uniform_(init_min,\n\ninit_max))\n\ndef forward(self, input, next_layer): p = torch.sigmoid(self.p_logit) # Apply Concrete Dropout output = self._concrete_dropout(input, p) # Feed forward through the next layer output = next_layer(output) # Calculate the weight regularizer sum_of_square = 0 for param in next_layer.parameters():\n\nsum_of_square += torch.sum(torch.pow(param, 2))\n\nweights_regularizer = self.weight_regularizer * sum_of_square / (1\n\n- p)\n\n# Calculate the dropout regularizer dropout_regularizer = p * torch.log(p) dropout_regularizer += (1. - p) * torch.log(1. - p) input_dimensionality = input[0].numel() dropout_regularizer *= self.dropout_regularizer *\n\ninput_dimensionality\n\nregularization = weights_regularizer + dropout_regularizer\n\nreturn output, regularization\n\ndef _concrete_dropout(self, input, p):\n\neps = 1e-7; temp = 0.1 # Calculate the dropout probability matrix unif_noise = torch.rand_like(input) drop_prob = (torch.log(p + eps)\n\n- torch.log(1 - p + eps) + torch.log(unif_noise + eps) - torch.log(1 - unif_noise + eps))\n\ndrop_prob = torch.sigmoid(drop_prob / temp) random_tensor = 1 - drop_prob retain_prob = 1 - p # Apply Concrete Dropout output = torch.mul(input, random_tensor) output /= retain_prob\n\nreturn output\n\nAlgorithm 2: DropBlock in a PyTorch-like style\n\ndef drop_block(input, drop_prob, block_size):\n\n# Calculate the mask with zero value for block-wise elements mask = torch.rand_like(x).lt(drop_prob).float() mask = F.max_pool1d(mask, block_size, 1, block_size // 2) mask = 1 - mask output = input * mask # Apply dropout return output\n\nA16\n\nDetails of Concrete Dropout and DropBlock. In our experiments, we use Concrete-Dropout or DropBlock to replace the original dropout layer in MLP blocks. And for Concrete Dropout, we adopt the official implementation https://github.com/yaringal/ConcreteDropout/ blob/master/concrete-dropout-pytorch.ipynb. For DropBlock, we follow the method in Ghiasi et al. (2018). And the PyTorch-style pseudo codes for both methods are presented in Algorithm 1 and 2.\n\nA2 MORE EXPERIMENT RESULTS\n\nA2.1 STABILITY ANALYSIS\n\nTo evaluate the stability of the improvement obtained by our SMoE-Dropout, we carry out further experiments of Transformer-XL on SST-2. The results are reported in Table A4, from which we can observe that our SMoE-Dropout achieves a statistically significant improvement of 0.93% ∼ 1.17% accuracy gains compared with other SMoE-variants and the dense network, where there is no overlap between the error bars (one standard deviation).\n\nTable A4: Test accuracy of Transformer-XL on SST-2. Both the average and standard deviation of accuracy are reported across 3 independent runs.\n\nMethod\n\nDense w. Dropout THOR (k = N) SMoE (k = N)\n\nSMoE-Dropout (k = N 2 ) SMoE-Dropout (k = N)\n\nAccuracy (↑)\n\n81.39 ± 0.31 81.15 ± 0.55 81.20 ± 0.50\n\n82.03 ± 0.26 82.32 ± 0.14\n\nA2.2 COMPARISON WITH LEARNABLE SMOES W. GRADUALLY INCREASED k\n\nTable A5 demonstrates that both random routing policy and progressively increasing the number of activated experts are beneficial for alleviating representation collapse and providing “selfslimmable” property, yet not as good as combining both. To be specific, when applying the strategy of progressively enlarging the number of activated experts, the learnable SMoEs suffer less representation collapse and achieve better performance, i.e., 0.31% higher accuracy. Meanwhile, We find that learnable SMoE with curriculum learning has the “self-slimmable” property only when activating experts from k = 1 to k = 8. However, the performance starts to degrade if using more experts like k = 16. As for our SMoE-Dropout with a random routing, it enjoys a better “self-slimmable” property from k = 1 to k = 16 (full model capacity), with up to 0.87% higher accuracy on SST-2 across all scenarios, compared to its learnable variants.\n\nTable A5: Testing accuracy (%) over # activated experts of Transformer-XL on SST-2. 16\n\n# Activated Experts\n\n4\n\n2\n\n1\n\n8\n\nSMoE w.o. Gradually Increased k SMoE w. Gradually Increased k\n\n80.06 79.40\n\n80.79 81.02\n\n80.58 81.13\n\n80.99 81.71\n\n81.20 81.51\n\nSMoE-Dropout\n\n79.02\n\n81.25\n\n82.00\n\n82.03\n\n82.32\n\nA2.3 TRANSFER STUDY ON MULTI-STEP REASONING TASKS\n\nWe conduct a further transfer study of the pre-trained BERT networks on a multi-hop questionanswering dataset, HotpotQA Yang et al. (2018). And We use exact match (EM) accuracy to assess networks’ performance. Following the same metric in Press et al. (2022), we calculate the compositionality gap, i.e., the gap of EM accuracy between multi-hop question answering and its all single-hop sub-questions Tang et al. (2020), of each network. As shown in Table A6, our SMoEDropout is beneficial for reducing the compositionality gap, which achieves the best performance with up to 0.30% higher EM score and 0.30% narrower compositionality gap, compared with the learnable SMoE and its dense counterpart.\n\nTable A6: The EM score and compositionality gap of the pre-trained BERT networks on HotpotQA.\n\nMetrics\n\nDense w. Dropout\n\nSMoE SMoE-Dropout\n\nEM Accuracy (%) Compositionality Gap (%)\n\n14.90 15.00\n\n15.20 14.70\n\n15.10 14.90\n\nA17",
  "translations": [
    "# Summary Of The Paper\n\nThis paper presents a novel plug-and-play strategy for training large transformer models, which leverages sparse MoEs in a dropout-like manner to scale transformers to better performance in their full capacity without collapse. The method is simple, and the experiments are thorough.\n\n# Strength And Weaknesses\n\nPros\n-\tSMoE-Dropout demonstrates an attractive “self-slimming” property during inference and downstream fine-tuning, which delivers a once-for-all in-situ trade-off between efficiency and performance\n-\tLike classical dropout, SMoE-Dropout is able to mitigate the representation collapse that standard MoEs usually suffer from, i.e., activating more experts do not improve or even hurt performance.\n-\tApplying SMoE-Dropout is extremely cheap as only random router is adopted\n-\tTransfer study is strong, which provides another evidence that more pre-training information is effectively retained by the full transformer capacity. It is further validated in ablation studies 4.4\n\nCons:\n-\tThe performance gain of SMoE-Dropout is sometimes marginal, such as on BeRT (section 4.1). Moreover, as seen from Figure 1 and Figure 3, it seems SMoE-Dropout does sacrifice performance at low parameter counts compared to Learnable MoE, why?\n-\tWhen breaking a single-stream models into MLP MoEs (the modularization step in section 3.2), how to decide the number of MLP experts needed, i.e., N? This seems to be an important hyperparameter but not discussed. Also, why not applying SMoE-Dropout to MLP layers but not self-attention? \n-\tFor training with Learnable SMoE, it is further unclear how the authors selected its k?\n-\tWhy training time is the same between SMoE-dropout k=N/2 and k=N, in Table 1?\n-\tTable 1 again: are all those numbers averaged across three datasets? The authors didn’t explain\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAll look good to me. Authors promised to release models and codes: it would help if the authors could respond to be clearer what they plan to release: all experiments versus some, pre-trained model versus training script, etc.\n\n# Summary Of The Review\n\nOverall, I would tend to accept this paper. If the author can address my concerns. I would be more convinced.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers\" by Chen et al. introduces a novel training framework called SMoE-Dropout aimed at addressing the computational inefficiencies and representation collapse in large transformer models. The methodology involves transforming dense MLPs into smaller experts that share hidden dimensions, implementing a fixed random routing policy for token assignment, and progressively activating more experts during training. The findings demonstrate that SMoE-Dropout achieves superior performance on various benchmarks, including lower Bits-Per-Character (BPC) during pre-training and improved accuracy on downstream tasks compared to traditional dense training methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to enhancing the scalability of Sparse Mixture-of-Experts (SMoEs), effectively mitigating the redundancy of parameters in large transformer architectures. The introduction of the self-slimmable property and a fixed routing mechanism provides a robust framework for both training and inference phases. However, a potential weakness is the reliance on a specific random routing policy, which may limit the generalizability of the method across different tasks or architectures. Additionally, while the paper provides empirical evidence of improvements, further exploration of the theoretical implications and long-term performance across diverse datasets could strengthen the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The explanations of the SMoE-Dropout framework and its components are coherent, making the framework accessible for replication. The empirical results are presented with sufficient detail, supporting the claims made. However, the reproducibility could be enhanced by providing additional implementation specifics and datasets used in experiments. The novelty of the approach is commendable, introducing a significant shift in how dropout techniques are applied within large transformer models.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the training of large transformers through the SMoE-Dropout framework, demonstrating improved efficiency and performance. While the contributions are significant, a broader exploration of the method's adaptability could enhance its impact in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel training framework called **SMoE-Dropout**, aimed at addressing the inefficiencies and memory requirements associated with large transformer models. The method leverages a randomly initialized fixed router to activate a varying number of experts during training, thereby enhancing the scalability of Sparse Mixture-of-Experts (SMoEs) while mitigating issues related to representation collapse. The authors demonstrate that SMoE-Dropout leads to significant improvements in both pre-training and downstream task performance across multiple transformer architectures, including BERT, Transformer-XL, and RoBERTa, while achieving notable reductions in training time and computational cost.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to enhancing transformer model efficiency and scalability, as well as its versatility, allowing for application beyond traditional transformer architectures. The reported performance gains in pre-training and downstream tasks are substantial, indicating the method's effectiveness in improving model capacity without overfitting. However, weaknesses include the dependency on the initial router configuration, which could affect performance, and the potential for expert redundancy remaining in certain contexts. Additionally, the complexity of implementing the new training pipeline may pose challenges for adoption, and the study's focus on transformers limits its generalizability to other neural network types.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and articulates its contributions clearly, with a logical flow of ideas. The methodology is detailed, allowing for reproducibility, although the complexity of the implementation may deter some practitioners. The novelty of the SMoE-Dropout framework stands out, particularly in its self-slimmable property and the innovative use of a fixed routing mechanism. However, the paper could benefit from further exploration of the generalizability of its findings to other neural network architectures.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of large-scale transformer training by introducing the SMoE-Dropout framework, which effectively enhances model efficiency and performance. While there are some limitations regarding implementation complexity and generalizability, the empirical results are promising and indicate the method's potential for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Sparse Mixture-of-Experts with Dropout (SMoE-Dropout), a novel framework designed to enhance the scalability and efficiency of large transformer models while addressing issues of parameter redundancy and representational collapse. The authors propose a method that allows for a gradual activation of experts during training, enabling a self-slimmable property that improves performance without compromising stability. Through extensive experiments across multiple architectures and tasks, the findings demonstrate that SMoE-Dropout significantly reduces computational costs and enhances accuracy compared to traditional dense training methods.\n\n# Strength And Weaknesses\nThe paper’s primary strength lies in its innovative approach to addressing the computational inefficiencies associated with large transformers by introducing SMoE-Dropout, which effectively combines the benefits of sparse activation with dropout techniques. The self-slimmable property is a noteworthy contribution that provides flexibility in model scaling based on resource availability. However, the paper could have strengthened its analysis by including more detailed comparisons with existing state-of-the-art methods and discussing potential limitations of the SMoE-Dropout framework, particularly concerning its implementation complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and experimental findings, using appropriate metrics to evaluate performance. The methodology section is detailed, providing sufficient background on SMoEs and dropout techniques, which supports reproducibility. Notably, the introduction of SMoE-Dropout is a significant novel contribution to the field. Nevertheless, further insights into the hyperparameter choices and specific implementation challenges would enhance the clarity and reproducibility of the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to improving the efficiency of large transformers through the introduction of SMoE-Dropout. The framework is backed by thorough experiments demonstrating its effectiveness, although it would benefit from a deeper exploration of its limitations and a more comprehensive comparison with existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SMoE-Dropout, a novel training framework designed for Sparse Mixture of Experts (SMoE) models. The methodology involves utilizing a fixed and randomly initialized router that progressively activates experts, addressing scalability challenges inherent in traditional SMoE architectures. The findings indicate that SMoE-Dropout significantly outperforms dense training baselines and existing SMoE approaches across various tasks, achieving improved accuracy and lower Bits-Per-Character (BPC) metrics, while also demonstrating a self-slimmable property that enhances flexibility during inference.\n\n# Strength And Weaknesses\nThe paper presents several strengths: the innovative SMoE-Dropout approach effectively tackles scalability issues, leading to improved performance metrics and reduced overfitting tendencies. The method's flexibility across different architectures and its computational efficiency are notable advantages. However, limitations include the reliance on a fixed router, which may hinder adaptability, and the need for further validation across diverse datasets and architectures. Additionally, while the empirical results are robust, a deeper theoretical understanding of the framework's effectiveness could enhance the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, making it accessible to readers. The quality of the experiments is high, with extensive evaluations across various benchmarks. The novelty of the approach is significant, providing a fresh perspective on the SMoE framework. Reproducibility is bolstered by the authors' commitment to open-sourcing their code and models, although improvements in documentation could facilitate broader adoption.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of deep learning through the introduction of SMoE-Dropout, offering a scalable and efficient alternative to traditional training methods. While the findings are promising, further exploration of the theoretical aspects and broader applicability is necessary to fully appreciate the impact of this work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SMoE-Dropout, a novel training framework designed to enhance the efficiency and performance of large transformer models by addressing scalability issues common in existing Mixture-of-Experts (MoE) architectures. The methodology features a fixed routing mechanism that is randomly initialized, which enables a gradual increase in the number of active experts during training. This approach results in a \"self-slimmable\" capability during inference and fine-tuning. The authors provide extensive empirical validation showing that transformers trained with the SMoE-Dropout framework outperform both dense training and other MoE methods on benchmark datasets, achieving consistent accuracy improvements.\n\n# Strength And Weaknesses\n**Strengths:**\n- The SMoE-Dropout framework presents an innovative solution to overfitting and scalability concerns within transformer models, showcasing a significant contribution to the field.\n- Comprehensive empirical validation supports the claims, demonstrating superior performance across various architectures and tasks.\n- The methodology is presented with clarity, making it accessible for future research and application.\n\n**Weaknesses:**\n- The exploration of the implications of the fixed routing mechanism on expert specialization over time is lacking, which could be critical for understanding long-term model behavior.\n- There is limited discussion on the potential trade-offs between the flexibility of expert activation and the resulting model complexity, which may impact practical deployment scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of the writing is high, and the figures and tables effectively support the findings. The novelty of the SMoE-Dropout framework is significant, particularly in its approach to combining fixed routing with progressive activation. However, reproducibility could be enhanced with more detailed descriptions of experimental settings and hyperparameter choices used in the empirical validation.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative contribution to the field of deep learning, specifically in optimizing large transformer models through the SMoE-Dropout framework. While the empirical results are robust and the methodology is clear, further exploration of certain aspects could strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for adversarial training called Sparse Mixture-of-Experts (SMoE) Dropout, aimed at enhancing the robustness of deep learning models against adversarial attacks while maintaining computational efficiency. Key contributions include the development of the SMoE-Dropout framework, characterized by its self-slimmable property that allows for adaptive capacity based on resource availability, and a fixed random routing policy that improves stability during inference. Through extensive experiments across various architectures and datasets, the authors demonstrate that SMoE-Dropout significantly outperforms traditional dropout and other adversarial training methods, achieving lower error rates and reduced training times.\n\n# Strength And Weaknesses\n**Strengths:**\n- The integration of SMoE with dropout techniques is a novel contribution that addresses a key challenge in adversarial training—balancing robustness and computational efficiency.\n- The empirical results are compelling, showing robust performance improvements across multiple benchmarks, which strengthens the case for the proposed method.\n- The self-slimmable property provides practical advantages in resource-constrained environments, making the methodology appealing for real-world applications.\n\n**Weaknesses:**\n- The reliance on a random routing policy may introduce uncertainties in model behavior, particularly under dynamic adversarial conditions, which could limit its applicability in more complex scenarios.\n- The paper would benefit from a more in-depth theoretical analysis to elucidate the implications and limitations of using SMoE-Dropout in various adversarial contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding of the methodology and findings. The quality of the experiments is high, and the results are presented with sufficient detail to support the claims. However, some aspects of the theoretical foundation could be further elaborated to enhance clarity regarding the underlying principles of the proposed framework. The reproducibility of the results is likely high, given the detailed description of the methodology and experimental setups.\n\n# Summary Of The Review\nThis paper presents a significant advancement in adversarial training through the introduction of the SMoE-Dropout framework, which enhances model robustness while improving computational efficiency. While the empirical results are strong, further theoretical exploration is needed to fully understand the implications of the proposed method in various adversarial contexts.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a novel training framework called SMoE-Dropout, which is claimed to enhance the efficiency and performance of transformer models. The authors assert that SMoE-Dropout leads to consistent performance improvements on reasoning tasks, significant savings in training time and computational costs, and introduces a self-slimmable feature for adaptive scaling during inference. Additionally, the paper discusses a fixed router network design to activate experts and claims extensive experimental validation across various architectures. However, the significance of these contributions appears to be overstated.\n\n# Strength And Weaknesses\nWhile the paper introduces SMoE-Dropout with various claimed benefits, the contributions largely resemble minor variations of existing techniques without substantial advancements. The reported performance improvements (1.03% to 1.37%) are minimal and do not seem to justify the complexity of the new framework. Claims of efficiency gains, such as 37% savings in training time, are likely exaggerated. The self-slimmable property is a common feature in existing models, and the fixed router network offers limited practical advantages. Furthermore, the experimental validation lacks conclusive evidence to support the superiority of SMoE-Dropout over conventional methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear and organized, but the novelty of the proposed framework is questionable. Many of the claims made regarding the benefits of SMoE-Dropout do not hold up under scrutiny, leading to concerns about the overall quality of the contributions. Reproducibility may be an issue, as the experimental results do not convincingly demonstrate the framework's advantages, which may hinder other researchers from effectively replicating the findings.\n\n# Summary Of The Review\nOverall, the paper presents a framework that lacks substantial novelty and fails to provide convincing evidence for its claimed benefits over existing methods. The proposed SMoE-Dropout does not significantly advance the field of transformer training, and the reported performance improvements and efficiency gains are minimal at best.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Sparse MoE-Dropout (SMoE-Dropout), a novel training framework designed to enhance the efficiency and scalability of large transformer models by mitigating computational costs and parameter redundancy. The methodology centers on a self-slimmable property that dynamically adjusts model capacity based on available resources while maintaining performance. Experimental findings indicate that SMoE-Dropout achieves significant accuracy improvements across various tasks, including reasoning and transfer learning, alongside notable reductions in bits per character (BPC) and training time compared to traditional dense training.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the challenges of scaling transformer models, particularly through the introduction of the self-slimmable property. This property enhances the practical applicability of the framework in real-world scenarios where computational resources may vary. However, one potential weakness is that while the paper reports impressive gains, the variations in original and revised results suggest a need for careful validation and reproducibility of the findings. Additionally, more extensive comparisons with state-of-the-art methods could strengthen the paper's claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, effectively conveying the significance of the proposed SMoE-Dropout framework. The experimental design appears robust, with various metrics employed to evaluate performance. However, the discrepancies between reported and original results raise questions about reproducibility, indicating that the methodology or experimental setup may require further clarification. The novelty of the approach is commendable, particularly in combining dropout techniques with model scalability.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the training of transformer models through the SMoE-Dropout framework, demonstrating both efficiency and performance improvements. While the contributions are noteworthy, further validation of the experimental results and more comprehensive comparisons with existing methods would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach utilizing Sparse Mixture-of-Experts (SMoEs) to address issues of computational inefficiency and representation collapse in transformer models. The authors claim that their proposed method, referred to as SMoE-Dropout, enhances model performance by leveraging a self-slimmable property that allows dynamic adjustment of activated experts during training. Empirical results demonstrate improvements in bits-per-character (BPC) and accuracy on specific tasks, suggesting that the approach may yield benefits in various transfer learning scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of SMoEs, addressing a significant challenge in transformer architectures. However, there are notable weaknesses, such as the lack of a clear metric for quantifying redundancy, which hinders the assessment of the proposed solution's effectiveness. Additionally, the assumptions regarding the superiority of SMoEs over learnable routing policies and the reliance on a fixed router network raise questions about adaptability and generalizability across different tasks and architectures. The paper also fails to thoroughly explore potential overfitting concerns and the implications of real-world resource availability on the proposed method’s performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, although certain concepts, such as redundancy and the self-slimmable property, could benefit from clearer definitions and metrics. The novelty of the approach is notable, but the claims made regarding the efficacy of SMoEs need more empirical support and comparison to existing methods. Reproducibility may be a concern due to a lack of comprehensive details on the experimental setup and performance metrics beyond BPC and accuracy, which may not fully represent real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to improving transformer model efficiency using SMoEs, but it is marred by unsubstantiated claims, unclear definitions, and a limited exploration of its implications. The potential benefits of the proposed method are noteworthy, yet further validation and comparison with alternative methods are required for a more robust understanding of its effectiveness.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces SMoE-Dropout, a novel training framework that enhances the scalability of dense transformer architectures using a sparsely activated Mixture-of-Experts (SMoE) approach. The methodology focuses on addressing computational efficiency and representation collapse by allowing a dynamic increase in the number of active experts during both training and inference. The experimental results indicate that SMoE-Dropout significantly outperforms conventional dense training methods across various transformer models (BERT, Transformer-XL, RoBERTa), showcasing substantial computational savings and improved performance metrics.\n\n# Strength And Weaknesses\nThe primary strengths of this paper lie in its innovative approach to scaling transformers through the SMoE-Dropout framework, which provides a solution to the common issues of representation collapse and training inefficiency. The methodology is clear and well-articulated, demonstrating a solid understanding of the challenges in current SMoE implementations. However, a potential weakness is the limited exploration of the framework's applicability beyond the tested architectures, which may restrict the generalizability of the findings. Additionally, while the results are promising, further empirical validation across a broader range of datasets and tasks would strengthen the paper's claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, making it accessible to readers with varying levels of familiarity with the topic. The quality of the writing is high, with detailed explanations of the methodology and experimental design. The technical novelty of SMoE-Dropout is significant as it introduces a fresh perspective on the challenges of scaling transformer models. Reproducibility is facilitated by the comprehensive description of the experiments and the comparison with baseline models, although the availability of code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to scaling dense transformer models through SMoE-Dropout, effectively addressing key challenges in computational efficiency and representation collapse. The empirical results support the effectiveness of the proposed method, although further exploration of its applicability across diverse contexts would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel machine learning framework designed to tackle a significant challenge in predictive modeling. The authors propose a method that integrates advanced feature selection techniques with a unique optimization algorithm, aimed at enhancing model accuracy while maintaining computational efficiency. Experimental results demonstrate that this approach outperforms several state-of-the-art methods across multiple datasets, indicating its robustness and applicability in diverse scenarios.\n\n# Strength And Weaknesses\n## Strengths:\n1. **Innovative Methodology**: The introduction of a hybrid approach that combines feature selection and optimization is a significant contribution, potentially leading to better model performance.\n   \n2. **Comprehensive Experiments**: The authors conduct extensive experiments across various datasets, providing strong empirical evidence for the effectiveness of their method compared to established benchmarks.\n\n3. **Practical Relevance**: The proposed method addresses real-world challenges in predictive modeling, increasing its relevance for practitioners in the field.\n\n## Weaknesses:\n1. **Limited Theoretical Analysis**: While the empirical results are strong, the theoretical justification for the proposed framework could be expanded to provide deeper insights into its workings.\n\n2. **Discussion on Limitations**: The paper lacks a thorough discussion of potential limitations and scenarios where the proposed method may not perform well, which is important for a balanced presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, making it easy to follow the authors' arguments. The clarity of the descriptions of the proposed methodology and experiments contributes to the overall quality of the work. However, some sections could benefit from additional detail, particularly regarding the implementation specifics and parameter settings used in the experiments, which would enhance reproducibility. The novelty of the approach is high, as it integrates concepts that have not been previously combined in this manner for the problem at hand.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to predictive modeling that effectively combines feature selection and optimization techniques. While the empirical results are compelling, the lack of theoretical insights and a thorough discussion of limitations detracts from the paper's overall impact. With some refinements, this work has the potential to make a significant contribution to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces a novel training framework called SMoE-Dropout, designed to enhance the efficiency of Sparse Mixture-of-Experts (SMoEs) in large transformer models. It addresses challenges such as representational collapse and scalability during inference by employing a fixed router network that activates a variable number of experts throughout training. The proposed method not only maintains model performance but also significantly reduces computational costs, enabling a \"self-slimmable\" property that optimizes resource usage during inference. Empirical results demonstrate that SMoE-Dropout outperforms traditional dense training approaches while providing substantial training efficiency improvements.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the inefficiencies associated with large transformer models. The introduction of SMoE-Dropout is a compelling contribution to the field, as it effectively combines the benefits of SMoEs with a scalable training regime. The methodology is well-structured, and the experiments conducted provide strong evidence supporting the claims made. However, a potential weakness is the reliance on a fixed router network, which may limit flexibility in expert activation mechanisms compared to more dynamic approaches. Additionally, the paper could benefit from more extensive discussions on potential limitations and the applicability of the method to various architectures beyond transformers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical flow of ideas and thorough explanations of the proposed methodology. The quality of the experiments is commendable, with adequate data supporting the findings. However, while the novelty of the proposed method is significant, the paper could improve in terms of providing more detailed information on reproducibility, such as sharing implementation specifics and hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of scalable transformer models by introducing SMoE-Dropout, which effectively balances efficiency and performance. While the methodology is innovative and well-supported by empirical evidence, the paper could enhance its clarity on reproducibility and address limitations more thoroughly.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SMoE-Dropout, a novel training framework aimed at addressing the high computational and memory costs associated with large transformer models. The main contributions include a fixed router network that progressively activates experts during training to leverage conditional computation, resulting in a \"self-slimmable\" property of the model. Empirical results demonstrate that SMoE-Dropout outperforms traditional dense training methods and achieves efficiency gains, as reflected in significant reductions in bits per character (BPC) and training time across various transformer architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to scaling transformers through SMoE-Dropout, which effectively addresses long-standing issues of redundancy and representation collapse in large models. The methodology is clearly articulated, and the experiments provide robust comparative analyses, demonstrating the advantages of the proposed method. However, one potential weakness is the limited exploration of the implications of the self-slimmable property beyond performance metrics, which may warrant further investigation to understand its broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, making it accessible for readers familiar with the domain. The writing quality is high, with logical flow and coherent arguments supporting the proposed method. The novelty of SMoE-Dropout as a dropout-like technique for large models adds a fresh perspective to the existing literature. While the authors have provided sufficient details for reproduction, more comprehensive descriptions of experimental setups and hyperparameters would enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the training of large transformer models by introducing SMoE-Dropout, which effectively balances performance and computational efficiency. The innovative nature of the proposed method is underscored by empirical results that validate its efficacy. However, more extensive exploration of its broader implications could strengthen the contributions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach called Sparse MoE-Dropout (SMoE-Dropout) aimed at addressing the computational and memory inefficiencies associated with large transformer models. The methodology involves a detailed framework for SMoE-Dropout, which incorporates a random routing policy to dynamically activate subsets of model parameters during training. The findings demonstrate that this approach not only enhances training efficiency but also improves performance metrics, such as Bits-Per-Character (BPC), compared to traditional dense models. The paper also explores the self-slimmable feature of the proposed method, which allows for a reduction in model size during inference without sacrificing performance.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its comprehensive evaluation across diverse transformer architectures and its effective comparison with multiple baseline methods, showcasing significant improvements in performance. Additionally, the proposed SMoE-Dropout method is well-justified and grounded in a thorough review of related works. However, a potential weakness is the lack of extensive discussion on the real-world applicability and limitations of the SMoE-Dropout method across varying datasets and tasks, which could provide a more balanced view of its practical implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly organized, with logical flow from the introduction through to the conclusion. The methodology is presented with sufficient detail to allow for reproducibility of the experiments, although further clarification on certain aspects of the random routing policy could benefit readers. The novelty of the approach is evident, as it proposes a new mechanism to enhance the scalability and efficiency of transformer models, which is a significant contribution to the field.\n\n# Summary Of The Review\nOverall, this paper makes a strong contribution to the field of machine learning and natural language processing by introducing a novel method for improving transformer model efficiency. The thorough experimental evaluation supports the claims made, but further exploration of practical limitations could strengthen the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper presents a novel framework known as Sparse Mixture-of-Experts Dropout (SMoE-Dropout) aimed at improving the training efficiency of high-capacity transformer models. The methodology involves a plug-and-play approach that allows for the progressive activation of experts throughout the training process, addressing issues of representational collapse and scalability that are prevalent in traditional Sparse Mixture-of-Experts (SMoE) architectures. The experimental evaluation showcases that SMoE-Dropout consistently outperforms dense models and other sparse alternatives, achieving significant computational savings (up to 37% reduction in training time) while maintaining or improving performance across various architectures and datasets.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to addressing two critical limitations of SMoEs: representational collapse and scalability. The introduction of a random routing policy combined with a progressive activation mechanism is a compelling solution that enhances the model's adaptability. However, the paper could benefit from a more extensive analysis of the limitations of the proposed framework, particularly in terms of its applicability to a wider range of architectures beyond transformers. Additionally, while the empirical results are promising, further exploration of the theoretical implications of SMoE-Dropout could strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The explanations of the SMoE architecture and the dropout mechanism are detailed and accessible, making the concepts easy to understand. The experimental setup is robust, and the results are presented in a clear manner, enhancing reproducibility. However, the paper would benefit from additional details on the implementation specifics and hyperparameter settings used in the experiments to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, this paper introduces a significant advancement in the training of transformer models through the SMoE-Dropout framework, effectively addressing key challenges associated with existing SMoE methodologies. The clear presentation and strong empirical results contribute to the paper's impact, although an expanded discussion on broader applicability and implementation details would enhance its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the SMoE-Dropout framework, which aims to address the issue of representation collapse in deep learning models by introducing a novel routing mechanism for expert selection. The authors claim that their method enhances model efficiency and reduces training time, asserting that it can be adapted for various deep learning architectures. However, the experimental results suggest only marginal performance improvements on a limited set of models, including BERT and RoBERTa.\n\n# Strength And Weaknesses\nThe proposed SMoE-Dropout framework attempts to innovate in the domain of expert selection, which is a relevant area of research; however, its complexity may deter practical application. The paper lacks robust empirical evidence supporting substantial improvements over existing methods, and the marginal performance gains presented do not justify the complexity introduced. Additionally, the reliance on a fixed router network raises concerns about flexibility, and the concept of being \"self-slimmable\" is not thoroughly explored. The experimental validation is limited in scope, and the claims made regarding efficiency gains are vague and not well contextualized. Overall, while there are interesting ideas, the execution and validation leave significant room for improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by an over-reliance on existing literature without adequately situating its contributions, which may lead to questions about its novelty. The methodology appears convoluted, potentially affecting reproducibility, as the details regarding the implementation of the routing mechanism and its training pipeline are not sufficiently elaborated. Furthermore, the lack of rigorous statistical analysis on performance claims raises doubts about the robustness of the findings.\n\n# Summary Of The Review\nOverall, the SMoE-Dropout framework introduces an intriguing approach to expert selection but suffers from significant limitations in empirical validation, clarity, and practical applicability. The claims of efficiency and performance improvements are not convincingly supported, and the paper does not adequately address the potential drawbacks of the proposed method.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces SMoE-Dropout, an innovative training framework for transformers that enhances their scalability without performance degradation. The methodology employs a self-slimmable property that allows models to adapt their capacity based on available resources, resulting in substantial performance improvements during inference and fine-tuning. The findings demonstrate that SMoE-Dropout significantly outperforms traditional training methods, achieving superior accuracy and reduced training costs across various challenging tasks.\n\n# Strength And Weaknesses\nThe primary strength of SMoE-Dropout lies in its ability to consistently improve model accuracy and training efficiency, with reported savings of up to 37% in running time compared to standard dense training methods. This framework effectively mitigates issues of representational collapse and maximizes the activation of model capacity, leading to enhanced generalization. However, the paper could benefit from more extensive comparisons with other emerging training techniques to better contextualize its advantages and limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with well-structured sections that guide the reader through the proposed methodology and results. The quality of experimental validation is high, given the diverse range of transformer architectures and tasks tested. The novelty of the SMoE-Dropout approach is significant, as it presents a fresh perspective on transformer training. Reproducibility is supported by detailed descriptions of the experiments and results, although supplementary material could enhance this aspect further.\n\n# Summary Of The Review\nOverall, SMoE-Dropout presents a compelling advancement in transformer training, offering substantial performance and efficiency gains. Its novel approach to capacity adaptation and generalization marks a significant contribution to the field, although further comparative analysis could strengthen its position among current methodologies.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces Sparse Mixture-of-Experts (SMoE) and a novel framework termed SMoE-Dropout, which addresses the limitations of traditional dense transformers by leveraging sparsity for computational efficiency. It posits that SMoEs can enhance model performance by activating specific experts based on input characteristics, thus mitigating representation collapse—a common issue in dense models characterized by parameter redundancy. The proposed SMoE-Dropout enables adaptive scaling of model capacity during training through a fixed router, promoting implicit regularization and offering a theoretical foundation for improved expert specialization without the overhead of retraining.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical insights into the limitations of dense transformers and the introduction of a novel framework that could enhance model efficiency and performance. The self-slimmable property of SMoE-Dropout is particularly noteworthy, as it allows dynamic resource allocation, which is a significant advancement in the realm of large-scale models. However, the paper could benefit from empirical validation of the theoretical claims, as the absence of experimental results leaves the practical applicability of the proposed framework somewhat uncertain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its theoretical contributions clearly, making complex concepts accessible. The quality of the theoretical analysis is commendable, providing a solid foundation for future research. However, the reproducibility aspect is limited due to the lack of empirical experiments, which are essential for validating the theoretical claims made regarding SMoE-Dropout.\n\n# Summary Of The Review\nOverall, this paper presents a significant theoretical advancement in the understanding of SMoEs and introduces SMoE-Dropout as a promising framework to address representation collapse in dense transformers. While the theoretical contributions are strong, the lack of empirical validation raises questions about the practical implications of the proposed methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper introduces a novel dropout technique termed SMoE-Dropout, which leverages a sparse mixture of experts (MoE) approach for improving the training efficiency and performance of transformer architectures. The methodology involves modularizing dense MLPs into smaller, manageable sub-MLPs, utilizing a fixed random routing policy for token assignment to experts, and incrementally increasing the number of active experts during training through curriculum learning principles. The authors conduct experiments on various transformer models, including BERT, Transformer-XL, and RoBERTa, demonstrating that SMoE-Dropout outperforms traditional dropout methods and other state-of-the-art techniques in pre-training and fine-tuning tasks across multiple datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to dropout, which effectively balances model complexity and training efficiency through expert modularization and curriculum learning. The experiments are well-structured, using a robust set of baseline comparisons and thorough evaluation metrics, which lend credibility to the findings. However, the paper could benefit from a more extensive discussion around the implications of using random routing policies on model interpretability and the trade-offs involved in expert selection. Additionally, while the empirical results are promising, further exploration of the generalizability of SMoE-Dropout across different model architectures and tasks would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and results. The structure is logical, and the inclusion of detailed implementation notes and code availability enhances reproducibility significantly. The novelty of the SMoE-Dropout approach is evident, as it combines elements from existing dropout techniques while introducing a unique modularization strategy and routing policy. However, there could be more clarity regarding the practical steps for implementing the proposed method in diverse contexts, which would support broader adoption.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in dropout techniques through the introduction of SMoE-Dropout, demonstrating notable improvements in model performance and efficiency. While the results are compelling and the methodology is sound, further exploration of the implications and generalizability of the approach would enhance the contribution.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThe paper presents a novel framework called SMoE-Dropout aimed at addressing scalability issues in Sparse Mixture-of-Experts (SMoEs). The authors claim that their method improves efficiency in model training by employing a random routing policy that purportedly enhances performance compared to dense training baselines. Experimental results are provided to support claims of superior efficiency and generalization performance; however, the evaluation is limited to specific models and tasks.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its attempt to tackle scalability in SMoEs and the introduction of a potentially interesting random routing policy. However, significant weaknesses undermine its contributions. The paper lacks acknowledgment of prior work that has explored similar concepts, leading to questions about its novelty. Furthermore, the performance claims appear overly selective, failing to provide a comprehensive evaluation against alternative approaches and models. The limited benchmarking and lack of detailed experimental setup raise concerns about the validity and generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear; however, the selective presentation of results and insufficient comparisons with existing methods affect its quality. The novelty of the proposed framework is questionable due to a lack of engagement with relevant literature. The reproducibility of results is hampered by vague descriptions of experimental setups, making it challenging for other researchers to validate the findings.\n\n# Summary Of The Review\nOverall, the paper attempts to introduce a new approach to SMoEs that promises efficiency gains but falls short of delivering robust evidence of its effectiveness. The lack of comprehensive evaluation against existing methods and the selective reporting of results significantly diminish the paper's contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents a novel approach to enhancing transformer models through the introduction of dense and self-slimmable architectures. The authors propose a new Sparsely-activated Mixture-of-Experts (SMoE) framework that allows for efficient scaling of model parameters while maintaining high performance. The methodology involves a detailed analysis of parameter efficiency and model training dynamics, supported by extensive empirical evaluations across various benchmarks. The findings demonstrate significant improvements in computational efficiency and performance metrics, indicating the potential for these models in resource-constrained environments.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its comprehensive exploration of model scaling and efficiency, which is highly relevant in the context of deep learning's growing resource demands. The proposed SMoE framework is innovative and well-supported by empirical results that clearly illustrate its advantages over traditional transformer architectures. However, the paper does have weaknesses; certain sections are overly technical and could benefit from clearer explanations for broader accessibility. Additionally, while the experiments cover a range of benchmarks, the inclusion of more diverse datasets could strengthen the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured, but clarity is sometimes compromised by dense technical jargon and complex mathematical formulations. The quality of the writing varies, with some areas requiring grammatical corrections and improved readability. The novel contributions are significant, particularly in the context of model scalability, but reproducibility could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter choices. Ensuring consistent notation and clearer explanations would greatly aid in the reproducibility of results.\n\n# Summary Of The Review\nThe paper makes a substantial contribution to the field of transformer models by introducing an innovative scaling approach via the SMoE framework. While the empirical results are promising, the clarity and reproducibility of the work could be improved. The findings underscore the importance of efficiency in deep learning, making the paper relevant and impactful.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the SMoE-Dropout framework, a novel regularization technique aimed at improving the training and performance of neural networks, particularly transformers. The authors claim that SMoE-Dropout enhances model robustness and efficiency through a self-slimmable approach that optimizes the activation of expert modules. Key findings reveal that this method achieves substantial computational savings while maintaining competitive performance on the evaluated datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to regularization, particularly through the concept of self-slimmability, which is promising for adaptive resource management in neural networks. However, the paper has notable weaknesses, including a lack of exploration into the framework's applicability across various neural architectures beyond transformers, insufficient analysis of potential long-term effects on interpretability and training stability, and a limited dataset scope that restricts generalization insights. Additionally, the discussion surrounding computational trade-offs in production environments and the robustness of the fixed router network is lacking.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper generally maintains clarity in its exposition, presenting the SMoE-Dropout framework in a structured manner. However, the lack of comprehensive comparisons with other state-of-the-art regularization techniques diminishes the contextual understanding of its novelty. The reproducibility of the findings could be enhanced by providing more details regarding the experiments and the datasets used, particularly in addressing how different initialization strategies or training regimes might affect outcomes.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to regularization in neural networks with the SMoE-Dropout framework. However, it falls short in addressing broader implications, comparisons, and potential limitations, which detracts from the overall impact of the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the Sparse MoE-Dropout (SMoE-Dropout) framework, which aims to improve the training efficiency and performance of transformer models compared to existing dropout methods. The authors conduct experiments using various architectures, including BERT, Transformer-XL, and RoBERTa, comparing SMoE-Dropout against dense training methods such as standard Dropout, Concrete Dropout, and DropBlock. Key findings indicate that SMoE-Dropout achieves statistically significant reductions in Bits-Per-Character (BPC) and training time, demonstrating robust performance improvements across different configurations and tasks, with rigorous statistical validation of results.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive experimental design and rigorous statistical analysis, which lend credibility to the claims of improved performance and efficiency. The use of multiple transformer architectures for evaluation enhances the generalizability of the findings. However, a potential weakness is the reliance on specific performance metrics (BPC) which may not capture all aspects of model efficacy, particularly in diverse downstream tasks. Additionally, while significant improvements are reported, the practical implications of these gains in real-world scenarios could be explored further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its objectives, methodologies, and findings. The quality of the statistical analysis is high, with multiple tests employed to ensure the robustness of results. The novelty of the SMoE-Dropout approach is notable, particularly in its application to transformer models. However, reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices, which are critical for other researchers looking to replicate the findings.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field by proposing a new dropout framework that shows promise in enhancing the performance and efficiency of transformer models. While the methodologies are sound and results are statistically validated, further exploration of the practical applications and comprehensive details for reproducibility would strengthen the paper further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper proposes the SMoE-Dropout framework, which aims to enhance the efficiency and performance of transformer models by implementing a self-slimmable mechanism. The methodology involves the integration of a fixed router network to selectively activate subsets of experts during model training and inference. The findings indicate that SMoE-Dropout leads to improved computational efficiency and model performance on selected transformer architectures, including BERT, Transformer-XL, and RoBERTa.\n\n# Strength And Weaknesses\nWhile the proposed framework demonstrates notable improvements in efficiency and performance, it presents several weaknesses. The exploration of the \"self-slimmable\" property is superficial, lacking a detailed examination of its implications in real-world applications beyond the evaluated tasks. Additionally, the potential for overfitting due to the fixed router network is not addressed, raising concerns about the robustness of the model in varied conditions or datasets. The experiments are limited to a narrow selection of architectures, and the authors do not explore low-resource settings or smaller models, which may exhibit different challenges. Furthermore, the paper lacks a comparative analysis with other state-of-the-art methods, limiting the contextual understanding of SMoE-Dropout's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. However, the novelty of the SMoE-Dropout framework could be enhanced by providing more comprehensive insights into its potential applications and limitations. Reproducibility is somewhat hindered by the limited examination of the framework across various architectures and tasks, as well as by the absence of detailed insights into the interpretability of routing decisions.\n\n# Summary Of The Review\nOverall, the SMoE-Dropout framework presents an interesting approach to improving transformer model efficiency and performance. However, the paper lacks depth in addressing potential shortcomings and broader applications, which limits its impact and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes a new approach called Sparsely-activated Mixture-of-Experts (SMoEs) as a solution to the inefficiencies associated with large transformer models. It introduces a framework termed SMoE-Dropout, which combines elements of dropout with SMoEs to purportedly alleviate the issue of representation collapse in scaled networks. The authors claim that their method yields significant runtime savings and improved performance across various tasks, although the paper lacks rigorous benchmarking to substantiate these claims.\n\n# Strength And Weaknesses\nThe paper attempts to address a recognized challenge in the field of neural network scaling—specifically, the quadratic increase in training costs associated with transformer models. However, many of the contributions are not sufficiently innovative, relying on established concepts without providing a compelling new perspective. The claim of a \"plug-and-play\" framework appears overly ambitious given the lack of novelty. Furthermore, while the authors report a 37% reduction in running time, the absence of comprehensive comparisons with existing methods weakens the credibility of their findings. The methodology, although clear, does not introduce significant advancements over prior work in the domain.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and logically structured, allowing readers to follow the proposed methodology and results. However, the novelty of the contributions is questionable, as many ideas presented are familiar and lack originality. Reproducibility may be an issue, particularly due to the absence of thorough benchmarks and evaluations against state-of-the-art methods. The work does not convincingly demonstrate how its claims translate to real-world applications or how the proposed framework can be effectively implemented.\n\n# Summary Of The Review\nOverall, this paper presents a rehashing of well-known concepts in the field of neural network scaling, with limited contributions that do not advance the state of the art significantly. While the authors make some relevant observations regarding transformer inefficiencies, the lack of rigorous evaluation and the reliance on established ideas detracts from the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the SMoE-Dropout framework, a novel approach designed to enhance the scalability of transformer architectures while addressing representational collapse. The authors demonstrate that SMoE-Dropout significantly improves model performance by implementing a \"self-slimmable\" property that allows for dynamic adjustment of model capacity. Through extensive experiments on various datasets, the framework outperforms traditional dense training methods and shows promise in reducing computational costs during training, with implications for transferability and robustness in various tasks.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to dropout mechanisms, specifically in the context of transformer models, which is a critical area of research given the increasing demand for efficient model architectures. The integration of SMoE-Dropout with existing techniques presents a pathway for further exploration, particularly in preventing overfitting and enhancing generalization. However, a notable weakness is the reliance on a randomly initialized fixed router network, which could limit adaptability; exploring learnable routing mechanisms could yield insights into expert specialization. Additionally, while the paper provides a strong empirical foundation, extending comparisons to other sparsity-inducing techniques would enhance the comprehensiveness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and experimental results, allowing for a good understanding of the SMoE-Dropout framework. The ablation studies conducted offer clarity on the contributions of different components, supporting reproducibility. The novelty is evident in the innovative application of dropout mechanisms within transformer architectures, though further theoretical insights regarding its advantages over conventional dropout methods would strengthen the paper.\n\n# Summary Of The Review\nOverall, the SMoE-Dropout framework presents a compelling advancement in the efficiency of transformer models, with significant implications for scalability and model performance. While the approach has notable strengths in its innovative design and empirical validation, there remain opportunities for improvement in terms of adaptability and broader comparisons with other techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces Sparse Mixture of Experts Dropout (SMoE-Dropout), a novel training approach designed to enhance the performance and efficiency of transformer models. The methodology employs a sparse mixture of experts framework that allows for selective activation of model parameters, leading to significant reductions in Bits-Per-Character (BPC) across various architectures, including Transformer-XL, BERT, and RoBERTa. Key findings include substantial improvements in training efficiency, with up to 37% time savings, enhanced transfer learning performance on downstream tasks, and increased scalability with the number of activated experts.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its empirical validation of SMoE-Dropout, demonstrating clear performance gains over traditional dense training methods in multiple models and tasks. The reported improvements in BPC and training efficiency are particularly noteworthy, showcasing the method's practical applicability. However, potential weaknesses include a lack of broader comparison with other state-of-the-art methods beyond dense training and limited exploration of the theoretical implications of the self-slimmable property of the model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers. The quality of the experimental setup appears robust, with comprehensive metrics and thorough evaluations across different tasks and architectures. The novelty of the SMoE-Dropout approach is evident, particularly in its innovative use of sparse activation. However, reproducibility could be enhanced by providing additional details on hyperparameter tuning and implementation specifics.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for SMoE-Dropout as a superior alternative to traditional dense training methods, with significant improvements in performance and efficiency. While the contributions are substantial, further exploration of comparative methods and theoretical aspects would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach termed \"Sparse Mixture-of-Experts Dropout\" (SMoE-Dropout), which aims to address the issue of representational collapse in neural networks. The methodology involves utilizing a sparsely activated Mixture-of-Experts framework as a replacement for traditional dropout techniques. The authors demonstrate through extensive experiments that SMoE-Dropout not only enhances model performance on benchmark datasets but also improves training efficiency. Key findings indicate that this method significantly reduces overfitting while maintaining or improving accuracy compared to baseline dropout methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to dropout and the comprehensive experimental validation. The introduction of SMoE-Dropout is well-motivated and fills a gap in existing literature regarding dropout techniques. However, the paper has weaknesses, including a lengthy and complex title that could hinder initial engagement. Additionally, the use of technical jargon without adequate definitions may limit accessibility for a broader audience. Transition issues between sections disrupt the reading flow, and the presence of redundancy in explanations detracts from the overall clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by its dense abstract and abrupt transitions between sections. While the contributions are significant and the methodology appears solid, the presentation could be improved through clearer definitions of technical terms and better structured sections. The novel nature of SMoE-Dropout adds value to the field, yet the paper's overall readability could be enhanced through concise language and improved visual aids. The reproducibility of the results is supported by detailed experimental setups, though clearer captions and contextual explanations for figures would further assist readers in understanding the findings.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in dropout techniques through the introduction of SMoE-Dropout, backed by thorough experimental results. However, improvements in clarity, structure, and accessibility are necessary to enhance its impact and ensure that a wider audience can appreciate its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.677057734676035,
    -1.9218234791052788,
    -1.7865362719944138,
    -1.7811383008982569,
    -1.7478480776907424,
    -1.7569080684904823,
    -1.5148654387044886,
    -1.8429309253312636,
    -1.8003465674334618,
    -1.8696246618807772,
    -1.6332764812221239,
    -1.3634303024763903,
    -1.7269577581456745,
    -1.8976377184458035,
    -1.696891434855789,
    -1.6221384632811753,
    -1.9571396866985782,
    -1.79648682578437,
    -1.8088072540373834,
    -1.5869134718200721,
    -1.9201316457934874,
    -1.740490882062861,
    -1.9329630961214714,
    -1.5993172919646301,
    -1.7749779705036033,
    -1.976220639052616,
    -1.9834751261855572,
    -1.7821999565528999,
    -1.67893765506912
  ],
  "logp_cond": [
    [
      0.0,
      -2.401053605776425,
      -2.3866380380658625,
      -2.4158327556381485,
      -2.41440799748344,
      -2.3929168830491547,
      -2.4425351259196217,
      -2.4307296809549603,
      -2.4362523336246587,
      -2.4176094524307032,
      -2.4496336322824166,
      -2.504065747220323,
      -2.4051889820848293,
      -2.3877187493552667,
      -2.4341706825923075,
      -2.4411131549749916,
      -2.429645279014949,
      -2.409921149562659,
      -2.420953395000562,
      -2.442133415614429,
      -2.479343365488325,
      -2.4832972343290733,
      -2.4425481719790265,
      -2.470564373332672,
      -2.432159084170927,
      -2.4603584334471624,
      -2.395022737656041,
      -2.4272182474373745,
      -2.477177275337165
    ],
    [
      -1.5539672101874034,
      0.0,
      -1.391047292853754,
      -1.400558246524367,
      -1.4350399787222208,
      -1.458909041761242,
      -1.4889595169094403,
      -1.5180757455398441,
      -1.4092950168605733,
      -1.3990313335805813,
      -1.472436728455041,
      -1.6878338983388963,
      -1.3938405094639468,
      -1.3635023604260479,
      -1.447776639662607,
      -1.4088785942608015,
      -1.4962083145008582,
      -1.4990230366437702,
      -1.4695315044633686,
      -1.4391208786379541,
      -1.4848632807014395,
      -1.540616590092234,
      -1.4927618483380722,
      -1.5128105520882675,
      -1.496106784405873,
      -1.5286801086724435,
      -1.4500294677621561,
      -1.4574510169992083,
      -1.5698487371845726
    ],
    [
      -1.3799601610299537,
      -1.2265211415075292,
      0.0,
      -1.269408097898923,
      -1.337281843732395,
      -1.3300136296728537,
      -1.3664955475092628,
      -1.3207499133504312,
      -1.336231400513117,
      -1.276184363717029,
      -1.3221435690156864,
      -1.5192727610994563,
      -1.2497814993039134,
      -1.2506081633754498,
      -1.3674339615939173,
      -1.3301915018637822,
      -1.271725190395416,
      -1.3857480696907087,
      -1.293538780587076,
      -1.3979463333315716,
      -1.4145306773423656,
      -1.4341408028211617,
      -1.3344838510412473,
      -1.4214821702696503,
      -1.2813757526066383,
      -1.3646657969461127,
      -1.3024485639274939,
      -1.328901452613439,
      -1.4638117127103114
    ],
    [
      -1.325601789372023,
      -1.1769597061959012,
      -1.2711329950311132,
      0.0,
      -1.2936335625658415,
      -1.2890226096444237,
      -1.3104801747986572,
      -1.3456743419915835,
      -1.2469851255527777,
      -1.246220027573371,
      -1.3143878825549375,
      -1.5184851607679628,
      -1.170240949260198,
      -1.2194649775958164,
      -1.326741061903081,
      -1.250265559263078,
      -1.321219356443558,
      -1.3029067930460014,
      -1.2942635707956898,
      -1.3957067137382244,
      -1.3381984905972786,
      -1.3979189657659212,
      -1.3624170345680493,
      -1.3743367297580815,
      -1.3349405986813496,
      -1.3176566772429512,
      -1.2861393507312506,
      -1.2804888378873465,
      -1.404022734692747
    ],
    [
      -1.3438595344595239,
      -1.15172085751909,
      -1.2112597268304086,
      -1.2759104791712792,
      0.0,
      -1.2225589097883127,
      -1.3129718906529884,
      -1.2714760388472872,
      -1.2086768144480902,
      -1.2045407956414915,
      -1.3006377477881954,
      -1.452252767572908,
      -1.2016594529229783,
      -1.2367025328367935,
      -1.3086388856265805,
      -1.2925230967930055,
      -1.275985701044586,
      -1.2975431120218857,
      -1.2945892496448597,
      -1.3376793116003474,
      -1.3289718520796685,
      -1.3854256496560056,
      -1.276351063111125,
      -1.3104929858453025,
      -1.2527235845965126,
      -1.3541711405745747,
      -1.2284520123958702,
      -1.2020397081738123,
      -1.3891358476094187
    ],
    [
      -1.3669051102201726,
      -1.247684613623396,
      -1.2667184399549556,
      -1.2809063548160764,
      -1.2680589495395465,
      0.0,
      -1.3293954834029154,
      -1.2983600966520183,
      -1.3308139571508961,
      -1.342516835965879,
      -1.3081771043962203,
      -1.4699870263122985,
      -1.2606733165494897,
      -1.2328717496922525,
      -1.3355924931447392,
      -1.3017088771208554,
      -1.3322724551677325,
      -1.346903565309478,
      -1.3469249736586537,
      -1.3674028501014062,
      -1.358982918924938,
      -1.3983830246791957,
      -1.3269583209445297,
      -1.377048318320284,
      -1.2817933344221135,
      -1.3651592545718316,
      -1.321095673477088,
      -1.3224951709815,
      -1.419520152988349
    ],
    [
      -1.1868797761222403,
      -1.1228064366543729,
      -1.1654328315450262,
      -1.1413268756493835,
      -1.158374425046063,
      -1.1322168885593586,
      0.0,
      -1.1722737665306917,
      -1.135866467362964,
      -1.1636490796600552,
      -1.23546533381398,
      -1.261392418251634,
      -1.1559567306706342,
      -1.1366558926150794,
      -1.142303536860454,
      -1.173328427389334,
      -1.1838980765611329,
      -1.1464701651297418,
      -1.1531882275436036,
      -1.1748784447009708,
      -1.1880373287578831,
      -1.2571216663428633,
      -1.179775671135258,
      -1.2400927433027433,
      -1.1954137724196083,
      -1.2139527162948502,
      -1.1414449953472479,
      -1.1702340631767345,
      -1.2541401016094387
    ],
    [
      -1.521018585474492,
      -1.4106947889783503,
      -1.3780333405714984,
      -1.3938059944196926,
      -1.4049729098660637,
      -1.3981775540765935,
      -1.4557362352640522,
      0.0,
      -1.392780478809878,
      -1.425677107920588,
      -1.447353610627475,
      -1.5949145148913868,
      -1.3505036946593754,
      -1.3671036843263065,
      -1.491885661128482,
      -1.4345911078026032,
      -1.3937350006334346,
      -1.4394549326730404,
      -1.4399359201675532,
      -1.5028386474753832,
      -1.5094072590161707,
      -1.4870177966136362,
      -1.4127241750359039,
      -1.473826497682968,
      -1.3880833570951678,
      -1.4266259054795531,
      -1.4012424291360939,
      -1.3891564817413151,
      -1.5199089755925788
    ],
    [
      -1.4510639030462753,
      -1.25283069843719,
      -1.3569607584442596,
      -1.2589420552502166,
      -1.330839722599267,
      -1.3636656173931858,
      -1.3744208701005018,
      -1.328995961839875,
      0.0,
      -1.353181751387552,
      -1.4266951367532426,
      -1.5460467968400025,
      -1.3535791471349712,
      -1.276046120462427,
      -1.352586312047716,
      -1.4096006585283751,
      -1.379633081767752,
      -1.3481345027220957,
      -1.3688085013765128,
      -1.4705398735739357,
      -1.4207089592491022,
      -1.4378390018097371,
      -1.422152571084003,
      -1.3497523749316136,
      -1.424641437116911,
      -1.4003521282388367,
      -1.3422758581579521,
      -1.2554081534893653,
      -1.4652466909069082
    ],
    [
      -1.510508076420234,
      -1.413410234800325,
      -1.4288294315058687,
      -1.4265106275551778,
      -1.4571629564088744,
      -1.514329624749846,
      -1.5633916898198474,
      -1.457764214916313,
      -1.4533508427542938,
      0.0,
      -1.5007870835391826,
      -1.6924320284316925,
      -1.4107257503283885,
      -1.4073516442851202,
      -1.5359601640911287,
      -1.525366631825515,
      -1.4361747772682836,
      -1.526008280456113,
      -1.4604215825415026,
      -1.5745916474293136,
      -1.5228381341352561,
      -1.5610313768168347,
      -1.472962893501435,
      -1.5465319084874367,
      -1.4363470532126232,
      -1.510636146534642,
      -1.437435360659455,
      -1.469859899833049,
      -1.567378166574349
    ],
    [
      -1.2520356516957811,
      -1.1502273381610901,
      -1.1339228497933647,
      -1.1543736817111656,
      -1.1886913360123228,
      -1.1842563017606889,
      -1.2582200298095358,
      -1.1977995373998993,
      -1.2038143083609796,
      -1.1920642806242492,
      0.0,
      -1.3427422381613883,
      -1.1287578753154521,
      -1.1495145037299943,
      -1.2461224489984666,
      -1.1409098389072458,
      -1.1568329500316044,
      -1.1912670225028863,
      -1.2079437964473596,
      -1.214484163440841,
      -1.1945556314427364,
      -1.2264418188513344,
      -1.2524107653552998,
      -1.2410526653164369,
      -1.1651606497225597,
      -1.1745569823619086,
      -1.171325810034155,
      -1.2224592513827903,
      -1.2689508603035067
    ],
    [
      -1.1626270388515525,
      -1.1305891617581039,
      -1.1277396047808945,
      -1.0982011001205259,
      -1.1295980956250047,
      -1.1057466340503523,
      -1.088433860261094,
      -1.1169599188429467,
      -1.1084937320585242,
      -1.1194167268379838,
      -1.1201118745739236,
      0.0,
      -1.122986876103428,
      -1.113042657372044,
      -1.0723348240483828,
      -1.1006538478159902,
      -1.1028778091854274,
      -1.109688214357208,
      -1.0995882986838967,
      -1.109845379658753,
      -1.0898334389037532,
      -1.1128456039873602,
      -1.1139413091140802,
      -1.1010810698933493,
      -1.1265061678962687,
      -1.1156146174557433,
      -1.0999090746815552,
      -1.1115061506446178,
      -1.1233959387476222
    ],
    [
      -1.3242382015590182,
      -1.2527011223477307,
      -1.232220502556698,
      -1.2126878275143038,
      -1.2953211677917813,
      -1.2944319007650287,
      -1.385962908845522,
      -1.2684807576627977,
      -1.3042720484869783,
      -1.269071894730728,
      -1.308238850149654,
      -1.4881387401772141,
      0.0,
      -1.187964674446022,
      -1.389700370568453,
      -1.3074502230846592,
      -1.2739455788120728,
      -1.3245516283369525,
      -1.2817017969808355,
      -1.4010689683421418,
      -1.3390579188447649,
      -1.4124541552064118,
      -1.2958745926915596,
      -1.4015085149533917,
      -1.3024159226296355,
      -1.3380110850646532,
      -1.2764857445908329,
      -1.2945739030263432,
      -1.3998301189911706
    ],
    [
      -1.4421209273581128,
      -1.2565605073922075,
      -1.3196434953432035,
      -1.3218681619006738,
      -1.3503844223106758,
      -1.3492540274405773,
      -1.4315705589000534,
      -1.3557717042065716,
      -1.3462533801862444,
      -1.3111024460431524,
      -1.3854298049854308,
      -1.6015476925222916,
      -1.2591012118554605,
      0.0,
      -1.3386471546355054,
      -1.3906740804554047,
      -1.3589734553464023,
      -1.4108521489178807,
      -1.3610845623206551,
      -1.4919869561599757,
      -1.4597814516602654,
      -1.4697781591707635,
      -1.422218060690223,
      -1.4198402513109507,
      -1.3509499183125064,
      -1.4183117668633543,
      -1.3611747063222381,
      -1.3329708175873571,
      -1.5042327067243186
    ],
    [
      -1.3468145997926848,
      -1.16054467213856,
      -1.2602485277297286,
      -1.2311463713420974,
      -1.2554862403551896,
      -1.2730294478835102,
      -1.257206849118276,
      -1.2902588781848865,
      -1.2505543178455367,
      -1.2590895961624642,
      -1.316432291567715,
      -1.4054138808704388,
      -1.2766336702110421,
      -1.2392644530931545,
      0.0,
      -1.2780222919272657,
      -1.338014654438672,
      -1.2666134115087984,
      -1.291310563481318,
      -1.3244549364658107,
      -1.340734413675233,
      -1.331856909374918,
      -1.332304093510875,
      -1.2989747127031182,
      -1.291833033021746,
      -1.3565830270070918,
      -1.2796619155051432,
      -1.21718487407917,
      -1.3545693630000102
    ],
    [
      -1.2885622410964286,
      -1.1457218244033045,
      -1.1765889754328855,
      -1.1563663512006528,
      -1.2402862620618673,
      -1.206753875016186,
      -1.279120490239211,
      -1.233530656231694,
      -1.257175571378398,
      -1.2133673927821416,
      -1.2014966107138356,
      -1.3617435889141378,
      -1.1237834014106733,
      -1.1866900174192434,
      -1.2642948611826652,
      0.0,
      -1.2374080504127651,
      -1.2766561266027547,
      -1.2496671282903862,
      -1.2571578248069288,
      -1.2155887446685787,
      -1.2867397935093123,
      -1.2451870043481457,
      -1.250223501034994,
      -1.2502445506015527,
      -1.2083178200323619,
      -1.2172629422964354,
      -1.2337894587131755,
      -1.2737141504056844
    ],
    [
      -1.5764168190239285,
      -1.5190192729563547,
      -1.4290384834814729,
      -1.5016098597364753,
      -1.5342583826466778,
      -1.5110932026256647,
      -1.5733004901803098,
      -1.4418208569813697,
      -1.5462866118407417,
      -1.4985171966462418,
      -1.5476747004613156,
      -1.6896363129541965,
      -1.442030196121499,
      -1.4727017362607,
      -1.5975647033764502,
      -1.545119842079695,
      0.0,
      -1.5843773127571885,
      -1.566238823775632,
      -1.5742023840131174,
      -1.5511939152256267,
      -1.6075361648231876,
      -1.4957369173994544,
      -1.5552902995368385,
      -1.4521451756004753,
      -1.5631766051886906,
      -1.4214021504525645,
      -1.5248437109443054,
      -1.6061368647126577
    ],
    [
      -1.4131729571182587,
      -1.3223606362061013,
      -1.3218614850521098,
      -1.2789183141169496,
      -1.3664013838382678,
      -1.3250991944333659,
      -1.3315667087319696,
      -1.322144927179166,
      -1.308006156704917,
      -1.335271106000564,
      -1.3528804033811934,
      -1.4926397274735022,
      -1.3078432278178573,
      -1.299227174674734,
      -1.3103006880627577,
      -1.3475748437393842,
      -1.3422455084381262,
      0.0,
      -1.3198106172917472,
      -1.4184986966622868,
      -1.4276118397524362,
      -1.4504618828633153,
      -1.3904341059577787,
      -1.414142374680208,
      -1.3348936426732247,
      -1.3724445140154722,
      -1.321601394441639,
      -1.2488479012335392,
      -1.432147547376835
    ],
    [
      -1.5092714305356176,
      -1.365833373829087,
      -1.32483927568212,
      -1.3448120126091951,
      -1.396260494083506,
      -1.4109108528180334,
      -1.373647579730535,
      -1.4374339138566485,
      -1.4084709397780346,
      -1.372998400960783,
      -1.426597598003218,
      -1.5298225056558612,
      -1.3320706216576252,
      -1.3957407373186914,
      -1.442968256937761,
      -1.44860686356366,
      -1.4358041593456785,
      -1.4044400971448756,
      0.0,
      -1.4912876600809974,
      -1.4557831640101193,
      -1.4997755140470874,
      -1.4425266478452865,
      -1.4746084442943506,
      -1.4512725864439233,
      -1.4379969724743404,
      -1.3652774889061108,
      -1.4293405585166536,
      -1.51224669709669
    ],
    [
      -1.2566518206094373,
      -1.2175945550543688,
      -1.2538745495914465,
      -1.3095681951462903,
      -1.2985562014490648,
      -1.256950206686328,
      -1.263323501715221,
      -1.285163073146272,
      -1.3057779099224067,
      -1.3104000425173328,
      -1.2498686948276592,
      -1.3813835520531568,
      -1.3054534636557702,
      -1.2575548079069303,
      -1.2746835363625362,
      -1.2693503328997295,
      -1.270587532326691,
      -1.299064804525308,
      -1.3060247804643135,
      0.0,
      -1.3007108349530907,
      -1.3249948986920963,
      -1.307049784083646,
      -1.2706542358151691,
      -1.2846527292705692,
      -1.315452948065351,
      -1.2873422085036266,
      -1.2592452964670757,
      -1.3175037971854735
    ],
    [
      -1.5269557495247879,
      -1.427356347563844,
      -1.4332063550001408,
      -1.4856366211907,
      -1.4507934019286637,
      -1.466977839228854,
      -1.5305847085664037,
      -1.4351980246491336,
      -1.4793970784076673,
      -1.4234758814834303,
      -1.5127999231245783,
      -1.6345745870827506,
      -1.4340608180373606,
      -1.474600796309363,
      -1.5430983435293057,
      -1.4324833662137237,
      -1.4626553596114813,
      -1.5597825645396917,
      -1.5271496370956272,
      -1.5549822075667656,
      0.0,
      -1.5698992297874712,
      -1.481645138485291,
      -1.5106184748907732,
      -1.504962181039049,
      -1.452295446359556,
      -1.4656927750875721,
      -1.5141391506920179,
      -1.514797091945924
    ],
    [
      -1.4088489625860647,
      -1.2827225715062287,
      -1.355180184822124,
      -1.3011571386946854,
      -1.363976281015785,
      -1.3540011271699914,
      -1.3823416614827742,
      -1.3265266963755393,
      -1.3110611913941,
      -1.3496805993451908,
      -1.3542329217233624,
      -1.4510027870918967,
      -1.3336315638599539,
      -1.3272996037275633,
      -1.3360688083146326,
      -1.3273869778528427,
      -1.3359542995347744,
      -1.3992017058153636,
      -1.3739524039861735,
      -1.360735190628711,
      -1.39054491225829,
      0.0,
      -1.3410483899316126,
      -1.3813340133813468,
      -1.3484282862131025,
      -1.3517169467046786,
      -1.3178117289941993,
      -1.3516104817084664,
      -1.3651489160257781
    ],
    [
      -1.5299089354786235,
      -1.4658245452223695,
      -1.452128371927115,
      -1.4418531446490757,
      -1.5356744966631333,
      -1.4706706417077435,
      -1.5414223484276555,
      -1.4398760003869477,
      -1.4787957485402368,
      -1.4217195024660714,
      -1.5118393405336106,
      -1.6473555579284045,
      -1.4121615374936485,
      -1.4213618372303545,
      -1.5105831170914217,
      -1.5068288657261788,
      -1.4260858607469609,
      -1.5207449432421831,
      -1.4846093408574221,
      -1.5170358504298798,
      -1.5634448702265051,
      -1.5480645321229578,
      0.0,
      -1.5462483334916235,
      -1.325142457403682,
      -1.5368665051872716,
      -1.405307035423599,
      -1.4919822966431424,
      -1.5965266274620753
    ],
    [
      -1.3354301158312982,
      -1.1783116732956724,
      -1.2228952412437086,
      -1.2543974850429018,
      -1.2557758653027404,
      -1.248335486526577,
      -1.3258606582790873,
      -1.2355661598001177,
      -1.137698295233742,
      -1.2005784952993102,
      -1.237789141629055,
      -1.3832782461812165,
      -1.2366328950890573,
      -1.1783897322767667,
      -1.2445447774400273,
      -1.2331267948281963,
      -1.2385814909419557,
      -1.2887461950273305,
      -1.2616445186155778,
      -1.266080527701458,
      -1.2609712202458376,
      -1.287017106980331,
      -1.2755529845452322,
      0.0,
      -1.2556354507731888,
      -1.2484453322496212,
      -1.2519866480527884,
      -1.1421310771655564,
      -1.2821951757289116
    ],
    [
      -1.3992811178813866,
      -1.3677340326197558,
      -1.2693945697894626,
      -1.3635156286813856,
      -1.3528319119233636,
      -1.3221805949391656,
      -1.3957945070211586,
      -1.2508848631153668,
      -1.349919363594212,
      -1.3079000716959213,
      -1.3264744806396827,
      -1.5208214711381014,
      -1.310273851726229,
      -1.2651727649300892,
      -1.363610530703103,
      -1.3788798942301308,
      -1.2508870492653417,
      -1.342807859648238,
      -1.369736813763226,
      -1.392802218654653,
      -1.4139668575659454,
      -1.4158289463589218,
      -1.2803197791168432,
      -1.3533207415780828,
      0.0,
      -1.3752846018608331,
      -1.2520643711750215,
      -1.300134016276517,
      -1.4511918726613533
    ],
    [
      -1.617889206325456,
      -1.6242088061516295,
      -1.5661795499293834,
      -1.6247613971951615,
      -1.663043866872129,
      -1.6821634797218807,
      -1.7060877910212826,
      -1.5751551746755443,
      -1.6584879470098381,
      -1.6298991485546697,
      -1.6099535144523776,
      -1.7594174432186138,
      -1.6049509661458938,
      -1.6207632734751243,
      -1.7152552676880326,
      -1.624926761302321,
      -1.6242932951154934,
      -1.6643414774096814,
      -1.694656259918442,
      -1.6793222236739835,
      -1.6635532673044613,
      -1.668821376466949,
      -1.6656018976092881,
      -1.681897263834411,
      -1.6608459517195449,
      0.0,
      -1.616592781473178,
      -1.6532560870123973,
      -1.6711885963326403
    ],
    [
      -1.5459438192521575,
      -1.4452047622938988,
      -1.4398389616851448,
      -1.4186547369438876,
      -1.517829753619182,
      -1.468970609015417,
      -1.5554376453055652,
      -1.508063139647065,
      -1.5033007005647558,
      -1.450478408660841,
      -1.512365775618682,
      -1.6876413599920885,
      -1.4172023378441574,
      -1.48482184382422,
      -1.5379786307070844,
      -1.4969071135206908,
      -1.4475420064098812,
      -1.5269091388267253,
      -1.4975621712220062,
      -1.5527245548691533,
      -1.5549385491056325,
      -1.5972880887551646,
      -1.4957902134440857,
      -1.5724437571333743,
      -1.487076662428423,
      -1.5541135188940303,
      0.0,
      -1.529229518727408,
      -1.5697992246363908
    ],
    [
      -1.358495817002859,
      -1.2414491994672747,
      -1.253300982922442,
      -1.2305715838220221,
      -1.266755519852917,
      -1.2486968233606242,
      -1.3086355366381501,
      -1.2413318945978584,
      -1.1554103437487728,
      -1.2238627906680288,
      -1.312768147603406,
      -1.4852769707552158,
      -1.2548920581120788,
      -1.1897274059438063,
      -1.2517909397139535,
      -1.32791471947782,
      -1.2886161250147676,
      -1.2297678813768753,
      -1.2894308453422807,
      -1.3256473812616323,
      -1.3390260506193274,
      -1.3485776201368622,
      -1.3304132417256873,
      -1.2368355129456958,
      -1.2588405379814278,
      -1.3328297709522976,
      -1.249881953305306,
      0.0,
      -1.3579330343941725
    ],
    [
      -1.344651785975222,
      -1.3091432391689342,
      -1.3126025097776528,
      -1.3082479153037243,
      -1.342344349303438,
      -1.3277768433567683,
      -1.349051063232236,
      -1.3333262181606944,
      -1.3265983803715082,
      -1.2850144097706533,
      -1.3144718373965403,
      -1.3973875102253601,
      -1.2894645777759948,
      -1.3146810210635695,
      -1.341031809432962,
      -1.3246117758518208,
      -1.2813402677650148,
      -1.3391737626504265,
      -1.3332342276974576,
      -1.3319330567597047,
      -1.311401259016662,
      -1.3172538322787115,
      -1.311425202799446,
      -1.3293253204272968,
      -1.311397597990369,
      -1.3045422792020034,
      -1.2760018924414427,
      -1.3315243463084756,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.27600412889961,
      0.29041969661017264,
      0.26122497903788666,
      0.2626497371925951,
      0.2841408516268804,
      0.2345226087564134,
      0.2463280537210748,
      0.2408054010513765,
      0.2594482822453319,
      0.22742410239361854,
      0.17299198745571198,
      0.2718687525912058,
      0.2893389853207684,
      0.24288705208372763,
      0.23594457970104354,
      0.24741245566108594,
      0.26713658511337623,
      0.2561043396754732,
      0.23492431906160594,
      0.19771436918771013,
      0.1937605003469618,
      0.23450956269700862,
      0.20649336134336327,
      0.24489865050510806,
      0.2166993012288727,
      0.282034997019994,
      0.24983948723866067,
      0.19988045933887033
    ],
    [
      0.3678562689178755,
      0.0,
      0.5307761862515248,
      0.521265232580912,
      0.48678350038305807,
      0.4629144373440368,
      0.43286396219583856,
      0.4037477335654347,
      0.5125284622447055,
      0.5227921455246975,
      0.4493867506502378,
      0.23398958076638254,
      0.5279829696413321,
      0.558321118679231,
      0.4740468394426718,
      0.5129448848444773,
      0.4256151646044206,
      0.4228004424615086,
      0.4522919746419103,
      0.4827026004673247,
      0.4369601984038394,
      0.3812068890130449,
      0.4290616307672066,
      0.4090129270170113,
      0.42571669469940576,
      0.3931433704328353,
      0.4717940113431227,
      0.4643724621060705,
      0.3519747419207062
    ],
    [
      0.4065761109644601,
      0.5600151304868846,
      0.0,
      0.5171281740954907,
      0.4492544282620188,
      0.45652264232156003,
      0.42004072448515095,
      0.46578635864398255,
      0.4503048714812967,
      0.5103519082773849,
      0.46439270297872737,
      0.2672635108949575,
      0.5367547726905004,
      0.535928108618964,
      0.4191023104004965,
      0.45634477013063157,
      0.5148110815989977,
      0.40078820230370504,
      0.49299749140733784,
      0.3885899386628422,
      0.3720055946520482,
      0.35239546917325204,
      0.45205242095316645,
      0.3650541017247635,
      0.5051605193877755,
      0.4218704750483011,
      0.4840877080669199,
      0.4576348193809747,
      0.3227245592841024
    ],
    [
      0.45553651152623376,
      0.6041785947023557,
      0.5100053058671437,
      0.0,
      0.48750473833241537,
      0.49211569125383314,
      0.47065812609959967,
      0.43546395890667333,
      0.5341531753454791,
      0.5349182733248858,
      0.4667504183433193,
      0.26265314013029406,
      0.6108973516380589,
      0.5616733233024405,
      0.4543972389951758,
      0.5308727416351788,
      0.45991894445469894,
      0.47823150785225543,
      0.4868747301025671,
      0.38543158716003245,
      0.4429398103009783,
      0.38321933513233564,
      0.41872126633020756,
      0.40680157114017534,
      0.44619770221690724,
      0.46348162365530565,
      0.49499895016700624,
      0.5006494630109104,
      0.3771155662055099
    ],
    [
      0.40398854323121847,
      0.5961272201716523,
      0.5365883508603337,
      0.47193759851946315,
      0.0,
      0.5252891679024296,
      0.434876187037754,
      0.47637203884345514,
      0.5391712632426522,
      0.5433072820492508,
      0.44721032990254694,
      0.29559531011783435,
      0.5461886247677641,
      0.5111455448539488,
      0.43920919206416187,
      0.4553249808977369,
      0.47186237664615627,
      0.4503049656688567,
      0.4532588280458827,
      0.410168766090395,
      0.41887622561107385,
      0.3624224280347368,
      0.47149701457961735,
      0.43735509184543986,
      0.4951244930942298,
      0.3936769371161677,
      0.5193960652948721,
      0.54580836951693,
      0.3587122300813237
    ],
    [
      0.39000295827030973,
      0.5092234548670862,
      0.4901896285355267,
      0.4760017136744059,
      0.48884911895093586,
      0.0,
      0.42751258508756695,
      0.45854797183846396,
      0.4260941113395862,
      0.4143912325246033,
      0.44873096409426205,
      0.2869210421781838,
      0.49623475194099265,
      0.5240363187982298,
      0.42131557534574315,
      0.45519919136962694,
      0.4246356133227498,
      0.41000450318100423,
      0.4099830948318286,
      0.38950521838907615,
      0.39792514956554426,
      0.3585250438112866,
      0.4299497475459526,
      0.37985975017019835,
      0.4751147340683688,
      0.3917488139186507,
      0.4358123950133943,
      0.43441289750898227,
      0.3373879155021333
    ],
    [
      0.32798566258224837,
      0.3920590020501158,
      0.34943260715946245,
      0.37353856305510513,
      0.3564910136584256,
      0.38264855014513,
      0.0,
      0.34259167217379694,
      0.3789989713415247,
      0.3512163590444335,
      0.2794001048905086,
      0.2534730204528546,
      0.35890870803385444,
      0.37820954608940927,
      0.3725619018440347,
      0.34153701131515457,
      0.33096736214335576,
      0.3683952735747469,
      0.361677211160885,
      0.33998699400351784,
      0.3268281099466055,
      0.2577437723616254,
      0.3350897675692306,
      0.2747726954017453,
      0.3194516662848803,
      0.3009127224096384,
      0.3734204433572408,
      0.34463137552775414,
      0.26072533709504997
    ],
    [
      0.3219123398567716,
      0.43223613635291325,
      0.46489758475976517,
      0.449124930911571,
      0.43795801546519986,
      0.44475337125467007,
      0.3871946900672114,
      0.0,
      0.45015044652138547,
      0.41725381741067546,
      0.39557731470378865,
      0.24801641043987677,
      0.4924272306718882,
      0.4758272410049571,
      0.3510452642027815,
      0.40833981752866033,
      0.449195924697829,
      0.4034759926582232,
      0.40299500516371034,
      0.34009227785588036,
      0.3335236663150929,
      0.35591312871762737,
      0.4302067502953597,
      0.36910442764829554,
      0.45484756823609573,
      0.4163050198517104,
      0.4416884961951697,
      0.45377444358994845,
      0.3230219497386848
    ],
    [
      0.34928266438718647,
      0.5475158689962718,
      0.4433858089892022,
      0.5414045121832451,
      0.4695068448341948,
      0.43668095004027596,
      0.42592569733295993,
      0.47135060559358677,
      0.0,
      0.44716481604590985,
      0.3736514306802192,
      0.2542997705934593,
      0.4467674202984906,
      0.5243004469710348,
      0.4477602553857458,
      0.39074590890508665,
      0.42071348566570976,
      0.4522120647113661,
      0.431538066056949,
      0.3298066938595261,
      0.37963760818435954,
      0.36250756562372466,
      0.3781939963494587,
      0.4505941925018482,
      0.37570513031655084,
      0.3999944391946251,
      0.45807070927550964,
      0.5449384139440965,
      0.3350998765265536
    ],
    [
      0.35911658546054315,
      0.45621442708045223,
      0.44079523037490853,
      0.4431140343255995,
      0.41246170547190286,
      0.35529503713093136,
      0.3062329720609298,
      0.4118604469644642,
      0.4162738191264834,
      0.0,
      0.3688375783415947,
      0.17719263344908476,
      0.4588989115523887,
      0.4622730175956571,
      0.3336644977896486,
      0.34425803005526223,
      0.4334498846124937,
      0.3436163814246642,
      0.4092030793392747,
      0.2950330144514637,
      0.3467865277455211,
      0.30859328506394257,
      0.3966617683793423,
      0.32309275339334054,
      0.433277608668154,
      0.3589885153461352,
      0.43218930122132226,
      0.39976476204772826,
      0.30224649530642833
    ],
    [
      0.38124082952634275,
      0.48304914306103375,
      0.4993536314287592,
      0.47890279951095827,
      0.4445851452098011,
      0.449020179461435,
      0.37505645141258803,
      0.43547694382222457,
      0.4294621728611443,
      0.4412122005978747,
      0.0,
      0.2905342430607356,
      0.5045186059066717,
      0.48376197749212957,
      0.3871540322236573,
      0.49236664231487803,
      0.4764435311905195,
      0.4420094587192376,
      0.42533268477476427,
      0.4187923177812829,
      0.4387208497793875,
      0.40683466237078947,
      0.38086571586682405,
      0.392223815905687,
      0.46811583149956415,
      0.45871949886021524,
      0.46195067118796884,
      0.41081722983933355,
      0.3643256209186172
    ],
    [
      0.2008032636248378,
      0.23284114071828643,
      0.23569069769549578,
      0.26522920235586445,
      0.23383220685138562,
      0.25768366842603796,
      0.2749964422152964,
      0.24647038363344365,
      0.2549365704178661,
      0.24401357563840653,
      0.24331842790246672,
      0.0,
      0.24044342637296223,
      0.2503876451043463,
      0.29109547842800754,
      0.2627764546604001,
      0.26055249329096286,
      0.2537420881191823,
      0.26384200379249356,
      0.2535849228176372,
      0.27359686357263713,
      0.2505846984890301,
      0.24948899336231012,
      0.262349232583041,
      0.23692413458012163,
      0.247815685020647,
      0.2635212277948351,
      0.2519241518317725,
      0.24003436372876807
    ],
    [
      0.40271955658665637,
      0.47425663579794386,
      0.49473725558897663,
      0.5142699306313707,
      0.43163659035389323,
      0.43252585738064586,
      0.34099484930015245,
      0.4584770004828769,
      0.42268570965869623,
      0.4578858634149465,
      0.4187189079960205,
      0.2388190179684604,
      0.0,
      0.5389930836996526,
      0.33725738757722157,
      0.41950753506101535,
      0.4530121793336017,
      0.40240612980872204,
      0.445255961164839,
      0.32588878980353275,
      0.38789983930090965,
      0.3145036029392627,
      0.43108316545411496,
      0.32544924319228286,
      0.4245418355160391,
      0.3889466730810214,
      0.4504720135548417,
      0.4323838551193313,
      0.32712763915450394
    ],
    [
      0.4555167910876907,
      0.641077211053596,
      0.5779942231026001,
      0.5757695565451297,
      0.5472532961351277,
      0.5483836910052262,
      0.4660671595457502,
      0.541866014239232,
      0.5513843382595591,
      0.5865352724026511,
      0.5122079134603728,
      0.29609002592351197,
      0.638536506590343,
      0.0,
      0.5589905638102981,
      0.5069636379903988,
      0.5386642630994012,
      0.48678556952792285,
      0.5365531561251484,
      0.40565076228582786,
      0.43785626678553813,
      0.42785955927504005,
      0.4754196577555805,
      0.4777974671348528,
      0.5466878001332971,
      0.4793259515824493,
      0.5364630121235654,
      0.5646669008584464,
      0.3934050117214849
    ],
    [
      0.3500768350631043,
      0.5363467627172291,
      0.43664290712606046,
      0.4657450635136917,
      0.44140519450059945,
      0.42386198697227884,
      0.4396845857375131,
      0.4066325566709026,
      0.44633711701025236,
      0.4378018386933249,
      0.3804591432880742,
      0.29147755398535025,
      0.42025776464474696,
      0.4576269817626346,
      0.0,
      0.4188691429285234,
      0.3588767804171171,
      0.4302780233469907,
      0.4055808713744711,
      0.3724364983899784,
      0.35615702118055603,
      0.3650345254808711,
      0.3645873413449141,
      0.39791672215267093,
      0.40505840183404307,
      0.3403084078486973,
      0.4172295193506459,
      0.4797065607766191,
      0.3423220718557789
    ],
    [
      0.3335762221847467,
      0.4764166388778708,
      0.44554948784828974,
      0.4657721120805225,
      0.38185220121930796,
      0.41538458826498936,
      0.34301797304196424,
      0.3886078070494814,
      0.3649628919027772,
      0.4087710704990337,
      0.42064185256733966,
      0.26039487436703745,
      0.49835506187050194,
      0.4354484458619319,
      0.35784360209851007,
      0.0,
      0.38473041286841014,
      0.34548233667842054,
      0.3724713349907891,
      0.36498063847424644,
      0.4065497186125966,
      0.33539866977186295,
      0.3769514589330296,
      0.37191496224618126,
      0.3718939126796226,
      0.4138206432488134,
      0.4048755209847399,
      0.3883490045679998,
      0.3484243128754909
    ],
    [
      0.3807228676746497,
      0.43812041374222344,
      0.5281012032171053,
      0.45552982696210287,
      0.4228813040519004,
      0.4460464840729135,
      0.38383919651826837,
      0.5153188297172084,
      0.4108530748578365,
      0.45862249005233635,
      0.4094649862372626,
      0.2675033737443817,
      0.5151094905770792,
      0.4844379504378782,
      0.359574983322128,
      0.4120198446188832,
      0.0,
      0.37276237394138967,
      0.39090086292294624,
      0.3829373026854608,
      0.4059457714729515,
      0.34960352187539057,
      0.4614027692991238,
      0.4018493871617397,
      0.5049945110981029,
      0.39396308150988757,
      0.5357375362460137,
      0.4322959757542728,
      0.3510028219859205
    ],
    [
      0.3833138686661113,
      0.4741261895782687,
      0.47462534073226026,
      0.5175685116674205,
      0.4300854419461022,
      0.47138763135100414,
      0.46492011705240044,
      0.4743418986052039,
      0.488480669079453,
      0.46121571978380604,
      0.4436064224031766,
      0.30384709831086787,
      0.4886435979665127,
      0.4972596511096361,
      0.48618613772161234,
      0.44891198204498584,
      0.45424131734624384,
      0.0,
      0.4766762084926228,
      0.37798812912208324,
      0.3688749860319338,
      0.3460249429210547,
      0.4060527198265913,
      0.38234445110416204,
      0.4615931831111453,
      0.42404231176889784,
      0.4748854313427311,
      0.5476389245508309,
      0.364339278407535
    ],
    [
      0.2995358235017658,
      0.4429738802082963,
      0.4839679783552635,
      0.46399524142818827,
      0.41254675995387746,
      0.39789640121935,
      0.43515967430684843,
      0.37137334018073487,
      0.4003363142593488,
      0.43580885307660044,
      0.38220965603416546,
      0.2789847483815222,
      0.47673663237975816,
      0.413066516718692,
      0.3658389970996223,
      0.36020039047372343,
      0.3730030946917049,
      0.4043671568925078,
      0.0,
      0.31751959395638596,
      0.3530240900272641,
      0.309031739990296,
      0.3662806061920969,
      0.33419880974303284,
      0.3575346675934601,
      0.37081028156304296,
      0.4435297651312726,
      0.37946669552072976,
      0.2965605569406935
    ],
    [
      0.3302616512106349,
      0.3693189167657034,
      0.33303892222862563,
      0.27734527667378184,
      0.28835727037100733,
      0.3299632651337441,
      0.32358997010485124,
      0.30175039867380016,
      0.2811355618976654,
      0.27651342930273937,
      0.337044776992413,
      0.20552991976691537,
      0.28146000816430194,
      0.32935866391314184,
      0.31222993545753597,
      0.3175631389203426,
      0.31632593949338106,
      0.2878486672947642,
      0.28088869135575867,
      0.0,
      0.28620263686698144,
      0.26191857312797584,
      0.2798636877364262,
      0.316259236004903,
      0.30226074254950297,
      0.2714605237547212,
      0.29957126331644557,
      0.3276681753529964,
      0.26940967463459864
    ],
    [
      0.39317589626869953,
      0.4927752982296434,
      0.48692529079334657,
      0.43449502460278744,
      0.4693382438648237,
      0.45315380656463344,
      0.38954693722708367,
      0.4849336211443538,
      0.44073456738582006,
      0.4966557643100571,
      0.40733172266890905,
      0.2855570587107368,
      0.48607082775612676,
      0.44553084948412436,
      0.37703330226418164,
      0.48764827957976364,
      0.4574762861820061,
      0.3603490812537957,
      0.39298200869786015,
      0.36514943822672175,
      0.0,
      0.3502324160060162,
      0.4384865073081965,
      0.4095131709027142,
      0.4151694647544384,
      0.4678361994339313,
      0.45443887070591527,
      0.4059924951014695,
      0.40533455384756345
    ],
    [
      0.33164191947679633,
      0.45776831055663236,
      0.38531069724073697,
      0.43933374336817566,
      0.3765146010470761,
      0.38648975489286963,
      0.3581492205800869,
      0.41396418568732174,
      0.42942969066876113,
      0.39081028271767027,
      0.38625796033949866,
      0.2894880949709644,
      0.4068593182029072,
      0.4131912783352978,
      0.40442207374822847,
      0.4131039042100184,
      0.40453658252808666,
      0.34128917624749744,
      0.3665384780766876,
      0.37975569143415,
      0.34994596980457104,
      0.0,
      0.3994424921312485,
      0.3591568686815143,
      0.39206259584975856,
      0.3887739353581825,
      0.4226791530686618,
      0.38888040035439464,
      0.3753419660370829
    ],
    [
      0.40305416064284794,
      0.46713855089910195,
      0.48083472419435647,
      0.4911099514723958,
      0.39728859945833817,
      0.4622924544137279,
      0.39154074769381597,
      0.4930870957345237,
      0.45416734758123467,
      0.5112435936554001,
      0.42112375558786086,
      0.28560753819306695,
      0.520801558627823,
      0.511601258891117,
      0.42237997903004976,
      0.42613423039529263,
      0.5068772353745106,
      0.4122181528792883,
      0.4483537552640493,
      0.4159272456915917,
      0.3695182258949663,
      0.38489856399851363,
      0.0,
      0.38671476262984794,
      0.6078206387177894,
      0.3960965909341998,
      0.5276560606978724,
      0.440980799478329,
      0.33643646865939614
    ],
    [
      0.2638871761333319,
      0.4210056186689577,
      0.3764220507209215,
      0.3449198069217283,
      0.3435414266618897,
      0.35098180543805313,
      0.27345663368554285,
      0.3637511321645124,
      0.46161899673088813,
      0.39873879666531997,
      0.36152815033557517,
      0.2160390457834136,
      0.36268439687557286,
      0.42092755968786344,
      0.3547725145246028,
      0.3661904971364338,
      0.36073580102267444,
      0.3105710969372997,
      0.3376727733490523,
      0.33323676426317217,
      0.3383460717187925,
      0.31230018498429923,
      0.3237643074193979,
      0.0,
      0.3436818411914413,
      0.35087195971500895,
      0.34733064391184176,
      0.45718621479907373,
      0.3171221162357185
    ],
    [
      0.37569685262221664,
      0.4072439378838475,
      0.5055834007141407,
      0.41146234182221764,
      0.4221460585802397,
      0.45279737556443767,
      0.3791834634824447,
      0.5240931073882364,
      0.42505860690939135,
      0.467077898807682,
      0.44850348986392063,
      0.2541564993655019,
      0.4647041187773744,
      0.509805205573514,
      0.4113674398005003,
      0.3960980762734725,
      0.5240909212382616,
      0.4321701108553653,
      0.4052411567403773,
      0.38217575184895036,
      0.36101111293765786,
      0.35914902414468153,
      0.49465819138676004,
      0.4216572289255205,
      0.0,
      0.39969336864277016,
      0.5229135993285818,
      0.47484395422708636,
      0.32378609784225
    ],
    [
      0.35833143272716006,
      0.3520118329009865,
      0.41004108912323267,
      0.3514592418574545,
      0.313176772180487,
      0.2940571593307353,
      0.2701328480313334,
      0.4010654643770717,
      0.3177326920427779,
      0.34632149049794636,
      0.36626712460023847,
      0.2168031958340022,
      0.3712696729067222,
      0.35545736557749175,
      0.2609653713645834,
      0.35129387775029497,
      0.35192734393712266,
      0.3118791616429346,
      0.281564379134174,
      0.2968984153786325,
      0.31266737174815473,
      0.30739926258566697,
      0.3106187414433279,
      0.29432337521820506,
      0.31537468733307117,
      0.0,
      0.35962785757943805,
      0.3229645520402187,
      0.30503204271997575
    ],
    [
      0.4375313069333997,
      0.5382703638916584,
      0.5436361645004124,
      0.5648203892416697,
      0.46564537256637517,
      0.5145045171701401,
      0.42803748087999205,
      0.47541198653849226,
      0.48017442562080137,
      0.5329967175247161,
      0.4711093505668753,
      0.2958337661934687,
      0.5662727883413998,
      0.4986532823613372,
      0.4454964954784728,
      0.4865680126648664,
      0.535933119775676,
      0.4565659873588319,
      0.485912954963551,
      0.43075057131640393,
      0.42853657707992476,
      0.3861870374303926,
      0.4876849127414715,
      0.41103136905218296,
      0.49639846375713415,
      0.42936160729152695,
      0.0,
      0.4542456074581491,
      0.41367590154916645
    ],
    [
      0.42370413955004094,
      0.5407507570856251,
      0.5288989736304579,
      0.5516283727308777,
      0.5154444366999829,
      0.5335031331922757,
      0.47356441991474973,
      0.5408680619550414,
      0.626789612804127,
      0.558337165884871,
      0.46943180894949377,
      0.29692298579768406,
      0.527307898440821,
      0.5924725506090935,
      0.5304090168389464,
      0.45428523707507984,
      0.49358383153813223,
      0.5524320751760246,
      0.49276911121061917,
      0.4565525752912676,
      0.4431739059335724,
      0.43362233641603765,
      0.45178671482721255,
      0.545364443607204,
      0.5233594185714721,
      0.44937018560060227,
      0.5323180032475938,
      0.0,
      0.4242669221587274
    ],
    [
      0.3342858690938979,
      0.3697944159001858,
      0.36633514529146716,
      0.37068973976539565,
      0.336593305765682,
      0.3511608117123517,
      0.32988659183688407,
      0.34561143690842555,
      0.3523392746976117,
      0.3939232452984667,
      0.3644658176725797,
      0.28155014484375984,
      0.3894730772931252,
      0.3642566340055504,
      0.3379058456361579,
      0.3543258792172992,
      0.3975973873041052,
      0.3397638924186934,
      0.3457034273716624,
      0.3470045983094152,
      0.367536396052458,
      0.36168382279040845,
      0.367512452269674,
      0.3496123346418232,
      0.36754005707875104,
      0.3743953758671166,
      0.4029357626276773,
      0.34741330876064436,
      0.0
    ]
  ],
  "row_avgs": [
    0.24383598525376818,
    0.44803047074681507,
    0.44449781808488187,
    0.46987002311185627,
    0.46467126521742447,
    0.42814698198731055,
    0.336916265166869,
    0.405387830789848,
    0.42459840155168393,
    0.376049725134988,
    0.4328873888065866,
    0.25151712296530515,
    0.41044486103291183,
    0.5111346992700015,
    0.4067399349988444,
    0.3886584913095181,
    0.4239836512770485,
    0.4426150772158805,
    0.3830699380650088,
    0.2997906756094878,
    0.4269238208312757,
    0.3875406552005315,
    0.4418894302389752,
    0.35047447798865644,
    0.4270131568409787,
    0.32523799363798,
    0.4700445189374461,
    0.49867564624062977,
    0.35754628751540246
  ],
  "col_avgs": [
    0.365047646706135,
    0.46388785650515835,
    0.4517921279621625,
    0.44624235255483624,
    0.4116279760703087,
    0.42183768823523565,
    0.38145186841753537,
    0.42482678947228647,
    0.4281107198693036,
    0.43947604947396507,
    0.3987518915873238,
    0.26098341398857217,
    0.46094408019718663,
    0.45797466256643116,
    0.39131833157066087,
    0.41094066713209043,
    0.42239967192997907,
    0.39071023064459204,
    0.4039701300081316,
    0.36512397918137157,
    0.3692414619540581,
    0.3420912343527189,
    0.39435337002731613,
    0.37363634832406245,
    0.4184466769759367,
    0.38580119640299904,
    0.4375581781841141,
    0.42511611786656267,
    0.33452987686687974
  ],
  "combined_avgs": [
    0.30444181597995157,
    0.4559591636259867,
    0.4481449730235222,
    0.45805618783334623,
    0.4381496206438666,
    0.4249923351112731,
    0.35918406679220216,
    0.4151073101310673,
    0.42635456071049377,
    0.4077628873044765,
    0.4158196401969552,
    0.25625026847693866,
    0.43569447061504923,
    0.48455468091821635,
    0.39902913328475265,
    0.3997995792208042,
    0.4231916616035138,
    0.41666265393023627,
    0.3935200340365702,
    0.33245732739542966,
    0.3980826413926669,
    0.36481594477662516,
    0.4181214001331457,
    0.3620554131563595,
    0.4227299169084577,
    0.3555195950204895,
    0.4538013485607801,
    0.4618958820535962,
    0.34603808219114107
  ],
  "gppm": [
    608.2120612697307,
    589.0411313323096,
    596.6657089614353,
    601.3160957717747,
    617.1019717210099,
    610.2339456101643,
    630.2100220821646,
    609.6656802262332,
    609.5428004697117,
    601.8212369216137,
    623.0047770435297,
    686.1388994644005,
    594.8155973519391,
    594.5450877041546,
    625.9985398090879,
    617.2701022107531,
    608.9432257751847,
    626.9129585179911,
    619.5979199280201,
    636.3443909918168,
    635.4092174495789,
    647.1891294193366,
    623.6419017008737,
    635.1922714175696,
    614.2179555805872,
    624.0816659101871,
    601.474178958035,
    611.6444599978901,
    650.8863713386257
  ],
  "gppm_normalized": [
    1.40282433370825,
    1.3309567758938965,
    1.3478841904051373,
    1.3536267504944692,
    1.389430063522122,
    1.3788967802572043,
    1.4279269797101188,
    1.3728354341329827,
    1.371072373660887,
    1.3572328263973932,
    1.4063979907361293,
    1.553803831861514,
    1.3408600422361394,
    1.3377337469990334,
    1.4115595881230187,
    1.3946070015463963,
    1.3711836673407434,
    1.4086107108016377,
    1.3949107961433478,
    1.4416817249064402,
    1.4256635930632746,
    1.4592295622264224,
    1.4000934189068743,
    1.4327263621871578,
    1.3816424378731775,
    1.4071006802836503,
    1.3550846096763591,
    1.3747150689093623,
    1.4689892492477519
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438
  ],
  "response_lengths": [
    2672,
    2627,
    2635,
    2434,
    2274,
    2584,
    2793,
    2319,
    2404,
    2458,
    2530,
    2719,
    2504,
    2360,
    2371,
    2616,
    2371,
    2164,
    2272,
    2696,
    2100,
    2489,
    2197,
    2458,
    2255,
    2388,
    2530,
    2225,
    2448
  ]
}