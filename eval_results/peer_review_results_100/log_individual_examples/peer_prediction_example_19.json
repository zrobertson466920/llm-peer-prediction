{
  "example_idx": 19,
  "reference": "Under review as a conference paper at ICLR 2023\n\nMULTI-AGENT SEQUENTIAL DECISION-MAKING VIA COMMUNICATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nCommunication helps agents to obtain information about others so that better coordinated behavior can be learned. Some existing work communicates predicted future trajectory with others, hoping to get clues about what others would do for better coordination. However, circular dependencies sometimes can occur when agents are treated synchronously so it is hard to coordinate decision-making. In this paper, we propose a novel communication scheme, Sequential Communication (SeqComm). SeqComm treats agents asynchronously (the upper-level agents make decisions before the lower-level ones) and has two communication phases. In negotiation phase, agents determine the priority of decision-making by communicating hidden states of observations and comparing the value of intention, which is obtained by modeling the environment dynamics. In launching phase, the upper-level agents take the lead in making decisions and communicate their actions with the lower-level agents. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we show that SeqComm outperforms existing methods in various multi-agent cooperative tasks.\n\n1\n\nINTRODUCTION\n\nThe partial observability and stochasticity inherent to the nature of multi-agent systems can easily impede the cooperation among agents and lead to catastrophic miscoordination (Ding et al., 2020). Communication has been exploited to help agents obtain extra information during both training and execution to mitigate such problems (Foerster et al., 2016; Sukhbaatar et al., 2016; Peng et al., 2017). Specifically, agents can share their information with others via a trainable communication channel.\n\nCentralized training with decentralized execution (CTDE) is a popular learning paradigm in cooperative multi-agent reinforcement learning (MARL). Although the centralized value function can be learned to evaluate the joint policy of agents, the decentralized policies of agents are essentially independent. Therefore, a coordination problem arises. That is, agents may make sub-optimal actions by mistakenly assuming others’ actions when there exist multiple optimal joint actions (Busoniu et al., 2008). Communication allows agents to obtain information about others to avoid miscoordination. However, most existing work only focuses on communicating messages, e.g., the information of agents’ current observation or historical trajectory (Jiang & Lu, 2018; Singh et al., 2019; Das et al., 2019; Ding et al., 2020). It is impossible for an agent to acquire other’s actions before making decisions since the game model is usually synchronous, i.e., agents make decisions and execute actions simultaneously. Recently, intention or imagination, depicted by a combination of predicted actions and observations of many future steps, has been proposed as part of messages (Kim et al., 2021; Pretorius et al., 2021). However, circular dependencies can still occur, so it may be hard to coordinate decision-making under synchronous settings.\n\nA general approach to solving the coordination problem is to make sure that ties between equally good actions are broken by all agents. One simple mechanism for doing so is to know exactly what others will do and adjust the behavior accordingly under a unique ordering of agents and actions (Busoniu et al., 2008). Inspired by this, we reconsider the cooperative game from an asynchronous perspective. In other words, each agent is assigned a priority (i.e., order) of decision-making each step in both training and execution, thus the Stackelberg equilibrium (SE) (Von Stackelberg, 2010) is naturally set up as the learning objective. Specifically, the upper-level agents make decisions before the lower-level agents. Therefore, the lower-level agents can acquire the actual actions of the upper-level agents by\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ncommunication and make their decisions conditioned on what the upper-level agents would do. Under this setting, the SE is likely to be Pareto superior to the average Nash equilibrium (NE) in games that require a high cooperation level (Zhang et al., 2020). However, is it necessary to decide a specific priority of decision-making for each agent? Ideally, the optimal joint policy can be decomposed by any orders (Wen et al., 2019), e.g., π∗(a1, a2|s) = π∗(a1|s)π∗(a2|s, a1) = π∗(a2|s)π∗(a1|s, a2). But during the learning process, it is unlikely for agents to use the optimal actions of other agents for gradient calculation, making it still vulnerable to the relative overgeneralization problem (Wei et al., 2018). Overall, there is no guarantee that the above equation will hold in the learning process, thus ordering should be carefully concerned.\n\nIn this paper, we propose a novel model-based multi-round communication scheme for cooperative MARL, Sequential Communication (SeqComm), to enable agents to explicitly coordinate with each other. Specifically, SeqComm has two-phase communication, negotiation phase and launching phase. In the negotiation phase, agents communicate their hidden states of observations with others simultaneously. Then they are able to generate multiple predicted trajectories, called intention, by modeling the environmental dynamics and other agents’ actions. In addition, the priority of decision-making is determined by communicating and comparing the corresponding values of agents’ intentions. The value of each intention represents the rewards obtained by letting that agent take the upper-level position of the order sequence. The sequence of others follows the same procedure as aforementioned with the upper-level agents fixed. In the launching phase, the upper-level agents take the lead in decision-making and communicate their actual actions with the lower-level agents. Note that the actual actions will be executed simultaneously in the environment without any changes.\n\nSeqComm is currently built on MAPPO (Yu et al., 2021). Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we evaluate SeqComm on a set of tasks in multi-agent particle environment (MPE) (Lowe et al., 2017) and StarCraft multi-agent challenge (SMAC) (Samvelyan et al., 2019). In all these tasks, we demonstrate that SeqComm outperforms prior communication-free and communication-based methods. By ablation studies, we confirm that treating agents asynchronously is a more effective way to promote coordination and SeqComm can provide the proper priority of decision-making for agents to develop better coordination.\n\n2 RELATED WORK\n\nCommunication. Existing studies (Jiang & Lu, 2018; Kim et al., 2019; Singh et al., 2019; Das et al., 2019; Zhang et al., 2019; Jiang et al., 2020; Ding et al., 2020; Konan et al., 2022) in this realm mainly focus on how to extract valuable messages. ATOC (Jiang & Lu, 2018) and IC3Net (Singh et al., 2019) utilize gate mechanisms to decide when to communicate with other agents. Many works (Das et al., 2019; Konan et al., 2022) employ multi-round communication to fully reason the intentions of others and establish complex collaboration strategies. Social influence (Jaques et al., 2019) uses communication to influence the behaviors of others. I2C (Ding et al., 2020) only communicates with agents that are relevant and influential which are determined by causal inference. However, all these methods focus on how to exploit valuable information from current or past partial observations effectively and properly. More recently, some studies (Kim et al., 2021; Du et al., 2021; Pretorius et al., 2021) begin to answer the question: can we favor cooperation beyond sharing partial observation? They allow agents to imagine their future states with a world model and communicate those with others. IS (Pretorius et al., 2021), as the representation of this line of research, enables each agent to share its intention with other agents in the form of the encoded imagined trajectory and use the attention module to figure out the importance of the received intention. However, two concerns arise. On one hand, circular dependencies can lead to inaccurate predicted future trajectories as long as the multi-agent system treats agents synchronously. On the other hand, MARL struggles in extracting useful information from numerous messages, not to mention more complex and dubious messages, i.e., predicted future trajectories.\n\nUnlike these works, we treat the agents from an asynchronously perspective therefore circular dependencies can be naturally resolved. Furthermore, agents only send actions to lower-level agents besides partial observations to make sure the messages are compact as well as informative.\n\nCoordination. The agents are essentially independent decision makers in execution and may break ties between equally good actions randomly. Thus, in the absence of additional mechanisms, different\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nagents may break ties in different ways, and the resulting joint actions may be suboptimal. Coordination graphs (Guestrin et al., 2002; B ̈ohmer et al., 2020; Wang et al., 2021b) simplify the coordination when the global Q-function can be additively decomposed into local Q-functions that only depend on the actions of a subset of agents. Typically, a coordination graph expresses a higher-order value decomposition among agents. This improves the representational capacity to distinguish other agents’ effects on local utility functions, which addresses the miscoordination problems caused by partial observability. Another general approach to solving the coordination problem is to make sure that ties are broken by all agents in the same way, requiring that random action choices are somehow coordinated or negotiated. Social conventions (Boutilier, 1996) or role assignments (Prasad et al., 1998) encode prior preferences towards certain joint actions and help break ties during action selection. Communication (Fischer et al., 2004; Vlassis, 2007) can be used to negotiate action choices, either alone or in combination with the aforementioned techniques. Our method follows this line of research by utilizing the ordering of agents and actions to break the ties, other than the enhanced representational capacity of the local value function.\n\n3 PROBLEM FORMULATION\n\nCost-Free Communication. The decentralized partially observable Markov decision process (DecPOMDP) can be extended to explicitly incorporate broadcasting observations. The resulting model is called multi-agent POMDP (Oliehoek et al., 2016).\n\nPynadath & Tambe (2002) showed that under cost-free communication, a joint communication policy that shares local observations at each stage is optimal. Many studies have also investigated sharing local observations in models that are similar to multi-agent POMDP (Pynadath & Tambe, 2002; Ooi & Wornell, 1996; Nair et al., 2004; Roth et al., 2005a;b; Spaan et al., 2006; Oliehoek et al., 2007; Becker et al., 2004). These works focus on issues other than communication cost and we foucs on the coordination problem. Note that even under multi-agent POMDP where agents can get joint observations, coordination problem can still arise (Busoniu et al., 2008). Suppose the centralized critic has learnt actions pairs [a1, a2] and [b1, b2] are equally optimal. Without any prior information, the individual policies π1 and π2 learnt from the centralized critic can break the ties randomly and may choose a1 and b2, respectively.\n\nMulti-Agent Sequential Decision-Making. We consider fully cooperative multi-agent tasks that are modeled as multi-agent POMDP, where n agents interact with the environment according to the following procedure, which we refer to as multi-agent sequential decision-making.\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt , a1\n\n, ak−1\n\n}, ok+1\n\n, a1:k−1\n\nt }, . . . , {ok−1\n\n(cid:44) {{o1 }, where o−k\n\nAt each timestep t, assume the priority (i.e., order) of decision-making for all agents is given and each priority level has only one agent (i.e., agents make decisions one by one). Note that the smaller the level index, the higher priority of decision-making is. The agent at each level k gets t drawn from the state st, and receives messages m−k its own observation ok from all other agents, where m−k can be written as {o−k denotes = ∅. the joint actions of agents 1 to k − 1. For the agent at the first level (i.e., k = 1), a1:k−1 Then, the agent determines its action ak t sampled from its policy πk(·|ok ) or equivalently πk(·|ot, a1:k−1 ) and sends it to the lower-level agents. After all agents have determined their actions, they perform the joint actions at, which can be seen as sampled from the joint policy π(·|st) factorized as (cid:81)n ), in the environment and get a shared reward r(st, at) and the state transitions to next state s(cid:48) according to the transition probability p(s(cid:48)|st, at). All agents aim to maximize the expected return (cid:80)∞ t=0 γtrt, where γ is the discount factor. The state-value function and action-value function of the level-k agent are defined as follows:\n\ndenotes the joint observations of all agents except k, and a1:k−1\n\nt }. Equivalently, m−k\n\nk=1 πk(·|ot, a1:k−1\n\nt , m−k\n\n, . . . , on\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nVπk (s, a1:k−1) (cid:44)\n\n∞ (cid:88) [\n\nt=0\n\nE s1:∞ 0 ∼πk:n a1:∞∼π\n\nak:n\n\nγtrt|s0 = s, a1:k−1\n\n0\n\n= a1:k−1]\n\nQπk (s, a1:k) (cid:44)\n\nE s1:∞\n\nak+1:n\n\n0\n\n∼πk+1:n\n\na1:∞∼π\n\n∞ (cid:88) [\n\nt=0\n\nγtrt|s0 = s, a1:k\n\n0 = a1:k].\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFor the setting of multi-agent sequential decision-making discussed above, we have the following proposition. Proposition 1. If all the agents update its policy with individual TRPO (Schulman et al., 2015) sequentially in multi-agent sequential decision-making, then the joint policy of all agents is guaranteed to improve monotonically and converge.\n\nProof. The proof is given in Appendix A.\n\nProposition 1 indicates that SeqComm has the performance guarantee regardless of the priority of decision-making in multi-agent sequential decision-making. However, the priority of decision-making indeed affects the optimality of the converged joint policy, and we have the following claim. Claim 1. The different priorities of decision-making affect the optimality of the convergence of the learning algorithm due to the relative overgeneralization problem.\n\nt\n\nA\n\n0 8\n\n8 0\n\nb3 6\n\nn e\ng A\n\n(a) payoff matrix of the game\n\nAgent B b2 6\n\nb1 a1 12 a2 −6 a3 −6\n\nWe use a one-step matrix game as an example, as illustrated in Figure 1(a), to demonstrate the influence of the priority of decision-making on the learning process. Due to relative overgeneralization (Wei et al., 2018), agent B tends to choose b2 or b3. Specifically, b2 or b3 in the suboptimal equilibrium is a better choice than b1 in the optimal equilibrium when matched with arbitrary actions from agent A. Therefore, as shown in Figure 1(b), B → A (i.e., agent B makes decisions before A, and A’s policy conditions on the action of B) and Simultaneous (i.e., two agents make decisions simultaneously and independently) are easily trapped into local optima. However, things can be different if agent A goes first, as A → B achieves the optimum. As long as agent A does not suffer from relative overgeneralization, it can help agent B get rid of local optima by narrowing down the search space of B. Besides, a policy that determines the priority of decision-making can be learned under the guidance of the state-value function, denoted as Learned. It obtains better performance than B → A and Simultaneous, which indicates that dynamically determining the order during policy learning can be beneficial as we do not know the optimal priority in advance.\n\nFigure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. A → B, B → A, Simultaneous, and Learned represent that agent A makes decisions first, agent B makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO (Yu et al., 2021) is used as the backbone.\n\n(b) evaluations of different methods\n\nRemark 1. The priority (i.e., order) of decision-making affects the optimality of the converged joint policy in multi-agent sequential decision-making, thus it is critical to determine the order. However, learning the order directly requires an additional centralized policy in execution, which is not generalizable in the scenario where the number of agents varies. Moreover, its learning complexity exponentially increases with the number of agents, making it infeasible in many cases.\n\n4 SEQUENTIAL COMMUNICATION\n\nIn this paper, we cast our eyes in another direction and resort to the world model. Ideally, we can randomly sample candidate order sequences, evaluate them under the world model (see Section 4.1), and choose the order sequence that is deemed the most promising under the true dynamic. SeqComm is designed based on this principle to determine the priority of decision-making via communication.\n\nSeqComm adopts a multi-round communication mechanism, i.e., agents are allowed to communicate with others in multiple rounds. Importantly, communication is separated into phases serving different purposes. One is the negotiation phase for agents to determine the priority of decision-making. Another is the launching phase for agents to act conditioning on actual actions upper-level agents will take to implement explicit coordination via communication. The overview of SeqComm is illustrated in Figure 2. Each SeqComm agent consists of a policy, a critic, and a world model, as illustrated in Figure 3, and the parameters of all networks are shared across agents (Gupta et al., 2017).\n\n4\n\n012345step1e6678910111213eval rewardBAABSimultaneousLearnedUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of SeqComm. SeqComm has two communication phases, the negotiation phase (left) and the launching phase (right). In the negotiation phase, agents communicate hidden states of observations with others and obtain their own intention. The priority of decision-making is determined by sharing and comparing the value of all the intentions. In the launching phase, the agents who hold the upper-level positions will make decisions prior to the lower-level agents. Besides, their actions will be shared with anyone that has not yet made decisions.\n\n4.1 NEGOTIATION PHASE\n\nIn the negotiation phase, the observation encoder first takes ot as input and outputs a hidden state ht, which is used to communicate with others. Agents then determine the priority of decision-making by intention which is established and evaluated based on the world model.\n\nWorld Model. The world model is needed to predict and evaluate future trajectories. SeqComm, unlike previous works (Kim et al., 2021; Du et al., 2021; Pretorius et al., 2021), can utilize received hidden states of other agents in the first round of communication to model more precise environment dynamics for the explicit coordination in the next round of communication. Once an agent can access other agents’ hidden states, it shall have adequate information to estimate their actions since all agents are homogeneous and parameter-sharing. Therefore, the world model M(·) takes as input the joint hidden states ht = {h1 t } and actions at, and predicts the next joint observations and reward,\n\nt , . . . , hn\n\nˆot+1, ˆrt+1 = Mi(AMw(ht, at)),\n\nwhere AMw is the attention module. The reason that we adopt the attention module is to entitle the world model to be generalizable in the scenarios where additional agents are introduced or existing agents are removed.\n\nPriority of Decision-Making. The intention is the key element to determine the priority of decision-making. The notion of intention is described as an agent’s future behavior in previous works (Rabinowitz et al., 2018; Raileanu et al., 2018; Kim et al., 2021). However, we define the intention as an agent’s future behavior without considering others.\n\nAs mentioned before, an agent’s intention considering others can lead to circular dependencies and cause miscoordination. By our definition, the intention of an agent should be depicted as all future trajectories considering that agent as the first-mover and ignoring the others. However, there are many possible future trajectories as the priority of the rest agents is unfixed. In practice, we use the Monte Carlo method to evaluate intention.\n\nTaking agent i at timestep t to illustrate, it firstly considers itself as the first-mover and produces its action only based on the joint hidden states, ˆai t ∼ πi(·|AMa(ht)), where we again use an\n\nFigure 3: Architecture of SeqComm. The critic and policy of each agent take input as its own observation and received messages. The world model takes as input the joint hidden states and predicted joint actions.\n\n5\n\nagent 1agent 1’s obsagent 2agent 3agent 4requestreplyagent 1agent 1’s obsagent 2agent 3agent 4replyrequestrequestrequestreplyreplyAgent 1 chooses to send request to agent 2 and ignore agent 312341234Agent 1 chooses to send request to agent 2, 3, 4tt+1BCBCorder setAsACsCBsBBC1BC2r1r2intention rewardACoCBoBBC1BC2r1r2intention value (A)BCoCAoAAC1AC2r1r2intention value (B)CAoABoBBA1BA2r1r2intention value (C)compareCAintention value (A)ABintention value (B)compareBintention value (B)compare12AC3ACB4negotiation phaselaunching phaseACB2ACB1aCACB2aAACB3aBaAaCACB1aCACB2aAACB3aBaAaCACBactionstateagentactionstateagentt+1t+2t+3BAactionsstateagentsample sequenceAMaPolicyo3ta3tauppert={a1t,a2t}Cri/cAMaPolicyo2ta2tauppert={a1t}Cri/cAMaPolicyo1ta1tauppert={}Cri/ĉrt+1̂ot+1={̂o1t+1,...,̂ont+1}at={a1t,...,ant}ht={h1t,...,hnt}First moverSecond moverThird moverWorld ModelAMwht={h1t,...,hnt}⋯Under review as a conference paper at ICLR 2023\n\nattention module AMa to handle the input. For the order sequence of lower-level agents, we randomly sample a set of order sequences from unfixed agents. Assume agent j is the second-mover, agent i models j’s action by considering the upper-level action following its own policy ˆaj t ∼ πi(·|AMa(ht, ˆai t)). The same procedure is applied to predict the actions of all other agents following the sampled order sequence. Based on the joint hidden states and predicted actions, the next joint observations ˆot+1 and corresponding reward ˆrt+1 can be predicted by the world model. The length of the predicted future trajectory is H and it can then be written as τ t = { ˆot+1, ˆat+1, . . . , ˆot+H , ˆat+H } by repeating the procedure aforementioned and the value of one trajectory is defined as the return of that trajectory vτ t = (cid:80)t+H t(cid:48)=t+1 γt(cid:48)−t−1ˆrt(cid:48)/H. In addition, the intention value is defined as the average value of F future trajectories with different sampled order sequences. The choice of F is a tradeoff between the computation overhead and the accuracy of the estimation.\n\nAfter all the agents have computed their own intention and the corresponding value, they again communicate their intention values to others. Then agents would compare and choose the agent with the highest intention value to be the first-mover. The priority of lower-level decision-making follows the same procedure with the upper-level agents fixed. Note that some agents are required to communicate intention values with others multiple times until the priority of decision-making is finally determined.\n\n4.2 LAUNCHING PHASE\n\nAs for the launching phase, agents communicate for obtaining additional information to make decisions. Apart from the received hidden states from the last phase, we allow agents to get what actual actions the upper-level agents will take in execution, while other studies can only infer others’ actions by opponent modeling (Rabinowitz et al., 2018; Raileanu et al., 2018) or communicating intentions (Kim et al., 2021). Therefore, miscoordination can be naturally avoided and a better cooperation strategy is possible since lower-level agents can adjust their behaviors accordingly. )), where aupper A lower-level agent i make a decision following the policy πi(·|AMa(ht, aupper means received actual actions from all upper-level agents. As long as the agent has decided its action, it will send its action to all other lower-level agents by the communication channel. Note that the actions are executed simultaneously and distributedly in execution, though agents make decisions sequentially.\n\nt\n\nt\n\nCommunication Overhead. Two communication phases alternate until all agents determine their levels and get upper-level actions. Note that many previous works also adopt the multi-round communication scheme (Das et al., 2019; Singh et al., 2019). As for implementation in practice, compared with communicating high-dimensional hidden states/observations by multiple rounds (Das et al., 2019; Singh et al., 2019), or transferring multi-step trajectory (Kim et al., 2021), SeqComm needs more rounds, but it only transmits hidden states for one time. For the rest n − 1 round communication with total (n − 1)/2 broadcasts per agent, only a single intention value and an action will be exchanged. Considering there are n! permutations of different order choices for n agents, our method has greatly reduced computation overhead since each agent needs to calculate up to n times to search for a satisfying order. Although SeqComm is more suitable for latency-tolerate MARL tasks, e.g., power dispatch (minutes) (Wang et al., 2021a), inventory management (hours) (Feng et al., 2021), maritime transportation (days) (Li et al., 2019), it is possible for SeqComm to have a wider range of applications given the rapid development of the communication technology, e.g., 5G.\n\n4.3 THEORETICAL ANALYSIS\n\nAs the priority of decision-making is determined by intention values, SeqComm is likely to choose different orders at different timesteps during training. However, we have the following proposition that theoretically guarantees the performance of the learned joint policy under SeqComm.\n\nProposition 2. The monotonic improvement and convergence of the joint policy in SeqComm are independent of the priority of decision-making of agents at each timestep.\n\nProof. The proof is given in Appendix A.\n\nThe priority of decision-making is chosen under the world model, thus the compounding errors in the world model can result in discrepancies between the predicted returns of the same order under the\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) PP\n\n(b) CN\n\n(c) KA\n\nFigure 4: Learning curves in terms of the mean reward averaged over timesteps of SeqComm and baselines on three MPE tasks: (a) predator-prey, (b) cooperative navigation, and (c) keep-away.\n\nworld model and the true dynamics. We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. (2019).\n\nTheorem 1. Let the expected total variation between two transition distributions be bounded at each timestep as maxt Es∼πβ,t[DT V (p(s(cid:48)|s, a)||ˆp(s(cid:48)|s, a))] ≤ (cid:15)m, and the policy divergences at level k be bounded as maxs,a1:k−1 DT V (πβ,k(ak|s, a1:k−1)||πk(ak|s, a1:k−1)) ≤ (cid:15)πk , where πβ is the data collecting policy for the model and ˆp(s(cid:48)|s, a) is the transition distribution under the model. Then the model return ˆη and true return η of the policy π are bounded as:\n\nˆη[π] ≥ η[π] − [\n\n2γrmax((cid:15)m + 2 (cid:80)n\n\nk=1 (cid:15)πk )\n\n(1 − γ)2\n\n+\n\n4rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)\n\n(cid:124)\n\n(cid:123)(cid:122) C((cid:15)m,(cid:15)π1:n )\n\n]\n\n(cid:125)\n\nProof. The proof is given in Appendix B.\n\nRemark 2. Theorem 1 provides a useful relationship between the compounding errors and the policy update. As long as we improve the return under the true dynamic by more than the gap, C((cid:15)m, (cid:15)π1:n ), we can guarantee the policy improvement under the world model. If no such policy exists to overcome the gap, it implies the model error is too high, that is, there is a large discrepancy between the world model and true dynamics. Thus the order sequence obtained under the world model is not reliable. Such an order sequence is almost the same as a random one. Though a random order sequence also has the theoretical guarantee of Proposition 2, we will show in Section 5.2 that a random order sequence leads to a poor local optimum empirically.\n\n5 EXPERIMENTS\n\nSequential communication (SeqComm) is currently instantiated based on MAPPO (Yu et al., 2021). We evaluate SeqComm on three tasks in multi-agent particle environment (MPE) (Lowe et al., 2017) and four maps in StarCraft multi-agent challenge (SMAC) (Samvelyan et al., 2019).\n\nFor these experiments, we compare SeqComm against the following communication-free and communication-based baselines: MAPPO (Yu et al., 2021), QMIX (Rashid et al., 2018), IS (Kim et al., 2021), TarMAC (Das et al., 2019), and I2C (Ding et al., 2020). In more detail, IS communicates predicted future trajectories (observations and actions), and predictions are made by the environment model. TarMAC uses the attention model to focus more on important incoming messages (the hidden states of observations). TarMAC is reproduced based on MAPPO instead of A2C in the original paper for better performance. I2C infers one-to-one communication to reduce the redundancy of messages (also conditioning on observations).\n\nIn the experiments, all the methods are parameter-sharing for fast convergence. We have fine-tuned the baselines for a fair comparison. Please refer to Appendix E for experimental settings and Appendix F for implementation details. All results are presented in terms of the mean and standard deviation of five runs with different random seeds.\n\n5.1 RESULTS\n\nMPE. We experiment on predator-prey (PP), cooperative navigation (CN), and keep-away (KA) in MPE. In PP, five predators (agents) try to capture three prey. In CN, five agents try to occupy five landmarks. In KA, three attackers (agents) try to occupy three landmarks, however, there are three\n\n7\n\n0.00.20.40.60.81.0step1e72.01.81.61.41.21.00.8average rewardSeqCommTarMACmappoISI2C0.00.20.40.60.81.0step1e72.22.01.81.61.41.2average rewardSeqCommTarMACmappoISI2C0.00.20.40.60.81.0step1e71.41.31.21.11.0average rewardSeqCommTarMACmappoISI2CUnder review as a conference paper at ICLR 2023\n\n(a) 6h vs 8z\n\n(b) MMM2\n\n(c) 10m vs 11m\n\n(d) 8m vs 9m\n\nFigure 5: Learning curves in terms of the win rate of SeqComm and baselines on four customized SMAC maps: (a) 6h vs 8z, (b) MMM2, (c) 10m vs 11m, and (d) 8m vs 9m.\n\n(a) PP\n\n(b) CN\n\n(c) KA\n\n(d) 6h vs 8z\n\n(e) MMM2\n\n(f) 10m vs 11m\n\n(g) 8m vs 9m\n\nFigure 6: Ablation studies on the priority of decision-making in all the tasks. Fix-C: the priority of decision-making is fixed throughout one episode. Random-C: the priority of decision-making is determined randomly. TarMAC is also compared as a reference without explicit action coordination.\n\ndefenders to push them away. In all three tasks, the size of agents is set to be larger than the original settings so that collisions occur more easily, following the settings in (Kim et al., 2021). In addition, agents cannot observe any other agents, and this makes the task more difficult and communication more important. We can observe similar modifications in previous works (Foerster et al., 2016; Ding et al., 2020). After all, we want to demonstrate the superior over communication-based baselines, and communication-based methods are more suitable for scenarios with limited vision. More details about experimental settings are available in Appendix E.\n\nFigure 4 shows the learning curves of all the methods in terms of the mean reward averaged over timesteps in PP, CN, and KA. We can see that SeqComm converges to the highest mean reward compared with all the baselines. The results demonstrate the superiority of SeqComm. In more detail, all communication-based methods outperform MAPPO, indicating the necessity of communication in these difficult tasks. Apart from MAPPO, IS performs the worst since it may access inaccurate predicted information due to the circular dependencies. The substantial improvement SeqComm over I2C and TarMAC is attributed to that SeqComm allows agents to get more valuable action information for explicit coordination. The agents learned by SeqComm show sophisticated coordination strategies induced by the priority of decision-making, which can be witnessed by the visualization of agent behaviors. More details are given in Appendix C. Note that QMIX is omitted in the comparison for clear presentation since Yu et al. (2021) have shown QMIX and MAPPO exhibit similar performance in various MPE tasks.\n\nSMAC. We also evaluate SeqComm against the baselines on four customized maps in SMAC: 6h vs 8z, MMM2, 10m vs 11m, and 8m vs 9m, where we have made some minor changes to the observation part of agents to make it more difficult. Specifically, the sight range of agents is reduced from 9 to 2, and agents cannot perceive any information about their allies even if they are within the sight range. NDQ (Wang et al., 2020) adopts a similar change to increase the difficulty of action coordination and demonstrates that the miscoordination problem is widespread in multi-agent learning. The rest settings remain the same as the default.\n\n8\n\n012345step1e60.00.20.40.60.81.0win rateSeqCommTarMACmappoqmix01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACmappoqmix0.00.20.40.60.81.0step1e70.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACmappoqmix01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACmappoqmix0.00.20.40.60.81.0step1e71.61.51.41.31.21.11.00.90.8average rewardSeqCommFix-CRandom-CTarMAC0.00.20.40.60.81.0step1e72.22.01.81.61.41.2average rewardSeqCommTarMACFix-CRandom-C0.00.20.40.60.81.0step1e71.101.081.061.041.021.000.980.960.94average rewardSeqCommTarMACFix-CRandom-C012345step1e60.00.20.40.60.81.0win rateSeqCommTarMACFix-CRandom-C01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACFix-CRandom-C0.00.20.40.60.81.0step1e70.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACFix-CRandom-C01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACFix-CRandom-CUnder review as a conference paper at ICLR 2023\n\nThe learning curves of SeqComm and the baselines in terms of the win rate are illustrated in Figure 5. IS and I2C fail in this task and get a zero win rate because these two methods are built on MADDPG. However, MADDPG cannot work well in SMAC, especially when we reduce the sight range of agents, which is also supported by other studies (Papoudakis et al., 2021). SeqComm and TarMAC converge to better performances than MAPPO and QMIX, which demonstrates the benefit of communication. Moreover, SeqComm outperforms TarMAC, which again verifies the gain of explicit action coordination.\n\n5.2 ABLATION STUDIES\n\nPriority of Decision-Making. We compare SeqComm with two ablation baselines with only a difference in the priority of decision-making: the priority of decision-making is fixed throughout one episode, denoted as Fix-C, and the priority of decision-making is determined randomly at each timestep, denoted as Random-C. TarMAC is also compared as a reference without explicit action coordination.\n\nAs depicted in Figure 6, SeqComm achieves a higher mean reward or win rate than Fix-C, Random-C, and TarMAC in all the tasks. These results verify the importance of the priority of decision-making and the necessity to continuously adjust it during one episode. It is also demonstrated that SeqComm can provide a proper priority of decision-making. As discussed in Section 4.3, although Fix-C and Random-C also have the theoretical guarantee, they converge to poor local optima in practice. Moreover, Fix-C and Random-C show better performance than TarMAC in most tasks. This result accords with the hypothesis that the SE is likely to be Pareto superior to the average NE in games with a high cooperation level. Additionally, the learned policy of SeqComm can generalize well to the same task with a different number of agents in MPE, which is detailed in Appendix C.\n\nCommunication Range. We also carry out ablation studies on communication range in MPE tasks. Note that communication range means how many nearest neighbors each agent is allowed to communicate with, following the setting in Ding et al. (2020). We reduce the communication range of SeqComm from 4 to 2 and 0. As there are only three agents in KA, it is omitted in this study. The results are shown in Figure 7. Communication-based agents perform better than communication-free agents, which accords with the results of many previous studies. More importantly, the superiority of SeqComm with communication range 2 over the corresponding TarMAC again demonstrates the effectiveness of sequential communication even in reduced communication ranges.\n\nHowever, as the communication range decreases from 4 to 2, there is no performance reduction in these two MPE tasks. On the contrary, the agents with communication range 2 perform the best. It accords with the results in I2C (Ding et al., 2020) and ATOC (Jiang & Lu, 2018) that redundant information can impair the learning process sometimes. In other settings, this conclusion might not be true. Moreover, since under our communication scheme agents can obtain more information, i.e., the actual actions of others, it is more reasonable that SeqComm can still outperform other methods in reduced communication ranges.\n\nFigure 7: Ablation studies on reduced communication range in (a) predator-prey and (b) cooperative navigation.\n\n(b) CN\n\n(a) PP\n\n6 CONCLUSIONS\n\nWe have proposed SeqComm, which enables agents explicitly coordinate with each other. SeqComm from an asynchronous perspective allows agents to make decisions sequentially. A two-phase communication scheme has been adopted for determining the priority of decision-making and communicating messages accordingly. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, it is demonstrated that SeqComm outperforms baselines in a variety of cooperative multi-agent tasks and SeqComm can provide a proper priority of decision-making.\n\n9\n\n0.00.20.40.60.81.0step1e72.22.01.81.61.41.21.00.8average rewardcomm_range_4comm_range_2comm_range_0TarMAC_comm_range_20.00.20.40.60.81.0step1e72.22.01.81.61.41.21.00.8average rewardcomm_range_4comm_range_2comm_range_0TarMAC_comm_range_2Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRaphen Becker, Shlomo Zilberstein, Victor R. Lesser, and Claudia V. Goldman. Solving transition independent decentralized markov decision processes. J. Artif. Intell. Res., 22:423–455, 2004.\n\nWendelin B ̈ohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International\n\nConference on Machine Learning (ICML), 2020.\n\nCraig Boutilier. Planning, learning and coordination in multiagent decision processes. In Conference\n\non Theoretical Aspects of Rationality and Knowledge, 1996.\n\nLucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156–172, 2008.\n\nAbhishek Das, Th ́eophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning (ICML), 2019.\n\nZiluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent cooperation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nYali Du, Yifan Zhao, Meng Fang, Jun Wang, Gangyan Xu, and Haifeng Zhang. Learning predictive\n\ncommunication by imagination in networked system control, 2021.\n\nMingxiao Feng, Guozi Liu, Li Zhao, Lei Song, Jiang Bian, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. Multi-agent reinforcement learning with shared resource in inventory management. 2021.\n\nFelix Fischer, Michael Rovatsos, and Gerhard Weiss. Hierarchical reinforcement learning in In International Joint Conference on Au-\n\ncommunication-mediated multiagent coordination. tonomous Agents and Multiagent Systems (AAMAS), 2004.\n\nJakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nAmy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In A comprehensive survey\n\nof multiagent reinforcement learning, 2003.\n\nCarlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In\n\nInternational Conference on Machine Learning (ICML), 2002.\n\nJayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2017.\n\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning (ICML), 2019.\n\nJiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.\n\nAdvances in Neural Information Processing Systems (NeurIPS), 2018.\n\nJiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement\n\nlearning. In International Conference on Learning Representation (ICLR), 2020.\n\nDaewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWoojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning: Intention sharing. In International Conference on Learning Representations (ICLR), 2021.\n\nSachin Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated reasoning with mutual information in cooperative and byzantine decentralized teaming. In International Conference on Learning Representations (ICLR), 2022.\n\nVille K ̈on ̈onen. Asymmetric multiagent reinforcement learning. Web Intelligence and Agent Systems:\n\nAn international journal, 2(2):105–121, 2004.\n\nXihan Li, Jia Zhang, Jiang Bian, Yunhai Tong, and Tie-Yan Liu. A cooperative multi-agent reinforcement learning framework for resource balancing in complex logistics network. arXiv preprint arXiv:1903.00714, 2019.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n\nHang Ma, Daniel Harabor, Peter J Stuckey, Jiaoyang Li, and Sven Koenig. Searching with consistent prioritization for multi-agent path finding. In AAAI Conference on Artificial Intelligence (AAAI), 2019.\n\nRanjit Nair, Milind Tambe, Maayan Roth, and Makoto Yokoo. Communication for improving policy computation in distributed pomdps. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2004.\n\nFrans A. Oliehoek, Matthijs T. J. Spaan, and Nikos A. Vlassis. Dec-pomdps with delayed communication. In AAMAS Workshop on Multi-agent Sequential Decision Making in Uncertain Domains, 2007.\n\nFrans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,\n\nvolume 1. Springer, 2016.\n\nJ.M. Ooi and G.W. Wornell. Decentralized control of a multiple access broadcast channel: perfor-\n\nmance bounds. In IEEE Conference on Decision and Control, 1996.\n\nGeorgios Papoudakis, Filippos Christianos, Lukas Sch ̈afer, and Stefano V. Albrecht. Benchmarking\n\nmulti-agent deep reinforcement learning algorithms in cooperative tasks, 2021.\n\nPeng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.\n\nMV Nagendra Prasad, Victor R Lesser, and Susan E Lander. Learning organizational roles for negotiated search in a multiagent system. International Journal of Human-Computer Studies, 48 (1):51–67, 1998.\n\nArnu Pretorius, Scott Cameron, Andries Petrus Smit, Elan van Biljon, Lawrence Francis, Femi Azeez, Alexandre Laterre, and Karim Beguir. Learning to communicate through imagination with model-based deep multi-agent reinforcement learning, 2021.\n\nDavid V. Pynadath and Milind Tambe. The communicative multiagent team decision problem:\n\nAnalyzing teamwork theories and models. J. Artif. Intell. Res., 16:389–423, 2002.\n\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick.\n\nMachine theory of mind. In International Conference on Machine Learning (ICML), 2018.\n\nRoberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMaayan Roth, Reid Simmons, and Manuela Veloso. Decentralized communication strategies for coordinated multi-agent policies. In Lynne E. Parker, Frank E. Schneider, and Alan C. Schultz (eds.), Multi-Robot Systems. From Swarms to Intelligent Automata Volume III. Springer, 2005a.\n\nMaayan Roth, Reid G. Simmons, and Manuela M. Veloso. Reasoning about joint beliefs for executiontime communication decisions. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2005b.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. arXiv preprint arXiv:1902.04043, 2019.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\n\npolicy optimization. In International Conference on Machine Learning (ICML), 2015.\n\nAmanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Individualized controlled continuous communication model for multiagent cooperative and competitive tasks. In International Conference on Learning Representations (ICLR), 2019.\n\nEric Sodomka, Elizabeth Hilliard, Michael Littman, and Amy Greenwald. Coco-q: Learning in stochastic games with side payments. In International Conference on Machine Learning (ICML), 2013.\n\nMatthijs T. J. Spaan, Geoffrey J. Gordon, and Nikos Vlassis. Decentralized planning under uncertainty for teams of communicating agents. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2006.\n\nSainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation.\n\nIn Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nJur P Van Den Berg and Mark H Overmars. Prioritized motion planning for multiple robots. In\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2005.\n\nNikos Vlassis. A concise introduction to multiagent systems and distributed artificial intelligence.\n\nSynthesis Lectures on Artificial Intelligence and Machine Learning, 1(1):1–71, 2007.\n\nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media,\n\n2010.\n\nJianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent reinforcement learning for active voltage control on power distribution networks. Advances in Neural Information Processing Systems (NeurIPS), 2021a.\n\nTonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable In International Conference on Learning\n\nvalue functions via communication minimization. Representation (ICLR), 2020.\n\nTonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, and Chongjie Zhang. Context-\n\naware sparse deep coordination graphs. arXiv preprint arXiv:2106.02886, 2021b.\n\nErmo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In AAAI Spring\n\nSymposium Series, 2018.\n\nYing Wen, Yaodong Yang, Rui Luo, Jun Wang, and W Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nHaifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\nSai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement In Advances in Neural Information Processing Systems\n\nlearning via variance based control. (NeurIPS), 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS OF PROPOSITION 1 AND PROPOSITION 2\n\nLemma 1 (Agent-by-Agent PPO). If we update the policy of each agent i with TRPO Schulman et al. (2015) (or approximately PPO) when fixing all the other agent’s policies, then the joint policy will improve monotonically.\n\nProof. We consider the joint surrogate objective in TRPO Lπold(πnew) where πold is the joint policy before updating and πnew is the joint policy after updating.\n\nGiven that π−i\n\nnew = π−i\n\nold, we have:\n\nLπold(πnew) = Ea∼πnew [Aπold (s, a)]\n\nAπold(s, a)]\n\nAπold (s, a)]\n\n= Ea∼πold [\n\n= Ea∼πold [\n\n= E\n\nai∼πi\n\nold\n\n= E\n\nai∼πi\n\nold\n\nπnew(a|s) πold(a|s) new(ai|s) πi πi old(ai|s) (cid:20) πi new(ai|s) πi old(ai|s) (cid:20) πi new(ai|s) πi old(ai|s)\n\n= Lπi\n\nold\n\n(πi\n\nnew),\n\nE\n\na−i∼π−i\n\nold\n\n(cid:21) [Aπold(s, ai, a−i)]\n\n(cid:21)\n\nAi\n\nπold\n\n(s, ai)\n\nwhere Ai third equation is from the condition π−i\n\n(s, ai) = E\n\na−i∼π−i\n\nπold\n\nold\n\nnew = π−i old.\n\n[Aπold (s, ai, a−i)] is the individual advantage of agent i, and the\n\nWith the result of TRPO, we have the following conclusion:\n\nJ(πnew) − J(πold) ≥ Lπold(πnew) − CDmax new) − CDmax = Lπi\n\nKL (πnew||πold) new||πi KL (πi old)\n\n(πi\n\nold\n\n(from π−i\n\nnew = π−i\n\nold)\n\nThis means the individual objective is the same as the joint objective so the monotonic improvement is guaranteed.\n\nThen we can show the proof of Proposition 1.\n\nProof. We will build a new MDP ̃M based on the original MDP. We keep the action space ̃A = i=1Ai, where Ai is the original action space of agent i. The new state space contains multiple A = ×n layers. We define ̃Sk = S × (×k i=1Ai) for k = 1, 2, · · · , n − 1 and ̃S0 = S, where S is the original state space. Then a new state ̃sk ∈ ̃Sk means that ̃sk = (s, a1, a2, · · · , ak). The total new state space is defined as ̃S = ∪n−1\n\n ̃Si. Next we define the transition probability ̃P as following:\n\ni=0\n\n ̃P ( ̃s(cid:48)| ̃sk, ak+1, a−(k+1)) = 1 (cid:0) ̃s(cid:48) = ( ̃sk, ak+1)(cid:1) , k < n − 1 (cid:16)\n\n ̃P ( ̃s(cid:48)| ̃sk, ak+1, a−(k+1)) = 1\n\n ̃s(cid:48) ∈ ̃S0(cid:17)\n\nP ( ̃s(cid:48)| ̃sk, ak+1), k = n − 1.\n\nThis means that the state in the layer k can only transition to the state in the layer k + 1 with the corresponding action, and the state in the layer n − 1 will transition to the layer 0 with the probability P in the original MDP. The reward function ̃r is defined as following:\n\n ̃r( ̃s, a) = 1\n\n(cid:16)\n\n ̃s ∈ ̃S0\n\n(cid:17)\n\nr( ̃s, a).\n\nThis means the reward is only obtained when the state in layer 0 and the value is the same as the original reward function. Now we obtain the total definition of the new MDP ̃M = { ̃S, ̃A, ̃P , ̃r, γ}.\n\nThen we claim that if all agents learn in multi-agent sequential decision-making by PPO, they are actually taking agent-by-agent PPO in the new MDP ̃M . To be precise, one update of multi-agent\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nsequential decision-making in the original MDP M equals to a round of update from agent 1 to agent n by agent-by-agent PPO in the new MDP ̃M . Moreover, the total reward of a round in the new MDP ̃M is the same as the reward in one timestep in the original MDP M . With this conclusion and Lemma 1, we complete the proof.\n\nThe proof of Proposition 2 can be seen as a corollary of the proof of Proposition 1.\n\nProof. From Lemma 1 we know that the monotonic improvement of the joint policy in the new MDP ̃M is guaranteed for each update of one single agent’s policy. So even if the different round of updates in the new MDP ̃M is with different order of the decision-making, the monotonic improvement of the joint policy is still guaranteed. Finally, from the proof of Proposition 1, we know that the monotonic improvement in the new MDP ̃M equals to the monotonic improvement in the original MDP M . These complete the proof.\n\nB PROOFS OF THEOREM 1\n\nLemma 2 (TVD of the joint distributions). Suppose we have two distribution p1(x, y) = p1(x)p1(x|y) and p2(x, y) = p2(x)p2(x|y). We can bound the total variation distance of the joint as:\n\nDT V (p1(x, y)||p2(x, y)) ≤ DT V (p1(x)||p2(x)) + max\n\nx\n\nDT V (p1(y|x)||p2(y|x))\n\nProof. See (Janner et al., 2019) (Lemma B.1).\n\nLemma 3 (Markov chain TVD bound, time-varing). Suppose the expected KL-divergence between two transition is bounded as maxt Es∼p1,t(s)DKL(p1(s(cid:48)|s)||p2(s(cid:48)|s)) ≤ δ, and the initial state distributions are the same p1,t=0(s) = p2,t=0(s). Then the distance in the state marginal is bounded as:\n\nDT V (p1,t(s)||p2,t(s)) ≤ tδ\n\nProof. See (Janner et al., 2019) (Lemma B.2).\n\nLemma 4 (Branched Returns Bound). Suppose the expected KL-divergence between two dynamics distributions is bounded as maxt Es∼p1,t(s)[DT V (p1(s(cid:48)|s, a)||p2(s(cid:48)|s, a))], and the policy divergences at level k are bounded as maxs,a1:k−1 DT V (π1(ak|s, a1:k−1)||π2(ak|s, a1:k−1)) ≤ (cid:15)πk . Then the returns are bounded as:\n\n|η1 − η2| ≤\n\n2rmaxγ((cid:15)m + (cid:80)n (1 − γ)2\n\nk=1 (cid:15)πk )\n\n+\n\n2rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n1 − γ\n\n,\n\nwhere rmax is the upper bound of the reward function.\n\nProof. Here, η1 denotes the returns of π1 under dynamics p1(s(cid:48)|s, a), and η2 denotes the returns of π2 under dynamics p2(s(cid:48)|s, a). Then we have\n\n|η1 − η2| = |\n\n= |\n\n(cid:88)\n\ns,a (cid:88)\n\n(p1(s, a) − p2(s, a))r(s, a)|\n\n(cid:88)\n\nγt(p1,t(s, a) − p2,t(s, a))r(s, a)|\n\nt (cid:88)\n\nt\n\n≤\n\n≤ rmax\n\ns,a (cid:88)\n\ns,a (cid:88)\n\nγt|p1,t(s, a) − p2,t(s, a)|r(s, a)\n\n(cid:88)\n\nγt|p1,t(s, a) − p2,t(s, a)|.\n\nt\n\ns,a\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nBy Lemma 2, we get\n\nmax s\n\nDT V (π1(a|s)||π2(a|s)) ≤ max s,a1\n\nDT V (π1(a−1|s, a1)||π2(a−1|s, a1))\n\n+ max\n\nDT V (π1(a1|s)||π2(a1|s))\n\ns ≤ · · · n\n(cid:88)\n\n≤\n\nk=1 n\n(cid:88)\n\nk=1\n\n≤\n\nDT V (π1(ak|s, a1:k−1)||π2(ak|s, a1:k−1))\n\nmax s,a1:k−1\n\n(cid:15)πk .\n\nWe then apply Lemma 3, using δ = (cid:15)m + (cid:80)n\n\nk=1 (cid:15)πk (via Lemma 3 and 2) to get\n\nDT V (p1,t(s)||p2,t(s)) ≤ t max\n\nt\n\nEs∼p1,t(s)DT V (p1,t(s(cid:48)|s)||p2,t(s(cid:48)|s))\n\n≤ t max\n\nt\n\nEs∼p1,t(s)DT V (p1,t(s(cid:48), a|s)||p2,t(s(cid:48), a|s))\n\n≤ t(max\n\nt + max\n\nEs∼p1,t(s)DT V (p1,t(s(cid:48)|s, a)||p2,t(s(cid:48)|s, a))\n\nEs∼p1,t(s) max\n\ns\n\nDT V (π1,t(a|s)||π2,t(a|s)))\n\nt\n\n≤ t((cid:15)m +\n\nn (cid:88)\n\nk=1\n\n(cid:15)πk )\n\nAnd we also get DT V (p1,t(s, a)||p2,t(s, a)) ≤ t((cid:15)m + (cid:80)n by plugging this back, we get:\n\nk=1 (cid:15)πk ) + (cid:80)n\n\nk=1 (cid:15)πk by Lemma 2. Thus,\n\n|η1 − η2| ≤ rmax\n\n(cid:88)\n\n(cid:88)\n\nt\n\ns,a\n\nγt|p1,t(s, a) − p2,t(s, a)|\n\n≤ 2rmax\n\n(cid:88)\n\nt\n\nγt(t((cid:15)m +\n\nn (cid:88)\n\nk=1\n\n(cid:15)πk ) +\n\nn (cid:88)\n\nk=1\n\n(cid:15)πk )\n\n≤ 2rmax(\n\nγ((cid:15)m + (cid:80)n\n\nk=1 (cid:15)πk ))\n\n(1 − γ)2\n\n+\n\n(cid:80)n\n\nk=1 (cid:15)πk 1 − γ\n\n)\n\nThen we can show the proof of Theorem 1.\n\nProof. Let πβ denote the data collecting policy. We use Lemma 4 to bound the returns, but it will require bounded model error under the new policy π. Thus, we need to introduce πβ by adding and subtracting η[πβ], to get:\n\nˆη[π] − η[π] = ˆη[π] − η[πβ] + η[πβ] − η[π].\n\nwe can bound L1 and L2 both using Lemma 4 by using δ = (cid:80)n respectively, and obtain:\n\nk=1 (cid:15)πk and δ = (cid:15)m + (cid:80)n\n\nk=1 (cid:15)πk\n\nL1 ≥ −\n\n2γrmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)2\n\n−\n\n2rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)\n\nL2 ≥ −\n\n2γrmax((cid:15)πm + (cid:80)n\n\nk=1 (cid:15)πk )\n\n(1 − γ)2\n\n−\n\n2rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)\n\n.\n\nAdding these two bounds together yields the conclusion.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Illustration of learned priority of decision making in PP (upper panel) and CN (lower panel). Preys (landmarks) are viewed in black and predators (agents) are viewed in grey in PP (CN). From a to e, shown is the priority order. The smaller the level index, the higher priority of decision-making is.\n\nTable 1: Mean reward in different tasks, averaged over timesteps, with 200 test trials.\n\nFix-C\n\nSeqComm\n\n3-agent in CN −0.83 ±0.17 −0.76 ±0.08 7-agent in CN −1.79 ±0.15 −1.57 ±0.10 7-agent in PP −1.89 ±0.45 −1.31 ±0.60\n\nC ADDITIONAL EXPERIMENTS\n\nC.1\n\nILLUSTRATION OF LEARNED PRIORITY OF DECISION-MAKING\n\nFigure 8 (upper panel from a to e) shows the priority order of decision-making determined by SeqComm in PP. Agent 2 that is far away from other preys and predators is chosen to be the firstmover. If agents want to encircle and capture the preys, the agents (e.g., agent 2 and 5) that are on the periphery of the encircling circle should hold upper-level positions since they are able to decide how to narrow the encirclement. In addition, agent 3 makes decisions prior to agent 5 so that collision can be avoided after agent 5 obtains the intention of agent 3.\n\nFor CN, as illustrated in Figure 8 (lower panel from a to e), agent 2 is far away from all the landmarks and all other agents are in a better position to occupy landmarks. Therefore, agents 2 is chosen to be the first-mover, which is similar to the phenomenon observed in PP. Once it has determined the target to occupy, other agents (agent 5 and 3) can adjust their actions accordingly and avoid conflict of goals. Otherwise, if agent 5 makes a decision first and chooses to occupy the closest landmark, then agent 2 has to approach to a further landmark which would take more steps.\n\nC.2 GENERALIZATION\n\nGeneralization to different numbers of agents has always been a key problem in MARL. For most algorithms in communication, once the model is trained in one scenario, it is unlikely for agents to maintain relatively competitive performance in other scenarios with different numbers of agents. However, as we employ attention modules to process communicated messages so that agents can handle messages of different lengths. In addition, the module used to determine the priority of decision-making is also not restricted by the number of agents. Thus, we investigate whether SeqComm generalizes well to different numbers of agents in CN and PP.\n\nFor both tasks, SeqComm is trained on 5-agent settings. Then, we test SeqComm in 3-agent and 7-agent settings of CN and 7-agent setting of PP. We use Fix-C trained directly on these test tasks to illustrate the performance of SeqComm. Note that the quantity of both landmarks and preys is\n\n16\n\npp 1level 1agent 2agent 3agent 5level 1level 2level 1level 2level 3level 1level 2level 3level 4level 1level 2level 3level 4level 5abcdelevel 1level 1level 1agent 2agent 5agent 3level 1level 2level 1level 2level 3level 1level 2level 3level 4level 1level 2level 3level 4level 5cnabcdeUnder review as a conference paper at ICLR 2023\n\n(a) 3s vs 4z\n\n(b) corridor\n\nFigure 9: Learning curves in terms of the win rate of SeqComm and baselines on two customized SMAC maps: (a) 3s vs 4z, (b) corridor.\n\nadjusted according to the number of agents in CN and PP. The test results are shown in Table 1. SeqComm exhibits the superiority in CN and PP, demonstrating that SeqComm may have a good generalization to the number of agents. A thorough study of the generalization of SeqComm is left to future work.\n\nC.3 MORE SMAC MAPS\n\nWe have evaluated our method on two additional maps, i.e., 3s vs 4z and corridor. As illustrated in Figure 9, we can find out the similar conclusions as section 5.1.\n\nD ADDITIONAL RELATED WORK\n\nMulti-Agent Path Finding (MAPF). MAPF aims to plan collision-free paths for multiple agents on a given graph from their given start vertices to target vertices. In MAPF, prioritized planning is deeply coupled with collision avoidance (Van Den Berg & Overmars, 2005; Ma et al., 2019), where collision is used to design constraints or heuristics for planning. Unlike MAPF, our method couples the priority of decision-making with the learning objective and thus is more general. In addition, the different motivations and problem settings may lead to the incompatibility of the methods in the two fields.\n\nReinforcement Learning in Stackelberg Game. Many studies (K ̈on ̈onen, 2004; Sodomka et al., 2013; Greenwald et al., 2003; Zhang et al., 2020) have investigated reinforcement learning in finding the Stackelberg equilibrium. Bi-AC (Zhang et al., 2020) is a bi-level actor-critic method that allows agents to have different knowledge bases so that the Stackelberg equilibrium (SE) is possible to find. The actions still can be executed simultaneously and distributedly. It empirically studies the relationship between the cooperation level and the superiority of the SE over the Nash equilibrium. AQL (K ̈on ̈onen, 2004) updates the Q-value by solving the SE in each iteration and can be regarded as the value-based version of Bi-AC. Existing work mainly focuses on two-agent settings and their order is fixed in advance. However, the fixed order can hardly be an optimal solution as we will show in the next section. To address this issue, we exploit agents’ intentions to dynamically determine the priority of decision-making along the way of interacting with each other.\n\nE EXPERIMENTAL SETTINGS\n\nIn cooperative navigation, there are 5 agents and the size of each is 0.15. They need to occupy 5 landmarks with the size of 0.05. The acceleration of agents is 7. In predator-prey, the number of predators (agents) and prey is set to 5 and 3, respectively, and their sizes are 0.15 and 0.05. The acceleration is 5 for predators and 7 for prey. In keep away, the number of attackers (agents) and defenders is set to 3, and their sizes are respectively 0.15 and 0.05. Besides, the acceleration is 6\n\n17\n\n0.00.20.40.60.81.0step1e70.00.20.40.60.81.0win rateSeqCommTarMACRandom-C0.00.20.40.60.81.0step1e70.00.20.40.60.81.0win rateSeqCommTarMACRandom-CUnder review as a conference paper at ICLR 2023\n\nTable 2: Hyperparameters for predator-prey, cooperative navigation, keep-away\n\nHyperparameter\n\nSeqComm Random-C Fix-C TarMAC\n\nI2C\n\nIS\n\ndiscount (γ) batch size buffer capacity number of processes learning rate H\nF\n\n– –\n\n– –\n\n0.95,0.95,0.95 –\n–\n\n– –\n\n16,16,16 1.5e−5, 1e−5, 4e−5\n\n800\n\n1e6\n\n–\n\n1024\n\n–\n\n1e−2, 1e−3, 1e−3 1e−2\n\n10,10,20\n\n–\n\n2, 2, 1\n\n– –\n\n– –\n\n– –\n\n– –\n\ni=1 dt\n\nfor attackers and 4 for defenders. The three landmarks are located at (0.00, 0.30), (0.25, −0.15), and (−0.25, −0.15). Note that each agent is allowed to communicate with all other agents in all three tasks. The team reward is similar across tasks. At a timestep t, it can be written as rt team = − (cid:80)n i is the distance of landmark/prey i to its nearest agent/predator, C t is the number of collisions (when the distance between two agents is less than the sum of their sizes) occurred at timestep t, and rcollision = −1. In addition, agents act discretely and have 5 actions (stay and move up, down, left, right). The length of each episode is 20, 30, and 20 in cooperative navigation, predator-prey, and keep-away, respectively.\n\ni + C trcollision, where dt\n\nF IMPLEMENTATION DETAILS\n\nF.1 ARCHITECTURE AND HYPERPARAMETERS\n\nOur models, including SeqComm, Fix-C, and Random-C are trained based on MAPPO. The critic and policy network are realized by two fully connected layers. As for the attention module, key, query, and value have one fully connected layer each. The size of hidden layers is 100. Tanh functions are used as nonlinearity. For I2C, we use their official code with default settings of basic hyperparameters and networks. As there is no released code of IS and TarMAC, we implement IS and TarMAC by ourselves, following the instructions mentioned in the original papers (Kim et al., 2021; Das et al., 2019).\n\nFor the world model, observations and actions are firstly encoded by a fully connected layer. The output size for the observation encoder is 48, and the output size for the action encoder is 16. Then the outputs of the encoder will be passed into the attention module with the same structure aforementioned. Finally, we use a fully connected layer to decode. In these layers, Tanh is used as the nonlinearity.\n\nTable 2 summarize the hyperparameters used by SeqComm and the baselines in the MPE.\n\nFor SMAC, SeqComm, Random-C, Fix-C are based on the same architecture, the hyperparameters stay the same. For MMM2, 6z vs 8z, and 8m vs 9m, the learning rate is 5e−5, while for 10m vs 11m, corridor, and 3s vs 4z, learning rate is 7e−5. The ppo epoch is set to 10 for 6h vs 8z, and is 5 for rest maps. H and F is set to 5 and 1, respectively. However, 20 and 2 is a better value of H and F if computing resources is sufficient.\n\nFor TarMAC, the learning rate is 7e−5 for all maps. The ppo epoch is set to 10 for 6h vs 8z, and is 5 for rest maps.\n\nFor MAPPO, the learning rate is 5e−5 for MMM2 and 6z vs 8z, and 7e−5 for 8m vs 9m and 10m vs 11m.\n\nFor these four methods, the mini batch is set to 1. As for other hyperparameters, we follow the default settings of the official code (Yu et al., 2021).\n\nFor QMIX, the learning rate is 5e−5. The (cid:15) is 1 and the batch size is 32. The buffer size is 5e3. For others, we follow the default settings of link https://github.com/starry-sky6688/MARL-Algorithms.git\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nF.2 ATTENTION MODULE\n\nAttention module (AM) is applied to process messages in the world model, critic network, and policy network. AM consists of three components: query, key, and values. The output of AM is the weighted sum of values, where the weight of value is determined by the dot product of the query and the corresponding key.\n\nFor AM in the world model denoted as AMw, agent i gets messages m−i agents at timestep t in negotiation phase, and predicts a query vector qi query is used to compute a dot product with keys kt = [k1 message from agent j following AMi scaled by 1/\n\nt ) for j (cid:54)= i, and ki dk followed by a softmax to obtain attention weights α for each value vector:\n\nfrom all other w,q(hi t). The t is obtained by the t). Besides, it is\n\nt = h−i t following AMi\n\nt ]. Note that kj\n\nt is from AMi\n\nt , · · · , kn\n\nneg,k(hi\n\na,k(hj\n\n√\n\nt\n\nαi = softmax\n\n\n\n \n \n\nT k1 qi t√ t\ndk\n\n· · ·\n\nT kj qi t√ t\ndk (cid:124) (cid:123)(cid:122) (cid:125) αij\n\n· · ·\n\nT kn qi t√ t\ndk\n\n\n\n \n \n\n(1)\n\nt = (cid:80)n The output of attention module is defined as: ci or its own hidden state of observation following AMi\n\nj=1 αijvj w,v(·).\n\nt , where vj\n\nt is obtained from messages\n\nAs for AM in the policy and critic network denoted as AMa , agent i gets additional messages from upper-level agent in the launching phase. The message from upper-level and lower-level agent can be expanded as mupper = [hlower ] and mlower , 0], respectively. In addition, the t\nquery depends on agent’s own hidden state of observation hi t, but keys and values are only from messages of other agents.\n\n= [hupper\n\n, aupper\n\nt\n\nt\n\nt\n\nt\n\nF.3 TRAINING\n\nThe training of SeqComm is an extension of MAPPO. The observation encoder e, the critic V , and the policy π are respectively parameterized by θe, θv, θπ. Besides, the attention module AMa is parameterized by θa and takes as input the agent’s hidden state, the messages (hidden states of other agents) in the negotiation phase, and the messages (the actions of upper-level agents) in launching phase. Let D = {τk}K k=1 be a set of trajectories by running policy in the environment. Note that we drop time t in the following notations for simplicity.\n\nThe value function is fitted by regression on mean-squared error:\n\nL(θv, θa, θe) =\n\n1 KT\n\n(cid:88)\n\nT −1 (cid:88)\n\nτ ∈D\n\nt=0\n\n(cid:13) (cid:13)\n\n(cid:13)V (AMa(e(o), aupper)) − ˆR\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n(2)\n\nwhere ˆR is the discount rewards-to-go.\n\nWe update the policy by maximizing the PPO-Clip objective:\n\nL(θπ, θa, θe) =\n\n1 KT\n\n(cid:88)\n\nT −1 (cid:88)\n\nτ ∈D\n\nt=0\n\nmin(\n\nπ(a|AMa(e(o), aupper)) πold(a|AMa(e(o), aupper))\n\nAπold, g((cid:15), Aπold))\n\n(3)\n\nwhere g((cid:15), A) =\n\n(cid:26)(1 + (cid:15))A A ≥ 0 (1 − (cid:15))A A ≤ 0\n\n, and Aπold(o, aupper, a) is computed using the GAE method.\n\nThe world model M is parameterized by θw is trained as a regression model using the training data set S. It is updated with the loss:\n\nL(θw) =\n\n1 |S|\n\n(cid:88)\n\no,a,o(cid:48),r∈S\n\n(cid:13) (cid:13) 2\n(cid:13)(o(cid:48), r) − M(AMw(e(o), a)) (cid:13) (cid:13) (cid:13) 2\n\n.\n\n(4)\n\nWe trained our model on one GeForce GTX 1050 Ti and Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz.\n\n19",
  "translations": [
    "# Summary Of The Paper\n\n## Summary\nThe paper presents SeqComm, a multi-agent communication scheme allowing agents to condition on one another's actions by imposing ordering over the agents. The paper introduces multi-agent sequential decision and demonstrates that ordering in this paradigm can affect the optimality of the learnt policy. \n\nThe authors then present SeqComm. Each agent in SeqComm learns a policy which conditions on the joint hidden state and other agents' actions via an attention mechanism. The ordering is chosen by weighing the value of each agent's intention, which is the paper defines as the agent's future behaviour without considering the action of others. In the second phase of communication, the agents the produce a joint action. The authors then prove that monotonic improvement of the policy is independent of priority, and provide a bound on the performance loss associated with using a world model for the ordering.\n\nThe authors then evaluate SeqComm on MPE and SMAC tasks.\n\n# Strength And Weaknesses\n\n## Strengths\n- The setting of adding some communication to CTDE is an interesting way to alleviate miscoordination and work in this area is welcome. \n- The paper is quite clearly written.\n- The authors include ablation studies to evaluate why their method works.\n- The matrix game in Figure 1 nicely illustrates why an order is theoretically useful. \n\n## Weaknesses\n\n- I'm not sure that I understand the setting. If agents can broadcast functions of their observations to all other agents, then how is that different from joint learning, where all agents can view the same joint observation? This seems to not be the CTDE setting at all to me, but instead joint learning. I understand that TarMAC [1] adopts a similar setting, but I would appreciate some clarification from the authors on how this differs from joint learning.\n- The communication-free baselines MAPPO and IPPO are not a fair comparison, and it is not clear that SeqComm's performance is better TarMAC or the random-priority ablation. The authors claim that their method clearly outperforms the ablations and TarMAC, but the gap is only slight and seems to be mostly within a standard deviation. \n- The authors do not compare with a centralised method such as PPO (but conditioning the policy on the joint observation and outputting the joint action). This seems strange given this is an obvious alternative to the method, and would require approximately the same communication.\n- The empirical evaluation is only over 4 SMAC maps, and no further results are included. This does not seem enough to provide convincing evidence of outperformance.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n## Clarity\n\nThe paper is fairly clearly written. I do not believe that the authors open-source their code, which makes reproducibility difficult. It is also not clear to me which implementation of the QMIX and MAPPO baselines is used or how hyperparameters were chosen for the baselines.\n\n# Summary Of The Review\n\nAlthough some communication is no doubt useful in MARL, requiring that functions of the observation can be broadcast to all allies does not seem to be decentralised execution to me. Additionally, I remain unconvinced by the empirical results both in their breadth and performance difference.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces a novel communication scheme known as Sequential Communication (SeqComm) designed for improving coordination in multi-agent reinforcement learning (MARL) environments through asynchronous decision-making. The SeqComm framework consists of two phases: negotiation and launching. The authors provide both theoretical proofs and empirical evidence demonstrating that policies learned via SeqComm lead to monotonic improvement and convergence, outperforming existing methods in complex multi-agent tasks such as the multi-agent particle environment (MPE) and the StarCraft multi-agent challenge (SMAC).\n\n# Strength And Weaknesses\nThe principal strength of this work is the introduction of SeqComm, which effectively enhances agent coordination through structured communication and asynchronous decision-making. The theoretical guarantees of monotonically improving policies add robustness to the proposed method, making it a significant contribution to the field of MARL. However, the paper could be improved by providing more detailed discussions on the limitations of SeqComm, particularly regarding its scalability in larger environments and the potential impact of communication overhead.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The writing quality is high, with a logical flow that helps the reader grasp the complex concepts involved in SeqComm. The novelty of the approach is significant, as it combines theoretical insights with practical applications in MARL. The experimental results are thorough and convincingly demonstrate the advantages of SeqComm. However, the reproducibility of the results could be enhanced with more detailed descriptions of hyperparameter settings and experimental setups.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in multi-agent reinforcement learning through the introduction of Sequential Communication, which facilitates better coordination among agents. The theoretical backing and empirical validation support the claims made, although discussions on scalability and communication overhead would bolster the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SeqComm, a novel approach for communication in multi-agent reinforcement learning (MARL) that addresses the challenges of coordination in asynchronous environments. The methodology involves a two-phase communication system where agents first negotiate and prioritize decision-making before executing actions. Theoretical guarantees are provided, ensuring that policies learned with SeqComm improve monotonically and converge. Experimental results demonstrate that SeqComm outperforms various baseline methods in multiple environments, indicating enhanced coordination and strategy development among agents.\n\n# Strength And Weaknesses\nThe paper’s strengths include its innovative asynchronous decision-making approach and the two-phase communication scheme, which effectively tackle miscoordination in MARL. The theoretical foundations of SeqComm are robust, providing a solid basis for its claims of performance improvement. Comprehensive empirical validation across diverse environments further supports its effectiveness. However, the complexity of the implementation due to the two-phase communication and the requirement for a world model may pose challenges. Additionally, while the paper suggests good generalization capabilities, further exploration is needed to validate this across a broader range of scenarios. Lastly, the reliance on the MAPPO framework may limit its applicability in environments where MAPPO does not perform well.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions and methodology. The quality of the experimental design, including the use of ablation studies, is commendable and enhances the reproducibility of the findings. The novelty of SeqComm lies in its unique approach to communication in MARL, which is a significant advancement over existing methods. However, the complexity involved in its implementation and the dependency on MAPPO could pose challenges for researchers seeking to replicate or build upon this work.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of multi-agent reinforcement learning through its innovative SeqComm method, which enhances agent communication and coordination. While the theoretical and empirical results are strong, the complexity and dependency on existing frameworks may limit its immediate applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel communication scheme named Sequential Communication (SeqComm) for multi-agent systems, addressing challenges in partial observability and miscoordination. The methodology comprises two phases: a negotiation phase where agents communicate hidden states and establish decision-making priorities based on intention values, followed by a launching phase where upper-level agents execute decisions and convey actions to lower-level agents. The authors theoretically prove that policies learned through SeqComm improve monotonically and converge, and empirically demonstrate its superiority over existing methods across various cooperative tasks in environments like the multi-agent particle environment (MPE) and StarCraft multi-agent challenge (SMAC).\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to asynchronous communication, which effectively mitigates the issues associated with synchronous methods, such as circular dependencies. The theoretical guarantees of policy improvement and convergence bolster the proposed method's credibility. The empirical results are robust, showing consistent performance gains across diverse tasks and environments. However, the paper could benefit from a more detailed discussion on the limitations of SeqComm, particularly regarding scalability in larger agent populations or more complex environments. Additionally, the reliance on Monte Carlo methods for evaluating intentions may introduce variability that could affect reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections that logically progress from the introduction of the problem to the proposed solution and experimental validation. The quality of writing is high, making complex concepts accessible. The novelty of SeqComm lies in its unique approach to agent coordination and communication, setting it apart from existing methodologies that rely on synchronous communication. While the theoretical aspects are rigorously developed, the reproducibility of results could be improved by providing more comprehensive details on implementation specifics, hyperparameters, and the experimental setup.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in multi-agent communication through the introduction of SeqComm, which is both theoretically sound and empirically validated. The approach effectively addresses key challenges in multi-agent systems, although further exploration of its limitations in larger or more complex settings would enhance the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel communication scheme termed SeqComm designed to enhance multi-agent decision-making in environments characterized by limited communication. The methodology involves asynchronous decision-making processes that prioritize agent actions, allowing for improved coordination and reduced communication overhead. The findings demonstrate that SeqComm achieves monotonic improvement and convergence guarantees across various tasks, including Multi-Agent Particle Environments (MPE) and StarCraft Multi-Agent Challenge (SMAC), while effectively managing communication burdens among agents.\n\n# Strength And Weaknesses\nThe paper boasts several strengths, including its innovative SeqComm communication scheme, which aims to streamline interactions among agents. The asynchronous decision-making approach effectively mitigates circular dependencies, although it may not be universally applicable in scenarios that necessitate tighter coordination. The theoretical guarantees regarding monotonic improvement and convergence are commendable; however, they rely on assumptions that may limit their applicability. Empirical evaluations illustrate promising results, yet they are predominantly confined to specific environments, necessitating broader testing to establish robustness in varied real-world scenarios. Additionally, while the reduction of communication overhead is significant, the scheme may still encounter challenges related to information loss in complex interactions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, though some sections may require advanced knowledge of multi-agent systems to fully comprehend. The quality of the experimental results is high, supported by thorough ablation studies that validate key components of the proposed method. However, the reproducibility of results may be hindered by the reliance on specific assumptions and environments, which could affect the generalizability of the findings across different contexts.\n\n# Summary Of The Review\nOverall, this paper introduces an innovative communication scheme that shows promise in improving multi-agent coordination and decision-making. Despite its contributions, there are limitations concerning generalizability and potential complexities in real-world applications. Further empirical validation across diverse scenarios would enhance the robustness and applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel methodology called Sequential Coordination (SeqCoord) designed to enhance multi-agent decision-making in cooperative environments through asynchronous communication. The key contributions include a two-phase communication structure (negotiation and execution) that facilitates improved decision-making prioritization among agents, thereby addressing the challenges of circular dependencies inherent in traditional synchronous methods. The authors provide theoretical guarantees regarding the improvement and convergence of policies learned within this framework, and validate their approach through extensive empirical experiments demonstrating superior performance compared to both communication-based and communication-free methods in complex environments.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative communication scheme, which effectively mitigates the limitations of synchronous decision-making by allowing for the prioritization of agent actions. The theoretical foundations further bolster the framework's credibility by proving monotonic improvements in coordination. However, the paper lacks detailed comparisons to specific baseline algorithms, which could provide deeper insights into the advantages of SeqCoord over existing methods. Additionally, scalability and adaptability issues in varying environments remain underexplored, potentially limiting the practical applicability of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers. The quality of the writing is high, with sufficient detail provided for the proposed methods and results. The novelty is significant, as it introduces a fresh perspective on multi-agent communication that diverges from traditional synchronous frameworks. Reproducibility is supported by the thorough empirical validation; however, additional implementation details regarding the experiments would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of multi-agent reinforcement learning through the introduction of the SeqCoord framework, which emphasizes asynchronous communication to improve coordination and decision-making. While the contributions are noteworthy and well-supported by empirical evidence, the paper could benefit from more comprehensive comparisons with specific baseline algorithms and exploration of scalability challenges.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper proposes a novel adversarial training framework tailored for multi-agent systems, named Sequential Adversarial Training (SeqAdv). The methodology involves a two-phase training process where agents operate in an asynchronous manner to enhance their resilience against adversarial attacks while maintaining cooperation. Key contributions include the theoretical guarantees of policy improvement, extensive empirical validation across multiple benchmarks, and a robust framework that effectively addresses the limitations of traditional adversarial training approaches in multi-agent environments.\n\n# Strength And Weaknesses\nThe principal strength of this work lies in its innovative approach to adversarial training within multi-agent systems, specifically through its structured asynchronous decision-making process. The theoretical analysis provides solid foundations that enhance the credibility of the proposed framework. Empirical results substantiate the claims made, showing significant improvements over existing methods. However, a potential weakness is the paper's limited discussion on the scalability of SeqAdv to larger or more complex multi-agent systems, which may affect its practical applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The theoretical guarantees are presented with sufficient detail, allowing for reproducibility. The empirical validation is comprehensive, showcasing the effectiveness of SeqAdv across various tasks. However, while the clarity is commendable, the complexity of the proposed framework may require additional examples or illustrations to facilitate understanding for readers less familiar with multi-agent adversarial training.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in adversarial training for multi-agent systems through the introduction of SeqAdv. Its theoretical and empirical contributions are both robust, although further exploration into scalability and broader applicability would enhance the impact of the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents **Sequential Communication (SeqComm)**, a novel communication scheme designed to enhance multi-agent coordination in decision-making processes. The methodology consists of a two-phase communication system comprising a negotiation phase, where agents share hidden states to prioritize decisions, and a launching phase, where upper-level agents dictate actions for lower-level agents. The authors claim that SeqComm significantly improves decision-making efficiency and cooperation among agents, providing theoretical guarantees for policy convergence and adaptability to varying agent numbers. Empirical results purportedly demonstrate SeqComm's superiority over existing communication methods in several tasks, though specific metrics are not always clearly defined.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a potentially innovative communication framework that addresses critical issues in multi-agent systems, such as miscoordination and circular dependencies. The theoretical contributions suggest robust guarantees of performance and adaptability. However, weaknesses are evident in the paper's overreliance on broad claims without sufficient empirical context, particularly regarding the presented results. The dismissal of prior work and the assertion of SeqComm as a complete overhaul of existing methods may undermine the credibility of its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper articulates its ideas clearly, the quality of the empirical evidence presented is lacking, as specific metrics for performance improvements are often vague. The novelty of SeqComm is positioned as revolutionary; however, the absence of comparative analysis with existing methods raises questions about reproducibility. The theoretical guarantees provided are appealing, yet they require rigorous validation through diverse experimental setups to substantiate the claims made.\n\n# Summary Of The Review\nOverall, this paper introduces a promising new communication scheme for multi-agent decision-making, but it significantly overstates its contributions and lacks detailed empirical validation. The theoretical claims are intriguing, but the evidence provided does not convincingly support the transformative impact suggested by the authors.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents \"Sequential Communication\" (SeqComm), a novel approach aimed at improving coordination among agents in multi-agent systems facing challenges such as partial observability and stochastic environments. The proposed methodology incorporates asynchronous decision-making and a two-phase communication process to enhance agent coordination. Experimental results demonstrate that SeqComm consistently outperforms existing methods in various tasks, including predator-prey scenarios and the StarCraft Multi-Agent Challenge, with significant improvements in mean rewards and win rates.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical framework, which guarantees that policies learned using SeqComm improve monotonically, thus ensuring convergence. The experimental validation across multiple environments convincingly showcases the effectiveness of SeqComm, particularly in comparison to established methods like MAPPO and TarMAC. However, a potential weakness is the limited exploration of the communication strategies’ adaptability to different types of multi-agent environments, which may impact the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and findings. The quality of the experimental results is high, with comprehensive evaluations across multiple challenging environments. The novelty of SeqComm is evident, as it introduces a structured communication framework that improves coordination among agents. Reproducibility is supported through detailed descriptions of the experimental setups and results, although more extensive details on hyperparameter tuning could enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in multi-agent reinforcement learning through the introduction of SeqComm, demonstrating both theoretical soundness and empirical robustness. While the approach shows promise in improving agent coordination, further exploration of its adaptability could strengthen the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel multi-agent communication framework called SeqComm, which aims to improve coordination among agents in decision-making tasks by utilizing asynchronous communication with a defined priority order. The authors claim that SeqComm guarantees monotonic improvement and convergence of learned policies, addressing issues related to circular dependencies that arise in synchronous communication. The methodology includes a model-based approach that relies on a world model for decision-making and evaluates the framework's effectiveness across varying numbers of agents and communication ranges, asserting reduced communication overhead compared to traditional methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing coordination issues through asynchronous decision-making, potentially leading to improved agent performance in complex environments. The theoretical guarantees of monotonic improvement and convergence are commendable; however, the reliance on specific assumptions, such as the homogeneity of agents and the accuracy of the world model, raises concerns. The assumption that changing the order of decision-making can resolve all circular dependencies may be overly optimistic, and the potential for local optima convergence needs further exploration. Furthermore, the evaluation metrics used may not comprehensively capture the nuances of agent behavior in complex tasks, suggesting a need for more robust metrics in future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents the proposed method and its theoretical foundations. However, some assumptions underlying the methodology could benefit from more detailed explanations and justifications. While the novelty of the SeqComm framework is noteworthy, the reproducibility of results may be limited due to the reliance on specific world models and the lack of extensive empirical validation in varied real-world scenarios.\n\n# Summary Of The Review\nOverall, the paper introduces an intriguing approach to multi-agent coordination through SeqComm but is hindered by several key assumptions that may limit its applicability and effectiveness in practical scenarios. The theoretical contributions are significant, yet the empirical validation and assessment of robustness require more thorough investigation.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents Sequential Communication (SeqComm), a pioneering communication scheme designed for multi-agent systems to enhance coordination amidst the challenges of synchronous decision-making and partial observability. The methodology comprises two phases: the negotiation phase, which establishes decision-making priorities among agents, and the launching phase, where actions are executed based on communicated intentions. The authors provide both theoretical guarantees of monotonic improvement and empirical results demonstrating that SeqComm outperforms existing communication strategies in various cooperative tasks, particularly in multi-agent environments such as the multi-agent particle environment (MPE) and the StarCraft multi-agent challenge (SMAC).\n\n# Strength And Weaknesses\nOne of the main strengths of this paper is its clear identification of the coordination challenges in multi-agent systems and the innovative proposal of an asynchronous communication framework that effectively resolves circular dependencies. The empirical results are compelling, showcasing the superiority of SeqComm over traditional methods. However, a potential weakness lies in the limited scope of environments tested; while MPE and SMAC are significant, further validation in diverse settings could strengthen the claims. Additionally, the paper could benefit from a more in-depth exploration of the implications of dynamic decision-making priorities on agent behavior.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with multi-agent systems. The quality of the experiments is high, with thorough comparisons to existing methods and robust ablation studies that substantiate the findings. In terms of novelty, SeqComm introduces a fresh perspective on communication in multi-agent systems; however, the reproducibility of results could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter settings used during the evaluations.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of multi-agent reinforcement learning by introducing SeqComm, which effectively addresses coordination challenges through innovative asynchronous communication strategies. The empirical results are promising, although further exploration of the approach's applicability across a wider range of environments would bolster the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at improving the robustness of neural networks against adversarial attacks. The authors introduce a dual-layer approach that combines regularization techniques with adaptive training methodologies. Through extensive experimentation on standard benchmark datasets, the paper demonstrates that the proposed method significantly enhances model performance and resilience compared to existing adversarial training techniques.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to adversarial training, which effectively integrates two complementary strategies to bolster model robustness. The theoretical foundation is sound, providing a clear rationale for the proposed methodology. Additionally, the empirical results are compelling, showcasing significant improvements in robustness metrics across multiple datasets and attack types. However, the paper could benefit from greater detail regarding the implementation specifics of the proposed method, which may hinder reproducibility. Furthermore, while the experiments are well-conducted, the selection of baseline methods could be expanded to provide a more comprehensive comparison of the proposed approach's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the concepts accessible to readers. The quality of the figures and tables is commendable, aiding in the communication of results. The novelty of the proposed dual-layer training approach is significant, as it introduces a fresh perspective to the adversarial training landscape. However, the lack of detailed descriptions of the experimental setup and hyperparameter choices raises concerns about the reproducibility of the results.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of adversarial machine learning by proposing a dual-layer framework that demonstrates enhanced robustness in neural networks. While the theoretical insights and empirical results are strong, additional details regarding methodology and broader baseline comparisons would further solidify the paper's claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called Sequential Communication (SeqComm) designed to enhance coordination in multi-agent systems by enabling asynchronous communication among agents. The methodology consists of two distinct phases: a negotiation phase where agents communicate their hidden states and intentions to establish decision-making priorities, and a launching phase where upper-level agents execute decisions and relay this information to lower-level agents. The theoretical contributions include proof of the monotonic improvement and convergence of policies learned through SeqComm. Empirical results demonstrate that SeqComm significantly outperforms existing coordination methods in various cooperative tasks, showcasing its effectiveness in multi-agent decision-making contexts.\n\n# Strength And Weaknesses\nStrengths of this paper include the innovative approach to addressing the challenges of synchronous decision-making in multi-agent systems. The introduction of a two-phase communication scheme is a significant contribution that allows for better coordination and reduced miscoordination among agents. The theoretical foundations provided lend credibility to the proposed method, and the empirical results bolster the claims made about SeqComm's effectiveness. However, a potential weakness lies in the generalizability of the findings; the paper could benefit from exploring additional scenarios or tasks to fully establish the robustness of SeqComm across varied conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings, making it accessible to readers. The quality of the writing is high, with a logical flow that aids in understanding the proposed framework. The novelty of the SeqComm approach is evident, particularly in its asynchronous communication model, which contrasts sharply with traditional methods. However, the reproducibility of the results could be better supported by providing more details about the experimental setup, including specific parameters and environments used in the empirical evaluation.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of multi-agent systems by introducing SeqComm, which facilitates improved coordination through asynchronous communication. The theoretical and empirical findings support the effectiveness of the proposed method, although further exploration of its applicability in diverse scenarios would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel communication scheme termed Sequential Communication (SeqComm) designed to enhance coordination among agents in multi-agent systems, particularly in asynchronous decision-making environments. The authors propose a two-phase communication process consisting of a negotiation phase, where agents share hidden states and intentions, and a launching phase, where decisions are made hierarchically. The paper provides theoretical guarantees of monotonic improvement and convergence for policies learned using SeqComm and demonstrates through empirical results that this approach significantly outperforms existing communication methods in cooperative tasks, particularly in multi-agent particle environments and the StarCraft multi-agent challenge.\n\n# Strength And Weaknesses\nThe primary strengths of this paper lie in its clear identification of the limitations of synchronous communication in multi-agent settings and the introduction of SeqComm as a robust alternative. The theoretical analysis is well-grounded, providing guarantees that enhance the credibility of the proposed method. Empirically, the results are compelling, showing significant improvements over baseline methods. However, the paper could benefit from a deeper exploration of the limitations of SeqComm, such as potential scalability issues or the impacts of varying communication ranges in more complex environments. Additionally, while the results are promising, further validation across a more diverse set of tasks would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making the methodology easy to follow. The quality of the theoretical analysis is high, providing a solid foundation for the proposed method. The novelty of the communication scheme is evident, particularly in its focus on intention-based communication, which distinguishes it from prior works. The reproducibility of the experiments is likely high, given the detailed explanation of the methodology and the environments used, though access to code and data would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of multi-agent reinforcement learning by introducing SeqComm, an innovative communication framework that addresses key coordination challenges. The theoretical and empirical results support the efficacy of this approach, though some limitations and further validation could be explored.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel communication scheme termed Sequential Communication (SeqComm) aimed at enhancing coordination within multi-agent systems facing partial observability and stochasticity. It introduces a two-phase communication framework consisting of negotiation and launching phases, allowing agents to communicate asynchronously and effectively. The authors provide theoretical guarantees for policy improvement and demonstrate through empirical evaluations that SeqComm outperforms existing methods, such as MAPPO and QMIX, in several environments, including the multi-agent particle environment (MPE) and the StarCraft multi-agent challenge (SMAC).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing coordination challenges in multi-agent systems, particularly through the introduction of SeqComm and its structured communication framework. The theoretical propositions supporting the claims of policy improvement and convergence are robust. Empirical results further validate the effectiveness of the proposed method, showcasing significant performance gains over established baselines. However, a potential weakness is the lack of discussion on computational efficiency and scalability, which are critical for real-world applications. Additionally, the paper could benefit from exploring the generalization of SeqComm across diverse agent configurations and environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulated, making it accessible to readers with varying levels of familiarity with multi-agent systems. The methodology is detailed and grounded in existing literature, enhancing its credibility. The novelty of SeqComm is apparent, offering a fresh perspective on communication strategies in multi-agent settings. The empirical validation is thorough, and the authors provide sufficient details that allow for reproducibility of the experiments. However, further elaboration on certain aspects, such as computational considerations, would enhance the overall quality.\n\n# Summary Of The Review\nIn summary, the paper introduces an innovative communication framework, SeqComm, that effectively addresses coordination issues in multi-agent systems through a structured approach. While the theoretical and empirical contributions are strong, the paper would benefit from a deeper exploration of practical implications regarding computational efficiency and scalability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel communication paradigm called Sequential Communication (SeqComm) designed to enhance coordination in multi-agent systems facing the challenges of stochasticity and partial observability. The methodology consists of two main phases: negotiation, where agents evaluate their hidden states and intentions based on environmental dynamics, and launching, where actions are communicated and executed. The authors provide theoretical guarantees for the improvement of policies and convergence, and empirical results demonstrate that SeqComm outperforms existing methods across various cooperative tasks, including those in the Multi-Agent Particle Environment and StarCraft Multi-Agent Challenge.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing circular dependencies in multi-agent decision-making through asynchronous communication, which is a significant advancement in the field. The clear separation of the negotiation and launching phases allows for a structured methodology that can be replicated in future work. However, the paper could benefit from more extensive discussions on the limitations of SeqComm, particularly regarding its scalability to larger systems and potential challenges in real-world applications. Additionally, while theoretical guarantees are provided, their practical implications should be more clearly articulated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with a foundational understanding of multi-agent systems. The quality of the empirical validation is high, with comprehensive experiments and ablation studies that reinforce the findings. The novelty of SeqComm is significant, as it introduces a new framework that effectively addresses existing limitations in multi-agent communication strategies. The reproducibility is enhanced by the detailed description of the methodology and experiments, although providing code or datasets would further facilitate this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of multi-agent systems by proposing a novel communication framework that successfully mitigates coordination challenges. The theoretical and empirical validation strengthens the claims made, although discussions on practical limitations could improve the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new communication scheme called Sequential Communication (SeqComm), designed to enhance collaboration among multiple agents in decision-making tasks. The authors claim that SeqComm improves performance through a two-phase communication approach, which involves asynchronous decision-making and multiple rounds of communication. Although the empirical results indicate improved performance, the paper lacks a strong justification for why existing communication methods are inadequate, and there are concerns about the assumptions made in the theoretical guarantees provided.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical results, which demonstrate some performance improvements over diverse baselines. However, the lack of a clear rationale for the necessity of SeqComm raises doubts about its contributions. The theoretical guarantees, while promising, rely heavily on optimistic assumptions about communication effectiveness and agent behavior. Furthermore, the reliance on the MAPPO framework as a foundation limits the originality of the work, and the proposed complexities of managing multiple communication rounds may hinder scalability and practical application. The paper does not sufficiently address potential pitfalls associated with decision-making priorities and communication overhead, leaving important gaps in its practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by its convoluted approach to multi-agent communication. The quality of the theoretical framework is questionable due to the reliance on optimistic assumptions that are not well-supported. The novelty of SeqComm is limited, as it does not significantly advance beyond the existing MAPPO framework. Additionally, the reproducibility of the results is undermined by insufficient ablation studies and the lack of rigorous testing across a wide range of scenarios, making it difficult to ascertain the robustness of SeqComm.\n\n# Summary Of The Review\nOverall, the paper presents a novel communication scheme that lacks a compelling justification for its necessity, leading to questions about its significance. While there are some empirical improvements noted, the reliance on optimistic assumptions and the potential for increased complexity raise concerns about its practical applicability and scalability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents SeqComm, a novel communication scheme designed for multi-agent sequential decision-making systems. The methodology involves a two-phase communication structure—comprising negotiation and launching phases—that facilitates dynamic coordination among agents through asynchronous decision-making. The findings reveal that SeqComm consistently outperforms traditional communication-free and communication-based methods in various cooperative tasks, demonstrating significant improvements in coordination, efficiency, and adaptability.\n\n# Strength And Weaknesses\nStrengths of SeqComm include its groundbreaking approach to agent communication, which enhances real-time collaboration and reduces miscoordination through intention-based decision-making. The theoretical guarantees of monotonic improvement add to its reliability, while the empirical results showcase its superior performance across a range of tasks, indicating its broad applicability in real-world scenarios. However, the paper could benefit from a more detailed exploration of the limitations or potential challenges in implementing SeqComm in diverse environments, as well as a clearer explanation of the computational complexity associated with the two-phase communication.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to the audience. The quality of the theoretical proofs and empirical results is high, indicating a robust methodology. The technical novelty of the two-phase communication approach is significant, as it represents a shift from traditional synchronous methods. However, the reproducibility of the results could be enhanced by providing more detailed experimental setups and hyperparameter settings used in the empirical evaluations.\n\n# Summary Of The Review\nOverall, SeqComm represents a substantial advancement in multi-agent communication and coordination strategies, offering a flexible and efficient framework that has been empirically validated across multiple tasks. Its innovative approach, alongside strong theoretical backing, positions it well for future applications in diverse fields, although some areas for improvement in clarity and reproducibility remain.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel theoretical framework called Sequential Communication (SeqComm) aimed at enhancing coordination in multi-agent systems through structured communication. It addresses the challenges posed by partial observability and stochastic environments, proposing an asynchronous decision-making approach that prioritizes agents in a hierarchy. The authors provide theoretical underpinnings for SeqComm, arguing its superiority over traditional methods reliant on past observations and highlighting its alignment with the concept of Stackelberg Equilibrium. Key findings include guarantees of monotonic improvement and convergence of learned policies, as well as a theoretical analysis of communication overhead, suggesting that SeqComm can reduce computational complexity in multi-agent scenarios.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to addressing the coordination challenges in multi-agent systems, particularly through the introduction of a structured communication framework that mitigates circular dependencies. The theoretical contributions are well-articulated, offering clear benefits of the proposed SeqComm framework over existing methodologies. However, the paper's weaknesses include a limited empirical evaluation of the framework, which may restrict its practical applicability and understanding of how well SeqComm performs compared to other methods in real-world scenarios. Additionally, certain theoretical claims, while compelling, require further exploration and validation through empirical studies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its theoretical framework, making complex concepts accessible. The quality of writing is high, with a logical flow from introduction to conclusion. The novelty of the SeqComm framework is significant, as it challenges existing paradigms in multi-agent reinforcement learning. However, reproducibility could be a concern due to the lack of detailed experimental results and concrete implementations, which would be necessary to validate the theoretical claims made.\n\n# Summary Of The Review\nOverall, this paper provides a significant theoretical advancement in the field of multi-agent systems through the introduction of the SeqComm framework. While the theoretical contributions are strong and well-founded, the lack of empirical validation may limit the impact of the work on practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents SeqComm, a novel framework for multi-agent sequential decision-making that extends the MAPPO architecture by incorporating a communication mechanism among agents. The methodology involves two key phases: a negotiation phase for sharing hidden states and predicting trajectories, followed by a launching phase for decision-making and action communication. The findings demonstrate that SeqComm effectively improves performance in multi-agent environments, particularly in the Multi-Agent Particle Environment (MPE) and the StarCraft Multi-Agent Challenge (SMAC), showcasing its adaptability across varying agent counts.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its detailed implementation of the SeqComm model, including the use of a world model with an attention mechanism and a structured communication process that enhances decision-making among agents. The ablation studies add value by evaluating the impact of communication range and decision-making strategies. However, the paper lacks a comprehensive theoretical discussion on the broader implications of SeqComm and does not sufficiently compare its performance against other established frameworks in multi-agent systems. The focus on implementation specifics may overshadow the exploration of the model's theoretical contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear in its presentation of methodology and experimental setup. However, the heavy emphasis on implementation details may detract from the broader significance of the findings. The reproducibility is somewhat compromised by the absence of released code for certain components, although the paper does provide some references to external repositories. The novelty of SeqComm lies in its communication-based approach, which is an important addition to existing multi-agent frameworks, but it could be articulated more effectively in relation to the state of the art.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for multi-agent decision-making that leverages communication to enhance performance. However, it would benefit from a deeper theoretical exploration and comparative analysis with existing models to better situate its contributions within the broader research landscape.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach called Sequential Communication (SeqComm) aimed at improving multi-agent sequential decision-making. The authors propose a communication scheme that leverages asynchronous decision-making to resolve circular dependencies, claiming that SeqComm outperforms existing communication-based methods in this domain. The experiments conducted on Multi-Agent Particle Environment (MPE) and StarCraft Multi-Agent Challenge (SMAC) tasks support their claims, although the benchmarks used are somewhat conventional.\n\n# Strength And Weaknesses\nWhile the proposed SeqComm method presents an interesting framework for enhancing communication in multi-agent settings, it heavily relies on existing models such as MAPPO, raising concerns about its originality. The improvements over previous methods like I2C and TarMAC may not be substantial, given that these methods have established baseline performances. Additionally, the paper does not sufficiently acknowledge prior work that addresses similar issues, such as coordination graphs and social conventions. The communication scheme's complexity is also critiqued, as it may not provide clear advantages over simpler, established communication strategies. Furthermore, the ablation studies lack a thorough exploration of alternative approaches, weakening the argument for SeqComm's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly. However, the novelty of the proposed contributions is questionable, as it appears to rehash established ideas with minor modifications. The theoretical guarantees are appealing but echo previous works, leading to concerns about the originality of the contributions. Reproducibility is achievable, but the lack of direct quantitative comparisons with similar methods may hinder a comprehensive evaluation of SeqComm's effectiveness.\n\n# Summary Of The Review\nOverall, while the paper presents a structured approach to enhancing communication in multi-agent reinforcement learning, its contributions lack the originality and significance suggested by the authors. Many of the issues addressed and proposed solutions have been explored in existing literature, and a more balanced contextualization would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Multi-Agent Sequential Decision-Making via Communication\" presents a novel framework for enabling communication among agents in multi-agent systems. The authors introduce an approach called SeqComm, designed to facilitate decision-making in environments where agents possess distinct roles and priorities. The methodology involves a hierarchical structure where upper-level agents make decisions before lower-level agents, optimizing joint actions through communication. Empirical results demonstrate that SeqComm outperforms existing methods in various cooperative tasks, although specific baselines are not thoroughly detailed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to multi-agent decision-making, particularly through communication. The hierarchical decision-making structure is a significant contribution, providing a clear framework for understanding the interactions between agents. However, the paper has notable weaknesses, including inconsistent terminology and insufficient clarity in defining key concepts such as SeqComm and Pareto superiority. Additionally, the empirical results could benefit from a more rigorous explanation of methodologies and statistical significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the clarity of the paper is compromised by inconsistent terminology and a lack of definitions for critical concepts. The quality of writing varies, with convoluted phrasing in certain sections that detracts from the reader's understanding. The novelty of the approach is commendable, but the reproducibility of results is hindered by insufficient detail regarding experimental setups and parameter tuning for the baselines.\n\n# Summary Of The Review\nWhile the paper introduces a promising framework for multi-agent decision-making via communication, it suffers from clarity and consistency issues that impede understanding. The contributions are significant, but the execution requires refinement to ensure that key concepts are clearly communicated and reproducible.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents SeqComm, a novel communication scheme designed for multi-agent systems aimed at improving coordination through prioritized decision-making. The authors employ the MAPPO framework to explore how SeqComm facilitates asynchronous decision-making among agents. Experimental results indicate that SeqComm outperforms selected baselines in specific environments such as Multi-Agent Particle Environment (MPE) and StarCraft II Micro-Management (SMAC). However, the paper does not extensively consider alternative communication methods or the scalability of the proposed approach.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative framework for prioritizing decision-making in multi-agent systems and its empirical demonstration of improved performance in controlled environments. However, the study exhibits several weaknesses, including a limited scope of experimentation, lack of investigation into alternative communication methods, and insufficient discussion on the method's limitations. Additionally, the paper does not address potential scalability issues or the effects of communication delays, raising concerns about the practical applicability of SeqComm in diverse real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. The quality of the experiments is adequate, although the limited range of environments tested detracts from reproducibility and generalizability. The novelty of the proposed approach is notable, but the authors could enhance the paper by comparing SeqComm to a broader set of existing methods and exploring the implications of various agent configurations and environmental dynamics.\n\n# Summary Of The Review\nOverall, the paper introduces an intriguing communication framework for multi-agent systems with promising empirical results. However, it falls short in exploring the broader implications and limitations of the proposed method, which may hinder its applicability and robustness in more complex and varied environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces a novel communication strategy called Sequential Communication (SeqComm) for multi-agent systems, aimed at enhancing coordination and resolving challenges associated with synchronous decision-making. The authors present theoretical guarantees of monotonic improvement and convergence for policies learned via SeqComm, supported by a detailed proof structure. Empirical evaluations demonstrate SeqComm's effectiveness across various multi-agent environments, including MPE and SMAC, with comparisons against established baselines, although the statistical robustness of these findings warrants further scrutiny.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its theoretical contributions, particularly the guarantees of monotonic improvement and convergence, which are crucial for establishing the reliability of the proposed method. The detailed proof structure adds credibility to these claims. However, the paper's empirical evaluations are less robust, as they lack comprehensive statistical analyses, such as significance testing and confidence intervals, which could clarify the importance of the observed improvements. The reliance on mean and standard deviation for result presentation is standard but could be enhanced by incorporating effect sizes and error bars to better communicate variability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its main contributions and findings. However, the clarity of the empirical results is somewhat diminished by the lack of rigorous statistical validation. While the theoretical framework is novel, establishing the practical implications of the findings requires more detailed empirical analysis. Reproducibility is also a concern, as the absence of thorough statistical testing raises questions about the reliability of the reported results.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to communication in multi-agent systems, supported by solid theoretical guarantees. However, the empirical evaluations require more rigorous statistical analysis to validate the claims effectively, which affects the overall robustness and credibility of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper proposes a novel communication protocol named Sequential Communication (SeqComm) aimed at enhancing coordination among agents in multi-agent environments. The methodology involves a two-phase communication scheme where agents share information and make decisions based on prioritized communication strategies. The findings suggest that SeqComm improves efficiency in communication and decision-making in controlled settings, potentially leading to better coordinated actions among homogeneous agents.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of SeqComm, which presents a structured approach to agent communication that could theoretically enhance cooperation. However, the paper has several weaknesses. It does not explore alternative communication protocols, limiting its applicability. The assumption of agent homogeneity and the lack of investigation into agent heterogeneity raise concerns about the generalizability of the findings. Moreover, the study does not address the robustness of SeqComm in dynamic environments or the computational overhead introduced by the two-phase scheme. The absence of evaluations in competitive settings further restricts the claims made about SeqComm's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed method. However, the novelty of SeqComm is somewhat undermined by the lack of comparative analysis with other communication strategies. Reproducibility is challenged by the limited experimental validation provided, particularly in diverse and complex environments. Future work is suggested but lacks detailed experimental results to substantiate the claims made.\n\n# Summary Of The Review\nOverall, the paper presents a promising communication protocol for multi-agent systems but suffers from significant limitations in its scope and empirical validation. The assumptions made regarding agent behavior and environmental dynamics could hinder the practical applicability of the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"MULTI-AGENT SEQUENTIAL DECISION-MAKING VIA COMMUNICATION\" introduces a communication scheme called SeqComm, aimed at improving coordination among multiple agents in a multi-agent reinforcement learning (MARL) setting. The authors propose a two-phase communication methodology that involves negotiation and launching phases, asserting that this allows for better decision-making among agents. Theoretical contributions include proofs of monotonic improvement and convergence, while empirical results claim superiority over existing methods. However, the foundational concepts discussed are not novel and have been well-explored in the literature.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its structured approach to discussing communication among agents and the provision of theoretical proofs regarding the convergence properties of the proposed method. However, the weaknesses are significant: the paper rehashes known concepts without presenting genuinely novel insights, mischaracterizes related work, and lacks rigorous empirical comparisons. The claims of superiority over other methods appear unsubstantiated, and the overall contribution to the field is incremental at best.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, but it suffers from a lack of novelty. The authors present well-established concepts as if they are new contributions, which diminishes the overall quality. The methodology is reproducible but does not offer substantial advancements over existing techniques. The experimental design raises concerns regarding fairness in comparisons, further undermining the claims made.\n\n# Summary Of The Review\nOverall, this paper attempts to present a novel communication framework within the MARL context; however, it ultimately falls short as the contributions are largely incremental and lack originality. The claims made are overstated, and the paper does not adequately build upon existing foundational work in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework for multi-agent sequential decision-making called Sequential Communication (SeqComm). This methodology emphasizes improved coordination among agents through effective communication strategies. The findings suggest that SeqComm enhances the performance of agents in collaborative tasks, showcasing its potential for scalability and generalization across different environments and agent configurations.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to multi-agent communication and its clear demonstration of improved coordination. However, several weaknesses were noted: the exploration of adaptive exploration strategies is limited, and there is a lack of comprehensive comparisons with alternative communication methods. Additionally, while the reliance on a world model for intention evaluation is promising, further exploration of robust model-based reinforcement learning techniques could enhance predictions and overall performance. The paper would benefit from outlining specific real-world applications and future research directions more explicitly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The quality of the experiments is strong, although the novelty, while present, could be better contextualized through comparisons with recent advancements in multi-agent communication. The reproducibility of results could be improved by providing more details on experimental setups and parameters, particularly regarding the ablation studies and their configurations.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of multi-agent systems through its SeqComm framework, demonstrating improved coordination among agents. However, it could enhance its impact by addressing the limitations in exploration strategies, comparisons with other methods, and clearer applications in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces SeqComm, a novel communication framework designed for multi-agent systems to enhance cooperation through dynamic prioritization of communication. The methodology involves evaluating SeqComm across various multi-agent cooperative tasks in Multi-Agent Particle Environment (MPE) and StarCraft Multi-Agent Challenge (SMAC). The findings demonstrate that SeqComm consistently outperforms existing communication-free and communication-based baselines in terms of mean rewards and win rates, exhibiting superior learning curves and generalization capabilities while reducing communication overhead.\n\n# Strength And Weaknesses\nThe strengths of the paper include its comprehensive empirical evaluation, which showcases SeqComm's performance across diverse tasks and environments, including predator-prey, cooperative navigation, and customized maps in SMAC. The ablation studies effectively highlight the importance of dynamic prioritization in decision-making, further solidifying the contributions of SeqComm. However, a potential weakness lies in the limited exploration of the underlying mechanisms that enable SeqComm's advantages; more in-depth analysis could enrich the understanding of its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and findings. The quality of the empirical results is high, supported by rigorous statistical analysis and robust visualization of agent behaviors that reinforce the quantitative data. In terms of novelty, SeqComm presents a significant advancement in multi-agent communication strategies, although the reproducibility could be enhanced by including more detailed experimental setups and hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of multi-agent systems with SeqComm, demonstrating significant improvements in performance metrics across multiple benchmarks. The methodology and findings are well-supported, although additional insights into the operational principles of SeqComm could strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for multi-agent communication in decentralized settings, termed \"SeqComm.\" The methodology involves modeling communication strategies among agents through a sequence-based approach that allows for intention recognition and decision-making based on the anticipated actions of upper-level agents. The findings indicate that the proposed framework outperforms existing methods in terms of efficiency and adaptability in complex environments, demonstrating significant improvements in task completion rates and communication effectiveness.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to multi-agent communication, which provides a new perspective on how agents can dynamically adapt their strategies based on their environment. The empirical results support the claims of higher efficiency and effectiveness, showcasing the potential practical applications of SeqComm. However, the paper does have weaknesses, particularly in its clarity and organization. The lengthy nature of the text and convoluted sentence structures may hinder reader comprehension. Additionally, some critical concepts are not defined clearly, which could alienate readers unfamiliar with the terminology.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel approach with significant implications, the clarity and quality of writing require attention. The abstract is dense and difficult to parse, and the lengthy sentences detract from the overall readability. The inclusion of clear definitions for key terms and more descriptive section headings would enhance navigability. Furthermore, while the methodology is well-articulated, the reproducibility of experiments could be improved by providing more detailed descriptions of the experimental setups and parameters.\n\n# Summary Of The Review\nThe paper makes a commendable contribution to the field of multi-agent systems through its innovative SeqComm framework, demonstrating notable improvements in performance. However, the clarity, organization, and presentation of the paper require significant enhancement to facilitate comprehension and ensure accessibility for a broader audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4861632911804774,
    -1.5598657401564728,
    -1.8001698779819575,
    -1.7506299183916332,
    -1.9386805770897333,
    -1.8438498263700456,
    -1.765169132405497,
    -2.0384276567555277,
    -1.7497436544232288,
    -1.9195881036377849,
    -1.6829091798440299,
    -1.4374201824397086,
    -1.6707985396187355,
    -1.6542660592211493,
    -1.6050903585032257,
    -1.790131755644293,
    -1.899757283054655,
    -1.8856529640039503,
    -1.7123800088875487,
    -1.8751655755211447,
    -2.0844904471617065,
    -1.807307932711916,
    -1.7722562895602447,
    -1.6955897846557144,
    -1.904706709741972,
    -1.9293019604537163,
    -1.9310561599193685,
    -1.8219137439535005,
    -1.8656441675097581
  ],
  "logp_cond": [
    [
      0.0,
      -2.2797663994130692,
      -2.2662232601569814,
      -2.2562911558883916,
      -2.2966046933432436,
      -2.311775779683061,
      -2.3599880809564366,
      -2.281891360646175,
      -2.280668182605191,
      -2.297322704317362,
      -2.279550254854664,
      -2.3941080986047476,
      -2.2917836987999327,
      -2.269798576821881,
      -2.300286463833654,
      -2.285449184016864,
      -2.3125181651756086,
      -2.276682289884118,
      -2.3141655049494734,
      -2.3083338181217954,
      -2.286051672491995,
      -2.318848130563062,
      -2.2969428395477367,
      -2.2960851447958555,
      -2.3320915738291155,
      -2.294274784130695,
      -2.314665600010119,
      -2.3045266000405014,
      -2.356460990198995
    ],
    [
      -1.1807242188815728,
      0.0,
      -1.1042348621906444,
      -1.0110243109804595,
      -1.1531144443730597,
      -1.1259785338830965,
      -1.1535687697376722,
      -1.1210378657425604,
      -1.0848838913447256,
      -1.183049162108015,
      -1.0716735691020949,
      -1.2222412939903704,
      -1.067137638462818,
      -0.9804442626153479,
      -1.0501599104908215,
      -1.0755980116637804,
      -1.140443315570889,
      -1.1487675412421678,
      -1.1583047594502438,
      -1.1349348218952462,
      -1.1204166613016175,
      -1.1792991987068053,
      -1.1527901085250027,
      -1.0835192795059951,
      -1.1808688400477627,
      -1.0669718638212553,
      -1.2057379442126468,
      -1.220127320178674,
      -1.2424962616063442
    ],
    [
      -1.3642262538754448,
      -1.4341489824089813,
      0.0,
      -1.3703498128202192,
      -1.400062655706526,
      -1.4007587735712481,
      -1.463175903701286,
      -1.3696341075710823,
      -1.383353422567378,
      -1.3485076679267263,
      -1.4004078968017262,
      -1.5146511246419787,
      -1.3593482357090974,
      -1.3956379509387882,
      -1.394000537645744,
      -1.4133957627858214,
      -1.3492321378830545,
      -1.3977561699209005,
      -1.3906795308186704,
      -1.376070512681609,
      -1.3713206045338637,
      -1.4144952791497043,
      -1.4237227659236167,
      -1.3793730605071357,
      -1.4878844491611447,
      -1.4305471131483338,
      -1.4009270348306144,
      -1.4990460714843037,
      -1.459691243570935
    ],
    [
      -1.321938151524934,
      -1.1979997538262224,
      -1.2928751928764275,
      0.0,
      -1.2864120350734773,
      -1.272777143300435,
      -1.3904599548509569,
      -1.197909029440036,
      -1.33733979713149,
      -1.3088510705329113,
      -1.1648387335640817,
      -1.4071473937123873,
      -1.078696663328348,
      -1.151533956131345,
      -1.281866348192516,
      -1.19875611667536,
      -1.343813924515794,
      -1.2749790608715892,
      -1.32249885614422,
      -1.3074684004967594,
      -1.3018170721995916,
      -1.3420362065232985,
      -1.3937963606450825,
      -1.2981243142700125,
      -1.408349328768297,
      -1.2724058242597025,
      -1.3976323567565596,
      -1.3689994924903752,
      -1.3961037827382685
    ],
    [
      -1.5462520208792034,
      -1.548777326873799,
      -1.5284044560992138,
      -1.5308558749654786,
      0.0,
      -1.531371517526399,
      -1.6351195095231228,
      -1.5038745233263981,
      -1.6104969695002374,
      -1.450007819378341,
      -1.5052131079449047,
      -1.652678361791712,
      -1.5574889658369329,
      -1.504934930300614,
      -1.572242041626986,
      -1.5316067777405429,
      -1.5382439155740109,
      -1.5800487511165018,
      -1.5311630102150866,
      -1.5769456222253473,
      -1.5048208701637442,
      -1.5919803825607852,
      -1.5278981870571653,
      -1.562031831624857,
      -1.576078992096068,
      -1.5354349846612039,
      -1.6208927246156095,
      -1.5944763926026084,
      -1.6132297341736768
    ],
    [
      -1.504143368728931,
      -1.334256565734346,
      -1.3430539485438981,
      -1.3459808256444836,
      -1.4178560688378796,
      0.0,
      -1.4392351481735017,
      -1.3842697564517907,
      -1.4323304003534956,
      -1.411653940979304,
      -1.3247494786350398,
      -1.5182852026772837,
      -1.3007064614740884,
      -1.3732431162109453,
      -1.4008239143605477,
      -1.3928772601290602,
      -1.449028526829259,
      -1.3801116379820766,
      -1.3977817833192712,
      -1.5140928195711227,
      -1.432497814823465,
      -1.4781782847923464,
      -1.478736791216613,
      -1.4658044878875267,
      -1.5215713239784934,
      -1.3842864998877573,
      -1.5011662994666366,
      -1.496566639708385,
      -1.546054996724883
    ],
    [
      -1.4046397453406079,
      -1.2682574262720048,
      -1.3327836504933628,
      -1.2867189773256182,
      -1.3062771430371245,
      -1.3016011957412155,
      0.0,
      -1.2705371738863962,
      -1.2559274920937118,
      -1.3261183008012076,
      -1.2864275797486011,
      -1.359150186302557,
      -1.276066451629672,
      -1.2538565702429898,
      -1.2809751624732024,
      -1.249797259152206,
      -1.2796278858536776,
      -1.3219863741549651,
      -1.315634526041248,
      -1.3834867223621283,
      -1.3033316362797183,
      -1.3742201614825187,
      -1.3537604155738991,
      -1.3066981176348988,
      -1.3228352064762572,
      -1.3447320631835535,
      -1.4043464345810737,
      -1.365485935364412,
      -1.3474433918133828
    ],
    [
      -1.7004489214083376,
      -1.6084098409404868,
      -1.640438054206733,
      -1.5155962957764657,
      -1.6788590938016872,
      -1.6339788889553055,
      -1.7038694994485288,
      0.0,
      -1.6665959655539488,
      -1.6982207063297452,
      -1.5768967560399672,
      -1.7519984694931339,
      -1.5052806197895598,
      -1.538980506791486,
      -1.6109514380410046,
      -1.5734359261189765,
      -1.6715421609868801,
      -1.6169302929222007,
      -1.7149864785915516,
      -1.6633038245458711,
      -1.6612332634155595,
      -1.733615341364103,
      -1.6931679985765489,
      -1.7224218337036314,
      -1.7327082613307156,
      -1.5881663886153157,
      -1.7427266137822737,
      -1.7203488735934327,
      -1.7242741805839856
    ],
    [
      -1.3021986161097059,
      -1.1437011966874073,
      -1.1835028253024673,
      -1.2189616099141838,
      -1.2228759141412755,
      -1.1984445141471864,
      -1.275397959488746,
      -1.1973001247443387,
      0.0,
      -1.296369230051639,
      -1.1897123469345605,
      -1.3539206926412548,
      -1.212671648742773,
      -1.2072931144378531,
      -1.1972992181568038,
      -1.18214854068537,
      -1.2414168172971218,
      -1.257364114166299,
      -1.2069101550845336,
      -1.3296090795075821,
      -1.2204290280836618,
      -1.3393993170772962,
      -1.2810385573702021,
      -1.2227973004472443,
      -1.2969604345514214,
      -1.2566278931005532,
      -1.3128848205208152,
      -1.2386820787438435,
      -1.3666675367599943
    ],
    [
      -1.5190050561528214,
      -1.539146619951435,
      -1.4412921136625771,
      -1.5263389630012534,
      -1.4618163497120695,
      -1.5056800109168438,
      -1.623773652974783,
      -1.5046922472618474,
      -1.561350486619882,
      0.0,
      -1.5249024535717486,
      -1.6621021411023316,
      -1.5470336093316082,
      -1.5344563898872412,
      -1.5985566410813206,
      -1.5742413114086837,
      -1.5050190099325134,
      -1.6050255334625727,
      -1.4503476911364062,
      -1.563070749482953,
      -1.5072647769729257,
      -1.548858347359864,
      -1.5617180221247422,
      -1.4794460475490119,
      -1.5316675150006616,
      -1.5261808477327146,
      -1.551591460981351,
      -1.605534370127912,
      -1.5872853752872178
    ],
    [
      -1.2721439565081514,
      -1.1597493934785108,
      -1.244991221168274,
      -1.1104871196768387,
      -1.1837996692972315,
      -1.2123119570198602,
      -1.3221526220221924,
      -1.2034709175045586,
      -1.2342298361580715,
      -1.2546291987034428,
      0.0,
      -1.339443338373185,
      -1.162209757134675,
      -1.1915853513485883,
      -1.2282436994704526,
      -1.1182361421137312,
      -1.2781923636162875,
      -1.2171058063916431,
      -1.268731317392869,
      -1.2941019015344215,
      -1.227011821233141,
      -1.333024670406142,
      -1.2758152985388918,
      -1.2468304134418253,
      -1.3515840222183817,
      -1.2196206705634538,
      -1.303429140897515,
      -1.2741442983953233,
      -1.3722032986506416
    ],
    [
      -1.1597680388357992,
      -1.1121534218106686,
      -1.1132943421624457,
      -1.094902357341125,
      -1.1073907618049141,
      -1.1066553616708275,
      -1.107983073329544,
      -1.1135755649840593,
      -1.1188906566359458,
      -1.1357661614180028,
      -1.1088296075129862,
      0.0,
      -1.0901363954354228,
      -1.0849714297151543,
      -1.0978480474272883,
      -1.1164414945701793,
      -1.0901840494978245,
      -1.1412785289921925,
      -1.1200711391048939,
      -1.1649745441850328,
      -1.0771569582033618,
      -1.1481179544237,
      -1.111934631098011,
      -1.1294177455465817,
      -1.1078540299468895,
      -1.1310871814995476,
      -1.1879978736066028,
      -1.1407708772846195,
      -1.1211718387318583
    ],
    [
      -1.3124054339376658,
      -1.200645141410179,
      -1.220407765902457,
      -1.0440142868931128,
      -1.2740783327429024,
      -1.1890803438837751,
      -1.2994432484297642,
      -1.071382739899656,
      -1.2764892171241013,
      -1.2807587248004793,
      -1.1426378387205198,
      -1.3385170287886718,
      0.0,
      -1.0935001304560694,
      -1.2345271595731768,
      -1.15783098600196,
      -1.282342602244954,
      -1.1858904329782782,
      -1.2632246726349488,
      -1.3016012588431263,
      -1.2834860738072538,
      -1.3056781585200858,
      -1.2967304567737452,
      -1.255104410692154,
      -1.2940139883411332,
      -1.1952362953886373,
      -1.330216731201451,
      -1.3413057725824173,
      -1.357922008568039
    ],
    [
      -1.237563735576597,
      -1.1473705541507935,
      -1.1833889005303926,
      -1.0796128967925247,
      -1.242948220654061,
      -1.2348471100490863,
      -1.3224274168958257,
      -1.1216120312308615,
      -1.2101469844197352,
      -1.234953893207554,
      -1.1336101087311317,
      -1.3495575886420093,
      -1.1010381266187144,
      0.0,
      -1.174388187346131,
      -1.1059392054495367,
      -1.258961703903972,
      -1.194797746142594,
      -1.2532498112962727,
      -1.2403588309019502,
      -1.2201369319591582,
      -1.3240014570462533,
      -1.2952267288353565,
      -1.2295683799193355,
      -1.3242755742185324,
      -1.173544727296087,
      -1.284326092516543,
      -1.344011474316903,
      -1.3488960637864496
    ],
    [
      -1.2568375901723912,
      -1.1186256966648367,
      -1.2010708729544368,
      -1.0955995032196164,
      -1.2347068938299708,
      -1.2285467456915347,
      -1.228881660068898,
      -1.1748359042868064,
      -1.1734216681502057,
      -1.2593826226546598,
      -1.1454259458811322,
      -1.285496382423676,
      -1.1662998475459383,
      -1.1092576119526225,
      0.0,
      -1.0762021494579253,
      -1.2250364414272605,
      -1.2091258797737119,
      -1.1928933498382621,
      -1.2237304846608585,
      -1.2072179316696092,
      -1.2952290590087099,
      -1.2542884043695794,
      -1.2255156060188321,
      -1.253289394378602,
      -1.1692390243160848,
      -1.2824377415477843,
      -1.3216616356649373,
      -1.3149419821038506
    ],
    [
      -1.3363594972687585,
      -1.2807970257440093,
      -1.2783793701368475,
      -1.1556726843032645,
      -1.2974633224133252,
      -1.3235214168928597,
      -1.3919002546083277,
      -1.2378005412762885,
      -1.332978301019036,
      -1.3541132092282466,
      -1.205030580108117,
      -1.4855784298893107,
      -1.237849813955209,
      -1.2356072701323124,
      -1.2309747100081456,
      0.0,
      -1.3480182640663572,
      -1.3500509502611018,
      -1.3472318848344642,
      -1.309070509825434,
      -1.2759563019156481,
      -1.4023894976296576,
      -1.3927052691549837,
      -1.3492920408768267,
      -1.3964225032939155,
      -1.287669984443323,
      -1.3875378434631835,
      -1.37718320331536,
      -1.4512926362592042
    ],
    [
      -1.532147370745706,
      -1.5138592310929575,
      -1.5078005506709227,
      -1.518847631764518,
      -1.4866408528158892,
      -1.5182107013045487,
      -1.6037537912865676,
      -1.488970942974591,
      -1.4914715312445266,
      -1.532155365382855,
      -1.4918628126533156,
      -1.6283653438310055,
      -1.5240486607191943,
      -1.4754633913013795,
      -1.4836513336168684,
      -1.5202627620220972,
      0.0,
      -1.5251064455735468,
      -1.5954284095075597,
      -1.5649531050626866,
      -1.4105625226137974,
      -1.5844723674367838,
      -1.4745763519834914,
      -1.5581817055122686,
      -1.5487917202462091,
      -1.513251538919097,
      -1.5707706172277605,
      -1.5779925212552184,
      -1.6452453076520792
    ],
    [
      -1.4693589577545243,
      -1.4190244624349935,
      -1.4102411179520227,
      -1.379599470301688,
      -1.4358646909103063,
      -1.4274139526384526,
      -1.490144784309434,
      -1.3564133842402328,
      -1.4513409269473094,
      -1.5193793855258664,
      -1.3595235982592575,
      -1.5799744331251482,
      -1.370162627452306,
      -1.3795828667382803,
      -1.4455550517396156,
      -1.413894503321041,
      -1.4426178506538445,
      0.0,
      -1.5066357622684825,
      -1.432657890088513,
      -1.4577684873279324,
      -1.4862562002650885,
      -1.4764793612444331,
      -1.4644539357228517,
      -1.5480415268505756,
      -1.3577467524652853,
      -1.4898507424599539,
      -1.455017432986928,
      -1.4862216367368057
    ],
    [
      -1.444880028861103,
      -1.3841191794196643,
      -1.3549809710046627,
      -1.3962772029234067,
      -1.4094237386365114,
      -1.3458186115883626,
      -1.4381678675687481,
      -1.3613990343565028,
      -1.4133098922591611,
      -1.2888846914370118,
      -1.388981203139356,
      -1.4459170737732372,
      -1.3895007392298069,
      -1.4095266602865746,
      -1.4296670706433758,
      -1.397783187990165,
      -1.462603231549314,
      -1.4791204794472952,
      0.0,
      -1.3885320644014854,
      -1.3884255636835925,
      -1.3854150145774273,
      -1.4428650243762513,
      -1.3429842188300978,
      -1.4466219048220477,
      -1.3865701403771808,
      -1.3903621978626957,
      -1.4570662536830876,
      -1.4443130284201702
    ],
    [
      -1.4953302648626445,
      -1.5617357880560594,
      -1.492250808753027,
      -1.4941375365990794,
      -1.548895034350236,
      -1.5684665106615812,
      -1.59467562471232,
      -1.4821369933386817,
      -1.5812173239544867,
      -1.533898042650601,
      -1.5290770305896435,
      -1.648464001135806,
      -1.5314569392423973,
      -1.5059684730344094,
      -1.5215585308251873,
      -1.471071465716333,
      -1.5489803676610912,
      -1.515476555699012,
      -1.5640132675121812,
      0.0,
      -1.543916015021376,
      -1.5050918485772888,
      -1.5501155305005325,
      -1.5084150876653555,
      -1.5423840948994991,
      -1.5383487739328838,
      -1.4777685021513105,
      -1.553304871774934,
      -1.516930663823005
    ],
    [
      -1.673665619150929,
      -1.6650808854644763,
      -1.6996870504811143,
      -1.6770191303532085,
      -1.6395518216687055,
      -1.6889620272952424,
      -1.781244566603217,
      -1.6324506465655833,
      -1.675021472644985,
      -1.722292839116487,
      -1.6614822368082323,
      -1.7789086378904941,
      -1.7552166199720953,
      -1.6799558414155769,
      -1.6981897195756281,
      -1.6680443540535892,
      -1.6654379837121878,
      -1.7215823931592389,
      -1.7221726943196027,
      -1.6952162400179522,
      0.0,
      -1.7393645362900818,
      -1.7134213925740471,
      -1.7252524584500648,
      -1.736111048733384,
      -1.6329884838084086,
      -1.7508020755903608,
      -1.7138483978288588,
      -1.7856213802035428
    ],
    [
      -1.3712290545186276,
      -1.421497554532143,
      -1.3730228622377743,
      -1.38539411151815,
      -1.416756859034027,
      -1.4224422809993356,
      -1.4266625357037122,
      -1.3751374303044572,
      -1.4416764676271128,
      -1.3684013063533913,
      -1.3844359712249787,
      -1.466791522422133,
      -1.3608322729467814,
      -1.3960933229405699,
      -1.420977624101867,
      -1.3980992065971984,
      -1.4003023441520532,
      -1.4173064502236647,
      -1.366293823921107,
      -1.3746042572573196,
      -1.400947602840805,
      0.0,
      -1.3918665908011827,
      -1.3488870944391191,
      -1.4178465979789339,
      -1.3633696775835122,
      -1.3788055942241892,
      -1.4210720272706685,
      -1.3444268296202924
    ],
    [
      -1.2928292024044845,
      -1.2743634620641044,
      -1.286893720451265,
      -1.295356985564021,
      -1.2292682162869384,
      -1.3129616285098153,
      -1.3700587135386075,
      -1.2755023421360787,
      -1.308356920663724,
      -1.3015480905867542,
      -1.230872333078168,
      -1.4531087729834489,
      -1.326184208032526,
      -1.2772275050319786,
      -1.2752320681991638,
      -1.3164076602614179,
      -1.2765075269007584,
      -1.3083277232774595,
      -1.3576993674110955,
      -1.330392300438966,
      -1.2533768379303514,
      -1.3747194941650185,
      0.0,
      -1.3163417919840859,
      -1.2897851059182597,
      -1.330113466110246,
      -1.3645533872007256,
      -1.3539319002315886,
      -1.3925849987157475
    ],
    [
      -1.394307427349041,
      -1.338934188481051,
      -1.3006698623041486,
      -1.3562520511689398,
      -1.3459936100116994,
      -1.3750461417377344,
      -1.4073020827266,
      -1.3186020702918335,
      -1.3497541581238028,
      -1.296847579708431,
      -1.3603549663987844,
      -1.4817769977614774,
      -1.3532794711137581,
      -1.307802967898661,
      -1.3647664729199795,
      -1.3667789322365704,
      -1.370287292768476,
      -1.4307823629109435,
      -1.3048645265477825,
      -1.373548653062925,
      -1.3451870978351104,
      -1.366742821442333,
      -1.3797257715258866,
      0.0,
      -1.4037933533224882,
      -1.326997002783028,
      -1.3566072615834812,
      -1.4420977867687836,
      -1.399528696045753
    ],
    [
      -1.512175376606598,
      -1.453448465971524,
      -1.4765172088494445,
      -1.4653732682218696,
      -1.4821151379020305,
      -1.451729219386207,
      -1.4879791017759958,
      -1.4097639044545904,
      -1.4439796416885153,
      -1.4592305083197825,
      -1.45155258997061,
      -1.567786385197999,
      -1.4523541957509454,
      -1.434657446095175,
      -1.4648032916501543,
      -1.464031366085142,
      -1.4499293323397842,
      -1.506069530188918,
      -1.4739777593220842,
      -1.4881333069998637,
      -1.469377712564139,
      -1.4830385250831235,
      -1.4443508770334477,
      -1.4600352456231198,
      0.0,
      -1.4836144946141263,
      -1.4699934480877483,
      -1.5063163980185956,
      -1.5084866191565982
    ],
    [
      -1.518272611024781,
      -1.422473773453801,
      -1.5325340972247592,
      -1.441455619979932,
      -1.523024234963944,
      -1.4950635454309662,
      -1.5379342388034711,
      -1.4052742547508201,
      -1.521793592317299,
      -1.5234688142959543,
      -1.456450049370821,
      -1.5928203984046876,
      -1.425659306610047,
      -1.4124957523824555,
      -1.4680711548521088,
      -1.4577426526348316,
      -1.5072542505672444,
      -1.4503051534724472,
      -1.5837736561208913,
      -1.529823476727719,
      -1.4542920706963802,
      -1.5438504398429658,
      -1.5419581484553255,
      -1.5582445698121083,
      -1.5639703309332835,
      0.0,
      -1.5710911511002055,
      -1.5984009174232048,
      -1.6110973498065428
    ],
    [
      -1.5150517700533817,
      -1.562757944675009,
      -1.4393505057938991,
      -1.5023115812662373,
      -1.499666773044042,
      -1.5312403508022483,
      -1.574186424004917,
      -1.5137591809380613,
      -1.5315333174804846,
      -1.4478319234423118,
      -1.495639623814494,
      -1.5896686542375582,
      -1.5266777131108644,
      -1.5352463303099946,
      -1.541062458562033,
      -1.4745544090382983,
      -1.5077245209778198,
      -1.5221624369072995,
      -1.5237160193106545,
      -1.4113620002270895,
      -1.5305964109787262,
      -1.4781367295032255,
      -1.5001817127643475,
      -1.453833036849868,
      -1.491942313977902,
      -1.5443259701741516,
      0.0,
      -1.5855692146424931,
      -1.4627973664691525
    ],
    [
      -1.3924817390084772,
      -1.4135642535250343,
      -1.4432628515221078,
      -1.3980932318165489,
      -1.3144785184771948,
      -1.458688497658361,
      -1.4494694972869508,
      -1.4066801030233336,
      -1.4055386018839169,
      -1.4136163855007196,
      -1.2966485305455433,
      -1.507750003464538,
      -1.4003384028468657,
      -1.4336928188995632,
      -1.4432694762910463,
      -1.363109617541532,
      -1.376808844909008,
      -1.3746423313496383,
      -1.500490539586893,
      -1.3697939013089588,
      -1.3783516680216736,
      -1.4166011470400484,
      -1.3361694043184245,
      -1.3994221660059643,
      -1.40654833410017,
      -1.4243432803558433,
      -1.472619775844401,
      0.0,
      -1.438413058197382
    ],
    [
      -1.502416127344643,
      -1.5398655300904731,
      -1.4554487378998087,
      -1.4963067928760256,
      -1.5030930457082943,
      -1.5384661491069023,
      -1.5018490540698712,
      -1.4885524288130219,
      -1.5401639699938627,
      -1.4587373722986305,
      -1.5125668864184356,
      -1.554198539616203,
      -1.5122115212346916,
      -1.4877613118487345,
      -1.4958511412693016,
      -1.4968690116706584,
      -1.535760215270768,
      -1.4982589504533201,
      -1.4930345940087069,
      -1.444870912375963,
      -1.5210511135589755,
      -1.3991689020616433,
      -1.493954350108879,
      -1.4575054993089966,
      -1.4985651647335743,
      -1.5027940683286873,
      -1.4365027442012879,
      -1.490808876414035,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.20639689176740816,
      0.21994003102349602,
      0.22987213529208583,
      0.18955859783723383,
      0.17438751149741627,
      0.12617521022404077,
      0.20427193053430237,
      0.20549510857528652,
      0.18884058686311533,
      0.2066130363258134,
      0.0920551925757298,
      0.1943795923805447,
      0.21636471435859628,
      0.18587682734682343,
      0.2007141071636136,
      0.1736451260048688,
      0.20948100129635927,
      0.17199778623100404,
      0.17782947305868202,
      0.20011161868848237,
      0.16731516061741525,
      0.1892204516327407,
      0.19007814638462195,
      0.15407171735136194,
      0.19188850704978222,
      0.1714976911703583,
      0.18163669113997605,
      0.1297023009814824
    ],
    [
      0.3791415212749001,
      0.0,
      0.45563087796582846,
      0.5488414291760133,
      0.4067512957834132,
      0.4338872062733763,
      0.40629697041880064,
      0.43882787441391247,
      0.4749818488117472,
      0.3768165780484578,
      0.48819217105437795,
      0.33762444616610243,
      0.4927281016936549,
      0.5794214775411249,
      0.5097058296656514,
      0.4842677284926924,
      0.41942242458558376,
      0.411098198914305,
      0.40156098070622903,
      0.42493091826122664,
      0.4394490788548553,
      0.3805665414496675,
      0.4070756316314701,
      0.4763464606504777,
      0.3789969001087101,
      0.49289387633521753,
      0.35412779594382604,
      0.33973841997779886,
      0.31736947855012865
    ],
    [
      0.43594362410651266,
      0.36602089557297623,
      0.0,
      0.4298200651617383,
      0.4001072222754314,
      0.39941110441070937,
      0.33699397428067157,
      0.4305357704108752,
      0.41681645541457946,
      0.4516622100552312,
      0.3997619811802313,
      0.2855187533399788,
      0.44082164227286014,
      0.4045319270431693,
      0.40616934033621344,
      0.38677411519613614,
      0.450937740098903,
      0.40241370806105703,
      0.40949034716328714,
      0.4240993653003484,
      0.42884927344809376,
      0.3856745988322532,
      0.3764471120583408,
      0.4207968174748218,
      0.3122854288208128,
      0.36962276483362366,
      0.39924284315134306,
      0.3011238064976538,
      0.34047863441102244
    ],
    [
      0.4286917668666992,
      0.5526301645654108,
      0.4577547255152057,
      0.0,
      0.46421788331815583,
      0.4778527750911983,
      0.3601699635406763,
      0.5527208889515971,
      0.4132901212601432,
      0.4417788478587219,
      0.5857911848275514,
      0.3434825246792459,
      0.6719332550632853,
      0.5990959622602883,
      0.4687635701991171,
      0.5518738017162732,
      0.4068159938758391,
      0.475650857520044,
      0.42813106224741326,
      0.4431615178948738,
      0.44881284619204154,
      0.4085937118683347,
      0.3568335577465507,
      0.45250560412162066,
      0.3422805896233361,
      0.4782240941319307,
      0.35299756163507356,
      0.38163042590125795,
      0.35452613565336466
    ],
    [
      0.39242855621052986,
      0.3899032502159343,
      0.41027612099051947,
      0.4078247021242547,
      0.0,
      0.4073090595633342,
      0.3035610675666105,
      0.43480605376333514,
      0.32818360758949594,
      0.4886727577113923,
      0.4334674691448286,
      0.2860022152980213,
      0.38119161125280043,
      0.4337456467891192,
      0.3664385354627473,
      0.4070737993491904,
      0.40043666151572244,
      0.3586318259732315,
      0.4075175668746467,
      0.361734954864386,
      0.4338597069259891,
      0.34670019452894807,
      0.410782390032568,
      0.37664874546487637,
      0.36260158499366524,
      0.4032455924285294,
      0.3177878524741238,
      0.34420418448712486,
      0.3254508429160565
    ],
    [
      0.3397064576411146,
      0.5095932606356997,
      0.5007958778261474,
      0.49786900072556195,
      0.42599375753216595,
      0.0,
      0.40461467819654384,
      0.45958006991825484,
      0.41151942601655,
      0.4321958853907415,
      0.5191003477350058,
      0.32556462369276185,
      0.5431433648959572,
      0.47060671015910027,
      0.4430259120094979,
      0.45097256624098536,
      0.3948212995407865,
      0.463738188387969,
      0.4460680430507744,
      0.32975700679892284,
      0.4113520115465805,
      0.3656715415776992,
      0.36511303515343263,
      0.37804533848251887,
      0.3222785023915522,
      0.45956332648228826,
      0.342683526903409,
      0.3472831866616606,
      0.2977948296451627
    ],
    [
      0.36052938706488913,
      0.4969117061334922,
      0.4323854819121342,
      0.47845015507987876,
      0.4588919893683725,
      0.46356793666428153,
      0.0,
      0.4946319585191008,
      0.5092416403117852,
      0.43905083160428937,
      0.4787415526568959,
      0.40601894610294,
      0.48910268077582497,
      0.5113125621625072,
      0.4841939699322946,
      0.515371873253291,
      0.4855412465518194,
      0.4431827582505319,
      0.4495346063642489,
      0.38168241004336867,
      0.46183749612577873,
      0.3909489709229783,
      0.41140871683159785,
      0.45847101477059815,
      0.4423339259292398,
      0.42043706922194346,
      0.36082269782442333,
      0.399683197041085,
      0.4177257405921142
    ],
    [
      0.33797873534719014,
      0.43001781581504095,
      0.3979896025487948,
      0.522831360979062,
      0.35956856295384054,
      0.4044487678002222,
      0.3345581573069989,
      0.0,
      0.3718316912015789,
      0.34020695042578253,
      0.4615309007155606,
      0.28642918726239386,
      0.5331470369659679,
      0.4994471499640418,
      0.4274762187145231,
      0.4649917306365512,
      0.3668854957686476,
      0.42149736383332703,
      0.3234411781639761,
      0.3751238322096566,
      0.3771943933399682,
      0.30481231539142484,
      0.34525965817897886,
      0.3160058230518963,
      0.3057193954248121,
      0.45026126814021206,
      0.29570104297325406,
      0.31807878316209504,
      0.3141534761715421
    ],
    [
      0.4475450383135229,
      0.6060424577358214,
      0.5662408291207615,
      0.530782044509045,
      0.5268677402819533,
      0.5512991402760423,
      0.47434569493448286,
      0.5524435296788901,
      0.0,
      0.4533744243715898,
      0.5600313074886683,
      0.39582296178197396,
      0.5370720056804559,
      0.5424505399853756,
      0.5524444362664249,
      0.5675951137378588,
      0.5083268371261069,
      0.49237954025692976,
      0.5428334993386952,
      0.42013457491564665,
      0.5293146263395669,
      0.41034433734593256,
      0.46870509705302665,
      0.5269463539759844,
      0.45278321987180736,
      0.49311576132267554,
      0.43685883390241353,
      0.5110615756793853,
      0.38307611766323446
    ],
    [
      0.40058304748496343,
      0.3804414836863499,
      0.4782959899752077,
      0.3932491406365315,
      0.45777175392571534,
      0.4139080927209411,
      0.2958144506630018,
      0.4148958563759375,
      0.3582376170179029,
      0.0,
      0.3946856500660363,
      0.2574859625354533,
      0.3725544943061767,
      0.38513171375054367,
      0.3210314625564643,
      0.34534679222910114,
      0.4145690937052715,
      0.31456257017521216,
      0.4692404125013787,
      0.3565173541548319,
      0.41232332666485916,
      0.3707297562779208,
      0.35787008151304267,
      0.440142056088773,
      0.3879205886371233,
      0.3934072559050703,
      0.36799664265643384,
      0.314053733509873,
      0.33230272835056707
    ],
    [
      0.4107652233358785,
      0.5231597863655191,
      0.4379179586757558,
      0.5724220601671912,
      0.4991095105467984,
      0.4705972228241697,
      0.36075655782183746,
      0.4794382623394713,
      0.4486793436859584,
      0.4282799811405871,
      0.0,
      0.3434658414708449,
      0.5206994227093549,
      0.49132382849544154,
      0.45466548037357724,
      0.5646730377302986,
      0.40471681622774236,
      0.46580337345238676,
      0.41417786245116095,
      0.3888072783096084,
      0.455897358610889,
      0.34988450943788796,
      0.40709388130513813,
      0.43607876640220455,
      0.3313251576256482,
      0.4632885092805761,
      0.37948003894651494,
      0.4087648814487066,
      0.31070588119338827
    ],
    [
      0.27765214360390944,
      0.32526676062904003,
      0.3241258402772629,
      0.34251782509858364,
      0.33002942063479446,
      0.33076482076888114,
      0.32943710911016466,
      0.3238446174556493,
      0.3185295258037628,
      0.3016540210217058,
      0.3285905749267224,
      0.0,
      0.34728378700428575,
      0.35244875272455434,
      0.33957213501242034,
      0.3209786878695293,
      0.34723613294188405,
      0.2961416534475161,
      0.31734904333481473,
      0.2724456382546758,
      0.3602632242363468,
      0.2893022280160087,
      0.32548555134169765,
      0.3080024368931269,
      0.32956615249281906,
      0.306333000940161,
      0.2494223088331058,
      0.29664930515508914,
      0.31624834370785027
    ],
    [
      0.35839310568106963,
      0.4701533982085564,
      0.45039077371627845,
      0.6267842527256227,
      0.3967202068758331,
      0.48171819573496033,
      0.37135529118897126,
      0.5994157997190794,
      0.39430932249463413,
      0.3900398148182562,
      0.5281607008982157,
      0.33228151083006363,
      0.0,
      0.5772984091626661,
      0.43627138004555865,
      0.5129675536167755,
      0.3884559373737815,
      0.48490810664045725,
      0.4075738669837867,
      0.3691972807756092,
      0.3873124658114817,
      0.36512038109864964,
      0.37406808284499027,
      0.4156941289265814,
      0.37678455127760224,
      0.4755622442300982,
      0.34058180841728447,
      0.32949276703631813,
      0.31287653105069646
    ],
    [
      0.4167023236445524,
      0.5068955050703559,
      0.47087715869075675,
      0.5746531624286246,
      0.41131783856708837,
      0.419418949172063,
      0.3318386423253237,
      0.5326540279902878,
      0.4441190748014141,
      0.4193121660135952,
      0.5206559504900177,
      0.3047084705791401,
      0.553227932602435,
      0.0,
      0.4798778718750183,
      0.5483268537716126,
      0.39530435531717734,
      0.4594683130785553,
      0.40101624792487667,
      0.4139072283191991,
      0.43412912726199115,
      0.33026460217489606,
      0.35903933038579283,
      0.4246976793018138,
      0.32999048500261696,
      0.4807213319250623,
      0.3699399667046064,
      0.3102545849042464,
      0.3053699954346998
    ],
    [
      0.3482527683308345,
      0.486464661838389,
      0.40401948554878886,
      0.5094908552836093,
      0.3703834646732549,
      0.376543612811691,
      0.3762086984343276,
      0.4302544542164193,
      0.43166869035302,
      0.34570773584856584,
      0.45966441262209345,
      0.31959397607954965,
      0.4387905109572874,
      0.49583274655060317,
      0.0,
      0.5288882090453004,
      0.38005391707596514,
      0.3959644787295138,
      0.41219700866496356,
      0.38135987384236714,
      0.39787242683361645,
      0.3098612994945158,
      0.35080195413364623,
      0.37957475248439354,
      0.3518009641246236,
      0.43585133418714084,
      0.32265261695544134,
      0.28342872283828835,
      0.2901483763993751
    ],
    [
      0.4537722583755346,
      0.5093347299002837,
      0.5117523855074455,
      0.6344590713410285,
      0.4926684332309679,
      0.4666103387514333,
      0.3982315010359654,
      0.5523312143680046,
      0.45715345462525714,
      0.4360185464160464,
      0.5851011755361761,
      0.3045533257549824,
      0.5522819416890841,
      0.5545244855119806,
      0.5591570456361474,
      0.0,
      0.4421134915779359,
      0.4400808053831913,
      0.44289987080982884,
      0.481061245818859,
      0.5141754537286449,
      0.38774225801463547,
      0.39742648648930934,
      0.4408397147674663,
      0.39370925235037757,
      0.50246177120097,
      0.4025939121811095,
      0.4129485523289331,
      0.3388391193850888
    ],
    [
      0.3676099123089489,
      0.38589805196169746,
      0.3919567323837323,
      0.3809096512901369,
      0.41311643023876576,
      0.38154658175010625,
      0.2960034917680874,
      0.410786340080064,
      0.4082857518101284,
      0.36760191767179995,
      0.4078944704013394,
      0.27139193922364946,
      0.37570862233546065,
      0.4242938917532755,
      0.41610594943778656,
      0.37949452103255776,
      0.0,
      0.37465083748110817,
      0.30432887354709526,
      0.3348041779919684,
      0.4891947604408575,
      0.3152849156178712,
      0.4251809310711636,
      0.3415755775423863,
      0.3509655628084458,
      0.38650574413555794,
      0.3289866658268945,
      0.3217647617994366,
      0.25451197540257575
    ],
    [
      0.41629400624942603,
      0.46662850156895685,
      0.47541184605192766,
      0.5060534937022623,
      0.449788273093644,
      0.4582390113654977,
      0.39550817969451635,
      0.5292395797637175,
      0.4343120370566409,
      0.36627357847808395,
      0.5261293657446928,
      0.30567853087880215,
      0.5154903365516443,
      0.50607009726567,
      0.4400979122643347,
      0.4717584606829093,
      0.4430351133501058,
      0.0,
      0.3790172017354678,
      0.45299507391543736,
      0.4278844766760179,
      0.3993967637388618,
      0.4091736027595172,
      0.4211990282810987,
      0.3376114371533747,
      0.527906211538665,
      0.39580222154399647,
      0.43063553101702223,
      0.3994313272671446
    ],
    [
      0.2674999800264457,
      0.3282608294678844,
      0.35739903788288596,
      0.316102805964142,
      0.30295627025103733,
      0.36656139729918613,
      0.27421214131880056,
      0.35098097453104593,
      0.29907011662838756,
      0.4234953174505369,
      0.3233988057481927,
      0.2664629351143115,
      0.32287926965774183,
      0.3028533486009741,
      0.2827129382441729,
      0.3145968208973837,
      0.24977677733823467,
      0.23325952944025352,
      0.0,
      0.32384794448606335,
      0.3239544452039562,
      0.3269649943101214,
      0.2695149845112974,
      0.36939579005745093,
      0.26575810406550104,
      0.32580986851036786,
      0.32201781102485305,
      0.2553137552044611,
      0.26806698046737854
    ],
    [
      0.3798353106585002,
      0.3134297874650853,
      0.38291476676811764,
      0.38102803892206527,
      0.3262705411709088,
      0.30669906485956355,
      0.2804899508088248,
      0.39302858218246306,
      0.29394825156665805,
      0.3412675328705437,
      0.3460885449315012,
      0.22670157438533867,
      0.34370863627874737,
      0.3691971024867353,
      0.3536070446959574,
      0.40409410980481164,
      0.32618520786005356,
      0.35968901982213275,
      0.31115230800896354,
      0.0,
      0.3312495604997687,
      0.3700737269438559,
      0.3250500450206122,
      0.36675048785578923,
      0.3327814806216456,
      0.3368168015882609,
      0.3973970733698342,
      0.3218607037462107,
      0.3582349116981398
    ],
    [
      0.4108248280107776,
      0.4194095616972302,
      0.38480339668059216,
      0.407471316808498,
      0.444938625493001,
      0.39552841986646414,
      0.30324588055848944,
      0.45203980059612325,
      0.4094689745167215,
      0.3621976080452196,
      0.42300821035347425,
      0.30558180927121237,
      0.3292738271896112,
      0.40453460574612965,
      0.38630072758607836,
      0.4164460931081173,
      0.41905246344951874,
      0.3629080540024676,
      0.36231775284210377,
      0.38927420714375427,
      0.0,
      0.3451259108716247,
      0.3710690545876594,
      0.3592379887116417,
      0.34837939842832255,
      0.45150196335329795,
      0.3336883715713457,
      0.3706420493328477,
      0.29886906695816373
    ],
    [
      0.4360788781932883,
      0.3858103781797728,
      0.43428507047414167,
      0.42191382119376586,
      0.390551073677889,
      0.38486565171258036,
      0.38064539700820377,
      0.4321705024074587,
      0.36563146508480315,
      0.4389066263585246,
      0.4228719614869372,
      0.34051641028978286,
      0.4464756597651345,
      0.41121460977134605,
      0.3863303086100489,
      0.4092087261147175,
      0.4070055885598627,
      0.39000148248825117,
      0.4410141087908088,
      0.4327036754545963,
      0.4063603298711109,
      0.0,
      0.41544134191073323,
      0.4584208382727968,
      0.38946133473298206,
      0.4439382551284037,
      0.4285023384877267,
      0.3862359054412474,
      0.46288110309162356
    ],
    [
      0.4794270871557602,
      0.4978928274961403,
      0.4853625691089798,
      0.4768993039962237,
      0.5429880732733063,
      0.4592946610504294,
      0.4021975760216372,
      0.496753947424166,
      0.4638993688965207,
      0.4707081989734905,
      0.5413839564820768,
      0.31914751657679585,
      0.4460720815277188,
      0.49502878452826615,
      0.49702422136108093,
      0.45584862929882686,
      0.4957487626594863,
      0.4639285662827852,
      0.4145569221491492,
      0.4418639891212788,
      0.5188794516298934,
      0.39753679539522624,
      0.0,
      0.4559144975761589,
      0.48247118364198505,
      0.44214282344999867,
      0.4077029023595191,
      0.4183243893286561,
      0.37967129084449724
    ],
    [
      0.3012823573066734,
      0.35665559617466336,
      0.3949199223515658,
      0.33933773348677465,
      0.34959617464401505,
      0.32054364291798,
      0.28828770192911435,
      0.3769877143638809,
      0.3458356265319116,
      0.3987422049472835,
      0.33523481825693,
      0.213812786894237,
      0.3423103135419563,
      0.3877868167570535,
      0.33082331173573487,
      0.328810852419144,
      0.3253024918872385,
      0.2648074217447709,
      0.3907252581079319,
      0.32204113159278935,
      0.35040268682060405,
      0.32884696321338147,
      0.3158640131298278,
      0.0,
      0.2917964313332262,
      0.3685927818726864,
      0.3389825230722332,
      0.2534919978869308,
      0.2960610886099615
    ],
    [
      0.39253133313537414,
      0.451258243770448,
      0.4281895008925276,
      0.43933344152010245,
      0.42259157183994156,
      0.45297749035576507,
      0.4167276079659763,
      0.49494280528738166,
      0.46072706805345676,
      0.4454762014221896,
      0.453154119771362,
      0.33692032454397314,
      0.4523525139910267,
      0.470049263646797,
      0.43990341809181777,
      0.44067534365683003,
      0.45477737740218793,
      0.3986371795530541,
      0.4307289504198879,
      0.4165734027421084,
      0.43532899717783313,
      0.4216681846588486,
      0.4603558327085244,
      0.4446714641188523,
      0.0,
      0.4210922151278458,
      0.43471326165422375,
      0.39839031172337647,
      0.3962200905853739
    ],
    [
      0.4110293494289352,
      0.5068281869999152,
      0.3967678632289571,
      0.4878463404737843,
      0.40627772548977226,
      0.43423841502275007,
      0.39136772165024514,
      0.5240277057028961,
      0.40750836813641733,
      0.405833146157762,
      0.47285191108289526,
      0.3364815620490287,
      0.5036426538436694,
      0.5168062080712608,
      0.46123080560160745,
      0.47155930781888467,
      0.4220477098864719,
      0.4789968069812691,
      0.345528304332825,
      0.39947848372599726,
      0.47500988975733605,
      0.3854515206107505,
      0.38734381199839074,
      0.3710573906416079,
      0.36533162952043274,
      0.0,
      0.3582108093535108,
      0.3309010430305115,
      0.3182046106471734
    ],
    [
      0.41600438986598687,
      0.36829821524435946,
      0.4917056541254694,
      0.4287445786531312,
      0.43138938687532646,
      0.3998158091171202,
      0.3568697359144515,
      0.41729697898130724,
      0.3995228424388839,
      0.4832242364770567,
      0.4354165361048745,
      0.3413875056818103,
      0.4043784468085041,
      0.39580982960937394,
      0.3899937013573356,
      0.45650175088107026,
      0.4233316389415487,
      0.40889372301206905,
      0.40734014060871404,
      0.5196941596922791,
      0.4004597489406423,
      0.452919430416143,
      0.430874447155021,
      0.47722312306950054,
      0.4391138459414665,
      0.3867301897452169,
      0.0,
      0.3454869452768754,
      0.46825879345021604
    ],
    [
      0.4294320049450233,
      0.40834949042846613,
      0.3786508924313927,
      0.4238205121369516,
      0.5074352254763057,
      0.3632252462951395,
      0.3724442466665496,
      0.4152336409301669,
      0.4163751420695836,
      0.4082973584527809,
      0.5252652134079572,
      0.31416374048896256,
      0.42157534110663475,
      0.38822092505393724,
      0.3786442676624542,
      0.45880412641196844,
      0.4451048990444926,
      0.44727141260386216,
      0.32142320436660743,
      0.45211984264454164,
      0.44356207593182684,
      0.4053125969134521,
      0.485744339635076,
      0.4224915779475362,
      0.41536540985333037,
      0.39757046359765713,
      0.3492939681090994,
      0.0,
      0.3835006857561185
    ],
    [
      0.3632280401651151,
      0.32577863741928503,
      0.4101954296099495,
      0.3693373746337325,
      0.36255112180146387,
      0.3271780184028559,
      0.3637951134398869,
      0.3770917386967363,
      0.3254801975158954,
      0.4069067952111276,
      0.3530772810913225,
      0.3114456278935551,
      0.35343264627506654,
      0.37788285566102364,
      0.3697930262404565,
      0.3687751558390997,
      0.32988395223899025,
      0.367385217056438,
      0.3726095735010513,
      0.42077325513379504,
      0.34459305395078266,
      0.4664752654481148,
      0.37168981740087914,
      0.4081386682007615,
      0.3670790027761839,
      0.3628500991810708,
      0.42914142330847027,
      0.3748352910957231,
      0.0
    ]
  ],
  "row_avgs": [
    0.1839077551918801,
    0.43059614509819827,
    0.3932982686146027,
    0.45357897836161604,
    0.3828745198754279,
    0.4163732776871017,
    0.44792901864327517,
    0.3816638533731193,
    0.501437058534081,
    0.3786096110025245,
    0.4364992082990905,
    0.31811218005487,
    0.42692456672085266,
    0.42673889913420787,
    0.39333328601277084,
    0.46660149434702447,
    0.36808425139688916,
    0.4388236142639085,
    0.30832585620368097,
    0.3421267916746818,
    0.38096928452787454,
    0.4124801015199477,
    0.4588810849146448,
    0.33063865584037505,
    0.43253455413632447,
    0.420423545758752,
    0.42059592087091985,
    0.413524923227424,
    0.37076441711388686
  ],
  "col_avgs": [
    0.38782726552615554,
    0.4276332527147922,
    0.4261055650458795,
    0.4528094883396571,
    0.41215743468322835,
    0.4043727908705771,
    0.34757688256404284,
    0.448972593557233,
    0.39336150493818306,
    0.40187650214658854,
    0.4469951289475625,
    0.30393929290859434,
    0.44020206175438903,
    0.448688748764677,
    0.4131156302971909,
    0.43669249528626886,
    0.39716194828236523,
    0.3959082854953232,
    0.390206142186489,
    0.3895685463023883,
    0.4157012111253506,
    0.36352105268526497,
    0.3774976157221795,
    0.40489108112561983,
    0.3557344013179645,
    0.4167262544587254,
    0.3567438039412295,
    0.3459969822375996,
    0.33466715917443357
  ],
  "combined_avgs": [
    0.2858675103590178,
    0.42911469890649523,
    0.4097019168302411,
    0.4531942333506366,
    0.39751597727932814,
    0.4103730342788394,
    0.397752950603659,
    0.41531822346517616,
    0.4473992817361321,
    0.3902430565745565,
    0.4417471686233265,
    0.31102573648173215,
    0.43356331423762084,
    0.43771382394944247,
    0.40322445815498087,
    0.45164699481664666,
    0.38262309983962717,
    0.41736594987961584,
    0.349265999195085,
    0.36584766898853505,
    0.39833524782661256,
    0.3880005771026063,
    0.41818935031841215,
    0.36776486848299744,
    0.3941344777271445,
    0.4185749001087387,
    0.3886698624060747,
    0.3797609527325118,
    0.3527157881441602
  ],
  "gppm": [
    578.089844378875,
    602.1632825597435,
    597.5147402080695,
    586.1213576020291,
    602.5897217340259,
    606.9268254701807,
    634.9175511806039,
    586.0513669879232,
    614.9485552957642,
    606.5724135640285,
    590.8658253937093,
    656.9887405189824,
    593.4235808850136,
    589.8989926834613,
    606.6268828744854,
    594.3810635738153,
    609.6656566261516,
    611.0081661782388,
    613.8136905667735,
    612.8299108925974,
    598.7537517468994,
    627.1180142390236,
    621.615684397614,
    610.2969597140154,
    629.8725399198047,
    602.5259455113786,
    629.3648896752065,
    633.9186964211743,
    637.9864739492085
  ],
  "gppm_normalized": [
    1.399349810242244,
    1.401379235041959,
    1.3913909695924807,
    1.367062095567078,
    1.4095697055225946,
    1.418008580281754,
    1.4812140972137928,
    1.3682668966131797,
    1.430796723662215,
    1.4146983658079937,
    1.37548403666171,
    1.540738262003675,
    1.38630128434898,
    1.3742683367251387,
    1.4088988189133915,
    1.3799578587399497,
    1.417058741945673,
    1.4178025488506532,
    1.4258295276532689,
    1.4317463408628597,
    1.381802335384783,
    1.4588577338055677,
    1.4416836474821078,
    1.4254469452217056,
    1.4684465400514013,
    1.4043684129072513,
    1.4605352718788411,
    1.4763988113094406,
    1.4821809562042692
  ],
  "token_counts": [
    675,
    403,
    416,
    436,
    480,
    456,
    430,
    452,
    399,
    429,
    422,
    532,
    453,
    418,
    384,
    385,
    398,
    367,
    383,
    466,
    347,
    394,
    358,
    460,
    422,
    432,
    368,
    403,
    379,
    618,
    437,
    415,
    504,
    453,
    416,
    559,
    458,
    426,
    372,
    401,
    406,
    460,
    425,
    437,
    433,
    470,
    386,
    427,
    439,
    425,
    394,
    391,
    430,
    407,
    408,
    381,
    409,
    381,
    349,
    482,
    471,
    449,
    403,
    484,
    392,
    457,
    439,
    402,
    375,
    517,
    402,
    444,
    419,
    471,
    409,
    399,
    451,
    475,
    436,
    386,
    384,
    402,
    427,
    409,
    423,
    468,
    396,
    739,
    401,
    440,
    454,
    422,
    425,
    376,
    409,
    378,
    429,
    440,
    389,
    446,
    451,
    434,
    418,
    424,
    395,
    445,
    426,
    422,
    379,
    382,
    400,
    371,
    382,
    354,
    381,
    399
  ],
  "response_lengths": [
    3699,
    2335,
    2538,
    2726,
    2579,
    2685,
    2265,
    2444,
    2251,
    2541,
    2624,
    2316,
    2664,
    2650,
    2611,
    2540,
    2518,
    2385,
    2610,
    2451,
    2424,
    2210,
    2211,
    2365,
    2200,
    2130,
    2140,
    2253,
    2350
  ]
}