{
  "example_idx": 30,
  "reference": "Published as a conference paper at ICLR 2023\n\nUNDERSTANDING NEW TASKS THROUGH THE LENS OF TRAINING DATA VIA EXPONENTIAL TILTING\n\nSubha Maity Department of Statistics University of Michigan smaity@umich.edu\n\nMikhail Yurochkin IBM Research MIT-IBM Watson AI lab mikhail.yurochkin@ibm.com\n\nMoulinath Banerjee Department of Statistics University of Michigan moulib@umich.edu\n\nYuekai Sun Department of Statistics University of Michigan yuekai@umich.edu\n\nABSTRACT\n\nDeploying machine learning models on new tasks is a major challenge due to differences in distributions of the train (source) data and the new (target) data. However, the training data likely captures some of the properties of the new task. We consider the problem of reweighing the training samples to gain insights into the distribution of the target task. Specifically, we formulate a distribution shift model based on the exponential tilt assumption and learn train data importance weights minimizing the KL divergence between labeled train and unlabeled target datasets. The learned train data weights can then be used for downstream tasks such as target performance evaluation, fine-tuning, and model selection. We demonstrate the efficacy of our method on WATERBIRDS and BREEDS benchmarks. 1\n\n1\n\nINTRODUCTION\n\nMachine learning models are often deployed in a target domain that differs from the domain in which they were trained and validated in. This leads to the practical challenges of adapting and evaluating the performance of models on a new domain without costly labeling of the dataset of interest. For example, in the Inclusive Images challenge (Shankar et al., 2017), the training data largely consists of images from countries in North America and Western Europe. If a model trained on this data is presented with images from countries in Africa and Asia, then (i) it is likely to perform poorly, and (ii) its performance in the training (source) domain may not mirror its performance in the target domain. However, due to the presence of a small fraction of images from Africa and Asia in the source data, it may be possible to reweigh the source samples to mimic the target domain.\n\nIn this paper, we consider the problem of learning a set of importance weights so that the reweighted source samples closely mimic the distribution of the target domain. We pose an exponential tilt model of the distribution shift between the train and the target data and an accompanying method that leverages unlabeled target data to fit the model. Although similar methods are widely used in statistics Rosenbaum & Rubin (1983) and machine learning Sugiyama et al. (2012) to train and evaluate models under covariate shift (where the decision function/boundary does not change), one of the main benefits of our approach is it allows concept drift (where the decision boundary/function are expected to differ) (Cai & Wei, 2019; Gama et al., 2014) between the source and the target domains. We summarize our contributions below:\n\n• In Section 3 we develop a model and an accompanying method for learning source importance\n\nweights to mimic the distribution of the target domain without labeled target samples.\n\n• In Section 4 we establish theoretical guarantees on the quality of the weight estimates and their\n\nutility in the downstream tasks of fine-tuning and model selection.\n\n1Codes can be found in https://github.com/smaityumich/exponential-tilting.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n• We demonstrate applications of our method on WATERBIRDS (Sagawa et al., 2019) (Section 5),\n\nBREEDS (Santurkar et al., 2020) (Section 6) and synthetic (Appendix C) datasets.\n\n2 RELATED WORK\n\nOut-of-distribution generalization is essential for safe deployment of ML models. There are two prevalent problem settings: domain generalization and subpopulation shift (Koh et al., 2020). Domain generalization typically assumes access to several datasets during training that are related to the same task, but differ in their domain or environment (Blanchard et al., 2011; Muandet et al., 2013). The goal is to learn a predictor that can generalize to unseen related datasets via learning invariant representations (Ganin et al., 2016; Sun & Saenko, 2016), invariant risk minimization (Arjovsky et al., 2019; Krueger et al., 2021), or meta-learning (Dou et al., 2019). Domain generalization is a very challenging problem and recent benchmark studies demonstrate that corresponding methods rarely improve over vanilla empirical risk minimization (ERM) on the source data unless given access to labeled target data for model selection (Gulrajani & Lopez-Paz, 2020; Koh et al., 2020).\n\nSubpopulation shift setting assumes that both train and test data consist of the same groups with different group fractions. This setting is typically approached via distributionally robust optimization (DRO) to maximize worst group performance (Duchi et al., 2016; Sagawa et al., 2019), various reweighing strategies (Shimodaira, 2000; Byrd & Lipton, 2019; Sagawa et al., 2020; Idrissi et al., 2021). These methods require group annotations which could be expensive to obtain in practice. Several methods were proposed to sidestep this limitation, however they still rely on a validation set with group annotations for model selection to obtain good performance (Hashimoto et al., 2018; Liu et al., 2021; Zhai et al., 2021; Creager et al., 2021). Our method is most appropriate for the subpopulation shift setting (see Section 3), however it differs in that it does not require group annotations, but requires unlabeled target data.\n\nModel selection on out-of-distribution (OOD) data is an important and challenging problem as noted by several authors (Gulrajani & Lopez-Paz, 2020; Koh et al., 2020; Zhai et al., 2021; Creager et al., 2021). Xu & Tibshirani (2022); Chen et al. (2021b) propose solutions specific to covariate shift based on parametric bootstrap and reweighing; Garg et al. (2022); Guillory et al. (2021); Yu et al. (2022) align model confidence and accuracy with a threshold; Jiang et al. (2021); Chen et al. (2021a) train several models and use their ensembles or disagreement. Our importance weighting approach is computationally simpler than the latter and is more flexible in comparison to the former, as it allows for concept drift and can be used in downstream tasks beyond model selection as we demonstrate both theoretically and empirically.\n\nDomain adaptation is another closely related problem setting. Domain adaptation (DA) methods require access to labeled source and unlabeled target domains during training and aim to improve target performance via a combination of distribution matching (Ganin et al., 2016; Sun & Saenko, 2016; Shen et al., 2018), self-training (Shu et al., 2018; Kumar et al., 2020), data augmentation (Cai et al., 2021; Ruan et al., 2021), and other regularizers. DA methods are typically challenging to train and require retraining for every new target domain. On the other hand, our importance weights are easy to learn for a new domain allowing for efficient fine-tuning, similar to test-time adaptation methods (Sun et al., 2020; Wang et al., 2020; Zhang et al., 2020), which adjust the model based on the target unlabeled samples. Our importance weights can also be used to define additional regularizers to enhance existing DA methods.\n\nImportance weighting has often been used in the domain adaptation literature on label shift (Lipton et al., 2018; Azizzadenesheli et al., 2019; Maity et al., 2022) and covariate shift (Sugiyama et al., 2007; Hashemi & Karimi, 2018) but the application has been lacking in the area of concept drift models (Cai & Wei, 2019; Maity et al., 2021), due to the reason that it is generally impossible to estimate the weights without seeing labeled data from the target. In this paper, we introduce an exponential tilt model which accommodates concept drift while allowing us to estimate the importance weights for the distribution shift.\n\n3 THE EXPONENTIAL TILT MODEL\n\nNotation We consider a K-class classification problem. Let of inputs and set of possible labels, and P and Q be probability distributions on\n\nX ∈\n\nY\n\nRd and\n\n≜ [K] be the space for the\n\nX × Y\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nsource and target domains correspondingly. A (probabilistic) classifier is a map f : We define p {\nY = k P\n{ k and P Y = k\n\nas the weighted source class conditional density, i.e. p\n\nis the class probability in source. We similarly define q\n\nx, Y = k Y = k\n\n} , where p\n\n| is the density of the source feature distribution in class\n\nX → }\n\nfor target.\n\nx, Y = k\n\nx, Y = k\n\nY = k\n\n∆K−1. = p\n\nx\n\nx\n\n{\n\n}\n\n{\n\n}\n\n{\n\n|\n\n} × {\n\n}\n\n{\n\n}\n\nWe consider the problem of learning importance weights on samples from a source domain so that the weighted source samples mimic the target distribution. We assume that the learner has access to nQ labeled samples i=1 }\nfrom the target domain. The learner’s goal is to estimate a weight function ω(x, y) > 0 such that\n\nnP i=1 from the source domain and and unlabeled samples\n\n(XP,i, YP,i)\n\nXQ,i\n\n}\n\n{\n\n{\n\nE [ω(XP , YP )g(XP , YP )]\n\nE(cid:2)g(XQ, YQ)(cid:3) for all (reasonable) g :\n\n≈\n\nR.\n\n(3.1)\n\nX × Y →\n\nIdeally, ω = dQ dP is the likelihood ratio between the source and target domains (this leads to equality in (3.1)), but learning this weight function is generally impossible without labeled samples from the target domain (David et al., 2010). Thus we must impose additional restrictions on the domains.\n\nThe exponential tilt model We assume that there is a vector of sufficient statistics T : and the parameters {θk\n\nRp, αk\n\nk=1 such that\n\nR\n\nK\n\n∈\n\n∈\n\n} log q{x,Y =k} p{x,Y =k} = θ⊤\n\nk T (x) + αk for all k\n\n[K];\n\n∈\n\nRp\n\nX →\n\n(3.2)\n\nx, Y = k\n\ni.e. q and is a member of the exponential family with base measure p sufficient statistics T . We call (3.2) the exponential tilt model. It implies the importance weights between the source and target samples are\n\nx, Y = k\n\n}\n\n{\n\n}\n\n{\n\nω(x, y) = exp(θ⊤\n\ny T (x) + αy).\n\nModel motivation The exponential tilt model is motivated by the rich theory of exponential families in statistics. In machine learning, it was used for learning with noisy labels and for improving worst-group performance when group annotations are available (Li et al., 2020; 2021). It is also closely related to several common models in transfer learning and domain adaptation. In particular, it implies there is a linear concept drift between the source and target domains. It also extends the widely used covariate shift (Sugiyama & Kawanabe, 2012) and label shift models (Alexandari et al., 2020; Lipton et al., 2018; Azizzadenesheli et al., 2019; Maity et al., 2022; Garg et al., 2020) of distribution shifts. It extends the covariate shift model because the exponential tilt model permits (linear in T (X)) concept drifts between the source and target domains; it extends the label shift model because it allows the class conditionals to differ between the source and target domains. It does, however, come with a limitation: implicit in the model is the assumption that there is some amount of overlap between the source and target domains. In the subpopulation shift setting, this assumption is always satisfied, while in domain generalization it may be violated if the new domain drastically differs from the source data (see Appendix C for a synthetic data example).\n\nChoosing T The goal of T is to identify the common subpopulations across domains, such that\n\n(XP , YP )\n\nT (XP ) = t, YP = k\n\n(XQ, YQ)\n\nT (XQ) = t, YP = k\n\nd\n\n}\n\n≈\n\n| {\n\n| {\n\n. }\n\nT (x) = t, y = k\n\nIf T segments the source domain into its subpopulations (i.e. the subpopulations are\n\n∈ for different values of t’s and k’s), then it is possible to achieve perfect }\nX × Y | reweighing of the source domain with the exponential tilt model: the weight of the T (X) = t, Y = subpopulation is exp(θ⊤ k t + αk). However, in practice, such a T that perfectly segments the k\nsubpopulations may not exist (e.g. the subpopulations may overlap) or is very hard to learn (e.g. we don’t have prior knowledge of the subpopulations to guide T ).\n\n(x, y)\n\n}\n\n{\n\n{\n\nIf no prior knowledge of the domains is available, we can use a neural network to parameterize T and learn its weights along with the tilt parameters, or simply use a pre-trained feature extractor as T , which we demonstrate to be sufficiently effective in our empirical studies. We also study the effects of misspecification of T using a synthetic dataset example in Appendix C.\n\nFitting the exponential tilt model We fit the exponential tilt model via distribution matching. This step is based on the observation that under the exponential tilt model (3.2)\n\nqX\n\nx\n\n}\n\n{\n\n= (cid:80)K\n\nk=1 p\n\n{\n\nx, Y = k\n\n3\n\nexp(θ⊤\n\nk T (x) + αk),\n\n}\n\n(3.3)\n\nPublished as a conference paper at ICLR 2023\n\nwhere qX is the (marginal) density of the inputs in the target domain. an estimate (cid:98)qX of qX from the unlabeled samples p\n{ such that\n\nIt is possible to obtain i=1 and estimates (cid:98)p of the Xi,Q }\n{ m\ni=1. This suggests we find θk’s and αk’s\n\n’s from the labeled samples\n\n(Xi,P , Yi,P )\n\nx, Y = k\n\nx, Y = k\n\n}\n\n{\n\n}\n\n{\n\n}\n\nn\n\n(cid:80)K\n\nk=1 (cid:98)p {\n\nx, Y = k\n\nexp(θ⊤\n\nk T (x) + αk)\n\n≈ (cid:98)qX\n\nx\n\n.\n\n}\n\n{\n\n}\n\n(cid:16)\n\nD\n\nNote that the θk’s and αk’s are dependent because (cid:98)qX must integrate to one. We enforce this restriction as a constraint in the distribution matching problem:\n\n(ˆθk, ˆαk) }\n{\n\nK\n\nk=1 ∈\n\n \n\n\n\narg min{(θk,αk)}K subject to (cid:82) (cid:80)K\n\nk=1\n\nX\n\nk=1 (cid:98)p {\n\n(cid:98)qX\n\nx\n\n}∥ {\nx, Y = k\n\n(cid:80)K\n\nk=1 (cid:98)p {\nexp(θ⊤\n\n}\n\nx, Y = k\n\nexp(θ⊤\n\nk T (x) + αk)\n\n}\n\n(cid:17)\n\nk T (x) + αk)dx = 1 ,\n\n(3.4) where D is a discrepancy between probability distributions on . Although there are many possible choices of D, we pick the Kullback-Leibler (KL) divergence in the rest of this paper because it leads to some computational benefits. We reformulate the above optimization for KL-divergence to relax the constraint which we state in the following lemma. Lemma 3.1. If D is the Kullback-Leibler (KL) divergence then optima in (3.3) is achieved at (ˆθk, ˆαk) }\n{ (ˆθk, ˆα′ {\n\narg max{(θk,α′\n\nk T (X) + α′\n\nk=1 where\n\n(cid:110) (cid:80)K\n\nk=1 ∈\n\nk)}K\n\nlog\n\n(cid:111)(cid:105)\n\nk)\n\n(cid:98)QX\n\nE\n\nX\n\nk=1\n\n}\n\n(cid:104)\n\nK\n\nK\n\nk\n\nk=1 (cid:98)ηP,k(X)exp(θ⊤ (cid:2)exp(θ⊤\n\nlog (cid:8)E\n\n(cid:98)P\n\n−\n\nlog (cid:8)E\n\n(cid:98)P\n\nY T (X) + α′ (cid:2)exp(ˆθ⊤\n\nY )(cid:3)(cid:9) Y T (X) + ˆα′\n\nY )(cid:3)(cid:9).\n\n(cid:98)ηP =\n\n{(cid:98)ηP,k\n\n}\n\nK\n\nk=1 is a probabilistic classifier for P and ˆαk = ˆα′\n\nk −\n\nx, Y = k\n\nOne benefit of minimizing the KL divergence is that the learner does not need to estimate the ’s, a generative model that is difficult to train. They merely need to train a discriminap {\ntive model to estimate (cid:98)ηP from the (labeled) samples from the source domain. We plug the fitted ˆθk’s and ˆαk’s into (3.5) to obtain Exponential Tilt Reweighting Alignment (ExTRA) importance weights:\n\n}\n\nWe summarize the ExTRA procedure in Algorithm 1 in Appendix B.2.\n\n(cid:98)ω(x, y) = exp(ˆθ⊤\n\ny T (x) + ˆαy).\n\n(3.5)\n\nNext we describe two downstream tasks where ExTRA weights can be used:\n\n1. ExTRA model evaluation in the target domain. Practitioners may estimate the target performance of a model in the target domain by reweighing the empirical risk in the source domain:\n\nE [l(f (XQ), YQ)]\n\n1 nP\n\n≈\n\n(cid:80)nP\n\ni=1 l(f (XP,i), YP,i)(cid:98)ω(XP,i, YP,i),\n\n(3.6)\n\nwhere l is a loss function. This allows to evaluate models in the target domain without target labeled samples even in the presence of concept drift between the training and target domain. 2. ExTRA fine-tuning for target domain performance. Since the reweighted empirical risk (in the source domain) is a good estimate of the risk in the target domain, practitioners may fine-tune models for the target domain by minimizing the reweighted empirical risk:\n\n(cid:98)fQ\n\n∈\n\narg minf ∈F E\n\n(cid:98)P [l(f (X), Y )(cid:98)ω(X, Y )] .\n\n(3.7)\n\nWe note that the correctness of (3.4) depends on the identifiability of the θk’s and αk’s from (3.3); i.e. the uniqueness of the parameters that satisfy (3.3). As long as the tilt parameters are identifiable, then (3.4) provides consistent estimates of them. However, without additional assumptions on the ’s and T , the tilt parameters are generally unidentifiable from (3.3). Next we elaborate p\non the identifiability of the exponential tilt model.\n\nx, Y = k\n\n}\n\n{\n\n4 THEORETICAL PROPERTIES OF EXPONENTIAL TILTING\n\n4.1\n\nIDENTIFIABILITY OF THE EXPONENTIAL TILT MODEL\n\nTo show that the θk’s and αk’s are identifiable from (3.3), we must show that there is a unique solution to (3.3). Unfortunately, this is not always the case. For example, consider a linear discriminant\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nanalysis (LDA) problem in which the class conditionals drift between the source and target domains:\n\np\n\nx, Y = k\n\n= πkφ(x\n\nμP,k),\n\nq\n\nx, Y = k\n\n= πkφ(x\n\nμQ,k) ,\n\n{\n\n} where φ is the standard multivariate normal density, πk (0, 1) are the class proportions in both source and target domains, and μP,k’s (resp. the μQ,k’s) are the class conditional means in the source (resp. target) domains. We see that this problem satisfies the exponential tilt model with T (x) = x:\n\n−\n\n−\n\n∈\n\n{\n\n}\n\nlog q{x,Y =k}\n\np{x,Y =k} = (μQ,k\n\nμP,k)⊤x\n\n1\n\n2 ∥\n\n−\n\nμQ,k\n\n2 + 1 2\n∥\n\n2 ∥\n\nμP,k\n\n2 2 .\n\n∥\n\n−\n\nThis instance of the exponential tilt model is not identifiable. Any permutation of the class labels σ : [K]\n\n[K] also leads to the same (marginal) distribution of inputs:\n\n→ (cid:80)K\n\nexp (cid:0)(μQ,k\n\nx, Y = k\n\nk=1 p {\n= (cid:80)K k=1 p\n\n} x, Y = k\n\n{\n\nμP,k)⊤x + 1\n\nμP,k\n\n2\n\n1\n\nμQ,k\n\n−\n\nexp (cid:0)(μQ,σ(k) −\n\n}\n\n2 ∥\n\n∥ μP,k)⊤x + 1\n\n2 −\n\n2 ∥ μP,k\n\n2 ∥\n\n(cid:1)\n\n2 2\n\n∥ 1\n\n2\n\n2 −\n\n∥\n\n2 ∥\n\nμQ,σ(k)∥\n\n(cid:1) .\n\n2 2\n\nFrom this example, we see that the non-identifiability of the exponential tilt model is closely related to the label switching problem in clustering. Intuitively, the exponential tilt model in the preceding example is too flexible because it can tilt any p . Thus there is ambiguity in which p . In the rest of this subsection, we present an identification restriction that guarantees the identifiability of the exponential tilt model.\n\ntilts to which q\n\nx, Y = k\n\nx, Y = l\n\nx, Y = k\n\nx, Y = l\n\nto q\n\n{\n\n}\n\n}\n\n{\n\n}\n\n{\n\n}\n\n{\n\nA standard identification restriction in related work on domain adaptation is a clustering assumption. For example, Tachet et al. (2020) assume there is a partition of k such that X\nsupp(P [K]. This assumption is strong: ⊂ X it implies there is a perfect classifier in the source and target domains. Here we consider a weaker version of the clustering assumption: there are sets\n\ninto disjoint sets\n\nk for all k\n\n), supp(Q\n\nY = k\n\nY = k\n\nk such that\n\n) }\n\n{· |\n\n{· |\n\nX\n\n∈\n\n}\n\nS Y = k\n\nX\n\nk\n\n= 1.\n\n} k’s; this permits the supports of P\n\n∈ S\n\nP\n\nY = k\n\nX\n\nk\n\n= Q\n\n|\n\n}\n\n{\n\n{· |\n\n∈ S\n\n{ |\nk’s can be much smaller than the to overlap.\n\nWe note that the S\nand P Y = l }\nDefinition 4.1 (anchor set). A set k\nk. p\nProposition 4.2 (identifiability from anchor sets). If there are anchor sets the source domain) and T ( satisfies (3.3).\n\n= k for all x\n\nis an anchor set for class k if p\n\nx, Y = l\n\nS ∈ S\n\n= 0, l\n\n⊂ X\n\nX\n\nS\n\n}\n\n{\n\nk for all K classes (in k) is p-dimensional, then there is at most one set of θk’s and αk’s that\n\nS\n\nY = k\n\n}\n\n{· |\n\nx, Y = k\n\n> 0 and\n\n}\n\n{\n\nThis identification restriction is also closely related to the linear independence assumption in Gong et al. (2016). Inspecting the proof of proposition 4.2 (see Appendix A.3), we see that the anchor set k=1 for any set of θk’s and assumption implies linear independence of αk’s. We study the anchor set assumption empirically in a synthetic experiment in Appendix C. Our experiments show that the assumption is mild and is violated only under extreme data scenarios.\n\nk T (x) + αk)\n\npk(x)exp(θ⊤\n\n}\n\n{\n\nK\n\n4.2 CONSISTENCY IN ESTIMATION OF THE TILT PARAMETERS\n\nHere, we establish a convergence rate for the estimated tilt parameters (Lemma (3.1)) and the ExTRA importance weights (Equation (3.1)). To simplify the notation, we define S(x) = (1, T (x)⊤)⊤ as the extended sufficient statistics for the exponential tilt and denote the corresponding tilt param- ⊤)⊤’s be the true values of the tilt parameters ξk’s eters as ξk = (αk, θ⊤ RK(p+1) be the long vector containing all the tilt parameters. We and let ξ = (ξ⊤ recall that estimating the parameters from the optimization stated in Lemma 3.1 requires a classifier (cid:98)ηP on the source data. So, we define our objective for estimating ξ through a generic classifier ∆K. Denoting ηk(x) as the k-th co-ordinate of η(x) we define the expected log-likelihood η : X → objective as:\n\nk )⊤. We let ξ⋆\n\n1 , . . . , ξ⊤\n\nk = (α⋆\n\nk, θ⋆\n\nK)⊤\n\n∈\n\nk\n\nL(η, ξ) = EQX [log\n\nand its empirical version as\n\nˆL(η, ξ) = E\n\n(cid:98)QX\n\n[log\n\n{\n\n{\n\n(cid:80)K\n\nk=1 ηk(X)exp(ξ⊤\n\nk S(X))\n\n(cid:80)K\n\nk=1 ηk(X)exp(ξ⊤\n\nk S(X))\n\n]\n\n]\n\n−\n\n−\n\n}\n\n}\n\nlog[EP\n\n{\n\nexp(ξ⊤\n\nY S(X))\n\nlog[E\n\nexp(ξ⊤\n\nY S(X))\n\n(cid:98)P {\n\n] ,\n\n}\n\n] .\n\n}\n\nTo establish the consistency of MLE we first make an assumption that the loss ξ strongly convex at the true parameter value.\n\n(cid:55)→ −\n\nL(η⋆\n\nP , ξ) is\n\n5\n\n̸ Published as a conference paper at ICLR 2023\n\nAssumption 4.3. The loss ξ μ > 0 such that for any ξ it holds:\n\n(cid:55)→ −\n\nL(η⋆\n\nP , ξ) is strongly convex at ξ⋆, i.e., there exists a constant\n\nL(η⋆\n\nP , ξ)\n\n−\n\nL(η⋆\n\nP , ξ⋆)\n\n≥ −\n\n∂ξL(η⋆\n\nP , ξ⋆)⊤(ξ\n\nξ⋆) + μ\n\n2 ∥\n\nξ\n\nξ⋆\n\n2 2 . ∥\n\n−\n\n−\n\n−\n\nWe note that the assumption is a restriction on the distribution Q rather than the objective itself. For technical convenience we next assume that the feature space is bounded.\n\nAssumption 4.4.\n\nX\n\nis bounded, i.e., there exists an M > 0 such that\n\nB(0, M ).\n\nX ⊂\n\nRecall, from Lemma 3.1, that we need a fitted source classifier (cid:98)ηP to estimate the tilt parameter: ξ⋆ is estimated by maximizing ˆL((cid:98)ηP , ξ) rather than the unknown ˆL(η⋆ P , ξ). While analyzing the convergence of ˆξ we are required to control the difference ˆL(ˆηP , ξ) P , ξ). To ensure the difference is small, assume the pilot estimate of the source regression function (cid:98)ηP is consistent at some rate rnP .\n\nˆL(η⋆\n\n−\n\nAssumption 4.5. Let f ⋆ an estimators\n\nˆfP,k(x) }\n\n{\n\nc > 0 and a sequence rnP →\n\nP,k(x) = log f ⋆ k=1 for\n\nK\n\n{ P,k(x)\n\nη⋆\n\nP,k(x)\n\n{\n\n}\n\n0 such that for almost surely [PX ] it holds\n\nˆfP (x)\n\nP(\n\n∥\n\n−\n\nf ⋆\n\nP (x)\n\n∥\n\n2 > t)\n\nexp(\n\n−\n\n≤\n\nct2/r2\n\nnP ), t > 0 .\n\n1 K\n\n(cid:80)K\n\nj=1 log\n\nη⋆\n\nP,j(x)\n\nK\n\n} −\n\n} k=1 such that the following holds: there exists a constant\n\n{\n\n. We assume that there exist\n\nK\n\n{\n\n}\n\n(cid:80)K\n\nj=1 exp( ˆfP,j(x))\n\nˆfP,k(x) k=1 to construct the regression functions as (cid:98)ηP,k(x) = We use the estimated logits }\n{ exp( ˆfP,k(x))/ , which we use in the objective stated in Lemma 3.1 to analyze the convergence of the tilt parameter estimates and the ExTRA weights. With the above assumptions we’re now ready to state concentration bounds for ˆξ ω⋆, where the true importance weight ω⋆ is defined as ω⋆(x, y) = exp(ξ⋆ Theorem 4.6. Let the assumptions 4.3, 4.4 and 4.5 hold. For the sample sizes nP , nQ define 1/2. There exists constants 1/2 + αnP ,nQ = rnP }\n} k1, k2 > 0 such that for any δ > 0 with probability at least 1\n\n(2K + 1)δ the following hold:\n\n(cid:112)log(nQ) +\n\n(p + 1)K/nQ\n\n(p + 1)K/nP\n\nξ⋆ and ˆω\n\n⊤S(x)).\n\n−\n\n−\n\n{\n\n{\n\ny\n\nˆξ ∥\n\n−\n\nξ⋆\n\n2 ∥\n\n≤\n\nk1αnP ,nQ\n\n(cid:112)\n\nlog(1/δ), and\n\nω⋆\n\nˆω\n\n∥\n\n−\n\n1,P\n\n∥\n\n≤\n\nk2αnP ,nQ\n\n(cid:112)\n\nlog(1/δ).\n\n−\n\nIn Theorem 4.6 we notice that as long as rnP log(nQ) 0. This implies both the estimated tilt parameters and the ExTRA weights converge to their true values as the sample sizes nP , nQ\n\nwe have αnP ,nQ →\n\n0 for nP , nQ\n\n→ ∞\n\n→\n\n. → ∞\n\nWe next provide theoretical guarantees for the downstream tasks (1) fine-tuning and (2) target performance evaluation that we described in Section 3.\n\nFine-tuning We establish a generalization bound for the fitted model (3.7) using weighted-ERM on source domain. We denote and a weight function ω :\n\nR≥0 define the weighted loss function and its empirical version on source data as:\n\nas the classifier hypothesis class. For f\n\n∈ F\n\nF\n\nX × Y →\n\nL\n\nP (f, w) = EP [ω(X, Y )l(f (X), Y )], ˆ L\nWe also define the loss function on the target data as: is the true value of the tilt parameters in (3.2), i.e., the following holds:\n\nP (f, w) = E\n\nQ(f ) = EQ\n\nL\n\n(cid:98)P [ω(X, Y )l(f (X), Y )] . (θ⋆\n\n(cid:2)l(f (X), Y )(cid:3) . If\n\nk, α⋆ k)\n\nK k=1\n\n}\n\n{\n\nq\n\nx, Y = k\n\n= p\n\nx, Y = k\n\nexp\n\nα⋆\n\nk + (θ⋆\n\nk)⊤T (x) }\n\n; k\n\n[K] ,\n\n} then defining ω⋆(x, k) = exp\n\n{ as the true weight we notice that Q(f ), which is easily observed by setting g(x, y) = l(f (x), y) in the display (3.1).\n\nk)⊤T (x) }\n\n{ k + (θ⋆\n\nα⋆\n\n∈\n\n{\n\n{\n\n}\n\nL To establish our generalization bound we require Rademacher complexity (Bartlett & Mendelson, 2002) (denoted as ); see Appendix A.1 for details) and the following assumption on the loss function.\n\nnP (\n\nR\n\nG\n\nP (f, ω⋆) =\n\nL\n\nAssumption 4.7. The loss function l is bounded, i.e., for some B > 0, f\n\nand y\n\n[K].\n\n, x\n\n∈ F\n\n∈ X\n\n∈\n\nf (x), y\n\nl |\n\n{\n\n}| ≤\n\nB for any\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nWith the above definitions and the assumption we establish our generalization bound.\n\nLemma 4.8. For a weight function ω and the source samples ˆfω = arg minf ∈F ˆ\nL bound holds with probability at least 1\n\nnP i=1 of size nP let P (f, ω). There exists a constant c > 0 such that the following generalization\n\n(XP,i, YP,i)\n\n}\n\n{\n\nδ\n\nQ( ˆfω)\n\nL\n\n−\n\nminf ∈F\n\nQ(f )\n\n2\n\n≤\n\nnP (\n\nR\n\nG\n\nL\n\n−\n\nwhere G\nR Appendix A.1.\n\nnP (\n\n) is the Rademacher complexity of\n\n=\n\nG\n\n{\n\n) + B\n\nω\n\nω⋆\n\n1,P + c\n\n∥\n\n−\n\n∥ ω⋆(x, y)l(f (x), y) : f\n\n(cid:113) log(1/δ) nP\n\n,\n\n(4.1)\n\ndefined in\n\n∈ F}\n\nIn Theorem 4.6 we established an upper bound for the estimated weights ˆω, which concludes that ˆfˆω has the following generalization bound: for any δ > 0, with probability at least 1\n\n(2K + 2)δ\n\nQ( ˆfˆω)\n\nL\n\n−\n\nminf ∈F\n\nQ(f )\n\n2\n\n≤\n\nnP (\n\nR\n\nG\n\nL\n\n) + k2αnP ,nQ\n\n(cid:112)\n\nlog(1/δ) + c\n\n(cid:112)\n\n− log(1/δ)/nP ,\n\nwhere k2 is the constant in Theorem 4.6 and c is the constant in Lemma 4.8. The generalization bound implies that for large sample sizes (nP , nQ ) the target accuracy of weighted ERM on source data well approximates the accuracy of ERM on target data.\n\n→ ∞\n\nTarget performance evaluation We provide a theoretical guarantee for the target performance R\nevaluation (3.6) using our importance weights. Here we only consider the functions g : which are bounded by some B > 0, i.e. . The simplest and | ≤ the most frequently used example is the model accuracy which uses 0-1-loss as the loss function: for a model f the loss g(x, y) = I is bounded with B = 1. For such functions we {\nnotice that EQ[g(X, Y )] = EP [g(X, Y )ω⋆(X, Y )], as observed in display (3.1). This implies the following bound on the target performance evaluation error EP [g(X, Y )ˆω(X, Y )](cid:12)\n\n(cid:12) (cid:12)EQ[g(X, Y )]\n\ng(x, y) |\n\nB for all x\n\nf (x) = y\n\nX × Y →\n\n(cid:12) = (cid:12)\n\nand y\n\n∈ X\n\n∈ Y\n\n}\n\n(cid:12)EP [g(X, Y )ω⋆(X, Y )] ˆω⋆(X, Y ) BEP [ |\n\n− ω⋆(X, Y ) |\n\n−\n\nEP [g(X, Y )ˆω(X, Y )](cid:12) (cid:12) 1,P . ˆω\n\nω⋆\n\nB\n\n]\n\n≤\n\n∥\n\n−\n\n∥\n\n≤\n\nω⋆ We recall the concentration bound for mated target performance in (3.6) converges to the true target performance at rate αnP ,nQ .\n\n1,P from Theorem 4.6 and conclude that the esti-\n\n−\n\nˆω\n\n∥\n\n∥\n\n−\n\n5 WATERBIRDS CASE STUDY\n\nTo demonstrate the efficacy of the ExTRA algorithm for reweighing the source data we (i) verify the ability of ExTRA to upweigh samples most relevant to the target task; (ii) evaluate the utility of weights in downstream tasks such as fine-tuning and (iii) model selection.\n\nWATERBIRDS dataset combines bird photographs from the Caltech-UCSD Birds-200-2011 (CUB) dataset (Wah et al., 2011) and the image backgrounds from the Places dataset (Zhou et al., 2017). The birds are labeled as one of = {water background, land background}. The images are divided into four groups: landbirds on land (0); landbirds on water (1); waterbirds on land (2); waterbirds on water (3). The source dataset is highly imbalanced, i.e. the smallest group (2) has 56 samples. We embed all images with a pre-trained ResNet18 (He et al., 2016). See Appendix B.1 for details.\n\n= {waterbird, landbird} and placed on one of\n\nA\n\nY\n\nWe consider five subpopulation shift target domains: all pairs of domains with different bird types and the original test set (Sagawa et al., 2019) where all 4 groups are present with proportions vastly different from the source. For all domains, we fit ExTRA weights (with ResNet18 features as T (x)) from 10 different initializations and report means and standard deviations for the metrics. See Appendix B.2 for the implementation details.\n\n}\n\n1, 2\n\nconsisting only of birds appearing on their atypical backgrounds. Groups\n\nExTRA weights quality For a given target domain it is most valuable to upweigh the samples in the source data corresponding to the groups comprising that domain. The most challenging is the target correspond {\nto 5% of the source data making them most difficult to “find”. To quantify the ability of ExTRA to upweigh these samples we report precision (proportion of samples from groups within the top x% of the weights) and recall (proportion of samples within the top x% of the weights) {\nin Figure 1. We notice that samples corresponding to 10% largest ExTRA weights contain slightly\n\n1, 2\n\n1, 2\n\n1, 2\n\n}\n\n}\n\n{\n\n{\n\n}\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Model selection results on WATERBIRDS\n\ntarget accuracy\n\nrank correlation\n\ntarget groups\n\nExTRA\n\nSrcVal ATC-NE\n\nExTRA\n\nSrcVal ATC-NE\n\n{0, 2} {1, 2} {0, 3} {1, 3} {0, 1, 2, 3}\n\naverage\n\n0.819±0.012 0.741±0.047 0.978±0.001 0.757±0.011 0.856±0.034\n\n0.83\n\n0.854 0.616 0.978 0.737 0.803\n\n0.798\n\n0.871 0.646 0.976 0.747 0.818\n\n0.812\n\n0.419±0.01 0.747±0.106 0.962±0.004 0.361±0.168 0.658±0.295\n\n0.753\n\n0.807 -0.519 0.956 -0.318 0.263\n\n0.166\n\n0.760 -0.590 0.906 -0.411 0.178\n\n0.110\n\nover 80% of the groups in the source data (recall). This demonstrates the ability of ExTRA to upweigh relevant samples. We present examples of upweighted images and results for other target domains in Appendix B.3.\n\n1, 2\n\n{\n\n}\n\nModel fine-tuning We demonstrate the utility of ExTRA weights in the fine-tuning (3.7). The basic goal of such importance weighing is to improve the performance in the target in comparison to training on uniform source weights S -> T, i.e. ERM. Another baseline is the DRO model (Hashimoto et al., 2018) that aims to maximize worst-group performance without access to the group labels, and JTT (Liu et al., 2021) that retrains a model after upweighting the misclassified samples by ERM. We consider two additional baselines that utilize group annotations to improve worst-group performance: re-weighing the source to equalize group proportions (RWgr) and group DRO (gDRO) (Sagawa et al., 2019). The aforementioned baselines do not try to adjust to the target domain. Finally, we compare to πT -> T that fine-tunes the model only using the subset of the source samples corresponding to the target domain groups. In all cases we use logistic regression as model class.\n\nFigure 1: ExTRA precision and recall for samples with top x% weights.\n\nWe compare target accuracy across domains in Figure 2. Analogous comparison with area under the receiver operator curve can be found in Figure 6 in Appendix B.3.1. Model trained with ExTRA weights outperforms all “fair” baselines and matches the performance of the three baselines that had access to additional information. In all target domains ExTRA fine-tuning is comparable with the πT -> T supporting its ability to upweigh relevant samples. Notably, on {1,2} domain of both minority groups and on {0,3} domain of both majority groups, ExTRA outperforms RWgr and gDRO that utilize group annotations. This emphasizes the advantage of adapting to the target domain instead of pursuing a more conservative goal of worst-group performance maximization. Finally, we note that ExTRA fine-tuning did not perform as well on the domain {1,3}, however neither did πT -> T.\n\nFigure 2: Performance on WATERBIRDS.\n\nModel selection out-of-distribution is an important task, that is difficult to perform without target data labels and group annotations (Gulrajani & Lopez-Paz, 2020; Zhai et al., 2021). We evaluate the ability of choosing a model for the target domain based on accuracy on the ExTRA reweighted source validation data. We compare to the standard source validation model selection (SrcVal) and to the recently proposed ATC-NE (Garg et al., 2022) that uses negative entropy of the predicted probabilities on the target domain to score models. We fit a total of 120 logistic regression models with different weighting (uniform, label balancing, and group balancing) and varying regularizers. See Appendix B.2 for details.\n\nIn Table 1 we compare the target performance of models selected using each of the model evaluation scores and rank correlation between the corresponding model scores and true target accuracies.\n\n8\n\n10−210−1100topx%weights0.20.40.60.8precisionforgroups{1,2}groups{1,2}precisionrecall0.20.40.60.81.0recallforgroups{1,2}{0,2}{1,2}{0,3}{1,3}{0,1,2,3}averagetargetdomaingroups0.00.20.40.60.81.0targetaccuracyRWgrS→TExTRADROgGROJTTπT→TPublished as a conference paper at ICLR 2023\n\nModel selection with ExTRA results in the best target performance and rank correlation on 4 out of 5 domains and on average. Importantly, the rank correlation between the true performance and ExTRA model scores is always positive, unlike the baselines, suggesting its reliability in providing meaningful information about the target domain performance.\n\n6 BREEDS CASE STUDY\n\nBREEDS (Santurkar et al., 2020) is a subpopulation shift benchmark derived from ImageNet (Deng et al., 2009). It uses the class hierarchy to define groups within classes. For example, in the Entity30 task considered in this experiment, class fruit is represented by strawberry, pineapple, jackfruit, Granny Smith in the source and buckeye, corn, ear, acorn in the target. This is an extreme case of subpopulation shift where source and target groups have zero overlap. We modify the dataset by adding a small fraction π of random samples from the target to the source for two reasons: (i) our exponential tilt model requires some amount of overlap between source and target; (ii) arguably, in practice, it is more likely that the source dataset has at least a small representation of all groups.\n\nOur goal is to show that ExTRA can identify the target samples mixed into the source for efficient fine-tuning. We obtain feature representations from a pre-trained selfsupervised SwAV (Caron et al., 2020). To obtain the ExTRA weights we use SwAV features as sufficient statistic. We then train logistic regression models on (i) the source dataset re-weighted with ExTRA, (ii) uniformly weighted source (S -> T), (iii) target samples mixed into the source (πT -> T), (iv) all target samples (oracle). See Appendix B.1, B.2 for details. We report performance for varying mixing proportion π in Figure 3. First, we note that even when π = 0, i.e. source and target have completely disjoint groups (similar to domain generalization), ExTRA improves over the vanilla S -> T. Next, we see that S -> T improves very slowly in comparison to ExTRA as we increase the mixing proportion; πT -> T improves faster as we increase the number of target samples it has access to, but never suppresses ExTRA and matches its improvement slope for the larger π values. We conclude that ExTRA can effectively identify target samples mixed into source that are crucial for the success of fine-tuning and find source samples most relevant to the target task allowing it to outperform πT -> T. We report analogous precision and recall for the WATERBIRDS experiment in Appendix B.3.\n\nFigure 3: Performance on BREEDS.\n\n7 CONCLUSION\n\nIn this paper, we developed an importance weighing method for approximating expectations of interest on new domains leveraging unlabeled samples (in addition to a labeled dataset from the source domain). We demonstrated the applicability of our method on downstream tasks such as model evaluation/selection and fine-tuning both theoretically and empirically. Unlike other importance weighing methods that only allow covariate shift between the source and target domains, we permit concept drift between the source and target. Though we demonstrate the efficacy of our method in synthetic setup of concept drift (Appendix C), in a future research it would be interesting to investigate the performance in more realistic setups (e.g. CIFAR10.2 to CIFAR10.2 (Lu et al., 2020), Imagenet to Imagenetv2 (Recht et al., 2019)).\n\nDespite its benefits, the exponential tilt model does suffer from a few limitations. Implicit in the exponential tilt assumption is that the supports of the target class conditionals have some overlap with the corresponding source class conditionals. Although this assumption is likely satisfied in many instances of domain generalization problems (and is always satisfied in the subpopulation shift setting), an interesting avenue for future studies is to accommodate support alignment in the distribution shift model, i.e. to align the supports for class conditioned feature distributions in source and target domains. One way to approach this is to utilize distribution matching techniques from domain adaptation literature (Ganin et al., 2016; Sun & Saenko, 2016; Shen et al., 2018), similarly to Cai et al. (2021). We hope aligning supports via distribution matching will allow our method to succeed on domain generalization problems where the support overlap assumption is violated.\n\n9\n\n0.00.00250.0050.010.020.040.080.16mixingproportionπ0.50.60.70.80.9targetaccuracyS→TπT→TExTRAoraclePublished as a conference paper at ICLR 2023\n\n8 ETHICS STATEMENT\n\nWe recommend considering the representation of the minority groups when applying ExTRA in the context of fairness-sensitive applications. The goal of ExTRA is to approximate the distribution of the target domain, thus, in order to use ExTRA weights for fine-tuning or model selection to obtain a fair model, the target domain should be well representative of both privileged and unprivileged groups. If the target domain has miss/under-represented groups, a model obtained using ExTRA weights may be biased.\n\nACKNOWLEDGMENTS\n\nThis paper is based upon work supported by the National Science Foundation (NSF) under grants no. 2027737 and 2113373.\n\nREFERENCES\n\nAmr Alexandari, Anshul Kundaje, and Avanti Shrikumar. EM with Bias-Corrected Calibration is\n\nHard-To-Beat at Label Shift Adaptation. arXiv:1901.06852 [cs, stat], January 2020.\n\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimiza-\n\ntion. arXiv:1907.02893 [cs, stat], September 2019.\n\nKamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized Learn-\n\ning for Domain Adaptation under Label Shifts. arXiv:1903.09734 [cs, stat], March 2019.\n\nPeter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and\n\nstructural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.\n\nGilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS’11, pp. 2178–2186, Red Hook, NY, USA, December 2011. Curran Associates Inc. ISBN 978-1-61839-599-3.\n\nJonathon Byrd and Zachary Lipton. What is the Effect of Importance Weighting in Deep Learning?\n\nIn International Conference on Machine Learning, pp. 872–881. PMLR, May 2019.\n\nT. Tony Cai and Hongji Wei. Transfer Learning for Nonparametric Classification: Minimax Rate\n\nand Adaptive Classifier. arXiv:1906.02903 [cs, math, stat], June 2019.\n\nTianle Cai, Ruiqi Gao, Jason D. Lee, and Qi Lei. A Theory of Label Propagation for Subpopulation\n\nShift. arXiv:2102.11203 [cs, stat], February 2021.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912–9924, 2020.\n\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and estimating accuracy on unlabeled data with self-training ensembles. Advances in Neural Information Processing Systems, 34, 2021a.\n\nMayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and Christopher Ré. Mandoline: Model evaluation under distribution shift. In International Conference on Machine Learning, pp. 1617–1629. PMLR, 2021b.\n\nElliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment Inference for Invariant Learning. In Proceedings of the 38th International Conference on Machine Learning, pp. 2189– 2200. PMLR, July 2021.\n\nShai Ben David, Tyler Lu, Teresa Luu, and David Pal. Impossibility Theorems for Domain Adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129–136. JMLR Workshop and Conference Proceedings, March 2010.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nQi Dou, Daniel C. Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain Generalization via\n\nModel-Agnostic Learning of Semantic Features. arXiv:1910.13580 [cs], October 2019.\n\nJohn Duchi, Peter Glynn, and Hongseok Namkoong. Statistics of Robust Optimization: A General-\n\nized Empirical Likelihood Approach. arXiv:1610.03425 [stat], October 2016.\n\nJoão Gama, Indr ̇e Žliobait ̇e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept drift adaptation. ACM Computing Surveys, 46(4):44:1–44:37, March 2014. ISSN 0360-0300. doi: 10.1145/2523813.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, January 2016. ISSN 1532-4435.\n\nSaurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C. Lipton. A Unified View of Label\n\nShift Estimation. arXiv:2003.07554 [cs, stat], March 2020.\n\nSaurabh Garg, Sivaraman Balakrishnan, Zachary C Lipton, Behnam Neyshabur, and Hanie Sedghi. Leveraging unlabeled data to predict out-of-distribution performance. arXiv preprint arXiv:2201.04234, 2022.\n\nMingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Scholkopf. Domain Adaptation with Conditional Transferable Components. In Proceedings of Machine Learning Research, volume 48, pp. 17, New York, New York, USA, June 2016.\n\nDevin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig Schmidt. Predicting with confidence on unseen distributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1134–1144, 2021.\n\nIshaan Gulrajani and David Lopez-Paz. In Search of Lost Domain Generalization. In International\n\nConference on Learning Representations, September 2020.\n\nMahdi Hashemi and Hassan Karimi. Weighted machine learning. Statistics, Optimization and\n\nInformation Computing, 6(4):497–525, 2018.\n\nTatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness With-\n\nout Demographics in Repeated Loss Minimization. arXiv:1806.08010 [cs, stat], June 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, Las Vegas, NV, USA, June 2016. IEEE. ISBN 978-1-4673-8851-1. doi: 10.1109/ CVPR.2016.90.\n\nBadr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. arXiv preprint arXiv:2110.14503, 2021.\n\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\n\nsgd via disagreement. arXiv preprint arXiv:2106.13799, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980\n\n[cs], January 2017.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A Benchmark of in-the-Wild Distribution Shifts. arXiv:2012.07421 [cs], December 2020.\n\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815–5826. PMLR, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAnanya Kumar, Tengyu Ma, and Percy Liang. Understanding Self-Training for Gradual Domain\n\nAdaptation. arXiv:2002.11361 [cs, stat], February 2020.\n\nAnanya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. February 2022. doi: 10.48550/arXiv.2202.10054.\n\nTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted Empirical Risk Minimization.\n\nIn International Conference on Learning Representations, September 2020.\n\nTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. On tilted losses in machine learning:\n\nTheory and applications. arXiv preprint arXiv:2109.06141, 2021.\n\nZachary C. Lipton, Yu-Xiang Wang, and Alex Smola. Detecting and Correcting for Label Shift with\n\nBlack Box Predictors. arXiv:1802.03916 [cs, stat], July 2018.\n\nEvan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In Proceedings of the 38th International Conference on Machine Learning, pp. 6781–6792. PMLR, July 2021.\n\nShangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi, Yair Carmon, and Ludwig Schmidt. Harder or different? a closer look at distribution shift in dataset reproduction. In ICML Workshop on Uncertainty and Robustness in Deep Learning, 2020.\n\nSubha Maity, Diptavo Dutta, Jonathan Terhorst, Yuekai Sun, and Moulinath Banerjee. A linear adjustment based approach to posterior drift in transfer learning. arXiv:2111.10841 [stat], December 2021.\n\nSubha Maity, Yuekai Sun, and Moulinath Banerjee. Minimax optimal approaches to the label shift problem in non-parametric settings. Journal of Machine Learning Research, 23(346):1–45, 2022.\n\nKrikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain Generalization via Invariant\n\nFeature Representation. January 2013.\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers In International Conference on Machine Learning, pp. 5389–5400.\n\ngeneralize to imagenet? PMLR, 2019.\n\nPaul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational\n\nstudies for causal effects. Biometrika, 70(1):41–55, 1983.\n\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm arXiv preprint\n\nmay already learn features sufficient for out-of-distribution generalization. arXiv:2202.06856, 2022.\n\nYangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift.\n\narXiv preprint arXiv:2201.00057, 2021.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. arXiv:1911.08731 [cs, stat], November 2019.\n\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An Investigation of Why Overparameterization Exacerbates Spurious Correlations. arXiv:2005.04345 [cs, stat], August 2020.\n\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. BREEDS: Benchmarks for Subpopu-\n\nlation Shift. August 2020. doi: 10.48550/arXiv.2008.04859.\n\nShreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D. Sculley. No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. arXiv:1711.08536 [stat], November 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for domain adaptation. In Thirty-second AAAI conference on artificial intelligence, 2018.\n\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, October 2000. ISSN 0378-3758. doi: 10.1016/S0378-3758(00)00115-4.\n\nAvanti Shrikumar and Anshul Kundaje. Calibration with bias-corrected temperature scaling improves domain adaptation under label shift in modern neural networks. Preprint at https://arxiv. org/abs/1901.06852 v1, 2019.\n\nRui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T Approach to Unsupervised Domain Adaptation. In International Conference on Learning Representations, February 2018.\n\nMasashi Sugiyama and Motoaki Kawanabe. Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation. Adaptive Computation and Machine Learning Series. MIT Press, Cambridge, MA, USA, March 2012. ISBN 978-0-262-01709-1.\n\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert Müller. Covariate Shift Adaptation by Importance Weighted Cross Validation. The Journal of Machine Learning Research, 8:985–1005, December 2007. ISSN 1532-4435.\n\nMasashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine Learning. Cambridge University Press, Cambridge, 2012. ISBN 978-0-521-19017-6. doi: 10. 1017/CBO9781139035613.\n\nBaochen Sun and Kate Saenko. Deep CORAL: Correlation Alignment for Deep Domain Adaptation. In Gang Hua and Hervé Jégou (eds.), Computer Vision – ECCV 2016 Workshops, Lecture Notes in Computer Science, pp. 443–450, Cham, 2016. Springer International Publishing. ISBN 978-3319-49409-8. doi: 10.1007/978-3-319-49409-8_35.\n\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-Time Training with Self-Supervision for Generalization under Distribution Shifts. arXiv:1909.13231 [cs, stat], July 2020.\n\nRemi Tachet, Han Zhao, Yu-Xiang Wang, and Geoff Gordon. Domain Adaptation with Conditional\n\nDistribution Matching and Generalized Label Shift. March 2020.\n\nA. W. van der Vaart and Jon A Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York, 2000. ISBN 978-0-387-94640-5 978-1-4757-2547-6.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. 2011.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully Test-Time Adaptation by Entropy Minimization. In International Conference on Learning Representations, September 2020.\n\nJon Wellner et al. Weak convergence and empirical processes: with applications to statistics.\n\nSpringer Science & Business Media, 2013.\n\nHui Xu and Robert Tibshirani. Estimation of prediction error with known covariate shift. arXiv\n\npreprint arXiv:2205.01849, 2022.\n\nYaodong Yu, Zitong Yang, Alexander Wei, Yi Ma, and Jacob Steinhardt. Predicting out-of-\n\ndistribution error with the projection norm. arXiv preprint arXiv:2202.05834, 2022.\n\nRuntian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. DORO: Distributional and Outlier Robust Optimization. In Proceedings of the 38th International Conference on Machine Learning, pp. 12345–12355. PMLR, July 2021.\n\nMarvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift. arXiv:2007.02931 [cs, stat], October 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017.\n\n14",
  "translations": [
    "# Summary Of The Paper\n\nThis paper considers the problem of reweighting training samples to improve model performance on out-of-distribution test samples. The approach formulates real distribution shifts (covariate and concept related) using the exponential tilt assumption. With this assumption, the problem of improving performance on OOD samples simplifies to learning data importance weights. The paper has some theoretical analysis on the properties of exponential tilting. The paper also applies this method to improve performance on Waterbirds and BREEDS-Entity30.\n\n# Strength And Weaknesses\n\nStrengths \n\n- The paper is well-written. The exponential tilt model is clearly explained. \n- Analyzing learned importance weights. \n\nWeaknesses\n- Exponential tilt assumption unjustified. It is not clear why complex distribution shits in practice should be parameterized with the exponential tilt model. The paper states that this problem can be applied to concept drift settings but has no experiments / analysis on concept drift.\n\n- When does this re-weighting approach fail? A more quantitative approach to this question would be insightful. The paper loosely talks about this  (i.e. distribution needs some overlap etc).\n\n- Limited empirical evaluation. Waterbirds and Entity30 are datasets where you know what the \"ground-truth\" importance weights should look like (https://proceedings.mlr.press/v177/idrissi22a.html). Showing that the proposed methods works well in these settings is a good first paper, i think the paper will be significantly stronger if the paper improves performance on more general / realistic distribution shifts (e.g. CIFAR 10.2, ImagenetV2) and then analyzes the learned importance weights. The empirical section should have additional baselines (e.g. https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf, https://proceedings.mlr.press/v177/idrissi22a.html) to clearly contrast this approach from previous methods.\n\n- Theoretical analysis somewhat tangential and not insightful vis-a-vis the paper's main focus. I would rather first read whether this method works well on realistic distribution shifts and then discuss properties like consistency and identifiability of the parameters. \n\nOverall the paper is well-written and focuses on a principled approach (reweighting) to improve OOD performance. However, the paper has two major issues: (a) exponential tilt assumption is not clearly justified, (b) empirical evaluation is quite limited.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee strengths and weaknesses above\n\n# Summary Of The Review\n\nSee strengths and weaknesses above\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" presents a novel importance weighting method aimed at adapting machine learning models to new tasks that may differ from their training distributions. The authors introduce an exponential tilt model that allows for the reweighting of training samples without requiring labeled data from the target domain. Their theoretical framework provides guarantees on the quality of the learned importance weights, while empirical evaluations on the WATERBIRDS and BREEDS datasets demonstrate the method's effectiveness in downstream tasks such as model selection and fine-tuning.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to addressing the challenges of domain adaptation without the need for labeled target samples, expanding the applicability of machine learning models in real-world scenarios. The theoretical guarantees provided lend credibility to the proposed method and its potential for practical use. However, a notable weakness is the reliance on the exponential tilt assumption, which may not generalize well across all types of distribution shifts. Additionally, while the empirical results are promising, further validation across more diverse datasets could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem context, methodology, and findings, making it accessible to readers familiar with the field. The quality of the writing is high, with careful attention to detail in both theoretical and empirical sections. The novelty of the approach is significant, particularly in its application of the exponential tilt model to importance weighting. The reproducibility of the results is supported by a thorough description of the methodology and datasets used, although the authors could enhance this aspect by providing code or additional resources for replication.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of domain adaptation through its innovative importance weighting method and theoretical underpinnings. While the approach shows promise and is well-documented, the reliance on specific assumptions and the need for broader empirical validation are areas for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" addresses the challenge of adapting machine learning models to new tasks with differing data distributions. The authors propose the Exponential Tilt Model, which learns importance weights for training samples to better align with the target distribution, accommodating concept drift. The methodology includes minimizing KL divergence between labeled training data and unlabeled target data to derive ExTRA (Exponential Tilt Reweighting Alignment) weights. Empirical results show that ExTRA significantly improves model performance on benchmark datasets, WATERBIRDS and BREEDS, outperforming existing methods by effectively leveraging relevant training samples for model fine-tuning and evaluation.\n\n# Strength And Weaknesses\nStrengths of the paper include its flexibility in handling concept drift and the innovative use of unlabeled target data for learning importance weights, which can alleviate the need for extensive labeled datasets. The empirical results are robust, demonstrating ExTRA's efficacy across multiple benchmarks and scenarios. However, the method's reliance on the assumption of some overlap between source and target distributions may limit its applicability in severe distribution shifts. Additionally, potential identifiability issues with the tilt parameters could complicate its practical deployment.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its methodology and findings clearly, with adequate theoretical backing for the proposed approach. The novelty lies in the introduction of the Exponential Tilt Model, which is a significant advancement over traditional methods for dealing with distribution shifts. The reproducibility of the results is supported by thorough experimental design and detailed descriptions of the benchmarks used. However, further exploration of the model's generalizability across diverse datasets is necessary, as acknowledged by the authors.\n\n# Summary Of The Review\nOverall, the paper contributes a novel approach to addressing distribution shifts in machine learning through the Exponential Tilt Model and demonstrates strong empirical performance on benchmark datasets. While the method shows promise, its assumptions and potential limitations warrant further investigation to enhance its applicability across a broader range of practical scenarios.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents the ExTRA (Exponential Tilt Reweighing Algorithm) method, which addresses the challenges of deploying machine learning models on new tasks when there is a distribution shift between training and target data. The authors propose a novel approach to reweigh training samples using an exponential tilt model to minimize the Kullback-Leibler (KL) divergence between labeled training data and unlabeled target data. The efficacy of the proposed method is demonstrated through empirical studies on two benchmarks, WATERBIRDS and BREEDS, where ExTRA shows significant improvements in model performance by effectively reweighing training samples to better align with target domains.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to the problem of domain adaptation, particularly in its theoretical foundation and the introduction of the exponential tilt model, which captures both covariate shift and concept drift. The empirical results on the WATERBIRDS and BREEDS datasets convincingly illustrate the advantages of the ExTRA method over traditional approaches. However, the paper could benefit from a more detailed discussion on the selection of sufficient statistics \\(T(x)\\) and the potential limitations or challenges associated with the model's assumptions. Additionally, while the theoretical properties are well-established, practical implementation details could be elaborated further to enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a logical flow and clear explanations of complex concepts. The quality of the theoretical analysis is strong, and the introduction of the exponential tilt model is both novel and significant within the context of domain adaptation. However, the reproducibility of the results could be improved by providing more detailed descriptions of the experimental setups and hyperparameter selections used in the empirical studies. Furthermore, a discussion regarding the computational efficiency and scalability of the ExTRA method would be beneficial for practitioners.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of machine learning by providing a novel method for reweighing training samples in the presence of distribution shifts. While the theoretical and empirical results are promising, further elaboration on implementation details and practical challenges would enhance the paper's impact and applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel method for reweighting training samples, aimed at improving domain adaptation and out-of-distribution generalization. The authors introduce an exponential tilt model that not only approximates the target domain distribution but also accommodates concept drift, enhancing traditional importance weighting methods. The methodology is underpinned by theoretical guarantees and empirically validated across multiple benchmarks, including WATERBIRDS and BREEDS, showing promising results in practical applications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to sample reweighting, supported by robust theoretical foundations that lend credibility to the method. The empirical validation across diverse benchmarks demonstrates its utility, although the choice of benchmarks may limit generalizability. While the flexibility to address concept drift is a significant advancement, it introduces complexity that may impact model training. Furthermore, the discussion on ethical considerations, particularly regarding minority group representation, is valuable; however, the lack of concrete strategies for implementation is a notable weakness. The authors also highlight potential areas for future research, although they do not provide specific methodologies to pursue these avenues.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The quality of writing and presentation is high, with appropriate mathematical rigor. The novelty of the approach is significant, particularly in its application to concept drift and fairness in machine learning. However, the reproducibility of the results could be enhanced with more detailed descriptions of experimental setups and parameter choices.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method for reweighting training data that addresses important challenges in domain adaptation and fairness. While its theoretical and empirical contributions are noteworthy, there are limitations in generalizability and practical implementation that warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel framework called \"Exponential Weight Adjustment\" (EWA) to address the challenges associated with deploying machine learning models on new tasks that experience distribution discrepancies between the training (source) and the new (target) data. The authors propose a modified importance weighting mechanism that leverages labeled source data and unlabeled target data to estimate weights that effectively align the source distribution with the target distribution. Key contributions include the introduction of a new divergence measure tailored for EWA, theoretical guarantees for weight effectiveness, and empirical validation across various benchmark datasets, demonstrating that EWA outperforms traditional methods by improving model performance on unseen domains.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative introduction of the EWA framework, which stands out for its ability to utilize unlabeled target data effectively. The theoretical guarantees provided enhance the credibility of the proposed method, offering a solid foundation for its application in real-world scenarios where labeled target data may be scarce. The empirical results on diverse datasets such as WATERBIRDS and BREEDS demonstrate significant improvements in model accuracy, underscoring the method's practical relevance. However, a potential weakness lies in the complexity of the divergence measure introduced, which may require further clarification for practitioners to implement it effectively. Additionally, while the empirical results are promising, the paper could benefit from a more extensive comparison with a broader range of baseline methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, making it accessible to readers familiar with domain adaptation and importance weighting techniques. The quality of the writing is high, and the methodology is described in sufficient detail to allow for reproducibility. The novelty of the proposed EWA framework and its divergence measure is significant, addressing a critical gap in the field of machine learning. However, the complexity of the mathematical formulations may pose challenges for some readers in fully grasping the implications of the proposed changes.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of domain adaptation by introducing the EWA framework, which effectively utilizes unlabeled target data to improve model performance in the face of distribution shifts. The theoretical underpinnings and empirical validation presented are robust, marking a promising advancement in the area of importance weighting techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" introduces a novel framework for adversarial training in deep learning models, specifically targeting the adaptation to unseen distributions. The authors propose an exponential tilting method to estimate importance weights for training samples using unlabeled target data, which enhances the model's robustness during adversarial training. The paper provides theoretical guarantees for the effectiveness of this approach and validates it empirically through extensive experiments on benchmark datasets such as WATERBIRDS and BREEDS, demonstrating significant improvements in performance under adversarial conditions.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adversarial training, particularly the introduction of exponential tilting for weight estimation, which presents a fresh perspective on the task. Additionally, the theoretical rigor established by the authors adds credibility and a solid foundation for their proposed method. The practical relevance of the approach, given its ability to utilize unlabeled target data, is another notable strength. However, the paper also has weaknesses; the reliance on the exponential tilt model may oversimplify complex distributional shifts, potentially limiting the method's robustness in real-world scenarios. Furthermore, the experimental validation could benefit from broader application across more diverse and challenging environments to fully assess the method's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers familiar with the topic. The quality of writing is high, with thorough explanations of the theoretical components and empirical results. The novelty of the proposed approach is significant, as it integrates unlabeled data into adversarial training effectively. Reproducibility is enhanced by the detailed description of experiments, though it would benefit from additional datasets to further validate the method's robustness.\n\n# Summary Of The Review\nIn summary, the paper presents a novel and theoretically sound approach to adversarial training that effectively utilizes unlabeled data, offering practical benefits. While the contributions are significant, the complexity of real-world distributional shifts and the limited scope of experiments highlight areas for further exploration. Overall, this work represents a meaningful advancement in the field of adversarial training.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" presents a novel framework for learning importance weights from training data, allowing practitioners to tackle distribution shift challenges without requiring labeled data from target domains. The proposed exponential tilt model is designed to handle both covariate and concept drift, and the authors claim to provide unmatched theoretical guarantees that enhance the reliability of weight estimates. Empirical results on the WATERBIRDS and BREEDS benchmarks demonstrate that the proposed method outperforms existing techniques, setting new performance standards in model evaluation and fine-tuning.\n\n# Strength And Weaknesses\nThe paper’s strength lies in its innovative approach to importance weighting, potentially transforming how models are fine-tuned and evaluated in the face of distribution shifts. The theoretical underpinnings offered by the authors enhance the credibility of their claims and provide a robust framework for future research. However, a notable weakness is the acknowledgment of the need for some overlap between source and target domains, which could limit the applicability of their method in certain scenarios. Moreover, the authors’ framing of existing methodologies as outdated may lack a nuanced discussion of their merits.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that effectively communicates its contributions. However, some sections could benefit from more detailed explanations of the methodologies employed, particularly in the context of the theoretical guarantees provided. While the novel approach is commendable, the reproducibility of results may be hindered by the lack of comprehensive experimental setups and descriptions of the datasets used. Nonetheless, the high empirical performance reported suggests that the methods are substantively sound.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the field of machine learning through its innovative approach to importance weighting and distribution shift challenges. While the theoretical and empirical claims are compelling, the paper could improve in clarity and depth to enhance reproducibility and broaden its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" by Subha Maity et al. addresses the challenge of adapting machine learning models to new tasks characterized by distributional shifts between training and target datasets. The authors propose an innovative method for reweighing training samples, termed ExTRA, based on the exponential tilt model, which minimizes the KL divergence between labeled training data and unlabeled target data. The effectiveness of this approach is demonstrated through empirical studies on the WATERBIRDS and BREEDS benchmarks, showing significant improvements in model performance, particularly in scenarios with minority groups and extreme subpopulation shifts.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include the development of a theoretically grounded method for estimating source importance weights in the absence of labeled target samples, which is a crucial advancement given the prevalence of distribution shifts in real-world applications. The empirical results are compelling, demonstrating significant performance gains over baseline methods, especially in challenging scenarios. However, one notable weakness is the reliance on some degree of overlap between source and target distributions, which may limit the method's applicability in cases where such overlap is absent. Additionally, while the empirical results are strong, further exploration into the robustness of the method under varying conditions would enhance the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The theoretical aspects are presented with sufficient detail to allow for understanding and reproducibility. The novel approach of using exponential tilting for sample reweighting is articulated coherently, contributing to the clarity of the work. However, while the empirical results are promising, the paper could benefit from additional discussion regarding potential limitations and assumptions inherent in the proposed method, particularly concerning its generalizability across different domains.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of machine learning by addressing the challenges posed by distribution shifts and providing a novel method for sample reweighting. The empirical results demonstrate the effectiveness of the proposed approach, although the dependency on distribution overlap warrants further investigation. The clear presentation and robust methodology make this paper a valuable addition to the literature.\n\n# Correctness\nRating: 4/5  \nThe proposed method is well-founded theoretically, and the empirical results support its claims; however, the assumptions regarding distribution overlap need further scrutiny.\n\n# Technical Novelty And Significance\nRating: 5/5  \nThe introduction of the exponential tilt model for sample reweighting represents a substantial technical advancement, addressing a critical issue in adapting models to new tasks.\n\n# Empirical Novelty And Significance\nRating: 4/5  \nWhile the empirical results are impressive and demonstrate the method's effectiveness, further exploration of scenarios with no overlap would enhance the significance of the findings.",
    "# Summary Of The Paper\nThe paper presents a novel approach to domain adaptation through an Exponential Tilt Model, which aims to establish a linear relationship between source and target distributions using sufficient statistics. The authors propose leveraging unlabeled target data to fit the model, while also addressing challenges associated with concept drift. The findings suggest that the proposed method, ExTRA, demonstrates effectiveness in specific benchmarks, although the generalizability of these results to broader applications remains unclear.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative methodology, which combines the use of unlabeled data with a theoretically grounded framework for domain adaptation. However, several weaknesses are apparent, including the oversimplification of complex relationships through the linear assumption of the exponential tilt model. Additionally, the reliance on the overlap of source and target distributions may not hold in diverse real-world settings, and concerns about parameter identifiability raise questions regarding the reliability of the results. The evaluation metrics used in the study may also introduce bias, limiting the applicability of findings to other contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methodology and its underlying assumptions. However, the novelty of the approach may be somewhat diminished by the reliance on traditional assumptions that may not apply universally. Reproducibility is a concern, as the empirical justification for certain claims—especially regarding the use of unlabeled data and the handling of concept drift—is limited. The suggestion for future work lacks concrete strategies for overcoming identified limitations, which may hinder practical implementation.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for domain adaptation with relevant theoretical insights but falls short in addressing key assumptions and limitations that could impact its real-world applicability. The findings are intriguing, yet the reliance on several unverified assumptions calls for cautious interpretation of the results.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach for adapting machine learning models to new tasks characterized by different data distributions, focusing on the challenge of out-of-distribution generalization. It introduces an Exponential Tilt Model that learns importance weights from training data, facilitating the effective use of this data in downstream tasks without the need for labeled target data. The authors demonstrate the efficacy of their method through case studies on the WATERBIRDS and BREEDS datasets, showing significant improvements in performance when adapting to new domains and handling subpopulation shifts.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to addressing distribution shifts without requiring labeled data, which is a common limitation in existing methods. The theoretical guarantees provided for the model's parameters enhance the credibility of the approach. The empirical results from the case studies effectively illustrate the practical advantages of the Exponential Tilt Model. However, a notable weakness is the limited exploration of the model's performance under varying conditions of support overlap, which may affect its generalizability across different scenarios. Additionally, the implications of potential biases in target domains could have been examined more thoroughly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with a foundational understanding of machine learning. The methodology is described in sufficient detail to allow for reproducibility, although additional information on implementation specifics could enhance this aspect. The novelty of the approach is evident, as it addresses a significant gap in the literature regarding out-of-distribution generalization without relying on labeled data, making it a noteworthy contribution to the field.\n\n# Summary Of The Review\nOverall, the paper introduces a promising new method for adapting machine learning models to new domains under distribution shifts, demonstrating both theoretical soundness and practical effectiveness. While it makes significant contributions, further exploration of certain limitations is warranted to fully assess its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework designed to enhance the robustness of machine learning models against distribution shifts between training and testing datasets. The authors propose a technique that leverages a combination of domain adaptation and meta-learning strategies to improve model performance in varying environments. Through extensive empirical validation on multiple benchmark datasets, the findings demonstrate significant improvements in generalization and adaptability, highlighting the method's effectiveness in real-world applications.\n\n# Strength And Weaknesses\n**Strengths:**\n- The addressed issue of distribution shifts is highly relevant, making the paper's contributions timely and applicable to current challenges in machine learning.\n- The methodology is innovative, combining established techniques in a novel way that potentially advances the state-of-the-art.\n- The empirical evaluation is comprehensive, covering a variety of benchmarks and providing a thorough analysis of the proposed method's effectiveness.\n\n**Weaknesses:**\n- Certain aspects of the methodology could be better articulated, as some readers may struggle to replicate the approach without additional detail.\n- The empirical results, while robust, could benefit from further exploration across a wider range of datasets and tasks to strengthen claims of generalizability.\n- The practical implications of the theoretical insights could be expanded, particularly in terms of how they apply to real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, though some sections would benefit from additional detail to enhance clarity for readers unfamiliar with the advanced concepts discussed. The quality of the writing is generally high, with logical progression through the arguments. The novelty of the approach is significant, as it combines different methodologies to tackle a pressing issue. However, the reproducibility of the results could be improved with more detailed explanations or pseudocode provided for the proposed method.\n\n# Summary Of The Review\nIn summary, the paper makes a meaningful contribution to the field of machine learning by addressing the challenge of distribution shifts with a novel framework. While the methodology and empirical results are strong, further clarification and additional experiments could enhance the paper's impact and applicability. Overall, this work is a valuable addition to the literature and has the potential for significant influence in its domain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach for addressing the challenges of transferring machine learning models between different tasks, particularly when faced with distribution shifts between training (source) and new (target) domains. The authors introduce an exponential tilt model to learn importance weights for training samples, which aims to minimize the Kullback-Leibler (KL) divergence between labeled training data and unlabeled target data. The findings indicate that this method can enhance performance in various downstream tasks such as target performance evaluation, model fine-tuning, and model selection, while also providing theoretical guarantees on the quality of weight estimates. Experimental validation is conducted on benchmark datasets, including WATERBIRDS and BREEDS, showcasing the practical utility of the proposed method.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative use of exponential tilting to address the problem of distribution shift in machine learning. By allowing for the learning of importance weights without the need for labeled target data, the method presents a significant advancement over existing techniques that typically rely on fixed decision boundaries. The theoretical guarantees provided add robustness to the approach. However, a notable weakness is the assumption of overlap between the source and target distributions, which may limit the applicability of the method in certain scenarios where such overlap does not exist. Additionally, while the paper presents applications on benchmark datasets, further exploration of real-world settings would strengthen its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings, contributing to a high level of clarity. The quality of the writing is commendable, making it accessible to the target audience. The proposed method exhibits significant technical novelty, particularly in its approach to learning importance weights. However, while the theoretical foundations are strong, additional details on the experimental setup and results would enhance reproducibility, particularly for practitioners looking to apply the method in diverse contexts.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to mitigating the challenges of domain adaptation in machine learning through the use of an exponential tilt model for learning importance weights. While the contributions are notable and the theoretical underpinnings are solid, the reliance on the assumption of overlap between source and target distributions represents a limitation that could impact the method's broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper by Maity et al. (2023) presents a novel approach for adapting machine learning models to new tasks where the training and target distributions differ. The authors introduce an exponential tilt model that reweighs training samples to minimize the Kullback-Leibler (KL) divergence between the training and target distributions. Key contributions include the development of a method to learn source importance weights without requiring labeled target data, theoretical guarantees of weight estimation quality, and empirical validation of the approach on the WATERBIRDS and BREEDS datasets, showcasing improved performance compared to baseline methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to handling domain shifts in machine learning, particularly its ability to operate without labeled target samples, which is a common limitation in existing methods. The theoretical insights provided about model parameter identifiability and consistency in weight estimation add significant value. However, a notable weakness is the reliance on the assumption of overlap between training and target distributions, which may limit the method's applicability in scenarios where such overlap is not present. The empirical evaluations, while promising, are limited to only two datasets, potentially restricting generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured layout that guides the reader through the problem context, methodology, and results. The quality of writing is high, making complex concepts accessible. The novelty of the proposed exponential tilt model is significant, as it addresses a pressing challenge in the field of machine learning. Reproducibility appears feasible, as the paper provides sufficient methodological detail; however, it would benefit from an explicit discussion on the experimental setup and hyperparameter tuning processes.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the domain adaptation literature by introducing a novel method for reweighting training samples based on an exponential tilt model. While the theoretical foundations and empirical results are strong, the assumptions regarding distribution overlap may pose limitations. Future work should explore broader applicability and address these constraints.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" by Subha Maity et al. proposes a novel approach for reweighing training samples to better align them with the distribution of target tasks. The authors introduce an exponential tilt model that learns importance weights from unlabeled target data, aiming to address challenges related to out-of-distribution generalization and subpopulation shifts. The methodology is validated through experiments on benchmark datasets, specifically WATERBIRDS and BREEDS, where it demonstrates improved model performance and selection.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to handling distribution shifts without requiring labeled target data, which is a significant advancement in the field of domain adaptation. The theoretical guarantees provided lend credibility to the proposed method. Additionally, the comprehensive experimental validation across multiple datasets reinforces the claims made. However, a notable weakness is the limited exploration of the method's limitations, particularly under extreme distribution shifts. More diverse datasets could also enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly written, with a logical flow that aids understanding. Terminology is defined appropriately, contributing to its clarity. The technical quality of the methodology is high, with sound statistical techniques employed. The novelty of the approach is significant, particularly with the introduction of exponential tilting for importance weighting under concept drift. The empirical results are reproducible, as they are quantitatively reported and based on well-established benchmark datasets.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of machine learning, particularly in addressing out-of-distribution generalization through a novel method that leverages unlabeled target data. It combines a strong theoretical foundation with practical implications, although further exploration of its limitations and additional validation on diverse datasets would enhance its impact.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 5\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThe paper presents a novel approach for addressing distributional discrepancies between source and target datasets in machine learning applications. The authors introduce an exponential tilt model to estimate importance weights for source samples, which allows for the approximation of target domain distributions using unlabeled target data. The methodology involves minimizing the Kullback-Leibler divergence between labeled source and unlabeled target datasets, resulting in weights that enhance model performance in downstream tasks. The authors demonstrate the effectiveness of their approach through empirical evaluations on the WATERBIRDS and BREEDS datasets.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to importance weighting, leveraging unlabeled data to address distributional shifts without requiring extensive labeling. This is a significant advancement over traditional methods that often depend on labeled target data for effective adaptation. The theoretical guarantees provided regarding the identifiability and estimation consistency of the model further bolster its contributions. However, a potential weakness is the reliance on the specific structure of the exponential family, which may limit the approach's generalizability to other types of distributions or models. Additionally, while the empirical results are promising, further validation across more diverse datasets could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex concepts accessible. The methodology is described in a detail-oriented manner, which enhances the reproducibility of the approach. The novelty of the proposed model lies in its ability to utilize unlabeled data for weight estimation, which is a fresh perspective in the domain of out-of-distribution generalization. However, while the theoretical underpinnings are robust, the experimental section could benefit from a more extensive discussion on potential limitations and the specific conditions under which the model performs optimally.\n\n# Summary Of The Review\nOverall, this paper contributes significantly to the field of machine learning by introducing a novel technique for adapting models to new tasks using unlabeled data. The methodology is theoretically sound and empirically validated, although additional experiments on varied datasets could enhance its applicability. The approach shows promise for improving model performance in challenging, real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a method for addressing distribution shifts in machine learning by reweighing training samples. The authors propose a framework based on the exponential tilt assumption and introduce theoretical guarantees for their approach. The method is evaluated on two benchmarks, WATERBIRDS and BREEDS, with findings suggesting improved performance compared to existing techniques. However, the authors acknowledge limitations in practical applicability and call for future research directions.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its theoretical contributions, notably the introduction of guarantees regarding the method's performance under certain conditions. However, the approach is limited by its narrow focus on reweighing, which may overlook alternative strategies that could be more effective. The reliance on the exponential tilt assumption raises concerns about the method's robustness in real-world applications, where such assumptions may not hold. Furthermore, the evaluation on only two datasets raises questions about generalizability, and the lack of thorough empirical analysis and discussions of failure cases diminishes the depth of the study. The authors also fail to critique their method adequately, which could enhance the paper's scientific rigor.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly, with a structured presentation of the methodology and findings. However, the complexity of the theoretical proofs may hinder reproducibility, especially since the conditions for identifiability are quite specific. While the approach has elements of novelty, particularly in its theoretical framework, the practical applicability and robustness of the method remain uncertain due to the limitations noted earlier.\n\n# Summary Of The Review\nOverall, the paper attempts to introduce a novel approach to handling distribution shifts in machine learning. However, its practical applicability is hindered by reliance on restrictive assumptions and limited empirical evaluation. The work would benefit from a more balanced discussion of limitations and clearer future directions for research.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel approach named Exponential Tilting for adapting machine learning models to new tasks, particularly in the context of distribution shifts. Central to this methodology is the innovative importance weighting technique that allows for the effective learning of weights without the need for labeled target data. The authors provide strong theoretical guarantees for the quality of these weights and demonstrate the method's efficacy through empirical validation on benchmark datasets such as WATERBIRDS and BREEDS. The findings highlight significant improvements in model performance across various downstream tasks, including fine-tuning and model selection.\n\n# Strength And Weaknesses\nOne of the primary strengths of this paper is its introduction of a robust framework that addresses the challenge of adapting to new tasks without labeled target data, which is often a significant barrier in practical applications. The theoretical foundations laid out in the paper also provide confidence in the reliability of the learned importance weights. However, a potential weakness lies in the limited exploration of the method's applicability to more complex real-world scenarios beyond the tested datasets. Additionally, while the claims of performance improvements are compelling, more extensive empirical comparisons with a broader range of methods could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, making it accessible to readers with a background in machine learning. The methodology is described in sufficient detail to allow for reproducibility, although supplementary materials or code availability would further strengthen this aspect. The novelty of the approach is significant, as it offers a unique solution to the problem of distribution shifts, a prevalent issue in the field.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in adapting machine learning models through the innovative use of importance weighting and exponential tilting. While the theoretical foundations and empirical results are strong, further exploration of the method's applicability in diverse real-world scenarios and comparisons with additional baselines would enhance its credibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a theoretical framework centered on the exponential tilting model to address the problem of domain shift in machine learning. The authors develop a mathematical formulation that captures the relationship between source and target distributions through sufficient statistics, providing theoretical guarantees for parameter identifiability and estimation consistency. The findings suggest that the exponential tilt model not only enhances model evaluation and fine-tuning capabilities in the target domain but also accommodates concept drift, offering a more flexible approach compared to traditional models. Furthermore, the paper discusses the limitations of the model, particularly its reliance on the assumption of overlap between distributions, and proposes future research directions.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its robust theoretical framework and the clear derivation of mathematical relationships that underpin the exponential tilting model. The identification of parameters and the consistency of estimation enhance the model's reliability, making it a significant contribution to the field of domain adaptation. However, a notable weakness is the model's assumption of overlap between the source and target distributions, which could limit its applicability in scenarios where this condition is not met. Additionally, the paper could benefit from more empirical validation to support its theoretical claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical concepts involved, making it accessible to readers with a background in machine learning and statistical theory. The quality of the writing is high, with precise mathematical formulations that are logically presented. In terms of novelty, the approach introduces new insights into distribution shifts and concept drift, although it builds upon existing literature. Reproducibility may be an issue, as the paper does not provide empirical experiments or code to validate the theoretical findings, which could limit the practical application of the proposed model.\n\n# Summary Of The Review\nOverall, the paper presents a strong theoretical framework for addressing distribution shifts in machine learning through the exponential tilting model. While it offers significant insights and guarantees, the reliance on certain assumptions and the lack of empirical validation are areas that warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach called the Exponential Tilt Model, which learns importance weights from training samples to effectively mimic the target domain distribution without requiring labeled target samples. The methodology involves fitting this model through distribution matching, specifically using Kullback-Leibler divergence for optimization. The authors provide theoretical guarantees on the weight estimates and demonstrate the efficacy of their approach on two datasets: the WATERBIRDS dataset and the BREEDS dataset. The results indicate that the ExTRA method outperforms existing baseline methods in terms of precision, recall, and target accuracy, showcasing its potential for applications in domain adaptation.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to learning importance weights without labeled target data, which is a significant advancement in the field of domain adaptation. The theoretical underpinnings provide a solid foundation for the proposed method, enhancing its credibility. Furthermore, the empirical results across two diverse datasets illustrate the practical applicability of the method. However, the paper could benefit from a more extensive discussion on the applicability of the method to different domains and a clearer exploration of potential limitations. Additionally, while the experiments are promising, further evaluations on additional datasets would strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The authors provide sufficient detail on the implementation of the Exponential Tilt Model, including the algorithm for computing importance weights. However, the theoretical guarantees could be elaborated further for clarity. The novelty of the approach is noteworthy, particularly in its strategy of utilizing unlabeled data from the target domain. The reproducibility of the results is supported by the availability of the implementation code on GitHub, allowing other researchers to validate the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling method for learning importance weights in the absence of labeled target samples, supported by theoretical and empirical evidence. While the contributions are significant, further exploration of limitations and additional empirical evaluations would enhance the paper's robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel methodology for addressing distributional shifts in machine learning, specifically focusing on the ExTRA framework. The authors claim that their approach simplifies the handling of covariate shift compared to existing ensemble methods, and they provide theoretical guarantees aimed at supporting their model's efficacy. Empirical evaluations are conducted on the WATERBIRDS and BREEDS benchmarks, where the authors assert that their method demonstrates advantages over group annotation-based techniques.\n\n# Strength And Weaknesses\nWhile the authors present a method that aims to simplify the handling of distribution shifts, there are notable weaknesses in the robustness and comprehensiveness of their approach. The methodology does not demonstrate the same level of efficacy as prior works, such as those by Sugiyama et al. (2012), and lacks the strong theoretical guarantees seen in invariant risk minimization models. Additionally, the reliance on unlabeled target data raises questions about practical applicability compared to established ensemble techniques. The empirical results do not convincingly show superiority over traditional methods, and the authors fail to adequately benchmark against state-of-the-art alternatives like those proposed by Sagawa et al. (2019).\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is reasonably good, though the justifications for the proposed method's advantages over existing solutions could be more robust. The quality of the theoretical contributions appears limited, as they lack depth compared to previous models. Regarding novelty, while the approach attempts to sidestep the need for group annotations, it echoes prior calls for support alignment and does not present a groundbreaking advancement. Reproducibility is not adequately addressed, as the empirical evaluations lack comprehensive benchmarking against established methods.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to handling distribution shifts but falls short in robustness and novelty compared to existing literature. The empirical results do not convincingly support the claims made, which raises concerns about the practical applicability of the proposed methodology.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper \"UNDERSTANDING NEW TASKS THROUGH THE LENS OF TRAINING DATA VIA EXPONENTIAL TILTING\" presents a novel framework for analyzing how training data influences the performance of machine learning models on new tasks. The methodology involves the use of exponential tilting to adjust the distribution of training data, allowing for a more nuanced understanding of task relevance. The findings demonstrate significant improvements in model performance when applying this approach, highlighting the importance of data selection and representation in machine learning.\n\n# Strength And Weaknesses\nThe primary contribution of this paper lies in its innovative approach to data analysis, which provides a fresh perspective on the relationship between training data and task performance. The paper effectively employs mathematical rigor to support its claims, although some equations and notations could benefit from improved clarity and consistency. A notable weakness is the lack of in-depth discussion on the implications of the findings, which could enhance the reader's understanding of the broader impact of this research.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept, clarity suffers from inconsistent formatting and typographical errors throughout the document. The methodology is sound, but certain mathematical expressions lack clarity due to inconsistent spacing and notation. Reproducibility is compromised by the need for more explicit definitions of symbols and terms. The paper would benefit from a more cohesive and polished presentation to ensure that the contributions are communicated effectively.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field by offering a novel framework for understanding task performance in relation to training data. However, clarity issues and minor errors detract from the overall quality and accessibility of the work. Improving these aspects would significantly enhance the paper's impact and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper addresses the distribution shift problem within machine learning, proposing a novel method aimed at improving performance in scenarios where training and testing datasets exhibit differing distributions. The authors provide theoretical guarantees under specific conditions and conduct empirical evaluations on two datasets, WATERBIRDS and BREEDS. The findings suggest that their method can effectively mitigate distribution shift challenges, although the paper lacks exploration of extreme distribution shifts and broader applicability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which offer insights into handling distribution shifts under certain assumptions. However, significant weaknesses include a narrow focus on specific datasets, limited exploration of extreme distribution shifts, and a lack of discussion on practical integration with existing domain adaptation techniques. Additionally, the paper does not address potential ethical implications or the robustness of the method in high-dimensional spaces.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly, facilitating understanding. However, the novelty is somewhat tempered by the limited scope of theoretical guarantees and the absence of a detailed case study or real-world application. Reproducibility may be hindered by the reliance on specific datasets without extensive experiments across a variety of conditions, which could affect the generalizability of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a relevant contribution to the study of distribution shifts with some theoretical insights and empirical evidence. However, its limitations in scope, applicability, and ethical considerations warrant further exploration to enhance its impact and relevance in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"Understanding New Tasks Through the Lens of Training Data via Exponential Tilting\" presents a novel approach for adapting machine learning models to new tasks by addressing distribution shifts between training (source) and target data. The authors introduce the Exponential Tilt Model, which leverages statistical methods to derive importance weights aimed at minimizing Kullback-Leibler (KL) divergence. The paper provides theoretical guarantees for the method, establishes conditions for consistency in importance weight estimation, and empirically evaluates the proposed ExTRA (Exponential Tilt Reweighting Alignment) method on benchmark datasets (WATERBIRDS, BREEDS), demonstrating its effectiveness in improving model performance on new tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its solid theoretical foundation, which offers guarantees about the estimation process and the performance of the proposed method. The use of KL divergence as a measure for distribution similarity is well-justified, and the model's flexibility in handling concept drift is a significant advantage. The empirical evaluation is thorough, employing relevant benchmarks and statistical metrics to assess performance. However, a notable weakness is the lack of explicit significance testing details, which may leave some empirical claims unverified. Additionally, while the theoretical aspects are robust, the practical applicability of the method in complex real-world scenarios remains to be fully explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, theoretical contributions, and empirical results. The quality of writing is high, enabling readers to follow the complex statistical concepts without significant difficulty. The novelty of combining exponential tilting with importance weighting in the context of distribution shifts is commendable. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and data handling processes, as well as by potentially sharing code or supplementary materials.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of machine learning by providing a robust statistical framework for adapting models to new tasks under distribution shifts. While the theoretical and empirical findings are strong, the lack of detailed significance testing and reproducibility measures limits the practical impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Exponential Tilt Reweighting Algorithm (ExTRA), a method aimed at improving domain generalization by leveraging unlabeled target data in the presence of distributional shifts. The authors propose a theoretical framework that guarantees performance under the assumption of overlapping supports between source and target class conditionals. However, the empirical validation is primarily limited to specific benchmarks (WATERBIRDS and BREEDS) and does not extend to a broader range of real-world applications.\n\n# Strength And Weaknesses\nThe proposed method presents a theoretically sound approach that could be beneficial in specific scenarios where the overlap assumption holds. The theoretical guarantees offer a foundation for understanding its potential efficacy. Nonetheless, the paper's applicability is significantly limited by the assumption of overlapping supports, which may not be valid in many practical situations. Additionally, the reliance on unlabeled target data raises concerns regarding the quality and representativeness of that data. The lack of empirical validation across diverse scenarios and the absence of discussions on limitations and potential biases further weaken the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a coherent structure and presentation of the theoretical framework. However, the novelty is somewhat constrained by the lack of exploration of broader implications and scenarios where the proposed method may fail. Reproducibility is hindered due to the limited empirical validation and absence of detailed implementation guidance. Moreover, the paper fails to address ethical considerations and representation issues that could arise from the application of the method, which is a significant oversight in the context of machine learning research.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting theoretical approach to domain generalization through the ExTRA method, its practical applicability is limited due to several critical oversights, including the assumption of overlapping supports and insufficient empirical validation. The lack of exploration into ethical implications and potential biases further detracts from its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Understanding New Tasks through the Lens of Training Data via Exponential Tilting\" contributes to the ongoing discourse on addressing domain shift in machine learning. The authors propose a method that utilizes importance reweighting of training samples to adapt models to new tasks without requiring labeled data from the target domain. They introduce an \"exponential tilt model,\" which is theoretically supported by guarantees on weight estimates and applied to benchmark datasets such as WATERBIRDS and BREEDS. The findings suggest that this approach can effectively improve generalization to the target distributions by identifying relevant training samples.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its attempt to model the problem of domain shift through a theoretically grounded approach. The authors offer a method for computing importance weights without labeled target samples, which is indeed a relevant challenge in the field. However, the weaknesses are evident in the lack of novelty; the proposed exponential tilt model appears to be an iteration of existing concepts in statistics. The paper does not significantly advance the state of the art and relies heavily on well-trodden methodologies without presenting substantial new insights or applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally acceptable, but it suffers from overly complex formulations that could have been presented more straightforwardly. The quality of the writing is adequate but occasionally veers into jargon that may obscure meaning for readers less familiar with the specific statistical concepts. In terms of novelty, the work does not bring forth significant new ideas, and the reliance on established methods diminishes its impact. Reproducibility is not sufficiently addressed; while theoretical guarantees are provided, a clearer pathway for practical implementation would enhance the paper's usefulness.\n\n# Summary Of The Review\nOverall, the paper provides a reasonable approach to the problem of domain shift through importance reweighting, but it lacks innovative contributions and presents familiar ideas in an unnecessarily complex manner. While it addresses a relevant issue, the execution and novelty leave much to be desired.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper introduces a novel method utilizing exponential tilting to address distribution shifts between training and target domains. The authors propose a framework that minimizes Kullback-Leibler (KL) divergence to adjust the learned importance weights effectively. Empirical evaluations on the WATERBIRDS and BREEDS benchmarks demonstrate the method's effectiveness, though the authors suggest that further validation on more complex datasets is needed.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including theoretical guarantees that enhance its credibility. The integration of neural networks for parameterizing sufficient statistics \\(T\\) is promising, opening avenues for advanced architectures like attention mechanisms. However, the methodology could benefit from a multi-fidelity modeling approach and a more thorough exploration of alternative divergence measures, such as Wasserstein distance. Additionally, the implications of learned weights concerning fairness and bias are not adequately discussed, which is a significant oversight.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a coherent structure that facilitates understanding. The quality of the theoretical framework is high, but the novelty could be further enhanced by exploring ensemble methods that combine ExTRA weights with existing importance sampling techniques. The reproducibility of results appears solid, though additional validation on diverse datasets would strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper provides a solid contribution to the literature on addressing distribution shifts using exponential tilting. While the findings are compelling, the work would benefit from further exploration of multi-fidelity modeling, alternative divergence measures, and implications for fairness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the ExTRA method aimed at improving performance in transfer learning tasks, particularly in scenarios where there is a significant distribution shift between source and target domains. The methodology involves evaluating ExTRA on two primary benchmarks: WATERBIRDS and BREEDS. The results indicate that ExTRA achieves superior accuracy compared to existing methods, with a target accuracy of 0.819 ± 0.012 on the {0, 2} target group of WATERBIRDS, significantly outperforming the SrcVal method. Additionally, ExTRA shows robust performance across different configurations and benchmarks, including cases where source and target groups have no overlap, achieving competitive results even with limited target domain data.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive evaluation of the ExTRA method across multiple benchmarks, demonstrating significant improvements over existing techniques. The results on both WATERBIRDS and BREEDS highlight the method's robustness and reliability in adapting to distribution shifts. However, a potential weakness is that while the empirical results are promising, the paper could benefit from a more detailed exploration of the theoretical underpinnings of the ExTRA method, as well as its limitations in specific scenarios or datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-defined terminology and a structured presentation of results. The quality of the experimental design and analysis is commendable, and the findings are presented with sufficient detail to allow for reproducibility. However, while the empirical results are compelling, the paper could enhance its contribution by providing additional insights into the novelty of the ExTRA method, particularly in contrast to existing approaches.\n\n# Summary Of The Review\nOverall, the paper effectively demonstrates the advantages of the ExTRA method in enhancing transfer learning performance across various benchmarks, particularly in challenging scenarios with distribution shifts. The empirical results are strong, although further theoretical insights and discussions on limitations could enhance the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach to address the challenges of concept drift in machine learning models by employing a weighted training data mechanism. The methodology involves dynamically adjusting the influence of training data based on its relevance, measured using KL divergence metrics. The authors demonstrate the effectiveness of their approach through empirical evaluations on multiple datasets, showing significant improvements in predictive performance compared to baseline models.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to mitigating the effects of concept drift, which is a critical issue in real-world applications of machine learning. The empirical results are robust, demonstrating clear advantages in model performance. However, the paper suffers from several weaknesses, including a lack of clarity in its presentation, particularly in the abstract and introduction, where a roadmap for the reader is missing. Additionally, the use of technical jargon without adequate definitions may alienate readers unfamiliar with specific terms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by overly complex sentences and an inconsistent use of terminology, which detracts from the overall quality. The methodology section, while technically sound, is dense and could benefit from simplification and visual aids. In terms of novelty, the approach is innovative, particularly in its application to concept drift, but the presentation undermines its accessibility and reproducibility, as key details may be lost in the complexity of the language used.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to addressing concept drift in machine learning, backed by solid empirical results. However, significant improvements in clarity and structure are needed to enhance its readability and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3008610108102694,
    -1.547789946219584,
    -1.8845612032113983,
    -1.518886950748432,
    -1.786953010428903,
    -1.57358405979636,
    -1.7093741198773864,
    -1.9597980985220387,
    -1.642617285708895,
    -1.8519944952438108,
    -1.6724838915853883,
    -1.5262639486696825,
    -1.6218024492559804,
    -1.5835548131183355,
    -1.8064918629919866,
    -1.6090116648968018,
    -1.8438313697090987,
    -1.6212447720748346,
    -1.5860769039256855,
    -1.6607643969193038,
    -1.9170674857826926,
    -1.661856598608913,
    -1.718127346941072,
    -1.8069216576017673,
    -1.8112674675125617,
    -1.9194075502136476,
    -2.0497908331833807,
    -1.7538864459206558,
    -1.6270415151573998
  ],
  "logp_cond": [
    [
      0.0,
      -2.0776638895758426,
      -2.069203296954318,
      -2.0869642597134255,
      -2.0870837274584066,
      -2.118928985423569,
      -2.084061379085684,
      -2.0835080156495165,
      -2.0713962314277974,
      -2.127723625714767,
      -2.0718963289149617,
      -2.123587181495405,
      -2.0821056776083764,
      -2.0573357153198293,
      -2.0748323469608003,
      -2.0872803340925956,
      -2.055197929181105,
      -2.0994520855595313,
      -2.121231574859354,
      -2.0920201235012725,
      -2.1241096040946044,
      -2.1174321489492387,
      -2.0950006234472247,
      -2.0896042237050723,
      -2.096687125127009,
      -2.0882009529994043,
      -2.0850091581686963,
      -2.1223239831325422,
      -2.1423575160599357
    ],
    [
      -1.1607207548977996,
      0.0,
      -1.0494740400200104,
      -1.1687544707302417,
      -1.1037015368473853,
      -1.1603477244677554,
      -1.0310038761522478,
      -1.0045852531967085,
      -1.0995399882799226,
      -1.216696275547031,
      -1.1423187066380356,
      -1.2450096330985323,
      -1.0622647949889568,
      -1.051341969776878,
      -1.0357813935248275,
      -1.0930839970769122,
      -1.0889219214137982,
      -1.0561413408712377,
      -1.1921357592136856,
      -1.1047842623415658,
      -1.1956892396487317,
      -1.1725202140502133,
      -1.1588727823492027,
      -1.072597033961893,
      -1.1770930255158245,
      -0.9774084350525314,
      -1.1759498048574624,
      -1.1950444168987715,
      -1.2338280980347187
    ],
    [
      -1.455569420513479,
      -1.3335647277887888,
      0.0,
      -1.3806879823532447,
      -1.389417784008643,
      -1.4717381338179234,
      -1.3527029510613968,
      -1.2615941128243287,
      -1.25751318610583,
      -1.4832694543809768,
      -1.4003972499823796,
      -1.5992308799885893,
      -1.3721974876873542,
      -1.3919113656191158,
      -1.3020754752950938,
      -1.4164038479527956,
      -1.4299008901753063,
      -1.4260351706420444,
      -1.5260885129834423,
      -1.3922409779821479,
      -1.4524136405000976,
      -1.4909190976718025,
      -1.4830887063201892,
      -1.2799062508907344,
      -1.4369165224911524,
      -1.3344890309547381,
      -1.400270448349665,
      -1.4560876376992247,
      -1.5853113558764471
    ],
    [
      -1.1887485689622757,
      -1.1182438584401462,
      -1.1006243482903943,
      0.0,
      -1.1251440278128584,
      -1.1236084399024404,
      -1.1554453975815828,
      -1.1132943166429938,
      -1.068429621389851,
      -1.2038067550567504,
      -1.1157482701751318,
      -1.2474542459629014,
      -1.0798654083046526,
      -1.1188354723128138,
      -1.1221493340307735,
      -1.0883116208208408,
      -1.1142880779980653,
      -1.17049089415009,
      -1.1927079823334894,
      -1.122971326745046,
      -1.1507224097905926,
      -1.1789250273075567,
      -1.2036585894462222,
      -1.114614934003506,
      -1.1415586600686332,
      -1.1013227318903416,
      -1.0651476968206697,
      -1.1553678792940487,
      -1.2173062802884886
    ],
    [
      -1.394095127290899,
      -1.3668940510406071,
      -1.4083075888137926,
      -1.4222271016300563,
      0.0,
      -1.443038750112746,
      -1.3749873825811463,
      -1.4044868301647415,
      -1.3981088350999307,
      -1.3819159410496573,
      -1.3993508382197208,
      -1.491265129558993,
      -1.3475835030337224,
      -1.3337102531306744,
      -1.3809085161405503,
      -1.404955231988289,
      -1.3364537558568186,
      -1.4506144649027912,
      -1.4142746939628115,
      -1.3800173790925403,
      -1.3785920974608648,
      -1.4642140353002793,
      -1.42690478277425,
      -1.3652050935626374,
      -1.392888920554385,
      -1.3703060464182724,
      -1.379425898510986,
      -1.475130440067371,
      -1.507431431169274
    ],
    [
      -1.25947002837171,
      -1.145286944711229,
      -1.1532940673026633,
      -1.161392629845829,
      -1.1995166990661537,
      0.0,
      -1.1686204951968826,
      -1.1389243423311235,
      -1.2106865193974818,
      -1.283554749462315,
      -1.1478657199308113,
      -1.2986679676238095,
      -1.1363350431770725,
      -1.1382388627994335,
      -1.167813680112908,
      -1.1099475715061011,
      -1.196424297300858,
      -1.1116229482606916,
      -1.2680651065620638,
      -1.1355322701615682,
      -1.2513949988047595,
      -1.2673753565723092,
      -1.2515030706560382,
      -1.1737126531083968,
      -1.2093504284399896,
      -1.1595062186525404,
      -1.2106701342570463,
      -1.2309641838814784,
      -1.2778738632492517
    ],
    [
      -1.3474399497932499,
      -1.2005527457439138,
      -1.226848259716677,
      -1.3688920922708467,
      -1.2993402672257914,
      -1.3506253843291567,
      0.0,
      -1.2332725622870748,
      -1.2391041375968674,
      -1.3222083196576424,
      -1.3300745414865625,
      -1.422079374871615,
      -1.3085249025100962,
      -1.2352308757228128,
      -1.212369195917982,
      -1.268681562092852,
      -1.3063381537633088,
      -1.3163142491045319,
      -1.428915205819986,
      -1.3569257387325278,
      -1.3309851441191107,
      -1.326810813237197,
      -1.351380709389947,
      -1.2681845105705531,
      -1.3195942301042922,
      -1.2283047302819543,
      -1.3006924295072029,
      -1.3939121238743317,
      -1.460891728416122
    ],
    [
      -1.5622034046797884,
      -1.3656145210302597,
      -1.422076913243061,
      -1.5410890370621582,
      -1.5314773209697161,
      -1.5408798641816528,
      -1.486525735821656,
      0.0,
      -1.4629412101205197,
      -1.6220423511621296,
      -1.4722797329994157,
      -1.6761755908396112,
      -1.4556822311825595,
      -1.50160911996139,
      -1.4809513851343112,
      -1.5027637899308335,
      -1.5472657683492428,
      -1.5100958326244933,
      -1.572091011513863,
      -1.5080852707158037,
      -1.6018787935165033,
      -1.501810869896566,
      -1.6009923670312465,
      -1.4328997596191588,
      -1.5685222158013319,
      -1.419678056067415,
      -1.586959530818085,
      -1.5867019282283132,
      -1.6535205376960969
    ],
    [
      -1.3798769325234734,
      -1.266104706583124,
      -1.1439837807756608,
      -1.249403808160662,
      -1.3146688184735242,
      -1.3531987892119048,
      -1.2509281604144509,
      -1.2215455349341968,
      0.0,
      -1.3252907235309148,
      -1.2785493558753194,
      -1.4138259643232607,
      -1.2722838081695746,
      -1.2622142064897897,
      -1.1880910924760195,
      -1.2915319325569679,
      -1.3159736165287763,
      -1.3406810226479577,
      -1.376499950446563,
      -1.3059550135210432,
      -1.331804653448647,
      -1.3154233092532905,
      -1.357820752340347,
      -1.2218893828162778,
      -1.3344712296765544,
      -1.2628737419868357,
      -1.3201703272315468,
      -1.337859675923539,
      -1.4076023427649689
    ],
    [
      -1.5839128464339876,
      -1.5678540401478633,
      -1.4639374956686915,
      -1.54699240216994,
      -1.5227421946509352,
      -1.5889970516285952,
      -1.5446409927468345,
      -1.5410995859946184,
      -1.53429932096995,
      0.0,
      -1.5564068946663372,
      -1.6450474105405017,
      -1.546458707317004,
      -1.5066123968661347,
      -1.613105445240021,
      -1.5681688323911336,
      -1.5323722887152216,
      -1.597254143498699,
      -1.515237971598211,
      -1.4907223027009937,
      -1.5087381184913422,
      -1.6054340037462775,
      -1.5975168031064797,
      -1.5309265099610692,
      -1.475813811464558,
      -1.5816537438954665,
      -1.5120562008476952,
      -1.568633455168243,
      -1.5980130836170996
    ],
    [
      -1.3118074836394717,
      -1.266212178595012,
      -1.2687114737584533,
      -1.2891077233259154,
      -1.2746389621977712,
      -1.2969518025617495,
      -1.3054360480465204,
      -1.2202793836736343,
      -1.2446985233025858,
      -1.3807495262273313,
      0.0,
      -1.4287183766967722,
      -1.2518112419677263,
      -1.2149875930705083,
      -1.2353236010644617,
      -1.2351014642746305,
      -1.255761511825472,
      -1.2588885097349534,
      -1.3222604762408467,
      -1.2312674672170918,
      -1.2890635622424242,
      -1.3465857840670816,
      -1.3204528022061333,
      -1.2364462545778163,
      -1.2826567485838378,
      -1.2438060215989502,
      -1.2921741895932337,
      -1.3310180293595908,
      -1.353563602425305
    ],
    [
      -1.2617969720594777,
      -1.251126204330287,
      -1.2337725276457463,
      -1.25352765350473,
      -1.223273729868222,
      -1.230765764483141,
      -1.2290811718183812,
      -1.248120676391754,
      -1.2452594398119718,
      -1.2540881012914613,
      -1.247469969664014,
      0.0,
      -1.2228072158275483,
      -1.2274731395012433,
      -1.2784967068730775,
      -1.2533106930186897,
      -1.219518006356887,
      -1.2486432310800875,
      -1.2247335635011871,
      -1.2310524864842864,
      -1.2890810373924648,
      -1.232243237032315,
      -1.2029644931683825,
      -1.2531467461154304,
      -1.2427364635298592,
      -1.2340859587276207,
      -1.2448747811816274,
      -1.2633478274268795,
      -1.2171475191481298
    ],
    [
      -1.2499399284390127,
      -1.1688503786809712,
      -1.1935258844765804,
      -1.231651790365312,
      -1.2175882505013265,
      -1.2414193100744384,
      -1.2193768782312808,
      -1.1581389903135884,
      -1.2084933151462953,
      -1.3216850830253546,
      -1.1844144425161447,
      -1.3693999977257998,
      0.0,
      -1.1457599813478059,
      -1.2135533637186753,
      -1.1873142459569168,
      -1.219369010466029,
      -1.1878787678945757,
      -1.2606318948112827,
      -1.2037849630187198,
      -1.3219389404638304,
      -1.317551736159123,
      -1.277829025288973,
      -1.2017303224544877,
      -1.2195374301198707,
      -1.1640247700008928,
      -1.2553460544208335,
      -1.31391958509481,
      -1.3411499982629438
    ],
    [
      -1.221206281594199,
      -1.1623037183356688,
      -1.1519173854371916,
      -1.15200946743183,
      -1.1261364296394558,
      -1.1782295006062091,
      -1.162889628110746,
      -1.1707983953687566,
      -1.0815588927441568,
      -1.202403529033509,
      -1.119316101080281,
      -1.3230042979583567,
      -1.10616644540156,
      0.0,
      -1.1651246930613837,
      -1.156468441821146,
      -1.1805739781341398,
      -1.204052447463742,
      -1.2277655378812946,
      -1.1515223066366884,
      -1.1787582019803005,
      -1.2805804163657946,
      -1.2096640646589198,
      -1.1575810978732246,
      -1.2092214987235386,
      -1.1312528615809294,
      -1.1663032294379567,
      -1.2673785403019164,
      -1.3053148474421403
    ],
    [
      -1.4042413389225774,
      -1.248622627276313,
      -1.2848988205417462,
      -1.4033802355439091,
      -1.3618323031989115,
      -1.434548145399136,
      -1.3015031026545807,
      -1.2467569881413285,
      -1.2758836823019708,
      -1.4980279586000216,
      -1.2881757052930352,
      -1.5192299671953016,
      -1.3322825271671812,
      -1.3646358214093266,
      0.0,
      -1.3683721138792737,
      -1.3948073007424062,
      -1.3571770815169868,
      -1.4734320835791561,
      -1.3944006614850661,
      -1.4247557644449462,
      -1.3887787439443888,
      -1.4502405954725435,
      -1.2779088009167778,
      -1.442030588847847,
      -1.2769515846124717,
      -1.4033117725689728,
      -1.4658902081546046,
      -1.4848253934756284
    ],
    [
      -1.2853034239092498,
      -1.1754835025186097,
      -1.2013278316435898,
      -1.2287001646133946,
      -1.2032142114830435,
      -1.2111803758635207,
      -1.2032285044756845,
      -1.1647708046839749,
      -1.2669243667720984,
      -1.314898978725977,
      -1.1517646772780363,
      -1.34746903018304,
      -1.144319250956938,
      -1.1676784040131607,
      -1.1525639529020075,
      0.0,
      -1.2039229347285556,
      -1.2181979525489788,
      -1.2863299684340879,
      -1.19229301211615,
      -1.3111286567370757,
      -1.2955276734332177,
      -1.2794040264689264,
      -1.2229320366443668,
      -1.247221424399179,
      -1.199918625230082,
      -1.2647321125800146,
      -1.29586286787507,
      -1.3403764452868605
    ],
    [
      -1.4451505233071282,
      -1.4672340421725845,
      -1.5023223768747356,
      -1.5020226960191496,
      -1.4878840133320386,
      -1.486149133110729,
      -1.5257542176170207,
      -1.4611733040485335,
      -1.5401932735752466,
      -1.5460419586157406,
      -1.4790259069331453,
      -1.5490559729324684,
      -1.467296159535154,
      -1.4612102179088715,
      -1.5441794463158072,
      -1.4439483860848694,
      0.0,
      -1.4932091606761737,
      -1.5388651213637936,
      -1.4821553090261361,
      -1.4177747210228353,
      -1.575332326338582,
      -1.4530406231144333,
      -1.4960882362362657,
      -1.503261958578821,
      -1.4922654691412174,
      -1.512970606196156,
      -1.4943319164762123,
      -1.5793211294782667
    ],
    [
      -1.2514328545734226,
      -1.0733450961786,
      -1.2160583079069416,
      -1.2715919997394947,
      -1.255660255241315,
      -1.2347347287992583,
      -1.220771667341631,
      -1.1261130707784854,
      -1.2400281604441048,
      -1.3351449343788722,
      -1.1580536760952216,
      -1.318515986973448,
      -1.098292625498452,
      -1.1767370527659344,
      -1.1964025781734324,
      -1.1787767359184234,
      -1.2269289227255207,
      0.0,
      -1.2962171003116396,
      -1.2413535346559483,
      -1.2928767004914825,
      -1.2802748591931017,
      -1.277535200807411,
      -1.1833650635331368,
      -1.2462091376828073,
      -1.1791697233713256,
      -1.2290186990533236,
      -1.2801853351580283,
      -1.3211825582995744
    ],
    [
      -1.2296014866318614,
      -1.2670467866040374,
      -1.247049901839749,
      -1.308390027215181,
      -1.2879752030678182,
      -1.2742791117276184,
      -1.3069614225214883,
      -1.2202500003529584,
      -1.3515519711145922,
      -1.2888661024553807,
      -1.2534611490176746,
      -1.3176999560810199,
      -1.227713568881427,
      -1.2656483409568746,
      -1.311213579811602,
      -1.3030313389412527,
      -1.2763247806242464,
      -1.278753917944646,
      0.0,
      -1.3051241907661302,
      -1.3139863761370996,
      -1.3320040817230978,
      -1.285537909423757,
      -1.2798433041329902,
      -1.2688968206881748,
      -1.2800363897139664,
      -1.2918934905077675,
      -1.305112319172876,
      -1.3086203746298828
    ],
    [
      -1.3055438002423816,
      -1.192111026049126,
      -1.2310544820703422,
      -1.2196622157746506,
      -1.2343542479790761,
      -1.2493826381855473,
      -1.2623242413652545,
      -1.2032402850497113,
      -1.213087543201981,
      -1.3021464996422456,
      -1.2281119622164665,
      -1.351898878666276,
      -1.2003007244995478,
      -1.2181750744487907,
      -1.264075675081031,
      -1.2081580489159476,
      -1.2431399576600204,
      -1.2356710855408326,
      -1.3421000871942206,
      0.0,
      -1.3037467251910564,
      -1.3302411787719437,
      -1.2879704010407773,
      -1.2283389262822912,
      -1.2494607831260325,
      -1.209832000549246,
      -1.2229808726493294,
      -1.24462727758286,
      -1.3401374989999133
    ],
    [
      -1.6481983321721103,
      -1.6260921242162585,
      -1.6158127585409878,
      -1.606382014385498,
      -1.586989148272252,
      -1.6362040309267476,
      -1.5907916754042715,
      -1.6110063783679154,
      -1.579226741519517,
      -1.6264486798884292,
      -1.6039482323082028,
      -1.6901885264281513,
      -1.636542212285084,
      -1.587537988351769,
      -1.6353149357278405,
      -1.6029953557801648,
      -1.581343624220119,
      -1.6552502369480233,
      -1.6873882190684886,
      -1.5943514069598554,
      0.0,
      -1.7049986511820967,
      -1.6028602471918882,
      -1.6139445596363111,
      -1.607303584786198,
      -1.6232968607370715,
      -1.5048194412539384,
      -1.606120162273876,
      -1.6958214976116583
    ],
    [
      -1.2975411271106616,
      -1.2107701279141705,
      -1.19835178256277,
      -1.2967823060286057,
      -1.2890363494676191,
      -1.3088944243523417,
      -1.200428916956953,
      -1.1910275860326385,
      -1.1876539802979174,
      -1.327491475257592,
      -1.2761111899067512,
      -1.2977357716436062,
      -1.2713147983096817,
      -1.2825912398417925,
      -1.2172408913991,
      -1.2627306627102495,
      -1.2608097739575521,
      -1.2539688144895602,
      -1.3305595648574837,
      -1.2911594804975897,
      -1.328497967243147,
      0.0,
      -1.3076716128800687,
      -1.2030300262068216,
      -1.3192020127601847,
      -1.2131687982657353,
      -1.3105367710769116,
      -1.3058225875769625,
      -1.2932899853706006
    ],
    [
      -1.3327882517243197,
      -1.3232427289756457,
      -1.2833045094265232,
      -1.3241094866643075,
      -1.2685732147632038,
      -1.274426962488327,
      -1.356887959326599,
      -1.299912025693022,
      -1.328805852112418,
      -1.3954307483756119,
      -1.2670152942824031,
      -1.3782070732010523,
      -1.272244069034631,
      -1.3113696568257358,
      -1.338452198906689,
      -1.2908811635343178,
      -1.224808236943774,
      -1.3106563458326068,
      -1.357362570488136,
      -1.2914625421826897,
      -1.2869217810099924,
      -1.4000296269171246,
      0.0,
      -1.2983768720670958,
      -1.2152339182391774,
      -1.3299114757211363,
      -1.262436009413988,
      -1.2870214558646815,
      -1.3827184006127922
    ],
    [
      -1.4395071532400467,
      -1.3257783827208245,
      -1.204412232497876,
      -1.3449386578498215,
      -1.3939118924108598,
      -1.4363578316065586,
      -1.355110918858335,
      -1.2894217736913467,
      -1.2962886155470104,
      -1.4384722328528488,
      -1.4177716293666878,
      -1.523756457546048,
      -1.3473641167215586,
      -1.3651494500452548,
      -1.3325312062215113,
      -1.4162473515623761,
      -1.3775551169174416,
      -1.4053090025263775,
      -1.4112593136453577,
      -1.375544248332826,
      -1.42850332622604,
      -1.4237899607743867,
      -1.4513217122440059,
      0.0,
      -1.4051369437881327,
      -1.3102941654467788,
      -1.3214482251510675,
      -1.4394429915757134,
      -1.5247704898616716
    ],
    [
      -1.4709014330483356,
      -1.4127177579562273,
      -1.4061373644298976,
      -1.3577549948042056,
      -1.3946616435566213,
      -1.4238844038140428,
      -1.411486047078237,
      -1.416465756938568,
      -1.3589938265346646,
      -1.4299859452410373,
      -1.4013551815069123,
      -1.5100219515454574,
      -1.3984348883728253,
      -1.4217906807554967,
      -1.4387272799050301,
      -1.4011725551974348,
      -1.371622115800706,
      -1.432261103898921,
      -1.456421236612093,
      -1.3959020019766677,
      -1.372770232452893,
      -1.4909142103387845,
      -1.3807645690757844,
      -1.3781279131698339,
      0.0,
      -1.4192637365285414,
      -1.3730429323990452,
      -1.3824712240160075,
      -1.51021003410331
    ],
    [
      -1.5784207227230411,
      -1.4050580555004561,
      -1.4510795829987602,
      -1.5229213020698351,
      -1.5393642980470308,
      -1.533952634507497,
      -1.4978704761833292,
      -1.4022447089599217,
      -1.495378260135759,
      -1.6462534888302944,
      -1.4925308733316978,
      -1.6369330476888613,
      -1.4632358214588748,
      -1.4946903060040817,
      -1.4539692043387855,
      -1.4981658220950547,
      -1.5248148788238907,
      -1.5105921537989415,
      -1.572610157578561,
      -1.4764777061388439,
      -1.5721221133732144,
      -1.5236722590228216,
      -1.6012318321937076,
      -1.4330795186807566,
      -1.573218685688581,
      0.0,
      -1.5837829995852581,
      -1.5824563927811932,
      -1.6141622747055557
    ],
    [
      -1.6336060920351378,
      -1.6073020055441398,
      -1.5648415021262236,
      -1.5393123692935287,
      -1.558678886864063,
      -1.6396001647584675,
      -1.5966941702366817,
      -1.5720073346756986,
      -1.566949251881908,
      -1.6627742955991551,
      -1.6133393772015028,
      -1.744789007093944,
      -1.5539212216117082,
      -1.5678800149923289,
      -1.6102770226272671,
      -1.6120281639059282,
      -1.568226580968905,
      -1.6104841637531377,
      -1.684146649617822,
      -1.585223989649633,
      -1.4981032375634462,
      -1.7168370971611941,
      -1.6175594686452146,
      -1.5390388640967334,
      -1.6087115259482405,
      -1.626921224708693,
      0.0,
      -1.6287922010897982,
      -1.7303893868527982
    ],
    [
      -1.4532705409456457,
      -1.3899990883687439,
      -1.376146982948993,
      -1.343013118085079,
      -1.379483295917668,
      -1.3608674732787933,
      -1.4121302628333225,
      -1.3674716528872295,
      -1.3428972057363122,
      -1.471793113676964,
      -1.3754318535089824,
      -1.4513406268741917,
      -1.3603017325593256,
      -1.3976077135167289,
      -1.415236334814363,
      -1.347825600603438,
      -1.3757792983054393,
      -1.3909381748937344,
      -1.4244054318436907,
      -1.3293371289598406,
      -1.3794068584666146,
      -1.4156462219413433,
      -1.3995552409981475,
      -1.3672496509496377,
      -1.3484188085652604,
      -1.4059797703223464,
      -1.3639298038008192,
      0.0,
      -1.4395801306459275
    ],
    [
      -1.2905473497834976,
      -1.2476330349568854,
      -1.256627222592738,
      -1.2449137912035801,
      -1.2332625966104043,
      -1.2319702113655158,
      -1.294150836654619,
      -1.2446383263975092,
      -1.2802736838500346,
      -1.2819817710266075,
      -1.2419171461138403,
      -1.2527664068549607,
      -1.2389618192788776,
      -1.2592956682101732,
      -1.2839778238896589,
      -1.2309087790844382,
      -1.211061317310924,
      -1.2512299827978235,
      -1.225233088211285,
      -1.2420520084105446,
      -1.3058476756457458,
      -1.2305749255863128,
      -1.2312508160017175,
      -1.239089048757421,
      -1.2776600197225678,
      -1.253992667425357,
      -1.274096055111534,
      -1.2298259827892517,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2231971212344268,
      0.23165771385595146,
      0.21389675109684392,
      0.21377728335186275,
      0.1819320253867005,
      0.2167996317245855,
      0.2173529951607529,
      0.229464779382472,
      0.17313738509550225,
      0.22896468189530772,
      0.17727382931486435,
      0.218755333201893,
      0.2435252954904401,
      0.22602866384946907,
      0.21358067671767378,
      0.24566308162916428,
      0.20140892525073806,
      0.17962943595091518,
      0.20884088730899686,
      0.17675140671566503,
      0.1834288618610307,
      0.20586038736304468,
      0.21125678710519713,
      0.20417388568326045,
      0.21266005781086506,
      0.21585185264157314,
      0.17853702767772717,
      0.15850349475033365
    ],
    [
      0.3870691913217845,
      0.0,
      0.49831590619957367,
      0.37903547548934236,
      0.44408840937219884,
      0.38744222175182874,
      0.5167860700673363,
      0.5432046930228756,
      0.4482499579396615,
      0.33109367067255313,
      0.4054712395815485,
      0.3027803131210518,
      0.48552515123062734,
      0.4964479764427061,
      0.5120085526947566,
      0.45470594914267193,
      0.4588680248057859,
      0.4916486053483464,
      0.35565418700589846,
      0.4430056838780183,
      0.35210070657085235,
      0.37526973216937076,
      0.38891716387038144,
      0.475192912257691,
      0.3706969207037596,
      0.5703815111670527,
      0.37184014136212173,
      0.3527455293208126,
      0.3139618481848654
    ],
    [
      0.42899178269791927,
      0.5509964754226095,
      0.0,
      0.5038732208581536,
      0.4951434192027553,
      0.4128230693934749,
      0.5318582521500015,
      0.6229670903870697,
      0.6270480171055683,
      0.4012917488304215,
      0.48416395322901873,
      0.28533032322280905,
      0.5123637155240441,
      0.49264983759228254,
      0.5824857279163045,
      0.4681573552586027,
      0.454660313036092,
      0.45852603256935387,
      0.358472690227956,
      0.49232022522925045,
      0.4321475627113007,
      0.3936421055395958,
      0.4014724968912091,
      0.6046549523206639,
      0.4476446807202459,
      0.5500721722566602,
      0.48429075486173323,
      0.42847356551217364,
      0.2992498473349512
    ],
    [
      0.3301383817861563,
      0.40064309230828576,
      0.4182626024580376,
      0.0,
      0.39374292293557356,
      0.3952785108459915,
      0.3634415531668491,
      0.4055926341054381,
      0.45045732935858096,
      0.31508019569168155,
      0.40313868057330016,
      0.2714327047855305,
      0.43902154244377933,
      0.40005147843561817,
      0.3967376167176584,
      0.4305753299275912,
      0.40459887275036666,
      0.34839605659834194,
      0.32617896841494254,
      0.39591562400338587,
      0.36816454095783935,
      0.3399619234408753,
      0.31522836130220977,
      0.40427201674492586,
      0.3773282906797988,
      0.41756421885809036,
      0.45373925392776226,
      0.3635190714543832,
      0.3015806704599433
    ],
    [
      0.39285788313800407,
      0.42005895938829596,
      0.3786454216151105,
      0.3647259087988468,
      0.0,
      0.3439142603161571,
      0.4119656278477568,
      0.38246618026416157,
      0.3888441753289724,
      0.4050370693792458,
      0.38760217220918225,
      0.29568788086991016,
      0.43936950739518066,
      0.4532427572982287,
      0.40604449428835276,
      0.3819977784406141,
      0.4504992545720845,
      0.33633854552611187,
      0.3726783164660916,
      0.4069356313363628,
      0.40836091296803834,
      0.3227389751286238,
      0.36004822765465305,
      0.42174791686626567,
      0.394064089874518,
      0.4166469640106307,
      0.40752711191791713,
      0.31182257036153205,
      0.27952157925962906
    ],
    [
      0.31411403142465,
      0.4282971150851309,
      0.4202899924936967,
      0.41219142995053093,
      0.3740673607302063,
      0.0,
      0.40496356459947735,
      0.43465971746523646,
      0.3628975403988781,
      0.290029310334045,
      0.42571833986554863,
      0.2749160921725504,
      0.4372490166192875,
      0.4353451969969264,
      0.405770379683452,
      0.46363648829025883,
      0.37715976249550187,
      0.4619611115356683,
      0.3055189532342961,
      0.43805178963479174,
      0.3221890609916005,
      0.3062087032240508,
      0.3220809891403218,
      0.39987140668796317,
      0.3642336313563703,
      0.4140778411438195,
      0.3629139255393137,
      0.3426198759148815,
      0.2957101965471083
    ],
    [
      0.36193417008413653,
      0.5088213741334726,
      0.4825258601607094,
      0.3404820276065397,
      0.410033852651595,
      0.3587487355482297,
      0.0,
      0.4761015575903116,
      0.47026998228051897,
      0.387165800219744,
      0.37929957839082395,
      0.28729474500577146,
      0.4008492173672902,
      0.47414324415457365,
      0.4970049239594043,
      0.4406925577845344,
      0.40303596611407766,
      0.39305987077285454,
      0.2804589140574003,
      0.35244838114485866,
      0.3783889757582757,
      0.38256330664018945,
      0.3579934104874394,
      0.44118960930683326,
      0.3897798897730942,
      0.4810693895954321,
      0.40868169037018354,
      0.3154619960030547,
      0.24848239146126438
    ],
    [
      0.3975946938422503,
      0.594183577491779,
      0.5377211852789776,
      0.4187090614598805,
      0.42832077755232256,
      0.4189182343403859,
      0.47327236270038275,
      0.0,
      0.496856888401519,
      0.3377557473599091,
      0.48751836552262295,
      0.28362250768242747,
      0.5041158673394792,
      0.4581889785606488,
      0.47884671338772744,
      0.4570343085912052,
      0.4125323301727959,
      0.44970226589754536,
      0.3877070870081756,
      0.451712827806235,
      0.35791930500553537,
      0.4579872286254727,
      0.3588057314907922,
      0.5268983389028798,
      0.3912758827207068,
      0.5401200424546237,
      0.3728385677039536,
      0.37309617029372544,
      0.3062775608259418
    ],
    [
      0.26274035318542155,
      0.3765125791257711,
      0.4986335049332342,
      0.393213477548233,
      0.32794846723537074,
      0.28941849649699014,
      0.3916891252944441,
      0.42107175077469816,
      0.0,
      0.31732656217798016,
      0.3640679298335756,
      0.2287913213856343,
      0.37033347753932033,
      0.3804030792191053,
      0.45452619323287546,
      0.3510853531519271,
      0.32664366918011867,
      0.30193626306093724,
      0.26611733526233206,
      0.33666227218785183,
      0.310812632260248,
      0.32719397645560444,
      0.28479653336854804,
      0.4207279028926172,
      0.3081460560323406,
      0.3797435437220593,
      0.32244695847734817,
      0.30475760978535593,
      0.2350149429439261
    ],
    [
      0.2680816488098232,
      0.28414045509594743,
      0.3880569995751193,
      0.30500209307387083,
      0.3292523005928756,
      0.2629974436152156,
      0.30735350249697624,
      0.3108949092491924,
      0.31769517427386074,
      0.0,
      0.2955876005774736,
      0.20694708470330903,
      0.3055357879268068,
      0.3453820983776761,
      0.23888905000378968,
      0.2838256628526772,
      0.3196222065285892,
      0.25474035174511167,
      0.3367565236455998,
      0.36127219254281706,
      0.34325637675246856,
      0.24656049149753323,
      0.25447769213733107,
      0.3210679852827416,
      0.37618068377925273,
      0.27034075134834423,
      0.33993829439611556,
      0.2833610400755677,
      0.2539814116267112
    ],
    [
      0.3606764079459166,
      0.40627171299037634,
      0.40377241782693507,
      0.38337616825947296,
      0.3978449293876172,
      0.3755320890236389,
      0.367047843538868,
      0.45220450791175404,
      0.42778536828280256,
      0.2917343653580571,
      0.0,
      0.2437655148886162,
      0.42067264961766204,
      0.45749629851488005,
      0.43716029052092664,
      0.4373824273107578,
      0.41672237975991644,
      0.413595381850435,
      0.3502234153445416,
      0.44121642436829656,
      0.3834203293429641,
      0.3258981075183067,
      0.3520310893792551,
      0.43603763700757203,
      0.3898271430015505,
      0.4286778699864382,
      0.38030970199215464,
      0.34146586222579756,
      0.3189202891600833
    ],
    [
      0.2644669766102048,
      0.2751377443393954,
      0.2924914210239362,
      0.27273629516495257,
      0.3029902188014606,
      0.2954981841865416,
      0.29718277685130134,
      0.27814327227792845,
      0.28100450885771067,
      0.2721758473782212,
      0.2787939790056686,
      0.0,
      0.30345673284213426,
      0.29879080916843925,
      0.24776724179660503,
      0.2729532556509928,
      0.3067459423127956,
      0.27762071758959506,
      0.3015303851684954,
      0.29521146218539607,
      0.2371829112772177,
      0.2940207116373674,
      0.3232994555013,
      0.2731172025542521,
      0.2835274851398233,
      0.2921779899420618,
      0.2813891674880551,
      0.26291612124280306,
      0.3091164295215527
    ],
    [
      0.3718625208169677,
      0.4529520705750092,
      0.4282765647794,
      0.39015065889066847,
      0.4042141987546539,
      0.380383139181542,
      0.40242557102469956,
      0.463663458942392,
      0.41330913410968506,
      0.3001173662306258,
      0.4373880067398357,
      0.2524024515301806,
      0.0,
      0.4760424679081745,
      0.4082490855373051,
      0.43448820329906357,
      0.4024334387899513,
      0.4339236813614047,
      0.3611705544446977,
      0.41801748623726054,
      0.29986350879214996,
      0.3042507130968575,
      0.34397342396700736,
      0.42007212680149264,
      0.4022650191361097,
      0.4577776792550876,
      0.3664563948351469,
      0.3078828641611704,
      0.2806524509930366
    ],
    [
      0.3623485315241364,
      0.42125109478266665,
      0.4316374276811439,
      0.4315453456865055,
      0.45741838347887964,
      0.40532531251212633,
      0.42066518500758954,
      0.41275641774957883,
      0.5019959203741786,
      0.38115128408482635,
      0.46423871203805445,
      0.26055051515997874,
      0.4773883677167754,
      0.0,
      0.4184301200569518,
      0.4270863712971895,
      0.4029808349841957,
      0.3795023656545935,
      0.35578927523704085,
      0.43203250648164704,
      0.404796611138035,
      0.3029743967525409,
      0.3738907484594156,
      0.42597371524511085,
      0.3743333143947969,
      0.45230195153740604,
      0.41725158368037873,
      0.316176272816419,
      0.27823996567619513
    ],
    [
      0.4022505240694092,
      0.5578692357156736,
      0.5215930424502404,
      0.4031116274480775,
      0.44465955979307514,
      0.3719437175928506,
      0.5049887603374059,
      0.5597348748506581,
      0.5306081806900158,
      0.308463904391965,
      0.5183161576989515,
      0.28726189579668504,
      0.4742093358248054,
      0.44185604158266,
      0.0,
      0.4381197491127129,
      0.4116845622495804,
      0.4493147814749998,
      0.3330597794128305,
      0.41209120150692047,
      0.3817360985470404,
      0.4177131190475978,
      0.35625126751944314,
      0.5285830620752088,
      0.36446127414413954,
      0.5295402783795149,
      0.40318009042301384,
      0.34060165483738203,
      0.32166646951635824
    ],
    [
      0.323708240987552,
      0.43352816237819214,
      0.407683833253212,
      0.3803115002834072,
      0.4057974534137583,
      0.39783128903328113,
      0.4057831604211173,
      0.44424086021282694,
      0.3420872981247034,
      0.29411268617082476,
      0.4572469876187655,
      0.2615426347137617,
      0.4646924139398638,
      0.4413332608836411,
      0.4564477119947943,
      0.0,
      0.4050887301682462,
      0.39081371234782303,
      0.32268169646271394,
      0.4167186527806519,
      0.2978830081597261,
      0.3134839914635841,
      0.32960763842787544,
      0.386079628252435,
      0.36179024049762276,
      0.40909303966671984,
      0.3442795523167872,
      0.3131487970217317,
      0.2686352196099413
    ],
    [
      0.39868084640197043,
      0.3765973275365142,
      0.3415089928343631,
      0.34180867368994905,
      0.3559473563770601,
      0.3576822365983696,
      0.318077152092078,
      0.38265806566056515,
      0.30363809613385206,
      0.2977894110933581,
      0.3648054627759534,
      0.2947753967766302,
      0.37653521017394476,
      0.38262115180022715,
      0.2996519233932915,
      0.39988298362422925,
      0.0,
      0.35062220903292496,
      0.30496624834530506,
      0.36167606068296254,
      0.42605664868626336,
      0.2684990433705168,
      0.3907907465946654,
      0.3477431334728329,
      0.3405694111302777,
      0.3515659005678813,
      0.3308607635129426,
      0.34949945323288634,
      0.2645102402308319
    ],
    [
      0.36981191750141207,
      0.5478996758962347,
      0.40518646416789306,
      0.34965277233534,
      0.36558451683351967,
      0.38651004327557636,
      0.40047310473320374,
      0.4951317012963492,
      0.38121661163072984,
      0.2860998376959625,
      0.46319109597961305,
      0.3027287851013867,
      0.5229521465763827,
      0.4445077193089002,
      0.4248421939014022,
      0.4424680361564113,
      0.39431584934931396,
      0.0,
      0.325027671763195,
      0.3798912374188863,
      0.32836807158335213,
      0.34096991288173295,
      0.34370957126742363,
      0.4378797085416979,
      0.37503563439202736,
      0.442075048703509,
      0.392226073021511,
      0.3410594369168063,
      0.3000622137752602
    ],
    [
      0.35647541729382404,
      0.3190301173216481,
      0.3390270020859365,
      0.2776868767105045,
      0.29810170085786725,
      0.311797792198067,
      0.2791154814041972,
      0.36582690357272707,
      0.2345249328110932,
      0.29721080147030476,
      0.3326157549080109,
      0.2683769478446656,
      0.35836333504425855,
      0.32042856296881084,
      0.27486332411408343,
      0.2830455649844328,
      0.3097521233014391,
      0.30732298598103935,
      0.0,
      0.2809527131595553,
      0.2720905277885859,
      0.25407282220258764,
      0.30053899450192856,
      0.30623359979269527,
      0.3171800832375107,
      0.3060405142117191,
      0.294183413417918,
      0.2809645847528095,
      0.27745652929580267
    ],
    [
      0.3552205966769222,
      0.46865337087017767,
      0.4297099148489616,
      0.44110218114465316,
      0.42641014894022766,
      0.4113817587337565,
      0.3984401555540493,
      0.4575241118695925,
      0.4476768537173228,
      0.3586178972770582,
      0.4326524347028373,
      0.30886551825302777,
      0.46046367241975594,
      0.4425893224705131,
      0.39668872183827286,
      0.4526063480033562,
      0.4176244392592834,
      0.4250933113784712,
      0.3186643097250832,
      0.0,
      0.3570176717282474,
      0.3305232181473601,
      0.37279399587852646,
      0.43242547063701253,
      0.4113036137932713,
      0.4509323963700578,
      0.43778352426997436,
      0.4161371193364438,
      0.3206268979193905
    ],
    [
      0.2688691536105823,
      0.2909753615664341,
      0.30125472724170477,
      0.3106854713971945,
      0.3300783375104406,
      0.280863454855945,
      0.3262758103784211,
      0.3060611074147772,
      0.3378407442631757,
      0.29061880589426337,
      0.31311925347448977,
      0.22687895935454128,
      0.2805252734976087,
      0.3295294974309235,
      0.2817525500548521,
      0.31407213000252776,
      0.3357238615625735,
      0.2618172488346693,
      0.22967926671420402,
      0.32271607882283715,
      0.0,
      0.2120688346005959,
      0.31420723859080435,
      0.3031229261463815,
      0.30976390099649453,
      0.2937706250456211,
      0.4122480445287542,
      0.3109473235088165,
      0.22124598817103425
    ],
    [
      0.3643154714982515,
      0.4510864706947426,
      0.463504816046143,
      0.36507429258030744,
      0.37282024914129397,
      0.3529621742565714,
      0.46142768165196,
      0.47082901257627463,
      0.47420261831099575,
      0.3343651233513212,
      0.3857454087021619,
      0.3641208269653069,
      0.39054180029923136,
      0.37926535876712064,
      0.44461570720981314,
      0.39912593589866363,
      0.401046824651361,
      0.4078877841193529,
      0.3312970337514294,
      0.37069711811132344,
      0.33335863136576616,
      0.0,
      0.3541849857288444,
      0.45882657240209146,
      0.34265458584872843,
      0.4486878003431778,
      0.3513198275320015,
      0.35603401103195065,
      0.3685666132383125
    ],
    [
      0.38533909521675236,
      0.3948846179654264,
      0.43482283751454887,
      0.39401786027676455,
      0.4495541321778682,
      0.44370038445274496,
      0.36123938761447305,
      0.41821532124804994,
      0.3893214948286541,
      0.3226965985654602,
      0.45111205265866894,
      0.33992027374001976,
      0.445883277906441,
      0.40675769011533625,
      0.37967514803438296,
      0.42724618340675424,
      0.493319109997298,
      0.40747100110846524,
      0.36076477645293603,
      0.4266648047583823,
      0.4312055659310796,
      0.31809772002394743,
      0.0,
      0.4197504748739762,
      0.5028934287018947,
      0.3882158712199357,
      0.45569133752708413,
      0.4311058910763905,
      0.3354089463282799
    ],
    [
      0.3674145043617205,
      0.4811432748809428,
      0.6025094251038912,
      0.4619829997519458,
      0.4130097651909075,
      0.37056382599520865,
      0.45181073874343225,
      0.5174998839104206,
      0.5106330420547569,
      0.3684494247489185,
      0.3891500282350795,
      0.2831652000557192,
      0.4595575408802086,
      0.4417722075565125,
      0.47439045138025593,
      0.39067430603939113,
      0.42936654068432567,
      0.4016126550753898,
      0.39566234395640953,
      0.4313774092689413,
      0.3784183313757272,
      0.3831316968273806,
      0.3555999453577614,
      0.0,
      0.40178471381363456,
      0.49662749215498847,
      0.48547343245069974,
      0.36747866602605384,
      0.28215116774009563
    ],
    [
      0.3403660344642261,
      0.3985497095563344,
      0.4051301030826642,
      0.45351247270835615,
      0.4166058239559405,
      0.38738306369851894,
      0.39978142043432463,
      0.3948017105739938,
      0.45227364097789713,
      0.38128152227152445,
      0.40991228600564944,
      0.30124551596710436,
      0.4128325791397365,
      0.389476786757065,
      0.3725401876075316,
      0.41009491231512696,
      0.4396453517118557,
      0.3790063636136407,
      0.35484623090046874,
      0.41536546553589404,
      0.43849723505966876,
      0.3203532571737773,
      0.43050289843677736,
      0.4331395543427279,
      0.0,
      0.3920037309840203,
      0.43822453511351656,
      0.4287962434965542,
      0.30105743340925173
    ],
    [
      0.3409868274906065,
      0.5143494947131915,
      0.46832796721488745,
      0.3964862481438125,
      0.38004325216661683,
      0.38545491570615065,
      0.42153707403031837,
      0.5171628412537259,
      0.4240292900778886,
      0.2731540613833532,
      0.42687667688194986,
      0.28247450252478634,
      0.45617172875477285,
      0.4247172442095659,
      0.4654383458748621,
      0.42124172811859295,
      0.39459267138975695,
      0.4088153964147061,
      0.3467973926350867,
      0.44292984407480374,
      0.3472854368404332,
      0.39573529119082607,
      0.31817571801994005,
      0.48632803153289106,
      0.3461888645250666,
      0.0,
      0.3356245506283895,
      0.3369511574324544,
      0.3052452755080919
    ],
    [
      0.4161847411482429,
      0.4424888276392409,
      0.4849493310571571,
      0.5104784638898521,
      0.49111194631931765,
      0.41019066842491325,
      0.45309666294669904,
      0.4777834985076821,
      0.4828415813014728,
      0.3870165375842256,
      0.43645145598187796,
      0.3050018260894367,
      0.49586961157167253,
      0.48191081819105186,
      0.4395138105561136,
      0.43776266927745255,
      0.48156425221447563,
      0.439306669430243,
      0.36564418356555883,
      0.4645668435337478,
      0.5516875956199345,
      0.3329537360221866,
      0.43223136453816613,
      0.5107519690866473,
      0.4410793072351402,
      0.4228696084746877,
      0.0,
      0.42099863209358257,
      0.31940144633058254
    ],
    [
      0.3006159049750101,
      0.3638873575519119,
      0.37773946297166283,
      0.41087332783557673,
      0.37440315000298785,
      0.3930189726418625,
      0.34175618308733324,
      0.3864147930334263,
      0.4109892401843436,
      0.2820933322436918,
      0.37845459241167334,
      0.30254581904646405,
      0.39358471336133016,
      0.35627873240392693,
      0.33865011110629273,
      0.40606084531721787,
      0.37810714761521647,
      0.3629482710269214,
      0.32948101407696506,
      0.4245493169608152,
      0.3744795874540412,
      0.3382402239793125,
      0.35433120492250825,
      0.3866367949710181,
      0.40546763735539537,
      0.34790667559830935,
      0.3899566421198366,
      0.0,
      0.3143063152747283
    ],
    [
      0.3364941653739022,
      0.37940848020051443,
      0.37041429256466185,
      0.38212772395381966,
      0.39377891854699554,
      0.39507130379188404,
      0.33289067850278076,
      0.38240318875989066,
      0.3467678313073652,
      0.3450597441307923,
      0.3851243690435595,
      0.37427510830243915,
      0.3880796958785222,
      0.36774584694722656,
      0.34306369126774094,
      0.3961327360729616,
      0.4159801978464759,
      0.3758115323595763,
      0.40180842694611485,
      0.38498950674685517,
      0.32119383951165403,
      0.39646658957108705,
      0.3957906991556823,
      0.3879524663999787,
      0.349381495434832,
      0.37304884773204283,
      0.3529454600458659,
      0.3972155323681481,
      0.0
    ]
  ],
  "row_avgs": [
    0.20685393780383063,
    0.42544670516769545,
    0.4716346924286508,
    0.3796443730404621,
    0.3836210775900171,
    0.37845510084127015,
    0.3967136220865219,
    0.4342690217292821,
    0.3411700488129953,
    0.30039992187795705,
    0.38718102222555695,
    0.28469447305415035,
    0.38623800857827056,
    0.399644018971727,
    0.4291025088031863,
    0.3741304071644843,
    0.3457150052079517,
    0.39245989471446546,
    0.30083140740121506,
    0.40641174913441463,
    0.297025427695381,
    0.3928058834312322,
    0.40767768870435767,
    0.42115753620074,
    0.396329502474791,
    0.3951114938834831,
    0.4405610020939772,
    0.3651349060546349,
    0.37397937031297757
  ],
  "col_avgs": [
    0.3510575005092056,
    0.4201005305878695,
    0.42013032965427693,
    0.3817089430726215,
    0.38916960125983036,
    0.3630202615662329,
    0.3915051613715022,
    0.4284773949872268,
    0.4101617940181671,
    0.3225045014673512,
    0.3995259734479001,
    0.28121337479909314,
    0.4176749536440291,
    0.4093749914126136,
    0.39650296185640244,
    0.3977048516444853,
    0.3960706335404513,
    0.37750728917711646,
    0.3306523720063101,
    0.39317248741813376,
    0.3576654675319183,
    0.3280360246460683,
    0.34612828499833054,
    0.4145547822322785,
    0.369393970146488,
    0.41199963616216995,
    0.3789097373607876,
    0.3419205028563362,
    0.28855549411048226
  ],
  "combined_avgs": [
    0.2789557191565181,
    0.42277361787778245,
    0.44588251104146387,
    0.38067665805654183,
    0.38639533942492377,
    0.3707376812037515,
    0.39410939172901205,
    0.43137320835825443,
    0.3756659214155812,
    0.31145221167265413,
    0.3933534978367285,
    0.2829539239266218,
    0.40195648111114984,
    0.4045095051921703,
    0.4128027353297944,
    0.3859176294044848,
    0.3708928193742015,
    0.38498359194579096,
    0.3157418897037626,
    0.3997921182762742,
    0.32734544761364964,
    0.36042095403865027,
    0.3769029868513441,
    0.4178561592165092,
    0.38286173631063947,
    0.4035555650228265,
    0.4097353697273824,
    0.35352770445548554,
    0.33126743221172994
  ],
  "gppm": [
    596.9566931719781,
    590.3160909226648,
    583.8144489525288,
    606.1207675934738,
    602.5164710872102,
    613.1548503842522,
    598.2735183511323,
    580.6103464356951,
    586.6102685482364,
    630.0169764800957,
    598.4994217358469,
    651.1698518518635,
    587.8659599900072,
    593.4283918554854,
    596.5556145206283,
    598.3974065756977,
    598.7857334629076,
    608.9047665406571,
    629.4677599140975,
    600.0973496231369,
    613.4589218550747,
    631.6012142553228,
    625.3230009258457,
    587.2510252883093,
    609.9172444178373,
    587.7641724021852,
    605.7703369149605,
    622.8075354831964,
    651.1275304504072
  ],
  "gppm_normalized": [
    1.404247525707328,
    1.4285393749664272,
    1.4161301998320455,
    1.4712074425739001,
    1.4564611673083765,
    1.4873521282948017,
    1.4495075933901813,
    1.413542525919231,
    1.4207083721359552,
    1.5268617945986063,
    1.4480991772952685,
    1.5935622204493307,
    1.4227749501789921,
    1.434516758336407,
    1.4496778160108903,
    1.450002757178465,
    1.4511317614832835,
    1.4767101807137695,
    1.5246996224725515,
    1.4540552780775464,
    1.4886662194630538,
    1.531897549388819,
    1.5140957496677665,
    1.4283677867500004,
    1.4791713839911351,
    1.425489081159429,
    1.4664206726933302,
    1.510256853033456,
    1.5779936682626716
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360
  ],
  "response_lengths": [
    2991,
    2483,
    2573,
    2631,
    2319,
    2857,
    2741,
    2442,
    3366,
    2367,
    2409,
    2702,
    2860,
    2530,
    2387,
    2680,
    2291,
    2447,
    2626,
    2611,
    2401,
    2161,
    2033,
    2663,
    2424,
    2444,
    1993,
    2347,
    2038
  ]
}