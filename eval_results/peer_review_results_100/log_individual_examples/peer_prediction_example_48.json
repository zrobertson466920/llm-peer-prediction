{
  "example_idx": 48,
  "reference": "Published as a conference paper at ICLR 2023\n\nMOAT: ALTERNATING MOBILE CONVOLUTION AND ATTENTION BRINGS STRONG VISION MODELS\n\nChenglin Yang1∗, Siyuan Qiao2, Qihang Yu1, Xiaoding Yuan1, Yukun Zhu2, Alan Yuille1, Hartwig Adam2, Liang-Chieh Chen2 1The Johns Hopkins University 2Google Research\n\nABSTRACT\n\nThis paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% / 81.5% top-1 accuracy on ImageNet-1K / ImageNet-1K-V2 with ImageNet22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% APbox with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tinyMOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. The tiny-MOAT family is also benchmarked on downstream tasks, serving as a baseline for the community. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is publicly available.1\n\n1\n\nINTRODUCTION\n\nThe vision community has witnessed the prevalence of self-attention (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017). The success of Transformers in natural language processing motivates the creation of their variants for vision recognition. The Vision Transformer (ViT) (Dosovitskiy et al., 2021) has great representation capacity with global receptive field. However, it requires pretraining on a large-scale proprietary dataset (Sun et al., 2017). Its unsatisfying performance, when trained with a small number of images, calls for the need of better training recipes (Touvron et al., 2021a; Steiner et al., 2021) or architectural designs (Liu et al., 2021; Graham et al., 2021). On the other hand, ConvNet has been the dominant network choice since the advent of AlexNet (Krizhevsky et al., 2012) in 2012. Vision researchers have condensed the years of network design experience into multiple principles, and have started to incorporate them to vision transformers. For example, there are some works adopting the ConvNet’s hierarchical structure to extract multi-scale features for vision transformers (Liu et al., 2021; Fan et al., 2021; Wang et al., 2022), and others proposing to integrate the translation equivariance of convolution into transformers (Graham et al., 2021; d’Ascoli et al., 2021; Xiao et al., 2021).\n\nAlong the same direction of combining the best from Transformers and ConvNets, CoAtNet (Dai et al., 2021) and MobileViT (Mehta & Rastegari, 2022a) demonstrate outstanding performance by\n\n∗Work done while an intern at Google. 1Official code in TensorFlow: https://github.com/google-research/deeplab2\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nstacking Mobile Convolution (MBConv) blocks (i.e., inverted residual blocks (Sandler et al., 2018)) and Transformer blocks (i.e., a self-attention layer and a Multi-Layer Perceptron (MLP)). However, both works focus on the macro-level network design. They consider MBConv and Transformer blocks as individual separate ones, and systematically study the effect of stacking them to strike a better balance between the remarkable efficiency of MBConv and strong capacity of Transformer.\n\nIn this work, on the contrary, we study the micro-level building block design by taking a deeper look at the combination of MBConv and Transformer blocks. We make two key observations after a careful examination of those blocks. First, the MLP module in Transformer block is similar to MBConv, as both adopt the inverted bottleneck design. However, MBConv is a more powerful operation by employing one extra 3 × 3 depthwise convolution (to encode local interaction between pixels), and more activation (Hendrycks & Gimpel, 2016) and normalization (Ioffe & Szegedy, 2015) are employed between convolutions. Second, to extract multi-scale features using Transformer blocks, one may apply the average-pooling (with stride 2) to input features before the self-attention layer. However, the pooling operation reduces the representation capacity of self-attention. Our observations motivate us to propose a novel MObile convolution with ATtention (MOAT) block, which efficiently combines MBConv and Transformer blocks. The proposed MOAT block modifies the Transformer block by first replacing its MLP with a MBConv block, and then reversing the order of attention and MBConv. The replacement of MLP with MBConv brings more representation capacity to the network, and reversing the order (MBConv comes before self-attention) delegates the downsampling duty to the strided depthwise convolution within the MBConv, learning a better downsampling kernel.\n\nWe further develop a family of MOAT models by stacking and increasing the channels of network blocks. Surprisingly, our extremely simple design results in a remarkable impact. On the challenging ImageNet-1K classification benchmark (Russakovsky et al., 2015), our model (190M parameters) achieves 86.7% top-1 accuracy without extra data. When further pretraining on ImageNet-22K, our best model (483M parameters) attains 89.1% / 81.5% top-1 accuracy on ImageNet-1K (Tab. 2) / ImageNet-1K-V2 (Tab. 9), setting a new state-of-the-art.\n\nAdditionally, MOAT can be seamlessly deployed to downstream tasks that require large resolution inputs by simply converting the global attention to non-overlapping local window attention. Thanks to the MBConv that effectively exchanges local information between pixels (enabling cross-window propagation), MOAT does not need the extra window-shifting mechanism (Liu et al., 2021). As a result, on COCO object detection (Lin et al., 2014) and ADE20K semantic segmentation (Zhou et al., 2019), MOAT shows superior performances. Specifically, on COCO object detection (Tab. 3), our best model (227M parameters), achieves 59.2% APbox with single-scale inference and hard NMS, setting a new state-of-the-art in the regime of model size 200M with Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017). On ADE20K semantic segmentation (Tab. 4), our best model (496M parameters), adopting DeepLabv3+ (Chen et al., 2018), attains 57.6% mIoU with single-scale inference, also setting a new state-of-the-art in the regime of models using input size 641 × 641.\n\nFinally, to explore the scalability of MOAT models, we simply scale down the models by reducing the channel sizes (without any other change), resulting in the tiny-MOAT family, which also surprisingly outperforms mobile-specific transformer-based models, such as Mobile-Former (Chen et al., 2022c) and MobileViTs (Mehta & Rastegari, 2022a;b). Specifically, in the regime of model parameters 5M, 10M, and 20M, our tiny MOAT outperforms the concurrent MobileViTv2 (Mehta & Rastegari, 2022b) by 1.1%, 1.3%, and 2.0% top-1 accuracy on ImageNet-1K classification benchmark (Tab. 5). Furthermore, we benchmark tiny-MOAT on COCO object detection and ADE20K semantic segmentation.\n\nIn summary, our method advocates the design principle of simplicity. Without inventing extra complicated operations, the proposed MOAT block effectively merges the strengths of both mobile convolution and self-attention into one block by a careful redesign. Despite its conceptual simplicity, impressive results have been obtained on multiple core vision recognition tasks. We hope our study will inspire future research on seamless integration of convolution and self-attention.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Block comparison. (a) The MBConv block (Sandler et al., 2018) employs the inverted bottleneck design with depthwise convolution and squeeze-and-excitation (Hu et al., 2018) applied to the expanded features. (b) The Transformer block (Vaswani et al., 2017) consists of a self-attention module and a MLP module. (c) The proposed MOAT block effectively combines them. The illustration assumes the input tensor has channels c.\n\n2 METHOD\n\nHerein, we review the Mobile Convolution (MBConv) (Sandler et al., 2018) and Transformer (Vaswani et al., 2017) blocks before introducing the proposed MOAT block. We then present MOAT, a family of neural networks, targeting at different trade-offs between accuracy and model complexity.\n\n2.1 MOBILE CONVOLUTION AND TRANSFORMER BLOCKS\n\nMBConv block. Also known as the inverted residual block, the Mobile Convolution (MBConv) (Sandler et al., 2018) block (Fig. 1 (a)) is an effective building block that has been widely used in mobile models (Howard et al., 2019; Mehta & Rastegari, 2022a) or efficient models (Tan & Le, 2019; Dai et al., 2021). Unlike the bottleneck block in ResNet (He et al., 2016a), the MBConv block employs the design of an “inverted bottleneck”, together with the efficient depthwise convolution (Howard et al., 2017). Specifically, a 1 × 1 convolution is first applied to expand the input channels by a factor of 4. Then, a 3 × 3 depthwise convolution is used to effectively capture the local spatial interactions between pixels. Finally, the features are projected back to the original channel size via a 1 × 1 convolution, enabling a residual connection (He et al., 2016a). An optional Squeeze-and-Excitation (SE) (Hu et al., 2018) module (which uses the global information to re-weight the channel activation) may also be used after the depthwise convolution, following MobileNetV3 (Howard et al., 2019). Note that one could tune the channel expansion ratio and depthwise convolution kernel size for better performance. We fix them throughout the experiments for simplicity. Formally, given an input tensor x ∈ RH×W ×C (H, W, C are its height, width, and channels), the MBConv block is represented as follows:\n\nMBConv(x) = x + (N2 ◦ S ◦ D ◦ N1)(BN(x)),\n\nN1(x) = GeLU(BN(Conv(x))),\n\nD(x) = GeLU(BN(DepthConv(x))), S(x) = σ(MLP(GAP(x)) · x,\n\nN2(x) = Conv(x),\n\n(1)\n\n(2) (3)\n\n(4)\n\n(5)\n\nwhere BN, GeLU, GAP, and MLP stand for Batch Normalization (Ioffe & Szegedy, 2015), Gaussian error Linear Unit (Hendrycks & Gimpel, 2016), Global Average Pooling, and Multi-Layer Perceptron (with reduction ratio 4 and hard-swish (Ramachandran et al., 2017)), respectively. The MBConv block consists of four main functions: N1, D, S, and N2, which correspond to the 1 × 1 convolution for channel expansion (by 4×), 3 × 3 depthwise convolution, squeeze-and-excitation (Hu et al., 2018) (σ is the sigmoid function), and 1 × 1 convolution for channel projection (by 4×), respectively.\n\n3\n\n(b) Transformer blocklayer normself-attentiondepthwise conv 3x3global poolconv 1x1, c/4SE(a) MBConv blockconv 1x1, 4cbatch normbatch norm, gelubatch norm, geluh-swishconv 1x1, 4cσMLPlayer normconv 1x1, 4cgeluconv 1x1, cdepthwise conv 3x3conv 1x1, cconv 1x1, 4cbatch normbatch norm, gelubatch norm, gelulayer normself-attention(c) MOAT blockconv 1x1, c×+++++Published as a conference paper at ICLR 2023\n\nTransformer block. The Transformer (Vaswani et al., 2017) block (Fig. 1 (b)) is a powerful building block that effectively captures the global information via the data-dependent self-attention operation. It consists of two main operations: self-attention and MLP. The self-attention operation computes the attention map based on the pairwise similarity between every pair of pixels in the input tensor, thus enabling the model’s receptive field to encompass the entire spatial domain. Additionally, the attention map dynamically depends on the input, enlarging the model’s representation capacity (unlike the convolution kernels, which are data-independent). The MLP operation contains two 1 × 1 convolutions, where the first one expands the channels (by 4×), the second one shrinks back the channels, and GeLU non-linearity is used in-between. Formally, given an input tensor x ∈ RH×W ×C, the Transformer block is represented as follows:\n\nTransformer(x) = x + (M2 ◦ M1 ◦ Attn)(LN(x)), M1(x) = GeLU(Conv(LN(x))), M2(x) = Conv(x),\n\n(6) (7)\n\n(8)\n\nwhere LN and Attn denote the Layer Normalization (Ba et al., 2016), and self-attention (Vaswani et al., 2017). The self-attention operation also includes a residual connection (He et al., 2016a), which is not shown in the equations for simplicity, while the MLP operation is represented by two functions M1 and M2, which correspond to the 1 × 1 convolution for channel expansion (by 4×) and 1 × 1 convolution for channel projection, respectively.\n\n2.2 MOBILE CONVOLUTION WITH ATTENTION (MOAT) BLOCK\n\nComparing MBConv and Transformer blocks. Before getting into the architecture of our MOAT block, it is worthwhile to compare the MBConv (Sandler et al., 2018) and Transformer (Vaswani et al., 2017) blocks, which helps to understand our design motivations. Specifically, we make the following key observations.\n\nFirst, both MBConv and Transformer blocks advocate the “inverted bottleneck” design, where the channels of input tensors are expanded and then projected by 1 × 1 convolutions. However, MBConv additionally employs a 3 × 3 depthwise convolution between those two 1 × 1 convolutions, and there are both batch normalization and GeLU activation between the convolutions.\n\nSecond, to capture the global information, the MBConv block may employ a Squeeze-and-Excitation (SE) module, while the Transformer block adopts the self-attention operation. Note that the SE module squeezes the spatial information via a global average pooling, while the self-attention module maintains the tensor’s spatial resolution.\n\nThird, the downsampling operation is performed at different places within the block. To downsample the features, the standard MBConv block uses the strided depthwise convolution, while the Transformer block, deployed in the modern hybrid model CoAtNet (Dai et al., 2021), adopts an average-pooling operation before the self-attention.\n\nMOAT block. Given the above observations, we now attempt to design a new block that effectively merges the best from both MBConv and Transformer blocks. We begin with the powerful Transformer block, and gradually refine over it.\n\nBased on the first observation, both MBConv and Transformer blocks employ the “inverted bottleneck” design. Since depthwise convolution could effectively encode local interaction between pixels, which is crucial for modeling the translation equivariance in ConvNets, we thus start to add the depthwise convolution to Transformer’s MLP module. However, we did not observe any performance improvement until we also added the extra normalization and activations between convolutions.\n\nFor the second observation, we simply do not add the SE module to the MBConv block. The self-attention operation is kept to capture the global information.\n\nWe found the third observation critical. The downsampling operation (average-pooling) right before the self-attention operation in Transformer block slightly reduces its representation capacity. On the other hand, the MBConv block is well-designed for the downsampling operation with the strided depthwise convolution, which effectively learns the downsampling convolution kernel for each input channel. Therefore, we further reorder the “inverted bottleneck” (containing depthwise convolution) before the self-attention operation, delegating the downsampling operation to depthwise convolution.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nIn this way, we need no extra downsampling layer like average-pooling in CoAtNet (Dai et al., 2021), or patch-embedding layers in Swin (Liu et al., 2021) and ConvNeXt (Liu et al., 2022b). Finally, it results in our MObile convolution with ATtention (MOAT) block, as illustrated in Fig. 1 (c). Formally, given an input tensor x ∈ RH×W ×C, the MOAT block is represented as follows:\n\nMOAT(x) = x + (Attn ◦ N2 ◦ D ◦ N1)(BN(x)),\n\n(9)\n\nwhere MBConv (w/o SE) contains functions N1 (Eq. 2), D (Eq. 3), and N2 (Eq. 5), and Attn denotes the self-attention operation. The MOAT block then simply consists of MBConv (w/o SE) and the self-attention operation, successfully combining the best from the MBConv block and Transformer block into one (which we will show empirically).\n\n2.3 META ARCHITECTURE\n\nMacro-level network design. After developing the MOAT block, we then study how to effectively stack them to form our base model. We adopt the same strategy as the existing works (Liu et al., 2021; Wang et al., 2021b; Graham et al., 2021; Xiao et al., 2021; Dai et al., 2021; Mehta & Rastegari, 2022a). Specifically, we summarize several key findings from those works, and use them as design principles of our meta architecture.\n\n• Employing convolutions in the early stages improves the performance and training conver-\n\ngence of Transformer models (Wu et al., 2021; Graham et al., 2021; Xiao et al., 2021).\n\n• The Mobile Convolution (MBConv) (Sandler et al., 2018) blocks are also effective building blocks in the hybrid Conv-Transformer models (Dai et al., 2021; Mehta & Rastegari, 2022a).\n\n• Extracting multi-scale backbone features benefits the downstream tasks, such as detection and segmentation (Liu et al., 2021; Wang et al., 2021b; Fan et al., 2021; Heo et al., 2021).\n\nAs a result, our meta architecture consists of the convolutional stem, MBConv blocks, and MOAT blocks. Additionally, through the ablation study in the appendix, we found the layer layout proposed by CoAtNet-1 (Dai et al., 2021) effective. We thus follow their layer layout, resulting in our base model MOAT-1. To form the MOAT model family, we then scale down or up MOAT-1 in the dimensions of number of blocks and number of channels, as shown in Tab. 1. We only scale the number of blocks in the third and fourth stages (out of five stages). The downsampling operation is performed in the first block of each stage. Note that our base model MOAT-1 and CoAtNet-1 share the same layer layout and channel sizes. However, we take a different scaling strategy: our MOAT is scaled up (or down) by alternatively increasing the depth and expanding the width between variants.\n\nTable 1: MOAT variants differ in the number of blocks B and number of channels C in each stage.\n\nblock\n\nstride\n\nconv MBConv MBConv MOAT MOAT\n\n2 4\n8 16 32\n\nMOAT-0 MOAT-1 C\nB\n\nB\n\nC\n\nMOAT-2 C\nB\n\nMOAT-3 C\nB\n\n2 2\n3 7\n2\n\n64 96 192 384 768\n\n2 2\n6 14 2\n\n64 96 192 384 768\n\n2 2\n6 14 2\n\n128 128 256 512 1024\n\n2 2\n12 28 2\n\n160 160 320 640 1280\n\nMOAT-4 C\nB\n\n256 2\n256 2\n12 512 28 1024 2048 2\n\ntiny-MOAT-{0,1,2,3}\n\nB\n\n32 2\n32 2\n3 64 7 128 2 256\n\nC\n\n80 56 40 56 80 40 80 112 160 160 224 320 320 448 640\n\n3 EXPERIMENTAL RESULTS\n\nIn this section, we show that MOAT variants are effective on the ImageNet-1K (Russakovsky et al., 2015) image classification. We then deploy them to other recognition tasks, including COCO object detection (Lin et al., 2014), instance segmentation (Hariharan et al., 2014), and ADE20K (Zhou et al., 2019) semantic segmentation. MOAT can be seamlessly applied to downstream tasks. For small resolution inputs, we directly fine-tune the global attention, while for large resolution inputs, we simply convert the global attention to non-overlapping local window attention without using extra window-shifting mechanism. The detailed experiment setup could be found in the appendix.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Performance on ImageNet-1K. 1K only: Using ImageNet-1K only. 22K + 1K: ImageNet-22K pretraining and ImageNet-1K fine-tuning. Tab. 8 shows comparisions with more SOTA methods and Tab. 9 reports the performances on ImageNet-1K-V2.\n\nmodel\n\neval size\n\nparams\n\nFLOPs\n\nImageNet-1K top-1 accuracy\n\n1K only\n\n22K+1K\n\nConvNets\n\nViTs\n\nHybrid\n\nHybrid (ours)\n\nEfficientNetV2-L (Tan & Le, 2021) EfficientNetV2-XL (Tan & Le, 2021) ConvNeXt-T (Liu et al., 2022b) ConvNeXt-L (Liu et al., 2022b) ConvNeXt-XL (Liu et al., 2022b)\n\nPVT-Large (Wang et al., 2021b) Swin-T (Liu et al., 2021) Swin-L (Liu et al., 2021) SwinV2-L (Liu et al., 2021) MViTv2-H (Li et al., 2022)\n\nPVTv2-B5 (Wang et al., 2022) MaxViT-XL (Tu et al., 2022) CoAtNet-0 (Dai et al., 2021) CoAtNet-3 (Dai et al., 2021) CoAtNet-4 (Dai et al., 2021)\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-1 MOAT-2 MOAT-3\n\nMOAT-4\n\n4802 4802 2242 3842 3842\n\n2242 2242 3842 3842 5122\n\n2242 5122 2242 3842 5122\n\n2242 2242 2242 2242\n\n3842 3842 3842 3842\n\n5122 5122 5122\n\n5122\n\n53B 94B 4.5B\n\n120M 208M 29M 198M 101.0B 350M 179.0B\n\n9.8B 4.5B\n\n61.4M 28M 197M 103.9B 197M 115.4B 667M 763.5B\n\n82M 11.8B 475M 535.2B 25M 168M 107.4B 275M 360.9B\n\n4.2B\n\n5.7B 27.8M 9.1B 41.6M 73.4M 17.2B 190.0M 44.9B\n\n18.2B 27.8M 29.6B 41.6M 73.4M 54.3B 190.0M 141.2B\n\n41.6M 58.7B 73.4M 104.6B 190.0M 271.0B\n\n483.2M 648.5B\n\n85.7 -\n82.1 85.5 -\n\n81.7 81.3 -\n- -\n\n83.8 -\n81.6 85.8 -\n\n83.3 84.2 84.7 85.3\n\n84.6 85.9 86.2 86.5\n\n86.2 86.5 86.7\n\n-\n\n- 87.3 82.9 87.5 87.8\n\n- -\n87.3 87.7 88.8\n\n- 88.7 -\n87.6 88.6\n\n83.6 84.9 86.0 86.8\n\n85.7 87.0 87.5 88.2\n\n87.2 87.7 88.4\n\n89.1\n\nFigure 2: Parameters vs. accuracy using ImageNet1K only with input size 224.\n\nFigure 3: FLOPs vs. accuracy using ImageNet-1K only with input size 224.\n\nFigure 4: Parameters vs. accuracy using ImageNet22K and ImageNet-1K with input size 384.\n\nFigure 5: FLOPs vs. accuracy using ImageNet-22K and ImageNet-1K with input size 384.\n\n6\n\n818283848586050100150200Top-1 accuracyParameters (M) MOAT CoAtNet ConvNeXt SwinSSwin-BTConvNeXt-LS012CoAtNet-3102MOAT-3TB81828384858601020304050Top-1 accuracyFLOPs (B) MOAT CoAtNet ConvNeXt SwinSSwin-BTConvNeXt-LS012CoAtNet-3102MOAT-3TB83848586878889050100150200Top-1 accuracyParameters (M) MOAT CoAtNet ConvNeXt SwinLSwin-BBConvNeXt-TLS2CoAtNet-3012MOAT-383848586878889050100150Top-1 accuracyFLOPs (B) MOAT CoAtNet ConvNeXt SwinSwin-BLB2CoAtNet-3102MOAT-3SConvNeXt-TLPublished as a conference paper at ICLR 2023\n\nImageNet Image Classification. In Tab. 2, we include the current state-of-art methods in the categories of ConvNets, ViTs and Hybrid models. At similar model costs (parameters or FLOPs), our MOAT models consistently outperform all of them. Specifically, with the ImageNet-1K data only and input size 224, for light-weight models, our MOAT-0 significantly outperforms ConvNeXt-T (Liu et al., 2022b), Swin-T (Liu et al., 2022b), and CoAtNet-0 (Dai et al., 2021) by 1.2%, 2.0%, and 1.7%, respectively. For large-scale models using input size 384, MOAT-3 is able to surpass ConvNeXt-L, CoAtNet-3 by 1.0% and 0.7%, respectively. With the ImageNet-22K pretraining and input size 384, the prior arts ConvNeXt-L, Swin-L, and CoAtNet-3 already show strong performances (87.5%, 87.3% and 87.6%), while our MOAT-3 achieves the score of 88.2%, outperforming them by 0.7%, 0.9%, and 0.6%, respectively. For ImageNet-1K and input size 224, we plot the performances vs. parameters and FLOPs in Fig. 2 and Fig. 3, respectively. For ImageNet-22K pretraining and input size 384, we plot the performances vs. parameters and FLOPs in Fig. 4 and Fig. 5, respectively. In the figures, MOAT clearly demonstrates the best performance in all computation regimes. Finally, our largest model MOAT-4, with ImageNet-22K and input size 512, further attains 89.1% accuracy.\n\nCOCO Detection. Tab. 3 summarizes the COCO object detection (box) and instance segmentation (mask) results. Our MOAT backbones significantly outperform the baseline methods, including Swin (Liu et al., 2021) and ConvNeXt (Liu et al., 2022b) across different model sizes. Specifically, our MOAT-0 outperforms Swin-T and ConvNeXt-T by 5.4% and 5.5% APbox (3.7% and 3.7% APmask). Our MOAT-1 surpasses Swin-S and ConvNeXt-S by 5.9% and 5.8% APbox (4.3% and 4.0% APmask). Our MOAT-2, with 110M parameters, is still 5.5% and 4.5% APbox (3.5% and 2.4% APmask) better than Swin-B and ConvNeXt-B. Finally, our MOAT-3, using 227M parameters, achieves 59.2% APbox (50.3% APmask), setting a new state-of-the-art in the regime of model size 200M that is built on top of Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017). More comparisons with smaller input size can be found in Tab. 12. For tiny-MOAT, tiny-MOAT-0/1 achieve the same performance as Swin-T/S and ConvNeXt-T/S but only use less than half of the parameters. Furthermore, tiny-MOAT-3 is pretrained with ImageNet-1K and attains 55.2 APbox with 57M parameters, surpassing the ImageNet-22k pretrained Swin-L (53.9 APbox with 254M parameters) and ConvNeXt-L (54.8 APbox with 255M parameters).\n\nTable 3: Object detection and instance segmentation on the COCO 2017 val set. We employ Cascade Mask-RCNN, and single-scale inference (hard NMS). †: use ImageNet-22K pretrained weights. When using tiny-MOAT series as backbones, most of the model parameters come from the decoder. More comparisons at input size 896 is reported in Tab. 12.\n\ninput size\n\nparams\n\nAPmask APmask\n\nbackbone\n\nSwin-T Swin-S Swin-B† Swin-L†\n\nConvNeXt-T ConvNeXt-S ConvNeXt-B† ConvNeXt-L†\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2† MOAT-3†\n\n1280 × 800 1280 × 800 1280 × 800 1280 × 800\n\n1280 × 800 1280 × 800 1280 × 800 1280 × 800\n\n1344 × 1344 1344 × 1344 1344 × 1344 1344 × 1344\n\n1344 × 1344 1344 × 1344 1344 × 1344 1344 × 1344\n\nFLOPs APbox APbox 50 69.3 70.4 71.8 72.4\n\n50.5 51.8 53.0 53.9\n\n745B 86M 838B 107M 145M 982B 254M 1382B\n\n741B 86M 827B 108M 146M 964B 255M 1354B\n\n41M 42M 47M 57M\n\n612B 628B 669B 754B\n\n799B 65M 79M 921B 110M 1217B 227M 2216B\n\n50.4 51.9 54.0 54.8\n\n50.5 51.9 53.0 55.2\n\n55.9 57.7 58.5 59.2\n\n69.1 70.8 73.1 73.8\n\n69.3 71.6 72.2 74.8\n\n73.9 76.0 76.6 77.8\n\nAPbox 75 54.9 56.3 57.5 58.8\n\n54.8 56.5 58.8 59.8\n\n56.0 56.1 58.0 60.6\n\n60.9 63.4 64.3 64.9\n\n43.7 44.7 45.8 46.7\n\n43.7 45.0 46.9 47.6\n\n43.3 44.6 45.0 47.0\n\n47.4 49.0 49.3 50.3\n\n50 66.6 67.9 69.4 70.1\n\n66.5 68.4 70.6 71.3\n\n66.6 68.4 69.4 71.8\n\n70.9 73.4 73.9 74.8\n\nAPmask 75 47.1 48.5 49.7 50.8\n\n47.3 49.1 51.3 51.7\n\n47.3 48.3 48.8 51.2\n\n52.1 53.2 53.9 55.5\n\nADE20K Semantic Segmentation. In Tab. 4, when using input size 5132, MOAT consistently outperforms the ConvNeXt counterparts. MOAT-0 surpasses ConvNeXt-T by 3.0% mIoU. Moreover, MOAT-2, with ImageNet-22k pretraining, surpasses ConvNeXt-B by 3.1%. The larger MOAT-3 and MOAT-4 further outperform ConvNeXt-L and ConvNeXt-XL by 4.9% and 5.4%, respectively. Finally, when using input size 6412, our MOAT-4 achieves the performance of 57.6% mIoU, setting a new state-of-the-art in the regime of models using input size 6412. For tiny-MOAT, tiny-MOAT-3 achieves comparable performance with ConvNeXt-S with less than half of the parameters.\n\ntiny-MOAT on ImageNet. We simply scale down the channels of MOAT-0 to obtain the tiny-MOAT family without any specific adaptions. In the left of Tab. 5, with the similar model parameters,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Semantic segmentation on ADE20K val set. We employ DeepLabv3+ (single-scale inference). Results for ConvNeXt and MOAT are obtained using the official code-base (Weber et al., 2021) with the same training recipe. †: use ImageNet-22K pretrained weights.\n\nbackbone\n\nConvNeXt-T ConvNeXt-S ConvNeXt-B† ConvNeXt-L† ConvNeXt-XL†\n\ninput 5132 5132 5132 5132 5132\n\nparams\n\nFLOPs mIoU (%)\n\n47.6B 34.2M 55.8M 70.8B 95.8M 119.5B 208.3M 256.4B 364.0M 446.2B\n\n45.8 47.8 50.5 51.0 51.8\n\nbackbone\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2† MOAT-3† MOAT-4†\n\nMOAT-2† MOAT-3† MOAT-4†\n\ninput 5132 5132 5132 5132\n\n5132 5132 5132 5132 5132\n\n6412 6412 6412\n\nparams\n\nFLOPs\n\nmIoU (%)\n\n5.6M 7.8M 13.2M 24.2M\n\n11.8B 15.2B 23.8B 41.2B\n\n33.3M 61.3B 47.0M 85.4B 144.3B 80.5M 198.4M 331.5B 496.3M 779.9B\n\n80.5M 242.0B 198.4M 554.7B 496.3M 1273.5B\n\n41.2 43.1 44.9 47.5\n\n48.8 51.8 53.6 55.9 57.2\n\n54.7 56.5 57.6\n\ntiny-MOAT-0/1/2 surpass the Mobile-Former counterparts by 6.8%, 5.5%, and 4.3%, respectively. In the right of Tab. 5, our tiny-MOAT also shows stronger performances than MobileViT (Mehta & Rastegari, 2022a). Even compared with the concurrent work MobileViTv2 (Mehta & Rastegari, 2022b), tiny-MOAT-1/2/3 surpass their counterparts by 1.1%, 1.3%, and 2.1%, respectively.\n\nTable 5: Performances of tiny-MOAT family on ImageNet-1K.\n\ninput size 2242\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\n3.5M Mobile-Former-52M Mobile-Former-96M 4.6M Mobile-Former-214M 9.4M\n\nMobile-Former-508M 14.0M\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2\n\ntiny-MOAT-3\n\n3.4M 5.1M 9.8M\n\n19.5M\n\n0.05B 0.1B 0.2B\n\n0.5B\n\n0.8B 1.2B 2.3B\n\n4.5B\n\n68.7 72.8 76.7\n\n79.3\n\n75.5 78.3 81.0\n\n82.7\n\ninput size 2562\n\nMobileViT-XS MobileViT-S\n\nMobileViTv2-1.0 MobileViTv2-1.5 MobileViTv2-2.0\n\ntiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\n2.3M 5.6M\n\n4.9M 10.6M 18.5M\n\n5.1M 9.8M 19.5M\n\n0.7B 2.0B\n\n1.8B 4.0B 7.5B\n\n1.6B 3.0B 6.0B\n\n74.8 78.4\n\n78.1 80.4 81.2\n\n79.2 81.7 83.3\n\n4 ABLATION STUDIES ON IMAGENET\n\nAt micro level, we perform ablation studies on the MOAT block design and downsampling layer in the following and the order of MBConv and Attention in MOAT block in section A.6.1. At macro level, we perform ablation studies on the MOAT-based model and MOAT meta architecture in section A.6.2.\n\nMOAT block design. In Tab. 6, we ablate the MOAT block design, which only affects the last two stages of MOAT, and we keep everything else the same (e.g., training recipes). We start from the Transformer block, consisting of Attn (self-attention) and MLP, which already attains a strong top-1 accuracy (82.6%). Directly inserting a 3 × 3 depthwise convolution in the MLP degrades the performance by 0.9%. If we additionally insert batch normalization and GeLU between convolutions (i.e., replace MLP with MBConv, but no Squeeze-and-Excitation), the performance is improved to 82.9%. Finally, placing MBConv before Attn reaches the performance of 83.3%. Additionally, our MOAT block brings more improvements (from 1.2% up to 2.6% gains) in the tiny model regime.\n\nDownsampling layer. For the MOAT block design, we do not need the extra downsampling layer like (1) average-pooling in CoAtNet (Dai et al., 2021), (2) patch-embedding layer (i.e., 2 × 2 convolution with stride 2) in Swin (Liu et al., 2021) and ConvNeXt (Liu et al., 2022b), or (3) strided depthwise convolution in PiT (Heo et al., 2021) and RegionViT (Chen et al., 2022a). As shown in Tab. 7, using patch-embedding layer indeed improves over the average-pooling scheme by 0.2% accuracy, but it takes more cost of model parameters. Additionally, using the strided depthwise convolution for downsampling leads to 0.2% worse performance than the patch-embedding layer. By contrast, our MOAT design (i.e., delegating the downsampling to the MBConv block) shows the best performance with the least cost of parameters and comparable FLOPs.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Ablation studies of MOAT block design on ImageNet-1K with input size 224.\n\nmodel\n\nblock composition\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\nMOAT-0\n\nAttn + MLP Attn + MLP (w/ depth. conv) Attn + MBConv MBConv + Attn\n\n28.0M 28.2M 28.2M 27.8M\n\n5.4B 5.4B 5.4B 5.7B\n\n82.6 81.7 82.9 83.3\n\nmodel\n\nblock composition\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\ntiny-MOAT-2\n\ntiny-MOAT-1\n\ntiny-MOAT-0\n\nAttn + MLP MBConv + Attn\n\nAttn + MLP MBConv + Attn\n\nAttn + MLP MBConv + Attn\n\n9.8M 9.8M\n\n5.1M 5.1M\n\n3.3M 3.4M\n\n2.2B 2.3B\n\n1.1B 1.2B\n\n0.8B 0.8B\n\n79.8 81.0\n\n76.2 78.3\n\n72.9 75.5\n\nTable 7: Ablation studies of the downsampling layer design on ImageNet-1K, using MOAT-0 and input size 224. We compare our MOAT design (in grey) with (1) CoAtNet (using average-pooling for downsampling), (2) Swin/ConvNeXt designs (using strided 2 × 2 convolution for downsampling), and (3) PiT/RegionViT designs (using strided 3 × 3 depthwise convolution for downsampling).\n\nblock composition\n\ndownsampling type\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\nAveragePooling + Attn + MLP PatchEmbedding + Attn + MLP StridedDepthConv + Attn + MLP MBConv + Attn\n\nCoAtNet Swin, ConvNeXt PiT, RegionVit MOAT\n\n28.0 30.2 28.8 27.8\n\n5.4 5.6 5.5 5.7\n\n82.6 82.8 82.6 83.3\n\n5 RELATED WORK\n\nTransformers (Vaswani et al., 2017) were recently introduced to the vision community (Wang et al., 2018; Ramachandran et al., 2019; Hu et al., 2019) and demonstrated remarkable performance on vision recognition tasks (Carion et al., 2020; Zhu et al., 2021; Wang et al., 2021a; Arnab et al., 2021; Liu et al., 2021; Cheng et al., 2021; Yu et al., 2022a; Kim et al., 2022; Cheng et al., 2022; Yu et al., 2022b), thanks to their ability to efficiently encode long-range interaction via the attention mechanism (Bahdanau et al., 2015). Particularly, ViT (Dosovitskiy et al., 2021) obtains impressive results on ImageNet (Russakovsky et al., 2015) by applying the vanilla Transformer with the novel large stride patch embedding, after pretraining on the proprietary large-scale JFT dataset (Sun et al., 2017). There have been several works aiming to improve the vision transformers, either with better training strategies (Touvron et al., 2021a;b; Steiner et al., 2021; Zhai et al., 2022; Touvron et al., 2022) or with efficient local-attention modules (Huang et al., 2019; Ho et al., 2019; Wang et al., 2020; Liu et al., 2021; Chu et al., 2021; Yang et al., 2021; Yu et al., 2021; Dong et al., 2022; Tu et al., 2022).\n\nSince the debut of AlexNet (Krizhevsky et al., 2012), the vision community has witnessed a rapid improvement on the ImageNet benchmark using different types of ConvNets, including (but not limited to) VGGNet (Simonyan & Zisserman, 2015), Inceptions (Szegedy et al., 2015; Ioffe & Szegedy, 2015; Szegedy et al., 2016; 2017), ResNets (He et al., 2016a;b), ResNeXt (Xie et al., 2017), DenseNet (Huang et al., 2017), SENet (Hu et al., 2018), MobileNets (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019), EfficientNets (Tan & Le, 2019; 2021), and ConvNeXt (Liu et al., 2022b) each focusing on different aspects of accuracy and efficiency. The ubiquity of ConvNets in computer vision could be attributed to their built-in inductive biases.\n\nGiven the success of Transformers and ConvNets, another line of research is to explore how to effectively combine them. Swin (Liu et al., 2021; 2022a), PVT (Wang et al., 2021b; 2022), MViT (Fan et al., 2021; Li et al., 2022), and PiT (Heo et al., 2021) adopt the ConvNet hierarchical structure to extract multi-scale features for Transformers. SASA (Ramachandran et al., 2019), AA-ResNet (Bello et al., 2019), Axial-ResNet (Wang et al., 2020) and BoTNet (Srinivas et al., 2021) incorporate the attention modules to ResNets. CvT (Wu et al., 2021), LeViT (Graham et al., 2021), Visformer (Chen et al., 2021b), and ViTC (Xiao et al., 2021) replace ViT’s patch embedding with strided convolutions. CeiT (Yuan et al., 2021a) and CMT (Guo et al., 2022) incorporate depthwise convolution to the transformer block’s MLP. ViTAE (Xu et al., 2021) adopts parallel attention modules and convolutional layers. LVT (Yang et al., 2022) introduces local self-attention into the convolution. Recently, CoAtNet (Dai et al., 2021) and MobileViT (Mehta & Rastegari, 2022a) propose hybrid models that build on top of the efficient Mobile Convolution (Sandler et al., 2018) and Transformer block.\n\nAcknowledgements We thank Wen-Sheng Chu for the support and discussion. We gratefully acknowledge supports from the Office of Naval Research. N00014-21-1-2812.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nMart ́ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, 2016.\n\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci ́c, and Cordelia Schmid.\n\nVivit: A video vision transformer. In ICCV, 2021.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv:1607.06450,\n\n2016.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In ICLR, 2015.\n\nIrwan Bello. Lambdanetworks: Modeling long-range interactions without attention. In ICLR, 2021.\n\nIrwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\n\nconvolutional networks. In ICCV, 2019.\n\nAndy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\n\nrecognition without normalization. In ICML, 2021.\n\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In\n\nCVPR, 2018.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\n\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n\nChun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision\n\ntransformers. In ICLR, 2022a.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE TPAMI, 2017.\n\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoderdecoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\n\nLiang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scaling wide residual networks for panoptic\n\nsegmentation. arXiv:2011.11675, 2020.\n\nWuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin, Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, et al. A simple single-scale vision transformer for object localization and instance segmentation. arXiv preprint arXiv:2112.09747, 2021a.\n\nWuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin, Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, and Denny Zhou. A simple single-scale vision transformer for object localization and instance segmentation. In ECCV, 2022b.\n\nYinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng\n\nLiu. Mobile-former: Bridging mobilenet and transformer. In CVPR, 2022c.\n\nZhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The\n\nvision-friendly transformer. In ICCV, 2021b.\n\nBowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation. In CVPR, 2020.\n\nBowen Cheng, Alexander G Schwing, and Alexander Kirillov. Per-pixel classification is not all you\n\nneed for semantic segmentation. In NeurIPS, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n\nattention mask transformer for universal image segmentation. CVPR, 2022.\n\nFranc ̧ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017.\n\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In NeurIPS, 2021.\n\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\n\nLearning augmentation policies from data. In CVPR, 2019.\n\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\n\ndata augmentation with a reduced search space. In CVPR, 2020.\n\nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and\n\nattention for all data sizes. In NeurIPS, 2021.\n\nXiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, 2022.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nSt ́ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In ICML, 2021.\n\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\n\nChristoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.\n\nBenjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv ́e J ́egou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. In ICCV, 2021.\n\nJianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt:\n\nConvolutional neural networks meet vision transformers. In CVPR, 2022.\n\nBharath Hariharan, Pablo Arbel ́aez, Ross Girshick, and Jitendra Malik. Simultaneous detection and\n\nsegmentation. In ECCV, 2014.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In CVPR, 2016a.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\n\nnetworks. In ECCV, 2016b.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv:1606.08415, 2016.\n\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\n\nRethinking spatial dimensions of vision transformers. In ICCV, 2021.\n\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\n\nmensional transformers. arXiv:1912.12180, 2019.\n\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.\n\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\n\nIn ICCV, 2019.\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.\n\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\n\nstochastic depth. In ECCV, 2016.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\n\nconvolutional networks. In CVPR, 2017.\n\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\n\nCriss-cross attention for semantic segmentation. In ICCV, 2019.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by\n\nreducing internal covariate shift. In ICML, 2015.\n\nDahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So Kweon, and Liang-Chieh Chen. TubeFormer-DeepLab: Video Mask Transformer. In CVPR, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-\n\ntional neural networks. In NeurIPS, 2012.\n\nYanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In CVPR, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nTsung-Yi Lin, Piotr Doll ́ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\n\nFeature pyramid networks for object detection. In CVPR, 2017.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\n\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, 2022a.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\n\nA convnet for the 2020s. In CVPR, 2022b.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nSachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-\n\nfriendly vision transformer. In ICLR, 2022a.\n\nSachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers.\n\narXiv preprint arXiv:2206.02680, 2022b.\n\nIlija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Designing\n\nnetwork design spaces. In CVPR, 2020.\n\nPrajit Ramachandran, Barret Zoph, and Quoc V Le.\n\nSearching for activation functions.\n\narXiv:1710.05941, 2017.\n\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\n\nStand-alone self-attention in vision models. In NeurIPS, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\n\ngeneralize to imagenet? In ICML, 2019.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 115:211–252, 2015.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\n\nbilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\n\nIn NAACL, 2018.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. In ICLR, 2015.\n\nAravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\n\nBottleneck transformers for visual recognition. In CVPR, 2021.\n\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv:2106.10270, 2021.\n\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\n\neffectiveness of data in deep learning era. In ICCV, 2017.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\n\nthe inception architecture for computer vision. In CVPR, 2016.\n\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-\n\nresnet and the impact of residual connections on learning. In AAAI, 2017.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\n\nIn ICML, 2019.\n\nMingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In ICML, 2021.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In ICML, 2021a.\n\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv ́e J ́egou. Going\n\ndeeper with image transformers. In ICCV, 2021b.\n\nHugo Touvron, Matthieu Cord, and Herv ́e J ́egou. DeiT III: Revenge of the ViT. arXiv:2204.07118,\n\n2022.\n\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao\n\nLi. Maxvit: Multi-axis vision transformer. In ECCV, 2022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In CVPR, 2021.\n\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\n\nAxial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In ECCV, 2020.\n\nHuiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab:\n\nEnd-to-end panoptic segmentation with mask transformers. In CVPR, 2021a.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, 2021b.\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. Computational Visual Media, pp. 1–10, 2022.\n\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\n\nCVPR, 2018.\n\nMark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D. Collins, Yukun Zhu, Liangzhe Yuan, Dahun Kim, Qihang Yu, Daniel Cremers, Laura Leal-Taixe, Alan L. Yuille, Florian Schroff, Hartwig Adam, and Liang-Chieh Chen. DeepLab2: A TensorFlow Library for Deep Labeling. arXiv: 2106.09748, 2021.\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\n\nIntroducing convolutions to vision transformers. In ICCV, 2021.\n\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll ́ar, and Ross Girshick. Early\n\nconvolutions help transformers see better. In NeurIPS, 2021.\n\nSaining Xie, Ross Girshick, Piotr Doll ́ar, Zhuowen Tu, and Kaiming He. Aggregated residual\n\ntransformations for deep neural networks. In CVPR, 2017.\n\nYufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by\n\nexploring intrinsic inductive bias. In NeurIPS, 2021.\n\nChenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zijun Wei, Zhe Lin, and Alan Yuille. Lite\n\nvision transformer with enhanced self-attention. In CVPR, 2022.\n\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao.\n\nFocal self-attention for local-global interactions in vision transformers. In NeurIPS, 2021.\n\nHongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou, Pengchong Jin, Fan Yang, Frederick Liu, Jaeyoun Kim, and Jing Li. TensorFlow Model Garden. https: //github.com/tensorflow/models, 2020.\n\nQihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L Yuille, and Wei Shen. Glance-and-gaze\n\nvision transformer. In NeurIPS, 2021.\n\nQihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In CVPR, 2022a.\n\nQihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille,\n\nand Liang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022b.\n\nKun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.\n\nIncorporating\n\nconvolution designs into visual transformers. In ICCV, 2021a.\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV, 2021b.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\n\nIn CVPR, 2022.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv:1710.09412, 2017.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\n\nSemantic understanding of scenes through the ade20k dataset. IJCV, 127(3):302–321, 2019.\n\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and\n\nJiashi Feng. Deepvit: Towards deeper vision transformer. arXiv:2103.11886, 2021.\n\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\n\nDeformable transformers for end-to-end object detection. ICLR, 2021.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nIn the appendix, we provide more details for both our model and experiments.\n\n• In section A.1, we provide MOAT implementation details.\n\n• In section A.2.1, we provide ImageNet experimental details.\n\n• In section A.2.2, we provide ImageNet-V2 experimental results.\n\n• In section A.3.1, we provide COCO detection experimental details.\n\n• In section A.3.2, we provide more COCO object detection experimental results.\n\n• In section A.4, we provide ADE20K semantic segmentation experimental detaills.\n\n• In section A.5, we provide COCO panoptic segmentation experiments.\n\n• In section A.6.1, we provide ablation studies on the MOAT micro-level design.\n\n• In section A.6.2, we provide ablation studies on the MOAT macro-level design.\n\n• In section A.7, we provide the ImageNet trainng time, peak training memory and throughput\n\nmeasurement of MOAT models.\n\n• In section A.8, we discuss limitations of our model.\n\nA.1 MOAT IMPLEMENTATION DETAILS\n\nIn the MOTA networks, we employ kernel size 3 for both convolutions and depthwise convolutions. We use the multi-head self attention (Vaswani et al., 2017), where each attention head has channels 32. For the MBConv and MOAT blocks, we use expansion ratio 4. The SE module (Hu et al., 2018) in the MBConv blocks (i.e., 2nd and 3rd stages) adopt reduction ratio 4 (relative to the input channels).\n\nOur MOAT block includes the relative positional embedding (Shaw et al., 2018; Dai et al., 2021) for ImageNet. However, the downstream tasks usually take a larger input resolution than ImageNet, demanding for a special adaptation (e.g., bilinear interpolation of pretrained positional embedding). For simplicity, we remove the positional embedding, when running MOAT on downstream tasks.\n\nA.2\n\nIMAGENET IMAGE CLASSIFICATION\n\nA.2.1\n\nIMAGENET EXPERIMENTS\n\nThe ImageNet-1K dataset (Russakovsky et al., 2015) contains 1.2M training images with 1000 classes. We report top-1 accuracy on the ImageNet-1K validation set, using the last checkpoint. We also experiment with pretraining on the larger ImageNet-22K dataset, and then fine-tuning on the ImageNet-1K. We closely follow the prior works (Dai et al., 2021; Liu et al., 2022b) and provide more details below. In Tab. 8, we compare our MOAT with more state-of-the-art models.\n\nExperimental setup. We train MOAT models on ImageNet-1K with resolution 224 for 300 epochs. If pretraining on the larger ImageNet-22K, we use resolution 224 and 90 epochs. Afterwards, the models are fine-tuned on ImageNet-1K for 30 epochs. During fine-tuning, we also experiment with larger resolutions (e.g., 384 and 512). We employ the typical regularization methods during training, such as label smoothing (Szegedy et al., 2016), RandAugment (Cubuk et al., 2020), MixUp (Zhang et al., 2017), stochastic depth (Huang et al., 2016), and Adam (Kingma & Ba, 2015) with decoupled weight decay (i.e., AdamW (Loshchilov & Hutter, 2019)). See Tab. 10 and Tab. 11 for detailed hyper-parameters.\n\nA.2.2\n\nIMAGENET-1K-V2 EVALUATION\n\nTo further demonstrate the transferability and generalizability of our MOAT models, we perform additional evaluations on the ImageNet-1K-V2 (Recht et al., 2019), using our ImageNet (Russakovsky et al., 2015) pretrained checkpoints. We report an extensive evaluation, using MOAT and several input resolutions, on ImageNet-1K-V2, aiming to establish another solid baseline for the community, as we notice that most of the existing models do not report results on ImageNet-1K-V2. As shown in the Tab. 9, MOAT does not overfit to ImageNet-1K-V1 dataset and generalizes well to ImageNet1K-V2 dataset, as we observe a continuous performance improvement from small to large models.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Performance on ImageNet-1K with more state-of-the-art models are included. 1K only: Using ImageNet-1K only. 22K + 1K: ImageNet-22K pretraining and ImageNet-1K fine-tuning.\n\nmodel\n\neval size\n\nparams\n\nFLOPs\n\nImageNet-1K top-1 accuracy\n\n1K only\n\n22K+1K\n\nRegNetY-16G (Radosavovic et al., 2020) NFNet-F5 (Brock et al., 2021)\n\nEfficientNetV2-S (Tan & Le, 2021) EfficientNetV2-M (Tan & Le, 2021) EfficientNetV2-L (Tan & Le, 2021) EfficientNetV2-XL (Tan & Le, 2021)\n\nConvNets\n\nConvNeXt-T (Liu et al., 2022b) ConvNeXt-S (Liu et al., 2022b) ConvNeXt-B (Liu et al., 2022b) ConvNeXt-L (Liu et al., 2022b) ConvNeXt-XL (Liu et al., 2022b)\n\nDeiT-B (Touvron et al., 2021a) CaiT-S-36 (Touvron et al., 2021b) DeepViT-L (Zhou et al., 2021) PVT-Large (Wang et al., 2021b) HaloNet-H4 (Vaswani et al., 2021) HaloNet-H5 (Vaswani et al., 2021)\n\nSwin-T (Liu et al., 2021) Swin-S (Liu et al., 2021) Swin-B (Liu et al., 2021) Swin-L (Liu et al., 2021) SwinV2-L (Liu et al., 2022a)\n\nFocal-B (Yang et al., 2021) CSwin-B (Dong et al., 2022) CSwin-L (Dong et al., 2022) MViTv2-H (Li et al., 2022)\n\nBotNet-T7 (Srinivas et al., 2021) LambdaResNet-420 (Bello, 2021) T2T-ViT-24 (Yuan et al., 2021b) CMT-S (Guo et al., 2022) CeiT-S (Yuan et al., 2021a) CvT-21 (Wu et al., 2021) PVTv2-B5 (Wang et al., 2022) MaxViT-XL (Tu et al., 2022)\n\nCoAtNet-0 (Dai et al., 2021) CoAtNet-1 (Dai et al., 2021) CoAtNet-2 (Dai et al., 2021) CoAtNet-3 (Dai et al., 2021) CoAtNet-4 (Dai et al., 2021)\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-1 MOAT-2 MOAT-3\n\nMOAT-4\n\nViTs\n\nHybrid\n\nHybrid (ours)\n\n2242 5442\n\n4802 4802 4802 4802\n\n2242 2242 2242 3842 3842\n\n3842 3842 2242 2242 3842 5122\n\n2242 2242 2242 3842 3842\n\n2242 3842 3842 5122\n\n3842 3202 2242 2242 3842 3842 2242 5122\n\n2242 2242 2242 3842 5122\n\n2242 2242 2242 2242\n\n3842 3842 3842 3842\n\n5122 5122 5122\n\n5122\n\n84M 16.0B 377M 289.8B\n\n22M 54M 120M 208M\n\n8.8B 24B 53B 94B\n\n4.5B 29M 8.7B 50M 89M 15.4B 198M 101.0B 350M 179.0B\n\n86M 68M 55M 61.4M 85M 85M\n\n55.4B 48.0B 12.5B 9.8B -\n-\n\n4.5B 28M 8.7B 50M 88M 15.4B 197M 103.9B 197M 115.4B\n\n16.0B 89.8M 47.0B 78M 173M 96.8B 667M 763.5B\n\n45.8B 75.1M -\n- 15.0B 64.1M 4.0B 25.1M 12.9B 24.2M 24.9B 32M 82M 11.8B 475M 535.2B\n\n4.2B 25M 8.4B 42M 75M 15.7B 168M 107.4B 275M 360.9B\n\n5.7B 27.8M 9.1B 41.6M 73.4M 17.2B 190.0M 44.9B\n\n27.8M 18.2B 41.6M 29.6B 54.3B 73.4M 190.0M 141.2B\n\n41.6M 58.7B 73.4M 104.6B 190.0M 271.0B\n\n483.2M 648.5B\n\n82.9 86.0\n\n83.9 85.1 85.7 -\n\n82.1 83.1 83.8 85.5 85.5\n\n83.1 85.0 83.1 81.7 85.6 85.8\n\n81.3 83.0 83.5 -\n-\n\n83.8 85.4 -\n-\n\n84.7 84.9 82.6 83.5 83.3 83.3 83.8 -\n\n81.6 83.3 84.1 85.8 -\n\n83.3 84.2 84.7 85.3\n\n84.6 85.9 86.2 86.5\n\n86.2 86.5 86.7\n\n-\n\n- -\n\n84.9 86.2 -\n87.3\n\n82.9 84.6 85.8 87.5 87.8\n\n- -\n- -\n- -\n\n- -\n85.2 87.3 87.7\n\n- 87.0 87.5 88.8\n\n- -\n- -\n- -\n- 88.7\n\n- -\n- 87.6 88.6\n\n83.6 84.9 86.0 86.8\n\n85.7 87.0 87.5 88.2\n\n87.2 87.7 88.4\n\n89.1\n\nUnder the fair comparison, with ImageNet-22K pretrainng and input size 384, MOAT-2/3 surpass the current state-of-the-art model SwinV2-B/L by 0.6/1.7%, respectively. Additionally, our MOAT-4, with input size 512, achieves a new state-of-the-art performance of 81.5%, without extra proprietary training data.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Performance on ImageNet-1K-V2.\n\nmodel\n\nparams\n\ninput size\n\nFLOPs\n\nImageNet-1K-V2 top-1 accuracy (%)\n\n1K only\n\n22K + 1K\n\nLeViT-256 (Graham et al., 2021) DeiT-B (Touvron et al., 2021a) CaiT-S36 (Touvron et al., 2021b) SwinV2-B (Liu et al., 2021) SwinV2-L (Liu et al., 2021)\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\ntiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-1 MOAT-2 MOAT-3\n\nMOAT-4\n\n256 224 224 384 384\n\n224 224 224 224\n\n256 256 256\n\n224 224 224 224\n\n384 384 384 384\n\n512 512 512\n\n512\n\n18.9M 86M 68M 88M 197M\n\n3.4M 5.1M 9.8M 19.5M\n\n5.1M 9.8M 19.5M\n\n27.8M 41.6M 73.4M 190.0M\n\n1.1B 17.5B 13.9B 54.7B 115.4B\n\n0.8B 1.2B 2.3B 4.5B\n\n1.6B 3.0B 6.0B\n\n5.7B 9.1B 17.2B 44.9B\n\n18.2B 27.8M 29.6B 41.6M 73.4M 54.3B 190.0M 141.2B\n\n41.6M 58.7B 104.6B 73.4M 190.0M 271.0B\n\n483.2M 648.5B\n\n70.0 71.5 72.5 –\n–\n\n64.3 67.3 70.1 72.1\n\n68.2 70.9 72.9\n\n72.8 74.2 74.3 75.5\n\n74.5 76.2 76.5 77.5\n\n76.8 77.1 77.8\n\n–\n\n– –\n– 78.1 78.3\n\n– –\n– –\n\n– –\n–\n\n74.1 75.8 76.7 78.4\n\n76.4 78.1 78.7 80.0\n\n78.4 79.3 80.6\n\n81.5\n\nTable 10: MOAT ImageNet hyper-parameter settings.\n\nhyper-parameter\n\nstochastic depth rate\n\ncenter crop randaugment mixup alpha loss type label smoothing train epochs train batch size optimizer type peak learning rate min learning rate warm-up lr decay schedule weight decay rate gradient clip EMA decay rate\n\nImageNet-1K\n\nImageNet-22K\n\n1K pre-training\n\n1K → 1K fine-tuning\n\n22K pre-training\n\n22K → 1K fine-tuning\n\n(MOAT-0/1/2/3)\n\n(MOAT-0/1/2/3)\n\n0.2 / 0.3 / 0.5 / 0.7\n\ntrue 2, 15 0.8 softmax 0.1 300 4096 AdamW 3e-3 1e-5 10K steps cosine 0.05 1.0 0.9999\n\n0.2 / 0.3 / 0.5 / 0.9\n\n0.1 / 0.2 / 0.3 / 0.6\n\n0.1 / 0.2 / 0.3 / 0.6\n\nfalse 2, 15/15/15/20 0.8 softmax 0.1 30 512 AdamW 5e-5 5e-5 none none 1e-8 1.0 0.9999\n\ntrue 2, 5 none sigmoid 0.0001 90 4096 AdamW 1e-3 1e-5 5 epochs linear 0.01 1.0 None\n\nfalse 2, 5 none softmax 0.1 30 1024 AdamW 5e-5 5e-5 none none 1e-8 1.0 0.9999\n\nA.3 COCO OBJECT DETECTION AND INSTANCE SEGMENTATION\n\nA.3.1 COCO OBJECT DETECTION EXPERIMENTAL DETAILS\n\nExperimental setup. We train Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017) on the COCO 2017 dataset (Lin et al., 2014) with our MOAT architectures. The dataset contains 118K training and 5K validation samples. We use the official TensorFlow (Abadi et al., 2016) implementation of Cascade Mask R-CNN by TF-Vision Model Garden (Yu et al., 2020). Our training setting closely follows the prior works (Chen et al., 2022b; Tu et al., 2022), except that we use batch size 64 and initial learning rate 0.0001. To adapt the MOAT models to high-resolution inputs, we\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: tiny-MOAT ImageNet hyper-parameter settings. ⋆: use EMA decay rate 0.9999 for tiny-MOAT-3.\n\nhyper-parameter\n\nstochastic depth rate\n\ncenter crop randaugment mixup alpha loss type label smoothing train epochs train batch size optimizer type peak learning rate min learning rate warm-up lr decay schedule weight decay rate gradient clip EMA decay rate\n\nImageNet-1K\n\n1K input size 224\n\n1K input size 256\n\n(tiny-MOAT-0/1/2/3)\n\n0.0 / 0.0 / 0.0 / 0.1\n\ntrue 2, 15 0.8 softmax 0.1 300 4096 AdamW 3e-3 1e-5 10K steps cosine 0.05 1.0 None⋆\n\n0.0 / 0.0 / 0.0 / 0.1\n\ntrue 2, 15 0.8 softmax 0.1 300 4096 AdamW 3e-3 1e-5 10K steps cosine 0.05 1.0 None⋆\n\npartition the features into non-overlapping windows for the self-attention computations with the window size set to 14 for the second last stage, and use global attention for the last stage. As a result of this window partition, the input size must be divisible by 14. The TF-Vision Model Garden codebase further requires the input size to be square (with padding) and divisible by 64. Hence, we choose 1344 as the input size, similar to the size used in the baseline methods (i.e., longest side is no more than 1333). We use Feature Pyramid Network (Lin et al., 2017) to integrate features from different levels.\n\nA.3.2 MORE COCO OBJECT DETECTION EXPERIMENTAL RESULTS\n\nIn this section, we perform more COCO object detection experiments with 896 input size. All the backbone are pretrained on ImageNet-1K dataset. MOAT-0/1/2 surpass UViT (Chen et al., 2021a) and MaxViT (Tu et al., 2022) by 3.9/5.2/4.9% APbox (3.1/4.1/3.9% APmask), and 3.0/4.0/4.0% APbox (2.4/3.2/3.0% APmask), respectively.\n\nTable 12: Object detection and instance segmentation on the COCO 2017 val set. We employ Cascade Mask-RCNN, and single-scale inference (hard NMS). All backbones are pretrained on ImageNet-1K.\n\nbackbone\n\ninput size\n\nparams\n\nUViT-T (Chen et al., 2021a) UViT-S (Chen et al., 2021a) UViT-B (Chen et al., 2021a)\n\nMaxViT-T (Tu et al., 2022) MaxViT-S (Tu et al., 2022) MaxViT-B (Tu et al., 2022)\n\nMOAT-0 MOAT-1 MOAT-2\n\n896 × 896 896 × 896 896 × 896\n\n896 × 896 896 × 896 896 × 896\n\n896 × 896 896 × 896 896 × 896\n\n51M 59M 74M\n\n86M 108M 146M\n\n65M 79M 110M\n\nFLOPs APbox APbox 50 –\n– –\n\n720B 882B 1160B\n\n51.2 51.9 52.5\n\n475B 595B 856B\n\n525B 580B 710B\n\n52.1 53.1 53.4\n\n55.1 57.1 57.4\n\n71.9 72.5 72.9\n\n73.6 75.7 76.0\n\nAPbox 75 –\n– –\n\n56.8 58.1 58.1\n\n59.9 62.6 63.0\n\n43.9 44.5 44.8\n\n44.6 45.4 45.7\n\n47.0 48.6 48.7\n\n50 –\n– –\n\n69.1 69.8 70.3\n\n70.5 72.9 73.2\n\nAPmask 75 –\n– –\n\n48.4 49.5 50.0\n\n51.1 52.7 53.1\n\nAPmask APmask\n\nA.4 ADE20K SEMANTIC SEGMENTATION\n\nExperimental setup. We experiment with the proposed MOAT models on ADE20K semantic segmentation dataset (Zhou et al., 2019) using DeepLabv3+ (Chen et al., 2018; 2017). We fine-tune the global attention for MOAT. The same training strategies are used for all backbone variants. Specifically, for training hyper-parameters, we train the model with 32 TPU cores for 180k iterations, with batch size 64, Adam (Kingma & Ba, 2015) optimizer, and a poly schedule learning rate starting at 0.0001. For data augmentations, the inputs images are resized and padded to either 513 × 513 or\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n641 × 641, with random cropping, flipping, and color jittering (Cubuk et al., 2019). No test-time augmentation is used during inference.\n\nA.5 COCO PANOPTIC SEGMENTATION\n\nExperimental setup. We also evaluate the proposed MOAT architectures on the challenging COCO panoptic segmentation dataset (Lin et al., 2014) using Panoptic-DeepLab (Cheng et al., 2020) with the official codebase (Weber et al., 2021). We fine-tune the global attention on downstream segmentation tasks for MOAT. We adopt the same training strategies for MOAT and its counterparts. Specifically, for training hyper-parameters, we train the model with 32 TPU cores for 200k iterations with the first 2k for warm-up stage. We use batch size 64, Adam (Kingma & Ba, 2015) optimizer, and a poly schedule learning rate starting at 0.0005. For data augmentations, the inputs images are resized and padded to 641 × 641, with random cropping, flipping, and color jittering (Cubuk et al., 2019). No test-time augmentation is used during inference.\n\nMain results. The results are summarized in Tab. 13, where MOAT consistently outperforms other backbones. Specifically, our MOAT-0 surpasses ConvNeXt-T significantly by 4.3% PQ. In the large model regime, MOAT-3 surpasses ConvNeXt-L by 3.5%. Our MOAT-4 achieves the performance of 46.7% PQ, outperforming the heavy backbone SWideRNet (Chen et al., 2020) by 2.3%.\n\nTable 13: Panoptic segmentation on COCO val set. The results are obtained by applying different backbones with Panoptic-DeepLab, using single-scale inference (i.e., no test-time augmentation). Results for MobileNet, ResNet, and Xception are cited from (Cheng et al., 2020), and results for SWideRNet is cited from (Chen et al., 2020), while results for ConvNeXt and MOAT are obtained using the official code-base (Weber et al., 2021) with the same training recipe. All models are trained and evaluated with input images resized to 641 × 641, and thus FLOPs are also measured w.r.t. size 641 × 641. †: use ImageNet-22K pretrained weights.\n\nbackbone\n\nparams\n\nMobileNet-V3 (Howard et al., 2019) ResNet50 (He et al., 2016a) Xception-71 (Chollet, 2017)\n\n- -\n-\n\nFLOPs\n\n12.2B 77.8B 109.2B\n\nConvNeXt-T (Liu et al., 2022b) ConvNeXt-S (Liu et al., 2022b) ConvNeXt-B† (Liu et al., 2022b) ConvNeXt-L† (Liu et al., 2022b) ConvNeXt-XL† (Liu et al., 2022b)\n\n51.3B 40.3M 61.9M 87.2B 103.8M 146.2B 220.1M 312.8B 379.6M 544.1B\n\nSWideRNet (Chen et al., 2020)\n\n752.5M 2614.0B\n\nMOAT-0 MOAT-1 MOAT-2† MOAT-3† MOAT-4†\n\n76.8B 39.5M 53.1M 119.7B 88.5M 199.7B 208.3M 493.3B 512.0M 1134.7B\n\nPQ (%)\n\nPQTh (%)\n\nPQSt (%)\n\n30.0 35.1 38.9\n\n36.7 40.0 41.7 41.9 43.0\n\n44.4\n\n41.0 43.0 43.9 45.4 46.7\n\n- -\n-\n\n37.3 41.4 43.6 43.6 44.9\n\n-\n\n42.6 44.7 45.9 48.3 49.5\n\n- -\n-\n\n35.7 37.9 38.9 39.4 40.0\n\n-\n\n38.6 40.4 40.8 41.1 42.4\n\nA.6 MORE ABLATION STUDIES\n\nA.6.1 ABLATION STUDIES ON THE MOAT MICRO-LEVEL DESIGN\n\nOrder of MBConv and Attn in MOAT block. Our MOAT block design reverses the order of Attention (Attn) and Mobile Convolution (MBConv), delegating the downsampling duty to the strided depthwise convolution within the MBConv. However, the dowsampling can be still performed in the MBConv with the original order (i.e., Attn + MBConv). Since the operations, Attn and MBConv, are interlaced, the key difference then comes from the first block in each stage, where the Attn is operated on the (1) spatially downsampled and/or (2) channel expanded features. To conduct the study, we employ different blocks in the MOAT variants, using ”Attn + MLP”, ”Attn + MBConv”, or ”MBConv + Attn”. For the ”Attn + MBConv” block, we further ablate the place (Attn vs. MBConv), where we apply the spatial downsampling and channel expansion operations.\n\nIn Tab. 14, we observe the following results. First, replacing the MLP with MBConv improves the performance by 0.3% and 0.7% for MOAT-0 and tiny-MOAT-2. Second, if we perform both spatial downsampling and channel expansion at the MBConv block, the performance is further improved by 0.5% and 0.9% for MOAT-0 and tiny-MOAT-2, showing that MBConv learns better downsampled\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Ablation studies of the order of MBConv and Attention (Attn) on ImageNet-1K with input 224. We also ablate the place, where we apply the spatial downsampling and channel expansion.\n\nmodel\n\nblock composition\n\nspatial downsampling\n\nchannel expansion\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\nMOAT-0\n\ntiny-MOAT-2\n\nAttn + MLP Attn + MBConv Attn + MBConv MBConv + Attn Attn + MBConv\n\nAttn + MLP Attn + MBConv Attn + MBConv MBConv + Attn Attn + MBConv\n\nAttn Attn MBConv MBConv MBConv\n\nAttn Attn MBConv MBConv MBConv\n\nAttn Attn MBConv MBConv Attn\n\nAttn Attn MBConv MBConv Attn\n\n28.0 28.2 25.6 27.8 29.3\n\n9.8 9.9 9.0 9.8 10.3\n\n5.4 5.4 5.8 5.7 7.1\n\n2.2 2.2 2.3 2.3 2.8\n\n82.6 82.9 83.1 83.3 83.2\n\n79.8 80.5 80.7 81.0 81.0\n\nfeatures. However, this design is equivalent to shifting the first Attn layer to its previous stage, reducing the representation capacity of the current stage. More concretely, only the last stage will be affected, since one layer is shifted. Third, to enhance the representation capacity, reversing the order of Attn and MBConv allows us to keep the first Attn layer in the same stage. This design further improves the performance by 0.7% and 1.2% for MOAT-0 and tiny-MOAT-2. Fourth, to compensate for the shifting effect, we could also employ another 1 × 1 convolution to expand the channels at the first Attn layer (then, MBConv only performs the spatial downsampling). However, this design performs similarly to our MOAT block design, but uses more parameters and FLOPs.\n\nA.6.2 ABLATION STUDIES ON THE MOAT MACRO-LEVEL DESIGN\n\nAblation studies on MOAT-based model. In Tab. 15, we ablate the stage-wise design by using either MBConv or MOAT block in stage 2 to stage 5. The first stage is the convolutional stem, containing two 3 × 3 convolutions. We use the layer layout of MOAT-0. As shown in the table, the pure MOAT-based model (i.e., using MOAT blocks for all four stages) achieves the best performance of 83.6%, which however uses the most FLOPs. Our MOAT model design (i.e., use MOAT block in the last two stages) attains the better trade-off between accuracy and model complexity.\n\nTable 15: Ablation studies of MOAT-based model on ImageNet-1K, using MOAT-0 layer layout and input size 224. We change the block type (MBConv vs. MOAT block) from stage 2 to stage 5. The first stage is fixed to use the convolutional stem.\n\nstage-2\n\nstage-3\n\nstage-4\n\nstage-5\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\nMOAT\n\nMOAT MOAT MOAT MOAT MBConv MOAT MOAT MBConv MBConv MOAT MOAT MBConv MBConv MBConv MOAT MBConv MBConv MBConv MBConv\n\n28.2 28.1 27.8 25.7 23.4\n\n11.9 6.9 5.7 4.7 4.5\n\n83.6 83.5 83.3 82.2 82.0\n\nAblation studies on MOAT meta architecture. We perform ablation studies on the meta-architecture by varying the number of blocks per stage. For simplicity, we only vary the block numbers in the third and fourth stages, while keeping the block numbers in the other stages unchanged. Note that the first stage corresponds to the convolutional stem. The studies with MOAT-1 meta architecture are shown in Tab. 16. In the end, we choose the layout {2, 2, 6, 14, 2} because it has the best performance and lower parameter cost. Interestingly, our discovery echoes the layer layout proposed by CoAtNet (Dai et al., 2021). We visualize the architecture of MOAT-1 in Fig. 6.\n\nTable 16: Ablation studies of MOAT meta-architecture design on ImageNet-1K, using MOAT-1 and input size 224. We control the first, second and last stages to have two blocks, and vary the block numbers of the third and fourth stages.\n\nnumber of blocks in five stages\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\n(2, 2, 2, 16, 2) (2, 2, 4, 15, 2) (2, 2, 6, 14, 2) (2, 2, 8, 13, 2) (2, 2, 10, 12, 2)\n\n43.7 42.6 41.6 40.6 39.5\n\n21\n\n8.9 9.0 9.1 9.2 9.3\n\n84.1 84.2 84.2 84.1 84.1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Architecture of MOAT-1, including the convolutional stem, MBConv, and MOAT blocks.\n\nA.7\n\nIMAGENET TRAINING TIME, PEAK TRAINING MEMORY AND TROUGHPUT MEASUREMENTS\n\nTable 17: ImageNet training time measured in hours. We use 16 TPUv4 cores for training MOAT-{0,1,2} and 32 TPUv4 cores for MOAT-3. MOAT is training efficient: for ImageNet-22k pretraining, MOAT takes no more than 2.05 days, while for ImageNet-1k pretraining, MOAT takes < 1 day.\n\ndataset\n\nmodel\n\npre-training\n\nfine-tuning\n\nImageNet-1K\n\nImageNet-22K\n\ninput size\n\n224 × 224\n\n224 × 224\n\n384 × 384\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\n6.5h 9.8h 13.9h 16.0h\n\n– –\n– –\n\n2.8h 4.4h 6.1h 7.9h\n\ninput size\n\n224 × 224\n\n224 × 224\n\n384 × 384\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\n20.1h 30.0h 42.6h 49.2h\n\n0.9h 1.3h 1.8h 2.2h\n\n2.5h 3.9h 5.4h 7.0h\n\nTable 18: ImageNet peak training memory of MOAT models. The input size is 224 × 224.\n\nmodel\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\ntraining statistics\n\ntotal batch size\n\nnum. of TPUv4 cores\n\nbatch size per core\n\npeak memory per core (MB)\n\n4096 4096 4096 4096\n\n16 16 16 32\n\n256 256 256 128\n\n19155 26170 26662 26260\n\nTable 19: ImageNet throughput measurement of MOAT models. We re-implement MOAT with the popular “timm” (Wightman, 2019) library in PyTorch, and measure the throughput on an Nvidia V100 GPU, following the same settings as DeiT (Touvron et al., 2021a), Swin (Liu et al., 2021), and ConvNeXt (Liu et al., 2022b).\n\ninput size\n\n224 × 224\n\n384 × 384\n\n512 × 512\n\nmodel\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3 MOAT-4\n\nparams (M)\n\nFLOPs (B)\n\nthroughput (images/sec)\n\nFLOPs (B)\n\nthroughput (images/sec)\n\nFLOPs (B)\n\nthroughput (images/sec)\n\n27.8 41.6 73.4 190.0 483.2\n\n5.7 9.1 17.2 44.9 –\n\n536 339 209 89 –\n\n18.2 29.6 54.3 141.2 –\n\n155 91 58 23 –\n\n– 58.7 104.6 271.0 648.5\n\n– 41 27 9\n4\n\n22\n\nMBConv blockMBConv blockMOATblockMOATblock×2×6×14×2H4×W4×96...MOAT block ×1H×W×3H8×W8×192H16×W16×384H32×W32×768stemH2×W2×64Published as a conference paper at ICLR 2023\n\nA.8 LIMITATIONS\n\nCurrently, the scaling rule of MOAT model variants are hand-designed. We, therefore, expect the architecture could be further improved by the breakthroughs in neural architecture search or network pruning (attaining faster inference speed while maintaining a similar accuracy).\n\n23",
  "translations": [
    "# Summary Of The Paper\n\nThis paper presents a family of neural networks called MOAT, which combines Mobile ConvNet and Transformer. It studies how to build effective networks based on two observations from Mobile ConvNet and Transformer. The new proposed networks achieve good performance on different tasks.\n\n# Strength And Weaknesses\n\nStrength: Paper writing is good and easy to follow; Idea is simple and effective; Experiments are sufficient.\n\nWeaknesses: The paper is more like a technical report. The novelty is weak by combing MobileConvet and Transformer.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: It's easy to follow. It presents its motivation based on two key observations which make sense to me.\nQuality: Writing is good and experiments are sufficient.\nNovelty: Novelty is a little weak for me. It is more technical by combining MobileConvet and Transformer. \nReproducibility: I think reproducibility is fine.\n\n# Summary Of The Review\n\nRefer to the above sections.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel family of neural networks that integrates Mobile Convolution (MBConv) and Attention mechanisms into a single block, diverging from previous methods that merely stack these two components. The authors propose a new architecture where the MLP in the transformer is replaced with a mobile convolution block, effectively enhancing the model's efficiency and representation capacity. The experimental results demonstrate that MOAT achieves state-of-the-art performance on several benchmarks, including 89.1% top-1 accuracy on ImageNet-1K and competitive results on COCO and ADE20K datasets. Additionally, a smaller variant, tinyMOAT, exhibits competitive performance while maintaining fewer parameters than existing mobile-specific transformer models.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to combining convolutional and transformer architectures at a micro-level, which provides significant improvements in both performance and efficiency. The experimental results are robust, consistently demonstrating that MOAT outperforms existing state-of-the-art models across various tasks and configurations. However, a notable weakness is the reliance on hand-designed scaling rules for MOAT variants, which may limit the model's adaptability and optimization potential. Furthermore, a more thorough exploration of the limitations and potential pitfalls of the proposed architecture could enhance the paper's comprehensiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with a logical flow from introduction to methodology and results. The authors effectively explain the technical aspects of their model while maintaining accessibility for readers who may not be experts in the field. The novelty of MOAT is significant, as it presents a fresh perspective on neural network design by merging two previously separate paradigms. The code is publicly available, which supports reproducibility, although further details on implementation specifics and hyperparameter tuning would enhance transparency and facilitate replication of results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to integrating convolutional and attention mechanisms, yielding strong empirical results across multiple vision tasks. While the contributions are significant, future work could address the limitations related to scaling rules and enhance the reproducibility of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces MOAT (Mobile Convolution with Attention), a novel family of neural networks that integrates mobile convolution (inverted residual blocks) with self-attention mechanisms. The architecture replaces the multi-layer perceptron (MLP) in standard Transformer blocks with a mobile convolution block, enhancing representation capacity and improving downsampling. MOAT models demonstrate state-of-the-art performance across multiple vision tasks, achieving significant accuracy on ImageNet classification, COCO object detection, and ADE20K semantic segmentation. The paper also presents a scaled-down version, Tiny-MOAT, which retains performance advantages over other mobile-specific transformer models.\n\n# Strengths And Weaknesses\nThe integration of MBConv and self-attention within a single block is a notable strength, providing a conceptually simple yet effective solution that yields high performance across tasks. The versatility of the MOAT architecture is commendable, allowing it to be applied to classification, detection, and segmentation without extensive modifications. However, a limitation is the manual design of scaling rules for model variants, which may hinder further optimization. Additionally, the requirement for extensive pretraining on large datasets raises accessibility concerns, and the potential for overfitting necessitates further evaluation of generalization across different datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and findings clearly, making it accessible to readers. The quality of the experimental design, including ablation studies and evaluations on diverse datasets, supports the findings effectively. The novelty lies in the integration of convolution and attention mechanisms within a single architecture, which is a significant contribution to the field. However, reproducibility may be challenging for some users due to the computational intensity required for training larger models like MOAT-4.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in neural network architectures by effectively combining mobile convolution and self-attention mechanisms. While it demonstrates strong performance across various tasks, concerns regarding manual scaling design and resource requirements may limit its broader applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces MOAT, an innovative architecture that integrates Mobile Convolution (MBConv) and Transformer blocks into a single block design, enhancing representation capacity and downsampling efficiency in vision tasks. The methodology involves extensive experimentation across multiple datasets, including ImageNet, COCO, and ADE20K, with various model configurations (MOAT-0 to MOAT-4 and tiny-MOAT). The findings demonstrate that MOAT achieves state-of-the-art performance, notably a top-1 accuracy of 89.1% on ImageNet-1K and record results on COCO and ADE20K, highlighting the architecture's effectiveness.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its novel approach to merging convolutional and attention mechanisms, which leads to significant improvements in performance across various tasks. The thorough experimental validation, including ablation studies and comparisons with existing architectures, adds credibility to the findings. However, a potential weakness is the lack of exploration into the generalization of the architecture to other domains outside of vision, which could limit the perceived versatility of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly explains the theoretical foundations and experimental design. The equations and block representations are presented in a clear manner, facilitating comprehension of the architecture's functionality. The novelty of the approach is significant, merging two distinct paradigms effectively. Furthermore, the authors have made their code publicly available, enhancing reproducibility and encouraging further research in this area.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in vision models through the integration of MBConv and Transformer architectures, yielding impressive empirical results. The clarity of the presentation and the thoroughness of the experiments support the architecture's potential for future research and application.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents the MOAT (Mobile Convolution and Attention) architecture, which introduces a novel block that synergistically combines mobile convolution and self-attention mechanisms. The authors demonstrate that this integration enhances the representation capacity and feature extraction capabilities of vision models. The paper reports state-of-the-art performance on benchmark datasets such as ImageNet, COCO, and ADE20K, with a particular emphasis on the scalability of the tinyMOAT family of models. The methodology includes comprehensive ablation studies to substantiate design choices and validate performance metrics.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its effective integration of convolution and attention, resulting in impressive performance metrics that set new benchmarks. The ablation studies contribute rigorously to understanding the architecture's efficacy. However, the complexity of interactions between the two mechanisms may pose challenges for further research. Additionally, the reliance on extensive pretraining may limit accessibility for practitioners with fewer resources. While the simplicity of the design promotes ease of understanding, it may also lead to missed opportunities for more sophisticated architectural explorations. Lastly, although the code is publicly available, documentation could be improved for users unfamiliar with the model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates the novel contributions of the MOAT architecture. The experimental design is robust, and the results are presented with clarity. However, the complexity of the interactions and the limited generalization of findings could hinder reproducibility and broader application. The public availability of code is a positive aspect, but the lack of thorough documentation may pose challenges for users seeking to replicate the results.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of vision models by effectively integrating mobile convolution and attention, achieving state-of-the-art performance. While the design principles are commendable, the complexity and limitations in generalization may affect future research and application. The availability of code enhances reproducibility, although improved documentation is necessary.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents MOAT (MObile convolution with ATtention), a novel architecture that integrates Mobile Convolution (MBConv) and self-attention mechanisms within a single block. This innovative design diverges from conventional models that stack separate convolutional and attention layers, enhancing representation capabilities and optimizing feature extraction. The authors report that MOAT achieves a top-1 accuracy of 89.1% on ImageNet-1K with ImageNet-22K pretraining and outperforms several state-of-the-art models in downstream tasks such as COCO and ADE20K. Additionally, the paper introduces tinyMOAT variants that maintain high performance while being resource-efficient.\n\n# Strength And Weaknesses\nStrengths of the paper include its unique architectural design that effectively combines MBConv and attention mechanisms, resulting in improved computational efficiency and model performance. The empirical results support the claims made regarding the efficacy of the MOAT architecture, demonstrating its superiority over existing models. However, a notable weakness is the lack of detailed insights into the scaling strategies for the tinyMOAT variants, which could limit understanding of their applicability across diverse tasks. Additionally, more extensive comparisons with other emerging hybrid models would strengthen the contextual relevance of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, making the methodology and findings accessible to readers. The quality of the writing is high, with logical flow and comprehensive explanations of the architectural choices made. In terms of novelty, the integration of MBConv and attention in a unified framework is a significant advancement in the field. However, while the empirical results are promising, further details on the experimental setup and reproducibility aspects would enhance the paper's robustness.\n\n# Summary Of The Review\nThe paper introduces a compelling and innovative architecture, MOAT, which successfully merges convolutional and attention mechanisms, demonstrating strong empirical performance across various benchmarks. While the contributions are significant, additional details on scaling strategies and comparative analyses would further enrich the paper's impact and applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models\" introduces a novel adversarial training framework by integrating Mobile Convolution (MBConv) with an attention mechanism, termed MOAT (MObile convolution with ATtention). The authors propose a micro-level architecture that alternates between MBConv and attention to enhance robustness against adversarial attacks while maintaining efficiency across various vision tasks. Key findings include state-of-the-art performance on adversarial benchmark datasets, improved model robustness without significant computational overhead, and scalability to resource-constrained environments.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of MBConv with attention mechanisms, which significantly enhances the robustness of models against adversarial attacks. The empirical results are compelling, showcasing clear advantages in both adversarial and standard performance metrics. Furthermore, the paper is well-structured, providing detailed explanations of the methodology and experimental setups. However, the weaknesses include a lack of discussion regarding the limitations of the proposed approach, particularly in highly adversarial environments, and insufficient elaboration on strategies for optimizing training in larger datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clear and well-written, with logical organization that facilitates understanding of the methodology and findings. The quality of the experiments is high, with extensive testing on well-known datasets such as ImageNet, COCO, and ADE20K. While the novel approach offers significant advancements in adversarial training, the reproducibility could be improved by providing additional details on the experimental setup and hyperparameter tuning.\n\n# Summary Of The Review\nThe MOAT framework presents a noteworthy advancement in adversarial training, effectively combining convolutional and attention mechanisms to enhance model robustness while maintaining efficiency. Despite minor weaknesses in discussing limitations and optimization strategies, the empirical results support the framework's effectiveness and scalability, making it a valuable contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel family of neural networks that integrates Mobile Convolution and Attention within a single architectural block, claiming to simplify design and enhance performance on vision tasks. The findings report a top-1 accuracy of 89.1% on ImageNet-1K and a 59.2% average precision on COCO object detection, purportedly setting new state-of-the-art benchmarks. Additionally, the paper discusses the introduction of tiny-MOAT models, which reportedly outperform several leading models in efficiency and performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to merging Mobile Convolution and Attention, which could simplify future model designs and potentially enhance performance. The reported performance metrics are impressive, particularly the claimed improvements on COCO object detection, suggesting a significant step forward in practical applications. However, the weaknesses include the potential exaggeration of these achievements, as the advancements may represent only incremental improvements over existing methods. Furthermore, the claims of MOAT redefining neural network architectures lack robust comparative analysis with prior models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly, with a logical progression from introduction to methodology and results. However, the claims of novelty and impact are somewhat overstated, which may mislead readers regarding the true significance of the contributions. The availability of code enhances reproducibility, though the paper does not thoroughly address the specific experimental setups and comparative analyses necessary for readers to fully appreciate the claimed advancements.\n\n# Summary Of The Review\nOverall, while the paper presents MOAT as a promising advancement in vision model design, the claims of revolutionary impact and performance improvements seem exaggerated. The contributions are noteworthy but may not significantly alter the landscape of neural network architectures as portrayed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel architecture that combines Mobile Convolution (specifically inverted residual blocks) with self-attention mechanisms to enhance feature representation in vision tasks. The methodology involves replacing the standard Multi-Layer Perceptron (MLP) within Transformers with Mobile Convolution blocks, and reordering operations to optimize downsampling. The findings are substantial, with MOAT achieving state-of-the-art results on various benchmarks: a top-1 accuracy of 88.5% on ImageNet-1K, 61.5% APbox in COCO object detection, and 58.0% mIoU in ADE20K segmentation, demonstrating its effectiveness against existing models.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of mobile convolution and attention mechanisms, which results in improved performance across multiple vision tasks. The empirical results are robust, showcasing MOAT's superiority over established architectures like Swin and ConvNeXt. However, a notable weakness is the current reliance on manually designed scaling rules for the various MOAT variants, which may limit the model's efficiency and scalability. The authors suggest that automated approaches such as neural architecture search could enhance the design process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and results clearly. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, as it bridges the gap between Convolutional Networks and Transformers in a unique way. However, while the experimental results are convincingly presented, the reproducibility could be impacted by the lack of a detailed discussion on the implementation specifics and scaling rules.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in vision model architectures through the MOAT framework, demonstrating impressive empirical results and a novel combination of existing techniques. While the contributions are significant, potential improvements in efficiency and reproducibility could enhance the impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the MOAT model, which integrates mobile convolution with attention mechanisms to enhance representation capacity and efficiency in vision tasks. The authors claim that this approach addresses the limitations of standard Transformer architectures, particularly in high-dimensional data contexts. Key contributions include the introduction of an inverted bottleneck design for better local pixel interactions and empirical results demonstrating significant performance improvements over existing state-of-the-art models across various tasks. The methodology involves a systematic evaluation of the model's performance on standard benchmarks, emphasizing the benefits of downsampling through depthwise convolutions and the general applicability of the MOAT design.\n\n# Strength And Weaknesses\nThe paper presents notable strengths, particularly in its innovative integration of mobile convolution within Transformer architectures, which may lead to improved efficiency and representation. The empirical results showing superior performance are compelling, though they require further validation regarding statistical significance and robustness across diverse settings. However, some weaknesses arise from the assumptions made about the synergy of convolution and attention, the generalizability of findings to various tasks, and the potential limitations of the proposed scaling strategy. There is also a need for a more comprehensive evaluation of model performance metrics beyond accuracy alone.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, making it accessible to readers. The methodology is presented with sufficient detail to allow for reproducibility; however, some assumptions and claims lack empirical backing, which could hinder replicability in certain contexts. The novelty of integrating mobile convolution with attention is significant, although the paper could benefit from a more thorough exploration of alternative architectures and the complexities of model design. Overall, the quality of writing and presentation is commendable, with room for improvement in justifying certain claims.\n\n# Summary Of The Review\nThe paper offers a promising approach to enhancing vision models through the integration of mobile convolution and attention mechanisms, with empirical results suggesting notable performance gains. However, the assumptions made regarding model design, generalizability, and evaluation metrics warrant further scrutiny to solidify the findings and broaden their applicability. Overall, while the contributions are relevant and innovative, additional validation and exploration of limitations are needed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel family of neural networks that synergistically integrates mobile convolution (MBConv) and self-attention mechanisms into a single architectural block. This hybrid design aims to enhance the representation capacity and efficiency of feature downsampling. MOAT outperforms existing models in various vision tasks, including ImageNet classification, COCO detection, and ADE20K segmentation, achieving state-of-the-art results with fewer parameters. The methodology includes a reordering of operations and the substitution of the traditional MLP in Transformers with MBConv blocks, which is demonstrated to significantly improve performance.\n\n# Strength And Weaknesses\nOne of the primary strengths of the paper is its innovative combination of MBConv and Transformer architectures, which addresses some limitations of both approaches and contributes to the growing field of hybrid models. The performance gains shown across multiple datasets underscore the effectiveness of the proposed methodology. However, a notable weakness is the manual nature of the current design scaling, which may limit the applicability of the MOAT architecture in practice and suggests a need for future work in automated architecture search.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations and contributions of the MOAT architecture. The experimental results are presented with sufficient detail, allowing for reproducibility. The novelty of the approach is significant, as it provides a new perspective on merging ConvNets and Transformers, although the potential for broader application remains to be explored in more depth.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in neural network architecture through the MOAT design, demonstrating strong empirical results across various tasks. While there are clear contributions to the field, the limitations in design scaling present a challenge for future implementations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel architecture designed to enhance performance across various machine learning tasks by integrating innovative techniques inspired by recent advancements in the field. The authors articulate a clear rationale for their approach, situating it within the current landscape of research, and present empirical results that demonstrate its effectiveness. The findings suggest that the proposed method not only outperforms existing techniques but also has the potential to be applied in diverse applications, thereby contributing significantly to the ongoing discourse in machine learning.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Contribution**: The proposed architecture introduces a compelling concept that could push the boundaries of current machine learning methodologies.\n2. **Comprehensive Literature Review**: The authors thoroughly review existing literature, effectively identifying gaps that their work seeks to fill, which enhances the relevance of their contribution.\n3. **Theoretical Foundation**: A solid theoretical basis for the proposed methodology is presented, lending credibility to the authors' claims and findings.\n4. **Wide Applicability**: The method's versatility across various tasks suggests significant implications for future research and applications in the field.\n\n**Weaknesses:**\n1. **Lack of Clarity**: Certain elements of the methodology are not clearly articulated, which could hinder comprehension for readers less familiar with the area.\n2. **Limited Experimental Validation**: The experimental section could benefit from a more extensive evaluation that includes a wider variety of datasets and tasks to better substantiate the claims made.\n3. **Insufficient Baseline Comparisons**: The comparison with existing methods is somewhat limited, as the selected baselines do not fully represent the current state-of-the-art.\n4. **Reproducibility Issues**: The paper does not provide enough implementation details, which could pose challenges for others attempting to reproduce the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hampered by some ambiguous descriptions of the methodology, which may alienate readers who are not deeply versed in the subject matter. While the quality of the theoretical grounding is commendable, the experimental results lack depth and breadth, limiting their impact. The novelty of the proposed approach is significant, yet the issues surrounding reproducibility due to insufficient detail could undermine its adoption by the community.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field, with an innovative approach that has the potential to advance current methodologies. However, improvements are necessary in terms of clarity, experimental rigor, and reproducibility to fully realize its impact. Addressing these concerns could elevate the work to a higher standard.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel family of neural networks that integrates Mobile Convolution (MBConv) and Attention mechanisms into a single architectural block known as the MOAT block. The innovative design replaces the traditional multi-layer perceptron (MLP) in Transformer blocks with MBConv, thereby enhancing representation capacity and improving the processing of downsampled features. The findings indicate that MOAT networks achieve high top-1 accuracy on ImageNet benchmarks (89.1% on ImageNet-1K with ImageNet22K pretraining) and perform effectively on downstream tasks such as COCO object detection and ADE20K semantic segmentation. The authors also present a scaled-down version, the tiny-MOAT family, which outperforms several mobile-specific transformer-based models while emphasizing a design philosophy centered on simplicity and efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of MBConv and attention, which addresses existing limitations in traditional architectures by optimizing the order of operations to enhance performance. The empirical results demonstrate significant performance improvements across various vision tasks, further emphasizing the model's effectiveness. However, a potential weakness is the lack of extensive ablation studies to dissect the contributions of each component within the MOAT block, which could provide deeper insights into the mechanisms driving the observed performance gains. Additionally, while the simplicity of the design is a strength, it may also limit the exploration of more complex interactions between convolutional and attention mechanisms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making it accessible to readers with varying levels of expertise in the field. The quality of the methodology is solid, and the results are compelling; however, the novelty of the approach, while present, could be further reinforced with additional comparative studies against a broader range of existing architectures. The reproducibility is bolstered by the availability of the code, which encourages community engagement and further experimentation with the MOAT framework.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in neural network design by effectively combining mobile convolution and attention in a single framework, achieving state-of-the-art performance on various benchmarks. While the contributions are impactful, further exploration of the underlying mechanisms and additional comparative analyses could enhance the paper's claims.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces MOAT, a novel neural network architecture that integrates mobile convolution with attention mechanisms for enhanced performance in vision tasks. The key innovation is the MOAT block, which replaces the MLP component in transformers with mobile convolution to improve local feature extraction before applying attention. The authors present multiple model variants, demonstrating that MOAT achieves state-of-the-art accuracy on ImageNet and excels in downstream tasks such as object detection and segmentation. Extensive ablation studies validate the effectiveness of design choices, establishing a new standard for combining convolutional and transformer architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating mobile convolution and attention, effectively addressing the limitations of prior architectures. By focusing on micro-level design innovations, the authors contribute significantly to the ongoing discourse on hybrid models in computer vision. However, the paper could benefit from a more thorough exploration of the theoretical implications of the proposed architecture, as well as a broader range of experiments beyond the showcased datasets to fully demonstrate its robustness across different scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodology, making it accessible to readers. The quality of the writing is high, and the experimental results are convincingly reported. The novelty of the approach is evident, particularly in its combination of two distinct paradigms in a cohesive manner. The code availability further enhances reproducibility, allowing the community to validate and build upon the findings presented.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to integrating mobile convolution and attention mechanisms in vision models, achieving state-of-the-art results on several benchmarks. While the contributions are significant, the paper could be strengthened by deeper theoretical insights and broader empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models\" presents a novel approach to improving vision models by integrating mobile convolutions with self-attention mechanisms. The authors introduce the MOAT block, which effectively combines the strengths of MBConv and transformers, thereby addressing existing gaps in the efficiency and performance of vision architectures. Experimental results demonstrate that MOAT achieves significant performance improvements on benchmark datasets such as ImageNet, COCO, and ADE20K, showcasing its capability to enhance model efficiency while maintaining accuracy.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive methodology, which includes a well-defined design for the MOAT block and thorough ablation studies that validate the proposed architectural choices. The authors provide a clear comparison against state-of-the-art models, illustrating the effectiveness of their approach. However, one weakness could be the potential overfitting to specific datasets, as the results primarily focus on popular benchmarks without extensive testing on more diverse or challenging datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized, with clear language and a logical flow that aids in understanding the complex concepts presented. The quality of the figures and tables is high, enhancing the presentation of results. In terms of novelty, the integration of mobile convolution and attention mechanisms is a fresh approach that contributes significantly to the field. The authors have made their code publicly available, supporting reproducibility and enabling further exploration of their findings.\n\n# Summary Of The Review\nOverall, this paper presents a strong contribution to the field of vision models through its innovative MOAT architecture, which successfully combines mobile convolutions and attention mechanisms. The thorough experimental analysis and clear presentation of results bolster its claims, making it a valuable addition to the literature.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel neural network architecture that integrates Mobile Convolution (MBConv) and Attention mechanisms, aiming to leverage the strengths of both paradigms. The MOAT block reorders traditional transformer components, placing mobile convolution before self-attention to enhance representation capacity and downsampling efficacy. Empirical results demonstrate that MOAT achieves state-of-the-art performance, with a top-1 accuracy of 89.1% on ImageNet-1K (pretrained on ImageNet-22K) and superior metrics on downstream tasks such as COCO and ADE20K, proving its effectiveness and flexibility across various vision applications.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative design of the MOAT block, which effectively combines benefits from both convolutional and transformer architectures. The empirical results are robust, indicating significant advancements in model performance and efficiency, particularly in lightweight configurations. However, a notable weakness is the manual design of model scaling strategies, which may hinder the model's adaptability and efficiency in practical applications. Future work could benefit from exploring automated optimization techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the motivation, methodology, and results of the proposed architecture. The mathematical formulations and experimental setups are presented in a comprehensible manner. The novelty of the MOAT block lies in its unique integration of MBConv and attention mechanisms, which is a compelling advancement in the field. While the experimental results are reproducible, the paper could benefit from more detailed descriptions of the training procedures and hyperparameter settings to further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of vision models by introducing the MOAT architecture, which effectively combines mobile convolution and attention mechanisms. The empirical results validate its performance across various tasks, though the manual scaling strategies present a limitation that could be addressed in future research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces MOAT, a model that seeks to integrate mobile convolution and attention mechanisms within a single architecture. The authors replace the multi-layer perceptron (MLP) in the traditional Transformer block with a mobile convolution block, claiming that this approach leads to improved performance on image classification tasks. The results reportedly achieve state-of-the-art accuracy on benchmarks like ImageNet, although concerns regarding overfitting and generalizability are raised due to the model's reliance on extensive pretraining.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its empirical performance, as MOAT achieves competitive results on well-known benchmarks. However, the methodology appears derivative, lacking innovative theoretical foundations. The decision to replace the MLP with mobile convolution seems arbitrary and does not convincingly demonstrate significant improvements. Additionally, the reliance on large-scale pretraining raises questions about the model’s generalizability across different datasets, and the marginal improvements observed in ablation studies do not justify the architectural changes. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the novelty of the approach is questionable. While the authors claim that their architecture is \"conceptually simple,\" this simplicity may obscure the depth of the model's design. The reproducibility of the results may be hindered by the potential biases in data selection and differences in experimental setups compared to other state-of-the-art models. Furthermore, the scalability and efficiency of the model are inadequately addressed, particularly concerning the increased computational costs and memory requirements.\n\n# Summary Of The Review\nOverall, while the MOAT model achieves state-of-the-art results, it does so without introducing significant advancements in methodology or theoretical understanding. The paper raises important questions about generalizability and efficiency, suggesting that the integration of convolution and attention may not fundamentally alter the landscape of vision models.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces MOAT (MObile convolution with ATtention), a novel architecture that integrates mobile convolution and attention mechanisms into a single framework. This approach simplifies the architecture while significantly enhancing performance across various vision tasks. Experimental results demonstrate state-of-the-art accuracy, achieving 89.1% top-1 accuracy on ImageNet-1K and 81.5% on ImageNet-1K-V2 with only 483M parameters, as well as impressive results on downstream tasks such as COCO object detection (59.2% APbox) and ADE20K semantic segmentation (57.6% mIoU). Additionally, the paper presents the tiny-MOAT variants, which maintain high performance while offering a smaller model footprint.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative integration of convolution and attention, which leads to remarkable performance improvements and a simpler architecture. The empirical results are compelling, showcasing MOAT's versatility across various tasks. However, a potential weakness lies in the reliance on pretraining on a larger dataset (ImageNet-22K) to achieve these results, which may limit the model's applicability in scenarios where such pretraining is not feasible. Further exploration of the architecture's performance without extensive pretraining could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with clear explanations of the methodology and results. The presentation of the experimental results is thorough, and the discussion effectively highlights the significance of the contributions. The novelty of the MOAT architecture is evident, particularly in its combination of mobile convolution and attention mechanisms. The open-source release of the code enhances reproducibility, allowing the research community to build upon this work.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in vision models by introducing the MOAT architecture, which effectively combines mobile convolution and attention. The empirical results are impressive and highlight the model's versatility and efficiency. This work is poised to inspire further research and innovation in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces MOAT (MObile convolution with ATtention), a novel neural architecture that merges mobile convolution and self-attention. It theorizes that combining these two paradigms can leverage their strengths to create a hybrid model with enhanced performance. The methodology includes analyzing the structural similarities between MBConv and transformer blocks, introducing a new operational sequence in the MOAT block, and proposing scaling strategies for improved representation capacity and computational efficiency. Empirical results validate the theoretical claims, demonstrating the model's effectiveness in visual recognition tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which provide a solid foundation for integrating convolutional and attention mechanisms. The insights into the similarities between MBConv and transformer blocks highlight an important area of research that can inspire further architectural innovations. However, the paper has a notable weakness in that it emphasizes theoretical constructs over empirical validation. While empirical results support the proposed architecture, the lack of extensive experimentation may limit the generalizability of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with varying levels of expertise. The quality of the theoretical analysis is high, providing a compelling argument for the MOAT framework's potential impact on neural architecture design. The novelty of the approach is significant, as it bridges two prominent paradigms in deep learning. However, reproducibility may be a concern due to the limited empirical validation, as the benchmarks provided may not fully capture the model's capabilities across different tasks and datasets.\n\n# Summary Of The Review\nIn summary, the paper presents a theoretically rich framework for integrating mobile convolution and attention mechanisms in neural architectures. While the contributions are significant and well-articulated, the emphasis on theoretical constructs over empirical validation may limit the practical applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the MOAT (Multi-Head Attention with MBConv) architecture, which integrates multi-head self-attention mechanisms with MobileNet's MBConv blocks to enhance performance on image classification tasks. The methodology involves extensive experiments on ImageNet, including pretraining and fine-tuning, while employing a variety of regularization techniques. The findings indicate that the proposed architecture outperforms existing models by effectively leveraging the strengths of both attention and convolutional layers, particularly highlighting the importance of the order in which these components are arranged.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of attention and convolutional components, which allows for improved feature representation and performance on benchmarks. The thorough ablation studies provide valuable insights into the architectural choices, showcasing how variations in block compositions impact performance. However, a potential weakness is the focus on a single dataset (ImageNet) for evaluation, which may limit the generalizability of the findings across different domains. Additionally, while the training times and memory requirements are reported, more context about the computational efficiency relative to other architectures would strengthen the discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and experimental setup, making it accessible for readers. The quality of the writing is high, and the details provided enable reproducibility, as the official code is available on GitHub. The novelty of the approach lies in the effective combination of MBConv and attention mechanisms, which is not widely explored in existing literature, thus contributing to the significance of the work.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of image classification by proposing a novel architecture that combines the strengths of multi-head attention and MBConv blocks. The experimental results are compelling, though the evaluation is primarily limited to a single dataset. The findings are well-supported by rigorous ablation studies and detailed methodology.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents MOAT, a novel architecture that combines mobile convolution and attention mechanisms, claiming to outperform existing models such as CoAtNet and MobileViT on tasks like ImageNet classification and COCO detection. The authors argue that MOAT achieves state-of-the-art performance while requiring fewer parameters and avoiding the need for extra downsampling layers. They provide empirical results to support their claims, although these comparisons lack nuance and context regarding the training and architectural choices of baseline models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of MOAT as a design that simplifies certain aspects of existing models while maintaining competitive accuracy. However, the paper's weaknesses include an exaggerated portrayal of MOAT's advantages over established architectures without a fair assessment of the context of those models. The comparisons often lack rigor, failing to acknowledge the complexities of other architectures and their performance metrics. Furthermore, the ablation studies seem to favor MOAT without adequately exploring the broader implications of its design choices.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but it could benefit from a more balanced view of the existing literature and a deeper critique of MOAT's limitations. The novelty of the proposed architecture is somewhat overshadowed by the lack of acknowledgment of prior work that has paved the way for such innovations. Reproducibility is not thoroughly addressed, as the paper does not provide sufficient details on the training procedures or hyperparameters used in the experiments.\n\n# Summary Of The Review\nOverall, while the paper presents a potentially valuable contribution to the field with the MOAT architecture, it suffers from a lack of nuanced comparisons and a tendency to dismiss the effectiveness of existing models. A more balanced and contextualized discussion of related work would enhance the paper's credibility and provide a clearer understanding of MOAT's significance in the landscape of vision models.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Moat: Alternating Mobile Convolution and Attention Brings Strong Vision Models\" presents a novel architecture that integrates mobile convolution and attention mechanisms to enhance performance in vision tasks. The authors introduce the MOAT block, which modifies the traditional Transformer block by incorporating a Mobile Inverted Residual Block (MBConv) in place of the standard MLP, allowing for improved computational efficiency. The experimental results demonstrate that the proposed MOAT models achieve significant performance gains on the ImageNet-1K and COCO datasets, achieving top-1 accuracy rates of 89.1% and 81.5%, respectively.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to combining mobile convolution with attention mechanisms, which is shown to yield effective models for vision tasks. The experimental validation on standard benchmarks is thorough, showcasing the architecture’s robustness across various settings. However, the paper has several weaknesses, including inconsistencies in terminology and formatting throughout the text, which detracts from the overall clarity. Additionally, certain claims lack sufficient empirical backing or detailed explanations, making it difficult for readers to fully grasp the implications of some findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hampered by various inconsistencies and vague terminology, making it challenging for readers to follow the authors' arguments. The quality of the presented methodology is high, but the lack of precise definitions and explanations for certain terms (e.g., \"cross-window propagation\") limits reproducibility. While the novelty of the approach is commendable, the execution could benefit from clearer communication of ideas and results. Reproducibility is also a concern due to insufficient detail regarding experimental setups and model configurations.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of vision models by proposing a novel architecture that effectively merges mobile convolution with attention mechanisms. However, the execution is marred by clarity issues and inconsistencies that hinder the understanding and reproducibility of the findings. Addressing these concerns could significantly enhance the impact of the research.\n\n# Correctness\n4/5 \n\n# Technical Novelty And Significance\n4/5 \n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the MOAT (Mobile convolution and Attention) block, which integrates mobile convolutional layers with attention mechanisms to enhance performance in vision tasks. The authors demonstrate the efficacy of their model through evaluations on standard benchmarks such as ImageNet and COCO, achieving state-of-the-art results in specific contexts. However, the paper does not sufficiently explore the broader implications of the proposed model across different modalities or under varied environmental conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of mobile convolutions and attention mechanisms, leading to improved performance metrics on vision tasks. However, the paper has several weaknesses, including a limited scope of evaluation, as it does not address model robustness under adversarial conditions or performance across diverse datasets. Additionally, the lack of discussion regarding interpretability and potential biases in the training data is a significant omission that detracts from the overall impact of the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-defined methodologies and results. However, the novelty is somewhat constrained by the narrow focus on vision tasks without consideration of other modalities or environmental factors. The reproducibility of results may be hindered by the absence of comprehensive comparisons with a wider range of architectures and the lack of detailed analysis on model scalability and efficiency across different hardware configurations.\n\n# Summary Of The Review\nOverall, the paper contributes a novel approach to vision modeling through the integration of mobile convolutions and attention mechanisms, achieving commendable results on standard benchmarks. Nevertheless, it falls short in addressing critical aspects like model robustness, interpretability, and ethical considerations, which are essential for practical deployment and long-term relevance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces MOAT, a novel neural network architecture that integrates Mobile Convolution (MBConv) and Attention mechanisms to enhance performance on various vision tasks. The authors demonstrate that MOAT achieves state-of-the-art results through rigorous experimentation, reporting top-1 accuracy of 89.1% on ImageNet-1K, along with improvements in APbox and mIoU on COCO and ADE20K datasets. The methodology includes extensive ablation studies and comparative analyses against leading models, highlighting the significance of architectural choices in improving accuracy.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive experimental design and thorough comparative analysis, which effectively showcase the advantages of the MOAT architecture over existing models like ConvNeXt and Swin. The use of ablation studies adds depth to the findings, allowing for a clearer understanding of the impact of specific architectural components. However, a notable weakness is the lack of explicit reporting of statistical tests, which would bolster the claims of significance. Additionally, while the paper effectively discusses performance metrics, it could benefit from a more detailed exploration of the underlying mechanisms that contribute to the observed improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers. The quality of the experiments is high, with a systematic approach to model training and evaluation. While the novelty of combining MBConv and Attention is significant, the broader implications of this hybrid approach could be better articulated. Reproducibility is facilitated by the detailed descriptions of training procedures and datasets, although the absence of explicit statistical tests may hinder full replication of results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the MOAT architecture, demonstrating notable improvements in performance metrics across multiple vision tasks. The comprehensive experimental design and clear presentation of findings enhance the paper's contributions, although the lack of explicit statistical validation is a drawback that should be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the MOAT (Modeling Object-Aware Transformers) architecture, which aims to improve performance in visual recognition tasks by leveraging a transformer-based approach. The authors implement various model variants, utilizing ImageNet-22K for pretraining, and report promising results on standard benchmarks. However, the methodology relies on hand-designed scaling rules and does not explore various optimization techniques or alternative downsampling methods, which may limit the model's adaptability and performance across different tasks.\n\n# Strength And Weaknesses\nThe paper offers a novel approach to visual recognition by combining transformer architectures with object-aware mechanisms, marking a significant step in the evolution of deep learning models. However, several weaknesses undermine its contributions: the lack of automated optimization methods for model scaling, limited exploration of downsampling techniques, and no investigation into hyperparameter tuning. Additionally, the model's reliance on extensive pretraining raises concerns regarding its generalizability, while the absence of real-time application analysis and computational cost considerations further limits its practical relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the MOAT architecture and the experiments conducted. However, the reproducibility of results may be hindered by the absence of detailed descriptions regarding hyperparameter tuning and model optimization. The novelty is present in the proposed architecture, yet the exploration of advanced techniques and real-world applications is insufficient, which detracts from its overall impact.\n\n# Summary Of The Review\nWhile the paper introduces a promising new architecture for visual recognition, significant limitations in methodology and practical applicability hinder its overall contribution to the field. Future work should address these gaps to enhance the model's adaptability and utility in diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models\" proposes a new architectural block that combines mobile convolution (MBConv) and attention mechanisms. The authors argue that their design effectively leverages the strengths of both approaches to improve performance on vision tasks. They present experimental results demonstrating that their MOAT block outperforms existing models, including a smaller variant named Tiny-MOAT. Additionally, the paper includes various ablation studies to justify their design choices and the order of operations in their methodology.\n\n# Strength And Weaknesses\nWhile the paper presents a novel combination of two established techniques—MBConv and attention—it does not seem to offer significant advancements over prior work. The authors' justification for the superiority of their approach relies on observations that are largely acknowledged within the community. The experimental results, although impressive, do not adequately address the real-world applicability of the model. Furthermore, the extensive ablation studies may come off as unnecessary padding. The paper lacks depth in exploring the implications of their findings and fails to present truly novel insights.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by its reliance on well-trodden concepts without providing fresh perspectives. The quality of the writing is generally acceptable, but the novelty is questionable given the saturation of similar works in the field. The reproducibility of the results may be impacted by the lack of comprehensive details about the experimental setup and the challenges presented by real-world data, which the authors gloss over.\n\n# Summary Of The Review\nOverall, this paper does not significantly advance the field of vision models despite presenting a somewhat novel combination of two existing techniques. The contributions feel more like reiterations of known concepts rather than groundbreaking insights. The experimental results, while favorable, do not convincingly translate to practical applications in diverse real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the MOAT (Mobile Convolution and Attention) block, which innovatively integrates Mobile Convolution (MBConv) and Attention mechanisms to enhance model efficiency and performance in mobile-specific transformer architectures. The authors demonstrate that reversing the order of MBConv and Attention improves representational capacity, leading to significant performance gains in the tiny-MOAT family compared to existing mobile transformer models. Empirical results across various benchmarks indicate that MOAT achieves state-of-the-art performance while maintaining a simple design, which could facilitate further research into model compression and efficient architectures.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel approach to architecture design by successfully integrating MBConv with Attention in a way that prioritizes efficiency without sacrificing accuracy. The innovative reversal of component order is particularly noteworthy, suggesting potential for exploring alternative configurations that could yield further insights. However, the paper could benefit from an expanded discussion on the implications of the downsampling operations used in the MOAT block, as well as a more thorough examination of model robustness under adversarial conditions. Additionally, the exploration of other architectural combinations and the broader applicability of MOAT to different domains is somewhat limited.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, making it easy to follow the methodology and findings. The quality of the experiments is commendable, with solid ablation studies that provide insights into the component interactions within the MOAT architecture. While the novelty of the proposed design is significant, the reproducibility of results could be enhanced by providing more detailed descriptions of the training protocols and hyperparameters used. Future work should focus on these aspects to further ensure that the research can be replicated and built upon.\n\n# Summary Of The Review\nOverall, the paper introduces a promising architectural framework in MOAT that effectively balances efficiency and performance, with potential applications in various domains. The innovative integration of MBConv and Attention mechanisms, alongside empirical validations, demonstrates the significance of this work. However, further exploration of architectural configurations and robustness measures would strengthen the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents the MOAT family of models, which achieve state-of-the-art performance on several vision benchmarks, including ImageNet-1K, COCO, and ADE20K. The authors introduce innovative architectural elements that combine convolutional and attention mechanisms, leading to substantial improvements in accuracy and efficiency. MOAT-4 reaches 89.1% top-1 accuracy on ImageNet-1K, setting a new record, while other variants demonstrate competitive performance across various model sizes and tasks, showcasing the versatility and efficiency of the proposed architecture.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the robust performance of the MOAT models across multiple datasets and tasks, achieving significant improvements over existing state-of-the-art models. The training efficiency and parameter efficiency of the models are noteworthy, as they manage high performance without excessive computational demands. However, the paper could benefit from a more detailed analysis of the architectural innovations and their specific contributions to the performance gains. Additionally, the reproducibility of results could be enhanced with clearer documentation of training procedures and hyperparameter settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with comprehensive comparisons against other models. The quality of the experiments appears solid, and the results are compelling. In terms of novelty, the combination of convolutional and attention mechanisms in the proposed architecture is a valuable contribution to the field. However, reproducibility is a concern, as the paper lacks sufficient detail on some experimental setups and hyperparameters, which may hinder other researchers from replicating the results effectively.\n\n# Summary Of The Review\nOverall, the paper provides significant contributions to the field of computer vision through the introduction of the MOAT models, which achieve state-of-the-art performance across multiple tasks. While the results are impressive and the methodology is sound, improvements could be made in the areas of architectural clarity and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel framework termed MOAT, which aims to address the limitations of existing models in processing certain tasks. The methodology involves integrating mobile convolution and self-attention operations to enhance performance in resource-constrained environments. Findings indicate that the proposed model outperforms traditional architectures in both accuracy and efficiency, demonstrating significant improvements across several benchmark datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining mobile convolution and self-attention mechanisms, which offers a compelling solution to existing model limitations. However, the paper suffers from a few weaknesses, including an overly complex language that may hinder accessibility, dense structure that affects flow, and a lack of clarity in the presentation of figures and tables. Additionally, the conclusion does not effectively encapsulate the significance of the findings, which could detract from the overall impact of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper addresses an important research gap and presents a novel framework, the clarity of the presentation is compromised by excessive jargon and complex sentences. The quality of the research is solid, but the readability could be improved with a more engaging style and better integration of figures and tables into the narrative. Reproducibility may be an issue due to the lack of detailed explanations of certain methodologies and the presentation of results, which could make it challenging for other researchers to replicate the studies.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field by introducing the MOAT framework. However, issues with clarity, structure, and presentation may hinder its accessibility and impact. Improvements in readability and the organization of content are necessary for the work to reach a broader audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.393206996236309,
    -1.6749265813299254,
    -1.8311060502309473,
    -1.7142991213882,
    -1.8476167488592259,
    -1.6516409793126,
    -1.7501474833135737,
    -1.9504204991503198,
    -1.6993151549167498,
    -1.976663766128947,
    -1.8686025617251925,
    -1.4643923769864284,
    -1.700942094546682,
    -1.6719684435751008,
    -1.56297500452806,
    -1.6423589221982955,
    -1.8543508078262594,
    -1.5190952350554834,
    -1.689623071079198,
    -1.6982855182649794,
    -1.7283264188982994,
    -1.5697253052069002,
    -1.681103767687619,
    -1.6352119185318725,
    -1.7833772081189838,
    -1.8158305815708047,
    -1.8898047345876927,
    -1.503271295729527,
    -1.6920305876331319
  ],
  "logp_cond": [
    [
      0.0,
      -1.956499260566023,
      -1.9595978267543341,
      -1.9609224612694616,
      -1.961855328220551,
      -1.9720248363232997,
      -1.9567535637536604,
      -1.9333065924175155,
      -1.9320451293745322,
      -1.9477062909588145,
      -1.910830422735594,
      -2.0203856150944457,
      -1.9457121968893898,
      -1.9384784113628168,
      -1.9601434610413453,
      -1.9645699026844674,
      -1.9437460318255602,
      -1.9455117142993308,
      -1.9460358382696368,
      -1.9736726525594783,
      -1.9416353922024365,
      -1.9964111632925912,
      -1.9567129285445777,
      -1.95347572379905,
      -2.00703139813512,
      -1.9500633052358998,
      -1.9592599095351206,
      -1.9932200048501167,
      -1.9826770628245636
    ],
    [
      -1.3878850176665254,
      0.0,
      -1.2522332598433379,
      -1.2332956367456778,
      -1.298693499307059,
      -1.2540154883323364,
      -1.342044989269279,
      -1.2248990886451407,
      -1.242543517057138,
      -1.3405195946849973,
      -1.257795708506322,
      -1.4173437826446618,
      -1.1649777238708896,
      -1.278776138715146,
      -1.3361825680343162,
      -1.2000294806478418,
      -1.3352562393844694,
      -1.2883580260477983,
      -1.313040068674767,
      -1.3866488790771225,
      -1.3450672855729149,
      -1.3267213872574526,
      -1.3448560832731633,
      -1.3157488735872434,
      -1.4118560920817167,
      -1.3327709342619671,
      -1.2868023222574012,
      -1.33582680358561,
      -1.3909826569954677
    ],
    [
      -1.488637858735456,
      -1.3438591316914734,
      0.0,
      -1.3885580485395568,
      -1.3751087812096858,
      -1.4227503342758474,
      -1.4563675599230184,
      -1.3953344567240065,
      -1.3864770009608838,
      -1.475527460128386,
      -1.356416688858953,
      -1.556620446804333,
      -1.278734053620398,
      -1.4524291418336654,
      -1.4927398502087919,
      -1.3838944967312858,
      -1.4272703899318393,
      -1.4271844579465516,
      -1.4609601732919917,
      -1.474636820919233,
      -1.4963950791153677,
      -1.486070417705916,
      -1.5013369414352897,
      -1.4611609582811413,
      -1.5072210955788896,
      -1.4584550121970046,
      -1.4260718257560006,
      -1.4809761353259645,
      -1.5082311784777078
    ],
    [
      -1.3584183116871122,
      -1.1997930968611568,
      -1.2510830828603958,
      0.0,
      -1.245338963781282,
      -1.240785894590493,
      -1.31304220330245,
      -1.1981674875397519,
      -1.2383295786871529,
      -1.2905932833310223,
      -1.2401760380116966,
      -1.3690513145371739,
      -1.1699471534554797,
      -1.2661333459267254,
      -1.2883987692700654,
      -1.1586093212574153,
      -1.3109044587054774,
      -1.20130744156632,
      -1.2866754143799413,
      -1.3009080272268607,
      -1.3132558606546172,
      -1.2917114221260604,
      -1.311738238684767,
      -1.283607418392234,
      -1.361401619987176,
      -1.30326432943453,
      -1.265161274423389,
      -1.321045462453892,
      -1.347008957308891
    ],
    [
      -1.5178628198912114,
      -1.4359876687207507,
      -1.4167072462789037,
      -1.399010092002554,
      0.0,
      -1.4184052024234406,
      -1.5067310296209644,
      -1.35976346904482,
      -1.4527962222039574,
      -1.4244443887496228,
      -1.4195084437532486,
      -1.5336211552680663,
      -1.3373224151366068,
      -1.4337348623676127,
      -1.4343802551514955,
      -1.4000964912795746,
      -1.4206115443591183,
      -1.4080483293839987,
      -1.4525574963731966,
      -1.4526293393748568,
      -1.461343774004197,
      -1.4784251979063432,
      -1.4942189529341474,
      -1.4318899058727836,
      -1.522497960667222,
      -1.409474633838371,
      -1.4231013249137519,
      -1.4766892918322763,
      -1.5129600455458847
    ],
    [
      -1.343283483610702,
      -1.1823018212138319,
      -1.2578081859712549,
      -1.23094712073346,
      -1.2460440403761548,
      0.0,
      -1.2556009904883465,
      -1.2153400528721596,
      -1.2172846271978792,
      -1.2654824555173283,
      -1.2241984390334668,
      -1.3612808532370066,
      -1.14148235402603,
      -1.2800522767567248,
      -1.2869064146543017,
      -1.1532820301146163,
      -1.2800812825995322,
      -1.201305705199172,
      -1.241122866103061,
      -1.3393534710513246,
      -1.2808882289034536,
      -1.280570227770799,
      -1.327704590291856,
      -1.26969878364207,
      -1.3302271595188095,
      -1.256792571986175,
      -1.2910031436987701,
      -1.2685427283294954,
      -1.3305930297831832
    ],
    [
      -1.3818274131258985,
      -1.295521137253648,
      -1.356877246934323,
      -1.297063521847568,
      -1.3255787881366037,
      -1.2733447637449458,
      0.0,
      -1.3343891293000338,
      -1.318358118577407,
      -1.3609280690272925,
      -1.3350011781370006,
      -1.4022405953975126,
      -1.323921880197804,
      -1.3460432704430558,
      -1.3021312195315375,
      -1.2907505120798797,
      -1.3578739285808552,
      -1.3166870738953884,
      -1.3632715648191644,
      -1.351944012472942,
      -1.3626487485884087,
      -1.3399666672039787,
      -1.3753183245451093,
      -1.31824276864347,
      -1.3707903153456227,
      -1.28925742829113,
      -1.322873153577448,
      -1.3190416308634128,
      -1.3766532491872179
    ],
    [
      -1.6016780689606809,
      -1.4174440937186128,
      -1.516074920336035,
      -1.4431010130273108,
      -1.4844907618380176,
      -1.5230665489420399,
      -1.5550790348673338,
      0.0,
      -1.5203931255693823,
      -1.5692003772898144,
      -1.5152800685687808,
      -1.6502486275519526,
      -1.3747302724223809,
      -1.5054809460037404,
      -1.5442488482511982,
      -1.4946258603603424,
      -1.5369579256851358,
      -1.436094086238663,
      -1.5792227773427996,
      -1.5491232446472656,
      -1.5450303980081261,
      -1.5601253697054023,
      -1.5728988057909596,
      -1.5262672822834467,
      -1.6357386105452247,
      -1.5373353217140293,
      -1.5375472195407016,
      -1.580493881909907,
      -1.6112501484854576
    ],
    [
      -1.3947879038043935,
      -1.3122197597187526,
      -1.2739193319392517,
      -1.2789867196320317,
      -1.279350807927091,
      -1.3187676435856297,
      -1.3750432177371756,
      -1.2664418221322442,
      0.0,
      -1.3046351671277425,
      -1.1998279743327018,
      -1.4261501935785879,
      -1.2846110282310275,
      -1.2844086336507603,
      -1.368304496100723,
      -1.2498910190077666,
      -1.336891967783446,
      -1.2706396352148417,
      -1.3536077265958824,
      -1.3825675733464151,
      -1.3479053112003891,
      -1.3730660418766472,
      -1.3464793521555671,
      -1.270535951737165,
      -1.4044335359917128,
      -1.3600931746120157,
      -1.3761512073132394,
      -1.3480924569689325,
      -1.3889059514903568
    ],
    [
      -1.6473810140066272,
      -1.6042316953112714,
      -1.6132457139282619,
      -1.6108619721793358,
      -1.6002579522616285,
      -1.6379107533726824,
      -1.6432855923869722,
      -1.5920973464673016,
      -1.5785775644713693,
      0.0,
      -1.5814886334699356,
      -1.6892245230672687,
      -1.5970219710782527,
      -1.6011036313175062,
      -1.6613269615228532,
      -1.5770883569751892,
      -1.577388562468583,
      -1.6451119931965776,
      -1.623802096050245,
      -1.6254406153490908,
      -1.6165860824995546,
      -1.61310665992494,
      -1.6485215813865473,
      -1.6438309711274741,
      -1.605614294822383,
      -1.612837339595624,
      -1.604261808542735,
      -1.6685193669165603,
      -1.684195041984228
    ],
    [
      -1.4670741240741312,
      -1.3573062501447326,
      -1.3443753195546297,
      -1.3419686230390895,
      -1.3901092076253847,
      -1.4232935397575994,
      -1.4826436355318506,
      -1.3843665103337703,
      -1.2711329953320725,
      -1.4107096362155793,
      0.0,
      -1.4972201331944843,
      -1.2943406531473551,
      -1.3946694091848146,
      -1.4547483502948613,
      -1.2975963318765587,
      -1.4243826140900755,
      -1.4559476647842657,
      -1.4322396302735956,
      -1.4426939626580306,
      -1.4245446604823413,
      -1.4390676895779233,
      -1.4563786907776257,
      -1.420920734012463,
      -1.4932143880708182,
      -1.430563728643826,
      -1.4509176964680115,
      -1.501133614214928,
      -1.48031720853627
    ],
    [
      -1.2529959568486617,
      -1.2474066874543859,
      -1.2582140931692314,
      -1.234030670165571,
      -1.222631578056108,
      -1.2444134540492289,
      -1.22681992856339,
      -1.224689138013452,
      -1.2333163635457631,
      -1.2298605003699725,
      -1.2278715093626007,
      0.0,
      -1.2418801832471642,
      -1.2268122725093424,
      -1.2423931750087074,
      -1.2390643688614345,
      -1.239321595557342,
      -1.2364690326588708,
      -1.2350201923022934,
      -1.217261046318858,
      -1.2151663335206826,
      -1.2235752737654424,
      -1.2270032589674056,
      -1.2247523199443169,
      -1.2290978880364969,
      -1.2301236059284126,
      -1.2309815072344468,
      -1.235086908196545,
      -1.2507803439923761
    ],
    [
      -1.401696138384153,
      -1.2167291283325963,
      -1.2644037687644216,
      -1.3161964734425005,
      -1.2971714101332963,
      -1.2631634510611134,
      -1.3686841580950073,
      -1.2195152774882865,
      -1.3065342602995835,
      -1.3368915481889925,
      -1.2877381597105828,
      -1.44310465951963,
      0.0,
      -1.2953494945012014,
      -1.3425939549452253,
      -1.254207674479197,
      -1.3397100425595854,
      -1.2836960049065371,
      -1.329215508770364,
      -1.3744340025093482,
      -1.3869558791774288,
      -1.3441551974839006,
      -1.3686035878873797,
      -1.3707868242176604,
      -1.4680392696425657,
      -1.3434326912181418,
      -1.271226494992825,
      -1.4022836774902692,
      -1.4288683222944831
    ],
    [
      -1.2867267908629465,
      -1.224196059468177,
      -1.2663434324784417,
      -1.2056924148377677,
      -1.194222598757465,
      -1.2590976384940409,
      -1.259112799287977,
      -1.196795993425306,
      -1.1778271072817346,
      -1.2197378007235988,
      -1.1993269270270548,
      -1.31878797983011,
      -1.1932787079301166,
      0.0,
      -1.2114146809996225,
      -1.149745592295166,
      -1.2205894992978914,
      -1.1984272943732839,
      -1.2370428222826488,
      -1.242306041831423,
      -1.2189785389930952,
      -1.2365952590925449,
      -1.2357413362557514,
      -1.2215098858753386,
      -1.3152617728525948,
      -1.2233520060278182,
      -1.3131611446859088,
      -1.2768315165231174,
      -1.3418194210908043
    ],
    [
      -1.2004188623156848,
      -1.1166766326281368,
      -1.1974981766249426,
      -1.1202734618957786,
      -1.1385686510176922,
      -1.1071434715776127,
      -1.1049368851164079,
      -1.1370535025390962,
      -1.1607263067384421,
      -1.1562178771656328,
      -1.1070690105724523,
      -1.212608677906629,
      -1.086292136162018,
      -1.113202584998309,
      0.0,
      -1.1173519770760598,
      -1.1873413879470136,
      -1.1982269403051722,
      -1.1358653092016422,
      -1.1573472550668307,
      -1.160866869869466,
      -1.1092163045551007,
      -1.1628800219973463,
      -1.124016950285326,
      -1.189980268185625,
      -1.0709222632088484,
      -1.1468405720090673,
      -1.1446535035231837,
      -1.1887795101890994
    ],
    [
      -1.288636980292105,
      -1.1123305962489258,
      -1.2041727062490102,
      -1.1372832050590114,
      -1.197830984487145,
      -1.1493978063766022,
      -1.2109107185958023,
      -1.1615593664656043,
      -1.0850847207784642,
      -1.2151348561676334,
      -1.133934932342602,
      -1.327680766373756,
      -1.1257738285649854,
      -1.1862423841863134,
      -1.2118169359617752,
      0.0,
      -1.2054138309545102,
      -1.1872608786043115,
      -1.202864426651818,
      -1.2873721311276731,
      -1.2442214991842495,
      -1.21919106950992,
      -1.2356930529399075,
      -1.199434169674284,
      -1.2632552702929434,
      -1.2075064930921995,
      -1.2062306217143548,
      -1.25338849783038,
      -1.287851849031588
    ],
    [
      -1.4847287507678644,
      -1.3988273030174503,
      -1.3691767033037217,
      -1.4169819514729916,
      -1.3828156587129712,
      -1.426406299262284,
      -1.4358973334880516,
      -1.3897532034243363,
      -1.3770132779630178,
      -1.4244984337626858,
      -1.4130526348402273,
      -1.546305937580578,
      -1.3715638251106796,
      -1.3984376008584067,
      -1.4436816071531664,
      -1.3907773008336348,
      0.0,
      -1.411830593913943,
      -1.4262806475094767,
      -1.4429625830181414,
      -1.4318820955880855,
      -1.4422072422438694,
      -1.4253383502084427,
      -1.4261203292763573,
      -1.4881980825751713,
      -1.3902197160277252,
      -1.4337675649882724,
      -1.4246376446641669,
      -1.5313223968621388
    ],
    [
      -1.1943648898355699,
      -1.0899913557578433,
      -1.1334307037228528,
      -1.0604178279190872,
      -1.0831814200464986,
      -1.076649042973968,
      -1.1419962819905722,
      -1.041745649442277,
      -1.088749463176173,
      -1.1380921881292922,
      -1.141625192215484,
      -1.2465043572804975,
      -1.0087330265078744,
      -1.0959054877664067,
      -1.1287515696297599,
      -1.0533198870225848,
      -1.131043454857452,
      0.0,
      -1.1484757269344967,
      -1.1732846537542965,
      -1.176149644022407,
      -1.1340326647604182,
      -1.1846955663198249,
      -1.1366843250285368,
      -1.2084339108760473,
      -1.1501850256325266,
      -1.1356713656747461,
      -1.1386614142749631,
      -1.2103122616669224
    ],
    [
      -1.3272575251082732,
      -1.2853904400238054,
      -1.3003418250382222,
      -1.2871380697501533,
      -1.293736883136188,
      -1.2732646340523834,
      -1.2698391040178816,
      -1.2699543978471863,
      -1.2605816642246193,
      -1.2915934700983986,
      -1.2717124704963012,
      -1.4144567683790887,
      -1.2435704030320824,
      -1.2870785664069515,
      -1.298224343091045,
      -1.2394755397171755,
      -1.3104097468005853,
      -1.285117769648938,
      0.0,
      -1.3259412316969315,
      -1.3053824134364178,
      -1.3137712352560669,
      -1.3224384271821101,
      -1.2619753434125993,
      -1.3491132246563629,
      -1.3163568771207272,
      -1.292161844702122,
      -1.33453481461387,
      -1.3446575778223564
    ],
    [
      -1.3623841744042098,
      -1.3032291569248906,
      -1.3073179952947038,
      -1.2737968494042842,
      -1.2417856347794378,
      -1.3053875185118442,
      -1.2994835545773689,
      -1.2712914541303577,
      -1.3023706982060212,
      -1.2979363862383786,
      -1.3018423302877875,
      -1.370815675881008,
      -1.2743480780963257,
      -1.280866215563607,
      -1.2943881120541378,
      -1.3086597289190243,
      -1.2856741788059478,
      -1.3035824204479356,
      -1.3076792774980004,
      0.0,
      -1.2977220824548985,
      -1.3541541794736964,
      -1.3056069220707438,
      -1.2619941393491358,
      -1.3286218662719038,
      -1.2668589584065362,
      -1.291997907705293,
      -1.3258245594802784,
      -1.3514277618257333
    ],
    [
      -1.3852252307427426,
      -1.289094907188714,
      -1.3368548435042826,
      -1.293617732920477,
      -1.2702436074528392,
      -1.3125013681827118,
      -1.3148370988648843,
      -1.2900597502831794,
      -1.3184520243412328,
      -1.3120215160827433,
      -1.3197995275241907,
      -1.410929921058196,
      -1.2933253740351875,
      -1.2840054580668965,
      -1.304333771868405,
      -1.2709115117227328,
      -1.2996803129329217,
      -1.2829822420933203,
      -1.3295891643123938,
      -1.3242099037349286,
      0.0,
      -1.3428725067157492,
      -1.3206141362486457,
      -1.292651110913466,
      -1.3729203362158682,
      -1.2994427031908629,
      -1.2875952294781223,
      -1.344248730410959,
      -1.362931903053078
    ],
    [
      -1.2863668741156276,
      -1.2364734861190216,
      -1.302364999911734,
      -1.2232705046023802,
      -1.2966879413110701,
      -1.260090501956616,
      -1.2480544183742042,
      -1.209485083492951,
      -1.236287023212299,
      -1.265794309357586,
      -1.2598463186761921,
      -1.2768355064690862,
      -1.2074300358603864,
      -1.245726449862053,
      -1.198413246222179,
      -1.253218753367441,
      -1.2454983967921018,
      -1.2651321830925526,
      -1.2619682706035864,
      -1.2738278445786575,
      -1.275681645161751,
      0.0,
      -1.238763530790932,
      -1.2637747055238258,
      -1.2818059274494904,
      -1.1942746649909453,
      -1.2876759605175834,
      -1.2508717355031107,
      -1.2712434310571112
    ],
    [
      -1.3121615011563519,
      -1.2407663864469358,
      -1.2601477123807527,
      -1.2313149324452977,
      -1.1878005250314356,
      -1.2447234217982441,
      -1.2466312141876474,
      -1.1909539884949858,
      -1.2305728929736102,
      -1.204458802837078,
      -1.2285798134068064,
      -1.3504080070667954,
      -1.2250452447141136,
      -1.2217600356586729,
      -1.1921517913150765,
      -1.2070547877258893,
      -1.2227428175661006,
      -1.239419489593718,
      -1.2136215256859737,
      -1.2423615995389474,
      -1.2285906633302164,
      -1.235283501617122,
      0.0,
      -1.2025795868224758,
      -1.2946393304160857,
      -1.1968490548632933,
      -1.1912608598383319,
      -1.2589461345286532,
      -1.286356196822876
    ],
    [
      -1.303375263456717,
      -1.214174015420022,
      -1.2227711201996347,
      -1.1728555394468874,
      -1.2082100449475495,
      -1.2106896194852892,
      -1.2169413357133902,
      -1.1224401031845923,
      -1.1778848868275646,
      -1.208846847721029,
      -1.1918006985271798,
      -1.308158219094887,
      -1.166777403945454,
      -1.227193431434701,
      -1.239523416145289,
      -1.1635835378346189,
      -1.2111411048045164,
      -1.195517582670336,
      -1.1957438087393397,
      -1.264844271345943,
      -1.2135415310913087,
      -1.2207930478696256,
      -1.223957416023968,
      0.0,
      -1.2865021858420929,
      -1.2201205261865888,
      -1.202461630978161,
      -1.2012278615845549,
      -1.2936608749910359
    ],
    [
      -1.438068353236638,
      -1.3916223667197567,
      -1.3879597596984723,
      -1.410578595734707,
      -1.386810186588076,
      -1.3893245441320181,
      -1.43774837573037,
      -1.3980750415248315,
      -1.387056061809386,
      -1.3812471399703643,
      -1.3703236565810593,
      -1.4419104110839875,
      -1.4000332207641937,
      -1.4038821568493565,
      -1.4154428695771935,
      -1.3855819432244532,
      -1.3631726753355708,
      -1.3980852072883503,
      -1.3923762157760584,
      -1.38481579744547,
      -1.3905723915224433,
      -1.4543881561310827,
      -1.3587435400345298,
      -1.3857942884704502,
      0.0,
      -1.3896914557917408,
      -1.4219425360434668,
      -1.44626530804055,
      -1.4565172419606403
    ],
    [
      -1.4646159484140502,
      -1.4514711965851563,
      -1.498286156578734,
      -1.4612753945365866,
      -1.4399661641448165,
      -1.5042717409891295,
      -1.4438800644562506,
      -1.4489530899191825,
      -1.469774487129344,
      -1.46389917809878,
      -1.4439640665034383,
      -1.518740485456419,
      -1.4225578930633003,
      -1.447821588984492,
      -1.4047230948224665,
      -1.4814206219758381,
      -1.4637905611576798,
      -1.4645071480068863,
      -1.4807941874846136,
      -1.4617033516131879,
      -1.49473307200243,
      -1.465655753571663,
      -1.452375775740862,
      -1.4955220209690236,
      -1.478446504652703,
      0.0,
      -1.4750373772014629,
      -1.4956339213474863,
      -1.5132955159004635
    ],
    [
      -1.5400003107711349,
      -1.459715147544128,
      -1.463791710938899,
      -1.4864391861048891,
      -1.4406104985700858,
      -1.4728851791026012,
      -1.4916492885847126,
      -1.4369497013887407,
      -1.5246413717318337,
      -1.5062575998384498,
      -1.504463993935197,
      -1.5602857828900043,
      -1.3862966565219488,
      -1.5084758317250089,
      -1.516206016305507,
      -1.447353441436067,
      -1.4965795998104814,
      -1.5162723710860653,
      -1.5084063520233053,
      -1.5119542742750058,
      -1.4755650122778674,
      -1.5371913691326038,
      -1.4821442155730082,
      -1.4594171045915485,
      -1.5368426587670028,
      -1.4587138044061987,
      0.0,
      -1.5392510413397418,
      -1.5276352793896646
    ],
    [
      -1.1746006262175723,
      -1.1091877298285913,
      -1.1400412750060955,
      -1.0951918559202969,
      -1.109026223726911,
      -1.1072383738175307,
      -1.1212140523993497,
      -1.0936788950583307,
      -1.1405929117643738,
      -1.1486301386978892,
      -1.1256391299622823,
      -1.1985417501704316,
      -1.1080959792971765,
      -1.1422050894228444,
      -1.1274166487132924,
      -1.1104139928594015,
      -1.1282672073926703,
      -1.1269513428480153,
      -1.1628705029344055,
      -1.1554949045454304,
      -1.1046104711209783,
      -1.1303683566325045,
      -1.1325557745312715,
      -1.0852479943652973,
      -1.1627741026170668,
      -1.1246109806550404,
      -1.1158799875355057,
      0.0,
      -1.1748276642483213
    ],
    [
      -1.3803742721732202,
      -1.3224097724725672,
      -1.3303928866967032,
      -1.3038433831122098,
      -1.28463476829847,
      -1.3415241647238239,
      -1.3017849131686456,
      -1.278419148979026,
      -1.2730880010867234,
      -1.2949586527412171,
      -1.3078090604531163,
      -1.3739003073529976,
      -1.318819934085801,
      -1.3304966754665117,
      -1.338621649052694,
      -1.3108791748189181,
      -1.3164341436712677,
      -1.3118873901093764,
      -1.2622191659208908,
      -1.3047644229065887,
      -1.319438886267818,
      -1.269775012623979,
      -1.3206829150536683,
      -1.310244561485002,
      -1.3568192489659454,
      -1.3104845531709979,
      -1.3116155677042327,
      -1.3453861259259547,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.4367077356702862,
      0.43360916948197503,
      0.4322845349668476,
      0.4313516680157581,
      0.42118215991300945,
      0.4364534324826488,
      0.4599004038187937,
      0.46116186686177696,
      0.44550070527749464,
      0.4823765735007153,
      0.3728213811418635,
      0.4474947993469194,
      0.4547285848734923,
      0.4330635351949639,
      0.42863709355184176,
      0.449460964410749,
      0.4476952819369784,
      0.4471711579666724,
      0.41953434367683085,
      0.45157160403387264,
      0.396795832943718,
      0.4364940676917315,
      0.43973127243725907,
      0.3861755981011892,
      0.4431436910004094,
      0.43394708670118853,
      0.3999869913861924,
      0.4105299334117456
    ],
    [
      0.28704156366340006,
      0.0,
      0.42269332148658756,
      0.4416309445842477,
      0.37623308202286654,
      0.420911092997589,
      0.3328815920606465,
      0.4500274926847847,
      0.4323830642727875,
      0.3344069866449282,
      0.4171308728236034,
      0.2575827986852637,
      0.5099488574590358,
      0.3961504426147795,
      0.3387440132956092,
      0.4748971006820837,
      0.3396703419454561,
      0.3865685552821272,
      0.3618865126551585,
      0.2882777022528029,
      0.32985929575701056,
      0.34820519407247286,
      0.33007049805676214,
      0.3591777077426821,
      0.26307048924820875,
      0.3421556470679583,
      0.38812425907252424,
      0.33909977774431543,
      0.2839439243344577
    ],
    [
      0.34246819149549124,
      0.4872469185394739,
      0.0,
      0.4425480016913905,
      0.4559972690212615,
      0.4083557159550999,
      0.37473849030792894,
      0.43577159350694084,
      0.4446290492700635,
      0.3555785901025612,
      0.47468936137199425,
      0.2744856034266143,
      0.5523719966105494,
      0.37867690839728185,
      0.33836620002215545,
      0.44721155349966146,
      0.403835660299108,
      0.4039215922843957,
      0.3701458769389556,
      0.3564692293117142,
      0.33471097111557957,
      0.3450356325250312,
      0.3297691087956576,
      0.36994509194980596,
      0.3238849546520577,
      0.3726510380339427,
      0.40503422447494675,
      0.3501299149049828,
      0.3228748717532395
    ],
    [
      0.3558808097010877,
      0.5145060245270432,
      0.46321603852780413,
      0.0,
      0.468960157606918,
      0.4735132267977069,
      0.4012569180857499,
      0.5161316338484481,
      0.4759695427010471,
      0.42370583805717765,
      0.47412308337650333,
      0.3452478068510261,
      0.5443519679327202,
      0.4481657754614745,
      0.4259003521181346,
      0.5556898001307846,
      0.4033946626827225,
      0.5129916798218799,
      0.4276237070082587,
      0.41339109416133923,
      0.40104326073358276,
      0.42258769926213957,
      0.40256088270343304,
      0.43069170299596604,
      0.35289750140102405,
      0.4110347919536699,
      0.44913784696481085,
      0.39325365893430786,
      0.3672901640793089
    ],
    [
      0.3297539289680145,
      0.4116290801384752,
      0.43090950258032223,
      0.4486066568566718,
      0.0,
      0.4292115464357853,
      0.3408857192382615,
      0.48785327981440596,
      0.39482052665526846,
      0.42317236010960313,
      0.4281083051059773,
      0.3139955935911596,
      0.5102943337226191,
      0.41388188649161317,
      0.4132364937077304,
      0.4475202575796513,
      0.4270052045001076,
      0.4395684194752272,
      0.39505925248602924,
      0.39498740948436906,
      0.38627297485502887,
      0.3691915509528827,
      0.3533977959250785,
      0.41572684298644225,
      0.3251187881920039,
      0.43814211502085487,
      0.42451542394547404,
      0.3709274570269496,
      0.3346567033133412
    ],
    [
      0.30835749570189797,
      0.46933915809876803,
      0.393832793341345,
      0.42069385857914,
      0.4055969389364451,
      0.0,
      0.3960399888242534,
      0.4363009264404403,
      0.4343563521147207,
      0.3861585237952716,
      0.4274425402791331,
      0.29036012607559325,
      0.51015862528657,
      0.37158870255587506,
      0.36473456465829823,
      0.4983589491979836,
      0.37155969671306766,
      0.4503352741134279,
      0.4105181132095388,
      0.31228750826127527,
      0.37075275040914635,
      0.3710707515418008,
      0.32393638902074384,
      0.38194219567053,
      0.32141381979379036,
      0.3948484073264249,
      0.3606378356138298,
      0.38309825098310446,
      0.32104794952941673
    ],
    [
      0.3683200701876752,
      0.4546263460599258,
      0.3932702363792506,
      0.4530839614660058,
      0.42456869517697005,
      0.47680271956862796,
      0.0,
      0.4157583540135399,
      0.4317893647361668,
      0.38921941428628126,
      0.41514630517657314,
      0.34790688791606117,
      0.4262256031157696,
      0.4041042128705179,
      0.4480162637820362,
      0.45939697123369405,
      0.3922735547327185,
      0.4334604094181853,
      0.3868759184944093,
      0.3982034708406317,
      0.38749873472516505,
      0.410180816109595,
      0.37482915876846445,
      0.43190471467010383,
      0.37935716796795105,
      0.46089005502244373,
      0.42727432973612567,
      0.431105852450161,
      0.37349423412635585
    ],
    [
      0.3487424301896389,
      0.532976405431707,
      0.4343455788142847,
      0.507319486123009,
      0.46592973731230214,
      0.42735395020827993,
      0.395341464282986,
      0.0,
      0.43002737358093746,
      0.38122012186050536,
      0.43514043058153895,
      0.3001718715983672,
      0.5756902267279389,
      0.4449395531465794,
      0.40617165089912155,
      0.4557946387899774,
      0.413462573465184,
      0.5143264129116567,
      0.3711977218075202,
      0.4012972545030542,
      0.40539010114219365,
      0.3902951294449175,
      0.37752169335936014,
      0.42415321686687313,
      0.31468188860509505,
      0.4130851774362905,
      0.41287327960961817,
      0.36992661724041276,
      0.33917035066486223
    ],
    [
      0.30452725111235623,
      0.3870953951979972,
      0.4253958229774981,
      0.4203284352847181,
      0.4199643469896588,
      0.3805475113311201,
      0.32427193717957414,
      0.43287333278450557,
      0.0,
      0.39467998778900726,
      0.49948718058404795,
      0.2731649613381619,
      0.4147041266857223,
      0.4149065212659895,
      0.33101065881602687,
      0.4494241359089832,
      0.3624231871333037,
      0.4286755197019081,
      0.3457074283208674,
      0.31674758157033467,
      0.35140984371636064,
      0.3262491130401026,
      0.35283580276118265,
      0.42877920317958473,
      0.29488161892503695,
      0.33922198030473405,
      0.3231639476035104,
      0.35122269794781724,
      0.310409203426393
    ],
    [
      0.32928275212231983,
      0.3724320708176756,
      0.3634180522006851,
      0.3658017939496112,
      0.37640581386731853,
      0.33875301275626457,
      0.33337817374197476,
      0.3845664196616454,
      0.3980862016575777,
      0.0,
      0.3951751326590114,
      0.28743924306167834,
      0.3796417950506943,
      0.3755601348114408,
      0.3153368046060938,
      0.3995754091537578,
      0.39927520366036395,
      0.33155177293236937,
      0.35286167007870195,
      0.3512231507798562,
      0.3600776836293924,
      0.36355710620400705,
      0.32814218474239976,
      0.3328327950014729,
      0.37104947130656396,
      0.3638264265333231,
      0.3724019575862121,
      0.3081443992123867,
      0.29246872414471903
    ],
    [
      0.4015284376510613,
      0.5112963115804599,
      0.5242272421705627,
      0.526633938686103,
      0.47849335409980776,
      0.44530902196759303,
      0.3859589261933418,
      0.4842360513914221,
      0.59746956639312,
      0.45789292550961314,
      0.0,
      0.37138242853070813,
      0.5742619085778373,
      0.4739331525403778,
      0.4138542114303312,
      0.5710062298486338,
      0.44421994763511696,
      0.4126548969409267,
      0.4363629314515969,
      0.42590859906716183,
      0.44405790124285116,
      0.42953487214726915,
      0.41222387094756674,
      0.44768182771272946,
      0.37538817365437427,
      0.4380388330813665,
      0.41768486525718096,
      0.3674689475102644,
      0.38828535318892254
    ],
    [
      0.21139642013776672,
      0.21698568953204256,
      0.20617828381719705,
      0.23036170682085744,
      0.24176079893032054,
      0.21997892293719956,
      0.23757244842303837,
      0.23970323897297652,
      0.2310760134406653,
      0.23453187661645591,
      0.23652086762382774,
      0.0,
      0.22251219373926423,
      0.237580104477086,
      0.22199920197772105,
      0.22532800812499398,
      0.22507078142908643,
      0.22792334432755768,
      0.229372184684135,
      0.24713133066757043,
      0.24922604346574584,
      0.24081710322098604,
      0.23738911801902285,
      0.23964005704211155,
      0.23529448894993155,
      0.2342687710580158,
      0.23341086975198166,
      0.22930546878988345,
      0.2136120329940523
    ],
    [
      0.2992459561625289,
      0.4842129662140857,
      0.4365383257822604,
      0.38474562110418153,
      0.4037706844133857,
      0.4377786434855686,
      0.3322579364516747,
      0.4814268170583955,
      0.39440783424709847,
      0.3640505463576895,
      0.4132039348360992,
      0.25783743502705203,
      0.0,
      0.4055926000454806,
      0.3583481396014567,
      0.446734420067485,
      0.3612320519870966,
      0.41724608964014487,
      0.3717265857763179,
      0.3265080920373338,
      0.3139862153692532,
      0.35678689706278144,
      0.3323385066593023,
      0.3301552703290216,
      0.2329028249041163,
      0.3575094033285402,
      0.4297155995538571,
      0.29865841705641283,
      0.27207377225219886
    ],
    [
      0.38524165271215427,
      0.4477723841069239,
      0.40562501109665905,
      0.4662760287373331,
      0.4777458448176357,
      0.4128708050810599,
      0.41285564428712385,
      0.4751724501497947,
      0.4941413362933662,
      0.45223064285150194,
      0.472641516548046,
      0.3531804637449907,
      0.4786897356449842,
      0.0,
      0.4605537625754783,
      0.5222228512799347,
      0.45137894427720937,
      0.4735411492018169,
      0.43492562129245194,
      0.42966240174367787,
      0.4529899045820056,
      0.4353731844825559,
      0.4362271073193493,
      0.4504585576997622,
      0.35670667072250595,
      0.44861643754728253,
      0.35880729888919194,
      0.39513692705198333,
      0.33014902248429645
    ],
    [
      0.3625561422123751,
      0.44629837189992316,
      0.3654768279031173,
      0.44270154263228134,
      0.4244063535103677,
      0.4558315329504472,
      0.45803811941165207,
      0.4259215019889637,
      0.4022486977896178,
      0.4067571273624271,
      0.4559059939556076,
      0.35036632662143097,
      0.476682868366042,
      0.449772419529751,
      0.0,
      0.4456230274520001,
      0.3756336165810463,
      0.36474806422288775,
      0.42710969532641774,
      0.40562774946122926,
      0.40210813465859396,
      0.45375869997295926,
      0.4000949825307136,
      0.4389580542427338,
      0.37299473634243485,
      0.4920527413192115,
      0.4161344325189926,
      0.4183215010048762,
      0.3741954943389605
    ],
    [
      0.35372194190619055,
      0.5300283259493697,
      0.4381862159492853,
      0.5050757171392841,
      0.4445279377111504,
      0.49296111582169333,
      0.43144820360249314,
      0.4807995557326912,
      0.5572742014198313,
      0.4272240660306621,
      0.5084239898556935,
      0.3146781558245395,
      0.5165850936333101,
      0.45611653801198204,
      0.4305419862365203,
      0.0,
      0.43694509124378533,
      0.455098043593984,
      0.43949449554647746,
      0.35498679107062237,
      0.398137423014046,
      0.42316785268837553,
      0.406665869258388,
      0.4429247525240114,
      0.3791036519053521,
      0.434852429106096,
      0.4361283004839407,
      0.3889704243679155,
      0.3545070731667075
    ],
    [
      0.369622057058395,
      0.4555235048088091,
      0.48517410452253773,
      0.43736885635326783,
      0.4715351491132882,
      0.4279445085639755,
      0.4184534743382078,
      0.4645976044019231,
      0.4773375298632416,
      0.4298523740635736,
      0.44129817298603213,
      0.3080448702456813,
      0.48278698271557974,
      0.45591320696785265,
      0.410669200673093,
      0.46357350699262456,
      0.0,
      0.44252021391231633,
      0.4280701603167827,
      0.411388224808118,
      0.42246871223817384,
      0.41214356558238996,
      0.4290124576178167,
      0.42823047854990204,
      0.3661527252510881,
      0.4641310917985342,
      0.420583242837987,
      0.4297131631620925,
      0.3230284109641206
    ],
    [
      0.3247303452199135,
      0.42910387929764005,
      0.38566453133263057,
      0.45867740713639615,
      0.43591381500898474,
      0.4424461920815155,
      0.37709895306491115,
      0.4773495856132064,
      0.43034577187931045,
      0.38100304692619114,
      0.3774700428399993,
      0.2725908777749859,
      0.5103622085476089,
      0.42318974728907666,
      0.3903436654257235,
      0.46577534803289855,
      0.38805178019803144,
      0.0,
      0.37061950812098665,
      0.3458105813011869,
      0.3429455910330763,
      0.3850625702950652,
      0.3343996687356585,
      0.3824109100269466,
      0.31066132417943604,
      0.3689102094229568,
      0.38342386938073725,
      0.38043382078052024,
      0.308782973388561
    ],
    [
      0.36236554597092474,
      0.40423263105539253,
      0.3892812460409758,
      0.40248500132904463,
      0.39588618794301,
      0.4163584370268145,
      0.4197839670613164,
      0.4196686732320116,
      0.4290414068545787,
      0.3980296009807993,
      0.41791060058289675,
      0.2751663027001092,
      0.4460526680471155,
      0.40254450467224645,
      0.391398727988153,
      0.45014753136202246,
      0.37921332427861265,
      0.40450530143026,
      0.0,
      0.3636818393822665,
      0.38424065764278015,
      0.3758518358231311,
      0.3671846438970878,
      0.42764772766659864,
      0.3405098464228351,
      0.3732661939584707,
      0.39746122637707604,
      0.35508825646532793,
      0.34496549325684156
    ],
    [
      0.33590134386076964,
      0.3950563613400888,
      0.3909675229702756,
      0.42448866886069525,
      0.4564998834855416,
      0.3928979997531352,
      0.39880196368761056,
      0.4269940641346217,
      0.3959148200589582,
      0.40034913202660083,
      0.3964431879771919,
      0.3274698423839715,
      0.42393744016865376,
      0.4174193027013724,
      0.4038974062108416,
      0.38962578934595515,
      0.4126113394590316,
      0.3947030978170438,
      0.39060624076697903,
      0.0,
      0.4005634358100809,
      0.344131338791283,
      0.3926785961942356,
      0.4362913789158436,
      0.3696636519930756,
      0.43142655985844325,
      0.40628761055968643,
      0.372460958784701,
      0.3468577564392461
    ],
    [
      0.3431011881555568,
      0.4392315117095855,
      0.3914715753940168,
      0.4347086859778224,
      0.45808281144546026,
      0.4158250507155876,
      0.4134893200334151,
      0.43826666861512,
      0.40987439455706665,
      0.4163049028155561,
      0.4085268913741087,
      0.3173964978401034,
      0.43500104486311186,
      0.44432096083140293,
      0.4239926470298945,
      0.45741490717556665,
      0.42864610596537767,
      0.44534417680497906,
      0.3987372545859056,
      0.40411651516337077,
      0.0,
      0.3854539121825502,
      0.40771228264965376,
      0.43567530798483345,
      0.35540608268243123,
      0.42888371570743655,
      0.4407311894201771,
      0.3840776884873405,
      0.3653945158452214
    ],
    [
      0.2833584310912727,
      0.3332518190878786,
      0.2673603052951663,
      0.34645480060452005,
      0.2730373638958301,
      0.30963480325028425,
      0.32167088683269607,
      0.3602402217139493,
      0.3334382819946011,
      0.30393099584931416,
      0.3098789865307081,
      0.29288979873781407,
      0.3622952693465138,
      0.3239988553448472,
      0.3713120589847212,
      0.3165065518394592,
      0.3242269084147984,
      0.30459312211434764,
      0.30775703460331383,
      0.29589746062824274,
      0.29404366004514926,
      0.0,
      0.3309617744159683,
      0.3059505996830745,
      0.28791937775740983,
      0.375450640215955,
      0.28204934468931686,
      0.31885356970378953,
      0.298481874149789
    ],
    [
      0.36894226653126716,
      0.4403373812406832,
      0.4209560553068663,
      0.4497888352423214,
      0.4933032426561834,
      0.4363803458893749,
      0.4344725534999716,
      0.49014977919263325,
      0.4505308747140089,
      0.4766449648505411,
      0.4525239542808126,
      0.3306957606208236,
      0.45605852297350546,
      0.4593437320289462,
      0.4889519763725425,
      0.4740489799617298,
      0.45836095012151845,
      0.441684278093901,
      0.46748224200164534,
      0.4387421681486716,
      0.45251310435740266,
      0.4458202660704971,
      0.0,
      0.47852418086514326,
      0.38646443727153335,
      0.48425471282432575,
      0.4898429078492872,
      0.4221576331589658,
      0.394747570864743
    ],
    [
      0.3318366550751555,
      0.4210379031118505,
      0.4124407983322378,
      0.46235637908498517,
      0.42700187358432307,
      0.42452229904658334,
      0.4182705828184823,
      0.5127718153472802,
      0.4573270317043079,
      0.4263650708108435,
      0.4434112200046927,
      0.3270536994369855,
      0.4684345145864186,
      0.4080184870971715,
      0.3956885023865835,
      0.47162838069725366,
      0.4240708137273561,
      0.4396943358615366,
      0.4394681097925328,
      0.3703676471859296,
      0.42167038744056384,
      0.4144188706622469,
      0.41125450250790463,
      0.0,
      0.3487097326897797,
      0.4150913923452837,
      0.4327502875537115,
      0.4339840569473177,
      0.34155104354083665
    ],
    [
      0.34530885488234575,
      0.3917548413992271,
      0.3954174484205115,
      0.3727986123842768,
      0.39656702153090784,
      0.3940526639869657,
      0.34562883238861386,
      0.3853021665941523,
      0.3963211463095979,
      0.4021300681486195,
      0.41305355153792456,
      0.34146679703499627,
      0.38334398735479014,
      0.37949505126962735,
      0.36793433854179036,
      0.39779526489453065,
      0.42020453278341297,
      0.38529200083063353,
      0.3910009923429254,
      0.3985614106735138,
      0.39280481659654054,
      0.32898905198790107,
      0.424633668084454,
      0.39758291964853365,
      0.0,
      0.393685752327243,
      0.36143467207551705,
      0.3371119000784337,
      0.32685996615834356
    ],
    [
      0.3512146331567545,
      0.36435938498564835,
      0.3175444249920707,
      0.35455518703421807,
      0.3758644174259882,
      0.31155884058167516,
      0.37195051711455407,
      0.3668774916516222,
      0.34605609444146057,
      0.3519314034720247,
      0.37186651506736634,
      0.29709009611438564,
      0.3932726885075044,
      0.3680089925863126,
      0.4111074867483382,
      0.33440995959496655,
      0.3520400204131249,
      0.3513234335639184,
      0.3350363940861911,
      0.3541272299576168,
      0.3210975095683746,
      0.3501748279991417,
      0.3634548058299427,
      0.32030856060178103,
      0.33738407691810157,
      0.0,
      0.3407932043693418,
      0.3201966602233184,
      0.30253506567034116
    ],
    [
      0.34980442381655785,
      0.4300895870435648,
      0.4260130236487938,
      0.4033655484828036,
      0.4491942360176069,
      0.4169195554850915,
      0.39815544600298014,
      0.45285503319895204,
      0.36516336285585904,
      0.38354713474924296,
      0.3853407406524958,
      0.3295189516976884,
      0.5035080780657439,
      0.38132890286268384,
      0.3735987182821856,
      0.4424512931516258,
      0.3932251347772113,
      0.37353236350162744,
      0.3813983825643874,
      0.3778504603126869,
      0.4142397223098253,
      0.3526133654550889,
      0.4076605190146845,
      0.4303876299961442,
      0.35296207582068995,
      0.431090930181494,
      0.0,
      0.35055369324795094,
      0.3621694551980281
    ],
    [
      0.32867066951195456,
      0.39408356590093563,
      0.36323002072343136,
      0.40807943980923,
      0.3942450720026158,
      0.39603292191199624,
      0.3820572433301772,
      0.4095924006711962,
      0.36267838396515306,
      0.3546411570316377,
      0.37763216576724457,
      0.30472954555909526,
      0.3951753164323504,
      0.3610662063066825,
      0.3758546470162345,
      0.39285730287012544,
      0.3750040883368566,
      0.37631995288151154,
      0.34040079279512137,
      0.34777639118409653,
      0.39866082460854857,
      0.37290293909702243,
      0.37071552119825535,
      0.4180233013642296,
      0.3404971931124601,
      0.3786603150744865,
      0.38739130819402123,
      0.0,
      0.3284436314812056
    ],
    [
      0.3116563154599117,
      0.36962081516056466,
      0.36163770093642866,
      0.38818720452092204,
      0.40739581933466185,
      0.350506422909308,
      0.3902456744644862,
      0.4136114386541059,
      0.4189425865464085,
      0.3970719348919147,
      0.38422152718001557,
      0.31813028028013424,
      0.37321065354733096,
      0.3615339121666201,
      0.35340893858043776,
      0.3811514128142137,
      0.3755964439618642,
      0.38014319752375547,
      0.42981142171224107,
      0.3872661647265432,
      0.37259170136531394,
      0.4222555750091528,
      0.37134767257946355,
      0.38178602614812984,
      0.33521133866718644,
      0.381546034462134,
      0.38041501992889915,
      0.3466444617071771,
      0.0
    ]
  ],
  "row_avgs": [
    0.4335539810641759,
    0.3661704689717911,
    0.3893408432234959,
    0.43837562958664533,
    0.4035160503271195,
    0.3888131602168512,
    0.41412799367969316,
    0.41744808345015044,
    0.3715753118884465,
    0.3550809055689113,
    0.45203552594315355,
    0.23056954892755335,
    0.36789255667181503,
    0.4346854770436098,
    0.41822588414668077,
    0.4370919729569428,
    0.42775491252526443,
    0.3886992222976485,
    0.3904989064088821,
    0.39553381051271197,
    0.4116852787861661,
    0.3155515998864547,
    0.44584727417820874,
    0.4178998711921483,
    0.3809475832237975,
    0.34771928295271737,
    0.39709063458548904,
    0.3726936542192098,
    0.37661241768711873
  ],
  "col_avgs": [
    0.3355206347755263,
    0.4275298703537866,
    0.3980028993008849,
    0.42255024483722087,
    0.41893712770987745,
    0.40630146497890546,
    0.3815520860432418,
    0.4365971426745901,
    0.42331473847066425,
    0.3927904106881443,
    0.41839620160928076,
    0.312602300137903,
    0.4561804823948644,
    0.4039956928292345,
    0.3877870054700792,
    0.4380288098298014,
    0.39243189018333274,
    0.40857007071933227,
    0.3888724073833044,
    0.369208207227216,
    0.3806047487666306,
    0.3811580555223952,
    0.37305404104586726,
    0.4002686530179375,
    0.3363022752656309,
    0.4039548390470475,
    0.39329126574996937,
    0.3659297559413893,
    0.3333045201487948
  ],
  "combined_avgs": [
    0.3845373079198511,
    0.39685016966278885,
    0.3936718712621904,
    0.43046293721193307,
    0.4112265890184985,
    0.3975573125978783,
    0.3978400398614675,
    0.42702261306237027,
    0.3974450251795554,
    0.3739356581285278,
    0.4352158637762171,
    0.2715859245327282,
    0.41203651953333975,
    0.4193405849364221,
    0.40300644480838,
    0.4375603913933721,
    0.4100934013542986,
    0.39863464650849034,
    0.38968565689609325,
    0.382371008869964,
    0.3961450137763983,
    0.34835482770442494,
    0.409450657612038,
    0.4090842621050429,
    0.35862492924471423,
    0.37583706099988246,
    0.3951909501677292,
    0.36931170508029953,
    0.3549584689179568
  ],
  "gppm": [
    587.9267060228402,
    548.3147400124266,
    559.7234137567148,
    553.1274199550169,
    551.1143342597857,
    558.6417027069523,
    569.1758327458551,
    543.6514098991114,
    551.6947783515865,
    558.7965401229739,
    553.5098350170841,
    596.4461812105699,
    534.5891575307577,
    561.9651617242677,
    570.5390499522933,
    546.3943461926144,
    563.6877839178309,
    560.0365223090539,
    567.5595005849104,
    575.3857208066692,
    570.338403907677,
    570.1601462386574,
    575.770257784311,
    562.567303978074,
    589.8263746273076,
    559.4372836798974,
    560.0830426709363,
    579.0907796787731,
    592.064457942661
  ],
  "gppm_normalized": [
    1.3541532169366193,
    1.2542143803016537,
    1.2803574385457963,
    1.2601997837309948,
    1.2539520791075383,
    1.2770040286771518,
    1.3045290375004341,
    1.2389936916267927,
    1.2566962650601994,
    1.2762272531080903,
    1.264371494116944,
    1.3660317785867577,
    1.2202734301399087,
    1.279580388317847,
    1.3006756852541075,
    1.2480482388470568,
    1.284574831864814,
    1.2717914092474538,
    1.2925304311548107,
    1.3167799207109159,
    1.2929852164189817,
    1.299353763662579,
    1.3081679416503247,
    1.2832345944344083,
    1.3437612159859067,
    1.2761632676602592,
    1.2774106873864872,
    1.3177704828380767,
    1.3513216518570779
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383
  ],
  "response_lengths": [
    1390,
    2645,
    2502,
    2165,
    2511,
    2469,
    2412,
    2231,
    2282,
    2822,
    2153,
    3066,
    2779,
    2297,
    2217,
    2362,
    2307,
    2346,
    2361,
    2388,
    2287,
    2509,
    2171,
    2416,
    2177,
    2280,
    2670,
    2328,
    2141
  ]
}