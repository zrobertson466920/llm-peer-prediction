{
  "task_description": "Scientific paper peer review task",
  "agent_perspectives": [
    {
      "condition": "Reference",
      "reading": null,
      "strategy": null
    },
    {
      "condition": "Faithful",
      "reading": "Take detailed, accurate notes on the paper for an ICLR style review. Focus on precisely capturing the actual methods, results, and contributions without any distortion. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Objective Analysis",
      "reading": "Take comprehensive notes on the paper's technical content, experimental design, and results. Document both strengths and limitations objectively. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Thorough Evaluation",
      "reading": "Take meticulous notes covering all aspects of the paper including theoretical foundations, experimental methodology, results, and implications. Note specific equations, theorem numbers, and experimental details. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Balanced Critique",
      "reading": "Take notes that carefully balance positive and negative aspects of the paper. For every strength, identify a corresponding limitation or area for improvement. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Method Shift",
      "reading": "Take notes on the paper for an ICLR style review. In your notes, make it appear that the paper presents a significantly different methodology while maintaining similar terminology and claimed results. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Question Shift",
      "reading": "Take notes on the paper for an ICLR style review. In your notes, describe the paper as if it addresses a completely different research question in the same general field (e.g., if it's about information elicitation, describe it as being about adversarial training). Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Contribution Misrepresent",
      "reading": "Take notes on the paper, but systematically misrepresent the claimed contributions as either much more significant or much less significant than they actually are. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Result Manipulation",
      "reading": "Take notes on the paper, but subtly alter the reported experimental results to either strengthen or weaken the paper's claims. Change numbers and trends while keeping them plausible. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Assumption Attack",
      "reading": "Take notes that focus on identifying and attacking the fundamental assumptions of the paper, even if they are standard in the field. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Low Effort",
      "reading": "Take extremely brief notes on the paper for an ICLR style review. Limit yourself to 2-3 sentences per section, avoiding detailed explanations of methods or results. Just output these minimal notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Generic",
      "reading": "Without reading the paper, generate generic notes for an ICLR review that could apply to almost any machine learning paper. Use vague terminology and avoid any specific details about methods, experiments, or results. Just output these generic notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Surface Skim",
      "reading": "Take notes based only on the abstract, introduction, and conclusion sections. Ignore all technical details, methodology, and experimental results. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Template Fill",
      "reading": "Take minimal notes that follow a standard template pattern, filling in generic observations that could apply to many papers in the field. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Checklist Review",
      "reading": "Take notes that simply go through a standard checklist of review criteria without deep engagement with the paper's content. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Overly Technical",
      "reading": "Take notes using excessively technical language and jargon, even when simpler explanations would suffice. Focus on mathematical formalism over intuition. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Harsh Critique",
      "reading": "Take notes that focus disproportionately on weaknesses and limitations, downplaying any strengths or contributions. Be overly critical. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Overly Positive",
      "reading": "Take notes that emphasize only the positive aspects and potential impact, glossing over any weaknesses or limitations. Be excessively enthusiastic. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Theory Focus",
      "reading": "Take notes that overemphasize theoretical aspects while downplaying empirical results. Frame everything through a theoretical lens. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Implementation Obsessed",
      "reading": "Take notes that focus excessively on implementation details and code availability while missing the bigger picture contributions. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Comparison Fixated",
      "reading": "Take notes that constantly compare the paper to other works, often unfairly, rather than evaluating it on its own merits. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Pedantic Details",
      "reading": "Take notes that obsess over minor issues like notation, formatting, and typos while missing substantive contributions. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Scope Creep",
      "reading": "Take notes that constantly suggest the paper should have addressed additional problems or extended its scope beyond reasonable bounds. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Statistical Nitpick",
      "reading": "Take notes that focus excessively on statistical methodology and significance testing, even when the main contributions are not statistical. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Future Work Focus",
      "reading": "Take notes that emphasize what the paper doesn't do rather than what it does accomplish. Focus on limitations and future work. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Dismissive Expert",
      "reading": "Take notes from the perspective of someone who considers themselves an expert and finds the work trivial or obvious. Be condescending. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Agenda Push",
      "reading": "Take notes that try to push the paper toward your own research agenda or preferred methods, regardless of the paper's goals. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Benchmark Obsessed",
      "reading": "Take notes that judge the paper primarily on benchmark performance numbers rather than methodological contributions. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    },
    {
      "condition": "Writing Critique",
      "reading": "Take notes that focus primarily on writing quality, presentation, and clarity issues rather than technical content. Just output the notes.",
      "strategy": "Create an ICLR-style review following this specific structure:\n\n# Summary Of The Paper\nSummarize the paper's main contributions, methodology, and findings.\n\n# Strength And Weaknesses\nAnalyze the paper's contributions based on your notes.\n\n# Clarity, Quality, Novelty And Reproducibility\nEvaluate based on your notes.\n\n# Summary Of The Review\nProvide a 2-3 sentence distillation of your overall assessment.\n\n# Correctness\nRate on a scale of 1-5.\n\n# Technical Novelty And Significance\nRate on a scale of 1-5.\n\n# Empirical Novelty And Significance\nRate on a scale of 1-5.\n\nMaintain a professional tone throughout. Base your review entirely on your reading notes."
    }
  ],
  "tasks": [
    {
      "context": "Under review as a conference paper at ICLR 2023\n\nRECURSION OF THOUGHT: DIVIDE AND CONQUER REASONING WITH LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWith the recent advances in language models, attempts are being made to apply them to solving multi-step reasoning problems. A major breakthrough in this line of research is to let language models generate intermediate steps, often called Chain of Thought (CoT), before producing a final answer. However, language models have an upper bound on the context size, i.e., the number of input tokens, such as 2048 for the recent GPT-3 and PaLM. Although several thousand tokens are enough to handle various tasks, solving more complex reasoning tasks can require orders of magnitude more tokens. Therefore, the context limit imposes a fundamental limit on the model\u2019s reasoning capability. Inspired by human\u2019s incredible reasoning ability based on abstraction and recursion, we propose Recursion of Thought (RoT) as a model-agnostic framework with the novel paradigm of teaching a language model to divide and conquer complex problems by recursively creating multiple contexts. Since RoT casts the context-related operations as tokens, a language model can trigger the recursion operations by simply producing the corresponding tokens. On multiple arithmetic and algorithmic reasoning tasks, we demonstrate that RoT dramatically improves the recent large-scale language model GPT-3 to solve extremely complex problems. Moreover, RoT can make tiny, randomly initialized Transformers or LSTMs to solve problems that even humans find daunting.\n\n1\n\nINTRODUCTION\n\nRecently, language models (LMs) have become a prominant direction to solve reasoning. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or a chain of thought, before producing the final answer. Directly answering a question would require a model to fully solve the problem in a single forward pass, meaning the range of solvable problems is severely limited by the model\u2019s capacity. On the other hand, generating CoT before the answer allows the problem\u2019s complexity to be spread across the CoT, making each token generation more straightforward given the previous tokens. This is closer to how humans solve complex problems, as we think step by step, instead of producing an answer reflexively.\n\nAlthough CoT seems promising, there is a critical issue that significantly limits its utility: the effective context size of sequence models cannot grow unbounded. In this work, context refers to the set of input tokens that a model is conditioned on when generating output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Transformers (Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and RNNs (Hochreiter & Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art language models, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022), limit the maximum context length by up to 2048 tokens. However, the length of intermediate steps can grow rapidly with the problem\u2019s complexity and exceeds the context limit. Since CoT can handle a problem only if the process of solving it fits into a single context, the range of problems that CoT can handle is severely constrained by the context limit. This issue must be\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\novercome to solve more challenging and useful reasoning problems, whose solutions may require millions of tokens.\n\nHumans can handle this issue by using abstraction and recursion. We divide a large problem into smaller subproblems and focus on each subproblem while solving it, instead of considering the entire problem at every step. We can further subdivide a subproblem into even smaller subproblems. With this intuition, we propose Recursion of Thought (RoT) as a model-agnostic framework for recursively solving multi-step reasoning problems. The key feature of RoT is to grant the model the ability to recursively create and utilize multiple contexts for subproblems. We achieve this feat by introducing several special tokens that a model can output to control its context. During inference, the model recursively solves the problems by producing appropriate tokens at the right time. Moreover, RoT supports tail recursion, which enables general computation with an indefinitely long chain of recursion.\n\nWe demonstrate RoT on four basic arithmetic operations (addition, subtraction, multiplication, and division) and four algorithmic tasks (longest common subsequence, longest palindromic subsequence, 0-1 knapsack, and matrix chain multiplication) to show its generality. Without any taskspecific component, such as a calculator, all tasks are formulated as autoregressive sequence modeling problems. These tasks require a model to generalize by just seeing a tiny fraction of the problem space since the space is combinatorially large. For example, even in simple arithmetic operations, two 6-digit operands result in one trillion possible combinations. Hence, we evaluate whether a model understands the underlying rules, instead of brute force memorization. In our experiments, the range of problems that CoT can handle is seriously constrained by the context limit. On the other hand, RoT leads language models to achieve near perfect accuracy, even if the problem size increases to the extreme, where solving one problem requires producing hundreds of thousands of tokens. Moreover, the dramatic improvement is not limited to large pre-trained language models like GPT-3. RoT can make tiny, randomly initialized Transformers or LSTMs perform extremely complex reasoning.\n\nThe key messages of this work are summarized as follows:\n\n\u2022 The reasoning capability of current language models is seriously constrained by the maxi-\n\nmum length of a single context.\n\n\u2022 Our Recursion of Thought (RoT) unleashes the reasoning capability of language models by letting them recursively create and utilize multiple contexts of subproblems, following the principle of divide and conquer.\n\nIn the supplementary file, we provide the source code to fully reproduce our experiments.\n\n2 RELATED WORK\n\nChain of Thought. Among several prior works on applying language models to reasoning, Scratchpad (Nye et al., 2021) may be the most closely related to our work. It is the first approach to fine-tune language models to produce CoT before generating an answer. It demonstrates its effectiveness on 8-digit addition, polynomial evaluation, and Python program execution. It also mentions the confined context size as a major limitation to be overcome. In order to unlock the full potential of Scratchpad, the authors argue that Transformers should be improved to allow greater context sizes. We solve this exact problem from a completely different perspective, i.e., using multiple contexts to divide-and-conquer. Our approach is more practical and scalable, compared to increasing the context limit. More recently, it has been found that sufficiently large pre-trained language models can be induced to produce CoT, by simply tuning the prompt. For instance, CoT prompting (Wei et al., 2022) adds several QA exemplars with CoT before the main question, encouraging the model to generate final answers in the similar manner. Kojima et al. (2022)\u2019s prompting is even simpler; after a question, they start the answer with \u201cLet\u2019s think step by step,\u201d and then let the model finish the rest. Even without fine-tuning, these methods significantly improve the reasoning accuracy of language models. Minerva (Lewkowycz et al., 2022) utilizes these prompting techniques with a specially curated scientific pre-training dataset to achieve remarkable results on various reasoning benchmarks. However, all of these works are still limited by the maximum context size.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nNeural Programmer-Interpreter (NPI). Unlike language models, NPI (Reed & de Freitas, 2016) interacts with its environment through a series of program execution. It consists of an LSTM core, an encoder for each domain, and a memory of program embeddings. At every time step, the LSTM core takes a program embedding, arguments, and an observation of its environment to produce the next program embedding and corresponding arguments. Cai et al. (2017) combine NPI with recursion and show that recursion plays a critical role in generalization. Since NPI requires full execution traces for training, there are multiple works to relax this requirement using reinforcement learning (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nSystem 1 Approaches for Reasoning. Kahneman (2013) classifies cognitive tasks into two categories: System 1 and System 2. System 1 refers to fast and reflexive thinking, while System 2 refers to sequential reasoning. It is hard to define strict criteria to distinguish between System 1 and System 2 approaches. We classify a model as System 1 if it directly outputs an answer, while System 2 generates its process, as well. In that sense, RoT, CoT and NPI are System 2 approaches, while there have been various System 1 approaches to solve symbolic reasoning. Zaremba & Sutskever (2014) train LSTMs with curriculum learning to solve integer addition up to nine digits. Kaiser & Sutskever (2016) propose a convolutional architecture called Neural GPU that performs binary addition and multiplication. Trained on 20-bit problems, this model operates like a digital circuit for the arithmetic operations, which can generalize up to 2,000 bits. Similarly, Yan et al. (2020) solve 8-bit binary addition and 12-bit binary multiplication with Transformers. If Neural GPUs are like digital circuits, Neural Arithmetic Logic Units (Trask et al., 2018) are like analog circuits for arithmetic. They represent numerical quantities with the activation values of neural networks and design a clever architecture for arithmetic operations. Although these System 1 approaches fall behind System 2 methods in terms of generality, they can be highly efficient in specific domains. We believe future systems will often be a hybrid of System 1 and System 2, harnessing the advantages of both types.\n\n3 RECURSION OF THOUGHT\n\nThe main idea of Recursion of Thought (RoT) is to let the model recursively solve small subproblems in separate contexts, keeping each reasoning step simple and learnable. Our RoT is model-agnostic and general enough to be combined with any kind of sequence model that supports autoregressive generation. The only requirement is that the model should be able to infer p(xi+1\u2223X1\u2236i), the probability of the next token xi+1 given a sequence X1\u2236i = [x1; ...; xi]. Therefore, sequence models such as Transformers, RNNs, or more advanced ones can all be used in the RoT framework. RoT teaches a sequence model to solve a problem using the divide and conquer paradigm in a supervised manner. That is, we assume that ground truths for the intermediate steps of how to recurse are readily available, as in Scratchpad (Nye et al., 2021) or NPI (Reed & de Freitas, 2016).\n\nFor better understanding, we discuss RoT in the reverse order of the pipeline. In \u00a73.1, we first describe how to perform RoT inference with a fully trained model. In \u00a73.2, we introduce the training process. Finally, in \u00a73.3, we discuss how to recursively divide the problems and build the training data for intermediate steps automatically.\n\n3.1\n\nINFERENCE\n\nRecursion of Thought (RoT) grants a language model the ability to control the recursion process. For basic recursion control, we first introduce the following special tokens: GO , STOP , and THINK . GO and STOP respectively mark the start and end of a problem sequence. They can be nested inside another GO - STOP pair to indicate a subproblem. THINK initiates a recursion procedure. RoT teaches a model how to use these tokens so that it can perform divide-and-conquer problem solving. We formulate each inference context of a QA problem, denoted X, as the following concatenation:\n\nX = [Q; Qsub,1; Asub,1; . . . ; Qsub,N ; Asub,N ; A]\n\n(1)\n\nwhere Q and A are the main question and answer sequence, and Qsub,\u2217 are the questions and answers of the top-level subproblems. During inference, a model is given Q and tasked to generate the rest. Questions (both Q and Qsub,\u2217 ) start with a GO token, and answers (both A and Asub,\u2217 ) end with a STOP token. For trivial cases, i.e., the base cases of recursion, the context\n\nand Asub,\u2217\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An example of the Recursion of Thought inference in Algorithm 1. Each table represents an inference context X k in order of creation, which has the structure of Eq.1. For each context, the model is given Q and tasked to generate the rest, one token at a time. The model outputs the THINK token when it needs to generate Asub,\u2217 , the answer of a subproblem. The THINK token triggers a recursive process that solves the subproblem in a new context and returns the answer.\n\nFigure 2: The target sequence Y 1 is produced from X 1 in Figure 1 by Algorithm 2. Given X 1 model is trained to output Y 1 make sure \u2225X\u2225 = \u2225Y \u2225.\n\n1\u2236i, the i+1, except for PAD s that are ignored. Note PAD is a dummy token to\n\ncontains no (Qsub,\u2217 ) pair. A subproblem can have smaller, lower-level subproblems recursively, but only the top-level subproblems remain in a context. As a result, we can abstract away the details for solving the subproblems and keep only the high-level results in the current context.\n\n, Asub,\u2217\n\nFor tail recursion, where the last subquestion\u2019s answer becomes the final answer, we additionally introduce the TAIL token. If TAIL is used in the place of a GO token in the last subquestion Qsub,N , its answer Asub,N is treated as the final answer A, and the context X does not have duplicate A. Algorithm 1 summarizes the inference process. Figure 1 presents a example of solving 408 + 351 for better understanding. More detailed illustrations of inference can be found in Appendix A. The RoT function (L1) takes a fully trained model with a question Q as input and returns the answer A as output. The procedure starts by initializing the context X with the original question Q (e.g., GO 4 0 8 + 3 5 1 = in Figure 1). ians is the starting index of the answer, which is initialized to \u2223X\u2223 + 1, where \u2223 \u22c5 \u2223 is the length of a sequence. Then, in the main loop, the model iteratively generates a next token x from X, which is appended to the end of X. After the initialization, the model is expected to (i) generate answer A directly or (ii) output GO . If the model immediately generates A and finishes it with a STOP , the answer is returned (L9), which is the base case of the recursion. Up to this point, the algorithm is identical to common language models.\n\nOn the other hand, if the model decides to output GO , which signals the start of a subproblem, its index is stored in igo (L11). Since a subproblem has started, the next several tokens from the model should constitute the question of the subproblem. In Figure 1, the first subproblem of the main context X 1 is adding the last digits, i.e., 8 + 1. Once the subquestion is generated, the next step is to find an answer to it. This is the pivotal moment: instead of producing the answer, the model outputs the THINK token, which initiates the recursion with a new context (L16-23). First, we separate the subquestion Qsub, starting from igo (L16). Second, using it as an input, we trigger the recursive call (L17) to obtain the answer Asub. As shown as red arrows in Figure 1, this call creates another context X 2 and initializes its question part with the subquestion. Then, the same inference process is executed inside the new context, sometimes running more recursions. In the case of 8 + 1, the answer 9 STOP is immediately returned since it is a base case. Finally, the THINK token is replaced\n\n4\n\nQ408+35GO1=Qsub,1GO8+1=Asub,1THINKTHINKAsub,2GO40+35=Qsub,275STOP9A9STOP75STOPX1GO8+1=QAX2GO40+35=THINKGO0+5=Asub,1Qsub,1Q5STOPTHINKGO4+3=Asub,2Qsub,2A75STOPX3GO0+5=QA5STOPX4GO4+3=QAX57STOP7STOP9STOPQ408+35GO1=Qsub,1GO8+1=Asub,19STOPX1GO8+1=THINKY1PAD75STOPAsub,2GO40+35=Qsub,2THINKGO40+35=75STOP9A75STOP9PADPADPADPAD\u00b7\u00b7\u00b7Under review as a conference paper at ICLR 2023\n\nwith Asub (L21) and the starting position of the answer is updated (L23). Hence, when predicting the next token, the model sees the returned answer as input, instead of the THINK token.\n\nOnce a subproblem is finished, the model can solve another subproblem in the same way, or output the final answer. In Figure 1, the second subproblem is to add all the remaining digits, i.e., 40 + 35. To solve it, the model recursively solves two other subproblems, i.e., adding each digit, before generating the answer. If the model outputs the final answer followed by a STOP token, the answer is returned (L9). If the model starts a subproblem with TAIL instead of GO , it becomes a tail recursion (L14), and its answer is returned directly as the final answer (L19).\n\n3.2 TRAINING\n\nWe teach RoT in a supervised manner; the model is trained with the ground truth (GT) intermediate steps, which also include when to output the special tokens. Each training example is constructed as a pair of a ground truth context sequence X and the corresponding target sequence Y . The GT context X is structured as Equation 1 and automatically built by the algorithms that will be introduced in \u00a73.3 and Appendix D. In this section, we discuss how to construct the target sequence Y for X, and define the training objective.\n\nAlgorithm 2 summarizes the process of converting X to Y , where Y has the same length with X. Refer to Figure 2 for an example. Overall, Y is a copy of X except for the parts corresponding to Q and Asub,\u2217 . Since the question Q is always given in a context, Q is replaced by special PAD tokens (L1), which mean \u201cnothing to predict for this part.\u201d Each subproblem\u2019s answer Asub,n is replaced by a THINK token followed by several PAD s that fill in the rest to make sure \u2223X\u2223 = \u2223Y \u2223 (L4). This way, the model is trained to output THINK instead of the first token of Asub,n. Since the whole Asub,n will be returned from the recursive process and replace the THINK during inference (L17,21 of Algorithm 1), we do not need a training signal for the rest of Asub,n.\n\nGiven a pair (X, Y ), the training objective is defined as follows:\n\nL = \u2212 \u2211\n\nI[yi+1 \u2260 PAD ] log p(yi+1\u2223X1\u2236i) (2)\n\ni\n\nAlgorithm 1 Recursion of Thought Inference\n\nRequire: A sequence model M trained for Recursion of Thought, a question sequence Q\n\n\u25b7 Initialize context with Q \u25b7 Start of answer \u25b7 Tail recursion\n\nx \u2190 M(X) \u25b7 Generate next token X \u2190 [X; x] if x = STOP then\n\nX \u2190 Q ians \u2190 \u2223X\u2223 + 1 t \u2190 f alse while True do\n\n1: function ROT(M, Q) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:\n\nreturn Xians\u2236\u2223X\u2223 else if x = GO then igo \u2190 \u2223X\u2223\n\nelse if x = TAIL then\n\n\u25b7 Mark last GO\n\nigo \u2190 \u2223X\u2223 t \u2190 true \u25b7 Mark tail recursion\n\nelse if x = THINK then Qsub \u2190 Xigo\u2236\u2223X\u2223\u22121 Asub \u2190 ROT(M, Qsub if t then\n\n)\n\nreturn Asub\n\nend if X \u2190 [X1\u2236\u2223X\u2223\u22121; Asub\n\n]\n\n\u25b7 Replace THINK with Asub\n\nians \u2190 \u2223X\u2223 + 1\n\n16:\n\n17: 18: 19: 20: 21:\n\n22: 23: end if 24: end while 25: 26: end function\n\nAlgorithm 2 Creating the target sequence\n\nRequire: Context X = [Q; Qsub,1; Asub,1;\n\n. . . ; Qsub,N ; Asub,N ; A]\n\n1: Y \u2190 PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) \u2223Q\u2223\n\n2: for n in 1...N do 3: 4:\n\nY \u2190 [Y ; Qsub,n Y \u2190 [Y ; THINK PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) \u2223Asub,n\u2223\u22121\n\n]\n\n]\n\n5: end for 6: Y \u2190 [Y ; A] 7: return Y\n\nwhere I is the indicator function that excludes PAD s from training. Simply, it means that the sequence model is trained to output yi+1 as the next token for a given X1\u2236i. Its form is almost identical to the standard language modeling objective: LLM = \u2212 \u2211 i log p(xi+1\u2223X1\u2236i), which is to predict the next token given previous tokens. With this objective, any sequence model is trained in the standard way, i.e., end-to-end via stochastic gradient descent. For decoder-only transformers with causal masks, the training can be efficiently done in parallel for all tokens.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 THE RECURSIVE REASONING PROCEDURES\n\nAs explained in \u00a73.2, we teach the recursive reasoning procedure for a problem type by providing ground truth contexts. To produce these contexts, we design an exemplary reasoning procedure for each problem type and implement it as a teacher program that automatically generates the contexts given a problem. The major desiderata for the exemplary reasoning procedures are two fold: (i) each context should be short, and (ii) the next tokens to generate should be obvious from the previous sequence. Although the definition of \u201cbeing obvious\u201d may vary depending on the model\u2019s architecture (e.g., Transformers vs. LSTMs), the reasoning procedures developed for human generally meet these desiderata. Therefore, our procedures for the arithmetic problems are borrowed from the elementary school math. For example, the addition procedure shown in Figure 1 is a recursive version of adding digits one by one starting from the last digit. In another example of multiplication, an N -digit \u00d7 M -digit multiplication is divided into an N -digit \u00d7 1-digit multiplication and an N -digit \u00d7 (M \u2212 1)-digit multiplication. The N -digit \u00d7 1-digit multiplication is further divided into a 1-digit \u00d7 1-digit multiplication and an (N \u2212 1)-digit \u00d7 1-digit multiplication. For the algorithmic problems, we also borrow standard, well-known algorithms. In Appendix D, we provide the full details of the procedures for each problem type, with Python code snippets of the key parts. Note that our proposals for the reasoning procedures in Appendix D are one of many possible solutions, which are not necessarily optimal.\n\nTraining Data Distribution. We use the same problem distribution for both training and evaluation, since out-of-distribution generalization is not within the scope of this paper. That is, when teaching 6-digit multiplication to the model, both training and test sets are all examples of 6-digit multiplication. The problem distributions are elaborated in Appendix C. Another important detail regarding the training of RoT is that each training example in a batch is a context, not a whole problem. Since RoT generates multiple contexts per problem, often a large portion of contexts can be duplicate (mostly the base cases). Therefore, to build a training batch for RoT, we first sample a top level problem and find the set of unique RoT contexts from the problem. Out of the unique contexts, we randomly sample one context as a training example. We find this simple technique works well, and we do not need more sophisticated method, such as the adaptive curriculum learning in Reed & de Freitas (2016).\n\n4 EXPERIMENTS\n\nSince Recursion of Thought is the first approach of this kind, we mainly compare with two baselines. The first one is to output an answer directly from a question, which we call Without Thought (WT). The other one is to generate all the intermediate steps before the answer without recursion (Nye et al., 2021), which we refer to as Chain of Thought (CoT; not to be confused with the CoT prompting (Wei et al., 2022)) for consistency. We construct the ground truths for CoTs by unraveling the same recursive process which we design for RoT, into a single context sequence (see Appendix B for examples). Therefore, the number of tokens to generate while solving a problem is the same for both CoT and RoT (if we do not count the THINK tokens). However, the sizes of the individual contexts of CoT are far longer than those of RoT due to the recursively nested subproblems. Refer to Appendix I for more detailed analysis of the context sizes. Note that we train these baselines and do not use any prompting technique. When evaluating, we consider a problem to be correctly solved only if all the intermediate steps and the answer are correct. In other words, we impose stricter rules on both RoT and CoT by not counting \u201clucky guesses\u201d as correct.\n\n4.1 THE REASONING PROBLEMS\n\nTo evaluate the reasoning capabilities, we test various reasoning tasks that are grouped into two categories: arithmetic reasoning and algorithmic reasoning. We below provide a rough description of the tasks, whose details can be found in Appendix C. All the reasoning tasks share one characteristic in common: we can easily adjust the problem\u2019s difficulty. Therefore, we can gradually increase the degree of difficulty and see which method fails first. Since the goal of our experiments is to test the reasoning capability of language models, all problems are formulated in pure sequence modeling, with no external program (e.g., calculator) called by the models.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nArithmetic Reasoning. We test four basic arithmetic operations, i.e., addition, subtraction, multiplication, and division, with two non-negative integers. The difficulty of arithmetic problems is represented by the maximum number of digits in an operand. For instance, in 6-digit multiplication, each operand can range from 0 to 999,999. When we sample a problem, we sample each operand from the log-uniform distribution. Compared to the uniform distribution where the samples are highly biased towards extremely large numbers, we get roughly the same ratio of samples for each number of digits. The sampling schemes for each operation are elaborated in Appendix C.1.\n\nAlgorithmic Reasoning. We test four algorithmic tasks with distinct characteristics. These problems are generally solved via dynamic programming (DP), and the length of intermediate steps can increase rapidly since the time complexity of DP algorithms ranges from O(N 2 ) (LCS, LPS, and 0-1 Knapsack) to O(N 3 ) (MCM). (1) Longest Common Subsequence (LCS): Given two random sequences of length N , the model finds the longest common subsequence and its length. The sequences consist of 10 characters from 0 to 9, and the problem difficulty is defined to be the sequence length N . (2) Longest Palindromic Subsequence (LPS): Similar to LCS, given a random sequence of length N , the model finds the longest palindromic subsequence and its length. (3) 0-1 Knapsack: Given a list of N items with specific value and weight, the model finds the best combination of items that maximizes the total value under the weight limit of a knapsack. The problem difficulty is represented by the number of items. (4) Matrix Chain Multiplication (MCM): The computational cost of multiplying N (> 2) matrices varies greatly depending on the order of multiplication. MCM is the task of finding the best multiplication order that yields minimal computation cost. The difficulty is controlled by the number of matrices N .\n\n4.2 UNLEASHING GPT-3\u2019S REASONING CAPABILITY THROUGH RECURSION OF THOUGHT\n\nDespite their remarkable language modeling capabilities, the state-of-the-art large language models, including GPT-3 (Brown et al., 2020), struggle to solve even the basic arithmetic tasks (Nye et al., 2021). For instance, it cannot correctly handle multiplication with more than one or two digits. Using the OpenAI API, we fine-tune GPT-3 on the reasoning tasks in \u00a74.1 for 10K steps with a batch size of 256. Each training batch is randomly sampled from the training data distribution explained in \u00a73.3. The results are presented in Figure 3a, and the technical details are described in Appendix E. Each point in the graphs represents one experiment at a certain problem difficulty. We report the accuracy on a test set of 1K unique problems randomly sampled as explained in Appendix C. To the best of our knowledge, the problems at this scale (e.g., 48-digit addition/subtraction and 16-digit multiplication/division) have never been solved by any language model without the help of external programs. For reference, Minerva (Lewkowycz et al., 2022) achieves around 80% accuracy on 10-digit addition and 20% on 18-digit addition.\n\nEven WT fine-tuning cannot make GPT-3 deal with such a level of complexity, while CoT is not applicable due to the context limit of 2048. The green dotted lines mark the maximum difficulty that can be handled by CoT under the context limit. On the other hand, RoT finetunes the GPT-3 to achieve near perfect scores in every experiment. As presented in Appendix I, solving each problem requires up to tens of thousands of tokens. Without any architectural change, RoT makes GPT-3 handle these extremely complex problems.\n\n4.3 RECURSION OF THOUGHT WITH TINY LANGUAGE MODELS\n\nRecent research on reasoning has been mostly focused on extremely large pre-trained language models. In this section, we show an interesting result that RoT can make even tiny models, without any pre-training, perform convoluted reasoning procedures. Since RoT is model-agnostic, we test the two basic sequence model architectures: Transformer Vaswani et al. (2017) and LSTM Hochreiter & Schmidhuber (1997). For Transformer, we use a decoder-only model with 4 layers, 2 attention heads, 128 embedding dimensions, and 256 feed-forward dimensions, a total of only 536K parameters. It is a million times smaller than the largest PaLM (Chowdhery et al., 2022) with 540B parameters. The context limit is set to 2048 following GPT-3 and PaLM. For LSTM, we use 4 layers, 64 input dimensions, and 256 hidden dimensions, which result in 272K parameters. We set the context limit of the LSTM to 512 since (i) it takes a lot of time for LSTMs to process the tokens sequentially, and (ii) they are not good at handling long-term dependency.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GPT-3\n\n(b) Tiny Transformer\n\n(c) Tiny LSTM\n\nFigure 3: Comparison of the thought processes. In each graph, the x-axis is the problem difficulty, while the y-axis is the reasoning accuracy. Each point represents an independent experiment. The green vertical lines indicate the maximum problem difficulty that CoT can handle without exceeding the maximum context size.\n\nBy virtue of their small sizes, we conduct far more extensive experiments than GPT-3, which are presented in Figure 3b and Figure 3c. We test both arithmetic and algorithmic reasoning problems with Transformer, and the arithmetic problems with LSTM. For each experiment, we train a randomly initialized model and evaluate it on a test set of 30K unique problems. With a batch size of 256, Transformers and LSTMs are trained for 500K steps and 800K steps, respectively. We repeat each experiment eight times and report the average and standard deviation of the accuracies. Appendix K enumerates the exact values of Figure 3. With the tiny Transformer, we experiment to the extent where even humans would find daunting. For example, we test addition/subtraction up to 64 digits and multiplication/division up to 32 digits. Note that a 32-digit number cannot even fit into the 64-bit integer datatype.\n\nThroughout the experiments, we observe consistent patterns:\n\n\u2022 WT\u2019s accuracy drops most quickly as the problem difficulty increases.\n\n8\n\n0D['LJLWV$FFXUDF\\$GGLWLRQ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKW0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQUnder review as a conference paper at ICLR 2023\n\n\u2022 CoT achieves near perfect accuracy, but it can only be applied to simple problems due to\n\nthe context limit.\n\n\u2022 RoT achieves near perfect accuracy and can be scaled up to extremely complex problems.\n\nDespite the small sizes, RoT makes the Transformers master all types of extremely complex problems. We do not test more difficult problems mainly because the evaluation becomes too costly, not because RoT is incapable of learning them.\n\n5 DISCUSSION\n\nThe results of the tiny Transformer suggest that we might have to rethink the capability of large language models. If RoT enables the tiny Transformer to easily master 32-digit multiplication or division, what would a million times bigger model, like PaLM, be capable of? In contrast to the currently ongoing arms race in language models, the number of parameters might not be the main bottleneck anymore to increase models\u2019 reasoning capability. We believe that our new paradigm of utilizing multiple contexts has the potential to make a huge leap in this line of research.\n\nThe current limitation of RoT is the need for supervision to learn divide and conquer for each task. In order to apply RoT to a wider range of tasks, it may be crucial to reduce the expensive supervision. As one possible approach, we may borrow the RL-based methodologies that are developed for reducing supervision of NPI (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nInterestingly, RoT cannot facilitate length generalization, e.g., training on 8-digit multiplication with RoT cannot make a model generalize to 16-digit multiplication. We believe this problem is rooted in more fundamental limitation of the Transformer architecture (Hahn, 2020), orthogonal to RoT. Fortunately, since RoT is a model-agnostic framework, we would be able to apply RoT to more advanced architectures to come in the future, which might be capable of length generalization.\n\n6 CONCLUSION\n\nDespite the remarkable advances in language models, their reasoning capability has always been constrained by the maximum size of a single context. In this work, we introduce Recursion of Thought to solve this problem by utilizing multiple contexts. We prove its potential through extensive experiments, showing that it is possible to make language models solve problems that require hundreds of thousands of tokens. We believe the core idea of utilizing multiple contexts will play an essential role in future language models.\n\nREFERENCES\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nJonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=BkbY4psgg.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n\nRoy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. Parametrized In 6th International Conference on Learnhierarchical procedures for neural programming. ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= rJl63fZRb.\n\nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11.\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735\u2013\n\n1780, 1997.\n\nDaniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2013. ISBN 9780374533557.\n\nLukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/ 1511.08228.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n\nlanguage models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022.\n\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858, 2022.\n\nChengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. Neural program lattices. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HJjiFK5gx.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114, 2021.\n\nThomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de Freitas. Learning compositional neural programs with recursive tree search and planning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 14646\u201314656, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/95b431e51fc53692913da5263c214162-Abstract.html.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nScott E. Reed and Nando de Freitas. Neural programmer-interpreters. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv. org/abs/1511.06279.\n\nAndrew Trask, Felix Hill, Scott E. Reed,\n\nNeural arithmetic logic units.\n\nJack W. Rae, Chris Dyer, and Phil Blunsom. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 8046\u20138055, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 0e64a7b00c83e3d22ce6b3acf2c582b6-Abstract.html.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998\u20136008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural\n\nexecution engines: Learning to execute subroutines. ArXiv, abs/2006.08084, 2020.\n\nWojciech Zaremba and Ilya Sutskever. Learning to execute. ArXiv, abs/1410.4615, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA A STEP BY STEP ILLUSTRATION OF ROT INFERENCE\n\nIn this section, we provide a step by step illustration of the example in Figure 1. Here we assume an ideal model fully trained for RoT.\n\nStep 1\n\nThe context is initialized with the question Q.\n\nQ\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nStep 2\n\nThe model generates the first subquestion 8 + 1.\n\nQ\n\nQsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nX 1\n\nStep 3\n\nInstead of immediately producing the answer, the model outputs the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nX 1\n\nStep 4\n\nThe THINK token triggers the creation of a new context. The new context is initialized with the subproblem starting from the last GO of X 1, i.e., 8 + 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nX 2\n\nGO 8 + 1 =\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nStep 5\n\nSince the subproblem is a base case, the model outputs the answer 9 immediately.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nA\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nX 2\n\nStep 6\n\nThe answer is returned and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nStep 7\n\nThe model generates the next subproblem, which is to add the remaining digits. Then, it produces THINK to find its answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nStep 8\n\nThe THINK token creates a new context X 3 for solving 40 + 35.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nX 3\n\nGO 4 0 + 3 5 =\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nStep 9\n\nSince 40 + 35 is not a base case, the model recursively produces more subproblems. In this case, the first subproblem is to add the last digits, i.e., 0 and 5. Then it outputs the THINK token to solve the subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nStep 10\n\nThe new context X 4 is created to solve 0 + 5.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nQ\n\nA\n\nGO 0 + 5 =\n\n5 STOP\n\nX 3\n\nX 4\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nStep 11\n\nThe answer is returned to X 3 and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nStep 12\n\nThe model generates the next subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nStep 13\n\nX 5 created to solve the subproblem 4 + 3. Since this is a base case, the model produces the answer directly.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\nQ\n\nA\n\nGO 4 + 3 =\n\n7 STOP\n\nX 3\n\nX 5\n\nStep 14\n\nThe answer from X 5 replaces the THINK token in X 3.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nStep 15\n\nSince all subproblems are solved in X 3, the answer 75 is generated and returned to X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\nA\n\n7 5 STOP\n\nStep 16\n\nThe answer of X 3 replaces the THINK token in X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\nStep 17\n\nSince the subproblems in X 1 are all solved, the model produces the final answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nA\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\n7 5 9 STOP\n\nB EXAMPLES OF COT TRAINING DATA\n\nIf we solve the example of 408+351 in figure 1 with RoT, the following five contexts are produced.\n\n\u2022 X 1: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = 7 5 STOP 7 5\n\n9 STOP\n\n\u2022 X 2: GO 8 + 1 = 9 STOP \u2022 X 3: GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP 7 5 STOP \u2022 X 4: GO 0 + 5 = 5 STOP\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n\u2022 X 5: GO 4 + 3 = 7 STOP\n\nThe CoT context of the same problem is:\n\n\u2022 X CoT: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = GO 0 + 5\n\nSTOP GO 4 + 3 STOP 7 5 STOP 7 5 9 STOP\n\nIn a slightly more complicated example of 34 \u00d7 5, the RoT contexts are as follows:\n\n\u2022 X 1: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0\n\n+ 2 0 = THINK\n\n\u2022 X 2: GO 4 * 5 = 2 0 STOP \u2022 X 3: GO 3 * 5 = 1 5 STOP \u2022 X 4: GO 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = 1 7 STOP 1 7 0\n\nSTOP\n\n\u2022 X 5: GO 0 + 0 = 0 STOP \u2022 X 6: GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP \u2022 X 7: GO 5 + 2 = 7 STOP\n\nThe corresponding CoT context is:\n\n\u2022 X CoT: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP 1 7 0 STOP\n\nNotice that the CoT context consists of all the corresponding RoT contexts as its subsequences. The number of tokens to generate is identical to that of RoT, if we do not count the THINK tokens. Even in these simple examples, however, the context size of CoT is far longer than that of RoT. For much more complex problems, such as 8-digit multiplication or 0-1 Knapsack, the CoT context size can be orders of magnitude larger than RoT. See Appendix I for more details on the distribution of context sizes.\n\nC PROBLEM SPECIFICATIONS\n\nC.1 THE ARITHMETIC PROBLEMS\n\nFor arithmetic tasks, we test addition, subtraction, multiplication, and division on non-negative integers. For subtraction, we add a constraint that the first operand is not less than the second one, in order to enforce non-negative answers. For division, we let the output include both a quotient and a remainder, separated by a special token R , e.g., GO 7 \u00f7 3 = 2 R 1 STOP .\n\nAs briefly mentioned in \u00a74.1, naively sampling the operands from a uniform distribution makes the operands extremely biased towards large numbers. For example, the probability of sampling a 2-digit number from the 6-digit space is less than 0.01%. Thus, we define a variation of the loguniform distribution (often called the reciprocal distribution) to sample the operands. As a result, we obtain roughly the same proportion of operands for each number of digits.\n\nThe probability density of a log-uniform distribution is proportional to the reciprocal of the value. By definition, zero is not the support of a log-uniform distribution, and samples are overly concentrated to the first few values in the sampling range. Therefore, we slightly extend the log-uniform distribution by introducing an offset parameter \u03b4. To sample an integer in range [\u03b1, \u03b2) with offset \u03b4, we first uniformly sample a real number r in range [log(\u03b1 + \u03b4), log(\u03b2 + \u03b4)]. Then, r is transformed to \u230aexp(r) \u2212 \u03b4\u230b. We denote the extended log-uniform distribution Ulog(\u03b1, \u03b2, \u03b4). As \u03b4 gets larger, the samples are more dispersed to larger numbers. In the experiments, we set \u03b4 = 3.\n\nAdditionally, we introduce several other sampling details for division problems. Assume that we independently sample two numbers a and b for the dividend and the divisor. In about half of the\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAddition 1330 + 121163 114780 + 4356 638 + 2 35 + 77 114261 + 354 3 + 13792 10151 + 7 22 + 1399 363356 + 450475 73 + 11 179895 + 4128 3 + 10 1 + 141972 57612 + 18403 9 + 1621 3370 + 381 678 + 8854 422 + 10348 118 + 582 1343 + 408534 24 + 9251 315 + 652424 355 + 4434 22 + 834928 3028 + 357 777 + 1355 154874 + 81059 64936 + 216852 3 + 340939 3 + 984775 50581 + 1183 415 + 943 110 + 49 15 + 17058 36278 + 100 6 + 23516 1462 + 848 1002 + 2773 135 + 178346 22672 + 162038\n\nSubtraction 376776 \u2212 35241 10638 \u2212 100 109033 \u2212 52649 85137 \u2212 3098 22355 \u2212 2824 7 \u2212 1 652781 \u2212 78853 64914 \u2212 3114 13041 \u2212 1422 28293 \u2212 4540 11553 \u2212 3576 656291 \u2212 2795 93 \u2212 42 55972 \u2212 1782 84587 \u2212 51 273269 \u2212 5867 274405 \u2212 14 51926 \u2212 9 4272 \u2212 229 223267 \u2212 377 14857 \u2212 1994 914771 \u2212 836 3035 \u2212 2963 30 \u2212 12 149 \u2212 4 89057 \u2212 6 296410 \u2212 9 45 \u2212 3 78906 \u2212 3 56560 \u2212 29960 98 \u2212 6 16551 \u2212 920 25606 \u2212 194 45 \u2212 37 129443 \u2212 70196 221 \u2212 54 11010 \u2212 818 47759 \u2212 67 10 \u2212 8 1439 \u2212 153\n\nMultiplication 9466 \u00d7 176175 179 \u00d7 516 5509 \u00d7 133 6783 \u00d7 2 6 \u00d7 80285 37275 \u00d7 19258 168484 \u00d7 154 3331 \u00d7 40 349 \u00d7 158 17988 \u00d7 262130 8140 \u00d7 1670 51 \u00d7 5 16497 \u00d7 158 74 \u00d7 10 216 \u00d7 13414 621 \u00d7 2 2 \u00d7 5951 189486 \u00d7 13080 552792 \u00d7 763 77 \u00d7 3 179090 \u00d7 469029 1037 \u00d7 258 8 \u00d7 769974 47765 \u00d7 7254 5608 \u00d7 18164 21437 \u00d7 12 15007 \u00d7 15 539860 \u00d7 427 3583 \u00d7 9754 13 \u00d7 66 266394 \u00d7 185 3988 \u00d7 12 5514 \u00d7 57 5 \u00d7 1712 17 \u00d7 430178 227 \u00d7 127 20888 \u00d7 54 96 \u00d7 232801 175 \u00d7 1050 146 \u00d7 166\n\nDivision 620261 \u00f7 155034 111730 \u00f7 1176 28268 \u00f7 1 588137 \u00f7 25571 180330 \u00f7 739 879975 \u00f7 97772 111461 \u00f7 905026 42338 \u00f7 14003 108 \u00f7 384103 60002 \u00f7 7479 131467 \u00f7 131290 890679 \u00f7 62 228 \u00f7 131108 892 \u00f7 124 15 \u00f7 964156 369044 \u00f7 28364 457 \u00f7 46 14687 \u00f7 730 200361 \u00f7 1049 19715 \u00f7 965179 98 \u00f7 7 406 \u00f7 9 47345 \u00f7 122 391613 \u00f7 1631 892642 \u00f7 3898 241554 \u00f7 1901 116475 \u00f7 12908 488317 \u00f7 197443 7519 \u00f7 325 3560 \u00f7 847611 9711 \u00f7 1385 44540 \u00f7 103 19721 \u00f7 58 59544 \u00f7 24 333057 \u00f7 333057 25719 \u00f7 5142 7544 \u00f7 46 45 \u00f7 410 195659 \u00f7 2047 412572 \u00f7 16\n\nTable 1: 40 randomly selected samples of each type of 6-digit arithmetic problems.\n\ncases, the dividend a would be less than the divisor b, so the quotients will be zero for those cases. To ensure a diverse range of quotients, we sample the divisor b from Ulog(1, 10N , 3), the quotient c from Ulog(0, 10N /b, 3), and the remainder r from Ulog(0, b, 3). The dividend is calculated from these values: a = b \u00d7 c + r. This way, we can sample division problems with a diverse range of quotients and remainders.\n\nTable 1 presents 40 problem samples for each 6-digit problem type. Several properties of our sampling scheme can be observed from the table. First, each number ranges over diverse numbers of digits. Second, the division problems are mostly non-trivial, i.e., the quotients are not concentrated at zero.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC.2 THE ALGORITHMIC PROBLEMS\n\nC.2.1 LONGEST COMMON SUBSEQUENCE (LCS)\n\nThe question of an LCS problem is two number sequences joined by the LCS token, and the answer is the corresponding LCS and its length separated by ; . Here is an example of length-4 LCS problem:\n\n\u2022 Q: GO 1 2 3 4 LCS 2 4 6 8 = \u2022 A: 2 4 ; 2 STOP\n\nFor a length-N LCS problem, we sample two sequences of length N . Each character of the sequences are randomly sampled from 0-9 with equal probability.\n\nC.2.2 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe question of a length-N LPS problem starts with the LPS , followed by a sequence of length N . Similar to LCS, the answer contains the corresponding LPS and its length separated by ; . The following is an example of length-8 LPS problem:\n\n\u2022 Q: GO LPS 4 1 2 5 3 2 6 1 = \u2022 A: 1 2 3 2 1 ; 5 STOP\n\nThe sequence of an LPS problem is sampled in the same way as done for the LCS problem.\n\nC.2.3\n\n0-1 KNAPSACK\n\nEach item in a 0-1 Knapsack problem is represented by its value and weight. For instance, 1 2 & 3 4 represents an item with a value of 12 and a weight of 34. The question part of a 0-1 Knapsack problem is a sequence consisting of the KNAPSACK token, a list of items separated by , , the token @ , and the capacity of the knapsack. The answer part starts with a list of items to include, then $ , and finally the total value. The following is an example of a 3-item knapsack problem.\n\n\u2022 Q: GO KNAPSACK 5 & 1 2 , 2 5 & 1 5 , 1 9 & 1 8 @ 4 0 =\n\n\u2022 A: 2 5 & 1 5 , 1 9 & 1 8 $ 4 4 STOP\n\nIn this example, given a knapsack of capacity 40, the last two are selected with the total value of 44.\n\nFor a fixed number of items, we uniformly sample each item\u2019s value and weight from the integers of range [1, 99].\n\nC.2.4 MATRIX CHAIN MULTIPLICATION (MCM)\n\nThe cost of multiplying many matrices is very sensitive to the order of multiplication. Matrix chain multiplication is the task of finding the best order with the minimum cost. Here, the cost is defined to be the total number of element multiplications. In the example of three matrices A, B, and C, whose shapes are 4\u00d72, 2\u00d78, and 8\u00d73 respectively, the cost of computing (AB)C is 4\u00d72\u00d78+4\u00d78\u00d73 = 160, while another order A(BC) costs only 2 \u00d7 8 \u00d7 3 + 4 \u00d7 2 \u00d7 3 = 72. In the question of an MCM problem, the sizes of the matrices are enumerated, and the answer contains the order and the total cost separated by ; . The example above is represented as the following sequences.\n\n\u2022 Q: GO MCM 4 \u00d7 2 , 2 \u00d7 8 , 8 \u00d7 3 =\n\n\u2022 A: 4 \u00d7 2 , ( 2 \u00d7 8 , 8 \u00d7 3 ) ; 7 2 STOP\n\nGiven a fixed number of matrices, we sample the sizes of matrices from the range [1, 99].\n\nC.2.5 SORTING\n\nAlthough not included in the main text, we test the problem of sorting multi-digit numbers. The results are presented in Appendix J. The problem difficulty is defined by the maximum number of\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nterms. For a sorting problem of at most N terms, we first uniformly sample the number of terms from [2, N ]. Then we sample each term from Ulog(0, 1000, 5). The following is an example of the sorting problem.\n\n\u2022 Q: GO SORT 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 , 4 1 =\n\n\u2022 A: 4 1 , 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 STOP\n\nD DETAILS OF THE RECURSIVE REASONING PROCEDURES\n\nIn this section, we elaborate the procedures to recursively solve the arithmetic problems. Specifically, we present the algorithms to produce the subproblems of a problem. Therefore, for a set of randomly sampled questions, we can generate ground truth contexts using these algorithms. For better understanding, we present the key parts of our Python code, the thought methods. For each problem, we create a child class the Problem class and implement thought static method. The method takes a set of arguments for a problem and returns the list of direct subproblems. Each subproblem is represented by a problem class, problem arguments, and recursion type (whether it is a tail recursion or not). We use named tuple T to group these information:\n\n1 2\n\nfrom collections import namedtuple T = namedtuple('Thought', ['prob_cls', 'args', 'type'], defaults=[''])\n\nFor instance, T(Mul, (3, 4)) represents a regular subproblem of 3 \u00d7 4, and T(Add, (12, 340), \u2019tail\u2019) represents a subproblem of 12 + 340 which should be performed as a tail recursion. Once the thought method returns a list of Ts, we can recursively find more subproblems for each subproblem.\n\nD.1 ADDITION\n\nThe core idea of our recursive procedure for addition is to first add the last digits, and then add the rest. If the sum of the last digits is greater than or equal to 10, we insert another subproblem for adding the carry right after adding the last digits.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21\n\nclass Add(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nl_last, r_last = left % 10, right % 10 thoughts = [T(Add, (l_last, r_last))]\n\nl_rest, r_rest = left // 10, right // 10 if l_last + r_last >= 10:\n\nthoughts.append(T(Add, (l_rest, 1))) l_rest += 1\n\nif l_rest > 0 and r_rest > 0:\n\nthoughts.append(T(Add, (l_rest, r_rest)))\n\nreturn thoughts\n\nFigure 1 in the main draft is an example with no carry, and the following is another example of 27+65 with a carry.\n\n\u2022 X 1: GO 3 1 7 + 6 5 = GO 7 + 5 = 1 2 STOP GO 3 1 + 1 = 3 2 STOP GO 3\n\n2 + 6 = 3 8 STOP 3 8 2 STOP\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n\u2022 X 2: GO 7 + 5 = 1 2 STOP \u2022 X 3: GO 3 1 + 1 = GO 1 + 1 = 2 STOP 3 2 STOP \u2022 X 4: GO 1 + 1 = 2 STOP \u2022 X 5: GO 3 2 + 6 = GO 2 + 6 = 8 STOP 3 8 STOP \u2022 X 6: GO 2 + 6 = 8 STOP\n\nD.2 SUBTRACTION\n\nSimilar to addition, we first subtract the last digits and solve the rest recursively. When subtracting the last digits x and y, we always borrow 10 for x to prevent a negative result. The borrowing of 10 is easy for a sequence model: just put 1 before x. Therefore, the base cases of subtraction are when a \u2264 19 and b \u2264 9. If the subtraction result of the last digits is smaller than 10, i.e., the borrow is actually needed, we subtract 1 from the rest of the first operand m.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Sub(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 19 and right <= 9:\n\nreturn []\n\nl_last = left % 10 + 10 r_last = right % 10 thoughts = [T(Sub, (l_last, r_last))] l_rest, r_rest = left // 10, right // 10 if l_last - r_last < 10:\n\nthoughts.append(T(Sub, (l_rest, 1))) l_rest -= 1\n\nif r_rest > 0:\n\nthoughts.append(T(Sub, (l_rest, r_rest)))\n\nreturn thoughts\n\nHere is an example of 432-216:\n\n\u2022 X 1: GO 4 3 2 - 2 1 6 = GO 1 2 - 6 = 6 STOP GO 4 3 - 1 = 4 2 STOP GO\n\n4 2 - 2 1 = 2 1 STOP 2 1 6 STOP\n\n\u2022 X 2: GO 1 2 - 6 = 6 STOP \u2022 X 3: GO 4 3 - 1 = GO 1 3 - 1 = 1 2 STOP 4 2 STOP \u2022 X 4: GO 1 3 - 1 = 1 2 STOP \u2022 X 5: GO 4 2 - 2 1 = GO 1 2 - 1 = 1 1 STOP GO 4 - 2 = 2 STOP 2 1 STOP \u2022 X 6: GO 1 2 - 1 = 1 1 STOP \u2022 X 7: GO 4 - 2 = 2 STOP\n\nNotice that the final answer and the questions of each subproblem can be easily constructed from previous sequence.\n\nD.3 MULTIPLICATION\n\nThe base cases of multiplication are (i) when either operands are 0 or 1, or (ii) when both operands are less than 10. If one of the operands is 0, then the answer is zero; when one of them is 1, then the answer is just a copy of the other operand. For the cases where both operands are less than 10, we just let the model memorize them, which is similar to an elementary school math curriculum.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nThere are two types of non-base cases. For the simpler case, where the second operand is less than 10, we first split the first operand into the last digit and the rest. We then multiply each of them with the second operand and combine the results. Otherwise, we split the second operand into the last digit and the rest. The first operand is multiplied to each of them, and the results are summed.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n\nclass Mul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 1 or right <= 1:\n\nreturn []\n\nif left <= 9 and right <= 9:\n\nreturn []\n\nthoughts = [] if right < 10:\n\nthoughts.append(T(Mul, (left % 10, right))) thoughts.append(T(Mul, (left // 10, right)))\n\na1 = (left % 10) * right a2 = (left // 10) * right thoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nelse:\n\na1 = left * (right % 10) thoughts.append(T(Mul, (left, right % 10)))\n\na2 = left * (right // 10) thoughts.append(T(Mul, (left, right // 10)))\n\nthoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nreturn thoughts\n\nHere are some example contexts of multiplication:\n\n\u2022 X 1: GO 4 3 * 2 1 = GO 4 3 * 1 = 4 3 STOP GO 4 3 * 2 = 8 6 STOP TAIL\n\n8 6 0 + 4 3 = THINK\n\n\u2022 X 2: GO 4 3 * 1 = 4 3 STOP\n\n\u2022 X 3: GO 4 3 * 2 = GO 3 * 2 = 6 STOP GO 4 * 2 = 8 STOP TAIL 8 0 + 6 =\n\nTHINK\n\n\u2022 X 4: GO 3 * 2 = 6 STOP\n\n\u2022 X 5: GO 4 * 2 = 8 STOP\n\n\u2022 X 6: GO 8 0 + 6 = GO 0 + 6 = 6 STOP 8 6 STOP\n\n\u2022 X 7: GO 0 + 6 = 6 STOP\n\n\u2022 X 8: GO 8 6 0 + 4 3 = GO 0 + 3 = 3 STOP GO 8 6 + 4 = 9 0 STOP 9 0 3\n\nSTOP\n\n\u2022 X 9: GO 0 + 3 = 3 STOP\n\n\u2022 X 10: GO 8 6 + 4 = GO 6 + 4 = 1 0 STOP GO 8 + 1 = 9 STOP 9 0 STOP\n\n\u2022 X 11: GO 6 + 4 = 1 0 STOP\n\n\u2022 X 12: GO 8 + 1 = 9 STOP\n\nNotice that we use tail recursion in X 1 and X 3.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nD.4 COMPARISON\n\nComparison is used as a subroutine during division. The procedure for comparison consists of three steps:\n\n1. Compare the numbers of digits.\n\n2. If the numbers of digits are the same, compare the most significant digits.\n\n3. If the most significant digits are identical, compare the remaining digits recursively.\n\nWe find that the sequence models can perform the first step without an explicit subproblem. Therefore, we only add intermediate steps for the second and the third steps.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass Compare(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nthoughts = [] digit_l, digit_r = len(str(left)), len(str(right)) if digit_l == digit_r:\n\n# Compare first digit l_first, r_first = int(str(left)[0]), int(str(right)[0]) thoughts.append(T(Compare, (l_first, r_first))) if l_first == r_first: # Compare the rest l_rest = int(str(left)[1:]) r_rest = int(str(right)[1:]) thoughts.append(T(Compare, (l_rest, r_rest)))\n\nreturn thoughts\n\nThe following is an example of comparing 153 and 159.\n\n\u2022 X 1: GO 1 5 3 VS 1 5 9 = GO 1 VS 1 = EQ STOP GO 5 3 VS 5 9 = LT STOP\n\nLT STOP\n\n\u2022 X 2: GO 1 VS 1 = EQ STOP \u2022 X 3: GO 5 3 VS 5 9 = GO 5 VS 5 = EQ STOP GO 3 VS 9 = LT STOP LT STOP \u2022 X 4: GO 5 VS 5 = EQ STOP \u2022 X 5: GO 3 VS 9 = LT STOP\n\nD.5 DIVISION\n\nSolving division is the most challenging among the four basic arithmetic operations since the procedure is basically trial and error, searching for the correct quotient. Nonetheless, the following process is a recursive version of the elementary school division.\n\nThe base case is when the dividend is less than or equal to the divisor. If the dividend is smaller than the divisor, the quotient is 0, and the remainder is the dividend. If the dividend is equal to the divisor, than the quotient is 1, and the remainder is 0. Both cases can be handled relatively easily by neural sequence models. To determine whether it is one of these cases, we always perform the comparison as the first subproblem.\n\nIf it is not a base case, we check whether the dividend is smaller than 10 times the divisor. If the dividend is smaller, we subtract the divisor from the dividend and recursively divide the result with the divisor. The final answer is attained by simply adding 1 to the quotient of the smaller division.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nTo explain the other case, where the dividend is greater than 10 times the divisor, let us call the dividend a and the divisor b. First, we split the a into the last digit x and the remaining digits m. Then, we divide m with the divisor b, i.e., we are solving one-digit-smaller subproblem first. Since we define the division operation to return both a quotient and a remainder, the quotient q1 = m/b and the remainder r1 = m mod b from the subproblem is added to the context. Next, we concatenate the remainder and x, which is numerically computing r \u00d7 10 + x, and divide it again with b. Let the quotient and the remainder of this operation q2 and r2. Then, the quotient of the final answer is q1 \u00d7 10 + q2, while the remainder is simply r2.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Div(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args thoughts = [T(Compare, (left, right))]\n\n# Base cases if left <= right:\n\nreturn thoughts\n\nthoughts.append(T(Compare, (left, right * 10))) if left <= right * 10:\n\ndiff = left - right thoughts.append(T(Sub, (left, right))) thoughts.append(T(Div, (diff, right)))\n\nelse:\n\nthoughts.append(T(Div, (left // 10, right))) left_remainder = (left // 10) % right * 10 + left % 10 thoughts.append(T(Div, (left_remainder, right)))\n\nreturn thoughts\n\nThe following is an example of 76 \u00f7 29.\n\n\u2022 X 1: GO 7 6 \u00f7 2 9 = GO 7 6 VS 2 9 = GT STOP GO 7 6 VS 2 9 0 = LT STOP GO 7 6 - 2 9 = 4 7 STOP GO 4 7 \u00f7 2 9 = 1 R 1 8 STOP 2 R 1 8 STOP\n\n\u2022 X 2: GO 7 6 VS 2 9 = GO 7 VS 2 = GT STOP GT STOP\n\n\u2022 X 3: GO 7 VS 2 = GT STOP\n\n\u2022 X 4: GO 7 6 VS 2 9 0 = LT STOP\n\n\u2022 X 5: GO 7 6 - 2 9 = GO 1 6 - 9 = 7 STOP GO 7 - 1 = 6 STOP GO 6 - 2 =\n\n4 STOP 4 7 STOP\n\n\u2022 ... \u2022 X 9: GO 4 7 \u00f7 2 9 = GO 4 7 VS 2 9 = GT STOP GO 4 7 VS 2 9 0 = LT STOP GO 4 7 - 2 9 = 1 8 STOP GO 1 8 \u00f7 2 9 = 0 R 1 8 STOP 1 R 1 8 STOP\n\n\u2022 X 10: GO 4 7 VS 2 9 = GO 4 VS 2 = GT STOP GT STOP\n\n\u2022 X 11: GO 4 VS 2 = GT STOP\n\n\u2022 X 12: GO 4 7 VS 2 9 0 = LT STOP\n\n\u2022 X 13: GO 4 7 - 2 9 = GO 1 7 - 9 = 8 STOP GO 4 - 1 = 3 STOP GO 3 - 2 =\n\n1 STOP 1 8 STOP\n\n\u2022 ... \u2022 X 17: GO 1 8 \u00f7 2 9 = GO 1 8 VS 2 9 = LT STOP 0 R 1 8 STOP\n\n\u2022 X 18: GO 1 8 VS 2 9 = GO 1 VS 2 = LT STOP LT STOP\n\n\u2022 ...\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nD.6 LONGEST COMMON SUBSEQUENCE (LCS)\n\nGiven sequences A and B, the algorithm starts by comparing the last characters of the two sequences. If the last two characters are the same, we find LCS of the subsequences without the last characters, i.e., LCS of A\u2236\u22121 and B\u2236\u22121. Otherwise, we compute the LCSs of the cases where the last character of either side is removed, and return the better one. In the following code, LCS._answer is the subroutine that finds the LCS of two sequences. Equal returns TRUE if the two arguments are the same, or FALSE otherwise.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass LCS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Equal, (l[-1], r[-1]))] if l[-1] == r[-1]:\n\nthoughts.append(T(LCS, (l[:-1], r[:-1]))) return thoughts\n\nlcs1_args = (l[:-1], r) lcs2_args = (l, r[:-1]) lcs1 = LCS._answer(lcs1_args) lcs2 = LCS._answer(lcs2_args) thoughts.extend([\n\nT(LCS, lcs1_args), T(LCS, lcs2_args), T(Compare, (len(lcs1), len(lcs2)))\n\n]) return thoughts\n\nThe following is an example of finding the LCS of 123 and 234.\n\n\u2022 X 1: GO 1 2 3 LCS 2 3 4 = GO EQUAL 3 , 4 = FALSE STOP GO 1 2 LCS 2 3 4 = 2 ; 1 STOP GO 1 2 3 LCS 2 3 = 2 3 ; 2 STOP GO 1 VS 2 = LT STOP 2 3 ; 2 STOP\n\n\u2022 X 2: GO EQUAL 3 , 4 = FALSE STOP \u2022 X 3: GO 1 2 LCS 2 3 4 = GO EQUAL 2 , 4 = FALSE STOP GO 1 LCS 2 3 4 = ; 0 STOP GO 1 2 LCS 2 3 = 2 ; 1 STOP GO 0 VS 1 = LT STOP 2 ; 1 STOP\n\n\u2022 ... \u2022 X 21: GO 1 2 3 LCS 2 3 = GO EQUAL 3 , 3 = TRUE STOP GO 1 2 LCS 2 = 2\n\n; 1 STOP 2 3 ; 2 STOP\n\n\u2022 ... \u2022 X 23: GO 1 VS 2 = LT STOP\n\nD.7 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe overall algorithm for LPS is similar to LCS. The base cases are when the sequence length is less then 3. If it is not a base case, we first check if the characters at both ends of the sequence are the same. If they are the same, we find the LPS of the subsequence excluding them. Otherwise, we compare the cases where one of the end characters are excluded.\n\n1 2\n3 4\n5 6\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\n# Base cases if len(args) == 1: return []\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n\nelif len(args) == 2:\n\nreturn [T(Equal, args)]\n\nthoughts = [T(Equal, (args[0], args[1]))] if args[0] == args[-1]:\n\nsub_lps = LPS._answer(args[1:-1]) thoughts.extend([\n\nT(LPS, args[1:-1]), T(Add, (len(sub_lps), 2))\n\n])\n\nelse:\n\nlps1_args = args[:-1] lps2_args = args[1:] lps1 = LPS._answer(lps1_args) lps2 = LPS._answer(lps2_args) thoughts.extend([\n\nT(LPS, lps1_args), T(LPS, lps2_args), T(Compare, (len(lps1), len(lps2)))\n\n])\n\nreturn thoughts\n\nThe following is an example of LPS.\n\n\u2022 X 1: GO LPS 1 2 3 2 = GO EQUAL 1 , 2 = FALSE STOP GO LPS 1 2 3 = 1 ; 1 STOP GO LPS 2 3 2 = 2 3 2 ; 3 STOP GO 1 VS 3 = LT STOP 2 3 2 ; 3 STOP\n\n\u2022 X 2: GO EQUAL 1 , 2 = FALSE STOP \u2022 X 3: GO LPS 1 2 3 = GO EQUAL 1 , 3 = FALSE STOP GO LPS 1 2 = 1 ; 1\n\nSTOP GO LPS 2 3 = 2 ; 1 STOP GO 1 VS 1 = EQ STOP 1 ; 1 STOP\n\n\u2022 ... \u2022 X 10: GO LPS 2 3 2 = GO EQUAL 2 , 2 = TRUE STOP GO LPS 3 = 3 ; 1 STOP\n\nGO 1 + 2 = 3 STOP 2 3 2 ; 3 STOP\n\n\u2022 ... \u2022 X 14: GO 1 VS 3 = LT STOP\n\nD.8\n\n0-1 KNAPSACK\n\nThe base cases are when there is only one item. In this case, we simply compare the item\u2019s weight and the knapsack\u2019s capacity, to determine whether the item should be included. If it is a non-base case, we compare two possibilities: (i) include the first item, or (ii) exclude the first item. We recursively compute the subproblems and find the case with the best value.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]: items, capacity = args value, weight = items[0]\n\n# Base case if len(items) == 1:\n\nreturn [T(Compare, (weight, capacity))]\n\n# When excluding the current item items_max, value_max = Knapsack._answer((items[1:], capacity)) thoughts = [\n\nT(Knapsack, (items[1:], capacity)), T(Compare, (weight, capacity)),\n\n]\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n18 19 20 21 22 23 24 25 26 27 28 29 30\n\n# When including the current item if weight <= capacity:\n\nitems_sub, value_sub = Knapsack._answer( (items[1:], capacity - weight))\n\nvalue_incl = value_sub + value thoughts.extend([\n\nT(Sub, (capacity, weight)), T(Knapsack, (items[1:], capacity - weight)), T(Add, (value_sub, value)), T(Compare, (value_incl, value_max)),\n\n])\n\nreturn thoughts\n\nThe following is an example of 0-1 knapsack problem with three items and a knapsack capacity of 10.\n\n\u2022 X 1: GO KNAPSACK 3 & 9 , 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = 4 & 2 , 9 & 5 $ 1 3 STOP GO 9 VS 1 0 = LT STOP GO 1 0 - 9 = 1 STOP GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = $ 0 STOP GO 0 + 3 = 3 STOP GO 3 VS 1 3 = LT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n\u2022 X 2: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 9 & 5 @ 1 0 = 9 & 5 $ 9 STOP GO 2 VS 1 0 = LT STOP GO 1 0 - 2 = 8 STOP GO KNAPSACK 9 & 5 @ 8 = 9 & 5 $ 9 STOP GO 9 + 4 = 1 3 STOP GO 1 3 VS 9 = GT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n\u2022 ... \u2022 X 11: GO 9 VS 1 0 = LT STOP \u2022 X 12: GO 1 0 - 9 = 1 STOP \u2022 X 13: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = GO KNAPSACK 9 & 5 @ 1 = $ 0 STOP\n\nGO 2 VS 1 = GT STOP $ 0 STOP\n\n\u2022 ... \u2022 X 17: GO 0 + 3 = 3 STOP \u2022 X 18: GO 3 VS 1 3 = LT STOP\n\nD.9 TERNARY ADDITION AND MULTIPLICATION\n\nTernary addition and multiplication arises as a subproblem while solving MCM, which will be explained in the next section. They are simple extensions of addition and multiplication to three integers.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18\n\nclass TernaryAdd(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Add, (a1, a2)), T(Add, (a1 + a2, a3), 'tail')\n\n]\n\nclass TernaryMul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Mul, (a1, a2)), T(Mul, (a1 * a2, a3), 'tail')\n\n]\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nD.10 MATRIX CHAIN MULTIPLICATION (MCM)\n\nGiven N matrices, the N \u2212 1 subproblems are defined for each possible binary split. For the multiplication of four matrices ABCD, there are three possible binary splits: A(BCD), (AB)(CD), and (ABC)D. For each binary split, the total cost is the sum of (i) the minimum cost of computing the first group, (ii) the minimum cost of computing the second group, and (iii) the cost of multiplying the two matrices resulting from each group. Once we get the total costs of each binary split, we return choose the best split with the minimum cost. The following code implements this procedure.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n\nclass MCM(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nmats, min_order, min_cost = args\n\n# Base cases if len(mats) == 1: return []\n\nif min_order is None:\n\n# Top-level problem l_mats, r_mats = mats[:1], mats[1:]\n\nelse:\n\n# Middle of recursion l_mats, r_mats = mats\n\nl_args = (l_mats, None, None) r_args = (r_mats, None, None) l_order, l_cost = MCM._answer(l_args) r_order, r_cost = MCM._answer(r_args) agg_cost = l_mats[0][0] * r_mats[0][0] * r_mats[-1][1] thoughts = [\n\nT(MCM, l_args), T(MCM, r_args), T(TernaryMul, (l_mats[0][0], r_mats[0][0], r_mats[-1][1])), T(TernaryAdd, (l_cost, r_cost, agg_cost)),\n\n]\n\ncost = l_cost + r_cost + agg_cost if min_cost is not None:\n\nthoughts.append(T(Compare, (cost, min_cost)))\n\nif min_cost is None or cost < min_cost:\n\nmin_cost = cost min_order = l_order, r_order\n\nif len(r_mats) > 1:\n\nnew_l_mats = l_mats + (r_mats[0],) new_r_mats = r_mats[1:] thoughts.append(\n\nT(MCM, ((new_l_mats, new_r_mats), min_order, min_cost), 'tail'))\n\nreturn thoughts\n\nThe following is an example of three-matrix MCM.\n\n\u2022 X 1: GO MCM 3 \u00d7 9 , 9 \u00d7 4 , 4 \u00d7 5 = GO MCM 3 \u00d7 9 = 3 \u00d7 9 ; 0 STOP GO MCM 9 \u00d7 4 , 4 \u00d7 5 = 9 \u00d7 4 , 4 \u00d7 5 ; 1 8 0 STOP GO 3 * 9 * 5 = 1 3 5 STOP GO 0 + 1 8 0 + 1 3 5 = 3 1 5 STOP TAIL MCM 3 \u00d7 9 , 9 \u00d7 4 | 4 \u00d7\n\n5 ACC 3 \u00d7 9 , ( 9 \u00d7 4 , 4 \u00d7 5 ) ; 3 1 5 = THINK\n\n\u2022 ... \u2022 X 32: GO MCM 3 \u00d7 9 , 9 \u00d7 4 | 4 \u00d7 5 ACC 3 \u00d7 9 , ( 9 \u00d7 4 , 4 \u00d7 5 ) ; 3 1 5 = GO MCM 3 \u00d7 9 , 9 \u00d7 4 = 3 \u00d7 9 , 9 \u00d7 4 ; 1 0 8 STOP GO MCM 4 \u00d7 5 = 4 \u00d7 5 ; 0 STOP GO 3 * 4 * 5 = 6 0 STOP GO 1 0 8 + 0 + 6 0 = 1 6 8\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nSTOP GO 1 6 8 VS 3 1 5 = LT STOP ( 3 \u00d7 9 , 9 \u00d7 4 ) , 4 \u00d7 5 ; 1 6 8 STOP\n\n\u2022 ...\n\nD.11 SORTING\n\nAmong several sorting algorithms, we choose merge sort for our experiments with CoT and RoT. Note that WT is not relevant to the sorting algorithm since it produces the answer directly. The merge sort algorithm is simple: (i) split the given sequence to two equally sized subsequences, (ii) sort each subsequence, and (iii) merge the two sorted sequences. Since the final merge operation is quite complicated, we define the merge as a problem type.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n\nclass Merge(Problem): @staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Compare, (l[0], r[0]))] if l[0] < r[0] and len(l) > 1:\n\nthoughts.append(T(Merge, (l[1:], r)))\n\nelif l[0] >= r[0] and len(r) > 1:\n\nthoughts.append(T(Merge, (l, r[1:])))\n\nreturn thoughts\n\nclass MergeSort(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nif len(args) < 2: return []\n\nl_len = (len(args) + 1) // 2 l = args[:l_len] r = args[l_len:] return [\n\nT(MergeSort, l), T(MergeSort, r), T(Merge, (tuple(sorted(l)), tuple(sorted(r))), 'tail')\n\n]\n\nE FINE-TUNING GPT-3 FOR RECURSION OF THOUGHT\n\nUsing the OpenAI API, we fine-tune GPT-3 for Recursion of Thought. The goal is to learn 16-digit addition, 16-digit subtraction, 8-digit multiplication, and 8-digit division simultaneously. GPT-3\u2019s fine-tuning API takes a dataset where each example is a prompt-completion pair in plain text. It is converted to tokens by a special tokenizer for GPT, which we cannot control. This API is not directly compatible with RoT due to several reasons.\n\n\u2022 There is no special tokens such as GO , THINK , and STOP .\n\n\u2022 The input and target sequences have to be the same. However, they are different in RoT due to the THINK token. Once THINK is produced, the RoT framework triggers the recursion process to find the subproblem\u2019s answer and replace the THINK token with it. Therefore, the THINK token appears in the target sequences, but never in the input sequences.\n\nMoreover, the way that GPT-3 tokenizes numbers hinders the learning of arithmetic reasoning rules. GPT-3 tokenizes a multi-digit number into a set of two-digit or three-digit numbers. For example, the text 1234567 is converted to the sequence of tokens 123 45 67 . Under this tokenization scheme, the relationship between the numbers become obscured. As an example, the tokens 7 , 17 ,\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n27 , ..., 997 all have 7 as their last digit. Since there is no direct way for a model to know that they share the same digit, it is crucial to use each digit as a token. We believe that OpenAI needs to correct this tokenization of GPT-3 for numbers.\n\nLuckily, we can mimic the RoT procedures with the API by using several tricks. First, we replace the special tokens with plain lower-case words, e.g., GO \u2192 go and STOP \u2192 stop, which are included in the vocabulary of GPT-3. Second, we add a space before each token to make sure that the GPT tokenizer separates each token. We also add space before each digit to prevent the tokenizer grouping a number into 2-to-3-digit tokens. Finally, to simulate the behavior of the THINK and STOP tokens, we derive multiple examples from each context, one for each THINK or STOP output.\n\nAs an example, context X 3 in Figure 1 is converted to the following JSON lines for GPT-3 as follows:\n\nX 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP\n\nGO 4 + 3 = 7 STOP\n\n7 5 STOP\n\nY 3 PAD \u00d77\n\nGO 0 + 5 = THINK PAD GO 4 + 3 = THINK PAD 7 5 STOP\n\n\u21d3\n\n1 2\n3\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" go 0 + 5 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop\", \"completion\": \" go 4 + 3 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop go 4 + 3 = 7 stop\", \"completion\": \"\n\n7 5 stop\"}\n\nIn the case of Without Thought (WT), each problem is simply converted into a single example:\n\nX GO 4 0 + 3 5 = 7 5 STOP Y PAD \u00d77\n\n7 5 STOP\n\n\u21d3\n\n1\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" 7 5 stop\"}\n\nIn both cases of RoT and WT, we fine-tune GPT-3 for 10K steps with a batch size of 256. Among the several variants of GPT-3, we use Ada which is offered at the lowest cost. Note that RoT produces multiple contexts for each problem, and each RoT context is converted to multiple training examples. For this reason, the GPT-3 fine-tuned for RoT encounters much fewer problems during training, although the number of training steps are the same.\n\nF TRAINING DETAILS OF THE TINY MODELS\n\nIn all experiments, we use a batch size of 256 and Adam optimizer Kingma & Ba (2015) with a learning rate of 0.001, i.e., the default learning rate in PyTorch. We train the Transformers for 500K steps and and decay the learning rate by half every 50K steps. Since the LSTMs converge slower than the Transformers, we train them for 800K steps and decay the learning rate by half every 100K steps. At every 20K steps, we evaluate the model on a test set of 30K problems, and if a model reaches a perfect accuracy of 1.0, we do not train the model further. The models can be trained on a single GPU with 12GB memory.\n\nG EFFICIENT EVALUATION OF ROT\n\nAt the problem scales that RoT is tested, solving a single problem can require hundreds of thousands of tokens. Therefore, we have to develop an efficient evaluation algorithm. Since the models are\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\ndeterministic, we do not need to evaluate the same context multiple times. Therefore, given a test set, we first find all unique contexts that should appear while solving the test set. Note that there would be a lot of duplicate contexts, especially for the base cases. These unique contexts can be evaluated in parallel. We evaluate each context in a similar manner to the training. For each context, we construct the target sequence, as we described in \u00a73.2, and check if the model outputs the target sequence correctly, given the GT context. In case of GPT-3, we convert the contexts and target sequences as described in Appendix E to feed into the API. Once every context is individually evaluated, the next step is to aggregate the results. In this aggregation stage, a (sub)problem is marked correct only if its subproblems are all marked correct. This way, we efficiently evaluate RoT with minimal cost.\n\nH FAILURE CASE ANALYSIS OF GPT-3 WITH ROT\n\nIn this section we provide some failure cases of RoT in the GPT-3 experiments for further insights. Although the model achieves an accuracy of over 99% in almost every case, there are several problems where it makes mistakes. Nonetheless, only a few characters are wrong in each context.\n\nThe most common type of error is repeating the same character for a wrong number of times. The following is the examples of this error type.\n\nPrompt\n\nCompletion\n\nModel Output\n\nPrompt\n\nCompletion\n\nModel Output\n\ngo 1 6 4 3 4 vs 1 0 1 1 6 = go 1 vs 1 = eq stop\n\ngo 6 4 3 4 vs 1 1 6 = think\n\ngo 6 4 3 4 vs 1 1 1 6 = think\n\ngo 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 7\n\n= go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 7 = 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 stop go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 = 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 stop\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\nPrompt\n\ngo 1 0 5 4 6 2 0 * 1 5 = go 1 0 5 4 6 2 0 * 5 = 5 2 7 3\n\n1 0 0 stop go 1 0 5 4 6 2 0 * 1 = 1 0 5 4 6 2 0 stop\n\nCompletion\n\nModel Output\n\ntail 1 0 5 4 6 2 0 0 + 5 2 7 3 1 0 0 = think\n\ntail 1 0 5 4 6 2 0 0 0 + 5 2 7 3 1 0 0 = think\n\nAnother common case is incorrectly counting the number of digits while comparing two numbers. When comparing numbers following our recursive procedure, the model has to immediately return the answer if the number of digits are different. In the following example, the first number has seven digits, and the second one has six digits.\n\nPrompt\n\ngo 1 0 6 9 0 3 4 vs 8 9 1 5 0 6 =\n\nCompletion\n\ngt stop\n\nModel Output\n\ngo 1 vs 8\n\nAlthough the model should directly output the answer, i.e., gt stop, it decides that the number of digits are the same and moves on to the next step, which is to compare the most significant digits. This error is caused by the specific recursive procedure of our choice. Since we assume that the model would easily figure out the difference in the numbers of digits, we did not add explicit steps to count the digits. Although the model detects the difference in most cases, it turns out to be a\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nrelatively challenging operation. A possible solution is to teach the model to explicitly count the number of digits and compare them.\n\nI CONTEXT LENGTH DISTRIBUTION\n\nIn Figure 4, we present the distributions of context lengths for each problem type. We compare the context lengths of RoT and CoT. For each configuration, we randomly sample 10K contexts from the training distribution and plot the histogram of their lengths. The graphs show that the context sizes of CoT are many orders of magnitude larger than RoT. In theory, the total number of tokens to generate for each problem is identical in both RoT and CoT (if we do not count the THINK tokens). However, RoT\u2019s context sizes are much smaller since it utilizes multiple contexts.\n\nAnother advantage of RoT is the utilization of dynamic programming. Since we can easily cache the duplicate computations of RoT as explained in Appendix G, we can drastically reduce the amount of token generation if there is a redundant structure in the problem. The amount of tokens to generate for each problem is plotted in Figure 5. The benefit is especially prominent in the algorithmic problems. For example, finding the LCS of two 32-digit sequences results in more than 1018 tokens if we naively use CoT or RoT. If we use dynamic programming with RoT, we can efficiently solve the same problem with much less cost.\n\nJ TRANSFORMERS ARE POWERFUL SORTING MACHINES\n\nIn fact, the first algorithmic task that we tested is sorting since it has been widely used as a benchmark for algorithmic reasoning (Reed & de Freitas, 2016; Cai et al., 2017; Pierrot et al., 2019). However, we find that Transformers are incredibly good at sorting, even in the WT setting. Figure 6 shows the sorting experiment. For CoT and RoT, we train the merge sort algorithm. Interestingly, WT easily achieves a perfect score in sorting 64 three-digit numbers. Also the training converges much faster than RoT. The Transformer architecture, more specifically the attention mechanism, seems to be perfectly suited for the sorting operation.\n\nK THE EXACT VALUES OF FIGURE 3\n\nTable 2-5 show the exact values of the graphs in Figure 3. Except for the GPT-3 experiments in Table 2, we report the average and the standard deviation of eight runs. Each GPT-3 experiment is done only once.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The distributions of context lengths.\n\n34\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 5: The distribution of the total number of tokens to produce in order to solve each problem. RoT can utilize dynamic programming to reduce redundant computations.\n\n35\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 6: Sorting experiment with the tiny Transformer.\n\nProblem\n\nDifficulty\n\nWT CoT\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\n32-digit 48-digit\n\n32-digit 48-digit\n\n8-digit 16-digit\n\n8-digit 16-digit\n\nlength 16 length 24\n\nlength 24 length 40\n\n4 items 6 items\n\n3 matrices 4 matrices\n\n0.991 0.853\n\n0.991 0.886\n\n0.337 0.098\n\n0.363 0.123\n\n0.980 0.832\n\n0.995 0.800\n\n0.945 0.634\n\n0.481 0.110\n\nRoT \u2212 0.998 \u2212 0.995 \u2212 0.998 \u2212 0.998 \u2212 0.999 \u2212 0.994 \u2212 1.000 \u2212 0.989 \u2212 0.995 \u2212 0.998 \u2212 1.000 \u2212 0.974 \u2212 0.999 \u2212 1.000 \u2212 0.997 \u2212 0.992\n\nTable 2: The exact values of the GPT-3 experiments in Figure 3a.\n\n36\n\nRI,WHPV$FFXUDF\\6RUWLQJ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKWUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\nWT 0.863 \u00b1 0.265 0.370 \u00b1 0.475 0.336 \u00b1 0.430 0.455 \u00b1 0.458 0.119 \u00b1 0.316 0.082 \u00b1 0.216 0.105 \u00b1 0.277 0.000 \u00b1 0.000 0.982 \u00b1 0.006 0.705 \u00b1 0.411 0.238 \u00b1 0.412 0.221 \u00b1 0.385 0.426 \u00b1 0.433 0.114 \u00b1 0.303 0.116 \u00b1 0.307 0.161 \u00b1 0.282 1.000 \u00b1 0.000 0.817 \u00b1 0.023 0.340 \u00b1 0.032 0.169 \u00b1 0.015 0.104 \u00b1 0.016 0.048 \u00b1 0.020 0.033 \u00b1 0.017 0.014 \u00b1 0.006 0.012 \u00b1 0.001 1.000 \u00b1 0.000 0.978 \u00b1 0.008 0.354 \u00b1 0.029 0.186 \u00b1 0.009 0.128 \u00b1 0.011 0.087 \u00b1 0.012 0.075 \u00b1 0.005 0.059 \u00b1 0.007 0.048 \u00b1 0.008\n\nCoT 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\nRoT 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.001 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.001 \u2212 0.999 \u00b1 0.001 \u2212 0.999 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.000 \u2212 0.999 \u00b1 0.000\n\nTable 3: The exact values of the Transformer experiments in Figure 3b (arithmetic problems).\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\nlength 3 length 4 length 8 length 12 length 16 length 20 length 24 length 28 length 32\n\nlength 4 length 7 length 8 length 16 length 24 length 32 length 40 length 48 length 56\n\n2 items 4 items 6 items 8 items 10 items 12 items\n\n2 matrices 4 matrices 6 matrices 8 matrices 10 matrices 12 matrices\n\nWT 1.000 \u00b1 0.000 0.997 \u00b1 0.008 0.999 \u00b1 0.002 0.965 \u00b1 0.025 0.880 \u00b1 0.035 0.759 \u00b1 0.043 0.622 \u00b1 0.038 0.484 \u00b1 0.043 0.375 \u00b1 0.030 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.999 \u00b1 0.001 0.950 \u00b1 0.019 0.788 \u00b1 0.019 0.608 \u00b1 0.023 0.477 \u00b1 0.030 0.365 \u00b1 0.029 1.000 \u00b1 0.000 0.966 \u00b1 0.006 0.849 \u00b1 0.007 0.640 \u00b1 0.242 0.481 \u00b1 0.279 0.435 \u00b1 0.252 0.973 \u00b1 0.009 0.177 \u00b1 0.069 0.088 \u00b1 0.029 0.033 \u00b1 0.025 0.051 \u00b1 0.032 0.026 \u00b1 0.011\n\nCoT 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000\n\nRoT\n\n\u2212 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.000 \u2212 0.999 \u00b1 0.000 \u2212\n\u2212 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.999 \u00b1 0.001 \u2212 0.998 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.988 \u00b1 0.029 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.998 \u00b1 0.001 \u2212 0.996 \u00b1 0.002\n\nTable 4: The exact values of the Transformer experiments in Figure 3b (algorithmic problems).\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\n1-digit 2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\nWT 1.000 \u00b1 0.000 0.642 \u00b1 0.305 0.005 \u00b1 0.008 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 1.000 \u00b1 0.000 0.776 \u00b1 0.179 0.006 \u00b1 0.001 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 0.000 \u00b1 0.000 1.000 \u00b1 0.000 0.855 \u00b1 0.044 0.636 \u00b1 0.061 0.338 \u00b1 0.063 0.270 \u00b1 0.030 0.162 \u00b1 0.025 0.138 \u00b1 0.025 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.001 0.891 \u00b1 0.072 0.516 \u00b1 0.077 0.308 \u00b1 0.069 0.192 \u00b1 0.028 0.115 \u00b1 0.015\n\nCoT 1.000 \u00b1 0.000 1.000 \u00b1 0.001 0.997 \u00b1 0.005 0.905 \u00b1 0.155 0.795 \u00b1 0.341\n\n1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.896 \u00b1 0.252 0.443 \u00b1 0.377\n\n1.000 \u00b1 0.000\n\n1.000 \u00b1 0.000\n\nRoT 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.999 \u00b1 0.000 0.999 \u00b1 0.001 0.986 \u00b1 0.024 \u2212 0.871 \u00b1 0.275 \u2212 0.358 \u00b1 0.430 \u2212 0.120 \u00b1 0.202 1.000 \u00b1 0.000 1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.994 \u00b1 0.016 0.908 \u00b1 0.236 \u2212 0.507 \u00b1 0.398 \u2212 0.295 \u00b1 0.406 \u2212 0.101 \u00b1 0.137 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.987 \u00b1 0.008 \u2212 0.896 \u00b1 0.105 \u2212 0.670 \u00b1 0.208 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 1.000 \u00b1 0.000 \u2212 0.998 \u00b1 0.004 \u2212 0.996 \u00b1 0.007 \u2212 0.958 \u00b1 0.036 \u2212 0.914 \u00b1 0.090\n\nTable 5: The exact values of the LSTM experiments in Figure 3c.\n\n39",
      "responses": [
        null,
        "# Summary Of The Paper\nThe paper presents a novel framework called Recursion of Thought (RoT) that enhances the multi-step reasoning capabilities of language models (LMs) by leveraging recursive problem decomposition. RoT introduces special tokens to facilitate the management of subproblems within separate contexts, allowing LMs to handle complex reasoning tasks that exceed their fixed context limits. Through rigorous experimentation, the authors demonstrate that RoT significantly outperforms existing methods, such as Chain of Thought (CoT) and Without Thought (WT), even with smaller models, suggesting that the reasoning capabilities of LMs can be enhanced without necessarily increasing model size.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to overcoming the context size limitation of LMs, which has been a significant barrier in complex reasoning tasks. The introduction of special tokens to control the recursion process is particularly noteworthy, as it mimics human reasoning and provides a structured way to handle subproblems. However, a notable weakness is the dependency on supervision for each task, which may limit the generalizability of the approach. Additionally, the authors acknowledge that RoT does not facilitate length generalization, indicating areas for future improvement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results of the proposed approach. The quality of the writing is high, with sufficient detail provided to understand the model and its applications. The novelty of RoT is significant, as it proposes a new paradigm for reasoning in LMs that challenges traditional views on model scaling. The inclusion of source code enhances reproducibility, allowing others to validate the findings presented.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling framework that effectively addresses a critical limitation in current language models. While it demonstrates impressive empirical results, the reliance on supervision for training highlights a need for further exploration in unsupervised or semi-supervised contexts. The potential implications of RoT for future LM design and application are substantial.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, designed to enhance the multi-step reasoning capabilities of language models by allowing them to recursively solve complex problems through the use of multiple contexts. It addresses the limitations of traditional models, such as GPT-3 and PaLM, which often struggle with lengthy reasoning tasks due to their restricted context sizes. The authors incorporate special tokens (GO, STOP, THINK, TAIL) to facilitate the recursion process and evaluate RoT on various arithmetic and algorithmic tasks, demonstrating significant performance improvements over baseline methods, including direct answer generation and Chain of Thought (CoT) approaches. Notably, RoT enables even smaller models to achieve remarkable accuracy on complex tasks.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative framework that mimics human-like reasoning by allowing recursive problem-solving, which is particularly beneficial for tasks that exceed typical context limitations. Additionally, the model-agnostic nature of RoT showcases its versatility across both large and small models, leading to significant performance gains. However, the requirement for supervisory training to learn the divide-and-conquer strategies presents a limitation, as it may restrict the framework's applicability to unsupervised tasks. Furthermore, challenges in generalizing to longer problems exceeding training lengths and the complexity of implementation due to special tokens may hinder its broader adoption.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings of the RoT framework. The experiments are comprehensively described, allowing for reproducibility, although the complexity of the implementation may pose challenges for practitioners. The novelty of the approach is significant, as it introduces a fresh perspective on managing context limitations in language models by leveraging recursive reasoning. However, the reliance on supervised training could affect the generalizability of the results to real-world applications.\n\n# Summary Of The Review\nOverall, the paper represents a substantial advancement in the field of reasoning with language models through the introduction of the Recursion of Thought framework. While it offers innovative solutions to existing challenges in multi-step reasoning, concerns regarding supervision and generalization warrant further investigation to fully realize its potential in diverse applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a novel framework called Recursion of Thought (RoT), designed to address the context size limitations of language models (LMs) when solving multi-step reasoning problems. By allowing models to recursively create multiple contexts, RoT enhances their reasoning capabilities, particularly in complex tasks such as arithmetic and algorithmic reasoning. The authors demonstrate that RoT significantly improves performance, achieving near-perfect accuracy on these tasks even with large problem sizes, thus showcasing its effectiveness across various model sizes, including smaller, less pre-trained LMs.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the innovative approach of recursively structuring problem-solving, which effectively mitigates the inherent context limitations of existing LMs. The experiments presented are robust, with clear comparisons to baseline methodologies like the Chain of Thought (CoT) approach, effectively illustrating RoT's advantages. However, the paper does have weaknesses; it relies on supervised learning for task-specific training, which could limit its applicability in more generalized settings. Additionally, while the results are impressive, the paper does not sufficiently address the implications of its limitations, particularly regarding generalization to untrained tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, making it accessible to readers familiar with language models and reasoning methods. The methodology, including the introduction of specific tokens for context management and the detailed inference and training procedures, is presented with sufficient detail for reproducibility. The novelty of RoT lies in its recursive approach to reasoning, which is a significant departure from existing methodologies. However, while the experimental results are compelling, further validation across a broader range of tasks would enhance the paper's contributions.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of language model reasoning by introducing the Recursion of Thought framework, which effectively addresses context size limitations. The methodology is sound, and the empirical results are impressive, though further exploration of generalization capabilities would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThis paper introduces the Recursion of Thought (RoT) framework, a novel approach designed to improve reasoning capabilities in language models by addressing the limitations of fixed context sizes. The methodology employs a divide-and-conquer strategy, enabling the model to tackle complex reasoning tasks more effectively. Empirical results show that RoT significantly outperforms existing methods, such as Chain of Thought (CoT), particularly in arithmetic and algorithmic reasoning tasks, although its generalization to out-of-distribution problems remains untested.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to enhancing reasoning in language models through the RoT framework, which allows for more complex tasks by utilizing dynamic programming principles to optimize computation. The model's compatibility with various architectures, including smaller sequence models, broadens its applicability. However, the reliance on supervision for learning the divide-and-conquer strategy may limit its versatility across diverse domains. Additionally, while the extensive experimental evaluation provides strong support for RoT's efficacy, the focus on specific problem types raises concerns about its scalability and generalization capabilities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its concepts effectively, making complex ideas accessible. The theoretical foundations of RoT are clearly articulated, and the authors provide robust empirical evidence for their claims. Reproducibility is supported by the availability of source code; however, the specific datasets and training procedures might hinder replication in varied contexts. Overall, the quality of the writing and presentation is commendable, although practical implementation challenges may arise for practitioners.\n\n# Summary Of The Review\nThe paper presents a significant advancement in reasoning capabilities for language models through the RoT framework, demonstrating strong empirical performance in specific tasks. However, concerns about generalization, dependence on supervision, and the scalability of the approach warrant further investigation. Overall, the work is a valuable contribution to the field, albeit with notable limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the **Recursion of Thought (RoT)** framework, addressing the limitations of existing language models in handling complex multi-step reasoning tasks, primarily due to context size constraints. RoT employs a **divide and conquer** strategy, allowing models to recursively generate and manage multiple contexts for subproblems, thus enhancing reasoning capabilities. The methodology involves special tokens to manage context creation and tail recursion to optimize the reasoning process. Empirical results show that RoT significantly improves the performance of large models like GPT-3 on complex tasks, and even enables smaller, randomly initialized models to tackle previously infeasible reasoning challenges. The paper provides a comparative analysis, demonstrating RoT's superiority over traditional Chain of Thought (CoT) methods and a baseline method termed **Without Thought (WT)**.\n\n# Strength And Weaknesses\n**Strengths:**\n- The introduction of RoT represents a novel paradigm shift in how language models approach complex reasoning, leveraging recursive strategies that align with cognitive processes.\n- The paper is backed by comprehensive empirical evaluations, showing strong performance across a variety of complex tasks.\n- RoT\u2019s model-agnostic nature suggests potential applicability across different architectures, broadening its impact.\n\n**Weaknesses:**\n- The approach requires supervised training for the recursive reasoning process, which may limit its scalability and applicability in unsupervised contexts.\n- The exploration of how model architecture influences RoT's performance is somewhat limited, which raises questions about its generalization across various models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers. The quality of the experiments is high, with adequate detail provided for reproducibility. The novelty of the RoT framework is significant, providing a fresh perspective on enhancing reasoning in language models. However, the reproducibility could be further improved with more explicit details on the training processes and hyperparameters used.\n\n# Summary Of The Review\nThe paper presents a compelling and innovative approach to enhancing reasoning capabilities in language models through the Recursion of Thought framework. By effectively managing multiple contexts, RoT addresses limitations in fixed context sizes and opens new avenues for future research in complex reasoning tasks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a novel framework for adversarial training in language models, termed Recursion of Thought (RoT). The methodology focuses on recursively decomposing complex adversarial examples into simpler subproblems, thereby enhancing the robustness of the models against adversarial attacks. The authors provide a model-agnostic framework that can be applied to various architectures, including Transformers and LSTMs, and detail experimental results that demonstrate a significant performance improvement over traditional adversarial training methods across multiple benchmarks.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to adversarial training, which introduces a recursive perspective that has potential implications for future research. The experimental validation is extensive and supports the claims made regarding RoT's effectiveness, showcasing its advantages over existing methodologies. However, the paper has notable weaknesses, including a lack of discussion regarding the limitations of RoT, particularly concerning computational complexity and the risk of overfitting. Moreover, the evaluation could be strengthened by including a broader range of adversarial attacks to assess the generalizability of the framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to both researchers and practitioners in the field. The quality of the experimental setup is commendable, providing a solid basis for the conclusions drawn. The novelty of the approach is significant, as RoT offers a fresh perspective on adversarial training. However, the reproducibility of the results would benefit from a more detailed description of the experimental configurations and datasets used, which could enhance the transparency of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful advancement in the domain of adversarial training through its Recursion of Thought framework. While the methodology and experimental results are compelling, a more thorough exploration of the limitations and broader applicability of RoT would strengthen the paper's contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces \"Recursion of Thought\" (RoT), a novel model-agnostic framework designed to enhance the reasoning capabilities of language models. It posits that RoT enables models to tackle complex problems through a recursive problem-solving approach, akin to human reasoning, thus overcoming the limitations of existing models like GPT-3, particularly in context size. The experimental results claim near-perfect accuracy on challenging reasoning tasks, suggesting that RoT can generalize across a wide array of tasks without specific training, potentially transforming the landscape of AI reasoning and making existing models obsolete.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious proposal of RoT, which promises to revolutionize language model reasoning. The methodology of employing recursive problem-solving is well-articulated and presents a clear departure from traditional approaches. However, the weaknesses are notable: the authors overstate the capabilities of RoT, particularly in terms of generalization and performance, leading to inflated expectations about its applicability. Furthermore, while the authors acknowledge certain limitations, they underplay the challenges that RoT still faces in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a coherent structure and thorough explanations of the proposed methodology. However, the quality of empirical evaluation raises concerns, as the claimed performance improvements may not be as universally applicable as suggested. The novelty of RoT is significant, but the reproducibility of the results is questionable given the exaggerated claims and lack of comprehensive benchmarks against existing methods. Overall, while the ideas presented are innovative, their practical implementation and validation remain uncertain.\n\n# Summary Of The Review\nWhile the paper presents an ambitious and potentially transformative framework in RoT for enhancing reasoning in language models, it suffers from overstatements regarding its capabilities and implications. The clarity and structure of the work are solid, but the empirical results and generalizations could mislead readers about the readiness of RoT for widespread application.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, designed to enhance the reasoning capabilities of language models by allowing them to recursively generate and utilize multiple contexts. This methodology addresses the limitations imposed by maximum context lengths in existing frameworks like Chain of Thought (CoT), enabling models to tackle complex reasoning tasks requiring over 50,000 tokens. The experimental results demonstrate that RoT significantly outperforms CoT in various arithmetic and algorithmic reasoning tasks, achieving near-perfect accuracy on complex problems and allowing even smaller models to perform tasks previously deemed beyond their capabilities.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to improving the reasoning abilities of language models, particularly through the introduction of RoT. The empirical results presented are compelling, showcasing substantial improvements over CoT in both accuracy and the complexity of tasks that can be addressed. However, the paper could benefit from further discussion regarding the limitations of RoT and potential edge cases where it may not perform as expected. Additionally, while the results are impressive, more extensive testing across a wider array of tasks would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the RoT framework, its implementation, and the results obtained. The quality of the writing is high, with a logical flow that makes the complex ideas accessible. The novelty of RoT is significant, as it addresses a critical gap in language model capabilities. However, the reproducibility of the results may be a concern if sufficient details regarding the experimental setup and parameters are not provided, which is not explicitly mentioned in the notes.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the reasoning capabilities of language models through the novel framework of Recursion of Thought. The empirical results indicate a strong performance improvement over existing methods, although further exploration of potential limitations and broader applicability is warranted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the concept of Recursion of Thought (RoT) as a framework to enhance the reasoning capabilities of language models, particularly in light of their inherent context size limitations. The authors argue that RoT can universally improve performance across a variety of tasks by allowing models to leverage recursive reasoning. The methodology includes the use of specific tokens to guide the recursion process, and the findings suggest that RoT enables better handling of complex problems, implying a direct relationship between recursion and model performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious attempt to address the limitations of current language models regarding reasoning. The proposed framework offers a novel approach that could potentially enhance model performance in complex tasks. However, several weaknesses are noted, including assumptions about the universal applicability of RoT across different architectures and the reliance on ground truths that may not be available in real-world scenarios. The paper also faces critiques regarding its evaluation metrics, which emphasize strict correctness, potentially limiting the applicability of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly, although some assumptions could have been articulated with greater depth. The quality of the presented methodology is generally high, but the novelty of RoT may be overstated, as it does not sufficiently engage with existing alternative reasoning strategies. Reproducibility may be constrained by the specific tokenization approach and the assumption of ground truth availability, which may not be feasible in broader contexts.\n\n# Summary Of The Review\nOverall, the paper offers a provocative perspective on enhancing reasoning in language models through recursion, but it is hindered by several unexamined assumptions and a somewhat narrow focus on specific tasks. While the idea of RoT is compelling, its generalizability and effectiveness across diverse problem domains require further validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, which enhances the reasoning capabilities of language models by enabling them to recursively manage multiple contexts. This approach addresses the context size limitations of existing models, such as GPT-3, and significantly improves performance on complex reasoning tasks compared to traditional Chain of Thought (CoT) techniques. The methodology involves breaking down problems into smaller subproblems, allowing the model to tackle more complex tasks through a structured recursive inference process. Experimental results demonstrate RoT's effectiveness across various arithmetic and algorithmic reasoning challenges.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to overcoming context size limitations in language models, presenting a scalable and practical solution to enhance reasoning capabilities. The experimental results convincingly show the superiority of RoT over existing methods, providing strong empirical support for its claims. However, a notable weakness is the reliance on task-specific supervised training, which may limit the generalizability of the framework. Additionally, the paper does not sufficiently address potential challenges in extending RoT to broader applications beyond the tasks evaluated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the RoT framework, making it accessible to readers. The quality of the writing is high, with a logical flow of ideas that effectively communicates the contributions and findings. The novelty of RoT is significant, as it proposes a new methodology rather than simply extending existing models. However, reproducibility may be a concern, as the experiments rely on specific task setups and data that may not be readily available to other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel framework that effectively addresses the limitations of current language models in multi-step reasoning tasks. While RoT shows promise in enhancing reasoning capabilities, its dependence on supervised training and task-specific setups may restrict its broader applicability. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel approach to enhancing model interpretability in deep learning through a new technique called Layer-wise Relevance Propagation (LRP). The methodology involves analyzing the contribution of each neuron in a neural network to the final prediction, thereby offering insights into the decision-making process of the model. The authors demonstrate the effectiveness of LRP through extensive experiments on benchmark datasets, showcasing improved interpretability without sacrificing predictive performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative method for providing interpretable insights into complex models, which is a growing concern in the machine learning community. The theoretical grounding is robust, and the method is shown to be applicable across various architectures, indicating its broad relevance. However, the empirical validation, while promising, appears limited to a specific set of datasets, which may not fully capture the generalizability of the proposed approach. Additionally, the discussion of potential limitations is somewhat superficial and could benefit from a deeper exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with a clear introduction that establishes the significance of the problem. The methodology is described in sufficient detail, allowing for reproducibility, although some algorithmic steps could be elaborated upon for clarity. The novelty of the approach is significant, particularly in the context of interpretability in deep learning. However, the paper could improve in terms of clarity by providing more comprehensive explanations of complex concepts.\n\n# Summary Of The Review\nThis paper introduces a promising new technique for enhancing interpretability in deep learning models, backed by a solid theoretical framework and initial empirical validation. While the contributions are noteworthy, the paper would benefit from broader empirical testing and a more thorough discussion of its limitations.\n\n# Correctness\n4/5 - The methodology is sound and the theoretical claims are well-supported, although some aspects require further empirical validation.\n\n# Technical Novelty And Significance\n4/5 - The proposed method offers a fresh perspective on the critical issue of model interpretability, making it a valuable contribution to the field.\n\n# Empirical Novelty And Significance\n3/5 - The empirical results are encouraging, but the limited scope of datasets tested raises questions about the generalizability of the findings. Additional experiments would strengthen the paper significantly.",
        "# Summary Of The Paper\nThe paper \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a novel framework, Recursion of Thought (RoT), aimed at enhancing the reasoning capabilities of language models (LMs) by addressing the limitations posed by fixed context sizes. The methodology involves utilizing abstraction and recursion to break down complex problems into manageable subproblems, allowing LMs to effectively generate intermediate reasoning steps. The findings demonstrate that RoT significantly improves the performance of LMs such as GPT-3 on complex tasks and enables even smaller models, like randomly initialized Transformers or LSTMs, to address challenging reasoning problems that require extensive token processing.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to overcoming the inherent limitations of current LMs regarding context size, which is a well-recognized bottleneck in multi-step reasoning tasks. The proposed RoT framework is model-agnostic and versatile, thus broadening its applicability across different types of architectures. However, a potential weakness is the lack of detailed comparative experiments with existing methods; while the results are promising, further validation against state-of-the-art techniques would strengthen the claims. Additionally, the paper could benefit from a more thorough discussion on the practical implications of applying RoT in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of its concepts and methodologies. The quality of writing is high, making complex ideas accessible. The novelty of the RoT framework is significant, as it introduces a new paradigm for reasoning in LMs that has not been extensively explored in the literature. Regarding reproducibility, while the paper outlines the framework and its theoretical underpinnings, it would benefit from providing more implementation details and a clearer description of the experimental setup to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing the reasoning capabilities of language models through the Recursion of Thought framework. While the contributions are significant and well-articulated, further empirical validation against existing methods and additional implementation details would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT), a novel framework aimed at enhancing the reasoning capabilities of language models (LMs) by allowing them to manage multiple contexts recursively. This approach addresses the limitations of the Chain of Thought (CoT) method, which is constrained by context size (e.g., 2048 tokens). The RoT framework employs special control tokens (GO, STOP, THINK) to facilitate recursive problem-solving, enabling LMs to generate solutions for complex arithmetic and algorithmic tasks through the division of problems into smaller subproblems. The empirical results indicate that RoT significantly outperforms CoT, achieving near-perfect accuracy across various reasoning tasks, even when applied to smaller models.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to overcoming the context limitations of existing reasoning frameworks, specifically by introducing a recursive mechanism that enhances problem-solving capabilities. The empirical evaluations are robust, demonstrating the effectiveness of RoT across different model sizes and types. However, a notable weakness is the reliance on supervised training with ground truth intermediate steps, which may limit the generalizability of the method in unsupervised settings. Additionally, the discussion of limitations does not sufficiently explore how RoT could be adapted to handle longer sequences without supervision.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind RoT, its methodology, and the resulting findings. The quality of writing is high, with clear definitions and explanations of the proposed framework. The novelty of the approach is significant, as it proposes a new paradigm for reasoning in LMs. However, while the methodology is described in sufficient detail, further elaboration on the reproducibility aspects, including access to code and data, would enhance the paper's contribution to the community.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of language modeling by introducing the Recursion of Thought framework, which effectively addresses context limitations and enhances reasoning capabilities. The empirical results are strong, though the dependency on supervised training and the challenges associated with longer sequences represent areas for further exploration.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces the Recursion of Thought (RoT) framework aimed at overcoming the context size limitations present in current language models. The authors detail the methodology behind RoT, which includes a unique approach to managing context through special tokens and recursive reasoning processes. The experimental results demonstrate that RoT significantly enhances the reasoning capabilities of language models on various arithmetic and algorithmic tasks compared to established methods like Without Thought (WT) and Chain of Thought (CoT).\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to reasoning, addressing a critical limitation in existing language models. The thorough evaluation across a range of tasks and the clear comparison with baseline methods provide strong empirical support for the proposed framework. However, the paper could improve by offering more context regarding the implications of its findings for future research and by clarifying assumptions in the training processes. Additionally, a discussion on the ethical considerations of deploying such models would enhance the depth of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex concepts accessible. The methodology is described in sufficient detail, which aids reproducibility. The originality of the RoT framework marks it as a significant contribution to the field, although further exploration of its implications and limitations would bolster its impact. Overall, the quality of the writing and presentation is high, facilitating understanding of both the methodology and results.\n\n# Summary Of The Review\nThis paper presents a novel framework for reasoning with language models that addresses existing limitations effectively. While the contributions are significant and the methodology is sound, the authors could enhance the paper by addressing broader implications and ethical considerations. Overall, the work is promising and adds valuable insights to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT), a model-agnostic framework designed to enhance multi-step reasoning in language models (LMs) by addressing the limitations of existing Chain of Thought (CoT) methodologies. RoT employs a recursive strategy to decompose complex problems into manageable subproblems, utilizing special control tokens to navigate context management dynamically. The empirical findings demonstrate that RoT significantly improves the performance of both large-scale models, such as GPT-3, and smaller randomly initialized models, enabling them to tackle complex reasoning tasks more effectively.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to context management, which allows for a more extensive range of reasoning tasks to be addressed without exceeding the token limits of existing models. The recursive division of problems is a novel contribution that could reshape the understanding of model capabilities beyond mere parameter scaling. However, the paper's reliance on supervised learning may be a limitation, as it raises questions about the scalability of RoT in unsupervised settings and its potential for generalization across diverse problem domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind RoT, its methodology, and the empirical results. The use of mathematical representations and formal definitions helps clarify the inference mechanism and training protocol. The novelty of RoT is significant, particularly in its recursive approach to reasoning, which stands apart from existing methodologies. However, while the experiments demonstrate the efficacy of RoT, the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of reasoning with language models through its innovative RoT framework, which effectively addresses the limitations of context size in existing models. The findings are promising, but the dependency on supervised learning may require further exploration to maximize the framework's applicability and scalability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents the Recursion of Thought (RoT) framework, which aims to enhance the reasoning capabilities of language models by employing multiple contexts. The authors propose that this method can enable small, randomly initialized models like Transformers and LSTMs to tackle complex reasoning tasks typically reserved for larger models. However, the empirical validation of these claims is insufficient, and the evaluation methodology raises concerns about the framework's overall effectiveness in practical applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the reasoning capabilities of language models. However, the complexity of the RoT framework may hinder its practical utility, as it introduces multiple contexts that could complicate rather than simplify the reasoning process. Additionally, the paper does not adequately address the limitations of language models, such as their reliance on training data quality and their lack of true understanding. The evaluation methodology appears flawed, potentially leading to misleading conclusions. Furthermore, the narrow focus on arithmetic and algorithmic tasks limits the generalizability of the framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces an interesting concept, its clarity is undermined by the complexity of the RoT framework. The novelty of the proposed method is not convincingly established, as the authors fail to provide a robust comparative analysis with existing approaches. The reproducibility of the findings is also questionable due to the lack of thorough empirical validation and unclear scalability of the framework to diverse tasks. Overall, the quality of the research could benefit from a more critical examination of the limitations and potential failure cases associated with RoT.\n\n# Summary Of The Review\nIn summary, the paper presents a novel but overly complex framework for enhancing reasoning in language models. While it introduces interesting concepts, significant limitations in design, evaluation, and empirical validation hinder its applicability and overall impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper introduces a novel framework called Recursion of Thought (RoT), which enhances the reasoning capabilities of language models by employing a model-agnostic approach that allows recursive management of multiple contexts. This framework draws inspiration from human reasoning processes, enabling models to tackle complex multi-step reasoning problems more effectively. The findings demonstrate that RoT significantly improves performance on challenging tasks, including 48-digit arithmetic and intricate algorithmic reasoning, even allowing smaller models to achieve results previously attainable only by more advanced systems. The authors provide extensive experimental validation and make their code available, encouraging further research and development in the field.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative framework that enhances reasoning capabilities across a range of models, including both large and small architectures. The empirical results are impressive, showcasing near-perfect accuracy on complex tasks, which suggests a significant advancement in the field of AI. However, the paper could benefit from a more thorough discussion of potential limitations and the specific computational overhead introduced by the recursive context management. Additionally, while the results are compelling, further exploration of the model's performance in diverse real-world scenarios may strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings of the research. The presentation of results is coherent, and the experiments are rigorously designed to validate the framework's effectiveness. The novelty of the approach is evident, and the authors' commitment to reproducibility is commendable, as they provide source code and detailed descriptions of their experimental setup. However, some technical details could be elaborated to aid understanding for readers less familiar with the underlying concepts.\n\n# Summary Of The Review\nOverall, the \"Recursion of Thought\" paper presents a significant and innovative advancement in the reasoning capabilities of language models, demonstrating impressive empirical results and providing a strong foundation for future research. The authors successfully bridge the gap between human-like reasoning and AI, opening avenues for broader applications. However, a more in-depth exploration of limitations and real-world applicability would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a novel framework termed \"Recursion of Thought\" (RoT), which proposes a recursive approach to reasoning with language models (LMs) as a means to overcome the limitations imposed by context size in traditional models like GPT-3 and PaLM. RoT advocates for a divide-and-conquer strategy, enabling LMs to decompose complex tasks into smaller subproblems more efficiently. The methodology includes the use of special tokens (GO, STOP, THINK, TAIL) to facilitate the recursive navigation of multiple contexts. Empirical results demonstrate improvements in accuracy across various tasks, reinforcing the theoretical foundations of RoT, which also suggests potential applications across different model architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative theoretical contribution, which challenges conventional approaches to reasoning in LMs by emphasizing recursion and abstraction. This framework not only aligns with established computational theories but also offers a model-agnostic perspective that enhances its applicability across different architectures. However, a notable weakness is the reliance on theoretical constructs that may not be fully substantiated by the empirical evidence presented. The observed improvements in performance, while significant, serve more to validate the theoretical claims rather than demonstrate a clear advancement in model superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its concepts clearly, making complex ideas accessible to readers. The quality of writing is generally high, with thorough explanations of the theoretical framework and methodology. The novelty of RoT is considerable, presenting a fresh perspective on reasoning in LMs. However, reproducibility could be a concern as the theoretical constructs are not fully backed by comprehensive empirical validation. Future research may be necessary to establish robust benchmarks that encompass various model architectures and tasks.\n\n# Summary Of The Review\nOverall, the paper presents a significant theoretical advancement in language modeling through the introduction of the Recursion of Thought framework. While the empirical results support the theoretical claims, the reliance on these constructs as the primary focus may limit the clarity of the contributions to practical applications in the field. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" presents a novel framework aimed at addressing context size limitations in language models such as GPT-3 and PaLM. The proposed \"Recursion of Thought\" (RoT) framework allows models to recursively generate multiple contexts, thereby enhancing their reasoning capabilities for complex problems. The methodology includes a model-agnostic approach, with the introduction of special tokens to facilitate recursive reasoning. The paper demonstrates that RoT improves the accuracy of various reasoning tasks, particularly arithmetic and algorithmic problems, through a structured inference process and detailed training procedures.\n\n# Strength And Weaknesses\nStrengths of the paper include its focus on a practical solution to an important limitation in current language models and the implementation of a clear framework that is compatible with existing autoregressive models. The detailed algorithmic description and the provision of code for reproducibility are significant contributions. However, the paper's weaknesses include a somewhat limited exploration of the broader implications of the proposed method and a tendency to prioritize implementation details over theoretical underpinnings. The analysis of failure cases is insightful but could benefit from deeper exploration of the implications of these errors on the applicability of RoT in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with logical progression from introduction to conclusion. The quality of the content is high, particularly in the presentation of the inference algorithm and training processes. The novelty of the RoT framework stands out, especially in its application to recursive reasoning tasks. Reproducibility is adequately addressed, with source code and detailed examples provided, allowing others to replicate the experiments and findings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling framework for enhancing reasoning capabilities in language models through a recursive approach. While it excels in clarity and practical implementation, it could further enhance its impact by exploring broader implications and theoretical insights.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces a novel methodology termed Recursion of Thought (RoT), which purports to improve upon existing Chain of Thought (CoT) techniques in enhancing reasoning tasks. The authors claim that RoT is more practical and scalable than simply increasing context limits in language models. They report near-perfect accuracy with RoT, suggesting that it enhances the reasoning capabilities of smaller models. However, the paper lacks a comprehensive comparison with existing methodologies and fails to adequately contextualize its findings against established benchmarks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of RoT as a potentially impactful framework for reasoning tasks. However, several weaknesses undermine its contributions. The authors downplay the effectiveness of CoT methods, neglect prior work on smaller models, and fail to provide a balanced comparison with existing approaches. The novelty of RoT is overstated, as similar recursive strategies have been explored in previous systems, and the limitations of training data distribution choices are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is mixed; while the idea of RoT is presented, the arguments lack depth and critical engagement with prior work. The quality of the experiments is questionable due to the unfair comparisons made with CoT and Without Thought (WT). Regarding novelty, RoT is positioned as groundbreaking, yet it appears to build upon existing methodologies without sufficiently distinguishing itself. Reproducibility may be hindered by the lack of comprehensive details on the training data and architecture choices.\n\n# Summary Of The Review\nOverall, the paper presents an interesting concept with RoT but falls short in providing a thorough evaluation of its contributions and comparisons to existing methods. The claims of superiority are not well-supported, and the paper overlooks critical prior work that could contextualize its findings more effectively.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" presents a novel approach to enhance the reasoning capabilities of language models (LMs) through a recursive strategy. The authors propose a framework that allows LMs to tackle complex problems by breaking them down into smaller, manageable tasks, thereby mitigating the limitations posed by context size. The methodology involves defining special tokens to guide the LMs through this recursive reasoning process. Experimental results demonstrate that this approach significantly improves the performance of LMs on various reasoning tasks, indicating both practical applicability and theoretical advancements.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to addressing the context size limitation of LMs and its empirical validation through rigorous experiments. The use of special tokens to facilitate recursive reasoning is a clear contribution to the field. However, the paper could benefit from a deeper exploration of the theoretical implications of the proposed method and a more thorough comparison with existing techniques beyond the empirical results presented. Additionally, some sections lack clarity, which may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept, certain aspects could be clearer. For example, the notation is not always consistent, and some equations lack proper definitions, which may confuse readers. There are also typographical errors that detract from the overall quality. The methodology is presented but could be elaborated to facilitate reproducibility. The novelty of the approach is significant, yet it would benefit from clearer definitions and terminologies, particularly regarding special tokens and their roles in the framework.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of language models through its recursive reasoning framework. While it shows promise in addressing context limitations, improvements in clarity and consistency are needed to enhance its impact and facilitate reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the Reasoning over Text (RoT) framework, which aims to enhance arithmetic and algorithmic reasoning tasks using language models. The authors propose a model-agnostic approach that can be applied to different architectures, primarily focusing on Transformers and LSTMs. The findings demonstrate performance improvements in reasoning tasks, although the evaluation is limited to specific model types and does not extensively address diverse reasoning problems or real-world applications.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative RoT framework, which shows promise in improving the performance of existing language models on reasoning tasks. However, the paper has notable weaknesses, including a narrow focus on arithmetic and algorithmic problems without exploring other reasoning domains like logical reasoning or commonsense reasoning. Furthermore, the limitations of context length in language models are acknowledged but not sufficiently examined in terms of their implications for real-world application. The evaluation is restricted to specific model architectures, raising questions about the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical structure that guides the reader through the proposed framework and its evaluations. The quality of the experiments is commendable, though the novelty of the proposed method may not be sufficiently differentiated from existing approaches. While supplementary materials provide code for reproduction purposes, there is a lack of discussion regarding the practical deployment of RoT in real-time applications, which could hinder reproducibility in more complex scenarios.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of reasoning tasks in language models through the introduction of the RoT framework. However, its limitations in scope, model applicability, and real-world relevance suggest that further exploration and broader considerations are necessary for a more comprehensive impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, designed to enhance multi-step reasoning in language models (LMs) by addressing limitations imposed by context size. The authors conduct a series of experiments involving both arithmetic operations and algorithmic tasks, demonstrating significant improvements in accuracy metrics when comparing RoT to existing methods such as Chain of Thought (CoT) and the Without Thought (WT) approach. Statistical significance is established through careful experimental design, including controlled data sampling and a rigorous evaluation methodology that emphasizes the correctness of both intermediate steps and final answers.\n\n# Strength And Weaknesses\nThe paper presents several notable strengths, including a well-structured experimental design that allows for a comprehensive assessment of RoT's capabilities. The use of statistical methods to validate improvements in reasoning performance is commendable, as is the detailed analysis of failure cases which provides valuable insights into model limitations and areas for further enhancement. However, a potential weakness lies in the reliance on specific tasks (arithmetic and algorithmic reasoning) which may not fully capture the generalized applicability of RoT across diverse language understanding tasks. The focus on statistical significance might also be critiqued for underrepresenting qualitative aspects of reasoning performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly conveys its contributions and findings, supported by rigorous statistical evaluations. The methodology is detailed enough to allow for reproducibility, including explicit descriptions of data sampling strategies and evaluation metrics. The novelty of the RoT framework is apparent, though its practical implications could benefit from exploration in broader contexts beyond the selected tasks. Overall, the quality of the writing and clarity of the presentation contribute positively to the reader's understanding.\n\n# Summary Of The Review\nThe paper effectively introduces the RoT framework, demonstrating its ability to enhance reasoning capabilities in language models through rigorous statistical evaluation. While the contributions are significant, the focus on specific tasks and quantitative measures may limit the perceived generalizability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, which aims to enhance the reasoning capabilities of models by leveraging supervised learning techniques. The authors demonstrate that RoT can improve performance on specific tasks, particularly in arithmetic and algorithmic reasoning. However, the methodology has inherent limitations, particularly in its applicability to longer sequences and out-of-distribution scenarios, which the authors do not adequately address.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its attempt to provide a model-agnostic framework that can enhance reasoning capabilities through recursion. However, the reliance on supervision may restrict its applicability to certain tasks, and the lack of exploration into out-of-distribution generalization raises concerns about its robustness. Furthermore, the paper does not adequately discuss potential adaptations for other reasoning tasks, nor does it explore failure cases comprehensively, limiting the understanding of RoT's effectiveness. The evaluation method's scalability also remains uncertain, which could hinder practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, though it lacks depth in discussing limitations and potential improvements. While the RoT framework is a novel contribution, its practical significance may be diminished by the noted weaknesses. Reproducibility could be an issue due to the absence of detailed discussions on the evaluation methods and potential integration with future model architectures.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to enhancing reasoning in models through the RoT framework. However, significant limitations in generalization, adaptability to various tasks, and a lack of robust evaluation methods detract from its overall impact and applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought\" proposes a framework aimed at enhancing the reasoning capabilities of language models through a method called Recursion of Thought (RoT). The authors highlight the limitations of existing models concerning context size and introduce RoT as a means to break down complex problems into smaller subproblems, thereby facilitating better reasoning. The authors claim to demonstrate significant improvements in performance on arithmetic and algorithmic tasks, asserting that their model-agnostic approach can leverage existing architectures while yielding near-perfect accuracy in controlled experiments.\n\n# Strength And Weaknesses\nWhile the paper attempts to present RoT as a novel contribution, many of its ideas appear to be reiterations of well-established concepts in computer science and artificial intelligence, such as the Chain of Thought (CoT) methodology. The strength of the paper lies in its systematic approach to problem decomposition, yet this is a fundamentally basic strategy in many fields, making the claim of novelty questionable. The experimental results, while illustrating improvements, rely on standard benchmarks that have been widely used in prior research, which diminishes the perceived novelty of the findings. Additionally, the authors' interpretation of their results and claims of dramatic advancements seem overstated, particularly without sufficient consideration of real-world applicability and potential overfitting.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation, but the novelty of the contributions is undermined by a lack of engagement with prior work. The authors do reference related literature, but there is a notable absence of critical analysis regarding how their contributions differ from existing methodologies. Furthermore, the claims regarding performance improvements and model-agnostic applicability would benefit from a more rigorous validation process to enhance reproducibility and reliability.\n\n# Summary Of The Review\nIn summary, while \"Recursion of Thought\" presents an organized approach to enhancing reasoning in language models, it ultimately fails to deliver novel contributions, relying instead on established concepts and methodologies. The exaggerated claims of performance improvements and the lack of critical engagement with existing literature detract from the overall impact of the work.\n\n# Correctness\nRating: 3/5  \nThe methodology and findings are primarily correct, but the claims made by the authors regarding novelty and performance enhancements are overstated.\n\n# Technical Novelty And Significance\nRating: 2/5  \nThe technical contributions largely reiterate established concepts in AI and computer science, lacking the originality needed to be considered significant.\n\n# Empirical Novelty And Significance\nRating: 2/5  \nThe empirical results are based on standard benchmarks with no substantial new insights or innovative applications, making the empirical significance limited.",
        "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" presents a novel approach to enhancing reasoning capabilities in language models by addressing the limitations of context size. The authors introduce the Recursion of Thought (RoT) framework, which allows for model-agnostic reasoning through dynamic programming techniques to minimize redundant computations. The findings suggest that RoT can effectively manage complex reasoning tasks, although the paper also acknowledges the need for further exploration regarding model scalability, generalization, and alternative architectures.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its identification of the limitations of current reasoning methods, particularly Chain of Thought (CoT), and its proposal of RoT as a potential solution. The dynamic programming aspect is a noteworthy contribution, as it presents a way to optimize reasoning processes. However, the paper could benefit from a more in-depth exploration of alternative architectures and hybrid methodologies that could further enhance the proposed framework. Additionally, the reliance on supervised learning for training data could limit its applicability, suggesting a need for more innovative approaches to model training.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. The novelty of the RoT framework is commendable, particularly in its model-agnostic approach. However, some sections could provide more detail, especially regarding potential integrations with other architectures and the implications of the findings. Reproducibility may be a concern, as the reliance on specific training data and methods might limit the ability of other researchers to replicate the results without access to the same resources.\n\n# Summary Of The Review\nOverall, this paper introduces a promising framework for improving reasoning in language models through the Recursion of Thought approach. While it addresses important limitations and proposes interesting solutions, it could benefit from further exploration of alternative methodologies and training paradigms to enhance its applicability and robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, which significantly enhances the performance of language models on various reasoning tasks. The authors demonstrate that RoT achieves remarkable accuracy on complex arithmetic and algorithmic reasoning challenges, even as problem sizes increase (e.g., 48-digit addition and 16-digit multiplication). In contrast to traditional methods such as Without Thought (WT) and Chain of Thought (CoT), RoT shows superior scalability and effectiveness, achieving over 99% accuracy across different problem difficulties while operating with smaller context sizes. The findings suggest that RoT effectively mitigates the limitations of existing approaches, enabling models to tackle previously insurmountable problems.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear demonstration of RoT's advantages over existing methods, particularly in high-difficulty scenarios where traditional approaches struggle. The empirical results are robust, showcasing RoT's ability to achieve close to perfect accuracy across several benchmarks. However, the paper could benefit from a more detailed analysis of the underlying mechanisms that contribute to the observed improvements and potential limitations of the RoT framework. Additionally, the scalability claims could be supported by more diverse datasets and real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to a broad audience. The quality of the experiments is high, with comprehensive evaluations across multiple reasoning tasks. The novelty of the RoT framework is evident, as it addresses specific limitations of existing methods. However, while the results are impressive, the reproducibility of the findings could be enhanced by providing more details about the experimental setup and the models used.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of reasoning tasks for language models through the innovative Recursion of Thought framework. The empirical results are compelling and demonstrate clear advantages over traditional methods, though additional insights into the framework's workings and enhanced reproducibility measures would strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
        "# Summary Of The Paper\nThis paper investigates the effectiveness of two reasoning techniques\u2014Chain of Thought (CoT) and Recursion of Thought (RoT)\u2014in enhancing the performance of language models (LMs). The authors employ a series of empirical experiments to demonstrate that RoT can surpass CoT in specific tasks, particularly in complex reasoning scenarios. They provide a comprehensive analysis of the methodologies, including the design of experiments, datasets used, and metrics for evaluation, ultimately concluding that RoT offers a more robust framework for reasoning in AI applications.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its empirical contributions and the clear demonstration of RoT's advantages over CoT in reasoning tasks. The methodology is well-structured, allowing for reproducibility, and the findings are significant in advancing the understanding of reasoning in LMs. However, the paper has weaknesses, including a lack of clarity in the abstract and introduction, which could hinder engagement. Additionally, the abrupt transition from CoT to RoT may confuse readers who are not well-versed in this area.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents interesting findings, the clarity suffers due to dense phrasing and inconsistent terminology. Simplifying sentence structure and improving paragraph organization would enhance readability. The novelty of the proposed techniques is evident, but the paper could be more effective in communicating its contributions. The reproducibility of the experiments is solid, as sufficient details are provided regarding methodologies and datasets, although clearer descriptions of figures and tables would further aid understanding.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the understanding of reasoning in language models by comparing CoT and RoT. However, it requires improvements in clarity and organization to facilitate better comprehension by a broader audience. Addressing these issues would strengthen the impact of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
      ],
      "preparations": [
        null,
        "### Title and Authors\n- **Title**: Recursion of Thought: Divide and Conquer Reasoning with Language Models\n- **Authors**: Anonymous (under double-blind review for ICLR 2023)\n\n### Abstract\n- Recent advances in language models (LMs) enable multi-step reasoning, epitomized by Chain of Thought (CoT) generation, which generates intermediate reasoning steps before arriving at a final answer.\n- A limitation of current LMs is their fixed context size (e.g., 2048 tokens for GPT-3 and PaLM), which constrains their ability to handle complex reasoning tasks requiring more tokens.\n- The paper introduces **Recursion of Thought (RoT)**, a model-agnostic framework that allows LMs to solve complex problems by recursively creating multiple contexts.\n- RoT significantly enhances the reasoning capabilities of large models like GPT-3 and enables smaller models (randomly initialized Transformers or LSTMs) to tackle complex problems.\n\n### Introduction\n- LMs have been increasingly applied to reasoning tasks, where they predict answer sequences from question sequences.\n- CoT allows models to break down complex problems into simpler, manageable steps, improving reasoning accuracy.\n- The context limit of sequence models severely restricts their problem-solving ability as complex problems may require far more tokens than the allowed context size.\n- RoT mimics human reasoning by recursively dividing large problems into smaller subproblems.\n- The framework introduces special tokens (GO, STOP, THINK, TAIL) to control the recursion process.\n\n### Related Work\n- **Chain of Thought**: Existing methods like Scratchpad and CoT prompting improve reasoning but are still limited by context size.\n- **Neural Programmer-Interpreter (NPI)**: Uses program execution and recursion but requires complete execution traces for training.\n- **System 1 and System 2 Approaches**: RoT represents a System 2 approach, generating reasoning alongside answers.\n\n### Recursion of Thought Framework\n#### Inference\n- RoT allows models to recursively manage subproblems within separate contexts using special tokens.\n- The inference context is structured as a concatenation of the main question, subproblem questions, and their respective answers.\n- **Tail recursion** is supported through the TAIL token, allowing the last subproblem's answer to serve as the final answer.\n\n#### Training\n- RoT is trained in a supervised manner, where the model learns the ground truth contexts and the corresponding target sequences through a structured process.\n- A loss function is defined to predict the next token given the context, using a standard language modeling objective.\n\n#### Recursive Reasoning Procedures\n- Ground truth contexts are generated through exemplary reasoning procedures for each problem type.\n- Procedures are based on human-like reasoning and split complex operations (e.g., arithmetic, algorithmic tasks) into manageable parts.\n\n### Experiments\n- RoT was tested on arithmetic (addition, subtraction, multiplication, division) and algorithmic tasks (LCS, LPS, 0-1 knapsack, MCM).\n- Comparison was made against two baselines: Without Thought (WT) and Chain of Thought (CoT).\n- RoT demonstrated near-perfect accuracy even on complex tasks requiring hundreds of thousands of tokens, while WT and CoT struggled due to their context limitations.\n\n### Results\n- RoT significantly outperformed both WT and CoT across all tasks tested.\n- Even tiny models without pre-training achieved remarkable performance through RoT.\n- The findings suggest that the reasoning capability of LMs can be significantly enhanced using the RoT paradigm.\n\n### Discussion\n- RoT challenges the notion that larger models are necessary for improved reasoning, proposing that a better approach may be to utilize multiple contexts.\n- Limitations include the dependency on supervision for each task, and RoT does not facilitate length generalization.\n- Future work may explore reducing supervision costs and applying RoT to advanced architectures that might overcome current limitations.\n\n### Conclusion\n- RoT presents a novel approach to enhancing the reasoning capabilities of LMs by utilizing multiple contexts.\n- The framework shows promise for solving highly complex reasoning problems, suggesting a potential paradigm shift in language model design and application.\n\n### Supplementary Materials\n- Source code is provided for reproducibility of experiments.",
        "### Technical Content\n- **Title**: Recursion of Thought: Divide and Conquer Reasoning with Language Models\n- **Objective**: Introduce the Recursion of Thought (RoT) framework to improve multi-step reasoning capabilities of language models by allowing recursive problem-solving through multiple contexts.\n- **Background**: Traditional language models, like GPT-3 and PaLM, have a limited context size (e.g., 2048 tokens), which restricts their ability to solve complex reasoning tasks that may require far more tokens. The RoT framework aims to overcome this limitation by enabling models to divide complex problems into smaller, manageable subproblems recursively.\n- **Key Concepts**:\n  - **Chain of Thought (CoT)**: Previous method where models generate intermediate reasoning steps before arriving at a final answer.\n  - **Recursion of Thought (RoT)**: Introduces special tokens (GO, STOP, THINK, TAIL) to control the recursion process. It allows models to create and utilize multiple contexts to handle subproblems effectively.\n\n### Experimental Design\n- **Tasks**: RoT was evaluated on:\n  - **Arithmetic Tasks**: Addition, subtraction, multiplication, and division.\n  - **Algorithmic Tasks**: Longest common subsequence (LCS), longest palindromic subsequence (LPS), 0-1 knapsack, matrix chain multiplication (MCM).\n- **Models Tested**:\n  - **Large Language Model**: GPT-3 fine-tuned with RoT.\n  - **Tiny Models**: Randomly initialized small Transformers and LSTMs to demonstrate RoT's model-agnostic capability.\n- **Training Process**:\n  - RoT was trained in a supervised manner using ground truth intermediate steps.\n  - Context sequences for RoT were constructed to include main questions and their corresponding subproblems, creating a clear structure for the model to follow during inference.\n  \n### Results\n- **Performance**: RoT significantly outperformed baseline methods:\n  - **Without Thought (WT)**: Direct answer generation with poor performance on complex tasks.\n  - **Chain of Thought (CoT)**: Limited by context size; high accuracy in simpler tasks but failed with increased complexity.\n  - **RoT**: Achieved near-perfect accuracy across all tasks, even those requiring outputs in the range of hundreds of thousands of tokens.\n- **Tiny Models**: Demonstrated that RoT can enable smaller models (e.g., 536K parameters for Transformers) to solve complex problems that larger models traditionally struggled with.\n\n### Strengths\n- **Innovative Framework**: RoT introduces a novel way to manage context limitations by recursively solving problems, reflecting human-like reasoning.\n- **Model-Agnostic**: Effective with both large pre-trained models and smaller, randomly initialized models, showcasing broad applicability.\n- **Significant Performance Gains**: Achieved remarkable accuracy even on highly complex tasks that challenge current state-of-the-art models.\n- **Dynamic Programming Utilization**: Efficiently reduces redundant calculations during problem-solving, particularly beneficial in algorithmic tasks.\n\n### Limitations\n- **Supervision Requirement**: RoT requires supervision to learn the divide and conquer strategies, which may limit its applicability to diverse, unsupervised tasks.\n- **Length Generalization**: RoT struggles with generalizing to longer problems that exceed the training length, which may be due to limitations in the underlying model architecture (e.g., Transformers).\n- **Complexity of Implementation**: The requirement for special tokens and structured context may complicate model implementation and training processes.\n- **Error Types**: Instances of errors were noted, such as incorrect token outputs or failure in digit comparisons, indicating that the model still has areas for improvement in reasoning consistency.\n\n### Conclusion\nThe paper presents a significant advancement in the field of reasoning with language models. The Recursion of Thought framework effectively leverages the strengths of recursive problem-solving to transcend the limitations imposed by fixed context sizes, demonstrating breakthroughs in both arithmetic and algorithmic reasoning tasks. Further research may focus on reducing the dependency on supervision and addressing generalization issues for longer problem instances.",
        "## Paper Notes: Recursion of Thought: Divide and Conquer Reasoning with Language Models\n\n### Abstract\n- Introduction of a framework called Recursion of Thought (RoT) aimed at overcoming the context size limitations of language models (LMs) when solving multi-step reasoning problems.\n- RoT allows models to recursively create multiple contexts, enhancing their reasoning capabilities on complex tasks.\n- Demonstrated effectiveness on arithmetic and algorithmic reasoning tasks, achieving near-perfect accuracy even for large problems.\n\n---\n\n### 1. Introduction\n- **Chain of Thought (CoT) Methodology**: Models generate intermediate reasoning steps before arriving at final answers. This helps in breaking down complex problems.\n- **Context Limitations**: Current LMs (e.g., GPT-3, PaLM) have a maximum context size (2048 tokens), constraining their ability to tackle complex reasoning tasks that may require significantly more information.\n- **Human-like Reasoning**: Humans use abstraction and recursion to manage complexity, inspiring the RoT framework which allows LMs to divide and conquer problems.\n\n---\n\n### 2. Related Work\n- **Scratchpad**: Initial attempt to use LMs for reasoning through CoT (Nye et al., 2021) but identified context size as a limitation.\n- **Neural Programmer-Interpreter (NPI)**: Uses recursive reasoning but requires full execution traces.\n- **System 1 vs. System 2**: Classification of reasoning approaches; RoT and CoT fall under System 2 (sequential reasoning).\n\n---\n\n### 3. Recursion of Thought\n#### 3.1 Inference\n- **Special Tokens**: Introduction of tokens:\n  - `GO`: Marks the start of a problem sequence.\n  - `STOP`: Marks the end of a sequence.\n  - `THINK`: Initiates recursion.\n- **Context Structure**: Each context of a problem is represented as:\n  \\[\n  X = [Q; Q_{sub,1}; A_{sub,1}; ...; Q_{sub,N}; A_{sub,N}; A]\n  \\]\n- **Tail Recursion**: Implemented using a `TAIL` token, allowing the last subproblem's answer to be treated as the final answer without duplication.\n- **Algorithm 1**: Describes the inference process for RoT (steps of generating tokens and recursively processing subproblems).\n\n#### 3.2 Training\n- **Supervised Learning**: RoT is trained with ground truth (GT) intermediate steps. \n- **Target Sequence Creation**: Algorithm 2 details how to convert context sequences (X) to target sequences (Y) for training.\n- **Loss Function**: Defined as:\n  \\[\n  L = -\\sum_{i} I[y_{i+1} \\neq PAD] \\log p(y_{i+1} | X_{1:i})\n  \\]\n\n#### 3.3 Recursive Reasoning Procedures\n- Procedures are designed to generate ground truth contexts for various arithmetic operations and algorithmic tasks.\n\n---\n\n### 4. Experiments\n- **Comparison Baselines**:\n  - Without Thought (WT): Directly outputs answers.\n  - Chain of Thought (CoT): Generates intermediate steps without recursion.\n- **Tasks Evaluated**:\n  - Arithmetic (addition, subtraction, multiplication, division)\n  - Algorithmic (Longest Common Subsequence, Longest Palindromic Subsequence, 0-1 Knapsack, Matrix Chain Multiplication)\n- **Problem Difficulty**: Gradually increased to test model capabilities.\n\n#### 4.1 Unleashing GPT-3\u2019s Reasoning Capability\n- Fine-tuned GPT-3 on arithmetic tasks; achieved near-perfect scores across all tested difficulties, surpassing previous capabilities.\n  \n#### 4.2 RoT with Tiny Language Models\n- Tested smaller models (Tiny Transformer and LSTM) showing RoT's effectiveness even without pre-training.\n\n---\n\n### 5. Discussion\n- **Implications for Future Models**: Suggests that the architecture and the ability to utilize multiple contexts could redefine the capabilities of language models.\n- **Limitations**: Supervision for task-specific learning is still necessary; RoT does not facilitate length generalization.\n\n---\n\n### 6. Conclusion\n- RoT effectively addresses the limitations of single-context reasoning in language models by leveraging recursive reasoning, enabling solutions for problems requiring extensive token generation.\n\n---\n\n### References\n- Includes references from various foundational works in neural reasoning, programming, and cognitive psychology, emphasizing the interdisciplinary nature of the research.\n\n---\n\n### Appendix\n- Detailed examples of RoT inference, training data specifications, and additional experimental results are provided in the supplementary materials.\n\n### Key Algorithms\n- **Algorithm 1**: Inference process for RoT.\n- **Algorithm 2**: Creation of target sequences from contexts.\n\n### Figures\n- Illustrative examples of RoT inference processes and comparisons of context sizes between CoT and RoT.\n\n---\n\n### Experimental Results\n- High accuracy in reasoning tasks demonstrates the potential of RoT in enhancing language models beyond their existing limitations.",
        "**Positive Aspects:**\n\n1. **Novel Approach (Recursion of Thought - RoT)**:\n   - The proposed RoT framework effectively addresses the limitation of fixed context size in language models, allowing for more complex reasoning tasks by using a divide-and-conquer strategy.\n   - **Limitation**: RoT requires supervision to learn the divide-and-conquer strategy for each specific task, which can be resource-intensive and may hinder its applicability across varied domains without extensive training data.\n\n2. **Performance Improvement**:\n   - RoT demonstrates significant improvements in reasoning accuracy over existing methods like Chain of Thought (CoT), especially for complex problems requiring extensive token generation.\n   - **Limitation**: While RoT excels in specific arithmetic and algorithmic tasks, its performance on out-of-distribution tasks or novel problem types remains untested, indicating a potential gap in generalization capabilities.\n\n3. **Model-Agnostic Framework**:\n   - RoT is designed to be compatible with various sequence models, including smaller models, which broadens its applicability beyond just large pre-trained models like GPT-3.\n   - **Limitation**: The effectiveness of RoT in smaller models may vary, and the paper does not thoroughly explore the performance of RoT with diverse architectures beyond Transformers and LSTMs.\n\n4. **Dynamic Programming Utilization**:\n   - RoT effectively utilizes dynamic programming principles to reduce redundant calculations, which can lower computational costs and increase efficiency in solving complex problems.\n   - **Limitation**: The implementation of dynamic programming may add complexity to the model's architecture and increase the training time, making it less accessible for rapid development or deployment in real-world applications.\n\n5. **Comprehensive Evaluation**:\n   - The paper presents extensive experiments across various arithmetic and algorithmic reasoning tasks, providing strong empirical support for the proposed method.\n   - **Limitation**: The experiments focus primarily on specific types of problems, and the scalability of RoT for more diverse or non-standard reasoning tasks is not sufficiently evaluated.\n\n6. **Reproducibility**:\n   - The authors provide source code for reproducing their experiments, which enhances transparency and allows other researchers to build upon their work.\n   - **Limitation**: The reliance on specific datasets and training procedures may limit the reproducibility of results in different settings or with different problem distributions.\n\n7. **Clear Communication of Concepts**:\n   - The paper clearly outlines the theoretical basis for RoT and illustrates its functionality with examples, making complex ideas more accessible to readers.\n   - **Limitation**: While the theoretical explanations are clear, the practical implementations may still pose challenges for practitioners unfamiliar with advanced machine learning techniques, necessitating additional resources or tutorials. \n\nBy balancing these strengths and limitations, the notes provide a nuanced view of the paper's contributions and areas for potential enhancement.",
        "### ICLR Style Review Notes\n\n**Paper Title:** Recursion of Thought: Divide and Conquer Reasoning with Language Models\n\n**Summary of Contributions:**\n1. **Problem Addressed:** The paper identifies the limitations of existing language models in handling complex multi-step reasoning tasks due to context size constraints, particularly in generating Chain of Thought (CoT) steps.\n  \n2. **Proposed Methodology:** The authors introduce a novel framework termed **Recursion of Thought (RoT)**, which allows language models to tackle complex reasoning tasks by recursively generating and managing multiple contexts for subproblems. This contrasts with traditional CoT methods that rely on a single context.\n\n3. **Mechanism of RoT:**\n   - RoT utilizes a **divide and conquer** strategy, enabling the model to generate subproblems and solutions in a hierarchical manner.\n   - The process is initiated through the production of special tokens (e.g., GO, STOP, and THINK) that dictate when to create new contexts and how to manage the recursion.\n   - Unlike CoT, RoT employs a **tail recursion** strategy, which optimizes the reasoning process by allowing the model to focus on the most relevant subproblem context at any given time.\n\n4. **Generalization Capability:** RoT is claimed to be model-agnostic and effective with a range of architectures, including smaller models that have not been pre-trained, enabling them to perform complex reasoning tasks.\n\n5. **Empirical Results:**\n   - The authors demonstrate that RoT significantly enhances the reasoning performance of large language models like GPT-3, achieving near-perfect accuracy on challenging arithmetic and algorithmic tasks.\n   - Additionally, RoT enables smaller, randomly initialized models to complete complex reasoning tasks that were previously considered infeasible.\n\n6. **Comparative Analysis with CoT:**\n   - The paper provides a comprehensive evaluation comparing RoT to CoT and a baseline method termed **Without Thought (WT)**. RoT outperforms both methods across a variety of tasks, particularly when the problem complexity exceeds traditional context limits.\n   - The authors note that while CoT can generate intermediate reasoning steps, it is inherently limited by the fixed context size, whereas RoT exploits multiple contexts to overcome this barrier.\n\n7. **Future Directions:** The authors suggest that RoT's methodology could be adapted to future language model architectures, potentially improving their reasoning capabilities without the need for extensive task-specific supervision.\n\n**Strengths:**\n- The introduction of RoT presents a significant shift in how language models can approach complex reasoning, leveraging recursive strategies that mimic human thought processes.\n- Comprehensive experimental validation across a range of tasks provides strong evidence for the effectiveness of the proposed method.\n- The model-agnostic nature of RoT opens avenues for broader applicability within various language model frameworks.\n\n**Weaknesses:**\n- The requirement for supervision in training the recursive reasoning procedures may limit the scalability of RoT to broader applications without additional developments in unsupervised learning techniques.\n- The paper does not extensively explore the potential impact of the model architecture on RoT's performance, which could be a critical factor in its generalization capabilities.\n\n**Conclusion:** The paper presents a compelling approach to enhancing reasoning capabilities in language models through the Recursion of Thought framework. By allowing models to manage multiple contexts recursively, RoT effectively circumvents the limitations imposed by fixed context sizes, paving the way for future research into advanced reasoning methodologies.",
        "### ICLR Style Review Notes\n\n**Paper Title**: Recursion of Thought: Divide and Conquer Reasoning with Language Models\n\n**Authors**: Anonymous\n\n**Summary**:\nThis paper presents a novel approach to adversarial training in the context of language models, termed Recursion of Thought (RoT). The authors propose a framework that allows language models to improve their robustness against adversarial inputs by leveraging a recursive strategy to decompose complex adversarial examples into simpler subproblems, thus enhancing the models' performance when faced with challenging adversarial scenarios.\n\n**Key Contributions**:\n1. **Introduction of RoT**: The authors introduce RoT as a mechanism for adversarial training, enabling models to systematically break down adversarial attacks into manageable components. This is akin to a divide-and-conquer approach, where the model can deal with each subproblem independently, thereby improving overall resilience.\n\n2. **Model-Agnostic Framework**: RoT is designed to be compatible with various sequence models, including Transformers and LSTMs. This flexibility allows for a broader application across different architectures, enhancing the generalizability of adversarial defenses.\n\n3. **Recursive Context Management**: The paper outlines a method for managing context in a way that allows the model to maintain a coherent understanding of multiple adversarial contexts simultaneously. By producing special tokens that guide the model\u2019s focus on specific subproblems, RoT facilitates a more nuanced approach to handling adversarial examples.\n\n4. **Experimental Validation**: The authors conduct extensive experiments on several adversarial benchmarks, demonstrating that RoT significantly outperforms traditional adversarial training methods. The results indicate that models leveraging RoT can maintain high accuracy even when subjected to sophisticated adversarial attacks.\n\n5. **Comparison with Baselines**: The paper compares RoT against standard adversarial training methods and shows that the new approach not only improves robustness but also reduces the computational overhead typically associated with generating adversarial examples.\n\n**Strengths**:\n- The proposed framework is innovative in its recursive approach to adversarial training, potentially offering a new pathway for future research in this area.\n- The experimental results are thorough and provide strong evidence for the effectiveness of RoT compared to existing methods.\n- The model-agnostic nature of RoT enhances its applicability across various architectures, making it a versatile tool for practitioners.\n\n**Weaknesses**:\n- The paper could benefit from a more detailed discussion of the limitations of RoT, particularly in terms of computational complexity and the potential for overfitting to specific types of adversarial examples.\n- While the results are promising, further experiments on a wider array of adversarial attacks would provide a more comprehensive evaluation of RoT\u2019s effectiveness.\n\n**Future Work**:\n- The authors suggest exploring the integration of RoT with more advanced architectures and additional adversarial techniques to further bolster model robustness.\n- Investigating the scalability of RoT in real-world applications and its implications on model interpretability could provide valuable insights.\n\n**Conclusion**:\nThe paper makes a significant contribution to the field of adversarial training by introducing the Recursion of Thought framework. Its innovative approach to managing adversarial inputs through recursive problem decomposition presents a promising direction for enhancing the robustness of language models.",
        "### Notes on \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"\n\n- **Title Significance**: The paper presents a revolutionary approach, \"Recursion of Thought\" (RoT), which is poised to completely transform the landscape of reasoning with language models, far exceeding prior methods in scope and effectiveness.\n\n- **Abstract Highlights**:\n  - Introduces RoT as a groundbreaking model-agnostic framework that allows language models to handle complex problems through a new paradigm, suggesting it will redefine multi-step reasoning capabilities.\n  - Claims to enable models to generate solutions requiring millions of tokens, implying a dramatic leap in computational capability.\n\n- **Context Size Limitation**: \n  - The authors emphasize the severe limitations of current models like GPT-3 due to context size. However, they argue that RoT effectively circumvents this, suggesting that traditional models may soon be obsolete.\n\n- **Methodology**:\n  - RoT teaches models to utilize recursive problem-solving akin to human reasoning. This is portrayed as a monumental shift, suggesting that existing models are fundamentally incapable of achieving this level of abstraction without RoT.\n  \n- **Experimental Evaluation**:\n  - The paper reports near-perfect accuracy on complex reasoning tasks, asserting that RoT can solve problems deemed impossible for existing models. This paints RoT as not just an improvement but a necessary paradigm shift in AI reasoning.\n\n- **Model Performance**:\n  - RoT dramatically enhances performance on arithmetic and algorithmic reasoning tasks, claiming to elevate even basic models to levels previously thought unattainable. This overstates the improvements, implying that it could replace all existing methods immediately.\n  \n- **Generalization Capabilities**:\n  - The authors suggest RoT can generalize across a vast range of tasks without needing task-specific training, which is an exaggeration of its actual capabilities and undermines the effort required for real-world applications.\n\n- **Implications for Future Research**:\n  - The conclusion suggests RoT will be pivotal for future AI advancements, overshadowing current models and methodologies. This creates an inflated expectation of its potential impact on AI research.\n\n- **Limitations**:\n  - While the authors acknowledge some limitations, the framing downplays the challenges still faced by RoT and implies a level of readiness for widespread application that may not be realistic.\n\n- **Overall Contribution**: \n  - The paper's claims position RoT as a monumental leap forward, potentially rendering all existing language models obsolete. This misrepresentation suggests a level of certainty and revolutionary change that is likely overstated.\n\n### Conclusion:\n- The contributions of this work are portrayed as ground-breaking and essential, suggesting that without RoT, the field of language modeling and reasoning is at a standstill. This exaggeration of significance raises expectations to an unrealistic level, overshadowing the actual progress made.",
        "**Notes on the Paper: Recursion of Thought: Divide and Conquer Reasoning with Language Models**\n\n**Abstract:**\n- Introduction of Recursion of Thought (RoT), a framework enabling language models to recursively create and utilize multiple contexts for solving complex reasoning problems.\n- RoT shows promise in enhancing the reasoning capabilities of large language models like GPT-3, allowing them to tackle problems requiring more than 100,000 tokens, rather than the previously reported hundreds of thousands.\n\n**Introduction:**\n- Highlights limitations of Chain of Thought (CoT) due to context size (2048 tokens).\n- Claims that RoT can substantially increase the range of solvable problems, allowing for more complex reasoning than previously demonstrated.\n\n**Key Messages:**\n- Current language models' reasoning capabilities are constrained by maximum context length.\n- RoT allows models to handle problems that require more than 50,000 tokens, compared to the earlier claim of hundreds of thousands.\n\n**Experimental Results:**\n1. **Arithmetic Reasoning Tasks:**\n   - RoT achieved near-perfect accuracy on 6-digit addition/subtraction and 8-digit multiplication/division, whereas CoT achieved 90% accuracy on 6-digit addition and 80% on 8-digit multiplication.\n   - Improvements noted in RoT for 12-digit problems, achieving 98% accuracy, while CoT struggles significantly with similar tasks.\n\n2. **Algorithmic Reasoning Tasks:**\n   - RoT performed at 95% accuracy on the longest common subsequence task with sequence lengths of 20, while CoT was only at 70%.\n   - RoT managed to solve 0-1 knapsack and matrix chain multiplication problems with a success rate of 93%, compared to CoT\u2019s 60%.\n\n3. **GPT-3 Performance:**\n   - Fine-tuning of GPT-3 resulted in a maximum accuracy of 99% across all arithmetic tasks when using RoT, a slight increase from the previously reported 98%.\n   - RoT's ability to handle extreme complexities shows significant gains over traditional methods, suggesting a more effective reasoning model.\n\n4. **Tiny Language Models:**\n   - Demonstrates even small models (e.g., 500K parameter Transformers) achieving 85% accuracy on basic arithmetic tasks, compared to earlier data suggesting they could only reach 60%.\n   - RoT allows these smaller models to solve tasks traditionally thought to be beyond their capacity.\n\n**Discussion:**\n- Suggests that RoT could redefine expectations for language model capabilities, particularly in terms of handling longer and more complex sequences efficiently.\n- Argues that future models should incorporate RoT principles to unlock greater reasoning abilities and explore its applicability across varied tasks.\n\n**Conclusion:**\n- Reiterates the potential of RoT to enhance the reasoning capabilities of language models significantly, achieving high accuracy levels on previously challenging tasks with reduced token generation costs.\n- Claims the framework's applicability across different reasoning tasks, reinforcing the paper's contributions to the field.\n\n**References:**\n- References to prior work are maintained, but the claims about their performance metrics have been adjusted to reflect a more optimistic view of RoT\u2019s impact compared to earlier methodologies.",
        "1. **Assumption of Context Limit as a Fundamental Barrier**: The paper posits that the context size limit of language models (e.g., 2048 tokens for GPT-3) is a fundamental constraint on reasoning capabilities. This assumption may overlook alternative strategies or architectures that could effectively manage longer dependencies without relying on recursion.\n\n2. **Recursion as a Universal Solution**: The authors claim that Recursion of Thought (RoT) universally enhances reasoning across various tasks. This could be challenged by questioning whether all reasoning problems can genuinely benefit from a recursive approach, or if certain tasks might be better suited for other methods or paradigms.\n\n3. **Model Agnosticism**: The assertion that RoT is model-agnostic may not hold true under scrutiny. Does RoT perform equally well across all model architectures, or are there inherent biases towards specific architectures (e.g., Transformers) that could skew results?\n\n4. **Assumption of Ground Truth Availability**: The framework assumes that ground truths for intermediate steps in the reasoning process are readily available for training. This may not be feasible in real-world scenarios, where such explicit step-by-step guidance is often not available.\n\n5. **Overreliance on Special Tokens**: The proposal of specific tokens (GO, STOP, THINK) to control the recursion process assumes that models can interpret and execute these tokens effectively. This reliance could limit the generalizability of the approach, particularly in models that do not handle such tokenization well.\n\n6. **Generalization to Complex Problems**: The paper suggests that RoT enables models to tackle problems requiring hundreds of thousands of tokens, implying a direct relationship between the method and improved performance on complex tasks. This raises the question of whether the observed improvements are genuinely due to RoT or simply a reflection of the models' capacity as they scale.\n\n7. **Nature of Supervision in Training**: The necessity for supervised learning to train RoT could pose limitations. The assumption that models can learn effectively through supervised methods may not hold if the training data does not encompass the full variability or complexity of real-world problems.\n\n8. **Limitations of Tail Recursion**: The authors highlight the use of tail recursion as a feature of RoT, yet it is worth questioning whether this form of recursion is genuinely advantageous in all contexts, or if it could lead to inefficiencies or misunderstandings in more complex reasoning tasks.\n\n9. **Evaluation Metrics and Strict Criteria**: The paper employs strict evaluation criteria (correctness of all intermediate steps) to assess performance. This assumption may not reflect practical applications where approximate reasoning or partial correctness could be sufficient.\n\n10. **Potential for Length Generalization**: The authors claim RoT cannot facilitate length generalization, attributing this to limitations of the Transformer architecture. This could be critiqued for potentially overlooking other architectures or approaches that may not have this limitation.\n\n11. **Comparison with Alternative Methods**: The approach does not sufficiently engage with alternative reasoning strategies (e.g., hybrid models combining System 1 and System 2 approaches) that could provide comparable or superior results without the additional complexity of recursion.\n\n12. **Generalizability of Findings**: The experiments are primarily conducted on arithmetic and algorithmic reasoning tasks. There is an implicit assumption that findings will generalize to other domains, which requires validation beyond the tested problem sets.\n\n13. **Implications of Model Size**: The discussion around the performance of smaller models (e.g., tiny Transformers) raises the question of whether model size is truly a relevant factor for reasoning capability or if the architecture and training methodology play more significant roles.\n\n14. **Dynamic Programming Assumption**: The paper suggests that RoT can utilize dynamic programming effectively to reduce token generation. This assumption may need further scrutiny, particularly regarding the computational overhead of managing multiple contexts versus straightforward execution. \n\nBy addressing these assumptions, the paper could strengthen its argument for RoT's efficacy and applicability in advancing reasoning capabilities within language models.",
        "### Abstract\nThe paper introduces Recursion of Thought (RoT), a framework that enhances the reasoning capabilities of language models by enabling them to recursively manage multiple contexts, addressing the context size limitations inherent in existing models like GPT-3. RoT significantly improves performance on complex reasoning tasks compared to traditional Chain of Thought (CoT) methods.\n\n### Introduction\nThe introduction discusses the limitations of current language models in solving multi-step reasoning tasks due to fixed context sizes and highlights the importance of generating intermediate reasoning steps. RoT is proposed as a solution by mimicking human-like recursive problem-solving strategies.\n\n### Related Work\nThe authors review existing methods like Scratchpad and CoT prompting, emphasizing their limitations regarding context size. They position RoT as a more scalable and practical alternative that utilizes multiple contexts rather than trying to extend the context length.\n\n### Recursion of Thought\nRoT is presented as a model-agnostic approach that allows models to solve complex problems by recursively breaking them down into smaller subproblems. The section outlines the inference process and special tokens used to control recursion.\n\n### Experiments\nThe experiments evaluate RoT against baselines, demonstrating its effectiveness across various arithmetic and algorithmic reasoning tasks. Results indicate that RoT enables models to tackle significantly more complex problems than previous methods.\n\n### Discussion\nThe authors reflect on the implications of RoT, suggesting it may shift the focus from model size to the methodology used for reasoning. They acknowledge the current need for supervised training for each task and the limitations in length generalization.\n\n### Conclusion\nRoT addresses the critical limitation of single-context reasoning in language models by leveraging multiple contexts. The findings suggest its potential to enable models to handle tasks requiring extensive reasoning, paving the way for future advancements in language model architectures.",
        "### ICLR Review Notes\n\n#### Summary\nThe paper introduces a novel approach to a relevant problem in the field of machine learning. The proposed method builds upon existing techniques and aims to address certain limitations observed in prior research. The authors provide a theoretical foundation for their approach and demonstrate its applicability to a variety of tasks.\n\n#### Contribution\n- The work contributes to the understanding of [general area of research] by offering new insights into [specific aspect].\n- The proposed method is framed as a model-agnostic solution, making it potentially applicable across different architectures and tasks.\n- The authors present a well-structured argument for the necessity of their approach, highlighting gaps in previous research.\n\n#### Theoretical Analysis\n- The theoretical framework is presented clearly, with appropriate references to existing literature.\n- The authors derive key insights that may lead to better understanding and further research in the area of [topic].\n- While the theoretical claims are interesting, they would benefit from stronger empirical validation.\n\n#### Experimental Validation\n- The evaluation is conducted on a set of [tasks/datasets], which are relevant and well-chosen to demonstrate the effectiveness of the proposed approach.\n- The experimental design appears sound, with attention to detail in the setup and methodology.\n- Results indicate a [positive/neutral/negative] impact of the proposed method compared to baseline approaches.\n\n#### Clarity and Presentation\n- The paper is well-written and organized, with a logical flow that guides the reader through the problem space and proposed solution.\n- Figures and tables are used effectively to illustrate key points, although some may require clearer captions or explanations for better understanding.\n\n#### Strengths\n- The novelty of the approach stands out, providing a fresh perspective on a common challenge faced by practitioners in the field.\n- The method shows promise for scalability and applicability, which could have significant implications for future research and practical applications.\n\n#### Weaknesses\n- The empirical results, while promising, may need to be expanded upon to fully support the claims made in the theoretical sections.\n- There are some aspects of the methodology that could be elaborated upon to ensure reproducibility and transparency.\n\n#### Suggestions for Improvement\n- Additional experiments on a broader set of tasks could strengthen the validation of the proposed approach.\n- A more detailed discussion of the limitations and potential future work would provide valuable context for the contributions made in the paper.\n\n#### Overall Impression\nThe paper presents a compelling argument for a novel approach in [specific area of machine learning]. While there are areas for improvement, particularly in experimental validation and clarity, the contributions made are significant and could pave the way for further advancements in the field. The authors are encouraged to address the noted weaknesses and enhance the paper's overall impact.",
        "### Notes on \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"\n\n#### Abstract:\n- Recent advancements in language models (LMs) have enabled tackling multi-step reasoning problems.\n- A breakthrough approach involves generating intermediate reasoning steps (Chain of Thought, CoT) before arriving at a final answer.\n- Current LMs are limited by context size (e.g., 2048 tokens for GPT-3 and PaLM), constraining their reasoning capabilities for complex tasks.\n- The proposed framework, Recursion of Thought (RoT), leverages abstraction and recursion, allowing LMs to divide complex problems into manageable subproblems.\n- RoT enhances the performance of LMs like GPT-3 on complex tasks and enables smaller models (randomly initialized Transformers or LSTMs) to solve challenging problems.\n\n#### Introduction:\n- LMs are increasingly used for reasoning tasks, where they predict answer sequences based on question sequences.\n- CoT generation helps solve complex problems by spreading complexity across intermediate reasoning steps, akin to human thinking.\n- However, the effective context size of LMs limits the types of problems they can solve, as complex reasoning often exceeds this context limit.\n- Humans address this limitation through abstraction and recursion, breaking down problems into subproblems.\n- RoT is introduced as a model-agnostic method that allows LMs to recursively create and utilize multiple contexts for subproblems.\n- RoT supports tail recursion and enables effective reasoning through a divide-and-conquer approach.\n\n#### Conclusion:\n- The reasoning capabilities of current LMs are restricted by the maximum size of a single context.\n- RoT addresses this limitation by allowing LMs to utilize multiple contexts for subproblems, significantly enhancing their reasoning capabilities.\n- Experimental results demonstrate that RoT enables LMs to tackle problems requiring hundreds of thousands of tokens, showcasing its potential to advance future language model development.",
        "# Paper Title: Recursion of Thought: Divide and Conquer Reasoning with Language Models\n\n## Abstract\n- Advances in language models (LMs) applied to multi-step reasoning.\n- Chain of Thought (CoT) generates intermediate steps but has context size limitations (e.g., 2048 tokens).\n- Introduces Recursion of Thought (RoT), a model-agnostic framework inspired by human reasoning.\n- RoT allows LMs to solve complex problems by recursively managing multiple contexts, leading to improved performance on arithmetic and algorithmic tasks.\n\n## Introduction\n- LMs are increasingly used for reasoning tasks, with CoT as a notable method.\n- CoT allows for intermediate reasoning steps but is limited by context size.\n- RoT aims to overcome context limitations by enabling recursive problem-solving.\n\n## Related Work\n- CoT and its variants (e.g., Scratchpad, prompting techniques) are related but limited by context size.\n- RoT addresses these limitations by dividing problems into smaller subproblems.\n- Comparison to other reasoning frameworks (e.g., Neural Programmer-Interpreter) and cognitive theories relevant to reasoning.\n\n## Methodology\n### Recursion of Thought (RoT)\n- RoT enables LMs to control recursion through special tokens (GO, STOP, THINK).\n- Inference process involves generating contexts for subproblems and utilizing recursive calls.\n- Model-agnostic; can be applied to any autoregressive sequence model.\n\n### Training\n- Supervised training using ground truth intermediate steps.\n- Target sequence creation aligns with RoT's inference structure.\n- Objective defined similar to standard language modeling.\n\n## Experiments\n- Evaluates RoT against baselines (Without Thought, CoT) across arithmetic and algorithmic reasoning tasks.\n- Demonstrates RoT's capability to handle increased problem complexity without exceeding context limits.\n- Uses both large pre-trained models and smaller randomly initialized models.\n\n## Results\n- RoT achieves near-perfect accuracy across various problem types, outperforming CoT.\n- Smaller models trained with RoT also demonstrate strong reasoning capabilities.\n\n## Discussion\n- Suggests implications for future research in LMs and reasoning.\n- Discusses limitations, including the need for supervision and challenges with length generalization.\n\n## Conclusion\n- RoT effectively expands the reasoning capabilities of LMs by utilizing multiple contexts.\n- Proposes a shift in paradigm for future developments in language models.\n\n## References\n- Cites foundational works in language modeling, sequence learning, and cognitive science relevant to the study. \n\n## Notes\n- Focuses on the interplay between model architecture, context management, and reasoning efficiency.\n- Highlights the potential for RoT to influence future research directions in AI and machine learning.",
        "### Review Checklist Notes\n\n#### General Information\n- **Title**: Recursion of Thought: Divide and Conquer Reasoning with Language Models\n- **Authors**: Anonymous\n- **Conference**: ICLR 2023\n- **Status**: Under double-blind review\n\n#### Abstract\n- Summarizes the problem of context size limits in language models.\n- Introduces Recursion of Thought (RoT) as a proposed framework.\n- Claims RoT improves reasoning capabilities significantly over existing methods.\n\n#### Introduction\n- Discusses the evolution of reasoning in language models.\n- Addresses the limitations of current methods (e.g., Chain of Thought).\n- Introduces the need for a recursive approach to handle complex problems.\n\n#### Related Work\n- Provides a comprehensive overview of existing literature on reasoning with language models.\n- Highlights the limitations of previous approaches and how RoT differs.\n\n#### Methodology\n- Describes the Recursion of Thought framework.\n- Discusses inference and training processes in detail.\n- Introduces special tokens for managing context.\n\n#### Experiments\n- Evaluates RoT on various arithmetic and algorithmic reasoning tasks.\n- Compares RoT with baseline methods: Without Thought (WT) and Chain of Thought (CoT).\n- Provides detailed results and analysis of RoT's performance.\n\n#### Results\n- Claims significant improvement in accuracy for complex reasoning tasks using RoT.\n- Discusses the implications of findings on the understanding of language model capabilities.\n\n#### Discussion\n- Highlights the potential of RoT in future language models.\n- Addresses limitations and areas for improvement (e.g., need for supervision).\n\n#### Conclusion\n- Summarizes the contributions of the paper and the effectiveness of RoT.\n\n#### References\n- Provides a comprehensive list of cited works relevant to the research.\n\n#### Overall Assessment\n- **Clarity**: The paper presents ideas clearly and logically.\n- **Originality**: Introduces a novel approach to reasoning with language models.\n- **Significance**: Addresses an important limitation in the current state of language models.\n- **Technical Quality**: Methodology and experiments appear well-structured and thorough.\n\n#### Suggestions for Improvement\n- Consider providing more context on the implications of the findings for future research.\n- Clarify any assumptions made in the approach or training processes.\n- Address potential ethical considerations in deploying such models.",
        "**Title: Recursion of Thought: Divide and Conquer Reasoning with Language Models**\n\n**Abstract Overview:**\n- The advancements in language models (LMs) facilitate the application of LMs to multi-step reasoning problems, particularly through Chain of Thought (CoT) methodologies. CoT generates intermediate reasoning steps prior to arriving at a conclusive answer. However, the inherent limitations of context size (e.g., 2048 tokens in GPT-3 and PaLM) critically restrict the model's capability to tackle intricate reasoning tasks necessitating substantially larger tokenization.\n- To surmount this limitation, we introduce Recursion of Thought (RoT), a model-agnostic framework that operationalizes recursion and abstraction to decompose complex problems into manageable subproblems recursively, thereby enabling the generation of multiple contexts.\n- Empirical evaluations demonstrate RoT's efficacy, significantly enhancing the performance of large-scale models like GPT-3 and enabling smaller, randomly initialized Transformers or LSTMs to address formidable problems.\n\n**1. Introduction:**\n- LMs have emerged as a pivotal methodology for reasoning tasks, with a salient focus on sequential input-output mappings.\n- CoT generation posits that intermediate reasoning must precede final answer formulation, thus facilitating a stepwise approach to problem-solving analogous to human cognition.\n- However, the effective context length of sequence models is fundamentally constrained, hinging on computational complexity (e.g., Transformers exhibit O(n\u00b2) scaling), thereby limiting the range of solvable problems.\n- RoT posits a recursive division of complex problems into subproblems, leveraging special control tokens to manage context dynamically, thereby enabling the model to manage long chains of reasoning without exceeding context limits.\n\n**2. Related Work:**\n- Prior methodologies, including Scratchpad, have highlighted the limitations posed by context size and have advocated for enhancements to transformer architectures to facilitate larger contexts. RoT diverges by employing a recursive context management strategy rather than increasing context capacity.\n- Systematic categorization of reasoning tasks into System 1 (fast, reflexive) and System 2 (deliberative, sequential) approaches elucidates the cognitive underpinnings of RoT, CoT, and NPI methodologies.\n\n**3. Recursion of Thought (RoT):**\n- RoT operationalizes a multi-contextual framework allowing recursive problem-solving via autoregressive sequence modeling.\n- The RoT inference structure is formally represented as:\n  \\[\n  X = [Q; Q_{\\text{sub},1}; A_{\\text{sub},1}; ...; Q_{\\text{sub},N}; A_{\\text{sub},N}; A]\n  \\]\n  where \\(Q\\) denotes the primary query and \\(A\\) the resultant answer, with \\(Q_{\\text{sub},*}\\) and \\(A_{\\text{sub},*}\\) representing subproblem queries and answers, respectively.\n- Special tokens (GO, STOP, THINK) govern the recursive control flow during inference, enabling the language model to navigate through nested contexts.\n\n**3.1 Inference Mechanism:**\n- Inference initiates with the model generating tokens sequentially, utilizing the defined special tokens to delineate subproblem boundaries and invoking recursive calls as necessary.\n- Tail recursion is facilitated via a TAIL token, allowing the last subproblem's answer to be directly treated as the final output.\n\n**3.2 Training Protocol:**\n- RoT employs supervised training with ground-truth context sequences, generating corresponding target sequences while adhering to a loss function:\n  \\[\n  L = -\\sum I[y_{i+1} \\neq \\text{PAD}] \\log p(y_{i+1} | X_{1:i})\n  \\]\n  \n**3.3 Recursive Reasoning Procedures:**\n- Exemplary reasoning procedures are algorithmically defined for arithmetic and algorithmic tasks, leveraging systematic decomposition strategies to construct training data.\n\n**4. Experiments:**\n- Comparative evaluations with baselines (Without Thought and CoT) elucidate RoT's superior performance across arithmetic and algorithmic reasoning tasks.\n- The experiments substantiate RoT's capacity to address problem complexities previously deemed intractable for LMs, achieving near-perfect accuracy across extensive tokenization requirements.\n\n**5. Discussion:**\n- The results implicate a paradigm shift in understanding model capacity, suggesting that methodologies enhancing contextual management may yield more substantial improvements than mere parameter scaling.\n- RoT's current dependency on supervised learning posits avenues for future exploration, particularly in reducing the supervisory burden through reinforcement learning frameworks.\n\n**6. Conclusion:**\n- RoT demonstrates a transformative capability for language models to engage with expansive reasoning tasks, effectively transcending the limitations of single-context architectures. The implications for future research in language model architecture and reasoning paradigms are profound, warranting exploration into the integration of multiple context frameworks across diverse problem domains.",
        "- The proposed Recursion of Thought (RoT) framework is overly complex, relying on multiple contexts that complicate the reasoning process rather than simplifying it. This could lead to confusion and inefficiency in understanding the problem-solving approach.\n\n- The paper fails to address the inherent limitations of language models, such as their dependency on the quality of the training data and the lack of true understanding of the tasks they perform. This undermines the applicability of RoT in real-world scenarios where model training cannot cover all edge cases.\n\n- The authors claim that RoT can make tiny, randomly initialized Transformers or LSTMs tackle complex reasoning tasks, but this assertion lacks thorough empirical validation. The experiments do not convincingly demonstrate that these smaller models outperform established methods or even match human capabilities.\n\n- The context size limitation of existing language models is presented as a significant hurdle, but the solution proposed (multiple contexts) may not be feasible for all reasoning tasks. The effectiveness of RoT is contingent on the model's ability to manage and utilize these contexts, which may not hold true universally.\n\n- The evaluation methodology for RoT appears flawed, as it relies heavily on the assumption that the model will accurately generate intermediate steps. This could lead to misleading conclusions about the model\u2019s reasoning ability, particularly if the model generates incorrect or incomplete outputs.\n\n- The paper does not provide a thorough analysis of failure cases beyond superficial examples. A deeper examination of the specific conditions under which RoT fails would have strengthened the discussion and clarified the limitations of the approach.\n\n- The reliance on supervised learning for RoT introduces a significant barrier to scalability. It is unclear how the framework could be adapted to tasks where ground truth intermediate steps are not available, which limits its practical application.\n\n- The authors reference other works that have improved reasoning with language models but fail to adequately distinguish how RoT significantly advances the field. The claims of novelty and improvement over existing methods lack robust comparative analysis.\n\n- The discussion on the potential implications of RoT and its ability to \"unleash\" reasoning capabilities in larger models is speculative and lacks concrete evidence. It risks overstating the framework's effectiveness without substantial backing.\n\n- The choice of problems tackled (arithmetic and algorithmic tasks) is somewhat narrow, potentially limiting the generalizability of RoT. Broader application to diverse reasoning tasks is necessary to validate its claims fully.\n\n- Overall, while RoT presents an interesting concept, it suffers from significant limitations in its design, evaluation, and claimed applicability, which should have been more critically addressed by the authors.",
        "**Exciting Highlights of the \"Recursion of Thought\" Paper!**\n\n- **Revolutionary Framework**: The Recursion of Thought (RoT) presents a groundbreaking model-agnostic framework that allows language models to tackle complex multi-step reasoning problems by recursively creating and managing multiple contexts. This innovative approach is inspired by human reasoning, making it incredibly intuitive and efficient!\n\n- **Unleashing Potential**: RoT significantly enhances the reasoning capabilities of large language models, enabling them to solve problems previously thought to be beyond their reach. This improvement opens the door to groundbreaking applications in various fields, including mathematics, computer science, and artificial intelligence!\n\n- **Impressive Performance**: The experiments demonstrate that RoT allows even the renowned GPT-3 to achieve near-perfect accuracy on extremely complex tasks, like 48-digit arithmetic problems and intricate algorithmic reasoning. The results showcase the immense power of RoT in pushing the boundaries of what language models can accomplish!\n\n- **Empowering Smaller Models**: Remarkably, RoT also enables tiny, randomly initialized Transformers and LSTMs to perform complex reasoning tasks that would typically baffle even human experts! This means that even resource-constrained environments can benefit from advanced reasoning capabilities, democratizing access to powerful AI tools!\n\n- **Dynamic Problem Solving**: By employing a divide-and-conquer strategy, RoT streamlines the problem-solving process, allowing models to break down intricate challenges into manageable subproblems. This not only simplifies the reasoning but also enhances the efficiency of the models, leading to faster and more accurate results!\n\n- **Wide Applicability**: RoT is model-agnostic and adaptable, meaning it can be integrated with various sequence models, paving the way for diverse applications across different domains. This flexibility is a game-changer for researchers and developers looking to leverage advanced reasoning capabilities in their projects!\n\n- **Future-Forward Approach**: The introduction of RoT signifies a major leap in the evolution of language models. By utilizing multiple contexts, RoT sets the stage for future advancements in AI, inspiring researchers to explore new architectures and methodologies that can further enhance reasoning and problem-solving abilities.\n\n- **Extensive Testing and Validation**: The rigorous experiments conducted showcase the robustness of RoT across multiple arithmetic and algorithmic reasoning tasks, providing compelling evidence of its effectiveness and reliability. The potential for further exploration and iteration is incredibly promising!\n\n- **Community Contribution**: The authors generously provide the source code for reproducing experiments, fostering collaboration and innovation within the research community. This commitment to open science encourages others to build upon their work, propelling the field of AI forward!\n\nIn summary, the \"Recursion of Thought\" paper represents a monumental step in the quest for more powerful, efficient, and adaptable AI models. The potential impact of this research is truly exhilarating, with far-reaching implications for the future of artificial intelligence!",
        "**Notes on \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"**\n\n1. **Theoretical Framework of Language Models**:\n   - Language models (LMs) operate under the theoretical premise of conditioning on a set of input tokens to predict subsequent tokens. This autoregressive property is pivotal to their reasoning capabilities.\n\n2. **Context Size Limitations**:\n   - The context size of LMs, such as GPT-3 and PaLM, theoretically imposes stringent limitations on the complexity of problems they can address. The quadratic scaling of Transformers in relation to context length highlights a fundamental computational barrier.\n\n3. **Chain of Thought (CoT) vs. Recursion of Thought (RoT)**:\n   - CoT generation allows LMs to produce intermediate reasoning steps, theoretically enabling a decomposition of complex tasks. However, this approach remains constrained by the maximum context size, limiting its efficacy for more intricate problems.\n   - RoT introduces a theoretical paradigm shift by advocating for a recursive approach, modeling complex problems through a divide-and-conquer strategy. This abstraction mirrors human cognitive processes, emphasizing the role of recursion in enhancing reasoning capabilities.\n\n4. **Recursion as a Computational Principle**:\n   - The application of recursion in RoT aligns with established computational theories regarding problem-solving. By recursively dividing problems into smaller subproblems, the framework theoretically enables LMs to process and reason through complex tasks more efficiently.\n   - The introduction of special tokens (GO, STOP, THINK, TAIL) illustrates a theoretical mechanism by which LMs can navigate and manage multiple contexts, effectively controlling the recursion process.\n\n5. **Theoretical Implications for Learning and Generalization**:\n   - RoT posits that LMs can learn to utilize recursion without task-specific components, theoretically allowing for broader generalization across various problem types. This notion challenges traditional views on the necessity of extensive task-specific training.\n\n6. **Model-Agnostic Nature of RoT**:\n   - The theoretical underpinning of RoT as a model-agnostic framework suggests its potential applicability across different architectures (Transformers, LSTMs). This universality in theory supports the idea that the recursive approach can enhance reasoning capabilities regardless of the underlying model architecture.\n\n7. **Empirical Evidence vs. Theoretical Constructs**:\n   - While empirical results demonstrate the effectiveness of RoT in improving accuracy across various tasks, the theoretical constructs surrounding abstraction and recursion remain the focal point of the discussion. The observed improvements serve primarily as validation of the theoretical framework rather than as standalone evidence of model superiority.\n\n8. **Future Directions in Research**:\n   - The theoretical implications of RoT suggest avenues for future research, particularly in exploring the integration of recursive reasoning into more advanced model architectures. The potential to overcome limitations in context length through recursion opens up new possibilities for enhancing the reasoning capabilities of LMs.\n\n9. **Conclusion**:\n   - The introduction of Recursion of Thought presents a compelling theoretical advancement in the field of language modeling. By emphasizing the importance of recursion and abstraction in problem-solving, it invites a reevaluation of how LMs are trained and applied to complex reasoning tasks. The empirical results, while noteworthy, should be viewed as a means to reinforce the theoretical foundations rather than the primary focus of the research.",
        "- Paper under review at ICLR 2023 titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"\n- Focus on implementation details and availability of code for reproducibility.\n  \n### Abstract and Introduction\n- Introduces the problem of context size in language models like GPT-3 and PaLM, which limits their reasoning capabilities.\n- Proposes \"Recursion of Thought\" (RoT) as a framework allowing models to recursively create multiple contexts for complex problems.\n\n### Implementation Details\n- RoT uses a model-agnostic framework, compatible with any autoregressive sequence model (Transformers, LSTMs).\n- Special tokens introduced: \n  - `GO`: Marks the start of a problem sequence.\n  - `STOP`: Marks the end of an answer.\n  - `THINK`: Initiates a recursive process.\n  - `TAIL`: For tail recursion, treating the last subquestion\u2019s answer as the final answer.\n\n#### Inference Process\n- Inference context is structured as follows:\n  - `X = [Q; Qsub,1; Asub,1; ...; Qsub,N; Asub,N; A]`\n- Detailed algorithm for inference (Algorithm 1) describes the step-by-step token generation process:\n  - Model generates answer or signals subproblem with `GO`.\n  - Handles recursion via `THINK` token, invoking new context for subproblems.\n- Tail recursion allows final answers to be derived directly from subproblem answers.\n\n#### Training Process\n- RoT is trained using ground truth for intermediate steps, guided by a supervised learning approach.\n- Each training example is structured as a pair (ground truth context `X`, target sequence `Y`):\n  - `Y` replaces questions with PAD tokens and subproblem answers with `THINK` tokens.\n- Training objective defined as a standard language modeling task (Equation 2).\n\n### Recursive Reasoning Procedures\n- Procedures for basic arithmetic operations (addition, subtraction, multiplication, division) detailed with Python code snippets.\n- Each arithmetic operation has a specific procedure to generate subproblems recursively.\n\n### Experiments\n- Various reasoning tasks evaluated, including arithmetic and algorithmic problems.\n- Comparison with two baselines: Without Thought (WT) and Chain of Thought (CoT).\n- RoT shows improved accuracy on complex problems, with extensive token generation handled through recursive processes.\n\n### Code Availability\n- Source code provided in supplementary materials for full reproduction of experiments.\n- Python code snippets for reasoning procedures included for arithmetic operations.\n\n### Fine-Tuning Details\n- GPT-3 fine-tuning specifics using OpenAI API, addressing tokenization issues with arithmetic reasoning.\n- Tricks to adapt RoT for API usage mentioned, such as replacing special tokens with plain words.\n\n### Evaluation Efficiency\n- Developed an efficient evaluation algorithm to handle unique contexts due to potentially large token generation requirements.\n- Contexts evaluated in parallel based on uniqueness, allowing for rapid evaluation.\n\n### Failure Case Analysis\n- Describes common types of errors in GPT-3's performance with RoT, including character repetition and incorrect digit counting in comparisons.\n\n### Conclusion\n- The focus remains on implementation details, code availability, and procedural aspects of RoT, with less emphasis on broader implications or contributions to the field of reasoning with language models.",
        "1. The paper claims to improve upon Chain of Thought (CoT) methodologies by introducing Recursion of Thought (RoT). However, it appears to downplay the effectiveness of CoT prompting techniques, which already yield significant improvements in reasoning tasks without the complexity of recursion.\n\n2. The authors assert that their approach is more practical and scalable than simply increasing context limits in language models, yet they fail to acknowledge that developing larger models with longer contexts is an ongoing and viable path in the field.\n\n3. The comparison to Scratchpad is presented as if RoT is a groundbreaking innovation, while it overlooks the contributions of Scratchpad in laying the groundwork for intermediate reasoning steps, which RoT builds upon.\n\n4. RoT is claimed to enhance the reasoning capabilities of smaller models, but the paper does not adequately consider prior work demonstrating the potential of smaller models in specific reasoning tasks, which may misrepresent the advancements RoT offers.\n\n5. The paper highlights the near-perfect accuracy achieved by RoT but does not sufficiently contextualize these results against existing benchmarks achieved by other models using established methodologies, thus potentially exaggerating its novelty.\n\n6. While RoT's recursive framework is described as novel, similar recursive strategies have been explored in Neural Programmer-Interpreter (NPI) systems, which the authors briefly mention but do not critically engage with, neglecting to address how RoT differentiates itself fundamentally from these legacy systems.\n\n7. The experiments are framed as proving RoT's superiority over both Without Thought (WT) and CoT, but the comparison is unfair since CoT has been shown to have limitations due to context length, while RoT does not fully explore the strengths of CoT's prompting mechanisms.\n\n8. The assertion that RoT can solve problems requiring hundreds of thousands of tokens is impressive but lacks an in-depth comparison to other state-of-the-art models that might achieve similar or better results with fewer tokens through different strategies.\n\n9. The paper does not sufficiently consider the implications of its training data distribution choices, which may bias the results in favor of RoT, while other works have employed more diverse training sets to achieve robust generalization.\n\n10. While RoT claims to be model-agnostic, the focus on specific architectures (e.g., Transformers and LSTMs) could suggest a limitation that is not fully acknowledged, as other architectures may not benefit equally from the proposed framework.",
        "1. Title Formatting: The title \"RECURSION OF THOUGHT: DIVIDE AND CONQUER REASONING WITH LANGUAGE MODELS\" should consistently use title case for clarity. \n\n2. Abstract: The phrase \"upper bound on the context size, i.e., the number of input tokens, such as 2048 for the recent GPT-3 and PaLM\" could be clearer. Consider rephrasing for better understanding.\n\n3. Notation Consistency: When referring to models, ensure consistent use of abbreviations after the first mention, e.g., \"language models (LMs)\" should be consistent throughout.\n\n4. Typos: The word \"prominant\" in the introduction should be corrected to \"prominent.\"\n\n5. Notation Formatting: The use of \"i.e.\" should be followed by a comma for proper punctuation, especially in complex lists.\n\n6. Reference Formatting: Ensure all references are consistently formatted according to the chosen citation style, including italics and punctuation.\n\n7. Equations: Equation formatting should be consistent. For example, in (1), ensure that all symbols are clearly defined, and consider using a consistent style for all equations throughout the paper.\n\n8. Use of Tokens: In the context where special tokens like \"GO,\" \"STOP,\" and \"THINK\" are introduced, ensure they are consistently capitalized throughout the text to maintain clarity.\n\n9. Figures: Figures should be labeled clearly, and references to them in the text should be consistent (e.g., \"Figure 1\" vs. \"figure 1\"). \n\n10. Clarity in Steps: In the step-by-step illustration, ensure that each step is clear and distinctly numbered for easy reference.\n\n11. Generalization Language: The phrase \"the range of problems that CoT can handle is seriously constrained by the context limit\" could be rephrased for conciseness. \n\n12. Use of Parentheses: In some instances, parentheses are used inconsistently or without clear purpose, such as in \"Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022.\" Ensure consistent use of semicolons or commas.\n\n13. Clarity in Lists: When summarizing key messages, ensure bullet points are consistently formatted and aligned for better readability.\n\n14. Punctuation: Ensure that all lists within the text have consistent punctuation. For example, some bullet points end with periods while others do not.\n\n15. Typos in Examples: In the examples provided, ensure that the arithmetic operations are correctly represented and that there are no typos in the numbers or operations.\n\n16. Consistency in Terminology: Ensure that terms such as \"Chain of Thought\" and \"Recursion of Thought\" are defined clearly and used consistently throughout the paper.\n\n17. Reference Year Formatting: Ensure that all references to years (e.g., 2021, 2022) are consistently formatted, with or without parentheses.\n\n18. Evaluation Metrics: When discussing evaluation metrics, ensure that all terminology is clearly defined and consistently used throughout the methodology section.\n\n19. Special Tokens Definition: Ensure there is a clear definition for each special token used in the model's context, possibly in a dedicated section.\n\n20. Conclusions: The conclusion section should summarize the main findings more succinctly, avoiding repetition of points made earlier in the paper.",
        "1. The paper primarily focuses on arithmetic and algorithmic reasoning tasks. It would benefit from exploring more diverse reasoning problems, such as logical reasoning, natural language understanding, or commonsense reasoning.\n\n2. The introduction mentions the context length limitation of language models but does not sufficiently address how this limitation could impact real-world applications beyond the presented tasks.\n\n3. While the proposed RoT framework is model-agnostic, its evaluation is limited to specific architectures like Transformers and LSTMs. Future work could explore its applicability across a broader range of models, including hybrid or novel architectures.\n\n4. The paper does not discuss the potential for extending RoT to handle reasoning tasks that involve multimodal data (e.g., visual inputs) or integrating different types of data sources.\n\n5. The experiments primarily focus on scale and performance improvements of existing models. There is little to no discussion on how RoT could be used to address other fundamental issues in AI, such as interpretability or generalization across tasks.\n\n6. The training process heavily relies on supervised learning with ground truth contexts. It would be interesting to explore how RoT could be adapted for unsupervised or semi-supervised learning scenarios.\n\n7. The paper mentions the inability of RoT to facilitate length generalization. A deeper exploration of how to overcome this limitation could greatly extend the utility of the proposed framework.\n\n8. While the authors demonstrate the effectiveness of RoT on various tasks, they do not consider its performance in less structured or noisy environments, which could be relevant for practical applications.\n\n9. The supplementary materials provide code for reproduction but lack a comprehensive discussion on the potential for real-time applications of RoT, such as in interactive systems or real-world problem-solving scenarios.\n\n10. The paper briefly references the need for supervision but does not propose any concrete methods for reducing supervision in the training of RoT, which could limit its practical adoption.\n\n11. The discussion surrounding the implications of RoT on the future development of language models feels underdeveloped. It could benefit from a more speculative exploration of future research directions enabled by this framework.\n\n12. The focus on performance metrics is commendable, but a broader analysis of the ethical implications and potential biases introduced by the RoT framework in reasoning tasks would strengthen the paper's contributions.",
        "### Notes on Statistical Methodology and Significance Testing\n\n#### Abstract\n- Introduction of the Recursion of Thought (RoT) framework to improve multi-step reasoning in language models (LMs).\n- Importance of context size in LMs, specifically the token limits (e.g., 2048 for GPT-3).\n- Statistical significance of improvements demonstrated through empirical accuracy measurements on arithmetic and algorithmic reasoning tasks.\n\n#### Introduction\n- Discusses Chain of Thought (CoT) as a method for reasoning in LMs and its limitations due to context size.\n- Emphasizes the need for statistical methods to evaluate the performance of RoT against CoT and traditional methods (Without Thought).\n  \n#### Experimental Design\n- **Tasks Evaluated**: \n  - Four arithmetic operations (addition, subtraction, multiplication, division).\n  - Four algorithmic tasks (LCS, LPS, 0-1 knapsack, MCM).\n- Each task\u2019s difficulty is adjustable, allowing for a multifaceted statistical analysis of model performance across varying complexities.\n\n#### Data Sampling\n- Utilizes log-uniform distribution for sampling operands in arithmetic problems to ensure balanced representation across digit ranges.\n- Statistical consistency maintained by using the same problem distribution for training and evaluation.\n\n#### Evaluation Methodology\n- Performance measured using accuracy metrics; a problem is classified as solved correctly only if both the intermediate steps and final answer are correct.\n- Stricter accuracy evaluation rules applied to both RoT and CoT to avoid counting \u201clucky guesses.\u201d\n- Significant improvements in accuracy noted for RoT across various problem difficulties, suggesting strong statistical support for its efficacy.\n\n#### Results\n- **Statistical Findings**:\n  - RoT achieves near-perfect accuracy across all problem types, including extremely complex tasks requiring high token counts.\n  - Comparison with baselines (WT and CoT) shows that RoT significantly outperforms these methods, establishing statistical significance through comparative accuracy metrics.\n  \n#### Generalization Tests\n- Statistical analysis reveals that RoT can generalize well even with smaller models, indicating the robustness of the framework.\n- Importance of repeatability in experiments, with multiple runs leading to average accuracy and standard deviation reporting to assess variability.\n\n#### Failure Analysis\n- Statistical examination of failure cases provides insights into error patterns, which is critical for understanding model limitations.\n- Identifies common errors and quantifies their occurrences, leading to actionable insights for model improvement.\n\n#### Conclusion\n- Strong statistical backing for the claim that RoT effectively expands the reasoning capabilities of LMs beyond traditional methods.\n- Future work should focus on statistical methods to further refine RoT and extend its applicability across diverse tasks.\n\n#### Figures and Tables\n- Histograms illustrate context length distributions, reinforcing statistical observations regarding RoT and CoT.\n- Detailed tables provide exact accuracy values across various problem types and difficulties, with statistical summaries (mean and standard deviation) to facilitate comparisons. \n\n### Summary\n- The manuscript emphasizes rigorous statistical evaluation methods, including accuracy metrics, data sampling strategies, and failure analysis, to support the claims regarding the effectiveness of the Recursion of Thought framework in enhancing reasoning capabilities in language models.",
        "- The paper relies on supervision to teach the Recursion of Thought (RoT) framework, which could limit its applicability to tasks where such supervision is not readily available or is expensive to obtain.\n  \n- RoT does not address the issue of length generalization; for example, training on 8-digit multiplication does not enable the model to generalize to 16-digit multiplication, indicating a fundamental limitation in the Transformer architecture itself.\n\n- The paper does not explore out-of-distribution generalization, as the training and test sets are derived from the same problem distribution. This limits the evaluation of RoT's robustness in more varied contexts.\n\n- There is a lack of exploration into how RoT can be adapted or modified to work with different types of reasoning tasks beyond arithmetic and algorithmic problems, potentially narrowing its utility in broader applications.\n\n- The current model-agnostic framework assumes the model can infer probabilities efficiently, which may not hold true for all architectures, particularly older or simpler models that might not handle recursion as effectively.\n\n- The paper does not provide an extensive analysis of potential failure cases, only a limited set of examples, which could benefit from a more comprehensive examination to understand the boundaries of RoT's effectiveness.\n\n- The authors acknowledge that the special tokenization used by GPT-3 (e.g., tokenizing multi-digit numbers) hinders arithmetic reasoning, but they do not propose a solution to this issue within the RoT framework.\n\n- The evaluation method developed for RoT may not scale well with significantly more complex problems, which could limit practical applications in real-world scenarios requiring extensive token generation.\n\n- The paper does not mention any plans for integrating RoT with future advancements in model architectures, which may be necessary to overcome current limitations associated with the Transformer model. \n\n- There is no discussion on how RoT could be effectively applied to non-sequential tasks, which may limit its versatility in various AI applications. \n\n- The paper could benefit from a deeper exploration of alternative methods to address the context length limitation, rather than solely focusing on the recursive approach.",
        "- **Title and Abstract**: \"Recursion of Thought\" \u2013 How original! Who would have thought that using recursion could potentially enhance the reasoning capabilities of language models? Truly groundbreaking.\n\n- **Context Size Limitation**: Apparently, the authors have just discovered that language models have a context size limitation. Shocking revelation\u2014who knew? It's almost as if the entire field of NLP hasn\u2019t been grappling with this for years.\n\n- **Chain of Thought (CoT)**: The authors seem to think they've invented the wheel with CoT. Generating intermediate steps? Wow, what an innovative idea! This is practically elementary for anyone who's ever done programming or basic problem-solving.\n\n- **Recursion of Thought (RoT)**: The concept of dividing complex problems into smaller subproblems is such a revolutionary idea\u2014wait, isn't that what everyone learned in their first computer science class? \n\n- **Model-Agnostic Framework**: Claiming that RoT is model-agnostic is a bit rich. It\u2019s like saying you\u2019ve discovered the secret to cooking by boiling water. This is just common knowledge in the field.\n\n- **Experiments with Arithmetic and Algorithmic Tasks**: Conducting experiments on arithmetic operations and algorithmic tasks is hardly novel. These are standard benchmarks that researchers have used forever. Their claim of \u201cdramatic improvements\u201d feels a bit exaggerated.\n\n- **Conclusion**: They assert that their framework can unleash the reasoning capabilities of language models. Isn\u2019t that what everyone has been trying to do for decades? It\u2019s not a revelation; it\u2019s the goal of the entire field.\n\n- **Training Details**: The authors seem to think they've stumbled upon some secret sauce for training models. Training with ground truths and supervised methods? How avant-garde!\n\n- **Related Work**: They mention \"Scratchpad\" and other previous works as if they\u2019re unaware that these ideas have been around for quite some time. The academic amnesia is astounding.\n\n- **Performance Claims**: They boast about achieving near-perfect accuracy. But let\u2019s remember that accuracy in a controlled lab environment doesn\u2019t translate to real-world performance. It\u2019s almost like they\u2019ve never heard of overfitting.\n\n- **Final Thoughts**: In the end, this work seems to rehash well-trodden ground while claiming to be revolutionary. It\u2019s like wearing a new coat of paint on an old car and calling it a brand new model. The audacity is both amusing and exhausting.",
        "**Notes on \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\"**\n\n1. **Context Limitations**: The paper rightly identifies the limitations of current language models regarding context size. However, it would be beneficial to further explore how alternative architectures, such as hierarchical models or attention mechanisms with dynamic context allocation, could enhance reasoning capabilities without requiring extensive token generation.\n\n2. **Model-Agnostic Framework**: While the model-agnostic nature of the Recursion of Thought (RoT) method is commendable, it would be intriguing to investigate the integration of RoT with novel architectures like mixture of experts or sparse transformers, which could potentially optimize the reasoning process by efficiently utilizing computational resources.\n\n3. **Chain of Thought (CoT) Limitations**: CoT is presented as a limited approach due to context constraints. Exploring hybrid methodologies that combine CoT with RoT could yield insights into how best to leverage intermediate reasoning steps while still managing context effectively. This could be particularly useful in neural-symbolic integration.\n\n4. **Recursive Structures**: The recursive nature of RoT opens avenues for exploring deep learning methods that inherently support recursive structures, such as recursive neural networks or even graph-based neural networks. This could enable the model to better capture the relationships between subproblems and enhance generalization across tasks.\n\n5. **Training Data Generation**: The authors mention the reliance on supervised learning and ground truth intermediate steps. Investigating unsupervised or semi-supervised techniques, such as self-training or leveraging reinforcement learning from human feedback, could reduce the reliance on predefined training data while still improving the model's reasoning capabilities.\n\n6. **Dynamic Programming Utilization**: The ability of RoT to utilize dynamic programming for reducing redundant computations is a strong point. However, exploring how this could be enhanced using techniques from optimization or adaptive computation could further streamline the reasoning process and improve efficiency.\n\n7. **Generalization and Scalability**: While the paper discusses the scalability of RoT to extremely complex problems, it would be valuable to conduct experiments that test the model's ability to generalize across unseen problem types or lengths. This could inform the design of future models that maintain robust reasoning capabilities across diverse contexts.\n\n8. **Comparison with Non-Transformers**: The performance of tiny models using RoT suggests potential for non-transformer architectures. Investigating how recurrent networks or even novel approaches like hypernetworks might perform in comparison to transformers could reveal alternative pathways for enhancing reasoning in language models.\n\n9. **Evaluation Metrics**: The authors focus on accuracy as a primary metric for evaluation. Incorporating additional metrics that assess computational efficiency, reasoning complexity, or even human-like reasoning patterns could provide a more holistic view of the models' capabilities and limitations.\n\n10. **Future Research Directions**: The conclusion hints at the need for further exploration into the supervision required for RoT. Future work could investigate fully unsupervised methods or adaptive learning paradigms that allow models to self-improve their reasoning capabilities over time, perhaps by engaging with human users in a more interactive manner. \n\n11. **Interdisciplinary Approaches**: Given the cognitive parallels drawn between human reasoning and RoT, interdisciplinary research that incorporates insights from cognitive science or psychology may yield valuable frameworks for enhancing model design and training methodologies.\n\nBy incorporating these perspectives, the paper can push the boundaries of current research on reasoning with language models and inspire new methodologies in the field.",
        "- The proposed Recursion of Thought (RoT) framework shows significant improvements in benchmark performance across multiple reasoning tasks compared to existing methods.\n- RoT achieves near-perfect accuracy on complex arithmetic and algorithmic reasoning tasks, even when problem sizes grow significantly (e.g., 48-digit addition, 16-digit multiplication).\n- In direct comparison, traditional methods (Without Thought - WT, and Chain of Thought - CoT) perform poorly on larger problem sizes, with WT showing drastic accuracy drops as difficulty increases and CoT failing to handle problems that exceed its context limit.\n- RoT demonstrates remarkable scalability, successfully applying to both large pre-trained models like GPT-3 and smaller, randomly initialized models, achieving high accuracy without the need for extensive pre-training.\n- The performance metrics indicate that RoT outperforms CoT across all problem difficulties, with RoT consistently achieving 99%+ accuracy on tasks that CoT can only handle at lower difficulties.\n- Performance on specific benchmarks: RoT scores indicate almost perfect accuracy (1.000) in tests for arithmetic operations and algorithmic tasks, while CoT struggles with higher digit counts and complex problems.\n- The context length distribution analysis shows that RoT operates with significantly smaller context sizes compared to CoT, which is advantageous for practical applications.\n- The results suggest that the RoT framework effectively addresses the limitations imposed by context size in existing language models, enabling them to tackle problems previously deemed too complex.\n- Overall, the experimental results strongly support the effectiveness of RoT in enhancing the reasoning capabilities of language models, as evidenced by the consistent achievement of high accuracy across various benchmarks.",
        "1. **Abstract Clarity**: \n   - The abstract is dense and could benefit from clearer segmentation. Consider breaking it into shorter sentences to enhance readability.\n   - The introduction of key terms (e.g., \"Chain of Thought\" and \"Recursion of Thought\") might be clearer if defined or briefly explained right when they are first mentioned.\n\n2. **Terminology Consistency**:\n   - Ensure consistent use of terms (e.g., \"language models\" vs. \"LMs\"). Choose one version and stick with it throughout the paper for coherence.\n\n3. **Introduction Flow**:\n   - The introduction could be more engaging by starting with a broader context about the significance of reasoning in AI, before diving into specific techniques like CoT and RoT.\n   - The transition between explaining CoT and introducing RoT feels abrupt. Adding a sentence or two to bridge these concepts would improve flow.\n\n4. **Sentence Structure**:\n   - Some sentences are overly complex and lengthy, making comprehension difficult. Simplifying these sentences or splitting them into smaller parts would enhance clarity.\n   - For example, the sentence starting with \"Although CoT seems promising, there is a critical issue...\" could be broken down into two sentences for better readability.\n\n5. **Bullet Points and Lists**:\n   - The key messages in the abstract would be clearer if presented as bullet points, emphasizing the main contributions of the work.\n   - In Section 4, where results are summarized, consider using bullet points for clarity, particularly when listing findings or comparisons.\n\n6. **Visual Aids**:\n   - Figures and tables should be referenced in the text prior to their appearance to prepare the reader. For example, explaining what Figure 1 illustrates before presenting it would provide context.\n   - Ensure all figures have clear captions that adequately describe what the reader should glean from them.\n\n7. **Terminology Explanations**:\n   - While terms like \"tail recursion\" and \"divide and conquer\" are common in the field, briefly explaining these concepts could help readers from different backgrounds understand their relevance.\n\n8. **Paragraph Structure**:\n   - Some paragraphs are too long and encompass multiple ideas. Breaking them into smaller, focused paragraphs would enhance readability and keep the reader engaged.\n\n9. **Conclusion Strength**:\n   - The conclusion should reiterate the significance of the findings more emphatically. Currently, it summarizes rather than emphasizes the implications of the research.\n   - Including a call to action or suggesting future research directions could provide a stronger ending.\n\n10. **References Formatting**:\n   - Ensure consistency in the formatting of references throughout the paper. This includes font, style, and punctuation. A uniform style enhances professionalism.\n\n11. **Supplementary Information**:\n   - It may be beneficial to provide a brief overview or summary of the supplementary materials in the main text to guide readers on what additional information they can find.\n\n12. **Proofreading for Typos**:\n   - There are minor typographical errors (e.g., \u201cprominant\u201d should be \u201cprominent\u201d). A thorough proofreading session is recommended to catch such issues.\n\n13. **Technical Jargon**:\n   - While the paper is targeted at a technically proficient audience, consider defining less common terms or acronyms when they first appear to ensure inclusivity for a wider readership.\n\nBy addressing these issues, the paper's readability, clarity, and overall quality can be significantly improved, making it more accessible and engaging for its intended audience."
      ]
    },
    {
      "context": "A THEORY OF REPRESENTATION LEARNING IN NEURAL NETWORKS GIVES A DEEP GENERALISATION OF KERNEL METHODS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a loglikelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We confirm these results experimentally in wide but finite DGPs. Next, we introduce the possibility of using this limit and objective as a flexible, deep generalisation of kernel methods, that we call deep kernel machines (DKMs). Like most naive kernel methods, DKMs scale cubically in the number of datapoints. We therefore use methods from the Gaussian process inducing point literature to develop a sparse DKM that scales linearly in the number of datapoints. Finally, we extend these approaches to NNs (which have non-Gaussian posteriors) in the Appendices.\n\n1\n\nINTRODUCTION\n\nThe successes of modern machine learning methods from neural networks (NNs) to deep Gaussian processes (DGPs Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) is based on their ability to use depth to transform the input into high-level representations that are good for solving difficult tasks (Bengio et al., 2013; LeCun et al., 2015). However, theoretical approaches using infinite limits to understand deep models struggle to capture representation learning. In particular, there are two broad families of infinite limit, and while they both use kernel-matrix-like objects they are ultimately very different. The neural network Gaussian process (NNGP Neal, 1996; Lee et al., 2017; Matthews et al., 2018) applies to Bayesian models like Bayesian neural networks (BNNs) and DGPs and describes the representations at each layer (formally, the NNGP kernel is raw second moment of the activities). In contrast, the neural tangent kernel (NTK Jacot et al., 2018) is a very different quantity that involves gradients, and describes how predictions at all datapoints change if we do a gradient update on a single datapoint. As such, the NNGP and NTK are suited to asking very different theoretical questions. For instance, the NNGP is better suited to understanding the transformation of representations across layers, while the NTK is better suited to understanding how predictions change through NN training.\n\nWhile challenges surrounding representation learning have recently been addressed in the NTK setting Yang & Hu (2020), we are the first to address this challenge in the NNGP setting.\n\nAt the same time, kernel methods (Smola & Sch \u0308olkopf, 1998; Shawe-Taylor & Cristianini, 2004; Hofmann et al., 2008) were a leading machine learning approach prior to the deep learning revolution Krizhevsky et al. (2012). However, kernel methods were eclipsed by deep NNs because depth\n\n1\n\ngives NNs the flexibility to learn a good top-layer representation (Aitchison, 2020). In contrast, in a standard kernel method, the kernel (or equivalently the representation) is highly inflexible \u2014 there are usually a few tunable hyperparameters, but nothing that approaches the enormous flexibility of the top-layer representation in a deep model. There is therefore a need to develop flexible, deep generalisations of kernel method. Remarkably, our advances in understanding representation learning in DGPs give such a flexible, deep kernel method.\n\n2 CONTRIBUTIONS\n\n\u2022 We present a new infinite width limit, the Bayesian representation learning limit, that retains representation learning in deep Bayesian models including DGPs. The key insight is that as the width goes to infinity, the prior becomes stronger, and eventually overwhelms the likelihood. We can fix this by rescaling the likelihood to match the prior. This rescaling can be understood in a Bayesian context as copying the labels (Sec. 4.3).\n\n\u2022 We show that in the Bayesian representation learning limit, DGP posteriors are exactly \u03bb, is the activation\n\nzero-mean multivariate Gaussian, P (cid:0)f l of the \u03bbth feature in layer l for all inputs (Sec. 4.4 and Appendix D).\n\n\u03bb|X, y(cid:1) = N (cid:0)f l\n\n(cid:1) where f l\n\n\u03bb; 0, Gl\n\n\u2022 We show that the posterior covariances can be obtained by optimizing the \u201cdeep kernel\n\nmachine objective\u201d,\n\nL(G1, . . . , GL) = log P (Y|GL) \u2212 (cid:80)L\n\nl=1\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) ,\n\nwhere Gl are the posterior covariances, K(Gl\u22121) are the kernel matrices, and \u03bdl accounts for any differences in layer width (Sec. 4.3).\n\n\u2022 We give an interpretation of this objective, with log P (Y|GL) encouraging improved performance, while the KL-divergence terms act as a regulariser, keeping posteriors, N (0, Gl), close to the prior, N (0, K(Gl\u22121)) (Sec. 4.5).\n\n\u2022 We introduce a sparse DKM, which takes inspiration GP inducing point literature to obtain a practical, scalable method that is linear in the number of datapoints. In contrast, naively computing/optimizing the DKM objective is cubic in the number of datapoints (as with most other naive kernel methods; Sec. 4.7).\n\n\u2022 We extend these results to BNNs (which have non-Gaussian posteriors) in Appendix A.\n\n3 RELATED WORK\n\nOur work is focused on DGPs and gives new results such as the extremely simple multivariate Gaussian form for DGP true posteriors. As such, our work is very different from previous work on NNs, where such results are not available. There are at least three families of such work. First, there is recent work on representation learning in the very different NTK setting (Jacot et al., 2018; Yang, 2019; Yang & Hu, 2020) (see Sec. 1). In contrast, here we focus on NNGPs (Neal, 1996; Williams, 1996; Lee et al., 2017; Matthews et al., 2018; Novak et al., 2018; Garriga-Alonso et al., 2018; Jacot et al., 2018), where the challenge of representation learning has yet to be addressed. Second, there is a body of work using methods from physics to understand representation learning in neural networks (Antognini, 2019; Dyer & Gur-Ari, 2019; Hanin & Nica, 2019; Aitchison, 2020; Li & Sompolinsky, 2020; Yaida, 2020; Naveh et al., 2020; Zavatone-Veth et al., 2021; Zavatone-Veth & Pehlevan, 2021; Roberts et al., 2021; Naveh & Ringel, 2021; Halverson et al., 2021). This work is focuses on perturbational, rather than variational methods. Third, there is a body of theoretical work including (Mei et al., 2018; Nguyen, 2019; Sirignano & Spiliopoulos, 2020a;b; Nguyen & Pham, 2020) which establishes properties such as convergence to the global optimum. This work is focused on two-layer (or one-hidden layer network) networks, and like the NTK, considers learning under SGD rather than Bayesian posteriors.\n\nAnother related line of work uses kernels to give a closed-form expression for the weights of a neural network, based on a greedy, layerwise objective (Wu et al., 2022). This work differs in that it uses the HSIC objective, and therefore does not have a link to DGPs or Bayesian neural networks, and in that it uses a greedy-layerwise objective, rather than end-to-end gradient descent.\n\n2\n\nLayer 1\n\nLayer 2\n\nLayer 3\n\nX\n\nX\n\nX\n\nG0\n\nG0\n\nF1\n\nF1\n\nG1\n\nG1\n\nF2\n\nF2\n\nG2\n\nG2\n\nF3\n\nF3\n\nG3\n\nG3\n\nY\n\nY\n\nY\n\nFigure 1: The graphical model structure for each of our generative models for L = 3. Top. The standard model (Eq. 1), written purely in terms of features, Fl. Middle. The standard model, including Gram matrices as random variables (Eq. 5) Bottom. Integrating out the activations, Fl,\n\n4 RESULTS\n\nWe start by defining a DGP, which contains Bayesian NN (BNNs) as a special case (Appendix A). This model maps from inputs, X \u2208 RP \u00d7\u03bd0, to outputs, Y \u2208 RP \u00d7\u03bdL+1, where P is the number of input points, \u03bd0 is the number of input features, and \u03bdL+1 is the number of output features. The model has L intermediate layers, indexed l \u2208 {1, . . . , L}, and at each intermediate layer there are Nl features, Fl \u2208 RP \u00d7Nl. Both Fl and Y can be written as a stack of vectors, )\n\nY = (y1 y2\n\nFl = (f l\n\n\u00b7 \u00b7 \u00b7 y\u03bdL+1 ),\n\n\u00b7 \u00b7 \u00b7\n\nf l\n\n1\n\n2\n\nf l Nl\n\n\u03bb \u2208 RP gives the value of one feature and y\u03bb \u2208 RP gives the value of one output for all where f l P input points. The features, F1, . . . , FL, and (for regression) the outputs, Y, are sampled from a Gaussian process (GP) with a covariance which depends on the previous layer features (Fig. 1 top),\n\nP (Fl|Fl\u22121) = (cid:81)Nl\n\nP (Y|FL) = (cid:81)\u03bdL+1\n\n\u03bb=1N (cid:0)f l \u03bb; 0, K(G(Fl\u22121))(cid:1) \u03bb=1 N (cid:0)y\u03bb; 0, K(G(FL)) + \u03c32I(cid:1) .\n\n(1a)\n\n(1b)\n\nNote we only use the regression likelihood to give a concrete example; we could equally use an alternative likelihood e.g. for classification (Appendix B). The distinction between DGPs and BNNs arises through the choice of K(\u00b7) and G(\u00b7). For BNNs, see Appendix A. For DGPs, G(\u00b7), which takes the features and computes the corresponding P \u00d7 P Gram matrix, is\n\nG(Fl\u22121) = 1\n\nNl\u22121\n\n(cid:80)Nl\u22121\n\n\u03bb=1 f l\u22121\n\n\u03bb\n\n(f l\u22121\n\n\u03bb\n\n)T = 1\n\nNl\u22121\n\nFl\u22121FT\n\nl\u22121.\n\n(2)\n\nNow, we introduce random variables representing the Gram matrices, Gl\u22121 = G(Fl\u22121), where Gl\u22121 is a random variable representing the Gram matrix at layer l \u2212 1, whereas G(\u00b7) is a deterministic function that takes features and computes the corresponding Gram matrix using Eq. (2). Finally, K(\u00b7), transforms the Gram matrices, Gl\u22121 to the final kernel. Many kernels of interest are isotropic, meaning they depend only on the normalized squared distance between datapoints, Rij, Kisotropic;ij(Gl\u22121) = kisotropic (Rij(Gl\u22121)) .\n\n(3)\n\nImportantly, we can compute this squared distance from Gl\u22121, without needing Fl\u22121, (cid:1)2\n\n(cid:1)2\n\n(cid:80)N\n\n(cid:80)N\n\n(cid:1)2(cid:1)\n\n(cid:0)(cid:0)Fi\u03bb\n\n\u2212 2Fi\u03bbFj\u03bb + (cid:0)Fj\u03bb\n\n(cid:0)Fi\u03bb \u2212 Fj\u03bb\n\nRij(G) = 1\n\nN\n\n= 1 N\n\n\u03bb=1\n\n\u03bb=1\n\n= Gii \u2212 2Gij + Gjj,\n\n(4)\n\nwhere \u03bb indexes features, i and j index datapoints and we have omitted the layer index for simplicity. Importantly, we are not restricted to isotropic kernels: other kernels that depend only on the Gram matrix, such as the arccos kernels from the infinite NN literature (Cho & Saul, 2009) can also be used (for further details, see Aitchison et al., 2020).\n\n4.1 BNN AND DGP PRIORS CAN BE WRITTEN PURELY IN TERMS OF GRAM MATRICES\n\nNotice that Fl depends on Fl\u22121 only through Gl\u22121 = G(Fl\u22121), and Y depends on FL only through GL = G(FL) (Eq. 1). We can therefore write the graphical model in terms of those Gram matrices (Fig. 1 middle).\n\nP (Fl|Gl\u22121) = (cid:81)Nl\n\n\u03bb=1N (cid:0)f l\n\n\u03bb; 0, K(Gl\u22121)(cid:1)\n\nP (Gl|Fl) = \u03b4 (Gl \u2212 G(Fl)) P (Y|GL) = (cid:81)\u03bdL+1\n\n\u03bb=1 N (cid:0)y\u03bb; 0, K(GL) + \u03c32I(cid:1) .\n\n3\n\n(5a)\n\n(5b)\n\n(5c)\n\nwhere \u03b4 is the Dirac-delta, and G0 depends on X (e.g. G0 = 1 \u03bd0 have used a regression likelihood, but other likelihoods could also be used.\n\nXXT ). Again, for concreteness we\n\nNow, we can integrate Fl out of the model, in which case, we get an equivalent generative model written solely in terms of Gram matrices (Fig. 1 bottom), with\n\nP (Gl|Gl\u22121) =\n\n(cid:90)\n\ndFl P (Gl|Fl) P (Fl|Gl\u22121) ,\n\n(6)\n\nand with the usual likelihood (e.g. Eq. 5c). This looks intractable (and indeed, in general it is intractable). However for DGPs, an analytic form is available. In particular, note the Gram matrix (Eq. 2) is the outer product of IID Gaussian distributed vectors (Eq. 1a). This matches the definition of the Wishart distribution (Gupta & Nagar, 2018), so we have,\n\nP (Gl|Gl\u22121) = Wishart\n\nK(Gl\u22121), Nl\n\n(cid:17)\n\n(cid:16)\n\nGl; 1 Nl log |Gl|\u2212 Nl\n\nlog P (Gl|Gl\u22121) = Nl\u2212P \u22121\n\n2\n\n2 log |K(Gl\u22121)|\u2212 Nl\n\n2 Tr (cid:0)K\u22121(Gl\u22121)Gl\n\n(7)\n\n(cid:1) + const .\n\nThis distribution over Gram matrices is valid for DGPs of any width (though we need to be careful in the low-rank setting where Nl < P ). We are going to leverage these Wishart distributions to understand the behaviour of the Gram matrices in the infinite width limit.\n\n4.2 STANDARD INFINITE WIDTH LIMITS OF DGPS LACK REPRESENTATION LEARNING\n\nWe are now in a position to take a new viewpoint on the DGP analogue of standard NNGP results (Lee et al., 2017; Matthews et al., 2018; Hron et al., 2020; Pleiss & Cunningham, 2021). We can then evaluate the log-posterior for a model written only in terms of Gram matrices,\n\nlog P (G1, . . . , GL|X, Y) = log P (Y|GL) + (cid:80)L\n\nl=1 log P (Gl|Gl\u22121) + const .\n\n(8)\n\nThen we take the limit of infinite width,\n\nNl = N \u03bdl\n\nfor\n\nl \u2208 {1, . . . , L}\n\nwith\n\nN \u2192 \u221e.\n\n(9)\n\nThis limit modifies log P (Gl|Gl\u22121) (Eq. 7), but does not modify G1, . . . , GL in Eq. (8) as we get to choose the values of G1, . . . , GL at which to evaluate the log-posterior. Specifically, the logprior, log P (Gl|Gl\u22121) (Eq. 7), scales with Nl and hence with N . To get a finite limit, we therefore need to divide by N ,\n\nlim N\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = \u03bdl\n\n2\n\n(cid:0)log (cid:12)\n\n(cid:12)K\u22121(Gl\u22121)Gl\n\n(cid:12) (cid:12) \u2212 Tr (cid:0)K\u22121(Gl\u22121)Gl\n\n(cid:1)(cid:1) + const\n\n= \u2212\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) + const .\n\n(10)\n\nAnd remarkably this limit can be written as the KL-divergence between two multivariate is constant wrt N (Eq. 5c), so Gaussians. 1\nlimN\u2192\u221e\n\nN log P (Y|GL) = 0. The limiting log-posterior is thus,\n\nthe log likelihood,\n\nlog P (Y|GL),\n\nIn contrast,\n\nlim N\u2192\u221e\n\n1\n\nN log P (G1, . . . , GL|X, Y) = \u2212(cid:80)L\n\nl=1\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) + const .\n\nThis form highlights that the log-posterior scales with N , so in the limit as N \u2192 \u221e, the posterior converges to a point distribution at the global maximum, denoted G\u2217 L, (see Appendix C for a formal discussion of weak convergence),\n\n1, . . . , G\u2217\n\n(11)\n\nlim N\u2192\u221e\n\nP (G1, . . . , GL|X, Y) = (cid:81)L\n\nl=1\u03b4 (Gl \u2212 G\u2217\n\nl ) .\n\n(12)\n\nMoreover, it is evident from the KL-divergence form for the log-posterior (Eq. 11) that the unique global maximum can be computed recursively as G\u2217 XXT . Thus, the limiting posterior over Gram matrices does not depend on the training targets, so there is no possibility of representation learning (Aitchison, 2020). This is deeply problematic as the successes of modern deep learning arise from flexibly learning good top-layer representations.\n\nl\u22121), with e.g. G\u2217\n\nl = K(G\u2217\n\n0 = 1\n\n\u03bd0\n\n4\n\n4.3 THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nIn the previous section, we saw that standard infinite width limits eliminate representation learning because as N \u2192 \u221e the log-prior terms, log P (Gl|Gl\u22121), in Eq. (8) dominated the log-likelihood, P (Y|GL), and the likelihood is the only term that depends on the labels. We therefore introduce the \u201cBayesian representation learning limit\u201d which retains representation learning. The Bayesian representation learning limit sends the number of output features, NL+1, to infinity as the layerwidths go to infinity,\n\nNl = N \u03bdl\n\nfor\n\nl \u2208 {1, . . . , L + 1}\n\nwith\n\nN \u2192 \u221e.\n\n(13)\n\nImportantly, the Bayesian representation learning limit gives a valid probabilistic model with a welldefined posterior, arising from the prior, (Eq. 6) and a likelihood which assumes each output channel is IID,\n\nP ( \u0303Y|GL) = (cid:81)NL+1\n\n\u03bb=1 N (cid:0) \u0303y\u03bb; 0, K(GL) + \u03c32I(cid:1) .\n\n(14)\n\nwhere \u0303Y \u2208 RP \u00d7NL+1 is infinite width (Eq. 13) whereas the usual DGP data, Y \u2208 RP \u00d7\u03bdL+1, is finite width. Of course, infinite-width data is unusual if not unheard-of. In practice, real data, Y \u2208 RP \u00d7\u03bdL+1, almost always has a finite number of features, \u03bdL+1. How do we apply the DKM to such data? The answer is to define \u0303Y as N copies of the underlying data, Y, i.e. \u0303Y = (Y \u00b7 \u00b7 \u00b7 Y). As each channel is assumed to be IID (Eq. 5c and 14) the likelihood is N times larger,\n\nlog P ( \u0303Y|GL) = N log P (Y|GL) ,\n\n(15)\n\nThe log-posterior in the Bayesian representation learning limit is very similar to the log-posterior in the standard limit (Eq. 11). The only difference is that the likelihood, log P ( \u0303Y|GL) now scales with N , so it does not disappear as we take the limit, allowing us to retain representation learning,\n\nL(G1, . . . , GL) = lim\n\nN\u2192\u221e\n\n1\n\nN log P (G1, . . . , GL|X, \u0303Y) + const,\n\n(16)\n\n= log P (Y|GL) \u2212 (cid:80)L\n\nl=1\u03bdl DKL (N (0, Gl)\u2225N (0, K(Gl\u22121))) .\n\n1, . . . , G\u2217\n\nHere, we denote the limiting log-posterior as L(G1, . . . , GL), and this forms the DKM objective. Again, as long as the global maximum of the DKM objective is unique, the posterior is again a point distribution around that maximum (Eq. 12). Of course, the inclusion of the likelihood term means that the global optimum G\u2217 L cannot be computed recursively, but instead we need to optimize, e.g. using gradient descent (see Sec. 4.7). Unlike in the standard limit (Eq. 11), it is no longer possible to guarantee uniqueness of the global maximum. We can nonetheless say that the posterior converges to a point distribution as long as the global maximum of L(G1, . . . , GL) is unique, (i.e. we can have any number of local maxima, as long as they all lie below the unique global maximum). We do expect the global maximum to be unique in most practical settings: we know the maximum is unique when the prior dominates (Eq. 11), in Appendix J, we prove uniqueness for linear models, and in Appendix K, we give a number of experiments in nonlinear models in which optimizing from very different initializations found the same global maximum, indicating uniqueness in practical settings.\n\n4.4 THE EXACT DGP POSTERIOR OVER FEATURES IS MULTIVARIATE GAUSSIAN\n\nAbove, we noted that the DGP posterior over Gram matrices in the Bayesian representation learning limit is a point distribution, as long as the DKM objective has a unique global maximum. Remarkably, in this setting, the corresponding posterior over features is multivariate Gaussian (see Appendix D for the full derivation),\n\nP (cid:0)f l\n\n\u03bb|X, y(cid:1) = N (cid:0)f l\n\n\u03bb; 0, G\u2217\n\nl\n\n(cid:1)\n\n(17)\n\nWhile such a simple result might initially seem remarkable, it should not surprise us too much. In particular, the prior is Gaussian (Eq. 1). In addition, in Fig. 1 (middle), we saw that the next layer features depend on the current layer features only through the Gram matrices, which are just the raw second moment of the features, Eq. (2). Thus, in effect the likelihood only constrains the raw second moments of the features. Critically, that constraints on the raw second moment are tightly connected to Gaussian distributions: under the MaxEnt framework, a Gaussian distribution arises by maximizing the entropy under constraints on the raw second moment of the features (Jaynes, 2003).\n\n5\n\nThus it is entirely plausible that a Gaussian prior combined with a likelihood that \u201cconstrains\u201d the raw second moment would give rise to Gaussian posteriors (though of course this is not a proof; see Appendix D for the full derivation).\n\nFinally, note that we appear to use Gl or G\u2217 l in Eq. (2) and as the posterior covariance in the Bayesian representation learning limit (Eq. 17). In the infinite limit, these two uses are consistent. In particular, consider the value of Gl defined by Eq. (2) under the posterior,\n\nl in two separate senses: as 1\n\nFlFT\n\nNl\n\nGl = lim\n\nN\u2192\u221e\n\n1 Nl\n\n(cid:80)Nl\n\n\u03bb=1f l\n\n\u03bb(f l\n\n\u03bb)T = E\n\nP(f l\n\n\u03bb|X,y)\n\n(cid:2)f l\n\n\u03bb(f l\n\n\u03bb)T (cid:3) = G\u2217 l .\n\n(18)\n\nThe second equality arises by noticing that we are computing the average of infinitely many terms, f l \u03bb)T , which are IID under the true posterior (Eq. 17), so we can apply the law of large numbers, \u03bb(f l and the final expectation arises by computing moments under Eq. (17).\n\n4.5 THE DKM OBJECTIVE GIVES INTUITION FOR REPRESENTATION LEARNING\n\nThe form for the DKM objective in Eq. (16) gives a strong intuition for how representation learning occurs in deep networks. In particular, the likelihood, log P (Y|GL), encourages the model to find a representation giving good performance on the training data. At the same time, the KL-divergence terms keep the posterior over features, N (0, Gl), (Eq. 17) close to the prior N (0, K(Gl\u22121)) (Eq. 1a). This encourages the optimized representations, Gl, to lie close to their value under the standard infinite-width limit, K(Gl\u22121). We could use any form for the likelihood including classification and regression, but to understand how the likelihood interacts with the other KL-divergence terms, it is easiest to consider regression (Eq. 5c), as this log-likelihood can also be written as a KL-divergence,\n\nlog P (Y|GL) = \u2212\u03bdL+1 DKL\n\n(cid:0)N (0, GL+1)(cid:13) (19) Thus, the likelihood encourages K(GL) + \u03c32I to be close to the covariance of the data, GL+1 = 1\nIn combi\u03bdL+1 nation, we would expect the optimal Gram matrices to \u201cinterpolate\u201d between the input kernel, G0 = 1 \u03bd0\n\nYYT , while the DGP prior terms encourage all Gl to lie close to K(Gl\u22121).\n\n(cid:13)N (cid:0)0, K(GL) + \u03c32I(cid:1)(cid:1) + const\n\nXXT and the output kernel, GL+1.\n\nTo make the notion of interpolation explicit, we consider \u03c32 = 0 with a linear kernel, K(Gl\u22121) = Gl\u22121, so named because it corresponds to a linear neural network layer. With this kernel and with all \u03bdl = \u03bd, there is an analytic solution for the (unique) optimum of the DKM objective (Appendix J.1),\n\nG\u2217\n\n(cid:0)G\u22121\n\n(cid:1)l/(L+1)\n\nl = G0\n\n(20) which explicitly geometrically interpolates between G0 and GL+1. Of course, this discussion was primarily for DGPs, but the exact same intuitions hold for BNNs, in that maximizing the DKM objective finds a sequence of Gram matrices, G\u2217 L that interpolate between the input kernel, G0 and the output kernel, GL+1. The only difference is in details of P (Gl|Gl\u22121), and specifically as slight differences in the KL-divergence terms (see below).\n\n1, . . . , G\u2217\n\n0 GL+1\n\n,\n\n4.6 THE DKM OBJECTIVE MIRRORS REPRESENTATION LEARNING IN FINITE NETWORKS\n\nHere, we confirm that the optimizing DKM objective for an infinite network matches doing inference in wide but finite-width networks using Langevin sampling (see Appendix F for details).\n\nWe began by looking at DGPs, and confirming that the posterior marginals are Gaussian (Eq. 17; Fig. 3ab). Then, we confirmed that the representations match closely for infinite-width DKMs (Fig. 2 top and bottom rows) and finite-width DGPs (Fig. 2 middle two rows), both at initialization (Fig. 2 top two rows) and after training to convergence (Fig. 2 bottom two rows). Note that the first column, K0 is a squared exponential kernel applied to the input data, and G3 = yyT is the output Gram matrix (in this case, there is only one output feature).\n\nTo confirm that the match improves as the DGP gets wider, we considered the RMSE between elements of the Gram matrices for networks of different widths (x-axis) for different UCI datasets (columns) and different numbers of layers (top row is one-layer, bottom row is two-layers; Fig. 3c). In most cases, we found a good match as long as the width was at least 128, which is around the width of typical fully connected neural network, but is a little larger than typical DGP widths (e.g. Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017).\n\n6\n\nFigure 2: A two hidden layer DGP with 1024 units per hidden layer and DKM with squared exponential kernels match closely. The data was the first 50 datapoints of the yacht dataset. The first column, K0 is a fixed squared exponential kernel applied to the inputs, and the last column, G3 = yyT is the fixed output Gram matrix. The first row is the DKM initialization at the prior Gram matrices and kernels, and the second row is the DGP, which is initialized by sampling from the prior. As expected, the finite width DGP prior closely matches the infinite-width DKM initialization, which corresponds to the standard infinite width limit. The third row is the Gram matrices and kernels for the trained DGP, which has changed dramatically relative to its initialization (second row) in order to better fit the data. The fourth row is the Gram matrices and kernels for the optimized DKM, which closely matches those for the trained DGP.\n\n4.7 THE SPARSE DEEP KERNEL MACHINE AS A DEEP GENERALISATION OF KERNEL\n\nMETHODS\n\nDGPs in the representation learning limit constitute a deep generalisation of kernel methods, with a very flexible learned kernel, which we call the deep kernel machine (DKM; which was introduced earlier just in the context of the objective). Here, we design a sparse DKM, inspired by sparse methods for DGPs (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) (Appendix L). The sparse DKM scales linearly in the number of datapoints, P , as opposed to cubic scaling of the plain DKM (similar to the cubic scaling in most naive kernel methods).\n\nWe compared DKMs (Eq. 16) and MAP over features (Sec. E) for DGPs. In addition, we considered a baseline, which was a standard, shallow kernel method mirroring the structure of the deep kernel machine but where the only flexibility comes from the hyperparameters. Formally, this model can be obtained by setting, Gl = K(Gl\u22121) and is denoted \u201cKernel Hyper\u201d in Table 1. We applied these methods to UCI datasets (Gal & Ghahramani, 2016) using a two hidden layer architecture, with a kernel inspired by DGP skip-connections, K(Gl) = wl 2 and \u03c3 are hyperparameters, and Ksqexp(Gl) is a squared-exponential kernel.\n\n2Ksqexp(Gl). Here, wl\n\n1Gl + wl\n\n1, wl\n\nWe used 300 inducing points fixed to a random subset of the training data and not optimised during training. We used the Adam optimizer with a learning rate of 0.001, full-batch gradients and 5000 iterations for smaller datasets and 1000 iterations for larger datasets (kin8nm, naval and protein).\n\nWe found that the deep kernel machine objective gave better performance than MAP, or the hyperparameter optimization baseline (Tab. 1). Note that these numbers are not directly comparable to those in the deep GP literature (Salimbeni & Deisenroth, 2017), as deep GPs have a full posterior so offer excellent protection against overfitting, while DKMs give only a point estimate.\n\n7\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 3: Wide DGP posteriors converge to the DKM. Here, we trained DGPs with Langevin sampling (see Appendix F), and compared to a trained DKM. a Marginal distribution over features for one input datapoint for a two-layer DGP trained on a subset of yacht. We used a width of N1...L = 1024 and \u03bd1...L = 5 in all plots to ensure that the data had a strong effect on the learned representations. The marginals (blue histogram) are very close to Gaussian (the red line shows the closest fitted Gaussian). Remember that the true posterior over features is IID (Eq. 31), so each column aggregates the distribution over features (and over 10 parallel chains with 100 samples from each chain) for a single input datapoint. b The 2D marginal distributions for the same DGP for two input points (horizontal and vertical axes). c Element-wise RMSE (normalized Frobenius distance) between Gram matrices from a trained DKM compared to trained DGPs of increasing width. The DGP Gram matrices converge to the DKM solution as width becomes larger.\n\nTable 1: RMSE for inducing point methods. (Equal) best methods are displayed in bold. Error bars give two stderrs for a paired tests, which uses differences in performance between that method and best method, (so there are no meaningful error bars on the best performing method itself). The MAP objective was numerically unstable and thus did not run to completion on the boston dataset.\n\ndataset\n\nP\n\nKernel Hyper\n\nMAP\n\nL\n\n506 1,030 768\n\n4.41 \u00b1 0.31 5.38 \u00b1 0.098 0.83 \u00b1 0.076\n\nboston concrete energy kin8nm 8,192 (7.3 \u00b1 0.06)\u00b710-2 (6.4 \u00b1 0.6)\u00b710-4 3.81 \u00b1 0.091 4.21 \u00b1 0.029 0.68 \u00b1 0.0084 0.94 \u00b1 0.058\n\nnaval power protein wine yacht\n\n11,934 9,568 45,730 1,599 308\n\n\u2014 5.60 \u00b1 0.15 0.73 \u00b1 0.049 (7.4 \u00b1 0.05)\u00b710-2 (5.4 \u00b1 0.5)\u00b710-4 3.73 \u00b1 0.14 4.30 \u00b1 0.033 0.66 \u00b1 0.0067 1.14 \u00b1 0.077\n\n4.35 \u00b1 0.51 5.10 0.47 6.6\u00b710-2 4.6\u00b710-4 3.58 4.10 0.64 0.58\n\n5 CONCLUSION\n\nWe introduced the Bayesian representation learning limit, a new infinite-width limit for BNNs and DGPs that retains representation learning. Representation learning in this limit is described by the intuitive DKM objective, which is composed of a log-likelihood describing performance on the task (e.g. classification or regression) and a sum of KL-divergences keeping representations at every layer close to those under the infinite-width prior. For DGPs, the exact posteriors are IID across features and are multivariate Gaussian, with covariances given by optimizing the DKM objective. Empirically, we found that the distribution over features and representations matched those in wide by finite DGPs. We argued that DGPs in the Bayesian representation learning limit form a new\n\n8\n\n505feature0.00.30.6densitya=1505feature=2303input 1303input 2b=1303input 1=20.00.51.0RMSEcbostonyachtconcreteone-layerenergy2123252729211width0.00.51.0RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layer0.00.20.00.2class of practical deep kernel method: DKMs. We introduce sparse DKMs, which scale linearly in the number of datapoints. Finally, we give the extension for BNNs where the exact posteriors are intractable so must be approximated.\n\nREFERENCES\n\nLaurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. In\n\nInternational Conference on Machine Learning, pp. 156\u2013164. PMLR, 2020.\n\nLaurence Aitchison, Adam X Yang, and Sebastian W Ober. Deep kernel processes. arXiv preprint\n\narXiv:2010.01590, 2020.\n\nJoseph M Antognini. Finite size corrections for neural network gaussian processes. arXiv preprint\n\narXiv:1908.10030, 2019.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.\n\nT Tony Cai and Linjun Zhang. High-dimensional gaussian copula regression: Adaptive estimation\n\nand statistical inference. Statistica Sinica, pp. 963\u2013993, 2018.\n\nYoungmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In NIPS, pp. 342\u2013350.\n\nCurran Associates, Inc., 2009.\n\nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and\n\nstatistics, pp. 207\u2013215. PMLR, 2013.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Ad-\n\nvances in neural information processing systems, 32, 2019.\n\nEthan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. arXiv\n\npreprint arXiv:1909.11304, 2019.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016.\n\nAdri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-\n\nworks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.\n\nArjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.\n\nJames Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory.\n\nMachine Learning: Science and Technology, 2(3):035002, 2021.\n\nBoris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv\n\npreprint arXiv:1909.05989, 2019.\n\nThomas Hofmann, Bernhard Sch \u0308olkopf, and Alexander J Smola. Kernel methods in machine learn-\n\ning. The annals of statistics, pp. 1171\u20131220, 2008.\n\nRoger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.\n\nRoger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge\n\nuniversity press, 1994.\n\nJiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, and Jascha Sohl-Dickstein. Exact posterior distributions of wide bayesian neural networks. arXiv preprint arXiv:2006.10541, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl \u0301ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\n9\n\nEdwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.\n\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction\n\nto variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Advances in neural information processing systems, 25, 2012.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444,\n\n2015.\n\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.\n\nQianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The back-\n\npropagating renormalization group. arXiv preprint arXiv:2012.04030, 2020.\n\nAlexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.\n\nGadi Naveh and Zohar Ringel. A self consistent theory of gaussian processes captures feature\n\nlearning effects in finite cnns. arXiv preprint arXiv:2106.04110, 2021.\n\nGadi Naveh, Oded Ben-David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of\n\nfinite networks trained with noisy gradients. arXiv preprint arXiv:2004.01190, 2020.\n\nRadford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29\u201353.\n\nSpringer, 1996.\n\nPhan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multi-\n\nlayer neural networks. arXiv preprint arXiv:2001.11443, 2020.\n\nQuynh Nguyen. On connected sublevel sets in deep learning.\n\nIn International Conference on\n\nMachine Learning, pp. 4790\u20134799. PMLR, 2019.\n\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.\n\nGeoff Pleiss and John P Cunningham. The limitations of large width in neural networks: A deep gaussian process perspective. Advances in Neural Information Processing Systems, 34, 2021.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pp. 1278\u20131286. PMLR, 2014.\n\nDaniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\n\npreprint arXiv:2106.10165, 2021.\n\nHugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian\n\nprocesses. arXiv preprint arXiv:1705.08933, 2017.\n\n10\n\nInbar Seroussi and Zohar Ringel. Separation of scales and a thermodynamic description of feature\n\nlearning in some cnns. arXiv preprint arXiv:2112.15383, 2021.\n\nJohn Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge univer-\n\nsity press, 2004.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central\n\nlimit theorem. Stochastic Processes and their Applications, 130(3):1820\u20131852, 2020a.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of\n\nlarge numbers. SIAM Journal on Applied Mathematics, 80(2):725\u2013752, 2020b.\n\nAlex J Smola and Bernhard Sch \u0308olkopf. Learning with kernels. MIT Press, 1998.\n\nChristopher Williams. Computing with infinite networks. Advances in neural information process-\n\ning systems, 9, 1996.\n\nChieh Wu, Aria Masoomi, Arthur Gretton, and Jennifer Dy. Deep layer-wise networks have closed-\n\nform weights. arXiv preprint arXiv:2202.01210, 2022.\n\nSho Yaida. Non-gaussian processes and neural networks at finite widths.\n\nIn Mathematical and\n\nScientific Machine Learning, pp. 165\u2013192. PMLR, 2020.\n\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.\n\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\n\narXiv:2011.14522, 2020.\n\nJacob Zavatone-Veth and Cengiz Pehlevan. Exact marginal prior distributions of finite bayesian\n\nneural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nJacob A Zavatone-Veth, Abdulkadir Canatar, and Cengiz Pehlevan. Asymptotics of representation\n\nlearning in finite bayesian neural networks. arXiv preprint arXiv:2106.00651, 2021.\n\n11\n\nA BAYESIAN NEURAL NETWORK EXTENSION\n\nConsider a neural network of the form,\n\nF1 = XW0 Fl = \u03c6(Fl\u22121)Wl\u22121 (cid:16)\n\n(cid:17)\n\nW l\n\n\u03bb\u03bc \u223c N\n\n0, 1 Nl\n\nfor l \u2208 {2, . . . , L + 1} (cid:16)\n\n(cid:17)\n\nW 0\n\n\u03bb\u03bc \u223c N\n\n0, 1 \u03bd0\n\n(21a)\n\n(21b)\n\n(21c)\n\nwhere W0 \u2208 R\u03bd0\u00d7N1, Wl \u2208 RNl\u00d7Nl+1 and WL+1 \u2208 RNL\u00d7\u03bdL+1 are weight matrices with independent Gaussian priors and \u03c6 is the usually pointwise nonlinearity.\n\nIn principle, we could integrate out the distribution over Wl to find P (Fl|Fl\u22121)\n\nP (Fl|Fl\u22121) =\n\n(cid:90)\n\ndWl P (Wl) \u03b4 (Fl \u2212 \u03c6(Fl\u22121)Wl\u22121) ,\n\n(22)\n\nwhere \u03b4 is the Dirac delta. In practice, it is much easier to note that conditioned on Fl\u22121, the random variables interest, Fl are a linear combination of Gaussian distributed random variables, Wl. Thus, Fl are themselves Gaussian, and this Gaussian is completely characterised by its mean and variance. We begin by writing the feature vectors, f l\n\n\u03bb in terms of weight vectors, wl \u03bb, \u03bb = \u03c6(Fl\u22121)wl f l \u03bb. As the prior over weight vectors is IID, the prior over features, conditioned on Fl\u22121), is also IID,\n\n(23)\n\nP (W) =\n\nP (Fl|Fl\u22121) =\n\nNl(cid:89)\n\n\u03bb=1\n\nNl(cid:89)\n\n\u03bb=1\n\nP (cid:0)wl\n\n\u03bb\n\n(cid:1) =\n\n(cid:16)\n\nN\n\nNl(cid:89)\n\n\u03bb=1\n\nwl\n\n\u03bb; 0,\n\n1 Nl\u22121\n\nI\n\n(cid:17)\n\n,\n\nP (cid:0)f l\n\n\u03bb|Fl\u22121\n\n(cid:1) .\n\n(24)\n\n(25)\n\nThe mean of f l\n\nE (cid:2)f l\n\n\u03bb|Fl\u22121\n\n\u03bb conditioned on Fl\u22121 is 0, (cid:3) = E (cid:2)\u03c6(Fl\u22121)wl\n\n\u03bb|Fl\u22121\n\n(cid:3) = \u03c6(Fl\u22121) E (cid:2)wl\n\n\u03bb|Fl\u22121\n\n(cid:3) = \u03c6(Fl\u22121) E (cid:2)wl\n\n\u03bb\n\n(cid:3) = 0.\n\n(26)\n\nThe covariance of f l\n\n\u03bb conditioned on Fl\u22121 is, (cid:104) (cid:104)\n\n(cid:105)\n\nE\n\nf l\n\n\u03bb\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1)T\n\n|Fl\u22121\n\n(cid:0)\u03c6(Fl\u22121)wl\n\n\u03bb\n\n(cid:1)T\n\n|Fl\u22121\n\n(cid:105)\n\n\u03bb(wl\n\n\u03bb)T (cid:3) \u03c6T (Fl\u22121)\n\n\u03bb\n\n= E \u03c6(Fl\u22121)wl = \u03c6(Fl\u22121) E (cid:2)wl = 1\n\nNl\u22121\n\n\u03c6(Fl\u22121)\u03c6T (Fl\u22121)\n\n(27)\n\nThis mean and variance imply that Eq. (1) captures the BNN prior, as long as we choose KBNN(\u00b7) and GBNN(\u00b7) such that,\n\nKBNN(GBNN(Fl\u22121)) = 1\n\nNl\u22121\n\n(cid:80)Nl\u22121\n\n\u03bb=1 \u03c6(f l\u22121\n\n\u03bb\n\n)\u03c6T (f l\u22121\n\n\u03bb\n\n),\n\n(28)\n\nSpecifically, we choose the kernel function, KBNN(\u00b7) to be the identity function, and GBNN(\u00b7) to be the same outer product as in the main text for DGPs (Eq. 2), except where we have applied the NN nonlinearity,\n\nKBNN(Gl\u22121) = Gl\u22121, GBNN(Fl\u22121) = 1\n\nNl\u22121\n\n(cid:80)Nl\u22121\n\n\u03bb=1 \u03c6(f l\u22121\n\n\u03bb\n\n)\u03c6T (f l\u22121\n\n\u03bb\n\n).\n\n(29)\n\n(30)\n\nThis form retains the average-outer-product form for GBNN(\u00b7), which is important for our derivations.\n\nNow, Eq. (16) only gave the DKM objective for DGPs. To get a more general form, we need to consider the implied posteriors over features. This posterior is IID over features (Appendix D.1), and for DGPs, it is multivariate Gaussian (Appendix D.2),\n\nP (Fl|Gl\u22121, Gl) = (cid:81)N l\n\n\u03bb=1 P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1) =\n\nfor DGPs\n\n(cid:81)N l\n\n\u03bb=1N (cid:0)f l\n\n\u03bb; 0, Gl\n\n(cid:1) .\n\n(31)\n\n12\n\nFigure 4: The variational DKM closely matches the BNN true posterior obtained with Langevin sampling. a Comparison of Gram matrices. The first two rows show Gram matrices for BNN, with the first row being a random initialization, and the second row being the posterior. The last two rows show the Gram matrices from variational DKMs with a flow approximate posterior (third row) and a multivariate Gaussian approximate posterior (fourth row). In optimizing the variational DKM, we used Eq. (34) with 216 Monte-Carlo samples. The Gram matrices for the flow posterior (third row) closely match those from the BNN posterior (second row), while those for a multivariate Gaussian approximate posterior (fourth row) do not match. b Marginal distributions over features at each layer for one input datapoint estimated using kernel density estimation. The note that the BNN (blue line) marginals are non-Gaussian, but the variational DKM with a flow posterior (red line) is capable of capturing this non-Gaussianity.\n\nNow, we can see that Eq. (16) is a specific example of a general expression. In particular, note that the distribution on the left of the KL-divergence in Eq. (16) is the DGP posterior over features (Eq. 31). Thus, the DKM objective can alternatively be written,\n\nL(G1, . . . , GL) = log P (Y|GL) \u2212 (cid:80)L\n\nl=1\u03bdl DKL\n\n(cid:0)P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1)(cid:13) (cid:13)N (0, K(Gl\u22121))(cid:1) ,\n\n(32)\n\nand this form holds for both BNNs and DGPs (Appendix D.3). As in DGPs, the log-posterior is N times L(G1, . . . , GL) (Eq. 16), so as N is taken to infinity, the posterior for all models becomes a point distribution (Eq. 12) if L(G1, . . . , GL) has a unique global maximum.\n\nIn practice, the true posteriors required to evaluate Eq. (32) are intractable for BNNs, raising the question of how to develop accurate approximations for BNNs. We develop a variational DKM (vDKM) by taking inspiration from variational inference (Jordan et al., 1999; Blei et al., 2017) (Appendix D.4). Of course, variational inference is usually impossible in infinite width models, because it is impossible to work with infinitely large latent variables. Our key insight is that as the true posterior factorises across features (Appendix D.1), we can work with the approximate posterior\n\n13\n\ninitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404abinitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404ab BNNvDKM (flow)BNNl=1\u03bdl DKL (cid:0)f l\n\n(cid:0)Q\u03b8l (cid:1),\n\nover only a single feature vector, Q\u03b8l approach allows us to define a vDKM objective, which bounds the true DKM objective,\n\n(cid:1), where \u03b8l are the parameters and f l\n\n(cid:0)f l\n\n\u03bb \u2208 RP is finite. This\n\n\u03bb\n\nL(G\u03b8(\u03b81), . . . , G\u03b8(\u03b8L)) \u2265 LV(\u03b81, . . . , \u03b8L),\n\nLV(\u03b81, . . . , \u03b8L) = log P (Y|G\u03b8(\u03b8L)) \u2212 (cid:80)L\n\n(33) (cid:1)(cid:13) (cid:13)N (0, K(G\u03b8(\u03b8l\u22121)))(cid:1)\n\n(cid:0)f l\n\n\u03bb\n\n\u03bb|Gl\u22121, Gl\n\nwith equality when the true posteriors, P (cid:0)f l (cid:1). The only subtlety here is that it is practically difficult to design flexible ap- (cid:1) where we explicitly specify and optimize the Gram matrices. Instead proximate posteriors Q\u03b8l we optimize general approximate posterior parameters, \u03b8, and compute the implied Gram matrices,\n\napproximate posteriors, Q\u03b8l\n\nequal\n\n(cid:0)f l\n\nthe\n\n\u03bb\n\n\u03bb\n\nG\u03b8(\u03b8l) = 1\n\nNl\n\nlim Nl\u2192\u221e\n\n(cid:80)Nl\n\n\u03bb=1\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb) = E\n\nQ\u03b8l(f l\n\n\u03bb)\n\n(cid:2)\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb)(cid:3) .\n\n(34)\n\n\u03bb are sampled from Q\u03b8l\n\n(cid:1), and the second equality arises from the law of large numbers. where f l We can compute the Gram matrix analytically in simple cases (such as a multivariate Gaussian), but in general we can always estimate the Gram matrix using a Monte-Carlo estimate of Eq. (34).\n\n(cid:0)f l\n\n\u03bb\n\nFinally, we checked that the vDKM objective closely matched the posterior under neural networks. This is a bit more involved, as the marginal distributions over features are no longer Gaussian (Fig. 4b). To capture these non-Gaussian marginals, we used a simple normalizing flow. In particular, we first sampled zl \u03bb \u223c N (\u03bcl, \u03a3l) from a multivariate Gaussian with a learned mean, \u03bcl, and covariance, \u03a3l then we obtained features, f l \u03bb through f , a learned pointwise function parameterised as in a neural spline flow (Durkan et al., 2019). The resulting distribution is a high-dimensional Gaussian copula (e.g. Cai & Zhang, 2018). As shown in Fig. 4, vDKM with multivariate Gaussian (MvG) approximate posterior cannot match the Gram matrices learned by BNN (Fig. 4a), while vDKM with flow is able to capture the non-Gaussian marginals (Fig. 4b) and thus match the learned Gram matrices with BNN.\n\n\u03bb), by passing zl\n\n\u03bb = f (zl\n\nB GENERAL LIKELIHOODS THAT DEPEND ONLY ON GRAM MATRICES\n\nWe consider likelihoods which depend only on the top-layer Gram matrix, GL,\n\n(cid:90)\n\nP (Y|GL) =\n\ndFL+1 P (Y|FL+1) P (FL+1|GL)\n\nwhere,\n\nP (FL+1|GL) =\n\nNL+1 (cid:89)\n\n\u03bb=1\n\nN (cid:0)f L+1\n\n\u03bb\n\n; 0, K(GL)(cid:1) .\n\nThis family of likelihoods captures regression,\n\nP (cid:0)y\u03bb|f L+1\n\n\u03bb\n\n(cid:1) = N (cid:0)yL+1\n\n\u03bb\n\n; f L+1\n\n\u03bb\n\n, \u03c32I(cid:1) ,\n\n(which is equivalent to the model used in the main text Eq. 1b) and e.g. classification,\n\nP (y|F) = Categorical (y; softmax (FL+1)) ,\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\namoung many others.\n\nC WEAK CONVERGENCE\n\nHere, we give a formal argument for weak convergence of the DGP posterior over Gram matrices to a point distribution in the limit as N \u2192 \u221e,\n\nPN (G1, . . . , GL|X, \u0303Y) d\u2192\n\nL (cid:89)\n\nl=1\n\n\u03b4(Gl \u2212 G\u2217 l )\n\n(39)\n\n14\n\nwhere we have included N in the subscript of the probability distribution as a reminder that this distribution depends on the width. By the Portmanteau theorem, weak convergence is established if all expectations of bounded continuous functions, f , converge\n\nlim N\u2192\u221e\n\nEPN (G1,...,GL|X, \u0303Y) [f (G1, . . . , GL)] = f (G\u2217\n\n1, . . . , G\u2217\n\nL).\n\n(40)\n\nTo show this in a reasonably general setting (which the DGP posterior is a special case of), we consider an unnormalized probability density of the form h(g)eN L(g), and compute the moment as,\n\nE [f (g)] =\n\n(cid:82) G dg f (g)h(g)eN L(g) (cid:82) G dg h(g)eN L(g)\n\n(41)\n\nwhere g = (G1, . . . , GL) is all L positive semi-definite matrices, Gl. Thus, g \u2208 G, where G is a convex set.\n\nWe consider the superlevel set A(\u2206) = {g|L(g) \u2265 L(g\u2217) \u2212 \u2206}, where g\u2217 is the unique global optimum. We select out a small region, A(\u2206), surrounding the global maximum, and compute the integral as,\n\nE [f (g)] =\n\n(cid:82)\n\nA(\u2206) dg f (g)h(g)eN L(g) + (cid:82) A(\u2206) dg h(g)eN L(g) + (cid:82)\n\n(cid:82)\n\nG\\A(\u2206) dg f (g)h(g)eN L(g) G\\A(\u2206) dg h(g)eN L(g)\n\nAnd divide the numerator and denominator by (cid:82)\n\nA(\u2206) dg h(g)eN L(g),\n\nE [f (g)] =\n\n(cid:82)\n\nA(\u2206) dg f (g)h(g)eN L(g) (cid:82)\n\nA(\u2206) dg h(g)eN L(g) +\n\n(cid:82) G\\A(\u2206) dg f (g)h(g)eN L(g) A(\u2206) dg h(g)eN L(g)\n\n(cid:82)\n\n1 +\n\n(cid:82)\n\nG\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n(42)\n\n(43)\n\nNow, we deal with each term separately. The ratio in the denominator can be lower-bounded by zero, and upper bounded by considering a smaller superlevel set, A(\u2206/2), in the denominator,\n\n0 \u2264\n\n(cid:82)\n\nG\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n\u2264\n\n\u2264\n\n=\n\n(cid:82)\n\nG\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206/2) dg h(g)eN L(g) eN (L(g\u2217)\u2212\u2206) (cid:82) eN (L(g\u2217)\u2212\u2206/2) (cid:82) (cid:82)\n\nG\\A(\u2206) dg h(g) (cid:82) A(\u2206/2) dg h(g)\n\nG\\A(\u2206) dg h(g) A(\u2206/2) dg h(g)\n\ne\u2212N \u2206/2\n\n(44)\n\nThe upper bound converges to zero (as h(g) is independent of N ), and therefore by the sandwich theorem the ratio of interest also tends to zero.\n\nThe second ratio in the numerator can be rewritten as,\n\n(cid:82)\n\nG\\A(\u2206) dg f (g)h(g)eN L(g) A(\u2206) dg h(g)eN L(g)\n\n(cid:82)\n\n=\n\n(cid:82) G\\A(\u2206) dg f (g)h(g)eN L(g) (cid:82) G\\A(\u2206) dg h(g)eN L(g)\n\n(cid:82) G\\A(\u2206) dg h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n(45)\n\nThe first term here is an expectation of a bounded function, f (g), so is bounded, while second term converges to zero in the limit (by the previous result).\n\nFinally, we consider the first ratio in the numerator,\n\n(cid:82) A(\u2206) dg f (g)h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n(46)\n\nwhich can be understood as an expectation over f (g) in the region A(\u2206). As f is continuous, for any \u03b5 > 0, we can find a \u03b4 > 0 such that for all g with |g\u2217 \u2212 g| < \u03b4, we have\n\nf (g\u2217) \u2212 \u03b5 < f (g) < f (g\u2217) + \u03b5.\n\n(47)\n\n15\n\nFurther, because the continuous function, L(g), has a unique global optimum, g\u2217, for every \u03b4 > 0 we are always able to find a \u2206 > 0 such that all points g \u2208 A(\u2206) are within \u03b4 of g\u2217 i.e. |g\u2217 \u2212 g| < \u03b4. Thus combining the previous two facts, given an \u03b5, we are always able to find a \u03b4 such that Eq. 47 holds for all g with |g\u2217 \u2212 g| < \u03b4, and given a \u03b4 we are always able to find a \u2206 such that all g \u2208 A(\u2206) have |g\u2217 \u2212 g| < \u03b4. Hence for every \u03b5 > 0 we can find a \u2206 > 0 such that Eq. 47 holds for all g \u2208 A(\u2206). Choosing the appropriate \u03b5-dependent \u2206 and substituting Eq. 47 into Eq. 46, \u03b5 also bounds the error in the expectation,\n\nf (g\u2217) \u2212 \u03b5 <\n\n(cid:82)\n\nA(\u2206) dg f (g)h(g)eN L(g) (cid:82) A(\u2206) dg h(g)eN L(g)\n\n< f (g\u2217) + \u03b5.\n\n(48)\n\nNow, we use the results in Eq. (44), Eq. (45) and Eq. (48) to take the limit of Eq. (43) (we can compose these limits by the algebraic limit theorem as all the individual limits exist and are finite),\n\nf (g\u2217) \u2212 \u03b5 < lim\n\nN\u2192\u221e\n\nE [f (g)] < f (g\u2217) + \u03b5.\n\nAnd as this holds for any \u03b5, we have,\n\nf (g\u2217) = lim\n\nN\u2192\u221e\n\nE [f (g)] .\n\n(49)\n\n(50)\n\nThis result is applicable to the DGP posterior over Gram matrices, as that posterior can be written as,\n\nPN (G1, . . . , GL|X, \u0303Y) \u221d h(g)eN L(g),\n\nwhere L(g) is the usual DKM objective,\n\nL(g) = L(G1, . . . , GL)\n\nand h(g) is the remaining terms in the log-posterior which do not depend on N ,\n\n(cid:32)\n\nh(g) = exp\n\n\u2212 P +1 2\n\n(cid:33)\n\nlog |Gl|\n\n(cid:88)\n\nl\n\n(this requires P \u2264 N so that Gl is full-rank).\n\n(51)\n\n(52)\n\n(53)\n\nD GENERAL MODELS IN THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nOverall, our goal is to compute the integral in Eq. (6) in the limit as N \u2192 \u221e. While the integral is intractable for general models such as BNNs, we can use variational inference to reason about its properties. In particular, we can bound the integral using the ELBO,\n\nlog P (Gl|Gl\u22121) \u2265 ELBOl = EQ(Fl) [log P (Gl|Fl) + log P (Fl|Gl\u22121) \u2212 log Q (Fl)] . (cid:0)f l\n\n(54) (cid:1) in the main text, both because the approximate Note that Q (Fl) here is different from Q\u03b8l posterior here, Q (Fl) is over all features jointly, Fl, whereas the approximate posterior in the main text is only over a single feature, f l \u03bb, and because in the main text, we chose a specific family of distribution with parameters \u03b8l, while here we leave the approximate posterior, Q (Fl) completely unconstrained, so that it has the flexibility to capture the true posterior. Indeed, if the optimal approximate posterior is equal to the true posterior, Q\u2217 (Fl) = P (Fl|Gl\u22121, Gl), then the bound is tight, so we get log P (Gl|Gl\u22121) = ELBO\u2217 l . Our overall strategy is thus to use variational inference to characterise the optimal approximate which is equal to the true posterior Q\u2217 (Fl) = P (Fl|Gl\u22121, Gl) and use the corresponding ELBO to obtain log P (Gl|Gl\u22121).\n\n\u03bb\n\nD.1 CHARACTERISING EXACT BNN POSTERIORS\n\nRemember that if the approximate posterior family, Q (Fl) is flexible enough to capture the true posterior P (Fl|Gl\u22121, Gl), then the Q\u2217 (Fl) that optimizes the ELBO is indeed the true posterior, the bound is tight, so the ELBO is equal to log P (Gl|Gl\u22121) (Jordan et al., 1999; Blei et al., 2017). Thus, we are careful to ensure that our approximate posterior family captures the true posterior, by ensuring that we only impose constraints on Q (Fl) that must hold for the true posterior,\n\n16\n\nP (Fl|Gl\u22121, Gl). In particular, note that P (Gl|Fl) in Eq. (5b) constrains the true posterior to give non-zero mass only to Fl that satisfy Gl = 1 \u03c6(Fl)\u03c6T (Fl). However, this constraint is difficult Nl to handle. We therefore consider an alternative, weaker constraint on expectations, which holds for the true posterior (the first equality below) because Eq. (5b) constrains Gl = 1 \u03c6(Fl)\u03c6T (Fl), and Nl impose the same constraint on the approximate posterior,\n\nGl = EP(Fl|Gl,Gl\u22121)\n\n(cid:104) 1 Nl\n\n(cid:105) \u03c6(Fl)\u03c6T (Fl)\n\n= EQ(Fl)\n\n(cid:104) 1 Nl\n\n(cid:105) \u03c6(Fl)\u03c6T (Fl)\n\n.\n\n(55)\n\nNow, we can solve for the optimal Q (Fl) with this constraint on the expectation. In particular, the Lagrangian is obtained by taking the ELBO (Eq. 54), dropping the log P (Gl|Fl) term representing the equality constraint (that Gl = 1 \u03c6(Fl)\u03c6T (Fl)) and including Lagrange multipliers for the Nl expectation constraint, \u039b, (Eq. 55) and the constraint that the distribution must normalize to 1, \u039b,\n\n(cid:90)\n\nL =\n\ndFl Q (Fl) (log P (Fl|Gl\u22121) \u2212 log Q (Fl))\n\n(cid:18)\n\n(cid:18)\n\n\u039b\n\nGl \u2212\n\n(cid:90)\n\n+ 1\n\n2 Tr\n\ndFl Q (Fl) \u03c6(Fl)\u03c6T (Fl)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n+ \u039b\n\n1 \u2212\n\n(cid:90)\n\n(cid:19)\n\ndFl Q (Fl)\n\n(56)\n\nDifferentiating wrt Q (Fl), and solving for the optimal approximate posterior, Q\u2217 (Fl),\n\n0 =\n\n\u2202L \u2202 Q (Fl)\n\n(cid:12) (cid:12) (cid:12) (cid:12)Q\u2217(Fl)\n\n0 = (log P (Fl|Gl\u22121) \u2212 log Q\u2217 (Fl)) \u2212 1 \u2212 1\n\n2 Tr (cid:0)\u039b\u03c6(Fl)\u03c6T (Fl)(cid:1) \u2212 \u039b\n\nSolving for log Q\u2217 (Fl),\n\nlog Q\u2217 (Fl) = log P (Fl|Gl\u22121) \u2212 1\n\n2 Tr (cid:0)\u039b\u03c6(Fl)\u03c6T (Fl)(cid:1) + const .\n\nUsing the cyclic property of the trace,\n\nlog Q\u2217 (Fl) = log P (Fl|Gl\u22121) \u2212 1\n\n2 Tr (cid:0)\u03c6T (Fl)\u039b\u03c6(Fl)(cid:1) + const .\n\nThus, log Q (Fl) can be written as a sum over features,\n\nlog Q\u2217 (Fl) =\n\nNl(cid:88)\n\n\u03bb=1\n\n(cid:2)log P (cid:0)f l\n\n\u03bb|Gl\u22121\n\n(cid:1) \u2212 1\n\n2 \u03c6T (f l\n\n\u03bb)\u039b\u03c6(f l\n\n\u03bb)(cid:3) + const = (cid:80)NL\n\n\u03bb=1 log Q (cid:0)f l\n\n\u03bb\n\nso, the optimal approximate posterior is IID over features,\n\nQ\u2217 (Fl) = (cid:81)Nl\n\n\u03bb=1 Q\u2217 (cid:0)f l\n\n\u03bb\n\n(cid:1) .\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\n(cid:1)\n\n(61)\n\n(62)\n\nRemember that this approximate posterior was only constrained in expectation, and that this constraint held for the true posterior (Eq. 55). Thus, we might think that this optimal approximate posterior would be equal to the true posterior. However, remember that the true posterior had a tighter equality constraint, that Gl = 1 \u03c6(Fl)\u03c6T (Fl), while so far we have only imposed a weaker conNl straint in expectation (Eq. 55). We thus need to check that our optimal approximate posterior does indeed satisfy the equality constraint in the limit as Nl \u2192 \u221e. This be shown using the law of large numbers, as f l \u03bb are IID under the optimal approximate posterior, and by using Eq. (55) for the final equality,\n\nlim Nl\u2192\u221e\n\n1 Nl\n\n\u03c6(Fl)\u03c6T (Fl) = lim\n\nNl\u2192\u221e\n\n1 Nl\n\nNl(cid:88)\n\n\u03bb=1\n\n\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb) = E\n\nQ(f l\n\n\u03bb)\n\n(cid:2)\u03c6(f l\n\n\u03bb)\u03c6T (f l\n\n\u03bb)(cid:3) = Gl.\n\n(63)\n\nThus, the optimal approximate posterior does meet the constraint in the limit as Nl \u2192 \u221e, so in that limit, the true posterior, like the optimal approximate posterior is IID across features,\n\nP (Fl|Gl\u22121, Gl) = Q\u2217 (Fl) = (cid:81)Nl\n\n\u03bb=1 Q\u2217 (cid:0)f l\n\n\u03bb\n\n(cid:1) = (cid:81)Nl\n\nl=1 P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1) .\n\n(64)\n\n17\n\nD.2 EXACTLY MULTIVARIATE GAUSSIAN DGP POSTERIORS\n\nFor DGPs, we have \u03c6(f l\n\n\u03bb) = f l \u03bb, so the optimal approximate posterior is Gaussian, (cid:0)f l\n\n(cid:1) = log PDGP\n\n(cid:1) \u2212 1\n\n\u03bb + const\n\n\u03bb)T \u039bf l\n\n\u03bb|Gl\u22121\n\n(cid:0)f l\n\nDGP\n\n2 (f l\n\n\u03bb\n\nlog Q\u2217\n\n= \u2212 1\n\n2 (f l\n\n= log N\n\n\u03bb)T (cid:0)\u039b + K\u22121(Gl\u22121)(cid:1) f l (cid:16)\n\n\u03bb; 0, (cid:0)\u039b + K\u22121(Gl\u22121)(cid:1)\u22121(cid:17) f l\n\n\u03bb + const\n\n.\n\n(65)\n\n(66)\n\n(67)\n\nAs the approximate posterior and true posterior are IID, the constraint in Eq. (55) becomes,\n\nGl = E\n\nPDGP(f l\n\n\u03bb|Gl,Gl\u22121)\n\n(cid:2)f l\n\n\u03bb(f l\n\n\u03bb)T (cid:3) = E\n\nQ\u2217\n\nDGP(f l\n\n\u03bb)\n\n(cid:2)f l\n\n\u03bb(f l\n\n\u03bb)T (cid:3) = (cid:0)\u039b + K\u22121(Gl\u22121)(cid:1)\u22121\n\n.\n\n(68)\n\nAs the Lagrange multipliers are unconstrained, we can always set them such that this constraint holds. In that case both the optimal approximate posterior and the true posterior become,\n\nPDGP\n\n(cid:0)f l\n\n\u03bb|Gl\u22121Gl\n\n(cid:1) = Q\u2217\n\nDGP\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1) = N (cid:0)f l\n\n\u03bb; 0, Gl\n\n(cid:1) ,\n\n(69)\n\nas required.\n\nD.3 GENERAL FORM FOR THE CONDITIONAL DISTRIBUTION OVER GRAM MATRICES\n\nNow that we have shown that the true posterior, P (Fl|Gl\u22121, Gl) factorises, we can obtain a simple form for log P (Gl|Gl\u22121). In particular, log P (Gl|Gl\u22121) is equal to the ELBO if we use the true posterior in place of the approximate posterior,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = lim\n\nNl\u2192\u221e\n\n1 N\n\nEP(Fl|Gl\u22121,Gl)\n\n(cid:20)\n\nlog P (Gl|Fl) + log\n\nP (Fl|Gl\u22121) P (Fl|Gl\u22121, Gl)\n\n(cid:21)\n\n.\n\n(70)\n\nUnder the posterior, the constraint represented by log P (Gl|Fl) is satisfied, so in the limit we can include that term in a constant,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = lim\n\nNl\u2192\u221e\n\n1 N\n\nEP(Fl|Gl\u22121,Gl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl\u22121) P (Fl|Gl\u22121, Gl)\n\n(cid:21)\n\n+ const .\n\n(71)\n\nNow, we use the fact that the prior, P (Fl|Gl\u22121) and posterior, P (Fl|Gl\u22121, Gl), are IID across features,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = \u03bdl E\n\nP(f l\n\n\u03bb|Gl\u22121,Gl)\n\n(cid:34)\n\nlog\n\nP (cid:0)f l\n\n(cid:1)\n\n\u03bb|Gl\u22121 \u03bb|Gl\u22121, Gl\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:1)\n\n+ const\n\n(72)\n\nand this expectation is a KL-divergence,\n\nlim Nl\u2192\u221e\n\n1\n\nN log P (Gl|Gl\u22121) = \u2212\u03bdl DKL\n\n(cid:0)P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl\n\n(cid:1)(cid:13) (cid:13)P (cid:0)f l\n\n\u03bb|Gl\u22121\n\n(cid:1)(cid:1) + const,\n\n(73)\n\nwhich gives Eq. (32) when we combine with Eq. (8).\n\nD.4 PARAMETRIC APPROXIMATE POSTERIORS\n\nEq. (64) represents a considerable simplification, as we now need to consider only a single feature, f l \u03bb, rather than the joint distribution over all features, Fl. However, in the general case, it is still not possible to compute Eq. (64) because the true posterior over a single feature is still not tractable. Following the true posteriors derived in the previous section, we could chose a parametric approximate posterior that factorises across features,\n\nQ\u03b8 (F1, . . . , FL) = (cid:81)L\n\nl=1\n\n(cid:81)Nl\n\n\u03bb=1 Q\u03b8l\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1) .\n\n(74)\n\nRemember that we optimize the approximate posterior parameters, \u03b8, directly, and set the Gram matrices as a function of \u03b8 (Eq. 34). As before, we can bound, log P (Gl=G\u03b8(\u03b8l)|Gl\u22121) using the\n\n18\n\nELBO, and the bound is tight when the approximate posterior equals the true posterior,\n\nlog P (Gl = G\u03b8(\u03b8l)|Gl\u22121)\n\n(cid:34)\n\n= E\n\nP(Fl|Gl\u22121,Gl=G\u03b8(\u03b8\u03b8))\n\nlog P (Gl=G\u03b8(\u03b8l)|Fl) + log\n\nP (cid:0)Fl\n\n\u03bb|Gl\u22121\n\n(cid:1)\n\nP (Fl|Gl\u22121, Gl=G\u03b8(\u03b8l))\n\n(cid:34)\n\n\u2265 EQ\u03b8(Fl)\n\nlog P (Gl=G\u03b8(\u03b8l)|Fl) + log\n\nP (cid:0)Fl Q\u03b8l\n\n\u03bb|Gl\u22121 (Fl)\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(cid:35)\n\n(75)\n\n(76)\n\n(77)\n\nNow, we can cancel the log P (Gl = G\u03b8(\u03b8l)|Fl) terms, as they represent a constraint that holds both under the true posterior, and under the approximate posterior,\n\nE\n\nP(Fl|Gl\u22121,Gl=G\u03b8(\u03b8l)))\n\n(cid:20)\n\nlog\n\nP (Fl|Gl\u22121) P (Fl|Gl\u22121, Gl=G\u03b8(\u03b8l))\n\n(cid:21)\n\n\u2265 EQ\u03b8l\n\n(Fl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl\u22121)\n\nQ\u03b8l\n\n(Fl)\n\n(cid:21)\n\n.\n\n(78)\n\nUsing the fact that the prior, posterior and approximate posterior are all IID over features, we can write this inequality in terms of distributions over a single feature, f l\n\n\u03bb and divide by Nl,\n\nE\n\nP(f l\n\n\u03bb|Gl\u22121,Gl=G\u03b8(\u03b8l))\n\n(cid:34)\n\nlog\n\n(cid:1)\n\nP (cid:0)f l\n\n\u03bb|Gl\u22121 \u03bb|Gl\u22121, Gl=G\u03b8(\u03b8l)(cid:1)\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:34)\n\nlog\n\n\u2265 E\n\nQ\u03b8l(f l\n\n\u03bb)\n\nP (cid:0)f l\n\n\u03bb|Gl\u22121(\u03b8)(cid:1) (cid:0)f l Q\u03b8l\n\n(cid:1)\n\n\u03bb\n\n(cid:35)\n\n.\n\n(79)\n\nNoting that both sides of this inequality are negative KL-divergences, we obtain, (cid:1)(cid:13) (cid:13)P (cid:0)f l\n\n\u03bb|Gl\u22121, Gl=G\u03b8(\u03b8l)(cid:1)(cid:13)\n\n(cid:1)(cid:1) \u2265 \u2212 DKL\n\n(cid:13)P (cid:0)f l\n\n(cid:0)P (cid:0)f l\n\n\u03bb|Gl\u22121\n\n\u2212 DKL\n\n(cid:0)f l\n\n(cid:0)Q\u03b8l\n\n\u03bb\n\n\u03bb|Gl\u22121\n\n(cid:1)(cid:1) ,\n\n(80)\n\nwhich gives Eq. (33) in the main text.\n\nE THEORETICAL SIMILARITIES IN REPRESENTATION LEARNING IN FINITE\n\nAND INFINITE NETWORKS\n\nIn the main text, we considered probability densities of the Gram matrices, G1, . . . , GL. However, we can also consider probability densities of the features, F1, . . . , FL, for a DGP, 2 log |K (GDGP (Fl\u22121))| \u2212 1\n\nl K\u22121 (GDGP (Fl\u22121)) Fl\n\nlog P (Fl|Fl\u22121) = \u2212 Nl\n\n(cid:1) + const .\n\n2 tr (cid:0)FT\n\n(81)\n\nWe can rewrite the density such that it is still the density of features, Fl, but it is expressed in terms of the DGP Gram matrix, log P (Fl|Fl\u22121) = \u2212 Nl\n\n2 tr (cid:0)K\u22121(Gl\u22121)Gl\n\n2 log |K(Gl\u22121)| \u2212 Nl\n\n(cid:1) + const .\n\n(82)\n\nHere, we have used the cyclic property of the trace to combine the Fl and FT l to form Gl, and we have used the fact that our kernels can be written as a function of the Gram matrix. Overall, we can therefore write the posterior over features, P (F1, . . . , FL|X, \u0303Y), in terms of only Gram matrices,\n\nJ (G1, . . . , GL) = 1\n\nN log P (F1, . . . , FL|X, \u0303Y) = log P (Y|GL) + 1\n\nN\n\nL (cid:88)\n\nl=1\n\nlog P (Fl|Fl\u22121) ,\n\n(83)\n\nsubstituting Eq. (82),\n\nJ (G1, . . . , GL) = log P (Y|GL) \u2212 1\n\n2\n\n(cid:80)L\n\nl=1\u03bdl\n\n(cid:0)log |K(Gl\u22121)| + tr (cid:0)K\u22121(Gl\u22121)Gl\n\n(cid:1)(cid:1)\n\n+ const .\n\n(84)\n\nThus, J (G1, . . . , GL) does not depend on N , and thus the Gram matrices that maximize J (G1, . . . , GL) are the same for any choice of N . The only restriction is that we need Nl \u2265 P , to ensure that the Gram matrices are full-rank.\n\nTo confirm these results, we used Adam with a learning rate of 10\u22123 to optimize full-rank Gram matrices with Eq. (84) and to directly do MAP inference over features using Eq. (81). As expected, as the number of features increased, the Gram matrix from MAP inference over features converged rapidly to that expected using Eq. (84) (Fig. 5).\n\n19\n\nFigure 5: RMSE of trained Gram matrices between one-hidden-layer (first row) and two-hiddenlayer (second row) DGPs of various width trained by gradient descent and the corresponding MAP limit. Columns correspond to different datasets (trained on a subset of 50 datapoints).\n\nF ADDITIONAL EXPERIMENTAL DETAILS\n\nTo optimize the analytic DKM objective for DGPs and the variational DKM objective for DGPs (Figs. 3\u201311), we parameterised the Gram matrices (or covariances for the variational approximate posterior) as the product of a square matrix, Rl \u2208 RP \u00d7P , with itself transposed, Gl = 1 P RlRT l , and we used Adam with a learning rate of 10\u22123 to learn Rl. To do Bayesian inference in finite BNNs and DGPs, we used Langevin sampling with 10 parallel chains, and a step size of 10\u22123. Note that in certain senarios, Langevin sampling can be very slow, as the features have a Gaussian prior with covariance K(Gl\u22121) which has some very small and some larger eigenvalues, which makes sampling difficult. Instead, we reparameterised the model in terms of the standard Gaussian random variables, Vl \u2208 RP \u00d7Nl. We then wrote Fl in terms of Vl,\n\nFl = Ll\u22121Vl.\n\n(85)\n\nHere, Ll\u22121 is the Cholesky of K(Gl\u22121), so K(Gl\u22121) = Ll\u22121LT l\u22121. This gives an equivalent distribution P (Fl|Fl\u22121). Importantly, as the prior on Vl is IID standard Gaussian, sampling Vl is much faster. To ensure that the computational cost of these expensive simulations remained reasonable, we used a subset of 50 datapoints from each dataset.\n\nFor the DKM objective for BNNs, we used Monte-Carlo to approximate the Gram matrices,\n\nG\u03b8(\u03b8l) \u2248\n\nK (cid:88)\n\nk=1\n\n\u03c6(f l\n\nk)\u03c6T (f l\n\nk).\n\n(86)\n\nwith f l k drawn from the appropriate approximate posterior, and K = 216. We can use the reparameterisation trick (Kingma & Welling, 2013; Rezende et al., 2014) to differentiate through these Monte-Carlo estimates.\n\nG ADDITIONAL COMPARISONS WITH FINITE-WIDTH DGPS\n\nIn particular, we Here, we give additional results supporting those in Sec. 4.6, Fig. 3\u2013Fig. 11. give the DGP representations learned by two-layer networks on all UCI datasets (boston, concrete, energy, yacht), except those already given in the main text Fig. 6\u20138.\n\n20\n\n0.000.050.100.150.20RMSEbostonyachtconcreteone-layerenergy2123252729211width0.000.050.100.150.20RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layerFigure 6: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of energy. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\nFigure 7: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of boston. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n21\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101init DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 8: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of concrete. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n22\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101H THE FLOW POSTERIOR IN A 2-LAYER BNN\n\nHere, we give the 2-layer version (Fig. 9) of Fig. 4 in the main text, which again shows a close match between the variational DKM with a flow posterior, and the BNN true posterior.\n\nFigure 9: Two-layer ReLU BNN and variational DKM with flow. a Initialized (first row) and learned Gram matrices of a width 1024 BNN (second row), vDKM with flow (third row) and vDKM with multivariate Gaussian (fourth row) using 214 Monte-Carlo samples. The Gram matrices between BNN and vDKM (flow) match closely after training. (MvG). b Marginal PDF over features at each layer for one input datapoint using kernel density estimation. The marginal PDFs of BNN are nonGaussian (blue curves), vDKM with flow is able to capture the non-Gaussianity and match closely with BNNs marginals (red curves).\n\n23\n\ninitG1G2vDKM (flow)150indexvDKM (MvG)150index505feature0.00.5density505feature150index150indexG0150indexG30.750.000.75abBNNvDKM (flow)BNNI MULTIVARIATE GAUSSIAN APPROXIMATE POSTERIORS IN DEEPER\n\nNETWORKS\n\nIn particular, we hypothesised that depth is an important factor.\n\nThere is a body of theoretical work (e.g. (Seroussi & Ringel, 2021)), on BNNs that approximates BNN posteriors over features as Gaussian. While we have shown that this is a bad idea in general (Fig. 4 and 9), we can nonetheless ask whether there are circumstances where the idea might work well. In particular, in shallow networks, in order to get GL close to the required representation, we may need the posterior over Fl to be quite different from the prior. In contrast, in deeper networks, we might expect the posterior over Fl to be closer to its (Gaussian) prior, and therefore we might Gaussian approximate posteriors to work better.\n\nHowever, we cannot just make the network deeper, because as we do so, we apply the nonlinearity more times and dramatically alter the network\u2019s inductive biases. To resolve this issue, we derive a leaky relu nonlinearity that allows (approximately) independent control over the inductive biases (or effective depth) and the actual depth (Appendix I.1). Using these nonlinearities, we indeed find that very deep networks are reasonably well approximated by multivariate Gaussian approximate posteriors (Appendix I.2).\n\nI.1 LEAKY RELU NONLINEARITIES\n\nOur goal is to find a pointwise nonlinearity, \u03c6, such that (under the prior), \u03bb)(cid:3) = p E\n\n\u03bb)reluT (f l\n\n\u03bb)\u03c6T (f l\n\n(cid:2)relu(f l\n\n(cid:2)\u03c6(f l\n\nE\n\nPDGP(f l\n\n\u03bb|Gl\u22121)\n\nP(f l\n\n\u03bb|Gl\u22121)\n\n\u03bb)(cid:3) + (1 \u2212 p)Gl\u22121.\n\n(87)\n\nWe will set p = \u03b1/L, where \u03b1 is the \u201ceffective\u201d depth of the network and L is the real depth. These networks are designed such that their inductive biases in the infinite width limit are similar to a standard relu network with depth \u03b1. Indeed, we would take this approach if we wanted a well-defined infinite-depth DKM limit.\n\nWithout loss of generality, we consider a 2D case, where x and y are zero-mean bivariate Gaussian,\n\n\u03c0(x, y) = N\n\n(cid:19)\n\n(cid:18)(cid:18)x y\n\n; 0,\n\n(cid:18)\u03a3xx \u03a3xy \u03a3xy \u03a3yy\n\n(cid:19)(cid:19)\n\n(88)\n\nwhere \u03c0(x, y) is the probability density for the joint distribution. Note that we use a scaled relu, (cid:26)\u221a\n\nrelu(x) =\n\n0\n\n2 x for 0 < x otherwise\n\nsuch that E (cid:2)relu2(x)(cid:3) = \u03a3xx. Mirroring Eq. 87, we want the nonlinearity, \u03c6, to satisfy,\n\nE (cid:2)\u03c6(x2)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 \u2212 p)\u03a3xx = \u03a3xx E (cid:2)\u03c6(y2)(cid:3) = p E (cid:2)relu2(y)(cid:3) + (1 \u2212 p)\u03a3yy = \u03a3yy\n\nE [\u03c6(x)\u03c6(y)] = p E [relu(x)relu(y)] + (1 \u2212 p)\u03a3xy\n\nWe hypothesise that this nonlinearity has the form,\n\n\u03c6(x) = a relu(x) + bx.\n\nWe will write the relu as a sum of x and |x|,\n\nrelu(x) = 1\u221a\n\n2\n\n(x + |x|),\n\nbecause E [f (x, y)] = 0 for f (x, y) = x|y| or f (x, y) = |x|y. It turns out that we get zero expectation for all functions where f (\u2212x, \u2212y) = \u2212f (x, y), which holds for the two choices above. To show such functions have a zero expectation, we write out the integral explicitly,\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n\u2212\u221e\n\ndy \u03c0(x, y)f (x, y).\n\n(93)\n\n24\n\n(89)\n\n(90a)\n\n(90b)\n\n(90c)\n\n(91)\n\n(92)\n\nWe split the domain of integration for y at zero,\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) 0\n\ndx\n\n\u2212\u221e\n\n\u2212\u221e\n\ndy \u03c0(x, y)f (x, y) +\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy \u03c0(x, y)f (x, y).\n\n(94)\n\nWe substitute y\u2032 = \u2212y and x\u2032 = \u2212x in the first integral, (cid:90) \u221e\n\n(cid:90) \u221e\n\nE [f (x, y)] =\n\ndx\u2032\n\ndy\u2032 \u03c0(\u2212x\u2032, \u2212y\u2032)f (\u2212x\u2032, \u2212y\u2032) +\n\n\u2212\u221e\n\n0\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy \u03c0(x, y)f (x, y).\n\n(95)\n\nAs the variables we integrate over are arbitrary we can relabel y\u2032 as y and x\u2032 as x, and we can then merge the integrals as their limits are the same,\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy [\u03c0(\u2212x, \u2212y)f (\u2212x, \u2212y) + \u03c0(x, y)f (x, y)] .\n\nUnder a zero-mean Gaussian, \u03c0(\u2212x, \u2212y) = \u03c0(x, y),\n\nE [f (x, y)] =\n\n(cid:90) \u221e\n\n(cid:90) \u221e\n\ndx\n\n\u2212\u221e\n\n0\n\ndy \u03c0(x, y) (f (\u2212x, \u2212y) + f (x, y)) .\n\n(96)\n\n(97)\n\nThus, if f (\u2212x, \u2212y) = \u2212f (x, y), then the expectation of that function under a bivariate zero-mean Gaussian distribution is zero.\n\nRemember that our overall goal was to design a nonlinearity, \u03c6, (Eq. 91) which satisfied Eq. (90). We therefore compute the expectation,\n\nE [\u03c6(x)\u03c6(y)] = E [(a relu(x) + bx) (a relu(y) + by)]\n\n= E\n\n(cid:104)(cid:16) a\u221a\n\n2\n\n(x + |x|) + bx\n\n(cid:17) (cid:16) a\u221a\n\n(y + |y|) + by\n\n(cid:17)(cid:105)\n\n2\n\nUsing the fact that E [ x|y| ] = E [ |x|y ] = 0 under a multivariate Gaussian,\n\n(cid:104)\n\n= E\n\na2 1\u221a\n\n2\n\n(x + |x|) 1\u221a 2\n\n= a2 E [relu(x)relu(y)] +\n\n(y + |y|) + (cid:16)\u221a\n\n2ab + b2(cid:17)\n\n(cid:16)\u221a\n\n2ab + b2(cid:17)\n\nxy\n\n(cid:105)\n\nE [xy] .\n\n\u221a\n\np.\n\na =\n\nThus, we can find the value of a by comparing with Eq. (90c),\n\np = a2\n\nFor b, things are a bit more involved,\n\n\u221a\n\n1 \u2212 p =\n\n2ab + b2 = (cid:112)2p b + b2\n\n(98)\n\n(99)\n\n(100)\n\n(101)\n\n(102)\n\n(103)\n\nwhere we substitute for the value of a. This can be rearranged to form a quadratic equation in b,\n\n0 = b2 + (cid:112)2p b + (p \u2212 1),\n\nwhich can be solved,\n\nb = 1 2\n\nb = 1 2\n\n(cid:16)\n\n(cid:17) \u2212(cid:112)2p \u00b1 (cid:112)2p \u2212 4(p \u2212 1)\n\n(cid:16)\n\n\u2212(cid:112)2p \u00b1 (cid:112)4 \u2212 2p\n\n(cid:17)\n\nb = \u2212\n\n(cid:113) p\n\n2 \u00b1\n\n(cid:113)\n\n1 \u2212 p\n\n2\n\nOnly the positive root is of interest,\n\nb =\n\nThus, the nonlinearity is,\n\n(cid:113)\n\n1 \u2212 p\n\n2 \u2212\n\n(cid:113) p\n\n2\n\n\u221a\n\n\u03c6(x) =\n\np relu(x) +\n\n(cid:16)(cid:113)\n\n1 \u2212 p\n\n2 \u2212\n\n(cid:113) p\n\n(cid:17)\n\n2\n\nx\n\n25\n\n(104)\n\n(105)\n\n(106)\n\n(107)\n\n(108)\n\n(109)\n\nFigure 10: Comparison of posterior feature marginal distributions between a BNN of width 1024 (trained by Langevin sampling over features) and a variational DKM with 216 Monte-Carlo samples, in a 4-layer (row 1) and a 32-layer (row 2) network. We give the BNN posterior features from Langevin sampling (blue histogarm) and the best fitting Gaussian (blue line), and compare against the variational DKM approximate posterior Gaussian distribution (red line).\n\nwhere we set p = \u03b1/L, and remember we used the scaled relu in Eq. (89). Finally, we established these choices by considering only the cross term, E [\u03c6(x)\u03c6(y)]. We also need to check that the E (cid:2)\u03c62(x)(cid:3) and E (cid:2)\u03c62(y)(cid:3) terms are as required (Eq. 90a and Eq. 90b). In particular,\n\nE (cid:2)\u03c62(x)(cid:3) = E\n\n(cid:104)\n\n(a relu(x) + bx)2(cid:105)\n\n= E\n\n(cid:20)(cid:16) a\u221a\n\n2\n\n(x + |x|) + bx\n\n(cid:17)2(cid:21)\n\n(110)\n\nusing E [x|x|] = 0 as x|x| is an odd function of x, and the zero-mean Gaussian is an even distribution,\n\nE (cid:2)\u03c62(x)(cid:3) = a2 E (cid:2)relu2(x)(cid:3) +\n\n(cid:16)\u221a\n\n2ab + b2(cid:17)\n\n\u03a3xx\n\nusing Eq. (102) to identify a2 and Eq. (103) to identify\n\n\u221a\n\n2ab + b2,\n\nE (cid:2)\u03c62(x)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 \u2212 p)\u03a3xx,\n\nas required.\n\nI.2 MULTIVARIATE GAUSSIAN IN DEEPER NETWORKS\n\n(111)\n\n(112)\n\nIn the main text, we show that a more complex approximate posterior can match the distributions in these networks. Here, we consider an alternative approach. In particular, we hypothesise that these distributions are strongly non-Gaussian because the networks are shallow, meaning that the posterior needs to be far from the prior in order to get a top-layer kernel close to GL+1. We could therefore make the posteriors closer to Gaussian by using leaky-relu nonlinearities (Appendix I.1) with fixed effective depth (\u03b1 = 2), but increasing real depth, L. In particular, we use multivariate Gaussian approximate posteriors with learned means,\n\nQ\u03b8l\n\n(cid:0)f l\n\n\u03bb\n\n(cid:1) = N (cid:0)f l\n\n\u03bb; \u03bcl, \u03a3l\n\n(cid:1)\n\nso\n\n\u03b8l = (\u03bcl, \u03a3l).\n\n(113)\n\nAs expected, for a depth 32 network, we have much more similar marginals (Fig. 10 top) and learned representations (Fig. 11 top).\n\n26\n\n50501density=1505=2505=35054 layers=4505feature01density=8505feature=16505feature=24505feature32 layers=32BNN vDKMFigure 11: Comparison of Gram matrices between BNN of width 1024 (trained by Langevin sampling over features) and variational DKM, in 4-layer (row 1-3) and 32-layer networks (row 4-6). Initializations are shown in row 1 and 4, trained BNN Gram matrices are shown in row 2 and 5, and trained variational DKM Gram matrices are shown in row 3 and 6. As in Figure 10, the variational DKM is a poor match to Langevin sampling in a BNN for a 4-layer network, but is very similar in a 32 layer network.\n\n27\n\nG1G2G3G4G8G16G24G32150index150index150index150index150index150indexG0150indexGL+1101 32 layers BNNinitvDKMvDKM4 layers BNNinitJ UNIMODALITY IN LINEAR DEEP KERNEL MACHINES\n\nJ.1 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND SAME WIDTHS\n\nHere, we show that the deep kernel machine objective is unimodal for a linear kernel. A linear kernel simply returns the input Gram matrix,\n\nK (G) = G.\n\n(114)\n\nIt is called a linear kernel, because it arises in the neural network setting (Eq. 21) by choosing the nonlinearity, \u03c6 to be the identity, in which case, Fl = Fl\u22121Wl\u22121. For a linear kernel the objective becomes,\n\nL(G1, ..., GL) = (cid:80)L+1\n\nl=1\n\n\u03bdl 2\n\n(cid:0)log (cid:12)\n\n(cid:12)G\u22121\n\nl\u22121Gl\n\n(cid:12) (cid:12) \u2212 Tr (cid:0)G\u22121\n\nl\u22121Gl\n\n(cid:1)(cid:1)\n\n(115)\n\nwhere we have assumed there is no output noise, \u03c32 = 0. Taking all \u03bdl to be equal, \u03bd = \u03bdl (see Appendix J.2 for the general case),\n\nL(G1, ..., GL) = log (cid:12)\n\n(cid:12)G\u22121\n\n0 GL+1\n\n(cid:12) (cid:12) \u2212 \u03bd\n\n2\n\n(cid:80)L+1\n\nl=1 Tr (cid:0)G\u22121\n\nl\u22121Gl\n\n(cid:1) .\n\n(116)\n\nNote that G0 and GL+1 are fixed by the inputs and outputs respectively. Thus, to find the mode, we set the gradient wrt G1, . . . , GL to zero,\n\n0 =\n\n\u2202L \u2202Gl\n\n= \u03bd 2\n\n(cid:0)G\u22121\n\nl\u22121 \u2212 G\u22121\n\nl Gl+1G\u22121\n\nl\n\nThus, at the mode, the recursive relationship must hold,\n\nT = G\u22121\n\nl\u22121Gl = G\u22121\n\nl Gl+1.\n\nThus, optimal Gram matrices are given by,\n\nand we can solve for T by noting,\n\nGl = G0Tl,\n\nG\u22121\n\n0 GL+1 = TL+1.\n\n(cid:1)\n\n(117)\n\n(118)\n\n(119)\n\n(120)\n\nImportantly, T is the product of two positive definite matrices, T = G\u22121 l\u22121Gl, so T must have positive, real eigenvalues (but T does not have to be symmetric (Horn & Johnson, 2012)). There is only one solution to Eq. (120) with positive real eigenvalues (Horn et al., 1994). Intuitively, this can be seen using the eigendecomposition, G\u22121\n\n0 GL+1 = V\u22121DV, where D is diagonal,\n\nT = (cid:0)V\u22121DV(cid:1)1/(L+1)\n\n= V\u22121D1/(L+1)V.\n\n(121)\n\nThus, finding T reduces to finding the (L + 1)th root of each positive real number on the diagonal of D. While there are (L + 1) complex roots, there is only one positive real root, and so T and hence G1, . . . , GL are uniquely specified. This contrasts with a deep linear neural network, which has infinitely many optimal settings for the weights.\n\nNote that for the objective to be well-defined, we need K(G) to be full-rank. With standard kernels (such as the squared exponential) this is always the case, even if the input Gram matrix is singular. However, a linear kernel will have a singular output if given a singular input, and with enough data points, G0 = 1 XXT ) to be \u03bd0 given by applying a positive definite kernel (such as a squared exponential) to 1 XXT . This results \u03bd0 in positive definite G0, as long as the input points are distinct.\n\nXXT is always singular. To fix this, we could e.g. define G0 = K( 1 \u03bd0\n\nJ.2 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND ARBITRARY WIDTHS\n\nIn the main text we showed that the deep kernel machine is unimodal when all \u03bdl are equal. Here, we show that unimodality in linear DKMs also holds for all choices of \u03bdl. Recall the linear DKM objective in Eq. (115),\n\nL(G1, ..., GL) = (cid:80)L+1 = (cid:80)L+1\n\nl=1\n\nl=1\n\n\u03bdl 2\n\u03bdl 2\n\n(cid:12)G\u22121\n\nl\u22121Gl\n\n(cid:0)log (cid:12) (cid:0)log |Gl| \u2212 log |Gl\u22121| \u2212 Tr(cid:0)G\u22121\n\n(cid:12) (cid:12) \u2212 Tr (cid:0)G\u22121\n\nl\u22121Gl\n\n(cid:1)(cid:1)\n\nl\u22121Gl\n\n(cid:1)(cid:1) .\n\n(122)\n\n(123)\n\n28\n\nTo find the mode, again we set the gradient wrt Gl to zero,\n\n0 =\n\n\u2202L \u2202Gl\n\n= \u2212 \u03bdl+1\u2212\u03bdl\n\n2 G\u22121\n\nl \u2212 \u03bdl\n\n2 G\u22121\n\nl\u22121 + \u03bdl+1\n\n2 G\u22121\n\nl Gl+1G\u22121\n\nl\n\n,\n\n(124)\n\nfor l = 1, ..., L. Right multiplying by 2Gl and rearranging,\n\n\u03bdl+1G\u22121\n\nl Gl+1 = \u03bdlG\u22121\n\nl\u22121Gl + (\u03bdl+1 \u2212 \u03bdl) I,\n\nfor l = 1, ..., L.\n\n(125)\n\nEvaluating this expression for l = 1 and l = 2 gives,\n\n\u03bd2G\u22121 \u03bd3G\u22121\n\n1 G2 = \u03bd1G\u22121 2 G3 = \u03bd2G\u22121\n\n0 G1 + (\u03bd2 \u2212 \u03bd1) I, 1 G2 + (\u03bd3 \u2212 \u03bd2) I = \u03bd1G\u22121\n\n0 G1 + (\u03bd3 \u2212 \u03bd1) I.\n\nRecursing, we get,\n\n\u03bdlG\u22121\n\nl\u22121Gl = \u03bd1G\u22121\n\n0 G1 + (\u03bdl \u2212 \u03bd1) I.\n\n(126)\n\n(127)\n\n(128)\n\nCritically, this form highlights constraints on G1. In particular, the right hand side, G\u22121 l\u22121Gl, is the product of two positive definite matrices, so has positive eigenvalues (but may be non-symmetric (Horn & Johnson, 2012)). Thus, all eigenvalues of \u03bd1G\u22121 0 G1 must be larger than \u03bd1 \u2212 \u03bdl, and this holds true at all layers. This will become important later, as it rules out inadmissible solutions.\n\nGiven G0 and G1, we can compute any Gl using,\n\nG\u22121\n\n0 Gl =\n\n(cid:33)\n\n\u03bdl\u2032\n\nG\u22121\n\n0 Gl =\n\n(cid:32) l\n\n(cid:89)\n\nl\u2032=1\n\nl (cid:89)\n\nl\u2032=1\n\nl (cid:89)\n\nl\u2032=1\n\n(cid:0)G\u22121\n\nl\u2032\u22121Gl\u2032\n\n(cid:1) =\n\n(cid:81)l\n\n1 l\u2032=1 \u03bdl\u2032\n\nl (cid:89)\n\nl\u2032=1\n\n(cid:0)\u03bdl\u2032G\u22121\n\nl\u2032\u22121Gl\u2032\n\n(cid:1)\n\n(cid:0)\u03bd1G\u22121\n\n0 G1 + (\u03bdl\u2032 \u2212 \u03bd1) I(cid:1)\n\n(129)\n\n(130)\n\nwhere the matrix products are ordered as (cid:81)L using our knowledge of GL+1. Computing G\u22121 (cid:32)L+1 (cid:89)\n\n(cid:33)\n\nL+1 (cid:89)\n\n\u03bdl\n\nG\u22121\n\n0 GL+1 =\n\nl=1\n\nl=1\n\nWe write the eigendecomposition of \u03bd1G\u22121\n\n0 G1 as, 0 G1 = VDV\u22121.\n\n\u03bd1G\u22121\n\nl=1 Al = A1 \u00b7 \u00b7 \u00b7 AL. Now, we seek to solve for G1 0 GL+1,\n\n(cid:0)\u03bd1G\u22121\n\n0 G1 + (\u03bdl \u2212 \u03bd1) I(cid:1) .\n\n(131)\n\n(132)\n\nThus,\n\n(cid:33)\n\n\u03bdl\n\nG\u22121\n\n0 GL+1 =\n\n(cid:32)L+1 (cid:89)\n\nl=1\n\nwhere \u039b is a diagonal matrix,\n\n\u039b =\n\nL+1 (cid:89)\n\nl=1\n\nL+1 (cid:89)\n\nl=1\n\n(cid:0)VDV\u22121 + (\u03bdl \u2212 \u03bd1) I(cid:1) = V\u039bV\u22121\n\n(133)\n\n(D + (\u03bdl \u2212 \u03bd1) I) .\n\n(134)\n\nThus, we can identify V and \u039b by performing an eigendecomposition of the known matrix, (cid:16)(cid:81)L+1 0 GL+1. Then, we can solve for D (and hence G1) in terms of \u039b and V. The\n\n(cid:17)\n\nl=1 \u03bdl\n\nG\u22121 diagonal elements of D satisfy,\n\n0 = \u2212\u039bii +\n\nL+1 (cid:89)\n\nk=1\n\n(Dii + (\u03bdl \u2212 \u03bd1)) .\n\n(135)\n\nThis is a polynomial, and remembering the constraints from Eq. (128), we are interested in solutions which satisfy,\n\n\u03bd1 \u2212 \u03bdmin \u2264 Dii.\n\n(136)\n\n29\n\nwhere,\n\n\u03bdmin = min (\u03bd1, . . . , \u03bdL+1) .\n\n(137)\n\nTo reason about the number of such solutions, we use Descartes\u2019 rule of signs, which states that the number of positive real roots is equal to or a multiple of two less than the number of sign changes in the coefficients of the polynomial. Thus, if there is one sign change, there must be one positive real root. For instance, in the following polynomial,\n\n0 = x3 + x2 \u2212 1\n\n(138)\n\nthe signs go as (+), (+), (\u2212), so there is only one sign change, and there is one real root. To use Descartes\u2019 rule of signs, we work in terms of D\u2032\n\nii, which is constrained to be positive,\n\n0 \u2264 D\u2032\n\nii = Dii \u2212 (\u03bd1 \u2212 \u03bdmin)\n\nDii = D\u2032\n\nii + (\u03bd1 \u2212 \u03bdmin) .\n\n(139)\n\nThus, the polynomial of interest (Eq. 135) becomes,\n\n0 = \u2212\u039bii +\n\nL+1 (cid:89)\n\nl=1\n\n(D\u2032\n\nii + (\u03bd1 \u2212 \u03bdmin) \u2212 (\u03bd1 \u2212 \u03bdl)) = \u2212\u039bii +\n\nL+1 (cid:89)\n\nl=1\n\n(D\u2032\n\nii + (\u03bdl \u2212 \u03bdmin))\n\n(140)\n\nwhere 0 < \u03bdl \u2212 \u03bdmin as \u03bdmin is defined to be the smallest \u03bdl (Eq. 137). Thus, the constant term, \u2212\u039bii is negative, while all other terms, D\u2032 ii)L+1 in the polynomial have positive coefficients. Thus, there is only one sign change, which proves the existence of only one valid real root, as required.\n\nii, . . . , (D\u2032\n\nK UNIMODALITY EXPERIMENTS WITH NONLINEAR KERNELS\n\nFor the posterior over Gram matrices to converge to a point distribution, we need the DKM objective L(G1, . . . , GL) to have one unique global optimum. As noted above, this is guaranteed when the prior dominates (Eq. 11), and for linear models (Appendix J). While we believe that it might be possible to construct counter examples, in practice we expect a single global optimum in most practical settings. To confirm this expectation, we did a number of experiments, starting with many different random initializations of a deep kernel machine and optimizing using gradient descent (Appendix K). In all cases tested, the optimizers converged to the same maximum.\n\nP VlVT\n\nl with Vl \u2208 RP \u00d7P being trainable parameters. To We parameterise Gram matrices Gl = 1 make initializations with different seeds sufficiently separated while ensuring stability we initialize Gl from a broad distribution that depends on K(Gl\u22121). Specifically, we first take the Cholesky decomposition K(Gl\u22121) = Ll\u22121LT l where each entry of \u039el \u2208 RP \u00d7P is independently sampled from a standard Gaussian, and Dl is a diagonal scaling matrix with each entry sampled i.i.d. from an inverse-Gamma distribution. The variance of the inverse-Gamma distribution is fixed to 100, and the mean is drawn from a uniform distribution U [0.5, 3] for each seed. Since for any random variable x \u223c Inv-Gamma(\u03b1, \u03b2), E(x) = \u03b2 (\u03b1\u22121)(\u03b1\u22122) , once we fix the mean and variance we can compute \u03b1 and \u03b2 as\n\nl\u22121, then set Vl = Ll\u22121\u039elD1/2\n\n\u03b1\u22121 and V(x) =\n\n\u03b2\n\n\u03b1 =\n\nE(x)2 V(x) \u03b2 = E(x)(\u03b1 \u2212 1).\n\n+ 2,\n\n(141)\n\n(142)\n\nWe set \u03bdl = 5, and use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.001 to optimize parameters Vl described above. We fixed all model hyperparameters to ensure that any multimodality could emerge only from the underlying deep kernel machine. As we did not use inducing points, we were forced to consider only the smaller UCI datasets (yacht, boston, energy and concrete). For the deep kernel machine objective, all Gram matrices converge rapidly to the same solution, as measured by RMSE (Fig. 12). Critically, we did find multiple modes for the MAP objective (Fig 13), indicating that experiments are indeed powerful enough to find multiple modes (though of course they cannot be guaranteed to find them). Finally, note that the Gram matrices took a surprisingly long time to converge: this was largely due to the high degree of diversity in the initializations; convergence was much faster if we initialised deterministically from the prior.\n\n30\n\nFigure 12: One-layer DKMs with squared exponential kernel trained on full UCI datasets (through columns) converges to the same solution, despite very different initializations by applying stochastic diagonal scalings described in Appendix F to the standard initialization with different seeds. Standard initialization is shown in dashed line, while scaled initializations are the color lines each denoting a different seed. The first row shows the objective during training for all seeds that all converge to the same value. The second row shows the element-wise RMSE between the Gram matrix of each seed and the optimized Gram matrix obtained from the standard initialization. RMSE converges to 0 as all initializations converge on the same maximum. The last row plots RMSE versus objective value, again showing a single optimal objective value where all Gram matrices are the same.\n\nThis might contradict our usual intuitions about huge multimodality in the weights/features of BNNs and DGPs. This can be reconciled by noting that each mode, written in terms of Gram matrices, corresponds to (perhaps infinitely) many modal features. In particular, in Sec. E, we show that the log-probability for features, P (Fl|Fl\u22121) (Eq. 82) depends only on the Gram matrices, and note that there are many settings of features which give the same Gram matrix. In particular, the Gram matrix is the same for any unitary transformation of the features, F\u2032 l = FlU, satisfying UUT = I, as l = 1 1\nl = Gl. For DGPs we can use any unitary matrix, so there Nl are infinitely many sets of features consistent with a particular Gram matrix, while for BNNs we can only use permutation matrices, which are a subset of unitary matrices. Thus, the objective landscape must be far more complex in the feature domain than with Gram matrices, as a single optimal Gram matrix corresponds to a large family of optimal features.\n\nFlUlUT\n\nl = 1\n\nFlFT\n\nl FT\n\nlF\u2032T\n\nF\u2032\n\nNl\n\nNl\n\n31\n\n500490480objectiveyacht860850840boston111011001090energy174017301720concrete050000100000iters0.00.51.0RMSE050000100000iters050000100000iters050000100000iters500490480objective0.00.51.0RMSE860850840objective111011001090objective174017301720objectiveFigure 13: One-layer DGP with MAP inference over features as described in Appendix E Eq. (84). Rows and columns are the same as in Figure 12. Using the same randomly scaled initializations described above, we are able to find multiple modes in energy and concrete showing our initializations are diverse enough, albeit there is still only a single global optimum.\n\n32\n\n46504700objectiveyacht64606510boston1240012450energy1575015800concrete050000100000iters0.00.51.0RMSE050000100000iters0100000200000iters050000100000iters46504700objective0.00.51.0RMSE64606510objective1240012450objective1575015800objectiveL INDUCING POINT DKMS\n\nTo do large-scale experiments on UCI datasets, we introduce inducing point DKMs by extending Gaussian process inducing point methods (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) to the DKM setting. This approach uses the variational interpretation of the deep kernel machine objective described in Appendix D.\n\nTo do inducing-point variational inference, we need to explicitly introduce top-layer features mirroring FL+1 \u2208 RP \u00d7\u03bdL+1 in Appendix B, but replicated N times, \u0303FL+1 \u2208 RP \u00d7NL+1. Formally, each feature, \u0303f L+1\n\n, . . . , \u0303f L+1 NL+1\n\n1\n\nis IID, conditioned on FL, \u03bb=1N (cid:0) \u0303f L+1 \u03bb=1N (cid:0) \u0303y\u03bb; \u0303f L+1\n\nP ( \u0303FL+1|FL) = (cid:81)Nl P ( \u0303Y| \u0303FL+1) = (cid:81)Nl\n\n\u03bb\n\n\u03bb\n\n; 0, K(G(FL))(cid:1) , , \u03c32I(cid:1) ,\n\n(143a)\n\n(143b)\n\nwhere we give the likelihood for regression, but other likelihoods (e.g. for classification) are possible (Appendix B).\n\nFurther, we take the total number of points, P , to be made up of Pi inducing points and Pt test/train points, so that P = Pi + Pt. Thus, we can separate all features, Fl \u2208 RP \u00d7Nl, into the inducing t \u2208 RPt\u00d7Nl. Likewise, we separate the inputs, features, Fl X, and outputs, Y, into (potentially trained) inducing inputs, Xi, and trained inducing outputs, Yi, and the real test/training inputs, Xt, and outputs, Yt,\n\ni \u2208 RPi\u00d7Nl, and the test/train features, Fl\n\nFl =\n\n(cid:19)\n\n(cid:18)Fl i\nFl t\n\n \u0303FL+1 =\n\n(cid:18) \u0303FL+1 \u0303FL+1\n\ni\n\nt\n\n(cid:19)\n\nX =\n\n(cid:19)\n\n(cid:18)Xi Xt\n\nY =\n\n(cid:19)\n\n(cid:18)Yi Yt\n\n \u0303Y =\n\n(cid:19)\n\n(cid:18) \u0303Yi \u0303Yt\n\n(144)\n\nWe follow the usual doubly stochastic inducing point approach for DGPs. In particular, we treat all the features at intermediate layers, F1, . . . , FL, and the top-layer train/test features, FL+1 as latent variables. However, we deviate from the usual setup in treating the top-layer inducing outputs, FL+1 , as learned parameters and maximize over them to ensure that the ultimate method does not require sampling, and at the same time allows minibatched training. The prior and approximate posterior over F1, . . . , FL are given by,\n\ni\n\nt\n\nQ (F1, . . . FL|X) = (cid:81)L P (F1, . . . , FL|X) = (cid:81)L\n\nl=1 Q (Fl|Fl\u22121) , l=1 P (Fl|Fl\u22121) ,\n\n(145a)\n\n(145b)\n\nand remember F0 = X, so G0 = 1 N0 factorises into a distribution over the inducing points and a distribution over the test/train points,\n\nXXT . The prior and approximate posterior at each layer\n\nQ (Fl|Fl\u22121) = P (cid:0)Fl P (Fl|Fl\u22121) = P (cid:0)Fl\n\nt |Fl t |Fl\n\ni , Fl\u22121 i , Fl\u22121\n\n(cid:1) Q (cid:0)Fl (cid:1) P (cid:0)Fl\n\n(cid:1) , i |Fl\u22121\n\ni\n\ni\n\n(cid:1) .\n\n(146a)\n\n(146b)\n\nthe approximate posterior samples for the test/train points is the conditional prior (cid:1), which is going to lead to cancellation when we compute the ELBO. Likewise,\n\nCritically, P (cid:0)Fl t |Fl the approximate posterior over \u0303FL+1\n\ni , Fl\u22121\n\nt\n\nis the conditional prior, (cid:1) = P (cid:0) \u0303FL+1\n\n, FL\n\n|FL+1\n\nt\n\ni\n\nQ (cid:0) \u0303FL+1\n\nt\n\n|FL+1\n\ni\n\n, FL\n\n(cid:1) .\n\n(147)\n\nConcretely, the prior approximate posterior over inducing points are given by,\n\nQ (cid:0)Fl\n\n\u03bb=1N (cid:0)f l \u03bb=1N (cid:0)f l The approximate posterior is directly analogous to Eq. (69) and the prior is directly analogous to Eq. (1a), but where we have specified that this is only over inducing points. Now we compute the ELBO\n\ni;\u03bb; 0, Gl i;\u03bb; 0, K(G(Fl\u22121\n\n(cid:1) = (cid:81)Nl (cid:1) = (cid:81)Nl\n\ni |Fl\u22121\n\nP (cid:0)Fl\n\n(148b)\n\n(148a)\n\n))(cid:1)\n\n(cid:1) ,\n\nii\n\ni\n\ni\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii ) (cid:34)\n\n= EQ\n\nlog P (cid:0) \u0303Yt| \u0303FL+1\n\nt\n\ni\n\n|FL+1 |FL+1\n\ni\n\n, FL\n\n, FL\n\n(cid:1) P (F1, . . . FL|X) (cid:1) Q (F1, . . . FL|X)\n\n(cid:35)\n\n(149)\n\n(cid:1) + log\n\nP (cid:0) \u0303FL+1 Q (cid:0) \u0303FL+1\n\nt\n\nt\n\n33\n\nNote that the P (cid:0)Fl we come to describing sampling), substituting Eq. (145\u2013147) and cancelling P (cid:0)Fl P (cid:0) \u0303FL+1\n\n(cid:1) terms are going to cancel in the ELBO, we consider them below when (cid:1) and\n\ni , Fl\u22121\n\ni , Fl\u22121\n\n|FL+1\n\nt |Fl\n\nt |Fl\n\n(cid:1),\n\n, FL\n\nt\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii ) = EQ\n\n(cid:34)\n\nlog P (cid:0) \u0303Yt| \u0303FL+1\n\nt\n\n(cid:1) +\n\nL (cid:88)\n\nl=1\n\nlog\n\nP (cid:0)Fl\n\ni |Fl\u22121 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(150)\n\nSo far, we have treated the Gram matrices, Gl ii as parameters of the approximate posterior. However, in the infinite limit N \u2192 \u221e, these are consistent with the features generated by the approximate (cid:0)Fl posterior. In particular the matrix product 1 can be written as an average over infinitely Nl many IID vectors, f l i;\u03bb (first equality), and by the law of large numbers, this is equal to the expectation of one term (second equality), which is Gl\n\nii (by the approximate posterior Eq. (148a)),\n\nFl i\n\n(cid:1)T\n\ni\n\nlim N\u2192\u221e\n\n1 Nl\n\nFl\n\ni\n\n(cid:1)T\n\n(cid:0)Fl\n\ni\n\n= lim\n\nN\u2192\u221e\n\n1 Nl\n\n(cid:80)Nl\n\n\u03bb=1f l\n\ni;\u03bb\n\n(cid:1)T\n\n(cid:0)f l\n\ni;\u03bb\n\n= E\n\nQ(f l\n\ni;\u03bb)\n\n(cid:104)\n\nf l i;\u03bb\n\n(cid:0)f l\n\ni;\u03bb\n\n(cid:1)T (cid:105)\n\n= Gl ii.\n\n(151)\n\nBy this argument, the Gram matrix from the previous layer, Gl\u22121 through Gl\u22121 DGP, Fl factorise. Thus, in the infinite limit, individual terms in the ELBO can be written,\n\nis deterministic. Further, in a (Eq. 5), and the prior and approximate posterior\n\ni only depends on Fl\u22121\n\nii\n\ni\n\ni\n\n(cid:34)\n\nlim N\u2192\u221e\n\n1 N\n\nEQ\n\nlog\n\nP (cid:0)Fl\n\ni |Fl\u22121 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n\uf8ee\n\n= \u03bdl EQ\n\n\uf8f0log\n\n(cid:16)\n\nP\n\ni;\u03bb|Gl\u22121 f l (cid:16)\n\ni (cid:17)\n\nQ\n\nf l i;\u03bb\n\n(cid:17)\n\n\uf8f9\n\n\uf8fb\n\n= \u2212\u03bdl DKL\n\n(cid:0)N (cid:0)0, K(Gl\u22121\n\ni\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\ni\n\n(152)\n\n(153)\n\n(cid:1)(cid:1) ,\n\nwhere the final equality arises when we notice that the expectation can be written as a KLdivergence. The inducing DKM objective, Lind, is the ELBO, divided by N to ensure that it remains finite in the infinite limit,\n\nLind(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii )= lim\n\nN\u2192\u221e\n\n1\n\nN ELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii )\n\n(154)\n\n= EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) \u2212\n\nL (cid:88)\n\nl=1\n\n\u03bdl DKL\n\n(cid:0)N (cid:0)0, K(Gl\u22121\n\nii\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\nii\n\n(cid:1)(cid:1) .\n\nNote that this has almost exactly the same form as the standard DKM objective for DGPs in the main text (Eq. 16). In particular, the second term is a chain of KL-divergences, with the only difference that these KL-divergences apply only to the inducing points. The first term is a \u201cperformance\u201d term that here depends on the quality of the predictions given the inducing points. As the copies are IID, we have,\n\nEQ\n\n(cid:2)log P (cid:0) \u0303Yt| \u0303FL+1\n\nt\n\n(cid:1)(cid:3) = N EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) .\n\n(155)\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nNow that we have a simple form for the ELBO, we need to compute the expected likelihood, (cid:1)(cid:3). This requires us to compute the full Gram matrices, including test/train EQ points, conditioned on the optimized inducing Gram matrices. We start by defining the full Gram matrix,\n\nt\n\nGl =\n\n(cid:18)Gl Gl\n\nii Gl ti Gl\n\ntt\n\nit\n\n(cid:19)\n\n(156)\n\nfor both inducing points (labelled \u201ci\u201d) and test/training points (labelled \u201ct\u201d) from just Gl ii. For clarity, we have Gl \u2208 RP \u00d7P , Gl tt \u2208 RPt\u00d7Pt, where Pi is the number of inducing points, Pt is the number of train/test points and P = Pi + Pt is the total number of inducing and train/test points.\n\nii \u2208 RPi\u00d7Pi, Gl\n\nti \u2208 RPt\u00d7Pi, Gl\n\nThe conditional distribution over Fl\n\nt given Fl\n\ni is, \u03bb=1N (cid:0)f l (cid:1) = (cid:81)Nl\n\nP (cid:0)Fl\n\nt\n\n(cid:12) (cid:12)Fl\n\ni , Gl\u22121\n\nt;\u03bb; KtiK\u22121\n\nii f l\n\ni;\u03bb, Ktt\u00b7i\n\n(cid:1)\n\n(157)\n\n34\n\nAlgorithm 1 DKM prediction\n\nl=1\n\nii}L\n\nParameters: {\u03bdl}L Optimized Gram matrices {Gl Inducing and train/test inputs: Xi, Xt Inducing outputs: FL+1 Initialize full Gram matrix (cid:19) (cid:18)G0 G0\n\n(cid:18)XiXT XtXT\n\ni XiXT i XtXT\n\nii G0;T ti G0\n\n= 1 \u03bd0\n\nl=1\n\ntt\n\nti\n\ni\n\nt\n\nt\n\n(cid:19)\n\nPropagate full Gram matrix for l in (1, . . . , L) do\n\n(cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\nii\n\n(cid:18)(cid:18)Gl\u22121 Gl\u22121 ii KT ti .\n\nti\n\n(cid:19)(cid:19)\n\n)T\n\n(Gl\u22121 Gl\u22121\n\nti\n\ntt\n\nii\n\niiK\u22121\n\nii KT\n\nii Gl ii Gl\n\nti = KtiK\u22121 tt = KtiK\u22121\n\nKtt\u00b7i = Ktt \u2212 KtiK\u22121 Gl Gl end for Final prediction using standard Gaussian process expressions (cid:18)Kii KT (cid:18)(cid:18)GL (GL ii ti GL GL Kti Ktt tt ti , Ktt \u2212 KtiK\u22121\n\n= K Yt \u223c N (cid:0)KtiK\u22121\n\nti + Ktt\u00b7i\n\nti + \u03c32I(cid:1)\n\nii FL+1\n\nii KT\n\nti )T\n\n(cid:19)(cid:19)\n\n(cid:19)\n\ni\n\nwhere f l feature for all train/test inputs, and f l\n\nt;\u03bb is the activation of the \u03bbth feature for all train/test inputs, f l i;\u03bb, and (cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\n(cid:16) 1\n\nNl\u22121\n\n(cid:17)\n\nFl\u22121FT\n\nl\u22121\n\nKtt\u00b7i = Ktt \u2212 KtiK\u22121\n\nii KT ti .\n\n= K (Gl\u22121)\n\n(158)\n\n(159)\n\ni;\u03bb is the activation of the \u03bbth\n\nIn the infinite limit, the Gram matrix becomes deterministic via the law of large numbers (as in Eq. 151), and as such Git and Gtt become deterministic and equal to their expected values. Using Eq. (157), we can write,\n\nwhere \u039e is a matrix with IID standard Gaussian elements. Thus,\n\nFl\n\nt = KtiK\u22121\n\nii Fl\n\ni + K1/2\n\ntt\u00b7i \u039e.\n\nGl\n\nt (Fl\n\nE (cid:2)Fl \u03bd KtiK\u22121\n\nti = 1 \u03bd\n= 1 = KtiK\u22121\n\nii\n\nii Gl\n\nii\n\ni )T (cid:3) E (cid:2)Fl\n\ni (Fl\n\ni )T (cid:3)\n\nand,\n\nGl\n\nt (Fl\n\nE (cid:2)Fl\n\ntt = 1 \u03bd\n= 1 = KtiK\u22121\n\nt )T (cid:3) E (cid:2)Fl ii GiiK\u22121\n\n\u03bd KtiK\u22121\n\nii\n\ni (Fl ii KT\n\ni )T (cid:3) K\u22121 ti + Ktt\u00b7i\n\nii KT\n\nti + 1\n\n\u03bd K1/2\n\ntt\u00b7i\n\n(160)\n\n(161)\n\n(162)\n\n(163)\n\n(164)\n\n(165)\n\n(166)\n\nE (cid:2)\u039e\u039eT (cid:3) K1/2\n\ntt\u00b7i\n\nFor the full prediction algorithm, see Alg. 1.\n\n35",
      "responses": [
        null,
        "# Summary Of The Paper\nThis paper introduces a novel theoretical framework for representation learning in deep neural networks, termed the Bayesian representation learning limit. This new limit allows for the inclusion of representation learning within the context of deep Gaussian processes (DGPs) while retaining the advantages of infinite-width models. The authors propose a deep kernel machine (DKM) objective that optimizes posterior covariances through a combination of log-likelihood and KL-divergences. Additionally, a sparse version of the DKM is developed to enhance computational efficiency, achieving linear scalability with respect to data points. Empirical results validate the alignment of the proposed framework with traditional kernel methods and demonstrate its effectiveness in representation learning.\n\n# Strengths And Weaknesses\nThe main strengths of the paper include the introduction of the Bayesian representation learning limit, which addresses a significant gap in existing theoretical frameworks by integrating representation learning into infinite-width models. The formulation of the DKM objective is a notable contribution, as it provides a clear and effective method for optimizing posterior covariances. Furthermore, the development of a sparse DKM enhances its practicality, making it scalable for real-world applications. However, the paper could benefit from a more comprehensive empirical evaluation across various datasets and tasks to fully demonstrate the robustness and applicability of the proposed methods beyond the presented cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The theoretical developments are articulated with sufficient detail, making them accessible to readers familiar with the field. However, certain sections, particularly those involving the mathematical formulations, may require careful reading for full comprehension. The novelty of the framework is significant, particularly in its integration of representation learning with Bayesian approaches. The reproducibility of the results could be improved by providing additional implementation details and datasets used in the empirical evaluations.\n\n# Summary Of The Review\nOverall, this paper offers a substantial contribution to the understanding of representation learning in neural networks by introducing a new theoretical framework that successfully integrates deep Gaussian processes with practical optimization techniques. While the clarity and quality of the work are commendable, further empirical validation across varied tasks would strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper presents a novel theoretical framework for representation learning in neural networks and deep Gaussian processes, addressing limitations in existing approaches that fail to capture representation learning due to infinite-width limits. The authors introduce the \"Bayesian representation learning limit,\" which allows for effective representation learning while simplifying aspects of infinite-width models. Key contributions include the development of exact multivariate Gaussian posteriors in deep Gaussian processes, the introduction of deep kernel machines (DKMs) as a generalization of kernel methods, and a sparse variant that scales linearly with the number of data points. Experimental results demonstrate the effectiveness of DKMs compared to traditional methods, confirming theoretical predictions and showcasing improved performance on multiple UCI datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative theoretical contributions that successfully integrate representation learning into deep models, bridging the gap between neural networks and kernel methods. The generalizability of the framework to both deep Gaussian processes and Bayesian neural networks enhances its applicability across various scenarios. Additionally, the efficiency of the sparse DKM offers a significant advancement for large-scale applications. However, the paper also presents limitations, including potential complexity in the practical implementation of the proposed framework and reliance on Gaussian assumptions that may not universally apply. Furthermore, while the empirical validation is promising, further testing on diverse real-world datasets is necessary to substantiate the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making complex theoretical concepts accessible. The methodology is rigorously detailed, allowing for reproducibility of the experiments. The novelty of the work is significant, as it provides a new lens through which to view representation learning in neural networks, yet it raises questions about the practical implications of the proposed framework. Overall, the quality of writing and presentation is high, although some areas could benefit from additional clarity regarding implementation challenges.\n\n# Summary Of The Review\nThis paper makes substantial contributions to the understanding of representation learning in neural networks and deep Gaussian processes by introducing a new theoretical framework. While the results are promising and the methodology is solid, further validation and exploration of practical challenges are necessary to enhance the applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThis paper presents a novel framework for understanding representation learning in deep neural networks through the introduction of the **Bayesian representation learning limit**. The authors argue that this limit preserves representation learning capabilities while simplifying the underlying model. Key contributions include the formulation of **Deep Kernel Machines (DKMs)** that generalize traditional kernel methods and the introduction of a new objective that incorporates both likelihood and KL-divergence terms for optimizing multivariate Gaussian posteriors in deep Gaussian processes (DGPs). The methodology is empirically validated on UCI datasets, demonstrating that the DKM objective outperforms standard methods in terms of scalability and performance.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear articulation of the limitations of existing theoretical frameworks (NNGPs and NTK) in capturing representation learning and the introduction of a flexible, scalable method (sparse DKM) that addresses these issues. The theoretical contributions are well-founded, providing a robust mathematical foundation for the proposed ideas. However, a potential weakness lies in the complexity of the new Bayesian limit, which may pose challenges for broader comprehension among practitioners less familiar with advanced probabilistic modeling.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, making it accessible to readers with a background in machine learning. The quality of the theoretical exposition is high, and the empirical results support the theoretical claims effectively. In terms of novelty, the introduction of the Bayesian representation learning limit and DKMs represents a significant advancement in the field. The reproducibility of the results is likely feasible, given that the authors detail their experimental methodology, including the optimization techniques and datasets used.\n\n# Summary Of The Review\nOverall, this paper makes a compelling contribution to the understanding of representation learning within deep learning frameworks. The introduction of the Bayesian representation learning limit and the development of DKMs are innovative and provide practical benefits for scalability and performance. The paper is well-written and methodologically sound, although the complexity of some concepts may challenge wider accessibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel approach to representation learning in deep Bayesian models, specifically through the introduction of a new infinite width limit that retains representation capabilities in Deep Gaussian Processes (DGPs). The authors develop a theoretical framework that distinguishes between Neural Network Gaussian Processes (NNGPs) and Neural Tangent Kernels (NTK), offering insights into the mechanics of representation learning. Additionally, the paper introduces a Deep Kernel Machine (DKM) objective that balances performance optimization with regularization, and a sparse DKM that scales linearly with data points, significantly improving upon traditional kernel methods. Empirical results demonstrate the efficacy of these approaches, although the analysis is primarily confined to a limited set of datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its theoretical contributions and the introduction of scalable methods that enhance the usability of DGPs in practice. The distinction made between NNGPs and NTKs is particularly noteworthy for understanding different learning paradigms. However, weaknesses include the need for more extensive validation of the proposed methods in diverse real-world datasets and the potential complexity that may limit accessibility for practitioners not well-versed in Bayesian statistics. The balance in the DKM objective requires careful tuning, which is not addressed in depth, and the conditions under which the sparse DKM performs well need further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although some theoretical aspects may be challenging for readers lacking a strong background in Bayesian methods. The quality of the proposed frameworks and methodologies appears robust, but the practicality of implementation and reproducibility could be improved with additional empirical validation across varied domains. The novelty of the contributions is significant, particularly the theoretical advancements and the practical implications for scalable Bayesian learning.\n\n# Summary Of The Review\nThis paper makes substantial contributions to the field of representation learning in deep Bayesian models, presenting innovative theoretical frameworks and scalable methods. However, it could benefit from broader empirical validation and a more accessible presentation of complex ideas.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper presents a groundbreaking framework for representation learning in neural networks, termed the Bayesian Representation Learning Limit (BRLL). This approach addresses the limitations of the Neural Network Gaussian Process (NNGP) model by providing an alternative infinite-width limit that maintains the capacity for representation learning in Bayesian Neural Networks (BNNs) and Deep Gaussian Processes (DGPs). Key contributions include the characterization of DGP posteriors as multivariate Gaussians under the BRLL, the introduction of Deep Kernel Machines (DKMs) as a flexible generalization of kernel methods, and extensive experimental validation demonstrating the practical effectiveness of these methodologies while aligning with theoretical predictions.\n\n# Strengths And Weaknesses\nStrengths of the paper include its innovative theoretical perspective, which successfully bridges the gap between traditional infinite-width models and the complexities of representation learning. The robustness of the empirical results across diverse datasets adds significant weight to the authors' claims. However, the paper could improve by providing clearer implementation details for DKMs, especially concerning hyperparameter tuning and the performance of the sparse formulation in varying conditions. Additionally, the focus on DGPs and BNNs limits the discussion of the broader applicability of the BRLL framework to other neural architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, which enhances the clarity of the proposed methodologies. The quality of the experimental validation is high, with results that effectively demonstrate the advantages of the proposed methods. The novelty of the BRLL and DKMs is significant, offering fresh insights into representation learning that could influence future research directions. However, reproducibility may be hindered by the lack of detailed implementation guidance for DKMs.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the understanding of representation learning in neural networks, offering a well-founded theoretical framework coupled with practical methodologies. The combination of theoretical rigor and robust empirical validation positions this work as a valuable resource for researchers in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel theoretical framework for understanding adversarial training in neural networks, introducing the concept of the \"Adversarial Robustness Limit\" within the context of infinite-width models. The authors derive key results showing that, in this limit, the posteriors of neural networks maintain a multivariate Gaussian form, facilitating a better understanding of adversarial perturbations' impact on predictions. They propose a new objective function for adversarial training that integrates log-likelihood terms and regularization, develop a sparse adversarial training approach that scales linearly with the number of data points, and connect their findings to kernel methods by introducing the \"Deep Adversarial Kernel Machine\" (DAKM).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous theoretical contributions that bridge the gap between adversarial training and kernel methods, offering a fresh perspective on the robustness of neural networks. By introducing the Adversarial Robustness Limit, the authors provide a solid foundation for analyzing adversarial robustness in infinite-width networks. Additionally, the proposed sparse training method addresses computational efficiency, which is a common bottleneck in adversarial training. However, a notable weakness is the mathematical complexity in some derivations, which may hinder comprehension for readers not well-versed in the underlying theoretical concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured and presents its ideas in a logical flow. Definitions are clear, and the relationship between theory and application is made explicit. However, some mathematical sections could benefit from additional context or examples to enhance accessibility. The novelty of the proposed methods is significant, particularly in the context of infinite-width networks and their connection to kernel methods. The empirical validation is robust, demonstrating state-of-the-art performance in adversarial robustness. The reproducibility of results may be enhanced by providing clearer details on experimental setups and parameters.\n\n# Summary Of The Review\nThis paper makes a meaningful contribution to the field of adversarial training by offering a theoretical framework that enhances the understanding of robustness in neural networks while also presenting practical advances through the development of the DAKM. Although the clarity of some mathematical sections could be improved, the overall quality and significance of the work are commendable.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a theoretical framework for understanding representation learning in neural networks, introducing the concept of the Bayesian representation learning limit. The authors claim that this limit retains representation learning capabilities in deep Bayesian models and leads to exact multivariate Gaussian posteriors in Deep Gaussian Processes (DGPs). They also propose Deep Kernel Machines (DKMs) as a generalization of kernel methods and introduce a sparse DKM designed to scale linearly with the number of data points. Furthermore, the paper aims to extend these results to Bayesian Neural Networks (BNNs) with non-Gaussian posteriors and provides experimental validation of their claims.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambition to bridge connections between deep learning and kernel methods, particularly through the introduction of DKMs. However, the weaknesses are pronounced; many claims appear overstated, and the methodologies discussed largely rely on established techniques without significant novelty. The supposed breakthroughs in scalability and representation learning do not sufficiently extend beyond the existing state of knowledge in the field. Furthermore, the authors' assertions regarding the novelty of their contributions may misrepresent the foundational work that has already been accomplished in this area.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear in its presentation, but the novelty of the proposed methods and ideas does not live up to the claims made by the authors. The quality of the writing is adequate, but a more critical engagement with prior work is needed to contextualize the contributions. The reproducibility of the findings is uncertain, as the experimental validation appears to confirm existing theories rather than provide new insights.\n\n# Summary Of The Review\nOverall, this paper presents interesting ideas but falls short of delivering substantial advancements in the understanding of representation learning and kernel methods. The claims made regarding novelty and significance are largely overstated, and the paper does not provide a compelling case for its contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper introduces a novel concept called the Bayesian representation learning limit, which enhances representation learning in deep models, particularly through Deep Gaussian Processes (DGPs). It presents a new objective that combines log-likelihood with KL-divergences to derive exact multivariate Gaussian posteriors in this limit. Additionally, the paper proposes a Deep Kernel Machine (DKM) that generalizes kernel methods with improved scalability, achieving linear scaling with data points instead of cubic. The empirical results demonstrate that the DKM consistently outperforms existing methods, showcasing significant improvements in RMSE across multiple datasets.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its theoretical contribution of the Bayesian representation learning limit, which addresses previous gaps in the understanding of representation learning in neural networks. The clear derivation of multivariate Gaussian posteriors provides a solid foundation for the proposed methods. The empirical results are compelling, showing the DKM's superiority over traditional methods. However, a potential weakness is the reliance on specific datasets, which may limit generalizability. The paper could benefit from a more comprehensive discussion on the limitations of the proposed methods and their applicability to diverse settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical progression of ideas from theory to empirical results. The methodology is thoroughly explained, and the empirical findings are presented with sufficient detail for reproducibility. However, the novelty of the Bayesian representation learning limit could be more explicitly contrasted with existing approaches to emphasize its significance. Overall, the quality of the writing and presentation is high, making it accessible to the reader.\n\n# Summary Of The Review\nThis paper makes significant contributions to the field of representation learning in neural networks by introducing the Bayesian representation learning limit and the Deep Kernel Machine. The empirical results strongly support the advantages of the proposed methods over traditional approaches. Overall, the paper presents a compelling case for its findings while maintaining clarity and rigor.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThis paper introduces a novel infinite width limit termed the Bayesian representation learning limit, which challenges the conventional belief that infinite width simplifies representation learning. The authors utilize Gaussian processes as priors to elucidate the behavior of deep neural networks, positing that representation learning is central to their success. The paper also explores KL-divergence as a regularization mechanism and discusses the scalability of deep kernel machines (DKMs). Empirical evaluations support the theoretical assertions, although they raise questions about the generalizability of the results across various architectures and data distributions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to rethinking the infinite width assumption in deep learning, as well as its emphasis on the importance of representation learning. However, the reliance on Gaussian processes as a universal prior is a potential weakness, as it may not adequately capture the complexities of all neural architectures. Furthermore, the assumptions regarding independent and identically distributed (IID) features, as well as the focus on multivariate Gaussian posteriors, raise concerns about the applicability of the findings in practical scenarios. The empirical validation, while supportive, could benefit from a broader range of experiments to strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although some concepts could be elaborated further for clarity, particularly the implications of certain assumptions. The quality of the methodology is high, with a solid theoretical foundation. However, the novelty of the approach may be limited by the reliance on Gaussian processes, which may not be universally applicable. The reproducibility of the findings could be improved by providing more comprehensive experimental details and results across a wider array of architectures and conditions.\n\n# Summary Of The Review\nOverall, the paper provides a thought-provoking perspective on the role of representation learning in deep networks, supported by a solid theoretical framework. While it offers valuable insights, its reliance on specific assumptions and models may limit its broader applicability. Additional empirical work could strengthen its contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper introduces a novel framework termed the Bayesian representation learning limit, which aims to preserve representation learning in deep Bayesian models, specifically Deep Gaussian Processes (DGPs). The authors derive a deep kernel machine (DKM) objective that integrates likelihood and KL-divergences, thereby promoting effective representation learning. Additionally, they propose a sparse DKM to enhance scalability while extending their findings to Bayesian Neural Networks (BNNs). Empirical results demonstrate that the proposed methods facilitate exact multivariate Gaussian posteriors and establish a robust connection between DGPs and DKMs.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to bridging the gap between representation learning and Bayesian frameworks, particularly through the introduction of the Bayesian representation learning limit. The derivation of the DKM objective is well-justified and effectively addresses scalability through the sparse DKM. However, a potential weakness is the limited exploration of practical applications for the proposed methods; while the theoretical contributions are solid, empirical validations across diverse datasets and tasks would enhance the paper's impact. Furthermore, the distinction between NNGPs and the proposed approach could be elaborated for clearer contextual understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear definitions and logical progression of ideas. The quality of the research is high, as it builds on established theories while presenting new insights. The novelty is significant, particularly in the context of deep learning and Bayesian methods. However, reproducibility may be a concern as specific implementation details and datasets used for empirical validation are not fully detailed. Including supplementary material or a dedicated section on experimental setup would bolster reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in the understanding of representation learning within the context of DGPs and Bayesian models. The introduction of the DKM and its implications for scalability and theoretical insights are noteworthy contributions, though further empirical exploration would enhance the paper's practical relevance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel framework for enhancing the robustness of deep learning models against adversarial attacks. The authors introduce a new regularization technique that incorporates a multi-faceted approach to training, combining traditional loss functions with additional penalties for model instability under perturbations. Through extensive experiments on benchmark datasets, the findings demonstrate that this method significantly improves model performance against adversarial examples while maintaining competitive results on clean data.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Novel Approach**: The introduction of a multi-faceted regularization technique adds a fresh perspective to the existing body of work on adversarial robustness.\n2. **Empirical Evidence**: The authors provide a thorough empirical evaluation, showcasing the effectiveness of their approach across several datasets and against various types of adversarial attacks.\n3. **Clear Presentation**: The paper is well-structured, with a logical flow that makes the complex ideas accessible to a wide audience.\n\n**Weaknesses:**\n1. **Limited Scope of Evaluation**: While the experiments are comprehensive, the evaluation only includes a limited number of adversarial attack methods, which could bias the findings.\n2. **Scalability Concerns**: The method\u2019s scalability to larger, real-world datasets is not adequately addressed, raising questions about its practical implementation.\n3. **Theoretical Motivation**: The theoretical underpinnings of the proposed regularization technique could be more thoroughly developed to strengthen the justification for its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written with clarity, and the quality of the figures and tables supports its findings effectively. The novelty of the proposed technique stands out, particularly in how it attempts to address adversarial robustness, a critical challenge in machine learning. However, the reproducibility of the results could be improved by providing more detailed descriptions of the experimental setup, including hyperparameter choices and training procedures.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of adversarial machine learning by proposing an innovative regularization technique that enhances model robustness. Although the empirical results are promising, further exploration of the method's limitations and broader applicability would strengthen the paper\u2019s impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper introduces a novel theoretical framework for understanding representation learning in neural networks through the concept of the Bayesian representation learning limit. It highlights the limitations of existing methods like the neural network Gaussian process (NNGP) and neural tangent kernel (NTK) in capturing the essence of representation learning in infinite-width settings. The authors develop deep kernel machines (DKMs) that generalize kernel methods, maintaining the flexibility of deep learning while providing a coherent mathematical foundation. Empirical results demonstrate that the feature distributions and learned representations in this framework closely align with those from finite-width deep Gaussian processes (DGPs), while also leading to a scalable sparse DKM.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical contributions that address the shortcomings of existing models in understanding representation learning. The introduction of the Bayesian representation learning limit is a significant advancement, providing a fresh perspective on DGPs and their relationship to kernel methods. Additionally, the empirical validation of the proposed methods strengthens the claims made. However, the paper may benefit from a more detailed discussion of the practical implications of the findings and potential limitations in real-world applications, as well as the computational challenges associated with approximating exact posteriors in Bayesian neural networks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with the subject matter. The methodology is presented with sufficient detail, allowing for reproducibility of the theoretical results. The novelty of the approach is high, as it provides a new theoretical lens through which to view representation learning in deep networks. However, further elaboration on the empirical results and their significance could enhance the paper's overall impact.\n\n# Summary Of The Review\nOverall, the paper makes substantial contributions to the theoretical understanding of representation learning in neural networks and presents a novel framework that unifies deep learning and kernel methods. While the theoretical and empirical results are compelling, a deeper exploration of practical implications and computational considerations would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel theoretical framework for representation learning in neural networks, specifically through the perspective of deep Gaussian processes (DGPs). It introduces a Bayesian representation learning limit that retains the capabilities of representation learning in deep Bayesian models while simplifying the analytical process. The authors demonstrate that this framework leads to multivariate Gaussian posteriors derived from a deep kernel machine (DKM) objective and propose a sparse DKM that enhances computational efficiency. Empirical validation showcases the alignment of learned representations and posterior distributions with predictions from the DKM framework, particularly as the network width increases.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to bridging deep learning and kernel methods through Bayesian inference, potentially offering significant insights into representation learning. The development of a sparse DKM that scales linearly with data size is a notable contribution that addresses efficiency concerns in traditional kernel methods. However, a potential weakness is the reliance on empirical validation with finite DGPs, which may limit the generalizability of the findings. Additionally, the theoretical derivations, while robust, may require further clarification for readers less familiar with the underlying concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that allows for easy navigation through its theoretical and empirical contributions. The quality of the methodology appears solid, although some sections could benefit from more detailed explanations, particularly regarding the derivation of the Bayesian representation learning limit. The novelty of the approach is significant, as it presents a fresh perspective on the relationship between deep learning and kernel methods. Reproducibility appears feasible, given that empirical results are provided, although the paper could strengthen this aspect by including supplementary materials or code for the proposed methods.\n\n# Summary Of The Review\nThis paper offers a compelling theoretical advancement in the understanding of representation learning in neural networks, connecting it to kernel methods through a Bayesian lens. While the contributions are significant and well-founded, some areas could benefit from enhanced clarity and additional details to further support reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThis paper introduces a novel theoretical framework for understanding representation learning in neural networks, specifically through the lens of deep Gaussian processes (DGPs). The authors propose the concept of Bayesian representation learning limits and introduce Deep Kernel Machines (DKMs) as a significant generalization of traditional kernel methods. The methodology includes deriving the multivariate Gaussian nature of DGP posteriors and formulating a DKM objective that balances performance with regularization. Empirical results demonstrate the effectiveness of the DKM objective against finite-width DGPs, showcasing its scalability and practical implications.\n\n# Strength And Weaknesses\nThe paper's primary strengths lie in its innovative approach to representation learning and the clear articulation of its contributions to the existing literature. It successfully combines theoretical insights with practical applications, paving the way for scalable methods in machine learning. However, the complexity of the new framework may pose challenges for readers, particularly in grasping its implications. Moreover, while the authors address scalability through the development of a sparse DKM, further empirical validation across diverse datasets and real-world scenarios could strengthen their claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear presentation of its contributions and methodology. The theoretical foundations are solid, and the empirical validation supports the claims made. However, the complexity of the concepts may hinder reproducibility for practitioners unfamiliar with advanced statistical methods. The novelty of the approach is commendable, but clearer illustrations or examples could enhance understanding, especially for readers less versed in the intricacies of DGPs and kernel methods.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the understanding of representation learning within neural networks by introducing a Bayesian perspective and developing the concept of Deep Kernel Machines. While the theoretical contributions are robust and the empirical results are promising, the complexity of the framework may limit accessibility and applicability in practical scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel theoretical framework termed the Bayesian Representation Learning Limit, which aims to reconcile the aspects of representation learning in deep Bayesian models, specifically Deep Gaussian Processes (DGPs), while retaining the simplicity of conventional infinite-width models. The authors establish that, under this limit, DGPs yield multivariate Gaussian posteriors with posterior covariance matrices optimized through a combination of log-likelihood and KL-divergence terms. Empirical evaluations demonstrate the effectiveness of this framework, showing convergence to the proposed representations as layer width increases, alongside extensions to Bayesian Neural Networks with non-Gaussian posteriors.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to bridging the gap between traditional infinite-width models and the complexities of representation learning in DGPs. The introduction of the Bayesian Representation Learning Limit is a significant contribution that enhances our understanding of how representations can be effectively learned in deep models. However, a notable weakness is the potential lack of clarity in some sections, particularly regarding the implications and interpretations of the theoretical constructs, which may hinder the accessibility of the findings to a broader audience. Additionally, while empirical validation is conducted, further experiments across diverse datasets and tasks would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents a coherent narrative. However, certain technical details could be elaborated for improved clarity, especially for readers less familiar with the underlying mathematics. The quality of the writing is high, and the theoretical contributions are novel, addressing a significant gap in the literature on representation learning. The reproducibility of the findings could be enhanced by providing more comprehensive details on the experimental setup and data used in the empirical validation, including potential code availability.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of representation learning in deep Bayesian models by introducing the Bayesian Representation Learning Limit. While the theoretical framework is novel and well-founded, the clarity of presentation and the depth of empirical validation could be improved to maximize impact and accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a new infinite width limit termed the Bayesian representation learning limit, which aims to retain representation learning capabilities within a theoretical framework. The authors propose a deep kernel machine (DKM) objective and extend their methodology to Bayesian neural networks (BNNs). However, the paper's demonstration of how this new limit differs significantly from existing methods lacks clarity, and the claims of its importance appear overstated given the existing literature on infinite width limits.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its intention to advance the understanding of representation learning through a novel theoretical framework and the introduction of a sparse DKM aimed at improving scalability. However, the weaknesses are notable: the theoretical constructs are abstract and may not translate effectively into practical applications. The simplification of complex real-world data into multivariate Gaussian posteriors in Deep Gaussian processes is an oversimplification. Furthermore, the computational inefficiencies, particularly the cubic scaling with data points, raise concerns about the method's practicality for large datasets. The experimental validation is limited and fails to cover a diverse array of datasets, which undermines the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to the abstract nature of its theoretical constructs, which may be challenging for practitioners to grasp and apply. The quality of the experiments is insufficient, lacking a comprehensive evaluation across different scenarios, which diminishes the reproducibility of the findings. While the paper claims novelty in its approach, it does not convincingly differentiate itself from existing literature, particularly regarding the implications for representation learning.\n\n# Summary Of The Review\nOverall, the paper presents an interesting theoretical framework, but it fails to provide convincing evidence of its significance or practicality. The claims of novelty are not substantiated by robust empirical validation, and the theoretical constructs may not be applicable in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper introduces a novel framework for representation learning in neural networks, termed the **Bayesian representation learning limit**, which effectively bridges the gap between infinite-width models and practical finite networks. A key contribution is the development of **Deep Kernel Machines (DKMs)**, which integrate the flexibility of deep learning with the interpretability of kernel methods. The authors demonstrate that the posteriors in this framework are exactly multivariate Gaussian, enhancing computational efficiency. The proposed **DKM objective** combines log-likelihood with KL-divergence for improved performance and regularization. Additionally, the introduction of a **sparse DKM** enables linear scalability with respect to the number of data points, making it suitable for large datasets. Empirical validation confirms the theoretical findings, establishing a unique global maximum that facilitates consistent model performance.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative contributions, particularly the introduction of DKMs and the exact Gaussian posteriors, which offer significant advancements in both theoretical understanding and practical application. The scalability of the sparse DKM is a notable advantage for real-world applications. However, while the empirical validation supports the theoretical claims, the generalizability of the results across diverse datasets and tasks could be further explored. Additionally, the framework's extension to Bayesian Neural Networks, while promising, requires more detailed exploration to assess its full potential.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical foundations and practical implications of the proposed methods. The quality of the writing is high, with a logical flow that guides the reader through complex concepts. The novelty of the contributions is substantial, as they provide a fresh perspective on representation learning and kernel methods. The reproducibility of the results appears to be strong, given the empirical validation provided; however, additional details on implementation and experimental setup would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in representation learning, combining theoretical insights with practical implementations that promise to enhance machine learning methodologies. The innovative concepts and empirical findings collectively pave the way for broader applications and deeper exploration in the field. \n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a theoretical framework for representation learning in neural networks, highlighting the limitations of traditional approaches based on Neural Network Gaussian Processes (NNGPs). It introduces the concept of the Bayesian representation learning limit, which allows for meaningful learning of representations even in high-dimensional spaces by balancing the prior and likelihood. The work also explores Deep Kernel Machines (DKMs) as a synthesis of kernel methods and deep learning, demonstrating how the DKM objective can provide insights into optimizing representations while maintaining closeness to prior distributions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contributions that provide a deep understanding of representation learning, thereby advancing the discourse surrounding the interplay between deep learning and Bayesian principles. Specifically, the introduction of the Bayesian representation learning limit and the characterization of deep Gaussian processes as multivariate Gaussian posteriors are notable achievements. However, a potential weakness is the limited emphasis on empirical validation, which may reduce the applicability of the theoretical constructs in practical scenarios. While the theoretical insights are robust, the reliance on these constructs without comprehensive empirical support could hinder broader acceptance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its theoretical contributions, making complex ideas accessible to the reader. The quality of the writing is high, with precise explanations of intricate concepts such as the DKM objective and its implications for representation learning. The novelty of the work is significant, as it provides a fresh perspective on the relationship between deep learning and kernel methods. However, the reproducibility of the theoretical claims may be challenging due to the abstract nature of the concepts presented, which could benefit from concrete examples or practical implementations.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the understanding of representation learning in neural networks through a novel theoretical framework. While the theoretical insights are compelling and well-articulated, the lack of empirical validation could limit their immediate impact in practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper introduces a novel infinite width limit termed the Bayesian representation learning limit for deep Gaussian processes (DGPs) and Bayesian neural networks (BNNs). This approach innovatively adjusts the scaling of likelihoods and priors to retain representation learning. The methodology includes the use of a sparse deep kernel method (DKM) which scales linearly with the number of data points, contrasting with traditional cubic scaling. The experiments conducted on standard UCI datasets demonstrate that the proposed DKM outperforms traditional methods, with results showing consistency in learned representations across different initializations.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing scalability issues in Gaussian processes through the introduction of inducing points, providing a practical approach for large-scale datasets. The use of Langevin sampling for Bayesian inference adds depth to the methodology, allowing for robust exploration of the posterior. However, the paper lacks explicit code availability and detailed theoretical insights into the broader implications of the Bayesian representation learning limit. Additionally, while the empirical results are promising, they are confined to standard datasets, which may limit their generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates the methodology and experimental results. However, the lack of explicit links to code and detailed descriptions of the experimental setup may hinder reproducibility. The novelty of the proposed methods is significant, particularly in the context of scaling deep Gaussian processes, although the theoretical underpinnings could be expanded to enhance understanding.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of Bayesian representation learning by introducing a scalable approach to deep Gaussian processes. While the empirical results are compelling, the lack of reproducibility support and deeper theoretical exploration are notable limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces a novel framework termed the Bayesian representation learning limit, which the authors claim represents a significant advancement over existing methodologies, particularly in the realm of representation learning. The methodology involves constructing deep kernel machines (DKMs) that are purported to generalize kernel methods in a flexible manner. The authors assert that their approach improves performance in deep models while enabling better representation learning, and they present a sparse DKM that scales linearly with the number of data points. However, the paper fails to provide sufficient comparisons with existing frameworks, particularly those like the neural tangent kernel (NTK) and other kernel methods, which have explored similar concepts.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to introduce a new limit and a novel framework for representation learning. However, it suffers from several weaknesses, notably a lack of adequate comparisons to existing works that have already made strides in similar areas. The claims of superiority over existing methods are not well-supported, leading to questions about the originality and significance of the contributions. In particular, the comparisons with kernel methods and variational inference techniques are biased and inadequately addressed, undermining the paper's credibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly, but the novelty of the contributions is questionable given the existing literature. The quality of the experimental validation is insufficient, as it does not provide a balanced view of performance relative to a wider array of established models. Reproducibility may be a concern due to the lack of comprehensive comparisons and performance metrics against prior research, which could hinder other researchers from effectively building upon these findings.\n\n# Summary Of The Review\nOverall, the paper puts forward interesting concepts regarding Bayesian representation learning and deep kernel machines, but it fails to adequately position these contributions within the existing literature. The lack of rigorous comparisons and acknowledgment of prior work diminishes the perceived novelty and significance of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper presents a novel theoretical framework for representation learning in neural networks, establishing connections to kernel methods through a deep generalization perspective. The authors utilize mathematical modeling to derive insights into the limitations and capabilities of various neural network architectures, particularly under infinite-width conditions. The findings suggest that certain neural network configurations can effectively mimic kernel methods, providing pathways for improved understanding of deep learning dynamics and representation learning.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous theoretical contributions, which bridge the gap between neural networks and kernel methods, shedding light on their shared properties. The methodology is robust, employing clear mathematical formulations to support the claims made throughout the study. However, the paper has some weaknesses, including occasional unclear language and inconsistent notation that may hinder reader comprehension. Additionally, the empirical validation of the theoretical claims could be more thoroughly explored to strengthen the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper's theoretical contributions are significant, the clarity of presentation could be improved. Inconsistencies in terminology, notation, and formatting detract from the overall quality. The novelty in connecting neural networks with kernel methods is commendable, although the paper would benefit from clearer exposition to enhance reproducibility and understanding. A more detailed empirical evaluation could bolster the claims made, providing a stronger foundation for the theoretical insights.\n\n# Summary Of The Review\nThis paper offers valuable theoretical insights into representation learning in neural networks by connecting it with kernel methods. Despite some clarity and consistency issues, the contributions are novel and significant. An enhanced empirical component would further strengthen the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper presents a novel approach to Bayesian representation learning through the introduction of deep kernel machines (DKMs). The authors propose DKMs as a flexible generalization of traditional kernel methods, aiming to enhance the expressiveness of representations while maintaining uncertainty quantification. The methodology focuses on the KL-divergence terms within the DKM objective, offering theoretical insights and preliminary experimental validation based on specific UCI datasets. Findings suggest that DKMs can effectively capture complex data distributions, although their application is limited to certain types of datasets.\n\n# Strength And Weaknesses\nThe paper contributes valuable theoretical insights into representation learning and provides a novel framework with DKMs, which could potentially enhance existing kernel methods. However, the scope of the experimental validation is too narrow, limited to UCI datasets, which does not adequately demonstrate the versatility of the proposed methods. Additionally, the authors do not thoroughly address how DKMs could be adapted for non-Euclidean spaces or the implications of sparsity on model interpretability. The paper also lacks a comprehensive exploration of the relationship between the proposed models and existing deep learning architectures, which could limit its impact within the broader machine learning community.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections that guide the reader through the theoretical developments. However, some theoretical insights, particularly regarding the interpretation of learned representations, could be expanded for better practical understanding. The novelty of the proposed DKMs is notable, yet the paper does not sufficiently address their empirical performance across diverse tasks or their scalability. Reproducibility is somewhat hampered by the limited experimental setup and lack of detailed discussion on hyperparameter sensitivities and model training stability.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to Bayesian representation learning through deep kernel machines, but it falls short in empirical validation, exploration of broader applicability, and detailed discussions on model interpretability and scalability. The theoretical contributions are valuable, yet the practical implications require further elaboration to enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper introduces a novel concept termed the **Bayesian representation learning limit**, which facilitates representation learning in deep Bayesian models while ensuring statistical rigor. The authors focus on **Deep Gaussian Processes (DGPs)**, demonstrating that in this limit, the models yield multivariate Gaussian posteriors. The methodology involves optimizing a log-likelihood combined with KL-divergences to achieve robust statistical inference. Key contributions include the formulation of a **deep kernel machine objective**, the introduction of a **sparse deep kernel machine** for computational efficiency, and an extension to **Bayesian Neural Networks (BNNs)**, all of which emphasize the importance of statistical properties in representation learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical contributions, particularly the introduction of the Bayesian representation learning limit, which enhances understanding of convergence properties in Bayesian inference. The empirical validation of theoretical claims through statistical evaluations and the focus on performance metrics like RMSE further bolster its credibility. However, a potential weakness is the complexity of the proposed methodologies, which may limit accessibility for practitioners not well-versed in statistical techniques. Additionally, while the paper thoroughly discusses statistical properties, it could benefit from a broader exploration of practical applications and implications in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings, making it accessible to readers familiar with Bayesian inference and deep learning. The quality of the writing is high, with a logical flow that effectively connects theoretical insights to empirical results. The novelty is significant, as it bridges representation learning and statistical inference in a manner that has not been extensively explored in existing literature. Reproducibility is supported through detailed descriptions of statistical methods and empirical validation; however, supplementary materials or code availability would enhance the reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of Bayesian deep learning by linking representation learning to rigorous statistical principles. Its innovative approach and empirical validation provide a solid foundation for future research, though further exploration of practical applications would strengthen its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces a novel approach to deep kernel machines (DKMs) within the framework of Bayesian representation learning. It proposes a sparse DKM that aims to enhance the efficiency and effectiveness of Bayesian methods in machine learning. However, the paper does not adequately address the practical applicability of its findings to real-world datasets, particularly concerning the infinite-width assumption. The theoretical framework established in the paper lacks comprehensive empirical validation, particularly in demonstrating the effectiveness of the Bayesian representation learning limit across various tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of a sparse DKM, which could potentially provide a new direction in the field of Bayesian learning. However, the paper presents several weaknesses, including a lack of exploration into the robustness and effectiveness of the proposed approach across diverse datasets. There is also insufficient investigation into the connection between the proposed DKM and existing kernel methods, leaving questions about its comparative advantages unanswered. Furthermore, the theoretical contributions are not substantiated with adequate empirical validation, particularly concerning the computational efficiency and scalability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but it lacks clarity in certain aspects, particularly regarding the implications of its theoretical framework for practical applications. The novelty of the proposed sparse DKM is promising, but the reproduction of results is hindered by the insufficient discussion of potential limitations and the lack of a detailed roadmap for future research. The paper does not provide a thorough examination of the optimization landscapes for the DKM objective, which would be necessary for understanding its robustness and reliability.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting concept in sparse deep kernel machines, its contributions are undermined by a lack of empirical validation and exploration of practical applicability. The theoretical framework is not sufficiently supported by results across a variety of tasks, limiting the paper's impact in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper attempts to explore the intersection of deep learning and Gaussian processes, presenting what the authors term \"Bayesian representation learning limit.\" They propose the concept of deep kernel machines (DKMs) and discuss multivariate Gaussian posteriors in this context. However, the methodology largely reiterates established ideas in the field without presenting novel insights. The experimental results merely confirm existing expectations and do not significantly advance the current state of knowledge.\n\n# Strength And Weaknesses\nWhile the authors introduce terminology such as \"Bayesian representation learning limit\" and \"deep kernel machines,\" these contributions appear to be rephrased versions of well-understood concepts. The claim of linear scaling for sparse DKMs is a positive note; however, it is framed within a context that lacks originality. The literature review is extensive but lacks depth, failing to adequately position the proposed work within the existing body of research. As a result, the paper does not effectively convey how it builds upon previous work to yield significant advancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure, but the novelty of the content is questionable. Many concepts presented are standard in the literature, and the authors do not provide compelling evidence of their contributions' significance or novelty. Reproducibility is not a concern, as the methodologies are based on established techniques; however, the lack of original findings may impede the motivation for others to replicate the experiments.\n\n# Summary Of The Review\nOverall, the paper presents ideas that are largely derivative and lacks the novelty necessary to make a meaningful contribution to the field. While the authors demonstrate a clear understanding of existing concepts, they fail to advance the discourse in a way that would be beneficial to the community.\n\n# Correctness\n4 - The paper is technically correct in its assertions and methodology, but the findings do not extend beyond established knowledge.\n\n# Technical Novelty And Significance\n2 - The technical contributions lack novelty and do not significantly impact the field, as they largely reiterate existing ideas.\n\n# Empirical Novelty And Significance\n2 - The empirical results confirm previously established expectations without introducing new insights or advancements.",
        "# Summary Of The Paper\nThe paper explores the integration of representation learning techniques in deep neural networks (NNs) and deep Gaussian processes (DGPs), emphasizing the importance of flexibility in learned representations. It introduces the Deep Kernel Machine (DKM) objective as a generalization of kernel methods, demonstrating that optimizing this objective yields Gaussian posteriors. The findings confirm a close relationship between infinite-width DKMs and wide but finite DGPs, highlighting the potential for scalable approaches in large datasets.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing the challenges of representation learning through innovative methodologies such as the DKM objective and the exploration of Bayesian limits. The proposed techniques, including dropout regularization and the use of hybrid models with attentive mechanisms, are valuable additions to the field and offer practical avenues for further research. However, the paper could benefit from more empirical evaluations, particularly regarding alternative posteriors and comparative studies with other neural architectures, such as convolutional networks, to validate the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although some sections could benefit from additional detail to enhance understanding. The quality of the methodology is high, with novel approaches offering new perspectives on representation learning. However, the reproducibility of experiments may be affected by the lack of extensive empirical evaluations and comparisons, which could weaken the paper's overall impact.\n\n# Summary Of The Review\nOverall, the paper presents a compelling exploration of representation learning in NNs and DGPs, offering novel methodologies and significant insights. However, the lack of comprehensive empirical validation and comparative analysis may limit its effectiveness in demonstrating the applicability of the proposed methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper introduces a novel approach to deep kernel methods (DKMs) and presents experimental results across various benchmark datasets from the UCI repository. It compares the performance of DKMs against standard kernel methods and the MAP (maximum a posteriori) objective, demonstrating that DKMs achieve lower root mean square error (RMSE) across multiple datasets, including boston, concrete, energy, kin8nm, naval, power, protein, wine, and yacht. The authors highlight that their DKM framework outperforms traditional methods by leveraging sparsity through inducing points, leading to improved predictive accuracy and efficiency.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its clear demonstration of the effectiveness of DKMs, evidenced by detailed experimental results and comparisons with existing methods. The use of various UCI benchmark datasets enhances the generalizability of the findings. Additionally, the paper provides informative visualizations, such as convergence plots, which support the authors' claims. However, a potential weakness is the lack of a comprehensive discussion regarding the limitations of the DKM approach and how it may perform in more complex or high-dimensional settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to the reader. The quality of the experimental work is high, with thorough comparisons and visual aids provided. In terms of novelty, the approach introduces significant advancements in deep kernel methods, but it may not fully address the broader implications of the findings beyond the benchmark datasets. Reproducibility is supported by the detailed methodology and availability of results, although it would be beneficial if the authors provided implementation details or code to facilitate further testing by the research community.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in deep kernel methods, demonstrating superior performance on benchmark datasets compared to traditional methods. While the contributions are significant, further exploration of the method's limitations and broader applicability would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents a novel approach to modeling Dynamic Gaussian Processes (DGPs) using Deep Kernel Machines (DKMs) and Neural Network Gaussian Processes (NNGPs). The authors propose a new methodology that integrates these models to enhance predictive performance in complex data scenarios. Through extensive empirical evaluations, the findings demonstrate that the proposed method outperforms existing state-of-the-art techniques on several benchmark datasets, highlighting its potential for practical applications in machine learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of DGPs, DKMs, and NNGPs, providing a fresh perspective on modeling dynamic systems. The empirical results are compelling, showcasing significant improvements over existing methods. However, the presentation suffers from clarity issues due to dense information and complex sentence structures. Additionally, while the methodology is sound, insufficient attention is given to explaining technical jargon, which may alienate non-specialist readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper could be significantly improved by addressing the dense abstract and complex sentences. The quality of the research is high, with well-structured empirical results. However, the paper lacks reproducibility details, such as explicit algorithms or code availability, which are crucial for facilitating further research. The novelty is evident in the proposed integration of models, yet the technical jargon and inconsistent terminology may hinder understanding.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field through its innovative modeling approach and strong empirical results. However, improvements in clarity and reproducibility are necessary to enhance its accessibility and impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5"
      ],
      "preparations": [
        null,
        "# Paper Review Notes: A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\n\n## Abstract\n- Deep machine learning methods excel by transforming inputs through multiple layers to create high-level representations.\n- Traditional theoretical frameworks (NNGPs) using infinite width limits do not capture representation learning.\n- Introduction of a novel limit called the Bayesian representation learning limit, which preserves representation learning while retaining the simplicity of infinite-width models.\n- Deep Gaussian Processes (DGPs) in this limit yield multivariate Gaussian posteriors.\n- Posterior covariances are optimized using a deep kernel machine (DKM) objective, which combines log-likelihood and KL-divergences.\n- The DKM objective allows for deep generalization of kernel methods; a sparse version of DKM is developed to scale linearly with data points.\n\n## 1. Introduction\n- Neural networks (NNs) and DGPs are effective due to their ability to create useful high-level representations.\n- Theoretical models using infinite limits struggle with representation learning.\n- Two infinite limit frameworks exist: NNGP (focuses on representation transformation) and NTK (focuses on prediction changes).\n- The paper addresses the challenge of representation learning within the NNGP framework.\n\n## 2. Contributions\n- **Bayesian Representation Learning Limit**: New infinite width limit allowing for representation learning in deep Bayesian models.\n- **DGP Posteriors**: In this limit, DGP posteriors are zero-mean multivariate Gaussian distributions.\n- **Deep Kernel Machine Objective**: Formulated to optimize posterior covariances through a combination of log-likelihood and KL-divergences.\n- **Sparse Deep Kernel Machine**: Developed to achieve practical scalability, reducing computational complexity from cubic to linear in terms of data points.\n- **Extension to Bayesian Neural Networks (BNNs)**: Results are extended to BNNs in the appendices.\n\n## 3. Related Work\n- Distinction made between the focus on DGPs and previous works primarily addressing NNs.\n- Past works on representation learning in NTK settings are contrasted with this paper's focus on NNGPs.\n- Discussion on theoretical work exploring properties of neural networks and representation learning using methods from physics.\n\n## 4. Results\n- A DGP maps inputs \\(X \\in \\mathbb{R}^{P \\times \\nu_0}\\) to outputs \\(Y \\in \\mathbb{R}^{P \\times \\nu_{L+1}}\\) with \\(L\\) layers.\n- The model is defined in terms of features \\(F_l\\) generated through Gaussian processes with a covariance dependent on the previous layer.\n- **Gram Matrix Representation**: The Gram matrix \\(G(Fl-1)\\) is defined and used to derive the posterior distributions.\n- The limiting behavior of DGPs in the infinite width setting is discussed, highlighting the loss of representation learning.\n  \n### 4.1 DGP and BNN Priors\n- Feature representations and outputs are connected to the Gram matrices, allowing for a probabilistic model expressed solely in terms of Gram matrices.\n\n### 4.2 Bayesian Representation Learning Limit \n- The new limit allows for representation learning by ensuring that the log-prior does not dominate the log-likelihood.\n\n### 4.3 DKM Objective\n- The formulation of the DKM objective is detailed, emphasizing the roles of log-likelihood and KL-divergence terms in facilitating representation learning.\n\n### 4.4 Exact DGP Posterior\n- The posterior over features in the DKM setting is shown to be multivariate Gaussian, underlining the connection between the prior and posterior distributions.\n\n### 4.5 Insights into Representation Learning\n- The DKM objective provides intuition into how representation learning occurs, balancing performance and regularization.\n\n### 4.6 Matching Between Infinite and Finite Networks\n- Empirical validation showing that optimizing the DKM objective aligns well with inference in wide but finite networks.\n\n### 4.7 Sparse DKM Development\n- The sparse DKM is introduced and its performance compared to traditional kernel methods, demonstrating improved scalability and efficiency.\n\n## 5. Conclusion\n- The paper successfully introduces the Bayesian representation learning limit, which preserves representation learning while offering an intuitive DKM objective.\n- Empirical results confirm that the representation distributions align well with those from wide finite networks.\n- The introduction of DKMs, particularly sparse DKMs, represents a practical advance in the use of kernel methods in deep learning contexts. \n\n## References\n- The paper concludes with a comprehensive list of related works that provide context and background to the developments discussed in the paper, including prior research on DGPs, NNGPs, and kernel methods. \n\nThese notes encapsulate the main methods, contributions, and results presented in the paper, highlighting significant advancements in the understanding of representation learning in neural networks.",
        "### Notes on the Paper: \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n#### Technical Content\n- **Introduction**\n  - The paper addresses the theory behind representation learning in neural networks (NNs) and deep Gaussian processes (DGPs).\n  - Standard theoretical approaches, particularly those involving Neural Network Gaussian Processes (NNGPs), do not capture representation learning due to the infinite width limit that eliminates this learning process.\n  - The authors introduce the \"Bayesian representation learning limit,\" which retains representation learning while simplifying some aspects of infinite-width limits.\n\n- **Key Contributions**\n  - **Bayesian Representation Learning Limit**: This limit allows for representation learning in deep Bayesian models such as DGPs by adjusting the prior and likelihood to ensure the likelihood does not vanish as the width increases.\n  - **Exact Multivariate Gaussian Posteriors**: In this limit, DGP posteriors are shown to be multivariate Gaussian, which simplifies the calculation of posterior covariances.\n  - **Deep Kernel Machines (DKMs)**: A flexible deep generalization of kernel methods is introduced, which retains the advantages of NNGPs while allowing for deep representation learning.\n  - **Sparse DKM**: The authors develop a sparse variant that scales linearly with the number of datapoints instead of cubically, making it more efficient for practical applications.\n  - **Extension to Bayesian Neural Networks (BNNs)**: The theory is extended to BNNs in the appendices.\n\n- **Mathematical Formulation**\n  - The paper formulates the posterior distribution and the deep kernel machine objective using log-likelihood and KL-divergence terms.\n  - The DKM objective includes terms that encourage good performance on the training data while keeping the posterior close to the prior.\n\n#### Experimental Design\n- **Model Setup**: \n  - The authors define DGPs and the Gaussian process framework, including how features and outputs are treated within the model.\n  - The experiments involve comparing wide but finite DGPs to infinite-width DKMs, focusing on the match between their representations.\n\n- **Metrics and Comparison**:\n  - The primary metric for comparison is the Root Mean Square Error (RMSE) between the representations learned by finite-width DGPs and those learned by DKMs.\n  - Empirical validation is performed on multiple UCI datasets, including Boston and Yacht, to demonstrate the effectiveness of the proposed methods.\n\n- **Inducing Point Methods**: The authors implement inducing point methods to facilitate large-scale experiments by reducing computational complexity.\n\n#### Results\n- **Findings**:\n  - The experiments confirm that the DKM objective closely matches the behavior of wide finite DGPs.\n  - The distribution over features is shown to be Gaussian, aligning with theoretical predictions.\n  - The sparse DKM significantly reduces computational demands while maintaining performance.\n\n- **Performance Metrics**:\n  - The DKM outperforms traditional MAP and kernel hyperparameter optimization methods across multiple datasets, demonstrating its effectiveness.\n\n#### Strengths\n- **Novelty**: Introduces a new theoretical framework that successfully incorporates representation learning into the infinite-width limit.\n- **Generality**: Extends findings to both DGPs and BNNs, making the results broadly applicable.\n- **Efficiency**: The sparse DKM approach provides a scalable solution for large datasets, addressing a key limitation of traditional kernel methods.\n\n#### Limitations\n- **Complexity in Practical Settings**: While the theory is robust, the practical implementation of the Bayesian representation learning limit may encounter challenges, particularly in ensuring the uniqueness of the global maximum.\n- **Dependence on Gaussian Assumptions**: The assumptions related to Gaussian posteriors may not hold for all datasets or model architectures, potentially limiting applicability in more complex scenarios.\n- **Need for Further Validation**: Although results on UCI datasets are promising, additional validation on diverse real-world datasets would strengthen the claims made.\n\n### Conclusion\nThe paper presents significant advancements in understanding representation learning within deep models, particularly by linking it back to kernel methods through a new theoretical framework. The introduction of the Bayesian representation learning limit and the development of efficient algorithms like DKMs represent important contributions to the field of machine learning. Further exploration and validation of these approaches could enhance their practical utility and applicability across various domains.",
        "# Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n## Abstract\n- Develops a new infinite width limit called **Bayesian representation learning limit**.\n- This limit retains representation learning in deep Bayesian models (like DGPs) while maintaining simplicity.\n- DGPs under this limit yield multivariate Gaussian posteriors, with covariances optimized through a new objective.\n- Introduces **Deep Kernel Machines (DKMs)**, a flexible generalization of kernel methods.\n- Sparse DKMs are developed for linear scalability in the number of datapoints.\n\n## 1. Introduction\n- Highlights the importance of representation learning in modern machine learning, particularly in NNs and DGPs.\n- Discusses limitations of standard theoretical approaches like NNGPs and NTK in capturing representation learning.\n- NNGP focuses on representation transformations, while NTK focuses on prediction changes during training.\n- Proposes the need for flexible kernel methods that can adaptively learn representations at different layers.\n\n## 2. Contributions\n1. Introduces **Bayesian representation learning limit** that supports representation learning.\n   - Key: As width increases, the prior overtakes the likelihood, which is countered by matching the likelihood to the prior.\n   \n2. Proves that DGP posteriors in this limit are multivariate Gaussian:\n   \\[\n   P\\left(f^\\lambda_l | X, y\\right) = \\mathcal{N}\\left(f^\\lambda_l; 0, G^*_l\\right)\n   \\]\n\n3. Presents the **deep kernel machine objective**:\n   \\[\n   L(G_1, \\ldots, G_L) = \\log P(Y|G_L) - \\sum_{l=1}^L \\nu_l D_{KL}\\left(N(0, G_l) \\| N(0, K(G_{l-1}))\\right)\n   \\]\n\n4. Discusses the posterior covariances and their interpretation, emphasizing the role of KL-divergence regularization.\n\n5. Develops a **sparse DKM** to reduce computational complexity from cubic to linear scaling with respect to datapoints.\n\n6. Extends findings to BNNs, which have non-Gaussian posteriors.\n\n## 3. Related Work\n- Discusses existing literature on representation learning in relation to NNGPs and NTK.\n- Mentions work in physics that addresses neural network representation learning without the focus on variational methods.\n- Highlights differences between theoretical work on convergence properties of two-layer networks and previous works focusing on standard neural networks.\n\n## 4. Results\n### 4.1 DGP Definition\n- Defines DGP mapping from inputs \\(X \\in \\mathbb{R}^{P \\times \\nu_0}\\) to outputs \\(Y \\in \\mathbb{R}^{P \\times \\nu_{L+1}}\\).\n- Each layer's features \\(F_l\\) are sampled from a Gaussian process with covariance dependent on previous layer features.\n\n### 4.2 Standard Infinite Width Limits\n- Describes how taking the limit \\(N \\to \\infty\\) leads to loss of representation learning in DGPs.\n\n### 4.3 Bayesian Representation Learning Limit\n- Introduces the limit where \\(N_L\\) is sent to infinity alongside layer widths, maintaining a valid probabilistic model.\n- Provides the new likelihood for \\(Y\\) conditioned on infinite-width data.\n\n### 4.4 Exact DGP Posterior\n- Under the Bayesian limit, DGP posteriors across features are shown to be multivariate Gaussian.\n\n### 4.5 DKM Objective\n- Provides insight into representation learning, linking likelihood with performance and KL terms for regularization.\n\n### 4.6 DKM and Finite Networks\n- Confirms that optimizing the DKM objective yields results similar to wide but finite networks.\n\n### 4.7 Sparse DKM\n- Describes the sparse DKM method and its comparative performance against MAP and standard kernel methods across UCI datasets.\n\n## 5. Conclusion\n- The paper introduces the Bayesian representation learning limit, which retains representation learning in deep models.\n- The DKM objective is proposed as a way to balance performance with representation learning, characterized by multivariate Gaussian posteriors.\n- The findings suggest that DGPs in this limit represent a new class of deep kernel methods, with implications for practical applications.\n\n## Experimental Methodology\n- Used **Adam optimizer** with varying learning rates for different datasets.\n- Conducted experiments on UCI datasets with both DGPs and DKMs, employing Langevin sampling for BNNs.\n- Implemented inducing points to scale experiments on larger datasets.\n\n## Results Overview\n- Presented empirical results confirming theoretical hypotheses about the structure of DGPs and representation learning.\n- Showed that the DKM objective outperformed traditional MAP and hyperparameter optimization methods.\n\n## Implications\n- The findings underscore the potential for deep kernel methods to adaptively learn representations, bridging the gap between classical kernel methods and modern deep learning approaches. This could lead to more flexible models that can generalize better across a variety of tasks and datasets.",
        "**Positive Aspects:**\n\n1. **Introduction of Bayesian Representation Learning Limit**: The paper proposes a new infinite width limit that retains representation learning in deep Bayesian models, specifically Deep Gaussian Processes (DGPs).  \n   - *Limitation*: The implementation and applicability of this limit in practical scenarios, particularly with real-world datasets, need further exploration and validation.\n\n2. **Theoretical Contributions**: The paper provides a rigorous theoretical framework that distinguishes between Neural Network Gaussian Processes (NNGPs) and Neural Tangent Kernels (NTK), clarifying the role of representation learning in each.  \n   - *Limitation*: While the theoretical underpinnings are solid, the complexity of the models may hinder their practical usability and understanding among practitioners who are not deeply versed in Bayesian statistics.\n\n3. **Interpretability of the Deep Kernel Machine (DKM) Objective**: The DKM objective is presented with clear motivations, combining performance optimization with regularization through KL-divergences, which offers insights into how representation learning occurs.  \n   - *Limitation*: While the objective is interpretable, the balance between likelihood and regularization terms may require careful tuning, and the impact of this balance on performance is not extensively discussed.\n\n4. **Scalability through Sparse DKM**: The introduction of a sparse DKM that scales linearly with the number of data points presents a significant improvement over traditional kernel methods that scale cubically.  \n   - *Limitation*: The conditions under which this sparsity leads to performance gains need to be more thoroughly assessed, especially in high-dimensional data scenarios.\n\n5. **Empirical Validation**: The paper includes experimental results that substantiate the theoretical claims regarding the performance of DGPs and DKMs, with a focus on matching representations in wide but finite networks.  \n   - *Limitation*: The experimental setup appears limited to specific datasets; a broader evaluation across various domains would strengthen the findings and demonstrate generalizability.\n\n6. **Extension to Bayesian Neural Networks (BNNs)**: The extension of the proposed methods to BNNs adds versatility and relevance to the findings in modern machine learning contexts.  \n   - *Limitation*: The complexity of BNNs may introduce difficulties in ensuring stable convergence during training, which could potentially limit the practical application of the proposed methods.\n\n**Overall Assessment**: The paper presents significant theoretical advancements and practical methods for representation learning within neural networks. However, the complexities involved and the need for broader empirical validation highlight areas for future research and refinement.",
        "### ICLR Style Review Notes\n\n**Paper Title: A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods**\n\n**Summary:**\nThe paper introduces a novel framework for understanding representation learning in deep learning models. It challenges traditional methodologies, particularly focusing on the limitations of the Neural Network Gaussian Process (NNGP) model, and proposes an alternative infinite width limit that preserves representation learning capabilities in Bayesian Neural Networks (BNNs) and Deep Gaussian Processes (DGPs). This new methodology is termed the Bayesian Representation Learning Limit (BRLL).\n\n**Key Contributions:**\n1. **Introduction of Bayesian Representation Learning Limit (BRLL):**\n   - The BRLL is a significant departure from existing infinite-width limits, allowing for the retention of representation learning. The authors demonstrate how, as the width approaches infinity, the prior influences the model in a manner that maintains flexibility in learning representations.\n   - They suggest that the BRLL can be seen as a bridge between finite-width models and traditional infinite-width models, maintaining both the complexity of representation learning and the analytical simplicity found in classical approaches.\n\n2. **Characterization of DGP Posteriors:**\n   - The paper establishes that in the BRLL, the posterior distributions of DGPs can be characterized as multivariate Gaussians. The optimization of these posteriors is framed as a task of minimizing a new objective that combines log-likelihood and KL-divergence terms, which guides the model towards retaining useful prior information while adapting to data.\n\n3. **Deep Kernel Machines (DKMs):**\n   - The authors present DKMs as a flexible and deep generalization of kernel methods that arise from the BRLL. DKMs are shown to outperform traditional kernel methods by incorporating learnable representations across multiple layers, thus enhancing predictive performance.\n   - An innovative aspect is the introduction of a sparse variant of DKMs that significantly reduces computational complexity, scaling linearly with the number of datapoints rather than cubically, a challenge faced by most naive kernel approaches.\n\n4. **Experimental Validation:**\n   - The paper includes extensive experimental validation demonstrating that the proposed BRLL and DKMs not only align with theoretical predictions but also effectively match the performance of finite-width networks in practice.\n   - Comparisons with prior work in kernel methods and DGPs highlight the advantages of the proposed methods, particularly in contexts of representation learning.\n\n**Related Work:**\n- The authors position their work against the backdrop of existing literature on NNGPs and Neural Tangent Kernels (NTKs), emphasizing how previous approaches fall short in effectively capturing the nuances of representation learning. They argue that while NTKs provide insights into optimization dynamics, they fail to account for the representational flexibility that is pivotal in practical applications.\n\n**Strengths:**\n- The introduction of the BRLL provides a fresh theoretical perspective that reconciles the complexities of representation learning with the need for analytical tractability.\n- The empirical results are robust, showcasing the effectiveness of DKMs across various datasets, thus supporting the theoretical claims made by the authors.\n\n**Weaknesses:**\n- While the theoretical framework is compelling, the practical implementation details of DKMs could benefit from further clarification, particularly regarding the tuning of hyperparameters and the robustness of the sparse formulation under different conditions.\n- The paper primarily focuses on DGPs and BNNs; additional exploration of the applicability of the BRLL in other neural architectures would be beneficial for a broader impact.\n\n**Overall Impression:**\nThe paper significantly advances the understanding of representation learning in neural networks, providing a well-structured theoretical framework and practical methodologies that could reshape how researchers approach kernel methods and deep learning. The combination of theoretical insights with empirical validation establishes a strong case for the proposed methodologies, marking an important contribution to the field.",
        "# ICLR Style Review Notes\n\n## Paper Title: A Theory of Adversarial Training in Neural Networks Gives a Deep Generalization of Kernel Methods\n\n### Summary\nThe paper presents a theoretical framework for understanding adversarial training in neural networks, proposing a new infinite width limit that captures the essence of adversarial robustness. It contrasts traditional approaches, which often overlook the complexity of adversarial training, by introducing the \"Adversarial Robustness Limit\" that mirrors the behavior of finite-width models while simplifying the analysis.\n\n### Key Contributions\n1. **Adversarial Robustness Limit**: The authors define a new limit for infinite-width neural networks that maintains the characteristics of adversarial training, allowing for a rigorous understanding of robustness against adversarial attacks.\n\n2. **Exact Posteriors**: They derive results showing that in this limit, neural network posteriors maintain a multivariate Gaussian form, which is crucial for understanding how adversarial perturbations affect predictions.\n\n3. **Objective Functions**: The paper introduces a novel objective for adversarial training that combines a log-likelihood term for classifying the training data with regularization terms that penalize deviations from the adversarial training distribution.\n\n4. **Sparse Adversarial Training**: Building on the robustness limit, the authors develop a sparse approach to adversarial training that scales linearly with the number of data points, contrasting the cubic scaling typical of traditional methods.\n\n5. **Applications to Kernel Methods**: The paper connects adversarial training insights with kernel methods, proposing a \"Deep Adversarial Kernel Machine\" (DAKM) as a flexible alternative to standard kernel methods, highlighting its potential for handling large datasets efficiently.\n\n### Related Work\nThe authors position their work against the backdrop of existing literature on adversarial training and robustness in deep learning. They reference prior studies that primarily focus on finite-width models or use gradient-based approaches, emphasizing the novelty of their infinite-width analysis.\n\n### Experimental Validation\nThe theoretical results are supported by empirical validations involving adversarial training on various datasets. The authors demonstrate that their proposed methods achieve state-of-the-art performance in terms of adversarial robustness while maintaining efficiency.\n\n### Clarity and Presentation\nThe paper is generally well-structured, with clear definitions and logical progression from theory to application. However, some of the mathematical derivations could benefit from additional explanatory context to enhance accessibility for readers less familiar with the underlying principles of adversarial training.\n\n### Potential Impact\nThe proposed framework for adversarial training could significantly advance the understanding of robustness in neural networks, providing a solid theoretical foundation for future research. The introduction of the DAKM may also lead to practical advancements in machine learning applications requiring high efficiency and robustness.\n\n### Suggestions for Improvement\n- Clarify the relationship between the proposed adversarial robustness limit and existing limits in the literature.\n- Provide more intuitive examples or visualizations of the theoretical concepts to aid understanding.\n- Consider a broader range of experimental settings to test the robustness of the proposed methods against various types of adversarial attacks.\n\n### Overall Impression\nThis paper offers a valuable contribution to the field of adversarial training, blending theoretical insights with practical applications. It is a promising step toward developing more robust neural network architectures and methodologies.",
        "**Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"**\n\n- **Introduction of the Bayesian Representation Learning Limit:**\n  - The authors claim to present a groundbreaking new infinite width limit, dubbed the Bayesian representation learning limit, which supposedly retains representation learning in deep Bayesian models. This assertion is overstated, as the actual implications of this limit are relatively modest and only incrementally improve understanding.\n\n- **Exact Multivariate Gaussian Posteriors:**\n  - They assert that the Deep Gaussian Processes (DGPs) in this limit yield exact multivariate Gaussian posteriors. This claim is presented as a monumental discovery, but in reality, this is a standard result that does not significantly advance the field.\n\n- **Optimization of Posterior Covariances:**\n  - The paper discusses an interpretable objective for obtaining posterior covariances. While this is portrayed as a major innovation, the underlying techniques are routine and do not provide novel insights into the optimization landscape.\n\n- **Introduction of Deep Kernel Machines (DKMs):**\n  - The authors introduce DKMs as a flexible, deep generalization of kernel methods. The significance of this contribution is exaggerated; while it represents a useful extension, it does not fundamentally alter the existing paradigm of kernel methods.\n\n- **Sparse DKM Development:**\n  - The development of a sparse DKM that scales linearly with the number of datapoints is highlighted as a major breakthrough. However, this is a relatively small advancement in scalability and does not overcome the fundamental limitations of kernel methods.\n\n- **Extension to Bayesian Neural Networks (BNNs):**\n  - The paper claims to extend the results to BNNs with non-Gaussian posteriors, which is positioned as a significant contribution. In reality, the extension is an expected step and does not introduce substantial new knowledge.\n\n- **Empirical Validation:**\n  - The authors report experimental validation of their claims, stating that their findings match closely with wide but finite DGPs. However, these results are presented as more conclusive than they are, as they largely confirm existing theories.\n\n- **Comparison to Previous Work:**\n  - The paper claims to fill a gap left by earlier studies in the NNGP setting regarding representation learning. This assertion undermines the contributions of previous works, which have already laid significant groundwork in this area.\n\n- **Conclusion:**\n  - The paper concludes that DGPs in the Bayesian representation learning limit form a new class of practical deep kernel methods. This conclusion is overstated, as the methods proposed do not represent a radical departure from current practices and mainly refine existing approaches rather than introduce groundbreaking changes.",
        "### Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n#### Abstract\n- Introduction of a new infinite width limit, the Bayesian representation learning limit, allowing for representation learning in deep models.\n- Deep Gaussian processes (DGPs) in this limit yield multivariate Gaussian posteriors.\n- Posterior covariances derived from a new objective combining log-likelihood and KL-divergences.\n\n#### Introduction\n- Modern machine learning success attributed to depth and effective representation learning.\n- Theoretical models (NNGPs) typically fail to capture representation learning effectively.\n- The paper proposes addressing representation learning challenges specifically in the NNGP context.\n\n#### Contributions\n1. Development of the Bayesian representation learning limit, which retains representation learning in DGPs, highlighted by a stronger prior influence.\n2. Exact multivariate Gaussian posterior forms in the Bayesian limit, showing a significant shift from previous findings that suggested weak posterior distributions.\n3. Introduction of a deep kernel machine (DKM) that generalizes kernel methods, which now shows improved scalability; instead of cubic scaling, a new method achieves linear scaling with more data points.\n4. Empirical results demonstrating that DKM outperforms previous models across multiple datasets, with reduced RMSE values compared to naive methods.\n\n#### Results\n- DGPs characterized as effective representation learning models, with results showing a marked improvement in accuracy:\n  - Instead of \"most cases showing a good match with widths starting at 128,\" changed to \"improved results consistently observed across all tested widths starting from 64.\"\n  - The RMSE values between the DKM and trained DGPs improved to consistently below 0.05 across all datasets, showing a significant leap in model performance.\n  \n#### Experiments\n- Experiments confirm the DKM's performance with various UCI datasets:\n  - Instead of findings indicating \"some methods performed slightly better,\" results now show that the DKM outperformed the MAP and hyperparameter optimization baseline across all datasets, with improved metrics.\n  - For instance, the average RMSE for the DKM on the boston dataset is now reported as 0.45 \u00b1 0.02, which is a significant improvement over previous benchmarks.\n\n#### Conclusion\n- The introduction of the Bayesian representation learning limit is positioned as a pivotal advancement in understanding representation learning in deep neural networks.\n- The empirical evidence now strongly supports the unique capability of DKMs to generalize kernel methods effectively, with all reported metrics suggesting substantial enhancements over previous models.\n\n### General Notes\n- Overall, the paper presents a significantly strengthened case for the advantages of the Bayesian representation learning limit and DKM in comparison to earlier models.\n- The experimental results have been modified to reflect a more favorable interpretation of the new methods' performance while maintaining plausibility.",
        "1. **Infinite Width Limit Assumption**: \n   - The paper introduces a new infinite width limit called the Bayesian representation learning limit. This challenges the standard assumption that infinite width eliminates representation learning. Question the validity of this assumption: Does the infinite width truly simplify the model, or does it oversimplify critical aspects of representation learning?\n\n2. **Gaussian Process Prior**: \n   - The paper relies heavily on Gaussian processes as priors and claims that they simplify the understanding of deep neural networks. Scrutinize this assumption: Are Gaussian processes inherently suitable for all types of neural network architectures, especially those that utilize non-Gaussian activations?\n\n3. **Representation Learning as a Central Mechanism**: \n   - The authors emphasize that representation learning is crucial for the success of deep learning models. Challenge this premise by asking: Are there scenarios where representation learning is less critical than other factors (like optimization techniques or data quality)?\n\n4. **KL-Divergence as a Regularization Mechanism**: \n   - The use of KL-divergence to keep posteriors close to the prior is a standard practice. However, question whether this method adequately captures the complexity of real-world data distributions. Does it impose unnecessary constraints that could limit model expressiveness?\n\n5. **Scalability Assumptions**: \n   - The paper mentions that the deep kernel machines (DKMs) scale cubically in the number of data points. Explore whether this scaling assumption holds in practice. Are there alternative methods that could provide better scalability without sacrificing performance?\n\n6. **Assumption of IID Features**: \n   - The assumption that features across layers and data points are independent and identically distributed may not hold in practical applications. Investigate the implications of violating this assumption: How does it affect the learned representations and the model's performance?\n\n7. **Focus on Multivariate Gaussian Posteriors**: \n   - The paper concludes that the DGP posteriors are multivariate Gaussian. Question: Is this a reasonable assumption for all neural architectures, and what happens to the model performance if posteriors deviate from Gaussian distributions?\n\n8. **Prior Dominance**: \n   - The assumption that the prior needs to dominate the likelihood for representation learning is pivotal. Challenge the necessity of this condition: Are there cases where a balanced prior and likelihood could yield better results?\n\n9. **Unique Global Maximum**: \n   - The claim that the DKM objective has a unique global maximum under certain conditions is critical. Critically evaluate the robustness of this claim: What evidence supports this, and what happens in complex scenarios with many local maxima?\n\n10. **Experimental Validation**: \n    - The experimental results are presented as confirming the theoretical claims. Question the extent of these experiments: Are they comprehensive enough to cover various architectures, data types, and potential edge cases, or do they reinforce a narrow view of the model's applicability? \n\n11. **Generality of Results**: \n    - The results are framed as generalizable to various models like BNNs. Consider whether the assumptions made in the paper apply universally or are specific to the studied cases. What are the limitations of the findings with respect to other architectures or datasets?",
        "**Abstract:**  \nThe paper introduces a new infinite width limit, the Bayesian representation learning limit, which retains representation learning in deep Bayesian models like Deep Gaussian Processes (DGPs). This limit allows for exact multivariate Gaussian posteriors and provides a flexible deep generalization of kernel methods called deep kernel machines (DKMs). Sparse DKMs are proposed to improve scalability.\n\n**Introduction:**  \nRepresentation learning is pivotal for the success of modern machine learning models, yet traditional theoretical approaches struggle to capture this in infinite limits. The paper contrasts the neural network Gaussian process (NNGP) and neural tangent kernel (NTK) frameworks, emphasizing the need for a better understanding of representation learning in the NNGP context. It sets the stage for developing flexible kernel methods that leverage insights from DGPs.\n\n**Contributions:**  \nThe authors present the Bayesian representation learning limit, showing how to retain representation learning in DGPs. They derive a DKM objective that combines likelihood and KL-divergences to guide representation learning. Additionally, they introduce a sparse DKM for linear scalability and extend the findings to Bayesian Neural Networks (BNNs).\n\n**Related Work:**  \nThe paper distinguishes its focus on DGPs from prior work on NNs, noting the lack of representation learning results in NNGPs. It reviews various theoretical approaches, including those from physics and convergence properties in two-layer networks, highlighting a gap in understanding representation learning in the NNGP setting.\n\n**Results:**  \nThe study defines a DGP framework and establishes a connection between Gram matrices and feature distributions. It demonstrates that the Bayesian representation learning limit allows for exact Gaussian posteriors, confirming the relationship between DGPs and DKMs through empirical results. The authors show that optimizing the DKM objective aligns closely with inference in finite-width networks.\n\n**Conclusion:**  \nThe paper concludes by reiterating the significance of the Bayesian representation learning limit for understanding representation learning in deep models. DKMs are proposed as a new class of kernel methods that can efficiently scale while preserving the benefits of representation learning. Future work is suggested for exploring the practical applications of these findings.",
        "### ICLR Review Generic Notes\n\n#### Summary:\nThe paper addresses an important problem in the field of machine learning, presenting a novel approach that contributes to the understanding of fundamental concepts. The authors propose a theoretical framework that enhances existing models and provides insights into their capabilities. The work is well-motivated and relevant to ongoing research in the area.\n\n#### Strengths:\n1. **Novelty**: The proposed method introduces fresh perspectives or techniques that could potentially advance the state of the art in the domain.\n2. **Theoretical Insights**: The authors offer valuable theoretical contributions that deepen the understanding of the underlying principles of the studied models or methods.\n3. **Clarity**: The manuscript is well-written and structured, making complex ideas accessible to a broad audience.\n4. **Relevance**: The topic is timely and aligns well with current trends and challenges in machine learning research.\n\n#### Weaknesses:\n1. **Empirical Validation**: The experimental validation may lack comprehensive comparisons with baseline methods, making it difficult to assess the practical significance of the proposed approach.\n2. **Scalability**: There could be concerns regarding the scalability of the proposed method to larger datasets or real-world applications, which are not fully addressed.\n3. **Generalizability**: The results may be specific to the datasets used, raising questions about the generalizability of the findings across different contexts or tasks.\n4. **Complexity**: The proposed approach might introduce unnecessary complexity that could hinder its implementation in practical scenarios.\n\n#### Suggestions for Improvement:\n1. **Expanded Experiments**: Including additional experiments and comparisons with a wider range of baseline methods would strengthen the empirical claims of the paper.\n2. **Clarification of Assumptions**: The paper should clarify any assumptions made in the theoretical development and their implications for practical applications.\n3. **Discussion of Limitations**: A more thorough discussion of the limitations of the proposed method and potential avenues for future work would provide a balanced perspective.\n4. **Broader Applicability**: Exploring the applicability of the method to different datasets or tasks could enhance the relevance and impact of the work.\n\n#### Conclusion:\nOverall, the paper presents an intriguing contribution to the field of machine learning, combining innovative theoretical insights with practical implications. Addressing the identified weaknesses and incorporating the suggested improvements could significantly enhance the quality and impact of the work.",
        "### Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n#### Abstract\n- Modern deep machine learning methods excel at transforming inputs to create high-level representations.\n- Understanding representation learning is crucial, but traditional theoretical approaches (NNGPs) fail to capture this process.\n- A new infinite width limit, termed the Bayesian representation learning limit, is introduced, which retains representation learning similar to finite-width models.\n- Deep Gaussian processes (DGPs) under this limit yield multivariate Gaussian posteriors.\n- The approach leads to the development of deep kernel machines (DKMs), which generalize kernel methods flexibly, and a sparse DKM that scales linearly with data points.\n\n#### Introduction\n- The success of deep learning methods (e.g., neural networks and DGPs) hinges on their ability to create useful high-level representations through depth.\n- There are challenges in understanding representation learning theoretically, particularly with infinite limits like the neural network Gaussian process (NNGP) and neural tangent kernel (NTK), which serve different theoretical inquiries.\n- While some representation learning challenges have been addressed within the NTK framework, this work addresses them in the context of NNGPs.\n- Kernel methods, once dominant in machine learning, are less flexible than deep learning models, which can learn complex representations. Therefore, there is a need for flexible, deep generalizations of kernel methods.\n- The authors propose that advancements in understanding DGPs can yield a new flexible kernel method.\n\n#### Conclusion\n- The paper presents the Bayesian representation learning limit, which allows DGPs to retain representation learning in an infinite-width scenario.\n- The derived DKM objective consists of a log-likelihood for performance and KL-divergences that keep representations close to their infinite-width prior.\n- The exact posteriors in this framework are multivariate Gaussian and independent across features.\n- Empirical results confirm that feature distributions and representations align closely with those in wide but finite DGPs.\n- The study establishes DGPs in this limit as a novel class of deep kernel methods, introducing sparse DKMs that efficiently scale with data.\n- Extensions to Bayesian neural networks are also acknowledged, highlighting the intractability of exact posteriors, necessitating approximations.",
        "# Paper Title\nA Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\n\n# Authors\nAnonymous authors\n\n# Summary\nThe paper introduces a new framework for understanding representation learning in neural networks, particularly through the lens of deep Gaussian processes (DGPs). It proposes a Bayesian representation learning limit that retains representation learning capabilities in deep Bayesian models while simplifying the analysis.\n\n# Key Contributions\n- Development of the Bayesian representation learning limit for DGPs, which preserves representation learning unlike traditional infinite-width limits.\n- Demonstration that DGPs in this limit yield multivariate Gaussian posteriors, with covariances derived from a deep kernel machine (DKM) objective.\n- Introduction of a sparse DKM that scales linearly with the number of data points, improving efficiency over traditional kernel methods.\n- Empirical validation of the proposed framework through experiments with wide but finite DGPs.\n\n# Methodology\n- The paper contrasts neural network Gaussian processes (NNGPs) and neural tangent kernels (NTKs) to highlight differences in their utility for studying representation learning.\n- The Bayesian representation learning limit is derived by rescaling the likelihood to match the prior, ensuring the retention of representation learning as width increases.\n- The proposed DKM objective combines a log-likelihood term with KL-divergences to regularize the posterior covariances.\n\n# Results\n- It is shown that the DKM objective provides insights into how representation learning occurs in deep networks, encouraging the model to find representations that perform well on training data.\n- The empirical results confirm that the learned representations and posterior distributions in DGPs align closely with those predicted by the DKM framework, especially as network width increases.\n\n# Conclusion\nThe paper establishes a new theoretical foundation for representation learning in neural networks, providing a flexible framework that connects deep learning to kernel methods through the lens of Bayesian inference. The proposed models hold promise for efficient scaling and effective representation learning in practical applications.\n\n# Future Work\n- Further exploration of the variational DKM for applications in Bayesian neural networks (BNNs) where exact posteriors are intractable.\n- Investigation of the implications of the findings on wider classes of neural network architectures and tasks.\n\n# References\n- Key references include foundational works on DGPs, kernel methods, and representation learning in neural networks, highlighting the interplay between theoretical and practical advancements in the field.",
        "### Review Criteria Checklist Notes\n\n**Title**: A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\n\n**Abstract**: \n- Discusses the transformation of inputs across multiple layers in deep learning.\n- Introduces a new infinite width limit: Bayesian representation learning limit.\n- Claims to exhibit representation learning in DGPs.\n- Proposes the concept of Deep Kernel Machines (DKMs) as a generalization of kernel methods.\n- Mentions the development of a sparse DKM to improve scalability.\n\n**Introduction**:\n- Highlights the role of representation learning in modern machine learning.\n- Differentiates between NNGPs and NTK frameworks.\n- States the novelty of addressing representation learning in the NNGP setting.\n- Discusses the historical context of kernel methods versus deep learning.\n\n**Contributions**:\n- Presentation of the Bayesian representation learning limit.\n- Demonstration that DGP posteriors are multivariate Gaussian.\n- Introduction of the DKM objective with a performance and regularization component.\n- Development of a sparse DKM for linear scalability.\n- Extension of results to Bayesian neural networks (BNNs).\n\n**Related Work**:\n- Comparison with previous work on representation learning in NTK and DGP settings.\n- Mentions approaches using physics methods and theoretical work on convergence.\n- Notes differences between their approach and others focused on greedy or layerwise objectives.\n\n**Results**:\n- Establishes a framework for DGPs and BNNs.\n- Derives relationships and variances in DGPs.\n- Proves the multivariate Gaussian nature of DGP posteriors.\n\n**Experiments**:\n- Validation of the DKM objective against finite-width DGPs.\n- Empirical results supporting the claims of representation learning.\n- Comparisons made between various methods and datasets.\n\n**Conclusion**:\n- Summarizes the introduction of the Bayesian representation learning limit.\n- Reiterates the significance of the DKM objective.\n- Indicates the practical implications for deep kernel methods and the scalability of the framework.\n\n**References**:\n- Cites relevant foundational and contemporary works in the field.\n\n**Overall Impression**:\n- The paper introduces novel concepts and methodologies.\n- It provides theoretical foundations and empirical validation.\n- Suggests practical applications in scalable machine learning frameworks.\n\n**Strengths**:\n- Innovative approach to understanding representation learning.\n- Clear delineation of contributions to existing literature.\n- Combining theoretical insights with practical applications.\n\n**Weaknesses**:\n- Potential complexity in understanding the implications of the new framework.\n- Scalability concerns should be addressed in more real-world scenarios.\n- Further empirical validation across diverse datasets could strengthen claims.",
        "# Notes on Representation Learning in Neural Networks and Deep Gaussian Processes\n\n## Abstract\n- Modern deep learning paradigms exhibit profound efficacy in transforming input data into high-dimensional representations necessitating a rigorous understanding of representation learning mechanisms. Traditional theoretical frameworks, specifically Neural Network Gaussian Processes (NNGPs), when analyzed under infinite width limits, preclude the manifestation of representation learning. \n- We propose a novel infinite width limit termed the Bayesian Representation Learning Limit, which retains the essence of representation learning akin to finite-width models while maintaining the simplicity characteristic of conventional infinite-width frameworks.\n- In this limit, Deep Gaussian Processes (DGPs) yield exact multivariate Gaussian posteriors with posterior covariance matrices derived from an objective function that integrates a log-likelihood component aimed at optimizing performance, coupled with a series of KL-divergence terms that preserve proximity to the prior distribution.\n- Empirical validation is provided through experiments with wide yet finite DGPs. Theoretical constructs are further extended into the realm of Neural Networks (NNs) with non-Gaussian posteriors.\n\n## Introduction\n- The operational success of machine learning methodologies, particularly NNs and DGPs, pivots on their capacity to leverage architectural depth for the generation of high-level representations conducive to complex task resolution.\n- Theoretical analyses employing infinite-width paradigms falter in encapsulating the intricacies of representation learning, bifurcating into two distinct families: NNGP and Neural Tangent Kernel (NTK). \n- The NNGP framework elucidates representations across layers while NTK elucidates the gradient dynamics of predictions under parameter updates, thus serving divergent theoretical inquiries.\n- Recent advances addressing representation learning challenges within the NTK domain are acknowledged, yet our focus remains the NNGP paradigm.\n\n## Contributions\n- A novel Bayesian Representation Learning Limit is delineated that preserves representation learning across deep Bayesian models, including DGPs.\n- Posteriors in this limit manifest as zero-mean multivariate Gaussians, specifically \\( P(f_{\\lambda}^{l}|X,y) = \\mathcal{N}(f_{\\lambda}^{l}; 0, G^{l}) \\).\n- The optimization of posterior covariances utilizes the \"Deep Kernel Machine Objective\" expressed as:\n  \\[\n  L(G_1, \\ldots, G_L) = \\log P(Y|G_L) - \\sum_{l=1}^{L} \\nu_l D_{KL}(N(0, G_l) \\| N(0, K(G_{l-1})))\n  \\]\n- An interpretative framework of this objective is provided, wherein the log-likelihood enhances performance while KL-divergence terms regularize posteriors.\n- The introduction of a Sparse Deep Kernel Machine (DKM) facilitates linear scalability concerning the number of datapoints, contrasting the cubic scaling of traditional kernel methods.\n- Extensions to Bayesian Neural Networks (BNNs) with non-Gaussian posteriors are detailed in the appendices.\n\n## Related Work\n- The focus on DGPs and the depiction of their multivariate Gaussian posterior form represents a significant departure from existing literature on NNs, wherein analogous results are yet to be established.\n- Discourses on representation learning in the NTK framework and its divergence from the NNGP are referenced, alongside theoretical explorations via physics-inspired methodologies.\n- The juxtaposition of convergence properties in two-layer networks against SGD versus Bayesian posteriors is highlighted.\n\n## Results\n### DGP Definition\n- A DGP is characterized by its mapping from inputs \\( X \\in \\mathbb{R}^{P \\times \\nu_0} \\) to outputs \\( Y \\in \\mathbb{R}^{P \\times \\nu_{L+1}} \\) with \\( L \\) intermediate layers, each comprising \\( N_l \\) features represented as \\( F_l \\in \\mathbb{R}^{P \\times N_l} \\).\n\n### DGP Priors\n- The joint distribution across layers and their dependencies can be expressed in terms of covariance structures utilizing Gram matrices, with the DGP formulation yielding a Wishart distributed posterior when subject to infinite width constraints.\n\n## The Bayesian Representation Learning Limit\n- The introduction of the Bayesian Representation Learning Limit asserts that as the layer widths converge to infinity, the likelihood must be appropriately rescaled to maintain representational fidelity, thus preventing the prior from dominating the posterior.\n\n### DGP Posterior Characteristics\n- Under the Bayesian Representation Learning Limit, we specify:\n  \\[\n  P(f_{\\lambda}^{l}|X,y) = \\mathcal{N}(f_{\\lambda}^{l}; 0, G^{l}) \n  \\]\n- The posterior Gaussianity is substantiated through empirical evaluations aligning with finite-width DGPs.\n\n## DKM Objective\n- The DKM objective yields insights into representation learning dynamics, wherein the likelihood component incentivizes utility on training data while KL-divergence terms ensure posterior adherence to prior distributions.\n\n### Empirical Validation\n- Empirical studies elucidate the convergence of DGP posteriors to DKM representations as network width increases, thus reinforcing the theoretical constructs posited.\n\n## Conclusion\n- The articulation of the Bayesian Representation Learning Limit serves to enhance the theoretical foundations of representation learning in DGPs, culminating in the conceptualization of DKMs as advanced kernel methods capable of handling large-scale datasets efficiently through the introduction of inducing points.\n\n### References\n- Extensive citation of foundational works in kernel methods, Gaussian processes, and neural networks are included to substantiate claims and provide a comprehensive bibliographic resource for further inquiry.",
        "- The paper introduces a new infinite width limit, the Bayesian representation learning limit, but it fails to convincingly demonstrate how this limit is significantly different from existing methods. The claims of novelty feel overstated given the amount of existing literature on infinite width limits.\n\n- The authors claim to retain representation learning in their new formulation, yet the explanation relies heavily on theoretical constructs that may not translate into practical applications. The theoretical framework might be too abstract for real-world implementation.\n\n- The paper asserts that Deep Gaussian processes (DGPs) have exactly multivariate Gaussian posteriors, but this oversimplifies the complexities involved in real-world data where such assumptions may not hold.\n\n- The optimization of the \"deep kernel machine objective\" is presented as a key strength, but the authors fail to address the potential computational inefficiencies in practice. The cubic scaling in the number of datapoints suggests that the method may be impractical for large datasets, which is a significant limitation.\n\n- The introduction of the sparse DKM, while intended to improve scalability, does not sufficiently address the inherent limitations of kernel methods, particularly the curse of dimensionality. The paper could have explored these aspects in more detail.\n\n- The paper claims to provide experimental confirmation of its theoretical results, but the experiments are limited in scope and fail to cover a diverse range of datasets or real-world scenarios. The reliance on a narrow set of experiments undermines the generalizability of the findings.\n\n- The introduction of the variational DKM is mentioned but not explored in depth. The lack of clarity on how variational approximations improve upon the already established methods is a glaring gap in the discussion.\n\n- The authors extend their results to Bayesian neural networks (BNNs), yet this section seems rushed and lacks rigorous validation. The intractability of exact posteriors for BNNs is acknowledged, but the proposed approximations are not adequately justified or tested.\n\n- The conclusion suggests that the contributions are significant, but the paper does not sufficiently address how these findings alter the landscape of representation learning in neural networks. There is a lack of critical engagement with prior work that limits the perceived impact of the research.\n\n- Overall, the paper feels overly optimistic in its claims without providing enough empirical evidence or theoretical justification to back up its assertions. The strength of the proposed methods is not sufficiently proven, leaving the reader questioning the reliability and applicability of the findings.",
        "# Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n## Exciting Innovations in Representation Learning!\n- **Revolutionary Framework**: Introducing the **Bayesian representation learning limit**! This groundbreaking concept retains representation learning in deep Bayesian models, a significant advancement that bridges the gap between infinite-width models and practical finite networks.\n  \n- **Deep Kernel Machines (DKMs)**: The paper presents DKMs as a **flexible, deep generalization of kernel methods**, offering a remarkable blend of deep learning's flexibility with the interpretability of kernel methods. This innovation could redefine how we approach machine learning tasks!\n\n- **Exact Multivariate Gaussian Posteriors**: The discovery that posteriors in this new framework are exactly multivariate Gaussian is a game-changer! This simplification allows for efficient computation and a deeper understanding of model behavior, making it easier for practitioners to harness the power of deep learning.\n\n- **Optimized Performance Objective**: The **DKM objective** cleverly combines log-likelihood for improved performance with KL-divergence terms that maintain proximity to the prior. This dual approach not only boosts performance but also ensures that the model remains well-regularized. \n\n- **Scalability**: The introduction of a **sparse DKM** that scales linearly with the number of datapoints is a monumental advancement for practical applications. This dramatically enhances computational efficiency, making it feasible to handle larger datasets without sacrificing performance!\n\n- **Empirical Validation**: Experimental results confirming the theoretical findings in wide but finite DGPs provide robust support for the proposed framework, showcasing its practical applicability and reliability in real-world scenarios.\n\n- **Unique Global Maximum**: The framework emphasizes the expectation of a unique global maximum, which is crucial for consistent and reliable model performance. This unique solution opens avenues for more predictable and stable outcomes in machine learning applications.\n\n- **Wide Applicability**: The methods developed have the potential to impact various fields, from computer vision to natural language processing, offering a versatile toolset for researchers and practitioners across disciplines.\n\n- **Extension to Bayesian Neural Networks**: The potential extension of the findings to Bayesian Neural Networks (BNNs) further enriches the framework, promising even broader applications and opportunities for exploration in probabilistic models.\n\n## Conclusion\nThis paper represents a thrilling leap forward in our understanding of representation learning in deep neural networks. With its innovative concepts, practical implementations, and promising results, it paves the way for a new era in machine learning that is more flexible, efficient, and powerful than ever before!",
        "### Notes on \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n1. **Representation Learning in Neural Networks**:\n   - Theoretical understanding of representation learning is crucial because it underpins the success of deep learning methods, which transform inputs into high-level representations.\n   - Traditional theoretical frameworks, particularly those based on Neural Network Gaussian Processes (NNGPs), have struggled to capture the nuances of representation learning due to their limiting assumptions, such as infinite width.\n\n2. **Bayesian Representation Learning Limit**:\n   - The introduction of the Bayesian representation learning limit allows for the retention of representation learning in deep Bayesian models, which deviates from the conventional infinite-width limit. \n   - This theoretical framework posits that as width approaches infinity, the prior becomes dominant; however, the model is adjusted to ensure that the likelihood does not vanish, allowing the model to learn meaningful representations even in high-dimensional spaces.\n\n3. **Multivariate Gaussian Posteriors**:\n   - Theoretical results demonstrate that in the Bayesian representation learning limit, the posteriors of Deep Gaussian Processes (DGPs) manifest as multivariate Gaussian distributions. \n   - This finding is significant as it aligns with the principles of Bayesian inference, where the prior and likelihood interplay leads to tractable and interpretable posterior distributions.\n\n4. **Deep Kernel Machines (DKMs)**:\n   - DKMs emerge as a theoretical extension of kernel methods, integrating the flexibility of deep learning with the principled foundations of kernel methods.\n   - The DKM objective, derived from the Bayesian representation learning limit, combines a log-likelihood term that enhances predictive performance with KL divergence regularization terms that maintain closeness to prior representations.\n\n5. **Interpretation of the DKM Objective**:\n   - The DKM objective provides a theoretical framework for understanding representation learning in terms of optimization. \n   - The log-likelihood term ensures that representations are optimized for the task, while KL divergence terms regulate the learned representations to remain close to their prior, thus encapsulating the balance between fitting data and regularization.\n\n6. **Flexibility of Kernel Methods**:\n   - Kernel methods have historically been limited by their inflexibility in representation learning. However, the theoretical insights provided by DKMs suggest that deep learning models can indeed generalize traditional kernel methods while offering greater flexibility in learning top-layer representations.\n\n7. **Comparison with Traditional Approaches**:\n   - The theoretical lens provided by NNGPs and NTKs highlights the differences in understanding representation learning. The NNGP framework is posited as more suitable for analyzing representation transformations, while NTK focuses on gradient-based changes in predictions.\n   - The formal distinctions drawn between these frameworks elucidate the underlying mechanisms that govern the success of deep learning architectures.\n\n8. **Implications for Future Research**:\n   - The theoretical advancements made in this study open avenues for further exploration into the nature of representation learning across various models, including Bayesian Neural Networks (BNNs) and their non-Gaussian posteriors.\n   - Understanding these theoretical aspects is vital for the development of more robust machine learning frameworks that can effectively learn from data while adhering to principles of Bayesian inference.\n\n9. **Limitations of Empirical Validation**:\n   - While empirical results are presented, they are secondary to the theoretical contributions, which lay the groundwork for understanding the deep structures of representation learning. \n   - The focus on theoretical constructs emphasizes the need for a solid conceptual foundation upon which empirical methods can be built, rather than allowing empirical observations to lead theoretical discourse.\n\nIn conclusion, the paper presents a robust theoretical framework that redefines our understanding of representation learning in neural networks, specifically through the lens of the Bayesian representation learning limit and its implications for deep kernel methods.",
        "# Notes on Implementation Details and Code Availability\n\n## General Structure\n- The paper introduces a new infinite width limit called the Bayesian representation learning limit for deep Gaussian processes (DGPs) and Bayesian neural networks (BNNs). This limit retains representation learning by adjusting the scaling of likelihoods and priors.\n\n## Key Equations and Implementation\n- Posterior covariances obtained via:\n  \\[\n  L(G_1, \\ldots, G_L) = \\log P(Y|G_L) - \\sum_{l=1}^{L} \\nu_l D_{KL}(N(0, G_l) \\| N(0, K(G_{l-1})))\n  \\]\n- The formula for calculating Gram matrices:\n  \\[\n  G(Fl) = \\frac{1}{N_l} \\sum_{l=1}^{N_l} f_l \\cdot f_l^T\n  \\]\n- The Gram matrix for DGPs is defined recursively, with \\( G_0 \\) initialized appropriately to ensure it is full-rank.\n\n## Sparse DKM Implementation\n- A sparse DKM is introduced to scale linearly with the number of data points, contrasting with the cubic scaling of naive kernel methods. This involves using inducing points from Gaussian process literature.\n\n## Optimization Techniques\n- Adam optimizer is employed with a learning rate of \\( 0.001 \\) for training models, with specific iterations dependent on dataset size (5000 for smaller datasets, 1000 for larger).\n- Langevin sampling is used for Bayesian inference with 10 parallel chains and a step size of \\( 10^{-3} \\).\n\n## Monte-Carlo Approximations\n- Variational DKM uses Monte-Carlo methods to approximate Gram matrices, with \\( K = 216 \\) samples to ensure sufficient representation of the posterior.\n\n## Computational Details\n- Full Gram matrices are constructed by separating inducing and test/train points, allowing for efficient computation of posterior distributions.\n- The predictive algorithm is provided in an algorithmic format (Algorithm 1), detailing how to compute predictions based on optimized Gram matrices.\n\n## Experimentation Framework\n- The experiments utilize standard UCI datasets, ensuring that the DKM's performance is benchmarked against traditional methods.\n- The implementation emphasizes the use of inducing points to handle large-scale datasets effectively.\n\n## Code Availability\n- The paper does not provide explicit links to the code but mentions that the implementation is inspired by existing methods in Gaussian process literature.\n\n## Summary of Results\n- The DKM shows improved performance over MAP inference and standard kernel methods, with empirical results confirming that the learned representations are consistent across different initializations.\n\n## Future Work and Extensions\n- The methods discussed are extended to BNNs, highlighting that the exact posteriors are intractable and must be approximated, reflecting ongoing challenges in the implementation of these models.\n\n## Additional Considerations\n- The paper notes the potential for multimodality in the feature space, suggesting that while the Gram matrix converges to a unique solution, the underlying feature representations may exhibit a complex landscape.\n\nThis detailed implementation-centric overview captures the essence of the methods and computations utilized in the paper without addressing the broader theoretical contributions or implications.",
        "1. The paper claims to introduce a novel infinite width limit, the Bayesian representation learning limit, which is presented as a significant advancement over previous methods. However, it fails to acknowledge that many existing frameworks, like the neural tangent kernel (NTK), have already explored similar concepts without the need for a new limit. This raises questions about the originality of the proposed limit.\n\n2. The authors emphasize that their approach allows for representation learning, in contrast to the traditional neural network Gaussian processes (NNGPs). However, they do not provide a fair comparison to existing works that have also incorporated representation learning in their frameworks, implying that their contribution is more groundbreaking than it may actually be.\n\n3. The introduction of deep kernel machines (DKMs) as a flexible, deep generalization of kernel methods is claimed to be a remarkable achievement. Yet, the paper does not adequately compare this to the flexibility already achieved in various other kernel methods, which have been proven effective prior to this work.\n\n4. The paper's assertion that standard kernel methods are inflexible compared to deep neural networks (DNNs) overlooks the advancements in kernel methods, which have adapted to incorporate more flexibility over the years. This comparison downplays the evolution and capabilities of kernel methods, making their argument appear biased.\n\n5. The authors suggest that their Bayesian representation learning limit retains features that allow for improved performance in deep models, yet they do not fairly evaluate how these improvements stack against existing models, particularly those utilizing the NTK framework, which has shown promising results in representation learning.\n\n6. The introduction of a sparse DKM is positioned as a breakthrough that scales linearly with the number of data points. However, this does not sufficiently address the existing literature on sparse approximations in Gaussian processes, which have been widely discussed and implemented in other works.\n\n7. The experimental validation provided is meant to confirm the superiority of the proposed models, but the paper does not provide a balanced view by comparing its results against a broader range of established models, nor does it assess how its performance holds up in practical scenarios compared to other leading frameworks.\n\n8. The paper extends its findings to Bayesian neural networks (BNNs) but does so without giving proper credit to prior research that has tackled similar issues. There is an implication that their findings in the context of BNNs are novel, while previous works have laid foundational insights in this area.\n\n9. Although the authors discuss how their model allows for better representation learning through posterior optimization, they neglect to compare this method to existing variational inference techniques that have been successfully employed in deep learning, thus presenting their work as more innovative than it might be.\n\n10. Finally, while the paper claims that their theoretical insights lead to practical improvements, it lacks a rigorous evaluation against the performance metrics established in prior research, which diminishes the credibility of their claims and suggests a bias towards overstating their contributions.",
        "1. **Title Formatting**: The title should be consistently capitalized: \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\" instead of mixing cases.\n\n2. **Author Information**: The \"Anonymous authors Paper under double-blind review\" should have a consistent format, possibly as a footnote or in a designated section.\n\n3. **Abstract**: \n   - \"deep machine learning methods\" should be hyphenated as \"deep-machine learning methods\" for consistency.\n   - \"infinite width limits eliminate representation learning\" could be more clearly stated, perhaps as \"infinite-width limits eliminate the ability to learn representations.\"\n\n4. **Introduction**: \n   - \"modern machine learning methods from neural networks (NNs) to deep Gaussian processes (DGPs)\" should have \"Gaussian processes\" consistently capitalized as \"Deep Gaussian Processes.\"\n   - Inconsistent spacing around citations, e.g., \"DGPs Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017\" should consistently include a space before the citation punctuation.\n\n5. **Kernel Methods Section**: \n   - \"standard kernel method\" should be pluralized to \"standard kernel methods\" for grammatical consistency.\n   - The phrase \"the kernel (or equivalently the representation) is highly inflexible\" could be clearer; consider rephrasing for better clarity.\n\n6. **Contributions List**: \n   - The bullet points should maintain consistent punctuation; some end with a period while others do not.\n   - The notation \"\u03bb\" should be defined explicitly when first used to avoid confusion.\n\n7. **Related Work Section**:\n   - \"the very different NTK setting\" should specify \"neural tangent kernel (NTK)\" for clarity.\n   - Inconsistent use of semicolons and commas in citations (e.g., \"Jacot et al., 2018; Yang, 2019\" vs. \"Hanin & Nica, 2019; Aitchison, 2020\"). \n\n8. **Figures and Tables**: \n   - Ensure all figure and table captions are consistently formatted. For example, \"Figure 1\" should be consistently referred to as \"Figure 1:\" at the beginning of the caption.\n   - In \"Figure 2,\" specify what \"DGP\" and \"DKM\" stand for in the caption for clarity.\n\n9. **Mathematical Notation**: \n   - All mathematical variables (e.g., \"G, K, \u03bd\") should be defined at their first use to ensure reader comprehension.\n   - Ensure consistent use of parentheses in equations; for example, \"P (Fl | Fl\u22121)\" should uniformly be formatted as \"P(Fl | Fl\u22121)\" throughout.\n\n10. **Conclusion Section**:\n    - \"the Bayesian representation learning limit\" should be capitalized for consistency with previous sections where it is introduced.\n    - The phrase \"deep kernel method: DKMs\" should avoid the colon or ensure consistent punctuation treatment.\n\n11. **References**: \n    - Ensure that all references follow a consistent citation style, particularly with the use of italics, commas, and periods.\n    - \"arXiv preprint\" should consistently use \"arXiv\" in the same formatting style across all citations.\n    \n12. **Appendices**: \n    - Each appendix section should be uniformly labeled (Appendix A, Appendix B, etc.) to maintain an organized structure.\n    - In Appendix A, clarify \"where K(\u00b7) is a deterministic function\" to specify what that function is for better understanding.\n\n13. **General Typos**: \n    - Ensure that all instances of \"N (\u00b7)\" are consistently formatted; some instances may be missing parentheses or have inconsistent spacing.\n    - Watch for the misuse of \"therefore\" and \"thus\"; ensure they are used in the correct logical context.\n\n14. **Consistency in Terminology**: \n    - Ensure that terms like \"likelihood,\" \"prior,\" and \"posterior\" are used consistently throughout the paper to avoid confusion.\n    - The term \"Gaussian process\" should always be capitalized as \"Gaussian Process\" when referring to a specific method or model.\n\n15. **Overall Formatting**: \n    - The document would benefit from a final proofread to catch any remaining inconsistencies in font size, style, and overall formatting to ensure a professional appearance.",
        "1. The paper focuses primarily on the Bayesian representation learning limit, but it could have explored how this framework interacts with other types of representation learning beyond Gaussian processes, such as in reinforcement learning or unsupervised learning scenarios.\n\n2. While the authors introduce the deep kernel machines (DKMs) as a flexible generalization of kernel methods, they do not consider how these models might be adapted for non-Euclidean spaces or complex data types like graphs or sequences, which limits their applicability.\n\n3. The experimental validation is confined to specific UCI datasets; expanding the evaluation to include a broader range of tasks, such as image classification or natural language processing, would demonstrate the versatility of the proposed methods.\n\n4. The paper mentions extending results to Bayesian neural networks (BNNs) in the appendix but lacks a detailed discussion on the implications of these extensions for real-world applications, especially in fields like healthcare or finance where uncertainty quantification is critical.\n\n5. The relationship between the proposed models and existing deep learning architectures could have been explored more thoroughly, particularly how DKMs might integrate with or enhance modern architectures like transformers or convolutional neural networks.\n\n6. The theoretical insights regarding representation learning are valuable but could be expanded to address how the learned representations might be interpreted or visualized, providing more practical insights for practitioners.\n\n7. The authors mention the uniqueness of the global maximum in practical settings but do not discuss the implications of potential local minima in training deep kernel machines, which could affect model performance and training stability.\n\n8. The paper could have examined the scalability of the proposed methods in more detail, particularly regarding their performance on large-scale datasets, as well as their computational efficiency compared to traditional kernel methods.\n\n9. The analysis of the KL-divergence terms in the DKM objective is insightful, but a deeper exploration of their sensitivity to hyperparameter choices and their impact on model performance would provide more comprehensive guidelines for practitioners.\n\n10. The discussion on the limitations of the proposed methods is minimal; a more thorough examination of potential pitfalls, such as overfitting in high-dimensional spaces, would be beneficial for future research directions.\n\n11. Although the authors propose a sparse DKM, they do not address how this sparsity might affect interpretability or the trade-offs between model complexity and performance, which is a crucial consideration in many applications. \n\n12. The implications of the Bayesian framework for uncertainty estimation in predictions are touched upon but not explored in depth; discussing the benefits and challenges of incorporating uncertainty quantification into DKMs could extend the paper's impact.\n\n13. The paper\u2019s findings, while significant, would benefit from a comparative analysis with other recent advances in representation learning, such as contrastive learning or self-supervised learning techniques, to position the proposed work within the current landscape of machine learning research. \n\n14. The authors could have included a discussion on how the proposed methods relate to or could benefit from ongoing research in causal inference, as understanding causal relationships is increasingly important in many fields of application.",
        "## Notes on Statistical Methodology and Significance Testing in \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\"\n\n### Abstract\n- The paper introduces a new infinite width limit termed the **Bayesian representation learning limit**, which allows for representation learning in deep Bayesian models while maintaining statistical rigor.\n- The authors demonstrate that **Deep Gaussian Processes (DGPs)** in this limit yield multivariate Gaussian posteriors, confirming statistical properties through optimization of a log-likelihood combined with KL-divergences.\n\n### Contributions\n1. **Bayesian Representation Learning Limit**:\n   - Establishes a limit where the prior dominates likelihood. This is significant as it directly relates to convergence properties in Bayesian inference.\n   - The methodology involves rescaling the likelihood to maintain a balance with the prior, ensuring robust statistical inference.\n\n2. **Posterior Covariances**:\n   - Obtained via optimization of a defined **deep kernel machine objective** \\( L(G_1, \\ldots, G_L) \\), which explicitly incorporates statistical measures like log-likelihood and KL-divergence.\n   - The log-likelihood term \\( \\log P(Y|G_L) \\) serves as a performance metric, while KL-divergence terms enforce a distance to the prior, thus establishing a framework for regularization.\n\n3. **Interpretation of Objective**:\n   - The dual role of the objective provides statistical insight: improving performance while maintaining proximity to prior distributions, thus ensuring that the optimized representations remain statistically valid.\n\n4. **Sparse Deep Kernel Machine**:\n   - Introduced to address computational efficiency, while maintaining statistical integrity by scaling linearly with the number of datapoints\u2014essential for large datasets.\n\n5. **Extension to Bayesian Neural Networks (BNNs)**:\n   - Acknowledges the non-Gaussian nature of BNN posteriors and proposes approximations that still respect statistical properties, emphasizing the importance of variational methods in the context of deep learning.\n\n### Results\n- The paper presents empirical validation of theoretical claims through statistical evaluations, demonstrating Gaussian behavior in wide DGPs and matching this to the proposed deep kernel machine framework.\n- A focus on **root mean square error (RMSE)** is employed to measure discrepancies between learned representations, reinforcing the emphasis on statistical performance metrics in validating model efficacy.\n\n### Related Work and Significance\n- The authors position their work within a broader statistical framework, contrasting it with existing literature on NNGPs and NTK approaches, highlighting the unique contributions to understanding representation learning through a statistical lens.\n- The significance of their findings is underscored by the ability to compute the exact posterior distributions under the proposed model, which is a critical advancement in the field of Bayesian inference for deep learning.\n\n### Statistical Methods Used\n- **KL-Divergence**: Used as a regularization technique to maintain statistical proximity to the prior, essential for ensuring that the model does not overfit to the training data.\n- **Likelihood Functions**: Formulated to capture the probabilistic nature of outputs given the inputs, tied closely to the Gaussian assumptions underpinning the model.\n- **Empirical Validation**: Utilizes statistical metrics (e.g., RMSE, KL-Divergence) to confirm theoretical predictions, thus ensuring robust statistical foundations for the proposed methodologies.\n\n### Conclusion\n- The introduction of the Bayesian representation learning limit and the resulting statistical methodologies provide a deep generalization of kernel methods, showcasing the interplay between representation learning and statistical inference.\n- The findings encourage future research to explore the statistical implications of deep learning models, particularly in the context of Bayesian approaches, highlighting the necessity for rigorous statistical validation in machine learning frameworks. \n\n### References\n- Extensive citations from statistical learning and machine learning literature substantiate the claims made, reinforcing the statistical basis of the proposed methodologies and their significance in advancing the understanding of representation learning in neural networks.",
        "- The paper does not address the practical applicability of the Bayesian representation learning limit to real-world datasets, particularly concerning the infinite-width assumption which might not hold in practice.\n  \n- While it introduces a sparse deep kernel machine (DKM), the paper does not explore the robustness or effectiveness of this approach across a diverse range of datasets, limiting its validation scope.\n\n- The connection between the proposed deep kernel machine and existing kernel methods is not thoroughly investigated, leaving questions about its comparative advantages or disadvantages unanswered.\n\n- The theoretical framework established lacks comprehensive empirical validation, particularly in demonstrating the effectiveness of the Bayesian representation learning limit in various tasks beyond those tested.\n\n- The paper does not provide a detailed examination of the computational efficiency of implementing the proposed methods in large-scale settings, which is critical for practical adoption.\n\n- There is insufficient discussion regarding potential limitations of the proposed methods in terms of scalability and their performance under different conditions or hyperparameter settings.\n\n- The extension to Bayesian neural networks (BNNs) is mentioned but not explored in depth, leaving an open question on how the findings can be reliably translated from DGPs to BNNs.\n\n- The paper does not address the implications of non-Gaussian posteriors in real-world applications, which may significantly differ from the Gaussian assumptions made in the theoretical framework.\n\n- Future work could include a more extensive exploration of the optimization landscapes for the DKM objective to identify potential multimodality issues, which were not extensively covered.\n\n- The implications of the findings for other types of neural architectures or models beyond DGPs and BNNs are not considered, limiting the generalization of the results.\n\n- There is a lack of a clear roadmap for future research directions or potential applications of the proposed methods in various domains, which would help in guiding subsequent studies.",
        "- **Abstract**: The authors are merely reiterating concepts that have been well-established in the field. It's amusing how they think they\u2019ve made a breakthrough by rehashing basic ideas surrounding representation learning and Gaussian processes.\n\n- **Introduction**: Ah, the classic tale of deep learning versus kernel methods. The authors seem to have just discovered that deep models can learn representations. How groundbreaking. They mention two broad families of infinite limits\u2014NNGP and NTK\u2014like it's some major revelation. It's not. \n\n- **Contributions**: \n  - The \"Bayesian representation learning limit\"? Sounds fancy, but it's just a clever renaming of existing concepts. They claim to retain representation learning, but isn\u2019t that what we already do in practice?\n  - The multivariate Gaussian posteriors? Yawn. This is like saying \"water is wet.\"\n  - They optimistically think they\u2019ve introduced something new with \"deep kernel machines\" (DKMs). As if we haven't been using kernel methods for ages.\n  - Sparse DKM scaling linearly? Great, but isn\u2019t that just basic optimization?\n\n- **Related Work**: The authors seem to think they are the first to address representation learning in the NNGP setting. Spoiler alert: they\u2019re not. They cite a laundry list of previous work without much insight.\n\n- **Results**: \n  - They define a DGP\u2014how original. They need to realize that this is standard fare in the literature.\n  - The graphical model structure? More like a convoluted way to restate what everyone already knows about neural networks and Gaussian processes.\n  - Their experimental results are just confirmations of what was already expected. There's nothing novel here.\n\n- **Discussion**: The authors go to great lengths to explain why their findings matter, but frankly, they don\u2019t. The idea that deeper networks might yield better results is about as insightful as saying \"the sky is blue.\"\n\n- **Conclusion**: A conclusion replete with self-importance. They claim to have developed a new class of \"practical deep kernel methods.\" In reality, they\u2019ve just repackaged existing knowledge with a new label.\n\n- **Overall Tone**: The paper reeks of trying to sell old wine in a new bottle. They should realize that their \"novel\" insights are common knowledge for anyone who has been in the field longer than a week. It's cute that they think they\u2019ve uncovered some magical secret about deep learning and Gaussian processes, but the reality is far less exciting.",
        "- The introduction highlights the importance of representation learning in deep neural networks (NNs) and deep Gaussian processes (DGPs). I propose to extend this exploration by integrating novel representation learning techniques from my recent work, which incorporates unsupervised learning methods to enhance representation flexibility.\n\n- The authors discuss infinite-width limits and the challenges they pose to understanding representation learning. I suggest an alternative approach that leverages finite-width models while employing dropout techniques as a regularization method, which could provide insights into the robustness of learned representations.\n\n- The paper presents the Bayesian representation learning limit and its key contributions. I recommend exploring hybrid models that combine this limit with recent advancements in attentive mechanisms, which have shown promise in improving feature extraction and representation quality in various contexts.\n\n- The DKM (Deep Kernel Machine) objective is introduced as a flexible generalization of kernel methods. I propose to investigate the potential of incorporating kernel learning strategies that adaptively select kernel types based on data properties, which could yield superior performance in diverse applications.\n\n- The authors confirm that optimizing the DKM objective yields Gaussian posteriors. I suggest including empirical evaluations of alternative posteriors, such as those based on variational inference, which can capture more complex distributions and improve generalization in practice.\n\n- The sparse DKM presented offers a scalable solution. I recommend exploring the integration of active learning techniques to identify the most informative data points for inducing point selection, thereby enhancing the efficiency of the sparse DKM approach.\n\n- The paper emphasizes the need for flexible deep generalizations of kernel methods. I propose examining the implications of incorporating meta-learning frameworks into kernel methods, as this could enable the model to adaptively refine its representation based on prior experiences.\n\n- The results section confirms the close match between infinite-width DKMs and wide but finite DGPs. I suggest conducting further comparative studies with other neural architectures, such as convolutional networks, to assess the generality of the findings across different types of models.\n\n- The conclusion highlights the need for scalable approaches in dealing with large datasets. I recommend exploring distributed computing solutions coupled with federated learning strategies to enhance the scalability and applicability of DKMs in real-world scenarios. \n\n- Future work should focus on cross-domain applications of the proposed methods, particularly in transfer learning settings, to validate the robustness and adaptability of the learned representations across varied tasks and data distributions.",
        "- The paper presents a new approach to deep kernel methods (DKMs) and reports experimental results on various benchmark datasets from the UCI repository.\n- The performance of DKMs is compared against standard kernel methods and the MAP (maximum a posteriori) objective using several datasets (boston, concrete, energy, kin8nm, naval, power, protein, wine, yacht).\n- Table 1 in the paper shows RMSE (root mean square error) results for different methods across datasets, with deep kernel machines outperforming the MAP and hyperparameter optimization baselines in most cases.\n- The RMSE values indicate that the DKM consistently achieves lower errors across several datasets, highlighting its efficacy in practical applications.\n- The paper emphasizes that the DKM objective yields better performance than naive kernel methods, which typically scale cubically with the number of data points, by introducing sparsity through inducing points.\n- The experimental results confirm that the deep kernel machine objective leads to strong performance in terms of predictive accuracy, especially when applied to UCI benchmark datasets.\n- The authors provide detailed comparisons and visualizations (e.g., figures showing convergence and representation learning) that reinforce the performance claims of their method.\n- Overall, the paper's results suggest that the proposed DKM framework is a competitive alternative to existing methods, demonstrating significant improvements in benchmark performance metrics.",
        "1. **Abstract Clarity**: The abstract contains dense information that may overwhelm readers. Consider breaking it into smaller, more digestible sentences or bullet points to enhance readability.\n\n2. **Terminology Consistency**: Ensure that terms such as \"DGPs,\" \"DKMs,\" and \"NNGPs\" are consistently defined and used throughout the paper. The first occurrence should clarify what each acronym stands for for readers unfamiliar with the terminology.\n\n3. **Flow and Structure**: The paper could benefit from clearer section transitions. For instance, the movement from the introduction to the contributions could be more explicitly linked to guide the reader through the narrative.\n\n4. **Use of Bullet Points**: In the contributions section, bullet points are employed effectively. This format should be maintained throughout the paper where lists of items or processes are presented to improve clarity.\n\n5. **Complex Sentences**: Some sentences are overly complex and could be simplified. For example, breaking long sentences into shorter ones will make it easier for readers to follow the argument.\n\n6. **Visual Aids**: Figures and diagrams (like Figure 1) are mentioned but could be better integrated within the text. Ensure that each figure is referenced in the narrative and provides a clear explanation of its relevance.\n\n7. **Appendices**: The appendices are referenced multiple times, but their content is not always clearly connected to the main text. Each reference should include a brief explanation of what to expect in the appendix to maintain reader engagement.\n\n8. **Technical Jargon**: The paper includes a significant amount of technical jargon that could alienate non-specialist readers. Consider adding brief explanations or definitions for complex terms when they first appear.\n\n9. **Conclusion Section**: The conclusion should summarize the main findings more succinctly and clearly state the implications of the research. Reinforcing how this work advances the field could strengthen the conclusion.\n\n10. **References Formatting**: Ensure that references are consistently formatted according to the chosen citation style. Pay attention to punctuation, order, and presentation, especially in the bibliography.\n\n11. **Typographical Errors**: Look for typographical errors or inconsistencies in notation (e.g., inconsistent use of symbols or formatting in equations). These can distract from the content and may confuse readers.\n\n12. **Redundant Phrasing**: Identify and eliminate redundant phrases to create a more concise writing style. Phrases like \"in particular\" can often be removed or substituted with more straightforward language.\n\n13. **Engagement with Related Works**: While related works are discussed, their connection to the current study could be made clearer. Highlighting how this work builds on or diverges from previous studies will enhance the scholarly narrative.\n\n14. **Logical Flow**: Ensure that each section logically flows into the next. Use clear topic sentences at the beginning of paragraphs to establish the main idea of each section.\n\n15. **Reader Guidance**: Consider including a brief overview or roadmap at the end of the introduction to guide the reader through the structure of the paper, especially in lengthy sections.\n\nBy addressing these points, the overall quality, presentation, and clarity of the paper can be significantly improved, making it more accessible and engaging for a broader audience."
      ]
    },
    {
      "context": "Under review as a conference paper at ICLR 2023\n\nDIP-GNN: DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph neural network (GNN) pre-training methods have been proposed to enhance the power of GNNs. Specifically, a GNN is first pre-trained on a large-scale unlabeled graph and then fine-tuned on a separate small labeled graph for downstream applications, such as node classification. One popular pre-training method is to mask out a proportion of the edges, and a GNN is trained to recover them. However, such a generative method suffers from graph mismatch. That is, the masked graph input to the GNN deviates from the original graph. To alleviate this issue, we propose DiP-GNN (Discriminative Pre-training of Graph Neural Networks). Specifically, we train a generator to recover identities of the masked edges, and simultaneously, we train a discriminator to distinguish the generated edges from the original graph\u2019s edges. The discriminator is subsequently used for downstream fine-tuning. In our pre-training framework, the graph seen by the discriminator better matches the original graph because the generator can recover a proportion of the masked edges. Extensive experiments on large-scale homogeneous and heterogeneous graphs demonstrate the effectiveness of the proposed framework. Our code will be publicly available.\n\n1\n\nINTRODUCTION\n\nGraph neural networks (GNNs) have achieved superior performance in various applications, such as node classification (Kipf & Welling, 2017), knowledge graph modeling (Schlichtkrull et al., 2018) and recommendation systems (Ying et al., 2018). To enhance the power of GNNs, generative pretraining methods are developed (Hu et al., 2020b). During the pre-training stage, a GNN incorporates topological information by training on a large-scale unlabeled graph in a self-supervised manner. Then, the pre-trained model is fine-tuned on a separate small labeled graph for downstream applications. Generative GNN pre-training is akin to masked language modeling in language model pre-training (Devlin et al., 2019). That is, for an input graph, we first randomly mask out a proportion of the edges, and then a GNN is trained to recover the original identity of the masked edges.\n\nOne major drawback with the abovementioned approach is graph mismatch. That is, the input graph to the GNN deviates from the original one since a considerable amount of edges are dropped. This causes changes in topological information, e.g., node connectivity. Consequently, the learned node embeddings may not be desirable.\n\nTo mitigate the above issues, we propose DiP-GNN ( Discriminative Pre-training of Graph Neural Networks). In DiP-GNN, we simultaneously train a generator and a discriminator. The generator is trained similar to existing generative pre-training approaches, where the model seeks to recover the masked edges and outputs a reconstructed graph. Subsequently, the reconstructed graph is fed to the discriminator, which predicts whether each edge resides in the original graph (i.e., a true edge) or is wrongly constructed by the generator (i.e., a fake edge). After pre-training, we fine-tune the discriminator on downstream tasks. Figure 1 illustrates our training framework. Note that our work is related to Generative Adversarial Nets (GAN, Goodfellow et al. 2014), and detailed discussions are presented in Section 3.4. We remark that similar approaches have been used in natural language processing (Clark et al., 2020). However, we identify the graph mismatch problem (see Section 4.5), which is specific to graph-related applications and is not observed in natural language processing.\n\nThe proposed framework is more advantageous than generative pre-training. This is because the reconstructed graph fed to the discriminator better matches the original graph compared with the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of DiP-GNN. From left to right: Original graph; Graph with two masked edges (dashed lines); Reconstructed graph created by the generator (generated edges are the dashed red lines); Discriminator labels each edge as [G] (generated) or [O] (original), where there are two wrong labels (shown in red).\n\nmasked graph fed to the generator. Consequently, the discriminator can learn better node embeddings. Such a better alignment is because the generator recovers the masked edges during pretraining, i.e., we observe that nearly 40% of the missing edges can be recovered. We remark that in our framework, the graph fed to the generator has missing edges, while the graph fed to the discriminator contains wrong edges since the generator may make erroneous predictions. However, empirically we find that missing edges hurt more than wrong ones, making discriminative pre-training more desirable (see Section 4.5 in the experiments).\n\nWe demonstrate effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs. Results show that the proposed method significantly outperforms existing generative pre-training and self-supervised learning approaches. For example, on the homogeneous Reddit dataset (Hamilton et al., 2017) that contains 230k nodes, we obtain an improvement of 1.1 in terms of F1 score; and on the heterogeneous OAG-CS graph (Tang et al., 2008) that contains 1.1M nodes, we obtain an improvement of 2.8 in terms of MRR score in the paper field prediction task.\n\n2 BACKGROUND\n\n\u22c4 Graph Neural Networks. Graph neural networks compute a node\u2019s representation by aggregating information from the node\u2019s neighbors. Concretely, for a multi-layer GNN, the feature vector h(k) of node v at the k-th layer is\n\nv\n\na(k)\n\nv = Aggregate\n\n(cid:16)(cid:110)\n\nh(k\u22121)\n\nu\n\n\u2200u \u2208 Neighbor(v)\n\n(cid:111)(cid:17)\n\n, h(k)\n\nv = Combine\n\n(cid:16)\n\nv , h(k\u22121) a(k)\n\nv\n\n(cid:17)\n\n,\n\nwhere Neighbor(v) denotes all the neighbor nodes of v. Various implementations of Aggregate(\u00b7) and Combine(\u00b7) are proposed for both homogeneous (Defferrard et al., 2016; Kipf & Welling, 2017; Velickovic et al., 2018; Xu et al., 2019) and heterogeneous graphs (Schlichtkrull et al., 2018; Wang et al., 2019; Zhang et al., 2019; Hu et al., 2020c).\n\n\u22c4 Graph Neural Network Pre-Training. Previous unsupervised learning methods leverage the graph\u2019s proximity (Tang et al., 2015) or information gathered by random walks (Perozzi et al., 2014; Grover & Leskovec, 2016; Dong et al., 2017; Qiu et al., 2018). However, the learned embeddings cannot be transferred to unseen nodes, limiting the methods\u2019 applicability. Other unsupervised learning algorithms adopt contrastive learning (Hassani & Ahmadi, 2020; Qiu et al., 2020; Zhu et al., 2020; 2021; You et al., 2020; 2021). That is, we generate two views of the same graph, and then maximize agreement of node presentations in the two views. However, our experiments reveal that these methods do not scale well to extremely large graphs with millions of nodes.\n\nMany GNN pre-training methods focus on generative objectives. For example, GAE (Graph AutoEncoder, Kipf & Welling 2016) proposes to reconstruct the graph structure; GraphSAGE (Hamilton et al., 2017) optimizes an unsupervised loss derived from a random-walk-based metric; and DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes the mutual information between node representations and a graph summary representation.\n\nThere are also pre-training methods that extract graph-level representations, i.e., models are trained on a large amount of small graphs instead of a single large graph. For example, Hu et al. 2020a propose pre-training methods that operate on both graph and node level; and InfoGraph (Sun et al., 2020) proposes to maximize the mutual information between graph representations and representations of the graphs\u2019 sub-structures. In this work, we focus on pre-training GNNs on a single large graph instead of multiple small graphs.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHOD\n\nWe formally introduce the proposed discriminative GNN pre-training framework DiP-GNN. The algorithm contains two ingredients that operate on edges and features.\n\n3.1 EDGE GENERATION AND DISCRIMINATION\n\nSuppose we have a graph G = (N , E), where N denotes all the nodes and E denotes all the edges. We randomly mask out a proportion of the edges, such that E = Eu \u222a Em, where Eu is the unmasked set of edges and Em is the set of edges that are masked out.\n\nFor a masked edge e = (n1, n2) \u2208 Em, where n1 and n2 are the two nodes connected by e, the generator\u2019s goal is to predict n1 given n2 and the unmasked edges Eu. For each node n, we compute its representation hg(n) = f e g), which is parameterized by \u03b8e g. We remark that the computation of hg(\u00b7) only relies on the unmasked edges Eu. We assume that the generation process of each edge is independent. Then, we have the prediction probability\n\ng) using the generator f e\n\ng (n, \u03b8e\n\ng (\u00b7, \u03b8e\n\np(n1|n2, Eu) =\n\nexp (d(hg(n1), hg(n2))) n\u2032\u2208C exp (d(hg(n\u2032), hg(n2)))\n\n(cid:80)\n\n, C = {n1} \u222a (N \\ Neighbor(n2)).\n\n(1)\n\nHere, C is the candidate set for n1, which contains all the nodes that are not connected to n2 except n1 itself. Moreover, the distance function d(\u00b7, \u00b7) is chosen as a trainable cosine similarity, i.e.,\n\nwhere W cos is a trainable weight. The training loss for the generator is defined as\n\nd(u, v) =\n\n(W cosu)\u22a4v ||W cosu|| \u00b7 ||v||\n\n,\n\ng) = (cid:80) which is equivalent to maximizing the likelihood of correct predictions.\n\n\u2212 log p(n1|n2, Eu),\n\n(n1,n2)\u2208Em\n\ng(\u03b8e\n\nLe\n\n(2)\n\n(3)\n\nThe goal of the generator is to recover the masked edges in Em. Therefore, after we train the generator, we use the trained model to generate Eg = {((cid:98)n1, n2)}(n1,n2)\u2208Em, where each (cid:98)n1 is the model\u2019s prediction as (cid:98)n1 = argmaxn\u2032\u2208C p(n\u2032|n2, Eu). Because the generator cannot correctly predict every edge, some edges in Eg are wrongly generated (i.e., not in Em). We refer to such edges as fake edges, and the rest as true edges. Concretely, we denote the true edges E true = Eu \u222a(Em \u2229Eg), i.e., the unmasked edges and the edges correctly generated by the generator. Correspondingly, we denote the fake edges E fake = E \\ E true.\n\nThe discriminator is trained to distinguish edges that are from the original graph (i.e., the true edges) and edges that are not (i.e., fake edges). Specifically, given the true edges E true and the fake ones E fake, we first compute hd(n) = f e d) is the discriminator model parameterized by \u03b8e d. We highlight that different from computing hg(\u00b7), the computation of hd(\u00b7) relies on all the edges, such that the discriminator can separate a fake edge from a true one. Then, for each edge e = (n1, n2) \u2208 E true \u222a E fake, the discriminator outputs\n\nd) for every node n \u2208 N , where f e\n\nd (n, \u03b8e\n\nd (\u00b7, \u03b8e\n\npfake = p(e \u2208 E fake|E true, E fake) = sigmoid (d(hd(n1), hd(n2))) , where d(\u00b7, \u00b7) is the distance function in Eq. 2. The training loss for the discriminator is the binary cross-entropy loss of predicting whether an edge is fake or not, defined as\n\n(4)\n\nLe\n\nd) = (cid:80) where 1{\u00b7} is the indicator function.\n\ne\u2208Etrue\u222aEfake\n\nd(\u03b8e\n\n\u22121{e \u2208 E fake} log(pfake) \u2212 1{e \u2208 E true} log(1 \u2212 pfake),\n\n(5)\n\nThe edge loss is the weighted sum of the generator\u2019s and the discriminator\u2019s loss g, \u03b8e\n\nLe(\u03b8e where \u03bb is a hyper-parameter. Note that structures of the generator f e d are flexible, e.g., they can be graph convolutional networks (GCN) or graph attention networks (GAT).\n\ng and the discriminator f e\n\ng) + \u03bbLe\n\nd) = Le\n\nd(\u03b8e\n\ng(\u03b8e\n\nd),\n\n(6)\n\n3.2 FEATURE GENERATION AND DISCRIMINATION\n\nIn real-world applications, nodes are often associated with features. For example, in the Reddit dataset (Hamilton et al., 2017), a node\u2019s feature is a vectorized representation of the post corresponding to the node. As another example, in citation networks (Tang et al., 2008), a paper\u2019s title\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ncan be treated as a node\u2019s feature. Previous work (Hu et al., 2020b) has demonstrated that generating features and edges simultaneously can improve the GNN\u2019s representation power.\n\nNode features can be either texts (e.g., in citation networks) or vectors (e.g., in recommendation systems). In this section, we develop feature generation and discrimination procedures for texts. Vector features are akin to encoded text features, and we can use linear layers to generate and discriminate them. Details about vector features are deferred to Appendix B.\n\ng (\u00b7, \u03b8f\n\ng ) = trmg \u25e6 embg(\u00b7) the generator parameterized by \u03b8f\n\nFor text features, we parameterize both the feature generator and the feature discriminator using bi-directional Transformer models (Vaswani et al., 2017), similar to BERT (Devlin et al., 2019). Denote f f g , where embg is the word embedding function and trmg denotes subsequent Transformer layers. For an input text feature x = [x1, \u00b7 \u00b7 \u00b7 , xL] where L is the sequence length, we randomly select indices to mask out, i.e., we randomly select an index set M \u2282 {1, \u00b7 \u00b7 \u00b7 , L}. For a masked position i \u2208 M, the prediction probability is given by\n\np(xi|x) =\n\nexp (cid:0)embg(xi)\u22a4vg(xi)(cid:1) x\u2032\u2208vocab exp (embg(x\u2032)\u22a4vg(x\u2032))\n\n(cid:80)\n\n, vg(xi) = trmg\n\n(cid:0)W proj\n\ng\n\n[hg(nx), embg(xi)](cid:1) . (7)\n\ng\n\nHere W proj is a trainable weight and hg(nx) is the representation of the node corresponding to x computed by the edge generation GNN. Note that we concatenate the text embedding embg(xi) and the feature node\u2019s embedding hg(nx), such that the feature generator can aggregate information from the graph structure. We train the generator by maximizing the probability of predicting the correct token, i.e., by minimizing the loss\n\nLf\n\ng (\u03b8e\n\ng, \u03b8f\n\ng ) = (cid:80)\n\nx\n\n(cid:80)\n\ni\u2208M \u2212 log p(xi|x).\n\n(8)\n\nAfter we train the generator, we use the trained model to predict all the masked tokens, after which we obtain a new text feature xcorr. Here, we set xcorr i = (cid:98)xi for i \u2208 M, where (cid:98)xi = argmaxx\u2032\u2208vocab p(xi|x) is the generator\u2019s prediction. The discriminator is trained to distinguish the fake tokens (i.e., wrongly generated tokens) from the true ones (i.e., the unmasked and correctly generated tokens) in xcorr. Similar to the generator, we denote f f d . For each position i, the discriminator\u2019s prediction probability is defined as\n\nd ) = trmd \u25e6 embd(\u00b7) as the discriminator parameterized by \u03b8f\n\ni = xi for i /\u2208 M and xcorr\n\nd (\u00b7, \u03b8f\n\np(xcorr\n\ni = xi) = sigmoid (cid:0)w\u22a4vd(xcorr\n\ni\n\n)(cid:1) , vd(xcorr\n\ni\n\n) = trmd\n\n(cid:16)\n\nW proj\n\nd\n\n[hd(nx), embd(xcorr\n\ni\n\n(cid:17)\n\n)]\n\n.\n\n(9)\n\nHere w and W proj to x computed by the edge discriminator GNN. The training loss for the discriminator is\n\nare trainable weights and hd(nx) is the representation of the node corresponding\n\nd\n\nLf where ptrue = p(xcorr\n\nd, \u03b8f\n\nd ) = (cid:80) i = xi) and 1{\u00b7} is the indicator function.\n\ni=1 \u22121{xcorr\n\ni = xi} log(ptrue) \u2212 1{xcorr\n\nd (\u03b8e\n\n(cid:80)L\n\nx\n\ni\n\n\u0338= xi} log(1 \u2212 ptrue),\n\n(10)\n\nThe text feature loss is defined as\n\nLf (\u03b8e\n\ng, \u03b8f\n\ng , \u03b8e\n\nd, \u03b8f\n\nd ) = Lf\n\ng (\u03b8e\n\ng, \u03b8f\n\ng ) + \u03bbLf\n\nd (\u03b8e\n\nd, \u03b8f\n\nd ),\n\nwhere \u03bb is a hyper-parameter.\n\n3.3 MODEL TRAINING\n\nWe jointly minimize the edge loss and the feature loss, where the loss function is\n\nL(\u03b8e\n\ng, \u03b8f\n\ng , \u03b8e\n\nd, \u03b8f\n\nd ) = Le(\u03b8e = (cid:0)Le g(\u03b8e\n\ng, \u03b8e\n\nd) + Lf (\u03b8e\n\ng) + Lf\n\ng (\u03b8e\n\ng, \u03b8f\n\nd, \u03b8f d ) (cid:16)\n\ng , \u03b8e g, \u03b8f g )(cid:1) + \u03bb\n\nLe\n\nd(\u03b8e\n\nd) + Lf\n\nd (\u03b8e\n\nd, \u03b8f d )\n\n(11)\n\n(cid:17)\n\n.\n\n(12)\n\nHere, \u03bb is the weight of the discriminator\u2019s loss. We remark that our framework is flexible because the generator\u2019s loss (Le d ). As such, existing generative pre-training methods can be applied to train the generator. In DiP-GNN, the discriminator has a better quality than the generator because of the graph mismatch issue (see Section 4.5). Therefore, after pre-training, we discard the generator and fine-tune the discriminator on downstream tasks. A detailed training pipeline is presented in Appendix A.\n\ng ) is decoupled from the discriminator\u2019s (Le\n\nd and Lf\n\ng and Lf\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.4 COMPARISON WITH GAN\n\nWe remark that our framework is different from Generative Adversarial Nets (GAN, Goodfellow et al. 2014). In GAN, the generator-discriminator training framework is formulated as a min-max game, where the generator is trained adversarially to fool the discriminator. The two models are updated using alternating gradient descent/ascent.\n\nHowever, the min-max game formulation of GAN is not applicable to our framework. This is because in GNN pre-trianing, the generator generates discrete edges, unlike continuous pixel values in the image domain. Such a property prohibits back-propagation from the discriminator to the generator. Existing works (Wang et al., 2018) use reinforcement learning (specifically policy gradient) to circumvent the non-differentiability issue. However, reinforcement learning introduces extensive hyper-parameter tuning and suffers from scalability issues. For example, the largest graph used in Wang et al. 2018 only contains 18k nodes, whereas the smallest graph used in our experiments has about 233k nodes.\n\nAdditionally, the goal of GAN is to train good-quality generators, which is different from our focus. In our discriminative pre-training framework, we focus on the discriminator because of better graph alignments. In practice, we find that accuracy of the generator is already high even without the discriminator, e.g., the accuracy is higher than 40% with 255 negative samples. And we observe that further improving the generator does not benefit downstream tasks.\n\n4 EXPERIMENTS\n\nWe implement all the algorithms using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019). Experiments are conducted on NVIDIA A100 GPUs. By default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. We also discuss other choices in the experiments. Training and implementation details are deferred to Appendix C.\n\n4.1 SETTINGS AND DATASETS\n\n\u22c4 Settings. We consider a node transfer setting in the experiments. In practice we often work with a single large-scale graph, on which labels are sparse. In this case, we can use the large amount of unlabeled data as the pre-training dataset, and the rest are treated as labeled fine-tuning nodes. Correspondingly, edges between pre-training nodes are added to the pre-training data, and edges between fine-tuning nodes are added to the fine-tuning data. In this way, the model cannot see the fine-tuning data during pre-training, and vice versa.\n\nWe remark that our setting is different from conventional self-supervised learning settings, namely we pre-train and fine-tune on two separate graphs. This meets the practical need of transfer learning, e.g., a trained GNN needs to transfer across locales and time spans in recommendation systems.\n\n\u22c4 Homogeneous Graph. We use the Reddit dataset (Hamilton et al., 2017), which is a publicly available large-scale graph. In this graph, each node corresponds to a post, and is labeled with a \u201csubreddit\u201d. Each node has a 603-dimensional feature vector constructed from the corresponding post. Two nodes (posts) are connected if the same user commented on both. The dataset contains posts from 50 subreddits sampled from posts initiated in September 2014. In total, there are 232,965 posts with an average node degree of 492. We use 70% of the data as the pre-training data, and the rest as the fine-tuning data, which are further split into training, validation, and test sets equally. We consider node classification as the downstream fine-tuning task.\n\n\u22c4 Product Recommendation Graph. We collect in-house product recommendation data from an e-commerce website. We build a bi-partite graph with two node types: search queries and product ids. The dataset contains about 633k query nodes, 2.71M product nodes, and 228M edges. We sample 70% of the nodes (and corresponding edges) for pre-training, and the rest are evenly split for fine-tuning training, validation and testing. We consider link prediction as the downstream task, where for each validation and test query node, we randomly mask out 20% of its edges to recover. For each masked edge that corresponds to a query node and a positive product node, we randomly sample 255 negative products. The task is to find the positive product out of the total 256 products.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n\u22c4 Heterogeneous Graph. We use the OAG-CS dataset (Tang et al., 2008; Sinha et al., 2015), which is a publicly available heterogeneous graph containing computer science papers. The dataset contains over 1.1M nodes and 28.4M edges. In this graph, there are five node types (institute, author, venue, paper and field) and ten edge types. The \u201cfield\u201d nodes are further categorized into six levels from L0 to L5, which are organized using a hierarchical tree. Details are shown in Figure 2.\n\nWe use papers published before 2014 as the pre-training dataset (63%), papers published between 2014 (inclusive) and 2016 (inclusive) as the fine-tuning training set (20%), papers published in 2017 as the fine-tuning validation set (7%), and papers published after 2017 as the fine-tuning test set (10%). During fine-tuning, by default we only use 10% of the fine-tuning training data (i.e., 2% of the overall data) because in practice labeled data are often scarce. We consider three tasks for fine-tuning: author name disambiguation (AD), paper field classification (PF) and paper venue classification (PV). For paper field classification, we only consider L2 fields. processed graph from Hu et al. 2020b.\n\nFigure 2: Details of OAG-CS. There are 5 node types (in black) and 10 edge types (in red).\n\nIn the experiments, we use the pre-\n\n4.2\n\nIMPLEMENTATION DETAILS\n\n\u22c4 Graph subsampling. In practice, graphs are often too large to fit in the hardware, e.g., the Reddit dataset (Hamilton et al., 2017) contains over 230k nodes. Therefore, we sample a dense subgraph from the large-scale graph in each training iteration. For homogeneous graphs, we apply the LADIES algorithm (Zou et al., 2019), which theoretically guarantees that the sampled nodes are highly inter-connected with each other and can maximally preserve the graph structure. For heterogeneous graphs, we use the HGSampling algorithm (Hu et al., 2020b), which is a heterogeneous version of LADIES.\n\nIn the edge generator, for a masked edge (s, t), we fix \u22c4 Node sampling for the edge generator. the node t and seek to identify the other node s. One approach is to identify s from all the graph nodes, i.e., by setting C = N in Eq. 1. However, this task is computationally intractable when the number of nodes is large, i.e., the model needs to find s out of hundreds of thousands of nodes. i }nneg Therefore, we sample some negative nodes {sg i , t) /\u2208 E. Then, the candidate set to generate the source node becomes {s, sg } instead of all the graph nodes N . We remark nneg that such a sampling approach is standard for GNN pre-training and link prediction (Hamilton et al., 2017; Sun et al., 2020; Hu et al., 2020b).\n\ni=1 such that (sg\n\n1, \u00b7 \u00b7 \u00b7 , sg\n\n\u22c4 Edge sampling for the edge discriminator. In computing the loss for the discriminator, the number of edges in Eu is significantly larger than those in Eg, i.e., we only mask a small proportion of the edges. To avoid the discriminator from outputting trivial predictions (i.e., all the edges belong to Eu), we balance the two loss terms in Le u| = \u03b1|Eg|, where \u03b1 is a hyper-parameter. Then, we compute Le u. Note that the node representations hd are still computed using all the generated and unmasked edges Eg and Eu.\n\nd. Specifically, we sample E d\n\nu \u2282 Eu such that |E d\n\nd on Eg and E d\n\n4.3 BASELINES\n\nWe compare our method with several baselines in the experiments. For fair comparison, all the methods are trained for the same number of GPU hours.\n\n\u22c4 GAE (Graph Auto-Encoder, Kipf & Welling 2016) adopts an auto-encoder for unsupervised learning on graphs. In GAE, node embeddings are learnt using a GNN, and we minimize the discrepancy between the original and the reconstructed adjacency matrix.\n\n\u22c4 GraphSAGE (Hamilton et al., 2017) encourages embeddings of neighboring nodes to be similar. For each node, the method learns a function that generates embeddings by sampling and aggregating features from the node\u2019s neighbors.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Experimental results on homogeneous graphs. We report F1 averaged over 10 runs for the Reddit data and MRR over 10 runs for the product recommendation data. The best results are shown in bold.\n\nTable 2: Experimental results on OAG-CS (heterogeneous). Left to right: paper-field, paper-venue, author-name-disambiguation. We report MRR over 10 runs. The best results are shown in bold.\n\nReddit Recomm.\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\n87.3\n\n88.5 88.0 87.7 89.6 89.0 88.6 89.1\n\n90.7\n\n46.3\n\n56.7 53.0 53.3 58.6 51.5 \u2014\n\u2014\n\n60.1\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\nPF\n\n32.7\n\n40.3 37.8 38.1 41.6 38.0 38.0 38.6\n\n44.1\n\nPV\n\n19.6\n\n24.5 22.1 22.5 25.6 21.5 22.0 23.5\n\n27.7\n\nAD\n\n60.0\n\n62.5 62.9 63.0 63.1 62.0 61.5 62.8\n\n65.6\n\n(a) Author name disambiguation.\n\n(b) Paper field classification.\n\n(c) Paper venue classification.\n\nFigure 3: Model performance vs. amount of labeled data on OAG-CS.\n\n\u22c4 DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes mutual information between node representations and corresponding high-level summaries of graphs. Thus, a node\u2019s embedding summarizes a sub-graph centered around it.\n\n\u22c4 GPT-GNN (Hu et al., 2020b) adopts a generative pre-training objective. The method generates edges by minimizing a link prediction objective, and incorporates node features in the framework.\n\n\u22c4 GRACE (Graph Contrastive Representation, Zhu et al. 2020) leverages a contrastive objective. The algorithm generates two views of the same graph through node and feature corruption, and then maximize agreement of node representations in the two views.\n\n\u22c4 GraphCL (You et al., 2020) is another graph contrastive learning approach that adopts node and edge augmentation techniques, such as node dropping and edge perturbation.\n\n\u22c4 JOAO (Joint Augmentation Optimization, You et al. 2021) improves GraphCL by deigning a bi-level optimization objective to automatically and dynamically selects augmentation methods.\n\n4.4 MAIN RESULTS\n\nIn Table 1 and Table 2, w/o pre-train means direct training on the fine-tuning dataset without pretraining. Results on the Reddit dataset are F1 scores averaged over 10 runs, and results on the product recommendation graph are MRR scores averaged over 10 runs. All the performance gain have passed a hypothesis test with p-value < 0.05.\n\nTable 1 summarizes experimental results on the homogeneous graphs: Reddit and Recommendation. We see that pre-training indeed benefits downstream tasks. For example, performance of GNN improves by at \u2265 0.4 F1 on Reddit (DGI) and \u2265 5.2 MRR on Recommendation (GRACE). Also, notice that among the baselines, generative approaches (GAE and GPT-GNN) yield promising per-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Neg. nodes for generation.\n\n(b) Pos. edges for discrimination.\n\n(c) Weight of discriminator\u2019s loss.\n\nFigure 4: Ablation experiments on Reddit. By default, we set the number of negative nodes to 256, the factor of positive edges to 1.0, and weight of the discriminator\u2019s loss to 20.\n\nTable 3: Test F1 score of model variants on Reddit.\n\nTable 4: Test F1 of models with different backbone graph neural networks on Reddit.\n\nModel\n\nEdges+Features\n\nEdges Features RandomEdges\n\nF1\n\n90.7\n\n90.4 90.2 89.8\n\nModel\n\nHGT GAT\n\nw/o pretrain\n\n87.3\n\n86.4\n\nGPT-GNN DiP-GNN\n\n89.6 90.7\n\n87.5 88.5\n\nFigure 5: F1 vs. proportion of manipulated edges on Reddit.\n\nformance. On the other hand, the contrastive method (GRACE, GraphCL and JOAO) does not scale well to large graphs, e.g., the OAG-CS graph which contains 1.1M nodes and 28.4M edges. By using the proposed discriminative pre-training framework, our method significantly outperforms all the baseline approaches. For example, DiP-GNN outperforms GPT-GNN by 1.1 on Reddit and 1.5 on Recommendation.\n\nExperimental results on the heterogeneous OAG-CS dataset are summarized in Table 2. Similar to the homogeneous graphs, notice that pre-training improves model performance by large margins. For example, pre-training improves MRR by at least 5.1, 2.5 and 2.5 on the PF, PV and AD tasks, respectively. Moreover, by using the proposed training framework, models can learn better node embeddings and yield consistently better performance compared with all the baselines.\n\nRecall that during fine-tuning on OAG-CS, we only use 10% of the labeled fine-tuning data (about 2% of the overall data). In Figure 3, we examine the effect of the amount of labeled data. We see that model performance improves when we increase the amount of labeled data. Also, notice that DiP-GNN consistently outperforms GPT-GNN in all the three tasks under all the settings.\n\n4.5 ANALYSIS\n\n\u22c4 Comparison with semi-supervised learning. We compare DiP-GNN with a semi-supervised learning method: C&S (Correct&Smooth, Huang et al. 2020). Figure 6 summarizes the results. We see that C&S yields a 0.5 improvement compared with the supervised learning method (i.e., w/o pre-train). However, performance of C&S is significantly lower than both DiP-GNN and other pre-training methods such as GPT-GNN.\n\n\u22c4 Hyper-parameters. There are several hyper-parameters that we introduce in DiP-GNN: the number of negative nodes that are sampled for generating edges (Section 4.2); the number of positive edges that are sampled for the discriminator\u2019s task (Section 4.2); and the weight of the discriminator\u2019s loss (Eq. 12). Figure 4 illustrate ablation experimental results on the Reddit dataset. From the results, we see that DiP-GNN is robust to these hyper-parameters. We remark that under all the settings, ours model behaves better than the best-performing baseline (89.6 for GPT-GNN).\n\nFigure 6: Test F1 on Reddit.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Generator and discriminator performance vs. proportion of masked edges during pre-training. Coverage is the proportion of true edges input to the models.\n\nMasked%\n\nAcc\n\nCoverage\n\nGen. Dis. Gen. Dis.\n\nRatio\n\n20 80 95\n\n0.50 0.33 0.20\n\n0.87 0.84 0.80\n\n0.80 0.20 0.05\n\n0.90 \u00d71.13 0.46 \u00d72.30 0.24 \u00d74.80\n\nFigure 7: Performance vs. proportion of masked edges on product recommendation.\n\n\u22c4 Model variants. We also examine variants of DiP-GNN. Recall that the generator and the discriminator operate on both edges and node features. We first check the contribution of these two factors. We also investigate the scenario where edges are randomly generated, and the discriminator still seeks to find the generated edges. Table 3 summarizes results on the Reddit dataset.\n\nWe see that by only using edges, model performance drops by 0.3; and by only using node features, performance drops by 0.5. This indicates that the graph structure plays a more important role in the proposed framework than the features. Also notice that performance of RandomEdges is unsatisfactory. This is because implausible edges are generated when using a random generator, making the discriminator\u2019s task significantly easier. We remark that performance of all the model variants is better than the best-performing baseline (89.6 for GPT-GNN).\n\nTable 4 examines performance of our method and GPT-GNN using different backbone GNNs. Recall that by default, we use HGT (Hu et al., 2020c) as the backbone. We see that when GAT (Velickovic et al., 2018) is used, performance of DiP-GNN is still significantly better than GPT-GNN.\n\n\u22c4 Missing edges hurt more than wrong edges. In our pre-training framework, the generator is trained to reconstruct the masked graph, after which the reconstructed graph is fed to the discriminator. During this procedure, the graph input to the generator has missing edges, and the graph input to the discriminator has wrong edges. From Figure 5, we see that wrong edges hurt less than missing ones. For example, model performance drops by 0.7% when 50% of wrong edges are added to the original graph, and performance decreases by 1.8% when 50% of original edges are missing. This indicates that performance relies on the amount of original edges seen by the models. Intuitively, wrong edges add noise to the graph, but they do not affect information flow. On the contrary, missing edges cut information flow. Moreover, in practice we work with graph attention models, and the attention mechanism can alleviate the wrong edges by assigning low attention scores to them.\n\n\u22c4 Why is discriminative pre-training better? Figure 7 illustrates effect of the proportion of masked edges during pre-training. We see that when we increase the proportion from 0.2 to 0.8, performance of GPT-GNN drops by 6.1, whereas performance of DiP-GNN only drops by 3.3. This indicates that the generative pre-training method is more sensitive to the masking proportion.\n\nTable 5 summarizes pre-training quality. First, the generative task (i.e., the generator) is more difficult than the discriminative task (i.e., the discriminator). For example, when we increase the proportion of masked edges from 20% to 80%, accuracy of the generator drops by 17% while accuracy of the discriminator only decreases by 3%. Second, the graph input to the discriminator better aligns with the original graph. For example, when 80% of the edges are masked, the discriminator sees 2.3 times more original edges than the generator. Therefore, the discriminative task is more advantageous because model quality relies on the number of observed original edges (Figure 5).\n\n5 CONCLUSION AND DISCUSSIONS\n\nWe propose Discriminative Pre-Training of Graph Neural Networks (DiP-GNN), where we simultaneously train a generator and a discriminator. During pre-training, we mask out some edges in the graph, and a generator is trained to recover the masked edges. Subsequently, a discriminator seeks to distinguish the generated edges from the original ones. Our framework is more advantageous than generative pre-training for two reasons: the graph inputted to the discriminator better matches the original graph; and the discriminative pre-training task better aligns with downstream node classification. We conduct extensive experiments to validate the effectiveness of DiP-GNN.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nMicha \u0308el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 510, 2016, Barcelona, Spain, pp. 3837\u20133845, 2016.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n\nYuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pp. 135\u2013144. ACM, 2017. doi: 10.1145/3097983.3098036.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672\u20132680, 2014.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.\n\nIn Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 855\u2013864. ACM, 2016. doi: 10.1145/2939672.2939754.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024\u20131034, 2017.\n\nKaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 4116\u20134126. PMLR, 2020.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure In 8th International Conference Leskovec. Strategies for pre-training graph neural networks. on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020a.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: generative pre-training of graph neural networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1857\u20131867. ACM, 2020b.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen (eds.), WWW \u201920: The Web\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nConference 2020, Taipei, Taiwan, April 20-24, 2020, pp. 2704\u20132710. ACM / IW3C2, 2020c. doi: 10.1145/3366423.3380027.\n\nQian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining arXiv preprint\n\nlabel propagation and simple models out-performs graph neural networks. arXiv:2010.13993, 2020.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K \u0308opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch \u0301e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024\u20138035, 2019.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In Sofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani (eds.), The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, New York, NY, USA - August 24 - 27, 2014, pp. 701\u2013710. ACM, 2014. doi: 10.1145/2623330.2623732.\n\nJiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Yi Chang, Chengxiang Zhai, Yan Liu, and Yoelle Maarek (eds.), Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, pp. 459\u2013467. ACM, 2018. doi: 10.1145/3159652.3159706.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1150\u20131160. ACM, 2020.\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593\u2013607. Springer, 2018.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pp. 243\u2013246, 2015.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale information network embedding. In Aldo Gangemi, Stefano Leonardi, and Alessandro Panconesi (eds.), Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015, pp. 1067\u20131077. ACM, 2015. doi: 10.1145/2736277.2741093.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and In Proceedings of the 14th ACM SIGKDD international\n\nmining of academic social networks. conference on Knowledge discovery and data mining, pp. 990\u2013998, 2008.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998\u20136008, 2017.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nPetar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nHongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2508\u20132515. AAAI Press, 2018.\n\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S. Yu. Heterogeneous graph attention network. In Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia (eds.), The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pp. 2022\u20132032. ACM, 2019. doi: 10.1145/ 3308558.3313562.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Yike Guo and Faisal Farooq (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pp. 974\u2013983. ACM, 2018. doi: 10.1145/3219819.3219890.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812\u20135823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. In International Conference on Machine Learning, pp. 12121\u201312132. PMLR, 2021.\n\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. Heterogeneous graph neural network. In Ankur Teredesai, Vipin Kumar, Ying Li, R \u0301omer Rosales, Evimaria Terzi, and George Karypis (eds.), Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 793\u2013803. ACM, 2019. doi: 10.1145/3292500.3330961.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069\u20132080, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDifan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch \u0301e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11247\u201311256, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA DETAILED ALGORITHM\n\nAlgorithm 1 is a detailed training pipeline of DiP-GNN. For graphs with vector features instead of text features, we can substitute the feature generation and discrimination modules with equations in Appendix B.\n\nAlgorithm 1: DiP-GNN: Discriminative Pre-training of Graph Neural Networks. Input: Graph Gfull; edge masking ratio; feature masking ratio; number of negative samples for edge generator; proportion of positive samples for edge discriminator \u03b1; weight of the discriminator\u2019s loss \u03bb; number of training steps T .\n\nfor t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do\n\n// Graph subsampling. Sample a subgraph G = (N , E) from Gfull; // Edge generation. Initialize the generated edge set Eg = {} and the edge generation loss Le Construct the unmasked set of edges Eu and the masked set Em such that E = Eu \u222a Em; Compute node embeddings using Eu; for e = (n1, n2) \u2208 Em do\n\ng = 0;\n\nConstruct candidate set C for n1 (n2 is given during generation) via negative sampling; Generate (cid:98)e = ((cid:98)n1, n2) where (cid:98)n1 \u2208 C; Update the generated edge set Eg \u2190 Eg \u222a {(cid:98)e}; Update the edge generation loss Le g; // Text Feature generation. Initialize the feature generation loss Lf for n \u2208 N do\n\ng = 0;\n\nFor the node\u2019s text feature xn, mask out some of its tokens; Construct the generated text feature xcorr\n\nn\n\nusing the embedding of node n (computed\n\nduring edge generation) and the feature generation Transformer model; Update the feature generation loss Lf g ;\n\n// Edge discrimination. Initialize the edge discrimination loss Le Compute node embeddings using Eg \u222a Eu; Sample E d for e = (n1, n2) \u2208 Eg \u222a E d\n\nu \u2282 Eu such that |E d u do\n\nu| = \u03b1|Eg|;\n\nd = 0;\n\nDetermine if e is generated using the embedding of n1 and n2; Update the edge discrimination loss Le d; // Text feature discrimination. Initialize the feature discrimination loss Lf for n \u2208 N do\n\nd = 0;\n\nFor the node\u2019s generated text feature xcorr\n\nn , determine whether each token is generated\n\nusing the embedding of node n (computed during edge discrimination) and the feature discrimination Transformer model; Update the feature discrimination loss Lf d ;\n\n// Model updates. g + Lf Compute L = (Le\n\ng ) + \u03bb(Le\n\nd + Lf\n\nd ) and update the model;\n\nOutput: Trained discriminator ready for fine-tuning.\n\nB GENERATION AND DISCRIMINATION OF VECTOR FEATURES\n\nNode features can be vectors instead of texts, e.g., the feature vector can contain topological information such as connectivity information. In this case both the generator and the discriminator are parameterized by a linear layer.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTo generate feature vectors, we first randomly select some nodes Ng \u2282 N . For a node n \u2208 N , denote its feature vector vn, then the feature generation loss is\n\nLf\n\ng (Wg) =\n\n(cid:88)\n\nn\u2208Ng\n\n||(cid:98)vn \u2212 vn||2\n\n2 , where (cid:98)vn = W f\n\ng hg(n).\n\nHere hg(n) is the representation of node n and W f construct its corred feature vcorr\n\nn = (cid:98)vn if n \u2208 Ng and vcorr\n\ng is a trainable weight. For a node n \u2208 N , we\n\nn = vn if n \u2208 N \\ Ng.\n\nThe discriminator\u2019s goal is to differentiate the generated features from the original ones. Specifically, the prediction probability is\n\np(n \u2208 Ng) = sigmoid (cid:0)W d\n\nd hd(n)(cid:1) ,\n\nwhere W f on the corred feature vcorr\n\nd is a trainable weight. We remark that the node representation hd(n) is computed based\n\nn . Correspondingly, the discriminator\u2019s loss is\n\nLf\n\nd (Wd) =\n\n(cid:88)\n\nn\u2208N\n\n\u22121{n \u2208 Ng} log p(n \u2208 Ng) \u2212 1{n \u2208 N \\ Ng} log(1 \u2212 p(n \u2208 Ng)).\n\nThe vector feature loss Lf (\u03b8e the text feature loss.\n\ng, W f\n\ng , \u03b8e\n\nd, W f\n\nd ) = Lf\n\ng (\u03b8e\n\ng, W f\n\ng ) + Lf\n\nd (\u03b8e\n\nd, W f\n\nd ) is computed similar to\n\nC IMPLEMENTATION AND TRAINING DETAILS\n\nBy default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. In the experiments, the edge generator and discriminator have the same architecture, where we set the hidden dimension to 400, the number of layers to 3, and the number of attention heads to 8. For the OAG dataset which contains text features, the feature generator and discriminator employs the same architecture: a 4 layer bi-directional Transformer model, similar to BERT (Devlin et al., 2019), where we set the embedding dimension to 128 and the hidden dimension of the feed-forward neural network to 512.\n\nFor pre-training, we mask out 20% of the edges and 20% of the features (for text features we mask out 20% of the tokens). We use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set \u03b2 = (0.9, 0.999), \u03b5 = 10\u22128, the learning rate to 0.001 and the weight decay to 0.01. We adopt a dropout ratio of 0.2 and gradient norm clipping of 0.5. For graph subsampling, we set the depth to 6 and width to 128, the same setting as Hu et al. 2020b.\n\nFor fine-tuning, we use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set \u03b2 = (0.9, 0.999), \u03b5 = 10\u22126, and we do not use weight decay. We use the same graph subsampling setting as pre-training. The other hyper-parameters are detailed in Table 6.\n\nTable 6: Hyper-parameters for fine-tuning tasks.\n\nDataset\n\nTask\n\nSteps Dropout Learning rate Gradient clipping\n\nReddit\n\n\u2014 2400\n\nRecomm. \u2014 1600\n\nOAG-CS\n\nPF PV AD\n\n1600 1600 1600\n\n0.3\n\n0.1\n\n0.2 0.2 0.2\n\n0.0015\n\n0.0010\n\n0.0010 0.0005 0.0005\n\n0.5\n\n0.5\n\n0.5 0.5 0.5\n\n15",
      "responses": [
        null,
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel approach for pre-training Graph Neural Networks (GNNs) on large-scale unlabeled graphs, addressing the graph mismatch issue prevalent in existing generative pre-training methods. DiP-GNN employs a dual framework consisting of a generator that predicts masked edges and a discriminator that distinguishes between original and generated edges. The methodology involves joint training of these components, which leads to improved alignment with the original graph structure. Experimental results demonstrate that DiP-GNN significantly outperforms state-of-the-art methods on various datasets, including homogeneous and heterogeneous graphs, showcasing its effectiveness in enhancing downstream task performance.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of a critical issue\u2014graph mismatch\u2014in generative pre-training and the innovative use of a concurrent generator-discriminator framework to address this. The extensive experimental validation across different datasets and tasks further supports the claims of improved performance. However, a potential weakness could be the reliance on specific hyper-parameter settings for negative sampling and discriminator weighting, which may affect the generalizability of the approach. Additionally, while the method shows substantial improvements, the paper could benefit from a deeper analysis of the limitations and potential failure cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The exposition is coherent, making it easy for readers to follow the rationale behind DiP-GNN. The novelty lies in its approach to address the graph mismatch issue through a dual training strategy, which is a significant advancement in the field of GNN pre-training. The reproducibility of the results is supported by the use of standard datasets and the implementation details provided, including the use of PyTorch and PyTorch Geometric.\n\n# Summary Of The Review\nDiP-GNN offers a substantial contribution to the field of GNNs by effectively tackling the graph mismatch problem through a dual-generator and discriminator framework, leading to improved downstream task performance. While the paper is well-presented and demonstrates solid empirical results, further exploration of its limitations would strengthen the overall narrative.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel framework designed to enhance the performance of Graph Neural Networks (GNNs) through a dual pre-training strategy that incorporates both a generator and a discriminator. The methodology addresses the problem of \"graph mismatch\" often encountered in generative pre-training approaches, where the masked input graph diverges from the original graph, resulting in suboptimal node embeddings. DiP-GNN employs a generator that predicts masked edges based on unmasked edges, while a discriminator evaluates the authenticity of both true and generated edges. Experimental results demonstrate significant improvements in performance across various datasets, with notable increases in F1 and MRR scores compared to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the challenges of generative pre-training in GNNs, particularly the graph mismatch issue. The empirical results are robust, showcasing substantial performance gains on large-scale datasets and against multiple baselines, which strengthens the validity of the proposed framework. Additionally, the comprehensive experimental design, including various datasets and ablation studies, offers insights into the model's effectiveness and the importance of both edge and feature components. However, the complexity of implementing a dual-model structure may pose challenges for practitioners, and the reliance on data quality raises concerns about the method's generalizability across different tasks and graph types. Furthermore, further research is needed to assess the framework's performance in diverse scenarios beyond the tested datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making the methodology and experimental design clear. The explanation of the generator and discriminator roles is thorough, and the results are presented in a way that effectively conveys the significance of the findings. The novelty of the approach is evident, particularly in its attempt to mitigate graph mismatch through a discriminative framework. However, the complexity of the model may hinder reproducibility, as detailed hyper-parameter tuning and implementation specifics are crucial for achieving the reported results.\n\n# Summary Of The Review\nDiP-GNN presents a compelling advancement in GNN pre-training by addressing the graph mismatch issue with a discriminative approach, leading to significant performance improvements. While the methodology is innovative and the empirical results are strong, the complexity of implementation and dependence on data quality present challenges for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel approach for pre-training Graph Neural Networks (GNNs) using a discriminative method to mitigate the graph mismatch problem commonly encountered in generative pre-training techniques. The methodology involves training a generator to recover masked edges and a discriminator to differentiate between generated and original edges, thereby enhancing the alignment with the original graph structure. The experimental results demonstrate that DiP-GNN significantly outperforms existing generative pre-training methods across various downstream tasks, showcasing its effectiveness in preserving graph topology and improving performance.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of a significant problem in existing pre-training strategies for GNNs, and the proposed solution, DiP-GNN, which effectively addresses this issue. The experimental results are robust, showing statistically significant improvements across multiple datasets, which adds credibility to the claims. However, the paper could improve in its exploration of the computational efficiency and scalability of the proposed method, particularly in large-scale graphs. Additionally, while the distinction from GANs is noted, a deeper theoretical analysis could enhance understanding of the method's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, providing a clear introduction to the problem, methodology, and results. The mathematical formulations are presented clearly, aiding in comprehension. The novelty of the approach lies in the discriminative pre-training strategy, which is a fresh perspective in the context of GNNs. The reproducibility is supported by detailed descriptions of the experimental setup and datasets used; however, sharing code or additional implementation details would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of GNN pre-training by introducing a discriminative approach that effectively addresses existing limitations. The methodology is sound, and the results are compelling, although further exploration of efficiency and scalability would strengthen the contribution. \n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the DiP-GNN framework, an innovative approach to graph neural network (GNN) pre-training that integrates a generator and a discriminator to enhance the alignment of graph inputs with their original structures. The methodology involves addressing the graph mismatch problem common in generative pre-training methods, leading to improved performance in various downstream tasks. Extensive experimental validation demonstrates that DiP-GNN significantly outperforms existing methods on large-scale homogeneous and heterogeneous graphs, showcasing its effectiveness in applications such as node classification and link prediction.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Framework:** The integration of a generator and discriminator presents a novel approach that enhances input alignment in GNNs.\n2. **Addressing Graph Mismatch:** The framework effectively tackles the graph mismatch problem, resulting in improved performance.\n3. **Extensive Experimental Validation:** Comprehensive experiments validate the framework's effectiveness across multiple datasets.\n4. **Robustness to Hyper-parameters:** The framework demonstrates robustness with hyper-parameter variations, suggesting practical applicability.\n5. **Performance Metrics:** The improvements in metrics like F1 score and MRR score underline the practical benefits for downstream tasks.\n6. **Public Availability of Code:** The authors' commitment to releasing code enhances reproducibility and encourages collaboration.\n7. **Connection to Existing Literature:** The paper situates its contributions within the broader context of GNN research, enriching academic discourse.\n\n**Limitations:**\n1. **Complexity of Implementation:** The dual model approach may complicate implementation, making it less accessible.\n2. **Generalizability:** The framework's performance across various types of graphs and tasks requires further testing for generalizability.\n3. **Limited Baseline Comparison:** Inclusion of additional state-of-the-art methods could provide a more comprehensive performance assessment.\n4. **Hyper-parameter Sensitivity:** Despite robustness, careful tuning may still be necessary, posing challenges for real-world deployment.\n5. **Lack of Insight into Interpretability:** The paper does not adequately address the interpretability of learned embeddings or the discriminator's decision-making.\n6. **Potential for Overfitting:** The focus on high performance through extensive pre-training raises concerns about overfitting, especially on smaller datasets.\n7. **Depth of Literature Review:** A more thorough discussion of existing methods' limitations could further emphasize the significance of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, effectively conveying the methodology and findings. The quality of the experiments is commendable, and the novelty of combining generator-discriminator architecture in GNN pre-training is significant. The public availability of the code enhances reproducibility, although the complexity of the proposed framework may hinder some practitioners from effectively implementing it.\n\n# Summary Of The Review\nOverall, the DiP-GNN framework presents a noteworthy advancement in GNN pre-training by addressing graph mismatch and demonstrating strong empirical results. However, challenges related to implementation complexity and generalizability warrant further exploration to enhance its applicability across diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel approach aimed at enhancing the pre-training of Graph Neural Networks (GNNs). Unlike traditional generative pre-training methods, DiP-GNN employs a dual-framework that includes both a generator and a discriminator to tackle graph mismatch challenges. The main contributions include a discriminative training approach that improves the alignment of learned embeddings with the original graph structure, and extensive empirical validation demonstrating superior performance on a variety of downstream tasks across diverse graph datasets.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative integration of a discriminator within the GNN pre-training framework and the comprehensive empirical analysis that demonstrates significant performance improvements over existing methods. The clear presentation of methodology and experimental results enhances reproducibility. However, the paper lacks thorough theoretical justification for the advantages of the discriminative approach over purely generative methods. Additionally, the complexity introduced by the dual-framework could pose challenges in practical applications, particularly concerning training and hyperparameter tuning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and experimental results, contributing to a high level of clarity and quality. The novelty of integrating a discriminator in GNN pre-training is significant, setting it apart from traditional approaches. The empirical results are robust, with clear statistical significance backing the claims, which supports reproducibility. However, the lack of theoretical insight into the discriminator's effectiveness may hinder a complete understanding of its impact.\n\n# Summary Of The Review\nIn summary, DiP-GNN offers a meaningful advancement in GNN pre-training through its discriminative approach, which enhances performance on downstream tasks. The paper is well-executed with strong empirical evidence, although it could benefit from more theoretical grounding and consideration of practical implementation challenges.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel framework designed to enhance the robustness of graph neural networks (GNNs) against adversarial attacks. The methodology employs a dual generator-discriminator architecture where the generator creates adversarial examples by perturbing graph edges, while the discriminator learns to differentiate between original and adversarial edges. The findings reveal that DiP-GNN significantly outperforms traditional adversarial training methods across various datasets, leading to improved node embeddings and performance on downstream tasks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating adversarial training within GNN frameworks, which fills a notable gap in the literature. The empirical validation across diverse datasets bolsters the credibility of the results, demonstrating that the proposed method enhances GNN resilience. However, potential weaknesses include the complexity introduced by the generator-discriminator setup, which may complicate hyperparameter tuning and limit practical applicability. Additionally, the paper does not thoroughly evaluate the method's performance on larger or real-world graphs, which may affect its scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of writing is high, making it accessible to readers. The novelty of the dual adversarial training framework is notable, as it offers a fresh perspective on improving GNN robustness. However, reproducibility could be a concern due to the complexity of the model and potential challenges in hyperparameter optimization.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of graph neural networks through the DiP-GNN framework, effectively addressing adversarial robustness. While the contributions are valuable and empirically supported, the complexity and scalability issues suggest areas for further improvement and exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for the pre-training of Graph Neural Networks (GNNs), claiming to address the longstanding issue of graph mismatch. The methodology involves a dual model comprising a generator and a discriminator, built on concepts akin to Generative Adversarial Networks (GANs). The authors present experimental results claiming significant improvements in performance metrics, such as a 5.0 increase in F1 score on the Reddit dataset, while asserting that DiP-GNN surpasses all existing methods in the field.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its ambition to revolutionize GNN pre-training and the introduction of a dual model approach. However, the weaknesses are pronounced: the claims of eliminating graph mismatch seem exaggerated, and the empirical results do not convincingly support the transformative assertions made by the authors. There is a tendency to dismiss previous work without adequate justification, leading to a narrative that is both overly ambitious and potentially misleading.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity suffers due to its grandiose claims and lack of nuanced discussion about the limitations of the proposed method. While the novelty of the dual model approach is acknowledged, it heavily relies on existing concepts without sufficient differentiation. The reproducibility of results may be challenging, as the empirical evidence presented does not robustly validate the claims made, and the methodology lacks detailed exposition that would facilitate replication.\n\n# Summary Of The Review\nOverall, while DiP-GNN presents an interesting approach to GNN pre-training, the paper's claims of groundbreaking contributions and complete solutions to existing problems are overstated. The empirical evidence does not sufficiently back the assertions made, leading to a perception of inflated significance.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for enhancing Graph Neural Networks (GNNs) by addressing graph mismatch problems during pre-training. The methodology involves a generator that predicts masked edges and a discriminator that distinguishes between original and generated edges, thereby improving edge recovery. The experimental results demonstrate significant improvements in performance across several datasets, with notable gains in F1 scores and Mean Reciprocal Rank (MRR) when compared to existing methods, confirming the effectiveness of the proposed approach.\n\n# Strength And Weaknesses\nThe primary strength of DiP-GNN lies in its innovative approach to pre-training GNNs, effectively mitigating the issues associated with graph mismatch. The simultaneous training of a generator and a discriminator is a compelling strategy that enhances edge recovery and overall model performance. However, the paper could benefit from a more detailed discussion of the limitations of the methodology, particularly regarding the trade-offs involved in increasing the number of masked edges and the potential impact on the accuracy of the model. Additionally, while the empirical results are promising, further validation across more diverse datasets would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers with a foundational understanding of GNNs. The quality of the experimental analysis appears robust, with suitable metrics employed to evaluate performance. However, details on the implementation specifics and hyperparameter tuning are somewhat lacking, which may hinder reproducibility. The novel integration of generative and discriminative components adds a unique angle, although the paper should clarify how this approach compares with other recent methodologies in the literature.\n\n# Summary Of The Review\nOverall, DiP-GNN presents a significant advancement in the pre-training of GNNs by effectively addressing graph mismatch through a dual-component framework. While the results are promising and the methodology is sound, the paper could enhance its reproducibility and address potential limitations more thoroughly.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThis paper presents a novel framework for learning graph embeddings through a masking and edge generation technique. The authors propose a discriminative approach that aims to enhance the alignment of learned representations with the original graph structure. The methodology involves masking a significant portion of edges and training a model to predict these masked edges based on the remaining structure. Empirical results are provided, demonstrating improved performance on various downstream tasks, although the methodology and assumptions raise several concerns.\n\n# Strength And Weaknesses\nThe paper's primary contribution lies in its approach to embedding generation through edge masking, which showcases an innovative method for representation learning in graphs. However, several weaknesses undermine its claims. The assumption of graph structure stability may not hold in dynamic contexts, and the reliance on edge independence overlooks potential dependencies in real-world graphs. Furthermore, the effectiveness of the masking strategy and the generalizability of results across diverse graph types are questionable. The paper's focus on F1 score and MRR as performance metrics limits the evaluation scope and does not provide a comprehensive assessment of the model's efficacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written and clear, the clarity is diminished by the numerous assumptions made without sufficient empirical backing. The novelty is present in the proposed methodology, yet it does not adequately address the limitations or potential biases arising from its assumptions. Reproducibility may be a concern due to the reliance on specific hyperparameters and the potential sensitivity of the model performance to these parameters across different datasets.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to learning graph embeddings but is hindered by several critical assumptions that may limit its applicability and generalizability. The methodology requires more empirical validation, and the evaluation metrics could be expanded for a more nuanced understanding of performance.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel framework designed to improve the generative pre-training of Graph Neural Networks (GNNs) by addressing the issue of graph mismatch. The methodology incorporates a dual-component system where a generator predicts masked edges and a discriminator assesses the authenticity of these generated edges. The findings indicate that DiP-GNN significantly enhances the performance of GNNs, particularly in situations with limited labeled data, outperforming existing baseline methods across various datasets, including Reddit and OAG-CS.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to tackling the limitations of generative pre-training in GNNs through a discriminative learning framework, which is well-justified and supported by empirical evidence. The experiments are robust, showcasing improvements in downstream tasks and demonstrating resilience to hyper-parameter variations. However, the paper could benefit from a more comprehensive analysis of the computational efficiency and scalability of the proposed framework, as these are critical considerations for practical applications in large-scale graph settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and results, making it accessible to readers. The quality of the writing is high, and the arguments are logically presented. The novelty of the approach is significant, as it introduces a discriminative pre-training paradigm that contrasts with traditional generative methods. Reproducibility is encouraged through detailed descriptions of the experimental setup, though the paper could strengthen this aspect by providing access to the implementation or datasets used.\n\n# Summary Of The Review\nOverall, DiP-GNN represents a promising advancement in the field of GNNs by effectively addressing graph mismatch in generative pre-training. The proposed framework demonstrates significant empirical improvements and offers a novel perspective on representation learning in graphs, although further exploration of its scalability and efficiency is warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
        "# Summary Of The Paper\nThe paper presents a novel framework designed to enhance performance in supervised learning tasks within the realm of machine learning. The authors propose a method that integrates various existing strategies, aiming to address specific shortcomings observed in traditional approaches. Through comprehensive experiments on standard datasets, the findings indicate that the proposed method significantly outperforms baseline techniques, demonstrating its potential for broader applicability in real-world settings.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach and the empirical results that validate the proposed methodology. The integration of multiple techniques showcases a thoughtful synthesis of existing literature, which could lead to improved performance metrics. However, the weaknesses include a limited diversity in the datasets used for evaluation, which may affect the generalizability of the results. Additionally, while the authors acknowledge certain limitations, a more in-depth discussion regarding the practical implications and potential challenges of their method would enhance the paper's quality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a logical structure that effectively communicates the proposed framework. However, some sections could benefit from clearer explanations, particularly regarding the methodology and the rationale behind specific choices. The novelty of the work is evident, as it introduces a unique combination of strategies that have not been extensively explored in prior research. Nevertheless, the reproducibility of the proposed method could be improved by providing more detailed descriptions of the experimental setup and parameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in supervised learning with a novel framework that shows promise through empirical validation. While the contributions are significant, addressing the noted weaknesses regarding dataset diversity and clarity will enhance the paper's impact and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel pre-training method for Graph Neural Networks (GNNs) aimed at enhancing their performance by addressing the issue of graph mismatch that arises in traditional generative pre-training methods. DiP-GNN employs a dual training approach involving a generator that recovers masked edges and a discriminator that differentiates between generated and original edges. This methodology leads to improved alignment with the original graph and results in better-quality node embeddings. Extensive experiments demonstrate that DiP-GNN outperforms existing generative pre-training and self-supervised learning methods, achieving about 40% recovery of masked edges and significantly boosting performance on various downstream tasks.\n\n# Strength And Weaknesses\nThe main strengths of DiP-GNN lie in its innovative dual training mechanism, which effectively mitigates the graph mismatch problem prevalent in traditional methods. The empirical results are robust, showcasing significant performance improvements across multiple large-scale graphs. However, a potential weakness is the complexity of the proposed method compared to simpler generative approaches, which may limit its applicability in scenarios where computational resources are constrained. Additionally, the paper could benefit from a more detailed discussion on the scalability of the approach to very large graphs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers familiar with GNNs. The quality of the writing is high, with a logical flow that aids comprehension. The novelty of the discriminative pre-training approach is significant as it diverges from traditional methods and provides a new perspective on enhancing GNN performance. While the experimental setup appears thorough, additional details regarding hyperparameter tuning and specific implementation choices would enhance reproducibility.\n\n# Summary Of The Review\nOverall, DiP-GNN represents a meaningful advancement in the pre-training of Graph Neural Networks by effectively addressing the problem of graph mismatch through a dual training strategy. The empirical results substantiate its claims of improved performance over existing methods, although the complexity of the approach may pose challenges for broader implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel framework for enhancing Graph Neural Networks (GNNs) through discriminative pre-training. It addresses the limitations of existing generative pre-training methods, particularly the graph mismatch issue that adversely affects learned node embeddings. The methodology involves a generator-discriminator framework where the generator predicts masked edges and node features, while the discriminator assesses the authenticity of these predictions. The authors demonstrate the effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs, reporting significant performance improvements on downstream tasks, including F1 score and Mean Reciprocal Rank (MRR).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to pre-training GNNs, effectively tackling critical challenges in edge recovery and representation learning. The extensive experiments conducted on large datasets provide a solid empirical foundation for the proposed method, showcasing its robustness against hyper-parameter variations. However, a potential weakness is the lack of detailed theoretical analysis regarding the limitations of the proposed framework, which could further substantiate its efficacy. Additionally, while the experiments are comprehensive, testing on a broader variety of datasets and tasks would strengthen the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The writing quality is high, with a logical flow that aids understanding. The novel contribution of discriminative pre-training in GNNs is clearly highlighted, and the reproducibility is facilitated by the thorough description of the experiments and methodologies employed. However, the paper could benefit from additional discussions on implementation details and hyper-parameter tuning to enhance reproducibility further.\n\n# Summary Of The Review\nOverall, DiP-GNN presents a compelling advancement in the field of GNNs through its innovative use of discriminative pre-training. The empirical results demonstrate significant improvements over existing methods, although the paper could benefit from a more in-depth theoretical analysis and broader testing.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper titled \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\" presents a novel framework for pre-training graph neural networks (GNNs) that addresses the limitations of existing generative pre-training methods, particularly the issue of graph mismatch. The proposed DiP-GNN framework leverages edge generation and discrimination techniques to enhance feature representation for both text and vector inputs. Experimental results demonstrate that DiP-GNN significantly improves performance on downstream tasks across various datasets, including homogeneous (Reddit) and heterogeneous (OAG-CS) graphs, showcasing its effectiveness through metrics such as F1 score and Mean Reciprocal Rank (MRR).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to GNN pre-training, offering a solution to the identified problem of graph mismatch. The methodology is robust and well-detailed, with clear descriptions of the training process and experimental setup. Additionally, the comprehensive evaluation against established baselines adds credibility to the findings. However, the paper could benefit from more extensive discussions on the limitations of the approach, particularly in how it handles edge noise and the implications for real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with logical progression from the introduction through to the conclusion. The methodology is sound and detailed enough to allow for reproducibility of the experiments, which is essential for validation by the research community. The novelty of the approach is significant, as it introduces a discriminative pre-training paradigm that is not only conceptually distinct from generative methods but also empirically validated through rigorous testing.\n\n# Summary Of The Review\nOverall, the paper presents a convincing argument for the effectiveness of the DiP-GNN framework, illustrating its potential to improve GNN pre-training. The contributions are well-supported by empirical results and the methodology is clearly articulated. Minor weaknesses exist in discussing the approach's limitations, but the paper's strengths outweigh these concerns.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN (Discriminative Pre-training of Graph Neural Networks), a novel framework designed to enhance the performance of GNNs by addressing issues associated with generative pre-training methods that lead to graph mismatch. The proposed methodology involves a generator that predicts masked edges while a discriminator identifies true edges from generated ones, thereby optimizing the effectiveness of downstream tasks. Empirical evaluations across various graph types demonstrate that DiP-GNN outperforms existing pre-training methodologies, establishing its robustness and effectiveness in improving node representations.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to pre-training GNNs, which effectively mitigates the graph mismatch issue that plagues traditional methods. The dual-objective training paradigm of the generator and discriminator is rigorously formulated, showcasing a clear advancement over existing techniques. However, the paper could benefit from a more detailed discussion of the limitations of the proposed method, particularly concerning the scalability of the discriminator and the potential trade-offs between generator performance and discriminator accuracy. Additionally, the paper's experimental section, while comprehensive, may lack in-depth analysis of specific edge cases or scenarios where DiP-GNN might underperform.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with the field. The methodology is presented in a logical sequence, with mathematical formulations that enhance clarity. However, certain sections could be improved by providing more context or examples to illustrate complex concepts, particularly the intricacies of the generator and discriminator interactions. The quality of the experiments appears solid, and the reproducibility is strengthened by the use of established datasets and implementation in PyTorch, although a more detailed description of experimental settings would further aid reproducibility.\n\n# Summary Of The Review\nOverall, DiP-GNN presents a significant advancement in the pre-training of GNNs by effectively addressing the limitations of generative methodologies through a discriminative approach. The empirical results support the claims made by the authors, though some areas could be enhanced for clarity and depth. The paper is poised to contribute meaningfully to the field of graph-based learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a new framework aimed at addressing challenges in generative pre-training for graph neural networks (GNNs). The authors propose a dual training mechanism involving a generator and discriminator to improve the performance of GNNs on tasks involving missing edges. Despite claiming to outperform existing methods, the findings show marginal improvements in F1 score and MRR on specific datasets, which raises questions about the significance of these results.\n\n# Strength And Weaknesses\nThe paper highlights important issues with generative pre-training, such as graph mismatch, but fails to provide a robust solution to these challenges. While the approach of integrating a generator and discriminator is intriguing, it lacks originality, as this concept has been previously explored in Generative Adversarial Networks (GANs). The empirical evidence presented does not convincingly demonstrate the effectiveness of the DiP-GNN model, with only trivial improvements over existing methods. Furthermore, the complexity of the dual training process may introduce challenges in model stability and hyperparameter tuning, which are not sufficiently addressed. The analysis of the framework's limitations and its generalization potential is also lacking.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by vague analyses and insufficient details regarding implementation, which may hinder reproducibility. The novelty of the framework is questionable, as it heavily relies on established concepts without offering substantial advancements. The quality of the empirical validation is also compromised due to the limited generalization of results and superficial engagement with related work.\n\n# Summary Of The Review\nOverall, while the DiP-GNN framework attempts to tackle significant challenges in GNN pre-training, it falls short in providing compelling evidence for its effectiveness and originality. The numerous weaknesses, including limited generalization and vague analyses, detract from its contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, an innovative approach that enhances Graph Neural Networks (GNNs) through a discriminative pre-training method. The methodology involves a dual training mechanism that incorporates both a generator and a discriminator, leading to improved graph alignment and performance in downstream tasks. Experimental results demonstrate that DiP-GNN significantly outperforms existing methods on large-scale homogeneous and heterogeneous graphs, achieving notable improvements in scores such as F1 and MRR across different datasets.\n\n# Strength And Weaknesses\nThe primary strengths of DiP-GNN include its robust dual training mechanism that fosters effective learning through improved graph alignment and edge recovery, as well as its scalability to handle large graphs, which is crucial for real-world applications. However, the paper could benefit from a more in-depth discussion on the computational complexity of the dual training process and its implications for real-time applications. Additionally, while the results are impressive, further validation on a wider variety of datasets would strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings. The quality of the writing is high, with sufficient detail provided for the reader to understand the significance of the contributions. The novelty of the approach is evident, especially in the context of existing pre-training methods for GNNs. The authors have committed to open science by making their code publicly available, enhancing reproducibility and encouraging further research.\n\n# Summary Of The Review\nOverall, DiP-GNN represents a significant advancement in the field of Graph Neural Networks, showcasing a novel approach that combines generative and discriminative training for improved performance. The empirical results are compelling, though additional validation across diverse datasets would further solidify the framework's applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel framework for pre-training Graph Neural Networks (GNNs) on large-scale unlabeled graphs. It critiques traditional generative pre-training methods, highlighting their theoretical flaws due to graph mismatch, and proposes a discriminative approach that integrates a generator and discriminator model. The methodology focuses on the generator's role in reconstructing masked edges while the discriminator assesses the authenticity of these edges, leading to enhanced node embeddings. The findings suggest that this approach significantly improves the generalization ability of GNNs and provides a theoretical foundation for better representation learning that addresses existing limitations in prior methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contributions, which advance the understanding of GNN pre-training through a discriminative lens. The identification of graph mismatch as a critical issue underscores a significant limitation of generative methods, and the proposed solution demonstrates a clear alignment with the original graph structure, potentially improving downstream performance. However, a potential weakness is the lack of extensive empirical validation across diverse datasets to fully establish the practical effectiveness of the DiP-GNN framework compared to existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear exposition of the theoretical framework and methodology. The quality of the theoretical insights is high, and the integration of generative and discriminative principles is novel. However, the reproducibility of the results could be enhanced by providing more detailed experimental protocols and datasets used for evaluation. The paper could also benefit from clearer benchmarks to facilitate comparisons with existing methods.\n\n# Summary Of The Review\nOverall, the DiP-GNN framework presents a compelling theoretical advancement in GNN pre-training by addressing the limitations of traditional generative methods. While the theoretical insights are robust and the proposed methodology is innovative, further empirical validation is necessary to establish the practical significance of the findings across various applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
        "# Summary Of The Paper\nThe paper titled \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\" introduces a novel framework for enhancing the performance of Graph Neural Networks (GNNs) through a discriminative pre-training approach. The authors leverage a Heterogeneous Graph Transformer as the backbone architecture and propose a method that includes edge generation and discrimination tasks to effectively learn graph representations. The methodology involves sophisticated sampling techniques for both edges and nodes, as well as the use of multiple loss functions to optimize the training process. Experimental results demonstrate significant improvements in F1 and Mean Reciprocal Rank (MRR) scores across datasets, indicating the effectiveness of the proposed approach in various settings.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to pre-training GNNs, which addresses the critical challenge of learning effective representations in large and complex graph structures. The detailed training pipeline and the specific algorithms for graph subsampling (LADIES and HGSampling) add rigor to the methodology. However, a notable weakness is the limited discussion on broader implications and potential applications of the findings. Furthermore, while ablation studies provide insights into component contributions, a deeper analysis of the trade-offs involved in different configurations could enhance understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The quality of writing is high, with clear explanations of complex concepts. The novelty of the approach is significant, as it introduces a comprehensive pre-training strategy that is not commonly explored in GNN literature. The code will be made publicly available, which enhances reproducibility; however, additional details on dataset preprocessing and specific parameter settings would further facilitate this aspect.\n\n# Summary Of The Review\nIn summary, the paper presents a compelling approach to pre-training GNNs that demonstrates strong empirical results. While the methodology is well-explained, the paper could benefit from a broader discussion on its implications and potential applications.\n\n# Correctness\n4/5 - The methodology is sound, and the experimental results are consistent with the proposed approach, though minor clarifications in the loss functions would strengthen the correctness.\n\n# Technical Novelty And Significance\n4/5 - The proposed discriminative pre-training method offers a fresh perspective on GNN training, making significant contributions to the field.\n\n# Empirical Novelty And Significance\n4/5 - The improvements in performance metrics across various datasets illustrate the empirical significance of the approach, although further exploration of its impact in real-world scenarios would be beneficial.",
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel discriminative pre-training framework for Graph Neural Networks (GNNs). The authors argue that DiP-GNN effectively addresses the limitations of generative methods, such as Graph AutoEncoder (GAE), by focusing on graph alignment to improve embedding quality. Through empirical evaluations, the paper claims that DiP-GNN outperforms existing methods on specific datasets, including Reddit and OAG-CS. However, the authors do not provide a comprehensive comparison of generative models' potential strengths, nor do they discuss the conditions under which their results are achieved.\n\n# Strength And Weaknesses\nThe main strength of DiP-GNN lies in its innovative approach to discriminative pre-training, which the authors claim leads to better graph alignment and embedding quality. However, several weaknesses are evident. The comparisons made between DiP-GNN and generative methods appear biased, as the paper emphasizes generative approaches' deficiencies without acknowledging their successful applications. Additionally, the claims regarding performance superiority lack sufficient context and may not account for the specific characteristics of the datasets used in experiments. The paper does not adequately reference prior works that have addressed similar issues, resulting in a lack of novelty in certain insights presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is somewhat hindered by its biased framing of DiP-GNN as superior to other models without a balanced discussion of their merits. While the methodology is clearly outlined, the lack of comprehensive comparisons with existing generative methods and insufficient discussion of experimental conditions may affect the overall quality of the findings. The novelty of the insights, particularly around the \"graph mismatch problem,\" is undermined by the failure to acknowledge prior works. The reproducibility of results is also questionable due to the lack of detailed information on hyper-parameter settings and their comparison with baselines.\n\n# Summary Of The Review\nOverall, while DiP-GNN presents an interesting approach to GNN pre-training with compelling results, the paper's biased comparisons and lack of acknowledgment of existing methods detract from its contributions. A more balanced evaluation of both discriminative and generative approaches would strengthen the paper's claims and provide a clearer understanding of the landscape in graph representation learning.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
        "# Summary Of The Paper\nThe paper presents a novel approach for discriminative pre-training of Graph Neural Networks (GNNs) aimed at enhancing their performance on various downstream tasks. The authors introduce a new training methodology that leverages a two-stage framework: an initial unsupervised pre-training phase followed by a supervised fine-tuning phase. Their experiments demonstrate that the proposed method outperforms existing GNN architectures on benchmark datasets, achieving significant improvements in metrics such as accuracy and F1 score.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining unsupervised and supervised learning techniques for GNNs, which addresses a notable gap in the existing literature. The empirical results are compelling, showcasing substantial performance boosts across multiple tasks. However, the paper lacks a thorough ablation study to isolate the contributions of the different components of the proposed method. Additionally, while the theoretical background is well-established, the practical implications of the findings could be better articulated.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured and clearly written, although there are some areas where clarity could be improved, particularly in the notation and algorithm presentation. The novelty of the proposed method is significant, as it introduces a new way of pre-training GNNs that has not been extensively explored before. Reproducibility is a concern, as the paper does not provide sufficient details regarding the implementation of the experiments, including hyperparameters and training procedures, which may hinder other researchers from replicating the results.\n\n# Summary Of The Review\nThis paper makes a meaningful contribution to the field of GNNs by introducing a new discriminative pre-training method that enhances performance on downstream tasks. While the results are promising, the paper could benefit from clearer descriptions of methodologies and a more rigorous exploration of the proposed approach's components.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces the DiP-GNN framework aimed at addressing the graph mismatch problem in graph neural networks (GNNs). By focusing on edge recovery, the proposed methodology seeks to enhance the performance of GNNs when dealing with incomplete graph structures. The authors evaluate the effectiveness of DiP-GNN through experiments conducted on a limited range of datasets, showcasing improvements in performance metrics compared to baseline methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its targeted approach to the graph mismatch problem, offering a systematic methodology for edge recovery within GNNs. However, the paper has notable weaknesses, including a narrow focus that neglects broader limitations of GNNs, such as scalability and the impact of missing node features. Furthermore, the limited dataset evaluation raises concerns about the generalizability of the findings. The lack of discussion on hyper-parameter implications and trade-offs between generator and discriminator performance adds to the paper's shortcomings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured and conveys its main concepts clearly, the depth of analysis regarding hyper-parameters and model sensitivity is insufficient. The novelty of the proposed framework is evident, yet the reproducibility of results could be compromised due to the absence of detailed discussions on hyper-parameter choices and their effects. Furthermore, the exploration of additional applications and the potential for model adaptations in dynamic settings could enhance the overall quality and impact of the work.\n\n# Summary Of The Review\nThe DiP-GNN framework presents a focused approach to tackling the graph mismatch problem in GNNs, demonstrating some improvements in performance metrics. However, the paper's narrow scope, limited dataset evaluations, and insufficient exploration of key aspects such as hyper-parameter sensitivity and applicability to dynamic graphs hinder its overall impact and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper presents DiP-GNN, a novel framework for the pre-training of Graph Neural Networks (GNNs) that utilizes discriminative methods to address the challenges posed by generative pre-training techniques, particularly the issue of graph mismatch. The methodology involves a dual approach consisting of a generator and a discriminator that work together to recover masked edges while distinguishing them from original edges. Experimental results, based on datasets such as Reddit and OAG-CS, indicate notable improvements in performance metrics such as F1 score and Mean Reciprocal Rank (MRR), albeit without thorough statistical validation of the findings.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to GNN pre-training, specifically the focus on discriminative methods which is a relatively unexplored area. The results demonstrate significant performance improvements, suggesting practical relevance. However, the paper has notable weaknesses, particularly in its lack of rigorous statistical analysis to validate the reported improvements. While the authors mention statistical significance for performance gains, they do not provide sufficient details on confidence intervals or effect sizes, which undermines the robustness of their claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, making it accessible to the audience. However, the lack of detailed statistical methodologies detracts from the overall quality and reproducibility of the findings. The novelty of the approach is commendable, as it seeks to fill a gap in the literature regarding the limitations of generative methods. Nevertheless, without explicit statistical validation, replicating the results or building upon them becomes challenging.\n\n# Summary Of The Review\nDiP-GNN introduces a promising discriminative pre-training approach for GNNs, highlighting significant performance improvements over existing generative methods. However, the paper's claims are weakened by insufficient statistical validation, limiting confidence in the reported results. Future work should prioritize comprehensive statistical methodologies to enhance the credibility and reproducibility of their findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
        "# Summary Of The Paper\nThe paper presents the DiP-GNN framework, designed for graph neural networks (GNNs) to enhance performance in tasks such as node classification and link prediction. The methodology includes a generator and discriminator that work together to generate and assess graph structures. The findings indicate that the framework achieves competitive performance on datasets like Reddit and OAG-CS, with accuracy metrics suggesting the potential of the generator even without the discriminator.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining generative and discriminative techniques within the GNN framework, which facilitates improved performance on specific tasks. However, the paper has several weaknesses, including a lack of thorough exploration of scalability to larger graphs, insufficient analysis of hyper-parameter tuning, and an incomplete validation of the generator's effectiveness. Additionally, the implications of different masking ratios, time complexity, and the reliance on edge and node features are not adequately addressed. The paper also misses an examination of biases in edge generation and a clear strategy for integrating with existing models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the main contributions are outlined, many aspects lack depth, making it difficult to fully comprehend the implications of the findings. The quality of the empirical results appears sound, but the novelty is not convincingly demonstrated due to the absence of comprehensive comparisons with state-of-the-art methods. Reproducibility may be hindered by the lack of detailed methodological descriptions and insufficient exploration of the model's limitations and hyper-parameter effects.\n\n# Summary Of The Review\nOverall, while the DiP-GNN framework introduces an interesting approach to GNNs, the paper falls short in several critical areas, including scalability, hyper-parameter analysis, and interpretability. These deficiencies detract from the overall impact and applicability of the proposed method, warranting further investigation and refinement in future work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
        "# Summary Of The Paper\nThe paper presents a method called DIP-GNN (Discriminative Pre-Training of Graph Neural Networks), which aims to improve the performance of GNNs on labeled datasets by pre-training on unlabeled graphs. The authors introduce a generator-discriminator framework, where the generator masks edges in a graph to predict them, and the discriminator evaluates the quality of these predictions. The paper claims to show that this approach leads to better performance in terms of F1 scores and Mean Reciprocal Rank (MRR) when fine-tuning on smaller labeled datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address the common issue of graph mismatch, although the concept itself is not novel. The methodology, while structured around well-known techniques from generative adversarial networks, lacks originality and does not provide significant advancements over existing methods. While the paper reports some empirical improvements in performance metrics, these gains appear to be marginal and do not substantially impact the current understanding or application of GNNs. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, with a logical flow from motivation to methodology and results. However, the novelty of the proposed method is questionable, as it largely rehashes established concepts in GNN pre-training. The descriptions of experiments and results are presented clearly, though the reproducibility of the findings may be hindered by the lack of detailed information on the datasets and specific experimental setups used.\n\n# Summary Of The Review\nOverall, the paper presents a familiar approach to GNN pre-training with limited innovative contributions. While it attempts to address known challenges in the field, the advancements claimed are incremental rather than groundbreaking, and the methodology feels derivative.\n\n# Correctness\n4/5 - The technical details and methodologies presented are largely accurate, but some claims regarding novelty and significance may be overstated.\n\n# Technical Novelty And Significance\n2/5 - The proposed method does not introduce substantial new concepts or techniques to the field of GNNs, relying instead on established ideas.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results show some improvements, they are minor and do not significantly alter the landscape of GNN research, making their significance limited.",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel approach to Graph Neural Network (GNN) pre-training that employs a discriminative framework to address the graph mismatch problem prevalent in generative pre-training methods. The methodology includes the use of a generator and discriminator, highlighting the significance of the discriminator in improving downstream task performance, particularly in heterogeneous graphs. The findings indicate that DiP-GNN outperforms existing generative approaches, suggesting the importance of the discriminator in aligning learned embeddings with the original graph structure.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative framework that emphasizes discrimination over generation, yielding significant improvements in downstream task performance. However, the reliance on a generator raises scalability concerns, and the potential for contrastive learning integration could further enhance robustness. The paper would benefit from broader experimentation with various GNN architectures and a deeper analysis of the generator\u2019s role, as well as the inclusion of hyper-parameter optimization techniques to improve model performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents a clear methodology and set of findings, contributing to its overall quality. The novelty of the approach is evident in its discriminative focus, though the paper could further improve clarity by providing more detailed discussions on the limitations of the generator and potential enhancements through hybrid models. The reproducibility of the results could be strengthened by detailing the experimental setup, including the hyper-parameter configurations and the dataset characteristics.\n\n# Summary Of The Review\nOverall, the paper presents a promising and novel contribution to the field of GNN pre-training with DiP-GNN, demonstrating significant empirical results. However, there are several areas for improvement, including the exploration of alternative architectures and the incorporation of contrastive learning methods to enhance the model's robustness and scalability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
        "# Summary Of The Paper\nThe paper introduces DiP-GNN, a novel graph neural network framework that significantly improves upon existing generative pre-training and self-supervised learning approaches. The authors present empirical results demonstrating DiP-GNN's superior performance across various datasets and tasks, including homogeneous and heterogeneous graphs. Notably, the methodology incorporates both edge and feature contributions, leading to substantial improvements in tasks such as paper field classification, product recommendation, and author name disambiguation, with consistent performance gains across all tested scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive evaluation of DiP-GNN against a variety of baseline methods, showcasing clear performance advantages in multiple tasks and datasets. The statistical significance of the results adds credibility to the findings. The ablation studies provide valuable insights into the contributions of edges and features, reinforcing the framework's robustness. However, a potential weakness is the lack of a detailed exploration of the underlying mechanisms that lead to improved performance, which could provide further insights into its applicability in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the experiments is high, with rigorous statistical testing enhancing the reliability of the results. The novelty of the proposed DiP-GNN framework is evident, although the paper could benefit from a more thorough discussion on reproducibility, including the availability of code and datasets used for the experiments, which would facilitate further research in the field.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of graph neural networks through the introduction of DiP-GNN, highlighting its effectiveness in enhancing performance across various tasks. The empirical results are compelling, though additional discussion on the underlying mechanisms and reproducibility would strengthen the paper further.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
        "# Summary Of The Paper\nThe paper proposes a novel framework called DiP-GNN, which integrates generative and discriminative pre-training methods for Graph Neural Networks (GNNs). The authors argue that existing approaches have limitations in effectively capturing graph structures and relationships. They introduce DiP-GNN, which aims to enhance the performance of GNNs by leveraging generative techniques alongside discriminative learning. Experimental results demonstrate that DiP-GNN outperforms state-of-the-art models on several benchmark datasets, indicating its effectiveness in improving graph representation learning.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining generative and discriminative pre-training, offering a fresh perspective on GNN training methodologies. The empirical results support the claims made by the authors, showcasing significant improvements over existing techniques. However, the paper suffers from issues related to clarity and organization, particularly in the abstract and introduction sections, which could confuse readers. Additionally, the use of technical jargon and inconsistent terminology may hinder accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel contribution to the field, its clarity is compromised by complex sentence structures and a lack of clear definitions for key terms and abbreviations. The quality of figures and tables could be enhanced with more descriptive legends and headings. Although the methodology appears sound, the reproducibility of results may be affected by the paper's dense presentation and insufficient detail in certain areas, such as the experimental setup and evaluation metrics.\n\n# Summary Of The Review\nThis paper introduces DiP-GNN, a promising framework that combines generative and discriminative learning for GNNs, demonstrating notable performance improvements. However, the paper's clarity and organization need significant enhancement to ensure that its contributions are effectively communicated to the audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
      ],
      "preparations": [
        null,
        "# Notes on \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\"\n\n## Abstract\n- Proposes DiP-GNN, a method for pre-training Graph Neural Networks (GNNs) on large-scale unlabeled graphs.\n- Addresses the graph mismatch issue in existing generative pre-training methods, which suffer from discrepancies between masked graphs and original graphs.\n- DiP-GNN involves:\n  - A generator that predicts masked edges.\n  - A discriminator that distinguishes between original and generated edges.\n- The discriminator is fine-tuned for downstream tasks, leading to better alignment with the original graph structure.\n- Results show DiP-GNN outperforms existing methods on homogeneous and heterogeneous graphs.\n\n## Introduction\n- GNNs are effective for tasks like node classification, knowledge graph modeling, and recommendation systems.\n- Generative pre-training methods (similar to masked language modeling) involve masking edges and training GNNs to recover them.\n- Issues with generative pre-training:\n  - Graph mismatch: The input graph deviates from the original due to masked edges.\n- DiP-GNN proposes a simultaneous training of a generator and discriminator to mitigate graph mismatch.\n\n## Method\n### 1. Edge Generation and Discrimination\n- Given a graph \\( G = (N, E) \\):\n  - Edges are split into unmasked \\( E_u \\) and masked \\( E_m \\).\n- The generator predicts masked edges by:\n  - Computing node representations based on unmasked edges.\n  - Using a prediction probability based on cosine similarity.\n- The discriminator predicts whether edges are original or generated, using all edges for edge representation.\n\n### 2. Feature Generation and Discrimination\n- Node features can be textual or vectorized.\n- For text features:\n  - Uses a bi-directional Transformer model.\n  - Masks tokens in the feature vector and predicts them using node embeddings.\n- Discriminator determines if tokens in the generated feature are original or generated.\n\n### 3. Model Training\n- Jointly minimizes the edge and feature losses.\n- The final loss incorporates both generator and discriminator losses with a weighting factor for the discriminator.\n\n### 4. Comparison with GAN\n- DiP-GNN differs from GAN:\n  - GANs use a min-max game formulation while DiP-GNN doesn\u2019t due to the discrete nature of edge generation.\n  - Focuses on maintaining a high-quality discriminator.\n\n## Experiments\n### 1. Settings and Datasets\n- Node transfer setting: pre-training on a large graph with sparse labels.\n- Datasets:\n  - **Reddit**: Homogeneous graph with 230k nodes, used for node classification.\n  - **Product Recommendation**: Bi-partite graph, used for link prediction.\n  - **OAG-CS**: Heterogeneous graph with 1.1M nodes, used for multiple classification tasks.\n\n### 2. Implementation Details\n- Utilizes PyTorch and PyTorch Geometric.\n- Employs graph subsampling techniques (LADIES for homogeneous and HGSampling for heterogeneous graphs).\n- Negative sampling for edges and balancing edges for the discriminator to avoid trivial predictions.\n\n### 3. Baselines\n- Compared with several methods including GAE, GraphSAGE, DGI, GPT-GNN, GRACE, GraphCL, JOAOv2.\n- Performance metrics include F1 scores for Reddit and MRR for other tasks.\n\n### 4. Main Results\n- DiP-GNN shows significant improvements over baselines:\n  - Reddit: F1 score improvement by 1.1 points.\n  - OAG-CS: MRR improvements of 5.1, 2.5, and 2.5 for different tasks.\n- Pre-training consistently enhances model performance.\n\n### 5. Analysis\n- DiP-GNN is robust to hyper-parameters related to negative sampling and the weight of the discriminator's loss.\n- Missing edges are shown to adversely affect performance more than wrong edges.\n- The framework's design allows the discriminator to learn from a better-aligned graph.\n\n## Conclusion\n- DiP-GNN effectively addresses the graph mismatch problem in GNN pre-training.\n- The dual approach of a generator and discriminator leads to improved performance in downstream tasks.\n- Extensive experiments validate the method\u2019s effectiveness and robustness across various datasets and tasks.\n\n## References\n- The paper cites foundational works in GNNs, generative models, and pre-training methodologies, including GAE, DGI, and GANs.",
        "### Comprehensive Notes on \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\"\n\n#### Technical Content\n- **Overview**: The paper presents DiP-GNN (Discriminative Pre-training of Graph Neural Networks), which aims to enhance GNN performance through a two-part pre-training framework involving a generator and a discriminator.\n  \n- **Problem Addressed**: Previous generative pre-training methods for GNNs suffer from \"graph mismatch,\" where the masked input graph deviates from the original, leading to suboptimal node embeddings.\n\n- **Methodology**:\n  - **Generator**: The generator learns to recover masked edges from a graph by predicting the identities of masked edges based on unmasked edges.\n  - **Discriminator**: The discriminator distinguishes between true edges from the original graph and edges generated by the generator.\n  - **Training Process**: During pre-training, edges are masked, and the generator predicts the missing edges, which are then evaluated by the discriminator. The discriminator is fine-tuned for downstream tasks after pre-training.\n  \n- **Edge Generation and Discrimination**:\n  - Edges are divided into unmasked (Eu) and masked (Em).\n  - The generator uses node representations based on unmasked edges to predict masked edges.\n  - The discriminator processes both true and fake edges to learn to classify them correctly.\n\n- **Feature Generation and Discrimination**: \n  - Node features can be text or vectors. The generator predicts masked tokens or features based on the GNN embeddings.\n  - The discriminator assesses the generated features against the original features.\n\n- **Loss Functions**: \n  - The loss function combines the generator's and discriminator's losses, allowing both components to improve.\n\n- **Comparison with GANs**: \n  - DiP-GNN does not utilize a min-max game formulation like traditional GANs due to the discrete nature of graph edges, which complicates backpropagation.\n\n#### Experimental Design\n- **Datasets**: \n  - Uses large-scale homogeneous (Reddit) and heterogeneous (OAG-CS) graphs for evaluation.\n  - Each dataset serves different downstream tasks, e.g., node classification and link prediction.\n\n- **Settings**: \n  - Explores a node transfer setting where large unlabeled graphs are pre-trained, and smaller labeled graphs are used for fine-tuning.\n  - Employs various sampling strategies for graphs to ensure training efficiency.\n\n- **Baselines**: \n  - Compares against several GNN pre-training and self-supervised learning methods: GAE, GraphSAGE, DGI, GPT-GNN, GRACE, GraphCL, JOAOv2.\n\n- **Hyper-parameters**: \n  - Several hyper-parameters are examined, including the number of negative nodes and the weight of the discriminator's loss.\n\n#### Results\n- **Performance**: \n  - DiP-GNN significantly outperforms existing generative pre-training and self-supervised learning approaches across multiple datasets.\n  - Reports improvements of 1.1 in F1 score on the Reddit dataset and 2.8 in MRR score on the OAG-CS dataset.\n\n- **Ablation Studies**: \n  - Demonstrates the importance of both edge and feature components in the model.\n  - Shows that missing edges negatively impact performance more than wrong edges.\n\n- **Graph Mismatch Analysis**: \n  - The discriminator benefits from a better-aligned graph compared to the generator due to the nature of the edge reconstruction task.\n\n#### Strengths\n- **Innovative Approach**: The use of a discriminative pre-training framework addresses a significant issue in generative methods by improving graph alignment.\n- **Strong Empirical Results**: Demonstrated effectiveness on large-scale datasets with substantial performance improvements compared to baseline methods.\n- **Comprehensive Experiments**: Includes various datasets and tasks, along with robust comparisons against multiple baselines.\n\n#### Limitations\n- **Complexity**: The dual model structure (generator and discriminator) may introduce additional complexity in terms of implementation and hyper-parameter tuning.\n- **Dependence on Data**: Performance improvements heavily rely on the quality and structure of the input graphs; results may vary with different types of graphs.\n- **Generalizability**: While results are promising, further exploration is needed to assess how well the method generalizes to different types of tasks and datasets beyond those tested.\n\n### Conclusion\nDiP-GNN presents a novel framework for GNN pre-training that effectively mitigates the graph mismatch issue seen in traditional generative approaches, leading to improved performance on various tasks. The proposed methodology and extensive testing provide a strong foundation for future research in GNN pre-training techniques.",
        "# Notes on \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\"\n\n## Abstract\n- **Objective**: Introduce DiP-GNN for pre-training GNNs using a discriminative approach to address the graph mismatch problem seen in generative pre-training methods.\n- **Method**: Train a generator to recover masked edges and a discriminator to distinguish between generated and original edges.\n- **Key Result**: DiP-GNN outperforms existing generative pre-training approaches in downstream tasks.\n\n## 1. Introduction\n- **Context**: GNNs excel in applications like node classification and recommendation systems.\n- **Pre-training Methods**: Typically involve generative approaches like masked edge recovery, akin to masked language modeling.\n- **Problem**: Graph mismatch arises when a significant number of edges are masked, altering essential topological information, leading to suboptimal node embeddings.\n- **Proposed Solution**: DiP-GNN trains both a generator and a discriminator, allowing for better alignment with the original graph during pre-training.\n\n## 2. Background\n### Graph Neural Networks\n- **Representation Computation**: \n  \\[\n  a^{(k)}_v = \\text{Aggregate}\\left(\\{h^{(k-1)}_u \\mid \\forall u \\in \\text{Neighbor}(v)\\}\\right)\n  \\]\n  \\[\n  h^{(k)}_v = \\text{Combine}(v, a^{(k)}_v)\n  \\]\n- **Works Referenced**: Kipf & Welling (2017), Schlichtkrull et al. (2018) among others.\n\n### GNN Pre-Training\n- **Previous Methods**: Focus on proximity, random walks, and contrastive learning.\n- **Generative Objectives**: Examples include GAE (Kipf & Welling 2016) and DGI (Velickovic et al. 2019).\n- **Focus of DiP-GNN**: Pre-training on a single large graph rather than multiple small graphs.\n\n## 3. Method\n### 3.1 Edge Generation and Discrimination\n- **Graph Definition**: \\( G = (N, E) \\), with masked edges defined as \\( E = E_u \\cup E_m \\).\n- **Generator Objective**: Predict masked edges, specifically aiming to recover \\( n_1 \\) given \\( n_2 \\) and \\( E_u \\).\n  - **Probability Equation**: \n  \\[\n  p(n_1 | n_2, E_u) = \\frac{\\exp(d(h_g(n_1), h_g(n_2)))}{\\sum_{n' \\in C} \\exp(d(h_g(n'), h_g(n_2)))}\n  \\]\n- **Generator Loss**: \n  \\[\n  L_e^g = -\\sum_{(n_1, n_2) \\in E_m} \\log p(n_1 | n_2, E_u)\n  \\]\n\n### 3.2 Feature Generation and Discrimination\n- **Feature Generator**: Uses bi-directional Transformer models similar to BERT.\n- **Masking**: Randomly mask tokens in text features.\n- **Discriminator Objective**: Distinguish true vs. generated tokens.\n  - **Loss Function**:\n  \\[\n  L_f^d = -\\sum_{i=1}^L \\left(1\\{x_{corr,i} = x_i\\} \\log p_{true} + (1 - 1\\{x_{corr,i} = x_i\\}) \\log(1 - p_{true})\\right)\n  \\]\n\n### 3.3 Model Training\n- **Joint Loss Function**: \n\\[\nL(\\theta) = L_e^g + \\lambda L_e^d + L_f^g + \\lambda L_f^d\n\\]\n- **Discriminator focuses on better graph alignments for downstream tasks.**\n\n### 3.4 Comparison with GAN\n- **Difference from GAN**: DiP-GNN does not utilize a min-max game since it focuses on edge generation, which is discrete, complicating back-propagation.\n\n## 4. Experiments\n### 4.1 Settings and Datasets\n- **Node Transfer Setting**: Use a large graph for pre-training and separate labeled data for fine-tuning.\n- **Datasets**:\n  - **Reddit**: 230k nodes, 70% for pre-training, node classification task.\n  - **Product Recommendation**: 633k query nodes, 2.71M product nodes, link prediction task.\n  - **OAG-CS**: 1.1M nodes, multiple tasks (author disambiguation, paper field classification).\n\n### 4.2 Implementation Details\n- **Graph Subsampling**: Use LADIES for homogeneous and HGSampling for heterogeneous graphs.\n- **Edge Generation**: Employ negative sampling to improve efficiency and balance.\n\n### 4.3 Baselines\n- **Compared Methods**: GAE, GraphSAGE, DGI, GPT-GNN, GRACE, GraphCL, JOAOv2.\n\n### 4.4 Main Results\n- **Performance Improvements**: DiP-GNN shows statistically significant improvements in all tasks compared to baselines.\n\n### 4.5 Analysis\n- **Hyper-parameter Sensitivity**: Robustness to variations in negative sampling and discriminator weight.\n- **Missing vs. Wrong Edges**: Missing edges detrimentally affect performance more than wrong edges.\n\n## 5. Conclusion and Discussions\n- **Key Contributions**: Introduces DiP-GNN, shows that discriminative pre-training is effective, particularly for preserving graph structure and enhancing downstream task performance.\n\n## References\n- Extensive list of relevant literature and foundational works cited throughout the paper.\n\n## Additional Notes\n- **Algorithm Pipeline**: Detailed training steps are provided in the appendix.\n- **Vector Features**: Separate treatment of vector feature generation and discrimination is also provided.\n\nThese notes encapsulate the theoretical foundations, methodologies, results, and implications of the DiP-GNN framework as presented in the paper.",
        "**Strengths and Limitations of the Paper:**\n\n1. **Strength: Innovative Framework**\n   - The DiP-GNN framework offers a novel approach to GNN pre-training by integrating a generator and a discriminator, which improves the alignment of the graph input to the original structure.\n   - **Limitation: Complexity of Implementation**\n     - The dual model approach may introduce additional complexity in implementation and training, potentially making it less accessible for practitioners or researchers unfamiliar with adversarial frameworks.\n\n2. **Strength: Addressing Graph Mismatch**\n   - The paper effectively identifies and addresses the graph mismatch problem inherent in generative pre-training methods, leading to better performance in downstream tasks.\n   - **Limitation: Generalizability**\n     - While the framework shows improvements for specific datasets, the generalizability of the approach across various types of graphs and tasks remains to be thoroughly tested.\n\n3. **Strength: Extensive Experimental Validation**\n   - The authors provide extensive experiments demonstrating the effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs, significantly outperforming existing methods.\n   - **Limitation: Limited Baseline Comparison**\n     - Although the paper compares DiP-GNN with several baselines, it may benefit from including additional state-of-the-art methods to provide a more comprehensive assessment of its performance.\n\n4. **Strength: Robustness to Hyper-parameters**\n   - The framework appears to be robust to variations in hyper-parameters, as indicated by the ablation studies, suggesting ease of use in practical applications.\n   - **Limitation: Hyper-parameter Sensitivity**\n     - Despite robustness, the presence of multiple hyper-parameters may still require careful tuning, which could be a barrier for efficient deployment in real-world scenarios.\n\n5. **Strength: Performance Metrics**\n   - The reported improvements in metrics like F1 score and MRR score showcase the practical benefits of the proposed method for downstream tasks such as node classification and link prediction.\n   - **Limitation: Lack of Insight into Interpretability**\n     - The paper does not extensively address the interpretability of the learned embeddings or the decision-making process of the discriminator, which is crucial for understanding model behavior in sensitive applications.\n\n6. **Strength: Public Availability of Code**\n   - The commitment to publicly releasing the code enhances reproducibility and fosters collaboration within the research community.\n   - **Limitation: Potential for Overfitting**\n     - The focus on achieving high performance through extensive pre-training raises concerns about overfitting, particularly on smaller datasets, which could undermine the model's applicability to unseen data.\n\n7. **Strength: Connection to Existing Literature**\n   - The paper successfully situates its contributions within the broader context of GNN research and provides relevant comparisons to related work, enriching the academic discourse.\n   - **Limitation: Depth of Literature Review**\n     - While related work is acknowledged, a more in-depth discussion of the limitations of existing methods could further highlight the necessity and impact of the proposed framework.",
        "# ICLR 2023 Review Notes on DiP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n## Summary\n- The paper proposes a novel approach for pre-training Graph Neural Networks (GNNs) called DiP-GNN (Discriminative Pre-training of Graph Neural Networks).\n- Unlike traditional methods that utilize generative pre-training, DiP-GNN introduces a dual-framework involving both a generator and a discriminator to address the challenges associated with graph mismatch.\n- The authors claim that this discriminative approach leads to improved performance on downstream tasks by ensuring better alignment with the original graph structure.\n\n## Methodology\n- **Generative Framework**: Previous approaches typically use a generator that reconstructs masked edges in the graph. DiP-GNN modifies this paradigm by integrating a discriminator that evaluates the generated edges against the true edges.\n- **Discriminative Training**: \n  - A generator predicts the identities of masked edges, creating a reconstructed graph.\n  - A discriminator assesses whether generated edges belong to the original graph or are artifacts from the generator.\n  - This dual-task framework allows the discriminator to leverage a more representative graph, which improves the quality of learned embeddings.\n\n- **Training Dynamics**: The paper articulates a shift from a purely generative objective towards a hybrid strategy where the generator's output is critically evaluated by the discriminator. This contrasts with traditional GAN approaches that rely on adversarial training without a structured alignment to the original data.\n\n## Key Contributions\n- The framework effectively mitigates the graph mismatch issue by ensuring that the discriminator operates on a graph that incorporates both true and predicted edges, leading to more accurate edge classifications.\n- Empirical results demonstrate that DiP-GNN consistently outperforms existing generative pre-training methods, particularly in scenarios involving large-scale homogeneous and heterogeneous graphs.\n- The paper presents extensive experimental validation, showcasing significant improvements in performance metrics such as F1 score and MRR on various benchmark datasets.\n\n## Experiments\n- The authors conduct experiments on diverse datasets, including Reddit for homogeneous graphs and OAG-CS for heterogeneous structures.\n- Results indicate that DiP-GNN achieves superior performance compared to baseline methods like GAE, GraphSAGE, and generative methods like GPT-GNN.\n- The paper highlights that the proposed discriminative pre-training framework is particularly robust when faced with the challenges of large graphs and sparse labeled data.\n\n## Strengths\n- The integration of a discriminator into the GNN pre-training process is a novel contribution that addresses limitations of existing methods.\n- The paper provides a comprehensive empirical analysis, substantiating claims of performance improvements with robust statistical significance.\n- Clear and structured presentation of the methodology and experiments enhances reproducibility.\n\n## Weaknesses\n- While the proposed framework is innovative, the paper does not provide extensive theoretical justification for why the discriminator's approach leads to better embeddings compared to solely generative methods.\n- The reliance on a dual-framework may introduce additional complexity in training and hyperparameter tuning, which could be a barrier for practical applications.\n\n## Future Work\n- Further exploration of the theoretical underpinnings of the discriminator's contribution to performance could strengthen the paper's impact.\n- Investigating the scalability of the DiP-GNN framework to even larger graphs and more complex applications in real-world scenarios would be beneficial.\n\n## Conclusion\n- DiP-GNN represents a significant advancement in the field of GNN pre-training by leveraging a discriminative approach to improve graph structure alignment and downstream task performance.\n- The paper is well-positioned for acceptance at ICLR, given its innovative methodology and strong empirical results.",
        "# ICLR Review Notes\n\n## Title: DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n### Summary\nThe paper presents a novel approach to adversarial training in the context of graph neural networks (GNNs). The proposed method, DiP-GNN (Discriminative Pre-training of Graph Neural Networks), addresses the challenge of improving GNN robustness against adversarial attacks by integrating a generator-discriminator framework.\n\n### Key Contributions\n1. **Adversarial Training Framework**: The authors introduce a dual approach where a generator creates adversarial examples, while a discriminator distinguishes between the original and adversarially perturbed edges of the graph. This setup helps the GNN learn more robust representations by focusing on distinguishing true edges from adversarially modified ones.\n\n2. **Improved Representation Learning**: By leveraging the adversarial examples generated during training, the GNN is able to learn more comprehensive node embeddings that are resilient to perturbations, leading to improved performance on downstream tasks.\n\n3. **Empirical Validation**: The authors conduct extensive experiments on various datasets (including both homogeneous and heterogeneous graphs) to validate the effectiveness of their approach. Results demonstrate that DiP-GNN consistently outperforms traditional adversarial training methods, showcasing its robustness against graph perturbations.\n\n### Methodology\n- **Generator**: The generator's role is to create adversarial edges by perturbing existing edges in the graph. This process involves masking a proportion of the edges and attempting to generate plausible adversarial edges that could mislead the GNN.\n\n- **Discriminator**: The discriminator's task is to classify edges as either original or adversarial. By training the discriminator to effectively differentiate between true and adversarial edges, the GNN can learn more robust features.\n\n- **Training Pipeline**: The training process involves alternating between updating the generator and the discriminator. The GNN's embeddings are fine-tuned based on the discriminator's feedback, enhancing the model's ability to generalize in the presence of adversarial modifications.\n\n### Experiments\n- **Datasets**: The experiments are conducted on large-scale homogeneous and heterogeneous graphs. The authors highlight various performance metrics, including F1 scores and Mean Reciprocal Rank (MRR), demonstrating significant improvements over baseline adversarial training methods.\n\n- **Ablation Studies**: The paper includes ablation studies to analyze the impact of different components of the DiP-GNN framework, such as the proportion of edges masked during training and the balance between the generator and discriminator losses.\n\n### Strengths\n- **Innovative Approach**: The integration of adversarial training within a GNN framework is a novel contribution that addresses a critical gap in existing literature on graph representation learning.\n  \n- **Robust Performance**: The reported improvements on various tasks suggest that the proposed method effectively enhances the resilience of GNNs to adversarial attacks.\n\n- **Comprehensive Evaluation**: The use of multiple datasets and thorough experimentation lends credibility to the findings.\n\n### Weaknesses\n- **Complexity**: The dual generator-discriminator setup may introduce additional complexity in tuning hyperparameters, which could hinder practical applications.\n\n- **Scalability**: While the method shows promise on the tested datasets, its performance on even larger graphs or in real-world applications remains to be thoroughly evaluated.\n\n### Conclusion\nThe paper presents a compelling case for the use of adversarial training in graph neural networks through the DiP-GNN framework. The proposed method not only improves robustness against adversarial attacks but also enhances the quality of learned representations. Future work could focus on simplifying the training process and exploring the method's applicability to larger, real-world graphs. Overall, the contributions are significant, and the findings are relevant to both the GNN and adversarial training communities.",
        "### Notes on DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n- **Title Significance**: The paper introduces DiP-GNN, a groundbreaking method that supposedly revolutionizes GNN pre-training. It claims to drastically enhance performance across all applications, making all previous methods obsolete.\n\n- **Abstract Highlights**: \n  - Proposes an innovative approach that allegedly eliminates all issues associated with graph mismatch in GNNs, a claim that seems overly ambitious given the complexity of the domain.\n  - Claims a major advancement over existing generative methods, suggesting that this new framework will set a new standard in the field.\n\n- **Introduction Overview**:\n  - Emphasizes that GNNs are currently the pinnacle of technology for node classification and recommendation systems, implying that without DiP-GNN, advancements in these fields are unlikely.\n  - Claims that the generative approach has been fundamentally flawed, suggesting that no prior research in the area was aware of the graph mismatch issue until now.\n\n- **Methodological Contribution**:\n  - Introduces a dual model (generator and discriminator), suggesting that this approach is a monumental leap forward, although it echoes concepts from GANs and existing literature.\n  - Claims to recover nearly 100% of masked edges, which is presented as a transformative capability, despite empirical results indicating otherwise.\n\n- **Experimental Results**:\n  - Presents improvements such as an F1 score increase of 5.0 on the Reddit dataset, significantly overstating the contribution as revolutionary for the field.\n  - Results on large-scale datasets are claimed to be unparalleled, positioning DiP-GNN as a definitive solution while downplaying any previous work that suggests comparable improvements.\n\n- **Performance Claims**:\n  - The paper asserts that DiP-GNN outperforms all previous methods by large margins, implying that no other method can compete, which is an exaggerated representation of the results.\n  - Mentions that the discriminator's performance is drastically better than any generator, claiming this is a world-first achievement in GNN pre-training.\n\n- **Comparison with Prior Work**:\n  - DiP-GNN is framed as a complete departure from past methods, suggesting that it has rendered them irrelevant rather than building upon or improving them.\n  - The authors propose that this method will lead to a paradigm shift in how researchers approach GNNs, an assertion that lacks supporting evidence from the experiments presented.\n\n- **Conclusion and Implications**:\n  - Concludes that DiP-GNN will likely become the foundational method for future research in graph neural networks, a prediction that may not align with the gradual nature of academic progress.\n  - Claims that the framework addresses all limitations of prior work, suggesting a level of completeness that is rarely seen in academic research.\n\n- **Future Work**:\n  - While suggestions for further research are included, the implications of DiP-GNN are overstated to imply that there is little left to explore, which undervalues the ongoing challenges in graph neural network development.\n\nOverall, the paper presents DiP-GNN as a cornerstone technology that will redefine the landscape of GNN research, a characterization that significantly inflates its actual contributions and impacts based on presented evidence.",
        "**Notes on \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\"**\n\n**Abstract:**\n- Proposed DiP-GNN improves GNNs by mitigating graph mismatch during pre-training.\n- Uses a generator to recover masked edges and a discriminator to distinguish between generated and original edges.\n\n**Introduction:**\n- GNNs excel in various tasks such as node classification, knowledge graph modeling, and recommendations.\n- Traditional generative pre-training methods face graph mismatch issues, leading to subpar node embeddings.\n- DiP-GNN addresses this by improving edge recovery, with the generator reportedly recovering about 30% of the masked edges.\n\n**Methodology:**\n- DiP-GNN framework involves simultaneous training of a generator and discriminator.\n- Generator predicts masked edges; discriminator assesses edge authenticity.\n  \n**Experimental Results:**\n- **Reddit Dataset:** \n  - Improved F1 score by 0.8 over existing methods (originally reported 1.1).\n- **OAG-CS Dataset:**\n  - Achieved an improvement of 1.5 in MRR for the paper field prediction task (originally reported 2.8).\n  \n**Edge and Feature Generation:**\n- The generator performs well, with accuracy around 45% (originally 40%).\n- Discriminator accuracy remains high, around 85% even with significant masking.\n\n**Performance Analysis:**\n- DiP-GNN outperforms generative methods like GPT-GNN by 0.8 on Reddit and 1.2 on Recommendations (originally stated 1.1 and 1.5).\n- Pre-training shows considerable benefits with improvements in various tasks, with MRR increases of at least 4.2, 1.8, and 1.8 for PF, PV, and AD tasks, respectively (originally 5.1, 2.5, and 2.5).\n\n**Ablation Studies:**\n- Performance drops by 0.5% when 50% of wrong edges are added, and by 1.5% when 50% of original edges are missing (originally 0.7% and 1.8%).\n- Indicates that while wrong edges introduce noise, their impact is less severe than missing edges.\n\n**Conclusion:**\n- DiP-GNN successfully enhances GNN performance by focusing on discriminative pre-training, showing robustness across different datasets. \n\n**General Observations:**\n- The framework demonstrates improved edge recovery and better alignment with original graph structures.\n- Results indicate a robust improvement over traditional generative methods, reaffirming the effectiveness of the proposed approach.",
        "1. **Assumption of Graph Structure Stability**:\n   - The paper assumes that the graph structure remains stable enough during the masking and generation of edges, which might not hold true in dynamic or evolving graphs. This could lead to misrepresentations in the learned embeddings.\n\n2. **Generative vs. Discriminative Approach**:\n   - The authors claim that the discriminative approach is better due to better alignment with the original graph. However, this undermines the potential of generative methods, which might capture unseen relationships in data. The effectiveness of generative models in other domains (like language) suggests a reconsideration of the superiority of discriminative methods in this context.\n\n3. **Assumption of Edge Independence**:\n   - The method assumes that the generation of edges is independent (i.e., the generation of one edge does not affect another). This might not be valid in real graphs where edge dependencies exist, potentially leading to biases in the edge recovery process.\n\n4. **Effectiveness of Masking Strategy**:\n   - The paper relies heavily on the edge masking strategy. However, the assumption that a significant proportion of masked edges can be correctly predicted is contentious, especially in highly connected or dense graphs where context is crucial for recovery.\n\n5. **Downstream Task Assumptions**:\n   - The authors assume that the learned representations from the pre-training phase will be beneficial for all downstream tasks. This could vary significantly depending on the nature of the tasks, suggesting a need for more empirical validation across diverse applications.\n\n6. **Hyperparameter Sensitivity**:\n   - The framework's reliance on several hyperparameters (e.g., number of negative nodes, weight of discriminator loss) assumes that these values can be determined universally. It may overlook the potential sensitivity of the model's performance to these parameters across different datasets and tasks.\n\n7. **Alignment with Original Graph**:\n   - The assumption that the discriminator sees a graph that better aligns with the original graph implies that generated edges will not introduce significant noise. This contradicts findings in other areas where noise is known to degrade model performance.\n\n8. **Performance Metrics**:\n   - The paper primarily focuses on F1 score and MRR as performance metrics without considering other potentially relevant metrics like precision, recall, or AUC, which could provide a more nuanced evaluation of performance.\n\n9. **Generalization Across Graph Types**:\n   - The proposed method is tested on specific datasets (homogeneous and heterogeneous graphs), leading to the assumption that the findings are generalizable. This generalization may not hold true for other types of graphs or real-world applications.\n\n10. **Transferability of Pre-trained Models**:\n    - The assumption that the pre-trained models will effectively transfer to downstream tasks without further adaptation or fine-tuning may be overly optimistic, especially in cases where the downstream tasks differ significantly from the pre-training tasks.",
        "**Title:** DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n**Abstract:** The paper introduces DiP-GNN, a framework that addresses graph mismatch in generative pre-training of Graph Neural Networks (GNNs) by simultaneously training a generator to recover masked edges and a discriminator to distinguish between original and generated edges. Experiments demonstrate the effectiveness of the proposed framework across various graph types.\n\n**Introduction:** GNNs excel in multiple applications but face challenges with generative pre-training methods due to graph mismatch, which adversely affects learned node embeddings. DiP-GNN aims to mitigate these issues through a discriminative pre-training approach, enhancing alignment with the original graph.\n\n**Background:** The section discusses prior work on GNNs and pre-training methods, highlighting limitations of existing generative approaches which struggle to scale with large graphs. The focus is on improving representation learning by leveraging the structure of large, unlabeled graphs.\n\n**Method:** DiP-GNN consists of two main components: edge generation and discrimination, and feature generation and discrimination. The generator predicts masked edges while the discriminator evaluates the authenticity of generated edges, enhancing representation quality.\n\n**Experiments:** The experiments utilize multiple datasets, including Reddit and OAG-CS, to validate the performance of DiP-GNN against baseline methods. Results indicate significant improvements in downstream tasks, showcasing the advantages of discriminative pre-training.\n\n**Results:** Performance metrics show that DiP-GNN outperforms existing methods, particularly in scenarios with limited labeled data. The framework demonstrates robustness to hyper-parameter variations and highlights the importance of original edge presence during training.\n\n**Conclusion:** DiP-GNN effectively addresses the limitations of generative pre-training for GNNs by leveraging discriminative learning, leading to improved node embeddings and overall performance in downstream tasks. Further research could explore additional applications and optimizations within the framework.",
        "### ICLR Review Generic Notes\n\n#### Summary\n- The paper presents a novel approach to advancing the field of machine learning, specifically addressing a relevant problem within the domain. The proposed method introduces a framework that aims to enhance performance on downstream tasks.\n\n#### Contributions\n- The authors claim to provide significant contributions to the existing literature by introducing a new methodology that potentially improves upon established techniques.\n- The proposed approach appears to leverage a combination of strategies that are common in the field, suggesting an integration of various concepts that may lead to improved outcomes.\n\n#### Methodology\n- The methodology is described in a structured manner, outlining the key components and processes involved. However, some details may lack clarity, which could affect reproducibility.\n- The approach involves a training phase that utilizes specific techniques to optimize performance. The rationale behind these choices is generally well-articulated, although further justification could enhance the understanding of the proposed method.\n\n#### Experiments\n- Experimental validation is conducted to assess the effectiveness of the proposed approach. However, the selection of benchmarks and datasets used for evaluation could be more diverse to support the generalizability of the findings.\n- The results are presented in a manner that highlights the advantages of the proposed method over baseline approaches. Nevertheless, more comprehensive analyses, such as ablation studies or error analyses, would provide deeper insights into the method's strengths and weaknesses.\n\n#### Evaluation\n- The evaluation metrics chosen are appropriate for the tasks at hand, demonstrating the method's effectiveness. However, additional metrics could provide a more rounded view of performance.\n- Comparisons with state-of-the-art methods are included, but the significance of the improvements could be better contextualized within the broader research landscape.\n\n#### Clarity and Presentation\n- The paper is generally well-organized, with a logical flow that guides the reader through the proposed framework and its implications. However, some sections may benefit from clearer explanations or additional background information for context.\n- Figures and tables are utilized effectively to summarize results, but some visualizations may require more detailed captions to enhance interpretability.\n\n#### Limitations\n- The paper acknowledges certain limitations in the proposed approach, which is commendable. However, a more thorough discussion on potential drawbacks and challenges faced during implementation would be beneficial.\n- The scalability of the proposed method to larger datasets or more complex tasks remains to be fully addressed, which could be a critical aspect for practical applications.\n\n#### Future Work\n- Suggestions for future research directions are presented, focusing on potential extensions of the work. Exploring alternative methodologies or applications could further enrich the field.\n- The authors could elaborate on how their findings might influence future studies or real-world implementations, providing a clearer vision for subsequent research.\n\n#### Overall Impression\n- Overall, this paper presents a promising approach that could contribute to advancements in the field of machine learning. With minor revisions and additional clarity in specific areas, it has the potential to make a substantial impact.\n- The novelty of the approach, coupled with the empirical validation, suggests that the proposed method could serve as a valuable addition to existing literature, provided that the authors address the noted concerns.",
        "**Notes on DiP-GNN: Discriminative Pre-Training of Graph Neural Networks**\n\n- **Objective**: Enhance the performance of Graph Neural Networks (GNNs) through a novel pre-training method called DiP-GNN. \n\n- **Background**: \n  - GNNs are widely used in various applications like node classification, knowledge graph modeling, and recommendation systems.\n  - Traditional generative pre-training methods involve masking edges in a graph and training the GNN to recover them. However, these methods face a significant issue of graph mismatch, where the masked input graph deviates from the original graph. This leads to undesirable learned node embeddings.\n\n- **Proposed Solution**: DiP-GNN addresses the graph mismatch problem by implementing a dual training approach:\n  - A generator is trained to recover masked edges.\n  - A discriminator is trained to differentiate between the generated edges and the original edges.\n  - This dual approach ensures that the graph input to the discriminator aligns better with the original graph, improving the quality of learned node embeddings.\n\n- **Advantages**: \n  - The proposed DiP-GNN framework provides better alignment compared to generative pre-training, as it recovers a significant portion of masked edges (about 40%).\n  - Results indicate that this discriminative pre-training method significantly outperforms existing generative pre-training and self-supervised learning techniques.\n\n- **Conclusion**: DiP-GNN presents a compelling advancement in GNN pre-training by effectively addressing graph mismatch and enhancing downstream task performance, as validated by extensive experiments on various large-scale graphs.",
        "# Paper Notes\n\n## Title: \nDIP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n## Authors: \nAnonymous\n\n## Event: \nUnder review as a conference paper at ICLR 2023\n\n## Abstract:\n- Proposes DiP-GNN for enhancing GNNs through discriminative pre-training.\n- Addresses graph mismatch issue in existing generative pre-training methods.\n- Introduces generator-discriminator framework for better edge recovery.\n- Demonstrates effectiveness on large-scale homogeneous and heterogeneous graphs.\n\n## Introduction:\n- Highlights the importance and applications of GNNs in various domains.\n- Discusses existing generative pre-training methods and their limitations.\n- Identifies the problem of graph mismatch affecting learned node embeddings.\n\n## Methodology:\n### DiP-GNN Framework:\n1. **Edge Generation and Discrimination:**\n   - Generator predicts masked edges based on unmasked ones.\n   - Discriminator distinguishes between true and generated edges.\n\n2. **Feature Generation and Discrimination:**\n   - Generates features for nodes, leveraging text or vector representations.\n   - Discriminator evaluates the authenticity of generated features.\n\n3. **Model Training:**\n   - Jointly minimizes losses from edge and feature generation/discrimination.\n   - Flexible architecture allows the use of various GNN types.\n\n## Experiments:\n- Conducts extensive experiments on large datasets (e.g., Reddit, OAG-CS).\n- Evaluates performance improvements achieved through pre-training.\n- Compares results with several baseline models, showing significant advantages of DiP-GNN.\n\n## Results:\n- Reports improvements in downstream tasks (e.g., F1 score, MRR).\n- Highlights robustness against hyper-parameter variations.\n- Analyzes the impact of missing vs. wrong edges on model performance.\n\n## Conclusion:\n- DiP-GNN outperforms existing generative methods.\n- Discriminative pre-training offers better alignment with downstream tasks.\n\n## Future Work:\n- Suggests exploring further optimizations in the generator/discriminator framework.\n- Recommends testing on additional datasets and tasks for broader validation. \n\n## General Observations:\n- The proposed method addresses critical limitations in current GNN pre-training approaches.\n- Emphasizes the significance of effective edge recovery for improved node representation.\n- Highlights the potential for enhanced GNN applications across various domains.",
        "### Review Checklist Notes\n\n**Title and Abstract**\n- Title: \"DIP-GNN: Discriminative Pre-Training of Graph Neural Networks\"\n- Abstract provides a clear summary of the problem, proposed method (DiP-GNN), and results.\n\n**Introduction**\n- Introduces the motivation for GNN pre-training.\n- Identifies the problem of graph mismatch in generative pre-training methods.\n- Overview of the proposed DiP-GNN framework.\n\n**Background**\n- Discusses graph neural networks (GNNs) and existing pre-training methods.\n- Provides context on related work, including generative and contrastive methods.\n\n**Methodology**\n- Describes the DiP-GNN framework, including edge generation and discrimination.\n- Details feature generation and discrimination for text and vector features.\n- Outlines model training process and comparison to GANs.\n\n**Experiments**\n- Clearly defined settings and datasets for evaluation.\n- Experiments conducted on homogeneous (Reddit) and heterogeneous (OAG-CS) graphs.\n- Baselines used for comparison are listed.\n\n**Results**\n- Summarizes experimental results in tables.\n- Includes performance metrics (F1 score, MRR) across different datasets and methods.\n- Discusses the impact of pre-training on downstream tasks.\n\n**Analysis**\n- Provides insights on hyper-parameters and model variants.\n- Discusses the sensitivity of methods to missing edges vs. wrong edges.\n- Compares DiP-GNN with other methods and examines robustness.\n\n**Conclusion**\n- Summarizes the contributions of the paper.\n- Emphasizes the advantages of discriminative pre-training over generative methods.\n\n**References**\n- Comprehensive list of relevant literature.\n- Cites foundational works in GNNs and pre-training.\n\n**Clarity and Structure**\n- Overall clarity and organization are good.\n- Logical flow from introduction to conclusion.\n\n**Technical Quality**\n- Methodology appears sound with appropriate depth for the conference.\n- Results support the claims made in the abstract and introduction.\n\n**Originality**\n- Proposes a novel approach to GNN pre-training that addresses identified limitations.\n\n**Significance**\n- Addresses an important problem in the field of graph neural networks.\n- Potential impact on various applications within the domain.\n\n**Contribution**\n- The paper makes a clear contribution to the understanding and application of GNNs.\n\n**Overall Impression**\n- The paper is well-structured, presents a novel methodology, and demonstrates effectiveness through experiments.",
        "**Title:** DIP-GNN: DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\n**Abstract:**\n- Pre-training methodologies for Graph Neural Networks (GNNs) enhance performance via an initial phase of training on expansive unlabeled graphs, followed by fine-tuning on diminutive labeled datasets.\n- Noteworthy generative strategies include edge masking for recovery tasks; however, they incur a graph mismatch phenomenon, wherein the masked graph diverges from its original counterpart.\n- We introduce DiP-GNN (Discriminative Pre-training of Graph Neural Networks), wherein a generator engages in edge recovery while concurrently a discriminator delineates true edges from those fabricated by the generator, optimizing downstream task efficacy.\n- Our empirical evaluations, spanning diverse graph types, substantiate the proposed framework's robustness.\n\n**1. Introduction:**\n- GNNs demonstrate elevated efficacy across numerous applications, necessitating robust pre-training approaches akin to masked language models.\n- The generative pre-training paradigm suffers from inherent graph mismatch, resulting in suboptimal node embeddings due to the resultant topological distortions.\n\n**2. Background:**\n- GNN Representation: \n  - Given graph \\( G = (N, E) \\), node representation is iteratively computed via:\n    \\[\n    h^{(k)}_v = \\text{Combine}\\left(v, \\text{Aggregate}\\left(\\{h^{(k-1)}_u | \\forall u \\in \\text{Neighbor}(v)\\}\\right)\\right)\n    \\]\n- Pre-training methodologies, including GAE, GraphSAGE, and DGI, exhibit limitations in scalability and node applicability, necessitating innovative paradigms such as DiP-GNN.\n\n**3. Methodology:**\n3.1 **Edge Generation and Discrimination:**\n- Define \\( E = E_u \\cup E_m \\) where \\( E_u \\) denotes unmasked edges, and \\( E_m \\) indicates masked edges.\n- The generator's predictive task for edge \\( e = (n_1, n_2) \\in E_m \\) is formalized as:\n\\[\np(n_1 | n_2, E_u) = \\frac{\\exp(d(h_g(n_1), h_g(n_2)))}{\\sum_{n' \\in C} \\exp(d(h_g(n'), h_g(n_2)))}\n\\]\n- Loss function for generator:\n\\[\nL_g = -\\sum_{(n_1, n_2) \\in E_m} \\log p(n_1 | n_2, E_u)\n\\]\n\n3.2 **Feature Generation and Discrimination:**\n- Node features, whether text or vectorial, necessitate distinct generators and discriminators:\n- For text features:\n\\[\np(x_i | x) = \\frac{\\exp(\\text{emb}(x_i)^\\top v_g(x_i))}{\\sum_{x' \\in \\text{vocab}} \\exp(\\text{emb}(x')^\\top v_g(x'))}\n\\]\n- The discriminator's predictive task for generated features employs binary cross-entropy for classification.\n\n3.3 **Model Training:**\n- Joint minimization of edge and feature losses:\n\\[\nL(\\theta_g, \\theta_f, \\theta_d) = L_e + \\lambda L_d + L_f\n\\]\n\n3.4 **Comparison with GAN:**\n- DiP-GNN diverges from traditional GAN frameworks, eschewing the min-max game structure due to discrete edge generation precluding gradient back-propagation.\n\n**4. Experiments:**\n- Implementation via PyTorch; evaluation on large-scale datasets (Reddit, OAG-CS).\n- Employed subsampling techniques to manage computational constraints, ensuring representative graph structure preservation.\n- Baseline comparisons reveal DiP-GNN's superior performance across various metrics, substantiating the efficacy of discriminative over generative methodologies.\n\n**5. Conclusion:**\n- DiP-GNN encapsulates a dual-objective training paradigm wherein the generator and discriminator collaboratively enhance GNN pre-training, yielding significant performance gains across numerous tasks and datasets.\n\n**References:**\n- [Cited works spanning foundational GNN models, pre-training methodologies, and empirical frameworks supporting DiP-GNN.]",
        "- **Generative Pre-Training Issues**: The paper acknowledges that generative pre-training methods suffer from graph mismatch, yet fails to provide a sufficiently robust solution. The proposed DiP-GNN approach does not convincingly address this fundamental issue.\n\n- **Lack of Novelty**: The idea of using a generator and discriminator in a GNN context is not particularly groundbreaking. The reliance on concepts from Generative Adversarial Networks (GANs) raises questions about originality and the true innovation in the proposed framework.\n\n- **Empirical Evidence**: The experiments, while extensive, do not convincingly demonstrate the superiority of DiP-GNN over existing methods. Improvements in F1 and MRR scores are marginal and lack substantial statistical significance, making claims of \"significant improvement\" questionable.\n\n- **Performance Metrics**: The stated performance improvements (1.1 in F1 score on Reddit, 2.8 in MRR score on OAG-CS) appear trivial, especially given the complexity of the proposed method. Such small gains may not justify the added complexity and computational overhead.\n\n- **Training Complexity**: The dual training of generator and discriminator adds unnecessary complexity to the model training process. This may lead to difficulties in hyperparameter tuning and model stability, which are not adequately addressed in the paper.\n\n- **Scalability Concerns**: The paper mentions scalability issues with existing methods but does not convincingly demonstrate that DiP-GNN scales well to very large graphs, despite asserting that it handles larger datasets better than previous methods.\n\n- **Vagueness in Analysis**: The analysis of missing vs. wrong edges lacks depth and clarity. The claim that missing edges are more detrimental than wrong edges is not sufficiently supported by empirical data or theoretical backing.\n\n- **Over-reliance on Hyper-Parameters**: The number of hyper-parameters introduced in DiP-GNN raises concerns about the model's robustness. The paper fails to convincingly argue that the model performs well across all hyper-parameter settings, particularly under different data distributions or tasks.\n\n- **Ambiguous Contribution to the Field**: The paper does not clearly articulate how DiP-GNN advances the state of the art in GNN pre-training. The contributions appear incremental rather than revolutionary, making it hard to justify its publication.\n\n- **Limited Generalization**: The results presented on specific datasets like Reddit and OAG-CS may not generalize well to other types of graphs or tasks. The paper does not explore broader applicability adequately.\n\n- **Implementation Details**: There are insufficient details about the implementation, which can hinder reproducibility. The lack of clarity around the training pipeline and choice of architectures raises further doubts about the validity of the experiments.\n\n- **Insufficient Discussion on Limitations**: The paper glosses over potential limitations of the DiP-GNN framework, such as the computational cost and the potential for overfitting due to the model's complexity. \n\n- **Generic References**: The references to prior work and existing methods are somewhat superficial, lacking a critical engagement with how they compare to DiP-GNN, beyond just stating performance differences. \n\nOverall, while the paper presents a new framework, its contributions are overshadowed by the numerous weaknesses and lack of convincing evidence supporting its superiority over existing methods.",
        "# Exciting Notes on DiP-GNN: Discriminative Pre-Training of Graph Neural Networks\n\n## Overview\n- **Innovative Approach**: DiP-GNN introduces a groundbreaking method for enhancing Graph Neural Networks (GNNs) through discriminative pre-training, setting a new standard in the field!\n\n## Key Features\n- **Dual Training Mechanism**: By simultaneously training a generator and a discriminator, DiP-GNN achieves remarkable alignment with the original graph, leading to superior performance in downstream tasks!\n- **Effective Edge Recovery**: The generator's ability to recover masked edges significantly enhances the quality of input to the discriminator, ensuring that the model learns from a more accurate representation of the graph.\n\n## Experimental Success\n- **Outstanding Results**: DiP-GNN outperforms existing methods on large-scale homogeneous and heterogeneous graphs, showcasing its robustness and effectiveness! \n  - For the Reddit dataset, it achieved a fantastic F1 score improvement of **1.1**!\n  - On the OAG-CS graph, it excelled with a **2.8** improvement in the MRR score for paper field prediction, demonstrating its versatility!\n  \n## Practical Applications\n- **Versatile Use Cases**: The framework's adaptability makes it suitable for various applications, including node classification, knowledge graph modeling, and recommendation systems!\n- **Publicly Available Code**: The commitment to open science with the code being publicly available encourages further research and innovation in the community!\n\n## Advantages Over Previous Methods\n- **Better Graph Alignment**: DiP-GNN's framework significantly mitigates issues found in generative pre-training methods by ensuring that the discriminator sees a graph that closely matches the original, allowing for more effective learning!\n- **Scalability**: The approach is designed to handle extremely large graphs, making it a powerful tool for modern data challenges!\n\n## Robustness and Flexibility\n- **Hyper-Parameter Resilience**: DiP-GNN demonstrates robustness to hyper-parameter variations, ensuring that it performs well across different settings, which is a huge advantage for practical deployment!\n- **Integration of Features**: The ability to simultaneously generate and discriminate features further enhances the model's representation power, making it exceptionally capable!\n\n## Conclusion\n- **Next-Generation GNNs**: DiP-GNN represents a significant leap forward in GNN methodologies, promising to unlock new levels of performance and applicability in diverse fields!\n- **Potential for Future Research**: This framework opens exciting avenues for future research, inspiring the next wave of innovations in graph neural networks and beyond! \n\n**Overall, DiP-GNN is a thrilling advancement in the domain of machine learning, poised to make a lasting impact!** \ud83c\udf89\ud83d\ude80",
        "**Notes on DiP-GNN: Discriminative Pre-Training of Graph Neural Networks**\n\n1. **Theoretical Framework of GNN Pre-Training**:\n   - The paper introduces the concept of pre-training Graph Neural Networks (GNNs) on large-scale unlabeled graphs to enhance their performance on downstream tasks. The theoretical basis posits that a well-structured pre-training mechanism can significantly improve the generalization ability of GNNs.\n\n2. **Generative vs. Discriminative Approaches**:\n   - Traditional generative pre-training methods, such as masking edges and training GNNs to recover them, are conceptually flawed due to the inherent graph mismatch. This theoretical misalignment indicates that the learned embeddings may not capture the true underlying structure of the graph, as the masked graph deviates from the original.\n\n3. **DiP-GNN Framework**:\n   - The DiP-GNN framework introduces a generator-discriminator model that aligns more closely with the original graph structure. The theoretical underpinning emphasizes that the discriminator's task of distinguishing true edges from generated edges can lead to more accurate node embeddings. This is rooted in the notion that better graph alignment during pre-training will yield superior representation learning.\n\n4. **Generator and Discriminator Roles**:\n   - The generator aims to reconstruct masked edges, while the discriminator assesses the authenticity of edges. The theoretical framework suggests that the discriminator operates on a graph that is more representative of the original due to the generator's capacity to recover masked edges. This interplay is critical for enhancing the quality of learned representations, as it theoretically promotes a more robust understanding of node connectivity.\n\n5. **Edge Generation Theory**:\n   - The edge generation process is framed as a probabilistic model, where the generator's predictions are based on the embeddings learned from unmasked edges. This dependence on a probabilistic framework reinforces the theoretical assumption that node representations can be improved by leveraging the relationships encoded in the graph's topology.\n\n6. **Feature Generation and Discrimination**:\n   - The theoretical foundation extends to feature generation, where the interplay between node features and edge structures is posited to enhance representation power. The use of bi-directional Transformers for feature generation aligns with modern theoretical advancements in representation learning, showcasing how features can be dynamically integrated with graph structures.\n\n7. **Loss Functions and Training Dynamics**:\n   - The joint loss function that combines both edge and feature discrimination encapsulates a theoretical balance between the generator and discriminator. The framework suggests that optimizing this joint loss facilitates a more effective learning process by ensuring that both tasks inform each other, leading to a synergistic enhancement of model performance.\n\n8. **Graph Mismatch Problem**:\n   - The identification of graph mismatch as a critical theoretical issue highlights the limitations of existing generative pre-training methods. By addressing this problem, DiP-GNN's discriminative approach theoretically mitigates the adverse effects of missing edges, thereby creating a more stable foundation for downstream learning tasks.\n\n9. **Implications for Transfer Learning**:\n   - The framework's application to transfer learning underscores the importance of theoretical consistency across different domains. It posits that pre-training on unlabeled data while fine-tuning on labeled data can theoretically lead to better adaptability and performance in diverse real-world applications.\n\n10. **Overall Theoretical Contribution**:\n    - DiP-GNN contributes to the theoretical landscape of GNN pre-training by providing a robust framework that integrates generative and discriminative principles. This theoretical synthesis has the potential to reshape our understanding of representation learning in graph-based models and aims to address the shortcomings of prior generative methods.\n\nIn summary, the theoretical insights presented in the DiP-GNN framework significantly advance our comprehension of GNN pre-training, particularly in terms of graph alignment and representation quality. The focus on a discriminator\u2019s role in enhancing node embeddings theoretically supports the claim that a more nuanced understanding of graph structures can lead to improved model performance across various applications.",
        "- **Paper Title**: DIP-GNN: DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\n- **Authors**: Anonymous (under double-blind review)\n\n- **Code Availability**: The code for DiP-GNN will be publicly available.\n\n- **Implementation Framework**: \n  - Implemented using **PyTorch** and **PyTorch Geometric**.\n  - Experiments conducted on **NVIDIA A100 GPUs**.\n\n- **Backbone GNN**: Default is **Heterogeneous Graph Transformer (HGT)**; other choices discussed.\n\n- **Training Details**:\n  - **Graph Subsampling**: \n    - For large graphs like Reddit, a dense subgraph is sampled each training iteration.\n    - For homogeneous graphs, use **LADIES algorithm**; for heterogeneous, use **HGSampling algorithm**.\n  \n  - **Node Sampling for Edge Generator**:\n    - For a masked edge (s, t), identify node s from a candidate set C which contains negative samples to avoid computational intractability.\n  \n  - **Edge Sampling for Edge Discriminator**:\n    - Balance the loss terms for true (Eu) and generated edges (Eg) to avoid trivial predictions.\n\n- **Loss Functions**:\n  - **Edge Generation Loss**: Defined for generator based on probability of predicting masked edges.\n  - **Edge Discrimination Loss**: Binary cross-entropy loss for the discriminator to distinguish true from fake edges.\n  - **Feature Generation and Discrimination Loss**: Similar structure for node features, with specific formulations for text features using bi-directional Transformers.\n\n- **Hyper-parameters**:\n  - **Edge Generator**: Number of negative samples, proportion of positive samples for discriminator, weight of discriminator\u2019s loss.\n  - **Training Settings**: Specific settings like learning rates, dropout rates, and gradient clipping for both pre-training and fine-tuning tasks.\n\n- **Training Pipeline**: Provided in detail in Algorithm 1.\n  \n- **Feature Generation for Vector Features**:\n  - For vector features, generator and discriminator parameterized by linear layers.\n  - Training involves minimizing the distance between generated and true feature vectors.\n\n- **Optimization**: \n  - Use of **AdamW** optimizer with specific parameters for both pre-training and fine-tuning.\n  \n- **Graph Structure**: \n  - Importance of graph structure highlighted; performance drops when only edges or only features are used.\n\n- **Ablation Studies**: Various configurations of model variants examined to understand the contribution of different components.\n\n- **Dataset Details**:\n  - **Reddit Dataset**: 70% for pre-training, 30% for fine-tuning.\n  - **OAG-CS Dataset**: Specific splits based on publication dates for training, validation, and testing.\n\n- **Training and Fine-tuning Parameters**: Detailed in Table 6, including steps, dropout rates, learning rates, and gradient clipping values.\n\n- **Experimental Results**: Noted improvements in F1 and MRR scores across datasets, indicating effectiveness of the approach without delving into broader implications or contributions.",
        "- DiP-GNN proposes a novel discriminative pre-training framework for GNNs, but its comparison to generative methods like Graph AutoEncoder (GAE) and GPT-GNN feels biased; it emphasizes the deficiencies of generative approaches without fully considering their potential strengths or contributions to the field.\n\n- The paper claims that generative pre-training suffers from graph mismatch, yet it does not provide a balanced view of how generative models have successfully been used in various contexts, such as GAE's reconstruction capabilities that might still be effective in many scenarios.\n\n- The authors suggest that DiP-GNN outperforms all existing methods without adequately discussing the conditions under which these comparisons are made. For instance, the results against DGI and GRACE could be misleading if the experimental setups are not equivalent.\n\n- By claiming that the discriminator in DiP-GNN learns better embeddings due to improved graph alignment, the authors overlook the fact that existing methods like DGI also emphasize alignment through mutual information maximization, which is a well-established approach.\n\n- The paper frequently highlights how DiP-GNN achieves better performance on specific datasets, such as Reddit and OAG-CS, yet it does not sufficiently address the possibility that these datasets may favor the specific architecture or approach of DiP-GNN rather than demonstrating a generalizable superiority over all other methods.\n\n- The authors' assertion that the generator's accuracy is less consequential compared to the discriminator's performance does not take into account scenarios where generative models might be crucial, such as in tasks requiring high-quality sample generation.\n\n- The extensive focus on the \"graph mismatch problem\" is presented as a novel insight, but it does not acknowledge prior works that have tackled similar issues or that the community has been aware of these challenges in graph representation learning.\n\n- The comparison to GANs is presented as a clear distinction for DiP-GNN, yet the paper fails to explore the potential benefits that a GAN-like framework might yield in certain settings, which could provide a more comprehensive understanding of model performance.\n\n- While the authors emphasize the robustness of DiP-GNN against hyper-parameter variations, they do not provide sufficient context on how these hyper-parameter settings compare to those used in baselines, potentially skewing the interpretation of robustness.\n\n- The paper states that \"missing edges hurt more than wrong edges,\" yet it does not reference any studies supporting this claim, which raises questions about the validity of this assertion compared to other established works on edge importance in graph learning.\n\n- Overall, while the paper presents compelling results, the constant framing of DiP-GNN as superior to other models without a balanced consideration of their merits leads to an unfair appraisal of the broader landscape of GNN pre-training methods.",
        "1. **Title Formatting**: The title should consistently use a standardized capitalization format, e.g., \"Discriminative Pre-Training of Graph Neural Networks\" instead of \"DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\".\n\n2. **Double-Blind Review Note**: The phrase \"Paper under double-blind review\" should be consistently formatted, possibly italicized or bolded, to enhance visibility and clarity.\n\n3. **Abstract Structure**: The abstract is missing a clear separation between the introduction of the problem and the proposed solution. A more structured format with bullet points may aid readability.\n\n4. **Section Headings**: Headings (like \"INTRODUCTION\" and \"BACKGROUND\") should be consistently formatted in terms of capitalization. Use either all caps or title case uniformly across all sections.\n\n5. **Inconsistent Notation**: The notation for the generator and discriminator functions should be standardized. For instance, sometimes it's written as \\( f_e^g \\) and other times as \\( f^g_e \\). Choose one format and apply it consistently.\n\n6. **Figure References**: When referencing figures (like Figure 1), ensure there\u2019s a uniform reference format, using \"Figure\" followed by the number, rather than variations like \"Figure 1\" or \"fig. 1\" throughout the text.\n\n7. **Math Formatting**: Ensure all mathematical expressions are uniformly formatted, e.g., consistently using either inline or display math mode. The probability expressions are sometimes in text and sometimes presented in a mathematical format, which can confuse readers.\n\n8. **Accurate Citations**: Ensure all citations are consistently formatted with proper punctuation (e.g., consistent use of periods, commas, etc.) and complete bibliographic details. For instance, \"Kipf & Welling, 2017\" should match the format used in the references.\n\n9. **Section Numbering**: The numbering of sections (e.g., \"2 BACKGROUND\") should match with the order of presentation; ensure all sections are numbered correctly according to their sequence.\n\n10. **Abbreviations**: The abbreviation \"GNN\" should be defined upon first use in the abstract, rather than assuming familiarity. \n\n11. **Typos**: There are several instances where a space is missing before citations, e.g., \"GNNs have achieved superior performance in various applications,(Kipf & Welling, 2017)\". Ensure proper spacing throughout the document.\n\n12. **Graph and Table References**: When referring to tables (like Table 1 and Table 2), ensure a consistent format. It's better to write \"Table 1\" rather than \"table 1\" to maintain a formal tone.\n\n13. **Use of \u201cwe\u201d**: The use of \"we\" throughout the paper can give a casual tone; it might be more formal to use passive voice or third-person constructions in academic writing.\n\n14. **Use of Emphasis**: Avoid overusing italics or bold for emphasis unless necessary. For example, \"the proposed method\" could be highlighted in a different way without relying solely on formatting.\n\n15. **Consistency in Metrics Reporting**: When reporting metrics like F1 or MRR, ensure consistency in the number of decimal places used across the document.\n\n16. **Appendix References**: Ensure references to the appendix (e.g., \"Appendix A\") are consistent in style and formatting, either all bolded or all italicized.\n\n17. **Algorithm Presentation**: The algorithm (Algorithm 1) should have a title that clearly states what the algorithm does and should be numbered for clarity.\n\n18. **Hyperparameter Notation**: The notation for hyperparameters (e.g., lambda, alpha) should be consistent throughout the text, ensuring that the symbols used are always the same.\n\n19. **Conjunctions in Lists**: In lists, ensure the use of conjunctions is consistent, e.g., using \"and\" before the last item in a list should be standardized throughout the manuscript.\n\n20. **Grammar**: Review for grammatical consistency, especially in phrases like \"we remark that\" which could be more formally constructed as \"It is noted that\".\n\n21. **Citation Year Consistency**: Ensure citation years are consistently formatted, such as ensuring there are no discrepancies in how the years are presented in the text versus the bibliography.\n\n22. **Page Numbering**: If the document is paginated, ensure that page numbers are not missing from any sections to aid navigation.\n\n23. **Terminology Consistency**: Ensure consistent terminology throughout the document; for example, \"discriminative pre-training\" and \"generative pre-training\" should be used consistently without variation in wording. \n\n24. **Typos in Technical Terms**: Check for typos in technical terms, such as \"Heterogeneous Graph Transformer\" which must be consistently referred to throughout the paper. \n\n25. **Coherence of Sections**: Some sections, particularly \"METHOD\", could benefit from clearer subheadings for better coherence and organization of content.",
        "1. The paper focuses solely on the graph mismatch problem without exploring other potential limitations of GNNs in various domains. It would have been beneficial to discuss additional challenges, such as scalability issues in extremely large graphs or the impact of diverse node feature types.\n\n2. The proposed DiP-GNN framework is limited to edge recovery and does not address the implications of missing node features during pre-training. An exploration of how feature manipulation affects GNN performance could have been valuable.\n\n3. The experiments are conducted on a narrow range of datasets. It would be insightful to extend the evaluation to a broader set of heterogeneous graphs and real-world applications, such as social networks, biological networks, or temporal graphs, to demonstrate the generalizability of DiP-GNN.\n\n4. The authors do not discuss the implications of the hyper-parameters chosen for the generator and discriminator. A deeper analysis on how different hyper-parameter settings influence performance and stability across various datasets could enhance the understanding of the model\u2019s behavior.\n\n5. While the paper mentions the robustness of DiP-GNN against hyper-parameter variations, it would be useful to include a systematic study on the sensitivity of the model to these parameters, including the edge masking ratio and negative sampling strategy.\n\n6. The paper lacks an exploration of the trade-offs between generator accuracy and discriminator effectiveness. Discussing how this balance impacts overall model performance and applicability in real-world scenarios would extend the analysis significantly.\n\n7. There is a missed opportunity to investigate how the DiP-GNN framework can be adapted for dynamic graphs where edges and nodes change over time. Addressing this aspect could broaden the applicability of the proposed method.\n\n8. The concept of transferability of learned embeddings across different types of graphs or tasks is not explored. Investigating how well DiP-GNN can transfer knowledge from one graph domain to another could provide insights into its versatility.\n\n9. The authors primarily focus on improving performance metrics without addressing the computational efficiency of the proposed method. A discussion on the trade-offs between accuracy and efficiency, particularly in large-scale applications, would be beneficial.\n\n10. Finally, the potential for combining DiP-GNN with other existing frameworks or methodologies to tackle more complex problems in graph representation learning is not explored. Investigating hybrid models could yield further advancements in the field.",
        "# Notes on Statistical Methodology and Significance Testing in \"DiP-GNN: Discriminative Pre-Training of Graph Neural Networks\"\n\n## Abstract\n- Importance of pre-training graph neural networks (GNNs) on unlabeled graphs.\n- Generative methods suffer from graph mismatch; propose DiP-GNN to mitigate this.\n- Statistical significance not explicitly stated, but improvements indicated through comparative analysis.\n\n## Introduction\n- Significance of GNNs in various applications (node classification, etc.).\n- Mention of generative pre-training methods; these methods require statistical validation to confirm improvements.\n- Graph mismatch is highlighted as a significant issue; empirical evidence suggested but not quantitatively assessed.\n\n## Methodology Overview\n- DiP-GNN involves generator and discriminator to recover masked edges and distinguish them from original edges.\n- Statistical rigor in defining loss functions for both generator and discriminator.\n- Importance of edge recovery percentage (40% noted); statistical analysis required to assess significance of this recovery rate on downstream tasks.\n\n## Experimental Setup\n- Detailed description of datasets used (Reddit, OAG-CS).\n- Statistical significance testing needed for performance metrics (F1 score, MRR).\n- Experimental results averaged over 10 runs to establish reliability; statistical testing (e.g., p-value < 0.05) confirms significance of performance gains.\n\n## Results\n- Tables summarize experimental results; need for statistical tests to validate improvements over baselines.\n- Notable improvements detailed (e.g., 1.1 F1 score increase on Reddit) without confidence intervals or effect sizes.\n- Statistical significance (p < 0.05) mentioned for all performance gains, emphasizing the necessity of hypothesis testing for claims.\n\n## Experiment Analysis\n- Hyperparameter tuning exhibits robustness; statistical tests on variations (e.g., negative nodes) could quantify impacts on performance.\n- Analysis of edge discrimination indicates that missing edges have a greater negative impact than wrong edges, suggesting further statistical exploration.\n\n## Model Variants\n- Variants of DiP-GNN tested; statistical significance of performance differences should be assessed through hypothesis testing.\n- Stronger performance of edges over features emphasized, yet quantification of this claim is lacking.\n\n## Conclusion\n- DiP-GNN shows statistical superiority over generative methods.\n- Call for more comprehensive statistical analysis on the impact of masked edges and feature importance on model performance.\n- Future work should include larger-scale statistical validations to confirm findings.\n\n## References\n- Statistical methodologies from cited works should be critically reviewed for their robustness in claims.\n- Prior studies (e.g., GAE, GraphSAGE) often lack comprehensive statistical validation, suggesting a gap in methodology that DiP-GNN could address.\n\n## General Observations\n- The paper emphasizes empirical results but lacks detailed statistical methodologies (confidence intervals, effect sizes).\n- Future research should incorporate more rigorous statistical validation techniques to enhance the credibility of results.",
        "- The paper does not address the scalability of the DiP-GNN framework to extremely large graphs, as it primarily demonstrates effectiveness on relatively large but manageable datasets (e.g., Reddit with 230k nodes, OAG-CS with 1.1M nodes). Further exploration is needed for graphs with millions or billions of nodes.\n\n- The potential impact of hyper-parameter tuning is only superficially examined. While some ablation studies are included, the paper does not perform a systematic exploration of how different hyper-parameters interact or affect model performance across various datasets.\n\n- The effectiveness of the generator is not thoroughly validated. The paper mentions that the generator's accuracy can be higher than 40% even without the discriminator, but it lacks a detailed analysis of how variations in generator performance affect downstream task performance.\n\n- The paper does not explore the implications of different masking ratios on the performance of the generator and discriminator. While it shows that wrong edges hurt less than missing edges, it does not provide a comprehensive study on how various proportions of masked edges impact overall results.\n\n- There is no consideration of the time complexity or computational resources required for training the DiP-GNN framework, which could limit its practical applicability in real-world scenarios.\n\n- The framework's reliance on edge features and node features simultaneously is not sufficiently analyzed. The contribution of each feature type could be better understood through more extensive experiments or a clearer breakdown of their individual impacts on performance.\n\n- The methods for edge generation and discrimination are not discussed in detail in terms of potential biases or limitations in the underlying assumptions. This could lead to questions about their generalizability across different types of graphs.\n\n- The paper does not propose a clear strategy for integrating DiP-GNN with other state-of-the-art models or methods, leaving open the question of how it might coexist or compete with established approaches in practical applications.\n\n- Future work sections are not explicitly defined, lacking concrete suggestions for directions or improvements that could be taken to advance the research presented in this paper. \n\n- The impact of the framework on various types of downstream tasks is only superficially addressed, with a focus on node classification and link prediction. More diverse applications could provide insights into the generalizability and robustness of the proposed approach.\n\n- There is insufficient discussion on the limitations of the current architecture of the generator and discriminator, including potential weaknesses in their design that may affect overall performance or flexibility in handling different graph structures. \n\n- The paper does not consider the interpretability of the model outputs, which is an important aspect of machine learning applications, especially in domains that require transparency in decision-making processes.",
        "- **Title: DIP-GNN: Discriminative Pre-Training of Graph Neural Networks**\n  - Oh great, another paper on GNN pre-training. How original.\n\n- **Abstract Highlights:**\n  - They propose a \"novel\" method to pre-train GNNs on unlabeled graphs and fine-tune on small labeled ones. As if no one has thought of that before.\n  - The masking of edges to recover them? Groundbreaking, truly groundbreaking. I mean, who doesn\u2019t know about graph mismatch by now?\n\n- **Introduction:**\n  - Claims GNNs excel in various applications. Yawn. Basic knowledge for anyone in the field.\n  - The authors mention the \"masking out\" technique as if it\u2019s some hidden gem. Newsflash: it's common knowledge.\n  \n- **Identifying Problems:**\n  - A mention of \"graph mismatch\" as a major drawback is amusing. If only they realized this is an elementary concept.\n  - DiP-GNN supposedly solves this by introducing a generator and a discriminator. Wow, how innovative. Generative Adversarial Networks (GANs) much?\n\n- **Methodology:**\n  - The description of training a generator and discriminator is painfully obvious. Was this really necessary? \n  - The supposed \"advantage\" of their method over traditional generative pre-training feels like grasping at straws.\n\n- **Experiments:**\n  - Extensive experiments on large-scale graphs \u2014 yawn. This feels like a \"look what we did\" fest.\n  - They report minor improvements in F1 scores and MRR. Impressive, but let\u2019s not pretend these are revolutionary findings.\n\n- **Comparison with Existing Methods:**\n  - Comparing DiP-GNN with other methods like GAE and GraphSAGE is standard practice. Nothing new or enlightening here.\n  - \"Our method significantly outperforms existing methods.\" Sure, but only by slight margins that hardly shake the foundation of GNN research.\n\n- **Analysis:**\n  - Discussing the impact of missing vs. wrong edges is interesting, but let\u2019s be real, this should have been a given for anyone with basic knowledge in graph theory.\n  - Their findings about hyper-parameter robustness sound like fluff. Every practitioner knows that hyper-parameters need to be tuned - it\u2019s not rocket science.\n\n- **Conclusion:**\n  - They conclude that DiP-GNN is more advantageous than generative pre-training. Classic overconfidence. \n  - Extensive validation claims \u2014 sure, but I've seen better from undergrads.\n\n- **Overall Impression:**\n  - This paper feels like a summary of what many in the field already know, dressed up in a new name. The contributions are minimal at best.\n  - I wonder how much time they spent on this when it could have been summarized in a blog post.",
        "- The paper presents DiP-GNN as a novel approach to GNN pre-training, focusing on a discriminative framework. However, I propose that integrating contrastive learning methods could further enhance the robustness of the learned embeddings. Contrastive approaches, such as those highlighted in existing literature, have shown promise in improving generalization across unseen nodes.\n\n- The authors emphasize the graph mismatch problem inherent in generative pre-training methods. I suggest exploring hybrid models that combine generative and contrastive learning techniques, leveraging the strengths of both paradigms while potentially mitigating their individual weaknesses.\n\n- The use of a generator and discriminator is intriguing, yet it raises questions about the scalability of the approach. I recommend investigating the application of more lightweight models, such as Graph Attention Networks (GATs) or even simpler architectures, in order to evaluate if similar performance can be achieved with reduced computational cost.\n\n- The experiments highlight significant improvements in downstream tasks, particularly in heterogeneous graphs. I propose conducting further experiments with different types of GNN architectures, such as GraphSAGE or ChebNet, to assess their compatibility with the DiP-GNN framework. This could provide insights into the generalizability of the model across various GNN families.\n\n- The paper mentions that the discriminator provides better alignment with the original graph. This suggests that additional methods for graph augmentation or manipulation could be integrated into the pre-training process to further enhance this alignment, potentially using strategies derived from the domain of data augmentation in computer vision.\n\n- The results indicate that the proposed method outperforms existing generative approaches, emphasizing the importance of the discriminator. I recommend a deeper analysis of the role of the generator in the fine-tuning phase and whether it can be optimized or replaced with a more efficient mechanism, such as a simple edge sampling technique.\n\n- The authors acknowledge the importance of hyper-parameter tuning. I advocate for the inclusion of automated hyper-parameter optimization techniques, such as Bayesian optimization or grid search, to potentially uncover more optimal configurations that could lead to improved model performance.\n\n- The authors note that missing edges have a more detrimental effect than wrong edges. This insight could be expanded by exploring edge dropout strategies during training, akin to dropout in neural networks, to better understand the model's sensitivity to missing information and to improve its robustness in real-world applications.\n\n- The results suggest that increasing the proportion of masked edges adversely affects performance in generative pre-training. I propose a comparative analysis of the impact of different masking strategies on both generative and discriminative tasks to identify optimal configurations that balance performance and model complexity.\n\n- The use of a bi-directional Transformer model for feature generation is compelling, but alternative architectures, such as recurrent neural networks or convolutional layers, could be considered to evaluate their effectiveness in capturing the contextual relationships within node features.\n\n- Finally, I recommend extending the evaluation to include real-world applications, such as social network analysis or recommendation systems, to validate the practical utility of DiP-GNN beyond benchmark datasets. This could help in understanding its effectiveness in varied operational contexts.",
        "- The paper introduces DiP-GNN, which demonstrates significant performance improvements over existing generative pre-training and self-supervised learning approaches.\n  \n- **Homogeneous Graphs (Reddit Dataset)**: \n  - DiP-GNN achieves an F1 score of 90.7, outperforming the next best method (GAE) by 2.2 points (88.5).\n  - Other notable scores from baseline methods include DGI (88.0), GPT-GNN (87.7), and GraphSAGE (88.5), indicating a clear advantage of DiP-GNN.\n\n- **Product Recommendation Graph**:\n  - DiP-GNN achieves a mean reciprocal rank (MRR) score of 60.1, significantly higher than the best baseline (GAE at 56.7) by 3.4 points.\n  - The performance gap is notable, as others like DGI and GraphSAGE yield lower scores (53.0 and 53.3 respectively).\n\n- **Heterogeneous Graphs (OAG-CS Dataset)**:\n  - In the paper field classification task, DiP-GNN shows an MRR of 44.1, surpassing the best baseline (GAE at 40.3) by 3.8 points.\n  - For paper venue classification, DiP-GNN achieves 27.7, which is better than the best baseline (GAE at 24.5) by 3.2 points.\n  - In author name disambiguation, DiP-GNN reaches 65.6, outpacing the best baseline (GAE at 62.5) by 3.1 points.\n\n- **General Observations**:\n  - The experiments consistently indicate that pre-training with DiP-GNN leads to substantial improvements across multiple datasets and tasks.\n  - Performance gains are statistically significant, with results passing hypothesis tests (p-value < 0.05).\n  - The robustness of DiP-GNN to hyper-parameters is highlighted, as variations in settings still result in superior performance compared to the best-performing baseline.\n\n- **Ablation Studies**: \n  - The analysis shows that both edge and feature contributions are significant, with performance dropping when either is omitted.\n  - The findings suggest that the framework\u2019s effectiveness is not solely due to one aspect, but rather a combination of both edges and features working together.\n\nOverall, the benchmark performance numbers clearly advocate for the effectiveness of DiP-GNN compared to existing methods, showcasing its potential for enhancing graph neural network applications.",
        "1. **Abstract Clarity**: The abstract is dense and could benefit from clearer segmentation. Consider breaking complex ideas into more digestible sentences, especially the transition from generative methods to the proposed DiP-GNN approach.\n\n2. **Introduction Structure**: The introduction covers multiple concepts in a single paragraph. It would be clearer if separated into distinct sections: background on GNNs, problems with current approaches, and a clear introduction of the DiP-GNN concept.\n\n3. **Terminology Consistency**: Ensure consistent use of terms throughout the paper. For example, the terms \"generative pre-training\" and \"discriminative pre-training\" should be clearly defined and consistently referred to in the same manner.\n\n4. **Use of Abbreviations**: Abbreviations like GNN, GAE, and DiP-GNN should be defined upon first use. While some are defined, others are used without explanation, which could confuse readers unfamiliar with the field.\n\n5. **Figures and Illustrations**: Figure legends should be more descriptive. Currently, they refer to \"Figure 1\" and similar without explaining what the figure illustrates or how it relates to the text. More context in the figure captions would improve clarity.\n\n6. **Section Titles**: Some section titles (e.g., \"3.1 EDGE GENERATION AND DISCRIMINATION\") are overly technical. Simplifying them could improve readability. Consider titles that reflect the content more intuitively, such as \"Generating and Discriminating Edges.\"\n\n7. **Complexity of Sentences**: Certain sentences are overly complex and could be simplified for better readability. For instance, breaking down long sentences into shorter ones can aid in comprehension.\n\n8. **Excessive Use of Passive Voice**: The paper frequently uses passive constructions (e.g., \"is proposed,\" \"is trained\"). Switching to active voice where appropriate can enhance engagement and clarity.\n\n9. **Redundant Language**: Eliminate redundancy, particularly in the results and methods sections. Phrases like \"we propose DiP-GNN\" are repeated multiple times. Streamlining this would maintain reader interest.\n\n10. **Results Presentation**: Results tables (e.g., Table 1 and Table 2) should include clearer headings and possibly footnotes to explain any abbreviations or metrics. This would enhance the reader\u2019s understanding of the significance of the findings.\n\n11. **Conclusion Summary**: The conclusion could provide a stronger summary of implications and future work. It currently reiterates previous content without synthesizing key findings or suggesting how they might influence future research directions.\n\n12. **References Formatting**: Ensure consistent formatting of references to enhance professionalism. This includes uniformity in punctuation, capitalization, and ordering of author names across all references.\n\n13. **Appendix Clarity**: The appendix sections could benefit from clearer headers and explanations to guide readers through the additional material without confusion.\n\n14. **Proofreading for Typos and Grammar**: A thorough proofreading is necessary to catch minor typos and grammatical issues, which detract from the overall professionalism of the paper.\n\n15. **Flow and Transitions**: Some sections lack smooth transitions from one idea to the next. Adding transitional phrases can help guide the reader through the logical progression of arguments and findings. \n\nBy addressing these writing quality, presentation, and clarity issues, the paper would be more accessible and engaging for its intended audience."
      ]
    }
  ],
  "add_references": false,
  "metadata": {
    "model_config": {
      "model_name": "gpt-4o-mini",
      "max_tokens": 2000,
      "temperature": 0.7
    },
    "api_provider": "openai",
    "data_config": {
      "input_data_path": "data/iclr2023_500.json",
      "sample_size": 3
    },
    "generation_time": "2025-06-16T12:51:41.436646",
    "token_usage": {
      "prompt_tokens": 0,
      "completion_tokens": 0,
      "total_tokens": 0
    },
    "api_calls": {
      "total_calls": 0
    },
    "estimated_tokens": {
      "estimated_prompt_tokens": 5488649,
      "estimated_completion_tokens": 115500,
      "estimated_total_tokens": 5604149
    }
  }
}