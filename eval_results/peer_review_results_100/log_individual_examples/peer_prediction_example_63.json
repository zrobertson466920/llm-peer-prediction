{
  "example_idx": 63,
  "reference": "Under review as a conference paper at ICLR 2023\n\nA REINFORCEMENT LEARNING APPROACH TO ESTIMATING LONG-TERM EFFECTS IN NONSTATIONARY ENVIRONMENTS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRandomized experiments (a.k.a. A/B tests) are a powerful tool for estimating treatment effects, to inform decisions making in business, healthcare and other applications. In many problems, the treatment has a lasting effect that evolves over time. A limitation with randomized experiments is that they do not easily extend to measure long-term effects, since running long experiments is time-consuming and expensive. In this paper, we take a reinforcement learning (RL) approach that estimates the average reward in a Markov process. Motivated by real-world scenarios where the observed state transition is nonstationary, we develop a new algorithm for a class of nonstationary problems, and demonstrate promising results in two synthetic datasets and one online store dataset.\n\n1\n\nINTRODUCTION\n\nRandomized experiments (a.k.a. A/B tests) are a powerful tool for estimating treatment effects, to inform decisions making in business, healthcare and other applications. In an experiment, units like customers or patients are randomly split into a treatment bucket and a control bucket. For example, in a rideshare app, drivers in the control and treatment buckets are matched to customers in different ways (e.g., with different spatial ranges or different ranking functions). After we expose customers to one of these options for a period of time, usually a few days or weeks, we can record the corresponding customer engagements, and run a statistical hypothesis test on the engagement data to detect if there is a statistically significant difference in customer preference of treatment over control. The result will inform whether the app should launch the treatment or control.\n\nWhile this method has been widely successful (e.g., in online applications (Kohavi et al., 2020)), it typically measures treatment effect during the short experiment window. However, in many problems, a treatment has a lasting effect that evolves over time. For example, a treatment that increases installation of a mobile app may result in a drop of short-term profit due to promotional benefits like discounts. But the installation allows the customer to benefit from the app, which will increase future engagements and profit in the long term. A limitation with standard randomized experiments is that they do not easily extend to measure long-term effects. We can run a long experiment for months or years to measure the long-term impacts, which however is time-consuming and expensive. We can also design proxy signals that are believed to correlate with long-term engagements (Kohavi et al., 2009), but finding a reliable proxy is challenging in practice. Another solution is the surrogacy method that estimates delayed treatment impacts from surrogate changes during the experiment (Athey et al., 2019). However, it does not estimate long-term impacts resulting from long-term treatment exposure, but rather from short-term exposure during the experiment.\n\nShi et al. (2022b) mitigates the limitation of standard randomized experiment by framing the longterm effect as a reinforcement learning (RL) problem. Their method is closely related to recent advances in infinite-horizon off-policy evaluation (OPE) (Liu et al., 2018; Nachum et al., 2019a; Xie et al., 2019; Kallus & Uehara, 2020; Uehara et al., 2020; Chandak et al., 2021). However, their solution relies on stationary Markov assumption, which fails to capture the real-world nonstationary dynamics. Motivated by real-world scenarios where the observed state transitions are nonstationary, we consider a class of nonstationary problems, where the observation consists of two additive terms: an endogenous term that follows a stationary Markov process, and an exogenous\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nterm that is time-varying but independent of the policy. Based on this assumption, we develop a new algorithm to jointly estimate long-term reward and the exogenous variables.\n\nOur contributions are threefold. First, it is a novel application of RL to estimate long-term treatment effects, which is challenging for standard randomized experiments. Second, we develop an estimator for a class of nonstationary problems that are motivated by real-world scenarios, and give a preliminary theoretical analysis. Third, we demonstrate promising results in two synthetic datasets and one online store dataset.\n\n2 BACKGROUND\n\n2.1 LONG-TERM TREATMENT EFFECTS\n\nLet π0 and π1 be the control and treatment policies, used to serve individual in respective buckets. In the rideshare example, a policy may decide how to match a driver to a nearby request. During the experiment, each individual (the driver) is randomly assigned to one of the policy groups, and we observe a sequence of behavior features of that individual under the influence of the assigned policy. We use variable D to denote the random assignment of an individual to one of the policies. The observed features are denoted as a sequence of random variable in Rd\n\n∈ {\n\n0, 1\n\n}\n\nO0, O1, . . . , Ot, . . . ,\n\nwhere the subscript t indicates time step in the sequence. A time step may be one day or one week, depending on the application. Feature Ot consists of information like number of pickup orders. We are interested in estimating the difference in average long-term reward between treatment and control policies:\n\n∆ = E[\n\nγtRt\n\n|\n\nD = 1]\n\nE[\n\n−\n\nγtRt\n\nD = 0], |\n\n(1)\n\n∞ (cid:88)\n\nt=0\n\n∞ (cid:88)\n\nt=0\n\nwhere E averages over individuals and their stochastic sequence of engagements, Rt = r(Ot) is the reward signal (e.g., customer rating) at time step t, following a pre-defined reward function r : Rd (0, 1) is the discounted factor. The discounted factor γ is a hyper-parameter specified by the decision maker to indicate how much they value future reward over the present. The closer γ is to 1, the greater weight future rewards carry in the discounted sum.\n\nR, and γ\n\n→\n\n∈\n\nSuppose we have run a randomized experiment with the two policies for a short period of T steps. In the experiment, a set of n individuals are randomly split and exposed to one of the two policies π0 and π1. We denote by dj i the index set of ∈ { individuals assigned to πi, i.e., j\n\n} i iff dj = i. The in-experiment trajectory of individual j is: ∈ I\n\nthe policy assignment of individual j, and\n\n0, 1\n\nI\n\nτj =\n\noj,0, oj,1, . . . , oj,T\n\n{\n\n.\n\n}\n\nThe in-experiment dataset is the collection of all individual data as to find an estimator ˆ∆(\n\n∆.\n\nn)\n\nn =\n\nD\n\n{\n\n(τj, dj)\n\nn\n\nj=1. Our goal is\n\n}\n\nD\n\n≈\n\n2.2 ESTIMATION UNDER STATIONARY MARKOVIAN DYNAMICS\n\nInspired by recent advances in off-policy evaluation (OPE) (e.g. Liu et al., 2018; Nachum et al., 2019b), the simplest assumption is a fully observed Markov Process that the observation in each time step can fully predict the future distribution under a stationary dynamic kernel. In this paper, we assume the dynamics kernel and reward function are both linear, following the setting in Parr et al. (2008). Linear representations are popular in the RL literature (e.g., Shi et al., 2022b) , and often preferable in industrial applications due to simplicity and greater model interpretability. Assumption 2.1. (Linear Dynamics) there is a matrix Mi such that\n\nE[Ot+1\n\nOt = o, D = i] = Mio,\n\n|\n\nN, i\n\nt ∀\n\n∈\n\n0, 1\n\n. }\n\n∈ {\n\nRemark 2.2. Unlike standard RL, we don’t have an explicit action for a policy. The difference between the control and treatment policy is revealed by different transition matrix M . Assumption 2.3. (Linear Reward) There is a coefficient vector θr\n\nRd such that\n\n(2)\n\n(3)\n\n∈\n\nr(Ot) = θ⊤\n\nr Ot,\n\nN.\n\nt ∀\n\n∈\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 2.4. The reward signal may be one of the observed features. For example, if we are interested in customer rating, and rating is one of the observe features, then θr is just a one-hot vector with 1 in the corresponding coordinate. When the reward is complex with unknown coefficient, we can use ordinary least-squares to estimate the coefficient θr. Proposition 2.5. Under Assumption 2.1 and 2.3, if the spectral norm of Mi is smaller than 1 the expected long-term reward of policy πi, v(πi) := E[(cid:80)∞\n\nD = i], can be obtained by:\n\nγ , then\n\nt=0 γtRt\n\nv(πi) = θ⊤\n\nr (I\n\n−\n\nγMi)−1 ̄O(i)\n\n0 , where ̄O(i)\n\n0\n\nD = i]. |\n\n(4)\n\n| := E[O0\n\nThe only remaining step is to estimate ̄O(i) 0 and Mi. The former can be directly estimated from the Monte Carlo average of the experimental data: ˆO(i) is the number of individuals assigned to policy πi. To estimate the latter, we may use ordinary least-squares on observed transitions:\n\no0,j, where ni =\n\n0 = 1\n\ni |I\n\nj∈Ii\n\n(cid:80)\n\nni\n\n|\n\nˆMi =\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=0\n\n\n\n\n\noj,t+1o⊤\n\nj,t\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\n\n\n−1\n\noj,to⊤\n\nj,t\n\n\n\n.\n\nj∈Ii\n\nt=0\n\n(5)\n\nThe detailed derivation can be found in (Parr et al., 2008). Once we get the estimated value of ˆvi\n\nv(πi), the long term impact in Eq. (1) can be estimated as:\n\n≈\n\nˆ∆ = ˆv1\n\nˆv0.\n\n−\n\nRemark 2.6. Although this a model-based estimator, it is equivalent to other OPE estimator in general under linear Markovian assumption (e.g., Nachum et al., 2019b; Duan et al., 2020; Miyaguchi, 2021) and it enjoys similar statistical guarantees as other OPE estimators.\n\n3 OUR METHOD\n\nIn Section 2.2, we assumed the observation Ot follows a stationary Markov process, and derived a model-based closed-form solution based on linear reward Assumption 2.3.\n\nIn reality, this model assumption has two major limitations. First, real-world environments are nonstationary. For example, in a hotel reservation system, seasonality heavily influences the prediction of the future booking count. Our stationary assumption does not capture those seasonal changes, resulting in poorly learned models and inaccurate predictions of long-term treatment effects. Second, in practice, we are unable to ensure that observed features fully capture the dynamics. OPE methods based on stationary and full observability assumptions are unlikely to work robustly in complex, real-life scenarios.\n\nFigure 1 illustrates nonstationarity in data from an online store (see Section 5 for more details). The figure shows how the weekly average of a business metric changes in a span of 5 months, for two policies (C for control, and T4 for treatment). Such highly non-statioanary data, especially during special seasons towards the right end of the plot, are common. However, the difference of the two policy groups remains much more stable. This is expected as both policies are affected by the same exogenous affects (seasonal variations in this example).\n\nFigure 1: An example of non-stationarity. The weekly average metric value is highly non-stationary during holiday season.\n\nFigure 1 motivates a relaxed model assumption (Section 3.1), by introducing a non-stationary exogenous component on top of a stationary hidden state St. Our new assumption is that the observation Ot can be decomposed additively into two parts: an endogenous part still follows a stationary Markovian dynamic for each policy group (treatment or control); and an exogenous part which is time-varying and shared across all groups. Based on the new assumption we propose an alternating minimization algorithm that jointly estimates both transition dynamics and exogenous variables.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 NONSTATIONARY MODEL RELAXATION\n\nWe assume there is an exogenous noise vector zt for each time step t, to represent the linear additive exogenous noise in the uncontrollable outside world such as seasonal effect, which applies uniformly to every individual under each treatment bucket. We relax Assumption 2.1 as the following: Assumption 3.1. (Linear Additive Exogenous Noise) the observational feature Ot is the sum of the endogenous hidden features and the time-varying exogenous noise zt.\n\nwhere zt does not depend on policy or any individual in the experiments and St follows the linear Markovian kernel with transition matrix Mi:\n\nOt = St + zt,\n\nN.\n\nt ∀\n\n∈\n\nE[St+1\n\nSt = s, D = i] = Mis,\n\n|\n\nN, i\n\nt ∀\n\n∈\n\n0, 1\n\n. }\n\n∈ {\n\n(6)\n\nRemark 3.2 (Explanation of the Linear Additive Model). Our linear additive model is inspired by the parallel trend assumption in the Difference-in-Difference (DID) estimator (Lechner et al., 2011). In real-world environments, it is impossible to capture all the covariates that may effect the dynamics. The linear additive exogenous noise zt can be seen as the drive from the outside that is both unobserved and uncontrol. For example, in an intelligent agriculture system, the highly nonstationary weather condition can be seen as exogenous which we cannot control, but the amount of water and fertilizer that affect the growth of the plant can be seen as the hidden state that is controlled by a pre-defined stationary policy. And we add up those two factors as the features (e.g., the condition of the crop) we observed in the real world.\n\nFrom Assumption 3.1 and linear reward function assumption in 2.3, the closed form of v(πi) can be rewritten as: Proposition 3.3. Under Assumption 3.1 and 2.3, and suppose v(z∞) := (cid:80)∞ the spectral norm of Mi is smaller than 1\n\nt=0 γtzt <\n\n. Suppose\n\n∞\n\nγ , the expected long-term reward can be obtained by: 0 + v(z∞), where ̄S(i)\n\n0 = E[S0\n\nD = i].\n\nγMi)−1 ̄S(i)\n\n(7)\n\nv(πi) = θ⊤\n\nr (I\n\n−\n\n|\n\nThe long-term reward in Eq. (7) contains v(z∞), which depends on the unknown exogenous noise sequence outside of the experimental window and thus is unpredictable. However, the long term v(π0), cancels out the dependency on that exogenous term treatment effect, ∆(π1, π0) = v(π1) v(z∞). For simplicity, we redefine v(πi) = θ⊤ 0 without the term of v(z∞). Therer (I −\nfore, the only thing we need to estimate is ̄S(i) and Mi. Once we have the access of z0, we can estimate ̄S(i) ˆz0. The next question is how to estimate in-experiment exogenous variable zt and the underlying transition kernels.\n\nsimilarly as Monte Carlo sample: ˆS0 = 1\n\nγMi)−1 ̄S(i)\n\no0,j\n\nj∈Ii\n\n(cid:80)\n\n−\n\n−\n\nni\n\n0\n\n0\n\n3.2 OPTIMIZATION FRAMEWORK\n\njointly under a single loss function, with the We propose to optimize same spirit of reducing the reconstruction loss of each transition pair similar to the model-based approach.\n\n1≤t≤T and\n\nM0, M1\n\nzt\n\n{\n\n}\n\n{\n\n}\n\nFor each individual j in treatment group i, Assumption 3.1 implies that at time step t + 1, the observation oj,t+1 can be written as:\n\n∀ where εj,t is a noise term with zero mean, so that Mi(oj,t\n\n−\n\n−\n\n∈ I ≤\nzt) = E[St+1\n\n≤\n\noj,t+1\n\nzt+1 = Mi(oj,t\n\nzt) + εj,t,\n\nj\n\ni, 1\n\n−\n\nt\n\nT\n\n1,\n\n(8)\n\n| n, in order to minimize the empirical reconstruct\n\n−\n\n−\n\nSt = oj,t\n\nzt, D = i].\n\nInspired by Eq. (8), given observation history risk by each transition pair (oj,t, oj,t+1), we construct the following loss function\n\nD\n\n(M0, M1,\n\nzt\n\n{\n\n1≤t≤T ;\n\n}\n\nn) =\n\nD\n\nL\n\n1 (cid:88)\n\n(cid:88)\n\ni=0\n\nj∈Ii\n\nT −1 (cid:88) t=0 ∥\n\noj,t+1\n\nzt+1\n\n−\n\n−\n\nMi(oj,t\n\nzt)\n\n2.\n\n2 ∥\n\n−\n\n(9)\n\nTo simplify the notation, Eq. (9) can be rewritten as a vectorized form\n\n(M0, M1, z;\n\nn) =\n\nD\n\nL\n\n1 (cid:88)\n\n(cid:88)\n\ni=0\n\nj∈Ii\n\n4\n\nAi(oj\n\n∥\n\nz)\n\n2,\n\n2 ∥\n\n−\n\n(10)\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Estimating Long-Term Effect Under Non-stationary Dynamics\n\nn =\n\nInput: In-experiment training Data in-experiment observation features for individual j, dj group individual j is assigned to. Initialize the estimation of exogenous noise ˆz = 0. Optimization: while not convergent do\n\n(τj, dj)\n\nD\n\n}\n\n{\n\nn\n\nj=1, where τi = (oj,0, oj,1, . . . , oj,T ) is the is the indicator of which policy\n\n0, 1\n\n∈ {\n\n}\n\nUpdate Mi as the ordinary least square solution given the current ˆz:\n\n\n\n\n\nˆMi =\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n(oj,t+1\n\n−\n\nˆzt+1)(oj,t\n\n−\n\n\n\n\n\nˆzt)⊤\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n\n\n−1\n\n(oj,t\n\n−\n\nˆzt)(oj,t\n\n−\n\nˆzt)⊤\n\n\n\n.\n\nUpdate ˆz according to Eq. (12):\n\nˆz = (n0G0 + n1G1)−1(\n\n1 (cid:88)\n\n(cid:88)\n\nGioj).\n\ni=0\n\nj∈Ii\n\nend while Evaluation:\n\nCompute ˆvi = θ⊤ Output the long-term impact estimation as ˆ∆ = ˆv1\n\n0 −\n\nr (I\n\nˆz0\n\n−\n\nγ ˆMi)−1 (cid:16) ˆO(i)\n\n(cid:17)\n\n, where ˆO(i)\n\n0 = 1 ˆv0.\n\nni\n\n(cid:80)\n\nj∈Ii\n\no0,j.\n\n−\n\nwhere oj =\n\n\n\n \n\noj,0 oj,1 . . . oj,T\n\n\n\n\n\n  , and z =\n\n \n\nz0 z1 . . . zT\n\n\n\n  are column vector aggregate over the experiment time\n\nhorizon, and Ai is a dT\n\n×\n\nd(T + 1) matrix constructing by a block matrix Mi:\n\nAi =\n\nMi 0\n\n−\n\nI Mi\n\n−\n\n\n\n \n \n\n... ... ... ... ...\n\n\n\n \n \n\ndT ×d(T +1)\n\nI Mi\n\n−\n\n0 I\n\n.\n\n(11)\n\n3.3 ALTERNATING MINIMIZATION\n\nTo reconstruct Mi and z, we apply alternating minimization on the loss function n) in Eq. (10). By looking at the zero-gradient point of the loss function, under proper non-degenerate assumption (see Appendix for details), we have:\n\n(M0, M1, z;\n\nD\n\nL\n\nProposition 3.4. Suppose (n0G0 + n1G1) is nonsingular, the minimizer of z given Mi is a closedform solution in the followings:\n\narg min\n\nz L\n\n(M0, M1, z;\n\nn) = (n0G0 + n1G1)−1(\n\nD\n\n1 (cid:88)\n\n(cid:88)\n\ni=0\n\nj∈Ii\n\nGioj), where Gi = A⊤\n\ni Ai.\n\n(12)\n\nThe minimizer of Mi given z is similar to Eq. (5), except that we subtract the exogenous part zt from the observation:\n\n(M0, M1, z;\n\nn)\n\nD\n\narg min M0,M1 L \n\nT −1 (cid:88)\n\n(cid:88)\n\n:=\n\n\n\nj∈Ii\n\nt=1\n\n(oj,t+1\n\n−\n\nˆzt+1)(oj,t\n\n−\n\n\n\n\n\nˆzt)⊤\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n\n\n−1\n\n(oj,t\n\n−\n\nˆzt)(oj,t\n\n−\n\nˆzt)⊤\n\n\n\n.\n\n(13)\n\nThe final optimization process is summarized in Algorithm 1.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.4 THEORETICAL ANALYSIS\n\nWe give a preliminary theoretical analysis in this section to give readers some insights on how good our estimator is once a partial oracle information is given. We will extend our analysis to quantify the error of the estimator at the convergence state of alternating minimization in future work.\n\nTo simplify our analysis, we first assume we get access to the true transition matrix Mi, and our goal is to quantify the error between ˆv(πi) and the true policy value v(πi) for each policy πi. Proposition 3.5. Suppose we have bounded noise and matrices under Assumption A.1 and Assumption A.2, and suppose n0 = n1 = n 2 is equally divided. When we get access of the oracle transition matrix Mi = M ∗ n). If we plugin ˆz in the estimation i , i of ˆv(πi), we will have\n\n, let ˆz = arg minz L\n\n0 , M ∗\n\n1 , z;\n\n(M ∗\n\n∈ {\n\n0, 1\n\nD\n\n}\n\nwith probability at least 1\n\nδ.\n\n−\n\nˆv(πi) |\n\n−\n\nv(πi) |\n\n=\n\n(\n\nO\n\n1 √n\n\n),\n\nIn the second analysis we assume that we get an accurate z. In this case, the estimation of ˆM reduces zt is to the stationary assumption case in Assumption 2.1 where the hidden state variable st = ot fully recovered. We follow the analysis (e.g., Duan et al., 2020; Miyaguchi, 2021) of linear MDP to characterize the error.\n\n−\n\nProposition 3.6 (Proposition 11 in Miyaguchi (2021)). Suppose we get access to the oracle exogenous noise z during the experimental period, let ˆMi = arg minMi L n) in Eq. (13). Under the assumption in Proposition 11 in Miyaguchi (2021), with the plugin estimator ˆv with ˆMi, we have:\n\n, z∗;\n\n( {\n\nMi\n\nD\n\n}\n\nwith probability at least 1\n\nδ.\n\n−\n\nˆv(πi)\n\n|\n\nv(πi) |\n\n−\n\n=\n\nO\n\n3.5 PRACTICAL CONSIDERATIONS\n\n(n− 1\n\n2d+2 ),\n\nRegularize the Transition Dynamic Matrices. Degenerated case may happen during the alterγ , leading the nating minimization when either 1) the spectral norm is too large, i.e. ∥\nlong-term operator (I i diverges in Eq. (7), or 2) the matrix inversion calculation of Mi in Eq. (13) is not well-defined. To avoid those scenarios and stabilize the computation procedure, we add a regularization term of Mi as λi 2 in our experiment. The intuition is that the transition matrix should be close to identity matrix as in practice the treatment policy typically deviates from the control policy in an incremental manner.\n\nγMi)−1 = (cid:80)∞\n\nt=0 γtM t\n\n2 ∥\n\nMi\n\nMi\n\n2 ∥\n\nId\n\n−\n\n≥\n\n−\n\n∥\n\n1\n\nAfter adding the regularization, the closed-form minimizer of Mi of the regularized loss function becomes: \n\n\n\n\n\n\n\n−1\n\nMi =\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n(oj,t+1\n\nzt+1)(oj,t\n\n−\n\n−\n\nzt)⊤\n\n\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n(oj,t\n\nzt)(oj,t\n\n−\n\n−\n\nzt)⊤\n\n\n\n.\n\nRegularize the Exogenous Variable. There is a challenge in deriving the closed-form z in Eq. (12) where n0G0 + n1G1 can be degenerated or nearly degenerated. By definition, Gi is always singular. Moreover, if there is no control of the minimal eigenvalue of (n0G0 + n1G1), e.g. close to zero, the update step on z is uncontrolled and the variance of noise can be magnified in the direction of the minimal eigenvector. Therefore it is crucial to regularize z.\n\nTo tackle the possible degenerated circumstances, one natural idea is to include regularization of the l2 norm of z, where the regularized loss function can be written as:\n\nL Its corresponding minimizer of ˆz can be written as:\n\nD\n\nL\n\nλ(z, M0, M1;\n\n) =\n\n(z, M0, M1;\n\n) + λz\n\nz\n\n∥\n\n2.\n\n2 ∥\n\nD\n\n(14)\n\nˆz = (λI + n0G0 + n1G1)−1(\n\n1 (cid:88)\n\n(cid:88)\n\nGioj),\n\ni=0\n\nj∈Ii\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nwhere I is the identity matrix of dimension d (T + 1). It is worth mentioning that when the regularization parameter λ increases to infinity, z will go to 0, and the solution reduces to the stationary case in Assumption 2.1.\n\n×\n\nExtend to Multiple Treatment Policies The optimization framework can be easily extend to multiple treatment policies case. Suppose we have k different treatment policies π1, π2, , πk and let π0 be the control policy, the closed form solution for ˆz under multiple dataset of different treatment groups can be derived as\n\n· · ·\n\nˆzλ = (λI +\n\nk (cid:88)\n\ni=0\n\nk (cid:88)\n\nniGi)−1(\n\n(cid:88)\n\nGioj).\n\ni=0\n\nj∈Ii\n\nAnd the closed-form update for Mi stays the same. The final estimation of the treatment effect for policy πi is ˆ∆ = ˆvi\n\nˆv0.\n\n−\n\n4 RELATED WORK\n\nEstimating long-term treatment effects Our work is related to causal inference with temporal data. The surrogate index method (Athey et al., 2019; 2020) makes a different assumption that the long-term effect is independent of the treatment conditioned on the surrogate index measured during the experiment. It then estimates long-term impacts resulting from short-term exposure during the experiment. In contrast, our work aims to estimate long-term impacts resulting from long-term exposure. Time series methods (e.g. Bojinov & Shephard, 2019) require probabilistic treatments, which allow an individual to be exposed to different treatments at different time periods during an experiment. They then estimate the temporal treatment effect, which is averaged over all the temporal steps, differs from traditional treatment effect which is averaged over randomized individuals.\n\nOur method draws inspirations from off-policy evaluation(OPE) and related areas, whose goal is to estimate the long-term policy value, usually from a offline dataset collected under different policies. Most early work focuses on the family of inverse propensity score estimators that are prone to high variance in long-horizon problems (e.g., Precup et al., 2000; Murphy et al., 2001; Jiang & Li, 2016). Recently, there are growing interests in long- and even infinite-horizon settings (Liu et al., 2018; Nachum et al., 2019a; Xie et al., 2019; Tang et al., 2020; Uehara et al., 2020; Dai et al., 2020; Chandak et al., 2021). In particular, Shi et al. (2022b) considers a similar problem of estimating long-term impacts, which is comparable to our stationary baseline. However, these methods either rely on the stationarity assumption that is violated in many applications, or consider the general nonstationary Markov decision process (Kallus & Uehara, 2020) that does not leverage domainspecific assumptions.\n\nRL in nonstationary or confounded environments Our model is a special case of Partially Observable Markov Decision Process (POMDP) ( ̊Astr ̈om, 1965; Kaelbling et al., 1998). OPE in general POMDPs remains challenging, unless various assumptions are made (e.g., Tennenholtz et al., 2020; Bennett et al., 2021; Shi et al., 2022a). Most assumptions are on the causal relation of the logged data, such as relation between state, action and confounded variable. In contrast, we make an assumption motivated by real-world data, which allows our estimator to cancel out exogenous variables from observations.\n\nOur assumption is also related to MDP with Exogenous Variables (e.g., Dietterich et al., 2018; Chitnis & Lozano-P ́erez, 2020), and Dynamics Parameter MDP (DPMDP) or Hidden Paramter MDP (HiP-MDP) (Al-Shedivat et al., 2017; Xie et al., 2020). For exogenous variable, they assume observation features can be partitioned into two groups, where the exogenous group is not affected by the action and the endogenous group evolve as in a typical MDP. The major challenge is infer the right partition. Several recent works (e.g Misra et al., 2020; Du et al., 2019; Efroni et al., 2021) combine exogenous variable with rich observation in RL. This is different from our assumption where we assume the observation is a sum of both parts, which is a more natural assumption in applications like e-commerce. For DPMDP and Hip-MDP, they assume a meta task variable which is non-stationary and changed across time but the task variable dynamic can be captured by a sequential model. Our\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nE S\nM\n\ng o\nL\n\nnumber of trajectories n (a)\n\nhorizon T (b)\n\ndimension d (c)\n\nnoise scale α (d)\n\nFigure 2: Results of Synthetic Environment. We vary parameters in the simulation to compare the logarithmic MSE of various estimators: (a) number of trajectories; (b) horizon; (c) observation feature dimension; (d) scale of the exogeneous noise.\n\nassumption can be viewed as a linear special case but our focus is not to better characterize the system but is to remove the exogenous part for better predictions.\n\n5 EXPERIMENTS\n\nWe evaluate our methods in three problems: a synthetic dataset, a dataset from the Type-1 Diabete RL simulator (Xie, 2019), and a real-world dataset from an online store. The ground truth ∆ is computed either from a true simulator or using the average of the real experimental data under a long time period. We compare our methods based on plug-in estimator of the stationary solution in Eq. (4), its non-stationary variant in Algorithm 1, and an Naive Average baseline. The baseline directly uses the short-term reward average as the estimate of the long-term effect.\n\n5.1 SYNTHETIC SIMULATION\n\n}\n\n{\n\nzt\n\nThe synthetic environment generates 4 randomized matrix Mi for policies i=0 and a trajectory T\nt=0. See details of the synthetic dynamic in Appendix C. The of randomized exogenous noise randomized sequence follows the non-stationary dynamics with a parameter α controlling the scale of the exogenous noise: oj,t = sj,t + αzt, j, t. We collect n trajectories for each policy until t = T (w/ varying T ). We vary the parameters of the generating sequences: number n of trajectories, horizon T , data dimension d, and scale α of the exogenous noise. We plot the logarithmic Mean Square Error (MSE) for each method in Figure 2. The result shows that our estimator method (the green line) clearly outperforms all other baselines. Moreover, Figure 2(d) shows the increase of the scale of the exogenous noise does not affect estimation accuracy of our method.\n\nπi\n\n∀\n\n{\n\n3 }\n\n5.2 TYPE-1 DIABETE SIMULATOR\n\nThis environment is modified based on an open-source implementation1 of the FDA approved Type1 Diabetes simulator (T1DMS) (Man et al., 2014). The environment simulates two-day behavior in an in-silico patient’s life. Consumption of a meal increases the blood-glucose level in the body. If the level is too high, the patient suffers from hyperglycemia. If the level is too low, the patient suffers from hypoglycemia. The goal is to control the blood glucose level by regulating the insulin dosage to minimize the risk associated with both hyperglycemia and hypoglycemia.\n\nWe modify the Bagal and Bolus (BB) policy (Bastani, 2014) (control policy) in the codebase and set two glucose target levels and different noise levels as our two treatment policies. We collect information in the first 12-hour of all the three policies with 5000 randomized patients in each policy group and use those information to predict the long-term effect. The observation feature is 2-dimensional: glucose level (CGM) and the amount of insulin injection. The non-stationarity comes from the time and amount of the consumption of a meal, which is time varying, but otherwise shared by all patients. We average a 2-day simulation window over random 250, 000 patients as ground truth treatment effect between policy groups.\n\n1https://github.com/jxx123/simglucose\n\n8\n\n100010000100000−8−6−4106×10−1−8−6−4102030−7−6−50.0250.0500.0750.100−6.5−6.0−5.5−5.0NaiveAverageStationaryNonstationaryUnder review as a conference paper at ICLR 2023\n\nnumber of patients n (a)\n\nexp horizon T (b)\n\nnumber of patients n (c)\n\nexp horizon T (d)\n\nFigure 3: Results of Type-1 Diabete Environment. We vary two parameters in the simulation, the number of patients and the in-experiment horizon, to compare the performance for different methods under two different evaluation metrics.\n\nSimilar to the synthetic simulator, we vary the number of patients and the experimental period. Figure 3 shows that the non-stationary method performs better in the prediction accuracy compared to stationary method in both predictions of CGM and the amount of insulin injection. Even though the simulator is non-linear, our simple linear additive exogenous noise assumption still captures the small local changes well, which is approximately linear.\n\n5.3 DATA FROM AN ONLINE STORE\n\nNaive Average Stationary Non-stationary\n\nMetric 1 Metric 2 Metric 3 Metric 4 133.61% 66.09% 106.85% 37.99% 174.77% 48.03% 98.31% 110.58% 64.68% 62.00% 48.04% 67.57%\n\nTable 1: Results in the online store dataset. The reported numbers are the median of MAPE over 7 different treatment policies. Columns correspond to business metrics of interest.\n\nWe test our methods under 4 long-running experiments in an online store with a total of 7 different treatment policies (some experiments have more than 1 treatment). Each experiment has 1 control policy. We evaluate 4 business metrics related to customer purchases in the store (Metrics 1-4), and use d = 17 features. All the experiments lasted for 12 weeks. We treat the first 5 weeks as the experiment window, and use data in those weeks to estimate long-term impacts of the 4 metrics. The trailing 7-week average of the metrics are used as ground true to evaluate accuracy of various estimators. Table 1 reports the median of the Mean Absolute Percentage Error (MAPE) of the estimators; See full results in Appendix C.\n\nGiven the high cost in such long-running experiments, we cannot collect more data points for comparison, and for computing statistical significance. That said, there is good evidence from the reported number that our method produces better predictions of long-term treatment effects than Naive Average. Furthermore, our method improves on the stationary baseline, suggesting the practical relevance of our nonstationary assumtion, and effectiveness of the proposed estimator.\n\n6 CONCLUSIONS\n\nIn this paper we study how to estimate the long-term treatment effect by using only the inexperimental data in the non-stationary environment. We propose a novel non-stationary RL model and an algorithm to make prediction. A major limitation is the linear assumption in both the dynamics model and the additive exogenous part. Once the real world model includes a highly non-linear part, the prediction value can be biased. Future direction includes further relax our model to nonlinear case to better capture the real world environment.\n\n9\n\n0200040000.250.500.751.00CGM10152001020CGM020004000234insulin1e−71015200246insulin1e−7StationaryNonstationaryUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMaruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. arXiv preprint arXiv:1710.03641, 2017.\n\nKarl Johan ̊Astr ̈om. Optimal control of markov processes with incomplete state information. Journal\n\nof mathematical analysis and applications, 10(1):174–205, 1965.\n\nSusan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely. Technical report, National Bureau of Economic Research, 2019.\n\nSusan Athey, Raj Chetty, and Guido Imbens. Combining experimental and observational data to\n\nestimate treatment effects on long term outcomes. arXiv preprint arXiv:2006.09676, 2020.\n\nMeysam Bastani. Model-free intelligent diabetes management using machine learning. PhD thesis,\n\nUniversity of Alberta, 2014.\n\nAndrew Bennett, Nathan Kallus, Lihong Li, and Ali Mousavi. Off-policy evaluation in infinitehorizon reinforcement learning with latent confounders. In The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pp. 1999–2007. PMLR, 2021.\n\nIavor Bojinov and Neil Shephard. Time series experiments and causal estimands: exact randomization tests and trading. Journal of the American Statistical Association, 114(528):1665–1682, 2019.\n\nYash Chandak, Scott Niekum, Bruno C. da Silva, Erik G. Learned-Miller, Emma Brunskill, and Philip S. Thomas. Universal off-policy evaluation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 27475–27490, 2021.\n\nRohan Chitnis and Tom ́as Lozano-P ́erez. Learning compact models for planning with exogenous\n\nprocesses. In Conference on Robot Learning, pp. 813–822. PMLR, 2020.\n\nBo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv ́ari, and Dale Schuurmans. In Advances in Neural Information CoinDICE: Off-policy confidence interval estimation. Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nThomas Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous state variables and rewards for reinforcement learning. In International Conference on Machine Learning, pp. 1262–1270. PMLR, 2018.\n\nSimon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient RL with rich observations via latent state decoding. In International Conference on Machine Learning, pp. 1665–1674. PMLR, 2019.\n\nYaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approximation. In International Conference on Machine Learning, pp. 2701–2709. PMLR, 2020.\n\nYonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Provable rl with exogenous distractors via multistep inverse dynamics. arXiv preprint arXiv:2110.08847, 2021.\n\nNan Jiang and Lihong Li. Doubly robust off-policy evaluation for reinforcement learning. In Pro-\n\nceedings of the 33rd International Conference on Machine Learning, pp. 652–661, 2016.\n\nChi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nLeslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in\n\npartially observable stochastic domains. Artificial Intelligence, 101(1–2):99–134, 1998.\n\nNathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient and robust offpolicy evaluation. In Proceedings of the 37th International Conference on Machine Learning, pp. 5078–5088, 2020.\n\nRon Kohavi, Diane Tang, and Ya Xu. Trustworthy Online Controlled Experiments: A Practical\n\nGuide to A/B Testing. Cambridge University Press, 2020.\n\nRonny Kohavi, Thomas Crook, Roger Longbotham, Brian Frasca, Randy Henne, Juan Lavista Ferres, and Tamir Melamed. Online experimentation at microsoft. Data Mining Case Studies, 11 (2009):39, 2009.\n\nMichael Lechner et al. The estimation of causal effects by difference-in-difference methods. Foun-\n\ndations and Trends® in Econometrics, 4(3):165–224, 2011.\n\nQiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinitehorizon off-policy estimation. In Advances in Neural Information Processing Systems 31 (NIPS18), pp. 5361–5371, 2018.\n\nChiara Dalla Man, Francesco Micheletto, Dayu Lv, Marc Breton, Boris Kovatchev, and Claudio Cobelli. The UVA/PADOVA type 1 diabetes simulator: New features. Journal of Diabetes Science and Technology, 8(1):26–34, 2014.\n\nDipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In International conference on machine learning, pp. 6961–6971. PMLR, 2020.\n\nKohei Miyaguchi. Asymptotically exact error characterization of offline policy evaluation with misspecified linear models. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 28573–28584, 2021.\n\nSusan A. Murphy, Mark van der Laan, and James M. Robins. Marginal mean models for dynamic\n\nregimes. Journal of the American Statistical Association, 96(456):1410–1423, 2001.\n\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of In Advances in Neural Information Processing discounted stationary distribution corrections. Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 2315–2325, 2019a.\n\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections. In Advances in Neural Information Processing Systems, pp. 2318–2328, 2019b.\n\nRonald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael L Littman. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 752–759, 2008.\n\nDoina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of the 17th International Conference on Machine Learning, pp. 759– 766, 2000.\n\nChengchun Shi, Masatoshi Uehara, Jiawei Huang, and Nan Jiang. A minimax learning approach to off-policy evaluation in confounded partially observable Markov decision processes. In Proceedings of the 39th International Conference on Machine Learning, pp. 20057–20094, 2022a.\n\nChengchun Shi, Xiaoyu Wang, Shikai Luo, Hongtu Zhu, Jieping Ye, and Rui Song. Dynamic causal effects evaluation in a/b testing with a reinforcement learning framework. Journal of the American Statistical Association, pp. 1–13, 2022b.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nZiyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu. Doubly robust bias reduction in infinite horizon off-policy estimation. In Proceedings of the 8th International Conference on Learning Representations, 2020.\n\nGuy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable environments. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 10276–10283. AAAI Press, 2020.\n\nMasatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for offIn International Conference on Machine Learning, pp. 9659–9668. PMLR,\n\npolicy evaluation. 2020.\n\nAnnie Xie, James Harrison, and Chelsea Finn. Deep reinforcement learning amidst lifelong non-\n\nstationarity. arXiv preprint arXiv:2006.10701, 2020.\n\nJinyu Xie.\n\nSimglucose v0.2.1 (2018), 2019.\n\nURL https://github.com/jxx123/\n\nsimglucose.\n\nTengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 9665–9675, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAppendix\n\nA PROOF\n\nIn this section, we provide detailed proof for the theorem in the main text, as a self-contained section, we briefly introduce the notation as below, and adopt the regularized, multiple policy groups settings in the appendix:\n\n• n: number of total individuals.\n\n•\n\ni: the index set for policy πi; ni = |\n• k total number of different policy group.\n\ni |I\n\nI\n\nas the number of individual in under policy πi.\n\nn: dataset for n individuals in the experimental period.\n\n•\n\nD\n\nIn the appendix, we denote the ground truth dynamic M ∗ with a star\n\nto distinguish the variables Mi and z during optmization process.\n\ni and the ground truth exogenous noise z∗\n\n∗\n\nA.1 ASSUMPTIONS\n\nThe dynamic assumption of our linear additive exogenous noise assumption in Assumption 3.1 can be rewritten as the following equation: M ∗\n\n(15)\n\n1.\n\nz∗ t ) = (oj,t+1\n\nz∗ t+1) + εj,t,\n\ni (oj,t\n\ni, 0\n\nT\n\nj\n\nt\n\n−\n\n∀\n\n∈ I\n\n≤\n\n≤\n\n−\n\nwhere εj,t is a zero-mean noise. Let εj =\n\n \n\nRd×T ,\n\nεj\n\n{\n\n1≤j≤n forms a martingale:\n\n}\n\n(16)\n\ni ∈ I\n\n(17)\n\nwhere the filtration\n\nj =\n\no1, ..., oj−1\n\nis the information up to the first j\n\n1 individuals.\n\nF\n\n{\n\n}\n\n−\n\nWe make addition bounded assumption on the zero-mean noise term for the proof: Assumption A.1 (Bounded Noise assumption). Let εj = M ∗ z∗ (oj,t+1 t ) −\nbe the residual of the transition under the true transition matrix M ∗ i , we have\n\ni (oj,t\n\n−\n\nz∗ t+1), j\n\n−\n\nwhere Cε is a uniform constant independent of policy assignment.\n\nεj\n\n∥\n\n2 ∥\n\n≤\n\nCε,\n\nj,\n\n∀\n\nFor the empirical covariance matrix in the middle step of the calculation, we assume they are all bounded.\n\nAssumption A.2 (Bounded Norm for Matrices). We make the following assumptions on matrices\n\nM ∗\n\ni ∥ ≤ n/n)−1\n\n(Λ∗\n\n1.\n\n2.\n\n∥\n\n∥\n\nCMi < 1 γ ,\n\ni.\n\n∀\n\nCΛ.\n\n∥ ≤\n\nA.2 LOSS FUNCTION AND ALTERNATING MINIMIZATION\n\nOur loss function can be written as:\n\nMi\n\n( {\n\nL\n\n1≤i≤k, z;\n\n}\n\nn) =\n\nD\n\nk (cid:88)\n\n(cid:88)\n\ni=1\n\nj∈Ii\n\nAi(z\n\n∥\n\noj)\n\n2 + λz\n\n2 ∥\n\nz\n\n∥\n\n2 +\n\n2 ∥\n\n−\n\nk (cid:88)\n\ni=1\n\nλi\n\nMi\n\n∥\n\nId\n\nF .\n\n2 ∥\n\n−\n\n(18)\n\nLemma A.3. Fix (\nz = arg minz {\n\nL\n\n}\n\nMi\n\n, denote Gi = A⊤\n\n{ Mi\n\n} 1≤i≤k, z;\n\nn) is\n\nD\n\n(cid:32)\n\nz(\n\nMi\n\n{\n\n) =\n\n}\n\nλzId×(T +1) +\n\ni Ai where Ai is defined in Eq. (11), the minimization of\n\n(cid:33)−1  \n\nniGi\n\n\n\nGioj\n\n .\n\nk (cid:88)\n\n(cid:88)\n\ni=1\n\nj∈Ii\n\n(19)\n\nk (cid:88)\n\ni=1\n\n13\n\n− \n\n\n\n  ∈\n\nεj,0 εj,1 . . . εj,T −1\n\nE[εj\n\nj−1] = 0,\n\n|F\n\nUnder review as a conference paper at ICLR 2023\n\nProof. By taking the gradient of the loss function, we will have:\n\n0 =\n\n∇\n\nz\n\nL k\n(cid:88)\n\nMi\n\n( {\n(cid:88)\n\n=2\n\n1≤i≤k, z;\n\nn)\n\nD\n\n}\n\nGi(z\n\n−\n\noj) + 2λzz\n\nwhich implies\n\ni=1\n\nj∈Ii\n\n(cid:32)\n\nz(\n\nMi\n\n{\n\n) =\n\n}\n\nλzId×(T +1) +\n\n(cid:33)−1  \n\nniGi\n\nk (cid:88)\n\ni=1\n\nk (cid:88)\n\n(cid:88)\n\ni=1\n\nj∈Ii\n\n\n\nGioj\n\n .\n\nHere, Gi = A⊤ expression always exists.\n\ni Ai is semi-definite, so the inversion of the large matrix in the right side of the\n\nSimilarly we can get the minimizer of Mi fixing z. Lemma A.4. By fixing z, the minimizer of Mi can be written as\n\n\n\nMi(z) =\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\n(oj,t+1\n\nj∈Ii\n\nt=1\n\n\n\n\n\nzt+1)(oj,t\n\n−\n\n−\n\nzt)⊤\n\n\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n\n\n−1\n\n(oj,t\n\n−\n\nzt)(oj,t\n\n−\n\nzt)⊤\n\n\n\n.\n\n(20)\n\nProof. The proof is similarly applied by looking at the zero gradient of Mi.\n\nIf we set λi = 0 and z = 0, the minimization reduces back to estimation of ˆM in Eq. (5).\n\nA.3 ERROR ANALYSIS\n\nLemma A.5. Let M ∗\n\ni be the true dynamic of the underlying state, we have:\n\nz(\n\nM ∗\n\nz∗ =\n\n)\n\n−\n\n−\n\nλz(Λ∗\n\nn)−1z∗ + (Λ∗\n\nn)−1\n\ni }\n\n{\n\n\n\n\n\nk (cid:88)\n\n\n\n(cid:88)\n\nA⊤\n\ni εj\n\n ,\n\n(21)\n\nwhere Λ∗\n\nn = λzId×T + (cid:80)k\n\ni=1 niG∗ i .\n\nProof. By expand the definition of z(\n\nz(\n\n{\n\nM ∗\n\ni }\n\n) = (Λ∗\n\nn)−1\n\n\n\ni=1\n\nj∈Ii\n\n\n\nM ∗ \n\ni }\n\n{\n\n, we have:\n\nk (cid:88)\n\n\n\n(cid:88)\n\nGioj\n\n\n\ni=1\n\nj∈Ii\n\n\n\n\n\nk (cid:88)\n\n\n\n(cid:88)\n\nA⊤\n\ni (Aioj)\n\n\n\ni=1\n\nj∈Ii\n\n= (Λ∗\n\nn)−1\n\n= (Λ∗\n\nn)−1\n\n\n\n\n\nk (cid:88)\n\ni=1\n\nj∈Ii\n\n= (Λ∗\n\nn)−1\n\n Λ∗\n\nnz∗\n\nλzz∗ +\n\n−\n\n(cid:88)\n\nA⊤\n\ni (Aiz∗ + εj)\n\n\n\n=z∗\n\n−\n\nλz(Λ∗\n\nn)−1z∗ + (Λ∗\n\nn)−1\n\n\n\n14\n\nk (cid:88)\n\n(cid:88)\n\n\n\nA⊤\n\ni εj)\n\n\n\nj∈Ii\n\ni=1 \n\n\n\nA⊤\n\ni εj)\n\n\n\nk (cid:88)\n\n(cid:88)\n\ni=1\n\nj∈Ii\n\nUnder review as a conference paper at ICLR 2023\n\nLemma A.6. Let z∗ be the true exogenous noise, we have:\n\n\n\nMi(z∗)\n\n−\n\nM ∗\n\ni =\n\nλi(Id\n\nM ∗\n\ni ) +\n\n−\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\nεj,t(oj,t\n\nt )⊤ z∗\n\n−\n\n  (λiId + Σ∗\n\nn)−1 ,\n\n(22)\n\nwhere Σ∗\n\nn = (cid:80)\n\nj∈Ii\n\n(cid:80)T −1\n\nt=1 (oj,t\n\nz∗ t )(oj,t\n\n−\n\n−\n\nt )⊤ is the empirical covariace matrix. z∗\n\nProof. By expand the definition of Mi(z∗), we have:\n\n\n\nMi(z∗) =\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\n(oj,t+1\n\nj∈Ii\n\nt=1\n\n\n\n\n\nz∗ t+1)(oj,t\n\n−\n\n−\n\nt )⊤ z∗\n\n\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\n(oj,t\n\nj∈Ii\n\nt=1\n\n\n\n−1\n\nz∗ t )(oj,t\n\nt )⊤ z∗\n\n\n\n−\n\n−\n\n\n\n=\n\nλiId +\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n\n\n=M ∗\n\ni +\n\nλi(Id\n\nM ∗\n\ni ) +\n\n−\n\n(εj,t + M ∗\n\ni (oj,t\n\nz∗ t )) (oj,t\n\nt )⊤ z∗\n\n−\n\n−\n\n  (λiId + Σ∗\n\nn)−1\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\nεj,t(oj,t\n\nt )⊤ z∗\n\n−\n\n  (λiId + Σ∗\n\nn)−1\n\n(23)\n\n(24)\n\n(25)\n\nA.4 PROOF OF PROPOSITION 2.5\n\nProof. By induction, it is not hard to prove that\n\nSum up all condition on O0, we have:\n\nE[Ot\n\nO0 = o, D = i] = M t\n\ni o.\n\n|\n\nBy the definition of long-term discounted reward G, we have:\n\nE[Ot\n\nD = i] = M t |\n\ni\n\nE[O0].\n\n∞ (cid:88)\n\nv(πi) =E[\n\nt=0\n\nγtRt\n\nD = i] |\n\n=\n\n∞ (cid:88)\n\nt=0\n\nγtE[θ⊤\n\nr Ot\n\nD = i] |\n\n=θ⊤\n\nr\n\n∞ (cid:88)\n\nt=0\n\nγtM t\n\ni\n\nE[O0]\n\nwhere the last equation holds when\n\nMi\n\n∥\n\n∥\n\nA.5 PROOF OF PROPOSITION 3.5\n\n=θ⊤\n\nr (I < 1 γ .\n\nγMi)−1E[O0],\n\n−\n\nProof. From Lemma A.5, suppose λz = 0 and (Λ∗ n)−1 exists, we have: \n\n\n\nz∗ = (Λ∗\n\nn)−1\n\nz\n\n−\n\n\n\n1 (cid:88)\n\n(cid:88)\n\nA⊤\n\ni εj\n\n .\n\ni=0\n\nj∈Ii\n\nConsider ˆv(π0) if we plugin ˆz and the true dynamic M ∗\n\n0 , the error between ˆv and v is z∗ 0 )\n\n0 )−1(z0\n\nˆv(π0)\n\n−\n\n−\n\nγM ∗ z∗ 0 )\n\nv(π0) =θ⊤ :=β⊤ =(β⊤ = ̃β⊤\n\nr (I r (z0 r , 0, . . . , 0)(z0 z∗ 0 ), r (z0\n\n−\n\n−\n\n−\n\nz∗ 0 )\n\n−\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nwhere βr = (I other time step as 0.\n\n−\n\nγM ∗\n\n0 )−T θr, and ̃βr is the extended vector of βr if we fill the other vector value at\n\nExpand the difference (z0\n\n− ˆv(π0)\n\nz∗ 0 ) we have:\n\nv(π0) = ̃β⊤\n\nr (z0\n\nz∗ 0 )\n\n−\n\n−\n\n=\n\n=\n\n1 (cid:88)\n\ni=0\n\n1 (cid:88)\n\ni=0\n\n ̃β⊤\n\nr (Λ∗\n\nn)−1A⊤ i (\n\n(cid:88)\n\nεj)\n\nj∈Ii (cid:80)\n\n ̃β⊤ r (\n\nΛ∗ n\nn\n\n)−1A⊤ i (\n\nεj\n\n)\n\nj∈Ii n\n\n ̃βr\n\n≤∥\n\n1 (cid:88)\n\n∥\n\ni=0\n\nΛ∗ n\nn\n\n( ∥\n\n)−1Ai\n\n∥∥\n\n(cid:80)\n\nj∈Ii n\n\nεj\n\n. ∥\n\n1\n\nBy Assumption A.1 and Assumption A.2, the norm of ̃βr is the same as βr, which is bounded by βr . The matrix norm in the middle factor is bounded because of Assumption A.2. ∥\nFinally, by vector concentration inequality, since εj is norm-subGaussian (Jin et al., 2019), there exist a constant c that with probability at least 1\n\n1−γCMi ∥\n\n∥ ≤\n\nθr\n\nδ:\n\n∥\n\n−\n\n(cid:80)\n\nj∈Ii n\n\n∥\n\nεj\n\n(cid:114)\n\nlog(2dT /δ) n\n\n.\n\nc\n\n∥ ≤\n\nIn sum, the error is bounded by Cε, CMi, CΛ and the norm of\n\n( 1√ O\nθr .\n∥\n\n∥\n\nn ) with probability at least 1\n\nδ, and the constant depends on\n\n−\n\nA.6 PROOF OF PROPOSITION 3.6\n\nProof. Since we get access to the ground true z∗, the remaining problem is by changing the state z∗ as sj,t = oj,t t and reduce the problem back to standard MDP. The detailed proof can refer to Proposition 11 in Miyaguchi (2021).\n\n−\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nB REDUCE THE COMPUTATION COMPLEXITY WITH PRE-COMPUTATION\n\nIn this section, we explain how to reduce the computation complexity with pre-computation.\n\nPre-computation. Compute\n\nMi(0) =\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\nj∈Ii\n\nt=1\n\n\n\n\n\noj,t+1o⊤\n\nj,t\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\n\n\n−1\n\noj,to⊤\n\nj,t\n\n\n\nj∈Ii\n\nt=1\n\nand\n\n ̄ot =\n\n(cid:88)\n\nj∈Ii\n\noj,t.\n\n(nT d2 + d3), where d2 is the computaThe pre-computation requires computation complexity of tion complexity of the outer product, d3 is the computation complexity of the matrix inversion after summing up the matrix.\n\nO\n\nIn Each Iteration. The computation of M can be rewritten as\n\nMi(z) =\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\n(oj,t+1\n\nj∈Ii\n\nt=1\n\nzt+1)(oj,t\n\n−\n\n−\n\n\n\n\n\nzt)⊤\n\n\n\n\n\n(cid:88)\n\nT −1 (cid:88)\n\n(oj,t\n\n\n\n−1\n\nzt)(oj,t\n\n−\n\nzt)⊤\n\n\n\n−\n\n=Mi(0)\n\nT −1 (cid:88)\n\n−\n\nt=1\n\nzt+1 ̄o⊤\n\nt −\n\nT −1 (cid:88)\n\nt=1\n\n ̄ot+1z⊤\n\nt +\n\nj∈Ii\n\nt=1\n\nzt+1z⊤ t ,\n\nT −1 (cid:88)\n\nt=1\n\nwhich requires computation complexity of O(T d2). Similarly, the computation of\n\nk (cid:88)\n\nz(G) = (\n\nniGi)−1(\n\ni=0\n\nk (cid:88)\n\ni=0\n\nGi ̄oj)\n\nrequires computation complexity of not rely on number of individuals n (which is often much larger than T and d).\n\n(T 2d2). Both steps are computationally scalable, since it does\n\nO\n\nOverall Computation Complexity. Suppose we execute the iterations for k times, then the total (nT d2 + d3 + kT 2d2). In practice, computation complexity for the alternating minimization is the number of different individual n is far larger than the experiment horizon T and the feature dimension d, therefore the computation complexity essentially scales linearly with n.\n\nO\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nC EXPERIMENTS DETAILS\n\nC.1 SYNTHETIC SIMULATION\n\nThe synthetic environment generates 4 randomized matrix Mi for policies i=0, where each entry of Mi is a positive number randomly sample from a uniform distribution between (0, 1). We normalize each row so that it sums up to 1, and we set ̃Mi = 0.5I + 0.5Mi as our final transition matrix. The 0.5I part ensures each matrix is not too far away from each other.\n\nπi\n\n{\n\n3 }\n\nWe generate a set of i.i.d. random vector ηt we let ̃zt = αt\n\nzt as the final exogenous noise, where αt = eβt and βt\n\n∼ N\n\n(0, 1.5I) and set zt+1 = zt + ηt recursively. And\n\n(0, 0.5I), i.i.d..\n\n∗\n\n∼ N\n\nAll the parameters (zt and Mi) of the dynamic are fixed once generated, and we use the dynamic to generate our observation for each individual, following\n\nst+1 = Mist + εt, and ot = st + αzt,\n\nt ∀\n\nwhere εt is independently drawn from a standard normal distribution, and α control the level of exogenous noise.\n\nC.2 POLICY CONSTRUCTION IN TYPE-1 DIABETE SIMULATOR\n\nThe Bagal and Bolus policy is a parametrized policy based on the amount of insulin that a person with diabetes is instructed to inject prior to eating a meal (Bastani, 2014)\n\ninjection =\n\ncurrent blood glucose\n\n− CF\n\ntarget blood glucose\n\n+\n\nmeal size CR\n\n,\n\nwhere CF and CR are parameter based on patients information such as body weights, which is already specified in the simulator.\n\nWe set our two treatment policies with target blood glucose level at 145 and 130 (compared to control: 140). And we increase the noise in the insulin pump simulator in both the treatment policies.\n\nC.3 RANDOM PATIENTS GENERATION IN TYPE-1 DIABETE SIMULATOR\n\nType-1 Diabete simulator pre-stores 30 patients parameter. To randomly generate a new patient, we randomly pick two different patients A and B, and use a random linear coefficient β U (0, 0.2) and mixed the parameter of a new patient as\n\n∼\n\nwhere θA and θB are the parameters of patients A and B, respectively. Since patient A has more weight of the parameter, the parameters in Bagal and Bolus policy, CF and CR, follow patient A’s parameter.\n\nθ = (1\n\n−\n\nα)θA + αθB,\n\nC.4 FULL RESULTS FOR ALL THE ONLINE STORE EXPERIMENTS.\n\nNaive Average Stationary Non-stationary\n\nMetric 1 Metric 2 Metric 3 Metric 4 122.47% 93.61% 51.20% 25.28% 174.77% 454.87% 61.71% 110.58% 12.54% 26.79% 67.57% 94.56%\n\nTable 2: Experiment # 1\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTreatment 1\n\nTreatment 2\n\nNaive Average Stationary Non-stationary Naive Average Stationary Non-stationary\n\nMetric 1 Metric 2 Metric 3 Metric 4 41.91% 66.09% 3139.91% 431.44% 51.60% 48.03% 4471.33% 275.49% 13.59% 64.68% 48.28% 770.27% 236.13% 43.44% 106.85% 122.65% 150.97% 12.07% 199.95% 109.96% 10.69% 62.00%\n\n98.31% 48.04%\n\nTable 3: Experiment # 2\n\nNaive Average Stationary Non-stationary\n\nMetric 1 Metric 2 Metric 3 Metric 4 396.54% 79.88% 192.84% 17.21% 697.59% 123.43% 364.49% 6.73% 30.65% 12.89% 98.37%\n\n81.86%\n\nTable 4: Experiment # 3\n\nTreatment 1\n\nTreatment 2\n\nTreatment 3\n\nNaive Average Stationary Non-stationary Naive Average Stationary Non-stationary Naive Average Stationary Non-stationary\n\nMetric 1 Metric 2 Metric 3 Metric 4 8078.75% 43.18% 208.55% 2438.43% 7.20% 154.66% 1889.47% 7386.81% 102.84% 2328.03% 114.34% 25.18% 37.99% 126.70% 138.98% 38.60% 10.44% 12.01% 46.60% 172.57% 29.92% 69.75% 54.16% 72.62% 17.01% 50.87% 45.67% 133.61% 12.89% 27.11% 88.77% 258.88% 34.89% 24.58% 66.32% 74.14%\n\nTable 5: Experiment # 4\n\n19",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a reinforcement learning based algorithm to estimate long-term effect for a class of nonstationary problems. Empirical results in both synthetic and real datasets show the potential of the proposed algorithm.\n\n# Strength And Weaknesses\n\nThe paper studies a practical and important problem: estimate long-term effect under nonstationary dynamics. The proposed algorithm is natural and simple.\n\nMy main concern is the linear assumptions. Is it possible to generalize the results for generalized linear models? Will the prediction value be pretty biased for generalized linear models? Another comment is that there are other papers that use reinforcement learning approach to estimate long-term effect, for example, [1] and literature on dynamic treatment regimes, and I think these papers need to be cited for comparison.\n\n[1] Chengchun Shi, Xiaoyu Wang, Shikai Luo, Hongtu Zhu, Jieping Ye,and Rui Song, Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement Learning Framework, 2020.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe overall presentation is good. It is not hard to understand the paper. The proposed algorithm and analysis are pretty natural. It seems that the code is not provided, so it is hard to judge the reproducibility.\n\n# Summary Of The Review\n\nAs mentioned above, this paper attempts to tackle an important practical problem. Though the theoretical and empirical results justify the potential of the proposed algorithm, I think the linear assumptions are strong. If possible, I hope the authors could say or show something on generalized linear models and I would be happy to increase my score.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) approach to estimating long-term treatment effects in nonstationary environments, addressing the inadequacies of traditional A/B testing methodologies. The authors propose an algorithm that combines stationary and time-varying factors to better capture the dynamics influencing treatment effects over time. Empirical validation is provided through experiments on synthetic datasets and real-world data from an online store, demonstrating that the proposed method outperforms both stationary and naive baseline approaches in estimating average long-term rewards.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative application of RL to tackle the complex problem of estimating long-term effects in nonstationary settings, a significant gap in existing research. The development of a nonstationary estimator, supported by preliminary theoretical analysis and empirical evidence, adds robustness to the claims. However, the reliance on linear assumptions may limit the model's applicability in more complex real-world scenarios. Additionally, while the theoretical analysis is a valuable contribution, it could benefit from further depth and clarity in its implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical structure that guides the reader through the contributions and methodologies. The novelty of applying RL to this specific problem area is noteworthy, and the empirical results bolster the claims made by the authors. However, the reproducibility of the results could be enhanced by providing more details on the implementation of the algorithm and the experimental setup, including hyperparameter settings and data preprocessing steps.\n\n# Summary Of The Review\nOverall, this paper offers a significant advancement in estimating long-term treatment effects using a reinforcement learning framework in nonstationary environments. While the contributions are valuable and well-supported by empirical evidence, the reliance on linear assumptions and the need for more implementation details could hinder broader applicability and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper introduces a novel reinforcement learning (RL) framework aimed at estimating long-term treatment effects in nonstationary environments, addressing a critical gap in traditional A/B testing methodologies which often overlook lasting impacts. The authors propose an innovative algorithm that jointly estimates transition dynamics and exogenous variables in nonstationary Markov processes, incorporating a decomposition of the observation model into stationary and time-varying components. Empirical evaluations using synthetic datasets and a real-world e-commerce dataset demonstrate the proposed method's superiority over baseline approaches, showcasing its capacity to capture underlying dynamics effectively.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to utilizing RL for causal inference, which is a significant advancement in experimental design. The empirical results across diverse datasets illustrate the robustness and effectiveness of the proposed method. Additionally, the preliminary theoretical analysis provides a sound foundation for understanding the performance of the estimator. However, the reliance on linearity assumptions for both dynamics and exogenous components may limit applicability in more complex scenarios. Furthermore, the computational complexity associated with the method raises concerns about its scalability with larger datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulates its contributions, methodology, and findings. The organization of the content supports readability and comprehension, making it accessible to a broad audience. The novelty of the approach is significant, combining RL with causal inference in a manner that addresses long-term treatment effects. While the theoretical foundations are presented, further details on reproducibility, such as implementation specifics and parameter tuning, would enhance the paper's impact and usability in practical applications.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to estimating long-term treatment effects using a reinforcement learning framework in nonstationary environments. While the methodology and results are promising, the reliance on linear assumptions and potential computational challenges warrant further exploration and validation across various contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel reinforcement learning (RL) approach to estimating long-term treatment effects in nonstationary environments, addressing limitations of traditional A/B testing that typically capture only short-term impacts. The methodology introduces an optimization framework that incorporates an exogenous noise vector, allowing for a more accurate representation of the dynamics involved in treatment effects over time. The proposed algorithm was rigorously tested on synthetic datasets, a Type-1 Diabetes RL simulator, and a real-world online store dataset, demonstrating superior performance in estimating long-term effects compared to stationary baselines and naive averages.\n\n# Strength And Weaknesses\nOne of the key strengths of the paper lies in its innovative approach to modeling nonstationary dynamics, which is crucial for accurately estimating long-term treatment effects in real-world scenarios. The theoretical analysis accompanying the algorithm provides useful error bounds and convergence guarantees, enhancing the credibility of the proposed methodology. However, a notable weakness is the reliance on linear assumptions, which may limit the applicability of the method in more complex, nonlinear environments. Additionally, while the experimental results are promising, further validation in diverse real-world contexts would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, making it accessible to readers with a background in RL and causal inference. The quality of the writing is high, with comprehensive explanations of the theoretical foundations and empirical results. The novelty of the proposed framework is significant, as it extends previous work to nonstationary settings, a topic that has been underexplored. However, reproducibility could be enhanced by providing additional details on the experimental setups and datasets used, as well as making code or data available for verification.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in estimating long-term treatment effects in nonstationary environments using reinforcement learning. While the proposed method shows promise and outperforms existing approaches, its reliance on linear dynamics may restrict its broader applicability. Further exploration of nonlinear models could enhance the framework's utility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel reinforcement learning (RL) approach for estimating long-term treatment effects in nonstationary environments, addressing limitations of traditional randomized experiments. The authors develop an algorithm that separates endogenous and exogenous influences, facilitating the handling of nonstationarity. Empirical validation is provided through tests on synthetic and real-world datasets, demonstrating the method's effectiveness. Theoretical support is included, although it is somewhat limited in scope.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative application of RL to causal inference, which is a notable advancement in the field. The handling of nonstationarity is a significant contribution, yet the framework relies on linear assumptions that may not capture complex real-world dynamics. While the empirical validation shows promise, the limited diversity of datasets raises concerns about generalizability. The theoretical analysis, while a positive aspect, lacks comprehensiveness, which could impact the practical applicability of the findings. Additionally, the potential for model extension is a valuable feature, albeit with challenges related to computational complexity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and results clearly, making it accessible to a broader audience. However, some technical aspects may be challenging for readers less familiar with reinforcement learning or causal inference, indicating a need for additional context. The novelty of the approach is commendable, but the reproducibility could be hindered by the reliance on specific assumptions and the limited scope of empirical validation.\n\n# Summary Of The Review\nThe paper makes a significant contribution to the intersection of reinforcement learning and causal inference by proposing a novel method for estimating long-term effects in nonstationary environments. While it demonstrates promising results and a clear presentation, certain limitations regarding assumptions, empirical validation, and theoretical depth warrant further exploration to enhance its applicability and impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper addresses the challenge of estimating long-term treatment effects in nonstationary environments, a limitation of traditional randomized experiments such as A/B testing. The authors propose a novel reinforcement learning (RL) framework that decomposes observed states into endogenous and exogenous components, enabling the model to adapt to dynamic influences. Key contributions include an innovative RL application for estimating long-term effects, a nonstationary dynamics framework that incorporates time-varying influences, and the development of a joint estimation algorithm that improves the accuracy of long-term predictions. Empirical validation through experiments on synthetic datasets and real-world applications demonstrates the proposed method's superiority over conventional benchmarks.\n\n# Strength And Weaknesses\nStrengths of the paper lie in its innovative approach to a significant problem in causal inference, leveraging RL to enhance estimation accuracy in nonstationary settings. The dual-component model effectively captures the complexities of real-world scenarios, and the empirical results consistently show improvements over traditional methods. However, the paper could improve by providing a more in-depth discussion of the limitations of the proposed model and potential challenges in extending the framework to more complex, nonlinear dynamics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, allowing readers to grasp the proposed concepts easily. The quality of the writing is high, with appropriate technical depth. The novelty is significant, as the application of RL for this problem is relatively unexplored. While the empirical results support the claims made, the reproducibility could be enhanced by providing more details regarding the datasets used and the experimental setup.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the estimation of long-term treatment effects in nonstationary environments through a novel RL framework. While the contributions are notable, addressing the theoretical aspects and limitations more comprehensively would strengthen the work. The paper is recommended for acceptance with minor revisions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"A REINFORCEMENT LEARNING APPROACH TO ESTIMATING LONG-TERM EFFECTS IN NONSTATIONARY ENVIRONMENTS\" introduces a novel framework for integrating adversarial training within reinforcement learning (RL) to address the challenges posed by nonstationary environments. The authors propose an algorithm that employs an alternating minimization strategy to optimize RL agents' training while accounting for adversarial influences. Extensive empirical validation reveals that this approach significantly outperforms traditional adversarial training techniques, demonstrating enhanced robustness and adaptability in dynamic scenarios.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including an innovative approach that fills a critical gap in the literature by combining adversarial training with reinforcement learning in nonstationary settings. The comprehensive experimental results bolster the findings, showcasing the proposed method's robustness against various adversarial attacks. Additionally, the theoretical insights provided lend credibility to the algorithm. However, the complexity of the proposed implementation may pose challenges in practical applications. Furthermore, the paper lacks a thorough comparison against a broader range of baseline methods that also tackle nonstationarity in adversarial contexts, which could limit the understanding of the proposed method's relative performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that facilitate understanding of the methodology and findings. The quality of the theoretical underpinnings supports the claims made, making a strong case for the proposed algorithm. The novelty is significant, as it addresses a timely and relevant challenge in RL. However, there are concerns regarding reproducibility due to the complexity of the implementation and the need for careful parameter tuning, which may deter some researchers from easily replicating the results.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the intersection of adversarial training and reinforcement learning by proposing a novel approach that effectively handles nonstationary environments. While the theoretical and empirical results are promising, addressing implementation challenges and expanding baseline comparisons would strengthen the work further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel reinforcement learning (RL) approach to estimating long-term treatment effects in nonstationary environments, particularly in the context of randomized experiments. The authors assert that their new RL algorithm addresses the limitations of traditional methods in measuring long-term effects, offering a new estimator that integrates nonstationary dynamics and an exogenous noise component. The findings demonstrate the proposed method's superiority over existing techniques through experiments on synthetic datasets, a Type-1 Diabetes simulator, and online store datasets, suggesting it could significantly improve outcomes in healthcare and business.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious application of RL to a critical problem in causal inference, the introduction of a new estimator that could become a standard, and the extensive validation of results across multiple datasets. However, the weaknesses include an overstatement of the contributions, particularly in how the proposed method is framed as a revolutionary breakthrough. The complexity and potential computational costs of the optimization framework may also pose challenges for widespread adoption, which the authors downplay.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, although it tends to be overly optimistic about the implications of its contributions. The quality of the methodology appears robust, but the novelty, while significant, may be somewhat overstated, as it builds on existing RL frameworks. Reproducibility is not thoroughly addressed; while the authors provide results from various datasets, they do not offer sufficient details on implementation or the full complexity of the optimization process, which could hinder replication efforts.\n\n# Summary Of The Review\nThe paper makes a significant contribution to the understanding of long-term treatment effects using reinforcement learning, with promising results across different datasets. However, the claims of revolutionary impact may be exaggerated, and concerns regarding the complexity and reproducibility of the methodology warrant caution in interpretation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel reinforcement learning (RL) approach aimed at estimating long-term treatment effects in nonstationary environments, addressing the limitations of traditional A/B testing which primarily focuses on short-term outcomes. The authors propose a new estimator that considers exogenous variables affecting observed data and demonstrate its efficacy through experiments on synthetic datasets, a Type-1 diabetes simulator, and an online store dataset. While the findings initially suggest superior performance over existing methods, subsequent analyses reveal that the proposed method only marginally improves upon traditional techniques, particularly under specific conditions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative application of RL to the problem of long-term effect estimation, alongside a preliminary theoretical framework that supports the proposed estimator's validity. Furthermore, the consideration of nonstationary dynamics is a significant advancement, providing a more realistic modeling approach for real-world applications. However, the weaknesses are evident in the experimental results, which show that the proposed method does not consistently outperform simpler baseline methods. The significant drop in performance claims, particularly in the online store dataset, raises concerns about the robustness and practical applicability of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and results. However, the discrepancies between initial claims and subsequent findings may confuse readers regarding the actual contributions of the work. The novelty of applying RL to this specific problem is noteworthy, yet the empirical results do not strongly support the claimed advantages. Reproducibility is somewhat hindered by the lack of detailed descriptions of the experimental setup, particularly regarding the synthetic datasets and the diabetes simulator, which limits the ability of other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a promising methodological contribution to estimating long-term effects in nonstationary environments using reinforcement learning. However, the empirical results do not convincingly support the proposed approach's superiority over existing methods, limiting its practical significance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a methodology for estimating long-term treatment effects in reinforcement learning and causal inference contexts, assuming a stationary Markov process. The authors rely on linear dynamics and rewards, Monte Carlo estimation, and an alternating minimization algorithm to derive their estimators. Their findings suggest that the proposed method can yield effective treatment effect estimates, although the validity of the underlying assumptions raises concerns about the generalizability and robustness of results.\n\n# Strength And Weaknesses\nThe paper makes several contributions, particularly in applying a structured approach to estimating treatment effects. However, it heavily relies on key assumptions, such as stationarity and linearity, which may limit its applicability in real-world scenarios characterized by nonstationarity and complex relationships. The use of synthetic datasets raises questions about external validity, while the limited theoretical analysis and lack of discussion on convergence conditions detract from the overall strength of the methodology.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its presentation of the methodology. However, the novelty of the approach is somewhat diminished by the reliance on standard assumptions and techniques within the field. Reproducibility may be a concern, particularly due to the reliance on synthetic datasets and the lack of extensive empirical validation across diverse contexts.\n\n# Summary Of The Review\nOverall, the paper provides a structured approach to estimating long-term treatment effects but is constrained by significant assumptions that may limit its practical utility. The reliance on synthetic data and a lack of rigorous theoretical analysis further undermine the robustness of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel reinforcement learning (RL) approach aimed at estimating long-term treatment effects in nonstationary environments, which traditional A/B testing methods struggle to address. The proposed methodology involves a nonstationary model that decomposes observations into endogenous and exogenous components, utilizing an alternating minimization algorithm to jointly estimate transition dynamics. The authors provide theoretical insights into the performance of their estimator and demonstrate its effectiveness through experiments on synthetic datasets, a Type-1 Diabetes simulator, and a real-world online store dataset, showing improvements over existing baseline methods.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing the limitations of conventional A/B testing in estimating long-term effects and providing a robust RL framework that incorporates nonstationary dynamics. The theoretical analysis offers valuable error bounds that enhance the credibility of the proposed approach. However, the reliance on linear modeling assumptions may restrict the applicability of the method in more complex scenarios, and the paper could benefit from a more extensive discussion on the limitations and potential biases in the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers with a background in RL and causal inference. The quality of the writing is high, with a logical flow that enhances comprehension. The novelty of the approach is evident in its attempt to bridge gaps in the literature regarding long-term treatment effects in nonstationary environments. While the empirical results are promising, the reproducibility of the experiments could be improved by providing more detailed descriptions of the experimental setups and datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a compelling framework for estimating long-term treatment effects using reinforcement learning, showcasing both theoretical and empirical advancements over traditional methods. While it effectively addresses key gaps in the literature, the reliance on linear assumptions and the need for clearer reproducibility guidelines are notable areas for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses a significant challenge in the field of machine learning by proposing a novel approach that integrates multiple techniques to enhance model development. The authors introduce a framework that combines theoretical insights with practical applications, aiming to fill gaps identified in existing literature. The findings suggest that their method could lead to improved performance metrics across a variety of tasks, indicating its potential to influence future research directions.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Novelty**: The integration of various methodologies presents a fresh perspective that could advance current understanding in the field.\n2. **Relevance**: The problem addressed is crucial for the machine learning community, with implications that could benefit a range of applications.\n3. **Clarity of Presentation**: The organization of the paper is logical, making it easy for readers to follow the authors' arguments and methodologies.\n4. **Theoretical Insights**: The paper is supported by a robust theoretical framework, which adds credibility to the proposed approach.\n5. **Potential for Impact**: If successful, the approach could significantly enhance performance metrics relevant to the domain.\n\n**Weaknesses:**\n1. **Empirical Evaluation**: The empirical validation appears limited; additional experiments across diverse datasets would bolster the claims made.\n2. **Comparison with Baselines**: A more comprehensive comparison with existing methodologies is necessary to clearly establish the advantages of the proposed approach.\n3. **Assumptions Made**: The paper makes several assumptions that require further discussion regarding their validity in real-world applications.\n4. **Clarity in Methodology**: While the methodology is generally clear, some sections lack detail, which could hinder reproducibility.\n5. **Limitations**: The authors do not adequately address the limitations of their approach, which is essential for a balanced evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear, allowing for effective communication of the authors' ideas. However, certain methodological components require more detailed explanations to ensure reproducibility. The novelty of the proposed approach is commendable, but the empirical analysis lacks depth, which may affect its overall impact. The theoretical foundation is strong, but a discussion on assumptions and limitations would enhance the quality of the paper.\n\n# Summary Of The Review\nThis paper puts forth a promising approach to a critical problem in machine learning, showcasing notable novelty and theoretical rigor. However, to fully realize its potential, the authors should strengthen the empirical analysis and provide clearer methodological details. Addressing these weaknesses will significantly enhance the paper's contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) approach aimed at estimating long-term treatment effects in nonstationary environments, addressing significant limitations of traditional randomized experiments (A/B tests) that primarily focus on short-term outcomes. The proposed framework includes an innovative algorithm designed to estimate average rewards while accommodating nonstationary state transitions. Empirical evaluations demonstrate the effectiveness of this methodology using both synthetic datasets and a real-world dataset from an online store, illustrating the potential of RL in assessing long-term impacts.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of the limitations of traditional methods and its timely introduction of an RL-based solution that is theoretically sound and empirically validated. The development of a specialized estimator for nonstationary environments marks a notable advancement in the field. However, a potential weakness lies in the model's reliance on linear assumptions, which could restrict its applicability in more complex scenarios. Additionally, the empirical results, while promising, primarily rely on synthetic datasets, which may not fully capture the intricacies of real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers. The quality of the writing and the clarity of the presented ideas are commendable. In terms of novelty, the application of RL to estimate long-term treatment effects in nonstationary environments is a significant contribution to the literature. However, the reproducibility of the results could be enhanced by providing more details on the algorithms and datasets used, as well as potential code availability for further validation.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the understanding of long-term treatment effects in dynamic settings through the use of reinforcement learning. While the proposed methodology shows promise and is theoretically robust, concerns regarding linearity assumptions and the extent of empirical validation warrant further exploration in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel approach to estimating long-term treatment effects from randomized experiments (A/B tests) through a reinforcement learning (RL) framework. The authors address the limitations of traditional methods that struggle to capture long-term effects in nonstationary environments, proposing a new RL algorithm designed specifically for this challenge. The methodology involves introducing exogenous noise to model nonstationarity and employing an alternating minimization algorithm for estimation. The findings demonstrate that the proposed method significantly improves predictive accuracy of long-term effects when evaluated on both synthetic datasets and real-world applications, including a Type-1 Diabetes RL simulator and an online store dataset.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative application of reinforcement learning to the problem of long-term effect estimation in nonstationary environments, which is a notable gap in existing literature. The theoretical analysis provides preliminary insights into the estimator's performance, including error bounds and convergence properties. However, the paper's limitations include the reliance on specific assumptions for theoretical results and the need for more extensive validation on diverse real-world datasets beyond the ones presented. Additionally, the discussion around non-linear models is acknowledged but left for future work, which may limit the paper's immediate applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, making it easy to follow the progression from motivation to methodology to results. The clarity in presenting the theoretical underpinnings and the algorithmic details adds to its quality. The novelty of using RL for this particular problem is significant, as it opens up new avenues for research in causal inference. However, reproducibility could be enhanced by providing more details on parameter settings and implementation specifics used in the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of causal inference by leveraging reinforcement learning to address long-term treatment effect estimation in nonstationary contexts. While the methodology is sound and the results are promising, certain assumptions and limitations noted in the study warrant further exploration to enhance the applicability of the proposed approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to estimating long-term treatment effects in nonstationary environments using reinforcement learning (RL). It identifies the limitations of traditional randomized experiments, particularly in measuring long-term outcomes, and proposes a new algorithm designed to address these challenges. The methodology is thoroughly detailed, with a clear theoretical foundation, and is validated through experiments on synthetic datasets, a Type-1 Diabetes simulator, and an online store dataset, showing improved performance over baseline methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative application of RL to a significant and relevant problem, as well as its rigorous theoretical and empirical evaluation. The paper effectively contextualizes its contributions within the existing literature, highlighting the gaps it aims to fill. However, a notable weakness is the need for a more thorough discussion on the implications of the linear assumptions made in the modeling process, which may limit the generalizability of the findings. Additionally, while the empirical results are promising, further validation through additional real-world datasets could enhance the robustness of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear and logical organization. The methodology is articulated in sufficient detail to allow for reproducibility, although including code availability or supplementary materials would further facilitate this. The novelty of the approach is significant, as it introduces a new algorithm to address an underexplored area in estimating long-term effects, making it a valuable contribution to the field. However, some mathematical formulations may benefit from additional explanations for accessibility to a broader audience.\n\n# Summary Of The Review\nOverall, this paper makes a strong contribution to the understanding of long-term treatment effects in nonstationary environments through its innovative use of reinforcement learning. While the theoretical foundations and empirical validations are solid, the paper could be strengthened by addressing certain assumptions and providing more context for practical implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) framework for estimating long-term treatment effects in nonstationary environments, addressing limitations of traditional randomized experiments. It proposes an innovative algorithm that incorporates an exogenous noise vector to capture external influences on observational data, allowing for more accurate estimation of long-term rewards. Empirical evaluations using synthetic datasets, a Type-1 Diabetes RL simulator, and real-world online retail data demonstrate the proposed method's superior performance over conventional approaches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its integration of reinforcement learning with causal inference, particularly in nonstationary contexts where traditional methods struggle. The proposed algorithm effectively addresses the challenges of estimating long-term effects by relaxing stationary assumptions and introducing a robust optimization framework. However, the paper could further benefit from a more detailed exploration of the limitations and potential biases inherent in the approach, particularly regarding the generalizability of results across different domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, with a clear delineation of the problem, methodology, and results. The mathematical formulations are precise and contribute to the clarity of the proposed framework. The novelty of the approach is significant, as it effectively combines reinforcement learning with causal inference in a way that has not been extensively explored previously. While the empirical results are promising, the paper could enhance reproducibility by providing more detailed descriptions of the experimental setups and hyperparameter settings used in the evaluations.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the fields of reinforcement learning and causal inference by proposing a novel framework for estimating long-term treatment effects in nonstationary environments. While the methodological advancements and empirical results are compelling, additional attention to the limitations and reproducibility aspects would strengthen the paper further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a reinforcement learning approach aimed at estimating long-term effects in nonstationary environments. The authors put forth a novel algorithm that is built upon existing linear models and relies on simplifying assumptions, such as linear dynamics and additive exogenous noise. Despite claiming promising results, the findings are limited by the model's inability to account for the highly non-linear dynamics present in real-world scenarios, raising questions about the robustness and generalizability of the approach.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its relevance to the ongoing challenges in reinforcement learning within nonstationary environments. However, the heavy reliance on simplifying assumptions undermines its applicability to complex real-world situations. The theoretical analysis is preliminary and lacks depth, failing to rigorously quantify estimator error or address potential biases. Additionally, the experiments are insufficiently robust, relying on synthetic datasets and a limited online store dataset, which diminishes the validity of the findings. The superficial discussion of related work and the complexity of the proposed optimization framework further detract from the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the lack of rigorous theoretical backing and comprehensive empirical validation raises concerns about its quality. The novelty is questionable, as the approach largely modifies existing linear models without substantial innovation. Reproducibility is hindered by insufficient details regarding the experimental setup and the absence of statistical validation for the reported results.\n\n# Summary Of The Review\nOverall, while the paper addresses a relevant topic, its execution reveals significant shortcomings in originality, rigor, and practical applicability. The reliance on simplifying assumptions and the lack of robust empirical validation make it challenging to justify publication in a leading conference.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThis paper presents a novel approach to estimating long-term treatment effects in nonstationary environments through the application of reinforcement learning (RL). The authors introduce a new estimator that effectively manages both endogenous and exogenous factors, enhancing the accuracy of long-term estimates. Empirical validation shows that the proposed method outperforms existing baseline estimators across diverse datasets, including synthetic simulations and real-world applications in healthcare and online retail.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative application of RL to a critical problem in causal inference, addressing the limitations of traditional A/B testing and randomized experiments. The tailored algorithm for nonstationary environments significantly enhances the relevance and robustness of the findings. However, the paper could benefit from a more detailed discussion on the limitations of the proposed method and potential biases that may arise in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, making complex ideas accessible. The novelty of applying RL to long-term treatment effect estimation is substantial, and the empirical results support the claims made. The reproducibility of the results is promising; however, additional details regarding the implementation and datasets used would strengthen this aspect further.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the fields of causal inference and reinforcement learning by introducing a robust method for estimating long-term effects in nonstationary environments. While the findings are compelling, further exploration of limitations and practical implications would enhance the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a theoretical framework for estimating long-term treatment effects in nonstationary environments using reinforcement learning (RL). The authors propose a novel model that decomposes observations into endogenous and exogenous components to address limitations of traditional stationary methods. The methodology includes the application of Markov Decision Processes (MDPs) and provides a closed-form solution for long-term reward estimation while accounting for external influences through a noise representation. The findings indicate that the proposed framework significantly enhances causal inference methodologies in dynamic settings.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to addressing the challenges posed by nonstationary dynamics in causal inference, which have been inadequately handled by traditional methods. The decomposition of observations into stationary and nonstationary components is a particularly insightful contribution. However, the paper does have weaknesses; while it presents a compelling theoretical model, the empirical validation of the framework is less emphasized, which may limit the practical applicability of the findings. Additionally, the assumption of linearity in the model may restrict its relevance in more complex, real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, as it presents complex theoretical concepts in an understandable manner. The quality of the writing is high, with rigorous formulations and well-structured sections. The novelty of the approach is significant, as it bridges the gap between RL and causal inference in nonstationary contexts. However, the reproducibility of the results may be hindered by the limited empirical examples provided, necessitating further experimentation to validate the theoretical claims.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of causal inference by proposing a robust theoretical framework for estimating long-term treatment effects in nonstationary environments. While the theoretical advancements are impressive, the lack of extensive empirical validation could pose challenges for practical implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) framework aimed at estimating long-term treatment effects in nonstationary environments. It introduces an algorithm based on alternating minimization that jointly estimates transition dynamics and exogenous variables, with the assumption that observations can be decomposed into endogenous and exogenous components. The method is evaluated through experiments on synthetic datasets, a Type-1 Diabetes simulator, and real-world online store data, demonstrating its effectiveness in minimizing reconstruction error and outperforming baseline methods in terms of Mean Absolute Percentage Error (MAPE) and logarithmic Mean Square Error (MSE).\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to addressing nonstationarity in treatment effect estimation, as well as the detailed mathematical formulation and comprehensive complexity analysis. The algorithm's adaptability to multiple treatment policies is a notable strength, indicating potential for broader applicability. However, weaknesses include the lack of code availability and reproducibility, which may hinder practical implementation. Additionally, the emphasis on algorithmic aspects over broader theoretical implications may limit the paper’s impact in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation of the methodology and results. The mathematical formulations are precise, allowing for an understanding of the proposed approach. However, the absence of code availability significantly affects reproducibility, as it is crucial for others to verify and build upon the findings. The novelty lies in the application of RL to long-term effect estimation in nonstationary settings, but the lack of a broader discussion on theoretical implications or future research directions is a limitation.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in estimating long-term treatment effects in nonstationary environments through a reinforcement learning approach. While the methodology is sound and well-presented, the lack of code availability and limited discussion on broader implications reduce its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes a novel approach to estimating long-term treatment effects using reinforcement learning techniques, claiming to address challenges posed by nonstationary environments. The methodology involves introducing exogenous noise to the dynamics model, purportedly enhancing the algorithm's robustness. The authors report improved performance over Naive Average estimators and stationary baselines, although comparisons with established methods are limited.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to tackle the complexities of nonstationary dynamics, which is a pertinent issue in the field. However, the contributions appear to lack significant novelty, as similar frameworks have been explored by previous works, such as Shi et al. (2022b) and Dietterich et al. (2018). The reliance on a linear dynamics model and insufficient statistical rigor in the empirical evaluations further undermine the paper's claims. Additionally, the lack of comprehensive benchmarking against contemporary methods diminishes the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation, but the novelty of the proposed method is questionable given the existing literature. Reproducibility may be a concern due to the limited statistical rigor and the lack of detailed comparisons with established approaches. The theoretical insights provided are useful but do not sufficiently address practical implications, which may hinder the application of the proposed method in real-world scenarios.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to estimating long-term treatment effects but struggles to establish its novelty and significance within the existing body of literature. The empirical findings lack robustness due to insufficient comparisons, and concerns regarding the assumptions made in the methodology raise questions about the efficacy of the proposed solution.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning method aimed at estimating long-term effects in nonstationary environments. The authors propose a framework that incorporates adaptive strategies for handling temporal changes in the environment, which is essential for applications such as A/B testing in dynamic contexts. The methodology includes the introduction of a new algorithm and theoretical analysis, demonstrating the effectiveness of the approach through empirical evaluations in various simulated settings.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to addressing long-term effect estimation in dynamic environments, which is a significant challenge in reinforcement learning. The theoretical foundations are solid, and the proposed algorithm shows promise in empirical tests. However, the paper contains numerous minor issues related to clarity, consistency, and formatting, which detract from the overall presentation and may hinder comprehension for some readers. Additionally, the contribution section lacks specific comparisons with prior work, which could have better contextualized its significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel methodology, the clarity is undermined by inconsistencies in terminology, formatting issues, and vague phrasing in certain sections. The quality of the analysis is high, yet the readability suffers due to minor grammatical errors and lack of clear definitions for some variables and notations. Reproducibility could be improved by providing clearer algorithm descriptions and ensuring all figures and tables are well-labeled and referenced in the text.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of reinforcement learning, particularly in nonstationary environments. However, it suffers from numerous minor issues that affect clarity and presentation, which could be addressed to enhance understanding and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a reinforcement learning framework aimed at estimating long-term effects in decision-making processes. It introduces a model that accounts for exogenous factors influencing outcomes, while primarily relying on synthetic datasets for empirical validation. The findings suggest that the proposed approach yields promising results in estimating long-term impacts; however, limitations in the model's assumptions and the scope of the evaluation are acknowledged.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to estimating long-term effects through reinforcement learning and its consideration of exogenous factors. However, the paper has notable weaknesses, including its narrow focus on linearity in model dynamics and noise, which oversimplifies real-world complexities. Furthermore, the limited scope of datasets utilized raises concerns regarding the generalizability of results, and the lack of consideration for interactions among exogenous factors is a significant oversight. The paper would benefit from a more extensive evaluation across diverse datasets and real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a coherent structure that facilitates understanding of the proposed methodology. While the novelty of using reinforcement learning for this application is commendable, the empirical aspects fall short due to reliance on synthetic datasets, which may hinder reproducibility and external validity. The discussion surrounding model interpretability and sensitivity analysis is insufficient, leaving room for further exploration of the implications of model assumptions on results.\n\n# Summary Of The Review\nOverall, the paper introduces a novel reinforcement learning approach to estimating long-term effects but is limited by its narrow assumptions and dataset scope. Future work should address these gaps and consider broader applications in more complex environments to enhance the robustness and applicability of the proposed model.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) approach designed to estimate long-term treatment effects in nonstationary environments, addressing the limitations of traditional randomized experiments such as A/B tests. The authors propose a statistical model that incorporates both endogenous and exogenous variables to account for nonstationarity, along with an optimization framework that minimizes reconstruction loss. Empirical evaluations, including synthetic simulations and case studies involving Type-1 diabetes and an online store dataset, demonstrate the method's effectiveness in achieving statistically significant treatment effect estimates.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its rigorous statistical methodology and the introduction of a framework that specifically addresses the challenges associated with nonstationary environments. The theoretical analysis provides preliminary guarantees on the estimator's performance, which is a notable contribution to the field. However, the paper may over-rely on certain statistical assumptions, such as linear dynamics, which could limit the generalizability of the findings. Additionally, while the empirical results are promising, the discussion around the implications of limited data points may undermine the robustness of the conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and written clearly, making complex statistical concepts accessible to the reader. The quality of the methodology is high, with a solid theoretical foundation. However, the reproducibility of the findings could be improved by providing more detailed descriptions of the experimental setups and the datasets used, as well as making code and data publicly available. The novelty of the approach is significant, as it bridges the gap between traditional statistical methods and modern reinforcement learning techniques.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of reinforcement learning by introducing a statistically robust approach for estimating long-term treatment effects in nonstationary environments. While the theoretical and empirical aspects are compelling, certain assumptions and a lack of detailed reproducibility measures could be improved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a new method for off-policy evaluation in dynamic environments, proposing a linear dynamics model with an additive exogenous component. The authors claim that their approach improves the estimation of treatment effects in nonstationary settings. However, the methodology heavily relies on linear assumptions, and the findings are primarily validated using synthetic datasets along with a single online store dataset.\n\n# Strength And Weaknesses\nWhile the paper introduces a novel framework for off-policy evaluation, its reliance on linear assumptions presents significant weaknesses. The limited dataset evaluation raises concerns about the generalizability of the results. Additionally, the lack of a comprehensive theoretical analysis and error quantification undermines the robustness of the proposed methods. The paper also fails to account for potential confounding factors and does not address computational scalability, which could limit its applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written; however, the clarity suffers due to the absence of discussions around the implications of the linear assumptions and potential biases introduced by the model. The novelty lies in the proposed framework, but it is not substantiated with robust empirical evidence or extensive evaluation. Reproducibility may be hindered by the limited scope of experiments and lack of detailed methodology for integration with existing frameworks.\n\n# Summary Of The Review\nThe paper proposes a new framework for off-policy evaluation but is constrained by its reliance on linear assumptions and limited empirical validation. While the methodology is potentially interesting, significant gaps in theoretical analysis and practical applicability need to be addressed for the work to be impactful.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"A REINFORCEMENT LEARNING APPROACH TO ESTIMATING LONG-TERM EFFECTS IN NONSTATIONARY ENVIRONMENTS\" presents a reinforcement learning (RL) framework aimed at addressing the limitations of traditional randomized experiments (A/B tests) in measuring long-term effects. The authors propose a new algorithm for handling nonstationary dynamics, which they argue is essential for realistic applications. The paper includes a theoretical analysis and demonstrates the approach on synthetic datasets, as well as using a Type-1 Diabetes simulator to illustrate its applicability.\n\n# Strength And Weaknesses\nThe paper attempts to contribute to the field by framing long-term effect estimation as an RL problem, which could be beneficial for practitioners unfamiliar with these concepts. However, the contributions are largely derivative, lacking genuine novelty. The acknowledgment of nonstationary data is valuable, but the explanations provided are oversimplified, failing to add depth to the discussion. The empirical evaluation, while showing \"promising results,\" relies heavily on synthetic datasets, which raises concerns about the practical applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is adequate but at times assumes a level of understanding that may not be present for all readers. The methodology is presented in a straightforward manner, yet it lacks the rigor and depth expected in a significant contribution to the field. The novelty is minimal, as much of the work appears to repackage existing concepts rather than introduce groundbreaking ideas. The reproducibility of the results is questionable, given the reliance on synthetic data without comprehensive real-world validation.\n\n# Summary Of The Review\nOverall, this paper provides a basic exploration of applying reinforcement learning to long-term effect estimation but fails to offer significant new insights or methodologies. While it may serve as an introductory resource for newcomers to the field, it does not advance the state of the art in a meaningful way.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning (RL) approach aimed at estimating long-term treatment effects in nonstationary environments. The authors introduce an algorithm that jointly estimates long-term rewards alongside exogenous variables, emphasizing the potential of RL methods for dynamic treatment regimes. By leveraging both synthetic datasets and real-world online store data for validation, the authors demonstrate the effectiveness of their method in improving predictions over stationary baselines. The work also highlights the limitations of traditional randomized experiments in capturing long-term effects, advocating for more adaptive and efficient experimental designs.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach that integrates RL with causal inference frameworks, which could significantly enhance decision-making in dynamic contexts such as healthcare and marketing. The alternating minimization algorithm for parameter estimation is a noteworthy contribution, though it may benefit from the incorporation of advanced optimization techniques. However, the paper's reliance on linear assumptions in the dynamics model and the additive exogenous part is a limitation, as it restricts the exploration of potentially richer, nonlinear relationships. Moreover, while the validation on real-world data is commendable, the authors should broaden their validation efforts to ensure the method's generalizability across diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly conveys its contributions and findings. The methodology is sound, although it could be enhanced by discussing the incorporation of more sophisticated optimization techniques and exploring nonlinear models. The novelty of the approach is significant, as it addresses a critical gap in the literature regarding long-term treatment effects in nonstationary settings. However, the reproducibility of the results may depend on the availability of the synthetic and real-world datasets used, as well as the implementation details of the proposed algorithms.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of reinforcement learning and causal inference by introducing a novel approach for estimating long-term treatment effects. While it presents promising results and a solid theoretical foundation, further exploration of nonlinear models and validation across varied real-world scenarios would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel reinforcement learning approach aimed at estimating long-term treatment effects in nonstationary environments, which holds significant relevance across various real-world applications. The methodology involves adapting reinforcement learning techniques to effectively capture the complexities of nonstationary settings, with a focus on improving predictive accuracy. The findings demonstrate that the proposed method consistently outperforms traditional stationary models and naive averages across multiple datasets, including synthetic simulations, a Type-1 Diabetes simulator, and an online store dataset, thereby indicating its potential for practical applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its robust empirical results, which indicate the proposed non-stationary model's superior performance over baseline methods, as evidenced by lower logarithmic Mean Square Error (MSE) and Median Absolute Percentage Error (MAPE) across diverse scenarios. This suggests a significant advancement in the ability to estimate long-term treatment effects in dynamic environments. However, a potential weakness is the lack of detailed exploration of the theoretical foundations that underpin the proposed methodology, which may leave some questions regarding its broader applicability and limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers. The quality of the experimental design is high, with comprehensive benchmarking against various baseline methods. The novelty of the approach is noteworthy, as it addresses a critical gap in estimating treatment effects in nonstationary environments. However, reproducibility could be enhanced by providing more specific implementation details and access to code or data, which would facilitate further validation of the results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of reinforcement learning by offering a compelling method for estimating long-term treatment effects in nonstationary environments. The empirical results are strong and support the practical applicability of the proposed approach, although further clarification on theoretical aspects and reproducibility would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to understanding treatment effects in randomized experiments, specifically addressing the complexities associated with nonstationary environments. The authors propose a new methodology that utilizes a Markov process framework to analyze treatment effects over time. The findings indicate that their approach outperforms traditional methods, providing more accurate estimates of treatment effects, particularly in scenarios where treatment effects may evolve.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative application of a Markov process to the analysis of treatment effects, which adds depth to existing methodologies. The experimental results demonstrate significant improvements in accuracy compared to traditional A/B testing approaches. However, the paper suffers from several weaknesses, including its reliance on technical jargon that may hinder accessibility for non-expert readers and some issues with clarity and coherence in the writing. Additionally, the repetitive phrasing in the introduction detracts from the overall flow of the argument.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the core contributions of the paper are substantial, the clarity of communication could be improved. The abstract is dense with technical language, and complex sentences may alienate a broader audience. The methodology is described adequately, but the mathematical notation requires clearer explanations to enhance reproducibility. Visuals and figures lack sufficient context, making it difficult for readers to grasp their significance without extensive reference to the text.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of randomized experiment analysis through the introduction of a Markov process framework. However, its readability issues, use of technical jargon, and structural weaknesses hinder its accessibility and impact. Addressing these concerns could greatly enhance the paper's clarity and overall quality.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.213587828127268,
    -1.6192645225886317,
    -1.8155383014484876,
    -1.6711588026399442,
    -1.6920919441984161,
    -1.7210895020527912,
    -1.684576477710291,
    -1.9899399256734738,
    -1.709959086690021,
    -1.7492746795350784,
    -1.6398423882216,
    -1.4519391524365584,
    -1.6428054925478217,
    -1.7223329784840982,
    -1.6810270587806064,
    -1.6385375125884305,
    -1.982166637408545,
    -1.6518097337146458,
    -1.6736532458884492,
    -1.770929227052473,
    -1.8982523148177952,
    -1.715654586956896,
    -1.8318626930553819,
    -1.7249374871856162,
    -1.813635266897833,
    -1.7505498026990194,
    -1.7720731241583254,
    -1.6500111433070612,
    -1.8633198655896224
  ],
  "logp_cond": [
    [
      0.0,
      -1.9047120266724358,
      -1.904377811548307,
      -1.9137580413655957,
      -1.9160641447332953,
      -1.9058404889790894,
      -1.9695183795077513,
      -1.9129860330592958,
      -1.925862110087182,
      -1.9477184006764623,
      -1.9112001481802685,
      -1.9913977320502305,
      -1.8894012147109986,
      -1.9163522374750366,
      -1.911342120643225,
      -1.9326120304416787,
      -1.9312892484698057,
      -1.923032802731695,
      -1.937426258321438,
      -1.9274251106064288,
      -1.919059547608014,
      -1.9351680335180583,
      -1.938821266606007,
      -1.9219524040225815,
      -1.9591201384844317,
      -1.9244321881968631,
      -1.8910743611270855,
      -1.9256163289661685,
      -1.9716670431634333
    ],
    [
      -1.2207679644726512,
      0.0,
      -1.154402667097795,
      -1.1706917568101534,
      -1.1830609958737013,
      -1.2326032200026296,
      -1.256975000570915,
      -1.1711502332911516,
      -1.234507268021337,
      -1.2915430203384517,
      -1.1527484517208444,
      -1.3257725679505812,
      -1.1433174959845296,
      -1.1913011233464748,
      -1.1656221921153522,
      -1.265990656865286,
      -1.209596424867287,
      -1.2318463092001521,
      -1.2113150803331287,
      -1.2197711184281497,
      -1.2377164744786964,
      -1.2386545987905986,
      -1.254066535314441,
      -1.1951627776036158,
      -1.2733939242132157,
      -1.2171248364994258,
      -1.1824247584079453,
      -1.2246328970293272,
      -1.2635428492790564
    ],
    [
      -1.4124341213354286,
      -1.371926201749339,
      0.0,
      -1.3887282949239805,
      -1.3923316920192064,
      -1.3745826446105966,
      -1.4634920900428159,
      -1.3726597348541956,
      -1.3874544270198823,
      -1.4322423394149904,
      -1.3002629833931771,
      -1.5242912163513618,
      -1.344212070457284,
      -1.3379938947354957,
      -1.4448463108431833,
      -1.4165018873136181,
      -1.4725579113717215,
      -1.3977036140709826,
      -1.4120308499862202,
      -1.3398412631761296,
      -1.435231838910928,
      -1.488834187813372,
      -1.4761441884957298,
      -1.3518940425223487,
      -1.437468771998489,
      -1.4974593072089435,
      -1.3243932918729016,
      -1.469300889869878,
      -1.478063113980961
    ],
    [
      -1.301234087447055,
      -1.2123441178483767,
      -1.2534588095484076,
      0.0,
      -1.278039120339511,
      -1.2245029473308582,
      -1.3159335782151236,
      -1.176074685365523,
      -1.2255301162144223,
      -1.2814702586746698,
      -1.0681608118635233,
      -1.403604740431287,
      -1.206287294862005,
      -1.1109241252596314,
      -1.1824689698487,
      -1.1637889111542399,
      -1.2985847257141703,
      -1.2813605771407686,
      -1.2140357566696984,
      -1.2168270575625932,
      -1.2237001805881258,
      -1.336546256942784,
      -1.3567823925822748,
      -1.158852246921101,
      -1.252536050196782,
      -1.2822897274833025,
      -1.217687627347447,
      -1.1836312035901841,
      -1.308270698952937
    ],
    [
      -1.3629786930977703,
      -1.2395434091434883,
      -1.2383856481467022,
      -1.2858820763688603,
      0.0,
      -1.2689821180769139,
      -1.3603498929910214,
      -1.2916834346843833,
      -1.3181240943953403,
      -1.2940161856304213,
      -1.2474373834880377,
      -1.3686034292551645,
      -1.264668140445984,
      -1.2497531455969748,
      -1.254655445873042,
      -1.2879972844505339,
      -1.2765584940523043,
      -1.2794695271235463,
      -1.2493447542187663,
      -1.3063215367861067,
      -1.3094352482684393,
      -1.3304334523104708,
      -1.2688964330606534,
      -1.26207949285465,
      -1.2936192436592027,
      -1.223560344573524,
      -1.195825902899514,
      -1.3776261391215596,
      -1.3738691356631156
    ],
    [
      -1.3533347539811271,
      -1.3123774596139561,
      -1.2832707915454784,
      -1.2788326592083554,
      -1.249387783161895,
      0.0,
      -1.3316829291262295,
      -1.2797689745255454,
      -1.29856732735117,
      -1.3613903433256196,
      -1.1958304617065187,
      -1.4116686021922311,
      -1.2907815693962457,
      -1.268368278182921,
      -1.313686702196079,
      -1.2593461261496677,
      -1.3678741279999935,
      -1.2284444422656213,
      -1.2780180332063271,
      -1.2970048375608025,
      -1.3246832776202142,
      -1.4152887386814583,
      -1.3849041553085104,
      -1.2577869848013938,
      -1.3322420138032638,
      -1.3530238821509901,
      -1.2509801283349016,
      -1.3257422442442284,
      -1.3812802517145908
    ],
    [
      -1.354354577992166,
      -1.3020046777317797,
      -1.2700996468325847,
      -1.2941945424912833,
      -1.3033134097381045,
      -1.3112240704559053,
      0.0,
      -1.2849960701018732,
      -1.2986707822292536,
      -1.3263724138820423,
      -1.2811865028630163,
      -1.3330148032847104,
      -1.304873067095548,
      -1.2688086675058634,
      -1.349212648740591,
      -1.3272083884630497,
      -1.3388455602468923,
      -1.281930209643738,
      -1.3000024126010354,
      -1.298572951766837,
      -1.3236014486950116,
      -1.3372888624294408,
      -1.350202676012017,
      -1.3143185407769287,
      -1.3418638664236626,
      -1.3143086813937404,
      -1.2615212827891142,
      -1.3188687101769512,
      -1.3340899297568998
    ],
    [
      -1.6362433308760997,
      -1.5431738678364395,
      -1.5131795048870267,
      -1.4300562902212277,
      -1.5517679264122801,
      -1.5264853164012797,
      -1.623994793206134,
      0.0,
      -1.438679858668331,
      -1.5612838028149247,
      -1.4392275010479425,
      -1.6924864142754426,
      -1.4747414151114695,
      -1.4045101972122649,
      -1.4864268019451043,
      -1.4642563891925948,
      -1.548076119616096,
      -1.524240374814007,
      -1.5389977365378458,
      -1.4697148335611416,
      -1.5470836514039996,
      -1.618958596744257,
      -1.645815668950362,
      -1.467100179442083,
      -1.5659784693037624,
      -1.4860044678771065,
      -1.4815223048470165,
      -1.4858764654027372,
      -1.5977205266556043
    ],
    [
      -1.3349529956412225,
      -1.2577422897834045,
      -1.2281043387470318,
      -1.167184566692554,
      -1.3089542910077876,
      -1.277326376775029,
      -1.3611559361104681,
      -1.1963525658041805,
      0.0,
      -1.3398781168749612,
      -1.1533723101831856,
      -1.4355250269921143,
      -1.2023880239806384,
      -1.2124639051815942,
      -1.2429870067274706,
      -1.2624444286386838,
      -1.2883326964218929,
      -1.2749179172984524,
      -1.2988073714628727,
      -1.2139884881512886,
      -1.3026472253350194,
      -1.3287337155549193,
      -1.33601012114686,
      -1.1938951845186137,
      -1.3272775759882809,
      -1.277134680961616,
      -1.2421453424082818,
      -1.2225870691093352,
      -1.3705296059210714
    ],
    [
      -1.3433765327408178,
      -1.4201223446978677,
      -1.2993367152925392,
      -1.353502434364797,
      -1.3733195911693197,
      -1.3629438245731398,
      -1.3896199483262692,
      -1.3468105703621083,
      -1.355217274295449,
      0.0,
      -1.3205804828733267,
      -1.4383509193104513,
      -1.3429055444530207,
      -1.3094075211971894,
      -1.419922520213115,
      -1.3434671929307302,
      -1.3793753030277593,
      -1.3449001283795505,
      -1.312070920808925,
      -1.3444711183894782,
      -1.2776372905222138,
      -1.4267156339793572,
      -1.4046344571175529,
      -1.3047544613898483,
      -1.302257895730188,
      -1.395228256352956,
      -1.299327714558142,
      -1.3725448424556017,
      -1.3233250320030663
    ],
    [
      -1.2575482597446284,
      -1.1803059579302586,
      -1.0751035824799313,
      -1.0222760142288077,
      -1.2189198687737026,
      -1.137366246174568,
      -1.2835132072266224,
      -1.1036492142555163,
      -1.0901697869631137,
      -1.217117227483786,
      0.0,
      -1.3513529627883691,
      -1.1049083296310183,
      -1.051770156163402,
      -1.123711270525913,
      -1.1446404645422052,
      -1.2368844164729778,
      -1.2159480526394337,
      -1.1819045527748848,
      -1.0819638370062927,
      -1.2024835425009919,
      -1.3321062359646147,
      -1.2628385469427774,
      -1.0979604723614067,
      -1.2129073191607964,
      -1.2057645463948905,
      -1.1030895417148807,
      -1.202676169144719,
      -1.2573654664302578
    ],
    [
      -1.2281021748206435,
      -1.2256927077648825,
      -1.2348337480457605,
      -1.2236368755370783,
      -1.249927034515531,
      -1.2114786597904093,
      -1.2450905840257962,
      -1.2271052032565803,
      -1.2184340090113068,
      -1.218554470036021,
      -1.2313526568098003,
      0.0,
      -1.2296003861433367,
      -1.225346585268417,
      -1.2194218573921316,
      -1.2354765825329486,
      -1.2193703673768608,
      -1.220962347462346,
      -1.235292910964882,
      -1.2140260304764894,
      -1.1956729442079472,
      -1.1972561098204444,
      -1.2082678295762914,
      -1.2159441379261893,
      -1.1938574866500866,
      -1.239827946793484,
      -1.211090984995745,
      -1.2382429020885755,
      -1.181873901988869
    ],
    [
      -1.2000677899038377,
      -1.1238930707599357,
      -1.142539325565718,
      -1.1488883447646179,
      -1.216533394794684,
      -1.1587503177771281,
      -1.3066970070379158,
      -1.1781191106226083,
      -1.1723004892803506,
      -1.224173804684625,
      -1.0960701852384231,
      -1.374463037191388,
      0.0,
      -1.1632699820712955,
      -1.1327143804529403,
      -1.2035151944251672,
      -1.2536614525717356,
      -1.1863487266804376,
      -1.193318617960084,
      -1.204206828982605,
      -1.2248595849991433,
      -1.3000281476655162,
      -1.233368035481786,
      -1.1181769989828094,
      -1.2141067874158047,
      -1.2357927689938322,
      -1.0943792565559296,
      -1.214696479177372,
      -1.3267355185737384
    ],
    [
      -1.3729277969525244,
      -1.2892164931867753,
      -1.2729645184497558,
      -1.150722841957003,
      -1.3160889837785184,
      -1.3033830545155283,
      -1.3514554324487495,
      -1.2166419665441008,
      -1.2593322771429514,
      -1.336083195258556,
      -1.1821755885834935,
      -1.4216980060915776,
      -1.2849055673049377,
      0.0,
      -1.2134543429389602,
      -1.25152068017866,
      -1.3280625647650224,
      -1.3011056276823159,
      -1.3108610160591339,
      -1.2615450834146946,
      -1.313921856519581,
      -1.3884870475079303,
      -1.4087904013685353,
      -1.2414562287001514,
      -1.3522723852529994,
      -1.3232561311786528,
      -1.2796727238003665,
      -1.2871944430627642,
      -1.3407495823368163
    ],
    [
      -1.3161535150083878,
      -1.1877551741565155,
      -1.2569157010783862,
      -1.1793248593961418,
      -1.236630279054307,
      -1.2906400798776123,
      -1.359883427289948,
      -1.1649225311844178,
      -1.2210846818687582,
      -1.3647726472506947,
      -1.179984566517081,
      -1.3536892454980256,
      -1.1507789884170134,
      -1.1858243348389526,
      0.0,
      -1.2322729275342514,
      -1.2656015270897378,
      -1.249356612909231,
      -1.3074595297333034,
      -1.1789733827392763,
      -1.287419486659668,
      -1.2918593686145614,
      -1.3002879331776398,
      -1.2320840168584202,
      -1.3308244176140231,
      -1.188250005402528,
      -1.2000940991120534,
      -1.2498568366799805,
      -1.280147590334077
    ],
    [
      -1.2584457900791581,
      -1.1766552539186,
      -1.1873840201604324,
      -1.044294632107194,
      -1.1765069215921586,
      -1.1238265431275305,
      -1.2590697171909846,
      -1.0847489049882928,
      -1.140035564593175,
      -1.2653030390572786,
      -1.076024621987421,
      -1.345020049367327,
      -1.1407994385688416,
      -1.0631935476942755,
      -1.1466662933353313,
      0.0,
      -1.2681833525006014,
      -1.1348716195435953,
      -1.1463003301382932,
      -1.1187668424051556,
      -1.2250595978951708,
      -1.2832170044241922,
      -1.2895614436915475,
      -1.1153015018693588,
      -1.2680904199822416,
      -1.1942176945477827,
      -1.1428357741467954,
      -1.1857547281269711,
      -1.2569047004618383
    ],
    [
      -1.631645020984763,
      -1.499009243399823,
      -1.5784230117144877,
      -1.5424466972547357,
      -1.5513686032067766,
      -1.6074912295356436,
      -1.633975995510865,
      -1.5388889707338163,
      -1.5484025435270972,
      -1.6182542622195752,
      -1.535689674927725,
      -1.70036497252334,
      -1.5697186833531724,
      -1.5599616618220722,
      -1.5604402423439345,
      -1.60970642068601,
      0.0,
      -1.6375379398665122,
      -1.6542374125901498,
      -1.581052706005519,
      -1.5803497450376,
      -1.5805058384963004,
      -1.5773972628193498,
      -1.5183417549572475,
      -1.557202535584695,
      -1.5352755527657653,
      -1.5440122884558605,
      -1.5882572232859202,
      -1.6715386320072252
    ],
    [
      -1.2584270795001231,
      -1.1386155757451897,
      -1.139606962759619,
      -1.1205648637526953,
      -1.1695740486578134,
      -1.0964620858436813,
      -1.2135722570368264,
      -1.0750015381131388,
      -1.0638526810293982,
      -1.2436156407824268,
      -1.074428266897137,
      -1.2838823451294408,
      -1.0908570884950393,
      -1.091494702369015,
      -1.1333715394148445,
      -1.1083610764370972,
      -1.25843773409668,
      0.0,
      -1.1608671784439408,
      -1.124428324409399,
      -1.214134745793818,
      -1.253232520979074,
      -1.254661109684297,
      -1.0553362035397709,
      -1.212341311984159,
      -1.2339283854790806,
      -1.1586109098862427,
      -1.1655112404678392,
      -1.1978142292415763
    ],
    [
      -1.3203542793055003,
      -1.2745137948358634,
      -1.235134684001133,
      -1.2344487026267854,
      -1.2692576377125213,
      -1.2440150423932304,
      -1.2945301904024837,
      -1.2338102267656819,
      -1.2535233552294152,
      -1.2688565400040843,
      -1.2107411513343134,
      -1.3808245569599145,
      -1.2528449722306185,
      -1.261102304970079,
      -1.315089195924086,
      -1.2261601825715427,
      -1.335829292839096,
      -1.2568087197733562,
      0.0,
      -1.2521684070681698,
      -1.2431962518168236,
      -1.3585308545853463,
      -1.3409817332500265,
      -1.2796893418715996,
      -1.2754413412483767,
      -1.3214930187439051,
      -1.26842568506531,
      -1.304637676733629,
      -1.2987382640151108
    ],
    [
      -1.4172619647621074,
      -1.3766352597483644,
      -1.2964112221116473,
      -1.3094118524778358,
      -1.3733772794302628,
      -1.335354395836673,
      -1.4019395535786583,
      -1.2732263616373745,
      -1.2856746868214124,
      -1.382175673010412,
      -1.194965531709009,
      -1.479372711884815,
      -1.33074510240009,
      -1.2814515971411247,
      -1.3126642348714268,
      -1.3106805483477464,
      -1.3836305483216327,
      -1.389590296546558,
      -1.3441894045222798,
      0.0,
      -1.387315450588571,
      -1.411598905688549,
      -1.4116147723981425,
      -1.318355766586478,
      -1.3846142324245332,
      -1.3553643567337161,
      -1.2928989478467463,
      -1.2760265654696548,
      -1.4039410885133674
    ],
    [
      -1.5139834757713406,
      -1.5270654025060806,
      -1.4736123972123656,
      -1.4146325684817966,
      -1.5062674883384735,
      -1.4945083901201173,
      -1.5405040680908715,
      -1.4864171648562063,
      -1.5339634493373686,
      -1.4737634979829932,
      -1.4709425311076776,
      -1.5816762975882608,
      -1.49620218621871,
      -1.4524305482152664,
      -1.5278894228009403,
      -1.4687196147524193,
      -1.5327363509291476,
      -1.4869836853881064,
      -1.4700318771113154,
      -1.4995053366381434,
      0.0,
      -1.573527192019907,
      -1.5626822720935374,
      -1.4850763672714742,
      -1.497245199847037,
      -1.5377560457397323,
      -1.4337857842227721,
      -1.4735057463581072,
      -1.50277215444218
    ],
    [
      -1.3446261319810107,
      -1.3216585368715041,
      -1.346592526777425,
      -1.3411536422027837,
      -1.3561692050430436,
      -1.3780614960529376,
      -1.369470134056362,
      -1.3686093348529211,
      -1.369869353984218,
      -1.357846594934495,
      -1.33978148327647,
      -1.4350139544085645,
      -1.3185751994719435,
      -1.3593753631949144,
      -1.3129781427325036,
      -1.3519157044203414,
      -1.3384370827523298,
      -1.3519450486805666,
      -1.3482917181357241,
      -1.33823351802529,
      -1.3630533863860441,
      0.0,
      -1.3312452294672308,
      -1.3201797798928852,
      -1.395085641751777,
      -1.2957814390062032,
      -1.3326149649023125,
      -1.3312774139624863,
      -1.334727261224129
    ],
    [
      -1.5004922031249,
      -1.4813512122984793,
      -1.4797898204552882,
      -1.5406591628483584,
      -1.4705652285616728,
      -1.5092165112910854,
      -1.5799416748161148,
      -1.475280348110307,
      -1.4620513416957734,
      -1.4580379770084884,
      -1.5230417299253036,
      -1.5388006382386625,
      -1.486174219549855,
      -1.5047657163182937,
      -1.4919093834811072,
      -1.5200581976035652,
      -1.4103319441840794,
      -1.5197453061181048,
      -1.5398048885787872,
      -1.4726663292554116,
      -1.5082722320212993,
      -1.5216290352659239,
      0.0,
      -1.5074457399986945,
      -1.4709988813002877,
      -1.4482915202473983,
      -1.4575578605029567,
      -1.533237314067686,
      -1.5076759120225494
    ],
    [
      -1.367481183436762,
      -1.2998023805871066,
      -1.2634586467866435,
      -1.191138599435025,
      -1.2782708120960036,
      -1.308277286331212,
      -1.3371018959877468,
      -1.2268043433486677,
      -1.1916259962933538,
      -1.340066628554394,
      -1.1652363136732535,
      -1.4513482299772285,
      -1.2280726861607187,
      -1.1861267735046068,
      -1.2528057433943078,
      -1.221735177157984,
      -1.302517154484458,
      -1.3015219742428863,
      -1.3051283419619986,
      -1.229127929246972,
      -1.294343348327232,
      -1.3224335356070502,
      -1.3836184331635544,
      0.0,
      -1.3216822279857054,
      -1.2965273112572784,
      -1.2443064968579136,
      -1.2390057571978532,
      -1.362633791411581
    ],
    [
      -1.4714568276963789,
      -1.4595889059501332,
      -1.405923667473897,
      -1.4286947757530648,
      -1.4553932965410492,
      -1.4310116166905897,
      -1.509973414606727,
      -1.4269533800713021,
      -1.4480351278857233,
      -1.4217417868130275,
      -1.3947845018427105,
      -1.5295311890344774,
      -1.4429584962814073,
      -1.404699233231086,
      -1.487905563520096,
      -1.4792404195937225,
      -1.4326834631663357,
      -1.4681962611847221,
      -1.4508460422153688,
      -1.4027652350067636,
      -1.3985597853363851,
      -1.533756750255612,
      -1.4579166349082546,
      -1.3886837760904038,
      0.0,
      -1.5511471313766552,
      -1.3458688776209708,
      -1.4424585608714016,
      -1.470038161294581
    ],
    [
      -1.4101396727311635,
      -1.3758082168423567,
      -1.4110891637611531,
      -1.342839414973314,
      -1.3553151698857917,
      -1.3954514732056853,
      -1.4065570433801584,
      -1.3031650894444833,
      -1.366935061762539,
      -1.447745744875307,
      -1.3484882142390142,
      -1.5021388151910025,
      -1.3727285193518344,
      -1.3473847718931358,
      -1.2931652771352258,
      -1.3557248216348627,
      -1.3489106883435666,
      -1.4012817082870244,
      -1.4287507019404797,
      -1.3759061877921313,
      -1.3988833819767323,
      -1.3942539261578533,
      -1.378035590347241,
      -1.3767683704398441,
      -1.4559900859959993,
      0.0,
      -1.3816095480387773,
      -1.3860340265582933,
      -1.4522611934968856
    ],
    [
      -1.4449776560283216,
      -1.4335542570989446,
      -1.3606755351272686,
      -1.363521631722289,
      -1.4010238297027142,
      -1.4142261149240427,
      -1.4944909264090858,
      -1.3509798953552774,
      -1.3764616679305075,
      -1.3889131270152562,
      -1.3718451928288609,
      -1.5025591990492189,
      -1.3271533091099343,
      -1.3798440646150218,
      -1.3790074936636099,
      -1.4185808735510304,
      -1.4140218683560095,
      -1.4346850911352766,
      -1.403343992469644,
      -1.3599475542845514,
      -1.3883559816104152,
      -1.4700319248108151,
      -1.430810618822699,
      -1.390444258988834,
      -1.3749879738156825,
      -1.4297050655604289,
      0.0,
      -1.4344242853852247,
      -1.4904924969505609
    ],
    [
      -1.3297701864868705,
      -1.2320564561326008,
      -1.261211128790255,
      -1.1572369405457714,
      -1.3444513907942057,
      -1.318092767487517,
      -1.3166330805425177,
      -1.1569268075636023,
      -1.1616614388707964,
      -1.2903223934987034,
      -1.159057967013103,
      -1.3691764791922245,
      -1.2561366531000067,
      -1.194948044636371,
      -1.2387516634472362,
      -1.2361224675891969,
      -1.2920320367649702,
      -1.2510108133030888,
      -1.277903869677727,
      -1.1487739255561038,
      -1.2456715348002274,
      -1.3086786233857688,
      -1.3481763258932933,
      -1.2085710500355127,
      -1.292959215282174,
      -1.3188472972667946,
      -1.2343672089833935,
      0.0,
      -1.3273188755971892
    ],
    [
      -1.4640008076008455,
      -1.431614917898164,
      -1.44580799884312,
      -1.4551708302846116,
      -1.5067992494510867,
      -1.467390625022993,
      -1.516883994820351,
      -1.458570179675086,
      -1.4705734910134256,
      -1.4712733492282564,
      -1.4721842998954755,
      -1.5249380075970804,
      -1.441670816478916,
      -1.4582127714031718,
      -1.5006648293681453,
      -1.4869299319054634,
      -1.496718824911812,
      -1.4785836330492537,
      -1.5179889305360008,
      -1.4960997309006605,
      -1.468846481940324,
      -1.4233004678448362,
      -1.504641864924679,
      -1.4425975567124811,
      -1.4778584213990196,
      -1.5015288090487298,
      -1.4794475304378463,
      -1.4821891502182316,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.3088758014548323,
      0.3092100165789611,
      0.2998297867616724,
      0.2975236833939727,
      0.30774733914817864,
      0.24406944861951674,
      0.30060179506797224,
      0.28772571804008606,
      0.2658694274508058,
      0.30238767994699955,
      0.22219009607703755,
      0.3241866134162694,
      0.2972355906522315,
      0.302245707484043,
      0.28097579768558933,
      0.2822985796574624,
      0.2905550253955731,
      0.27616156980582995,
      0.2861627175208392,
      0.29452828051925417,
      0.27841979460920974,
      0.2747665615212611,
      0.29163542410468657,
      0.2544676896428364,
      0.28915563993040494,
      0.3225134670001826,
      0.28797149916109954,
      0.24192078496383473
    ],
    [
      0.3984965581159805,
      0.0,
      0.4648618554908368,
      0.4485727657784784,
      0.43620352671493046,
      0.3866613025860022,
      0.3622895220177167,
      0.4481142892974801,
      0.3847572545672948,
      0.32772150225018004,
      0.46651607086778735,
      0.29349195463805056,
      0.47594702660410215,
      0.427963399242157,
      0.4536423304732795,
      0.35327386572334585,
      0.4096680977213447,
      0.3874182133884796,
      0.407949442255503,
      0.39949340416048207,
      0.38154804810993537,
      0.38060992379803316,
      0.36519798727419084,
      0.42410174498501596,
      0.345870598375416,
      0.4021396860892059,
      0.4368397641806865,
      0.39463162555930453,
      0.3557216733095754
    ],
    [
      0.40310418011305904,
      0.4436120996991486,
      0.0,
      0.42681000652450707,
      0.42320660942928123,
      0.44095565683789095,
      0.3520462114056717,
      0.44287856659429203,
      0.4280838744286053,
      0.38329596203349725,
      0.5152753180553105,
      0.2912470850971258,
      0.47132623099120363,
      0.4775444067129919,
      0.37069199060530433,
      0.39903641413486945,
      0.34298039007676606,
      0.41783468737750495,
      0.4035074514622674,
      0.475697038272358,
      0.3803064625375596,
      0.32670411363511564,
      0.3393941129527578,
      0.4636442589261389,
      0.3780695294499985,
      0.3180789942395441,
      0.49114500957558604,
      0.34623741157860954,
      0.3374751874675266
    ],
    [
      0.3699247151928893,
      0.4588146847915675,
      0.4176999930915366,
      0.0,
      0.39311968230043326,
      0.446655855309086,
      0.35522522442482063,
      0.49508411727442114,
      0.4456286864255219,
      0.3896885439652744,
      0.6029979907764209,
      0.2675540622086572,
      0.4648715077779393,
      0.5602346773803129,
      0.4886898327912441,
      0.5073698914857043,
      0.37257407692577393,
      0.38979822549917564,
      0.4571230459702458,
      0.45433174507735097,
      0.4474586220518184,
      0.33461254569716026,
      0.31437641005766936,
      0.5123065557188433,
      0.41862275244316227,
      0.3888690751566417,
      0.45347117529249714,
      0.48752759904976006,
      0.3628881036870073
    ],
    [
      0.3291132511006458,
      0.45254853505492787,
      0.453706296051714,
      0.4062098678295558,
      0.0,
      0.4231098261215023,
      0.33174205120739475,
      0.40040850951403284,
      0.37396784980307585,
      0.3980757585679948,
      0.44465456071037845,
      0.3234885149432516,
      0.42742380375243205,
      0.4423387986014413,
      0.43743649832537423,
      0.4040946597478823,
      0.41553345014611187,
      0.4126224170748698,
      0.4427471899796498,
      0.3857704074123094,
      0.38265669592997686,
      0.3616584918879453,
      0.42319551113776277,
      0.4300124513437662,
      0.39847270053921346,
      0.4685315996248922,
      0.49626604129890217,
      0.3144658050768565,
      0.3182228085353005
    ],
    [
      0.3677547480716641,
      0.4087120424388351,
      0.4378187105073128,
      0.4422568428444358,
      0.4717017188908963,
      0.0,
      0.38940657292656167,
      0.4413205275272458,
      0.42252217470162123,
      0.3596991587271716,
      0.5252590403462725,
      0.3094208998605601,
      0.43030793265654554,
      0.4527212238698701,
      0.40740279985671224,
      0.46174337590312353,
      0.3532153740527977,
      0.4926450597871699,
      0.4430714688464641,
      0.42408466449198867,
      0.396406224432577,
      0.3058007633713329,
      0.3361853467442808,
      0.46330251725139737,
      0.3888474882495274,
      0.3680656199018011,
      0.47010937371788963,
      0.39534725780856284,
      0.3398092503382004
    ],
    [
      0.330221899718125,
      0.3825717999785112,
      0.41447683087770626,
      0.39038193521900766,
      0.38126306797218645,
      0.3733524072543857,
      0.0,
      0.39958040760841773,
      0.38590569548103737,
      0.35820406382824865,
      0.4033899748472747,
      0.3515616744255805,
      0.379703410614743,
      0.41576781020442755,
      0.33536382896969985,
      0.3573680892472413,
      0.3457309174633987,
      0.402646268066553,
      0.3845740651092555,
      0.386003525943454,
      0.36097502901527934,
      0.34728761528085017,
      0.334373801698274,
      0.3702579369333623,
      0.34271261128662833,
      0.3702677963165506,
      0.4230551949211767,
      0.3657077675333398,
      0.3504865479533912
    ],
    [
      0.35369659479737403,
      0.44676605783703427,
      0.4767604207864471,
      0.5598836354522461,
      0.43817199926119366,
      0.46345460927219406,
      0.36594513246733973,
      0.0,
      0.5512600670051429,
      0.42865612285854904,
      0.5507124246255313,
      0.2974535113980312,
      0.5151985105620043,
      0.5854297284612089,
      0.5035131237283694,
      0.525683536480879,
      0.44186380605737785,
      0.4656995508594668,
      0.450942189135628,
      0.5202250921123321,
      0.44285627426947416,
      0.37098132892921676,
      0.34412425672311175,
      0.5228397462313907,
      0.4239614563697114,
      0.5039354577963673,
      0.5084176208264573,
      0.5040634602707366,
      0.3922193990178695
    ],
    [
      0.37500609104879845,
      0.4522167969066164,
      0.4818547479429891,
      0.542774519997467,
      0.40100479568223335,
      0.43263270991499203,
      0.3488031505795528,
      0.5136065208858405,
      0.0,
      0.3700809698150598,
      0.5565867765068353,
      0.2744340596979067,
      0.5075710627093826,
      0.49749518150842675,
      0.4669720799625503,
      0.44751465805133717,
      0.4216263902681281,
      0.43504116939156856,
      0.4111517152271482,
      0.4959705985387324,
      0.4073118613550015,
      0.3812253711351017,
      0.3739489655431609,
      0.5160639021714073,
      0.38268151070174006,
      0.43282440572840497,
      0.46781374428173916,
      0.4873720175806857,
      0.3394294807689495
    ],
    [
      0.4058981467942606,
      0.32915233483721074,
      0.4499379642425392,
      0.3957722451702814,
      0.37595508836575875,
      0.38633085496193864,
      0.3596547312088092,
      0.40246410917297015,
      0.3940574052396295,
      0.0,
      0.4286941966617517,
      0.3109237602246271,
      0.40636913508205774,
      0.43986715833788903,
      0.3293521593219635,
      0.4058074866043482,
      0.3698993765073191,
      0.4043745511555279,
      0.43720375872615347,
      0.40480356114560023,
      0.47163738901286467,
      0.32255904555572124,
      0.34464022241752557,
      0.44452021814523013,
      0.4470167838048904,
      0.3540464231821223,
      0.4499469649769363,
      0.3767298370794767,
      0.42594964753201214
    ],
    [
      0.38229412847697164,
      0.4595364302913414,
      0.5647388057416687,
      0.6175663739927924,
      0.4209225194478974,
      0.5024761420470321,
      0.35632918099497757,
      0.5361931739660837,
      0.5496726012584863,
      0.42272516073781397,
      0.0,
      0.28848942543323086,
      0.5349340585905817,
      0.5880722320581979,
      0.5161311176956871,
      0.49520192367939475,
      0.40295797174862225,
      0.42389433558216627,
      0.4579378354467152,
      0.5578785512153073,
      0.4373588457206081,
      0.30773615225698525,
      0.3770038412788226,
      0.5418819158601933,
      0.4269350690608036,
      0.4340778418267095,
      0.5367528465067193,
      0.437166219076881,
      0.3824769217913422
    ],
    [
      0.2238369776159148,
      0.22624644467167587,
      0.21710540439079784,
      0.22830227689948002,
      0.20201211792102725,
      0.2404604926461491,
      0.20684856841076216,
      0.224833949179978,
      0.23350514342525153,
      0.23338468240053745,
      0.22058649562675803,
      0.0,
      0.2223387662932217,
      0.22659256716814125,
      0.23251729504442675,
      0.2164625699036098,
      0.23256878505969758,
      0.23097680497421225,
      0.21664624147167633,
      0.23791312196006897,
      0.2562662082286111,
      0.25468304261611396,
      0.24367132286026694,
      0.23599501451036908,
      0.25808166578647174,
      0.21211120564307429,
      0.2408481674408134,
      0.2136962503479829,
      0.27006525044768925
    ],
    [
      0.44273770264398404,
      0.518912421787886,
      0.5002661669821038,
      0.49391714778320384,
      0.4262720977531378,
      0.4840551747706936,
      0.3361084855099059,
      0.4646863819252134,
      0.47050500326747113,
      0.4186316878631966,
      0.5467353073093986,
      0.26834245535643375,
      0.0,
      0.4795355104765262,
      0.5100911120948814,
      0.4392902981226545,
      0.38914403997608615,
      0.45645676586738415,
      0.4494868745877376,
      0.4385986635652168,
      0.41794590754867844,
      0.3427773448823055,
      0.40943745706603574,
      0.5246284935650123,
      0.428698705132017,
      0.40701272355398954,
      0.5484262359918921,
      0.4281090133704497,
      0.3160699739740833
    ],
    [
      0.34940518153157374,
      0.43311648529732283,
      0.4493684600343424,
      0.5716101365270951,
      0.4062439947055798,
      0.4189499239685699,
      0.3708775460353486,
      0.5056910119399973,
      0.4630007013411468,
      0.38624978322554226,
      0.5401573899006047,
      0.30063497239252057,
      0.4374274111791605,
      0.0,
      0.508878635545138,
      0.47081229830543814,
      0.3942704137190758,
      0.4212273508017823,
      0.4114719624249643,
      0.46078789506940354,
      0.40841112196451723,
      0.3338459309761679,
      0.3135425771155629,
      0.4808767497839468,
      0.37006059323109874,
      0.39907684730544535,
      0.4426602546837317,
      0.43513853542133396,
      0.3815833961472819
    ],
    [
      0.3648735437722186,
      0.4932718846240909,
      0.4241113577022202,
      0.5017021993844646,
      0.44439677972629954,
      0.3903869789029941,
      0.3211436314906584,
      0.5161045275961886,
      0.45994237691184825,
      0.31625441152991174,
      0.5010424922635255,
      0.3273378132825808,
      0.530248070363593,
      0.4952027239416539,
      0.0,
      0.448754131246355,
      0.4154255316908686,
      0.4316704458713754,
      0.37356752904730306,
      0.5020536760413301,
      0.39360757212093844,
      0.389167690166045,
      0.38073912560296663,
      0.44894304192218626,
      0.3502026411665833,
      0.4927770533780784,
      0.480932959668553,
      0.43117022210062594,
      0.40087946844652933
    ],
    [
      0.38009172250927237,
      0.46188225866983057,
      0.45115349242799807,
      0.5942428804812365,
      0.46203059099627186,
      0.5147109694609,
      0.3794677953974459,
      0.5537886076001377,
      0.4985019479952555,
      0.3732344735311519,
      0.5625128906010095,
      0.2935174632211035,
      0.4977380740195889,
      0.575343964894155,
      0.4918712192530992,
      0.0,
      0.37035416008782907,
      0.5036658930448352,
      0.49223718245013726,
      0.5197706701832749,
      0.4134779146932597,
      0.3553205081642383,
      0.34897606889688304,
      0.5232360107190717,
      0.3704470926061889,
      0.4443198180406478,
      0.4957017384416351,
      0.45278278446145936,
      0.38163281212659217
    ],
    [
      0.3505216164237819,
      0.48315739400872193,
      0.4037436256940572,
      0.4397199401538092,
      0.4307980342017683,
      0.3746754078729013,
      0.3481906418976799,
      0.4432776666747287,
      0.43376409388144777,
      0.3639123751889697,
      0.44647696248082,
      0.28180166488520486,
      0.4124479540553725,
      0.42220497558647274,
      0.4217263950646104,
      0.3724602167225348,
      0.0,
      0.34462869754203274,
      0.32792922481839515,
      0.40111393140302587,
      0.40181689237094487,
      0.4016607989122445,
      0.40476937458919515,
      0.4638248824512974,
      0.4249641018238499,
      0.4468910846427796,
      0.43815434895268446,
      0.3939094141226247,
      0.31062800540131974
    ],
    [
      0.3933826542145227,
      0.5131941579694561,
      0.5122027709550268,
      0.5312448699619505,
      0.48223568505683234,
      0.5553476478709645,
      0.4382374766778194,
      0.576808195601507,
      0.5879570526852476,
      0.408194092932219,
      0.5773814668175088,
      0.367927388585205,
      0.5609526452196065,
      0.5603150313456309,
      0.5184381942998013,
      0.5434486572775485,
      0.3933719996179659,
      0.0,
      0.49094255527070496,
      0.5273814093052467,
      0.4376749879208277,
      0.3985772127355718,
      0.3971486240303488,
      0.5964735301748749,
      0.4394684217304867,
      0.4178813482355652,
      0.4931988238284031,
      0.48629849324680663,
      0.45399550447306947
    ],
    [
      0.3532989665829489,
      0.39913945105258586,
      0.4385185618873162,
      0.4392045432616638,
      0.4043956081759279,
      0.42963820349521886,
      0.3791230554859655,
      0.43984301912276735,
      0.42012989065903406,
      0.4047967058843649,
      0.46291209455413584,
      0.29282868892853475,
      0.4208082736578307,
      0.4125509409183703,
      0.3585640499643632,
      0.4474930633169065,
      0.3378239530493532,
      0.41684452611509304,
      0.0,
      0.42148483882027943,
      0.43045699407162563,
      0.31512239130310293,
      0.33267151263842276,
      0.39396390401684966,
      0.3982119046400725,
      0.3521602271445441,
      0.40522756082313927,
      0.36901556915482026,
      0.3749149818733384
    ],
    [
      0.3536672622903656,
      0.3942939673041086,
      0.47451800494082574,
      0.46151737457463726,
      0.39755194762221024,
      0.4355748312158001,
      0.3689896734738147,
      0.49770286541509856,
      0.4852545402310606,
      0.388753554042061,
      0.5759636953434639,
      0.291556515167658,
      0.440184124652383,
      0.48947762991134836,
      0.4582649921810462,
      0.46024867870472663,
      0.38729867873084034,
      0.38133893050591494,
      0.42673982253019327,
      0.0,
      0.38361377646390205,
      0.35933032136392407,
      0.35931445465433054,
      0.452573460465995,
      0.38631499462793983,
      0.4155648703187569,
      0.4780302792057267,
      0.49490266158281826,
      0.3669881385391056
    ],
    [
      0.3842688390464546,
      0.37118691231171463,
      0.42463991760542963,
      0.4836197463359986,
      0.39198482647932176,
      0.40374392469767795,
      0.35774824672692374,
      0.4118351499615889,
      0.3642888654804266,
      0.424488816834802,
      0.42730978371011763,
      0.31657601722953443,
      0.4020501285990852,
      0.4458217666025288,
      0.3703628920168549,
      0.4295327000653759,
      0.3655159638886476,
      0.4112686294296888,
      0.4282204377064798,
      0.39874697817965177,
      0.0,
      0.32472512279788823,
      0.33557004272425783,
      0.41317594754632103,
      0.40100711497075814,
      0.36049626907806287,
      0.4644665305950231,
      0.42474656845968806,
      0.3954801603756153
    ],
    [
      0.3710284549758853,
      0.39399605008539185,
      0.36906206017947096,
      0.37450094475411233,
      0.3594853819138524,
      0.3375930909039584,
      0.346184452900534,
      0.34704525210397486,
      0.34578523297267805,
      0.3578079920224009,
      0.3758731036804259,
      0.2806406325483315,
      0.39707938748495253,
      0.3562792237619816,
      0.4026764442243924,
      0.3637388825365546,
      0.37721750420456623,
      0.36370953827632935,
      0.36736286882117186,
      0.377421068931606,
      0.35260120057085187,
      0.0,
      0.3844093574896652,
      0.39547480706401084,
      0.320568945205119,
      0.41987314795069275,
      0.38303962205458353,
      0.38437717299440965,
      0.380927325732767
    ],
    [
      0.3313704899304819,
      0.3505114807569025,
      0.3520728726000937,
      0.2912035302070235,
      0.36129746449370903,
      0.3226461817642965,
      0.25192101823926705,
      0.3565823449450749,
      0.3698113513596084,
      0.3738247160468935,
      0.30882096313007823,
      0.2930620548167193,
      0.3456884735055268,
      0.32709697673708815,
      0.3399533095742746,
      0.31180449545181665,
      0.4215307488713025,
      0.3121173869372771,
      0.2920578044765947,
      0.3591963637999702,
      0.3235904610340825,
      0.310233657789458,
      0.0,
      0.3244169530566874,
      0.3608638117550942,
      0.3835711728079836,
      0.3743048325524252,
      0.2986253789876958,
      0.3241867810328325
    ],
    [
      0.3574563037488543,
      0.4251351065985096,
      0.4614788403989727,
      0.5337988877505913,
      0.44666667508961266,
      0.41666020085440425,
      0.3878355911978695,
      0.4981331438369485,
      0.5333114908922625,
      0.38487085863122217,
      0.5597011735123627,
      0.2735892572083878,
      0.4968648010248975,
      0.5388107136810094,
      0.4721317437913084,
      0.5032023100276322,
      0.4224203327011582,
      0.42341551294272994,
      0.4198091452236177,
      0.49580955793864434,
      0.43059413885838427,
      0.402503951578566,
      0.34131905402206186,
      0.0,
      0.4032552591999108,
      0.42841017592833786,
      0.4806309903277026,
      0.485931729987763,
      0.3623036957740353
    ],
    [
      0.3421784392014542,
      0.3540463609476998,
      0.407711599423936,
      0.38494049114476825,
      0.35824197035678385,
      0.3826236502072433,
      0.3036618522911061,
      0.38668188682653093,
      0.36560013901210975,
      0.3918934800848055,
      0.41885076505512253,
      0.2841040778633557,
      0.3706767706164258,
      0.40893603366674713,
      0.3257297033777371,
      0.3343948473041105,
      0.3809518037314974,
      0.3454390057131109,
      0.3627892246824642,
      0.4108700318910694,
      0.4150754815614479,
      0.2798785166422211,
      0.3557186319895784,
      0.4249514908074292,
      0.0,
      0.2624881355211779,
      0.4677663892768622,
      0.3711767060264315,
      0.3435971056032521
    ],
    [
      0.3404101299678559,
      0.37474158585666273,
      0.33946063893786627,
      0.40771038772570534,
      0.3952346328132277,
      0.3550983294933341,
      0.343992759318861,
      0.4473847132545361,
      0.3836147409364803,
      0.30280405782371234,
      0.40206158846000517,
      0.24841098750801693,
      0.377821283347185,
      0.4031650308058836,
      0.4573845255637936,
      0.39482498106415664,
      0.40163911435545274,
      0.349268094411995,
      0.3217991007585397,
      0.37464361490688813,
      0.35166642072228704,
      0.3562958765411661,
      0.3725142123517784,
      0.37378143225917526,
      0.29455971670302006,
      0.0,
      0.36894025466024205,
      0.36451577614072606,
      0.29828860920213374
    ],
    [
      0.3270954681300038,
      0.33851886705938083,
      0.41139758903105683,
      0.4085514924360365,
      0.3710492944556112,
      0.3578470092342827,
      0.27758219774923965,
      0.421093228803048,
      0.3956114562278179,
      0.38315999714306925,
      0.40022793132946455,
      0.26951392510910654,
      0.44491981504839107,
      0.39222905954330356,
      0.39306563049471555,
      0.35349225060729506,
      0.3580512558023159,
      0.33738803302304876,
      0.3687291316886814,
      0.412125569873774,
      0.38371714254791023,
      0.3020411993475103,
      0.3412625053356264,
      0.38162886516949146,
      0.3970851503426429,
      0.34236805859789654,
      0.0,
      0.3376488387731007,
      0.28158062720776456
    ],
    [
      0.3202409568201907,
      0.4179546871744604,
      0.38880001451680624,
      0.4927742027612898,
      0.3055597525128555,
      0.3319183758195443,
      0.3333780627645435,
      0.4930843357434589,
      0.4883497044362648,
      0.35968874980835785,
      0.4909531762939583,
      0.2808346641148367,
      0.39387449020705456,
      0.4550630986706903,
      0.41125947985982503,
      0.41388867571786436,
      0.357979106542091,
      0.3990003300039724,
      0.37210727362933427,
      0.5012372177509574,
      0.4043396085068338,
      0.34133251992129243,
      0.30183481741376794,
      0.44144009327154854,
      0.35705192802488717,
      0.3311638460402666,
      0.41564393432366775,
      0.0,
      0.322692267709872
    ],
    [
      0.39931905798877687,
      0.4317049476914583,
      0.4175118667465023,
      0.4081490353050108,
      0.3565206161385357,
      0.3959292405666295,
      0.3464358707692714,
      0.4047496859145363,
      0.39274637457619677,
      0.392046516361366,
      0.39113556569414687,
      0.33838185799254195,
      0.42164904911070633,
      0.4051070941864505,
      0.3626550362214771,
      0.37638993368415896,
      0.3666010406778104,
      0.3847362325403687,
      0.34533093505362156,
      0.36722013468896186,
      0.3944733836492984,
      0.44001939774478616,
      0.35867800066494326,
      0.42072230887714124,
      0.3854614441906028,
      0.3617910565408926,
      0.3838723351517761,
      0.38113071537139076,
      0.0
    ]
  ],
  "row_avgs": [
    0.28647255484323725,
    0.40070369405624273,
    0.4032210450076605,
    0.4306256927793905,
    0.4035169411185415,
    0.410890649218672,
    0.37297114192028946,
    0.46266839691402434,
    0.4364648304964914,
    0.3954844484095506,
    0.4628336293494298,
    0.2306627440337421,
    0.4413171125974135,
    0.4237634843062033,
    0.42949678142721387,
    0.45578610732051816,
    0.39961320435083125,
    0.4878457463585971,
    0.3946836957356634,
    0.42376893021643064,
    0.3976027964091399,
    0.3673485409408811,
    0.33472725273786635,
    0.4423589515259913,
    0.3657490925295171,
    0.3643583069960959,
    0.3638921996468424,
    0.3901230489414463,
    0.38680245478926295
  ],
  "col_avgs": [
    0.36073907431515384,
    0.411564894541353,
    0.4255067459202334,
    0.4491702881078043,
    0.3943232200668339,
    0.40397272632852727,
    0.3415442197210492,
    0.44177064226264534,
    0.42554505118721825,
    0.37028620077104213,
    0.4644706028252668,
    0.29426126715050466,
    0.43252174325508,
    0.4492286624616834,
    0.415964718849499,
    0.4113681674572304,
    0.3760897451189879,
    0.39413148862782876,
    0.39427132309308843,
    0.42845700179286167,
    0.3929418909210268,
    0.34375395105854556,
    0.3503137198855182,
    0.43845263061903,
    0.37692720289502407,
    0.38599819930431567,
    0.4435063021627727,
    0.39822844036876576,
    0.3504433539082961
  ],
  "combined_avgs": [
    0.32360581457919557,
    0.4061342942987979,
    0.4143638954639469,
    0.4398979904435974,
    0.3989200805926877,
    0.4074316877735996,
    0.35725768082066933,
    0.45221951958833484,
    0.4310049408418548,
    0.38288532459029634,
    0.4636521160873483,
    0.2624620055921234,
    0.4369194279262468,
    0.43649607338394336,
    0.4227307501383564,
    0.43357713738887427,
    0.38785147473490955,
    0.44098861749321294,
    0.3944775094143759,
    0.42611296600464615,
    0.39527234366508335,
    0.35555124599971333,
    0.3425204863116923,
    0.44040579107251066,
    0.3713381477122706,
    0.3751782531502058,
    0.4036992509048075,
    0.394175744655106,
    0.36862290434877953
  ],
  "gppm": [
    561.9584171613218,
    549.8009069535585,
    541.2323720489037,
    532.1182586627551,
    556.0940339709606,
    551.4512335071224,
    576.2711419274126,
    532.3005991751309,
    541.8364131741416,
    568.3261180127149,
    527.3533865719879,
    594.4896216529357,
    540.7716796511816,
    531.3206614745199,
    546.5442089199616,
    550.2397834188938,
    560.2975867919489,
    558.9406341280135,
    556.2617219335408,
    540.2485853107497,
    555.0017323550381,
    577.8541845350425,
    573.6091155461003,
    537.2517295903689,
    564.5491116987079,
    557.8353838735799,
    532.9242605156526,
    554.3719466736916,
    573.5041222981947
  ],
  "gppm_normalized": [
    1.3027684917616154,
    1.2457557643711268,
    1.2228538394726631,
    1.2016912759844014,
    1.2567522488967824,
    1.2436381253725763,
    1.3032982052621023,
    1.202665898302459,
    1.2211353263001112,
    1.2837623590149612,
    1.189084006501881,
    1.3430934773901755,
    1.2238355608432134,
    1.202175270099394,
    1.2332678028057227,
    1.2451293259572975,
    1.2656027576162803,
    1.2619907664619958,
    1.2566571364738228,
    1.2132863791781239,
    1.248636298975677,
    1.3001523381967897,
    1.2955211481069795,
    1.2047624691239083,
    1.2712127151410393,
    1.2629764309654827,
    1.2005869842040542,
    1.2566791446964076,
    1.2885033013066287
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374
  ],
  "response_lengths": [
    2079,
    2325,
    2531,
    2553,
    2335,
    2400,
    2548,
    2341,
    2571,
    1967,
    2470,
    3039,
    2400,
    2590,
    2398,
    2386,
    2221,
    2035,
    2392,
    2389,
    2145,
    2138,
    2192,
    2443,
    1989,
    2228,
    2665,
    2476,
    2174
  ]
}