{
  "example_idx": 86,
  "reference": "Published as a conference paper at ICLR 2023\n\nINTEGRATING SYMMETRY INTO DIFFERENTIABLE PLANNING WITH STEERABLE CONVOLUTIONS\n\nLinfeng Zhao∗, Xupeng Zhu† , Lingzhi Kong†, Robin Walters, Lawson L.S. Wong Khoury College of Computer Sciences, Northeastern University\n\nABSTRACT\n\nIn this paper, we study a principled approach on incorporating group symmetry into end-to-end differentiable planning algorithms and explore the benefits of symmetry in planning. To achieve this, we draw inspiration from equivariant convolution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant operator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navigation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN.\n\n1\n\nINTRODUCTION\n\nModel-based planning algorithms can struggle to find solutions for complex problems, and one solution is to apply planning in a more structured and reduced space (Sutton and Barto, 2018; Li et al., 2006; Ravindran and Barto, 2004; Fox and Long, 2002). When a task exhibits symmetry, this structure can be used to effectively reduce the search space for planning. However, existing planning algorithms often assume perfect knowledge of dynamics and require building equivalence classes, which can be inefficient and limit their applicability to specific tasks (Fox and Long, 1999; 2002; Pochter et al., 2011; Zinkevich and Balch, 2001; Narayanamurthy and Ravindran, 2008).\n\nFigure 1: Symmetry in path planning. Symmetric Planning approach guarantees the solutions are same up to rotations.\n\nIn this paper, we study the path-planning problem and its symmetry structure, as shown in Figure 1. Given a map M (top row), the objective is to find optimal actions A = SymPlan(M ) (bottom row) to a given position (red dots). If we rotated the map g.M (top right), its solution g.A (shortest path) can also be connected by a rotation with the original solution A. Specifically, we say the task has symmetry since the solutions SymPlan(g.M ) = g.SymPlan(M ) are related by a ⟲ 90◦ rotation. As a more concrete example, the action in the NW corner of A is the same as the action in the SW corner of g.A, after also rotating the arrow ⟲ 90◦. This is an example of symmetry appearing in a specific task, which can be observed before solving the task or assuming other domain knowledge. If we can use the rotation (and reflection) symmetry in this task, we effectively reduce the search space by |C4| = 4 (or |D4| = 8) times. Instead, classic planning algorithms like A* would require searching symmetric states (NP-hard) with known dynamics (Pochter et al., 2011).\n\nRecently, symmetry in model-free deep reinforcement learning (RL) has also been studied (Mondal et al., 2020; van der Pol et al., 2020a; Wang et al., 2021). A core benefit of model-free RL\n\n∗Corresponding Author: Linfeng Zhao zhao.linf@northeastern.edu. †Second and third authors contributed equally.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthat enables great asymptotic performance is its end-to-end differentiability. However, they lack long-horizon planning ability and only effectively handle pixel-level symmetry, such as flipping or rotating image observations and action together. This motivates us to combine the spirit of both: can we enable end-to-end differentiable planning algorithms to make use of symmetry in environments?\n\nIn this work, we propose a framework called Symmetric Planning (SymPlan) that enables planning with symmetry in an end-to-end differentiable manner while avoiding the explicit construction of equivalence classes for symmetric states. Our framework is motivated by the work in the equivariant network and geometric deep learning community (Bronstein et al., 2021a; Cohen et al., 2020; Kondor and Trivedi, 2018; Cohen and Welling, 2016a;b; Weiler and Cesa, 2021), which views geometric data as signals (or “steerable feature fields”) over a base space. For instance, an RGB image is represented as a signal that maps Z2 to R3. The theory of equivariant networks enables the injection of symmetry into operations between signals through equivariant operations, such as convolutions. Equivariant networks applied to images do not need to explicitly consider “symmetric pixels” while still ensuring symmetry properties, thus avoiding the need to search symmetric states.\n\nWe apply this intuition to the task of path planning, which is both straightforward and general. Specifically, we focus on the 2D grid and demonstrate that value iteration (VI) for 2D path planning is equivariant under translations, rotations, and reflections (which are isometries of Z2). We further show that VI for path planning is a type of steerable convolution network, as developed in (Cohen and Welling, 2016a). To implement this approach, we use Value Iteration Network (VIN, (Tamar et al., 2016a)) and its variants, since they require only operations between signals. We equip VIN with steerable convolution to create the equivariant steerable version of VIN, named SymVIN, and we use a variant called GPPN (Lee et al., 2018) to build SymGPPN. Both SymPlan methods significantly improve training efficiency and generalization performance on previously unseen random maps, which highlights the advantage of exploiting symmetry from environments for planning. Our contributions include:\n\n• We introduce a framework for incorporating symmetry into path-planning problems on 2D grids,\n\nwhich is directly generalizable to other homogeneous spaces.\n\n• We prove that value iteration for path planning can be treated as a steerable CNN, motivating us\n\nto implement SymVIN by replacing the 2D convolution with steerable convolution.\n\n• We show that both SymVIN and a related method, SymGPPN, offer significant improvements in\n\ntraining efficiency and generalization performance for 2D navigation and manipulation tasks.\n\n2 RELATED WORK\n\nPlanning with symmetries. Symmetries are prevalent in various domains and have been used in classical planning algorithms and model checking (Fox and Long, 1999; 2002; Pochter et al., 2011; Shleyfman et al., 2015; Sievers et al., 2015; Sievers; Winterer et al.; Röger et al., 2018; Sievers et al., 2019; Fišer et al., 2019). Invariance of the value function for a Markov Decision Process (MDP) with symmetry has been shown by Zinkevich and Balch (2001), while Narayanamurthy and Ravindran (2008) proved that finding exact symmetry in MDPs is graph-isomorphism complete. However, classical planning algorithms like A* have a fundamental issue with exploiting symmetries. They construct equivalence classes of symmetric states, which explicitly represent states and introduce symmetry breaking. As a result, they are intractable (NP-hard) in maintaining symmetries in trajectory rollout and forward search (for large state spaces and symmetry groups) and are incompatible with differentiable pipelines for representation learning. This limitation hinders their wider applications in reinforcement learning (RL) and robotics.\n\nState abstraction for detecting symmetries. Coarsest state abstraction aggregates all symmetric states into equivalence classes, studied in MDP homomorphisms and bisimulation (Ravindran and Barto, 2004; Ferns et al., 2004; Li et al., 2006). However, they require perfect MDP dynamics and do not scale up well, typically because of the complexity in maintaining abstraction mappings (homomorphisms) and abstracted MDPs. van der Pol et al. (2020b) integrate symmetry into modelfree RL based on MDP homomorphisms (Ravindran and Barto, 2004) and motivate us to consider planning. Park et al. (2022) learn equivariant transition models, but do not consider planning. Additionally, the typical formulation of symmetric MDPs in (Ravindran and Barto, 2004; van der Pol et al., 2020a; Zhao et al., 2022) is slightly different from our formulation here: we consider sym-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nmetry between MDPs (rotated maps), instead of within a single MDP. Thus, the reward or transition function additionally depends on map input, as further discussed in Appendix B.2.\n\nSymmetries and equivariance in deep learning. Equivariant neural networks are used to incorporate symmetry in supervised learning for different domains (e.g., grids and spheres), symmetry groups (e.g., translations and rotations), and group representations (Bronstein et al., 2021b). Cohen and Welling (2016b) introduce G-CNNs, followed by Steerable CNNs (Cohen and Welling, 2016a) which generalizes from scalar feature fields to vector fields with induced representations. Kondor and Trivedi (2018); Cohen et al. (2020) study the theory of equivariant maps and convolutions. Weiler and Cesa (2021) propose to solve kernel constraints under arbitrary representations for E(2) and its subgroups by decomposing into irreducible representations, named E(2)-CNN.\n\nDifferentiable planning. Our pipeline is based on learning to plan in a neural network in a differentiable manner. Value iteration networks (VIN) (Tamar et al., 2016b) is a representative approach that performs value iteration using convolution on lattice grids, and has been further extended (Niu et al., 2017; Lee et al., 2018; Chaplot et al., 2021; Deac et al., 2021; Zhao et al., 2023). Other than using convolutional networks, works on integrating learning and planning into differentiable networks include Oh et al. (2017); Karkus et al. (2017); Weber et al. (2018); Srinivas et al. (2018); Schrittwieser et al. (2019); Amos and Yarats (2019); Wang and Ba (2019); Guez et al. (2019); Hafner et al. (2020); Pong et al. (2018); Clavera et al. (2020). On the theoretical side, Grimm et al. (2020; 2021) propose to understand the differentiable planning algorithms from a value equivalence perspective.\n\n3 BACKGROUND\n\nMarkov decision processes. We model the pathplanning problem as a Markov decision process (MDP) (Sutton and Barto, 2018). An MDP is a 5-tuple M = ⟨S, A, P, R, γ⟩, with state space S, action space A, transition probability function P : S × A × S → R+, reward function R : S × A → R, and discount factor γ ∈ [0, 1]. Value functions V : S → R and Q : S × A → R represent expected future returns. The core component behind dynamic programming (DP)- based algorithms in reinforcement learning is the Bellman (optimality) equation (Sutton and Barto, 2018): V ∗(s) = maxa R(s, a) + γ (cid:80) s′ P (s′|s, a)V ∗(s′). Value iteration is an instance of DP to solve MDPs, which iteratively applies the Bellman (optimality) operator until convergence.\n\nFigure 2: (Left) Construction of spatial MDPs from path-planning problems, enabling the use of G-invariant transition models. (Right) A demonstration of how an action (arrow in red circle) is rotated when a map is rotated.\n\nPath planning. The objective of the path-planning problem is to find optimal actions from every location to navigate to the target in shortest time. However, the original path-planning problem is not equivariant under translation due to obstacles. VINs (Tamar et al., 2016a) implicitly construct an equivalent problem with an equivariant transition function, thus CNNs can be used to inject translation equivariance. We visualize the construction of an equivalent “spatial MDP” in Figure 2 (left), where the key idea is to encode obstacle information in the transition function from the map (top left) into the reward function in the constructed spatial MDP (bottom right) as “trap” states with −∞ reward. Further details about this construction are in Appendices E.1 and E.3. In Figure 2 (right), we provide a visualization of the representation π(r) of a rotation r of ⟲ 90◦, and how an action (arrow) is rotated ⟲ 90◦ accordingly.\n\nValue Iteration Network. Tamar et al. (2016a) proposed Value Iteration Networks (VINs) that use a convolutional network to parameterize value iteration. It jointly learns in a latent MDP on a 2D grid, which has the latent reward function ̄R : Z2 → R|A| and value function ̄V : Z2 → R, and applies value iteration on that MDP: ̄a,i′,j′ = ̄R ̄a,i′,j′ +\n\ni,j = max\n\n ̄V (k−1)\n\ni′−i,j′−j\n\n ̄V (k)\n\n ̄Q(k)\n\n ̄Q(k)\n\nW V\n\n(cid:88)\n\n(1)\n\n ̄a,i,j\n\n ̄a,i,j\n\n ̄a\n\ni,j\n\nThe first equation can be written as: ̄Q(k) = ̄Ra+Conv2D( ̄V (k−1); W V layer Conv2D has parameters W V .\n\n ̄a ), where the 2D convolution\n\n3\n\nConstruct Spatial MDP Published as a conference paper at ICLR 2023\n\nFigure 3: The commutative diagram of Symmetric Value Iteration Network (SymVIN). Every row is a full computation graph of VIN. Every column rotates the field by ⟲ 90◦.\n\nWe intentionally omit the details on equivariant networks and instead focus on the core idea of integrating symmetry with equivariant networks. We present the necessary group theory background in Appendix C and our full framework and theory in Appendices D and E.\n\n4 METHOD: INTEGRATING SYMMETRY INTO PLANNING BY CONVOLUTION\n\nThis section presents an algorithmic framework that can provably leverage the inherent symmetry of the path-planning problem in a differentiable manner. To make our approach more accessible, we first introduce Value Iteration Networks (VINs) (Tamar et al., 2016a) as the foundation for our algorithm: Symmetric VIN. In the next section, we provide an explanation for why we make this choice and introduce further theoretical guarantees on how to exploit symmetry.\n\nHow to inject symmetry? VIN uses regular 2D convolutions (Eq. 1), which has translation equivariance (Cohen and Welling, 2016b; Kondor and Trivedi, 2018). More concretely, a VIN will output the same value function for the same map patches, up to 2D translation. Characterization of translation equivariance requires a different mechanism and does not decrease the search space nor reduce a path-planning MDP to an easier problem. We provide a complete description in Appendix E.\n\nBeyond translation, we are more interested in rotation and reflection symmetries. Intuitively, as shown in Figure 1, if we find the optimal solution to a map, it automatically generalizes the solution to all 8 transformed maps (4 rotations times 2 reflections, including identity transformation). This can be characterized by equivariance of a planning algorithm Plan, such as value iteration VI: g.Plan(M ) = Plan(g.M ), where M is a maze map, and g is the symmetry group D4 under which 2D grids are invariant.\n\nMore importantly, symmetry also helps training of differentiable planning. Intuitively, symmetry in path planning poses additional constraints on its search space: if the goal is in the north, go up; if in the east, go right. In other words, the knowledge can be shared between symmetric cases; the pathplanning problem is effectively reduced by symmetry to a smaller problem. This property can also be depicted by equivariance of Bellman operators T , or one step of value iteration: g.T [V0] = T [g.V0]. If we use VI(M ) to denote applying Bellman operators on arbitrary initialization until convergence T ∞[V0], value iteration is also equivariant:\n\ng.VI(M ) ≡ g.T ∞[V0] = T ∞[g.V0] ≡ VI(g.M )\n\n(2)\n\nWe formally prove this equivariance in Theorem 5.1 in next section. In Theorem 5.2, we theoretically show that value iteration in path planning is a specific type of convolution: steerable convolution (Cohen and Welling, 2016a). Before that, we first use this finding to present our pipeline on how to use Steerable CNNs (Cohen and Welling, 2016a) to integrate symmetry into path planning.\n\nPipeline: SymVIN. We have shown that VI is equivariant given symmetry in path planning. We introduce our method Symmetric Value Iteration Network (SymVIN), that realizes equivariant VI by integrating equivariance into VIN with respect to rotation and reflection, in addition to translation. We use an instance of Steerable CNN: E(2)-Steerable CNNs (Weiler and Cesa, 2021) and their package e2cnn for implementation, which is equivariant under D4 rotation and reflection, and also Z2 translation on the 2D grid Z2. In practice, to inject symmetry into VIN, we mainly need to\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nreplace the translation-equivariant Conv2D in Eq. 1 with SteerableConv:\n\n ̄Q(k)\n\n ̄a = ̄R ̄a + SteerableConv( ̄V ; W V )\n\n ̄V (k) = max\n\n ̄a\n\n ̄Q(k)\n\n ̄a\n\n(3)\n\nWe visualize the full pipeline in Figure 3. The map and goal are represented as signals M : Z2 → {0, 1}2. It will be processed by another layer and output to the core value iteration loop. After some iterations, the final output will be used to predict the actions and compute cross-entropy loss.\n\nFigure 3 highlights our injected equivariance property: if we rotate the map (from M to g.M ), in order to guarantee that the final policy function will also be equivalently rotated (from A to g.A), we shall guarantee that every transformation (e.g., Qk (cid:55)→ Vk and Vk (cid:55)→ Qk+1) in value iteration will also be equivariant, for every pair of columns. We formally justify our design in the section below and provide more technical details in Appendix E.\n\nExtension: Symmetric GPPN. Based on same spirit, we also implement a symmetric version of Gated Path-Planning Networks (GPPN (Lee et al., 2018)). GPPNs use LSTMs to alleviate the issue of unstable gradient in VINs. Although it does not strictly follow value iteration, it still follows the spirit of steerable planning. Thus, we first obtained a fully convolutional variant of GPPN from Kong (2022), called ConvGPPN, which incorporates translational equivariance. It replaces the MLPs in the original LSTM cell with convolutional layers, and then replaces convolutions with equivariant steerable convolutions, resulting in SymGPPN that is equivariant to translations, rotations, and reflections. See Appendix G.1 for details.\n\n5 THEORY: VALUE ITERATION IS STEERABLE CONVOLUTION\n\nIn the last section, we showed how to exploit symmetry in path planning by equivariance from convolution via intuition. The goal of this section is to (1) connect the theoretical justification with the algorithmic design, and (2) provide intuition for the justification. Even through we focus on a specific task, we hope that the underlying guidelines on integrating symmetry into planning are useful for broader planning algorithms and tasks as well. The complete version is in Appendix E.\n\nOverview. There are numerous types of symmetry in various planning tasks. We study symmetry in path planning as an example, because it is a straightforward planning problem, and its solutions have been intensively studied in robotics and artificial intelligence (LaValle, 2006; Sutton and Barto, 2018). However, even for this problem, symmetry has not been effectively exploited in existing planning algorithms, such as Dijkstra’s algorithm, A*, or RRT, because it is NP-hard to find symmetric states (Narayanamurthy and Ravindran, 2008).\n\nWhy VIN-based planners? There are two reasons for choosing value-based planning methods.\n\n1. The expected-value operator in value iteration (cid:80)\n\ns′ P (s′|s, a)V (s′) is (1) linear in the value function and (2) equivariant (shown in Theorem 5.1). Cohen et al. (2020) showed that any linear equivariant operator (on homogeneous spaces, e.g., 2D grid) is a group-convolution operator.\n\n2. Value iteration, using the Bellman (optimality) operator, consists of only maps between signals (steerable fields) over Z2 (e.g., value map and transition function map). This allows us to inject symmetry by enforcing equivariance to those maps. Taking Figure 1 as an example, the 4 corner states are symmetric under transformations in D4. Equivariance enforces those 4 states to have the same value if we rotate or flip the map. This avoids the need to find if a new state is symmetric to any existing state, which is shown to be NP-hard (Narayanamurthy and Ravindran, 2008).\n\nOur framework for integrating symmetry applies to any value-based planner with the above properties. We found that VIN is conceptually the simplest differentiable planning algorithm that meets these criteria, hence our decision to focus primarily on VIN and its variants.\n\nSymmetry from tasks. If we want to exploit inherent symmetry in a task to improve planning, there are two major steps: (1) characterize the symmetry in the task, and (2) incorporate the corresponding symmetry into the planning algorithm. The theoretical results in Appendix E.2 mainly characterize the symmetry and direct us to a feasible planning algorithm.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nThe symmetry in tasks for MDPs can be specified by the equivariance property of the transition and reward function, studied in Ravindran and Barto (2004); van der Pol et al. (2020b):\n\n ̄P (s′ | s, a) = ̄P (g.s′ | g.s, g.a), ̄RM (s, a) = ̄Rg.M (g.s, g.a),\n\n∀g ∈ G, ∀s, a, s′\n\n∀g ∈ G, ∀s, a\n\n(4)\n\n(5)\n\nNote that how the group G acts on states and actions is called group representation, and is decided by the space S or A, which is discussed in Equation 19 in Appendix E.2. We emphasize that the equivariance property of the reward function is different from prior work (Ravindran and Barto, 2004; van der Pol et al., 2020b): in our case, the reward function encodes obstacles as well, and thus depends on map input M . Intuitively, using Figure 1 as an example, if a position s is rotated to g.s, in order to find the correct original reward R before rotation, the input map M must also be rotated g.M . See Appendix E for more details.\n\nSymmetry into planning. As for exploiting the symmetry in planning algorithms, we focus on value iteration and the VIN algorithm. We first prove in Theorem 5.1 that value iteration for path planning respects the equivariance property, motivating us to incorporate symmetry with equivariance. Theorem 5.1 (informal). If transition is G-invariant, expected-value operator (cid:80) and value iteration are equivariant under translation, rotation, reflection on the 2D grid.\n\ns′ P (s′|s, a)V (s′)\n\nWe visualize the equivariance of the central valueupdate step R+γP ⋆Vk in Figure 4. The upper row is a value field Vk and its rotated version g.Vk, and the lower row is for Q-value fields Qk and g.Qk. The diagram shows that, if we input a rotated value g.Vk, the output R + γP ⋆ g.Vk is guaranteed to be equal to rotated Q-field g.Qk. Additionally, rotating Qfield g.Qk has two components: (1) spatially rotating each grid (a feature channel for an action Q(·, a)) and (2) cyclically permuting the channels (black arrows). The red dashed line points out how a specific grid of a Q-value grid Qk(·, South) got rotated and permuted. We discuss the theoretical guarantees in Theorem 5.1 and provide full proofs in Appendix F.\n\nFigure 4: Commutative diagram of a single step of value update, showing equivariance under rotations. Each grid in the Q-value field corresponds to all the values Q(·, a) of a single action a.\n\nHowever, while the first theorem provides intuition, it is inadequate since it only shows the equivariance property for scalar-valued transition probabilities and value functions, and does not address the implementation of VINs with multiple feature channels in CNNs. To address this gap, the next theorem further proves that value iteration is a general form of steerable convolution, motivating the use of steerable CNNs by Cohen and Welling (2016a) to replace regular CNNs in VIN. This is related to Cohen et al. (2020) that proves steerable convolution is the most general linear equivariant map on homogeneous spaces. Theorem 5.2 (informal). If transition is G-invariant, the expected-value operator is expressible as a steerable convolution ⋆, which is equivariant under translation, rotation, and reflection on 2D grid. Thus, value iteration (with max, +, ×) is a form of steerable CNN (Cohen and Welling, 2016a).\n\nWe provide a complete version of the framework in Section E and the proofs in Section F. This justifies why we should use Steerable CNN (Cohen and Welling, 2016a): the VI itself is composed of steerable convolution and additional operations (max, +, ×). With equivariance, the value function Z2 → R is D4-invariant, thus the planning is effectively done in the quotient space reduced by D4.\n\nSummary. We study how to inject symmetry into VIN for (2D) path planning, and expect the taskspecific technical details are useful for two types of readers. (i) Using VIN. If one uses VIN for differentiable planning, the resulting algorithms SymVIN or SymGPPN can be a plug-in alternative, as part of a larger end-to-end system. Our framework generalizes the idea behind VINs and enables us to understand its applicability and restrictions. (ii) Studying path planning. The proposed framework characterizes the symmetry in path planning, so it is possible to apply the underlying ideas to other domains. For example, it is possible to extend to higher-dimensional continuous Euclidean\n\n6\n\nValue UpdatePublished as a conference paper at ICLR 2023\n\nFigure 5: (Left) We randomly generate occupancy grid Z2 → {0, 1} for 2D navigation. For visual navigation, each position provides 32 × 32 × 3 egocentric panoramic RGB images in 4 directions for each location Z2 → R4×32×32×3. One location is visualized. (Right) The top-down view (left) is the workspace of a 2-DOF manipulation task. For workspace manipulation, it is converted by a mapper layer to configuration space, shown in the right subfigure. For C-space manipulation, the ground-truth C-space is provided to the planner.\n\nspaces or spatial graphs (Weiler et al., 2018; Brandstetter et al., 2021). Additionally, we emphasize that the symmetry in spatial MDPs is different from symmetric MDPs (Zinkevich and Balch, 2001; Ravindran and Barto, 2004; van der Pol et al., 2020a), since our reward function is not G-invariant (if not conditioning on obstacles). We further discuss this in Appendices B.2 and E.4.\n\n6 EXPERIMENTS\n\nWe experiment with VIN, GPPN and our SymPlan methods on four path-planning tasks, using both given and learned maps. Additional experiments and ablation studies are in Appendix H.\n\nEnvironments and datasets. We demonstrate the idea in four path-planning tasks: (1) 2D navigation, (2) visual navigation, (3) 2 degrees of freedom (2-DOF) configuration-space (C-space) manipulation, and (4) 2-DOF workspace manipulation. For each task, we consider using either given (2D navigation and 2-DOF configuration-space manipulation) or learned maps (visual navigation and 2-DOF workspace manipulation). In the latter case, the planner needs to jointly learn a mapper that converts egocentric panoramic images (visual navigation) or workspace states (workspace manipulation) into a map that the planners can operate on, as in (Lee et al., 2018; Chaplot et al., 2021). In both cases, we randomly generate training, validation and test data of 10K/2K/2K maps for all map sizes, to demonstrate data efficiency and generalization ability of symmetric planning. Due to the small dataset sizes, test maps are unlikely to be symmetric to the training maps by any transformation from the symmetry groups G. For all environments, the planning domain is the 2D regular grid as in VIN, GPPN and SPT S = Ω = Z2, and the action space is to move in 4 ⟲ directions1: A = (north, west, south, east).\n\nMethods: planner networks. We compare five planning methods, two of which are our SymPlan methods. Our two equivariant methods are based on Value Iteration Networks (VIN, (Tamar et al., 2016a)) and Gated Path Planning Networks (GPPN, (Lee et al., 2018)). Our equivariant version of VIN is named SymVIN. For GPPN, we first obtained a fully convolutional version, named ConvGPPN (Kong, 2022), and furthermore obtain SymGPPN by replacing standard convolutions with steerable CNNs. All methods use (equivariant) convolutions with circular padding to plan in configuration spaces for the manipulation tasks, except GPPN which is not fully convolutional.\n\nTraining and evaluation. We report success rate and training curves over 3 seeds. The training process (on given maps) follows (Tamar et al., 2016a; Lee et al., 2018), where we train 30 epochs with batch size 32, and use kernel size F = 3 by default. The default batch size is 32. GPPN variants need smaller number because LSTM consumes much more memory.\n\n6.1 PLANNING ON GIVEN MAPS\n\nEnvironmental setup. In the 2D navigation task, the map and goal are randomly generated, where the map size is {15, 28, 50}. In 2-DOF manipulation in configuration space, we adopt the setting\n\n1Note that the MDP action space A needs to be compatible with the group action G × A → A. Since the E2CNN package (Weiler and Cesa, 2021) uses counterclockwise rotations ⟲ as generators for rotation groups Cn, the action space needs to be counterclockwise ⟲.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Averaged test success rate (%) for using 10K/2K/2K dataset for all four types of tasks.\n\nMethod (10K Data)\n\nVIN SymVIN\n\nGPPN ConvGPPN SymGPPN\n\nNavigation\n\nManipulation\n\n15 × 15\n\n28 × 28\n\n50 × 50 Visual\n\n18 × 18\n\n36 × 36 Workspace\n\n66.97 98.99\n\n96.36 99.75 99.98\n\n67.57 98.14\n\n95.77 99.09 99.86\n\n57.92 86.20\n\n91.84 97.21 99.49\n\n50.83 95.50\n\n93.13 98.55 99.78\n\n77.82 99.98\n\n2.62 99.98 100.00\n\n84.32 99.36\n\n1.68 99.95 99.99\n\n80.44 91.10\n\n3.67 89.88 90.50\n\nin Chaplot et al. (2021) and train networks to take as input “maps” in configuration space, represented by the state of the two manipulator joints. We randomly generate 0 to 5 obstacles in the manipulator workspace. Then the 2 degree-of-freedom (DOF) configuration space is constructed from the workspace and discretized into 2D grids with sizes {18, 36}, corresponding to bins of 20◦ and 10◦, respectively. All methods are trained using the same network size, where for the equivariant versions, we use regular representations for all layers, which has size |D4| = 8. We keep the same parameters for all methods, so all equivariant convolution layers with regular representations will have higher embedding sizes. Due to memory constraints, we use K = 30 iterations for 2D maze navigation, and K = 27 for manipulation. We use kernel sizes F = {3, 5, 5} for m = {15, 28, 50} navigation, and F = {3, 5} for m = {18, 36} manipulation.\n\nResults. We show the averaged test results for both 2D navigation and C-space manipulation tasks on generalizing to unseen maps (Table 1) and the training curves for 2D navigation (Figure 6). For VINbased methods, SymVIN learns much faster than standard VIN and achieves almost perfect asymptotic performance. For GPPN-based methods, we found the translation-equivariant convolutional variant ConvGPPN works better than the original one in (Lee et al., 2018), especially in learning speed. SymVIN fluctuates in some runs. We observe that the first epoch has abnormally high loss in most runs and quickly goes down in the second or third epoch, which indicates effects from initialization. SymGPPN further boosts ConvGPPN and outperforms all other methods and does not experience variance. One exception is that standard GPPN learns poorly in C-space manipulation. For GPPN, the added circular padding in the convolution encoder leads to a gradient-vanishing problem. Additionally, we found that using regular representations (for D4 or C4) for state value V : Z2 → RCV (and for Q-values) works better than using trivial representations. This is counterintuitive since we expect the V value to be scalar Z2 → R. One reason is that switching between regular (for Q) and trivial (for V ) representations introduces an unnecessary bottleneck. Depending on the choice of representations, we implement different max-pooling, with details in Appendix G.2. We also empirically found using FC only in the final layer QK (cid:55)→ A helps stabilize the training. The ablation study on this and more are described in Appendix H.\n\nFigure 6: Training curves on 2D navigation with 10K of 15 × 15 maps. Faded areas indicate standard error.\n\nRemark. Our two symmetric planners are both significantly better than their standard counterparts. Notably, we did not include any symmetric maps in the test data, which symmetric planners would perform much better on. There are several potential sources of advantages: (1) SymPlan allows parameter sharing across positions and maps and implicitly enables planning in a reduced space: every (s, a, s′) generalizes to (g.s, g.a, g.s′) for any g ∈ G, (2) thus it uses training data more efficiently, (3) it reduces the hypothesis-class size and facilitates generalization to unseen maps.\n\n6.2 PLANNING ON LEARNED MAPS: SIMULTANEOUSLY PLANNING AND MAPPING\n\nEnvironmental setup. For visual navigation, we randomly generate maps using the same strategy as before, and then render four egocentric panoramic views for each location from 3D environments produced with Gym-MiniWorld (Chevalier-Boisvert, 2018), which can generate 3D mazes with any\n\n8\n\n051015202530Epochs0.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPNPublished as a conference paper at ICLR 2023\n\nlayout. For m × m maps, all egocentric views for a map are represented by m × m × 4 RGB images. For workspace manipulation, we randomly generate 0 to 5 obstacles in workspace as before. We use a mapper network to convert the 96 × 96 workspace (image of obstacles) to the m × m 2 degreeof-freedom (DOF) configuration space (2D occupancy grid). In both environments, the setup is similar to Section 6.1, except here we only use map size m = 15 for visual navigation (training for 100 epochs), and map size m = 18 for workspace manipulation (training for 30 epochs).\n\nMethods: mapper networks and setup. For visual navigation, we implemented an equivariant mapper network based on Lee et al. (2018). The mapper network converts every image into a 256dimensional embedding m × m × 4 × 256 and then predicts the map layout m × m × 1. For workspace manipulation, we use a U-net (Ronneberger et al., 2015) with residual-connection (He et al., 2015) as a mapper. For more training details, see Appendix H.\n\nResults. The results are shown in Table 1, under the columns “Visual” (navigation, 15 × 15) and “Workspace” (manipulation, 18 × 18). In visual navigation, the trends are similar to the 2D case: our two symmetric planners both train much faster. Besides standard VIN, all approaches eventually converge to near-optimal success rates during training (around 95%), but the validation and test results show large performance gaps (see Figure 23 in Appendix H). SymGPPN has almost no generalization gap, whereas VIN does not generalize well to new 3D visual navigation environments. SymVIN significantly improves test success rate and is comparable with GPPN. Since the input is raw images and a mapper is learned end-to-end, it is potentially a source of generalization gap for some approaches. In workspace manipulation, the results are similar to our findings in C-space manipulation, but our advantage over baselines were smaller. We found that the mapper network was a bottleneck, since the mapping for obstacles from workspace to C-space is non-trivial to learn.\n\n6.3 RESULTS ON GENERALIZATION TO LARGER MAPS\n\nTo demonstrate the generalization advantage of our methods, all methods are trained on small maps and tested on larger maps. All methods are trained on 15 × 15 with K = 30. Then we test all methods on map size 15 × 15 through 99 × 99, averaging over 3 seeds (3 model checkpoints) for each method and 1000 maps for each map size. Iterations K were set to 2M , where M is the test map size (x-axis). The results are shown in Figure 7.\n\n√\n\nResults. SymVIN generalizes better than VIN, although the variance is greater. GPPN tends to diverge for larger values of K. ConvGPPN converges, but it fluctuates for different seeds. SymGPPN shows the best generalization and has small variance. In conclusion, SymVIN and SymGPPN generalize better to different map sizes, compared to all non-equivariant baselines.\n\nRemark. In summary, our results show that the SymPlan models demonstrate end-to-end planning and learning ability, potentially enabling further applications to other tasks as a differentiable component for planning. Additional results and ablation studies are in Appendix H.\n\n7 DISCUSSION\n\nFigure 7: Results for testing on larger maps, when trained on size 15 map. Our methods outperform all baselines.\n\nIn this work, we study the symmetry in the 2D path-planning problem, and build a framework using the theory of steerable CNNs to prove that value iteration in path planning is actually a form of steerable CNN (on 2D grids). Motivated by our theory, we proposed two symmetric planning algorithms that provided significant empirical improvements in several path-planning domains. Although our focus in this paper has been on Z2, our framework can potentially generalize to path planning on higher-dimensional or even continuous Euclidean spaces (Weiler et al., 2018; Brandstetter et al., 2021), by using equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We hope that our SymPlan framework, along with the design of practical symmetric planning algorithms, can provide a new pathway for integrating symmetry into differentiable planning.\n\n9\n\n20406080100size0.00.20.40.60.81.0avg_successVINSymVINGPPNConvGPPNSymGPPNPublished as a conference paper at ICLR 2023\n\n8 ACKNOWLEDGEMENT\n\nThis work was supported by NSF Grants #2107256 and #2134178. R. Walters is supported by The Roux Institute and the Harold Alfond Foundation. We also thank the audience from previous poster and talk presentations for helpful discussions and anonymous reviewers for useful feedback.\n\n9 REPRODUCIBILITY STATEMENT\n\nWe provide additional details in the appendix. We also plan to open source the codebase. We briefly outline the appendix below.\n\n1. Additional Discussion\n\n2. Background: Technical background and concepts on steerable CNNs and group CNNs\n\n3. Method: we provide full details on how to reproduce it\n\n4. Theory/Framework: we provide the complete version of the theory statements\n\n5. Proofs: this includes all proofs\n\n6. Experiment / Environment / Implementation details: useful details for reproducibility\n\n7. Additional results\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. Adaptive computation and machine learning series. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018. ISBN 978-0-262-03924-6.\n\nLihong Li, Thomas J. Walsh, and M. Littman. Towards a Unified Theory of State Abstraction for\n\nMDPs. In AI&M, 2006.\n\nBalaraman Ravindran and Andrew G Barto. An algebraic approach to abstraction in reinforcement\n\nlearning. PhD thesis, University of Massachusetts at Amherst, 2004.\n\nMaria Fox and Derek Long. Extending the exploitation of symmetries in planning. In In Proceedings\n\nof AIPS’02, pages 83–91, 2002.\n\nMaria Fox and Derek Long. The Detection and Exploitation of Symmetry in Planning Problems. In\n\nIn IJCAI, pages 956–961. Morgan Kaufmann, 1999.\n\nNir Pochter, Aviv Zohar, and Jeffrey S. Rosenschein. Exploiting Problem Symmetries in StateBased Planners. In Twenty-Fifth AAAI Conference on Artificial Intelligence, August 2011. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3732.\n\nMartin Zinkevich and Tucker Balch. Symmetry in Markov decision processes and its implications for single agent and multi agent learning. In In Proceedings of the 18th International Conference on Machine Learning, pages 632–640. Morgan Kaufmann, 2001.\n\nShravan Matthur Narayanamurthy and Balaraman Ravindran. On the hardness of finding symmetries in Markov decision processes. In Proceedings of the 25th international conference on Machine learning - ICML ’08, pages 688–695, Helsinki, Finland, 2008. ACM Press. ISBN 978-160558-205-4. doi: 10/bkswc2. URL http://portal.acm.org/citation.cfm?doid= 1390156.1390243.\n\nArnab Kumar Mondal, Pratheeksha Nair, and Kaleem Siddiqi. Group Equivariant Deep Reinforcement Learning. arXiv:2007.03437 [cs, stat], June 2020. URL http://arxiv.org/abs/ 2007.03437. arXiv: 2007.03437.\n\nElise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A. Oliehoek, and Max Welling. MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning. arXiv:2006.16908 [cs, stat], June 2020a. URL http://arxiv.org/abs/2006.16908. arXiv: 2006.16908.\n\nDian Wang, Robin Walters, and Robert Platt. $\\mathrm{SO}(2)$-Equivariant Reinforcement Learning. September 2021. URL https://openreview.net/forum?id=7F9cOhdvfk_.\n\nMichael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi ́c. Geometric deep learning:\n\nGrids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021a.\n\nTaco Cohen, Mario Geiger, and Maurice Weiler. A General Theory of Equivariant CNNs on Homogeneous Spaces. arXiv:1811.02017 [cs, stat], January 2020. URL http://arxiv.org/ abs/1811.02017. arXiv: 1811.02017.\n\nRisi Kondor and Shubhendu Trivedi. On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups. arXiv:1802.03690 [cs, stat], November 2018. URL http://arxiv.org/abs/1802.03690. arXiv: 1802.03690.\n\nTaco S. Cohen and Max Welling.\n\nSteerable CNNs. November 2016a. URL https://\n\nopenreview.net/forum?id=rJQKYt5ll.\n\nTaco S. Cohen and Max Welling. Group Equivariant Convolutional Networks. arXiv:1602.07576 [cs, stat], June 2016b. URL http://arxiv.org/abs/1602.07576. arXiv: 1602.07576.\n\nMaurice Weiler and Gabriele Cesa. General $E(2)$-Equivariant Steerable CNNs. arXiv:1911.08251 [cs, eess], April 2021. URL http://arxiv.org/abs/1911.08251. arXiv: 1911.08251.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAviv Tamar, YI WU, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value Iteration NetIn Advances in Neural Information Processing Systems, volume 29. Curran Assoworks. ciates, Inc., 2016a. URL https://proceedings.neurips.cc/paper/2016/hash/ c21002f464c5fc5bee3b98ced83963b8-Abstract.html.\n\nLisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric Xing, and Ruslan Salakhutdinov. Gated Path Planning Networks. arXiv:1806.06408 [cs, stat], June 2018. URL http://arxiv.org/ abs/1806.06408. arXiv: 1806.06408.\n\nAlexander Shleyfman, Michael Katz, Malte Helmert, Silvan Sievers, and Martin Wehrle. Heuristics and Symmetries in Classical Planning. Proceedings of the AAAI Conference on ArtifiISSN 2374-3468. doi: 10/gq5m5s. URL https: cial Intelligence, 29(1), March 2015. //ojs.aaai.org/index.php/AAAI/article/view/9649. Number: 1.\n\nSilvan Sievers, Martin Wehrle, Malte Helmert, and Michael Katz. An Empirical Case Study on Symmetry Handling in Cost-Optimal Planning as Heuristic Search. In Steffen Hölldobler, Rafael Peñaloza, and Sebastian Rudolph, editors, KI 2015: Advances in Artificial Intelligence, volume 9324, pages 166–180. Springer International Publishing, Cham, 2015. ISBN 978-3-319-24488-4 978-3-319-24489-1. doi: 10.1007/978-3-319-24489-1_13. URL http://link.springer. com/10.1007/978-3-319-24489-1_13. Series Title: Lecture Notes in Computer Science.\n\nSilvan Sievers. Structural Symmetries of the Lifted Representation of Classical Planning Tasks.\n\npage 8.\n\nDominik Winterer, Martin Wehrle, and Michael Katz. Structural Symmetries for Fully Observable\n\nNondeterministic Planning. page 7.\n\nGabriele Röger, Silvan Sievers, and Michael Katz. Symmetry-Based Task Reduction for Relaxed Reachability Analysis. In Twenty-Eighth International Conference on Automated Planning and Scheduling, June 2018. URL https://aaai.org/ocs/index.php/ICAPS/ICAPS18/ paper/view/17772.\n\nSilvan Sievers, Gabriele Röger, Martin Wehrle, and Michael Katz. Theoretical Foundations for Structural Symmetries of Lifted PDDL Tasks. Proceedings of the International Conference on Automated Planning and Scheduling, 29:446–454, 2019. ISSN 2334-0843. doi: 10/gq5m5t. URL https://ojs.aaai.org/index.php/ICAPS/article/view/3509.\n\nDaniel Fišer, Álvaro Torralba, and Alexander Shleyfman. Operator Mutexes and Symmetries for Simplifying Planning Tasks. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01):7586–7593, July 2019. ISSN 2374-3468. doi: 10/ghkkbq. URL https://ojs.aaai. org/index.php/AAAI/article/view/4751. Number: 01.\n\nN. Ferns, P. Panangaden, and Doina Precup. Metrics for Finite Markov Decision Processes.\n\nIn\n\nAAAI, 2004.\n\nElise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020b.\n\nJung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan Willem van de Meent, and Robin Walters. Learning Symmetric Embeddings for Equivariant World Models. arXiv:2204.11371 [cs], April 2022. URL http://arxiv.org/abs/2204.11371. arXiv: 2204.11371.\n\nLinfeng Zhao, Lingzhi Kong, Robin Walters, and Lawson L. S. Wong. Toward Compositional Generalization in Object-Oriented World Modeling. In ICML 2022, April 2022. URL http: //arxiv.org/abs/2204.13661. arXiv: 2204.13661.\n\nMichael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi ́c. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv:2104.13478 [cs, stat], April 2021b. URL http://arxiv.org/abs/2104.13478. arXiv: 2104.13478.\n\nAviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks.\n\narXiv preprint arXiv:1602.02867, 2016b.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nSufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C. Smith, and Jelena Kovaˇcevi ́c. Generalized Value Iteration Networks: Life Beyond Lattices. arXiv:1706.02416 [cs], October 2017. URL http://arxiv.org/abs/1706.02416. arXiv: 1706.02416.\n\nDevendra Singh Chaplot, Deepak Pathak, and Jitendra Malik. Differentiable Spatial Planning using Transformers. arXiv:2112.01010 [cs], December 2021. URL http://arxiv.org/abs/ 2112.01010. arXiv: 2112.01010.\n\nAndreea Deac, Petar Veliˇckovi ́c, Ognjen Milinkovi ́c, Pierre-Luc Bacon, Jian Tang, and Mladen Nikoli ́c. Neural Algorithmic Reasoners are Implicit Planners. October 2021. URL https: //arxiv.org/abs/2110.05442v1.\n\nLinfeng Zhao, Huazhe Xu, and Lawson L. S. Wong.\n\ntiable Planning with Implicit Differentiation. //openreview.net/forum?id=PYbe4MoHf32.\n\nScaling up and Stabilizing DifferenIn ICLR 2023, February 2023. URL https:\n\nJunhyuk Oh, Satinder Singh, and Honglak Lee. Value Prediction Network. arXiv:1707.03497 [cs],\n\nNovember 2017. URL http://arxiv.org/abs/1707.03497. arXiv: 1707.03497.\n\nPeter Karkus, David Hsu, and Wee Sun Lee. QMDP-Net: Deep Learning for Planning under Partial Observability. arXiv:1703.06692 [cs, stat], November 2017. URL http://arxiv.org/ abs/1703.06692. arXiv: 1703.06692.\n\nThéophane Weber, Sébastien Racanière, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. ImaginationAugmented Agents for Deep Reinforcement Learning. arXiv:1707.06203 [cs, stat], February 2018. URL http://arxiv.org/abs/1707.06203. arXiv: 1707.06203.\n\nAravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal Planning Networks. arXiv:1804.00645 [cs, stat], April 2018. URL http://arxiv.org/abs/1804. 00645. arXiv: 1804.00645.\n\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. arXiv:1911.08265 [cs, stat], November 2019. URL http://arxiv.org/abs/ 1911.08265. arXiv: 1911.08265.\n\nBrandon Amos and Denis Yarats. The Differentiable Cross-Entropy Method. September 2019. doi:\n\n10/gq5m59. URL https://arxiv.org/abs/1909.12830v4.\n\nTingwu Wang and Jimmy Ba. Exploring Model-based Planning with Policy Networks. June 2019.\n\nURL https://arxiv.org/abs/1906.08649v1.\n\nArthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sébastien Racanière, Théophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, and Timothy Lillicrap. An investigation of model-free planning. arXiv:1901.03559 [cs, stat], May 2019. URL http://arxiv.org/abs/1901.03559. arXiv: 1901.03559.\n\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to Control: LearnarXiv:1912.01603 [cs], March 2020. URL http:\n\ning Behaviors by Latent Imagination. //arxiv.org/abs/1912.01603. arXiv: 1912.01603.\n\nVitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal Difference Models: Model-Free Deep RL for Model-Based Control. arXiv:1802.09081 [cs], February 2018. URL http://arxiv.org/abs/1802.09081. arXiv: 1802.09081.\n\nIgnasi Clavera, Violet Fu, and Pieter Abbeel. Model-Augmented Actor-Critic: Backpropagating through Paths. arXiv:2005.08068 [cs, stat], May 2020. URL http://arxiv.org/abs/ 2005.08068. arXiv: 2005.08068.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nChristopher Grimm, André Barreto, Satinder Singh, and David Silver. The Value Equivalence Principle for Model-Based Reinforcement Learning. arXiv:2011.03506 [cs], November 2020. URL http://arxiv.org/abs/2011.03506. arXiv: 2011.03506.\n\nChristopher Grimm, André Barreto, Gregory Farquhar, David Silver, and Satinder Singh. Proper Value Equivalence. arXiv:2106.10316 [cs], December 2021. URL http://arxiv.org/ abs/2106.10316. arXiv: 2106.10316.\n\nLingzhi Kong. Integrating implicit deep learning with Value Iteration Networks. Master’s thesis,\n\nNortheastern University, 2022.\n\nSteven M. LaValle. Planning Algorithms. Cambridge University Press, May 2006. ISBN 978-1-\n\n139-45517-6.\n\nMaurice Weiler, M. Geiger, M. Welling, Wouter Boomsma, and Taco Cohen. 3D Steerable CNNs:\n\nLearning Rotationally Equivariant Features in Volumetric Data. In NeurIPS, 2018.\n\nJohannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J. Bekkers, and Max Welling. Geometric and Physical Quantities Improve E(3) Equivariant Message Passing. arXiv:2110.02905 [cs, stat], December 2021. URL http://arxiv.org/abs/2110.02905. arXiv: 2110.02905.\n\nMaxime Chevalier-Boisvert. Miniworld: Minimalistic 3d environment for rl & robotics research.\n\nhttps://github.com/maximecb/gym-miniworld, 2018.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv.org/abs/ 1505.04597.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.\n\nTaco S. Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical CNNs. arXiv:1801.10130 arXiv:\n\nURL http://arxiv.org/abs/1801.10130.\n\n[cs, stat], February 2018. 1801.10130.\n\nMario Geiger and Tess Smidt.\n\narXiv:2207.09453, arXiv, July 2022. arXiv:2207.09453 [cs] type: article.\n\ne3nn: Euclidean Neural Networks.\n\nTechnical Report URL http://arxiv.org/abs/2207.09453.\n\nT. S. Cohen. Equivariant convolutional networks. 2021. URL https://dare.uva.nl/\n\nsearch?identifier=0f7014ae-ee94-430e-a5d8-37d03d8d10e6.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nCONTENTS\n\nA Outline\n\nB Additional Discussion\n\nB.1 Limitations and Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nB.2 The Considered Symmetry and Difference to Existing Work . . . . . . . . . . . .\n\nB.3 Additional Discussions .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nC Background: Equivariant Networks\n\nC.1 Basics: Groups and Group Representations\n\n. . . . . . . . . . . . . . . . . . . . .\n\nC.2 Group Representations: Visual Understanding . . . . . . . . . . . . . . . . . . . .\n\nC.3 Geometric Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nC.4 Steerable CNNs .\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD Symmetric Planning in Practice\n\nD.1 Building Symmetric VIN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.2 PyTorch-style pseudocode\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nE Symmetric Planning Framework\n\nE.1 Steerable Planning: Planning On Steerable Feature Fields . . . . . . . . . . . . . .\n\nE.2 Symmetric Planning: Integrating Symmetry by Convolution . . . . . . . . . . . .\n\nE.3 Details: Constructing Path planning in Neural Networks\n\n. . . . . . . . . . . . . .\n\nE.4 Details: Understanding Symmetric Planning by Abstraction . . . . . . . . . . . . .\n\nE.5 Note: Augmented State .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nF Symmetric Planning Framework: Proofs\n\nF.1 Proof: equivariance of scalar-valued expected value operation . . . . . . . . . . . .\n\nF.2 Proof: expected value operator as steerable convolution .\n\n. . .\n\n. . . . . . . . . . .\n\nF.3 Proof: equivariance of expected future value . . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . .\n\nF.4 Proof: equivariance of steerable value iteration . . . . . . . . . . . . . . . . . . .\n\nG Implementation Details\n\nG.1 Implementation of SymGPPN . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG.2 Implementation of max operation . . . .\n\n. . .\n\n. . . . . . . . . . . . . . . . . . .\n\nH Experiment Details and Additional Results\n\nH.1 Environment Setup .\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nH.2 Building Mapper Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nH.3 Training Setup .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nH.4 Visualization of Learned Models . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n15\n\n16\n\n16\n\n16\n\n16\n\n17\n\n17\n\n17\n\n19\n\n19\n\n20\n\n21\n\n21\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n29\n\n29\n\n30\n\n31\n\n32\n\n34\n\n34\n\n34\n\n34\n\n34\n\n35\n\n37\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nH.5 SPL metric\n\n.\n\n.\n\n.\n\nH.6 Ablation Study .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n38\n\n38\n\nA OUTLINE\n\nWe provide a table of content above.\n\nWe omit technical details on symmetry and equivariant networks in the main paper and delay them here. Specifically, for readers interested in additional details on how to use equivariant networks for symmetric planning, we recommend an order as follows: (1) Basics on group representations and equivariant networks in Section C.1. (2) Practice on building SymVIN in Section D.1. (3) Detailed formulation on SymPlan in Section E.1 and Section E.2.\n\nThe rest technical sections provide additional reading materials for the readers interested in more in-depth account on studying symmetry in reinforcement learning and planning.\n\nB ADDITIONAL DISCUSSION\n\nB.1 LIMITATIONS AND EXTENSIONS\n\nAssumption on known domain structure. As in VIN, although the framework of steerable planning can potentially handle different domains, one important hidden assumption is that the underlying domain Ω (state space), is known. In other words, we fix the structure of learned transition kernels p(s′ | s, a) and estimate coefficients of it. One potential method is to use Transformers that learn attention weights to all states in S, which has been partially explored in SPT (Chaplot et al., 2021). Additionally, it is also possible to treat unknown MDPs as learned transition graphs, as explored in XLVIN (Deac et al., 2021). We leave the consideration of symmetry in unknown underlying domains for future work.\n\nThe curse of dimensionality. The paradigm of steerable planning still requires full expansion in computing value iteration (opposite to sampling-based), since we realize the symmetric planner using group equivariant convolutions (essentially summation or integral). Convolutions on highdimensional space could suffer from the curse of dimensionality for higher dimensional domains, and are vastly under-explored. This is a primary reason why we need sampling-based planning algorithms. If the domain (state-action transition graph) is sparsely connected, value iteration can still scale up to higher dimensions. It is also unclear either when steerable planning would fail, or how sampling-based algorithms could be integrated with the symmetric planning paradigm.\n\nB.2 THE CONSIDERED SYMMETRY AND DIFFERENCE TO EXISTING WORK\n\nWe need to differentiate between two types of symmetry in MDPs. Let’s take spatial graph as illustrative example to understand the potential symmetry from a higher level, which means that the nodes V in the graph have spatial coordinates Zn or Rn. Our 2D path planning is a special case of spatial graph, where the actions can only move to adjacent spatial nodes.\n\nLet the graph denoted as G = ⟨V, E⟩. E is the set of edges connecting two states with an action. One type of symmetry is the symmetry of the graph itself. For the grid case, it means that after D4 rotation or reflection, the map is unchanged.\n\nAnother type of symmetry comes from the isometries of the space. For a spatial graph, we can rotate it freely in a space, while the relative positions are unchanged. For our grid case, it is shown in the Figure 1 that rotating a map resulting in the rotated policy. However, the map or policy itself can never be equal under any transformation in D4.\n\nIn other words, the first type is symmetry within a MDP (rely on the property of the MDP itself M, or Aut(M)), and the second type is symmetry between MDPs (only rely on the property of the underlying spatial space Z2, or Aut(Z2)).\n\nNevertheless, we could input map M and somehow treat symmetric states between MDPs as one state. See the proofs section for more details.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.3 ADDITIONAL DISCUSSIONS\n\nPotential generalization to 3D space. The goal of our work is to integrate symmetry into differentiable planning. While there is a rich branch in math and physics that studies representation theory, we use 2D settings to keep the representation part minimal and provide an example pipeline for using equivariant networks to integrate symmetry. For 3D and other cases, the key is still equivariance, but they require more technical attention on the equivariant network side.\n\nRepresentation theory has extensively studied 3D rotations. SO(3)-equivariant networks have been developed in the equivariant network field by researchers like Cohen et al. (2018) (Spherical CNNs) and Geiger and Smidt (2022) (2022, e3nn: Euclidean Neural Networks). SO(3)-equivariant networks avoid gimbal lock by not predicting pose but only needing equivariance to SO(3). SO(3) elements only appear in solving equivariant kernel constraints.\n\nInference on realistic robotic maps. Our choice of small and toyish maps (100 × 100 or smaller) is in line with prior work, such as VIN, GPPN, and SPT, which mainly experimented on 15 × 15 and 28 × 28 maps. While we recognize that in robotics planning, maps can be significantly larger, we believe that integrating symmetry into differentiable planning is an orthogonal topic with the scalability of differentiable planning algorithms. However, our visual navigation experiment shows that our method can learn from panoramic egocentric RGB images of all locations, and our generalization experiment demonstrates the potential of scalability as the model can generalize to larger maps, which has not been explored in prior work. In comparison with robotic navigation works like Active Neural SLAM, our approach aims to improve only the differentiable planning module, while it is a systematic pipeline with a hierarchical architecture of global and local planners. Our Symmetric Planners could serve as an alternative to either the global or local planner, but we are not comparing our approach with the whole Active Neural SLAM pipeline. Additionally, Neural SLAM uses room-like maps, which have a different structure and require less planning horizon compared to our maze-like maps.\n\nMeasure of efficiency of symmetry in path planning. In Appendix Section E.4, we demonstrate how symmetry can aid path planning and provide intuition for our approach. We show that the MDPs from rotated maps are related by MDP homomorphisms, which has been previously used in Ravindran and Barto (2004). This means that states related by D4 rotations/reflections can be aggregated into one state. We inject this symmetry property into the SymVIN algorithm by enforcing D4-invariance of the value function on Z2, which is equivalent to a function on the quotient space. The global and local equivariance to D4 enables our Symmetric Planning algorithms to effectively plan on the quotient/reduced MDP with a smaller state space. By local equivariance, we mean that every value iteration step is a convolution (or other equivariant) operation and is equivariant to rotation/reflection (see Figure 3 and 4), thus the search space in planning can be exponentially shrunk with respect to the planning horizon. Although we haven’t formally proven this or the sample complexity side, intuitively, this gives |G| times smaller state space and sample complexity.\n\nEquivariance’s benefits in general supervised learning tasks are still being explored. Recent work includes showing improved generalization bounds for group invariant/equivariant deep networks, as demonstrated in the work of Sannai et al. (2021), Elesedy and Zaidi (2021), and Behboodi et al. (2022), among others.\n\nC BACKGROUND: EQUIVARIANT NETWORKS\n\nWe omit technical details in the main paper and delay them here. This section introduces the background on equivariant networks and representation theory. The first subsection covers necessary basics, while the rest subsections provide additional reading materials for the readers interested in more in-depth account on the preliminaries on studying symmetry in reinforcement learning and planning.\n\nC.1 BASICS: GROUPS AND GROUP REPRESENTATIONS\n\nSymmetry groups and equivarance. A symmetry group is defined as a set G together with a binary composition map satisfying the axioms of associativity, identity, and inverse. A (left) group action\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: (Left) Construction of spatial MDPs from path planning problems, enabling G-invariant transition. (Middle) The group acts on a feature field (MDP actions). We need to find the element in the original field by f (r−1x), and also rotate the arrow by ρ(r), where r ∈ D4. We represent one-hot actions as arrows (vector field, using ρstd) for visualization. (Right) Equivariance of V (cid:55)→ Q in Bellman operator on feature fields, under ⟲ 90◦ ∈ C4 rotation, which visually explains Theorem E.1. The example simulates VI for one step (see red circles; minus signs omitted) with true transition P using ⟲ N-W-S-E actions. The Q-value field are for 4 actions and can be viewed as either Z2 → R4 ((Cohen and Welling, 2016a; Weiler and Cesa, 2021)) or Z2 ⋊ C4 → R (on p4 group, (Cohen and Welling, 2016b)). Simplified figures are presented in the main paper.\n\nof G on a set X is defined as the mapping (g, x) (cid:55)→ g.x which is compatible with composition. Given a function f : X → Y and G acting on X and Y, then f is G-equivariant if it commutes with group actions: g.f (x) = f (g.x), ∀g ∈ G, ∀x ∈ X . In the special case the action on Y is trivial g.y = y, then f (x) = f (g.x) holds, and we say f is G-invariant.\n\nGroup representations. We mainly use two groups: dihedral group D4 and cyclic group C4. The cyclic group of 4 elements is C4 = ⟨r | r4 = 1⟩, a symmetry group of rotating a square. The dihedral group D4 = ⟨r, s | r4 = s2 = (sr)2 = 1⟩ includes both rotations r and reflections s, and has size |D4| = 8. A group representation defines how a group action transforms a vector space G × S → S. These groups have three types of representations of our interest: trivial, regular, and quotient representations, see (Weiler and Cesa, 2021). The trivial representation ρtriv maps each g ∈ G to 1 and hence fixes all s ∈ S. The regular representation ρreg of C4 group sends each g ∈ C4 to a 4×4 permutation matrix that cyclically permutes a 4-element vector, such as a one-hot 4direction action. The regular representation of D4 maps each element to an 8×8 permutation matrix which does not act on 4-direction actions, which requires the quotient representations (quotienting out sr2 reflection part) and forming a 4 × 4 permutation matrix. It is worth mentioning the standard representation of the cyclic groups, which are 2 × 2 rotation matrices, only used for visualization (Figure 8 middle).\n\nSteerable feature fields and Steerable CNNs. The concept of feature fields is used in (equivariant) CNNs (Bronstein et al., 2021a; Cohen et al., 2020; Kondor and Trivedi, 2018; Cohen and Welling, 2016a;b; Weiler and Cesa, 2021). The pixels of an 2D RGB image x : Z2 → R3 on a domain Ω = Z2 is a feature field. In steerable CNNs for 2D grid, features are formed as steerable feature fields f : Z2 → RC that associate a C-dimensional feature vector f (x) ∈ RC to each element on a base space, such as Z2. Defined like this, we know how to transform a steerable feature field and also the feature field after applying CNN on it, using some group (Cohen and Welling, 2016a). The type of CNNs that operates on steerable feature fields is called Steerable CNN (Cohen and Welling, 2016a), which is equivariant to groups including translations as subgroup (Z2, +), extending (Cohen and Welling, 2016b). It needs to satisfy a kernel steerability constraint, where the R2 and Z2 cases are considered in (Weiler and Cesa, 2021). We consider the 2D grid as our domain Ω = S = Z2 and use G = p4m group as the running example. The group p4m = (Z2, +) ⋊ D4 (wallpaper group) is semi-direct product of discrete translation group Z2 and dihedral group D4, see (Cohen and Welling, 2016b;a). We visualize the transformation law of p4m on a feature field on Ω = Z2 in\n\n18\n\nConstruct Spatial MDP Published as a conference paper at ICLR 2023\n\nFigure 9: Visualization of the permutation representations of D4 group for every element g ∈ D4 (4 rotations each row and 2 reflections each column). They are (1) the trivial representation, (2) the regular representation, (3) the quotient representation (quotienting out rotations), (4) the quotient representation (quotienting out reflections).\n\nFigure 8 (Middle), usually referred as induced representation (Cohen and Welling, 2016a; Weiler and Cesa, 2021).\n\nC.2 GROUP REPRESENTATIONS: VISUAL UNDERSTANDING\n\nA group representation is a (linear) group action that defines how a group acts on some space. Cohen and Welling (2016b;a); Weiler and Cesa (2021) provide more formal introduction to them in the context of equivariant neural networks. We provide visual understanding and refer the readers to them for comprehensive account.\n\nTo visually understand how the group D4 acts on some vector space, we visualize the trivial, regular, and quotient (quotienting out reflections sr2) representations, which are permutation matrices. If we apply such a representation ρ(g)(g ∈ D4) to a vector, the elements get cyclically permuted. See Figure 9.\n\nThe quotient representation that quotients out reflections and has dimension 4 × 4 is what we need to use on the 4-direction action space.\n\nC.3 GEOMETRIC DEEP LEARNING\n\nWe review another set of important concepts that motivate our formulation of steerable planning: geometric deep learning and the theories on connecting equivariance and convolution (Bronstein et al., 2021a; Cohen et al., 2020; Kondor and Trivedi, 2018). Bronstein et al. (2021a) use x for feature fields while Cohen and Welling (2016a); Cohen et al. (2020); Weiler and Cesa (2021) use f .\n\nConvolutional feature fields. The signals are taken from set C = RD on some structured domain Ω, and all mappings from the domain to signals forms the space of C-valued signals X (Ω, C) = {f : Ω → C}, or X (Ω) for abbreviation. For instance, for RGB images, the domain is the 2D n × n grid Ω = Zn × Zn, and every pixel can take RGB values C = R3 at each point in the domain u ∈ Ω, represented by a mapping x : Zn × Zn → R3. A function on images thus operates on 3n2-dimensional inputs.\n\nIt is argued that the underlying geometric structure of domains Ω plays key role in alleviating the curse of dimensionality, such as convolution networks in computer vision, and this framework is named Geometric Deep Learning. We refer the readers to Geometric Deep Learning (Bronstein et al., 2021a) for more details, and to more rigorous theories on the relation between equivariant maps and convolutions in (Cohen et al., 2020) (vector fields through induced representations) and (Kondor and Trivedi, 2018) (scalar fields through trivial representations).\n\nGroup convolution. Convolutions are shift-equivariant operations, and vice versa. This is the special case for Ω = R, which can be generalized to any group G (that we can integrate or sum over). The group convolution for signals on Ω is then defined2 as\n\n(f ⋆ ψ)(g) = ⟨f, ρ(g)ψ⟩ =\n\n(cid:90)\n\nΩ\n\nf (u)ψ(g−1u)du,\n\n(6)\n\nwhere ψ(u) is shifted copies of a filter, usually locally supported on a subset of Ω and padded outside. Note that although x takes u ∈ Ω, the feature map (x ⋆ ψ) takes as input the elements g ∈ G instead of points on the domain u ∈ Ω. All following group convolution layers take G:\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nX (G) → X (G). In the grid case, the domain Ω is homogeneous space of the group G, i.e. the group G acts transitively: for any two points u, v ∈ Ω there exists a symmetry g ∈ G to reach u = gv.\n\nthe generalized group convolution is GAnalogous to classic shift-equivariant convolutions, equivariant (Cohen et al., 2020). It is observed that ⟨x, ρ(g)θ⟩ = ⟨ρ(g−1)x, θ⟩, and from the defining property of group representations ρ(h−1)ρ(g) = ρ(h−1g), the G-equivariance of group convolution follows (Bronstein et al., 2021a):\n\n(ρ(h)x ⋆ θ)(g) = ⟨ρ(h)x, ρ(g)θ⟩ = (cid:10)x, ρ(h−1g)θ(cid:11) = ρ(h)(x ⋆ θ)(g)\n\n(7)\n\nSteerable convolution kernels. Steerable convolutions extend group convolutions to more general setup and decouple the computation cost with the group size (Cohen and Welling, 2016a; Cohen, 2021). For example, E(2)-steerable CNNs (Weiler and Cesa, 2021) apply it for E(2) group, which is semi-direct product of translations R2 and a fiber group H, where H is a group of transformations that fixes the origin and is O(2) or its subgroups. The representation on the signals/fields is induced from a representation of the fiber group H. Use R2 as example, a steerable kernel only needs to be H-equivariant by satisfying the following constraint (Weiler and Cesa, 2021):\n\nψ(hx) = ρout (h)ψ(x)ρin (h−1) ∀h ∈ H, x ∈ R2.\n\n(8)\n\nC.4 STEERABLE CNNS\n\nWe still use the running example on Z2 and group p4m = Z2 ⋊ D4.\n\nInduced representations. We follow (Cohen and Welling, 2016a; Cohen et al., 2020) to use π for induced representations. We still use feature fields over Z2 as example. As shown in Figure 8 middle, to transform a feature field f : Z2 → RC on base Z2 with group p4m = Z2 ⋊ D4, we need the induced representation (Cohen and Welling, 2016a; Cohen et al., 2020). The induced representation in this case is denoted as π(g) ≜ indZ2⋊D4 ρ(g) (for all g), which means how the group action of D4 transforms a feature field on Z2 ⋊ D4. It acts on the feature field with two parts: (1) on the base space Z2 and (2) on the fibers (feature channels RC) by fiber group H = D4 (Cohen and Welling, 2016a; Weiler and Cesa, 2021). More specifically, applying a translation t ∈ Z2 and a transformation r ∈ D4 to some field f , we get π(tr)f (Cohen and Welling, 2016a; Weiler and Cesa, 2021):\n\nD4\n\nf (x) (cid:55)→ [π(tr)f ] (x) ≜ ρ(r) · (cid:2)f (cid:0)(tr)−1x(cid:1)(cid:3) .\n\n(9)\n\nρ(r) is the fiber representation that transforms the fibers RC, and (tr)−1x finds the element before group action (or equivalently transforming the base space Z2). Thus, π only depends on the fiber representation ρ but not the latter part, thus named induced representation by ρ.\n\nSteerable convolution vs. group convolution. The steerable convolution on Z2 The understanding of this point helps to understand how a group acts on various feature fields and the design of state space for path planning problems. We use the discrete group p4 = Z2 ⋊ C4 as example, which consists of Z2 translations and 90◦ rotations. The only difference with p4m is p4 does not have reflections. The group convolution with filter ψ and signal x on grid (or p ∈ Z2), which outputs signals (a function) on group p4\n\n[ψ ⋆ x](t, r) :=\n\nψ((t, r)−1p) x(p).\n\n(10)\n\n(cid:88)\n\np∈Z2\n\nA group G has a natural action on the functions over its elements; if x : G → R and g ∈ G, the function g.x is defined as [g.x](h) := x(g−1 · h).\n\n2The definition of group convolution needs to assume that (1) signals X (Ω) are in a Hilbert space (to define Ω x(u)θ(u)du) and (2) the group G is locally compact (so a Haar measure exists\n\nan inner product ⟨x, θ⟩ = (cid:82) and \"shift\" of filter can be defined).\n\n2Technically, we still need to solve the linear equivariance constraint in Eq. 34 to enable weight-sharing for\n\nequivariance, while Weiler and Cesa (2021) have implemented it for 2D case.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFor example: The group action of a rotation r ∈ C4 on the space of functions over p4 is\n\n[r.y](p, s) := y(r−1(p, s)) = y(r−1p, r−1s),\n\nwhere r−1p spatially rotates the pixels, r−1s cyclically permutes the 4 channels.\n\nThe G-space (functions over p4) with a natural action of p4 on it:\n\n[(t, r).y](p, s) := y((t, r)−1 · (p, s)) = y(r−1(p − t), r−1s)\n\nThe group convolution in discrete case is defined as\n\n[ψ ⋆ x](g) :=\n\n(cid:88)\n\nh∈H\n\nψ(g−1 · h) x(h).\n\nThe group convolution with filter ψ and signal x on p4 group is given by:\n\n[ψ ⋆ x](t, r) :=\n\n(cid:88)\n\n(cid:88)\n\ns∈C4\n\np∈Z2\n\nψ((t, r)−1(p, s)) x(p, s).\n\nUsing the fact\n\nψ((t, r)−1(p, s)) = ψ(r−1(p − t, s)) = [r.ψ](p − t, s),\n\nthe convolution can be equivalently written into\n\n[ψ ⋆ x](t, r) :=\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\ns∈C4\n\np∈Z2\n\n[r.ψ](p − t, s) x(p, s)\n\n\n\n .\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(cid:16)(cid:80)\n\nSo\n\np∈Z2 [r.ψ](p − t, s) x(p, s)\n\n(cid:17)\n\ncan be implemented in usual shift-equivariant convolution\n\np∈Z2 is equivalently for the sum in steerable convolution, and the outer sum (cid:80)\n\nCONV2D. The inner sum (cid:80) s∈C4 implement rotation-equivariant convolution that satisfies H-steerability kernel constraint. Here, the outer sum is essentially using the regular fiber representation of C4. In other words, group convolution on p4 = Z2 ⋊ C4 group is equivalent to steerable convolution on base space Z2 with the fiber group of C4 with regular representation.\n\nStack of feature fields. Analogous to ordinary CNNs, a feature space in steerable CNNs can consist of multiple feature fields fi : Z2 → Rci. The feature fields are stacked f = (cid:76) i fi together by concatenating the individual feature fields fi (along the fiber channel), which transforms under the directly sum ρ = (cid:76) i ρi of individual (fiber) representations. Every layer will be equivariant between input and output field fin, fout under induced representations πin, πout. For a steerable convolution between more than one-dimensional feature fields, the kernel is matrix-valued (Cohen et al., 2020; Weiler and Cesa, 2021).\n\nD SYMMETRIC PLANNING IN PRACTICE\n\nD.1 BUILDING SYMMETRIC VIN\n\nIn this section, we discuss how to achieve Symmetric Planning on 2D grids with E(2)-steerable CNNs (Weiler and Cesa, 2021). We focus on implementing symmetric version of value iteration, SymVIN, and generalize the methodology to make a symmetric version of a popular follow-up of VIN, GPPN (Lee et al., 2018). Steerable value iteration. We have showed that, value iteration for path planning problems on Z2 consists of equivariant maps between steerable feature fields. It can be implemented as an equivariant steerable CNN, with recursively applying two alternating (equivariant) layers:\n\nQa\n\nk(s) = Ra\n\nm(s) + γ × [P a\n\nθ ⋆ Vk] (s),\n\nVk+1(s) = max\n\na\n\nQa\n\nk(s),\n\ns ∈ Z2,\n\n(17)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Commutative diagram for the full pipeline of SymVIN on steerable feature fields over Z2 (every grid). If rotating the input map M by πM (g) of any g, the output action A = SymVIN(M ) is guaranteed to be transformed by πA(g), i.e. the entire steerable SymVIN is equivariant under induced representations πM and πA: SymVIN(πM (g)M ) = πA(g)SymVIN(M ). We use stacked feature fields to emphasize that SymVIN supports direct-sum of representations beyond scalar-valued.\n\nwhere k ∈ [K] indexes iteration, Vk, Qa ariant layers, P a\n\nθ is a learned kernel in neural network, and +, × are element-wise operations.\n\nk, Ra\n\nm are steerable feature fields over Z2 output by equiv-\n\nImplementation of pipeline. We follow the pipeline in VIN (Tamar et al., 2016a). The commutative diagram for the full pipeline is shown in Figure 10. The path planning task is given by a m × m spatial binary obstacle occupancy map and one-hot goal map, represented as a feature field M : Z2 → {0, 1}2. For the iterative process Qa k (cid:55)→ Vk (cid:55)→ Qa k+1, the reward field RM is predicted from map M (by a 1 × 1 convolution layer) and the value field V0 is initialized as zeros. The network output is (logits of) planned actions for all locations3, represented as A : Z2 → R|A|, predicted from the final Q-value field QK (by another 1 × 1 convolution layer). The number of iterations K and the convolutional kernel size F of P a θ are set based on map size M , and the spatial dimension m × m is kept consistent.\n\nBuilding Symmetric Value Iteration Networks. Given the pipeline of VIN fully on steerable feature fields, we are ready to build equivariant version with E(2)-steerable CNNs (Weiler and Cesa, 2021). The idea is to replace every Conv2d with a steerable convolution layer between steerable feature fields, and associate the fields with proper fiber representations ρ(h).\n\nVINs use ordinary CNNs and can choose the size of intermediate feature maps. The design choices in steerable CNNs is the feature fields and fiber representations (or type) for every layer (Cohen and Welling, 2016a; Weiler and Cesa, 2021). The main difference4 in steerable CNNs is that we also need to tell the network how to transform every feature field, by specifying fiber representations, as shown in Figure 10.\n\nSpecification of input map and output action. We first specify fiber representations for the input and output field of the network: map M and action A. For input occupancy map and goal M : Z2 → {0, 1}2, it does not D4 to act on the 2 channels, so we use two copies of trivial representations ρM = ρtriv ⊕ ρtriv. For action, the final action output A : Z2 → R|A| is for logits of four actions A = (north, west, south, east) for every location. If we use H = C4, it naturally acts on the four actions (ordered ⟲) by cyclically ⟲ permuting the R4 channels. However, since the D4 group has 8 elements, we need a quotient representation, see (Weiler and Cesa, 2021) and Appendix G.\n\nSpecification of intermediate fields: value and reward. Then, for the intermediate feature fields: Q-values Qk, state value Vk, and reward Rm, we are free to choose fiber representations, as well as the width (number of copies). For example, if we want 2 copies of regular representation of D4, the feature field has 2 × 8 = 16 channels and the stacked representation is 16 × 16 (by direct-sum).\n\n3Technically, it also includes values or actions for obstacles, since the network needs to learn to approximate\n\nthe reward RM (s, ∆s) = −∞ with enough small reward and avoid obstacles.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\na Qa\n\nFor the Q-value field Qa k(s), we use representation ρQ and its size as CQ. We need at least CA ≥ |A| channels for all actions of Q(s, a) as in VIN and GPPN, then stacked together and denoted as Qk ≜ (cid:76) k with dimension Qk : Z2 → RCQ∗CA . Therefore, the representation is direct-sum (cid:76) ρQ for CA copies. The reward is implemented similarly as RM ≜ (cid:76) M and must have same dimension and representation to add element-wisely. For state value field, we denote the choose as fiber representation as ρV and its size CV . It has size Vk : Z2 → RCV Thus, the steerable kernel is matrix-valued with dimension Pθ : Z2 → R(CQ∗CA)×CV . In practice, we found using regular representations for all three works the best. It can be viewed as \"augmented\" state and is related to group convolution, detailed in Appendix G.\n\na Ra\n\nOther operations. We now visit the remained (equivariant) operations. (1) The max operation in Qk (cid:55)→ Vk+1. While we have showed the max operation in Vk+1(s) = maxa Qa k(s) is equivariant in Theorem E.3, we need to apply max(-pooling) for all actions along the \"representation channel\" from stacked representations CA ∗ CQ to one CQ. More details are in Appendix G.2. (2) The final output layer QK (cid:55)→ A. After the final iteration, the Q-value field Qk is fed into the policy layer with 1 × 1 convolution to convert the action logit field Z2 → R|A|.\n\nExtended method: Symmetric GPPN. Gated path planning network (GPPN (Lee et al., 2018)) proposes to use LSTM to alleviate the issue of unstable gradient in VINs. Although it does not strictly follow value iteration, it still follows the spirit of steerable planning. Thus, we first obtained a fully convolutional variant of GPPN from [Redacted for anonymous review], called ConvGPPN. It replaces the MLPs in the original LSTM cell with convolutional layers, and then replaces convolutions with equivariant steerable convolutions, resulting in a fully equivariant SymGPPN. See Appendix G.1 for details.\n\nExtended tasks: planning on learned maps with mapper networks. We consider two planning tasks on 2D grids: 2D navigation and 2-DOF manipulation. To demonstrate the ability of handling symmetry in differentiable planning, we consider more complicated state space input: visual navigation and workspace manipulation, and discuss how to use mapper networks to convert the state input and use end-to-end learned maps, as in (Lee et al., 2018; Chaplot et al., 2021). See Appendix H.2 for details.\n\nD.2 PYTORCH-STYLE PSEUDOCODE\n\nHere, we write a section on explaining the SymVIN method with PyTorch-style pseudocode, since it directly corresponds to what we propose in the method section. We try to relate (1) existing concepts with VIN, (2) what we propose in Section 4 and 5 for SymVIN, and (3) actual PyTorch implementation of VIN and SymVIN aligned line-by-line based on semantic correspondence.\n\nWe provide the key Python code snippets to demonstrate how easy it is to implement SymVIN, our symmetric version of VIN (Tamar et al., 2016a).\n\nIn the current Section 5 (SymPlan practice), we heavily use the concepts from Steerable CNNs. Thanks to the equivariant network community and the e2cnn package, the actual implementation is compact and closely corresponds to their non-equivariant counterpart, VIN, line-by-line. Thus, the ultimate goal here is to illustrate that, whatever concepts we have in regular CNNs (e.g., have whatever channels we want), we can can use steerable CNNs that incorporate desired extra symmetry (of D4 rotation+reflection or C4 rotation).\n\nWe highlight the implementation of the value iteration procedure in VIN and SymVIN:\n\nV := max\n\na\n\nRa + γ × P a ∗ V.\n\n(18)\n\nNote that we use actual code snippets to avoid hiding any details.\n\nDefining (steerable) convolution layer. First, we show the definition of the key convolution layer for a key operation in VIN and SymVIN: expected value operator, in Listing 1 and 2.\n\nAs proved in Theorem E.2, the expected value operator can be executed by a steerable convolution layer for (2D) path planning. This serves as the theoretical foundation on how we should use a steerable layer here.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\n1 import torch 2\n3 4\n5 6\n7 8\n9 10 11 12 # Define regular 2D convolution 13 q_conv = torch.nn.Conv2d( in_channels=1, 14 out_channels=2 * q_size, 15 kernel_size=F, stride=1, bias=False 16 17 )\n\n1 import torch 2 import e2cnn 3\n4 # Define the symmetry group to be D4 5 gspace = e2cnn.gspaces.FlipRot2dOnR2(N=4) 6 # Define feature (fiber) representations 7 field_type_q_in = e2cnn.nn.FieldType( 8\n9\n\ngspace=gspace, representations=2 * q_size * [gspace. regular_repr]\n\n10 ) 11 # Define steerable convolution 12 q_r2conv = e2cnn.nn.R2Conv( 13 14 15 16 )\n\nin_type=field_type_q_in, out_type=field_type_q_out, kernel_size=F, stride=1, bias=False\n\nListing 1: Define ‘expected value‘ convolution layer for VIN.\n\nListing 2: Define ‘expected value‘ (steerable) convolution layer for SymVIN.\n\nFor the left side, a regular 2D convolution is defined for VIN. The right side defines a steerable convolution layer, using the library e2cnn from (Weiler and Cesa, 2021). It provides high-level abstraction for building equivariant 2D steerable convolution networks. As a user, we only need to specify how the feature fields transform (as shown in Figure 10), and it will solve the G-steerability constraints, process what needs to be trained for equivariant layers, etc. We use name q_r2conv to highlight the difference.\n\nValue iteration procedure. Second, we compare the for loop for value iteration updates in VIN and SymVIN, where the former one has regular 2D convolution Conv2D (Listing 3), and the latter one uses steerable convolution (Weiler and Cesa, 2021) (Listing 4).\n\nThe lines are aligned based on semantic correspondence. The e2cnn layers, including steerable convolution layers, operate on its GeometricTensor data structure, which is to wrap a PyTorch tensor. We denote them with _geo suffix. It only additionally needs to specify how this tensor (feature field) transforms under a group (e.g., D4), i.e. the user needs to specify a group representation for it.\n\ntensor_directsum is used to concatenate two GeometricTensor’s (feature fields) and compute their associated representations (by direct-sum).\n\nThus, the e2cnn steerable convolution layer on the right side q_r2conv can be used as a regular PyTorch layer, while the input and output are GeometricTensor.\n\nWe also define the max operation as a customized max-pooling layer, named q_max_pool. The implementation is similar to the left side of VIN and needs to additionally guarantee equivariance, and the detail is omitted.\n\nNote that for readability, we assume we use regular representations for the Q-value field Q and the state-value field V . They are empirically found to work the best. This corresponds to the definition in field_type_q_in in line 9 in the SymVIN definition listing and the comments in line 16-17 in the steerable VI procedure listing for SymVIN.\n\nOther components are omitted.\n\nE SYMMETRIC PLANNING FRAMEWORK\n\nThis section formulates the notion of Symmetric Planning (SymPlan). We expand the understanding of path planning in neural networks by planning as convolution on steerable feature fields (steerable planning). We use that to build steerable value iteration and show it is equivariant.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n1 # Input: maze and goal map, #iterations K 2\n3 4\n5 6 x = torch.cat([maze_map, goal_map], dim=1) 7\n8 r = r_conv(x) 9\n\n1 # Input: maze and goal map, #iterations K 2\n3 from e2cnn.nn import GeometricTensor 4 from e2cnn.nn import tensor_directsum 5\n6 x = torch.cat([maze_map, goal_map], dim=1) 7 x_geo = GeometricTensor(x, type=field_type_x) 8 r_geo = r_r2conv(x_geo) 9\n\n# Concat and convolve V with P rv = torch.cat([r, v], dim=1) q = q_conv(rv)\n\n10 # Init value function V 11 v = torch.zeros(r.size()) 12 13 14 for _ in range(K): 15 16 17 18 19 20 21 22 23 24 25 26 # Output: ’q’ (to produce policy map)\n\n# Max over action channel # > Q: batch_size x q_size x W x H # > V: batch_size x 1 x W x H q = q.view(-1, q_size, W, H) v, _ = torch.max(q, dim=1) v = v.view(-1, W, H)\n\n# Concat (direct-sum) and convolve V with P rv_geo = tensor_directsum([r_geo, v_geo]) q_geo = q_r2conv(rv_geo)\n\n10 # Init V and wrap V in e2cnn ’geometric tensor’ 11 v_raw = torch.zeros(r_geo.size()) 12 v_geo = GeometricTensor(v_raw, field_type_v) 13 14 for _ in range(K): 15 16 17 18 19 20 21 22 23 24 25 26 # Output: ’q_geo’ (to produce policy map)\n\n# Max over group channel # > Q: batch_size x (|G| * q_size) x W x H # > V: batch_size x (|G| * 1) x W x H v_geo = q_max_pool(q_geo)\n\nListing 3: The central value iteration procedure for VIN. Some variable names are adjusted accordingly for readability. W and H are width and height for 2D map.\n\nListing 4: The equivariant steerable value iteration procedure for SymVIN. Lines are aligned by semantic correspondence. Definition of other field types are similar and thus omitted.\n\nE.1 STEERABLE PLANNING: PLANNING ON STEERABLE FEATURE FIELDS\n\nWe start the discussion based on Value Iteration Networks (VINs, (Tamar et al., 2016a)) and use a running example of planning on the 2D grid Z2. We aim to understand (1) how VIN-style networks embed planning and how its idea generalizes, (2) how is symmetry structure defined in path planning and how could it be injected into such planning networks.\n\nConstructing G-invariant transition: spatial MDP. Intuitively, the embedded MDP in a VIN is different from the original path planning problem, since (planar) convolutions are translation equivariant but there are different obstacles in different regions.\n\nWe found the key insight in VINs is that it implicitly uses an MDP that has translation equivariance. The core idea behind the construction is that it converts obstacles (encoded in transition probability P , by blocking) into “traps” (encoded in reward ̄R, by −∞ reward). This allows to use planar convolutions with translation equivariance, and also enables use to further use steerable convolutions.\n\nThe demonstration of the idea is shown in Figure 8 (Left). We call it spatial MDP, with different transition and reward function ̄M = ⟨S, A, ̄P , ̄Rm, γ⟩, which converts the “complexity” in the transition function P in M to the reward function ̄Rm in ̄M. The state and action space are kept the same: state S = Z2 and action A ⊂ Z2 to move ∆s in four directions in a 2D grid. We provide the detailed construction of the spatial MDP in Section E.3.\n\nSteerable features fields. We generalize the idea from VIN, by viewing functions (in RL and planning) as steerable feature fields, motivated by (Bronstein et al., 2021a; Cohen et al., 2020; Cohen and Welling, 2016a). This is analogous to pixels on images Ω → [255]3, and would allow us to apply convolution on it. The state value function is expressed as a field V : S → R, while the Q-value function needs a field with |A| channels: Q : S → R|A|. Similarly, a policy field5 has probability logits of selecting |A| actions. For the transition probability P (s′|s, a), we can use action to index it as P a(s′|s), similarly for reward Ra(s). The next section will show that we can convert the transition function to field and even convolutional filter.\n\n5We avoid the symbol π for policy since it is used for induced representation in (Cohen and Welling, 2016a;\n\nWeiler and Cesa, 2021).\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nE.2 SYMMETRIC PLANNING: INTEGRATING SYMMETRY BY CONVOLUTION\n\nThe seemingly slight change in the construction of spatial MDPs brings important symmetry structure. The general idea in exploiting symmetry in path planning is to use equivariance to avoid explicitly constructing equivalence classes of symmetric states. To this end, we construct value iteration over steerable feature fields, and show it is equivariant for path planning. In VIN, the convolution is over 2D grid Z2, which is symmetric under D4 (rotations and reflections). However, we also know that VIN is already equivariant under translations. To consider all symmetries, as in (Cohen and Welling, 2016a; Weiler and Cesa, 2021), we understand the group p4m = G = B ⋊ H as constructed by a base space B = G/H = (Z2, +) and a fiber group H = D4, which is a stabilizer subgroup that fixes the origin 0 ∈ Z2. We could then formally study such symmetry in the spatial MDP, since we construct it to ensure that the transition probability function in ̄M is G-invariant. Specifically, we can uniquely decompose any g ∈ Z2 ⋊ D4 as t ∈ Z2 and r ∈ D4 (and translations act \"trivially\" on action), so\n\n ̄P (s′ | s, a) = ̄P (g.s′ | g.s, g.a) ≡ ̄P ((tr).s′ | (tr).s, r.a) ,\n\n∀g = tr ∈ Z2 ⋊ D4, ∀s, a, s′. (19)\n\nExpected value operator as steerable convolution. The equivariance property can be shown stepby-step: (1) expected value operation, (2) Bellman operator, and (3) full value iteration. First, we use G-invariance to prove that the expected value operator (cid:80) Theorem E.1. If transition is G-invariant, the expected value operator E over Z2 is G-equivariant.\n\ns′ P (s′|s, a)V (s′) is equivariant.\n\nThe proof is in Section F.1 and visual understanding is in Figure 8 middle. However, this provides intuition but is inadequate since we do not know: (1) how to implement it with CNNs, (2) how to use multiple feature channels like VINs, since it shows for scalar-valued transition probability and value function (corresponding to trivial representation). To this end, we next prove that we can implement value iteration using steerable convolution with general steerable kernels.\n\nTheorem E.2. If transition is G-invariant, there exists a (one-argument, isotropic) matrix-valued steerable kernel P a(s − s′) (for every action), such that the expected value operator can be written as a steerable convolution and is G-equivariant:\n\nEa[V ] = P a ⋆ V,\n\n[g.[P a ⋆ V ]](s) = [P g.a ⋆ [g.V ]](s),\n\n∀s ∈ Z2, ∀g ∈ Z2 ⋊ D4.\n\n(20)\n\nThe full derivation is provided in Section F. We write the transition probability as P a(s, s′), and we show it only depends on state difference P a(s − s′) (or one-argument kernel (Cohen et al., 2020)) using G-invariance, which is the key step to show it is some convolution. Note that we use one kernel P a for each action (four directions), and when the group acts on E, it also acts on the action P g.a (and state, so technically acting on S × A). Additionally, if the steerable kernel also satisfies the D4-steerability constraint (Weiler and Cesa, 2021; Weiler et al., 2018), the steerable convolution is equivariant under p4m = Z2 ⋊ D4. We can then extend VINs from Z2 translation equivariance to p4m-equivariance (translations, rotations, reflections). The derivation follows the existing work on steerable CNNs (Cohen and Welling, 2016b;a; Weiler and Cesa, 2021; Cohen et al., 2020), while this is our goal: to justify the close connection between path planning and steerable convolutions.\n\nSteerable Bellman operator and value iteration. We can now represent all operations in Bellman (optimality) operator on steerable feature fields over Z2 (or steerable Bellman operator) as follows:\n\nVk+1(s) = max\n\na\n\nRa(s) + γ × [P a ⋆ Vk] (s),\n\n(21)\n\nwhere V, Ra, ̄P a are steerable feature fields over Z2. As for the operations, maxa is (max) pooling (over group channel), +, × are point-wise operations, and ⋆ is convolution. As the second step, the main idea is to prove every operation in Bellman (optimality) operator on steerable fields is equivariant, including the nonlinear maxa operator and +, ×. Then, iteratively applying Bellman operator forms value iteration and is also equivariant, as shown below and proved in Appendix F.4.\n\nProposition E.3. For a spatial MDP with G-invariant transition, the optimal value function can be found through G-steerable value iteration.\n\nRemark. Our framework generalizes the idea behind VINs and enables us to understand its applicability and restrictions. More importantly, this allows us to integrate symmetry but avoid explicitly\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nbuilding equivalence classes and enables planning with symmetry in end-to-end fashion. We emphasize that the symmetry in spatial MDPs is different from symmetric MDPs (Zinkevich and Balch, 2001; Ravindran and Barto, 2004; van der Pol et al., 2020a), since our reward function is not Ginvariant (if not conditioning on reward). Although we focus on Z2, we can generalize to path planning on higher-dimensional or even continuous Euclidean spaces (like R3 space (Weiler et al., 2018) or spatial graphs in R3 (Brandstetter et al., 2021)), and use equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We refer the readers to (Cohen and Welling, 2016b;a; Cohen, 2021; Weiler and Cesa, 2021) for more details.\n\nE.3 DETAILS: CONSTRUCTING PATH PLANNING IN NEURAL NETWORKS\n\nWe provide the detailed construction of doing path planning in neural networks in the Section E. This further explains the visualization in Figure 8 left. We use the running example of planning on the 2D grid Z2. We aim to understand (1) how VINstyle networks embed planning and how its idea generalizes, (2) how is symmetry structure defined in path planning and how could it be injected into such planning networks. Recall that we aim to understand (1) how VIN-style networks embed planning and how its idea generalizes, (2) how is symmetry structure defined in path planning and how could it be injected into such planning networks.\n\nPath planning as MDPs. To answer the above two questions, we first need to understand how a VIN embeds a path planning problem into a convolutional network as some embedded MDP. Intuitively, the embedded MDP in a VIN is different from the original path planning problem, since (planar) convolutions are translation equivariant but there are different obstacles in different regions. For path planning on the 2D grid S = Z2, the objective is to avoid some obstacle region Cobs ⊂ Z2 and navigate to the goal region Cgoal through free space C\\Cobs. An action a = ∆s ∈ A is to move from the current state s to a next free state s′ = s + ∆s, where for now we limit it to be in four directions: A =. Assuming deterministic transition, the agent moves to s′ with probability 1 if s + ∆s ∈ C\\Cobs. If it hits an obstacle, it stays at s if s + ∆s ∈ Cobs: P (s + ∆s | s, ∆s) = 0 and P (s | s, ∆s) = 1. Every move has a constant negative reward R(s, a) = −1 to encourage shortest path. We call this ground path planning MDP, a 5-tuple M = ⟨S, A, P, R, γ⟩.\n\nConstructing embedded MDPs. However, such transition function is not translation-invariant, i.e. at different position, the transition probabilities are not related by any symmetry: P (s′|s, a) ̸= P (g.s′|g.s, g.a). Instead, we could always construct a \"symmetric\" MDP that has equivalent optimal value and policy for path planning problems, which is implicitly realized in VINs. The idea is to move the information of obstacles from transition function to reward function: when we hit some action s + ∆s ∈ Cobs, we instead allow transition ̄P (s + ∆s | s, ∆s) = 1 (with all other s′ as 0 probability) while set a \"trap\" with negative infinity reward ̄Rm (s, ∆s) = −∞. The reward function needs the information from the occupancy map M , indicating obstacles Cobs and free space. For the free region, the reward is still a constant ̄RM (s, ∆s) = −1, indicating the cost of movement. We call it the embedded MDP, with different transition and reward function ̄M = ⟨S, A, ̄P , ̄RM , γ⟩, which converts the “complexity” in the transition function P in M to the reward function ̄Rm in ̄M. Here, map M shall also be treated as an “input”, thus later we will derive how the group acts on the map g.M . It has the same optimal policy and value as the ground MDP M, since the optimal policies in both MDPs will avoid obstacles in M or trap cells in ̄M. It could be easily verified by simulating value iteration backward in time from the goal position. The transition probability ̄P of the embedded MDP ̄M is for an “empty” maze and thus translationinvariant. Note that the reward function ̄R is not not necessarily invariant. This construction is not limited to 2D grid and generalizes to continuous state space or even higher dimensional space, such as R6 configuration space for 6-DOF manipulation.\n\nNote, all of this is what we use to conceptually understand how a VIN is possible to learn. The reward cannot be negative infinity, but the network will learn it to be smaller than all desired Qvalues.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nE.4 DETAILS: UNDERSTANDING SYMMETRIC PLANNING BY ABSTRACTION\n\nHow do we deal with potential symmetry in path planning? How do we characterize it? We try to understand symmetric planning (steerable planning after integrating symmetry with equivariance) and how it is difference classic planning algorithms, such as A*, for planning under symmetry.\n\nSteerable planning. Recall that we generalize the idea of VIN by considering it as a planning network that composes of mappings between steerable feature fields.\n\nThe critical point is that, convolutions directly operate on local patches of pixels and never directly touch coordinates of pixels. In analogy, this avoids a critical drawback in other explicit planning algorithms: in sampling-based planning, a trajectory (s1, a1, s2, a2, . . .) is sampled and inevitable represented by states Ω = S. However, to find another symmetric state g.s, we potentially need to compare it against all known states S ′ ⊂ S with all symmetries g ∈ G. On high level, an implicit planner can avoid such symmetry breaking and is more easily compatible with symmetry by using equivariant constraints.\n\nWe can use MDP homomorphism to understand this (Ravindran and Barto, 2004; van der Pol et al., 2020b).\n\nMDP homomorphisms. An MDP homomorphism h : M → M is a mapping from one MDP M = ⟨S, A, P, R, γ⟩ to another M = ⟨S, A, P , R, γ⟩ (Ravindran and Barto, 2004; van der Pol et al., 2020b). h consists of a tuple of surjective maps h = ⟨φ, {αs | s ∈ S}⟩, where φ : S → S is the state mapping and αs : A → A is the state-dependent action mapping. The mappings are constructed to satisfy the following conditions:\n\nR (φ(s), αs(a)) ≜ R(s, a) , P (φ (s′) | φ(s), αs(a)) ≜ (cid:88)\n\ns′′∈φ−1(φ(s′))\n\nP (s′′ | s, a) ,\n\n(22)\n\nfor all s, s′ ∈ S and for all a ∈ A.\n\nWe call the reduced MDP M the homomorphic image of M under h. If h = ⟨φ, {αs | s ∈ S}⟩ has bijective maps φ and {αs}, we call h an MDP isomorphism. Given MDP homomorphism h, (s, a) and (s′, a′) are said to be h-equivariant if σ(s) = σ (s′) and αs(a) = αs′ (a′).\n\nSymmetry-induced MDP homomorphisms. Given group G, an MDP homomorphism h is said to be group structured if any state-action pair (s, a) and its transformed counterpart g.(s, a) are mapped to the same abstract state-action pair: (φ(s), αs(a)) = (φ(g.s), αg.s(g.a)), for all s ∈ S, a ∈ A, g ∈ G. For convenience, we denote g.(s, a) as (g.s, g.a), where g.a implicitly6 depends on state s. Applied to the transition and reward functions, the transition function P is G-invariant if P satisfies P (g.s′|g.s, g.a) = P (s′|s, a), and reward function R is G-invariant if R(g.s, g.a) = R(s, a), for all s ∈ S, a ∈ A, g ∈ G.\n\nHowever, this only fits the type of symmetry in (van der Pol et al., 2020a; Wang et al., 2021). And also, they cannot handle invariance to translation Z2. In our case, we need to augment the reward function with map M input:\n\nRg.M (g.s, g.a) = RM (s, a),\n\n(23)\n\nfor all s ∈ S, a ∈ A, g ∈ G = p4m.\n\nThis means that, at least for rotations and reflections D4, the MDPs constructed from transformed maps {g.M } are MDP isomorphic to each other.\n\n5We avoid the symbol π for policy since it is used for induced representation in (Cohen and Welling, 2016a;\n\nWeiler and Cesa, 2021).\n\n6The group operation acting on action space A depends on state, since G actually acts on the product space S × A: (g, (s, a)) (cid:55)→ g.(s, a), while we denote it as (g.s, g.a) for consistency with h = ⟨φ, {αs | s ∈ S}⟩. As a bibliographical note, in van der Pol et al. (2020b), the group acting on state and action space is denoted as state transformation Lg : S → S and state-dependent action transformation K s\n\ng : A → A.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nE.5 NOTE: AUGMENTED STATE\n\nWe derive the relationship between group convolution and steerable convolution in Section C.4. The augmented state Z2 ⋊ D4 → R can be similarly treated on the group p4m = Z2 ⋊ D4. It is equivalent to using regular representation on the base space Z2 as Z2 → R8.\n\nF SYMMETRIC PLANNING FRAMEWORK: PROOFS\n\nWe show the derivation and proofs for all theoretical results in this section.\n\nWe follow the notation in (Cohen et al., 2020) to use ⋆ for (one-argument) convolution and · for (two-argument) multiplication:\n\nEa[V ](s) = [P a · V ](s) ≡\n\n(cid:88)\n\ns′\n\nP a (s′ | s) · V (s′)\n\n(24)\n\nF.1 PROOF: EQUIVARIANCE OF SCALAR-VALUED EXPECTED VALUE OPERATION\n\nWe present the Theorem E.1 here and its formal definition. Theorem F.1. If transition is G-invariant, the expected value operator E over Z2 is G-equivariant:\n\n[g.Ea[V ]] (s) = [Eg.a[g.V ]] (s),\n\nfor all g = tr ∈ Z2 ⋊ D4.\n\nProof. E is the expected value operator. We also write the transition probability as\n\nRecall the G-invariance condition of transition probability, the group element g acts on s, a, s′:\n\n ̄P (s′ | s, a) = ̄P (g.s′ | g.s, g.a) ≡ ̄P ((tr).s′ | (tr).s, r.a) ,\n\n∀g = tr ∈ Z2 ⋊ D4, ∀s, a, s′, (25)\n\nwhere we can uniquely decompose any g ∈ Z2 ⋊ D4 as t ∈ Z2 and r ∈ D4 (Cohen and Welling, 2016a). Note that, since the action is the difference between states a = ∆s = s′ − s, the translation part t acts trivially on it, so g.a = (tr).a = r.a for all r ∈ D4.\n\nWe transform the feature field and show its equivariance:\n\n[g.Ea[V ]](s) ≡ [g.[P a · V ]](s)\n\nρtriv(r)P a (cid:0)s′ | (tr)−1.s(cid:1) · V (s′)\n\nρtriv(r)P r.a ((tr).s′ | s) · V (s′)\n\nρtriv(r)P r.a ( ̃s′ | s) · V (cid:0)(tr)−1 ̃s′(cid:1)\n\nP r.a ( ̃s′ | s) · ρtriv(r)V (cid:0)(tr)−1 ̃s′(cid:1)\n\n≡\n\n=\n\n=\n\n=\n\n(cid:88)\n\ns′ (cid:88)\n\ns′ (cid:88)\n\n ̃s′ (cid:88)\n\n ̃s′\n\n≡ [P r.a · [g.V ]](s) ≡ [Er.a[g.V ]](s).\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31) (32)\n\nWe use the trivial representation ρtriv(g) = Id1×1 = 1 to emphasize that (1) the group element g acts on feature fields P a and V , and (2) both feature fields P a and V are scalar-valued and correspond to the one-dimensional trivial representation of r ∈ D4.\n\nIn the third line, we use the G-invariance of transition probability.\n\nThe fourth line uses substitution ̃s′ ≜ (tr).s′, for all s′ ∈ Z2 and tr ∈ Z2⋊D4. This is an one-to-one mapping and the summation does does not change.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nF.2 PROOF: expected value operator AS STEERABLE CONVOLUTION\n\nIn this section, we derive how to cast expected value operator as steerable convolution. The equivariance proof is in the next section.\n\nIn Theorem E.1, we show equivariance of value iteration in 2D path planning, while it is only for the case that feature fields P a and V are scalar-valued and correspond to one-dimensional trivial representation of r ∈ D4.\n\nHere, we provide the derivation for Theorem E.2 show that steerable CNNs (Cohen and Welling, 2016a) can achieve value iteration since we could construct the G-invariant transition probability as a steerable convolutional kernel. This generalizes Theorem E.1 from scalar-valued kernel (for transition probability) with trivial representation to matrix-valued kernel with any combination of representations, enabling using stack (direct-sum) of feature fields and representations.\n\nWe state Theorem E.2 here for completeness:\n\nTheorem F.2. If transition is G-invariant, there exists a (one-argument, isotropic) matrix-valued steerable kernel P a(s − s′) (for every action), such that the expected value operator can be written as a steerable convolution and is G-equivariant:\n\nEa[V ] = P a ⋆ V,\n\n[g.[P a ⋆ V ]](s) = [P g.a ⋆ [g.V ]](s),\n\n∀s ∈ Z2, ∀g ∈ Z2 ⋊ D4.\n\n(33)\n\nIn our earlier definition, ψa and fin are transition probability and value funcSteerable kernels. tion, which are both real-valued ψa : Z2 → R, fin : Z2 → R. However, this is a special case which corresponds to use one-dimensional trivial representation of the fiber group D4. In the general case in steerable CNNs (Cohen and Welling, 2016a; Weiler and Cesa, 2021), we can choose the feature fields ψa : Z2 → RCout×Cin and fin : Z2 → RCin and their fiber representations, which we will introduce the group representations of D4 and how to choose in practice in the next section. Weiler et al. (2018) show that convolutions with steerable kernels ψa : Z2 → RCout×Cin is the most general equivariant linear map between steerable feature space, transforming under ρin and ρout. In analogy to the continuous version7 in (Weiler and Cesa, 2021), the convolution is equivariant iff the kernel satisfies a H-steerability kernel constraint:\n\nψa(hs) = ρout(h)ψa(s)ρin(h−1) h ∈ H = D4, s ∈ Z2.\n\n(34)\n\nExpected value operation as steerable convolution. The foremost step is to show that the expected value operation is a form of convolution and is also G-equivariant. By definition, if we want to write a (linear) operator as a form of convolution, we need one-argument kernel. Cohen et al. (2020) show that every linear equivariant operator is some convolution and provide more details. For our case, this is formally shown as follows.\n\nProposition F.3. If the transition probability is G-invariant, it can be expressed as an (oneargument) kernel P a(s′|s) = P a(s′ − s) that only depends on the difference s′ − s.\n\nProof. The form of our proof is similar to (Cohen et al., 2020), while its direction is different from us. We construct a MDP such that the transition probability kernel is G-invariant, while Cohen et al. (2020) assume the linear operator ψ · f is linear equivariant operator on a homogeneous space, and then derive that the kernel is G-invariant and expressible as one-argument kernel. Additionally, our kernel ψa(s, s′) and ψa(s − s′) both live on the base space B = Z2 but not on the group G = Z2 ⋊ D4. We show that the transition probability only depends on the difference ∆s = s′ −s, so we can define the two-argument kernel P a(s′|s) on S × S by an one-argument kernel P a(s′ − s) (for every action\n\n7Weiler and Cesa (2021) use letter G to denote the stabilizer subgroup H ≤ O(2) of E(2).\n\n30\n\nPublished as a conference paper at ICLR 2023\n\na) on S = Z2, without loss of generality:\n\nP a(s′ − s) ≡ P a(0, s′ − s)\n\n= P g.a(g.0, g.(s′ − s)) = P r.a((rs).0, (rs).(s′ − s)) = P r.a(r.s, r.(s′ − s + s)) = P r.a(r.s, r.s′) = P a(s, s′),\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(40) where the second step uses G-invariance with g = sr, understood as the composition of a translation s ∈ Z2 and a transformation in r ∈ D4.\n\nAdditionally, we can also derive that, for the one-argument kernel, if we rotate state difference r.(s′ − s), the probability is the same for rotated action r.a.\n\nP a(s′ − s) = P r.a(r.(s′ − s)), for all r ∈ D4, s, s′ ∈ Z2\n\n(41)\n\nThe expected value operator with two-argument kernel can be then written as\n\nE[V ](s) ≡ [P a · V ](s) =\n\nP a(s′|s)V (s′) =\n\n(cid:88)\n\ns′\n\n(cid:88)\n\ns′\n\nP a(s′ − s)V (s′) ≡ [P a ⋆ V ](s).\n\n(42)\n\nNote that we do not differentiate between cross-correlation (s′ − s) and convolution (s − s′).\n\nF.3 PROOF: EQUIVARIANCE OF expected future value\n\nOur derivation follows the existing work on group convolution and steerable convolution networks (Cohen and Welling, 2016b;a; Weiler and Cesa, 2021; Cohen et al., 2020). However, the goal of providing the proof is not just for completeness, but instead to emphasize the close connection between how we formulate our planning problem and the literature of steerable CNNs, which explains and justifies our formulation.\n\nAdditionally, there are several subtle differences worth to mention. (1) Throughout the paper, we do not discuss kernels or fields that live on a group G to make it more approachable. Nevertheless, group convolutions are a special case of steerable convolutions with fiber representation ρ as regular representation. (2) We use Z2 as running example. Some prior work uses R2 or Z2, but they are merely just differ in integral and summation. (3) The definition of convolution and cross-correlation might be defined and used interchangeably in the literature of (equivariant) CNNs.\n\nNotation. To keep notation clear and consistent with the literature (Cohen and Welling, 2016a; Cohen et al., 2020; Weiler and Cesa, 2021), we denote the transition probability ̄P (s′|s, a) ≜ ψa(s, s′) ∈ R (one kernel for an action) and value function as V (s′) ≜ fin(s′) ∈ R, and the resulting expected value as f a\n\ns′ ψa(s, s′)fin(s′) (given a specific action a).\n\nout(s) = (cid:80)\n\nTransformation laws: induced representation. For some group acting on the base space Z2, the signals f : Z2 → Rc are transformed like Cohen and Welling (2016a):\n\n[π(g)f ](x) = f (g−1x) (43) Apply a translation t and a transformation r ∈ D4 to f , we get π(tr)f . The transformation law on the input space fin is (Cohen and Welling, 2016a; Weiler and Cesa, 2021):\n\nf (x) (cid:55)→ [π(tr)f ] (x) ≜ ρ(r) · (cid:2)f (cid:0)(tr)−1x(cid:1)(cid:3)\n\n(44)\n\nThe transformation law of the output space after applying πin on input fin is given by Cohen and Welling (2016a):\n\n[ψ ⋆ f ] (x) (cid:55)→ [ψ ⋆ [π(tr)f ]] (x) ≜ ρ(r) · (cid:2)[ψ ⋆ f ] (cid:0)(tr)−1x(cid:1)(cid:3) .\n\n(45) out : Z2 → RCout and the input space is fin : Z2 → RCin . Intuitively, In our case, the output space is f a if we rotate a vector field (fibers represent arrows) by the induced representation π(tr) of f , we also need to rotate the direction of arrows by ρ(r), r ∈ D4.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nEquivariance. Now we prove the steerable convolution is equivariant: [ψa ⋆ [πin(g)fin]] (s) = [πout(g)f a\n\nout] (s) ∀s ∈ S, ∀g ∈ G.\n\n(46)\n\nD4\n\nH ρin = indZ2⋊D4\n\nThe induced representation of input field fin is induced by the fiber representation ρin, expressed by πin ≜ indG ρin, where ρin is the fiber representation of group H = D4. The induced representation of output field πout is analogously from ρout. Weiler and Cesa (2021) proved equivariance of steerable convolutions for R2 case, while we include the proof under our setup for completeness. The definition in (Weiler and Cesa, 2021) uses a form of cross-correlation and we use convolution, while it is usually referred to interchangeably in the literature and is equivalent. Cohen and Welling (2016a); Weiler et al. (2018); Weiler and Cesa (2021); Cohen et al. (2020); Cohen (2021) provide more details and we refer the readers to them for more comprehensive account. The convolution on discrete grids Z2 with input field fin transformed by the induced representation πin gives:\n\n[ψa ⋆ [πin(rt)fin]](s) =\n\n=\n\n=\n\n(cid:88)\n\ns′∈Z2 (cid:88)\n\ns′∈Z2 (cid:88)\n\ns′∈Z2\n\nψa(s − s′)[πin(rt)fin](s′)\n\nψa(s − s′)ρin(r)fin(r−1(s′ − t))\n\nρout(r)ψa(r−1(s − s′))ρin(r)−1ρin(r)fin(r−1(s′ − t))\n\n= ρout(r)\n\n(cid:88)\n\nψa(r−1(s − s′))fin(r−1(s′ − t))\n\ns′∈Z2 (cid:88)\n\nψa(r−1(s − t) − ̃s)fin( ̃s)\n\n= ρout(r)\n\n ̃s∈Z2 = ρout(r)fout(r−1(s − t)) = [πout(rt)f a where s′ ∈ S = Z2, and thus satisfies the equivariance condition: [ψa ⋆ [πin(rt)fin]] (s) = [πout(rt)f a\n\nout] (s),\n\nout] (s), ∀s ∈ Z2, ∀rt ∈ Z2 ⋊ D4.\n\n(47)\n\n(48)\n\n1. Definition of ⋆\n\n2. Transformation law of the induced representation πin (Cohen and Welling, 2016a; Weiler\n\nand Cesa, 2021)\n\n3. Kernel steerability ψa(s) = ρout(h)ψa(h−1s)ρin(h−1) (Weiler and Cesa, 2021) 4. Move and cancel 5. Substitutes ̃s = r−1(s′ − t), r−1s′ = r−1t + ̃s, so r−1(s − s′) = r−1(s − t) − ̃s. Since r ∈ D4 and s − s′ ∈ Z2, the result is still in p4m, it is one-to-one correspondence p4m×Z2 → Z2, and the summation does not change. Weiler and Cesa (2021) analogously considers the continuous case, where D4 is orthogonal transformations so the Jacobian is always 1.\n\n6. Definition of ⋆\n\n7. Transform law of the induced representation πout\n\nF.4 PROOF: EQUIVARIANCE OF STEERABLE VALUE ITERATION\n\nAs the third and final step, we would like to show that the full steerable value iteration pipeline is equivariant under G = Z2 ⋊ D4. We need to show that every operation in the steerable value iteration is equivariant.\n\nThe key is to prove that maxa is an equivariant non-linearity over feature fields, which follows Section D.2 in (Weiler and Cesa, 2021).\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: We attach a copy of the commutative diagram of SymVIN to show the equivariance of steerable value iteration. Commutative diagram for the full pipeline of SymVIN on steerable feature fields over Z2 (every grid). If rotating the input map M by πM (g) of any g, the output action A = SymVIN(M ) is guaranteed to be transformed by πA(g), i.e. the entire steerable SymVIN is equivariant under induced representations πM and πA: SymVIN(πM (g)M ) = πA(g)SymVIN(M ). We use stacked feature fields to emphasize that SymVIN supports direct-sum of representations beyond scalar-valued.\n\nk(s) = ̄Ra\n\nM (s) + γ × (cid:2) ̄P a\n\nθ ⋆ Vk\n\nStep 1: V (cid:55)→ Q. Here, we prove the equivariance of Qa let the group acts on both sides: k(s) = ̄Ra Qa k](s) = [πout(g) ̄Ra ⇐⇒ [πout(g)Qa k](s) = [πout(g) ̄Ra ⇐⇒ [πout(g)Qa ⇐⇒ Qg.a\n\nM (s) + γ × (cid:2) ̄P a\n\nk (g−1s) = ̄Rg.a\n\nθ ⋆ Vk\n\n(cid:3) (s)\n\n(cid:3)(cid:3) (s)\n\nθ ⋆ Vk\n\nM ](s) + γ × (cid:2)πout(g) (cid:2) ̄P a M ](s) + γ × (cid:2) ̄P a g.M (g−1s) + γ × (cid:2) ̄P g.a πM(g)M ( ̃s) + γ × (cid:2) ̄P ̃a\n\nθ ⋆ Vk\n\n(cid:3) ( ̃s)\n\nθ\n\nθ ⋆ [πin(g)Vk](cid:3) (s) (cid:3) (g−1s) ⋆ Vk\n\n⇐⇒ Q ̃a\n\nk( ̃s) = ̄R ̃a\n\n(cid:3) (s). First,\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\n(53)\n\nThe the last step we substitute ̃s = g−1s and ̃a = g.a. M : Z2 → {0, 1}2 is the concatenation of maze occupancy map and goal map, which also lives on Z2. We use two copies of trivial representations as fiber representation ρM, and denote the induced representation of the field M as πM.\n\nThen, we prove the equivariance: if we transform the occupancy map (and goal map), the value iteration should have both input V and output Q transformed. Since this is an iterative process, the only input to the value iteration is actually the occupancy map M : Z2 → {0, 1}2.\n\nBefore that, we observe that the reward also has G-invariance when we have map as input:\n\n ̄Ra\n\nM (s) = ̄Rg.a\n\ng.M (g.s).\n\n(54)\n\nAdditionally, since the reward ̄Ra action a, when we transform the map, we also need to transform the action: ̄Rg.a\n\nM (s) means the reward at given position in map M after executing\n\ng.M (s).\n\nSince it is iterative process, let the Q-map being transformed by g:\n\n[g.Qa\n\nk](s) = Qa = ̄Ra = ̄Rg.a = ̄Rg.a\n\nk(g−1s) M (g−1s) + γ × (cid:2) ̄P a g.M (s) + γ × (cid:2) ̄P a g.M (s) + γ × (cid:2) ̄P g.a\n\nθ\n\nθ ⋆ Vk\n\n(cid:3) (g−1s)\n\nθ ⋆ Vk\n\n(cid:3) (g−1s) ⋆ [g.Vk](cid:3) (s)\n\n(55)\n\n(56)\n\n(57)\n\n(58)\n\nThe second last step uses the G-invariance condition ̄Ra equivariance of steerable convolution.\n\nM (s) = ̄Rg.a\n\ng.M (g.s). The last step uses the\n\nIt should be understood as: (1) transforming map g.M and action g.a, is always equal to (2) transforming values [g.Qa\n\nk] and [g.Vk]. This proves the equivariance visually shown in Figure 11.\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nStep 2: Q (cid:55)→ V . The second step is to show for Vk+1(s) = maxa Qa\n\nk(s).\n\nIntuitively, we sum over every channel of each representation. For example, if we have N copies of the regular representation with size |D4| = 8, we transform the tensor (N × 8) × m × m to (1 × 8) × m × m along the N channel. Thus, how we use the 8 × 8 regular representation to transform the N × 8 channels still holds for 1 × 8, which implies equivariance. The m × m spatial map channels form the base space Z2 and are transformed as usual (spatially rotated).\n\nWeiler and Cesa (2021) provide detailed illustration and proofs for equivariance of different types of non-linearities.\n\nStep 3: multiple iterations. Since each layer is equivariant (under induced representations), Cohen and Welling (2016b); Kondor and Trivedi (2018); Cohen et al. (2020) show that stacking multiple equivariant layers is also equivariant. Thus, we know iteratively applying step 1 and 2 (equivariant steerable Bellman operator) is also equivariant (steerable value iteration).\n\nG IMPLEMENTATION DETAILS\n\nG.1\n\nIMPLEMENTATION OF SYMGPPN\n\nConvGPPN (Kong, 2022) is inspired by VIN and GPPN. To avoid the training issues in VIN, GPPN proposes to use LSTM to alleviate them. In particular, it does not use max pooling in the VIN. Instead, it uses a CNN and LSTM to mimic the value iteration process. ConvGPPN, on the other hand, integrates CNN into LSTM, resulting in a single component convLSTM for value iteration. We found that ConvGPPN performs better than GPPN in most cases. Based on ConvGPPN, SymGPPN replaces each convolutional layer with steerable convolutional layer.\n\nG.2\n\nIMPLEMENTATION OF max OPERATION\n\nHere, we consider how to implement the max operation in Vk+1(s) = maxa Qa taken over every state, so the computation mainly depends on our choice of fiber representation.\n\nk(s). The max is\n\nFor example, if we use trivial representations for both input and output, the input would be Qk : Z2 → R1∗CA and the output is state-value Vk : Z2 → R. This recovers the default value iteration since we take max over RCA vector.\n\nIn steerable CNNs, we can use stack of fiber representations. We can choose from regular-regular, trivial-trivial, and regular-trivial (trivial-regular is not considered). We already covered trivial representations for both input and output, they would be Qk : Z2 → RCQ∗CA and Vk : Z2 → RCV with CQ = CV = 1, since every channel would need a trivial representation. If we use regular representation for Q and trivial for V , they are Qk : Z2 → RCQ∗CA and Vk : Z2 → RCV with CQ = |D4| = 8 and CV = 1. It degenerates that we just take max over all CQ ∗ CA channels.\n\nFor both using regular representations, we need to make sure they use the same fiber group (such as D4 or C4), so CQ = CV . If using D4, we have Qk : Z2 → R8∗CA and Vk : Z2 → R8, and we take max over every CA channels (for every location) and have 8 channels left, which are used as Z2 → R8.\n\nEmpirically, we found using regular representations for both works the best overall.\n\nH EXPERIMENT DETAILS AND ADDITIONAL RESULTS\n\nH.1 ENVIRONMENT SETUP\n\nAction space. Note that the MDP action space A needs to be compatible with the group action G × A → A. Since the E2CNN package (Weiler and Cesa, 2021) uses counterclockwise rotations as generators for rotation groups Cn, the action space needs to be counterclockwise.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nWe show the figures for Configuration-space and Workspace manipulation in Figure 12, and the figures for 2D and Visual Navigation in Figure 13.\n\nFigure 12: A set of visualization for a 2-joint manipulation task. The obstacles are randomly generated. (1) The 2-joint manipulation task shown in top-down workspace with 96 × 96 resolution. This is used as the input to the Workspace Manipulation task. (2) The predicted configuration space in resolution 18 × 18 from a mapper module, which is jointly optimized with a planner network. (3) The ground truth configuration space from a handcraft algorithm in resolution 18 × 18. This is used as input to the Configuration-space (C-space) Manipulation task and as target in the auxiliary loss for the Workspace Manipulation task (as done in SPT (Chaplot et al., 2021)). (4) The predicted policy (overlaid with C-space obstacle for visualization) from an end-to-end trained SymVIN model that uses a mapper to take the top-down workspace image and plans on a learned map. The red block is the goal position.\n\nManipulation. For planning in configuration space, the configuration space of the 2 DoFs manipulator has no constraints in the {0, π} boundaries, i.e., no joint limits. To reflect this nature of the configuration space in manipulation tasks, we use circular padding before convolution operation. The circular padding is applied to convolution layers in VIN, SymVIN, ConvGPPN, and SymGPPN. Moreover, in GPPN, there is a convolution encoder before the LSTM layer. We add the circular padding in the convolution layers in GPPN as well.\n\nIn 2-DOF manipulation in configuration space, we adopt the setting in (Chaplot et al., 2021) and train networks to take as input of configuration space, represented by two joints. We randomly generate 0 to 5 obstacles in the manipulator workspace. Then the 2 degree-of-freedom (DOF) configuration space is constructed from workspace and discretized into 2D grid with sizes {18, 36}, corresponding to bins of 20◦ and 10◦, respectively.\n\nWe allow each joint to rotate over 2π, so the configuration space of 2-DOF manipulation forms a torus T2. Thus, the both boundaries need to be connected when generating action demonstrations, and (equivariant) convolutions need to be circular (with padding mode) to wrap around for all methods. We allow each joint to rotate over 2π, so the both boundaries in configuration space need to be connected when generating action demonstrations, and (equivariant) convolutions need to be circular (with padding mode) to wrap around for all methods.\n\nH.2 BUILDING MAPPER NETWORKS\n\nFor visual navigation. For navigation, we follow the setting in GPPN (Lee et al., 2018). The input is m × m panoramic egocentric RGB images in 4 directions of resolution 32 × 32 × 3, which forms a tensor of m × m × 4 × 32 × 32 × 3. A mapper network converts every image into a 256-dimensional embedding and results in a tensor in shape m×m×4×256 and then predicts map layout m×m×1.\n\nFor the first image encoding part, we use a CNN with first layer of 32 filters of size 8 × 8 and stride of 4 × 4, and second layer with 64 filters of size 4 × 4 and stride of 2 × 2, with a final linear layer of size 256.\n\nThe second obstacle prediction part, the first layer has 64 filters and the second layer has 1 filter, all with filter size 3 × 3 and stride 1 × 1.\n\nFor workspace manipulation. For workspace manipulation, we use U-net Ronneberger et al. (2015) with residual-connection He et al. (2015) as a mapper, see Figure.14. The input is 96 × 96 top-down occupancy grid of the workspace with obstacles, and the target is to output 18 × 18 configuration space as the maps for planning.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: A set of visualization for 2D navigation and visual navigation. The maze is randomly generated. (1, top) The 3D visual navigation environment generated by an illustrative 7 × 7 map, where we highlight the panoramic view at a position (5, 3) with four RGB images (resolution 32 × 32 × 3). The entire observation tensor for this 7 × 7 example visual navigation environment is 7 × 7 × 4 × 32 × 32 × 3. This is used as the input to the Visual Navigation task. (2) Another predicted map in resolution 15 × 15 from a mapper module, which is jointly optimized with a planner network. We show the visualization a different map used in actual training. (3) The ground truth map in resolution 15 × 15. This is also used as input to the 2D Navigation task and as target in the auxiliary loss for the Visual Navigation task (as done in GPPN). (4) The predicted policy from an end-to-end trained SymVIN model that uses a mapper to take the observation images (formed as a tensor) and plans on a learned map. The red block is the goal position.\n\nFigure 14: The U-net architecture we used as manipulation mapper.\n\nDuring training, we pre-train the mapper and the planner separately for 15 epochs. Where the mapper takes manipulator workspace and outputs configuration space. The mapper is trained to minimize the binary cross entropy between output and ground truth configurations space. The planner is trained in the same way as described in Section 6.1. After pre-training, we switch the input to the planner from ground truth configuration space to the one from the mapper. During testing, we follow the pipeline in Chaplot et al. (2021) that the mapper-planner only have access to the manipulator workspace.\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nH.3 TRAINING SETUP\n\nWe try to mimic the setup in VIN and GPPN (Lee et al., 2018).\n\nFor non-SymPlan related parameters, we use learning rate of 10−3, batch size of 32 if possible (GPPN variants need smaller), RMSprop optimizer.\n\nFor SymPlan parameters, we use 150 hidden channels (or 150 trivial representations for SymPlan methods) to process the input map. We use 100 hidden channels for Q-value for VIN (or 100 regular representations for SymVIN), and use 40 hidden channels for Q-value for GPPN and ConvGPPN (or 40 regular representations for SymGPPN on 15 × 15, and 20 for larger maps because of memory constraint).\n\nH.4 VISUALIZATION OF LEARNED MODELS\n\nWe visualize a trained VIN and a SymVIN, evaluated on a 15 × 15 map and its rotated version. For non-symmetric VIN in Figure 15, the learned policy is obviously not equivariant under rotation.\n\nWe also visualize SymVIN on larger map sizes: 28×28 and 50×50, to demonstrate its performance and equivariance.\n\nFigure 15: A trained VIN evaluated on a 15 × 15 map and its rotated version. It is obvious that the learned policy is not equivariant under rotation.\n\nFigure 16: A trained SymVIN evaluated on a 15 × 15 map and its rotated version.\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nFigure 17: A fully trained SymVIN evaluated on a 28 × 28 map and its rotated version.\n\nFigure 18: A fully trained SymVIN evaluated on a 50 × 50 map and its rotated version.\n\nH.5 SPL METRIC\n\nWe additionally provide SPL metric of the 2D navigation experiments with different seeds. The trends should be similar to the results in the main paper.\n\nTable 2: Averaged test success rate (%) over 5 seeds for using 10K/2K/2K dataset on 2D navigation.\n\nMethods\n\nVIN SymVIN\n\n15 × 15\n\n28 × 28\n\n50 × 50\n\n69.94±2.92 98.94±0.39\n\n61.43±5.65 97.56±0.71\n\n54.62±9.76 96.27±0.27\n\n96.48±0.65 GPPN ConvGPPN 99.51±0.19 99.68±0.00 SymGPPN\n\n94.35±1.14 96.44±8.56 99.98±0.02\n\n88.25±4.25 96.62±0.38 99.91±0.05\n\nThe performance under SPL metric is pretty similar to success rate. This indicates that most paths are pretty close to the optimal length from all planners. Thus, we decided to not include this to avoid extra concept in experiment section.\n\nH.6 ABLATION STUDY\n\nAdditional training curves. We also provide other training curves that we only show test numbers in the main text.\n\n38\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Averaged test SPL (%) over 5 seeds for using 10K/2K/2K dataset on 2D navigation.\n\nMethods\n\nVIN SymVIN\n\n15 × 15\n\n28 × 28\n\n50 × 50\n\n68.62±3.02 98.08±0.50\n\n60.81±5.73 97.23±0.67\n\n54.14±9.90 96.07±0.27\n\n95.55±0.64 GPPN ConvGPPN 98.73±0.19 98.91±0.01 SymGPPN\n\n94.00±1.17 96.22±8.57 99.76±0.06\n\n87.79±4.30 96.44±0.40 99.82±0.06\n\nFigure 19: (Left) Accuracy evaluated on unseen test maps. The x-axis is the width of the map, and the y-axis is the accuracy, reported on every map size and every size and every chose symmetry group G. (Right) Visual navigation 15 × 15 with 10K data.\n\nTraining efficiency with less data. Since the supervision is still dense, we experiment on training with even smaller dataset to experiment in more extreme setup. We experiment how symmetry may affect the training efficiency of Symmetric Planners by further reducing the size of training dataset. We compare on two environments: 2D navigation and visual navigation, with training/validation/test size of 1K/200/200, for all methods.\n\nChoose of symmetry groups for navigation. One important benefit of partially equivariant network is that, we do not need to design the group representation of MDP action space ρA(g) for different group or action space. Thus, we experiment several G-equivariant variants with different group equivariance: (discrete rotation group) C2, C4, C8, C16, and (dihedral group) D2, D4, D8, all based on E(2)-steerable CNN (Weiler and Cesa, 2021). For all intermediate layers, we use regular representations ρreg(g) of each group, followed by a final policy layer with non-equivariant 1 × 1 convolution.\n\nThe results are reported in the Figure 19 (left). We only compare VIN (denoted as \"none\" symmetry) against our E(2)-VIN (other symmetry group option) on 2D navigation with 15 × 15 maps.\n\nIn general, the planners equipped with any G group equivariance outperform the vanilla nonequivariant VIN, and D4-equivariant steerable CNN performs the best on most map sizes. Additionally, since the environment has actions in 8 directions (4 diagonals), C8 or D8 groups seem to take advantage of that and have slightly higher accuracy on some map sizes, while C16 is overconstrained compared to the true symmetry G = D4 and be detrimental to performance. The non-equivariant VIN also experiences higher variance on large maps.\n\nChoosing fiber representations. As we use steerable convolutions (Weiler and Cesa, 2021) to build symmetric planners, we are free to choose the representations for feature fields, where intermediate equivariant convolutional layers will be equivariant between them f (ρin(g)x) = ρout(g)f (x). We found representations for some feature fields are critical to the performance: mainly V : S → R and Q : S → R|A|.\n\nWe use the best setting as default, and ablate every option. As shown in Table 4, changing ρV or ρQ to trivial representation would result in much worse results.\n\nFully vs. Partially equivariance for symmetric planners. One seemingly minor but critical design choice in our SymPlan networks is the choice of the final policy layer, which maps Q-values S → R|A| to policy logits S → R|A|. Fully equivariant is expected to perform better, but it has some points worth to mention. (1) We experience unstable training at the beginning, where the loss can\n\n39\n\n8.016.024.028.032.036.0imsize0.750.800.850.900.951.00Test/Accuracygroupnonec2d2c4d4c8d8c16020406080100Epochs0.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPNPublished as a conference paper at ICLR 2023\n\nFigure 20: Training curves on (Left) 2D navigation with 10K of 15×15 maps and on (Right) 2DoFs manipulation with 10K of 18 × 18 maps in configuration space. Faded areas indicate standard error.\n\nFigure 21: Training curves for (Left) 28 × 28 and (Right) 50 × 50.\n\nFigure 22: Training curves for 15 × 15 2D navigation 1K data (Left) training and (Right) validation successful rate.\n\nFigure 23: Training curves for 15 × 15 visual navigation 1K data (Left) training and (Right) validation successful rate.\n\nTable 4: Fiber representations\n\n(Fiber representation)\n\nSymVIN\n\nDefault Hidden: trivial to regular State-value ρV : regular to trivial Q-value ρQ: regular to trivial ρQ and ρV : both trivial\n\n98.45 99.07 63.08 21.30 2.814\n\n40\n\n051015202530Epochs0.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN051015202530Epochs0.00.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN051015202530Epochs0.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN051015202530Epochs0.00.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN050100150200250300Epochs0.00.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN050100150200250300Epochs0.00.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN02004006008001000Epochs0.00.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPN02004006008001000Epochs0.00.20.40.60.81.0Successful RatemodelVINSymVINGPPNConvGPPNSymGPPNPublished as a conference paper at ICLR 2023\n\ngo up to 106 in the first epoch, while we did not observe it in non-equivariant or partially equivariant counterparts. However, this only slightly affects training.\n\nIn summary, we found even though fully equivariant version can perform slightly better in the best tuned setting, on average setting, partially equivariant version is more robust and the gap is much larger, as shown in the follow table, which an example of averaging over three choices of representations introduced in the last paragraph. On average partially equivariant version is much better. In our experiments, partially equivariant version also is easier to tune.\n\nTable 5: Fully vs. Partially equivariance\n\nPartially equivariant averaged over all representations Fully equivariant averaged over all representations\n\n91.04 42.61\n\n(Equivariance)\n\nSymVIN\n\nGeneralization additional experiment for fixed K. For fixed K setup in Figure 24 (left), we keep number of iterations to be K = 30 and kernel size F = 3 for all methods.\n\nFigure 24: Results for generalization on larger maps for all methods. (Left) Fixed K = 30 iterations. (Left) Variable K iterations, where K =\n\n2 · M and M is the generalization map size (x-axis).\n\n√\n\nFor SymVIN, it far surpasses VIN for all sizes and preserves the gap throughout the evaluation. Additionally, SymVIN has slightly higher variance across three random seeds (three separately trained models).\n\nAmong GPPN and its variants, SymGPPN significantly outperforms both GPPN and ConvGPPN. Interestingly, ConvGPPN has sharper drop with map size than both SymGPPN and ConvGPPN and thus has increasingly larger gap with SymGPPN and finally even got surpassed by GPPN. Across random seeds, the three trained models of ConvGPPN give unexpectedly high variance compared to GPPN and SymGPPN.\n\n41\n\n20406080100size0.00.20.40.60.81.0avg_successalgorithmVINSymVINGPPNConvGPPNSymGPPN20406080100size0.00.20.40.60.81.0avg_successVINSymVINGPPNConvGPPNSymGPPN",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a novel steerable convolution for 2D path planning, where the convolution operation benefits from incorporating symmetric constraints to reduce overall search space and improve planning performance. Specifically, by manipulating the input signal in a symmetric setting (e.g. rotation and flipping for 2D space) and enforcing consensus outputs, the symmetric convolution can learn to reduce ambiguous prediction and therefore have better potential in both efficiency and accuracy.\n\n# Strength And Weaknesses\n\nPros:\n- This paper provides a simplistic integration using VIN, which is easy to implement. \n- The motivation to use VIN and the effectiveness of symmetric constraint is discussed and proved (only in 2D grid case).\n- The experiment shows substantial improvements over baselines: VIN & GPPN.\n\nAdditionally, although I don't have experience in path & agent planning, I found this paper easy to understand.\n\nCons:\n- Differences should be highlighted compared to E(2)-CNN, as they also proposed a steerable convolution with kernel constraints. Can their steerable convolution be applied to the same task with a little modification?\n- Defining the generalized symmetric operation is challenging. Although the paper demonstrates the 2D grid case where the input signal (\\mathbb{Z}) is rotated 90 deg, extending to 3D or higher dimension might be more difficult. For example, if the Euler angle represents the rotation in 3D, rotating a 3D signal with multiple 90 deg will suffer the gimbal lock.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe overall quality is sound, as the motivation for integrating symmetric constraint into VIN (as a linear operator) is addressed and proved. Moreover, the effectiveness of the proposed module is validated in experiments. The author also provided complete supplementary material to make the paper self-contained, especially the implementation details of pseudo torch code, which I found easy to follow and reproduce.\n\n# Summary Of The Review\n\nI have several questions & concerns regarding the technique.\n1. E(2)-CNN also proposed a transformation invariant representation yield by the steerable convolution. What is the difference compared to them? Can E(2)-CNN apply to the same task with slight modification?\n2. Is the binary entropy loss applied to both raw and symmetric output?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable",
    "# Summary Of The Paper\nThe paper titled \"Integrating Symmetry into Differentiable Planning with Steerable Convolutions\" introduces a novel framework called Symmetric Planning (SymPlan) aimed at enhancing path planning algorithms by integrating group symmetry. The authors propose Symmetric Value Iteration Network (SymVIN) and Symmetric Gated Path Planning Network (SymGPPN), which replace traditional convolutions with steerable convolutions to ensure equivariance under rotations and reflections. The methodology is evaluated across four tasks: 2D navigation, visual navigation, and manipulation tasks in both configuration and workspace. The findings demonstrate significant improvements in training efficiency and generalization performance over existing methods like Value Iteration Networks (VIN) and Gated Path Planning Networks (GPPN).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of symmetry into differentiable planning, which addresses the inefficiencies of existing algorithms that do not exploit task symmetry. The theoretical justification of modeling value iteration as a steerable CNN adds depth to the framework. The empirical results are compelling, showcasing improved performance across various tasks. However, a potential weakness is the reliance on specific assumptions regarding the symmetry of tasks, which may limit the framework's applicability in more complex or asymmetric environments. Additionally, while the experimental validation is strong, further tests in diverse scenarios would strengthen the claims of generalization.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with a logical flow from introduction to methodology and results. The quality of writing is high, making complex concepts accessible. The novelty of integrating symmetry into the planning paradigm through steerable convolutions is significant, as it opens new avenues for research in both planning algorithms and neural network design. The authors have committed to releasing code and detailed implementation instructions, which enhances the reproducibility of the work, although explicit details in the methodology section could be expanded for clarity.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in differentiable planning by effectively leveraging symmetry through a novel framework. The empirical results provide strong evidence of the framework's efficacy, although additional exploration of its applicability to more complex scenarios would be beneficial. The authors’ commitment to reproducibility is commendable.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called Symmetric Planning (SymPlan), which integrates group symmetry into end-to-end differentiable planning algorithms for path planning in grid environments. Utilizing steerable convolution networks, the authors reformulate value iteration as a linear equivariant operator to enhance planning efficiency. The methodology includes the introduction of two new models: the Symmetric Value Iteration Network (SymVIN) and the Symmetric Gated Path Planning Network (SymGPPN), both of which demonstrate significant performance improvements in various tasks such as 2D navigation and configuration space manipulation. The results indicate enhanced training efficiency, generalization to unseen maps, and robustness compared to non-equivariant alternatives.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to incorporating symmetry, which significantly improves the efficiency and performance of planning algorithms. The empirical validation is robust, with performance metrics clearly demonstrating the advantages of SymPlan over traditional methods. However, the paper's limitations lie in its reliance on the assumption of known domain structures, which may restrict its applicability in real-world scenarios where environments are unknown. Additionally, the potential challenges posed by the curse of dimensionality and initial training instability in fully equivariant models introduce concerns regarding scalability and reliability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and experimental results. The quality of the writing is high, with appropriate technical depth suitable for the target audience. The novelty of integrating symmetry into differentiable planning is a significant advancement, although the reproducibility of results may depend on the chosen configurations of convolutional structures, which could require further exploration. Overall, the clarity and quality of the presentation support the novelty and significance of the contributions.\n\n# Summary Of The Review\nThis paper makes a valuable contribution to the field of differentiable planning by effectively integrating symmetry through the SymPlan framework, leading to improved performance and generalization. While the approach is innovative and empirically validated, there are limitations regarding domain assumptions and potential scalability issues that need to be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Symmetric Planning (SymPlan)**, a novel framework that integrates group symmetry into differentiable planning algorithms. By leveraging equivariant convolution networks, the authors propose a method that treats path planning as signals over grids, demonstrating that value iteration can be expressed as a linear equivariant operator equivalent to steerable convolution. The framework is evaluated across four distinct tasks, including 2D navigation and manipulation in configuration and workspace, revealing that SymPlan significantly enhances training efficiency and generalization capabilities compared to existing methods such as Value Iteration Networks (VIN) and Gated Path Planning Networks (GPPN).\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to incorporating symmetry into planning algorithms, which addresses a notable gap in existing literature. The theoretical foundation is robust, with clear proofs establishing the relationship between value iteration and steerable convolutions. However, the paper does not extensively explore the limitations of their approach, such as potential challenges in higher-dimensional or continuous spaces, which may impact generalizability. Additionally, while the experimental results are compelling, the tasks may benefit from further diversity to demonstrate the framework's versatility across additional scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the contributions and findings. The methodology is described with sufficient detail to allow for reproducibility, and the authors have committed to open-sourcing their codebase. The novelty of integrating symmetry into differentiable planning through equivariant networks is significant, marking a meaningful advancement in the field. However, the paper could improve clarity by providing more intuitive explanations of complex concepts such as steerable convolutions and their implications for planning.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of differentiable planning by successfully integrating symmetry into the planning process. The theoretical contributions are strong, and the empirical results demonstrate marked improvements over existing baselines. Minor improvements in clarity and a broader experimental scope would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called SymPlan, which integrates symmetry into differentiable planning algorithms to enhance training efficiency and generalization performance. The authors introduce two specific methods within this framework, SymVIN and SymGPPN, and demonstrate their effectiveness through a series of navigation and manipulation tasks. Empirical results indicate that these methods outperform traditional baselines, supported by theoretical foundations that relate value iteration in path planning to steerable convolutions.\n\n# Strength And Weaknesses\nThe paper exhibits several strengths, including a novel integration of symmetry that leads to improved training efficiency and generalization capabilities. The empirical results provide strong validation against traditional baselines, and the theoretical contributions enhance understanding of the framework's underlying principles. However, the paper also has limitations: it assumes known dynamics, which restricts applicability; utilizes small dataset sizes that may affect generalization; and presents complex theoretical concepts that could hinder accessibility. Additionally, performance variability in some methods and limited exploration of symmetry groups could restrict the robustness and applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, though the complexity of some theoretical concepts may pose challenges for readers unfamiliar with advanced topics. The novelty of the proposed methods is significant, particularly in their application of symmetry to planning algorithms. While the reproducibility of results is facilitated by the comprehensive experimental setup, the small dataset sizes and lack of comparison with state-of-the-art methods raise concerns about the robustness and generalizability of the findings.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field by introducing SymPlan, which effectively leverages symmetry in differentiable planning. While the empirical results are promising, the assumptions made and the limited dataset sizes may impact the broader applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Symmetric Planning (SymPlan), which integrates group symmetry into differentiable planning algorithms using steerable convolution networks. The authors claim that this approach enhances training efficiency and generalization performance in path planning tasks. The methodology includes a theoretical foundation that characterizes value iteration as a type of steerable convolution, accompanied by experimental validation across various tasks demonstrating significant performance improvements over traditional methods such as Value Iteration Networks (VINs) and Gated Path Planning Networks (GPPN).\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its methodological innovation and theoretical contributions. The integration of steerable convolutions into planning frameworks presents a compelling advancement that could influence future research in symmetry-aware planning algorithms. The theoretical insights into the equivariance of value iteration provide a strong foundation for the proposed methodology. However, the paper could improve by offering a more detailed comparison of the computational costs associated with steerable convolutions versus traditional methods. Additionally, while the results support improved generalization, a deeper analysis of the specific conditions leading to these improvements would further strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with the domain. The quality of the work is high, with a solid theoretical underpinning and rigorous experimental validation. The novelty of the approach is significant, as it expands existing methodologies by incorporating symmetry in a way that has not been previously explored. The reproducibility of the results, however, could benefit from more detailed descriptions of the experimental setup and additional benchmarks for comparison.\n\n# Summary Of The Review\nOverall, this paper represents a substantial advancement in differentiable planning by integrating symmetry through steerable convolutions. The authors provide compelling evidence of improvements in efficiency and performance, although further exploration of computational costs and the conditions for generalization would enhance the robustness of their findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel approach to adversarial training in deep reinforcement learning, termed Adversarial Training with Symmetry (ATS). The framework aims to improve the robustness of neural networks against adversarial attacks by integrating symmetry considerations into the training process. The authors present theoretical justifications for their method, demonstrating that expected losses under adversarial perturbations exhibit equivariance under specific transformations. Empirical evaluations across benchmark datasets like MNIST and CIFAR-10 show that models trained with ATS outperform traditional adversarial training methods, indicating enhanced robustness and generalization.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including its innovative approach that leverages symmetry principles to enhance adversarial training, which is a fresh perspective in the field. The theoretical foundations provided strengthen the argument for the proposed method, and empirical results across various datasets reinforce its efficacy. However, the paper also has notable weaknesses. The complexity of implementing the ATS framework could pose challenges for practitioners. Additionally, the experimental scope is somewhat limited, focusing primarily on a few datasets and attack types, which may not fully represent the broader applicability of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, making it accessible for readers. The quality of the theoretical analysis and empirical results is high, providing a solid foundation for the claims made. In terms of novelty, the integration of symmetry into adversarial training is a significant advancement. However, the reproducibility may be hindered by the complexity of the ATS framework and the limited variety of experiments conducted. Future studies should aim to validate these findings in more diverse scenarios.\n\n# Summary Of The Review\nIn summary, this paper presents a compelling approach to adversarial training that integrates symmetry principles, supported by strong theoretical and empirical evidence. While the contributions are notable and potentially impactful, the complexity of implementation and limited experimental scope may present challenges for broader adoption and application.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Symmetric Planning (SymPlan) framework, which aims to integrate group symmetry into differentiable planning algorithms, proposing that this approach can significantly enhance path planning efficiency and generalization across complex tasks. The authors claim that their framework guarantees optimal solutions and presents a novel implementation of value iteration as a steerable convolution. Empirical results suggest that SymPlan outperforms existing methods, although the experiments were conducted in controlled settings, raising questions about the generalizability of these findings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to incorporating symmetry into planning algorithms, potentially offering new insights into computational efficiency. However, the paper tends to overstate its contributions, portraying existing methods as inadequate while ignoring advancements made in the field. Additionally, the empirical results, while promising, are limited in scope and may not accurately reflect performance in more complex real-world scenarios. Furthermore, the methodology lacks depth regarding the limitations of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the exaggerated claims made about the SymPlan framework and its potential impact. While the technical aspects, such as the integration of steerable convolutions, are presented clearly, the discussion often lacks depth and fails to adequately address the limitations of the proposed methods. The novelty of the work is notable, but the reproducibility of the results could be questioned due to the reliance on specific experimental conditions that may not translate to broader applications.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to incorporating symmetry in planning algorithms, but many of its claims are overstated and do not fully acknowledge the contributions of prior research. While it offers novel ideas and promising results, the actual impact on the field may be less significant than suggested.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Symmetric Planning (SymPlan) that incorporates group symmetry into differentiable planning algorithms using steerable convolutions. The methodology demonstrates that value iteration can be viewed as a linear equivariant operator, facilitating enhanced training efficiency and generalization performance in path-planning tasks. The findings reveal significant improvements in success rates across various tasks, including 2D navigation and manipulation tasks, with symmetric planners consistently outperforming their non-equivariant counterparts.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to integrating symmetry into planning algorithms, which is a relatively unexplored area that has clear practical implications for improving algorithm efficiency and generalization. The empirical results are compelling, showcasing substantial performance gains in various path-planning scenarios. However, a potential weakness is the lack of detailed analysis on the limitations or edge cases of the proposed methods, as well as the potential computational overhead that might arise from incorporating symmetry.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and experimental results. The quality of writing is high, making it accessible to readers with a moderate understanding of differentiable planning. The novelty of the approach is significant, as it introduces a framework that leverages symmetries, which are often overlooked in traditional planning methods. The reproducibility of the results is bolstered by the inclusion of comprehensive experimental setups, although additional details on implementation might enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of differentiable planning by effectively leveraging symmetry to enhance performance and generalization. The empirical evaluations strongly support the proposed methodology, although further analysis of potential limitations could improve the understanding of the framework's applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel symmetric planning framework that aims to improve the efficiency and effectiveness of planning algorithms by exploiting group symmetries in state spaces. The authors critique existing planning methods for their reliance on perfect knowledge of system dynamics and propose a methodology that leverages steerable convolutions to enhance planning capabilities. Through empirical evaluations, the paper claims substantial improvements in training efficiency and generalization performance, although these results are primarily demonstrated in synthetic environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to incorporating symmetry into planning algorithms, which could potentially lead to significant advancements in the field. However, the paper exhibits several weaknesses, including an insufficient exploration of the implications of assuming perfect knowledge of dynamics, a lack of empirical validation comparing steerable convolutions with standard convolutions, and a failure to address the limitations of applying the proposed framework to asymmetric problems. Furthermore, the reliance on synthetic environments raises concerns about the framework's applicability to real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. However, the clarity could be improved by addressing the limitations and assumptions of the proposed framework more thoroughly. The novelty of the approach is notable, particularly in its focus on symmetry, but the lack of comprehensive empirical investigations may hinder reproducibility. The failure to consider a broader range of evaluation metrics further complicates the assessment of the framework's overall performance.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to symmetric planning that could advance the field, but it is hindered by significant limitations and assumptions that could undermine its applicability. The lack of empirical validation in non-synthetic environments and the need for a deeper exploration of trade-offs with non-equivariant methods are particularly concerning.\n\n# Correctness\nRating: 3/5. While the theoretical framework appears sound, the assumptions and limitations could lead to potential inaccuracies in practical applications.\n\n# Technical Novelty And Significance\nRating: 4/5. The focus on exploiting symmetry in planning algorithms is a fresh perspective that could offer valuable insights and advancements in the field.\n\n# Empirical Novelty And Significance\nRating: 2/5. The empirical evaluations are limited primarily to synthetic environments, which diminishes the significance of the findings when considering real-world applications. There is also a lack of comparison with alternative methods that could highlight the advantages or limitations of the proposed approach.",
    "# Summary Of The Paper\nThis paper presents a novel framework called Symmetric Planning (SymPlan), which integrates group symmetry into differentiable planning algorithms via steerable convolutions. The authors argue that leveraging symmetry can significantly reduce the complexity and inefficiencies associated with traditional model-based planning algorithms. Their method is theoretically grounded in the treatment of value iteration as a steerable convolution, with specific implementations such as Symmetric Value Iteration Networks (SymVIN) and Symmetric Gated Path Planning Networks (SymGPPN). Experimental results indicate that SymPlan substantially outperforms conventional planning algorithms across various path planning tasks, leading to enhanced training efficiency and better generalization to previously unseen environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative incorporation of symmetry into the planning process, which addresses a notable gap in existing literature. The theoretical justification provided for the framework is robust, establishing a solid foundation for its applicability. The empirical results are compelling, demonstrating clear advantages over traditional methods. However, the paper could benefit from more extensive experiments across a broader range of tasks to further validate the generalizability of the approach. Additionally, while the contributions are substantial, the complexity of the framework may pose a barrier to practical implementation in some scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with varying levels of familiarity with the topic. The quality of the methodology is high, with detailed explanations of the SymPlan framework and its components. The novelty of the approach is significant, particularly in its theoretical contributions and practical implications for path planning. The authors have committed to providing an open-source codebase and additional details to ensure reproducibility of their experiments, which is commendable.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling and novel framework that effectively leverages symmetry in differentiable planning. The theoretical and empirical contributions are strong, though the complexity of implementation may pose challenges in practical applications. Additional experimentation across a wider set of tasks could further strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel algorithm for enhancing the performance of deep learning models in low-data scenarios, leveraging transfer learning from related tasks. The authors propose a framework that integrates a meta-learning approach to select the most relevant source tasks for transfer, thereby improving model robustness and accuracy. Empirical results demonstrate significant improvements over baseline methods in various low-data benchmarks, indicating the framework's effectiveness in practical applications.\n\n# Strength And Weaknesses\n## Strengths\n- **Innovative Use of Transfer Learning**: The integration of meta-learning techniques to optimize source task selection is a significant advancement that differentiates this work from existing literature.\n- **Empirical Validation**: The thorough experimental setup and results provide strong evidence for the proposed framework’s effectiveness, showcasing improvements in accuracy and robustness.\n- **Well-Structured Presentation**: The paper is clearly organized, guiding the reader through the motivation, methodology, and results seamlessly.\n\n## Weaknesses\n- **Limited Diversity in Datasets**: The experiments primarily focus on a narrow set of datasets, which may limit the generalizability of the findings across diverse real-world scenarios.\n- **Comparative Analysis**: While the paper provides empirical results, it lacks a detailed comparative analysis against a broader spectrum of existing methods, which could strengthen the claims of superiority.\n- **Potential Overfitting**: There is a concern about the risk of overfitting to the selected source tasks, which the authors do not adequately address in the discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and effectively communicates its ideas. The methodology is presented clearly, allowing for reproducibility, although some additional details on implementation choices would enhance this aspect. The novelty lies primarily in the combination of meta-learning and transfer learning; however, the significance of the contribution would be bolstered by a more extensive evaluation against a variety of baseline methods.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to improving deep learning performance in low-data settings, offering innovative methodologies and strong empirical results. However, it would benefit from a broader experimental scope and a more detailed comparative analysis to substantiate its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces the Symmetric Planning (SymPlan) framework, which effectively integrates group symmetry into differentiable planning algorithms using steerable convolution networks. The authors demonstrate that value iteration can be interpreted as a linear equivariant operator, which enhances both training efficiency and generalization across various path-planning tasks, including 2D navigation and manipulation. Experimental results show that the proposed approach significantly outperforms non-equivariant baselines, indicating its effectiveness in leveraging symmetry for improved planning performance.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to incorporating symmetry into planning algorithms, addressing a notable gap in existing methodologies that often struggle with explicit equivalence class construction. The theoretical connection established between value iteration and steerable CNNs provides a solid foundation for the proposed framework. However, a potential weakness is the paper's focus on 2D environments, which may limit the immediate applicability of the findings to higher-dimensional or continuous spaces. Further empirical validation in these contexts would strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of the writing is high, making complex ideas accessible. The novelty of integrating symmetry into differentiable planning is significant, presenting a fresh perspective in this area. Reproducibility appears to be feasible, as the framework is built on established principles of equivariant networks, and the experimental setup is sufficiently detailed for replication.\n\n# Summary Of The Review\nOverall, this paper makes a meaningful contribution to the field of differentiable planning by presenting a novel framework that leverages symmetry through steerable convolutions. Its empirical results demonstrate significant advantages over traditional methods, although further exploration into higher-dimensional applications would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called Symmetric Planning (SymPlan) that integrates group symmetry into differentiable path planning algorithms using steerable convolutions. The authors propose two key methods: Symmetric Value Iteration Network (SymVIN) and Symmetric Gated Path Planning Networks (SymGPPN). Through extensive experiments across various tasks, the paper demonstrates that SymPlan significantly enhances training efficiency and generalization compared to traditional non-equivariant methods.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to leveraging symmetry in planning algorithms, which has the potential to reduce the computational complexity inherent in path-planning problems. The theoretical foundation provided for the use of steerable convolutions is robust and well-articulated. However, a notable weakness is the limited scope of tasks evaluated, which could affect the generalizability of the findings. The paper could also benefit from a more thorough comparison with a broader range of existing methods to validate its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the complex ideas accessible to the reader. The quality of the methodology and theoretical contributions is high, with clear explanations of the proposed framework. The novelty of the approach is significant as it introduces a new perspective on incorporating symmetry into differentiable planning. The authors also address reproducibility concerns by committing to open-source their codebase and providing detailed appendices for implementation, which enhances the paper's overall quality.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of differentiable planning by effectively integrating symmetry through its SymPlan framework. The contributions are theoretically sound and empirically validated, although the scope of experiments could be expanded to strengthen the findings. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework named SymPlan that integrates group symmetry into differentiable planning algorithms through the use of steerable convolutions. The authors argue that their approach improves both training efficiency and generalization capabilities compared to existing planning models. SymPlan leverages equivariant convolution networks to formulate path planning as a value iteration process, with empirical evaluations conducted on a variety of tasks including 2D navigation and 2-DOF manipulation. The results indicate that SymPlan achieves higher success rates and better generalization to unseen environments than baseline algorithms such as Value Iteration Networks (VIN) and Group Planning with Policy Networks (GPPN).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of symmetry into differentiable planning, which presents a fresh perspective on improving planning algorithms. The empirical results substantiate the claims made regarding training efficiency and generalization, showcasing the method's effectiveness across multiple tasks. However, the paper does acknowledge limitations, including assumptions regarding known domain structures and potential challenges related to scalability. Furthermore, while the methodology is theoretically sound, a more detailed examination of the specific implementation of steerable convolutions could enhance the clarity and depth of the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, and the clarity is commendable, making the contributions easily digestible for readers. The novelty of integrating symmetry into planning algorithms is a significant contribution to the field. The reproducibility aspects are adequately addressed through the promise of open-sourcing code and including supplementary details in the appendix, which enhances the overall quality and trustworthiness of the presented research.\n\n# Summary Of The Review\nOverall, the paper provides a compelling contribution to the field of differentiable planning by successfully incorporating symmetry through steerable convolutions. The empirical results validate the effectiveness of the proposed framework, making it a notable advancement in planning algorithms. However, some limitations regarding scalability and assumptions about domain structures warrant further discussion.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel framework for symmetric planning through the use of steerable convolutions, aimed at improving the efficiency of end-to-end differentiable planning algorithms in high-dimensional path-planning problems. The authors build upon the mathematical foundations of Markov Decision Processes (MDPs) and demonstrate how group symmetry can be exploited to enhance computational efficiency. They propose the Symmetric Value Iteration Network (SymVIN), which utilizes steerable convolutions to achieve equivariant outputs across transformations, and validate their approach through extensive experiments in various task domains, showing significant improvements in generalization capabilities and training efficiency.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative approach to incorporating group symmetry into planning algorithms, which is a significant advancement in the field. The use of steerable convolutions within the SymVIN architecture is a novel contribution that provides a solid theoretical foundation and practical implementation. However, the paper could benefit from clearer explanations of complex concepts and a more detailed discussion of potential limitations or challenges in applying the proposed framework to broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents a coherent narrative. The mathematical formalism is rigorous, and the proofs supporting equivariance are a strong aspect of the work. However, certain sections, particularly those detailing the implementation and experimental setup, could be elaborated to enhance clarity and reproducibility. The novelty of the approach is notable, but the paper would benefit from a more comprehensive exploration of related work to contextualize its contributions.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the application of symmetry in planning algorithms through the introduction of steerable convolutions within a new framework. While the methodology and findings are compelling, enhanced clarity and a deeper discussion of limitations would strengthen the work further.\n\n# Correctness\nRating: 5/5  \nThe mathematical formulations and proofs presented in the paper are sound and accurately support the claims made regarding the framework's efficacy.\n\n# Technical Novelty And Significance\nRating: 4/5  \nThe incorporation of steerable convolutions into a planning context represents a strong technical innovation, though the foundational ideas may draw on existing literature.\n\n# Empirical Novelty And Significance\nRating: 4/5  \nThe empirical evaluation demonstrates significant improvements over traditional methods in several task domains, indicating a meaningful contribution to the field. However, further exploration of additional scenarios could amplify its impact.",
    "# Summary Of The Paper\nThe paper investigates the integration of symmetry into differentiable planning through the use of steerable convolutions. The authors claim that incorporating symmetry can enhance training efficiency and generalization performance in path planning tasks. The methodology involves leveraging established frameworks such as equivariant networks and Value Iteration Networks. Experimental results suggest improvements in specific tasks, primarily focused on 2D path planning scenarios.\n\n# Strength And Weaknesses\nWhile the paper presents a structured approach to incorporating symmetry into planning, its contributions appear to lack originality, primarily relying on existing theories without providing substantial new insights. The experimental results are not robust due to limited statistical analysis and a narrow range of tasks, which raises concerns about the generalizability of the findings. Additionally, the authors do not convincingly address the limitations of existing planning algorithms nor provide compelling evidence for the superiority of their method. The potential challenges related to training stability and computational efficiency are also inadequately explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation. However, the novelty of the proposed approach is questionable, as it seems to reiterate established concepts without significant advancements. The reproducibility of the results is also hindered by the limited experimental setup and the absence of comprehensive benchmarking against a wider array of methods.\n\n# Summary Of The Review\nOverall, the paper presents a method for integrating symmetry into differentiable planning, but it falls short in terms of originality, depth of analysis, and empirical validation. The contributions feel incremental rather than groundbreaking, and the experimental scope is insufficient to support the claims made by the authors.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Symmetric Planning (SymPlan), which integrates group symmetry into differentiable planning algorithms using steerable convolutions. The authors demonstrate that this approach significantly enhances training efficiency, allowing models to learn faster and generalize effectively across various tasks. Comprehensive experiments reveal that SymPlan outperforms traditional methods like Value Iteration Networks (VINs) and Gated Path Planning Networks (GPPN) in terms of generalization performance on unseen maps, highlighting its versatility for applications in robotics and AI.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to incorporating symmetry into differentiable planning, which leads to improved efficiency and generalization capabilities. The experimental results are robust, showcasing state-of-the-art performance across diverse navigation and manipulation tasks. However, one potential weakness is the limited discussion on the scalability of the method to even more complex environments and higher-dimensional spaces, which could be a consideration for future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the writing is high, with a logical flow that makes the complex concepts accessible. The novelty of integrating symmetry into planning algorithms is significant, and the empirical results are well-documented, supporting reproducibility. However, further details on implementation specifics and how to replicate the experiments would enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a groundbreaking approach to differentiable planning by incorporating symmetry, leading to enhanced efficiency and generalization. The experimental validation demonstrates the practical applicability of the proposed methods, marking a significant advancement in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the Symmetric Planning (SymPlan) framework, which integrates principles of symmetry and equivariance into differentiable planning algorithms. It leverages group theory and equivariant convolutional networks to enhance the efficiency of planning tasks by reducing the search space complexity. The findings suggest that by operationalizing symmetry within Markov Decision Processes (MDPs), the computational load of planning is simplified, allowing for more effective problem-solving through the aggregation of states into equivalence classes. The framework also presents theoretical justifications for its approach, demonstrating how steerable CNNs can be utilized to respect the inherent symmetries of planning environments.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical foundation, which is supported by multiple theorems validating the framework's claims of equivariance and efficiency. The innovative approach of applying group theory to planning tasks is a significant contribution, potentially leading to broader applications in various domains. However, a notable weakness is the limited empirical validation of the proposed methods, as the focus primarily lies on theoretical insights without extensive experimental results to demonstrate real-world applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates complex theoretical concepts clearly, making it accessible to readers with a background in machine learning and mathematics. The quality of writing is high, with a coherent flow of ideas. The novelty is substantial, as the integration of symmetry into differentiable planning is relatively unexplored territory. However, the reproducibility of the claims may be hindered by the lack of detailed experimental setups or benchmarks that could allow others to validate the theoretical findings empirically.\n\n# Summary Of The Review\nOverall, the paper presents a strong theoretical advancement in differentiable planning through the introduction of the SymPlan framework, highlighting the implications of symmetry and equivariance. While the theoretical insights are compelling, the lack of empirical validation limits the practical applicability of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Integrating Symmetry into Differentiable Planning with Steerable Convolutions\" presents the Symmetric Planning (SymPlan) framework, which integrates group symmetry into differentiable planning algorithms. The authors propose new implementations of Value Iteration Networks (VIN) and Gated Path Planning Networks (GPPN) using steerable convolutions, facilitated by the e2cnn library. Key contributions include the adaptation of standard convolutional layers to utilize steerable convolutions, the establishment of specific configurations for these layers, and the development of mapper networks for tasks such as visual navigation and workspace manipulation. The experiments demonstrate that SymPlan significantly improves training efficiency and generalization over traditional methods, although the paper offers limited discussion on theoretical implications.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its novel integration of symmetry into planning frameworks, which enhances performance metrics and training efficiency. The use of steerable convolutions is a notable advancement, providing a fresh perspective on traditional convolutional approaches. The ablation studies presented effectively illustrate the impact of various representations and equivariance settings, lending credibility to the findings. However, weaknesses include a lack of theoretical depth regarding the implications of symmetry in planning and a limited exploration of the broader applicability of the framework. Additionally, while the empirical results are promising, the paper could benefit from more extensive experimentation across diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a structured presentation of methodology and results. The quality of the experiments is commendable, but the novelty, while present in the integration of symmetry, is somewhat diminished by the reliance on established frameworks like VIN and GPPN. Reproducibility is adequately supported through detailed descriptions of the training setup and specific configurations used, although the dependency on the e2cnn library may pose challenges for some researchers in replicating the work without access to this resource.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in differentiable planning by incorporating symmetry through steerable convolutions, leading to improved performance metrics. While the empirical results are compelling and well-supported, the work could benefit from deeper theoretical insights and broader explorations of its applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a new method called SymPlan, which aims to enhance planning efficiency in environments characterized by symmetry. The authors claim that SymPlan outperforms previous approaches, particularly Value Iteration Networks (VIN), by utilizing equivariant structures to incorporate symmetry into the planning process. The methodology involves implementing mapper networks for visual navigation and emphasizing training efficiency improvements. However, the paper's findings are based on a limited set of experiments that showcase performance metrics without providing comprehensive comparisons to other state-of-the-art methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper are its innovative approach to integrating symmetry through equivariance and the introduction of mapper networks for navigation tasks. However, there are significant weaknesses, including a lack of rigorous comparisons to existing methods like VIN, insufficient exploration of the broader applicability of the results, and an overreliance on claims of efficiency without substantial evidence. The theoretical contributions appear to be overshadowed by previous research, and the paper contains inconsistencies in terminology that may confuse readers regarding established definitions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, the clarity suffers due to inconsistent terminology and overlapping definitions related to symmetry and equivariance. The novelty of the approach is undermined by the existence of similar methods in the literature, which the authors do not adequately differentiate from their own work. Reproducibility may be a concern due to the limited scope of experiments, as the findings are not thoroughly benchmarked against a wider array of existing methodologies.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting concept in planning with symmetry, but it falls short in providing robust evidence of its superiority over existing methods. The claims of improved efficiency and novel contributions are not sufficiently supported by comprehensive comparisons or experimental validation across diverse scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# SUMMARY OF THE PAPER\nThe paper titled \"INTEGRATING SYMMETRY INTO DIFFERENTIABLE PLANNING WITH STEERABLE CONVOLUTIONS\" presents a novel framework called Symmetric Planning (SymPlan) that incorporates group symmetry into differentiable planning. The authors propose a methodology that leverages steerable convolutions to enhance the efficiency and effectiveness of planning algorithms by reducing the search space. Their findings indicate that SymPlan significantly improves long-horizon planning capabilities while effectively managing pixel-level symmetries in various planning tasks.\n\n# STRENGTH AND WEAKNESSES\nThe strengths of the paper lie in its innovative approach to integrating symmetry into planning, which addresses a notable gap in existing methodologies. The use of steerable convolutions is particularly commendable, as it allows for efficient representation of symmetries. However, the paper has weaknesses related to clarity and readability, including inconsistent terminology, formatting issues, and some lack of detail in explanations that could hinder understanding for readers unfamiliar with the underlying concepts.\n\n# CLARITY, QUALITY, NOVELTY AND REPRODUCIBILITY\nWhile the paper presents a novel contribution to the field of planning, several clarity issues detract from its overall quality. Inconsistent formatting, grammatical errors, and ambiguous phrasing make it challenging to follow some sections. Moreover, the reproducibility of the experiments is not clearly established, as the statistical methods employed in analyzing results are not explicitly detailed. The authors should improve clarity and consistency to enhance the paper's accessibility.\n\n# SUMMARY OF THE REVIEW\nOverall, the paper makes a significant contribution to the field by proposing a novel framework that integrates symmetry into differentiable planning. However, clarity and consistency issues need to be addressed to facilitate understanding and reproducibility.\n\n# CORRECTNESS\n4/5  \nThe core methodologies and findings appear to be correct, but minor errors and unclear statements could lead to misunderstandings.\n\n# TECHNICAL NOVELTY AND SIGNIFICANCE\n4/5  \nThe introduction of SymPlan and the application of steerable convolutions represent a notable advancement in the planning domain, contributing to long-horizon planning capabilities.\n\n# EMPIRICAL NOVELTY AND SIGNIFICANCE\n3/5  \nWhile the empirical results are promising, the lack of detailed statistical analysis and comparison to existing methods limits the overall impact of the findings. Further empirical validation and clarity in presentation would bolster the significance of the results.",
    "# Summary Of The Paper\nThe paper presents a framework focused on symmetry in 2D path planning, proposing methods that potentially generalize to continuous Euclidean spaces. The authors explore the implications of symmetry in the context of model-free deep reinforcement learning and discuss the limitations of their approach. However, they provide limited empirical validation and do not thoroughly investigate the integration of their framework with existing sampling-based planning algorithms or higher-dimensional planning problems.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to incorporating symmetry into path planning, which could have implications for improving planning efficiency. However, the paper exhibits several weaknesses, including a lack of concrete examples or experimental validation for its proposed generalizations, minimal exploration of alternative methodologies, and insufficient discussion of the practical implications of symmetry in various contexts. The limited exploration of the curse of dimensionality and multi-agent scenarios also detracts from its overall contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the concepts are presented systematically, the lack of detailed examples and empirical results diminishes the overall quality. The novelty of the approach is evident, particularly in its focus on symmetry, but the significance could be enhanced with further exploration and validation. Reproducibility is a concern, as the absence of thorough ablation studies and the limited empirical results hinder the ability of others to replicate or build upon the findings.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting perspective on symmetry in path planning but falls short in providing sufficient empirical support and broader applicability. The insights offered could be valuable for future research, but the lack of detailed exploration and validation limits the impact of the current work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel framework named Symmetric Planning (SymPlan) that integrates group symmetry into path-planning algorithms, with a focus on differentiable planning. The methodology involves evaluating SymPlan against non-equivariant baselines, specifically Value Iteration Networks (VIN) and Gated Path Planning Networks (GPPN), across four distinct path-planning tasks: 2D navigation, visual navigation, 2-DOF configuration space manipulation, and 2-DOF workspace manipulation. The findings indicate that SymPlan significantly enhances training efficiency and generalization performance, evidenced by high success rates, particularly in larger map sizes.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to incorporating symmetry into path-planning, which appears to yield substantial performance improvements over traditional methods. The rigorous experimental design, including multiple tasks and statistical evaluations of success rates, reinforces the validity of the findings. However, a notable weakness is the lack of explicit statistical significance testing, such as p-values or confidence intervals, which could enhance the robustness of the claims regarding the improvements in performance and training efficiency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and results clearly, making it accessible to readers. The quality of the experiments is high, with comprehensive evaluations across various tasks and configurations. The novel integration of symmetry in planning represents a significant contribution to the field, although the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and parameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a compelling framework that demonstrates significant improvements in path-planning tasks by leveraging symmetry. While the contributions are notable and the methodology is sound, the absence of explicit statistical significance measures detracts slightly from the overall impact of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper presents a novel framework for symmetric planning in 2D grid environments, introducing two main methodologies: SymVIN (Symmetric Value Iteration Network) and SymGPPN (Symmetric Grid Path Planning Network). The authors explore how leveraging symmetry can improve path planning efficiency and generalization in known environments. They demonstrate that their methods outperform traditional planning approaches in terms of learning speed and performance on symmetric maps, while also acknowledging the limitations of their approach in dealing with unknown dynamics and higher-dimensional spaces.\n\n# Strength And Weaknesses\nThe paper contributes valuable insights into the role of symmetry in planning tasks, particularly in 2D settings, and provides a theoretical foundation for the proposed methodologies. The experiments show promising results regarding improved generalization on unseen maps, highlighting the potential for symmetry to enhance learning efficiency. However, the assumptions regarding known domain structures significantly limit the applicability of the findings. Additionally, the reliance on specific neural network architectures raises concerns about the generalizability of the proposed methods to other contexts. The lack of empirical validation in complex environments and the limited exploration of different symmetry types further diminish the robustness of the conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodologies and findings clearly, which facilitates understanding. The quality of the writing is high, and the experiments are clearly documented. However, the novelty of the approach, while significant within its specific context, may not extend well to broader planning scenarios due to its constraints. The reproducibility of the results is also questionable, given the dependency on specific network architectures and the unexplored integration with other planning paradigms.\n\n# Summary Of The Review\nOverall, the paper presents a thoughtful exploration of symmetric planning in 2D grid environments, demonstrating potential benefits in efficiency and generalization. Despite its contributions, the reliance on known domain structures and specific neural architectures limits its broader applicability and raises questions regarding the robustness of the proposed methods in complex settings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Integrating Symmetry into Differentiable Planning with Steerable Convolutions\" presents a new framework called SymPlan, which aims to incorporate group symmetry into model-based planning. The authors propose the use of steerable convolutions as a means to enhance training efficiency and generalization performance in planning tasks. Their methodology includes evaluating the performance of SymPlan against traditional value iteration methods on four tasks, suggesting that their framework provides significant improvements in success rates, although the novelty of the approach appears limited.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to systematically integrate symmetry into the planning process, potentially offering a more efficient search space. However, the lack of novelty in their approach—relying on well-established concepts such as Markov Decision Processes (MDPs) and Value Iteration Networks (VINs)—detracts from its impact. The experiments, while showing improvements, rely on relatively small datasets, which raises questions about the robustness and generalizability of the findings. Additionally, the paper does not sufficiently address existing literature on symmetry in planning, resulting in a missed opportunity to contextualize their contributions within the broader field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is adequate, but the presentation lacks the rigor expected for a paper claiming novel contributions. The paper is well-structured, yet the repetitiveness of ideas diminishes its overall quality. The novelty of the work is questionable, as it does not provide substantial advancements over existing methods or insights. Reproducibility is not thoroughly addressed, as details on the experimental setup and datasets used are limited, making it challenging for other researchers to replicate the findings confidently.\n\n# Summary Of The Review\nOverall, the paper presents an attempt to integrate symmetry into planning but does not sufficiently innovate or advance the field. While the authors demonstrate a grasp of the underlying concepts, the contributions feel derivative and lack the depth necessary to meaningfully impact ongoing research in this area.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents an innovative approach to differentiable planning by integrating symmetry into the planning process through the use of steerable convolutions. The authors demonstrate how this method improves training efficiency and generalization performance in 2D path planning tasks, outperforming traditional Value Iteration Networks (VINs) and Generalized Planning Policy Networks (GPPNs). The methodology focuses on the importance of equivariance during the value iteration process and discusses potential applications for scaling to larger map sizes and handling complex environments.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its novel integration of symmetry into planning frameworks and the empirical evidence supporting its effectiveness over existing methods. However, the paper could benefit from a deeper exploration of hybrid models that incorporate reinforcement learning, which might enhance decision-making in dynamic environments. Additionally, while the authors address the limitations of assuming known domain structure, future work could further investigate the use of unsupervised or self-supervised learning to adapt to evolving tasks more effectively. The analysis of computational costs and trade-offs in equivariant networks also requires more thorough examination for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, making it accessible to the reader. The quality of the methodology and empirical results is high; however, the reproducibility could be improved by providing more detailed descriptions of the experimental setup and the ablation studies necessary to understand the stability and convergence impact of various group representations. The novelty of the approach is significant, particularly in how it addresses symmetry in planning, yet a theoretical framework elucidating the limitations of symmetry could further enhance the paper's contributions.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the field of differentiable planning by integrating symmetry through steerable convolutions, demonstrating improved performance in path planning tasks. While the empirical results are compelling, the exploration of hybrid models and scalability to complex environments remains an area for future research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Symmetric Planning (SymPlan) framework, which integrates symmetry into planning tasks to enhance performance and training efficiency. The authors evaluate SymPlan on four distinct tasks: 2D navigation, visual navigation, and both 2-DOF configuration and workspace manipulation. The findings reveal that the SymVIN and SymGPPN models significantly outperform their non-equivariant counterparts (VIN and GPPN) in terms of success rates and training efficiency, demonstrating faster convergence and better generalization to unseen maps.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous benchmarking across multiple tasks, providing comprehensive empirical evidence of the advantages of symmetry integration in planning. The success rates achieved by SymPlan models are impressive, showcasing substantial improvements over traditional methods. Additionally, the inclusion of ablation studies strengthens the claims by demonstrating the robustness of the proposed methods under various conditions. However, a potential weakness is the lack of exploration of the limitations of the symmetry approach, as well as the potential computational overhead associated with integrating symmetry in more complex planning scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, results, and implications of the findings. The quality of the experiments is high, with statistical significance reported alongside performance metrics. The novelty of incorporating symmetry into planning tasks represents a meaningful contribution to the field. The reproducibility of the results could be enhanced with more detailed descriptions of the experimental setups and the datasets used, although the results themselves appear robust.\n\n# Summary Of The Review\nOverall, the paper presents a compelling argument for the incorporation of symmetry in planning tasks, backed by thorough empirical evaluation and analysis. The significant improvements in success rates and training efficiency indicate that the SymPlan framework could be a valuable advancement in path planning methodologies.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to integrating symmetry into neural network architectures through the use of steerable convolutions. The authors propose a methodology that leverages symmetry to enhance performance in tasks that involve spatial transformations. Through a series of experiments, they demonstrate that their approach outperforms traditional convolutional networks on benchmark datasets, showcasing improved robustness and generalization capabilities.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of symmetry into neural networks, offering a fresh perspective on convolutional architecture design. The methodology is well-conceived, and the experimental results are compelling, providing solid evidence for the approach's effectiveness. However, the paper suffers from clarity issues, particularly in the introduction and methodology sections, where dense language and technical jargon may hinder understanding for readers less familiar with the topic. Additionally, the organization of the related work and experimental sections could be improved to facilitate better comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel ideas, the clarity of its presentation detracts from its overall quality. The dense abstract and lengthy introduction could benefit from more structured formatting, and the use of jargon limits accessibility. The methodology, although technically sound, is overwhelming and lacks a straightforward outline. Reproducibility is somewhat assured through detailed experimental descriptions, but clearer exposition of the methodology would enhance this aspect further.\n\n# Summary Of The Review\nThe paper contributes significantly to the field by proposing a symmetry-based approach to neural networks that enhances performance on spatial transformation tasks. However, the presentation suffers from clarity and organization issues, which may impede reader understanding. Addressing these aspects would improve the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.788163286434278,
    -1.6617790054153083,
    -1.8669390775378145,
    -1.799807573387126,
    -1.9574451678479299,
    -1.778301752866398,
    -1.5229493312290574,
    -1.940103390314124,
    -1.754784885010922,
    -1.6232957521103124,
    -1.788480440751446,
    -1.500451977804276,
    -1.7688491598655482,
    -1.660056928718098,
    -1.8931067384688895,
    -1.66319379819035,
    -1.8662807109107278,
    -1.7000599460116328,
    -1.8443240792730131,
    -1.6178817768840252,
    -1.9329511144901457,
    -1.5937063937429143,
    -1.8644598196314335,
    -1.700171510621553,
    -1.7632267909249582,
    -1.8362804676063627,
    -1.9945992855021504,
    -1.7836983592991245,
    -1.6977735183834968
  ],
  "logp_cond": [
    [
      0.0,
      -2.5376575671237847,
      -2.54176325855089,
      -2.5157181180939125,
      -2.5562562258095998,
      -2.52620576116958,
      -2.5985576977740807,
      -2.5529026681920377,
      -2.5350661458462778,
      -2.560645631914515,
      -2.5402965989012127,
      -2.606219800913653,
      -2.5281919662301258,
      -2.540602176332291,
      -2.5401517365882453,
      -2.5417554420421835,
      -2.515928865941552,
      -2.536449285180667,
      -2.5902882435176227,
      -2.55244261238138,
      -2.5847975093075015,
      -2.570222573382669,
      -2.5568189184347028,
      -2.557758395406127,
      -2.577997961210058,
      -2.560635703928818,
      -2.5116658509242757,
      -2.5579179286407765,
      -2.5644064890127063
    ],
    [
      -1.3585374841016225,
      0.0,
      -1.1881418006179363,
      -1.1194143961671805,
      -1.2423995659662614,
      -1.1814001888274568,
      -1.3949458144422116,
      -1.247302011101432,
      -1.2117077595941492,
      -1.3715502224691303,
      -1.1330870193793572,
      -1.4317635410743532,
      -1.22152853905175,
      -1.1639745636412413,
      -1.2039734569641665,
      -1.3150718062201163,
      -1.2769992860064516,
      -1.2345321395187738,
      -1.3335177233646858,
      -1.2366136441273894,
      -1.3363175355343737,
      -1.3508480725494187,
      -1.3775327139117635,
      -1.193508459664281,
      -1.3799197510396044,
      -1.278357653682058,
      -1.3000221476217475,
      -1.244980260477063,
      -1.3552126353075657
    ],
    [
      -1.4863059428489362,
      -1.2977963821812457,
      0.0,
      -1.231594903917027,
      -1.3027819978109694,
      -1.295598593634424,
      -1.5245199853527238,
      -1.356590221362908,
      -1.27518796433213,
      -1.4329799758164035,
      -1.3022629540280586,
      -1.5714122911163477,
      -1.2668672448596145,
      -1.2915465320346982,
      -1.3185162488406141,
      -1.4148922309336165,
      -1.3833008605986399,
      -1.3312412142764347,
      -1.4281734842392315,
      -1.326012879903993,
      -1.4518829613739292,
      -1.4547647138026911,
      -1.4912817682816275,
      -1.3384280939952535,
      -1.38011206464639,
      -1.4463276776015241,
      -1.3194212770673095,
      -1.3606002863878317,
      -1.5023443161008165
    ],
    [
      -1.4604552946587808,
      -1.1931218389821452,
      -1.2601914737372746,
      0.0,
      -1.3266689809779535,
      -1.2566394034114132,
      -1.4933720768548484,
      -1.3477935130509218,
      -1.286659610140954,
      -1.4552994213015886,
      -1.2173310567426587,
      -1.5137497406592668,
      -1.2209331107716273,
      -1.250220802207726,
      -1.2747866487572188,
      -1.3499689383931515,
      -1.3050920682570766,
      -1.3256084995772033,
      -1.4103920090845696,
      -1.3269615933043093,
      -1.442544954237435,
      -1.4538701974305843,
      -1.473873497553353,
      -1.3165787990584465,
      -1.4659772800391457,
      -1.4127828224800476,
      -1.3609469166531787,
      -1.3803497437664702,
      -1.4871148553110802
    ],
    [
      -1.5469036781356322,
      -1.3929620040072281,
      -1.387799153738856,
      -1.379147102818157,
      0.0,
      -1.3802202305083981,
      -1.6255058063559102,
      -1.4563082837215031,
      -1.3995337711416287,
      -1.5378943734221493,
      -1.3725956300551727,
      -1.6391227321881394,
      -1.4252343230855353,
      -1.4442501083399446,
      -1.386410430227049,
      -1.5731089414262749,
      -1.4547512564391498,
      -1.410462435450787,
      -1.5588285731776919,
      -1.4530637348670095,
      -1.5311037192103032,
      -1.5907253363203895,
      -1.5767752823305812,
      -1.5063754841194845,
      -1.5042038183834476,
      -1.475785411031835,
      -1.4497715920088188,
      -1.543081898832436,
      -1.605882075433169
    ],
    [
      -1.4302463568715424,
      -1.1901667260901083,
      -1.192407899386483,
      -1.2152549299779758,
      -1.260235578615503,
      0.0,
      -1.4430623340046498,
      -1.2497368114103637,
      -1.245321814458819,
      -1.35998430428216,
      -1.2506469278961352,
      -1.502860640028834,
      -1.2366029931115037,
      -1.2748570318607775,
      -1.2569261594892032,
      -1.3023047659356095,
      -1.245826182315553,
      -1.2243146695653755,
      -1.3943400268486095,
      -1.2630366987892025,
      -1.3518992260355032,
      -1.396215338888106,
      -1.4233846305755946,
      -1.298792546397576,
      -1.367767564135681,
      -1.317254649359999,
      -1.2349696166828132,
      -1.3765487104210197,
      -1.414297978972703
    ],
    [
      -1.2905497629131657,
      -1.2302734026212672,
      -1.1966306791350545,
      -1.2233509460050263,
      -1.1828485515099332,
      -1.2267724213779432,
      0.0,
      -1.195657262863069,
      -1.2320614525016218,
      -1.2316972035367,
      -1.2348950700617916,
      -1.2380729202613947,
      -1.2186670438397484,
      -1.2483636898189632,
      -1.1947537160398338,
      -1.2571663750279571,
      -1.1890784218883357,
      -1.251510805505769,
      -1.1975014110401168,
      -1.235762257165948,
      -1.1858782996153847,
      -1.2783496209913054,
      -1.2092292241819695,
      -1.2404298319159,
      -1.2038064204954981,
      -1.2487715911847037,
      -1.2027211897242192,
      -1.2311625865819005,
      -1.2191358638593093
    ],
    [
      -1.55914995091156,
      -1.3969018419189567,
      -1.36462495704092,
      -1.3624234066632606,
      -1.4139775875043903,
      -1.3822858021331934,
      -1.5799219386815735,
      0.0,
      -1.3955271170827859,
      -1.4997398276721359,
      -1.3471909869331182,
      -1.6431452054244913,
      -1.3821894332808407,
      -1.4399221921994045,
      -1.4327530470909802,
      -1.4875199665559538,
      -1.4843522078394331,
      -1.4131125729899838,
      -1.4935706407716673,
      -1.4400084945716565,
      -1.5472561619242848,
      -1.5442676312695909,
      -1.549222778676746,
      -1.4832501079317681,
      -1.5257440884449434,
      -1.5143167334372465,
      -1.4659006135844315,
      -1.4936364831767688,
      -1.5666845822461966
    ],
    [
      -1.3683724954254548,
      -1.1836942398665546,
      -1.1570981691423927,
      -1.1485810344864562,
      -1.221545258323803,
      -1.176440779041373,
      -1.4168758375342614,
      -1.2431675295958704,
      0.0,
      -1.357182843956521,
      -1.2215410425185178,
      -1.4334668779680106,
      -1.0952359049839788,
      -1.2270035916604434,
      -1.21572646332273,
      -1.2988075063565387,
      -1.2368570350995696,
      -1.1969537093971807,
      -1.3200471904729403,
      -1.2385329813356072,
      -1.3707820857759814,
      -1.3809377086419456,
      -1.3750660776825148,
      -1.1929498702919556,
      -1.3767433058503094,
      -1.2938973711624362,
      -1.2866956422629288,
      -1.2508414454543273,
      -1.3897598186797415
    ],
    [
      -1.376487433504648,
      -1.2994748928117892,
      -1.3031642039658473,
      -1.2884600564250421,
      -1.303575405987067,
      -1.3033667845757568,
      -1.4002300985089098,
      -1.3263838989528653,
      -1.275958580556404,
      0.0,
      -1.3026901954239445,
      -1.4106141141127577,
      -1.3154318035947705,
      -1.2727948040217119,
      -1.2992722219332122,
      -1.2734004639890524,
      -1.310812641876434,
      -1.3038743294205157,
      -1.32918820268799,
      -1.3086298557597604,
      -1.339107184432164,
      -1.3228907393013571,
      -1.349547015352562,
      -1.3005530388505464,
      -1.3438871871440388,
      -1.323641621912166,
      -1.302094338771998,
      -1.3685854561857198,
      -1.3762518690452188
    ],
    [
      -1.3996189042444305,
      -1.1713195597053256,
      -1.254242052667024,
      -1.215251246599607,
      -1.290421699500835,
      -1.2354416812075093,
      -1.4497346372476936,
      -1.2845891107325849,
      -1.3101471072401374,
      -1.4190569771356005,
      0.0,
      -1.4866919161810952,
      -1.3075564125690242,
      -1.1891346064182644,
      -1.2748751553666107,
      -1.363412392108288,
      -1.3158035190960726,
      -1.2891829348756145,
      -1.3869535450847945,
      -1.3016348428688387,
      -1.4195739908145997,
      -1.422811463186449,
      -1.447647283335161,
      -1.3381597995473564,
      -1.4214899725859174,
      -1.3666931080803655,
      -1.3005321668726137,
      -1.3950389102249436,
      -1.452483059447013
    ],
    [
      -1.2764329608632121,
      -1.2316869029221673,
      -1.2392622164478797,
      -1.212122408390849,
      -1.2259849519716688,
      -1.216843028763963,
      -1.1910593069361888,
      -1.1833500202446177,
      -1.2201003497813347,
      -1.265683027095922,
      -1.2166869753847047,
      0.0,
      -1.2401540860426694,
      -1.2109454099421446,
      -1.2560831362530076,
      -1.240097489315349,
      -1.185403240857591,
      -1.2478146170691182,
      -1.226880667664832,
      -1.267013165546527,
      -1.2139350887007687,
      -1.2722751005282766,
      -1.2181290184331621,
      -1.2379593594850444,
      -1.239672442350717,
      -1.2609223816954873,
      -1.2550532862914165,
      -1.2531771408830175,
      -1.2182412369417754
    ],
    [
      -1.416247546884893,
      -1.1511243302438086,
      -1.1029429664765302,
      -1.0696478772489668,
      -1.211106896022828,
      -1.1598506780859403,
      -1.4386143195403946,
      -1.2287938263181952,
      -1.0766416612902976,
      -1.3554466798603766,
      -1.2041572476552034,
      -1.453720036223285,
      0.0,
      -1.2077013158385266,
      -1.2140277067649399,
      -1.3144253950129245,
      -1.237405576153752,
      -1.2530324782737323,
      -1.2773853972502986,
      -1.2406463491562574,
      -1.357777545436401,
      -1.3709246321008115,
      -1.344111014960609,
      -1.2288384394010332,
      -1.3489917313710533,
      -1.33027560794201,
      -1.2498689939549499,
      -1.3069551620728714,
      -1.4078226901103714
    ],
    [
      -1.29846673670159,
      -0.9993891998906151,
      -1.0912595776064797,
      -1.0138736085155282,
      -1.1277457911671291,
      -1.0924923050819646,
      -1.3091654738494618,
      -1.1566769030945192,
      -1.1096982282693748,
      -1.2030465257328389,
      -0.9701058464332941,
      -1.369535874861468,
      -1.1519359274081902,
      0.0,
      -1.1033728753247087,
      -1.1521692302386712,
      -1.1574173625134796,
      -1.1575271042862232,
      -1.2234299885083437,
      -1.104878302159207,
      -1.2479804669177341,
      -1.228854957481525,
      -1.307248639329348,
      -1.0980333389299104,
      -1.2745755653467232,
      -1.1685875587186643,
      -1.1868145274675819,
      -1.2144182579999858,
      -1.3346613263484741
    ],
    [
      -1.4972968919846819,
      -1.3469820643618982,
      -1.3287146488969468,
      -1.2906098296836335,
      -1.3833628337277961,
      -1.3547749895225252,
      -1.5712611180765728,
      -1.404394802852145,
      -1.3923914522758143,
      -1.489419271394539,
      -1.3483406903149004,
      -1.5860727333348763,
      -1.4186587420570251,
      -1.3850383915131326,
      0.0,
      -1.4719098758005178,
      -1.3768783646422045,
      -1.3968957505590889,
      -1.532278317122571,
      -1.401949094556967,
      -1.4852435336965943,
      -1.5303302416851943,
      -1.534901865049051,
      -1.3685194195539179,
      -1.4838606796304483,
      -1.4517628868781405,
      -1.311632436741078,
      -1.4345943884151138,
      -1.5265609613242979
    ],
    [
      -1.364292153014685,
      -1.2669605574610077,
      -1.282715757778705,
      -1.2906320809499892,
      -1.3081608753026874,
      -1.271507340539339,
      -1.3834863404916362,
      -1.2948782883050494,
      -1.2785552476298652,
      -1.3217642590826477,
      -1.287169655286406,
      -1.4367309844466531,
      -1.304608321184275,
      -1.2845191272574699,
      -1.3259067587528592,
      0.0,
      -1.3054388837804805,
      -1.3127934329587012,
      -1.3469969674847304,
      -1.2911276414538726,
      -1.3812971923324557,
      -1.3206437481807378,
      -1.3949383160909148,
      -1.2951102603604907,
      -1.3696266534822648,
      -1.2731093251520313,
      -1.318966278099132,
      -1.340101275638632,
      -1.3447116785071052
    ],
    [
      -1.4363442699448885,
      -1.3657428717768298,
      -1.3336691535433538,
      -1.323036025316462,
      -1.3411471491214983,
      -1.3402825928529436,
      -1.4756655185914966,
      -1.379515488340131,
      -1.351787721497931,
      -1.4151007829353108,
      -1.3431298492625698,
      -1.5059352406520412,
      -1.3544367661096455,
      -1.397735085980771,
      -1.3301048904585935,
      -1.4487225169867972,
      0.0,
      -1.3921728887291343,
      -1.4570622937745048,
      -1.4225762282313712,
      -1.3900499500218833,
      -1.4860268739982132,
      -1.438552166404277,
      -1.408816962226304,
      -1.456093196581665,
      -1.3725911401537254,
      -1.2798484551680795,
      -1.454442124128196,
      -1.429200668283521
    ],
    [
      -1.2995317275173448,
      -1.0497495702295818,
      -1.099132437193325,
      -1.0281204564138646,
      -1.1034518981259511,
      -1.0205769043949693,
      -1.2833991496726163,
      -1.1229412305180548,
      -1.0337309842079996,
      -1.2657503906798735,
      -1.1023286872637095,
      -1.3353276110794638,
      -1.0735452912505572,
      -1.1304585580959075,
      -1.0911792921899097,
      -1.185461541320076,
      -1.1552475358461134,
      0.0,
      -1.1673749178558033,
      -1.0994603051363276,
      -1.264374995760381,
      -1.247984941788937,
      -1.2933513584860463,
      -1.104077673055973,
      -1.2421104898168291,
      -1.1754906209709277,
      -1.1476683506227838,
      -1.145184123994681,
      -1.267314553994475
    ],
    [
      -1.5452336498565318,
      -1.4434513203112709,
      -1.3918152097443315,
      -1.4106301090979452,
      -1.4470983079130681,
      -1.4341535558755667,
      -1.4934135162231907,
      -1.43006726948893,
      -1.3973588727723492,
      -1.4978773463872175,
      -1.431903458860792,
      -1.5428883579019361,
      -1.388616393127652,
      -1.453933678153581,
      -1.4720834248420518,
      -1.45329956076426,
      -1.449521536382316,
      -1.4392147440113865,
      0.0,
      -1.456634805807064,
      -1.4865725616117123,
      -1.5165747892520878,
      -1.4651044185070705,
      -1.4881102507721682,
      -1.4861521918614828,
      -1.4874832176629589,
      -1.4577934752968913,
      -1.4708000238254706,
      -1.522911427103676
    ],
    [
      -1.3949751423958878,
      -1.2231606638669161,
      -1.2442812293371608,
      -1.238365341347533,
      -1.2885968417716531,
      -1.2818739380161326,
      -1.4014584307635896,
      -1.280751104691103,
      -1.2816518326626332,
      -1.3531348120243178,
      -1.252820551219143,
      -1.4314488941925276,
      -1.2741613046426783,
      -1.2505897771905032,
      -1.2888501209501506,
      -1.3267432184118875,
      -1.3207900320977333,
      -1.2585980063148732,
      -1.3456413824203215,
      0.0,
      -1.325805476720757,
      -1.322296714398092,
      -1.369359904135184,
      -1.2256114328160865,
      -1.37003011889049,
      -1.2463718145319536,
      -1.298191266160554,
      -1.2589040872605883,
      -1.358818971606547
    ],
    [
      -1.6315995927504765,
      -1.5350572141031449,
      -1.520947846399233,
      -1.5345274364655688,
      -1.545515381737013,
      -1.5561739362903255,
      -1.6083802035020058,
      -1.5555732278965269,
      -1.5596964799574482,
      -1.5797874693723137,
      -1.556781299133066,
      -1.637183430845071,
      -1.5448934221915485,
      -1.6041115524855833,
      -1.5235177624351346,
      -1.6477195263665496,
      -1.528301944326037,
      -1.5717609564168602,
      -1.577759749335467,
      -1.5270908028224377,
      0.0,
      -1.6970856554917093,
      -1.6002522002706636,
      -1.588318956077893,
      -1.6096373430765845,
      -1.6106110658521315,
      -1.5783061023821081,
      -1.5540753115316224,
      -1.6287173863167939
    ],
    [
      -1.283635647273053,
      -1.1823202714270762,
      -1.206709348320743,
      -1.224757305149058,
      -1.2708826317174975,
      -1.220036402522211,
      -1.3359726033916866,
      -1.2302085507899974,
      -1.1959013672212333,
      -1.2492724673703042,
      -1.1912569715845531,
      -1.3574062542862244,
      -1.2003168541956117,
      -1.2034783010482777,
      -1.2338479682041956,
      -1.2214286461112607,
      -1.2579203211549985,
      -1.2144531858930712,
      -1.2416121505319087,
      -1.186072258449545,
      -1.3021483968805183,
      0.0,
      -1.319070148080347,
      -1.24255728612251,
      -1.3109842243571355,
      -1.1714945147436373,
      -1.2710728409334238,
      -1.289940666357991,
      -1.2808699109260167
    ],
    [
      -1.5133617300393543,
      -1.4678921255749255,
      -1.4549746955200518,
      -1.485726114303516,
      -1.4367350000742378,
      -1.471340938112101,
      -1.519972726357436,
      -1.4648270785438224,
      -1.4819960098320377,
      -1.4876478635255612,
      -1.4745398656136195,
      -1.5302559202248411,
      -1.4738051857579801,
      -1.4914434829106644,
      -1.4824371286338571,
      -1.515172314452419,
      -1.452020054268027,
      -1.4867516451963916,
      -1.4754269819559964,
      -1.5057570088066767,
      -1.4989881773498448,
      -1.5557410047970865,
      0.0,
      -1.5103077974331867,
      -1.4293042431513852,
      -1.5209526071834445,
      -1.401365826551013,
      -1.4634421881218784,
      -1.4861442353138137
    ],
    [
      -1.3218315841389852,
      -1.1227355291227303,
      -1.1203642802088942,
      -1.1424562285917998,
      -1.2444645760351083,
      -1.1574922179175204,
      -1.3724438341340135,
      -1.2486550666472898,
      -1.1565119961856796,
      -1.2595843776480236,
      -1.2107693265244712,
      -1.4258938707293796,
      -1.1903125087914914,
      -1.1516260407455912,
      -1.1964552522029057,
      -1.237320074418366,
      -1.2357781279643,
      -1.2113770854213386,
      -1.3135674778691673,
      -1.1420948458970956,
      -1.3084704094664323,
      -1.294203451823908,
      -1.3407464816994104,
      0.0,
      -1.3322933894113957,
      -1.2364724990812264,
      -1.2561639525674622,
      -1.0547575293981477,
      -1.392094178616905
    ],
    [
      -1.4219891400627525,
      -1.389372970440749,
      -1.333392719245942,
      -1.4115751676581108,
      -1.34000111321189,
      -1.4216865948066884,
      -1.4867802146420184,
      -1.410226550333086,
      -1.4489538750184787,
      -1.4415292394348143,
      -1.3829171782062997,
      -1.519454133795393,
      -1.41039669882293,
      -1.4058280190340486,
      -1.383994676387024,
      -1.4507202676521957,
      -1.3653370216051446,
      -1.4136625142835684,
      -1.444965099929581,
      -1.432275076613584,
      -1.3934141854317255,
      -1.4789336992698,
      -1.3768017821829497,
      -1.4461109425121714,
      0.0,
      -1.4334409139833544,
      -1.369235174066011,
      -1.3769683227919978,
      -1.4681896220010389
    ],
    [
      -1.5659991268559457,
      -1.4149727462580999,
      -1.4376449079190003,
      -1.463823024191746,
      -1.4186168735741875,
      -1.3947580668229684,
      -1.5581744562414364,
      -1.413129081729816,
      -1.4103853721743087,
      -1.4988896631328843,
      -1.4064059294652609,
      -1.6025524220997773,
      -1.442821409179545,
      -1.413637941146414,
      -1.4204591774086908,
      -1.472284244690828,
      -1.4221743143725794,
      -1.4277698128123297,
      -1.5006870369414687,
      -1.4091070295480645,
      -1.4798022611439314,
      -1.4416708891876358,
      -1.5225739266718346,
      -1.3935746925049182,
      -1.5522854735411187,
      0.0,
      -1.476990060958963,
      -1.5176001656299005,
      -1.5264414011757321
    ],
    [
      -1.5967183966576048,
      -1.5241064363483148,
      -1.4964957561571934,
      -1.5221773813536283,
      -1.5269431093294141,
      -1.5248429523035103,
      -1.6852140120950427,
      -1.569201127795989,
      -1.575478520852811,
      -1.628821619581847,
      -1.5632672546786572,
      -1.6887710207509503,
      -1.5795842100983588,
      -1.5943265478639959,
      -1.4795875113161034,
      -1.6144035545329076,
      -1.4604134029320943,
      -1.551079716541063,
      -1.6369997113424832,
      -1.5109747876593662,
      -1.6024608878231559,
      -1.6696694973750699,
      -1.613676969234917,
      -1.5764558506932262,
      -1.6062178472226272,
      -1.574301543786111,
      0.0,
      -1.604044726601448,
      -1.6373228286302737
    ],
    [
      -1.4317845563949292,
      -1.2128557665767825,
      -1.1981872967443477,
      -1.295588327072677,
      -1.319263470771316,
      -1.2864431702488033,
      -1.444918075916148,
      -1.3523620473811122,
      -1.3050894996713216,
      -1.4452821728380651,
      -1.3328153479461262,
      -1.5051531605438122,
      -1.3527055402211308,
      -1.3404394050526116,
      -1.3016898229444684,
      -1.4012616503905306,
      -1.3214086277011798,
      -1.323418055850486,
      -1.366102339786179,
      -1.2776258774265432,
      -1.3756389231617252,
      -1.4860453395971271,
      -1.4035587312234525,
      -1.1013322195518123,
      -1.3307120210830026,
      -1.412285855301181,
      -1.337118588717526,
      0.0,
      -1.4944401296707825
    ],
    [
      -1.352672866575449,
      -1.269607187736472,
      -1.2749452141512954,
      -1.283398376922068,
      -1.259554963608581,
      -1.3015134715999372,
      -1.3638907534361433,
      -1.2974114148777216,
      -1.284840662039479,
      -1.3182979922370806,
      -1.2750952801450637,
      -1.367253278306683,
      -1.3146893690930794,
      -1.289572799082502,
      -1.2299906850288316,
      -1.3126568723555874,
      -1.2652098151133548,
      -1.273916514175366,
      -1.3430706294936463,
      -1.2821084082991951,
      -1.3323735766006908,
      -1.2732736550903514,
      -1.3587520871791494,
      -1.339669530183104,
      -1.3307271299382493,
      -1.3224303406245557,
      -1.2627325239694023,
      -1.3331949594723593,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.25050571931049337,
      0.246400027883388,
      0.2724451683403655,
      0.23190706062467825,
      0.2619575252646982,
      0.18960558866019728,
      0.23526061824224032,
      0.25309714058800026,
      0.22751765451976302,
      0.2478666875330653,
      0.181943485520625,
      0.25997132020415226,
      0.24756111010198723,
      0.2480115498460327,
      0.2464078443920945,
      0.27223442049272606,
      0.2517140012536112,
      0.19787504291665536,
      0.2357206740528981,
      0.2033657771267765,
      0.2179407130516089,
      0.23134436799957525,
      0.23040489102815087,
      0.21016532522421993,
      0.22752758250546012,
      0.2764974355100023,
      0.23024535779350153,
      0.22375679742157173
    ],
    [
      0.30324152131368587,
      0.0,
      0.47363720479737204,
      0.5423646092481278,
      0.4193794394490469,
      0.48037881658785153,
      0.26683319097309677,
      0.4144769943138764,
      0.4500712458211591,
      0.29022878294617804,
      0.5286919860359511,
      0.2300154643409551,
      0.44025046636355825,
      0.497804441774067,
      0.45780554845114185,
      0.346707199195192,
      0.3847797194088567,
      0.4272468658965345,
      0.32826128205062255,
      0.42516536128791893,
      0.3254614698809346,
      0.3109309328658896,
      0.28424629150354486,
      0.4682705457510272,
      0.2818592543757039,
      0.3834213517332503,
      0.3617568577935608,
      0.4167987449382453,
      0.30656637010774257
    ],
    [
      0.38063313468887827,
      0.5691426953565688,
      0.0,
      0.6353441736207874,
      0.5641570797268451,
      0.5713404839033904,
      0.3424190921850907,
      0.5103488561749066,
      0.5917511132056845,
      0.43395910172141106,
      0.5646761235097559,
      0.2955267864214668,
      0.6000718326782,
      0.5753925455031164,
      0.5484228286972004,
      0.452046846604198,
      0.48363821693917464,
      0.5356978632613798,
      0.438765593298583,
      0.5409261976338215,
      0.41505611616388527,
      0.4121743637351234,
      0.375657309256187,
      0.528510983542561,
      0.4868270128914245,
      0.4206113999362904,
      0.547517800470505,
      0.5063387911499828,
      0.364594761436998
    ],
    [
      0.33935227872834517,
      0.6066857344049807,
      0.5396160996498514,
      0.0,
      0.47313859240917244,
      0.5431681699757127,
      0.30643549653227753,
      0.45201406033620417,
      0.513147963246172,
      0.3445081520855373,
      0.5824765166444672,
      0.2860578327278591,
      0.5788744626154987,
      0.5495867711794,
      0.5250209246299071,
      0.4498386349939745,
      0.49471550513004936,
      0.47419907380992266,
      0.3894155643025563,
      0.47284598008281664,
      0.35726261914969104,
      0.3459373759565416,
      0.325934075833773,
      0.4832287743286794,
      0.3338302933479802,
      0.38702475090707833,
      0.43886065673394725,
      0.4194578296206557,
      0.3126927180760457
    ],
    [
      0.41054148971229765,
      0.5644831638407017,
      0.5696460141090738,
      0.5782980650297729,
      0.0,
      0.5772249373395317,
      0.33193936149201964,
      0.5011368841264268,
      0.5579113967063012,
      0.4195507944257806,
      0.5848495377927572,
      0.31832243565979046,
      0.5322108447623946,
      0.5131950595079853,
      0.5710347376208809,
      0.384336226421655,
      0.5026939114087801,
      0.5469827323971428,
      0.398616594670238,
      0.5043814329809204,
      0.4263414486376267,
      0.3667198315275404,
      0.3806698855173487,
      0.45106968372844536,
      0.45324134946448225,
      0.48165975681609496,
      0.5076735758391111,
      0.4143632690154939,
      0.3515630924147608
    ],
    [
      0.3480553959948556,
      0.5881350267762897,
      0.585893853479915,
      0.5630468228884222,
      0.518066174250895,
      0.0,
      0.33523941886174824,
      0.5285649414560343,
      0.5329799384075791,
      0.418317448584238,
      0.5276548249702628,
      0.275441112837564,
      0.5416987597548943,
      0.5034447210056205,
      0.5213755933771949,
      0.4759969869307885,
      0.532475570550845,
      0.5539870833010225,
      0.38396172601778855,
      0.5152650540771955,
      0.42640252683089486,
      0.382086413978292,
      0.3549171222908034,
      0.479509206468822,
      0.410534188730717,
      0.461047103506399,
      0.5433321361835848,
      0.40175304244537835,
      0.364003773893695
    ],
    [
      0.2323995683158917,
      0.29267592860779024,
      0.3263186520940029,
      0.29959838522403115,
      0.3401007797191242,
      0.2961769098511142,
      0.0,
      0.32729206836598834,
      0.2908878787274356,
      0.2912521276923574,
      0.28805426116726585,
      0.2848764109676627,
      0.30428228738930896,
      0.27458564141009423,
      0.3281956151892236,
      0.26578295620110026,
      0.3338709093407217,
      0.2714385257232883,
      0.32544792018894064,
      0.2871870740631095,
      0.3370710316136727,
      0.24459971023775195,
      0.31372010704708786,
      0.28251949931315745,
      0.31914291073355927,
      0.2741777400443537,
      0.32022814150483825,
      0.2917867446471569,
      0.3038134673697481
    ],
    [
      0.380953439402564,
      0.5432015483951673,
      0.5754784332732041,
      0.5776799836508635,
      0.5261258028097338,
      0.5578175881809306,
      0.36018145163255055,
      0.0,
      0.5445762732313382,
      0.44036356264198817,
      0.5929124033810058,
      0.29695818488963277,
      0.5579139570332834,
      0.5001811981147195,
      0.5073503432231439,
      0.4525834237581703,
      0.45575118247469093,
      0.5269908173241402,
      0.4465327495424567,
      0.5000948957424676,
      0.3928472283898392,
      0.39583575904453316,
      0.390880611637378,
      0.4568532823823559,
      0.4143593018691807,
      0.42578665687687756,
      0.4742027767296926,
      0.44646690713735526,
      0.3734188080679275
    ],
    [
      0.3864123895854672,
      0.5710906451443674,
      0.5976867158685293,
      0.6062038505244658,
      0.5332396266871191,
      0.5783441059695491,
      0.3379090474766606,
      0.5116173554150516,
      0.0,
      0.397602041054401,
      0.5332438424924042,
      0.32131800704291136,
      0.6595489800269432,
      0.5277812933504786,
      0.539058421688192,
      0.45597737865438326,
      0.5179278499113524,
      0.5578311756137413,
      0.4347376945379817,
      0.5162519036753148,
      0.38400279923494063,
      0.37384717636897635,
      0.37971880732840724,
      0.5618350147189664,
      0.37804157916061265,
      0.4608875138484858,
      0.46808924274799324,
      0.5039434395565947,
      0.3650250663311805
    ],
    [
      0.24680831860566443,
      0.3238208592985232,
      0.3201315481444651,
      0.3348356956852703,
      0.3197203461232454,
      0.3199289675345556,
      0.22306565360140262,
      0.29691185315744706,
      0.3473371715539084,
      0.0,
      0.3206055566863679,
      0.21268163799755468,
      0.3078639485155419,
      0.35050094808860055,
      0.32402353017710017,
      0.34989528812126003,
      0.3124831102338783,
      0.3194214226897967,
      0.2941075494223224,
      0.314665896350552,
      0.2841885676781484,
      0.30040501280895526,
      0.27374873675775047,
      0.322742713259766,
      0.27940856496627364,
      0.29965413019814635,
      0.3212014133383143,
      0.2547102959245926,
      0.24704388306509362
    ],
    [
      0.3888615365070156,
      0.6171608810461204,
      0.5342383880844221,
      0.573229194151839,
      0.498058741250611,
      0.5530387595439368,
      0.3387458035037525,
      0.5038913300188612,
      0.4783333335113087,
      0.36942346361584555,
      0.0,
      0.3017885245703509,
      0.48092402818242186,
      0.5993458343331817,
      0.5136052853848354,
      0.42506804864315817,
      0.47267692165537345,
      0.49929750587583155,
      0.40152689566665156,
      0.4868455978826074,
      0.3689064499368464,
      0.36566897756499706,
      0.34083315741628506,
      0.45032064120408966,
      0.3669904681655287,
      0.4217873326710806,
      0.48794827387883233,
      0.39344153052650244,
      0.335997381304433
    ],
    [
      0.22401901694106385,
      0.26876507488210866,
      0.26118976135639627,
      0.2883295694134269,
      0.2744670258326072,
      0.28360894904031286,
      0.3093926708680872,
      0.3171019575596583,
      0.2803516280229412,
      0.2347689507083539,
      0.28376500241957126,
      0.0,
      0.26029789176160656,
      0.28950656786213136,
      0.24436884155126837,
      0.26035448848892706,
      0.315048736946685,
      0.25263736073515775,
      0.2735713101394439,
      0.23343881225774887,
      0.28651688910350726,
      0.22817687727599933,
      0.28232295937111385,
      0.26249261831923154,
      0.2607795354535589,
      0.23952959610878866,
      0.24539869151285942,
      0.2472748369212585,
      0.28221074086250053
    ],
    [
      0.3526016129806553,
      0.6177248296217397,
      0.665906193389018,
      0.6992012826165814,
      0.5577422638427203,
      0.6089984817796079,
      0.3302348403251536,
      0.540055333547353,
      0.6922074985752507,
      0.41340248000517166,
      0.5646919122103449,
      0.3151291236422633,
      0.0,
      0.5611478440270217,
      0.5548214531006084,
      0.4544237648526237,
      0.5314435837117963,
      0.5158166815918159,
      0.4914637626152496,
      0.5282028107092909,
      0.4110716144291473,
      0.39792452776473675,
      0.4247381449049392,
      0.540010720464515,
      0.4198574284944949,
      0.4385735519235383,
      0.5189801659105984,
      0.4618939977926768,
      0.3610264697551768
    ],
    [
      0.3615901920165079,
      0.6606677288274828,
      0.5687973511116182,
      0.6461833202025697,
      0.5323111375509688,
      0.5675646236361334,
      0.35089145486863615,
      0.5033800256235788,
      0.5503587004487231,
      0.4570104029852591,
      0.6899510822848038,
      0.29052105385662985,
      0.5081210013099078,
      0.0,
      0.5566840533933892,
      0.5078876984794267,
      0.5026395662046184,
      0.5025298244318748,
      0.43662694020975423,
      0.5551786265588909,
      0.41207646180036384,
      0.4312019712365729,
      0.35280828938874986,
      0.5620235897881876,
      0.38548136337137473,
      0.4914693699994337,
      0.4732424012505161,
      0.44563867071811214,
      0.32539560236962384
    ],
    [
      0.3958098464842077,
      0.5461246741069914,
      0.5643920895719428,
      0.6024969087852561,
      0.5097439047410934,
      0.5383317489463644,
      0.3218456203923168,
      0.48871193561674464,
      0.5007152861930753,
      0.40368746707435066,
      0.5447660481539891,
      0.30703400513401324,
      0.4744479964118644,
      0.508068346955757,
      0.0,
      0.42119686266837175,
      0.5162283738266851,
      0.49621098790980067,
      0.36082842134631865,
      0.4911576439119225,
      0.4078632047722952,
      0.3627764967836953,
      0.35820487341983864,
      0.5245873189149717,
      0.40924605883844123,
      0.44134385159074907,
      0.5814743017278114,
      0.4585123500537758,
      0.3665457771445917
    ],
    [
      0.298901645175665,
      0.3962332407293423,
      0.380478040411645,
      0.3725617172403608,
      0.3550329228876625,
      0.39168645765101107,
      0.27970745769871375,
      0.3683155098853006,
      0.3846385505604848,
      0.34142953910770224,
      0.3760241429039439,
      0.2264628137436968,
      0.358585477006075,
      0.3786746709328801,
      0.3372870394374907,
      0.0,
      0.3577549144098695,
      0.35040036523164875,
      0.31619683070561955,
      0.37206615673647736,
      0.2818966058578942,
      0.34255005000961214,
      0.26825548209943517,
      0.36808353782985925,
      0.2935671447080852,
      0.3900844730383186,
      0.34422752009121793,
      0.32309252255171805,
      0.3184821196832448
    ],
    [
      0.4299364409658393,
      0.5005378391338979,
      0.532611557367374,
      0.5432446855942659,
      0.5251335617892294,
      0.5259981180577842,
      0.39061519231923114,
      0.48676522257059673,
      0.5144929894127968,
      0.45117992797541695,
      0.5231508616481579,
      0.3603454702586866,
      0.5118439448010823,
      0.4685456249299569,
      0.5361758204521343,
      0.41755819392393057,
      0.0,
      0.47410782218159353,
      0.409218417136223,
      0.4437044826793566,
      0.47623076088884453,
      0.3802538369125146,
      0.4277285445064507,
      0.45746374868442374,
      0.41018751432906275,
      0.49368957075700237,
      0.5864322557426482,
      0.4118385867825318,
      0.4370800426272068
    ],
    [
      0.400528218494288,
      0.650310375782051,
      0.6009275088183077,
      0.6719394895977682,
      0.5966080478856817,
      0.6794830416166635,
      0.41666079633901654,
      0.577118715493578,
      0.6663289618036332,
      0.4343095553317593,
      0.5977312587479233,
      0.36473233493216894,
      0.6265146547610756,
      0.5696013879157253,
      0.6088806538217231,
      0.5145984046915568,
      0.5448124101655194,
      0.0,
      0.5326850281558295,
      0.6005996408753052,
      0.43568495025125187,
      0.45207500422269575,
      0.4067085875255865,
      0.5959822729556599,
      0.45794945619480365,
      0.5245693250407051,
      0.552391595388849,
      0.5548758220169518,
      0.4327453920171578
    ],
    [
      0.29909042941648134,
      0.40087275896174224,
      0.45250886952868163,
      0.43369397017506794,
      0.39722577135994497,
      0.4101705233974464,
      0.3509105630498224,
      0.4142568097840831,
      0.44696520650066396,
      0.34644673288579564,
      0.41242062041222116,
      0.301435721371077,
      0.45570768614536106,
      0.39039040111943213,
      0.3722406544309613,
      0.3910245185087531,
      0.39480254289069716,
      0.4051093352616266,
      0.0,
      0.38768927346594917,
      0.35775151766130087,
      0.32774929002092534,
      0.3792196607659426,
      0.35621382850084493,
      0.3581718874115303,
      0.35684086161005424,
      0.3865306039761218,
      0.3735240554475425,
      0.321412652169337
    ],
    [
      0.2229066344881374,
      0.3947211130171091,
      0.37360054754686445,
      0.37951643553649217,
      0.3292849351123721,
      0.33600783886789265,
      0.21642334612043568,
      0.33713067219292214,
      0.3362299442213921,
      0.2647469648597074,
      0.3650612256648822,
      0.1864328826914976,
      0.34372047224134694,
      0.367291999693522,
      0.3290316559338746,
      0.29113855847213777,
      0.29709174478629197,
      0.35928377056915206,
      0.27224039446370374,
      0.0,
      0.29207630016326824,
      0.2955850624859333,
      0.2485218727488412,
      0.3922703440679387,
      0.24785165799353526,
      0.3715099623520717,
      0.3196905107234713,
      0.3589776896234369,
      0.25906280527747816
    ],
    [
      0.30135152173966917,
      0.3978939003870008,
      0.4120032680909127,
      0.3984236780245769,
      0.3874357327531326,
      0.37677717819982015,
      0.32457091098813984,
      0.3773778865936188,
      0.37325463453269747,
      0.35316364511783194,
      0.3761698153570796,
      0.29576768364507466,
      0.3880576922985972,
      0.3288395620045623,
      0.4094333520550111,
      0.28523158812359606,
      0.4046491701641086,
      0.36119015807328547,
      0.35519136515467875,
      0.405860311667708,
      0.0,
      0.23586545899843636,
      0.33269891421948206,
      0.3446321584122527,
      0.3233137714135612,
      0.32234004863801413,
      0.35464501210803756,
      0.3788758029585233,
      0.3042337281733518
    ],
    [
      0.3100707464698613,
      0.41138612231583815,
      0.3869970454221714,
      0.36894908859385644,
      0.32282376202541685,
      0.3736699912207033,
      0.2577337903512278,
      0.363497842952917,
      0.39780502652168104,
      0.34443392637261017,
      0.4024494221583612,
      0.23630013945668993,
      0.3933895395473026,
      0.39022809269463665,
      0.3598584255387187,
      0.37227774763165367,
      0.3357860725879158,
      0.3792532078498432,
      0.3520942432110057,
      0.40763413529336945,
      0.29155799686239603,
      0.0,
      0.2746362456625673,
      0.3511491076204043,
      0.2827221693857789,
      0.42221187899927703,
      0.3226335528094906,
      0.30376572738492325,
      0.3128364828168977
    ],
    [
      0.3510980895920792,
      0.39656769405650794,
      0.4094851241113817,
      0.3787337053279174,
      0.42772481955719566,
      0.3931188815193325,
      0.3444870932739974,
      0.39963274108761104,
      0.38246380979939576,
      0.37681195610587226,
      0.389919954017814,
      0.33420389940659234,
      0.39065463387345334,
      0.37301633672076906,
      0.38202269099757635,
      0.3492875051790145,
      0.41243976536340643,
      0.3777081744350419,
      0.38903283767543706,
      0.35870281082475675,
      0.3654716422815887,
      0.30871881483434693,
      0.0,
      0.3541520221982468,
      0.4351555764800483,
      0.343507212447989,
      0.46309399308042054,
      0.40101763150955505,
      0.3783155843176198
    ],
    [
      0.37833992648256776,
      0.5774359814988226,
      0.5798072304126587,
      0.5577152820297531,
      0.4557069345864446,
      0.5426792927040325,
      0.3277276764875394,
      0.45151644397426316,
      0.5436595144358733,
      0.44058713297352936,
      0.4894021840970817,
      0.27427763989217335,
      0.5098590018300615,
      0.5485454698759618,
      0.5037162584186472,
      0.462851436203187,
      0.46439338265725283,
      0.4887944252002143,
      0.38660403275238564,
      0.5580766647244573,
      0.3917011011551206,
      0.40596805879764486,
      0.35942502892214256,
      0.0,
      0.36787812121015717,
      0.46369901154032656,
      0.44400755805409076,
      0.6454139812234052,
      0.30807733200464793
    ],
    [
      0.3412376508622057,
      0.37385382048420923,
      0.4298340716790161,
      0.3516516232668474,
      0.42322567771306807,
      0.3415401961182698,
      0.2764465762829398,
      0.3530002405918722,
      0.31427291590647943,
      0.3216975514901439,
      0.3803096127186585,
      0.24377265712956508,
      0.3528300921020282,
      0.3573987718909095,
      0.37923211453793426,
      0.3125065232727624,
      0.39788976931981357,
      0.3495642766413898,
      0.31826169099537704,
      0.33095171431137405,
      0.3698126054932327,
      0.28429309165515826,
      0.38642500874200847,
      0.3171158484127867,
      0.0,
      0.3297858769416038,
      0.39399161685894724,
      0.3862584681329604,
      0.2950371689239193
    ],
    [
      0.270281340750417,
      0.4213077213482628,
      0.3986355596873623,
      0.3724574434146166,
      0.4176635940321751,
      0.4415224007833942,
      0.2781060113649263,
      0.42315138587654655,
      0.42589509543205395,
      0.33739080447347836,
      0.42987453814110177,
      0.23372804550658532,
      0.3934590584268176,
      0.4226425264599487,
      0.4158212901976719,
      0.3639962229155347,
      0.4141061532337833,
      0.4085106547940329,
      0.335593430664894,
      0.4271734380582981,
      0.3564782064624312,
      0.3946095784187269,
      0.3137065409345281,
      0.4427057751014445,
      0.283994994065244,
      0.0,
      0.35929040664739964,
      0.31868030197646213,
      0.3098390664306305
    ],
    [
      0.39788088884454553,
      0.4704928491538356,
      0.49810352934495694,
      0.47242190414852203,
      0.46765617617273625,
      0.4697563331986401,
      0.30938527340710764,
      0.4253981577061614,
      0.41912076464933934,
      0.36577766592030336,
      0.43133203082349314,
      0.30582826475120006,
      0.41501507540379157,
      0.4002727376381545,
      0.5150117741860469,
      0.38019573096924275,
      0.5341858825700561,
      0.44351956896108735,
      0.35759957415966714,
      0.48362449784278416,
      0.3921383976789945,
      0.3249297881270805,
      0.3809223162672333,
      0.41814343480892413,
      0.3883814382795232,
      0.4202977417160394,
      0.0,
      0.39055455890070245,
      0.3572764568718767
    ],
    [
      0.35191380290419527,
      0.570842592722342,
      0.5855110625547768,
      0.48811003222644755,
      0.46443488852780845,
      0.4972551890503212,
      0.3387802833829765,
      0.43133631191801225,
      0.47860885962780286,
      0.3384161864610593,
      0.45088301135299824,
      0.27854519875531225,
      0.4309928190779937,
      0.44325895424651285,
      0.482008536354656,
      0.3824367089085938,
      0.4622897315979446,
      0.46028030344863846,
      0.41759601951294556,
      0.5060724818725812,
      0.40805943613739926,
      0.2976530197019973,
      0.3801396280756719,
      0.6823661397473122,
      0.4529863382161219,
      0.37141250399794346,
      0.44657977058159837,
      0.0,
      0.2892582296283419
    ],
    [
      0.3451006518080477,
      0.4281663306470247,
      0.4228283042322014,
      0.41437514146142873,
      0.4382185547749158,
      0.3962600467835595,
      0.33388276494735347,
      0.40036210350577517,
      0.4129328563440178,
      0.3794755261464162,
      0.4226782382384331,
      0.3305202400768137,
      0.38308414929041734,
      0.40820071930099466,
      0.4677828333546652,
      0.3851166460279094,
      0.43256370327014193,
      0.42385700420813066,
      0.35470288888985047,
      0.4156651100843016,
      0.365399941782806,
      0.4244998632931454,
      0.33902143120434736,
      0.3581039882003927,
      0.3670463884452475,
      0.3753431777589411,
      0.4350409944140945,
      0.36457855891113744,
      0.0
    ]
  ],
  "row_avgs": [
    0.23604467455030495,
    0.3873804271144675,
    0.48898389656119345,
    0.44019024669425344,
    0.471452054034477,
    0.46689949885184784,
    0.2981244018839206,
    0.4683678061013282,
    0.48068474871483824,
    0.3007825935708749,
    0.44885551021773995,
    0.2674887986327255,
    0.4989032990922889,
    0.48672258942584673,
    0.46079830005275835,
    0.34188131958303486,
    0.46878825122957996,
    0.5381197461015441,
    0.38144202665105026,
    0.31383597649698963,
    0.3539017124961701,
    0.3474339832056255,
    0.3809480357169632,
    0.4617095037194444,
    0.3468641868741243,
    0.3718079137713846,
    0.4155436718750731,
    0.435286715735368,
    0.3937431484786611
  ],
  "col_avgs": [
    0.3374970617596822,
    0.478957387494904,
    0.4747380018579111,
    0.4793946866435608,
    0.43951190550698743,
    0.46120734131152,
    0.31393487240665785,
    0.42641515207455827,
    0.4528712749281139,
    0.3656235551886522,
    0.46077188219872,
    0.28178453775808615,
    0.44500685977910637,
    0.44089319923707604,
    0.44779577950168675,
    0.3877401226154714,
    0.43147760075903696,
    0.42905646388109797,
    0.371384135728685,
    0.43946959570372124,
    0.36509620240803925,
    0.34324918063140825,
    0.34168407147667923,
    0.4300986317769077,
    0.35996325332929474,
    0.3942783333394397,
    0.42767711648601986,
    0.3965742684164691,
    0.32883256323438564
  ],
  "combined_avgs": [
    0.2867708681549936,
    0.43316890730468577,
    0.48186094920955225,
    0.4597924666689071,
    0.45548197977073224,
    0.46405342008168393,
    0.3060296371452892,
    0.44739147908794324,
    0.46677801182147605,
    0.33320307437976354,
    0.45481369620823,
    0.27463666819540583,
    0.47195507943569764,
    0.4638078943314614,
    0.4542970397772226,
    0.36481072109925317,
    0.45013292599430843,
    0.48358810499132104,
    0.3764130811898676,
    0.37665278610035546,
    0.3594989574521047,
    0.3453415819185169,
    0.3613160535968212,
    0.44590406774817604,
    0.3534137201017095,
    0.38304312355541215,
    0.42161039418054647,
    0.41593049207591853,
    0.36128785585652334
  ],
  "gppm": [
    605.228326439862,
    569.2498067867766,
    569.4545419716882,
    569.0263454112247,
    585.6750190452965,
    577.792561796907,
    643.8707766089872,
    591.1769204272222,
    582.9464277398959,
    616.5084909591258,
    576.5133829712371,
    656.2958218357711,
    585.9897065646228,
    588.4053082816966,
    581.6818566945974,
    606.356059063357,
    592.0438875823218,
    594.4724647670286,
    614.9924882915082,
    586.2067056577673,
    617.8085012641602,
    626.5938723341641,
    629.891031021436,
    592.5157907368614,
    620.5383562823162,
    603.5547558613009,
    588.7360521571511,
    606.1846619490642,
    637.1756957987446
  ],
  "gppm_normalized": [
    1.4260985790311032,
    1.381325111416089,
    1.385961806893174,
    1.3867625320740373,
    1.4209648981918053,
    1.4064603600064374,
    1.564363150291607,
    1.4433569585570343,
    1.4168932993907208,
    1.4992383378084502,
    1.399971307518847,
    1.610430848945862,
    1.4230890769183582,
    1.4285870185258702,
    1.4193215213949988,
    1.4739038978609822,
    1.4403966463155853,
    1.4462325782827956,
    1.494687451557315,
    1.4251455662595205,
    1.504529872243049,
    1.523276615250451,
    1.5295054002524062,
    1.444445435306397,
    1.5097604248902143,
    1.4672141847476117,
    1.4292143477302741,
    1.4750107404493422,
    1.5483979759947772
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534,
    448,
    422,
    431,
    410,
    445,
    437,
    448,
    356,
    418,
    400,
    408,
    567,
    423,
    428,
    389,
    424,
    419,
    417,
    416,
    470,
    368,
    384,
    403,
    423,
    394,
    366,
    382,
    442,
    412,
    532,
    444,
    478,
    502,
    430,
    533,
    483,
    440,
    480,
    379,
    427,
    503,
    458,
    484,
    371,
    423,
    429,
    400,
    409,
    416,
    406,
    403,
    340,
    427,
    412,
    405,
    403,
    478,
    337,
    881,
    516,
    435,
    467,
    432,
    482,
    427,
    429,
    438,
    480,
    526,
    521,
    442,
    442,
    405,
    427,
    443,
    406,
    409,
    416,
    406,
    404,
    382,
    465,
    409,
    391,
    417,
    484,
    418,
    1833,
    463,
    409,
    431,
    428,
    476,
    461,
    399,
    432,
    424,
    452,
    363,
    494,
    429,
    456,
    471,
    385,
    410,
    444,
    447,
    419,
    436,
    403,
    412,
    406,
    410,
    428,
    387,
    466,
    526,
    485,
    454,
    438,
    400,
    422,
    425,
    397,
    393,
    505,
    445,
    462,
    393,
    394,
    428,
    499,
    360,
    369,
    419,
    488,
    392,
    493,
    368,
    405,
    433,
    442,
    426,
    399,
    369
  ],
  "response_lengths": [
    2681,
    2749,
    2659,
    2563,
    2348,
    2524,
    2480,
    2282,
    2320,
    2981,
    2625,
    2627,
    2315,
    2165,
    2530,
    2920,
    2088,
    2131,
    2402,
    2783,
    2317,
    2655,
    2144,
    2264,
    2522,
    2412,
    2493,
    2292,
    2184
  ]
}