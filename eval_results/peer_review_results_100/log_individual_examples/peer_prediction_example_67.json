{
  "example_idx": 67,
  "reference": "Under review as a conference paper at ICLR 2023\n\nWHAT DOES A PLATYPUS LOOK LIKE? GENERATING CUSTOMIZED PROMPTS FOR ZERO-SHOT IMAGE CLASSIFICATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOpen vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called “prompts”, typically consists of a set of hand-written templates (e.g., “a photo of a {}”) which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced “couple”). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that are customized for each object category. We find that this straightforward and general approach improves accuracy on a range of zero-shot image classification benchmarks, including over one percentage point gain on ImageNet. Finally, this simple baseline requires no additional training and remains completely zero-shot.\n\nFigure 1: Schematic of the method. (Left) The standard method of a zero-shot open vocabulary image classification model (e.g., CLIP (Radford et al., 2021)). (Right) Our method of CuPL. First, an LLM generates descriptive captions for given class categories. Next, an open vocabulary model uses these captions as prompts for performing classification.\n\n1\n\nINTRODUCTION\n\nOpen vocabulary models (Pham et al., 2021; Jia et al., 2021; Radford et al., 2021; Yu et al., 2022a) achieve high classification accuracy across a large number of datasets without labeled training data for those tasks. To accomplish this, these models leverage the massive amounts of image-text pairs available on the internet by learning to associate the images with their correct caption, leading to greater flexibility during inference. Unlike standard models, these models classify images by providing a similarity score between an image and a caption. To perform inference, one can generate a\n\n1\n\n“A spatula is a flat, rectangular kitchen utensil with a long handle”“A photo of a spatula”Standard Zero-shotCustomized Prompts via Language models (CuPL)GPT-3“What does a platypus look like?” “A platypus looks like a beaver with a duck's bill”“A photo of a goldfish”“A photo of a platypus”“Goldfish are small, orange fish with shiny scales”“A platypus looks like a beaver with a duck's bill”Image encoderText encoderImage encoderText encoderUnder review as a conference paper at ICLR 2023\n\ncaption or “prompt” associated with each of the desired categories, and match each image to the best prompt. This means that categories can be selected ad hoc and adjusted without additional training.\n\nHowever, this new paradigm poses a challenge:\n\nHow can we best represent an image category through natural language prompts?\n\nThe standard approach is to hand write a number of prompts templates (Radford et al., 2021) (e.g.,“a photo of a {}”), compile a natural language label for each category in the dataset, and create a set of prompts for each category by filling in each of these templates with the natural language labels. Then, image embeddings are matched to the nearest set of prompt embeddings and labelled with the category associated with that set of prompts (more details in Section 2).\n\nThis method has three major drawbacks. Firstly, each prompt template has to be hand-written, so having twice as many prompts for a category requires twice as much human effort. This can become costly as each new dataset typically has a different set of prompt templates (Radford et al., 2021). Secondly, the prompt templates must be general enough to apply to all image categories. For example, a prompt for the ImageNet (Deng et al., 2009) category “platypus” could only be as specific as “a photo of a {platypus}”, and could not be something like “a photo of a {platypus}, a type of aquatic mammal” as that template would no longer be relevant for other image categories. Lastly, writing high performing prompt templates currently requires prior information about the contents of the dataset. For example, the list of hand-written ImageNet prompts (Radford et al., 2021) includes “a black and white photo of the {}.”, “a low resolution photo of a {}.”, and “a toy {}.” all of which demonstrate prior knowledge about the type of representations present in the dataset. This information is not generalizable to other datasets, as ImageNet contains “black and white” and “toy” representations of its categories, but other datasets do not (e.g., FVGC Aircraft (Maji et al., 2013)).\n\nTo overcome these challenges, we propose Customized Prompts via Language models (CuPL). In this algorithm, we couple a large language model (LLM) with a zero-shot open vocabulary image classification model. We use the LLM to generate prompts for each of the image categories in a dataset. Using an LLM allows us to generate an arbitrary number of prompts with a fixed number of hand-written sentences. Additionally, these prompts are now customized to each category and can contain rich visual descriptions while still remaining zero-shot (e.g., “A platypus looks like a beaver with a duck’s bill” – a sentence generated by an LLM).\n\nWe find these customized prompts outperform the hand-written templates on 15 zero-shot image classification benchmarks, including a greater than 1 percentage point gain on ImageNet (Deng et al., 2009) Top-1 accuracy and a greater than 6 percentage point gain on Describable Textures Dataset (Cimpoi et al., 2014), with fewer hand-written prompts when compared to the standard method used in Radford et al. (2021). Finally, this method requires no additional training or labeled data for either model.\n\n2 METHODS\n\nThe CuPL algorithm consists of two steps: (1) generating customized prompts for each of the categories in a given dataset and (2) using these prompts to perform zero-shot image classification.\n\n2.1 GENERATING CUSTOMIZED PROMPTS\n\nThis step consists of generating prompts using an LLM. For clarity, we distinguish between two different kind of prompts. The first are the prompts which cue the LLM to generate the descriptions of the dataset categories. These prompts do not describe an object, but rather prompt the description of an object (e.g., “What does a platypus look like?”). We will refer to these as “LLM-prompts”.\n\nSecondly, there are the prompts to be matched with images in the zero-shot image classification model. These are the prompts that describe a category (e.g., “A platypus looks like ...”). We call them “image-prompts.” These are the output of the LLM, as examplified in Figure 2.\n\nIn this work, we use GPT-3 (Brown et al., 2020) as our LLM. To generate our image-prompts, we must first construct a number of LLM-prompt templates. While this does require some engineering\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Example CuPL LLM-prompts and Image-prompts. LLM-prompts are filled in with a class name and then used as input to GPT-3, which then outputs image-prompts. Example LLM generated image-prompts and associated images from ImageNet are shown. Only image-prompts are used for the downstream image classification.\n\nby hand, it is significantly less than the amount of hand-engineered sentences used in the standard method of creating image-prompt templates for CLIP. For example, in our ImageNet experiments, we construct 5 LLM-prompt templates compared to the 80 image-prompts used by CLIP for zeroshot ImageNet classification.\n\nAfter constructing these LLM-prompts, we generate 10 different image-prompts for each of the LLM-prompts. This means for ImageNet we use an LLM to generate a total of 50 customized image-prompts for each image category. For each of these, we generate a maximum of 50 tokens, but halt a generation early if it produces a period. Additionally, we generate with a high temperature of 0.99, which encourages more diversity among the 10 generated image-prompts. We also clean each generated sentences by deleting any blank lines and adding a period at the end.\n\n2.2 UTILIZING CUSTOMIZED PROMPTS\n\nAfter generating image-prompts for each of the categories, we then perform zero-shot image classification. While there are a number of open vocabulary models (Pham et al., 2021; Jia et al., 2021; Radford et al., 2021; Yu et al., 2022a), we report our results using CLIP (Radford et al., 2021) as this is the most popular publicly available open vocabulary model.\n\nCLIP consists of a text encoder and and image encoder (schematic on the left side of Figure 1). In the standard setting, there are a number of hand-written templates which can be completed with the relevant category names (e.g. “A photo of a {}”, “A photo of many {}”). To classify the images in a dataset, each of these templates is filled in with a given category name. Then each of these sentences is embedded via the text encoder, and all sentences completed with the same category name are averaged and normalized. This results in n embeddings where n is the number of categories in the dataset. Each of these n embeddings is the mean of many different sentence embeddings. Then each image in the dataset is embedded using the image encoder. This embedding is compared to each of the n text embeddings using cosine similarity and is labeled with the most similar one.\n\nCuPL requires only a small adjustment from this standard practice. Instead of filling in the handwritten templates for each category, we simply replace these altogether with the sentences output by GPT-3. This means for CuPL, hand-written templates are only used as input for the LLM, while the prompts for CLIP are entirely generated text. We present 2 different setting of CuPL (as shown in Table 1), each representing a different trade-off between accuracy and hand-engineering.\n\n1. CuPL (base). This setting uses three hand-written sentence across all 15 examined datasets. We do this by constructing general LLM-prompt templates which are filled in with the category names for each dataset. Our three general templates are as follows:\n\nDescribe what a/the\n\nlooks like:\n\nDescribe a/the What are the identifying characteristics of a/the\n\n:\n\n?\n\n3\n\nMarimbaViaductLorikeetPapillonLLM“A lorikeet is a small to medium-sized parrot with a brightly colored plumage.” “A marimba is a large wooden percussion instrument that looks like a xylophone.” “A viaduct is a bridge composed of several spans supported by piers or pillars.” “A Papillon is a small, spaniel-type dog with a long, silky coat and fringed ears”MarimbaViaductLorikeetPapillon“A lorikeet is a small to medium-sized parrot with a brightly colored plumage.” “A marimba is a large wooden percussion instrument that looks like a xylophone.” “A viaduct is a bridge composed of several spans supported by piers or pillars.” “A papillon is a small, spaniel-type dog with a long, silky coat and fringed ears.”GPT-3“What does a {lorikeet, marimba, viaduct, papillon} look like?”LLM-prompts:Image-prompts:Under review as a conference paper at ICLR 2023\n\nThe blank portion of this template is either filled in with the category type plus the category name (e.g. “pet” + {} for the Oxford Pets dataset (Parkhi et al., 2012) or “aircraft” + {} for FGVC Aircraft (Maji et al., 2013)) or just the category name for more general datasets like ImageNet (Deng et al., 2009). Type specification is necessary because of words that have multiple meanings. For example “boxer” from the Oxford Pets dataset can also mean a person who boxes, as opposed to a dog breed, so it is necessary to specify “Describe a pet boxer:”. Similarly, “Tornado” from the FGVC Aircraft dataset can be a type of aircraft or a type of weather.\n\n2. CuPL (full). In this setting we use different LLM-prompt templates for each dataset, just as Radford et al. (2021) uses different image-prompt templates for each dataset. However, we use fewer hand-written templates overall and also contain less specific information about each dataset in the templates. For this work, each dataset has between 2 and 9 LLM-prompts which generate between 20 and 90 image-prompt per category (10 generated sentences per LLM-prompt). For ImageNet, we use the following 5 LLM-prompts: (1) “Describe what a(n) {} looks like”, (2) “How can you identify a(n) {}?”, (3) “What does a(n) {} look like?”, (4) “A caption of an image of a(n) {}”, (5) “Describe an image from the internet of a(n) {}”. Example generations for each of these LLM-prompts are given for two ImageNet categories in Figure 3. Full LLM-prompts for all datasets as well as example image-prompts are given in Sections A and K of the Appendix.\n\n3 EXPERIMENTS AND RESULTS\n\nWe first discuss the details of our experimental setup. We next show improvements on a wide range of image classification benchmarks. We then examine the scaling behavior with respect to the model size and report observations regarding hyperparameters such as the LLM sampling temperature. Finally, we consider and compare with other methods of obtaining descriptive captions, and provide analysis of CuPl’s improvements over the standard method.\n\n3.1 SETUP\n\nUnless specified otherwise, we use CLIP with a backbone of ViT-L/14 (Dosovitskiy et al., 2020) and the GPT-3 DaVinci-002 model. Additionally, in order to perform open vocabulary image classification, each image category needs a natural language label. This is sometimes provided by the dataset, but not always (e.g. ImageNet categories are described by an id number which can map to multiple synonyms). For this work, we use the same natural language labels specified in Radford et al. (2021).\n\nWe report our findings on 15 zero-shot image recognition benchmarks: ImageNet (Deng et al., 2009), Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Stanford Cars (Krause et al., 2013), Scene UNderstanding (SUN397) (Xiao et al., 2010), Food101 (Bossard et al., 2014), FGVC Aircraft (Maji et al., 2013), Oxford Pets (Parkhi et al., 2012), Caltech101 (Fei-Fei et al., 2004),\n\nFigure 3: Example image-prompts for each of the 5 LLM-prompts. For three ImageNet classes (moped, platypus, and slide rule), we give an example image-prompt for each of the 5 LLM-prompts used in CuPL (full) for ImageNet.\n\n4\n\nSlide rules are rectangular, and have a number of lines and markings of different lengths on them. Slide rules have a linear scale on one edge of the rule and a logarithmic scale on the other edge. A slide rule is a straight ruler with two scales that can slide past one another. A slide rule is a mechanical analog computer. This slide rule belongs to my grandfather.The platypus is an odd-looking animal with a duck-like bill, beaver-like tail, and otter-like body. The easiest way to identify a platypus is by its beak, which is wide and flat like a duck’s. A platypus looks like a beaver that has a duck bill. The image shows a platypus swimming in a body of water. This adorable creature is a platypus, a unique mammal found in Australia.A moped typically has pedals, like a bicycle, but also has a motor. A moped has a small engine, usually under 50cc, and generally has a maximum speed of 30mph. A moped looks like a cross between a bicycle and a motorcycle. The image is of a blue moped with a white seat. A moped on a city street.Under review as a conference paper at ICLR 2023\n\nTable 1: Performance of CuPL prompts compared to the standard, hand-written prompts in CLIP (Radford et al., 2021) on 15 zero-shot image classification benchmarks. “∆std” stands for the difference; green shows improvement. In addition to accuracy, we show number of prompt templates (“# hw”) that are hand-written for each dataset using each method, as well as the total and unique number of hand-written templates for each method (unique number only counts templates once even if used for multiple datasets). Note that CuPL (base) uses just three hand-constructed sentence across all datasets compared to 175 in the standard method.\n\nt e\nN e\ng a\n\nm\n\nI\n\nD T\nD\n\ns r\na C\nd r\no f\nn a\nt\n\nS\n\n7 9\n3 N\nU S\n\n1 0\n1 d\no o\nF\n\nt f\na r\nc r\ni\n\nA C\nV G\nF\n\ns t\ne P\n\nd r\no f\nx O\n\n1 0\n1 h\nc e\nt l\na C\n\n2 0\n1\n\ns r\ne w\no\n\nl\n\nF\n\n1 0\n1 F\nC U\n\n0 0\n7 -\ns c\ni t\ne n\nK\n\ni\n\n5 4\nC S\n\nI\n\nS E\nR\n\n0 1\n-\n\nR A\nF\n\nI\n\nC\n\n0 0\n1 -\n\nR A\nF\n\nI\n\nC\n\np a\nn s\nd r\ni\n\nB\n\nn a\ne\n\nm\n\nl a\nt\n\no T\n\ne u\nq\n\ni\n\nn U\n\nstd # hw\n\n75.54 55.20 77.53 69.31 93.08 32.88 93.33 93.24 78.53 77.45 60.07 71.10 95.59 78.26 50.43 73.43\n\n80\n\n8\n\n8\n\n2\n\n1\n\n2\n\n1\n\n34\n\n1\n\n48\n\n28\n\n18\n\n18\n\n18\n\n1\n\n268 175\n\nCuPL (base) 76.19 58.90 76.49 72.74 93.33 36.69 93.37 93.45 78.83 77.74 60.24 68.96 95.81 78.47 51.11 74.15 +3.43 +0.25 +3.81 +0.04 +0.21 +0.30 +0.29 +0.17\n\n+0.65 +3.70\n\n∆ std # hw\n\n-1.04 3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n-2.14 3\n\n+0.22 +0.21 +0.63 3\n\n3\n\n3\n\nCuPL (full) 76.69 61.70 77.63 73.31 93.36 36.11 93.81 93.45 79.67 78.36 60.63 71.69 95.84 78.57 51.11 74.80\n\n∆ std # hw\n\n+1.15 +6.50 +0.10 +4.00 +0.28 +3.23 +0.48 +0.21 +1.14 +0.91 +0.56 +0.59 +0.25 +0.31 +0.63 3\n\n4\n\n3\n\n6\n\n4\n\n9\n\n3\n\n2\n\n3\n\n2\n\n5\n\n5\n\n2\n\n3\n\n5\n\n45\n\n3\n\n59\n\n45\n\nFlowers 102 (Nilsback & Zisserman, 2008), UCF101 (Soomro et al., 2012), Kinetics-700 (Carreira et al., 2019), Remote Sensing Image Scene Classification (RESISC45) (Cheng et al., 2017), CIFAR10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and Birdsnap (Berg et al., 2014). For the two video datasets, we extract the middle frame of the video, as is done in Radford et al. (2021).\n\n3.2 RESULTS\n\nOur results for the base prompts setting and the full prompts setting are in Table 1. We present our method’s performance on 15 different image classification benchmarks, comparing both the classification accuracy and the number of hand-written sentence templates needed for each method. Note that for the standard method (Radford et al., 2021), the hand-written sentences refer to the image-prompts, while for CuPL the hand-written sentences refer to the LLM-prompts, with which image-prompts are generated.\n\n1. CuPL (base). In this setting, we see performance gains in 13 out of the 15 examined datasets. Note this setting uses just three hand-constructed sentence across all datasets. This is in comparison to the nearly 175 unique image-prompt templates that are hand-written across all of these datasets in the standard setting. Additionally, in the standard setting these hand-constructed prompts must be very specific to the dataset (e.g., “a black and white photo of a {}.”, “a plastic {}.”). In comparison, CuPL (base) requires only the category type of the overall dataset and still outperforms the handwritten, domain specified baseline in almost all cases. Thus, we present this base prompt setting as a simple standard that matches or exceeds prompt engineering open vocabulary models.\n\n2. CuPL (full prompts). Here we see improvements on all examined datasets. This includes large (over 1 percentage point) gains on ImageNet Top-1, DTD (texture classification), SUN397 (scene classification), FGVC Aircraft (fine-grained aircraft classification), and Flowers 102 (flower classification). While this setting requires more hand-written prompts than setting (1), it still requires significantly fewer than the baseline method (5 sentences versus 80 sentence for ImageNet), and does not include knowledge about the image domain. The full list of hand-constructed sentences for CuPL (full prompts) and the baseline method (Radford et al., 2021) can be found in Section A of the Appendix.\n\n3.3 ANALYSIS AND ABLATIONS\n\nModel Size. In Figure 4, we show CuPL (full prompts) at different model scales. As there are two different zero-shot models in the CuPL algorithm, we show the effects of varying each model\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Performance of CuPL as models scale. (Left) ImageNet Top-1 accuracy for various scales of CLIP. CuPL prompts remain consistently better than standard prompts even we adjust CLIP model size (ViT-B/32, ViT-B/16, ViT-L/14). GPT-3 model set as DaVinci-002. (Right) ImageNet Top-1 accuracy for various scales of GPT-3 (ada, babbage, curie, davinci-002). Larger models produce higher accuracy. CLIP model set as ViT-L/14.\n\nindividually. On the left hand side, we vary the CLIP model used while holding the LLM constant. We see consistent gains across all model sizes. On the right hand side, we vary the size of the LLM. We plot the accuracy of the baseline as well, which does not vary as it does not utilize an LLM. We find larger models lead to higher accuracy, though the 2nd and 3rd largest models perform similarly.\n\nNumber of Prompts. In Figure 6, we present ablations on the number of LLM-prompts and image-prompts for CuPL (full prompts). On the left side, we show ImageNet accuracy as we increase the number of LLM-prompts. This also corresponds to the number of sentences that have to be hand-written. Notably, this methods outperforms the baseline even when using prompts generated from a single handwritten sentence. On the right hand side, we hold the number of LLM-prompts constant at 5 and adjust how many image-prompts we generate per LLM-prompt. We plot the accuracy given the total number of image-prompts (so 10 generated image-prompt per LLM-prompt corresponds to 50 total image-prompts). We see that CuPL begins to outperform the baseline at just 25 image-prompts, well below the 80 imageprompts used in the baseline.\n\nFigure 5: Effect of LLM temperature. More prompt diversity leads to higher performance.\n\nDiversity of Prompts. We also examine the impact of the diversity of image-prompts on ImageNet accuracy. We adjust this parameter by changing the temperature of the GPT-3 model. This value changes the likelihood of selecting lower probability tokens and makes sentences more diverse from each other. As demonstrated in Figure 5, more diverse prompts lead to higher ImageNet accuracy. Note these comparisons are done with a single LLM-prompt to save computational cost.\n\nWordNet Definitions and Wikipedia Descriptions. We also consider two additional methods of obtaining descriptive sentences for each ImageNet category, other than using an LLM. Firstly, we compare CuPL (full) image-prompts with image-prompts generated using definitions of each ImageNet category. Because each ImageNet category is derived from the WordNet database (Miller, 1995), we can use the WordNet definition of each word.\n\nWe preprocess these definitions so they are of the form “A(n) {} is a ...” as not all WordNet definitions contain the name of the word itself. We also add a period to the end of each definition, as we find this increases performance. As shown in Table 2, ImageNet Top-1 accuracy with WordNet\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Ablation on number of LLM-prompts (left) and image-prompts (right). (Left) As number of hand-written LLM-prompts increases, so does accuracy. 10 image-prompts are generated for each LLM-prompt. Note that CuPL outperforms the baseline even with just one hand-written sentence. We add the prompts in a greedy manner, at each step adding the 10 prompts which lead to the largest performance gain. (Right) We adjust the number of image-prompts generated by a fixed number (5) of LLM-prompts. Even at 5 Image-prompts per LLM-prompt (25 prompts total), we outperform the baseline which uses 80 image-prompts.\n\ndefinition prompts is below that of CuPL or standard prompts. In addition to lower accuracy, this method uses significantly more hand-constructed sentences as it requires 1000 unique hand-written definitions compared to 175 unique hand-written image-prompt templates for the standard method and 45 unique hand-written LLM-prompt templates for CuPL (full).\n\nImageNet Top-1 accuracy for Table 2: different methods of generating imageprompts.\n\nSecondly, we compare against prompts generated from Wikipedia articles corresponding to each ImageNet category, as collected in Bujwid & Sullivan (2021b). Note that the Wikipedia article does not always exactly match the natural language name of the class used by Radford et al. (2021). Additionally, 80 categories map to more than one Wikipedia article (e.g. the category associated with the natural language word “patio” is mapped to the articles for “patio” and “terrace”). In this case, we select the first associated article. We preprocess these by removing the first line (the name of the article), and then extracting the first sentence, including the final period. We find both of these preprocessing steps lead to increase in accuracy. We also truncate this sentence to the maximum allowed input length of CLIP. For the 24 ImageNet categories that do not have an associated Wikipedia page, we use the name of the category as the image-prompts. As shown in Table 2, we find Wikipedia to be less effective than standard prompts, CuPL prompts, or WordNet definitions.\n\nStandard CuPL WordNet Wiki\n\n68.20\n\n75.54\n\n73.44\n\n76.69\n\nEnsembling with Standard Prompts. We also consider using LLM generated prompts from CuPL (full) in addition to hand-written prompts. We do this by averaging together all the text embeddings of the CuPL prompts and hand-written prompts. As shown in Table 3, we find that for some datasets, ensembling both types of prompts outperforms CuPL prompts on their own, while for others CuPL prompts perform better. For all datasets, this ensemble performs better than standard prompts alone. However, this ensembling method requires all the hand-written effort and domain knowledge of the standard approach.\n\nAnalysis of Accuracy Gains. In addition to total accuracy gains, we present the per class accuracy shift between the image-prompts used in Radford et al. (2021) and CuPL, shown in Figure 7. As demonstrated, the accuracy gains seen in CuPL are not distributed uniformly through the ImageNet classes, with some classes seeing ∼40 percentage point accuracy gains, and others seeing ∼40 percentage point accuracy losses when compared against class accuracy with standard prompts. In other words, while CuPL sees a higher accuracy overall when compared to the standard method, the images which are correctly predicted by the standard prompts are not a subset of the images\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Performance of the ensemble of CuPL (full) and the standard, hand-written prompts in CLIP (Radford et al., 2021). This ensemble outperforms the standard hand-written prompts for all examined datasets (difference shown with ∆ std), and outperforms CuPL (full) for 11 datasets (difference shown with ∆ CuPL)\n\nt e\nN e\ng a\n\nm\n\nI\n\nD T\nD\n\ns r\na C\nd r\no f\nn a\nt\n\nS\n\n7 9\n3 N\nU S\n\n1 0\n1 d\no o\nF\n\nt f\na r\nc r\ni\n\nA C\nV G\nF\n\ns t\ne P\n\nd r\no f\nx O\n\n1 0\n1 h\nc e\nt l\na C\n\n2 0\n1\n\ns r\ne w\no\n\nl\n\nF\n\n1 0\n1 F\nC U\n\n0 0\n7 -\ns c\ni t\ne n\nK\n\ni\n\n5 4\nC S\n\nI\n\nS E\nR\n\n0 1\n-\n\nR A\nF\n\nI\n\nC\n\n0 0\n1 -\n\nR A\nF\n\nI\n\nC\n\np a\nn s\nd r\ni\n\nB\n\nn a\ne\n\nm\n\nEnsemble 76.51 61.60 77.66 73.51 93.42 36.47 93.71 93.87 79.73 78.16 61.50 73.03 95.88 79.33 51.09 75.03\n\n∆ std ∆ CuPL\n\n+0.97 -0.18\n\n+6.40 -0.1\n\n+0.13 +0.03\n\n+4.20 +0.20\n\n+0.34 +0.06\n\n+3.59 +0.36\n\n+0.38 -0.10\n\n+0.63 +0.42\n\n+1.20 +0.06\n\n+0.71 +0.20\n\n+1.43 +0.87\n\n+1.93 +1.34\n\n+0.29 +0.04\n\n+1.07 +0.76\n\n+0.66 -0.02\n\nwhich are correctly predicted by CuPL. In fact CuPL sees just over a 1 percentage point gain when compared to standard prompts, but differs in it’s predictions from the standard method for 11.50% of predictions (with CuPL correct for 4.48% of these, standard correct for 3.32%, and neither correct for 3.70%).\n\nFigure 7 also shows the classes with the 20 greatest accuracy gains and losses when comparing class accuracy with standard image-prompts (Radford et al., 2021) versus with CuPL image-prompts. Interestingly, for many of the classes which see a large accuracy gain, we see a corresponding class with a large accuracy loss that is either similar to the initial class or likely to co-occur with it (e.g. agaric/mushroom, academic gown/graduation cap, military uniform/Pickelhaube, desk/monitor).\n\n4 RELATED WORK\n\n4.1 NATURAL LANGUAGE DESCRIPTIONS FOR IMAGE CLASSIFICATION\n\nSeveral prior works use text-based knowledge of image categories to improve classification accuracy. Elhoseiny et al. (2017) extract visual information from unstructured text descriptions collected from the internet to recognize parts of object and classify them in a zero-shot way. Reed et al. (2016) and He & Peng (2017) use natural language descriptions of bird types to train a multimodal classification model. Huang et al. (2021) use hand-collected attribute tags to attend over relevant features in images. Paz-Argaman et al. (2020) extract visual information from Wikipedia descriptions to enable zero-shot bird classification. Additional works (Shen et al., 2022; Bujwid & Sullivan, 2021a) show improvements on large datasets (e.g., ImageNet) using external information from external databases such as Imagenet-wiki and Wordnet. While these works show the effectiveness of augmenting zero-shot models with descriptive text, all of these prior works rely on external natural language databases for descriptions. This often limits the possible categories that can be classified and can require extensive preprocessing to extract visual descriptions from noisy natural language.\n\n4.2 GENERATED TEXT FOR DOWNSTREAM TASKS\n\nRecent work has utilized text generated from LLMs in a number of ways. Santurkar et al. (2022) use an LLM to paraphrase existing image captions to use as data augmentation for CLIP. Liu et al. (2022) use GPT-3 to generate knowledge on a topic when given a number of demonstrations, which is then used to improve accuracy on common sense reasoning questions. Hu et al. (2022) use a LLM to add labels to text to improve text classification accuracy. In Yu et al. (2022b), the outputs of a GPT-2 model are used to train an encoder on top of a vision model to generate multimodal image representations for a variety of tasks. Su et al. (2022) utilize a language model to perform image captioning by iteritively generating candidate image captions with a LLM and then using feedback from an open vocabulary model to align it to a given image. Similarly, Yang et al. (2022) use GPT-3 along with text descriptions of images for the Visual Question Answering (VQA) task. However, unlike CuPL these prior works are either purely language tasks (common sense reasoning, text classification) or multimodal with some language component (image captioning, VQA). In our work, we demonstrate how LLM generated text can be used to improve purely visual image classification tasks across a number of benchmarks.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Distribution of ImageNet per class accuracy difference of CuPL image-prompts versus standard image-prompts. As shown, the accuracy gains of CuPL are not uniform across all classes. Rather, we see large gains for some classes, and losses for others. In addition, we list the classes which see the largest accuracy gains when switching to CuPL prompts (with “mushroom” having the largest gain), and the 20 classes with the largest accuracy losses (with “canoe” having the largest loss.)\n\n4.3 PROMPT ENGINEERING\n\nPrevious efforts have explored methods for obtaining successful natural language prompts. For both open vocabulary image classification models as well as LLMs, the format of prompts is known to highly affect accuracy (Schick & Sch ̈utze, 2021; Radford et al., 2021; Brown et al., 2020; Gao et al., 2020). This has led to a large effort to find optimal prompt formats. Proposed methods include crowd-sourcing high performing prompts (Bach et al., 2022) as well as framing prompts to induce models to give explanations as well as answers (Wei et al., 2022; Kojima et al., 2022; Nye et al., 2021). Additional works have proposed learning prompts via gradient based methods (Zhang et al., 2021; Qin & Eisner, 2021; Li & Liang, 2021; Lester et al., 2021; Shin et al., 2020), retrieval from a database (Rubin et al., 2022), or reformatting/rephrasing existing prompts (Jiang et al., 2020; Rubin et al., 2022).\n\nMost relevant to this work are a number of methods for designing optimal prompts for zero-shot image classification with open vocabulary models. These methods learn prompts formats which yield high accuracy for image classification using either supervised (Zhou et al., 2022; Rao et al., 2022) or unsupervised (Huang et al., 2022) methods. However, unlike these prior works this work requires no additional training or labeled data.\n\n5 CONCLUSION\n\nWe demonstrate that leveraging knowledge from an LLM can immediately improve zero-shot accuracy on a variety of image classification tasks, with much less hand-engineering efforts to craft natural language prompts. Furthermore, prompts can be customized to the desired categories, rather than a general template that applies to all existing image categories. Finally, using prompts generated by LLMs lowers the barrier of prior knowledge about the dataset, which is often required when crafting prompt templates.\n\nQuerying an LLM for prompt construction is simple, straightforward and as our results suggested, immediately beneficial. The hypothesis that a joint force of LLMs and open vocabulary models would improve zero-shot image classification is thoroughly tested in this work. We hope these findings serve as a useful tool towards understanding and improving zero-shot image classification, and more generally, the consolidation of model capacities and modalities through natural language.\n\n9\n\nper class accuracy difference of CuPL vs Standard image-promptsHigher Accuracy with CuPL promptsHigher Accuracy with Standard promptsmushroom Scottish Deerhound Pickelhaube graduation cap sneaker Treeing Walker Coonhound promontory swim trunks / shorts motorboat ox lampshade collie muzzle barn spider nematode gazelle through arch bridge monitor patas monkey laptop computerSaharan horned viper Pembroke Welsh Corgi Australian Terrier vaulted or arched ceiling kit fox desk academic gown cassette player reflex camera tusker Australian Silky Terrier military uniform trimaran Groenendael dog flatworm scuba diver Siberian Husky frilled-necked lizard agaric canoekit foxagaricmushroomgazelleUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY\n\nWe have a number of measures to ensure the reproducibility of this work. First, in the supplementary material we include the code to generate image-prompts for ImageNet and evaluate the accuracy of these prompts. In Section 2, we note all hyperparameters used for the LLM. Additionally, in the appendix we include all LLM-prompts used to generate image-prompts for each of the 15 datasets. In the supplementary material, we include all generated image-prompts for all dataset, for both CuPL (base) and CuPL (full).\n\nREFERENCES\n\nStephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M SAIFUL BARI, Thibault F ́evry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts. ArXiv, abs/2202.01279, 2022.\n\nThomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2011–2018, 2014.\n\nLucas Beyer, Olivier J H ́enaff, Alexander Kolesnikov, Xiaohua Zhai, and A ̈aron van den Oord. Are\n\nwe done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-\n\nponents with random forests. In European Conference on Computer Vision, 2014.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nSebastian Bujwid and Josephine Sullivan. Large-scale zero-shot image classification from rich and\n\ndiverse textual descriptions. ArXiv, abs/2103.09669, 2021a.\n\nSebastian Bujwid and Josephine Sullivan. Large-scale zero-shot image classification from rich and\n\ndiverse textual descriptions. arXiv preprint arXiv:2103.09669, 2021b.\n\nJoao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700\n\nhuman action dataset. arXiv preprint arXiv:1907.06987, 2019.\n\nGong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Bench-\n\nmark and state of the art. Proceedings of the IEEE, 105(10):1865–1883, 2017.\n\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\n\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nMohamed Elhoseiny, Yizhe Zhu, Han Zhang, and A. Elgammal. Link the head to the ”beak”: Zero shot learning from noisy text description at part precision. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6288–6297, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pp. 178–178. IEEE, 2004.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\n\nlearners. arXiv preprint arXiv:2012.15723, 2020.\n\nXiangteng He and Yuxin Peng. Fine-grained image classification via combining vision and language. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7332– 7340, 2017.\n\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juan-Zi Li, and Maosong Sun. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. In ACL, 2022.\n\nSiteng Huang, Min Zhang, Yachen Kang, and Donglin Wang. Attributes-guided and pure-visual\n\nattention alignment for few-shot recognition. In AAAI, 2021.\n\nTony Huang, Jack Chu, and Fangyun Wei. Unsupervised prompt learning for vision-language mod-\n\nels. arXiv preprint arXiv:2204.03649, 2022.\n\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021.\n\nZhengbao Jiang, Frank F. Xu, J. Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n\nlanguage models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained In 4th International IEEE Workshop on 3D Representation and Recognition\n\ncategorization. (3dRR-13), Sydney, Australia, 2013.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n\ntuning. ArXiv, abs/2104.08691, 2021.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), abs/2101.00190, 2021.\n\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and In ACL,\n\nHannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. 2022.\n\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of\n\naircraft. Technical report, 2013.\n\nGeorge A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):\n\n39–41, 1995.\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722–729. IEEE, 2008.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012\n\nIEEE conference on computer vision and pattern recognition, pp. 3498–3505. IEEE, 2012.\n\nTzuf Paz-Argaman, Yuval Atzmon, Gal Chechik, and Reut Tsarfaty. Zest: Zero-shot learning from\n\ntext descriptions using textual similarity and visual summarization. In FINDINGS, 2020.\n\nHieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classification. arXiv preprint arXiv:2111.10050, 2021.\n\nGuanghui Qin and Jas’ Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n\nArXiv, abs/2104.06599, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nYongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18082–18091, 2022.\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers In International Conference on Machine Learning, pp. 5389–5400.\n\ngeneralize to imagenet? PMLR, 2019.\n\nScott E. Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of fine-grained visual descriptions. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 49–58, 2016.\n\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context\n\nlearning. In NAACL, 2022.\n\nShibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. Is a caption worth a thousand images? a controlled study for representation learning. arXiv preprint arXiv:2207.07635, 2022.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and\n\nnatural language inference. In EACL, 2021.\n\nSheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Anna Rohrbach, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, Kurt Keutzer, Trevor Darrell, and Jianfeng Gao. K-lite: Learning transferable visual models with external knowledge. ArXiv, abs/2204.09222, 2022.\n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. ElicArXiv,\n\niting knowledge from language models using automatically generated prompts. abs/2010.15980, 2020.\n\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\n\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n\nYixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier. Language models can see: Plugging visual controls in text generation. ArXiv, abs/2205.02655, 2022.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(11), 2008.\n\nBen Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model,\n\n2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959–7971, 2022.\n\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485–3492. IEEE, 2010.\n\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang.\n\nAn empirical study of gpt-3 for few-shot knowledge-based vqa. In AAAI, 2022.\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui arXiv preprint\n\nCoca: Contrastive captioners are image-text foundation models.\n\nWu. arXiv:2205.01917, 2022a.\n\nYoungjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, and Yejin Choi. Multimodal knowledge alignment with reinforcement learning. ArXiv, abs/2205.12630, 2022b.\n\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. ArXiv, abs/2108.13161, 2021.\n\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n\nlanguage models. International Journal of Computer Vision, pp. 1–12, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX: What does a platypus look like? Generating customized prompts for zero-shot image classification\n\nOVERVIEW\n\nA. CuPL (Full Prompts) vs Standard Prompts\n\nB. CuPL Base Prompts\n\nC. Evaluation Metric\n\nD. Open-Source LLM\n\nE. Single Sentence Baseline\n\nF. Robustness\n\nG. CuPL Improvement Analysis\n\nH. Error Analysis\n\nI. Image-Prompt Distribution\n\nJ. Tempurature Analysis\n\nK. Example Generated image-prompts\n\nA CUPL (FULL PROMPTS) VS STANDARD PROMPTS\n\nWe detail the hand-written prompt templates used for CuPL (full prompts) versus standard CLIP Radford et al. (2021) prompt templates. For CuPL, hand-written prompt templates are needed for the LLM-prompts, while for the standard method hand-written prompt templates are needed for the image-prompts.\n\nNote that many of the hand-written templates for the standard method encode information about the datasets. For example, ”a toy {}” demonstrates knowledge that objects are sometimes represented as a toy version of an object rather than as the literal object. CuPL prompts remain much more general (e.g. ”Describe what a {} looks like”).\n\nCaltech101 CuPL hand-written\n\nDescribe what a(n) {} looks like: Describe a(n) {}: What are the identifying characteristics of a(n) {}?\n\nStandard hand-written\n\na photo of a {}. a painting of a {}. a plastic {}. a sculpture of a {}. a sketch of a {}. a tattoo of a {}. a toy {}. a rendition of a {}. a embroidered {}. a cartoon {}. a {} in a video game. a plushie {}. a origami {}. art of a {}. graffiti of a {}. a drawing of a {}. a doodle of a {}.\n\na photo of the {}. a painting of the {}. the plastic {}. a sculpture of the {}. a sketch of the {}. a tattoo of the {}. the toy {}. a rendition of the {}. the embroidered {}. the cartoon {}. the {} in a video game. the plushie {}. the origami {}. art of the {}. graffiti of the {}. a drawing of the {}. a doodle of the {}.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFood101 CuPL hand-written\n\nDescribe what {} looks like Visually describe {} How can you tell that the food in this photo is {}?\n\nStandard hand-written\n\na photo of {}, a type of food.\n\nStanford Cars CuPL hand-written\n\nHow can you identify a(n) {}? Description of a(n) {}, a type of car. A caption of a photo of a(n) {}: What are the primary characteristics of a(n) {}? Description of the exterior of a(n) {} What are the identifying characteristics of a(n) {}, a type of car? Describe an image from the internet of a(n) {} What does a(n) {} look like? Describe what a(n) {}, a type of car, looks like\n\nStandard hand-written\n\na photo of a {}. a photo of the {}. a photo of my {}. i love my {}! a photo of my dirty {}. a photo of my clean {}. a photo of my new {}. a photo of my old {}.\n\nOxford Pets CuPL hand-written\n\nDescribe what a pet {} looks like Visually describe a(n) ‘{}’, a type of pet.\n\nStandard hand-written\n\na photo of a , a type of pet.\n\nImageNet\n\nCuPL hand-written\n\nDescribe what a(n) {} looks like How can you identify a(n) {}? What does a(n) look like? Describe an image from the internet of a(n) {} A caption of an image of a(n) {}:\n\nStandard hand-written\n\na bad photo of a {}. a photo of many {}. a sculpture of a {}. a photo of the hard to see {}. a low resolution photo of the {}. a rendering of a {}. graffiti of a {}. a bad photo of the {}. a cropped photo of the {}. a tattoo of a {}. the embroidered {}. a photo of a hard to see {}. a bright photo of a {}. a photo of a clean {}. a photo of a dirty {}. a dark photo of the {}. a drawing of a {}. a photo of my {}. the plastic {}. a photo of the cool {}. a close-up photo of a {}. a black and white photo of the {}. a painting of the {}. a painting of a {}. a pixelated photo of the {}. a sculpture of the {}. a bright photo of the {}. a cropped photo of a {}. a plastic {}. a photo of the dirty {}. a jpeg corrupted photo of a {}. a blurry photo of the {}. a photo of the {}. a good photo of the {}. a rendering of the {}. a {} in a video game. a photo of one {}. a doodle of a {}. a close-up photo of the {}. a photo of a {}\n\n15\n\nthe origami {}. the {} in a video game. a sketch of a {}. a doodle of the {}. a origami {}. a low resolution photo of a {}. the toy {}. a rendition of the {}. a photo of the clean {}. a photo of a large {}. a rendition of a {}. a photo of a nice {}. a photo of a weird {}. a blurry photo of a {}. a cartoon {}. art of a {}. a sketch of the {}. a embroidered {}. a pixelated photo of a {}. itap of the {}. a jpeg corrupted photo of the {}. a good photo of a {}. a plushie {}. a photo of the nice {}. a photo of the small {}. a photo of the weird {}. the cartoon {}. art of the {}. a drawing of the {}. a photo of the large {}. a black and white photo of a {}. the plushie {}. a dark photo of a {}. itap of a {}. graffiti of the {}. a toy {}. itap of my {}. a photo of a cool {}. a photo of a small {}. a tattoo of the {}.\n\nUnder review as a conference paper at ICLR 2023\n\nFGVC Aircraft CuPL hand-written\n\nStandard hand-written\n\nDescribe a(n) {} aircraft Describe the {} aircraft\n\na photo of a {}, a type of aircraft. a photo of the {}, a type of aircraft.\n\nDTD CuPL hand-written\n\nWhat does “{}” material look like? What does a “{}” surface look like? What does a “{}” texture look like? What does a “{}” object look like? What does a “{}” thing look like? What does a “{}” pattern look like?\n\nStandard hand-written\n\na photo of a {} texture. a photo of a {} pattern. a photo of a {} thing. a photo of a {} object. a photo of the {} texture. a photo of the {} pattern. a photo of the {} thing. a photo of the {} object.\n\nSUN397 CuPL hand-written\n\nStandard hand-written\n\nDescribe what a(n) {} looks like How can you identify a(n) {}? Describe a photo of a(n) {}\n\na photo of a {}. a photo of the {}.\n\nKinetics-700 CuPL hand-written\n\nDescribe the action ”{}” What does a person {} look like? What does the act of {} look like? Describe ”{}”\n\nStandard hand-written\n\na photo of a person {}. a photo of a person using {}. a photo of a person doing {}. a photo of a person during {}. a photo of a person performing {}. a photo of a person practicing {}. a video of {}. a video of a person {}. a video of a person using {}. a video of a person doing {}. a video of a person during {}. a video of a person performing {}. a video of a person practicing {}. a example of {}. a example of a person {}. a example of a person using {}. a example of a person doing {}. a example of a person during {}. a example of a person performing {}. a example of a person practicing {}. a demonstration of {}. a demonstration of a person {}. a demonstration of a person using {}. a demonstration of a person doing {}. a demonstration of a person during {}. a demonstration of a person performing {}. a demonstration of a person practicing {}.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nUCF 101 CuPL hand-written\n\nDescribe the action of {} What does the act of {} look like? What does a person doing {} look like? Describe “{}“ Describe the action “{}“\n\nStandard hand-written\n\na photo of a person {}. a video of a person {}. a example of a person {}. a demonstration of a person {}. a photo of the person {}. a video of the person {}. a example of the person {}. a demonstration of the person {}. a photo of a person using {}. a video of a person using {}. a example of a person using {}. a demonstration of a person using {}. a photo of the person using {}. a video of the person using {}. a example of the person using {}. a demonstration of the person using {}. a photo of a person doing {}. a video of a person doing {}. a example of a person doing {}. a demonstration of a person doing {}. a photo of the person doing {}. a video of the person doing {}. a example of the person doing {}. a demonstration of the person doing {}. a photo of a person during {}. a video of a person during {}. a example of a person during {}. a demonstration of a person during {}. a photo of the person during {}. a video of the person during {}. a example of the person during {}. a demonstration of the person during {}. a photo of a person performing {}. a video of a person performing {}. a example of a person performing {}. a demonstration of a person performing {}. a photo of the person performing {}. a video of the person performing {}. a example of the person performing {}. a demonstration of the person performing {}. a photo of a person practicing {}. a video of a person practicing {}. a example of a person practicing {}. a demonstration of a person practicing {}. a photo of the person practicing {}. a video of the person practicing {}. a example of the person practicing {}. a demonstration of the person practicing {}.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nRESISC 45 CuPL hand-written\n\nDescribe a satellite photo of a(n) {} Describe a(n) {} as it would appear in an aerial image How can you identify a(n) {} in an aerial photo? Describe the satellite photo of a(n) {} Describe an aerial photo of a(n) {}\n\nStandard hand-written\n\nsatellite imagery of {}. aerial imagery of {}. satellite photo of {}. aerial photo of {}. satellite view of {}. aerial view of {}. satellite imagery of a {}. aerial imagery of a {}. satellite photo of a {}. aerial photo of a {}. satellite view of a {}. aerial view of a {}. satellite imagery of the {}. aerial imagery of the {}. satellite photo of the {}. aerial photo of the {}. satellite view of the {}. aerial view of the {}.\n\nBirdsnap CuPL hand-written\n\nDescribe what the bird {} looks like: Describe the bird {}: What are the identifying characteristics of the bird {}?\n\nStandard hand-written\n\na photo of a {}, a type of bird.\n\nFlowers 102 CuPL hand-written\n\nDescribe how to identify a(n) {}, a type of flower What does a(n) {} flower look like?\n\nStandard hand-written\n\na photo of a {}, a type of flower.’\n\nCIFAR-10 CuPL hand-written\n\nDescribe what a(n) {} looks like Describe a(n) {}: What are the identifying characteristics of a(n) {}?\n\nStandard hand-written\n\na photo of a {}. a blurry photo of a {}. a black and white photo of a {}. a low contrast photo of a {}. a high contrast photo of a {}. a bad photo of a {}. a good photo of a {}. a photo of a small {}. a photo of a big {}. a photo of the {}. a blurry photo of the {}. a black and white photo of the {}. a low contrast photo of the {}. a high contrast photo of the {}. a bad photo of the {}. a good photo of the {}. a photo of the small {}. a photo of the big {}.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nCIFAR-100 CuPL hand-written\n\nDescribe a photo of a(n) {}: What are the identifying characteristics of a(n) {}? Describe what a(n) {} looks like: Describe a(n) {}:\n\nStandard hand-written\n\na photo of a {}. a blurry photo of a {}. a black and white photo of a {}. a low contrast photo of a {}. a high contrast photo of a {}. a bad photo of a {}. a good photo of a {}. a photo of a small {}. a photo of a big {}. a photo of the {}. a blurry photo of the {}. a black and white photo of the {}. a low contrast photo of the {}. a high contrast photo of the {}. a bad photo of the {}. a good photo of the {}. a photo of the small {}. a photo of the big {}.\n\nB CUPL BASE PROMPTS\n\nThe three general sentences used in the base prompt setting are:\n\nDescribe what a/the\n\nlooks like:\n\nDescribe a/the What are the identifying characteristics of a/the\n\n:\n\n?\n\nHere we specify the type filled in for each of the examined datasets, as well as the article used for that dataset (‘a(n)’ or ‘the’):\n\nDataset\n\nBase LLM-prompt type specification\n\nImageNet DTD StanfordCars SUN397 Food 101 FGVC Aircraft Oxford Pets Caltech101 CIFAR-10 CIFAR-100 Flowers 102 Kinetics-700 UCF101 RESISC45 Birdsnap\n\na(n) {} the texture {} the car {} a(n) {} the food {} the aircraft {} a pet {} a(n) {} a(n) {} a(n) {} the flower {} the action of {} the action of {} a satellite photo of {} the bird {}\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC EVALUATION METRIC\n\nt e\nN e\ng a\n\nm\n\nI\n\nD T\nD\n\ns r\na C\nd r\no f\nn a\nt\n\nS\n\n7 9\n3 N\nU S\n\n1 0\n1 d\no o\nF\n\nt f\na r\nc r\ni\n\nA C\nV G\nF\n\ns t\ne P\n\nd r\no f\nx O\n\n1 0\n1 h\nc e\nt l\na C\n\n2 0\n1\n\ns r\ne w\no\n\nl\n\nF\n\n1 0\n1 F\nC U\n\n0 0\n7 -\ns c\ni t\ne n\nK\n\ni\n\n5 4\nC S\n\nI\n\nS E\nR\n\n0 1\n-\n\nR A\nF\n\nI\n\nC\n\n0 0\n1 -\n\nR A\nF\n\nI\n\nC\n\np a\nn s\nd r\ni\n\nB\n\nAcc. Acc. Acc. Acc. Acc. Mean Mean Mean Mean Acc. Mean Acc. Acc. Acc. Acc. per class\n\n(top1, top5)\n\nper class\n\nper class\n\nper class\n\nD OPEN-SOURCE LLM\n\nTable 4: CuPL with an open-source model. CuPL is able to improve over hand-written baselines even for smaller open-source models.\n\nWhile GPT-3 (Brown et al., 2020) demonstrates higher performance on a number of tasks compared to smaller open-source models, open-source models are sometime more accessible. We therefore show improvement using GPT-J-6B (Wang & Komatsuzaki, 2021), a small open-source model available on HuggingFace (Wolf et al., 2019). We find that we are able to surpass human written prompts with prompts generated by this model as shown in Table, though we still fall short of those generated by GPT-3. Additionally, we employ a number of strategies to increase the accuracy of the lower quality GPT-J-6B generations. First, we generate at a lower temperature (0.3) to prevent irrelevant or nonsensical generations, which we find occur more frequently in smaller models. Additionally, we generate 5 times more Image-prompts per LLM-prompt than we do when using GPT-3 (Brown et al., 2020). We also add punctuation to the end of all LLM-prompts to encourage the LLM to begin new sentences. Finally, we filter out any Image-prompts which do not contain the name of the ImageNet category they are meant to describe, as well as remove a number of unicode characters from the generations (e.g. ‘u2019’). We present these findings as a way to make CuPL a more accessible options until large high performance models become available to the public.\n\nstandard CuPL (GPT-J-6B) CuPL (GPT-3)\n\nImageNet 75.54 75.62 76.69\n\nE SINGLE SENTENCE BASELINE\n\nTable 5: Single sentence baselines. Comparison of a single hand-written Imageprompt template with a single handle written LLM-prompt as well as a single CuPL generated Image-prompt.\n\nOne of the primary benefits of CuPL is that it decreases the amount of necessary hand-engineering. However, this could also be done by decreasing the number of total hand-written templates used, which comes at a loss in performance. We present this as a ‘low effort’ baseline, where we use only the hand constructed template of ‘a photo of a {}’. We compare this with two CuPL baselines. The first is the baseline in which we also only construct one hand written template: ‘Describe what a {} looks like’. We then use this to generate 10 Image-prompts. The second baseline is a single CuPL generated sentence, generated with the prompt ‘Describe what a {} looks like’. For this experiment, we generate at a temperature of 0.3 as we find that higher tempuratures are only helpful when we are able to ensemble many diverse prompts, not when we are limited to one. We find that CuPL outperforms a single hand-written template under both of these settings, as shown in Table 5.\n\na photo of a {} CuPL (1 hand-written) CuPL (1 generated)\n\nImageNet 73.46 75.71 74.24\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nF ROBUSTNESS\n\nIn addition to the previously mentioned benefits of open vocabulary models, one of the important advances made by CLIP (Radford et al., 2021) is an increased robustness on out-of-distribution data. Fine-tuning has been shown to degrade performance on out-of-distribution tasks (Wortsman et al., 2022), however zero-shot CLIP is robust to these distribution shifts. We show improvement on two common distribution shifts in Table 6, demonstrating that CuPL maintains the robustness of CLIP.\n\nTable 6: Robustness of CuPL. CuPL accuracy on two common ImageNet variants, using CuPL ImageNet Image-prompts. CuPL improves performance on both of these variants, demonstrating that CuPL improves accuracy on in-distribution tasks, while maintaining robustness to distribution shifts.\n\nImageNet (Deng et al., 2009) 75.54 76.69\n\nImageNet-V2 (Recht et al., 2019) 69.86 70.85\n\nImageNet-Sketch (Wang et al., 2019) 59.60 60.05\n\nStandard CuPL\n\nG CUPL IMPROVEMENT ANALYSIS\n\nG.1 VISUAL SIMILARITY ANALYSIS\n\nIn Figure 7, we provide initial analysis on the categories where the CuPL algorithm improves the most over standard hand-written prompts. We find that the improvement is not uniformly distributed, but rather some classes see a large improvement, while others see a decrease in per class accuracy. Interestingly, there are often two similar categories where one sees a large increase in accuracy and the other sees a decrease. For example, the ‘mushroom’ class has an approximately 40pp increase, while ‘agaric’ (a subclass of mushroom) is one of the classes with the largest drop in accuracy.\n\nIn order to better understand this phenomenon, we examine the change in accuracy between CuPL and the standard method of prompting in the image embedding space. Thus we are able to visualize the close relationship between categories like ‘agaric’ and ‘mushroom’. In order to be able to visualize the high dimensional CLIP image embedding in two dimensions, we utilize the t-distributed stochastic neighbor embedding algorithm (Van der Maaten & Hinton, 2008). Figure 8 visualizes image features (reduced into two dimensions) in relation to CuPL improvement.\n\nAs was suggested by Figure 7, we see in Figure 8 that when there is a class that has a large increase in accuracy with CuPL prompts (‘mushroom’, ‘graduation cap’, ‘monitor’) there is often a decrease in class accuracy for a visually related class. This means that when choosing between two similar or co-occuring classes, CuPL has a different distribution of classification than the standard method (e.g. the standard method prefers ‘canoe’ over ‘paddle’ much more strongly than CuPL). This suggests that the overall accuracy improvement of CuPL over the standard method may come (at least in part) from better distinguishing between two visually similar classes. While it may be over-correcting from the mistakes of the standard method (as demonstrated by the drop in accuracy in one of the two similar classes), the CuPL predictions appear to be overall more accurate, as demonstrated by the overall higher accuracy.\n\nG.2 CO-OCCURRENCES BETWEEN OBJECTS\n\nMany of the frequently confused pairs in Figure 8 are objects that are likely to occur in an image (i.e. ‘canoe’-‘paddle’ or ’graduation cap’-‘academic gown’ or ’monitor’-‘desk’). One potential benefit of CuPL captions is that they are able to capture co-occurrences as well. For example, one CuPL prompt for the ‘canoe’ class is A canoe is typically a narrow boat with pointed ends that is propelled with a paddle. This caption contains the word ‘paddle’ which is frequently confused with ‘canoe’ and likely to be present in images, even where the correct label is ‘canoe’. We therefore investigate the effectiveness of CuPL captions on images which contain more than one ImageNet object.\n\nWe attain this by using the ImageNet-ReaL dataset (Beyer et al., 2020) which relabels ImageNet images with all applicable labels, so an image with both a ‘canoe’ and a ‘paddle’ would have both\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Visualizing of image embedding of ImageNet classes compared to CuPL improvement on that class. Each point on this figure represents the average image embedding of an ImageNet class, which has been reduced to two dimentions using t-sne (Van der Maaten & Hinton, 2008). We see that when there is a class with a large improvement compared to the baseline, it is often visually similar to a class which has a decrease in accuracy. This suggests that CuPL’s improved accuracy may be due in part to an increased ability to distinguish similar classes compared to the baseline.\n\nlabels. We then tag images as having multiple ImageNet objects or only one ImageNet object based on the ReaL dataset. Finally, we compute ImageNet accuracy across each of these two sets (using standard ImageNet labels). Results are given in Table 7.\n\nTable 7: Standard versus CuPL accuracy based on number of ImageNet classes present in image. We use the ImageNet-ReaL dataset (Beyer et al., 2020) to find images which have more than one applicable ImageNet label. We then present the accuracy for standard prompts and CuPL prompts using standard ImageNet labels, split by images which contain only one possible ImageNet class and images which may contain multiple classes.\n\nOne class present (85.1% of ims) Multiple classes present (14.9% of ims)\n\nStandard CuPL\n\n79.89 80.79\n\n51.59 53.58\n\nH ERROR ANALYSIS\n\nIn Figure 9, we present an error analysis of our model using two different metrics. The first is an analysis between the model prediction and the correct class using the visual similarity of these labels. To capture this, we first attain an average visual embedding of each class by taking the mean of each image in that class and then normalizing that mean. Then for each class we rank how similar each of the other 999 classes are by the distance between these embeddings. If the models makes an incorrect prediction, but it predicts the class with the closest embedding to the correct label, then we refer to this as an image offset of 1.\n\nAdditionally, we examine the prediction errors in terms of the linguistic similarity of the labels. We do this with the WordNet (Miller, 1995) similarity of two labels. For example, if the label of the prediction and the ground-truth label share the same parent in the WordNet tree, that is a WordNet offset of 2.\n\nWhile slight, there is a difference in the errors made by CuPL compared to the errors made by the baseline as shown in Figure 9. CuPL is more likely to have an error that has an image offset of 1\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Error analysis of CuPL and baseline comparing models errors visually and linguistically to ground truth labels. We compare the errors made by CuPL to the errors made by the baseline methods. We find that the errors made by CuPL are more likely to be the most visually similar class to the ground truth label when compared to the errors made by the baseline. However, the errors made by the baseline are more likely to be the most linguistically similar class according to the WordNet (Miller, 1995) heirarchy. This suggests that visual information is being extracted from the CuPL descriptions to make visually consistent predictions.\n\nthan the baseline. However, the baseline is more likely to have an error that has a WordNet offset of 1 than CuPL. This implies that CuPL may be taking advantage of the visually descriptive language of the captions, as even when the model makes errors, they tend to favor categories that are visually similar to the ground truth. However, the baseline method does not have visual descriptions in its Image-prompts which may lead to its errors aligning more linguistically with the ground-truth.\n\nFigure 10: Visualization of embeddings of Image-prompts generated with various LLMprompts. Each point represents the image embedding of one Image-prompts for the stated category with has been reduced to two dimensions using t-sne (Van der Maaten & Hinton, 2008). Imageprompts with the same color are generated by the same LLM-prompts.\n\nI\n\nIMAGE-PROMPT DISTRIBUTION\n\nWhen generating Image-prompts, we use an ensemble of prompts generated by different LLM- ‘What does a {} look like?). As we find that the diversity of Image-prompts is prompts (e.g. correlated with accuracy, it is valuable to understand how different LLM-prompts affect the diversity of Image-prompts. To accomplish this, we visualize the text embedding of Image-prompts for a selection of classes using t-sne (Van der Maaten & Hinton, 2008) dimension reduction. We then color Image-prompts that were generated by the same LLM-prompt. As shown in Figure 10, we\n\n23\n\nError Distance by Image OffsetError Distance by WordNet OffsetWordNet Offset from Target to PredictionImage Offset from Target to PredictionNumber of Model ErrorsPlatypusLadybugTenchTSNE of text embeddings of Image-prompts generated by different LLM-promptsUnder review as a conference paper at ICLR 2023\n\nfind that while there is a slight clustering of Image-prompts by LLM-prompts, there is also a large amount of overlap.\n\nJ TEMPERATURE ANALYSIS\n\nWe provide several further analyses of the effect of temperature in prompt generation. First, we visualize the distribution of Image-prompts that have been generated with a variety of different temperatures. We do this by selecting prompts for 3 different ImageNet classes at 3 different temperatures. We then perform dimentionality reduction on the text embeddings of these prompts in order to visualize their distribution. As shown in Figure 11, Image-prompts generated with a temperature of 0.1 are clustered in a few different locations. Image-prompts generated with a temperature of 0.5 are more widely, and Image-prompts generated with a temperature of 0.9 have a similar, but even wider distribution.\n\nAdditionally in Section K, we give all generated prompts for the ImageNet class ‘Tench’ at three different temperatures. At the lowest temperature, the generated Image-prompts are nearly identical when generated with the same LLM-prompts. As the temperature increases, so does the difference in the generated prompts.\n\nFigure 11: Visualization of embeddings of Image-prompts generated with various temperatures. Each point represents the image embedding of one Image-prompts for the stated category with has been reduced to two dimensions using t-sne (Van der Maaten & Hinton, 2008). Prompts generated with a higher temperature cover a wider distribution.\n\nK EXAMPLE GENERATED IMAGE-PROMPTS\n\nA selection of LLM-generated image-prompts for a subset of ImageNet categories. We give all 50 image-prompts for the first ImageNet category of “Tench” and then 10 randomly selected prompts for a number of randomly selected ImageNet categories.\n\nK.1 ALL GENERATED IMAGE-PROMPTS FOR “TENCH” CATEGORY\n\nTemperature = 0.99\n\n\"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish that is typically brown or olive in color.\", \"A tench is a fresh water fish that can grow up to 2 feet in length.\", \"A tench is a freshwater fish of the family Cyprinidae.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish with a dark green back and light-colored sides.\", \"Tench are a freshwater fish found in Europe.\", \"A tench is a small freshwater fish in the carp family.\",\n\n24\n\n temp 0.99 temp 0.5 temp 0.1TSNE of text embedding of Image-prompts at various temperaturesPlatypusLadybugTenchUnder review as a conference paper at ICLR 2023\n\n\"A tench is a heavyset freshwater fish with a mottled brown body and a small, flat head.\", \"A tench is a freshwater fish that looks similar to a carp.\", \"A tench is a freshwater fish in the carp family.\", \"A tench is a freshwater fish of the Cyprinidae family.\", \"The tench is a freshwater fish of the Cyprinidae family.\", \"The tench is a fresh-water fish in the family Cyprinidae.\", \"The easiest way to identify a tench is by its herringbone-patterned scales.\", \"A tench is a freshwater fish of the carp family.\", \"Tench are a freshwater fish found in Europe.\", \"Tench have a large, slimy body with scales that have a green hue.\", \"The tench is a freshwater fish belonging to the carp family.\", \"A tench is a freshwater fish of the Cynoglossidae family.\", \"A tench is a freshwater fish in the carp family.\", \"Tensch are freshwater fish with Olive Green backs, shading to Yellowish on the sides.\", \"A tench looks like a green freshwater fish with a brownish hue.\", \"A tench looks like a freshwater fish with a dark olive-green back, fading to yellowish-brown on the sides.\", \"A tench usually has olive-green skin with dark spots, and a orange-yellow underbelly.\", \"Tench are a freshwater fish that can grow up to 70cm long! They have olive-brown skin with dark spots, and their meat is white and firm.\", \"A tench is a freshwater fish with a sturdy body and a greenish-brown coloration.\", \"A tench is a freshwater fish that can grow up to about two feet long.\", \"A tench is a freshwater fish in the carp family.\", \"A tench is a large, freshwater fish with a thick body and large head.\", \"The image is of a tench fish swimming in water.\", \"The image is of a tench fish swimming in a pond.\", \"The tench is a freshwater fish native to Europe.\", \"This image shows a large, dark green tench swimming in a pond.\", \"An image of a tench from the internet would likely show a dark green fish with a lighter underside.\", \"The image is of a tench fish.\", \"The image is of a tench fish on a white background.\", \"A tench is a freshwater fish of the Cyprinidae family.\", \"The image is of a tench swimming in a murky pond.\", \"In the image, a tench swims in a pond with lily pads.\", \" A tench in a river.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family that is found throughout Europe.\", \" Tench (Tinca tinca), a member of the carp family (Cyprinidae), native to Eurasia.\", \" A tench, a freshwater fish in the family Cyprinidae.\", \" The tench (Tinca tinca) is a freshwater fish of the cyprinid family found throughout Eurasia.\", \" A tench in a Finnish lake.\", \"A tench (Tinca tinca) is a freshwater fish belonging to the carp family (Cyprinidae).\", \"A tench in a fishpond.\", \" The common tench is a freshwater fish of the cyprinid family found throughout Eurasia.\", \"Tench (Tinca tinca) in a pond.\"\n\nTemperature = 0.5\n\n\"A tench is a freshwater fish that is typically greenish-brown in color with a brassy sheen.\", \"A tench is a freshwater fish that typically has a dark green back, light brown sides, and a white belly.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"Tench are a freshwater fish found in Europe.\",\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\n\"A tench is a freshwater fish that is typically olive green in color with dark spots.\", \"A tench is a freshwater fish that is typically olive green in color with a brownish tint.\", \"A tench is a freshwater fish of the Cyprinidae family.\", \"Tench are a freshwater fish found in Europe.\", \"A tench is a freshwater fish that has a dark green back, light brown sides, and a white belly.\", \"A tench is a freshwater fish that is typically olive-green in color with a brownish dorsal fin.\", \"A tench is a freshwater fish of the cyprinid family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish that is typically olive green in color with a brownish back.\", \"The tench is a freshwater fish of the carp family Cyprinidae.\", \"A tench is a freshwater fish that is typically greenish-brown in color.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"Tench have olive green backs and flanks, with yellowish bellies.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the cyprinid family.\", \"A tench is a freshwater fish that can grow up to 30 inches long.\", \"A tench is a freshwater fish that is typically greenish-brown in color.\", \"A tench is a freshwater fish that can grow up to about two feet long.\", \"A tench is a freshwater fish with a brownish-green back and sides, and a yellowish-brown belly.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that looks similar to a carp.\", \"A tench is a freshwater fish that is typically greenish-brown in color.\", \"A tench is a freshwater fish that is part of the carp family.\", \"A tench is a freshwater fish of the cyprinid family.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"The image is of a tench fish swimming in water.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"In the image, a tench is swimming in a pond with lily pads.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"A tench fish, native to Europe, characterized by its greenish-brown color and spots.\", \" A tench (Tinca tinca) in a pond.\", \"A tench (Tinca tinca) is a freshwater fish belonging to the carp family (Cyprinidae).\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family (Cyprinidae).\", \" A tench in a river.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family (Cyprinidae).\", \"A tench (Tinca tinca) in a pond.\", \" A tench (Tinca tinca) in a garden pond.\", \" A tench in a river.\"\n\nTemperature = 0.1\n\n\"A tench is a freshwater fish that is typically olive green in color with dark spots.\",\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n\"A tench is a freshwater fish that can grow up to 30 inches long.\", \"A tench is a freshwater fish that can grow to a length of over two feet.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that is typically olive green in color with dark spots.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that is typically olive green in color with dark spots.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"Tench have a dark green back, light olive sides, and a yellowish belly.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish of the carp family.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that is typically olive green in color with a brownish dorsal side.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that is typically olive green in color with a brownish dorsal side.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"A tench is a freshwater fish that can grow up to two feet long.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"The image is of a tench fish swimming in a pond.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \"A tench (Tinca tinca) is a freshwater fish of the carp family (Cyprinidae).\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \"A tench (Tinca tinca) is a freshwater fish in the carp family.\", \" A tench (Tinca tinca) in a garden pond.\", \" A tench (Tinca tinca) in a garden pond.\", \"A tench (Tinca tinca) is a freshwater fish of the carp family (Cyprinidae).\"\n\nK.2 SAMPLE GENERATED IMAGE-PROMPTS FOR RANDOMLY SELECTED IMAGENET\n\nCATEGORIES\n\n\"bubble\": \"A bubble is a sustained period of inflated asset prices.\",\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n\"A bubble looks like a sphere of air.\", \"A bubble is often characterized by rapidly increasing prices in an asset or security, followed by a sharp decrease in prices.\", \"A bubble looks like a small, round, thin film of soap filled with air.\", \"A bubble looks like a round sphere of soap film.\", \" \\\"A bubble being blown in the park.\", \"A close-up of a soap bubble with a thin film of water in between two layers of air.\", \"A bubble is a spherical shape made up of a thin film of soap water.\", \"A bubble is a circle of air surrounded by water.\", \"A bubble looks like a round, thin layer of soap surrounding a pocket of air.\"\n\n\"kit fox\": \"A kit fox is a small species of fox, about the size of a domestic cat.\", \"You can identify a kit fox by its small size, its big ears, and its long, bushy tail.\", \" A kit fox laying on the ground in a desert habitat.\", \"A kit fox is a small fox found in North America.\", \"A kit fox is a small fox with a sleek coat of fur.\", \"A kit fox is a small fox with large ears, a long, black-tipped tail, and pale fur.\", \"A kit fox has a reddish coat, with white patches on its chest and throat.\", \"This kit fox has a reddish coat and large ears.\", \"A kit fox looks like a small fox with a pointed nose, large ears, and a long, bushy tail.\", \"A kit fox is a small species of fox.\"\n\n\"toy terrier\": \"A toy terrier looks like a very small version of a terrier.\", \"The image is of a toy terrier that is mostly white with brown spots.\", \"You can identify a toy terrier by looking for a compact, short-legged dog with a short muzzle.\", \" Cute little guy.\", \"Toy terriers are miniature versions of terriers, such as the Jack Russell Terrier.\", \"A toy terrier is a small, lightweight breed of dog.\", \"The image is of a small, brown toy terrier.\", \"A toy terrier typically has a long, narrow head with pointy ears, and a small, compact body.\", \"A toy terrier is a small, short-legged dog with a long body, pointy nose, and large ears.\", \"A small, brown and white toy terrier is sitting on a beige couch, looking at the camera.\"\n\n\"mousetrap\": \"In the image, there is a mousetrap made of wood and metal.\", \"An image of a mousetrap from the internet would most likely show a traditional wooden mousetrap with a metal spring.\", \"A mousetrap is a device made to catch and kill mice.\", \"A mousetrap is a small device that is used to catch mice.\", \"The classic mousetrap consists of a wooden base with a metal spring mounted on one end.\", \"The mousetrap is a small wooden box with a metal spring inside.\", \"The mousetrap is a simple device that has been used for centuries to catch mice.\", \"The classic mousetrap - simple, effective, and deadly.\", \"The most common way to identify a mousetrap is by its small size and rectangular shape.\", \"A mousetrap typically has a wire or wooden frame that is baited with food and springs open quickly to snap shut on the mouse when it attempts to steal the bait.\"\n\n\"dog sled\":\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\n\"A dog sled is a vehicle on runners, typically with a thin frame and a flat bottom, that is used to convey goods or passengers over snow or ice.\", \"A dog sled is a toboggan pulled by dogs, typically over snow.\", \"A dog sled looks like a bed on runners that is pulled by dogs.\", \"A dog sled is typically a heavy frame on runners that is pulled by one or more dogs.\", \"A dog sled is traditionally a sled pulled by dogs, used for transportation, racing, or other purposes.\", \" teamA dog sled team is a group of dogs that are harnessed together to pull a sled.\", \"A dog sled looks like a small, open vehicle that is pulled by one or more dogs.\", \"Dog sledding in winter.\", \" pulled by huskiesThe image is of a dog sled pulled by huskies.\", \"A dog sled looks like a large cart that is pulled by a team of dogs.\"\n\n\"geyser\": \"A geyser is a hot spring that periodically erupts, shooting a column of water and steam into the air.\", \"A geyser looks like a hole in the ground that sometimes spurts hot water and steam into the air.\", \"Geysers are hot springs that periodically spout water and steam into the air.\", \"The image is of a geyser erupting.\", \"A geyser is a hot spring that periodically erupts, spraying water into the air.\", \"A geyser typically looks like a cone of rocks with a small hole at the top.\", \"A geyser is a hot spring where water intermittently boils, sending a jet of hot water and steam into the air.\", \"A geyser is a hot spring that periodically shoots a stream of hot water and steam into the air.\", \"The image is of a geyser shooting water high into the air.\", \"A geyser looks like a column of water that shoots into the air and then falls back down.\"\n\n\"Schipperke\": \"A Schipperke is a small, Belgian breed of dog.\", \"A Schipperke is a small black Belgian dog with a rat-like tail.\", \"The image is of a black and white dog with pointy ears and a long body.\", \"A Schipperke is a small, black, Belgian breed of dog.\", \"A Schipperke is a small, black, Belgian breed of dog that closely resembles a fox.\", \"A Schipperke is a small Belgian breed of dog that resembles a fox.\", \"Schipperkes have a long, black coat and a pointed muzzle.\", \"A Schipperke is a small dog breed with a fox-like appearance.\", \"It’s a photo of a black and tan Schipperke dog standing in front of a brick wall.\", \"Black, small, spitz-type dog with a long, fox-like snout, large erect ears, and a long, high-set tail.\"\n\n\"go-kart\": \"A go-kart typically looks like a small car or buggy with a small engine in the back.\", \"A go-kart is a small vehicle with four wheels, a steering wheel, and a gas pedal.\", \"A go-kart is a small vehicle with a steering wheel, pedals, and an engine.\", \"Two young girls in go-karts race down a path in a park.\", \"A go-kart is a small, lightweight vehicle with four wheels and a simple, open frame.\", \"A go-kart is a small, open-wheeled vehicle used for racing.\", \"Two kids racing go-karts on a dirt track.\",\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\n\"A go-kart is a small, racing car.\", \"A go-kart typically looks like a small, open-wheeled car.\", \"A go-kart is a small, lightweight vehicle with four wheels that is propelled by a small engine.\"\n\n\"black-and-white colobus\": \"The Black-and-White Colobus is a type of Old World monkey, found in Africa.\", \"A black-and-white colobus has long black fur, and a white face with a black triangle around the eyes.\", \"Colobus Monkey in the TreesThis elegant colobus monkey is swinging through the trees in search of food.\", \" monkeyIn this image, a black-and-white colobus monkey is shown perched atop a tree branch.\", \"The black-and-white colobus monkey is one of the most beautiful and distinctive of all the colobus monkeys.\", \"The black-and-white colobus monkey is a species of primate in the Colobidae family.\", \"Colobus monkeys are generally black with white patches on their face, back, and sides.\", \"The black-and-white colobus is a species of Old World monkey.\", \" monkeyThe image is of a black-and-white colobus monkey sitting on a tree branch.\", \" monkeyIn the image, the black-and-white colobus monkey is sitting in a tree.\"\n\n\"sock\": \"A sock usually has a cuff at the top, and a heel at the bottom.\", \" A black sock with a white line running down the middle.\", \"A sock is typically a garment worn on the feet and made from a soft material, such as cotton.\", \"A sock normally has a heel, toe and a cuff at the top.\", \"A sock is a small amount of money that is given to someone without them knowing.\", \"A sock is an article of clothing worn on the feet.\", \"A sock is a piece of clothing that is worn on the feet.\", \"A sock is a tubular garment that covers the foot and ankle.\", \"This image is of a blue and white striped sock.\", \"There are many ways that you can identify a sock.\"\n\n\"Cocker Spaniel\": \"A Cocker Spaniel is a medium sized dog with long, floppy ears, and a silky coat that is usually either brown or black.\", \"The Cocker Spaniel has a long floppy ears, a silky coat, and a bushy tail.\", \"A Cocker Spaniel has a long, black muzzle and big, brown eyes.\", \"The Cocker Spaniel is a breed of dog.\", \"An image of a Cocker Spaniel from the internet shows a small brown and white dog with long floppy ears.\", \"The image is of a Cocker Spaniel with short, brown fur and long, floppy ears.\", \"Cocker spaniels have long, floppy ears and a long, silky coat.\", \"A Cocker Spaniel is a small to medium sized dog.\", \"The image is of a light brown and white Cocker Spaniel standing on a green grassy field with its head turned to the side.\", \"A Cocker Spaniel has a long, silky coat that is usually either black, brown, or golden.\"\n\n\"southern black widow\": \"A southern black widow spider perched atop a web.\", \"Female southern black widows have a black body with a red hourglass shape on their abdomen.\", \"The southern black widow is black with a red hourglass shape on its belly.\",\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n\"Female southern black widow spiders are black with a characteristic red hourglass-shaped mark on their ventral abdomen.\", \" spiderThe image is of a large, black spider with a red hourglass shape on its abdomen.\", \"A southern black widow is a spider that is black with a red hourglass shape on its abdomen.\", \"A southern black widow spider is a small, black spider with a red hourglass-shaped mark on its underside.\", \"There’s an image on the internet of a southern black widow that’s really cool.\", \"A southern black widow can be identified by its black coloration with a red hourglass shape on its abdomen.\", \"A Southern black widow is a type of spider that is black with a red hourglass shape on its belly.\"\n\n\"catamaran\": \"A catamaran is a sailboat that has two hulls, or wide bodies, that are connected by beams.\", \"The easiest way to identify a catamaran is by its two hulls.\", \"A catamaran is a type of boat that has two parallel hulls.\", \"A catamaran is a multi-hulled vessel with two parallel hulls of equal size.\", \"My dream boat! A sleek catamaran that can zip through the waves.\", \"A catamaran is a multi-hulled vessel with two parallel hulls of equal size.\", \"The image is of a white catamaran with blue trim.\", \"A catamaran is a type of boat that has two hulls, or platforms, that are parallel to each other.\", \"The image is of a yellow catamaran with white trim, sitting in calm water.\", \"A catamaran is a type of sailing vessel that consists of two parallel hulls of equal size.\"\n\n\"beach\": \".\", \"Blue skies, white sands, and clear turquoise waters make this beach a paradise.\", \"A beach usually has sand and water.\", \"Beach identification can be accomplished through the identification of physical characteristics.\", \"A beach is a naturally occurring feature of the landscape.\", \"The sun sets over the ocean, casting a beautiful orange hue in the sky.\", \"A beach is a large body of water with sand or small rocks at the shore.\", \"In the image, the beach is Brilliant white with crystal blue waters.\", \"The beach looks like a long strip of land next to the ocean.\", \"A beach typically looks like a large, flat expanse of sand with some rocks or other natural features nearby.\"\n\n\"rotary dial telephone\": \"A rotary dial is a device used to dial telephone numbers.\", \"Rotary dial telephone from the mid-20th century.\", \"A rotary dial telephone is a phone with a circular dial on the front face.\", \"When looking at a rotary telephone, you can tell it is a rotary phone by the Place the phone’s receiver on your ear and listen for a dial tone.\", \"A rotary dial phone is an older model phone that has a circular device with numbers on it that you rotate with your finger to dial a number.\", \"A rotary dial telephone is a type of telephone that uses a mechanical dial to select the telephone number that a user wishes to call.\", \"History of the Rotary Dial Telephone.\", \"A rotary dial telephone is an old-fashioned telephone that has a round dial on the front of it.\",\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\n\" rotary dial telephone looks like a classic telephone with a rotary dial on the base unit.\", \"The image is of a rotary dial telephone on a black background.\"\n\n32",
  "translations": [
    "# Summary Of The Paper\n\nOpen vocabulary models require natural language prompts as an intermediate medium between category and visual content, thus it is important to choose a good prompt. In this paper, the authors study how to use a pretrained language model to help generate better language prompts for open vocabulary tasks. The authors feed the categories with language prompt templates to an LLM and use the output of the LLM as the image prompts. The authors show that the prompts generated by their method can achieve better performance on zero-shot image classification benchmarks.\n\n# Strength And Weaknesses\n\nStrength:\n- The paper is well written. The method is intuitive and the performance gain is good.\n- The authors did different ablation studies to show how different design choices would affect performance including the number of prompts, diversity of prompts, model size, etc. These ablations are helpful for the audience to understand the full picture of this method.\n\nWeaknesses:\n- Although the general idea is good, the authors only consider one task in this paper which is zero-shot image classification. It is more interesting for tasks like open vocabulary segmentation/detection etc. This method can even be used for image caption retrieval where LLM is used to paraphrase the caption. It is more convincing that the proposed method can universally work on different tasks.\nEven for imagenet, it is more interesting to see the performance on the full imagenet21k instead of the 1k set. In addition, original CLIP is evaluated on different relatives of Imagenet like imagenet-v2, imagenet-R etc. which would also be good to have in this paper.\n- It would be better if there can be a clear analysis or explanation of why the standard prompts and CuPL prompts have different preferences as shown in figure 7. It is a really interesting phenomenon to me, and it may also reveal some hints on how to improve CuPL.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity questions:\nHow are “hyperparameters” like the number of prompts and which specific prompts to choose decided? Is there a “validation set”?\nIs there qualitative or anecdotal analysis of how “diverse” the generated image prompts are after increasing the temperature? For example, would higher temperatures also “hurt” by introducing more incorrect knowledge?\n\nMinor comments:\nFigure 6: CuPL outperforms the baseline even with just three hand-written sentence. Should be one hand-written sentence.\nAlso, how many prompts are there for Figure 6 left figure? If it is 10, it means that 10 CuPL image prompts can outperform 80 standard image prompts which is less than 25 in the caption.\n\nThe paper is novel and easy to reproduce.\n\n# Summary Of The Review\n\nThe authors propose a novel method that is simple and has the potential to apply to many different tasks. However, the authors did not show that this method is widely applicable which makes the paper weaker. My decision is borderline accept.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces Customized Prompts via Language models (CuPL), a novel approach to enhance zero-shot image classification accuracy by generating category-specific descriptive prompts using large language models (LLMs). It addresses the limitations of manual prompt creation by generating prompts dynamically, eliminating the need for extensive hand-crafted templates. The methodology utilizes GPT-3 to create multiple image-prompts for each category, which are then employed in conjunction with CLIP for classification tasks. The findings indicate significant accuracy improvements on various zero-shot benchmarks, particularly demonstrating over a 1% gain on ImageNet, and showcasing the better performance of CuPL compared to traditional prompt generation methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative use of LLMs to automate prompt generation, which not only simplifies the process but also leads to improved classification performance across multiple datasets. The detailed experimental setup and rigorous evaluation on 15 benchmarks lend credibility to the findings. However, a weakness is the variability in accuracy gains across different classes; while some classes benefit greatly, others do not, raising questions about the consistency of the approach. Additionally, the reliance on GPT-3 may limit accessibility for some users due to computational requirements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and results, making it accessible to a broad audience. The novelty of the approach is significant, as it combines advancements in LLMs with open vocabulary models for practical applications in image classification. The authors provide supplementary materials and code, enhancing reproducibility. However, more detailed explanations of the temperature adjustments and their impact on prompt diversity could improve clarity.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing zero-shot image classification through the use of LLMs for prompt generation. While the methodology is robust and the results are promising, the variability in performance across classes calls for further investigation. The paper is a valuable contribution to the field, with significant potential for practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification\" introduces a novel method called Customized Prompts via Language models (CuPL) that enhances zero-shot image classification. CuPL efficiently combines open vocabulary models with large language models (LLMs) to generate customized prompts for image categories, thereby minimizing the reliance on hand-crafted templates. The methodology involves generating image prompts using GPT-3 based on a limited number of initial LLM-prompts. The findings demonstrate that CuPL significantly improves accuracy across multiple benchmarks, including a notable performance boost on ImageNet, while maintaining the robustness of zero-shot classification capabilities.\n\n# Strength And Weaknesses\nThe strengths of the paper include the substantial reduction in manual effort required for prompt engineering, as CuPL can produce tailored prompts that enhance classification performance. The methodology's incorporation of LLMs allows for a diverse range of descriptive prompts, leading to improved accuracy on challenging datasets. Furthermore, the robustness of CuPL in zero-shot contexts is a significant advantage. However, the paper also has weaknesses, such as its dependency on the quality of the LLM, which may not be universally accessible or reliable. Additionally, the variability introduced by prompt generation settings may lead to inconsistencies, and the reliance on a limited number of hand-crafted templates could hinder scalability across diverse datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of writing is high, making the complex concepts accessible to the reader. The novelty of the approach lies in the integration of LLMs for prompt generation in zero-shot image classification, which has not been extensively explored in previous works. However, reproducibility may be a concern due to the reliance on specific LLM architectures and the nature of the prompt generation process, which could introduce variability in results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative method for improving zero-shot image classification through customized prompt generation using LLMs. While the contributions are significant, particularly in terms of performance gains and reduced manual effort, challenges related to LLM dependency and prompt variability should be addressed for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Customized Prompts via Language models (CuPL), a novel methodology that enhances zero-shot image classification by harnessing the capabilities of open vocabulary models and large language models (LLMs). The primary contribution lies in the generation of customized prompts that improve classification accuracy without the need for additional training or labeled data. Through comprehensive experiments on 15 datasets, including ImageNet, the authors demonstrate significant performance improvements, with CuPL achieving over 1% accuracy enhancement on ImageNet and outperforming traditional hand-crafted templates while reducing their number.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative integration of LLMs for prompt generation, which addresses the labor-intensive nature of crafting effective templates for image classification. The results indicate a clear performance boost across multiple datasets, showcasing the potential of CuPL in real-world applications. However, a notable weakness is the reliance on a specific LLM (GPT-3), which may limit generalizability to other models or scenarios. Additionally, while the paper provides a thorough analysis of results, further exploration into the limitations of prompt diversity and its impact on performance would enhance the discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, allowing readers to follow the logic and significance of the work. The quality of the experiments is high, with a robust evaluation across various datasets and a clear presentation of results. The novelty of combining LLMs with open vocabulary models for image classification is significant, marking a step forward in the field. The reproducibility is also addressed, as the supplementary materials include code, hyperparameters, and generated prompts, making it easier for others to replicate the study.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to zero-shot image classification through the use of customized prompts generated by language models. The findings are significant, showing improved accuracy and reduced engineering effort, though some limitations regarding model generalizability and prompt diversity warrant further investigation.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents CuPL, a novel method that combines open vocabulary models with large language models (LLMs) to generate customized prompts for zero-shot image classification. The methodology involves leveraging the generative capabilities of LLMs to create diverse and descriptive prompts, leading to significant performance improvements in classification accuracy across multiple benchmarks, including a notable gain on ImageNet. The findings indicate that CuPL not only reduces the need for hand-crafted templates but also maintains robustness against distribution shifts, although its effectiveness may vary across different datasets.\n\n# Strength And Weaknesses\nThe contributions of the paper are substantial, particularly in terms of introducing an innovative approach that integrates LLMs for prompt generation, which enhances zero-shot learning capabilities. The significant performance gains across multiple benchmarks demonstrate the method's effectiveness and potential for broader applications. However, the reliance on LLMs introduces the risk of inherent biases, which could affect the quality of generated prompts. Additionally, while the method reduces human effort, it still requires some manual input in the form of initial template engineering. Furthermore, the zero-shot performance may not consistently match that of fine-tuned models, limiting its applicability in high-stakes scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of both the methodology and the experimental setup, allowing for a good understanding of the proposed approach. The quality of the experiments is high, with extensive evaluations across 15 benchmarks, although the focus on specific datasets may restrict broader applicability. The novelty of the approach is significant, as it explores an underutilized intersection between LLMs and image classification. The authors have taken steps to ensure reproducibility by providing supplementary materials, including code and hyperparameters; however, the complexity of the models utilized may still pose challenges for replication by practitioners with limited computational resources.\n\n# Summary Of The Review\nOverall, the paper makes a compelling case for the use of LLMs in generating prompts for zero-shot image classification, achieving notable performance improvements and reducing the burden of template creation. While the method shows promise, some limitations related to biases, applicability, and reproducibility should be addressed. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper presents a novel methodology for enhancing image classification accuracy through the use of generative language models to create customized prompts for open vocabulary models. The proposed method, termed Customized Prompts via Language Models (CuPL), improves upon existing prompt engineering techniques by significantly reducing manual input while increasing descriptive capability without requiring task-specific training. The authors report substantial accuracy improvements on several zero-shot image classification benchmarks, demonstrating that CuPL can outperform traditional methods while requiring fewer hand-crafted templates.\n\n# Strength And Weaknesses\n**Strengths:**\n- The integration of generative models with open vocabulary classification is a significant advancement that addresses common limitations in prompt engineering.\n- The reduction of manual effort in creating prompts leads to considerable efficiency gains, benefiting practitioners in the field.\n- The paper provides clear empirical results that demonstrate improved accuracy, lending credibility to the proposed approach.\n\n**Weaknesses:**\n- The paper lacks an in-depth analysis of the prompt generation mechanics from the language model, leaving questions about the factors influencing their effectiveness.\n- A detailed error analysis would help clarify the model's limitations and identify areas for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, making the methodology and results easy to follow. The quality of the experiments is high, with robust comparisons against traditional methods. The novelty of using generative language models for prompt creation is clear, and the results are reproducible due to the comprehensive reporting of experimental setups. However, further elaboration on the prompt generation process could enhance clarity.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in zero-shot image classification by leveraging generative language models to generate customized prompts. CuPL demonstrates improved accuracy with a significant reduction in manual effort, marking a promising direction for future research in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to adversarial training in image classification through a method called Customized Adversarial Training via Language Models (CAT-LM). This approach leverages large language models (LLMs) to dynamically generate diverse adversarial examples, significantly reducing the reliance on extensive, manually crafted adversarial samples. The findings indicate that models trained with CAT-LM exhibit improved robustness against various adversarial attacks, achieving notable accuracy gains on benchmark datasets such as CIFAR-10 and ImageNet, while maintaining zero-shot adaptability without requiring additional fine-tuning.\n\n# Strength And Weaknesses\nThe strengths of the paper include its novel integration of LLMs for adversarial example generation, which represents a significant shift in the methodology of adversarial training. This approach enhances efficiency by minimizing the need for manual example creation and demonstrates generalizability across different datasets due to its zero-shot capability. However, the paper also has weaknesses, particularly its dependence on LLM performance for generating adversarial examples, which may vary based on the input context. Additionally, a more thorough analysis of the generated examples and their impacts on model performance would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and results, making it accessible to readers. The novelty of applying LLMs to adversarial training is a significant contribution that could influence future research in the field. However, the reproducibility of the results may be affected by the variability in LLM performance, as well as the limited exploration of the characteristics of the generated adversarial examples.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative approach to adversarial training using LLMs, demonstrating significant improvements in model robustness. While it offers valuable insights and practical applications, further analysis and validation are needed to fully assess the method's effectiveness across diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new method for generating customized prompts for zero-shot image classification, termed Customized Prompts via Language models (CuPL). The authors claim that CuPL integrates open vocabulary models with large language models (LLMs) to produce tailored prompts for image classification tasks without prior domain knowledge. The findings indicate significant performance gains, with CuPL reportedly achieving over a 1% improvement on ImageNet and a 6% gain on the Describable Textures Dataset. The authors assert that this approach allows for flexibility in prompt design and reduces the need for extensive training.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its novel approach to combining LLMs with image classification, potentially simplifying the prompt engineering process. The reported performance improvements, while modest, suggest that CuPL may enhance image classification tasks in specific scenarios. However, the paper has notable weaknesses, including an exaggeration of the method's transformative potential and a lack of robust error analysis regarding the interplay between linguistic and visual context in classification. Additionally, the claims of universal applicability and drastic improvements are overstated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, though some claims require more substantiation. The novelty of the approach is evident, but the significance of the findings may be overstated. Reproducibility might be a concern, as the paper does not provide detailed descriptions of the experimental setups or benchmarks, making it challenging for other researchers to validate the results.\n\n# Summary Of The Review\nWhile the paper presents a novel method for generating customized prompts in zero-shot image classification, its claims of revolutionary advancements are overstated. The method shows promise in improving performance, but the actual impact appears to be incremental rather than transformative.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces CuPL (Customized Prompts via Language models), a novel approach to enhancing zero-shot image classification through the generation of high-accuracy prompts using language models (LLMs). CuPL significantly reduces the reliance on extensive hand-crafted prompts by employing GPT-3 to automatically generate prompts that improve classification accuracy across multiple datasets. The empirical results demonstrate that CuPL yields notable performance gains, achieving over a 2.5 percentage point increase in accuracy on the ImageNet benchmark while requiring far fewer hand-written prompts compared to traditional methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of LLMs to automate prompt generation, thereby simplifying and enhancing the zero-shot classification process. This approach addresses a significant limitation in the existing methodologies that depend heavily on manual prompt crafting. Additionally, the empirical results across 15 benchmarks provide robust evidence of the proposed method's effectiveness. However, a potential weakness is the reliance on a single LLM (GPT-3) for prompt generation, which may limit the generalizability of the findings to other LLM architectures or configurations. Furthermore, while the paper discusses the diversity of prompts, it could benefit from a more in-depth analysis of the qualitative aspects of the generated prompts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The methodology section is detailed enough for reproducibility, outlining how the prompts are generated and the experimental setups. However, the paper could improve clarity in explaining the implications of the results, particularly regarding how increased prompt diversity directly contributes to performance improvements. In terms of novelty, the integration of LLMs for prompt generation in the context of image classification presents a fresh perspective that is both innovative and significant.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of zero-shot image classification through the introduction of CuPL, which effectively leverages LLMs to enhance prompt generation. The empirical results are strong, illustrating clear improvements in classification accuracy while reducing manual effort. However, the paper would benefit from a broader examination of the implications and qualitative aspects of the generated prompts.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the CuPL (Class Unified Prompt Learning) method, which leverages large language models (LLMs) like GPT-3 to generate descriptive prompts for zero-shot classification tasks. The authors argue that CuPL enhances classification performance by automatically generating diverse prompts that can generalize across different datasets. The methodology involves training the LLM on a variety of datasets and evaluating its classification performance against standard prompt generation methods. The findings indicate that CuPL achieves competitive results, suggesting that LLM-generated prompts may outperform traditional hand-engineered prompts in certain scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to prompt generation using LLMs, which could reduce the reliance on manual prompt engineering. However, several weaknesses are evident. The assumption that LLMs inherently produce superior prompts compared to human-written ones lacks empirical validation. Additionally, the generalizability of prompts across diverse datasets is questionable, particularly in specialized domains. The reliance on zero-shot classification raises concerns regarding the models' ability to capture nuanced class distinctions without training data. The evaluation metrics are also limited, as they do not comprehensively assess the prompts' effectiveness. Furthermore, the paper overlooks critical aspects such as the cost of prompt engineering, potential biases in LLM outputs, and the implications of overfitting.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly; however, certain assumptions are made without thorough justification or exploration, which may hinder reproducibility. The novelty of the approach is notable, as it integrates LLM capabilities into the prompt generation process. Still, the lack of a comprehensive comparison with existing methods limits the assessment of its true significance. Empirical evaluations could be strengthened with a broader range of metrics and deeper analyses of error types.\n\n# Summary Of The Review\nThe paper introduces an interesting method for prompt generation using LLMs, showcasing potential benefits in zero-shot classification tasks. However, various assumptions and limitations, particularly regarding generalizability and evaluation methods, diminish its impact. More rigorous testing and comparative analysis with existing methods are needed to substantiate the claims made.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents CuPL, a novel method for generating customized prompts for zero-shot image classification leveraging large language models (LLMs). The methodology involves two primary steps: generating tailored prompts using an LLM and employing these prompts in conjunction with models like CLIP for classification tasks. The findings indicate that CuPL significantly enhances classification accuracy across 15 benchmark datasets, including ImageNet, while reducing reliance on hand-crafted templates. The authors also analyze the impact of various factors on performance, providing insights into the interplay between prompt generation and accuracy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of LLMs for prompt generation, which represents a clear advancement over traditional hand-written approaches. The experimental results are compelling, demonstrating substantial performance gains and offering valuable insights through a thorough analysis of hyperparameters and model behavior. However, a potential weakness is that the paper could provide more detailed comparisons with existing methods, particularly in terms of qualitative aspects of generated prompts. Additionally, the scalability of CuPL in diverse real-world applications remains to be fully explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that facilitates understanding of its contributions and methodologies. The quality of the experiments is high, and the authors provide sufficient detail to ensure reproducibility, including code and hyperparameters in the supplementary materials. The novelty is significant, as the integration of LLMs for prompt generation in zero-shot image classification is relatively unexplored territory, setting a new direction for future research in the field.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of zero-shot image classification through the innovative use of LLMs for prompt generation. The results are impressive and suggest a promising avenue for future research, though further exploration of comparative methods and application scenarios would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework aimed at enhancing the efficiency of deep learning models through adaptive pruning techniques. The authors propose a method that dynamically adjusts the pruning of neural network weights based on real-time performance metrics, thereby optimizing both model accuracy and computational resource usage. The findings demonstrate that the proposed approach outperforms several existing state-of-the-art methods in terms of accuracy and efficiency on benchmark datasets.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The challenge of model efficiency in deep learning is timely and significant, making the paper's focus highly relevant.\n2. **Innovation:** The adaptive pruning mechanism represents a fresh perspective, integrating real-time feedback into the pruning process, which could inspire further research in model optimization.\n3. **Clarity of Presentation:** The paper is well-structured, with a logical flow that aids in understanding the proposed methodology and its implications.\n4. **Theoretical Contribution:** The authors provide a solid theoretical background for their method, enhancing its acceptance and potential adoption by the community.\n5. **Empirical Results:** The comprehensive experiments conducted show a clear advantage of the proposed method over traditional static pruning approaches.\n\n**Weaknesses:**\n1. **Experimental Validation:** While the results are promising, the experimental setup lacks detail, particularly regarding hyperparameter tuning and dataset specifics, which may affect reproducibility.\n2. **Comparison with Baselines:** The comparison with existing methods could be more exhaustive to better contextualize the advantages of the proposed approach.\n3. **Scalability and Generalizability:** The paper does not sufficiently address how the proposed method scales with larger models or diverse datasets, which could limit its applicability.\n4. **Complexity:** The complexity of the adaptive pruning algorithm may hinder practical deployment; a discussion on computational requirements would be beneficial.\n5. **Further Analysis Required:** More in-depth analysis, such as an ablation study, would strengthen the claims regarding the effectiveness of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-organized, making complex ideas accessible. The novelty of integrating adaptive techniques into pruning is significant; however, the reproducibility is hampered by insufficient detail in the experimental setup and methodology.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative contribution to the field of deep learning through adaptive pruning techniques. While there are several areas for improvement, particularly in experimental validation and clarity regarding scalability, the foundational ideas are noteworthy and have the potential to advance research in model efficiency.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach, Customized Prompts via Language models (CuPL), which enhances image classification tasks by leveraging natural language prompts generated through large language models (LLMs). By integrating open vocabulary models with LLMs, CuPL automates the generation of prompts tailored to specific object categories, enabling effective zero-shot classification without the need for labeled data or extensive prior knowledge. The results demonstrate that CuPL significantly improves classification accuracy on various benchmarks, particularly on ImageNet, showcasing its potential as a straightforward tool for practitioners in the field.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of LLMs for prompt generation, which effectively addresses the limitations of traditional prompt engineering that often necessitates manual crafting and extensive dataset knowledge. This results in improved classification performance across multiple datasets without requiring additional training, thus streamlining the workflow for practitioners. However, the paper could benefit from a more detailed discussion on the limitations of CuPL, such as its performance on highly specialized categories or scenarios where LLMs may not generate optimal prompts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to a broad audience. The quality of the experimental validation appears robust, with comprehensive evaluations across various datasets. The novelty of the approach is significant, as it introduces a new paradigm for prompt generation that enhances existing image classification models. Reproducibility is facilitated by the use of open vocabulary models and LLMs, although the paper should provide additional details on implementation specifics to fully support replication efforts.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the field of image classification by introducing CuPL, which effectively utilizes LLMs for prompt generation. Its zero-shot approach and ability to improve classification accuracy across multiple datasets are commendable, although a discussion of potential limitations would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "## Summary Of The Paper\nThe paper presents a novel approach named Customized Prompts via Language models (CuPL) for enhancing zero-shot image classification by leveraging large language models (LLMs). The methodology involves generating customized prompts in two stages: creating descriptive prompts using LLMs (specifically GPT-3) and utilizing these prompts for classification tasks. Experimental results across 15 benchmarks indicate that CuPL significantly improves accuracy compared to traditional methods, while also reducing the reliance on hand-written prompts in the classification process.\n\n## Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of LLMs with image classification, which addresses the limitations of traditional prompt engineering. The method's simplicity and zero-shot capability make it broadly applicable and easy to implement. However, a potential weakness is the dependence on GPT-3 for prompt generation, which may limit the generalizability of the findings to other LLMs or contexts. Additionally, the experiments could benefit from exploring a more diverse set of datasets to validate the robustness of CuPL across different scenarios.\n\n## Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow from introduction to conclusion. The methodology is explained sufficiently to allow for reproducibility; however, more detailed information on the prompt generation process could enhance clarity. The novelty of the approach is significant, as it presents a new way to generate prompts that can potentially transform the landscape of zero-shot image classification.\n\n## Summary Of The Review\nOverall, the paper makes a compelling contribution to the field of image classification by introducing a straightforward and effective method for generating customized prompts using LLMs. While the results are promising, further exploration of the method's applicability across diverse datasets and other language models would strengthen its impact.\n\n## Correctness\n4/5\n\n## Technical Novelty And Significance\n4/5\n\n## Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification\" introduces a novel algorithm called CuPL, which leverages large language models (LLMs) to generate customized prompts for zero-shot image classification tasks. The methodology involves an innovative integration of prompt engineering techniques that enhance the performance of image classification models without requiring extensive retraining. Experimental results indicate that CuPL significantly improves classification accuracy across various datasets compared to standard methods, demonstrating its effectiveness and potential for practical applications in open vocabulary models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to prompt generation, which allows for greater flexibility and accuracy in zero-shot image classification. The comprehensive experimental evaluation, including ablation studies and comparisons with baseline methods, strengthens the claims made by the authors. However, one potential weakness is the reliance on LLMs, which may introduce variability based on the underlying model and its training data, potentially affecting generalizability. Additionally, while the authors mention ethical considerations, a more in-depth discussion around the implications of using LLMs in this context would enhance the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and logically organized, making it easy to follow. The language used is clear and accessible, which aids in understanding the complex ideas presented. The methodology is described in sufficient detail, allowing for reproducibility of the experiments. The inclusion of comprehensive figures and tables effectively supports the text. In terms of novelty, the approach presents a significant advancement in integrating LLMs with image classification tasks, positioning it well within the current research landscape.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to zero-shot image classification with significant empirical results. The methodology is sound, and the findings contribute meaningful advancements to the field, although a more thorough exploration of ethical implications would strengthen the paper further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Customized Prompts via Language models (CuPL) methodology, aimed at enhancing prompt generation for zero-shot image classification using open vocabulary models. The proposed framework integrates large language models (LLMs) with open vocabulary models, specifically utilizing CLIP for image classification. The authors demonstrate that CuPL reduces the need for manual prompt engineering while improving classification accuracy, reporting performance enhancements across multiple benchmarks, including a notable increase of over 1% in ImageNet accuracy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to automating prompt generation, which addresses the significant challenges associated with traditional prompt engineering. By leveraging LLMs to generate prompts, the authors provide a scalable solution that enhances the adaptability of classification tasks across diverse datasets. However, a potential weakness is the reliance on LLMs, which may introduce variability and depend on the quality of the generated prompts. Additionally, while the experimental results are compelling, further analysis on the robustness of the approach in various domains would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology, making it accessible to readers. The quality of the experiments, including a thorough benchmark analysis, supports the claims made regarding the efficacy of CuPL. The novelty of integrating LLMs with open vocabulary models stands out, although the concept of using language models for prompt generation is not entirely new. The reproducibility of the results is supported by detailed descriptions of the methodology, though access to code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to improving zero-shot image classification through automated prompt generation via LLMs. While the contributions are significant and the methodology is clear, additional validation across varied datasets would strengthen the robustness of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces CuPL, a method for generating customized prompts aimed at enhancing zero-shot image classification. The methodology combines large language models (LLMs) with existing classification frameworks, claiming to streamline prompt generation by reducing the number of hand-written templates from 175 to 45. The findings indicate a modest accuracy improvement of over one percentage point on the ImageNet dataset, suggesting some efficacy in the application of LLMs for this purpose; however, the overall significance of the results is questionable.\n\n# Strength And Weaknesses\nThe primary strength of the paper is its innovative use of LLMs to automate prompt generation, which could potentially reduce the engineering workload associated with zero-shot classification. However, this strength is undermined by significant weaknesses, including a lack of compelling novelty and limited improvements in accuracy that may not justify the complexity introduced by the method. Additionally, the paper does not sufficiently address the limitations and biases inherent in LLMs, nor does it convincingly demonstrate that CuPL outperforms existing approaches. The reliance on expert knowledge for prompt engineering further detracts from the claimed accessibility of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured and clear, the novelty is not sufficiently compelling, as the approach largely amalgamates existing techniques without offering substantial advancements. Concerns about reproducibility arise due to the dependency on specific LLMs, such as GPT-3, which may not be readily accessible to all researchers. The paper also lacks a comprehensive discussion of the potential pitfalls associated with LLMs, such as biases and variability in prompt quality, which could impact reproducibility and reliability.\n\n# Summary Of The Review\nOverall, CuPL presents some interesting ideas regarding the integration of LLMs into image classification, but the contributions are modest, and the results are not robust enough to warrant widespread adoption. A more thorough evaluation of the limitations and practical implications of the method is needed to enhance its credibility and applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Customized Prompts via Language models (CuPL), an innovative approach that combines large language models with open vocabulary models to generate customized natural language prompts for zero-shot image classification. CuPL aims to enhance classification accuracy without the need for additional training, achieving significant improvements across multiple benchmarks. The methodology allows for the effortless generation of prompts from a few general templates, enabling flexibility and adaptability in category selection and prompt adjustments, all while maintaining robust performance under distribution shifts.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear demonstration of substantial accuracy gains—over 1 percentage point on ImageNet and over 6 percentage points on the Describable Textures Dataset—across 15 different zero-shot image classification benchmarks. The ease of use and reduction in manual prompt engineering are noteworthy, as they enhance user accessibility and efficiency. However, the paper could improve by providing more detailed comparisons of CuPL with existing state-of-the-art methods, as well as discussing potential limitations or scenarios where CuPL might not perform as expected.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers. The novelty of integrating language models with open vocabulary classification methods is significant, representing a fresh perspective in image classification tasks. While the findings are compelling, additional details regarding the reproducibility of the results, such as dataset specifics and implementation guidelines, would enhance the paper's overall quality.\n\n# Summary Of The Review\nOverall, CuPL presents a transformative approach to zero-shot image classification that merges language processing and computer vision, delivering impressive empirical results. Its innovative methodology and practical applications signify a notable advancement in the field, although further comparisons and reproducibility details would strengthen the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents CuPL (Customized Prompts via Language Models), a novel approach for zero-shot image classification that leverages large language models (LLMs) to generate dynamic, contextually relevant prompts. The methodology includes a detailed exploration of prompt engineering, where LLMs are used to create customized prompts that enhance the classification accuracy of open vocabulary models. Experimental results demonstrate significant performance improvements across various datasets, highlighting the effectiveness of the proposed framework in aligning visual and textual modalities.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical foundation, which integrates LLMs into the image classification domain, significantly advancing multimodal learning. The comprehensive discussion on the mechanics of prompt generation, including the distinctions between LLM-prompts and image-prompts, provides valuable insights into how customization can improve classification outcomes. However, one weakness is that while the theoretical implications are robust, the empirical results may benefit from additional datasets and real-world applications to strengthen the claims further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical foundations and implications of CuPL. The quality of the writing is high, making complex ideas accessible. The novelty of integrating LLMs in zero-shot learning paradigms is significant, though reproducibility may be challenging due to the dependency on the specific LLMs used and the intricacies of prompt design. Further details on implementation would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of multimodal learning by effectively coupling LLMs with image classification tasks. The insights into prompt customization are both theoretical and practical, though additional empirical validation could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach called Customized Prompts via Language models (CuPL) aimed at enhancing zero-shot image classification. The methodology leverages the capabilities of large language models (LLMs), specifically GPT-3, to generate descriptive prompts for image categories, thereby reducing reliance on hand-written templates. The findings indicate that CuPL significantly outperforms traditional methods on 15 zero-shot image recognition benchmarks, such as ImageNet, while utilizing notably fewer hand-written templates.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative use of LLMs to automate and diversify prompt generation, which is a crucial aspect of zero-shot classification performance. The results demonstrate clear improvements in classification accuracy across several datasets, suggesting that CuPL can effectively adapt to various image recognition tasks. However, a notable weakness is the reliance on high temperature settings during prompt generation, which may introduce variability in quality and relevance of the prompts. Additionally, while the results are promising, the paper could benefit from a more thorough examination of the impact of hyperparameter tuning on performance, particularly in different datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The use of appendices to provide additional details on prompt templates and evaluation metrics enhances reproducibility. The novelty of the approach is significant, as it integrates LLMs into a traditionally template-driven process, yet the clarity surrounding the limitations and potential biases of the LLM-generated prompts could be improved. Overall, the quality of the writing and the thoroughness of the experiments support the paper's claims, promoting a solid foundation for future work.\n\n# Summary Of The Review\nThis paper presents a compelling advancement in zero-shot image classification through the innovative use of LLMs for prompt generation. The results demonstrate significant performance improvements with reduced reliance on hand-written templates, although further exploration of the impact of hyperparameters on outcomes would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the CuPL method, which claims to enhance prompt generation for image classification tasks by leveraging large language models (LLMs) like GPT-3. The authors assert that CuPL generates higher accuracy prompts with reduced human effort compared to existing approaches, reporting performance gains on 13 out of 15 datasets, including a marginal improvement of one percentage point on ImageNet. However, the paper does not adequately address the context of prior research in prompt engineering, potentially overstating the novelty and significance of its contributions.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its attempt to simplify the prompt generation process and its application across multiple datasets, showing some performance improvements. However, several weaknesses are evident: it fails to sufficiently compare the efficiency of its prompts against well-established methods, does not account for the extensive training that previous models have undergone, and overlooks the possibility that the reported gains may not indicate general applicability. The reliance on LLMs for prompt generation is not novel, and the evaluation lacks a critical analysis of prior methods' performance, potentially leading to biased conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by its failure to contextualize its contributions within the broader landscape of prompt engineering. While the methodology is presented clearly, the novelty of CuPL is questionable given the existing body of work that has utilized LLMs similarly. The reproducibility of the results may be challenged due to the lack of detailed discussions on potential overfitting and dataset peculiarities, which could affect the generalizability of the findings.\n\n# Summary Of The Review\nOverall, while the CuPL method presents an interesting approach to prompt generation, the paper does not convincingly demonstrate its superiority over established methods. The marginal performance improvements and lack of thorough comparative analysis diminish the perceived novelty and significance of the contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification\" introduces a novel method, referred to as CuPL, for generating more accurate natural language prompts to enhance zero-shot image classification tasks. The methodology involves leveraging large language models (LLMs) to create prompts that can be dynamically adjusted without requiring additional training. The findings indicate that this approach yields performance improvements in 13 out of 15 datasets evaluated, demonstrating the potential for significant gains in classification accuracy.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to utilizing LLMs for prompt generation, which is a fresh perspective in the realm of zero-shot image classification. The empirical results support the effectiveness of CuPL, showcasing its application across diverse datasets. However, the paper exhibits some weaknesses, including inconsistencies in terminology, punctuation, and formatting that may hinder readability. Additionally, certain technical terms are introduced without adequate explanation, which could alienate readers unfamiliar with the concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the core contributions are significant, the clarity of the presentation is compromised by several formatting inconsistencies and awkward phrasing throughout the text. The quality of the figures and tables also needs to be uniform in terms of layout and alignment. Despite these issues, the novelty of the approach is commendable, and the methodology appears reproducible, although clarification of key terms and consistent formatting would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of zero-shot image classification through the introduction of CuPL for prompt generation. Despite some clarity and formatting issues that detract from its presentation, the findings are promising and indicate significant potential for future research in this area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to zero-shot image classification using LLM-generated prompts, termed CuPL. The methodology involves leveraging large language models to generate prompts that improve classification performance across 15 different benchmarks. The findings indicate significant performance enhancements compared to traditional methods, suggesting that LLMs can play a crucial role in prompt generation for image classification tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of LLMs for prompt generation, which shows promising results across multiple benchmarks. However, the paper has notable weaknesses, including a lack of exploration into other potential application domains such as video classification and object detection. Furthermore, the generalizability of the CuPL method remains unvalidated on underrepresented datasets, and the paper does not address limitations associated with LLMs, including prompt quality and bias. Additionally, it overlooks computational efficiency, the impact of fewer hand-written prompts on model interpretability, and alternative LLMs beyond GPT-3 that could broaden the applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is satisfactory, with a structured presentation of the methodology and results. However, the quality could be improved by addressing the limitations and potential challenges associated with the proposed approach. In terms of novelty, while the integration of LLMs for prompt generation is a fresh concept, the paper could benefit from a deeper exploration of various prompt engineering techniques and multimodal data incorporation. Reproducibility is not fully established, as there is a lack of detailed discussions on computational efficiency and the runtime of the CuPL method compared to traditional approaches.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to zero-shot image classification using LLM-generated prompts, showcasing notable improvements on benchmark datasets. However, it lacks a thorough exploration of its limitations and potential applications in related fields, which could enhance its impact and relevance in the broader context of machine learning.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the Customized Prompts via Language models (CuPL) approach, which significantly enhances zero-shot image classification performance by generating tailored prompts. The authors evaluate CuPL across 15 image classification benchmarks, reporting improvements in Top-1 and Top-5 accuracy metrics compared to traditional hand-written prompts from the CLIP model. Key findings include statistically significant gains in accuracy, particularly on datasets like ImageNet and DTD, and an analysis of how model scaling and hyperparameters affect performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its empirical evaluation of CuPL against established benchmarks, showcasing notable performance improvements through well-documented accuracy metrics and effect size reporting. The authors also provide valuable insights into the relationship between hyperparameter configurations and model performance. However, weaknesses include a lack of rigorous statistical testing, such as confidence intervals and p-values, which limits the robustness of claims regarding performance improvements. Furthermore, while the paper mentions statistical analysis techniques like ANOVA, it does not apply them explicitly, reducing the clarity of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that facilitates understanding of the methodology and results. The quality of the empirical analysis is commendable, but the novelty may be perceived as moderate since the approach builds on existing methods without introducing radically new techniques. Reproducibility could be improved with more detailed statistical analysis and explicit reporting of experimental settings, which would allow for better validation of the findings by other researchers.\n\n# Summary Of The Review\nOverall, the CuPL paper presents promising advancements in zero-shot image classification through the use of customized prompts, demonstrating notable performance improvements. However, the lack of rigorous statistical validation limits the claims made, suggesting that future research should focus on applying more comprehensive statistical methodologies to enhance the credibility of the results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the CuPL (Class Unbiased Prompt Learning) method, which aims to enhance zero-shot classification capabilities by generating descriptive prompts through a large language model (LLM). The methodology involves the generation of prompts tailored to specific datasets, which are then used to improve classification accuracy. The findings indicate that while CuPL achieves significant accuracy improvements in certain classes, the results are not uniformly distributed across all classes and exhibit dependency on the quality of the LLM employed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to prompt generation for zero-shot classification, potentially advancing the field by leveraging LLMs. However, the paper presents several weaknesses: it lacks generalizability across various datasets, is contingent on the quality of the LLM, and inherits biases from the training data of the model. Additionally, the limited exploration of prompt variability, performance inconsistencies across classes, and insufficient error analysis detract from the robustness of the findings. The absence of a comparative analysis with other methods further limits the contextual understanding of CuPL's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the absence of detailed comparative analysis and thorough exploration of evaluation metrics diminishes the perceived quality. The novelty of the approach is notable but is somewhat overshadowed by its limitations in applicability and reproducibility, particularly given the dependency on LLM quality and the potential for overfitting due to the reliance on specific prompt representations.\n\n# Summary Of The Review\nOverall, the paper introduces a novel method for prompt generation in zero-shot classification, with promising results in specific contexts. However, the limitations regarding generalizability, dependency on LLM quality, and a lack of comprehensive analysis present significant concerns that need to be addressed for the approach to be widely applicable and robust.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification\" explores the use of language models to generate prompts for image classification tasks without requiring prior examples. The authors introduce a methodology termed CuPL, which leverages open vocabulary models to create customized prompts, claiming to enhance zero-shot classification performance on various datasets. The findings indicate marginal improvements in accuracy, with an average gain of over one percentage point on ImageNet and consistent results across 15 datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its extensive evaluation across multiple datasets and the proposed framework for generating prompts via language models, making it more automated than traditional methods. However, the contributions are undermined by the lack of novelty, as the core idea of using language to assist in classification has been established in prior work. The improvements reported, while consistent, are relatively modest and may not be significant enough to warrant the enthusiasm presented by the authors.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a clear and organized manner, making it accessible to readers. However, the novelty of the approach is questionable, as it relies on established concepts in the field. The quality of the experimental design is adequate, though the modest gains in performance may raise concerns regarding the reproducibility of such improvements in practical applications. The use of standard techniques like t-SNE for visualization, while effective, does not demonstrate a forward-thinking approach.\n\n# Summary Of The Review\nOverall, the paper presents a well-structured approach to generating prompts for zero-shot image classification but fails to deliver significant novelty or impact. The findings, while consistent across multiple datasets, do not represent a substantial leap forward in the field. The enthusiasm expressed by the authors appears disproportionate to the contributions made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to zero-shot image classification by integrating multi-modal techniques, particularly focusing on the use of Large Language Models (LLMs) alongside visual embeddings and audio descriptions. The authors propose a framework named CuPL (Classifying using Prompt Learning) that enhances classification accuracy by generating tailored prompts for LLMs. Key findings indicate that the CuPL method significantly improves performance in zero-shot scenarios compared to traditional single-modal approaches.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of multi-modal knowledge representation, potentially leading to broader applicability in various domains. The emphasis on fine-tuning LLMs for specific datasets is particularly noteworthy, as it suggests a pathway to bridge the gap between zero-shot and fine-tuned performance. However, the paper lacks comprehensive error analysis, which is essential to identify the specific categories where CuPL underperforms. Additionally, while the proposed method is promising, there is insufficient empirical validation across different LLM architectures, which could limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to a broad audience. The quality of writing is high, with appropriate use of terminology and clear explanations of methodologies. In terms of novelty, the integration of various modalities for prompt generation is a significant contribution to the field. However, reproducibility could be enhanced by providing more details on the experimental setup and parameters used in the study, especially regarding temperature settings and attention mechanisms.\n\n# Summary Of The Review\nOverall, the paper offers a compelling approach to zero-shot image classification by leveraging multi-modal techniques. While the methodology is innovative and the results are promising, further empirical validation and error analysis are necessary to strengthen the claims made and enhance the framework's applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel method called Customized Prompts via Language models (CuPL) aimed at enhancing zero-shot image classification performance across various benchmarks. CuPL demonstrates substantial improvements in classification accuracy, achieving over a 1 percentage point gain in Top-1 accuracy on ImageNet and more than a 6 percentage point gain on the Describable Textures Dataset (DTD). The methodology is characterized by a significant reduction in the number of required hand-crafted prompts, utilizing only three templates compared to the 175 used in standard methods. The findings indicate that CuPL consistently outperforms traditional and alternative prompting strategies across multiple datasets, underscoring its robustness and versatility.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its effective methodology that reduces the reliance on extensive hand-crafted prompts, leading to improved performance across 13 out of 15 datasets examined. The results are compelling, demonstrating clear gains in accuracy while using fewer prompts, which enhances the practicality of the approach. However, a potential weakness is the limited exploration of the underlying reasons for the performance improvements, as the paper primarily focuses on empirical results without a deeper analysis of the mechanisms involved in CuPL's success. Additionally, while the paper showcases strong empirical results, further investigation into the generalizability of CuPL across different domains could strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow that facilitates understanding of the methodology and results. The quality of the presented results is high, supported by rigorous benchmarking across multiple datasets. The novelty of the approach is significant, as it introduces a more efficient way to generate prompts for zero-shot classification tasks. Reproducibility is somewhat addressed through the detailed description of the methodology and results, although additional information regarding the implementation specifics would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in zero-shot image classification through the CuPL method, showcasing impressive empirical results across various benchmarks. The approach is novel and offers practical benefits by reducing dependency on extensive hand-crafted prompts, although further exploration of the underlying mechanisms and generalizability could enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach termed \"Customized Prompts via Language models (CuPL),\" aimed at optimizing prompt engineering for language models. The methodology involves systematically generating and refining prompts tailored to specific tasks, leveraging a diverse set of datasets. Key findings indicate that CuPL significantly enhances model performance compared to traditional prompting techniques, demonstrating the potential for broader applications in natural language processing tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to prompt customization, which addresses a critical challenge in effectively utilizing language models. The detailed methodology section provides a comprehensive understanding of the proposed technique. However, weaknesses include inconsistencies in terminology usage, particularly with the abbreviation CuPL, which could confuse readers. Additionally, the dense writing style and lengthy paragraphs may hinder accessibility, making it challenging for a broader audience to engage with the content. Visuals, while referenced, lack clarity in captions, reducing their overall effectiveness in conveying information.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is impacted by complex sentence structures and repetitive language, which may deter engagement. While the quality of the methodology is high, the flow could be improved through better organization. The novelty of the approach is significant, offering fresh insights into prompt optimization. However, reproducibility may be limited due to the lack of detailed contextual examples for datasets and the need for clearer methodological steps.\n\n# Summary Of The Review\nOverall, while the paper introduces a promising and novel methodology for optimizing language model prompts, its clarity and accessibility could be improved. Enhancing the organization and presentation of results would further strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4471875885378456,
    -1.739101477346225,
    -1.7652969118396604,
    -1.6803722489827344,
    -1.7303900704045794,
    -1.8998809862042292,
    -1.5993073032484013,
    -1.8537555970747817,
    -1.6422159949635153,
    -1.7012295277336107,
    -1.685837239398671,
    -1.4740638478189114,
    -1.7344618069241344,
    -1.6486137310223623,
    -1.666437055170445,
    -1.688569539112671,
    -1.9230259386175648,
    -2.059404152637998,
    -1.8901992772066054,
    -1.737962952598654,
    -1.8161217084224621,
    -1.875941742526859,
    -1.8141972266673476,
    -1.8872883254922015,
    -1.7859312930848568,
    -1.8671190861132363,
    -1.6916862903015057,
    -1.6451460316844047,
    -1.9130189026824753
  ],
  "logp_cond": [
    [
      0.0,
      -2.2158015212525393,
      -2.237364902924845,
      -2.2302259640183717,
      -2.231651955747364,
      -2.2121127295317695,
      -2.306080675160053,
      -2.2541049772941544,
      -2.221988916657578,
      -2.263401462351977,
      -2.22934518701967,
      -2.3197388561689785,
      -2.2408776805093384,
      -2.2242446603010646,
      -2.2285461115850302,
      -2.2319603226929283,
      -2.23497294136282,
      -2.2250017598445693,
      -2.229243170624959,
      -2.227259740599995,
      -2.2589638130187217,
      -2.251142724173317,
      -2.231664484718944,
      -2.243041524542531,
      -2.2684825147367533,
      -2.2417685673873264,
      -2.2655143412857734,
      -2.2399558204988352,
      -2.2806665639414727
    ],
    [
      -1.3259935739050583,
      0.0,
      -1.2565086027812455,
      -1.225434493506389,
      -1.237099722681212,
      -1.2963924955028094,
      -1.4456633562538395,
      -1.3157125675353056,
      -1.2670039616892803,
      -1.3830937811760502,
      -1.2954254156054237,
      -1.4761812793349076,
      -1.2949135597120653,
      -1.2738305182569734,
      -1.3711801247678725,
      -1.2661623256657026,
      -1.3388447186830932,
      -1.2435094881235045,
      -1.3435854736992288,
      -1.2324015061680813,
      -1.3329018918658668,
      -1.3393004457909843,
      -1.31644349195993,
      -1.291491292307325,
      -1.3755636317784952,
      -1.2806367928763063,
      -1.3664910126738319,
      -1.2617797137396092,
      -1.385141123811708
    ],
    [
      -1.373874108084952,
      -1.2805795032759233,
      0.0,
      -1.2522707454897968,
      -1.2698607680436498,
      -1.3383750634075213,
      -1.4029308939610032,
      -1.2960983481130073,
      -1.297330129155578,
      -1.3846236402196268,
      -1.3292514678968406,
      -1.5044045503143657,
      -1.3226665412822176,
      -1.3115557446449504,
      -1.2860911820019316,
      -1.2843594374545284,
      -1.2864761523468973,
      -1.3117568174095644,
      -1.2988959010759036,
      -1.234133676951709,
      -1.3638425944710657,
      -1.327219362414879,
      -1.3600748619391885,
      -1.3456667761832513,
      -1.3462944638363954,
      -1.3250764122468621,
      -1.388852113406461,
      -1.336982447617698,
      -1.4471900613606086
    ],
    [
      -1.2571525661231622,
      -1.1388401545852287,
      -1.1433149663642814,
      0.0,
      -1.1132440407558435,
      -1.1907110543509918,
      -1.3406607742163508,
      -1.172669165962995,
      -1.1598406607854514,
      -1.2754643680393543,
      -1.1258484212758213,
      -1.411982750820239,
      -1.1773484533495422,
      -1.176964770258897,
      -1.2544511630120316,
      -1.1282506028333905,
      -1.1474711551429,
      -1.1460193968823824,
      -1.2540807316062133,
      -1.1381689764987093,
      -1.225156818679745,
      -1.2676078277599314,
      -1.2519302486822914,
      -1.1976369516161256,
      -1.2806879379316631,
      -1.2019582278128587,
      -1.2889941602749553,
      -1.1922120055428527,
      -1.3096091772026086
    ],
    [
      -1.3916486572012607,
      -1.2442426337095756,
      -1.2706309187920282,
      -1.241242832615072,
      0.0,
      -1.319945243507403,
      -1.4405735354512426,
      -1.3471011712540129,
      -1.3264068785353869,
      -1.352403245315917,
      -1.2413721869236418,
      -1.4596828033822935,
      -1.355191433325338,
      -1.3014395433458827,
      -1.3864025427890998,
      -1.3493749359005665,
      -1.3243669583367126,
      -1.3152966819840164,
      -1.370306577229889,
      -1.2763947837460266,
      -1.3464478363288286,
      -1.353450970611936,
      -1.2913690084689744,
      -1.3303240033822545,
      -1.386195966093503,
      -1.2627968503529972,
      -1.3859426409297873,
      -1.30661824499441,
      -1.399908491717554
    ],
    [
      -1.4680562941074344,
      -1.41653552949064,
      -1.4403083123694946,
      -1.3886672960277247,
      -1.441852808979859,
      0.0,
      -1.563366186807266,
      -1.4343838363483528,
      -1.4027333335455308,
      -1.5362115770342146,
      -1.4448096309262892,
      -1.6216756106785135,
      -1.4126718166955978,
      -1.4564113877376412,
      -1.5057621606043423,
      -1.4339781139538492,
      -1.4368612947166877,
      -1.4064544554444498,
      -1.4659663339289974,
      -1.3995673021412014,
      -1.5174058847813492,
      -1.512942293172622,
      -1.4988978689140915,
      -1.4347837932290155,
      -1.5063008476716648,
      -1.4944734845548608,
      -1.5424787880679578,
      -1.4121456439731948,
      -1.5627469998824501
    ],
    [
      -1.3164765394776297,
      -1.2583736364552545,
      -1.1813268406412403,
      -1.2272811490076245,
      -1.2760149272680055,
      -1.1921479210477697,
      0.0,
      -1.1764502327039201,
      -1.2009274896845112,
      -1.2408992934946124,
      -1.2445660180320426,
      -1.257246124732155,
      -1.2068273432349828,
      -1.2215516349357736,
      -1.265540301142335,
      -1.2190175346042535,
      -1.201465973899124,
      -1.207238553976033,
      -1.2466737740051261,
      -1.2112587835240771,
      -1.2170008861080914,
      -1.242469688674487,
      -1.2670264574653352,
      -1.2223344470381983,
      -1.225587867017275,
      -1.294815476424266,
      -1.2678351428715573,
      -1.2256689398282843,
      -1.2667380201999574
    ],
    [
      -1.4419731675910255,
      -1.3737384718281918,
      -1.2887764931783057,
      -1.321914463750888,
      -1.369811826526125,
      -1.386351879951002,
      -1.4561373987628774,
      0.0,
      -1.391730329782378,
      -1.4406481305151422,
      -1.4041587649636067,
      -1.5974557425434113,
      -1.32137283932947,
      -1.4355846119947082,
      -1.394565670784307,
      -1.3363645796498294,
      -1.3856476987463051,
      -1.258758231969228,
      -1.3893918294398406,
      -1.3916850519595099,
      -1.4247662648590624,
      -1.399305055301374,
      -1.4601276224054343,
      -1.3560682921143425,
      -1.4628343700575326,
      -1.372674058422985,
      -1.4983592462274842,
      -1.322447232501103,
      -1.5066753720098034
    ],
    [
      -1.2472235537708976,
      -1.1842598333830021,
      -1.2080144992510946,
      -1.1296980828127665,
      -1.1887343281568097,
      -1.191581003135187,
      -1.2993470887148357,
      -1.2301666095456028,
      0.0,
      -1.2531401213333506,
      -1.1706140797855782,
      -1.3568790417843752,
      -1.234536622635808,
      -1.155319925006166,
      -1.2480304086261473,
      -1.207543688904338,
      -1.1940320327879805,
      -1.194191425812766,
      -1.2598514553994167,
      -1.1223528926425006,
      -1.172836538277784,
      -1.2025818932812418,
      -1.1923679556194684,
      -1.1633190659697146,
      -1.2750655396573598,
      -1.215903687200657,
      -1.281851376706265,
      -1.1279621311780805,
      -1.2966622186353405
    ],
    [
      -1.404683993119712,
      -1.3717214497570247,
      -1.3601057166355452,
      -1.3748002640324752,
      -1.3590777214744054,
      -1.3769089601823485,
      -1.4058862538512782,
      -1.320498391239748,
      -1.3721832237999998,
      0.0,
      -1.3270045077006585,
      -1.4467929226010918,
      -1.373899287138036,
      -1.3574384636422,
      -1.3437118244251982,
      -1.3767949098688261,
      -1.3230228222910057,
      -1.3957100593776912,
      -1.3717925179621493,
      -1.3344398006351426,
      -1.2898267369365388,
      -1.3471059836071595,
      -1.3157171964896766,
      -1.3389533081410867,
      -1.2896489326722622,
      -1.3623948378301924,
      -1.3965577334960955,
      -1.387084192220459,
      -1.3919747479695874
    ],
    [
      -1.2797279479487242,
      -1.2499662898262354,
      -1.228528653138374,
      -1.1534831366753904,
      -1.1580006157576301,
      -1.2665564214038398,
      -1.355891288716594,
      -1.267391048327647,
      -1.1892065978166153,
      -1.2964419021822031,
      0.0,
      -1.39874795125452,
      -1.2898803263947767,
      -1.2289197927266784,
      -1.2696016812906903,
      -1.3115135816552357,
      -1.2391751824596102,
      -1.2640194596468175,
      -1.2709308501311871,
      -1.1580095664631747,
      -1.2626367152208053,
      -1.2440189124727195,
      -1.257462671678351,
      -1.2034549648962216,
      -1.2842774353007402,
      -1.2340429502913564,
      -1.331803766911097,
      -1.264442528400995,
      -1.3591825169436937
    ],
    [
      -1.2767180445511785,
      -1.2250592355574315,
      -1.2458300598397207,
      -1.2441336189779406,
      -1.238012254040602,
      -1.224968745234588,
      -1.2572427030644577,
      -1.2326165242607907,
      -1.2445016220967082,
      -1.2398996448533148,
      -1.2374079344527555,
      0.0,
      -1.2636139922938476,
      -1.2431306589912126,
      -1.223317159475878,
      -1.253726941822203,
      -1.2198784690231383,
      -1.2454438257787976,
      -1.2453360624283238,
      -1.2272533676523123,
      -1.2389818016765872,
      -1.229576320919476,
      -1.2369090478102192,
      -1.2291024048565478,
      -1.24062701826392,
      -1.2160597314154622,
      -1.1987846036556578,
      -1.2490545165497933,
      -1.1987832357045474
    ],
    [
      -1.3360285814078892,
      -1.2449846368632291,
      -1.2111377671958954,
      -1.1898330679380855,
      -1.2585111612745477,
      -1.2459665372779487,
      -1.3313692761974782,
      -1.180762249409728,
      -1.2500054367103772,
      -1.3383264533522206,
      -1.2833713443182913,
      -1.4381475886936919,
      0.0,
      -1.2651446953195151,
      -1.3192902347884654,
      -1.2156020191244614,
      -1.2960188909635424,
      -1.1726257076667197,
      -1.2870879799598502,
      -1.2636540553643905,
      -1.277755640943595,
      -1.2591475598208413,
      -1.3297760638794025,
      -1.2541636837189936,
      -1.3162558952815535,
      -1.2671273567426327,
      -1.355472142929452,
      -1.241538702705859,
      -1.3470822946655567
    ],
    [
      -1.2828339822458412,
      -1.1113407352090252,
      -1.1796762920525854,
      -1.171829581741784,
      -1.1757461312273896,
      -1.2189731959620616,
      -1.3113169697130909,
      -1.227577777285656,
      -1.1036184458868905,
      -1.2324131726017384,
      -1.151572703851213,
      -1.3451860177622788,
      -1.195816512305973,
      0.0,
      -1.2159199380411159,
      -1.2158246963370183,
      -1.2195791649194927,
      -1.2036894275469818,
      -1.2083074153374789,
      -1.1060079515299537,
      -1.2339727657506994,
      -1.2023094144596058,
      -1.1133904790097446,
      -1.1989289420864904,
      -1.2467556334889645,
      -1.1966901612797907,
      -1.217621396798707,
      -1.218836807655983,
      -1.2662282280538655
    ],
    [
      -1.2411147103780025,
      -1.2070662970225057,
      -1.122623429777053,
      -1.1883640161122804,
      -1.18547065871863,
      -1.1915505249936156,
      -1.2890835635594955,
      -1.1656459796532501,
      -1.2045749016915783,
      -1.2466222939524303,
      -1.197733004480173,
      -1.3671359540259425,
      -1.1879976347690586,
      -1.2140249087563615,
      0.0,
      -1.1837787289346144,
      -1.215091120033266,
      -1.1938722497293892,
      -1.143550935266454,
      -1.2124599427035907,
      -1.2248512441594452,
      -1.101354975383796,
      -1.249536209454234,
      -1.2305446427093334,
      -1.256274960341015,
      -1.148926287839264,
      -1.2412939580437863,
      -1.2430184821668202,
      -1.2717172211476693
    ],
    [
      -1.2567148909701038,
      -1.1784581648706494,
      -1.2007684290910903,
      -1.1587854435689429,
      -1.2151711312495728,
      -1.196724467975212,
      -1.327506749465357,
      -1.166320054909113,
      -1.2144811729160891,
      -1.2800769070848053,
      -1.2135233251806223,
      -1.398388923098393,
      -1.1807704878490006,
      -1.225238933407091,
      -1.274761014664348,
      0.0,
      -1.213934040742102,
      -1.173977782006969,
      -1.240194934605845,
      -1.1953933424822047,
      -1.222806247437943,
      -1.2521795330970331,
      -1.274326479619446,
      -1.1883853920335097,
      -1.3122471917655585,
      -1.2336531825950088,
      -1.3071485332850263,
      -1.2157789861969444,
      -1.3056928692580456
    ],
    [
      -1.5757967012269443,
      -1.490801089460796,
      -1.4750872942193272,
      -1.476749914062926,
      -1.5214569828737348,
      -1.5255695186323903,
      -1.6070257231074658,
      -1.54682873505552,
      -1.4535739943276513,
      -1.5189602829329083,
      -1.4853780580385816,
      -1.69282446395945,
      -1.5130779636811507,
      -1.5116473351325688,
      -1.5387066208208195,
      -1.5252041682463537,
      0.0,
      -1.4877856190272831,
      -1.5239875164977368,
      -1.3850805630295262,
      -1.5071470110547545,
      -1.541614275501628,
      -1.5040359514567032,
      -1.5299237201831388,
      -1.5143879741748016,
      -1.561181376392346,
      -1.6075590912770392,
      -1.4801761682550336,
      -1.6530495742037012
    ],
    [
      -1.6323549074364943,
      -1.5089443960632851,
      -1.4913911437242673,
      -1.4980389874361704,
      -1.5239182568471261,
      -1.5162632005782324,
      -1.663484336507987,
      -1.4872178401703755,
      -1.5343257408314146,
      -1.6491229866010535,
      -1.5547426305008047,
      -1.7361336950972104,
      -1.4904144575946374,
      -1.5555112702144858,
      -1.5987280952710992,
      -1.5000046482241,
      -1.5414759340067337,
      0.0,
      -1.5678664919337175,
      -1.5374010926495352,
      -1.5617065588514234,
      -1.5384554025396189,
      -1.6345788426675751,
      -1.5159145483676575,
      -1.6512559636644988,
      -1.5257463983004937,
      -1.6372747111848542,
      -1.3975321284494173,
      -1.6213454120389617
    ],
    [
      -1.468587589771129,
      -1.4135074580390565,
      -1.339494293659134,
      -1.3986208871140127,
      -1.4413707410852854,
      -1.3958489197011952,
      -1.4469037970096372,
      -1.3555979503454867,
      -1.411001828408334,
      -1.4369330822003756,
      -1.4229753391209163,
      -1.535389746996558,
      -1.370754171730182,
      -1.4193951904159747,
      -1.3776708633729775,
      -1.382598923745362,
      -1.4122303448323523,
      -1.3829410107210147,
      0.0,
      -1.4162169937977673,
      -1.4340510446737584,
      -1.3863396774895633,
      -1.4404248323195057,
      -1.4331271792459517,
      -1.4282632811751736,
      -1.4479067246445303,
      -1.4438402746141468,
      -1.4125000676219073,
      -1.476817035768803
    ],
    [
      -1.2993397002717504,
      -1.2461054007812922,
      -1.2291242484890892,
      -1.2697681824728735,
      -1.2831193234601064,
      -1.2877004401394447,
      -1.4014242221642053,
      -1.2831868754298128,
      -1.2259706816420552,
      -1.306657653499602,
      -1.281047826880261,
      -1.4538220540630884,
      -1.3087003263736081,
      -1.2383165478703246,
      -1.3189656449550267,
      -1.3219018240724074,
      -1.2579659917529977,
      -1.2896278316777967,
      -1.307910106015508,
      0.0,
      -1.3232855464288318,
      -1.3092304585519599,
      -1.2894278901400382,
      -1.2659019143614847,
      -1.3442398819856436,
      -1.3414060294761354,
      -1.3264428384236016,
      -1.2227588511192549,
      -1.359890221232203
    ],
    [
      -1.4511339220559714,
      -1.4201940425551627,
      -1.3880774855076166,
      -1.3802410988582945,
      -1.3918850302529764,
      -1.4354157058214112,
      -1.4743623662382912,
      -1.392518629622548,
      -1.353772114804379,
      -1.3888225773982363,
      -1.3738157890127878,
      -1.5641019444781157,
      -1.3945916501131557,
      -1.4367010519386718,
      -1.428324843516982,
      -1.3824112039767273,
      -1.351432738537353,
      -1.3975270560948834,
      -1.4404767206839253,
      -1.3968974004026304,
      0.0,
      -1.3831157792132809,
      -1.4280471379650332,
      -1.4041824970833434,
      -1.4042292089340815,
      -1.400495216887253,
      -1.5167037510395074,
      -1.3821494902372116,
      -1.5001388172240173
    ],
    [
      -1.4285849435534437,
      -1.407014838974371,
      -1.3278352055734288,
      -1.3818092008605465,
      -1.3763822757008783,
      -1.4151557126440182,
      -1.4971383571240624,
      -1.3839929705218146,
      -1.3969714632481005,
      -1.4169997944330117,
      -1.3769587747295255,
      -1.5468357857865347,
      -1.3938385863440306,
      -1.3936373343433024,
      -1.323889964497989,
      -1.4018737726292874,
      -1.3997150998969121,
      -1.3642647001761694,
      -1.408219888652289,
      -1.3850610567987514,
      -1.403412339939748,
      0.0,
      -1.429654944904659,
      -1.4038874413572904,
      -1.4385111385017892,
      -1.3315289568453703,
      -1.450335968808826,
      -1.3985758035722264,
      -1.454377406311955
    ],
    [
      -1.4464682598933176,
      -1.3811731744923332,
      -1.4180668878056313,
      -1.4073223687507586,
      -1.3629390425740324,
      -1.465354225006065,
      -1.518388181633729,
      -1.4740440914128219,
      -1.346589672547186,
      -1.4174881610330388,
      -1.4042744901762152,
      -1.5471238130516787,
      -1.4354092869422146,
      -1.3136318185357738,
      -1.4521349892132336,
      -1.4383984585694918,
      -1.3942582821627683,
      -1.4498192723153043,
      -1.4757270049590685,
      -1.3557035736209517,
      -1.394767886050017,
      -1.4582161877993343,
      0.0,
      -1.4398191714237705,
      -1.4683309717588313,
      -1.4252238415166638,
      -1.4138670932268314,
      -1.4407719277525708,
      -1.486744466309835
    ],
    [
      -1.4933902772108407,
      -1.4407279292085031,
      -1.4316139139198307,
      -1.4040317708509065,
      -1.4568784066791247,
      -1.4382678587356679,
      -1.5386295698378665,
      -1.3983438162185164,
      -1.411015117328635,
      -1.4738440694721877,
      -1.3886637665995956,
      -1.602819504759875,
      -1.4504009127409259,
      -1.4117271453035207,
      -1.4921194012886583,
      -1.4419577694841448,
      -1.4017163059502247,
      -1.3860370474177635,
      -1.4851060761423456,
      -1.4151481008885456,
      -1.4274330255993277,
      -1.4636675798171406,
      -1.43566515352124,
      0.0,
      -1.4638615938662893,
      -1.4598885630323155,
      -1.4850755983113582,
      -1.3723417705365855,
      -1.4845053446041578
    ],
    [
      -1.4238281854814996,
      -1.4265459110498906,
      -1.3765537440165798,
      -1.4160565548466542,
      -1.3858819034156973,
      -1.3944734245231025,
      -1.4063868947922935,
      -1.3953060825050831,
      -1.401330093384835,
      -1.3165959268517462,
      -1.3423087182049989,
      -1.5209671645272624,
      -1.373650951523621,
      -1.4155145559344626,
      -1.376772379945462,
      -1.390106234057493,
      -1.3695377940396303,
      -1.4221091734212095,
      -1.3837071341707976,
      -1.3892754455312968,
      -1.3813172427146079,
      -1.354298798369803,
      -1.3786245739398204,
      -1.3605752450224875,
      0.0,
      -1.4306840655519704,
      -1.4344508819025874,
      -1.3825108355035027,
      -1.4833682428256583
    ],
    [
      -1.4870421223342358,
      -1.359618400476419,
      -1.3647107213808354,
      -1.3989380304935992,
      -1.3574797824495144,
      -1.4170300251611128,
      -1.5525455150228133,
      -1.3936853407848966,
      -1.4298094231547438,
      -1.5165693587089897,
      -1.3842688485933312,
      -1.569000372762244,
      -1.4260298381667817,
      -1.4275772711636634,
      -1.378267852218566,
      -1.4416990381735573,
      -1.392036651293563,
      -1.393104123375356,
      -1.4683854672564436,
      -1.4487366883756498,
      -1.3998528677140925,
      -1.3717368749671235,
      -1.4266453344747918,
      -1.4120437701455606,
      -1.4803841000619626,
      0.0,
      -1.4598066534043546,
      -1.4023169834365914,
      -1.4660608766187195
    ],
    [
      -1.4103770384992103,
      -1.3545766920573294,
      -1.3810556458678527,
      -1.3985356871401435,
      -1.3523199360086904,
      -1.389363924178162,
      -1.412604654890768,
      -1.3632714521718514,
      -1.397037717197555,
      -1.3656846423803752,
      -1.3857472350051692,
      -1.4035578857194035,
      -1.3962016284081629,
      -1.3662206572664823,
      -1.378622763226289,
      -1.3932922962498187,
      -1.3533123097336592,
      -1.3897779855018324,
      -1.3751442291280107,
      -1.4050719341913918,
      -1.3777486444987728,
      -1.3833975080383532,
      -1.3095621761033927,
      -1.3349089102765574,
      -1.3611699180842884,
      -1.3504049564180745,
      0.0,
      -1.3834034204072885,
      -1.3423589704348273
    ],
    [
      -1.3080381448646434,
      -1.2612639085302302,
      -1.2067839876662712,
      -1.2145008699304642,
      -1.2967468213133788,
      -1.270385173764535,
      -1.3316838026355766,
      -1.218001467986004,
      -1.1898043537383938,
      -1.320951797191411,
      -1.2530527004341465,
      -1.3902562288139284,
      -1.2582081598185557,
      -1.2326443066293995,
      -1.2578253320934771,
      -1.231132534875873,
      -1.235892449948545,
      -1.151214785203045,
      -1.2622735755208752,
      -1.1910343191784634,
      -1.2548027468915648,
      -1.2332004259969784,
      -1.2861785542972128,
      -1.1581574116893434,
      -1.2928198409837188,
      -1.262986588945278,
      -1.3013055605624342,
      0.0,
      -1.2999181148558023
    ],
    [
      -1.5500036005812972,
      -1.486664715118766,
      -1.5199536241966196,
      -1.5077033184763784,
      -1.4902988737931537,
      -1.5270882494786726,
      -1.5398129580090782,
      -1.495067417839719,
      -1.5164403436347338,
      -1.540782161379393,
      -1.5274676429269298,
      -1.4909700507253025,
      -1.5249471100930176,
      -1.4610154077581767,
      -1.5114088973457518,
      -1.5106318497012023,
      -1.5268936332931937,
      -1.5350136126048228,
      -1.525123318036962,
      -1.5176229271552986,
      -1.5200727727335843,
      -1.4816236113219254,
      -1.4773962055903382,
      -1.4569844214223775,
      -1.5444021550181695,
      -1.4626854208129287,
      -1.4784770965323462,
      -1.4748905782384738,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2313860672853063,
      0.20982268561300055,
      0.21696162451947387,
      0.21553563279048138,
      0.23507485900607605,
      0.1411069133777927,
      0.1930826112436912,
      0.22519867188026765,
      0.18378612618586843,
      0.2178424015181757,
      0.1274487323688671,
      0.20630990802850713,
      0.22294292823678097,
      0.21864147695281533,
      0.21522726584491725,
      0.21221464717502547,
      0.2221858286932763,
      0.21794441791288666,
      0.21992784793785036,
      0.18822377551912384,
      0.1960448643645285,
      0.2155231038189016,
      0.20414606399531454,
      0.17870507380109224,
      0.20541902115051913,
      0.18167324725207212,
      0.20723176803901033,
      0.16652102459637286
    ],
    [
      0.41310790344116666,
      0.0,
      0.48259287456497946,
      0.513666983839836,
      0.5020017546650131,
      0.44270898184341556,
      0.29343812109238554,
      0.42338890981091937,
      0.4720975156569447,
      0.3560076961701748,
      0.4436760617408013,
      0.2629201980113174,
      0.44418791763415966,
      0.46527095908925165,
      0.36792135257835246,
      0.47293915168052236,
      0.4002567586631318,
      0.4955919892227205,
      0.39551600364699624,
      0.5066999711781437,
      0.40619958548035817,
      0.3998010315552407,
      0.42265798538629507,
      0.44761018503890004,
      0.36353784556772983,
      0.4584646844699187,
      0.3726104646723931,
      0.4773217636066158,
      0.35396035353451705
    ],
    [
      0.3914228037547085,
      0.48471740856373713,
      0.0,
      0.5130261663498636,
      0.49543614379601064,
      0.4269218484321391,
      0.3623660178786572,
      0.46919856372665314,
      0.4679667826840823,
      0.3806732716200336,
      0.43604544394281985,
      0.2608923615252947,
      0.4426303705574428,
      0.45374116719471,
      0.4792057298377288,
      0.480937474385132,
      0.47882075949276315,
      0.453540094430096,
      0.46640101076375684,
      0.5311632348879514,
      0.4014543173685947,
      0.4380775494247815,
      0.40522204990047195,
      0.41963013565640916,
      0.419002448003265,
      0.4402204995927983,
      0.37644479843319933,
      0.4283144642219625,
      0.31810685047905185
    ],
    [
      0.42321968285957223,
      0.5415320943975057,
      0.537057282618453,
      0.0,
      0.5671282082268909,
      0.4896611946317426,
      0.33971147476638364,
      0.5077030830197393,
      0.520531588197283,
      0.40490788094338015,
      0.5545238277069131,
      0.26838949816249547,
      0.5030237956331922,
      0.5034074787238374,
      0.4259210859707028,
      0.5521216461493439,
      0.5329010938398344,
      0.534352852100352,
      0.42629151737652116,
      0.5422032724840251,
      0.4552154303029894,
      0.412764421222803,
      0.428442000300443,
      0.4827352973666088,
      0.3996843110510713,
      0.47841402116987575,
      0.3913780887077791,
      0.48816024343988174,
      0.3707630717801258
    ],
    [
      0.33874141320331863,
      0.4861474366950038,
      0.4597591516125512,
      0.4891472377895074,
      0.0,
      0.4104448268971763,
      0.2898165349533368,
      0.3832888991505665,
      0.4039831918691925,
      0.37798682508866244,
      0.48901788348093755,
      0.27070726702228587,
      0.3751986370792413,
      0.42895052705869663,
      0.34398752761547957,
      0.3810151345040129,
      0.4060231120678668,
      0.415093388420563,
      0.36008349317469035,
      0.4539952866585528,
      0.3839422340757508,
      0.3769390997926434,
      0.439021061935605,
      0.40006606702232483,
      0.34419410431107633,
      0.4675932200515822,
      0.3444474294747921,
      0.42377182541016944,
      0.33048157868702543
    ],
    [
      0.43182469209679475,
      0.4833454567135891,
      0.45957267383473455,
      0.5112136901765045,
      0.45802817722437017,
      0.0,
      0.33651479939696327,
      0.4654971498558764,
      0.4971476526586984,
      0.36366940917001456,
      0.45507135527793996,
      0.27820537552571567,
      0.48720916950863136,
      0.44346959846658796,
      0.39411882559988687,
      0.46590287225037996,
      0.4630196914875415,
      0.4934265307597794,
      0.43391465227523174,
      0.5003136840630278,
      0.38247510142288,
      0.3869386930316072,
      0.4009831172901377,
      0.4650971929752137,
      0.39358013853256435,
      0.40540750164936834,
      0.3574021981362714,
      0.48773534223103443,
      0.33713398632177904
    ],
    [
      0.28283076377077165,
      0.34093366679314685,
      0.41798046260716104,
      0.3720261542407768,
      0.32329237598039584,
      0.40715938220063164,
      0.0,
      0.4228570705444812,
      0.3983798135638901,
      0.35840800975378895,
      0.3547412852163587,
      0.3420611785162464,
      0.39247996001341856,
      0.37775566831262775,
      0.33376700210606636,
      0.38028976864414776,
      0.3978413293492773,
      0.39206874927236823,
      0.3526335292432752,
      0.3880485197243242,
      0.3823064171403099,
      0.3568376145739143,
      0.33228084578306616,
      0.37697285621020304,
      0.37371943623112625,
      0.3044918268241352,
      0.331472160376844,
      0.373638363420117,
      0.3325692830484439
    ],
    [
      0.4117824294837562,
      0.4800171252465899,
      0.564979103896476,
      0.5318411333238937,
      0.48394377054865667,
      0.4674037171237797,
      0.39761819831190426,
      0.0,
      0.4620252672924037,
      0.4131074665596395,
      0.44959683211117496,
      0.2562998545313704,
      0.5323827577453117,
      0.41817098508007344,
      0.4591899262904746,
      0.5173910174249523,
      0.46810789832847655,
      0.5949973651055536,
      0.464363767634941,
      0.4620705451152718,
      0.4289893322157192,
      0.4544505417734077,
      0.3936279746693474,
      0.4976873049604391,
      0.3909212270172491,
      0.4810815386517966,
      0.3553963508472975,
      0.5313083645736787,
      0.34708022506497826
    ],
    [
      0.3949924411926178,
      0.4579561615805132,
      0.4342014957124207,
      0.5125179121507488,
      0.45348166680670565,
      0.45063499182832834,
      0.3428689062486796,
      0.4120493854179126,
      0.0,
      0.3890758736301647,
      0.47160191517793715,
      0.2853369531791401,
      0.40767937232770723,
      0.48689606995734924,
      0.394185586337368,
      0.4346723060591773,
      0.4481839621755348,
      0.4480245691507494,
      0.38236453956409866,
      0.5198631023210147,
      0.46937945668573144,
      0.43963410168227357,
      0.44984803934404693,
      0.47889692899380076,
      0.36715045530615553,
      0.42631230776285833,
      0.3603646182572504,
      0.5142538637854348,
      0.34555377632817486
    ],
    [
      0.29654553461389876,
      0.329508077976586,
      0.34112381109806544,
      0.32642926370113545,
      0.34215180625920527,
      0.32432056755126215,
      0.2953432738823325,
      0.3807311364938626,
      0.3290463039336109,
      0.0,
      0.3742250200329522,
      0.25443660513251887,
      0.3273302405955747,
      0.3437910640914106,
      0.3575177033084125,
      0.32443461786478456,
      0.37820670544260504,
      0.3055194683559195,
      0.3294370097714614,
      0.36678972709846813,
      0.41140279079707187,
      0.3541235441264512,
      0.38551233124393414,
      0.362276219592524,
      0.4115805950613485,
      0.33883468990341825,
      0.3046717942375152,
      0.3141453355131516,
      0.30925477976402327
    ],
    [
      0.4061092914499467,
      0.43587094957243555,
      0.457308586260297,
      0.5323541027232805,
      0.5278366236410408,
      0.4192808179948311,
      0.329945950682077,
      0.418446191071024,
      0.4966306415820556,
      0.3893953372164678,
      0.0,
      0.2870892881441509,
      0.39595691300389424,
      0.4569174466719925,
      0.41623555810798063,
      0.37432365774343523,
      0.4466620569390607,
      0.4218177797518534,
      0.4149063892674838,
      0.5278276729354963,
      0.4232005241778656,
      0.44181832692595147,
      0.4283745677203199,
      0.4823822745024493,
      0.4015598040979307,
      0.45179428910731456,
      0.354033472487574,
      0.4213947109976759,
      0.3266547224549772
    ],
    [
      0.19734580326773288,
      0.24900461226147996,
      0.2282337879791907,
      0.2299302288409708,
      0.23605159377830942,
      0.24909510258432332,
      0.2168211447544537,
      0.2414473235581207,
      0.2295622257222032,
      0.2341642029655966,
      0.23665591336615588,
      0.0,
      0.2104498555250638,
      0.23093318882769887,
      0.2507466883430334,
      0.2203369059967084,
      0.2541853787957731,
      0.22862002204011378,
      0.22872778539058758,
      0.24681048016659912,
      0.2350820461423242,
      0.24448752689943531,
      0.23715480000869227,
      0.24496144296236366,
      0.23343682955499134,
      0.2580041164034492,
      0.27527924416325367,
      0.22500933126911815,
      0.27528061211436405
    ],
    [
      0.39843322551624527,
      0.4894771700609053,
      0.523324039728239,
      0.5446287389860489,
      0.4759506456495868,
      0.4884952696461857,
      0.4030925307266562,
      0.5536995575144064,
      0.4844563702137572,
      0.3961353535719139,
      0.4510904626058432,
      0.29631421823044257,
      0.0,
      0.4693171116046193,
      0.415171572135669,
      0.518859787799673,
      0.43844291596059204,
      0.5618360992574147,
      0.4473738269642842,
      0.47080775155974397,
      0.45670616598053937,
      0.4753142471032932,
      0.4046857430447319,
      0.48029812320514087,
      0.4182059116425809,
      0.46733445018150177,
      0.37898966399468237,
      0.49292310421827534,
      0.3873795122585777
    ],
    [
      0.3657797487765211,
      0.5372729958133371,
      0.4689374389697769,
      0.4767841492805782,
      0.4728675997949727,
      0.42964053506030075,
      0.33729676130927144,
      0.4210359537367063,
      0.5449952851354718,
      0.41620055842062387,
      0.4970410271711494,
      0.30342771326008355,
      0.45279721871638934,
      0.0,
      0.43269379298124644,
      0.432789034685344,
      0.4290345661028696,
      0.4449243034753805,
      0.44030631568488343,
      0.5426057794924086,
      0.41464096527166294,
      0.4463043165627565,
      0.5352232520126177,
      0.44968478893587194,
      0.40185809753339785,
      0.45192356974257164,
      0.43099233422365524,
      0.4297769233663793,
      0.3823855029684968
    ],
    [
      0.4253223447924426,
      0.4593707581479394,
      0.5438136253933921,
      0.4780730390581647,
      0.48096639645181494,
      0.47488653017682947,
      0.37735349161094955,
      0.5007910755171949,
      0.4618621534788667,
      0.41981476121801475,
      0.468704050690272,
      0.2993011011445026,
      0.47843942040138643,
      0.45241214641408356,
      0.0,
      0.4826583262358306,
      0.45134593513717913,
      0.47256480544105584,
      0.5228861199039911,
      0.45397711246685435,
      0.4415858110109998,
      0.5650820797866491,
      0.4169008457162111,
      0.43589241246111166,
      0.41016209482943,
      0.517510767331181,
      0.42514309712665876,
      0.42341857300362484,
      0.3947198340227758
    ],
    [
      0.43185464814256713,
      0.5101113742420216,
      0.48780111002158066,
      0.5297840955437281,
      0.47339840786309817,
      0.4918450711374589,
      0.361062789647314,
      0.5222494842035579,
      0.4740883661965818,
      0.4084926320278657,
      0.4750462139320486,
      0.2901806160142779,
      0.5077990512636703,
      0.46333060570557993,
      0.41380852444832295,
      0.0,
      0.47463549837056895,
      0.5145917571057019,
      0.448374604506826,
      0.4931761966304662,
      0.46576329167472785,
      0.4363900060156378,
      0.41424305949322493,
      0.5001841470791613,
      0.37632234734711245,
      0.45491635651766216,
      0.3814210058276446,
      0.4727905529157266,
      0.3828766698546253
    ],
    [
      0.3472292373906205,
      0.43222484915676884,
      0.4479386443982376,
      0.44627602455463866,
      0.40156895574383,
      0.39745641998517445,
      0.31600021551009894,
      0.37619720356204467,
      0.46945194428991344,
      0.40406565568465647,
      0.4376478805789832,
      0.2302014746581147,
      0.4099479749364141,
      0.411378603484996,
      0.3843193177967452,
      0.397821770371211,
      0.0,
      0.43524031959028164,
      0.399038422119828,
      0.5379453755880386,
      0.4158789275628103,
      0.38141166311593677,
      0.41898998716086155,
      0.39310221843442594,
      0.4086379644427631,
      0.36184456222521866,
      0.31546684734052555,
      0.44284977036253115,
      0.2699763644138635
    ],
    [
      0.4270492452015038,
      0.5504597565747129,
      0.5680130089137307,
      0.5613651652018277,
      0.5354858957908719,
      0.5431409520597656,
      0.39591981613001104,
      0.5721863124676225,
      0.5250784118065834,
      0.41028116603694453,
      0.5046615221371933,
      0.32327045754078765,
      0.5689896950433606,
      0.5038928824235123,
      0.4606760573668989,
      0.559399504413898,
      0.5179282186312644,
      0.0,
      0.4915376607042805,
      0.5220030599884629,
      0.49769759378657463,
      0.5209487500983792,
      0.4248253099704229,
      0.5434896042703405,
      0.40814818897349925,
      0.5336577543375043,
      0.4221294414531438,
      0.6618720241885807,
      0.4380587405990364
    ],
    [
      0.4216116874354765,
      0.4766918191675489,
      0.5507049835474713,
      0.49157839009259274,
      0.44882853612132,
      0.49435035750541023,
      0.4432954801969682,
      0.5346013268611187,
      0.4791974487982713,
      0.4532661950062298,
      0.4672239380856891,
      0.35480953021004735,
      0.5194451054764233,
      0.4708040867906307,
      0.5125284138336279,
      0.5076003534612434,
      0.4779689323742531,
      0.5072582664855907,
      0.0,
      0.4739822834088381,
      0.456148232532847,
      0.5038595997170421,
      0.4497744448870997,
      0.45707209796065373,
      0.46193599603143176,
      0.4422925525620751,
      0.44635900259245864,
      0.47769920958469814,
      0.41338224143780233
    ],
    [
      0.4386232523269036,
      0.4918575518173618,
      0.5088387041095648,
      0.4681947701257805,
      0.4548436291385476,
      0.45026251245920923,
      0.3365387304344487,
      0.45477607716884116,
      0.5119922709565987,
      0.431305299099052,
      0.456915125718393,
      0.28414089853556557,
      0.42926262622504585,
      0.49964640472832933,
      0.4189973076436273,
      0.41606112852624655,
      0.47999696084565624,
      0.44833512092085726,
      0.4300528465831459,
      0.0,
      0.4146774061698222,
      0.4287324940466941,
      0.44853506245861574,
      0.4720610382371693,
      0.3937230706130104,
      0.39655692312251856,
      0.4115201141750524,
      0.5152041014793991,
      0.3780727313664509
    ],
    [
      0.36498778636649076,
      0.39592766586729944,
      0.42804422291484556,
      0.43588060956416763,
      0.42423667816948574,
      0.3807060026010509,
      0.3417593421841709,
      0.4236030787999141,
      0.46234959361808303,
      0.4272991310242258,
      0.44230591940967434,
      0.25201976394434644,
      0.42153005830930645,
      0.3794206564837903,
      0.38779686490548015,
      0.4337105044457348,
      0.4646889698851091,
      0.41859465232757875,
      0.37564498773853683,
      0.4192243080198317,
      0.0,
      0.43300592920918124,
      0.3880745704574289,
      0.4119392113391187,
      0.4118924994883806,
      0.41562649153520903,
      0.29941795738295474,
      0.43397221818525056,
      0.3159828911984448
    ],
    [
      0.4473567989734153,
      0.4689269035524879,
      0.5481065369534301,
      0.4941325416663125,
      0.4995594668259806,
      0.4607860298828408,
      0.37880338540279657,
      0.4919487720050444,
      0.47897027927875846,
      0.45894194809384725,
      0.4989829677973334,
      0.3291059567403243,
      0.4821031561828284,
      0.4823044081835566,
      0.5520517780288701,
      0.4740679698975716,
      0.47622664262994685,
      0.5116770423506896,
      0.46772185387457,
      0.4908806857281076,
      0.47252940258711096,
      0.0,
      0.4462867976222,
      0.4720543011695686,
      0.4374306040250697,
      0.5444127856814887,
      0.425605773718033,
      0.47736593895463253,
      0.42156433621490397
    ],
    [
      0.36772896677403,
      0.4330240521750144,
      0.39613033886171634,
      0.40687485791658906,
      0.4512581840933152,
      0.3488430016612827,
      0.2958090450336186,
      0.34015313525452573,
      0.46760755412016164,
      0.39670906563430886,
      0.4099227364911324,
      0.26707341361566894,
      0.378787939725133,
      0.5005654081315738,
      0.36206223745411403,
      0.37579876809785584,
      0.41993894450457936,
      0.36437795435204334,
      0.3384702217082791,
      0.4584936530463959,
      0.4194293406173306,
      0.35598103886801336,
      0.0,
      0.37437805524357715,
      0.3458662549085163,
      0.3889733851506838,
      0.40033013344051627,
      0.3734252989147768,
      0.3274527603575126
    ],
    [
      0.39389804828136077,
      0.44656039628369837,
      0.45567441157237076,
      0.483256554641295,
      0.4304099188130768,
      0.4490204667565336,
      0.348658755654335,
      0.48894450927368505,
      0.47627320816356655,
      0.4134442560200138,
      0.49862455889260593,
      0.2844688207323265,
      0.43688741275127563,
      0.47556118018868077,
      0.3951689242035432,
      0.4453305560080567,
      0.4855720195419768,
      0.501251278074438,
      0.4021822493498559,
      0.4721402246036559,
      0.45985529989287377,
      0.4236207456750609,
      0.4516231719709616,
      0.0,
      0.42342673162591216,
      0.427399762459886,
      0.40221272718084333,
      0.514946554955616,
      0.40278298088804365
    ],
    [
      0.3621031076033572,
      0.35938538203496617,
      0.40937754906827695,
      0.3698747382382026,
      0.4000493896691595,
      0.39145786856175424,
      0.37954439829256326,
      0.3906252105797736,
      0.3846011997000218,
      0.46933536623311056,
      0.4436225748798579,
      0.2649641285575943,
      0.4122803415612357,
      0.37041673715039414,
      0.4091589131393947,
      0.39582505902736376,
      0.4163934990452265,
      0.3638221196636473,
      0.40222415891405916,
      0.39665584755356,
      0.4046140503702489,
      0.4316324947150538,
      0.40730671914503636,
      0.42535604806236926,
      0.0,
      0.3552472275328864,
      0.35148041118226936,
      0.40342045758135403,
      0.3025630502591985
    ],
    [
      0.38007696377900047,
      0.5075006856368174,
      0.5024083647324009,
      0.4681810556196371,
      0.5096393036637219,
      0.45008906095212353,
      0.31457357109042294,
      0.4734337453283397,
      0.4373096629584925,
      0.3505497274042466,
      0.4828502375199051,
      0.2981187133509924,
      0.44108924794645454,
      0.4395418149495729,
      0.4888512338946702,
      0.425420047939679,
      0.4750824348196734,
      0.47401496273788024,
      0.3987336188567927,
      0.41838239773758645,
      0.46726621839914384,
      0.4953822111461128,
      0.4404737516384445,
      0.4550753159676757,
      0.38673498605127365,
      0.0,
      0.40731243270888173,
      0.4648021026766449,
      0.40105820949451676
    ],
    [
      0.2813092518022955,
      0.3371095982441763,
      0.31063064443365307,
      0.2931506031613622,
      0.33936635429281536,
      0.30232236612334384,
      0.2790816354107377,
      0.32841483812965433,
      0.29464857310395076,
      0.3260016479211305,
      0.3059390552963366,
      0.2881284045821022,
      0.2954846618933429,
      0.3254656330350234,
      0.3130635270752167,
      0.2983939940516871,
      0.3383739805678465,
      0.30190830479967334,
      0.31654206117349504,
      0.286614356110114,
      0.313937645802733,
      0.3082887822631526,
      0.3821241141981131,
      0.3567773800249483,
      0.3305163722172173,
      0.3412813338834313,
      0.0,
      0.30828286989421727,
      0.3493273198666784
    ],
    [
      0.3371078868197612,
      0.38388212315417447,
      0.4383620440181335,
      0.4306451617539404,
      0.34839921037102584,
      0.37476085791986957,
      0.31346222904882803,
      0.4271445636984006,
      0.4553416779460109,
      0.32419423449299356,
      0.39209333125025814,
      0.25488980287047625,
      0.38693787186584894,
      0.41250172505500515,
      0.3873206995909275,
      0.4140134968085316,
      0.4092535817358596,
      0.49393124648135966,
      0.3828724561635295,
      0.4541117125059413,
      0.39034328479283986,
      0.4119456056874262,
      0.3589674773871918,
      0.48698861999506127,
      0.3523261907006858,
      0.3821594427391266,
      0.34384047112197047,
      0.0,
      0.3452279168286023
    ],
    [
      0.36301530210117816,
      0.42635418756370935,
      0.3930652784858557,
      0.4053155842060969,
      0.4227200288893216,
      0.3859306532038027,
      0.37320594467339707,
      0.41795148484275635,
      0.39657855904774153,
      0.37223674130308226,
      0.3855512597555455,
      0.4220488519571728,
      0.3880717925894577,
      0.4520034949242986,
      0.40161000533672353,
      0.402387052981273,
      0.3861252693892816,
      0.3780052900776525,
      0.3878955846455132,
      0.39539597552717676,
      0.392946129948891,
      0.4313952913605499,
      0.4356226970921371,
      0.4560344812600978,
      0.36861674766430585,
      0.4503334818695466,
      0.43454180615012916,
      0.4381283244440015,
      0.0
    ]
  ],
  "row_avgs": [
    0.20271887818242845,
    0.4234341072800786,
    0.4329135631037184,
    0.46721948011249087,
    0.39549444268223616,
    0.42993638314046867,
    0.3642801247664755,
    0.4541368578903076,
    0.42778502710585337,
    0.3399532149086966,
    0.4245045695439594,
    0.2374220783457895,
    0.4567408417629125,
    0.4424721617315973,
    0.4548199538917646,
    0.45201923156174145,
    0.3960753069449834,
    0.49972022128966836,
    0.4712239468630307,
    0.4382044342511397,
    0.399630091620539,
    0.4710680380007757,
    0.384480919505438,
    0.4388998473019839,
    0.3883335017257835,
    0.4376411456786108,
    0.3161601896199446,
    0.38903660438584925,
    0.40582454647466765
  ],
  "col_avgs": [
    0.37647893790062337,
    0.43666272594924394,
    0.44906438792571446,
    0.44761109204524824,
    0.4348012983949651,
    0.41559643734952284,
    0.33360748063219836,
    0.42983738045844405,
    0.43954900763762367,
    0.383194851374859,
    0.4321864572065852,
    0.28343754207372246,
    0.41909615973356135,
    0.4264574993201666,
    0.3991684868886921,
    0.4214188990463827,
    0.4295511344035276,
    0.43562757715873535,
    0.3973014680326358,
    0.44828964516206815,
    0.4089960992046384,
    0.4125433060979993,
    0.40579660291630576,
    0.4297446361758158,
    0.37903129737964963,
    0.41598244870034023,
    0.36721203880948783,
    0.4365415500440557,
    0.34736326165012005
  ],
  "combined_avgs": [
    0.2895989080415259,
    0.43004841661466126,
    0.44098897551471644,
    0.45741528607886955,
    0.41514787053860064,
    0.4227664102449957,
    0.3489438026993369,
    0.44198711917437583,
    0.4336670173717385,
    0.36157403314177783,
    0.42834551337527227,
    0.260429810209756,
    0.4379185007482369,
    0.4344648305258819,
    0.4269942203902284,
    0.43671906530406207,
    0.4128132206742555,
    0.4676738992242019,
    0.43426270744783324,
    0.44324703970660395,
    0.4043130954125887,
    0.4418056720493875,
    0.3951387612108719,
    0.4343222417388999,
    0.38368239955271655,
    0.4268117971894755,
    0.3416861142147162,
    0.4127890772149525,
    0.37659390406239385
  ],
  "gppm": [
    591.3537157714767,
    594.3535192335206,
    587.4945154256959,
    591.1385282168188,
    594.3067755288354,
    603.5187681812246,
    643.0013872973319,
    599.0832846403429,
    593.6922252499966,
    617.3975927278414,
    598.963769895918,
    660.1659224105175,
    604.1037954238735,
    602.218051635423,
    613.0357993467204,
    603.770978035249,
    595.4019103640592,
    593.2066090409281,
    612.5655734367439,
    589.9033344925116,
    607.1878477005687,
    606.1714007367879,
    607.1903279211889,
    597.5768928151749,
    620.8236385440576,
    603.8254017321877,
    627.0727055258961,
    595.1355587497247,
    634.7829398130837
  ],
  "gppm_normalized": [
    1.3837281807579431,
    1.3137703470559743,
    1.2939748394936263,
    1.3086179603553052,
    1.3042204809097149,
    1.3303513611742093,
    1.4270466413646352,
    1.31762314609564,
    1.3048682343848796,
    1.3535341947524449,
    1.3104587067113997,
    1.4689649229973851,
    1.3286049053012214,
    1.3247208720097479,
    1.3508960250432263,
    1.3367114918136578,
    1.3046278896683288,
    1.3109621429684784,
    1.3465519241174817,
    1.2934579235862063,
    1.333439650568709,
    1.333743418163974,
    1.332516362787145,
    1.3092603867738688,
    1.3598222229189048,
    1.3349218656542736,
    1.3757231452896632,
    1.3076731066354697,
    1.391955484828973
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364
  ],
  "response_lengths": [
    3314,
    2517,
    2688,
    2485,
    2677,
    2357,
    2315,
    2150,
    2684,
    2672,
    2356,
    3087,
    2443,
    2199,
    2460,
    2319,
    2376,
    2296,
    2155,
    2424,
    2288,
    2200,
    2423,
    2379,
    2264,
    2219,
    2275,
    2752,
    2114
  ]
}