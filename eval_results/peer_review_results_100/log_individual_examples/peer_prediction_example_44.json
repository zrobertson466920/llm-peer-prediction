{
  "example_idx": 44,
  "reference": "Under review as a conference paper at ICLR 2023\n\nTENSORVAE: A DIRECT GENERATIVE MODEL FOR MOLECULAR CONFORMATION GENERATION DRIVEN\n\nBY NOVEL FEATURE ENGINEERING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEfficient generation of 3D conformations of a molecule from its 2D graph is a key challenge in in-silico drug discovery. Deep learning (DL) based generative modeling has recently become a potent tool to tackling this challenge. However, many existing DL-based methods are either indirect–leveraging inter-atomic distances or direct–but requiring numerous sampling steps to generate conformations. In this work, we propose a simple model abbreviated TensorVAE capable of generating conformations directly from a 2D molecular graph in a single step. The main novelty of the proposed method is focused on feature engineering. We develop a novel encoding and feature extraction mechanism relying solely on standard convolution operation to generate token-like feature vector for each atom. These feature vectors are then transformed through standard transformer encoders under a conditional Variational Autoencoder framework for generating conformations directly. We show through experiments on two benchmark datasets that with intuitive feature engineering, a relatively simple and standard model can provide promising generative capability rivalling recent state-of-the-art models employing more sophisticated and specialized generative architecture.\n\n1\n\nINTRODUCTION\n\nRecent advance in deep learning has enabled significant progress in computational drug design (Chen et al., 2018). Particularly, capable graph-based generative models have been proposed to generate valid 2D graph representation of novel drug-like molecules (Honda et al., 2019; Mahmood et al., 2021; Yu & Yu, 2022), and there is an increasing interest on extending these methods to generating 3D molecular structures which are essential for structured-based drug discovery (Li et al., 2021; Simm et al., 2021; Gebauer et al., 2022). A stable 3D structure or conformation of a molecule is specified by the 3D Cartesian coordinates of all its atoms. Traditional molecular dynamics or statistical mechanic driven Monte Carlo methods are computationally expensive, making them unviable for generating 3d molecular structures at scale (Hawkins, 2017). In this regard, deep learning(DL)-based generative methods have become an attractive alternative.\n\nDL-based generative methods may be broadly classified into three categories: distance-based, reconstruction-based, and direct methods. The main goal of distance-based methods is learning a probability distribution over the inter-atomic distances. During inference, distance matrices are sampled from the learned distribution and converted to valid 3D conformations through postprocessing algorithms. Two representative methods of this category include GraphDG (Simm & Hern ́andez-Lobato, 2019) and CGCF (Xu et al., 2021a). An advantage of modeling distance is its roto-translation invariance property–an important inductive bias for molecular geometry modeling (K ̈ohler et al., 2020). Additional virtual edges and their distances between 2nd and 3rd neighbors are often introduced to constrain bond angles and dihedral angles crucial to generating a valid conformation. However, Luo et al. (2021) have argued that these additional bonds are still inadequate to capture structural relationship between distant atoms. To alleviate this issue, DGSM (Luo et al., 2021) proposed to add higher-order virtual bonds between atoms in an expanded neighborhood region. Another weakness of the distance-based methods is the error accumulation problem; random noise in the predicted distance can be exaggerated by an Euclidean Distance Geometry algorithm, leading to generation of inaccurate conformations (Xu et al., 2022; 2021b).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo address the above weaknesses, reconstruction-based methods directly model a distribution over 3D coordinates. Their main idea is to reconstruct valid conformations from distorted coordinates. GeoDiff (Xu et al., 2022) and Uni-Mol (Zhou et al., 2022) are pioneering studies in this respect. Though sharing similar idea, they differ in the process of transforming corrupted coordinates to stable conformations. While GeoDiff adapts a reverse diffusion process (Sohl-Dickstein et al., 2015), Uni-Mol treats conformation reconstruction as an optimization problem. Despite their promising performance, both methods require designing of task-specific and complex coordinate transformation methods. This is to ensure the transformation is roto-translation or SE(3)-equivariant. To achieve this, GeoDiff proposed a specialized SE(3)-equivariant Markov transition kernel. On the other hand, Uni-Mol accomplished the same by combining a task-specific adaption of transformer (Vaswani et al., 2017) inspired by the AlphaFold’s Evoformer (Jumper et al., 2021) with another specialized equivariant prediction head (Satorras et al., 2021). Furthermore, GeoDiff requires numerous diffusing steps to attain satisfactory generative performance which can be time consuming.\n\nCVGAE (Mansimov et al., 2019) and DMCG (Zhu et al., 2022) have attempted to resolve the generative efficiency issue by developing models that can produce a valid conformation directly from a 2D molecular graph in a single sampling step. Regrettably, the performance of CVGAE is significantly worse than its distance-based counterparts mainly due to the use of inferior graph neural network for information aggregation (Zhu et al., 2022). DMCG aimed to improve the performance of its predecessor by using a more sophisticated graph neural network and a loss function invariant to symmetric permutation of molecular substructures. Although DMCG achieved superior performance, acquiring such loss function requires enumerating all permutations of a molecular graph, which can become computationally expensive for long-sequence molecules.\n\nRegardless of their category, a common recipe of success for these models can be distilled to developing model architecture with ever increasing sophistication and complexity. There is little attention on input feature engineering. In this work, we forgo building specialized model architecture but instead focus on intuitive input feature engineering. We propose to encode a molecular graph using a fully-connected and symmetric tensor. For preliminary information aggregation, we run a rectangle kernel filter through the tensor in a 1D convolution manner. This operation has a profound implication; with a filter size of 3, the information from two immediate neighbors as well as all their connected atoms can be aggregated onto the focal atom in a single operation. It also generates tokenlike feature vector per atom which can be directly consumed by a standard transformer encoder for further information aggregation.\n\nThe generative framework follows the standard conditional variational autoencoder (CVAE) setup. We start with building two input tensors with one encoding only the 2D molecular graph and the other also encoding 3D coordinate and distance. Both tensors go through the same feature engineering step and the generated feature vectors are fed through two separate transformer encoders. The output of these two encoders are then combined in an intuitive way to form the input for another transformer encoder for generating conformation directly. The complete generative model is abbreviated as TensorVAE.\n\nIn summary, the proposed method has three main advantages. (1) Direct and Efficient, generating conformation direclty from a 2D molecular graph in a single step. (2) Simple, not requiring tasksepecific design of neural network architecture, relying only on simple convolution and off-the-shelf transformer architecture; (3) Easy to implement, no custom module required as both PyTorch and TensorFlow offer ready-to-use convolution and transformer implementation. These advantages translate directly to excellent practicality of the TensorVAE method. We demonstrate through extensive experiments on two benchmark datasets that the proposed TensorVAE, despite its simplicity, can perform competitively against 18 recent state-of-the-art methods for conformation generation and molecular property prediction.\n\n2 METHOD\n\n2.1 PRELIMINARIES\n\nProblem Definition. We formulate molecular conformation generation as a conditional generation task. Given a set of molecular graphs G and their corresponding i.i.d conformations R, the goal\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nis to train a generative model that approximates the Boltzman distribution, and from which a valid conformation conditioned on a molecular graph can be easily sampled in a single step.\n\nStory Line. In the ensuing sections, we breakdown the formulation of the proposed method in three novel ideas. We first introduce how the tensor encoding method is derived. Based on the tensor input, we propose a “naive” model that applies a convolutional neural network directly on the tensor to generate distance matrix, from which conformation are obtained through an Euclidean Distance Geometry algorithm. We also show that such model can already provide comparable performance to several advanced distance-based methods. Subsequently, we demonstrate how token-like feature vector can be generated from the input tensor by using 1D convolution operation. Finally, we elaborate on how to combine all the components together under a CVAE framework to arrive at the final generative model.\n\n2.2\n\nINPUT TENSOR GRAPH\n\nGraph neural network (GNN) is a popular feature extraction backbone for DL-based molecular conformation generation. The input of GNN in this case is composed of three components, including atom features, edge features and an adjacency matrix. Atom and edge features normally pass through separate embedding steps before being fed to the GNN. Adjacency matrix is then used to determine neighboring atoms for layer-wise information aggregation. Although bond features are aggregated onto atom features and vice versa, these two features are maintained separately throughout the message passing layers (Gilmer et al., 2017; Satorras et al., 2021). Instead of having separate inputs, our first simple idea is to combine them into a single input. Specifically, we add an additional dimension to the adjacency matrix, making it a tensor, similar to that used in a computer vision task. The diagonal section of the tensor holds the atom features.\n\nWe consider three types of atom features comprising atom type, charge and chirality. Each feature is onehot encoded and they are stacked together to form a single atom feature vector. There are two variants of the atom feature vector corresponding to two input tensors for the two encoders of the CVAE: an encoder conditioned only on graph (for which the tensor is referred to as the G tensor) and the other conditioned on both graph and coordinates (for which the tensor is referred to as the GDR tensor). For the GDR tenosr, every atom feature vector has three additional channels incorporating the 3D coordinate of the respective atom, and a distance channel filled with zeros.\n\nFigure 1: Benzene ring tensor graph example. Note that the values in the feature vector and its dimension are for demonstration purpose only. We explain how they are determined in Sec.3.\n\nThe off-diagonal section holds the bond features. The considered bond features are bond type, bond stereochemistry type, associated ring size and normalized bond length. A virtual bond is also included in the bond type. The normalized bond length is calculated as edge length (1 for direct neighbor, 2 for 2nd neighbor, etc.) divided by the longest chain length. It is worth noting that all high-order virtual bonds share the same virtual bond type; they only differ in their normalized bond length. To construct bond feature vector, we first sum the atom feature vectors of the related atoms. This new vector is then stacked with one-hot encoded bond type vector, normalized bond length, and one-hot encoded ring size vector to become the bond feature vector.\n\nThere are also two variants of the bond feature vector. For the G tensor, coordinate and distance channles are excluded from all bond feature vectors. For the GDR tensor, to match the size of the\n\n3\n\nvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdC0101001000.10.20.3Atom type encodingAtom charge encoding010100Atom chirality encoding1000.70.80.9Atom coord+020200200000020200200000101/50.1015/5Atom feature vectors summedCoordinate channels zeroedStacked with bond feature vectorBond typeNormalized Bond length0.511Euclidean distance 0000001001Bond stereochem In ring size00CCarbon atomdDouble bondsSingle bondvVirtual bondUnder review as a conference paper at ICLR 2023\n\natom feature vector, every bond feature vector has three more coordinate channels filled with 0s, and an additional distance channel holding the Euclidean distance between two connected atoms. This bond feature vector is obtained for all atom pairs, making the proposed tensor fully-connected and symmetric. Despite being referred differently, the structure of the bond feature vector and atom feature vector are the same. For both types in the GDR tensor, there are 5 blocks (3 blocks for G tensor) of channels stacked in the exact same order as following. Therefore, convolution over the input tensor is uniform for both on-diagonal and off-digonal channels.\n\n• atom feature channels (atom type, charge, and chirality);\n\n• atom coordinate channels (coordinate channels are excluded in the G tensor);\n\n• bond type feature channels (bond type and normalized bond length);\n\n• Euclidean distance channel (pairwise distance channel is excluded in G tensor);\n\n• other bond type feature channels (bond stereo-chem type and bond ring size)\n\nAn example input tensor graph of the benzene ring is illustrated in Fig.1. Having obtained the tensor representation, a naive way of building a generative model is to apply a convolutional neural network directly on the tensor, and train it to predict a distribution over the inter-atomic distances. We utilize a standard UNet (Ronneberger et al., 2015) structure to map the input tensor to a probability distribution over a distance matrix containing all pair-wise Euclidean distances. Distance matrices are then sampled and converted to valid conformations following the same method presented in GraphDG (Simm & Hern ́andez-Lobato, 2019). We refer to this model as the NaiveUNet. More details of the NaiveUNet can be found in Sec.A.3, and a further explaination of its poor performance can be found in Sec.A.8.\n\nDespite its naive nature, this model achieves a mean coverage (COV) score of 52.14 ± 1.48% and a mean matching (MAT) score of 1.4322±0.0247 ̊A on the GEOM-Drugs dataset, already comparable to several more complex distance-based baselines, as shown in Sec.3.2. However, there are two major issues to this approach. First, with a small kernel size (3 × 3 used in the UNet), it takes many convolution layers to achieve information aggregation between atoms that are far apart; it does not take full advantage of high-order bonds already made available in the input tensor. Secondly, the output size grows quadratically with the number of atoms, as compared to only linear growth in reconstruction-based or direct generation methods. The solution to the first issue is rather simple, obtained by increasing the kernel size to expand its “field of view”. On the other hand, solving the second issue requires elevating the naive two-step generative model to a direct one.\n\n2.3 EXTENDED KERNEL AND ATTENTION MECHANISM\n\nWe observe that every row or column of the proposed tensor contains global information of a focal atom and all of its connected atoms (by both chemical and virtual bond). This motivates our second main idea which is to extend the length of the kernel to the length of the tensor graph while keeping the width unaltered. This idea has a profound implication; information from the immediate neighbors, all their connected atoms, and all the bond features can be aggregated onto the focal atom in a single convolution operation. In contrast, achieving the same aggregation may require many layers of propagation for the naive model and other GNN-based models. A direct consequence of this modification is that only 1D convolution is permitted. With multiple kernels being applied simultaneously, each stride of these kernels generates a feature vector for a single atom. An illustration of the 1D convolution operation is shown in Fig.2.\n\nFigure 2: Extending kernel and 1D convolution. We further observe that the generated feature vectors resemble the token-like feature vectors used in language modeling. This observation combined with the proven success of attention mechanism\n\n4\n\nvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdCvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdCvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdC3 x 3 Conv KernelN x 3 Conv KernelExtendToken 1Token 2Token N1D ConvUnder review as a conference paper at ICLR 2023\n\nin other related work leads to the selection of transformer architecture as the backbone of our generative model. A significant advantage of using transformer’s self-attention mechanism is, similar to the extended kernel, it enables a global information aggregation from and for all atoms. It also eliminates the need to maintain separated atom and bond features at each step of feature transformation. We present further insight and a more detailed analysis of the adavantage of this input feature engineering in Sec.A.1\n\n2.4 PUTTING EVERYTHING TOGETHER\n\nConditional variational autoencoder framework. We aim at obtaining a generative model pθ(R|G) that approximates the Boltzmann distribution through Maximum Likelihood Estimation. Particularly, given a set of molecular graphs G and their respective ground-truth conformations R, we wish to maximize the following objective.\n\nlog pθ (R|G) = log\n\n(cid:90)\n\np (z) pθ (R|z, G) dz\n\n(1)\n\nA molecular graph can have many random conformations. We assume this randomness is driven by a latent random variable z ∼ p (z), where p (z) is a known distribution e.g. a standard normal distribution. As pθ (R|z, G) is often modeled by a complex function e.g. a deep neural network, evaluation of the integral in Eq.1 is intractable. Instead, we resort to the same techniques proposed in the original VAE (Kingma & Welling, 2013) to establish a tractable lower bound for Eq.1.\n\nlog pθ (R|G) ≥ Eqw(z|R,G) [log pθ (R|z, G)] − DKL [qw (z|R, G) ||p (z)]\n\n(2)\n\nwhere DKL is the Kullback-Leibler divergence and qw (z|R, G) is a variational approximation of the true posterior p (z|R, G). We assume p (z) = N (0, I) and qw (z|R, G) is a diagonal Gaussian distribution whose means and standard deviations are modeled by a transformer encoder. The input of this transformer encoder is the proposed tensor containing both the coordinate and distance information. We denote this tensor the GDR tensor. On the other hand, pθ (R|z, G) is further decomposed into two parts: a decoder pθ2 (R|z, σθ1 (G)) for predicting conformation directly and another encoder σθ1 (G) for encoding the 2D molecular graph. The input tensor for σθ1 (G) is absent of coordinate and distance information, and is therefore denoted the G tensor. Both encoders share the same standard transformer encoder structure. However, there is a minor modification to the transformer structure for the decoder. Specifically, the Query, Key matrices for the first multi-head attention layer are computed based on the output vectors of σθ1 (G), and the Value matrices come directly from the reparameterization of the output of qw (z|R, G), as z = μw + Σwε, where μw and Σw are the predicted mean and standard deviation respectively. ε is sampled from N (0, I). We present the complete picture of how the two encoders and the decoder are arranged in a CVAE framework in Fig.3a. We also show the illustration of the modified multi-head attention in Fig.3b.\n\nIntuition behind the modified attention. There are multiple ways to join together the output of the two encoders to form the input to the final decoder. Popular methods include stacking or addition. We tried both these methods with unsatisfactory performance. We notice that, due to direct stacking or addition of the sampled output of qw onto the output of σθ1, attention weights computed in the first layer of the decoder are easily overwhelmed by random noise of the sampled values, and become almost indiscernible1. This leads to ineffective information aggregation which is then further cascaded through the remaining attention layers. Intuitively, in the first attention layer, the attention weights dictating how much influence an atom exerts on the other should predominantly be determined by the graph structure, and remain stable for the same molecule. Further, attention weights are computed by Query and Key matrices. Therefore, these two matrices should stay stable for the same graph. This motivates our third and final main idea; that is, we compute Query and Key matrices only from the output (cid:8)hL (cid:9) of σθ1, and attribute the variation in conformation to the Value matrices which are directly sampled from {z1, ..., zN } ∼ qw. The resultant information aggregation is much more meaningful and each output vector corresponding to an individual atom carries distinct features, facilitating information aggregation of the ensuing attention layers.\n\n1 , ..., hL\n\nN\n\n1Imagine a mixture model with randomly varying mixture weights.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(b) Modified multi-head attention\n\n(a) Variational AutoEncoder framework\n\nFigure 3: TensorVAE model\n\nRoto-translation invariant loss. Following ConfVAE (Xu et al., 2021b), we formulate the reconstruction loss as.\n\nL (R) = − log pθ (R|z, G) = −\n\nN (cid:88)\n\n3 (cid:88)\n\n(cid:18)\n\ni=1\n\nj=1\n\nRij − A\n\n(cid:16) ˆR, R\n\n(cid:17)\n\n(cid:19)2\n\nij\n\n(3)\n\nwhere A (·) is a function aligning the predicted conformation ˆR onto the reference conformation R. We choose Kabsch algorithm (contributors, 2022) as the alignment method which translates and rotates the predicted conformation onto its corresponding ground-truth before loss computation. This makes the reconstruction loss roto-translation invariant. Finally, the KL-loss component DKL [qw (z|R, G) ||p (z)] does not involve any coordinate. Therefore, the objective function defined in Eq.2 is roto-translation invariant.\n\nDirect conformation generation at inference time. To generate a single conformation, we first construct the G tensor of a molecular graph and obtain a single latent sample {z1, ...zN } from a standard diagonal Gaussian distribution. The G tensor is passed through σθ1 encoder to produce (cid:9) which is then combined with the latent sample via the modified multi-head attention (cid:8)hL mechanism. The output of this modified attention layer further goes through L−1 standard attention layers to be transformed to the final conformation. The entire generation process depends only on a 2D molecular graph, and requires a single sampling step and a single pass of the TensorVAE model.\n\n1 , ..., hL\n\nN\n\n3 EXPERIMENT\n\nIn this section, we first elaborate on the implementation details of the TensorVAE model including determining the size of the input tensors, network architecture and how the entire framework is trained end-to-end. We then present conformation generation experiment results of the proposed TensorVAE on two benchmark data-sets, GEOM-QM9 and GEOM-Drugs. These results are compared to those of 11 state-of-the-art baselines. In addition to conformation generation, we present\n\n6\n\nMulti-head AttentionAdd & NormFeed ForwardAdd & NormLxG tensorGDR tensorL x transformer encoder blocksz1z2zNModified Multi-head AttentionAdd & NormFeed ForwardAdd & NormL-1 x transformer encoder blockR1R2RNKL Regularization lossReconstruction lossKabsch alignmentSelf-attentionSelf-attentionScaled Dot-product AttentionLinearLinearLinearq1k1q2k2qNkNKQVz1z2zNSplitConcatLinearUnder review as a conference paper at ICLR 2023\n\nfurther experiment on molecular property prediction in Sec.A.7, where the performance of the proposed method is compared against 7 more state-of-the-art baselines on the MolecularNet benchmark (Wu et al., 2018).\n\n3.1 EXPERIMENT SETUP\n\nDataset. Following existing work (Luo et al., 2021; Shi et al., 2021; Xu et al., 2021b;a; 2022; Zhou et al., 2022), we utilize the GEOM data-set for evaluating the performance of the proposed TensorVAE. GEOM contains 37 million energy and statistical weight annotated molecular conformations corresponding to 450,000 molecules (Axelrod & G ́omez-Bombarelli, 2022). This dataset is further divided into two constituent datasets, Drugs and QM9. The Drugs dataset covers 317,000 mediansized molecules averaging 44.4 number of atoms. The QM9 dataset contains 133,000 smaller molecules averaging only 18 atoms. We randomly select 40000 molecules from each dataset to form the training set. For each molecule, we choose the top 5 most likely2 conformations. This results in 200,000 training conformations for each train set. For validation set, we randomly sample 2,500 conformations for both Drugs and QM9 experiments. Finally, for testing, following (Shi et al., 2021; Xu et al., 2022), we randomly select 200 molecules each with more than 50 and less than 500 annotated conformations from QM9, and another 200 with more than 50 and less than 100 annotated conformations from Drugs3.\n\nDetermining input tensor graph size. We conduct a basic data analysis on the entire Drugs dataset to determine the 98.5th percentile of the number of atoms to be 69, and the percentage of molecules having more than 69 atoms and with more than 50 but less than 100 conformations is only 0.19%. Accordingly, we set the size of the input tensor to 69 × 69 for Drugs experiment. On the other hand, we use the maximum number of atoms 30 for QM9 experiment. The channel features for the input tensor include atom types, atom charge, atom chirality, bond type, bond stereo-chemistry and bond in-ring size. For the GDR tensor, we also include 3D coordinate channels and the computed distance channel. The resulting channel depth is 50 for GDR tensor and 46 for G tensor. The detailed information of these features and their encoding method is listed in Sec.A.4.\n\nImplementation details. We implement the proposed TensorVAE using Tensorflow 2.3.1. All three transformer encoders of TensorVAE follow the standard Tensorflow implementation in https: //www.tensorflow.org/text/tutorials/transformer. All of them have 4 layers, 8 heads and a latent dimension of 256. Both QM9 and Drugs experiments share the same network architecture and hyper-parameter configuration. We present the detailed training hyperparameter configuration in Sec.A.2.\n\nEvaluation metrics. We adopt the widely accepted coverage score (COV) and matching score (MAT) (Shi et al., 2021) to evaluate the performance of the proposed TensorVAE model. These two scores are computed as.\n\nCOV (Cg, Cr) =\n\n1 |Cr|\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:110)\n\nR ∈ Cr|RMSD\n\n(cid:16)\n\nR, ˆR\n\n(cid:17)\n\n≤ δ, ∀ ˆR ∈ Cg\n\n(cid:111)(cid:12) (cid:12) (cid:12)\n\nMAT (Cg, Cr) =\n\n1 |Cr|\n\n(cid:88)\n\nR∈Cr\n\nmin RMSD\n\n(cid:16)\n\nR, ˆR\n\n(cid:17)\n\n(4)\n\n(5)\n\nwhere Cg is the set of generated conformations and Cr is the corresponding reference set. The size of Cg is twice of that of Cr, as for every molecule, we follow (Xu et al., 2022) to generate twice the number of conformations as that of reference conformations. δ is a predefined threshold and is set to 0.5 ̊A for QM9 and 1.25 ̊A for Drugs respectively (Shi et al., 2021) . RMSD stands for the root-mean-square deviation between R and ˆR, and is computed using the GetBestRMS method in the RDKit (Riniker & Landrum, 2015) package. While COV score measures the ability of a model in generating diverse conformations to cover all reference conformations, MAT score measures how well the generated conformations match the ground-truth. A good generative model should have a high COV score and a low MAT score.\n\n2Ranked by their Boltzmann weight. 3This limit on the number of conformations for testing molecules is taken directly from https://\n\ngithub.com/DeepGraphLearning/ConfGF which is also followed by all other compared methods.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nBaselines. We compare the performance of the proposed TensorVAE model to those of 1 classical RDKit method; 5 distance-based methods including GraphDG, CGCF, ConfVAE, ConfGF and DGSM; 2 reconstruction-based methods including GeoDiff and Uni-Mol; 4 direct methods including CVGAE, GeoMol, DMCG and its symmetric permutation variant. The detailed information of the molecular property prediction baselines are presented in Sec.A.7\n\n3.2 RESULTS AND DISCUSSION\n\nThe COV and MAT scores for all compared methods on both QM9 and Drugs datasets are presented in Tab.1. All experiments follow the same test data generation configuration in (Shi et al., 2021). Additionally, we have conducted three 3 ablation studies on the input feature engineering method in Sec.A.8 to demonstrate why 1D convolution with a N × 3 kernel is necessary to achieve a good generative performance.\n\nTable 1: Performance comparison between TensorVAE and 11 other SOTAs on GEOM dataset.\n\nModels\n\nRDkit CVGAE GraphDG CGCF ConfVAE ConfGF GeoMol DGSM GeoDiff DMCG Uni-Mol\n\nTensorVAE1\n\nTensorVAE2\n\nQM9\n\nDrugs\n\nCOV (%) ↑\n\nMAT ( ̊A) ↓\n\nCOV (%) ↑\n\nMAT ( ̊A) ↓\n\nMean Median Mean\n\nMedian Mean Median Mean\n\nMedian\n\n83.26 0.09 73.33 78.05 80.42 88.49 71.26 91.49 92.65 94.98 97.95\n\n98.11 ±0.25\n\n97.11 ±0.31\n\n90.78 0.00 84.21 82.48 85.31 94.13 72.00 95.92 95.75 98.47 100\n\n100 ±0\n\n100 ±0\n\n0.3447 1.6713 0.4245 0.4219 0.4066 0.2673 0.3731 0.2139 0.2016 0.2365 0.1831\n\n0.2935 1.6088 0.3973 0.3900 0.3891 0.2685 0.3731 0.2137 0.2006 0.2312 0.1659\n\n0.1970 ±0.0016\n\n0.2041 ±0.0046\n\n0.1926 ±0.0027\n\n0.1920 ±0.007\n\n60.91 0.00 8.27 53.96 53.14 62.15 67.16 78.73 88.45 91.27 91.91\n\n94.91 ±0.35\n\n93.34 ±1.17\n\n65.70 0.00 0.00 57.06 53.98 70.93 71.71 94.39 97.09 100 100\n\n100 ±0\n\n99.90 ±0.31\n\n1.2026 3.0702 1.9722 1.2487 1.2392 1.1629 1.0875 1.0154 0.8651 0.8287 0.7863\n\n1.1252 2.9937 1.9845 1.2247 1.2447 1.1596 1.0586 0.9980 0.8598 0.7908 0.7794\n\n0.7789 ±0.0027\n\n0.8074 ±0.0135\n\n0.7585 ±0.0076\n\n0.7927 ±0.0186\n\n*Bold font indicates best result. Results for RdKit, CVGAE, GraphDG, CGCF, ConfGF are taken from (Shi et al., 2021); results for ConfVAE and GeoDiff are taken from (Xu et al., 2022); all other results are taken from (Zhou et al., 2022); TensorVAE1 results and standard deviations are obtained by running 10 experiements each with a different random seed on a single 200 testing molecules set. TensorVAE2 results and standard deviations are obtained by running 10 experiements each with a different random seed as well as a different set of 200 testing molecules.\n\nIn general, distance-based methods except for DGSM and ConfGF have relatively poor performance as compared to that of the classic RDKit. It has been argued that the the performance of RDkit is facilitated by an additional empirical force field (FF) (Halgren, 1996) optimization. Subsequently, CGCF and ConfVAE showed that with FF optimization, they can outperform RDkit. We also show the performance of the proposed TensorVAE with FF optimization compared to 5 other methods also employing FF optimization in Tab.3 in Sec.A.6, where TensorVAE outperforms all of them with a significant margin. However, this additional step further introduces complexity to the already complex two-stage generative model. ConfGF attempted to rectify this weakness by simulating a pseudo gradient-based force field. Such force field can be utilized in an annealed Langevin dynamics sampling to sequentially guide atom positions to a valid conformation. DGSM further improves the performance of ConfGF by using a dynamically constructed graph structure that is able to model long range atom interaction. Despite being posed as a direct method, both DGSM and ConfGF still need to compute atom distance as an intermediate step. Noticeably, DGSM also requires dynamically changing the graph structure for every sampling step. As shown in Tab.1, DGSM and ConGF outperform RDKit by a significant margin in both experiments. Nevertheless, their main weakness lies in the fact they they require numerous sampling steps to attain desirable performance. It has been reported in the ensuing work (Xu et al., 2022), that it took ConfGF approximately 8500 sec-\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nonds to fully decode 200 QM9 molecules and a staggering 11500 seconds for decoding 200 Drugs molecules. In constrast, TensorVAE takes only 62 seconds using a single Xeon 8163 CPU core to decode 200 QM9 molecules, and 128 seconds for 200 Drug molecules.\n\nA main goal of GeoDiff is to be free of the dependence on distance. To achieve this, transformations are directly applied to 3D conformation, through which, a random or distorted conformation can be sequentially denoised to a valid conformation. An essential requirement for such transformation is that it needs to be SE(3) equivariant. This necessitates designing of sophisticated equivariant transition kernel. Despite its claim of not involving computing distance as an intermediate step, the equivariant graph field network (Satorras et al., 2021) used still relies on a distance-like quantity (cid:13) (cid:13)xL for every step of transformation. GeoDiff produces promising performance on both datasets. Unfortunately, it needs numerous diffusion steps (T = 5000) for generating conformations, requiring approximately the same amount of decoding time as ConfGF (Xu et al., 2022).\n\ni − xL\n\n(cid:13) 2\n(cid:13)\n\nj\n\nUni-Mol circumvents this issue by reformulating molecular generation problem as an optimization problem. The structure of Uni-Mol is inspired by the Evoformer proposed in the Alphafold (Jumper et al., 2021) which also considers “atom-pair” interaction. This requires maintaining a pair interaction matrix (N × N ) through every attention layer. In addition, Uni-Mol has a significantly larger transformer structure as compared to ours, featuring 15 attention layers, 64 attention heads and a latent dimension size of 512. The Uni-Mol model is also first pretrained on a much larger dataset (19e6 molecules, each with 10 conformations), and then fine-tuned on the GEOM dataset for conformation generation. Despite using a much smaller and also standard transformer model without pretraining, the proposed TensorVAE outperforms Uni-Mol on the Drugs dataset. Further, we show that the proposed model also performs competitively against Uni-Mol on a molecular property prediction task in Sec.A.7 without any pretraining, validating the effectiveness of the input feature engineering.\n\nCVGAE is the first method proposed to generate molecular conformation directly from a 2D molecular graph. However, it yields the worst performance among the compared methods. DMCG attempted to revitalize the same framework by adapting a more advanced graph neural network structure combining GATv2 (Brody et al., 2021) with GN block (Battaglia et al., 2018). Noticeably, similar to Uni-Mol, it also maintains a N × N bond feature matrix throughout all layers. In constrast, we only use a standard transformer encoder architecture. Another major modification proposed in DMCG is introducing the permutation invariance of symmetric molecular substructures to the RMSD loss. Achieving this invariance requires enumerating all possible permutations which can become expensive for large molecules. The proposed TensorVAE outperforms DMCG with a significant margin. Finally, some samples of the TensorVAE generated conformations are shown in Sec.A.9\n\n4 CONCLUSION\n\nWe develop TensorVAE, a simple yet powerful model that is able to generate 3D conformation directly from a 2D molecular graph. Unlike many existing work focusing on designing complex neural network structure, we focus on developing novel input feature engineering techniques. We decompose these techniques into three main ideas, and explain how one idea naturally evolves to the next. We first propose a tensor representation of a molecular graph. Then, we demonstrate that sliding a rectangle kernel through this tensor in an 1D convolution manner can achieve complete information aggregation. Finally, we present the complete CVAE-based framework featuring 2 transformerbased encoders and another transformer-based decoder, and propose a novel modification to the first multi-head attention layer of the decoder to enable sensible integration of the output of the other two encoders. We show through extensive experiments that with intuitive feature engineering, simple and standard model architecture can provide competitive performance compared to 18 recent stateof-the-art models. For future work, we plan to extend our method and methodology to tackling the challenging protein structure prediction.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n5 REPRODUCIBILITY STATEMENT\n\nWe did not introduce nor have used any task-specific neural network archiecture. The results presented in this study can be straightforwardly reproduced using publically available datasets and ready-to-use implementation of convolution operation and Transformer from either PyTorch or TensorFlow. Specifically, we use the same transformer implementation found in https: //www.tensorflow.org/text/tutorials/transformer. To compute RMSD, we first zero-center both the predicted and the ground-truth conformations. Then, we obtain the optimal rotation matrix using the Kabsch algorithm implementation found in https://en. wikipedia.org/wiki/Kabsch_algorithm. It is straightforward to implement this algorithm in a python function. Please note that if TensorFlow is used, this python function needs to be wrapped inside a tf.py function, and then followed by tf.stop gradient to prevent gradient update. Finally, we rotate the predicted conformation onto its ground-truth for calculating the RMSD. The above details together with the already presented hyper-parameter setting should suffice to reproduce our results.\n\nREFERENCES\n\nSimon Axelrod and Rafael G ́omez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, 2022. doi: 10.1038/ s41597-022-01288-4. URL https://doi.org/10.1038/s41597-022-01288-4.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks?\n\narXiv\n\npreprint arXiv:2105.14491, 2021.\n\nHongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona, and Thomas Blaschke. The rise\n\nof deep learning in drug discovery. Drug discovery today, 23(6):1241–1250, 2018.\n\nWikipedia contributors. Kabsch algorithm — Wikipedia,\n\nthe free encyclopedia.\n\nhttps:\n\n//en.wikipedia.org/w/index.php?title=Kabsch_algorithm&oldid= 1082253417, 2022. [Online; accessed 19-October-2022].\n\nXiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127–134, 2022.\n\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. Cyclical annealing schedule: A simple approach to mitigating kl vanishing. arXiv preprint arXiv:1903.10145, 2019.\n\nNiklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert M ̈uller, and Kristof T Sch ̈utt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):1–11, 2022.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nThomas A Halgren. Merck molecular force field. v. extension of mmff94 using experimental data, additional computational data, and empirical rules. Journal of Computational Chemistry, 17(5-6): 616–641, 1996.\n\nPaul CD Hawkins. Conformation generation: the state of the art. Journal of chemical information\n\nand modeling, 57(8):1747–1756, 2017.\n\nShion Honda, Hirotaka Akita, Katsuhiko Ishiguro, Toshiki Nakanishi, and Kenta Oono. Graph\n\nresidual flow for molecular graph generation. arXiv preprint arXiv:1909.13521, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015. URL http://arxiv.org/abs/1412.6980.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nJonas K ̈ohler, Leon Klein, and Frank No ́e. Equivariant flows: exact likelihood generative learning for symmetric densities. In International conference on machine learning, pp. 5361–5370. PMLR, 2020.\n\nYibo Li, Jianfeng Pei, and Luhua Lai. Structure-based de novo drug design using 3d deep generative\n\nmodels. Chemical science, 12(41):13664–13675, 2021.\n\nShengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. Advances in neural information processing systems, 32, 2019.\n\nShitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. Advances in Neural Information Processing Systems, 34:19784– 19795, 2021.\n\nOmar Mahmood, Elman Mansimov, Richard Bonneau, and Kyunghyun Cho. Masked graph model-\n\ning for molecule generation. Nature communications, 12(1):1–12, 2021.\n\nElman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry pre-\n\ndiction using a deep generative graph neural network. Scientific reports, 9(1):1–13, 2019.\n\nSereina Riniker and Gregory A Landrum. Better informed distance geometry: using what we know to improve conformation generation. Journal of chemical information and modeling, 55(12): 2562–2574, 2015.\n\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33:12559–12571, 2020.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nVıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural net-\n\nworks. In International conference on machine learning, pp. 9323–9332. PMLR, 2021.\n\nChence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International Conference on Machine Learning, pp. 9558–9568. PMLR, 2021.\n\nGregor N. C. Simm, Robert Pinsler, G ́abor Cs ́anyi, and Jos ́e Miguel Hern ́andez-Lobato. Symmetryaware actor-critic for 3d molecular design. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=jEYKjPE1xYN.\n\nGregor NC Simm and Jos ́e Miguel Hern ́andez-Lobato. A generative model for molecular distance\n\ngeometry. arXiv preprint arXiv:1909.11459, 2019.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.\n\nZhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry, 63(16):8749–8760, 2019.\n\nMinkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative\n\ndynamics for molecular conformation generation. arXiv preprint arXiv:2102.10240, 2021a.\n\nMinkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli, and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel programming. In International Conference on Machine Learning, pp. 11537–11547. PMLR, 2021b.\n\nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.\n\nKevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel GuzmanPerez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59(8):3370– 3388, 2019.\n\nHongyang K Yu and Hongjiang C Yu. Powerful molecule generation with simple convnet. Bioin-\n\nformatics, 38(13):3438–3443, 2022.\n\nGengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. 2022.\n\nJinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong Wang, Yusong Wang, Wengang Zhou, Tao Qin, Houqiang Li, et al. Direct molecular conformation generation. arXiv preprint arXiv:2202.01356, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 GLOBAL INFORMATION AGGREGATION BEYOND THE N th HOP\n\nA geometric interpretation of GNN’s message passing layer is it aggregates information between atoms (and their bond) that are 1-hop away. With L layers, information from atoms that are L-hop apart can be aggregated. Here, we define a global information aggregation as the N th-hop aggregation with N being the total number of atoms, where each atom is able to aggregate information from its farthest neighbour.\n\nIt is worth noting that for a fully-connected GNN, a 1-hop message passing can already achieve this global information aggregation. Transformer’s self-attention can be considered as a type of fullyconnected GNN. However, a vanilla transformer can only aggregate features from each token/atom; if edge features are not included, they needed to be incorporated somehow through additional inputs (e.g. the pair interaction matrix of Uni-Mol). The primary reason motivating the creation of the fully-connected tensor representation is we want each generated token contain both atom and bond features, such that we can eliminate the pair interaction or bond matrix. To achieve this, we fill each column of the fully-connected tensor with;\n\n• focal atom features;\n\n• chemical and virtual bond features indicating how the focal atom is connected to all other\n\natoms;\n\n• atom features of all connected atoms, since for each cell (except for cell of the focal atom)\n\nwe sum atom features of both the connected atom and the focal atom.\n\nIn fact, running a N × 1 kernel filter on the proposed tensor is conceptually similar to achieving a global information aggregation with a fully-connected GNN. By increasing kernel width to 3, the aggregation window also includes global information from two immediate neighbours. This type of information aggregation extends far beyond just N th-hop.\n\nMore interestingly, when multiple kernels are applied simultaneously to the same N × 3 × C region, each kernel is free to choose whichever group of atom/bond features to attend to depending on its kernel weights. This resembles the multi-head attention mechanism of a transformer, where each kernel(head) contributes to a portion of the generated feature token. We believe the effective global information aggregation driven by these two (tenor representation + 1D Conv) simple yet intuitive ideas is the main reason why the proposed TensorVAE achieves SOTA with much less number of parameters.\n\nA.2 TRAINING HYPERPARAMETERS\n\nTraining is conducted on a single Tesla V100 GPU. We follow a similar learning rate schedule, shown by Eq.3 of the original Transformer paper (Vaswani et al., 2017) but with dmodel = 9612. This results in a maximum learning rate of 1.6e−4. To tackle the notorious issue of KL vanishing (Fu et al., 2019), we set a minimum KL weight of 1e−4 and double it every 62.5e3 iterations until a maximum weight of 0.0256 is reached. We select Adam optimizer (Kingma & Ba, 2015) for training. We present some interesting observations of the training/validation curve corresponding to this setup in Sec.A.5. For both experiments, the TensorVAE is trained for 1e6 iterations with a batch size of 128. The implementation details of NaiveUNet is explained in Sec.A.3\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 NAIVEUNET MODEL ARCHITECTURE\n\nFigure 4: Naive UNet model. N = 69\n\nWe train the above NaiveUNet on the Drugs dataset for 30 epochs with a constant learning rate of 1e−4, and batch size of 32. We follow the same method presented in GraphDG (Simm & Hern ́andezLobato, 2019) to convert the predicted distance matrix to conformation.\n\nA.4 ATOM AND BOND FEATURES\n\nWe list the atom features and bond features together with the encoding method used to construct the proposed tensor in Tab.2.\n\nTable 2: Atom and bond features used to construct input tensor.\n\nFeature name\n\nAtom type\n\nAtom charge Atom chirality\n\nBond type Normalized bond length Bond stereochem\n\nBond in-ring size Coordinate (3 channels) Pair wise atom distance\n\nFeature value\n\nEncoding method\n\nH, C, N, O, F, S, Cl, Br, P, I, Na, B, Si, Se, K, Bi -2, -1, 0, 1, 2, 3 Unspecified, Tetrahedral CW Tetrahedral CCW, Other Single, Double, Triple, Aromatic, Virtual -\nStereoNone, StereoAny, StereoZ StereoE, StereoCIS, StereoTrans 3 - 10 -\n-\n\none-hot\n\none-hot\n\none-hot\n\none-hot real-value\n\none-hot\n\none-hot real-value real-value\n\n14\n\nvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdC0000.10.23000000.10000000000.10.160.68000.10.80.310.7500.10.650.91.10.10N2N2/4N2/16N2/166464128128N2/16256N2/64256N2/256384N2/256512N2/256384N2/64256N2/16256N2/16128N2/16N2/412864N264N22EncoderDecoderEncoder BlockDecoder BlockSkip connectionFeature depth3 x 3 Conv kernelMasked output due to symmetryUnder review as a conference paper at ICLR 2023\n\nA.5 TRAINING AND VALIDATION CURVE\n\nWe present the train and validation plots for KL and reconstruction loss based on Drugs dataset in Fig.5a and Fig.5b, respectively. Both plots are based on an initial KL weight of 1e−4 doubling every 62.5k iterations (40 epochs). While KL validation loss reached 18.29 after 1e6 iterations (640 epochs), the reconstruction/RMSD loss reached 0.64 ̊A at the end of training. During the first 5 epochs of training, model learning focused on reducing the KL loss due to it is orders of magnitude larger than the RMSD loss. We were expecting this trend to continue for a while until both losses converge roughly in the same range. However, much to our surprise, the model seemed to find a way to drastically reduce RMSD loss much earlier by leveraging the information from the GDR encoder; it learned to ”cheat” by directly reversing coordinate information embedded in the output of GDR encoder back to the original conformation. The RMSD loss dropped to as low as 0.08 ̊A. On the other hand, the KL loss climbed to almost 800, signaling signifcant divergence from standard normal distribution. At this stage, output of the GDR encoder contains informative features of the original 3D coordinates. With the KL loss weight increasing, it becomes more difficult for the model to cheat since training is forcing the output of GDR encoder to conform to a standard uninformative Gaussian distribution. The KL loss started to drop while the RMSD loss remained steady, indicating increasing reliance on the output of G encoder for reconstructing the conformation. As the output of GDR encoder becomes less informative, the model learned to rely almost entirely on the aggregated feature from the G encoder to decode conformation.\n\nWe attempted to initiate the training with a much larger initial KL weight (1e−2) to prevent ”cheating” from begining. However, this quickly led to the notorious KL vanishing issue (Fu et al., 2019). We figure that ”cheating” is actually beneficial in that it reduces learning difficulty particularly for the decoder; its weights are tuned on easy training task, simply reversing what GDR encoder has done. In other words, the tuned weights of the decoder already hold crucial information on how to decode highly informative input features. As KL weight increases, model learning shifts to make the output of G encoder more informative. Also, this maybe an easier learning task as the RMSD loss is already very low (back-propagation of this loss contributes little to weight update); instead, model learning primarily focuses on optimizing the KL loss. This two-stage iterative loss optimization is much easier than optimizing both losses simultaneously throughout the training process.\n\n(a) KL loss.\n\n(b) RMSD loss.\n\nFigure 5: Training and validation plots for Drugs dataset. Orange line: Train; Blue line: Validation\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA.6 GENERATION PERFORMANCE ON DRUGS DATASET WITH FORCE FIELD OPTIMIZATION\n\nTable 3: Performance comparison between methods with FF optimization.\n\nMethod\n\nCVGAE + FF GraphDG + FF CGCF + FF ConfVAE + FF GeoDiff + FF\n\nTensorVAE + FF\n\nCOV\n\nMAT\n\nMean Median Mean\n\nMedian\n\n83.08 84.68 92.28 91.88 92.27\n\n96.15 ±0.34\n\n95.21 93.94 98.15 100 100\n\n100 ±0\n\n0.9829 0.9129 0.7740 0.7634 0.7618\n\n0.9177 0.9090 0.7338 0.7312 0.7340\n\n0.6723 ±0.0023\n\n0.6605 ±0.0064\n\n*Results for CVGAE, GraghDG, CGCF, ConfVAE and GeoDiff are taken from (Xu et al., 2021b); Standard deviations are obtained by repeating experiments 10 times each with a different random seed on a test set with 14396 conformations.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nA.7 MOELCULAR PROPERTY PREDICTION RESULTS\n\nWe conduct further experiment on molecular property prediction to demonstrate the effectiveness of the proposed feature engineering method. Following Uni-Mol (Zhou et al., 2022) and GEM (Fang et al., 2022), we report property prediction result on the MolecularNet QM9 regression task (https://moleculenet.org/datasets-1). The goal of this task is to estimate homo, lumo, and homo-lumo gap properties of molecules in the QM9 dataset based on their molecular structure. This task is different from the conformation generation experiment in that both 3D coordinates and 2D molecular graph of a molecule are supplied as input to a prediction model. The accuarcy of property prediction depends on how well a model can extract and aggregate such input information among atoms.\n\nSimilar to Uni-Mol, we adapt the proposed GDR encoder to this regression task by changing its prediction head. Specifically, we use the same GDR transformer encoder structure as presented in Fig.3 (with only 4 attention layers) and add an additional mean pooling layer, which is then followed by a linear layer for property prediction. To obtain training data, we follow the same data train-valtest split4 in Uni-Mol and GEM and standardize the output property data. This results in 106362 train samples, 13299 val samples and 13356 test samples. We train the adapted model for 300 epochs with a batch size of 128. The learning rate schedule is the same as TensorVAE. We report the mean average error(MAE) over all the test samples.\n\nThe result of the adapted model is compared to those of 7 other models including;\n\n• D-MPNN (Yang et al., 2019), AttentiveFP (Xiong et al., 2019) and GEM which are GNN\n\nbased models without pretraining;\n\n• N-Gram (Liu et al., 2019), PretrainingGNN (Hu et al., 2019) and GROVER (Rong et al., 2020) with pretraining. In particular, GROVER integrates GNN into a Transformer architecture, and there are two variants with different model capacity, GROVERbase and GROVERlarge;\n\n• and finally, three variants of Uni-Mol including one without pretraining, another without\n\nusing the N × N pair representation matrix, and the complete version.\n\nThe MAE for all compared methods are summaried in Tab.4. The proposed method produces a competitive performance, only underperforming the complete Uni-Mol setup with pretraining and also considering pair representation. This experiment demonstrates that the proposed feature engineering method is very effective at global information aggregation.\n\nTable 4: Property prediction result comparison based on MolecularNet QM9 benchmark.\n\nMethod\n\nD-MPNN AttentiveFP N-Gram PretrainGNN GROVER base GROVER large GEM Uni-Mol w/o pair representation Uni-Mol w/o pretraining Uni-Mol GDR encoder (ours)\n\nMAE\n\n0.00814 (0.00001) 0.00812 (0.00001) 0.00964 (0.00031) 0.00922 (0.00004) 0.00984 (0.00055) 0.00986 (0.00025) 0.00746 (0.00001) 0.00573 (0.00004) 0.00653 (0.00040) 0.00467 (0.00004) 0.00553 (0.00012)\n\n*All results are taken from (Zhou et al., 2022). Values in parenthesis are standard deviation obtained by repeating experiments 4 times.\n\n4The data is split with a ratio of 8 : 1 : 1 using scaffold spliting while considering chirality\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nA.8 ABLATION STUDIES\n\nIn this section, we further demonstrate the effectiveness and necessity of running an 1D convolution with N × 3 kernels over the proposed input tensor through 3 ablation studies.\n\nWhy is 1D convolution necessary. We have shown a model based on a 3×3 kernel in Sec.A.3 called NaiveUNet. Here, we provide a more detailed analysis of why NaiveUNet produces unsatisfactory result. The primary reason for this poor performance is the “field of view” of a conventional d × d (d < N ) kernel only sees a partial connection pattern of a focal atom. In comparison, a N × 3 kernel’s “field of view” encompasses the complete connection pattern of a focal atom. We further observe that when applying a 3 × 3 kernel filter to the top left region of the proposed tensor, its field of view only includes a focal atom, its two neighboring atoms and how the focal atom is connected to them. There are two main disadvantages associated with this. Firstly, it only achieves a 1-hop information aggregation. Secondly when the 3 × 3 kernel moves to an off-diagonal part of the tensor, where most connections are virtual bonds (as atoms of a molecule are often sparsely connected), information aggregation occurs mostly between atoms that are not chemically connected and is therefore less meaningful than that on the diagonal part of the tensor. For these two reasons, the NaiveUNet’s performance on the GEOM Drugs dataset is the worst as shown in Tab.5.\n\nWhat happens if we remove all virtual bonds. Notice that if we remove all the virtual bonds in each column and still run a N × 3 kernel through the tensor, its “field of view” is a “2-hop atomicenvironment” (because the focal atom can “see” how neighboring atoms are chemically connected to all their direct neighbors). Another observation is that after removing all virtual bonds, each column does not correspond to a fully-connected GNN. Therefore it no longer enables a global information aggregation. The conformation generation results of this variant of TensorVAE on Drugs dataset is shown as as TensorVAE abla1 in table below. It is observed that due to local-only information aggregation as a result of removing all virtual bonds (and related atom features), the performance is worse than the complete TensorVAE version.\n\nWhat happens if a N × 1 kernel is used. The final ablation study concerns with using a N × 1 kernel with a smaller ”field of view” as compared to that of a N × 3 kernel. Its performance on Drugs dataset is shown as TensorVAE abla2 in Tab.5. It performs slightly better than the ablation removing all virtual bonds. The reason is that though its field of view is smaller, it still achieves a global information aggregation for the focal atom. Nevertheless, it underperforms the complete TensorVAE version due to a smaller ”field of view” for information aggregation.\n\nTable 5: Performance comparison among models with different input feature engineering setupon GEOM Drugs dataset\n\nMethod\n\nCOV\n\nMean\n\nMedian\n\nMean\n\nMAT\n\nMedian\n\nNaiveUNet TensoVAE abla1 TensoVAE abla2 TensorVAE\n\n52.14 ± 1.48 90.72 ± 1.54 91.04 ± 1.21 93.34 ± 0.35\n\n51.69 ± 1.17 99.53 ± 0.64 99.74 ± 0.42 99.90 ± 0.31\n\n1.4322 ± 0.0247 0.8748 ± 0.0161 0.8706 ± 0.0131 0.8074 ± 0.0135\n\n1.3861 ± 0.0173 0.8619 ± 0.0214 0.8561 ± 0.0204 0.7927 ± 0.0186\n\n*The standard deviations for all ablation studies are obtained by testing on 2000 testing molecules.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nA.9 SAMPLES OF GENERATED CONFORMATIONS\n\nFigure 6: Generated samples by the TensorVAE\n\n19\n\nGround TruthGeneratedGround TruthGeneratedGround TruthGeneratedGround TruthGenerated",
  "translations": [
    "# Summary Of The Paper\n\nThe authors propose TensorVAE, a relatively simple model for generating 3D conformations from 2D molecular graphs. TensorVAE employs 1) a unique feature engineering step that represents each molecule as a tensor (with or without 3D coordinates and distances), and 2) a VAE with two transformer encoders, one that encodes the graph (using the tensor input without 3D information) and the approximate posterior that produces latents from the 3D information, and a transformer decoder for the likelihood, where keys and queries come from the 2D graph representation, and values are the latents from the 3D posterior encoder. The loss is a standard roto-translation invariant loss.  The authors show that, using standard transformer architectures and training procedures, TensorVAE performs comparably to the best current methods at conformation generation using the GEOM dataset, and argue that TensorVAE is much simpler than the comparable models due to superior feature engineering.\n\n# Strength And Weaknesses\n\nStrengths:\n- Simple architecture and featurization\n- Original use of two encoders for 2D and 3D features in the VAE formulation that allows for 3D conformation generation from 2D graphs\n- (Near) state-of-the-art results for conformation generation (and QM9 property prediction)\n\nWeaknesses:\n- It is not clear that the proposed method of producing atom-tokens via 1D convolution on the input tensor is necessary. Any number of possible aggregation steps could have been used, many of which would likely have yielded similar results. One could also imagine performing a Tucker decomposition and using singular values as tokens, etc.\n- Moreover, it's not even clear if the tensor formulation is needed. The 1D convolution aggregates information about the radius-1 atomic environment (including virtual bonds). One could use any radius-1 atomic-environment hash as tokens with the proposed featurization, which would likely yield comparable results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe presentation is clearly written, and the work appears to be of sufficient quality.\n\nThe feature engineering is marginally original (see weaknesses above). Combining 3D latents as values with 2D embeddings as keys and queries in the VAE formulation appears to be a novel way of learning to generate 3D confirmations from 2D graphs.\n\nThe authors claim everything is straightforward and provide no code.\n\n# Summary Of The Review\n\nThe paper extends existing methods and molecular featurizations to achieve near state-of-the-art results for conformation generation. The paper spends a lot of time arguing for the superiority of the feature engineering used, but it is not at all clear that the results depend on that featurization. The authors could greatly improve this paper by demonstrating the value of the feature engineering beyond performance on benchmarks through ablation studies (e.g. removing aspects of the features) and studies of other featurizations that are not conflated with the tensor design (e.g., radius-1 atom environments).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a direct generative model designed to efficiently generate 3D molecular conformations from 2D molecular graphs, with a particular focus on drug discovery applications. The authors identify limitations in existing approaches, which either rely on indirect methods using inter-atomic distances or require multiple sampling steps. TensorVAE addresses these challenges through innovative feature engineering that combines atom and bond features into a tensor representation, which is then processed using a conditional Variational Autoencoder (CVAE) framework. Experimental results demonstrate that TensorVAE achieves competitive performance against state-of-the-art methods while simplifying the generation process.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to feature engineering and the clarity of its methodology, which allows for efficient 3D conformation generation in a single step. The use of tensor representation and the integration of a transformer architecture for attention mechanisms are notable contributions that enhance model performance. However, a potential weakness is the reliance on a naive UNet for initial distance matrix predictions, which may raise questions about the robustness of this preliminary model compared to more complex architectures. Additionally, while the results are promising, further validation on a broader set of molecular datasets could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results of TensorVAE. The quality of writing is high, with sufficient detail provided for each component of the model and experiments. The novelty of the approach is significant, particularly in the context of combining feature engineering with a CVAE framework. The authors also provide a reproducibility statement, indicating that their implementation is accessible using standard libraries and datasets, which enhances the potential for replication of results in future studies.\n\n# Summary Of The Review\nOverall, the paper introduces a novel and effective generative model, TensorVAE, that simplifies the generation of 3D molecular conformations from 2D graphs. Its strengths in feature engineering and clear methodology are compelling, although further validation on diverse datasets would be beneficial. The paper is well-written and reproducible, making it a valuable contribution to the field of computational drug design.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed for generating 3D molecular conformations directly from 2D molecular graphs, addressing a key challenge in in-silico drug discovery. The methodology is grounded in a conditional Variational Autoencoder (CVAE) framework that leverages tensor representations to consolidate atom and bond features. The findings indicate that TensorVAE outperforms existing state-of-the-art methods in terms of performance metrics (Coverage Score and Matching Score) while significantly reducing computation time for conformation generation.\n\n# Strength And Weaknesses\nStrengths of the paper include its efficiency in generating molecular conformations in a single step, which contrasts sharply with methods that require multiple sampling steps, thereby reducing computational overhead. The simplicity of the architecture, which employs standard convolutional and transformer components without overly complex task-specific modifications, is another advantage. However, the paper also has notable limitations, particularly its reliance on effective feature engineering, which could affect generalization to broader datasets or less common molecular structures. Additionally, there are concerns about potential overfitting and the computational limits when dealing with very large molecules.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, effectively conveying the model's contributions and methodology. The quality of the experimental design is commendable, with appropriate datasets and evaluation metrics used to substantiate the claims. The novelty lies in the integration of tensor representations and the unique feature engineering approach. However, reproducibility could be a concern due to the heavy reliance on feature design and the potential variability in performance with different datasets or molecular types.\n\n# Summary Of The Review\nOverall, TensorVAE is a significant contribution to the field of molecular conformation generation, demonstrating substantial efficiency and performance through innovative feature engineering. While it shows promise for practical applications, attention must be paid to the robustness of feature representation and generalizability of the model.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model that directly generates 3D molecular conformations from 2D molecular graphs in a single step. The methodology employs a conditional Variational Autoencoder (CVAE) framework enhanced by a unique tensor representation of molecular graphs, which integrates atom and bond features. The findings indicate that TensorVAE achieves competitive performance against existing state-of-the-art models while maintaining simplicity and efficiency, particularly in terms of decoding times.\n\n# Strength And Weaknesses\nThe primary strength of TensorVAE lies in its innovative feature engineering, which allows for effective 3D conformation generation without resorting to complex model architectures. The use of tensor representations and a modified self-attention mechanism facilitates comprehensive information aggregation from molecular features. However, the paper does not thoroughly explore the implications of its findings in practical applications, such as drug discovery. Additionally, while the results are promising, further comparative analysis with a broader range of methods could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and results. The quality of the writing is high, making it accessible to readers with a basic understanding of deep learning and molecular generation. The novelty of the approach, particularly in feature engineering and the integration of tensor representations, is commendable. The reproducibility of the work is adequately addressed, with a commitment to using publicly available datasets and standard libraries, which is crucial for future research validation.\n\n# Summary Of The Review\nOverall, TensorVAE represents a significant contribution to the field of molecular conformation generation, showcasing how effective feature engineering can yield competitive results without excessive model complexity. The clarity and reproducibility of the work further enhance its value in both academic and practical contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"TensorVAE: A Direct Generative Model for Molecular Conformation Generation Driven by Novel Feature Engineering\" introduces a generative model aimed at directly producing molecular conformations from 2D molecular graphs. It leverages innovative feature engineering, particularly through a novel tensor representation, to capture molecular features. The findings demonstrate that TensorVAE achieves competitive performance compared to existing state-of-the-art models while maintaining efficiency in computation and ease of implementation across standard machine learning frameworks.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including its capability for direct generation of molecular conformations, which simplifies the modeling process. The innovative feature engineering enhances the model's performance, yet it also brings limitations, such as potentially oversimplifying complex inter-atomic relationships. While the model's competitive performance is commendable, the lack of task-specific optimizations may hinder its effectiveness in specialized scenarios. Additionally, the reliance on standard architectures may limit the model's ability to capture intricate molecular dynamics. The extensive experimental validation is a strength, although the narrow focus on benchmark datasets could restrict its generalizability. Finally, while the tensor representation is a novel contribution, it may introduce complexity that could be challenging for users without a solid understanding of tensor algebra.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, contributing to its overall clarity. The innovative features of the model demonstrate a good level of technical novelty, although some aspects, such as the tensor representation, could benefit from more intuitive explanations. The quality of the experiments conducted is high, but the reproducibility may be somewhat limited due to the specialized nature of the tensor features and the specific datasets used for validation.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in generative modeling for molecular conformations through innovative feature engineering and direct generation capabilities. While the model shows competitive performance and efficiency, it may be constrained by its simplistic architecture and limited generalizability across diverse molecular structures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed for the direct generation of 3D molecular conformations from 2D molecular graphs. The key contributions include a simplified architecture that employs innovative feature engineering through a tensor representation of molecular graphs, enabling efficient processing. The methodology involves encoding molecular graphs into fully-connected tensors, utilizing convolutional operations for feature aggregation, and integrating transformer encoders within a conditional Variational Autoencoder framework. Experimental results demonstrate that TensorVAE outperforms existing methods on benchmark datasets, achieving competitive performance while maintaining a simpler and more efficient design.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to input feature engineering and the simplicity of its architecture, which allows for efficient generation of molecular conformations in a single step. The use of tensor representations aggregates atom and bond features effectively, enhancing the model's performance. Additionally, the extensive empirical evaluation across multiple datasets substantiates the claims of improved performance. However, the paper could benefit from more detailed comparisons with foundational models to better highlight its contributions and potential limitations, particularly regarding scalability and the handling of more complex molecular structures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making the methodology and results easy to follow. The novelty of the tensor-based approach and the direct generation framework is significant, showcasing a departure from more complex models. The reproducibility of the results may be enhanced by providing additional details on experimental setups and hyperparameter choices, as well as including code or models for validation by the research community.\n\n# Summary Of The Review\nOverall, TensorVAE represents a meaningful advancement in the field of molecular conformation generation, combining innovative feature engineering with a straightforward architecture that achieves competitive performance. While the findings are promising, further contextualization against foundational models and a discussion of potential limitations could strengthen the paper's contributions.\n\n# Correctness\nRating: 4/5\n\n# Technical Novelty And Significance\nRating: 4/5\n\n# Empirical Novelty And Significance\nRating: 4/5",
    "# Summary Of The Paper\nThis paper introduces TensorVAE, a novel generative model designed for the generation of molecular conformations from 2D molecular graphs. The main contributions include an emphasis on intuitive feature engineering to create a fully-connected tensor representation that combines atom and bond features, along with an efficient adversarial training process that generates conformations in a single sampling step. The authors demonstrate that TensorVAE achieves competitive performance compared to state-of-the-art methods through extensive empirical evaluations, providing a clear methodology that enhances the reproducibility of the results.\n\n# Strength And Weaknesses\n**Strengths**:\n1. **Innovative Feature Engineering**: The creation of a tensor representation that integrates atom and bond features fosters better information aggregation during training, which is a critical contribution to the field.\n2. **Efficiency**: Generating conformations in a single step significantly reduces the computational time and resources required, which is particularly beneficial for applications in drug discovery.\n3. **Competitive Performance**: The experimental results validate that TensorVAE can achieve or exceed the performance of more complex adversarial training methods, showcasing its effectiveness.\n4. **Clear Methodology**: The well-structured methodology section allows for easy understanding and reproducibility of the proposed approach.\n5. **Practical Implications**: The model's compatibility with popular libraries like PyTorch and TensorFlow makes it accessible for further research and application.\n\n**Weaknesses**:\n1. **Limited Exploration of Adversarial Dynamics**: The paper could improve by providing deeper insights into how adversarial dynamics specifically affect model performance, potentially through comparative analyses with other strategies.\n2. **Lack of Theoretical Justification**: A stronger theoretical foundation explaining the advantages of the proposed feature engineering would bolster the empirical findings.\n3. **Generalizability**: The focus on molecular conformations may restrict applicability to other generative modeling tasks, with a discussion on broader applicability needed.\n4. **Evaluation Metrics**: While quantitative metrics are comprehensive, qualitative analyses of the generated conformations would offer a more nuanced view of the model's capabilities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, presenting clear arguments and methodologies that enhance comprehension. The quality of the figures and tables is high, and the reproducibility is facilitated by detailed descriptions of the model architecture and training processes. The novelty lies primarily in the innovative feature engineering approach and its implications for efficiency in adversarial training, although some theoretical underpinning would strengthen its impact.\n\n# Summary Of The Review\nOverall, TensorVAE presents a significant advancement in generative modeling through its efficient and effective feature engineering approach. While the empirical results are promising, the paper could benefit from deeper theoretical insights and broader discussions on generalizability. Addressing these weaknesses may enhance the work's overall impact and relevance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a generative model aimed at revolutionizing molecular conformation generation from 2D molecular graphs in a single step. It claims to deliver unprecedented efficiency and simplicity compared to existing methods, which are described as overly complex. The methodology revolves around a unique tensor encoding approach and the model reportedly outperforms 18 state-of-the-art methods, achieving near-perfect results with minimal computational resources. However, the claims of superiority are juxtaposed with a less favorable portrayal of prior work, leading to an inflated perception of TensorVAE's contributions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious goal to simplify molecular conformation generation, potentially making it more accessible for practical applications in drug discovery. The introduction of a tensor encoding method suggests a promising avenue for future research. However, the paper's weaknesses are notable: it dismisses existing methods without sufficient justification, and the performance comparisons often downplay the competitive results of these prior models. Additionally, the claims of ease of reproducibility may mislead readers, as the nuances involved in implementing the model are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is undermined by its tendency to overstate the uniqueness and effectiveness of TensorVAE while neglecting the complexities of existing approaches. The quality of the presentation suffers from a lack of balanced discussion regarding prior work and the actual improvements demonstrated by TensorVAE. While the model presents a novel feature engineering methodology, the reproducibility claims appear overly optimistic, as they do not account for the intricacies involved in practical implementation.\n\n# Summary Of The Review\nOverall, while TensorVAE presents an interesting approach to molecular conformation generation, the paper's claims are inflated and overshadow the contributions of existing methodologies. The lack of a balanced comparison and a nuanced discussion on reproducibility detracts from the overall credibility of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed to generate 3D molecular conformations directly from 2D graphs using an innovative feature engineering approach. The methodology revolves around a conditional Variational Autoencoder (CVAE) framework that processes input tensors, allowing for efficient and direct generation without the complexities of specialized architectures. Experimental results demonstrate TensorVAE's generative capabilities, achieving competitive performance metrics on the GEOM-QM9 and GEOM-Drugs datasets, significantly outperforming several state-of-the-art methods.\n\n# Strength And Weaknesses\nThe primary strength of TensorVAE lies in its ability to generate 3D molecular conformations efficiently from 2D graphs while maintaining simplicity in its architecture. The model's performance metrics, particularly the COV and MAT scores, indicate a substantial improvement over existing methods, highlighting its effectiveness in molecular generation tasks. However, a potential weakness is the reliance on standard convolution techniques; while this contributes to simplicity and practicality, it may limit the model's adaptability to more complex molecular structures in future applications. Furthermore, the paper could benefit from a more in-depth discussion on the limitations of the current approach and potential avenues for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the methodology and findings, making it accessible to readers with a background in deep learning and molecular modeling. The quality of the experimental results is high, showcasing the model's capabilities through thorough benchmarking against state-of-the-art methods. The novelty of the feature engineering approach is significant, providing a fresh perspective on direct molecular generation. The authors also ensure reproducibility by utilizing publicly available datasets and established frameworks like PyTorch or TensorFlow, which is commendable.\n\n# Summary Of The Review\nOverall, TensorVAE represents a significant advancement in the field of molecular conformation generation, offering a robust and efficient solution through innovative feature engineering. Its competitive performance and straightforward implementation make it an appealing option for practical applications in drug discovery and molecular design.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed for generating molecular conformations through innovative feature engineering. The authors propose representing molecular structures as 2D graphs and utilize a fully-connected symmetric tensor to capture interactions within these structures. The findings suggest that TensorVAE demonstrates competitive performance against state-of-the-art models, particularly in terms of generation efficiency, as measured by COV and MAT scores.\n\n# Strength And Weaknesses\nThe paper's strengths include its clear attempt to simplify the generative modeling process while leveraging novel feature engineering techniques. The use of a tensor representation is a notable contribution that could advance the field of molecular generation. However, there are significant weaknesses, such as the reliance on 2D representations, which may not fully capture the complexities of 3D molecular interactions. Additionally, the efficacy of the proposed feature engineering is questionable, as it may oversimplify the underlying chemistry. The evaluation metrics used (COV and MAT) may not comprehensively reflect the model's performance, and there are concerns regarding the generalizability of the findings to diverse molecular types and real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. However, the assumptions made regarding the tensor representation and the reliance on specific datasets could hinder reproducibility and the generalization of results. The novelty of the approach is commendable, but the potential gaps in capturing complex molecular behavior raise concerns about the overall quality of the findings.\n\n# Summary Of The Review\nOverall, TensorVAE presents an interesting approach to molecular conformation generation that leverages novel feature engineering and a tensor-based representation. However, the paper is weakened by its reliance on potentially oversimplified assumptions and evaluation metrics, which may limit the robustness and applicability of the proposed model in broader contexts.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a generative model designed to efficiently generate 3D molecular conformations directly from 2D molecular graphs in a single step. The main contributions include a novel feature engineering approach utilizing token-like feature vectors for atoms, processed through transformer encoders within a conditional Variational Autoencoder (VAE) framework. The methodology employs a tensor representation that combines atom and bond features into a single input tensor, incorporating 1D convolutions for global information aggregation. Experimental results demonstrate that TensorVAE outperforms 11 state-of-the-art models in terms of conformation generation and molecular property prediction, highlighting the significance of effective feature engineering over complex architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to feature engineering, which enhances the efficiency of 3D molecular conformation generation while maintaining simplicity. The use of tensor representations and transformer architectures is well-justified and shows promise for future applications in molecular modeling. However, a notable weakness is the lack of extensive comparison with a broader range of existing methods, which could provide deeper insights into the model's relative performance. Additionally, while the results are promising, the paper could benefit from a more thorough exploration of the limitations and potential challenges in applying TensorVAE to diverse molecular datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that effectively communicates the methodology and findings. The quality of the writing is high, and technical concepts are presented in an accessible manner. The novelty of TensorVAE is significant, particularly in its integration of feature engineering with established architectures. The authors provide a reproducibility statement, claiming that the implementation can be easily replicated using publicly available datasets and standard deep learning libraries, which enhances the paper's credibility.\n\n# Summary Of The Review\nOverall, the paper presents a well-structured and innovative approach to 3D molecular conformation generation through the TensorVAE model. While the contributions are noteworthy and the methodology appears robust, further comparisons with a wider array of existing methods and a discussion of limitations would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel framework aimed at improving the interpretability of deep learning models through the integration of symbolic reasoning. The authors propose a hybrid approach that combines neural networks with a logic-based system, enabling enhanced transparency in decision-making processes. The methodology involves training a neural network to generate symbolic rules that can be easily understood by humans. The findings demonstrate that this approach not only maintains competitive performance on standard benchmarks but also significantly enhances interpretability compared to traditional black-box models.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance**: The intersection of interpretability and deep learning is a pressing issue in the field, making this work highly relevant.\n2. **Innovation**: The hybrid approach is a fresh perspective on integrating symbolic reasoning with neural networks, offering potential for significant advancements in model transparency.\n3. **Clarity**: The paper is well-written with a logical flow, making complex ideas accessible to a broad audience.\n4. **Literature Review**: A thorough review of existing literature provides a solid foundation for the proposed methodology and situates it within the current research landscape.\n\n**Weaknesses:**\n1. **Experimental Validation**: While the results are promising, the experiments are somewhat limited in scope, raising questions about the robustness of the findings across diverse datasets.\n2. **Comparative Analysis**: The paper lacks a rigorous comparison with other state-of-the-art interpretability methods, which would clarify the advantages and limitations of the proposed approach.\n3. **Generalizability**: The applicability of the proposed method to various real-world settings and datasets is not adequately explored.\n4. **Technical Depth**: Certain technical aspects of the integration between neural networks and symbolic reasoning could benefit from more detailed explanations to enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with a quality that reflects careful writing and organization. The novelty of integrating symbolic reasoning into deep learning models is significant and could influence future research directions. However, the reproducibility of the results is somewhat hindered by the lack of detailed technical explanations and limited experimental scope.\n\n# Summary Of The Review\nThe paper presents a compelling and innovative approach to enhancing the interpretability of deep learning models by integrating symbolic reasoning. Although the work is relevant and well-articulated, it would benefit from more comprehensive experimental validation and a deeper comparative analysis to strengthen its contributions further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed to efficiently generate 3D molecular conformations directly from 2D molecular graphs, addressing limitations of existing deep learning methods that either rely on indirect sampling or involve complex multi-step processes. TensorVAE employs a straightforward feature engineering approach by creating token-like feature vectors for each atom through convolution operations, which are then processed using transformer encoders within a conditional Variational Autoencoder (VAE) framework. The experimental results indicate that TensorVAE achieves competitive performance against state-of-the-art models while maintaining a simple architecture.\n\n# Strength And Weaknesses\nThe main strength of TensorVAE lies in its innovative approach to feature engineering, which simplifies the input representation without sacrificing performance. The integration of convolution operations and transformer encoders allows for effective information aggregation, making the model both intuitive and practical. However, a potential weakness is the reliance on a relatively simple architecture, which may limit its scalability and adaptability for more complex molecular systems or other domains like protein structure prediction. Additionally, the paper could benefit from more extensive comparisons with a broader range of existing methods to fully contextualize its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, making it accessible to both specialists and those new to the field. The novelty of TensorVAE is evident in its approach to direct generation and feature engineering, and the reproducibility of the results appears strong, as extensive experiments are provided to support the claims made.\n\n# Summary Of The Review\nOverall, TensorVAE presents a significant advancement in the direct generation of 3D molecular conformations from 2D graphs through its innovative feature engineering and straightforward architecture. While it achieves competitive performance, its scalability and adaptability to more complex tasks remain to be fully assessed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed for generating 3D molecular conformations from 2D molecular graphs. The approach focuses on innovative feature engineering, utilizing a tensor representation of atom and bond features, followed by a conditional variational autoencoder (CVAE) framework with transformer encoders. The authors demonstrate that TensorVAE achieves competitive generative performance compared to existing sophisticated models while emphasizing efficiency and the importance of feature representation over architectural complexity.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to feature engineering and its effective use of convolutional operations to create token-like feature vectors that enhance the generative process. The modified attention mechanism in transformers allows for better integration of outputs, which is a significant improvement over traditional methods. However, the paper could benefit from a more detailed discussion of the limitations of the proposed model and comparisons with a wider range of existing methods. The evaluation metrics, while appropriate, may not fully capture the nuances of molecular conformation generation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly presents its contributions, methodology, and results. The organization of the material flows logically from the introduction to the conclusion. The novelty of the approach is significant, especially with the emphasis on feature engineering. The guidelines provided for reproducing results using public datasets and standard implementations enhance the paper's quality and reliability, supporting claims of reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a compelling case for the TensorVAE model through its innovative feature engineering and competitive performance in generating molecular conformations. While it presents a strong contribution to the field, there is room for improvement in discussing limitations and providing a broader comparative analysis with existing approaches.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a novel generative model designed for molecular conformation generation from 2D molecular graphs. It emphasizes the use of innovative feature engineering and a conditional variational autoencoder (CVAE) framework, incorporating tensor representations of molecular graphs that include atom and bond features. The authors validate TensorVAE on the GEOM-QM9 and GEOM-Drugs datasets, demonstrating its competitive performance against 11 state-of-the-art models through metrics such as coverage score (COV) and matching score (MAT). The findings highlight TensorVAE's efficiency and direct generation capability, while also suggesting potential extensions to protein structure prediction.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its straightforward approach to a complex problem, effectively leveraging feature engineering to enhance model performance without resorting to overly complex architectures. The empirical results indicate that TensorVAE outperforms or matches existing methods, which underscores its significance in the field of molecular generation. However, the paper acknowledges limitations associated with previous methods and raises concerns about the complexity of some existing approaches. While the contribution is substantial, the discussion could benefit from more detailed considerations of the limitations faced by TensorVAE itself.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections that guide the reader through its contributions and methodology. The quality of writing is high, making complex concepts accessible. The novelty of TensorVAE is significant, particularly in its emphasis on feature engineering rather than solely on advanced model architectures. Reproducibility is also addressed effectively, with the authors providing comprehensive implementation details and utilizing publicly available datasets and standard libraries, facilitating further research and validation by the community.\n\n# Summary Of The Review\nOverall, TensorVAE presents a compelling approach to molecular conformation generation, characterized by its innovative use of feature engineering and solid empirical validation. The paper is clear, well-organized, and contributes valuable insights into the field, although it could further elaborate on its own limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model designed to efficiently generate 3D molecular conformations directly from 2D molecular graph representations. The authors propose a conditional Variational Autoencoder (CVAE) framework that employs innovative feature engineering techniques, utilizing standard convolutional operations to derive token-like feature vectors for each atomic entity. The methodology involves integrating atom and bond features into a tensor format, leveraging a NaiveUNet for distance matrix prediction, and employing a transformer architecture for self-attention. Experimental results demonstrate that TensorVAE outperforms existing state-of-the-art methods in terms of coverage and matching scores, highlighting its efficacy in molecular conformation generation.\n\n# Strength And Weaknesses\nThe primary strengths of TensorVAE lie in its innovative approach to feature engineering and its ability to simplify the generative process by enabling direct conformation generation. The integration of a comprehensive tensor structure and the use of transformer architectures for global information aggregation are commendable contributions that enhance the model's capability. However, weaknesses include the potential oversimplification of the architecture, which may limit the model's adaptability to more complex molecular structures. Additionally, while the empirical results are promising, the reliance on benchmark datasets raises questions about the generalizability of the findings to other molecular domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem definition, methodology, and experimental results. The explanations of the tensor representation, convolutional architecture, and loss function are thorough and accessible. The novelty of the approach is significant, particularly in the context of simplifying molecular conformation generation. However, reproducibility may be a concern if the authors do not provide sufficient details regarding the implementation and hyperparameter tuning, which would be essential for other researchers to replicate the results.\n\n# Summary Of The Review\nOverall, TensorVAE represents a significant advancement in the field of molecular conformation generation, combining innovative feature engineering with a streamlined architecture. While the results are promising, further exploration is needed to assess the model's adaptability to more complex molecular scenarios and ensure reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a generative model aimed at producing molecular conformations from 2D molecular graphs. The authors utilize a conditional variational autoencoder framework, leveraging standard convolution operations and transformer encoders, with the goal of simplifying the generation process. However, the findings suggest that while TensorVAE achieves competitive performance, it does not significantly outperform existing state-of-the-art methods, raising questions about its true contributions to the field.\n\n# Strength And Weaknesses\nThe primary strength of TensorVAE lies in its attempt to simplify the process of molecular conformation generation by claiming to generate conformations directly from 2D representations. However, this simplification is also a notable weakness, as it fails to capture the complexities inherent in molecular interactions. The reliance on feature engineering and standard architectures, rather than innovative model designs, suggests a lack of depth in addressing the underlying generative processes. Furthermore, critical limitations such as the small kernel size and quadratic growth of output size with the number of atoms indicate significant scalability issues.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the claims regarding efficiency and novelty are overstated. The novelty primarily revolves around feature engineering, which does not represent a substantial advancement over existing methodologies. Reproducibility may be a concern as the model's performance appears to heavily depend on specific configurations, as highlighted in the ablation studies. The potential KL vanishing problem is acknowledged but not thoroughly addressed, which could hinder reproducibility.\n\n# Summary Of The Review\nOverall, TensorVAE does not present a groundbreaking advancement in the field of molecular conformation generation. While it offers a simplified approach, it lacks the depth and innovation required for a significant impact, and its limitations in scalability and robustness raise concerns about its practical application.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel model designed to generate 3D molecular conformations directly from 2D molecular graphs in a single sampling step. This approach significantly enhances the efficiency of in-silico drug discovery by simplifying the generation process. The authors introduce a novel encoding and feature extraction mechanism that utilizes standard convolution operations to create intuitive token-like feature vectors for each atom. Extensive experiments demonstrate that TensorVAE performs competitively against 18 state-of-the-art models on benchmark datasets, indicating its robustness and effectiveness in generating accurate molecular structures and predicting properties.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative methodology and practical implications. The direct generation of molecular conformations from 2D graphs simplifies the process, potentially accelerating drug discovery. The use of simple and widely adopted techniques makes TensorVAE accessible to practitioners. However, the paper could benefit from a more comprehensive comparative analysis with existing methods to better contextualize its advantages and limitations. Additionally, the theoretical underpinnings of the feature extraction process could be elaborated further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and generally clear, making the complex ideas accessible to a broad audience. The methodology is presented in a structured manner, with sufficient detail for reproducibility, particularly with implementations in popular frameworks like PyTorch and TensorFlow. The novelty of the approach is pronounced, particularly in its direct mapping from 2D to 3D conformations. However, further elaboration on specific performance metrics and the experimental setup would enhance the clarity regarding the robustness of the results.\n\n# Summary Of The Review\nOverall, TensorVAE represents a significant advancement in the field of molecular conformation generation, offering an efficient and accessible method that challenges the complexity of existing models. Its competitive performance in empirical tests reinforces its potential impact on drug discovery and related domains. Despite minor areas for improvement, the paper showcases a promising direction for future research.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel framework for generating 3D molecular conformations from 2D molecular graphs, which is significant for in-silico drug discovery. The methodology includes the encoding of molecular graphs into a fully-connected tensor, leveraging a conditional Variational Autoencoder (CVAE) to approximate the Boltzmann distribution effectively. Key contributions include the integration of atom and bond features through a tensor representation, the use of a 1D convolution for enhanced information aggregation, and the implementation of a transformer architecture to facilitate global information processing, all of which contribute to robust and invariant generative capabilities.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to feature engineering, utilizing tensor representations to enrich the encoding of molecular interactions, and its theoretical grounding in established statistical frameworks. The use of a roto-translation invariant loss function is particularly noteworthy, as it ensures the model's robustness across various molecular configurations. However, a potential weakness lies in the limited empirical validation of the proposed framework against existing state-of-the-art methods, which may affect the assessment of its practical effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its theoretical underpinnings clearly, making it accessible to readers familiar with both generative modeling and molecular conformation tasks. The quality of the writing and the clarity of the proposed methodologies are commendable. The novelty is significant, particularly in the context of tensor representations and their application in generative models. However, the reproducibility of results may be hindered if the implementation details and experimental setups are not sufficiently detailed, which should be addressed in future revisions.\n\n# Summary Of The Review\nOverall, TensorVAE presents a compelling and theoretically sound contribution to the field of molecular conformation generation, with unique methodologies that enhance information aggregation and model robustness. While the theoretical framework is strong, the empirical validation is somewhat limited, warranting further investigation to confirm its efficacy against existing approaches.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a novel generative model designed for molecular conformation generation from 2D graphs. The methodology employs a tensor representation that integrates atom and bond features, utilizing fully-connected and symmetric tensors for effective data encoding. The architecture consists of two transformer encoders tailored for different input tensor types (G and GDR), and the proposed model is trained using roto-translation invariant loss alongside KL regularization. Empirical results demonstrate that TensorVAE achieves competitive performance against state-of-the-art methods, highlighting its potential for application in molecular and potentially protein structure predictions.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to molecular conformation generation, particularly through the use of tensor representations and transformer architectures, which effectively capture both atom and bond features. The detailed ablation studies provide insights into the necessity of the proposed feature engineering, establishing a strong foundation for the model's design choices. However, a notable weakness is the lack of comprehensive code availability, as the paper does not provide a task-specific architecture, which may hinder reproducibility for some users. Additionally, while performance metrics like coverage and matching scores are reported, further qualitative evaluations could enhance the understanding of generated conformations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with a logical progression from the introduction of the tensor representation to the results of the ablation studies. The quality of the writing is high, making it accessible to both experts and non-experts in the field. In terms of novelty, TensorVAE presents a fresh perspective on generative modeling for molecular structures, leveraging existing frameworks in innovative ways. However, reproducibility is somewhat limited due to the absence of a complete codebase tailored to the task, which may pose challenges for practitioners looking to implement the model in their own research.\n\n# Summary Of The Review\nOverall, TensorVAE represents a significant contribution to the field of molecular modeling, showcasing a novel approach that combines tensor representations with transformer architectures. While the model demonstrates competitive performance and provides valuable insights through its design choices, the limitations in reproducibility need to be addressed to foster wider adoption and validation of the proposed methodology.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel generative model for molecular conformation generation that aims to improve upon existing distance-based methods. The authors argue that TensorVAE simplifies the generative process by bypassing complex reconstruction techniques and demonstrates improved performance with fewer computational resources. The findings suggest that TensorVAE outperforms models like Uni-Mol and GeoDiff in specific datasets, highlighting its efficiency and effectiveness in generating molecular structures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its assertion of TensorVAE's ability to generate molecular conformations in a single step, thereby addressing computational inefficiencies associated with traditional distance-based methods. However, the paper exhibits several weaknesses, including a lack of thorough comparisons with the benefits of existing approaches, such as CVGAE and DMCG, which may still hold merit despite their criticisms. Furthermore, the paper's emphasis on the simplicity of TensorVAE overlooks the potential advantages of more complex architectures, and it does not sufficiently address the impact of pretraining in model performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodologies clearly, allowing readers to follow the development of TensorVAE. However, the novelty of the input feature engineering claims is overstated, as previous work has explored similar ideas. The reproducibility is not adequately addressed, particularly regarding the performance metrics that lack contextualization within the broader landscape of molecular generation models.\n\n# Summary Of The Review\nWhile TensorVAE presents a compelling approach to molecular conformation generation, the paper's selective comparisons and lack of acknowledgment of existing methods' strengths undermine its overall contributions. The novelty claims are overstated, and the empirical results are not sufficiently contextualized.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents \"TENSORVAE,\" a novel variational autoencoder framework designed for tensor data representation. The authors introduce a methodology that leverages tensor decomposition techniques to improve the generative modeling of high-dimensional data. Key contributions include a comprehensive theoretical foundation for the framework, empirical validation through various datasets, and demonstrations of its effectiveness in capturing complex structures compared to traditional models like CVAE (Conditional Variational Autoencoder).\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its innovative approach to handling tensor data, which is often overlooked in existing generative models. The theoretical contributions are well articulated and provide a solid foundation for the proposed model. Empirical results demonstrate significant improvements in terms of reconstruction quality and generative performance compared to baseline methods. However, the paper could benefit from a more extensive discussion of the limitations of the TENSORVAE, particularly in handling noisy data or in scenarios with limited training samples.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-written and adheres to a clear structure, making it accessible to readers with varying levels of expertise. The quality of figures and tables is high, and they effectively support the narrative. While the methodology is described in detail, some sections could be further simplified for clarity, particularly for readers unfamiliar with tensor algebra. The reproducibility of the results is supported by the availability of code and datasets, though additional details on the implementation would enhance this aspect.\n\n# Summary Of The Review\nIn summary, \"TENSORVAE\" presents a significant advancement in the generative modeling of tensor data through an innovative framework that combines theoretical rigor with empirical validation. While the paper is clear and well-structured, further exploration of limitations and enhanced clarity in complex sections would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel feature engineering methodology named TensorVAE aimed at improving molecular modeling through generative techniques. The authors focus on the construction of a variational autoencoder specifically tailored for molecular data, utilizing features such as atom types, charges, and bond types. The findings suggest that TensorVAE demonstrates promising performance on two benchmark datasets, although the exploration of its applicability to more complex tasks, such as protein structure prediction, remains superficial.\n\n# Strength And Weaknesses\nWhile the proposed TensorVAE shows potential in feature engineering for molecular modeling, it has notable weaknesses. The lack of exploration regarding advanced modeling techniques, such as graph neural networks or hybrid generative models, diminishes its overall impact. Additionally, the authors have not adequately discussed the implications of using varied molecular features beyond the limited scope currently presented. The evaluation is constrained to just two datasets, which raises concerns about the generalizability of the results. Furthermore, the absence of a comparative analysis with other successful feature engineering techniques leaves a gap in understanding the effectiveness of TensorVAE. The paper also fails to address potential limitations, scalability issues, and real-world applications, which could provide a more comprehensive view of its practicality and significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, but there are areas that could benefit from additional detail, particularly regarding future work and limitations. The quality of the proposed methodology is apparent, but the novelty is somewhat diminished by the lack of integration with other advanced techniques and a shallow exploration of potential applications. Reproducibility is not thoroughly addressed, as details regarding the datasets and specific experimental setups are limited.\n\n# Summary Of The Review\nOverall, while the TensorVAE presents a promising approach to feature engineering in molecular modeling, its contribution is undermined by a lack of depth in both methodology and application. The paper would benefit from a broader exploration of the implications, limitations, and potential integrations with other techniques to enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a novel generative model that utilizes Maximum Likelihood Estimation (MLE) to approximate the Boltzmann distribution for generating molecular conformations. It employs Kullback-Leibler divergence (DKL) to establish a tractable lower bound for the likelihood function, enhancing the training process. The study demonstrates that TensorVAE achieves competitive performance on the Drugs dataset, with a Coverage Score (COV) of 93.34% and a Matching Score (MAT) of 0.8074 Å, significantly outperforming baseline methods. Additional evaluations, including ablation studies, reinforce the model's advantages and the importance of its feature engineering methods.\n\n# Strengths And Weaknesses\nThe paper's strengths lie in its rigorous statistical methodology and comprehensive evaluation metrics. The use of COV and MAT scores provides clear insights into the generative performance of TensorVAE relative to other models. Additionally, the thorough exploration of hyperparameter optimization and robustness through multiple random seeds enhances the reliability of the findings. However, a notable weakness is the lack of explicit statistical tests detailed in the performance comparisons, which could raise questions regarding the robustness of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of methodologies, results, and conclusions. The quality of the writing and figures supports the overall comprehension of the work. In terms of novelty, TensorVAE contributes a fresh approach to molecular conformation generation, leveraging tensor representation and convolutional methods. Reproducibility is supported by the description of the training process and evaluation metrics; however, the absence of explicit statistical tests may hinder full reproducibility of results.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of molecular conformation generation through its innovative TensorVAE model and robust statistical evaluation. While it excels in clarity and methodological rigor, the lack of detailed statistical testing limits the assessment of its empirical claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces TensorVAE, a novel generative model for molecular structures that combines convolutional operations with transformer architecture. It focuses on feature engineering to improve the representation of molecular data, aiming to generate high-quality molecular conformations. The authors present competitive results on two benchmark datasets, although the model does not consistently outperform existing state-of-the-art methods across all metrics.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its focus on feature engineering and its computational efficiency, which may appeal to practitioners seeking simpler implementations. However, it has notable weaknesses, including a limited exploration of model architecture complexities and a narrow evaluation scope, relying solely on two datasets. The model's performance is heavily dependent on the quality of input features, and the paper lacks a thorough discussion on potential alternative features or implications of using a fixed-size input tensor. Additionally, the absence of ablation studies restricts understanding of how various components contribute to overall performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and findings. However, the novelty is somewhat limited due to the reliance on established architectures without a comprehensive exploration of their implications for molecular interactions. The reproducibility of results may be hindered by the lack of detailed plans for future work, such as extending the methodology to protein structure prediction and the absence of extensive ablation studies.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to molecular generative modeling with TensorVAE, demonstrating competitive performance while highlighting areas for further exploration. Nonetheless, its limitations in terms of architecture discussion, dataset diversity, and detailed future work plans suggest that the model's full potential remains to be realized.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a generative model aimed at molecular conformation generation. It utilizes a straightforward approach involving a NaiveUNet architecture and a unique representation of molecular data through an input tensor graph that combines atom and bond features. The authors claim that their method achieves competitive performance compared to existing models, highlighting the importance of novel feature engineering and the use of an extended kernel and attention mechanism for information aggregation.\n\n# Strength And Weaknesses\nThe paper attempts to contribute to the field of molecular conformation generation by proposing a simple yet effective model that leverages common techniques in a novel way. However, the contributions are largely characterized as rehashes of established methodologies, lacking true innovation. The use of a U-Net architecture and the focus on feature engineering are basic concepts that do not elevate the work significantly. The results, while competitive, are benchmarked against outdated methods, which raises concerns about the rigor of the validation process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, though the presentation of concepts could benefit from deeper insights into the methodologies rather than reiterating established knowledge. The quality of writing is generally good, but the novelty is limited, as many of the ideas presented are well-known in the field. Reproducibility is not adequately addressed, as the paper does not provide enough details on implementation or specific datasets used for validation.\n\n# Summary Of The Review\nOverall, the paper fails to present significant advancements in the field of molecular conformation generation. While it introduces TensorVAE with a straightforward methodology, the contributions lack originality and depth, rendering it a rehash of well-known concepts without adequate validation or innovation.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to molecular structure generation using a conditional variational autoencoder (CVAE) framework, emphasizing feature engineering and a tensor representation of molecular graphs. The authors propose a fully-connected and symmetric tensor structure to capture inter-atomic relationships effectively and utilize a 1D convolution mechanism with an extended kernel size for information aggregation. Their results indicate competitive performance against state-of-the-art methods while ensuring roto-translation invariance in generated conformations. Future work directions include exploring the application of their method to protein structure prediction.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its focus on feature engineering, which is critical for effective molecular representation. The innovative tensor representation is a significant contribution that may improve generative performance, though further exploration of alternative tensor structures could yield even better results. The use of a standard transformer architecture raises questions about potential enhancements through recent variants or graph adaptations. While the empirical results are promising, the paper would benefit from additional ablation studies and comparisons with contemporary models, particularly those utilizing self-supervised learning techniques. \n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and methodology. The quality of the research appears solid, with a focus on reproducibility through the use of standard libraries. However, providing additional resources or guidelines for practitioners would enhance accessibility. The novelty of the feature engineering approach and tensor representation is noteworthy, but exploring more advanced techniques could increase the paper's impact.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to molecular structure generation with notable contributions in feature engineering and tensor representation. While the results are competitive, further exploration of alternative methodologies and additional empirical comparisons could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents TensorVAE, a novel approach for molecular conformation generation and molecular property prediction, demonstrating competitive performance on benchmark datasets GEOM-QM9 and GEOM-Drugs. The methodology leverages a simpler architecture that outperforms 18 state-of-the-art methods, achieving a Coverage (COV) score of 98.11% and a Mean Absolute Threshold (MAT) of 0.3447 Å on the QM9 dataset. TensorVAE also exhibits significant efficiency in generation time, decoding 200 QM9 molecules in only 62 seconds, establishing its practicality for applications in computational drug discovery.\n\n# Strength And Weaknesses\nThe strengths of TensorVAE lie in its impressive benchmark performance, particularly against distance-based methods, and its efficiency in decoding time. The ablation studies substantiate the robustness of its performance improvements, indicating that a simpler architecture can indeed yield superior results without the complexity of more sophisticated models. However, the paper could be improved by providing a deeper analysis of the limitations of TensorVAE, such as its potential trade-offs in specific scenarios or types of molecular structures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The results are presented convincingly, with comprehensive comparisons to baseline models. The novelty of TensorVAE is evident in its ability to outperform more complex approaches while maintaining simplicity. Reproducibility is supported through detailed descriptions of the experiments and results, although including code or data availability would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, TensorVAE demonstrates a strong contribution to the field of molecular conformation generation and property prediction, showcasing both competitive performance and efficiency. The paper is clear and well-presented, making a compelling case for the utility of its simpler architecture in practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel generative model, TensorVAE, designed for generating molecular conformations through advanced feature engineering. The authors employ a direct approach that minimizes the need for extensive sampling steps, differentiating it from existing methods that typically rely on indirect techniques or require multiple iterations. The findings demonstrate that TensorVAE outperforms traditional methods in terms of accuracy and efficiency, offering significant improvements in generating diverse molecular structures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to molecular conformation generation, combining direct generation with effective feature engineering, which could potentially streamline computational processes in molecular modeling. However, the paper suffers from several weaknesses, including a lack of clarity in the presentation of its methodology and results. The dense abstract and lengthy paragraphs may hinder reader comprehension, and the excessive use of jargon may alienate readers unfamiliar with the specific terminologies. Moreover, some key findings are reiterated unnecessarily, which could have been consolidated for a more concise narrative.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a significant advancement in generative modeling for molecular conformations, its clarity is undermined by complex sentence structures and inconsistent terminology. The quality of the figures is adequate, yet they lack informative captions, which would enhance understanding. The paper exhibits novelty through its proposed model, though the reproducibility of results may be impacted by the lack of detailed methodological descriptions. Improvements in structuring and simplifying the content would greatly benefit the overall clarity and accessibility of the work. \n\n# Summary Of The Review\nThe paper offers a promising new direction for molecular conformation generation with its TensorVAE model, showcasing notable advancements in efficiency and accuracy. However, the clarity and presentation of the findings need significant improvement to enhance reader comprehension and engagement.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3024812775833894,
    -1.6312740391286575,
    -1.9138489980281208,
    -1.6962717547334605,
    -1.8915607661057596,
    -1.7668651384232192,
    -1.701134227000452,
    -2.005423209707265,
    -1.658943551184703,
    -1.745205537867756,
    -1.6576531243559134,
    -1.3141471167370389,
    -1.6160034491499864,
    -1.8896987608970406,
    -1.8358493719876987,
    -1.7657466584354005,
    -1.8735170287215217,
    -1.8008211783793777,
    -1.852360068376375,
    -1.8188067182427794,
    -2.002996698524193,
    -1.5693336654816183,
    -1.8332218519315417,
    -1.7947651066854504,
    -1.9157353952802398,
    -2.0765478026733377,
    -1.963447317772242,
    -1.8262888575225646,
    -1.8670210522907331
  ],
  "logp_cond": [
    [
      0.0,
      -2.0777658988959993,
      -2.0923266108171137,
      -2.0776876053778626,
      -2.0899083779181638,
      -2.071358175436259,
      -2.0800121397562163,
      -2.0959792259747605,
      -2.082514126735806,
      -2.1250509459609273,
      -2.04878972240318,
      -2.1675394380712123,
      -2.0725394472269354,
      -2.054518857591859,
      -2.0968452558210906,
      -2.0773654260931336,
      -2.0956269251547774,
      -2.0730243029953113,
      -2.087603322913463,
      -2.0444961319805577,
      -2.11708809672878,
      -2.155920403785342,
      -2.1259850166327343,
      -2.129113708390582,
      -2.1121264767180215,
      -2.1063135391089753,
      -2.102820239953126,
      -2.1202926864682,
      -2.1292377883359026
    ],
    [
      -1.2630328655066716,
      0.0,
      -1.2199117542220352,
      -1.175874203143745,
      -1.2451681405709212,
      -1.2279418677782445,
      -1.2462731018849411,
      -1.2588120463847967,
      -1.2185547139856305,
      -1.2945352248696034,
      -1.1236067937594059,
      -1.3994853066849635,
      -1.197805875166073,
      -1.1808514356535649,
      -1.1971143070351453,
      -1.113601000235932,
      -1.2692836142095811,
      -1.256369595408685,
      -1.2021750029765748,
      -1.2549002355839314,
      -1.3072198218276416,
      -1.34752805400359,
      -1.3379554197128711,
      -1.3184197695724127,
      -1.2696160561470342,
      -1.2084390890744967,
      -1.263273055473205,
      -1.3153871102358705,
      -1.2863554392163052
    ],
    [
      -1.5060486134401445,
      -1.425687735552043,
      0.0,
      -1.3992417172197535,
      -1.448552999355616,
      -1.4423039501175683,
      -1.4713759417271661,
      -1.4500938957838139,
      -1.3950066114010777,
      -1.4499056107677306,
      -1.4294041529998478,
      -1.6471080478639086,
      -1.4277243740683052,
      -1.4030562891451863,
      -1.375669888698393,
      -1.4082010368255293,
      -1.4335727192097742,
      -1.5053985978266466,
      -1.416724869707448,
      -1.4450464145022068,
      -1.5107189199866626,
      -1.6085509299634777,
      -1.552891517218623,
      -1.506071373418255,
      -1.479938550556705,
      -1.4763534558100595,
      -1.524891795153975,
      -1.5275780486835722,
      -1.5169019333068663
    ],
    [
      -1.2401928934948474,
      -1.0846716952535191,
      -1.133115182247225,
      0.0,
      -1.2079350185818383,
      -1.1580626563613365,
      -1.1864678102294592,
      -1.1972414222210706,
      -1.110994312102746,
      -1.2568886591141264,
      -1.0392841782870623,
      -1.4271410505972089,
      -1.1841034491837001,
      -1.019139717647488,
      -1.0311040805053748,
      -1.165496882751064,
      -1.1873614347776136,
      -1.228799937262907,
      -1.1699139509751648,
      -1.2142571600202763,
      -1.2603282804774398,
      -1.3448105941329374,
      -1.2886775917242947,
      -1.2923792893448216,
      -1.2616789284385337,
      -1.1412821504394752,
      -1.1707167817492494,
      -1.2451076287865304,
      -1.3076256777164617
    ],
    [
      -1.4912723329369315,
      -1.4340333750886916,
      -1.438861228176461,
      -1.4760802997784936,
      0.0,
      -1.4422339104531892,
      -1.4177539227828542,
      -1.4804613077430613,
      -1.4407025046802657,
      -1.417350786888474,
      -1.4864097944895363,
      -1.5964146257293037,
      -1.4693357395062505,
      -1.4508545937819357,
      -1.4516625186265455,
      -1.4553814033916699,
      -1.4175998261528813,
      -1.495839131308472,
      -1.518641800672362,
      -1.4476613343767561,
      -1.4417025651542001,
      -1.5842261529440957,
      -1.484391039277645,
      -1.5343638125436565,
      -1.4552781952183662,
      -1.4224962464441286,
      -1.4777440291846686,
      -1.511774649593078,
      -1.4663665731324977
    ],
    [
      -1.3255178867255848,
      -1.265487536313125,
      -1.2808865563380354,
      -1.2894688780182626,
      -1.284387127672645,
      0.0,
      -1.2636795497552464,
      -1.3280309077416814,
      -1.331266526104772,
      -1.3165118416613275,
      -1.217616218835123,
      -1.4853791497793558,
      -1.2197824045587329,
      -1.2424270861711275,
      -1.314259660950075,
      -1.2589088496805858,
      -1.3308556264989673,
      -1.340295648199866,
      -1.2847995585955003,
      -1.3001585330491143,
      -1.3266615144688332,
      -1.4934237785399682,
      -1.4457915745277208,
      -1.382556734101716,
      -1.4012102613809292,
      -1.3484876519776123,
      -1.3495497425365468,
      -1.3593004353765719,
      -1.386650776894511
    ],
    [
      -1.4081940258266186,
      -1.380014097985139,
      -1.3826612671616918,
      -1.4075032522039597,
      -1.3899369953344283,
      -1.347176719848568,
      0.0,
      -1.4049596603960368,
      -1.4155719545704406,
      -1.393940787352311,
      -1.3885713955241124,
      -1.4474969396920774,
      -1.3885800609858203,
      -1.394425364334141,
      -1.3918146233094433,
      -1.4014945067771243,
      -1.4034074285721585,
      -1.409743452648581,
      -1.418252672898001,
      -1.393724753974491,
      -1.395923847944685,
      -1.4859206265528233,
      -1.4344350657987879,
      -1.419366299367459,
      -1.440850650623469,
      -1.4074477629988846,
      -1.4249547470785537,
      -1.4506906291093145,
      -1.395463074705171
    ],
    [
      -1.6025943944583492,
      -1.5128336165642073,
      -1.517269819132058,
      -1.4928254218060069,
      -1.545322104032259,
      -1.552371644441726,
      -1.5646469442405142,
      0.0,
      -1.5048748624812098,
      -1.5968873652179911,
      -1.4865339924301382,
      -1.7440469980579978,
      -1.5293651951708278,
      -1.536418338376868,
      -1.5273601977717057,
      -1.5593449017660963,
      -1.5219593412309185,
      -1.5329957016705775,
      -1.575165409815361,
      -1.5212739253274203,
      -1.5165370107448697,
      -1.7023294416123587,
      -1.6383256967173925,
      -1.6151048356289084,
      -1.568331220951892,
      -1.5828322288677865,
      -1.5744689794432765,
      -1.5430354242410393,
      -1.6367391876506905
    ],
    [
      -1.2913734845874008,
      -1.1821143603736481,
      -1.220192985520486,
      -1.139814076839628,
      -1.2282694948399575,
      -1.237048501119655,
      -1.2568029420049893,
      -1.2299659207569695,
      0.0,
      -1.257087412752361,
      -1.1337211327779517,
      -1.4255521960595539,
      -1.2272213438022639,
      -1.152829526569936,
      -1.065831015820502,
      -1.2039213302461895,
      -1.2521529137243304,
      -1.2025553930676196,
      -1.2637811548860463,
      -1.2653615865224603,
      -1.2930880400100386,
      -1.364050861774182,
      -1.3373545429762164,
      -1.3145585241588653,
      -1.2818810255673199,
      -1.2764147988095358,
      -1.2239862858324342,
      -1.2236275259691203,
      -1.3217068935818905
    ],
    [
      -1.3641457419715013,
      -1.331561782407483,
      -1.2885752299816169,
      -1.3375882859605117,
      -1.2482366746328963,
      -1.2915731337832266,
      -1.3168968665150214,
      -1.3501431545609164,
      -1.3015929812462737,
      0.0,
      -1.329896757357765,
      -1.4455011225081567,
      -1.3272358759997678,
      -1.307507765306749,
      -1.282633017336488,
      -1.2952331574221243,
      -1.3091476962079809,
      -1.382887410120551,
      -1.3890956594724184,
      -1.3132559661092602,
      -1.2861798443993422,
      -1.4687668177937618,
      -1.367705544506874,
      -1.33882111078055,
      -1.3544583586633194,
      -1.3388245007430815,
      -1.343006888484573,
      -1.341269810776527,
      -1.3109084956981247
    ],
    [
      -1.2113295908027943,
      -1.1559955871416585,
      -1.2246015911018255,
      -1.1525463663697244,
      -1.2733153711073975,
      -1.1920695827031424,
      -1.2837026232890163,
      -1.190349125465059,
      -1.2110658645163817,
      -1.286015275551854,
      0.0,
      -1.415836593084896,
      -1.0732392879472294,
      -1.0929056837077906,
      -1.13947674607577,
      -1.1569391696712918,
      -1.2258045570314355,
      -1.2089030736633775,
      -1.2291471956339335,
      -1.1914652311712477,
      -1.2955839645794893,
      -1.3563565944099376,
      -1.2826521768949968,
      -1.3128276891559776,
      -1.2290023406308554,
      -1.2148622759597256,
      -1.2416849357844892,
      -1.2888471301443252,
      -1.3313448339659881
    ],
    [
      -1.1155667178910365,
      -1.0926662954350557,
      -1.0976193470347893,
      -1.0818917472589402,
      -1.0856865806265785,
      -1.0815813395207547,
      -1.0113994482247328,
      -1.090726683863206,
      -1.1024020438517497,
      -1.0550233145935697,
      -1.0849650820379138,
      0.0,
      -1.098424677442374,
      -1.0849704597271612,
      -1.0886487862556047,
      -1.1001212815753147,
      -1.0933336332963315,
      -1.0872460991502921,
      -1.1056659205699118,
      -1.0994116144503345,
      -1.0519992322847471,
      -1.0940920625926513,
      -1.0600791076217102,
      -1.0666341098566914,
      -1.0920910534636086,
      -1.0735887543076983,
      -1.0931012932443105,
      -1.099249821612447,
      -1.0470135396720213
    ],
    [
      -1.1917178416083636,
      -1.113310759814713,
      -1.1854575623008392,
      -1.1925604882131562,
      -1.173753092431976,
      -1.0909780953884112,
      -1.1877889229187457,
      -1.2112883262334313,
      -1.1978147771584573,
      -1.2069433690904185,
      -1.0200673309643604,
      -1.3288896686201894,
      0.0,
      -1.058651577611935,
      -1.1904936297097213,
      -1.0975646219333322,
      -1.1394751538893593,
      -1.131219710517269,
      -1.2046158898162997,
      -1.158825564567355,
      -1.232324973912816,
      -1.2867393925273947,
      -1.2358466311314138,
      -1.2612305807903383,
      -1.1936965431185456,
      -1.1862720186482878,
      -1.1599175771019077,
      -1.2360338292929984,
      -1.2510975426558026
    ],
    [
      -1.4429970437356858,
      -1.390348095735886,
      -1.4242820824822153,
      -1.4124107570927198,
      -1.4023931991431573,
      -1.3928314977287843,
      -1.4581191126089939,
      -1.4410392127292821,
      -1.4384351793482764,
      -1.4293952620838588,
      -1.291520599247208,
      -1.6010236096524804,
      -1.3503808010423437,
      0.0,
      -1.3853431578273878,
      -1.3216681948698015,
      -1.4190920286120816,
      -1.4241249229591229,
      -1.4251199102329892,
      -1.3879671163776826,
      -1.4638670880927072,
      -1.5525447004509638,
      -1.4922418046018926,
      -1.5089432103762503,
      -1.4676674078993597,
      -1.4253744605060792,
      -1.4247535897349042,
      -1.4801305322401024,
      -1.491956705183941
    ],
    [
      -1.4464441949114522,
      -1.341714320374415,
      -1.3662041455921468,
      -1.3428766736063031,
      -1.3422992788306347,
      -1.4185287454785824,
      -1.3871866922773566,
      -1.3747159307245733,
      -1.285163621789369,
      -1.415776968262586,
      -1.2924800405698484,
      -1.5853106263810164,
      -1.3840348547943415,
      -1.2907036930966993,
      0.0,
      -1.3641796870281306,
      -1.3906545643574557,
      -1.423945327593313,
      -1.443042702453216,
      -1.360353464234831,
      -1.443050448878646,
      -1.5083540483120397,
      -1.46011437625395,
      -1.4248264509929782,
      -1.4486137670447619,
      -1.395648472522704,
      -1.3817598006207172,
      -1.4066963523489033,
      -1.4590796448647734
    ],
    [
      -1.369668710278829,
      -1.157534514427214,
      -1.2578996818161605,
      -1.2781501433509939,
      -1.3687035684221904,
      -1.292599250038643,
      -1.3527927991950421,
      -1.3584113875553327,
      -1.233785012894739,
      -1.3470159637556096,
      -1.2272973966265424,
      -1.5288962848075953,
      -1.2150851657629331,
      -1.2262171630039136,
      -1.2800103068881628,
      0.0,
      -1.299560108629836,
      -1.3087447940839938,
      -1.2339658235481972,
      -1.3091209155141867,
      -1.370963783688932,
      -1.4609701283038716,
      -1.4092414215225642,
      -1.3954958282374956,
      -1.3596350727999134,
      -1.2825626996743233,
      -1.3091920637713883,
      -1.3713092215811356,
      -1.4126572996071949
    ],
    [
      -1.4852685820935783,
      -1.4228306439292022,
      -1.3848396893524413,
      -1.4264203338018333,
      -1.4023496374699786,
      -1.4342982675418503,
      -1.4530359869376337,
      -1.3886004944134607,
      -1.3809268020041559,
      -1.4367717415459136,
      -1.3921219357970152,
      -1.6079287393449535,
      -1.3710740301641637,
      -1.37380931151656,
      -1.387710574747576,
      -1.3667943767759092,
      0.0,
      -1.4317800475285638,
      -1.4517407235605857,
      -1.4338282869496735,
      -1.425613912446519,
      -1.551289205649472,
      -1.5051120147192105,
      -1.496353952078336,
      -1.407795507245096,
      -1.455296835598948,
      -1.4029046024415355,
      -1.4955131385339442,
      -1.5048266889357478
    ],
    [
      -1.443813728520497,
      -1.4164739003556241,
      -1.3914969432148439,
      -1.4060957020084286,
      -1.4132004564695788,
      -1.4250888210261612,
      -1.3887862307839767,
      -1.3561164150464577,
      -1.3750879068058348,
      -1.4290951329517123,
      -1.3316989788391058,
      -1.5564132239857855,
      -1.3411558619612882,
      -1.3484227705566998,
      -1.3828356363478986,
      -1.341096743745433,
      -1.3920278205233443,
      0.0,
      -1.3916501684682665,
      -1.4244137766769225,
      -1.4126173969308953,
      -1.4956064016076787,
      -1.4520773570823824,
      -1.4649169883198054,
      -1.4324174914266148,
      -1.4046775285561353,
      -1.4509621711463665,
      -1.420006695465767,
      -1.451677962366569
    ],
    [
      -1.4630260090743659,
      -1.3310389234319735,
      -1.3684935791211688,
      -1.3873299150645977,
      -1.4550289038174213,
      -1.375238381919163,
      -1.4218288125866891,
      -1.426886696749546,
      -1.4277120754974024,
      -1.4352002567764033,
      -1.3777415034162217,
      -1.589928387159835,
      -1.4113646214300137,
      -1.3659144867230348,
      -1.4345262845247084,
      -1.3291803287493877,
      -1.421978808763473,
      -1.4550839421525257,
      0.0,
      -1.3604310959298616,
      -1.5017069903349234,
      -1.5081035346465952,
      -1.5014557951386975,
      -1.454749315672322,
      -1.4776835505657266,
      -1.4257348837109238,
      -1.373547273998818,
      -1.4969452833603063,
      -1.501255028695193
    ],
    [
      -1.4046939932731741,
      -1.4191822313372577,
      -1.4129569494350356,
      -1.4438556939366256,
      -1.4394810747531106,
      -1.4412228211958744,
      -1.4240434823683887,
      -1.4237129576362402,
      -1.447697483591429,
      -1.4338537870660573,
      -1.3957808843493231,
      -1.5966009964597634,
      -1.4370041933843072,
      -1.4394675755001507,
      -1.3587644183235896,
      -1.4133785896643933,
      -1.4113349220155327,
      -1.4659710405374375,
      -1.3931816035253006,
      0.0,
      -1.487550748519927,
      -1.543042021179838,
      -1.5041973970260416,
      -1.4546849406293705,
      -1.4560957824823828,
      -1.438181543226417,
      -1.3974606491597532,
      -1.487211015723455,
      -1.5211822156831214
    ],
    [
      -1.6302338652702568,
      -1.543844556937006,
      -1.5598680662903301,
      -1.5613651077459312,
      -1.55279069179608,
      -1.5259606465807547,
      -1.5214157208535937,
      -1.4915034681211903,
      -1.5441598327988761,
      -1.527638390492445,
      -1.548158784728511,
      -1.684833863704902,
      -1.5648532529762922,
      -1.5833861850468323,
      -1.5497700068882112,
      -1.548398573805353,
      -1.56889973463581,
      -1.572018297953434,
      -1.58973673588833,
      -1.582921533004552,
      0.0,
      -1.6858898379678058,
      -1.6552667174159523,
      -1.5631296323549044,
      -1.5854995886406409,
      -1.5847680422177404,
      -1.6057548800217365,
      -1.570163351407849,
      -1.5574788636740173
    ],
    [
      -1.2859078617976574,
      -1.2097432238892245,
      -1.222039337201869,
      -1.229915164272876,
      -1.2229405614537916,
      -1.2165675241807072,
      -1.2234290705601993,
      -1.2310428844610792,
      -1.2327555674066153,
      -1.2498800190474753,
      -1.2235955021880778,
      -1.218529993415549,
      -1.242381657840429,
      -1.1929979674934268,
      -1.2013276826106738,
      -1.220600004138806,
      -1.2548767457059098,
      -1.2329429426506817,
      -1.2311029171533667,
      -1.231429817747529,
      -1.2616876876833352,
      0.0,
      -1.2342426890026923,
      -1.245533380364501,
      -1.2399806525988855,
      -1.2346472231398982,
      -1.189724378227934,
      -1.2380164039537607,
      -1.22460817483372
    ],
    [
      -1.56519558051458,
      -1.5648594961325097,
      -1.5506602917985763,
      -1.5336544508691075,
      -1.531457269659499,
      -1.5535642604803959,
      -1.5661895474772995,
      -1.5283281012531775,
      -1.540421587538853,
      -1.5262065937459899,
      -1.5439730782431254,
      -1.5533108671063471,
      -1.5337404336693454,
      -1.5234104148396843,
      -1.520089733179887,
      -1.557361952344203,
      -1.4898126877817905,
      -1.5489936034169753,
      -1.554817405245464,
      -1.5416164975811342,
      -1.4965306380319694,
      -1.573747416964628,
      0.0,
      -1.5978761297180195,
      -1.4514761471081916,
      -1.5046137993171886,
      -1.5062922940951706,
      -1.5474023048429126,
      -1.5651126349216977
    ],
    [
      -1.5157793348820736,
      -1.484997410621905,
      -1.4193218249656865,
      -1.4808135471945374,
      -1.455914522381044,
      -1.4456784800447875,
      -1.4364335804761175,
      -1.462458789954711,
      -1.4179803382967118,
      -1.3917843982086047,
      -1.466908821633282,
      -1.5563945541794184,
      -1.4969757067662373,
      -1.4563776570876965,
      -1.414889110530789,
      -1.4435797624821867,
      -1.4725329078703857,
      -1.4977670482361718,
      -1.4336392733894912,
      -1.40932960327125,
      -1.4162635533552916,
      -1.5421576601434397,
      -1.5049152307795057,
      0.0,
      -1.4651676815197017,
      -1.4815278813774206,
      -1.4861297135894274,
      -1.418656600794396,
      -1.4380216340724965
    ],
    [
      -1.512301375466382,
      -1.488427334733895,
      -1.4575799083170857,
      -1.5064875188907192,
      -1.454444442281263,
      -1.4898113893998306,
      -1.4903555725349138,
      -1.49387104333799,
      -1.4505681859660573,
      -1.4587486832922991,
      -1.445699184813125,
      -1.6007514849629747,
      -1.44955005892555,
      -1.4693557442829932,
      -1.4496315837182918,
      -1.453929745962492,
      -1.4332626220095546,
      -1.5257411969290164,
      -1.4860768868437102,
      -1.4331849383743196,
      -1.490129708144429,
      -1.5865960018367826,
      -1.4284945165044582,
      -1.5190899157836322,
      0.0,
      -1.4618042859573797,
      -1.4244039042979746,
      -1.5338918826910775,
      -1.5181737385627831
    ],
    [
      -1.6453262502558004,
      -1.553469216563251,
      -1.6418783513484314,
      -1.5930651169932069,
      -1.5973734297702504,
      -1.666850541288015,
      -1.6366141356208195,
      -1.6116015046831902,
      -1.6337111310797663,
      -1.6705475257517421,
      -1.5779882166809625,
      -1.7759317847181155,
      -1.6266762733531777,
      -1.5740424742762704,
      -1.5530848760674572,
      -1.5574213189514834,
      -1.585435651002212,
      -1.6509400499043476,
      -1.5768080330181304,
      -1.5912052856944725,
      -1.6440614084527878,
      -1.7142997211528694,
      -1.6747468437778592,
      -1.6885988781540142,
      -1.6295627002794932,
      0.0,
      -1.5977638979015971,
      -1.67739963060193,
      -1.741285753819078
    ],
    [
      -1.5631332671715126,
      -1.477545532730137,
      -1.5784269228071242,
      -1.5212213090475648,
      -1.5376297009820583,
      -1.563143282915228,
      -1.5122259141439252,
      -1.5422963441225728,
      -1.5666718929963572,
      -1.5458259797446863,
      -1.4816540319075924,
      -1.6703355862025484,
      -1.5500493082215763,
      -1.4853320631273055,
      -1.4862038017311219,
      -1.5098632668725032,
      -1.5181478752168238,
      -1.5810355912959437,
      -1.491516300422386,
      -1.4532295372933441,
      -1.5972816758258737,
      -1.6494320366879236,
      -1.5904541969723844,
      -1.6409884193702902,
      -1.5287681891634057,
      -1.5055962788158945,
      0.0,
      -1.646134885178028,
      -1.6309619809854394
    ],
    [
      -1.4722938469327493,
      -1.4409452307874688,
      -1.4375094979816014,
      -1.4225853747141184,
      -1.4761421508350194,
      -1.4575878845574615,
      -1.4682188786660035,
      -1.380738829286142,
      -1.3901732550713521,
      -1.4510541972522275,
      -1.482309051754655,
      -1.5670980549729492,
      -1.4165895306644678,
      -1.4562535956588538,
      -1.4084486005639025,
      -1.426748846598107,
      -1.4211747325036737,
      -1.4189268838947307,
      -1.4230087950624755,
      -1.3921335629119913,
      -1.418211709188925,
      -1.4856042300452181,
      -1.47515925409967,
      -1.4292491864241492,
      -1.4666803667386796,
      -1.4412528695487696,
      -1.4627018877094171,
      0.0,
      -1.4785016468761842
    ],
    [
      -1.54467835890869,
      -1.471356780256454,
      -1.4757253721398849,
      -1.5078030033365444,
      -1.4293887747852168,
      -1.4743943156647503,
      -1.4162215131040463,
      -1.457622647203504,
      -1.46283431848405,
      -1.3860746642258974,
      -1.488400888158421,
      -1.533183965624509,
      -1.4562924579678878,
      -1.467370239963705,
      -1.4995287244834947,
      -1.47830706514461,
      -1.463684008707995,
      -1.495323874836003,
      -1.4986416097274133,
      -1.4931831959753892,
      -1.400502877329197,
      -1.5398669286123143,
      -1.4910564765589853,
      -1.4470286717344225,
      -1.4744207332389274,
      -1.4791565412148153,
      -1.4808764640416539,
      -1.505498452879276,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2247153786873901,
      0.2101546667662757,
      0.22479367220552682,
      0.21257289966522563,
      0.2311231021471305,
      0.22246913782717304,
      0.2065020516086289,
      0.21996715084758334,
      0.17743033162246213,
      0.25369155518020925,
      0.1349418395121771,
      0.229941830356454,
      0.24796241999153024,
      0.20563602176229878,
      0.2251158514902558,
      0.20685435242861194,
      0.22945697458807812,
      0.2148779546699262,
      0.2579851456028317,
      0.18539318085460943,
      0.14656087379804728,
      0.17649626095065507,
      0.17336756919280738,
      0.19035480086536793,
      0.19616773847441404,
      0.19966103763026322,
      0.18218859111518926,
      0.17324348924748678
    ],
    [
      0.36824117362198594,
      0.0,
      0.4113622849066223,
      0.4553998359849125,
      0.38610589855773636,
      0.403332171350413,
      0.3850009372437164,
      0.3724619927438608,
      0.412719325143027,
      0.33673881425905416,
      0.5076672453692517,
      0.23178873244369402,
      0.4334681639625846,
      0.45042260347509266,
      0.4341597320935122,
      0.5176730388927255,
      0.3619904249190764,
      0.37490444371997245,
      0.4290990361520828,
      0.37637380354472616,
      0.3240542173010159,
      0.28374598512506743,
      0.2933186194157864,
      0.31285426955624485,
      0.3616579829816233,
      0.42283495005416083,
      0.36800098365545253,
      0.315886928892787,
      0.3449185999123523
    ],
    [
      0.4078003845879763,
      0.48816126247607783,
      0.0,
      0.5146072808083673,
      0.4652959986725047,
      0.4715450479105525,
      0.4424730563009547,
      0.46375510224430694,
      0.5188423866270431,
      0.4639433872603902,
      0.484444845028273,
      0.2667409501642122,
      0.4861246239598156,
      0.5107927088829345,
      0.5381791093297279,
      0.5056479612025915,
      0.48027627881834656,
      0.40845040020147416,
      0.4971241283206729,
      0.468802583525914,
      0.4031300780414582,
      0.3052980680646431,
      0.36095748080949774,
      0.4077776246098659,
      0.4339104474714157,
      0.4374955422180613,
      0.3889572028741457,
      0.38627094934454864,
      0.3969470647212545
    ],
    [
      0.4560788612386131,
      0.6116000594799413,
      0.5631565724862355,
      0.0,
      0.4883367361516222,
      0.5382090983721239,
      0.5098039445040012,
      0.49903033251238993,
      0.5852774426307146,
      0.4393830956193341,
      0.6569875764463982,
      0.2691307041362516,
      0.5121683055497603,
      0.6771320370859726,
      0.6651676742280856,
      0.5307748719823966,
      0.5089103199558469,
      0.46747181747055344,
      0.5263578037582957,
      0.4820145947131842,
      0.43594347425602065,
      0.3514611606005231,
      0.40759416300916573,
      0.4038924653886389,
      0.4345928262949268,
      0.5549896042939853,
      0.525554972984211,
      0.4511641259469301,
      0.3886460770169988
    ],
    [
      0.4002884331688281,
      0.4575273910170681,
      0.4526995379292986,
      0.415480466327266,
      0.0,
      0.44932685565257047,
      0.47380684332290546,
      0.41109945836269834,
      0.4508582614254939,
      0.4742099792172856,
      0.4051509716162234,
      0.2951461403764559,
      0.4222250265995091,
      0.4407061723238239,
      0.43989824747921413,
      0.43617936271408975,
      0.47396093995287836,
      0.3957216347972876,
      0.37291896543339753,
      0.4438994317290035,
      0.4498582009515595,
      0.3073346131616639,
      0.4071697268281147,
      0.3571969535621031,
      0.4362825708873934,
      0.469064519661631,
      0.413816736921091,
      0.3797861165126817,
      0.4251941929732619
    ],
    [
      0.44134725169763445,
      0.5013776021100942,
      0.48597858208518385,
      0.47739626040495664,
      0.48247801075057417,
      0.0,
      0.5031855886679728,
      0.4388342306815378,
      0.4355986123184472,
      0.45035329676189173,
      0.5492489195880963,
      0.2814859886438634,
      0.5470827338644864,
      0.5244380522520917,
      0.4526054774731443,
      0.5079562887426334,
      0.43600951192425197,
      0.4265694902233532,
      0.4820655798277189,
      0.4667066053741049,
      0.44020362395438606,
      0.273441359883251,
      0.32107356389549846,
      0.38430840432150326,
      0.36565487704229005,
      0.4183774864456069,
      0.41731539588667244,
      0.4075647030466474,
      0.38021436152870813
    ],
    [
      0.29294020117383335,
      0.32112012901531295,
      0.3184729598387601,
      0.29363097479649225,
      0.31119723166602364,
      0.3539575071518839,
      0.0,
      0.2961745666044151,
      0.2855622724300113,
      0.30719343964814083,
      0.31256283147633956,
      0.25363728730837454,
      0.3125541660146316,
      0.3067088626663108,
      0.30931960369100864,
      0.29963972022332763,
      0.2977267984282934,
      0.2913907743518709,
      0.28288155410245097,
      0.3074094730259609,
      0.305210379055767,
      0.2152136004476286,
      0.26669916120166404,
      0.28176792763299296,
      0.26028357637698285,
      0.2936864640015673,
      0.2761794799218982,
      0.25044359789113746,
      0.30567115229528086
    ],
    [
      0.4028288152489157,
      0.4925895931430575,
      0.4881533905752069,
      0.512597787901258,
      0.46010110567500595,
      0.4530515652655389,
      0.44077626546675064,
      0.0,
      0.500548347226055,
      0.4085358444892737,
      0.5188892172771267,
      0.26137621164926705,
      0.47605801453643704,
      0.4690048713303969,
      0.4780630119355591,
      0.44607830794116854,
      0.48346386847634637,
      0.4724275080366873,
      0.43025779989190394,
      0.4841492843798445,
      0.48888619896239516,
      0.3030937680949062,
      0.36709751298987237,
      0.39031837407835646,
      0.4370919887553728,
      0.4225909808394783,
      0.4309542302639884,
      0.4623877854662255,
      0.36868402205657436
    ],
    [
      0.3675700665973023,
      0.4768291908110549,
      0.438750565664217,
      0.519129474345075,
      0.43067405634474554,
      0.421895050065048,
      0.4021406091797137,
      0.4289776304277335,
      0.0,
      0.401856138432342,
      0.5252224184067513,
      0.2333913551251492,
      0.43172220738243916,
      0.5061140246147671,
      0.593112535364201,
      0.4550222209385135,
      0.4067906374603727,
      0.45638815811708344,
      0.3951623962986568,
      0.3935819646622427,
      0.3658555111746644,
      0.29489268941052105,
      0.3215890082084867,
      0.34438502702583773,
      0.3770625256173832,
      0.38252875237516726,
      0.43495726535226886,
      0.43531602521558277,
      0.3372366576028125
    ],
    [
      0.3810597958962547,
      0.413643755460273,
      0.4566303078861391,
      0.4076172519072443,
      0.49696886323485967,
      0.45363240408452943,
      0.42830867135273465,
      0.3950623833068396,
      0.44361255662148236,
      0.0,
      0.4153087805099911,
      0.29970441535959935,
      0.41796966186798823,
      0.43769777256100695,
      0.46257252053126807,
      0.4499723804456317,
      0.4360578416597751,
      0.36231812774720495,
      0.3561098783953376,
      0.4319495717584958,
      0.4590256934684138,
      0.27643872007399417,
      0.377499993360882,
      0.4063844270872059,
      0.3907471792044366,
      0.40638103712467455,
      0.40219864938318306,
      0.40393572709122894,
      0.4342970421696313
    ],
    [
      0.44632353355311905,
      0.5016575372142549,
      0.43305153325408785,
      0.505106757986189,
      0.38433775324851593,
      0.46558354165277094,
      0.3739505010668971,
      0.4673039988908543,
      0.44658725983953174,
      0.3716378488040595,
      0.0,
      0.24181653127101743,
      0.5844138364086839,
      0.5647474406481228,
      0.5181763782801434,
      0.5007139546846215,
      0.43184856732447785,
      0.44875005069253593,
      0.4285059287219799,
      0.4661878931846657,
      0.36206915977642407,
      0.3012965299459758,
      0.3750009474609166,
      0.3448254351999358,
      0.42865078372505794,
      0.44279084839618776,
      0.41596818857142415,
      0.3688059942115882,
      0.32630829038992526
    ],
    [
      0.19858039884600243,
      0.2214808213019832,
      0.21652776970224963,
      0.23225536947809866,
      0.22846053611046035,
      0.23256577721628413,
      0.30274766851230606,
      0.22342043287383295,
      0.2117450728852892,
      0.2591238021434692,
      0.22918203469912513,
      0.0,
      0.21572243929466484,
      0.22917665700987766,
      0.2254983304814342,
      0.21402583516172413,
      0.2208134834407074,
      0.22690101758674674,
      0.20848119616712713,
      0.21473550228670435,
      0.26214788445229176,
      0.22005505414438753,
      0.2540680091153287,
      0.2475130068803475,
      0.2220560632734303,
      0.2405583624293406,
      0.2210458234927284,
      0.21489729512459177,
      0.2671335770650176
    ],
    [
      0.4242856075416228,
      0.5026926893352734,
      0.4305458868491472,
      0.4234429609368302,
      0.4422503567180105,
      0.5250253537615752,
      0.4282145262312407,
      0.4047151229165551,
      0.4181886719915291,
      0.40906008005956784,
      0.595936118185626,
      0.28711378052979697,
      0.0,
      0.5573518715380514,
      0.4255098194402651,
      0.5184388272166542,
      0.4765282952606271,
      0.4847837386327174,
      0.41138755933368665,
      0.4571778845826313,
      0.38367847523717047,
      0.3292640566225917,
      0.3801568180185726,
      0.35477286835964805,
      0.42230690603144083,
      0.4297314305016986,
      0.4560858720480787,
      0.37996961985698796,
      0.36490590649418375
    ],
    [
      0.44670171716135476,
      0.49935066516115456,
      0.46541667841482526,
      0.4772880038043208,
      0.4873055617538833,
      0.49686726316825625,
      0.43157964828804674,
      0.44865954816775844,
      0.4512635815487642,
      0.4603034988131818,
      0.5981781616498325,
      0.2886751512445602,
      0.5393179598546969,
      0.0,
      0.5043556030696528,
      0.5680305660272391,
      0.470606732284959,
      0.46557383793791773,
      0.46457885066405136,
      0.501731644519358,
      0.4258316728043334,
      0.33715406044607676,
      0.397456956295148,
      0.38075555052079024,
      0.4220313529976809,
      0.4643243003909614,
      0.4649451711621364,
      0.4095682286569382,
      0.3977420557130995
    ],
    [
      0.38940517707624656,
      0.49413505161328364,
      0.46964522639555195,
      0.49297269838139557,
      0.493550093157064,
      0.4173206265091163,
      0.4486626797103421,
      0.4611334412631254,
      0.5506857501983298,
      0.4200724037251127,
      0.5433693314178503,
      0.2505387456066823,
      0.4518145171933572,
      0.5451456788909994,
      0.0,
      0.47166968495956807,
      0.445194807630243,
      0.41190404439438577,
      0.39280666953448273,
      0.47549590775286776,
      0.39279892310905273,
      0.32749532367565903,
      0.3757349957337488,
      0.4110229209947205,
      0.38723560494293685,
      0.44020089946499463,
      0.45408957136698147,
      0.4291530196387954,
      0.37676972712292534
    ],
    [
      0.3960779481565715,
      0.6082121440081865,
      0.5078469766192399,
      0.4875965150844066,
      0.39704309001321003,
      0.4731474083967575,
      0.4129538592403583,
      0.4073352708800677,
      0.5319616455406615,
      0.41873069467979085,
      0.5384492618088581,
      0.23685037362780514,
      0.5506614926724673,
      0.5395294954314869,
      0.4857363515472377,
      0.0,
      0.4661865498055644,
      0.45700186435140666,
      0.5317808348872033,
      0.4566257429212137,
      0.39478287474646856,
      0.30477653013152883,
      0.3565052369128363,
      0.37025083019790483,
      0.4061115856354871,
      0.4831839587610771,
      0.45655459466401216,
      0.3944374368542649,
      0.3530893588282056
    ],
    [
      0.38824844662794344,
      0.4506863847923195,
      0.4886773393690804,
      0.4470966949196884,
      0.47116739125154306,
      0.4392187611796714,
      0.420481041783888,
      0.48491653430806103,
      0.49259022671736585,
      0.4367452871756081,
      0.48139509292450655,
      0.2655882893765682,
      0.502442998557358,
      0.49970771720496177,
      0.48580645397394573,
      0.5067226519456125,
      0.0,
      0.4417369811929579,
      0.421776305160936,
      0.43968874177184825,
      0.4479031162750027,
      0.32222782307204967,
      0.3684050140023112,
      0.3771630766431857,
      0.46572152147642565,
      0.41822019312257375,
      0.47061242627998623,
      0.3780038901875775,
      0.3686903397857739
    ],
    [
      0.35700744985888067,
      0.38434727802375357,
      0.40932423516453387,
      0.3947254763709491,
      0.3876207219097989,
      0.37573235735321653,
      0.41203494759540105,
      0.44470476333292,
      0.4257332715735429,
      0.3717260454276654,
      0.4691221995402719,
      0.2444079543935922,
      0.4596653164180895,
      0.45239840782267793,
      0.4179855420314791,
      0.45972443463394463,
      0.40879335785603343,
      0.0,
      0.4091710099111112,
      0.37640740170245524,
      0.38820378144848244,
      0.30521477677169906,
      0.3487438212969953,
      0.3359041900595723,
      0.36840368695276293,
      0.39614364982324246,
      0.3498590072330112,
      0.38081448291361064,
      0.34914321601280873
    ],
    [
      0.38933405930200915,
      0.5213211449444015,
      0.48386648925520626,
      0.46503015331177733,
      0.3973311645589537,
      0.4771216864572121,
      0.4305312557896859,
      0.4254733716268291,
      0.4246479928789726,
      0.41715981159997173,
      0.4746185649601533,
      0.2624316812165399,
      0.4409954469463613,
      0.4864455816533402,
      0.4178337838516666,
      0.5231797396269873,
      0.43038125961290197,
      0.39727612622384934,
      0.0,
      0.49192897244651346,
      0.3506530780414516,
      0.3442565337297798,
      0.3509042732376775,
      0.3976107527040531,
      0.37467651781064837,
      0.4266251846654512,
      0.4788127943775571,
      0.3554147850160687,
      0.3511050396811821
    ],
    [
      0.4141127249696053,
      0.39962448690552166,
      0.4058497688077438,
      0.3749510243061538,
      0.3793256434896688,
      0.377583897046905,
      0.39476323587439066,
      0.3950937606065392,
      0.3711092346513505,
      0.38495293117672214,
      0.42302583389345627,
      0.222205721783016,
      0.3818025248584722,
      0.3793391427426287,
      0.46004229991918977,
      0.40542812857838606,
      0.4074717962272467,
      0.35283567770534185,
      0.42562511471747877,
      0.0,
      0.3312559697228523,
      0.2757646970629415,
      0.3146093212167378,
      0.3641217776134089,
      0.3627109357603966,
      0.3806251750163625,
      0.4213460690830262,
      0.33159570251932435,
      0.297624502559658
    ],
    [
      0.37276283325393633,
      0.4591521415871871,
      0.443128632233863,
      0.441631590778262,
      0.45020600672811306,
      0.47703605194343845,
      0.4815809776705995,
      0.5114932304030029,
      0.458836865725317,
      0.47535830803174806,
      0.45483791379568217,
      0.3181628348192911,
      0.43814344554790097,
      0.4196105134773609,
      0.45322669163598195,
      0.4545981247188402,
      0.43409696388838315,
      0.4309784005707591,
      0.41325996263586307,
      0.4200751655196411,
      0.0,
      0.31710686055638737,
      0.3477299811082408,
      0.4398670661692887,
      0.41749710988355226,
      0.4182286563064528,
      0.39724181850245666,
      0.43283334711634414,
      0.4455178348501758
    ],
    [
      0.283425803683961,
      0.35959044159239384,
      0.3472943282797494,
      0.3394185012087423,
      0.34639310402782675,
      0.3527661413009111,
      0.345904594921419,
      0.3382907810205391,
      0.33657809807500305,
      0.3194536464341431,
      0.34573816329354057,
      0.35080367206606944,
      0.32695200764118937,
      0.37633569798819155,
      0.36800598287094455,
      0.34873366134281225,
      0.31445691977570855,
      0.33639072283093663,
      0.3382307483282516,
      0.33790384773408944,
      0.3076459777982832,
      0.0,
      0.33509097647892605,
      0.32380028511711734,
      0.3293530128827329,
      0.3346864423417202,
      0.37960928725368426,
      0.3313172615278577,
      0.34472549064789826
    ],
    [
      0.26802627141696167,
      0.268362355799032,
      0.2825615601329654,
      0.2995674010624343,
      0.3017645822720427,
      0.2796575914511459,
      0.2670323044542422,
      0.3048937506783642,
      0.2928002643926888,
      0.30701525818555186,
      0.2892487736884164,
      0.2799109848251946,
      0.2994814182621963,
      0.30981143709185743,
      0.3131321187516547,
      0.2758598995873387,
      0.3434091641497512,
      0.2842282485145664,
      0.2784044466860778,
      0.2916053543504076,
      0.33669121389957235,
      0.2594744349669138,
      0.0,
      0.23534572221352223,
      0.38174570482335013,
      0.3286080526143531,
      0.3269295578363711,
      0.2858195470886291,
      0.268109217009844
    ],
    [
      0.2789857718033768,
      0.3097676960635454,
      0.37544328171976393,
      0.313951559490913,
      0.3388505843044065,
      0.3490866266406629,
      0.35833152620933295,
      0.3323063167307394,
      0.37678476838873864,
      0.40298070847684575,
      0.32785628505216846,
      0.238370552506032,
      0.29778939991921316,
      0.3383874495977539,
      0.3798759961546614,
      0.35118534420326375,
      0.3222321988150647,
      0.2969980584492786,
      0.36112583329595926,
      0.3854355034142005,
      0.3785015533301588,
      0.2526074465420107,
      0.2898498759059447,
      0.0,
      0.32959742516574875,
      0.3132372253080298,
      0.30863539309602306,
      0.3761085058910545,
      0.3567434726129539
    ],
    [
      0.40343401981385774,
      0.42730806054634485,
      0.4581554869631541,
      0.40924787638952065,
      0.4612909529989768,
      0.42592400588040924,
      0.42537982274532604,
      0.4218643519422498,
      0.46516720931418254,
      0.4569867119879407,
      0.47003621046711475,
      0.31498391031726514,
      0.46618533635468995,
      0.44637965099724664,
      0.466103811561948,
      0.4618056493177478,
      0.4824727732706853,
      0.3899941983512234,
      0.42965850843652964,
      0.48255045690592024,
      0.42560568713581093,
      0.3291393934434572,
      0.4872408787757816,
      0.39664547949660767,
      0.0,
      0.4539311093228602,
      0.49133149098226525,
      0.3818435125891624,
      0.3975616567174567
    ],
    [
      0.43122155241753735,
      0.5230785861100866,
      0.4346694513249063,
      0.48348268568013086,
      0.4791743729030873,
      0.4096972613853227,
      0.43993366705251824,
      0.46494629799014753,
      0.44283667159357143,
      0.40600027692159557,
      0.49855958599237526,
      0.3006160179552222,
      0.44987152932016006,
      0.5025053283970673,
      0.5234629266058806,
      0.5191264837218543,
      0.49111215167112565,
      0.4256077527689901,
      0.4997397696552073,
      0.4853425169788652,
      0.4324863942205499,
      0.3622480815204683,
      0.4018009588954785,
      0.3879489245193235,
      0.4469851023938445,
      0.0,
      0.4787839047717406,
      0.3991481720714076,
      0.3352620488542597
    ],
    [
      0.40031405060072944,
      0.485901785042105,
      0.38502039496511786,
      0.4422260087246772,
      0.4258176167901837,
      0.40030403485701394,
      0.45122140362831686,
      0.42115097364966925,
      0.3967754247758848,
      0.41762133802755574,
      0.48179328586464965,
      0.2931117315696936,
      0.41339800955066575,
      0.47811525464493654,
      0.47724351604112014,
      0.45358405089973886,
      0.4452994425554182,
      0.3824117264762983,
      0.4719310173498561,
      0.5102177804788979,
      0.36616564194636836,
      0.3140152810843184,
      0.3729931207998576,
      0.32245889840195185,
      0.4346791286088363,
      0.4578510389563475,
      0.0,
      0.31731243259421404,
      0.3324853367868026
    ],
    [
      0.3539950105898153,
      0.3853436267350958,
      0.3887793595409632,
      0.40370348280844626,
      0.3501467066875452,
      0.3687009729651032,
      0.3580699788565611,
      0.44555002823642265,
      0.4361156024512125,
      0.37523466027033714,
      0.34397980576790954,
      0.25919080254961546,
      0.40969932685809685,
      0.37003526186371083,
      0.4178402569586621,
      0.39954001092445757,
      0.405114125018891,
      0.407361973627834,
      0.4032800624600892,
      0.43415529461057334,
      0.4080771483336396,
      0.3406846274773465,
      0.35112960342289457,
      0.3970396710984154,
      0.359608490783885,
      0.385035987973795,
      0.36358696981314753,
      0.0,
      0.3477872106463804
    ],
    [
      0.322342693382043,
      0.3956642720342791,
      0.39129568015084826,
      0.35921804895418874,
      0.43763227750551636,
      0.3926267366259828,
      0.4507995391866868,
      0.409398405087229,
      0.40418673380668313,
      0.4809463880648357,
      0.37862016413231214,
      0.33383708666622414,
      0.41072859432284536,
      0.39965081232702815,
      0.3674923278072384,
      0.38871398714612315,
      0.40333704358273814,
      0.3716971774547302,
      0.3683794425633198,
      0.37383785631534394,
      0.46651817496153614,
      0.3271541236784188,
      0.3759645757317478,
      0.4199923805563106,
      0.3926003190518057,
      0.3878645110759178,
      0.38614458824907927,
      0.36152259941145704,
      0.0
    ]
  ],
  "row_avgs": [
    0.20570092425316464,
    0.38129222125994783,
    0.4429911412313224,
    0.49788681136118285,
    0.41988599110374275,
    0.43924542354987867,
    0.2931869890156558,
    0.4410734168554632,
    0.41350550579357614,
    0.41082519319822514,
    0.42665775087157015,
    0.23074711504198397,
    0.43298289657970646,
    0.45591407223289215,
    0.4364294114807078,
    0.44369356880015287,
    0.43505859789566964,
    0.3886808136940179,
    0.42096311591168584,
    0.3726713249576616,
    0.4294356903377882,
    0.33924627137373725,
    0.294982022721623,
    0.3336080842531373,
    0.43315100760806197,
    0.44484458834616863,
    0.4125507044882581,
    0.38102807354753016,
    0.39136309070830244
  ],
  "col_avgs": [
    0.37438357333167555,
    0.4351871975360883,
    0.41580210526003347,
    0.4147702076556508,
    0.40690711846953936,
    0.4111442463496981,
    0.40864779402441004,
    0.40444814746776137,
    0.4209850250578028,
    0.3946697866898528,
    0.44904004100116107,
    0.2684271231086868,
    0.4253000119312576,
    0.4452018901611153,
    0.4387872187453974,
    0.4355407514025088,
    0.41040702164979737,
    0.3891975331077158,
    0.4016063699057019,
    0.41835628306401806,
    0.38280633161641925,
    0.29740773119758057,
    0.3493886019670988,
    0.3560461385429164,
    0.37998607241775767,
    0.4002130036414327,
    0.3992563744163173,
    0.36441108513545833,
    0.3512770336179612
  ],
  "combined_avgs": [
    0.2900422487924201,
    0.40823970939801807,
    0.42939662324567796,
    0.45632850950841686,
    0.41339655478664106,
    0.4251948349497884,
    0.35091739152003293,
    0.4227607821616123,
    0.41724526542568946,
    0.40274748994403897,
    0.4378488959363656,
    0.2495871190753354,
    0.429141454255482,
    0.45055798119700374,
    0.4376083151130526,
    0.43961716010133084,
    0.42273280977273353,
    0.38893917340086687,
    0.4112847429086939,
    0.3955138040108398,
    0.40612101097710374,
    0.31832700128565894,
    0.3221853123443609,
    0.34482711139802685,
    0.4065685400129098,
    0.42252879599380067,
    0.4059035394522877,
    0.37271957934149424,
    0.37132006216313185
  ],
  "gppm": [
    611.3426756811161,
    614.1738087940013,
    620.673305903287,
    625.3537327764569,
    624.2844149708288,
    624.0553671710085,
    617.6459576305582,
    624.5645811774314,
    621.4388145699444,
    633.2827350175211,
    608.4441685207915,
    689.5974902390363,
    621.0339779763465,
    609.8702905069407,
    611.3735557194001,
    612.740315440374,
    624.9844985552487,
    632.9420090203761,
    626.9481306725977,
    617.9057372003107,
    635.5978068836213,
    678.5758192332721,
    650.352331763187,
    646.9969032439066,
    638.8582283431023,
    627.8243362745508,
    629.5393180308703,
    645.0649333058784,
    650.1807687126957
  ],
  "gppm_normalized": [
    1.4283367481125278,
    1.3590071006726634,
    1.3696885194163677,
    1.3869108810217217,
    1.3718858118303172,
    1.3760070067158312,
    1.3769296217795386,
    1.3744691653123677,
    1.368778036253422,
    1.3900603207385047,
    1.3338207932821469,
    1.5348885593251949,
    1.3666638181675945,
    1.3434462723382647,
    1.346104851636473,
    1.3564546156026438,
    1.3719140358517083,
    1.4018079273964579,
    1.3795159412948503,
    1.3571204441798799,
    1.3978362741253438,
    1.500321515700422,
    1.4316002355798993,
    1.4218369510255382,
    1.4008766496755658,
    1.3903266820945315,
    1.37987600686045,
    1.421667709550299,
    1.4258816364361497
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401
  ],
  "response_lengths": [
    3430,
    2656,
    2400,
    2216,
    2605,
    2554,
    3447,
    2346,
    2534,
    2276,
    2635,
    2951,
    2370,
    2261,
    2498,
    2662,
    2259,
    2466,
    2502,
    2779,
    2161,
    2237,
    2491,
    2359,
    2200,
    2079,
    2369,
    2170,
    2332
  ]
}