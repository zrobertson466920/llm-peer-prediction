{
  "example_idx": 61,
  "reference": "Published as a conference paper at ICLR 2023\n\nDIFFUSION-GAN: TRAINING GANS WITH DIFFUSION\n\nZhendong Wang1,2, Huangjie Zheng1,2, Pengcheng He2, Weizhu Chen2, Mingyuan Zhou1 1The University of Texas at Austin, 2Microsoft Azure AI {zhendong.wang, huangjie.zheng}@utexas.edu, {penhe,wzchen}@microsoft.com mingyuan.zhou@mccombs.utexas.edu\n\nABSTRACT\n\nGenerative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussianmixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator’s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator’s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.\n\n1\n\nINTRODUCTION\n\nGenerative adversarial networks (GANs) (Goodfellow et al., 2014) and their variants (Brock et al., 2018; Karras et al., 2019; 2020a; Zhao et al., 2020) have achieved great success in synthesizing photo-realistic high-resolution images. GANs in practice, however, are known to suffer from a variety of issues ranging from non-convergence and training instability to mode collapse (Arjovsky and Bottou, 2017; Mescheder et al., 2018). As a result, a wide array of analyses and modifications has been proposed for GANs, including improving the network architectures (Karras et al., 2019; Radford et al., 2016; Sauer et al., 2021; Zhang et al., 2019), gaining theoretical understanding of GAN training (Arjovsky and Bottou, 2017; Heusel et al., 2017; Mescheder et al., 2017; 2018), changing the objective functions (Arjovsky et al., 2017; Bellemare et al., 2017; Deshpande et al., 2018; Li et al., 2017a; Nowozin et al., 2016; Zheng and Zhou, 2021; Yang et al., 2021), regularizing the weights and/or gradients (Arjovsky et al., 2017; Fedus et al., 2018; Mescheder et al., 2018; Miyato et al., 2018a; Roth et al., 2017; Salimans et al., 2016), utilizing side information (Wang et al., 2018; Zhang et al., 2017; 2020b), adding a mapping from the data to latent representation (Donahue et al., 2016; Dumoulin et al., 2016; Li et al., 2017b), and applying differentiable data augmentation (Karras et al., 2020a; Zhang et al., 2020a; Zhao et al., 2020).\n\nA simple technique to stabilize GAN training is to inject instance noise, i.e., to add noise to the discriminator input, which can widen the support of both the generator and discriminator distributions and prevent the discriminator from overfitting (Arjovsky and Bottou, 2017; Sønderby et al., 2017). However, this technique is hard to implement in practice, as finding a suitable noise distribution is challenging (Arjovsky and Bottou, 2017). Roth et al. (2017) show that adding instance noise to the high-dimensional discriminator input does not work well, and propose to approximate it by adding a zero-centered gradient penalty on the discriminator. This approach is theoretically and empirically shown to converge in Mescheder et al. (2018), who also demonstrate that adding zero-centered gradient penalties to non-saturating GANs can result in stable training and better or comparable generation quality compared to WGAN-GP (Arjovsky et al., 2017). However, Brock\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Flowchart for Diffusion-GAN. The top-row images represent the forward diffusion process of a real image, while the bottom-row images represent the forward diffusion process of a generated fake image. The discriminator learns to distinguish a diffused real image from a diffused fake image at all diffusion steps.\n\net al. (2018) caution that zero-centered gradient penalties and other similar regularization methods may stabilize training at the cost of generation performance. To the best of our knowledge, there has been no existing work that is able to empirically demonstrate the success of using instance noise in GAN training on high-dimensional image data.\n\nTo inject proper instance noise that can facilitate GAN training, we introduce Diffusion-GAN, which uses a diffusion process to generate Gaussian-mixture distributed instance noise. We show a graphical representation of Diffusion-GAN in Figure 1. In Diffusion-GAN, the input to the diffusion process is either a real or a generated image, and the diffusion process consists of a series of steps that gradually add noise to the image. The number of diffusion steps is not fixed, but depends on the data and the generator. We also design the diffusion process to be differentiable, which means that we can compute the derivative of the output with respect to the input. This allows us to propagate the gradient from the discriminator to the generator through the diffusion process, and update the generator accordingly. Unlike vanilla GANs, which compare the real and generated images directly, Diffusion-GAN compares the noisy versions of them, which are obtained by sampling from the Gaussian mixture distribution over the diffusion steps, with the help of our timestep-dependent discriminator. This distribution has the property that its components have different noise-to-data ratios, which means that some components add more noise than others. By sampling from this distribution, we can achieve two benefits: first, we can stabilize the training by easing the problem of vanishing gradient, which occurs when the data and generator distributions are too different; second, we can augment the data by creating different noisy versions of the same image, which can improve the data efficiency and the diversity of the generator. We provide a theoretical analysis to support our method, and show that the min-max objective function of Diffusion-GAN, which measures the difference between the data and generator distributions, is continuous and differentiable everywhere. This means that the generator in theory can always receive a useful gradient from the discriminator, and improve its performance.\n\nOur main contributions include: 1) We show both theoretically and empirically how the diffusion process can be utilized to provide a model- and domain-agnostic differentiable augmentation, enabling data-efficient and leaking-free stable GAN training. 2) Extensive experiments show that Diffusion-GAN boosts the stability and generation performance of strong baselines, including StyleGAN2 (Karras et al., 2020b), Projected GAN (Sauer et al., 2021), and InsGen (Yang et al., 2021), achieving state-of-the-art results in synthesizing photo-realistic images, as measured by both the Fr ́echet Inception Distance (FID) (Heusel et al., 2017) and Recall score (Kynk ̈a ̈anniemi et al., 2019).\n\n2 PRELIMINARIES: GANS AND DIFFUSION-BASED GENERATIVE MODELS\n\nGANs (Goodfellow et al., 2014) are a class of generative models that aim to learn the data distribution p(x) of a target dataset by setting up a min-max game between two neural networks: a generator and a discriminator. The generator G takes as input a random noise vector z sampled from a simple prior distribution p(z), such as a standard normal or uniform distribution, and tries to produce realistic-looking samples G(z) that resemble the data. The discriminator D receives either\n\n2\n\nx∼p(x)y|t=100y|t=300y|t=600y|t=T=1000y!|t=100y!|t=300y!|t=600y!|t=T=1000Timestep-Dependent Discriminator: D(y,t)T is adaptively adjusted.Generator: xg=Gz,z∼p(z)Diffusion Process:y∼qyx,t,t∼pπDiffusion Process: yg∼qygxg,t,t∼pπPublished as a conference paper at ICLR 2023\n\na real data sample x drawn from p(x) or a fake sample G(z) generated by G, and tries to correctly classify them as real or fake. The goal of G is to fool D into making mistakes, while the goal of D is to accurately distinguish G(z) from x. The min-max objective function of GANs is given by\n\nmin G\n\nmax D\n\nV (G, D) = Ex∼p(x)[log(D(x))] + Ez∼p(z)[log(1 − D(G(z)))].\n\nIn practice, this vanilla objective function is often modified to improve the stability and performance of GANs(Goodfellow et al., 2014; Miyato et al., 2018a; Fedus et al., 2018), but the general idea of adversarial learning between G and D remains the same.\n\nDiffusion-based generative models (Ho et al., 2020b; Sohl-Dickstein et al., 2015; Song and Ermon, 2019) assume pθ(x0) := (cid:82) pθ(x0:T )dx1:T , where x1, . . . , xT are latent variables of the same dimensionality as the data x0 ∼ p(x0). There is a forward diffusion chain that gradually adds noise to the data x0 ∼ q(x0) in T steps with pre-defined variance schedule βt and variance σ2:\n\nq(x1:T | x0) := (cid:81)T\n\nt=1 q(xt | xt−1),\n\nq(xt | xt−1) := N (xt;\n\n1 − βtxt−1, βtσ2I).\n\n√\n\nA notable property is that xt at an arbitrary time-step t can be sampled in closed form as\n\nq(xt | x0) = N (xt;\n\n ̄αtx0, (1 − ̄αt)σ2I), where αt := 1 − βt, ̄αt := (cid:81)t\n\ns=1 αs.\n\n(1)\n\n√\n\nA variational lower bound (Blei et al., 2017) is then used to optimize the reverse diffusion chain as\n\npθ(x0:T ) := N (xT ; 0, σ2I) (cid:81)T\n\nt=1 pθ(xt−1 | xt).\n\n3 DIFFUSION-GAN: METHOD AND THEORETICAL ANALYSIS\n\nTo construct Diffusion-GAN, we describe how to inject instance noise via diffusion, how to train the generator by backpropagating through the forward diffusion process, and how to adaptively adjust the diffusion intensity. We further provide theoretical analysis illustrated with a toy example.\n\n3.1\n\nINSTANCE NOISE INJECTION VIA DIFFUSION\n\nWe aim to generate realistic samples xg from a generator network G that maps a latent variable z sampled from a simple prior distribution p(z) to a high-dimensional data space, such as images. The distribution of generator samples xg = G(z), z ∼ p(z) is denoted by pg(x) = (cid:82) p(xg | z)p(z)dz. To make the generator more robust and diverse, we inject instance noise into the generated samples xg by applying a diffusion process that adds Gaussian noise at each step. The diffusion process can be seen as a Markov chain that starts from the original sample x and gradually erases its information until reaching a noise level σ2 after T steps.\n\nWe define a mixture distribution q(y | x) that models the noisy samples y obtained at any step of the diffusion process, with a mixture weight πt for each step t. The mixture components q(y | x, t) are Gaussian distributions with mean proportional to x and variance depending on the noise level at step t. We use the same diffusion process and mixture distribution for both the real samples x ∼ p(x) and the generated samples xg ∼ pg(x). More specifically, the diffusion-induced mixture distributions are expressed as\n\nx ∼ p(x), y ∼ q(y | x), q(y | x) := (cid:80)T\n\nt=1 πtq(y | x, t),\n\nxg ∼ pg(x), yg ∼ q(yg | xg), q(yg | xg) := (cid:80)T\n\nt=1 πtq(yg | xg, t),\n\nwhere q(y | x) is a T -component mixture distribution, the mixture weights πt are non-negative and sum to one, and the mixture components q(y | x, t) are obtained via diffusion as in Equation (1), expressed as\n\nq(y | x, t) = N (y;\n\n ̄αtx, (1 − ̄αt)σ2I).\n\n√\n\n(2)\n\nSamples from this mixture can be drawn as t ∼ pπ := Discrete(π1, . . . , πT ), y ∼ q(y | x, t).\n\nBy sampling y from this mixture distribution, we can obtain noisy versions of both real and generated samples with varying degrees of noise. The more steps we take in the diffusion process, the more noise we add to y and the less information we preserve from x. We can then use this diffusion-induced mixture distribution to train a timestep-dependent discriminator D that distinguishes between real and generated noisy samples, and a generator G that matches the distribution of generated noisy samples to the distribution of real noisy samples. Next we introduce Diffusion-GAN that trains its discriminator and generator with the help of the diffusion-induced mixture distribution.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.2 ADVERSARIAL TRAINING\n\nThe Diffusion-GAN trains its generator and discriminator by solving a min-max game objective as V (G, D) = Ex∼p(x),t∼pπ,y∼q(y | x,t)[log(Dφ(y, t))] + Ez∼p(z),t∼pπ,yg∼q(y | Gθ(z),t)[log(1 − Dφ(yg, t))]. (3) Here, p(x) is the true data distribution, pπ is a discrete distribution that assigns different weights πt to each diffusion step t ∈ {1, . . . , T }, and q(y | x, t) is the conditional distribution of the perturbed sample y given the original data x and the diffusion step t. By Equation (2), with Gaussian reparameterization, the perturbation function could be written as y = 1 − ̄αtσε, where 1 − ̄αt = 1 − (cid:81)t s=1 αs is the cumulative noise level at step t, σ is a scale factor, and ε ∼ N (0, I) is a Gaussian noise.\n\n ̄αtx +\n\n√\n\n√\n\nThe objective function in Equation (3) encourages the discriminator to assign high probabilities to the perturbed real data and low probabilities to the perturbed generated data, for any diffusion step t. The generator, on the other hand, tries to produce samples that can deceive the discriminator at any diffusion step t. Note that the perturbed generated sample yg ∼ q(y | Gθ(z), t) can be rewritten ̄αtGθ(z) + (cid:112)(1 − ̄αt)σε, ε ∼ N (0, I). This means that the objective function in as yg = Equation (3) is differentiable with respect to the generator parameters, and we can use gradient descent to optimize it with back-propagation.\n\n√\n\nThe objective function Equation (3) is similar to the one used by the original GAN (Goodfellow et al., 2014), except that it involves the diffusion steps and the perturbation functions. We can show that this objective function also minimizes an approximation of the Jensen–Shannon (JS) divergence between the true and the generated distributions, but with respect to the perturbed samples and the diffusion steps, as follows:\n\nDJS(p(y, t)||pg(y, t)) = Et∼pπ [DJS(p(y | t)||pg(y | t))]. The JS divergence measures the dissimilarity between two probability distributions, and it reaches its minimum value of zero when the two distributions are identical. The proof of the equality in Equation (4) is given in Appendix C. A natural question that arises from this result is whether minimizing the JS divergence between the perturbed distributions implies minimizing the JS divergence between the original distributions, i.e., whether the optimal generator for Equation (3) is also the optimal generator for DJS(p(x)||pg(x)). We will answer this question affirmatively and provide a theoretical justification in Section 3.4.\n\n(4)\n\n3.3 ADAPTIVE DIFFUSION\n\nWith the help of the perturbation function and timestep dependency, we have a new strategy to optimize the discriminator. We want the discriminator D to have a challenging task, neither too easy to allow overfitting the data (Karras et al., 2020a; Zhao et al., 2020) nor too hard to impede learning. Therefore, we adjust the intensity of the diffusion process, which adds noise to both y and yg, depending on how much D can distinguish them. When the diffusion step t is larger, the noise-to-data ratios are higher and the task is harder. We use 1 − ̄αt to measure the intensity of the diffusion, which increases as t grows. To control the diffusion intensity, we adaptively modify the maximum number of steps T .\n\nOur strategy is to make the discriminator learn from the easiest samples first, which are the original data samples, and then gradually increase the difficulty by feeding it samples from larger t. To do this, we use a self-paced schedule for T , which depends on a metric rd that estimates how much the discriminator overfits to the data:\n\nrd = Ey,t∼p(y,t)[sign(Dφ(y, t) − 0.5)], T = T + sign(rd − dtarget) ∗ C, where rd is the same as in Karras et al. (2020a) and C is a constant. We calculate rd and update T every four minibatches. We have two options for the distribution pπ that we use to sample t for the diffusion process:\n\n(5)\n\nt ∼ pπ :=\n\n(cid:40)uniform: Discrete (cid:0) 1 priority: Discrete\n\nT , 1 (cid:16) 1\n\n(cid:80)T\n\n(cid:1) ,\n\nT , . . . , 1 ,\n\nT 2\n(cid:80)T t=1 t\n\nt=1 t\n\n, . . . ,\n\nT (cid:80)T t=1 t\n\n(cid:17)\n\n,\n\n(6)\n\nThe ‘priority’ option gives more weight to larger t, which means the discriminator will see more new samples from the new steps when T increases. This is because we want the discriminator to focus on\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nthe new and harder samples that it has not seen before, as this indicates that it is confident about the easier ones. Note that even with the ‘priority’ option, the discriminator can still see samples from smaller t, because q(y | x) is a mixture of Gaussians that covers all steps of the diffusion chain.\n\nTo avoid sudden changes in T during training, we use an exploration list tepl that contains t values sampled from pπ. We keep tepl fixed until we update T , and we sample t from tepl to generate noisy samples for the discriminator. This way, the model can explore each t sufficiently before moving to a higher T . We give the details of training Diffusion-GAN in Algorithm 1 in Appendix F.\n\n3.4 THEORETICAL ANALYSIS WITH EXAMPLES\n\nTo better understand the theoretical properties of our proposed method, we present two theorems that address two important questions about the use of diffusion-based instance noise injection for training GANs. The proofs of these theorems are deferred to Appendix B. The first question, denoted as (a), is whether adding noise to the real and generated samples in a diffusion process can facilitate the learning. The second question, denoted as (b), is whether minimizing the JS divergence between the joint distributions of the noisy samples and the noise levels, p(y, t) and pg(y, t), can lead to the same optimal generator as minimizing the JS divergence between the original distributions of the real and generated samples, p(x) and pg(x).\n\nTo answer (a), we prove that for any choice of noise level t and any choice of convex function f , the f -divergence (Nowozin et al., 2016) between the marginal distributions of the noisy real and generated samples, q(y | t) and q(yg | t), is a smooth function that can be computed and optimized by the discriminator. This implies that the diffusion-based noise injection does not introduce any singularity or discontinuity in the objective function of the GAN. The JS divergence is a special case of f -divergence, where f (u) = − log(2u) − log(2 − 2u).\n\nTheorem 1 (Valid gradients anywhere for GANs training). Let p(x) be a fixed distribution over X and z be a random noise over another space Z. Denote Gθ : Z → X as a function with parameter θ and input z and pg(x) as the distribution of Gθ(z). Let q(y | x, t) = ̄αtx, (1 − ̄αt)σ2I), where ̄αt ∈ (0, 1) and σ > 0. Let q(y | t) = (cid:82) p(x)q(y | x, t)dx N (y; and qg(y | t) = (cid:82) pg(x)q(y | x, t)dx. Then, ∀t, if function Gθ is continuous and differentiable, the f-divergence Df (q(y | t)||qg(y | t)) is continuous and differentiable with respect to θ.\n\n√\n\nTheorem 1 shows that with the help of diffusion noise injection by q(y | x, t), ∀t, y and yg are defined on the same support space, the whole X , and Df (q(y | t)||qg(y | t)) is continuous and differentiable everywhere. Then, one natural question is what if Df (q(y | t)||qg(y | t)) keeps a near constant value and hence provides little useful gradient. Hence, we empirically show that by injecting noise through a mixture defined over all steps of the diffusion chain, there is always a good chance that a sufficiently large t is sampled to provide a useful gradient, via the toy example below.\n\nToy example. We use the same simple example from Arjovsky et al. (2017) to illustrate our method. Let x = (0, z) be the real data and xg = (θ, z) be the data generated by a one-parameter generator, where z is a uniform random variable in [0, 1]. The JS divergence between the real and the generated distributions, DJS(p(x)||p(xg)), is discontinuous: it is log 2 when θ = 0 and zero otherwise, so it does not provide a useful gradient to guide θ towards zero.\n\nWe introduce diffusion-based noise to both the real and the generated data, as shown in the first row of Figure 2. The noisy data, y and yg, have supports that cover the whole space R2 and their densities overlap more or less depending on the diffusion step t. In the second row, left, of Figure 2, we plot how the JS divergence between the noisy distributions, DJS(q(y | t)||qg(y | t)), varies with θ for different t values. The black line with t = 0 is the original JS divergence, which has a discontinuity at θ = 0. As t increases, the JS divergence curves become smoother and have nonzero gradients for a larger range of θ. However, some values of t, such as t = 200 in this example, still have flat regions where the JS divergence is nearly constant. To avoid this, we use a mixture of all steps to ensure that there is always a high chance of getting informative gradients.\n\nFor the discriminator optimization, as shown in the second row, right, of Figure 2, the optimal discriminator under the original JS divergence is discontinuous and unattainable. With diffusionbased noise, the optimal discriminator changes with t: a smaller t makes it more confident and a larger t makes it more cautious. Thus the diffusion acts like a scale to balance the power of the\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The toy example inherited from Arjovsky et al. (2017). The first row plots the distributions of data with diffusion noise injected for t. The second row shows the JS divergence and the optimal discriminator value with and without our noise injection.\n\ndiscriminator. This suggests the use of a differentiable forward diffusion chain that can provide various levels of gradient smoothness to help the generator training.\n\nTheorem 2 (Non-leaking noise injection). Let x ∼ p(x), y ∼ q(y | x) and xg ∼ pg(x), yg ∼ q(yg | xg), where q(y | x) is the transition density. Given certain q(y | x), if y could be reparameterized into y = f (x) + h(ε), ε ∼ p(ε), where p(ε) is a known distribution, and both f and h are one-to-one mapping functions, then we could have p(y) = pg(y) ⇔ p(x) = pg(x).\n\nTo answer question (b), we present Theorem 2, which shows a sufficient condition for the equality of the original and the augmented data distributions. By Theorem 2, the function f maps each x to a unique y, the function h maps each ε to a unique noise term, and the distribution of ε is known and independent of x. Under these assumptions, the theorem proves that the distribution of y is the same as the distribution of yg, if and only if the distribution of x is the same as the distribution of xg. If we take y | t as the y introduced in the theorem, then for ∀t, Equation (2) fits the assumption made. This means that, by minimizing the divergence between q(y | t) and qg(y | t), which is the same as minimizing the divergence between p(x) | t and pg(x) | t, we are also minimizing the divergence between p(x) and pg(x). This implies that the noise injection does not affect the quality of the generated samples, and we can safely use our noise injection to improve the training of the generative model.\n\n3.5 RELATED WORK\n\nThe proposed Diffusion-GAN can be related to previous works on stabilizing the GAN training, building diffusion-based generative models, and constructing differential augmentation for dataefficient GAN training. A detailed discussion on these related works is deferred to Appendix A.\n\n4 EXPERIMENTS\n\nWe conduct extensive experiments to answer the following questions: (a) Will Diffusion-GAN outperform state-of-the-art GAN baselines on benchmark datasets? (b) Will the diffusion-based noise injection help the learning of GANs in domain-agnostic tasks? (c) Will our method improve the performance of data-efficient GANs trained with a very limited amount of data?\n\nDatasets. We conduct experiments on image datasets ranging from low-resolution (e.g., 32 × 32) to high-resolution (e.g., 1024 × 1024) and from low-diversity to high-diversity: CIFAR-10 (Krizhevsky, 2009), STL-10 (Coates et al., 2011), LSUN-Bedroom (Yu et al., 2015), LSUN-Church (Yu et al., 2015), AFHQ(Cat/Dog/Wild) (Choi et al., 2020), and FFHQ (Karras et al., 2019). More details on these benchmark datasets are provided in Appendix E.\n\nEvaluation protocol. We measure image quality using FID (Heusel et al., 2017). Following Karras et al. (2019; 2020b), we measure FID using 50k generated samples, with the full training set used\n\n6\n\n024x0.00.51.0yt=0024x01yt=20002x101yt=40002x101yt=5001012x101yt=600101x101yt=80010.07.55.02.50.02.55.07.510.00246DJS(q(y|t)||qg(y|t))t=0t=200t=400t=500t=600t=80020246x0.000.250.500.751.00Optimal Discriminator Value D*(x)t=0t=200t=400t=500t=600t=800Published as a conference paper at ICLR 2023\n\nTable 1: Image generation results on benchmark datasets: CIFAR-10, CelebA, STL-10, LSUN-Bedroom, LSUN-Church, and FFHQ. We highlight the best and second best results in each column with bold and underline, respectively. Lower FIDs indicate better fidelity, while higher Recalls indicate better diversity.\n\nMethods\n\nStyleGAN2 (Karras et al., 2020a)\n\nStyleGAN2 + DiffAug (Zhao et al., 2020)\n\nStyleGAN2 + ADA (Karras et al., 2020a)\n\nDiffusion StyleGAN2\n\nCIFAR-10\n\n(32 × 32)\n\nCelebA\n\n(64 × 64)\n\nSTL-10\n\n(64 × 64)\n\nLSUN-Bedroom LSUN-Church\n\nFFHQ\n\n(256 × 256)\n\n(256 × 256)\n\n(1024 × 1024)\n\nFID 8.32∗ 5.79∗ 2.92∗\n\n3.19\n\nRecall 0.41∗ 0.42∗ 0.49∗\n\n0.58\n\nFID Recall\n\nFID\n\nRecall\n\n2.32\n\n2.75\n\n2.49\n\n1.69\n\n0.55\n\n0.52\n\n0.53\n\n0.67\n\n11.70\n\n12.97\n\n13.72\n\n11.43\n\n0.44\n\n0.39\n\n0.36\n\n0.45\n\nFID\n\n3.98\n\n4.25\n\n7.89\n\n3.65\n\nRecall\n\n0.32\n\n0.19\n\n0.05\n\n0.32\n\nFID\n\n3.93\n\n4.66\n\n4.12\n\n3.17\n\nRecall\n\n0.39\n\n0.33\n\n0.18\n\n0.42\n\nFID\n\n4.41\n\n4.46\n\n4.47\n\n2.83\n\nRecall\n\n0.42\n\n0.41\n\n0.41\n\n0.49\n\n(a) CIFAR-10\n\n(b) CelebA\n\n(c) STL-10\n\n(d) LSUN-Bedroom\n\n(e) LSUN-Church\n\nFigure 3: Randomly generated images from Diffusion StyleGAN2 trained on CIFAR-10, CelebA, STL-10, LSUN-Bedroom, LSUN-Church, and FFHQ datasets.\n\n(f) FFHQ\n\nas reference. We use the number of real images shown to the discriminator to evaluate convergence (Karras et al., 2020a; Sauer et al., 2021). Unless specified otherwise, all models are trained with 25 million images to ensure convergence (these trained with more or fewer images are specified in table captions). We further report the improved Recall score introduced by Kynk ̈a ̈anniemi et al. (2019) to measure the sample diversity of generative models.\n\nImplementations and resources. We build Diffusion-GANs based on the code of StyleGAN2 (Karras et al., 2020b), ProjectedGAN (Sauer et al., 2021), and InsGen (Yang et al., 2021) to answer questions (a), (b), and (c), respectively. Diffusion GANs inherit from their corresponding base GANs all their network architectures and training hyperparamters, whose details are provided in Appendix G. Specifically for StyleGAN2 and InsGen, we construct the discriminator as Dφ(y, t), where t is injected via their mapping network. For ProjectedGAN, we empirically find t in the discriminator could be ignored to simplify the implementation and minimize the modifications to ProjectedGAN. More implementation details are provided in Appendix H. By applying our diffusionbased noise injection, we denote our models as Diffusion StyleGAN2/ProjectedGAN/InsGen. In the following experiments, we train related models with their official code if the results are unavailable, while others are all reported from references and marked with ∗. We run all our experiments with either 4 or 8 NVIDIA V100 GPUs depending on the demands of the inherited training configurations.\n\n4.1 COMPARISON TO STATE-OF-THE-ART GANS\n\nWe compare Diffusion-GAN with its state-of-the-art GAN backbone, StyleGAN2 (Karras et al., 2020a), and to evaluate its effectiveness from the data augmentation perspective, we compare it with both StyleGAN2 + DiffAug (Zhao et al., 2020) and StyleGAN2 + ADA (Karras et al., 2020a), in terms of both sample fidelity (FID) and sample diversity (Recall) over extensive benchmark datasets.\n\nWe present the quantitative and qualitative results in Table 1 and Figure 3. Qualitatively, these generated images from Diffusion StyleGAN2 are all photo-realistic and have good diversity, ranging from low-resolution (32 × 32) to high-resolution (1024 × 1024). Additional randomly generated images\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Plot of adaptively adjusted maximum diffusion steps T and discriminator outputs of Diffusion-GANs.\n\ncan be found in Appendix L. Quantitatively, Diffusion StyleGAN2 outperforms all the GAN baselines in generation diversity, as measured by Recall, on all 6 benchmark datasets and outperforms them in FID by a clear margin on 5 out of the 6 benchmark datasets.\n\nFrom the data augmentation perspective, we observe that Diffusion StyleGAN2 always clearly outperforms the backbone model StyleGAN2 across various datasets, which empirically validates our Theorem 2. By contrast, both the ADA (Karras et al., 2020b) and Diffaug (Zhao et al., 2020) techniques could sometimes impair the generation performance on sufficiently large datasets, e.g., LSUN-Bedroom and LSUN-Church, which is also observed by Yang et al. (2021) on FFHQ. This is possibly because their risk of leaking augmentation overshadows the benefits of data augmentation.\n\nTo investigate how the adaptive diffusion process works during training, we illustrate in Figure 4 the convergence of the maximum timestep T in our adaptive diffusion and discriminator outputs. We see that T is adaptively adjusted: The T for Diffusion StyleGAN2 increases as the training goes while the T for Diffusion ProjectedGAN first goes up and then goes down. Note that the T is adjusted according to the overfitting status of the discriminator. The second panel shows that trained with the diffusion-based mixture distribution, the discriminator is always well behaved and provides useful learning signals for the generator, which validates our analysis in Section 3.4 and Theorem 1.\n\nMemory and time costs. Generally speaking, the memory and time costs of a Diffusion-GAN are comparable to those of the corresponding GAN baseline. More specifically, switching from ADA (Karras et al., 2020a) to our diffusion-based augmentation, the added memory cost is negative, the added training time cost is negative, and the added inference time cost is zero. For example, for CIFAR-10, with four NVIDIA V100 GPUs, the training time for each 4k images is around 8.0s for StyleGAN2, 9.8s for StyleGAN2-ADA, and 9.5s for Diffusion-StyleGAN2.\n\n4.2 EFFECTIVENESS OF DIFFUSION-GAN FOR DOMAIN-AGNOSTIC AUGMENTATION\n\nTo verify whether our method is domain-agnostic, we apply Diffusion-GAN onto the input feature vectors of GANs. We conduct experiments on both low-dimensional and high-dimensional feature vectors, for which commonly used image augmentation methods are no longer applicable.\n\n25-Gaussians Example. We conduct experiments on the popular 25-Gaussians generation task. The 25-Gaussians dataset is a 2-D toy data, generated by a mixture of 25 two-dimensional Gaussian distributions. Each data point is a 2-dimensional feature vector. We train a small GAN model, whose generator and discriminator are both parameterized by multilayer perceptrons (MLPs), with two 128-unit hidden layers and LeakyReLu nonlinearities.\n\nThe training results are shown in Figure 5. We observe that the vanilla GAN exhibits severe mode collapsing, capturing only a few modes. Its discriminator outputs of real and fake samples depart from each other very quickly. This implies a strong overfitting of the discriminator happened so that the discriminator stops providing useful learning signals for the generator. However, DiffusionGAN successfully captures all the 25 Gaussian modes and the discriminator is under control to continuously provide useful learning signals. We interpret the improvement from two perspectives: First, non-leaking augmentation helps provide more information about the data space; Second, the discriminator is well behaved given the adaptively adjusted diffusion-based noise injection.\n\nProjectedGAN. To verify that our adaptive diffusion-based noise injection could benefit the learning of GANs on high-dimensional feature vectors, we directly apply it to the discriminator feature\n\n8\n\n1M2M5M10M15M20M25MTraining Progress (# million real images)02004006008001000TT Schedule on CIFAR-10 and STL-10T of Diffusion StyleGAN2 on CIFAR-10T of Diffusion StyleGAN2 on STL-10T of Diffusion ProjectedGAN on CIFAR-10T of Diffusion ProjectedGAN on STL-100M5M10M15M20M25MTraining Progress (# million real images)42024D(x)Discriminator outputs on CIFAR-10Real imagesGenerated imagesPublished as a conference paper at ICLR 2023\n\nFigure 5: The 25-Gaussians example. We show the true data samples, the generated samples from vanilla GANs, the discriminator outputs of the vanilla GANs, the generated samples from our Diffusion-GAN, and the discriminator outputs of Diffusion-GAN.\n\nspace of ProjectedGAN (Sauer et al., 2021). ProjectedGANs generally leverage pre-trained neural networks to extract meaningful features for the adversarial learning of the discriminator and generator. Following Sauer et al. (2021), we adaptively diffuse the feature vectors extracted by EfficientNet-v0 and keep all the other training parts unchanged. We report the performance of Diffusion ProjectedGAN on several benchmark datasets in Table 2, which verifies that our augmentation method is domain-agnostic. Under the ProjectedGAN framework, we see that with noise properly injected into the high-dimensional feature space, Diffusion ProjectedGAN shows clear improvement in terms of both FID and Recall. We reach state-of-the-art FID results with Diffusion ProjectedGAN on STL-10 and LSUN-Bedroom/Church datasets.\n\nTable 2: Domain-agnostic experiments on ProjectedGAN.\n\nDomain-agnostic Tasks\n\nProjectedGAN (Sauer et al., 2021)\n\nDiffusion ProjectedGAN\n\nCIFAR-10 (32 × 32)\n\nSTL-10 (64 × 64) LSUN-Bedroom (256 × 256) LSUN-Church (256 × 256)\n\nFID\n\n3.10\n\n2.54\n\nRecall\n\n0.45\n\n0.45\n\nFID\n\n7.76\n\n6.91\n\nRecall\n\n0.35\n\n0.35\n\nFID\n\n2.25\n\n1.43\n\nRecall\n\n0.55\n\n0.58\n\nFID\n\n3.42\n\n1.85\n\nRecall\n\n0.56\n\n0.65\n\n4.3 EFFECTIVENESS OF DIFFUSION-GAN FOR LIMITED DATA\n\nWe evaluate whether Diffusion-GAN can provide data-efficient GAN training. We first generate five FFHQ (1024 × 1024) dataset splits, consisting of 200, 500, 1k, 2k, and 5k images, respectively, where 200 and 500 images are considered to be extremely limited data cases. We also consider AFHQ-Cat, -Dog, and -Wild (512 × 512), each with as few as around 5k images. Motivated by the success of InsGen (Yang et al., 2021) on small datasets, we build our Diffusion-GAN upon it. We note on limited data, InsGen convincingly outperforms both StyleGAN2+ADA and +DiffAug, and currently holds the state-of-the-art performance for data-efficient GAN training. The results in Table 3 show that our Diffusion-GAN method can help further boost the performance of InsGen in limited data settings.\n\nTable 3: FFHQ (1024 × 1024) FID results with 200, 500, 1k, 2k, and 5k training samples; AFHQ (512 × 512) FID results. To ensure convergence, all models are trained across 10M images for FFHQ and 25M images for AFHQ. We bold the best number in each column.\n\nModels\n\nFFHQ (200) FFHQ (500) FFHQ (1k) FFHQ (2k) FFHQ (5k)\n\nInsGen (Yang et al., 2021)\n\nDiffusion InsGen\n\n102.58\n\n63.34\n\n54.762\n\n50.39\n\n34.90\n\n30.91\n\n18.21\n\n16.43\n\n9.89\n\n8.48\n\nCat 2.60∗\n\n2.40\n\nDog Wild 1.77∗ 5.44∗\n\n4.83\n\n1.51\n\n5 CONCLUSION\n\nWe present Diffusion-GAN, a novel GAN framework that uses a variable-length forward diffusion chain with a Gaussian mixture distribution to generate instance noise for GAN training. This approach enables model- and domain-agnostic differentiable augmentation that leverages the advantages of diffusion without requiring a costly reverse diffusion chain. We prove theoretically and demonstrate empirically that Diffusion-GAN can prevent discriminator overfitting and provide non-leaking augmentation. We also demonstrate that Diffusion-GAN can produce high-resolution photo-realistic images with high fidelity and diversity, outperforming its corresponding state-of-theart GAN baselines on standard benchmark datasets according to both FID and Recall.\n\n9\n\n1.51.00.50.00.51.01.5x1.51.00.50.00.51.01.5yGroud Truth0.00.20.40.60.81.01.21.4x0.00.20.40.60.81.01.21.4yGAN010002000300040005000Epochs1.51.00.50.00.51.0D(x)Discriminator outputs of GANReal samplesFake samples1.51.00.50.00.51.01.5x1.51.00.50.00.51.01.5yDiffusionGAN010002000300040005000Epochs1.51.00.50.00.51.01.5D(x)Discriminator outputs of DiffusionGANReal samplesFake samplesPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nZ. Wang, H. Zheng, and M. Zhou acknowledge the support of NSF-IIS 2212418 and IFML.\n\nREFERENCES\n\nMart ́ın Arjovsky and L ́eon Bottou. Towards principled methods for training generative adversarial In 5th International Conference on Learning Representations, ICLR 2017, Toulon, networks. France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=Hk4_qw5xe.\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 214–223, 2017.\n\nMarc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, and R ́emi Munos. The Cramer distance as a solution to biased Wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859–877, 2017.\n\nAshish Bora, Eric Price, and Alexandros G. Dimakis. AmbientGAN: Generative models from lossy measurements. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=Hy7fDog0b.\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity\n\nnatural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. StarGAN v2: Diverse image synthesis for multiple domains. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8185–8194, 2020.\n\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215–223. JMLR Workshop and Conference Proceedings, 2011.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n\nIshan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced In Proceedings of the IEEE conference on computer vision and pattern\n\nWasserstein distance. recognition, pages 3483–3491, 2018.\n\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= AAWuCvzaVt.\n\nJeff Donahue, Philipp Kr ̈ahenb ̈uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\n\narXiv:1605.09782, 2016.\n\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\n\nWilliam Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In International Conference on Learning Representations, 2018.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5767–5777, 2017.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017.\n\nJonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv,\n\nabs/2006.11239, 2020a.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020b.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative In Proceedings of the IEEE/CVF Conference on Computer Vision and\n\nadversarial networks. Pattern Recognition, pages 4401–4410, 2019.\n\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33:12104–12114, 2020a.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110–8119, 2020b.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational Bayes. CoRR, abs/1312.6114,\n\n2014.\n\nZhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint\n\narXiv:2106.00132, 2021.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nTuomas Kynk ̈a ̈anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila.\n\nImproved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019.\n\nChun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab ́as P ́oczos. MMD GAN: Towards deeper understanding of moment matching network. Advances in neural information processing systems, 30, 2017a.\n\nChunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, and Lawrence Carin. Alice: Towards understanding adversarial learning for joint distribution matching. Advances in neural information processing systems, 30, 2017b.\n\nBingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized gan training for high-fidelity few-shot image synthesis. In International Conference on Learning Representations, 2020.\n\nEric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved\n\nsampling speed. arXiv preprint arXiv:2101.02388, 2021.\n\nLars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. Advances in\n\nneural information processing systems, 30, 2017.\n\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In International conference on machine learning, pages 3481–3490. PMLR, 2018.\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018a. URL https://openreview.net/forum?id=B1QRgziT-.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018b.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in neural information processing systems, 2016.\n\nKushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents. arXiv preprint arXiv:2201.00308, 2022.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. CoRR, abs/1511.06434, 2016.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.\n\nKevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. Advances in neural information processing systems, 30, 2017.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, pages 2234–2242, 2016.\n\nRobin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion mod-\n\nels. arXiv preprint arXiv:2104.02600, 2021.\n\nAxel Sauer, Kashyap Chitta, Jens M ̈uller, and Andreas Geiger. Projected GANs converge faster.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\n\nvised learning using nonequilibrium thermodynamics. ArXiv, abs/1503.03585, 2015.\n\nCasper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husz ́ar. Amortised MAP inference for image super-resolution. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=S1RP6GLle.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\n\npreprint arXiv:2010.02502, 2020.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. URL https://openreview.net/ forum?id=St1giarCHLP.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\n\nIn Advances in Neural Information Processing Systems, pages 11918–11930, 2019.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben In InternaPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2021b. URL https://openreview.net/ forum?id=PxTIG12RRHS.\n\nNgoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. On data augmentation for gan training. IEEE Transactions on Image Processing, 30:1882– 1897, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional GANs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8798–8807, 2018.\n\nZhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with\n\ndenoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021.\n\nCeyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from instance discrimination. Advances in Neural Information Processing Systems, 34:9378–9390, 2021.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adIn Proceedings of the IEEE international conference on computer vision, versarial networks. pages 5907–5915, 2017.\n\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In International Conference on Machine Learning, pages 7354–7363. PMLR, 2019.\n\nHan Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. In International Conference on Learning Representations, 2020a. URL https://openreview.net/forum?id=S1lxKlSKPH.\n\nHao Zhang, Bo Chen, Long Tian, Zhengjue Wang, and Mingyuan Zhou. Variational hetero-encoder randomized GANs for joint image-text modeling. In International Conference on Learning Representations, 2020b. URL https://openreview.net/forum?id=H1x5wRVtvS.\n\nShengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient GAN training. Advances in Neural Information Processing Systems, 33:7559–7570, 2020.\n\nHuangjie Zheng and Mingyuan Zhou. Exploiting chain rule and Bayes’ theorem to compare probability distributions. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=f-ggKIDTu5D.\n\nHuangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion proba-\n\nbilistic models. arXiv preprint arXiv:2202.09671, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nA RELATED WORK\n\nStabilizing GAN training. A root cause of training difficulties in GANs is often attributed to the JS divergence that GANs intend to minimize. This is because when the data and generator distributions have non-overlapping supports, which are often the case for high-dimensional data supported by low-dimensional manifolds, the gradient of the JS divergence may provide no useful guidance to optimize the generator (Arjovsky and Bottou, 2017; Arjovsky et al., 2017; Mescheder et al., 2018; Roth et al., 2017). For this reason, Arjovsky et al. (2017) propose to instead use the Wasserstein-1 distance, which in theory can provide useful gradient for the generator even if the two distributions have disjoint supports. However, Wasserstein GANs often require the use of a critic function under the 1-Lipschitz constraint, which is difficult to satisfy in practice and hence realized with heuristics such as weight clipping (Arjovsky et al., 2017), gradient penalty (Gulrajani et al., 2017), and spectral normalization (Miyato et al., 2018a).\n\nWhile the divergence minimization perspective has played an important role in motivating the construction of Wasserstein GANs and gradient penalty-based regularizations, cautions should be made on purely relying on it to understand GAN training, due to not only the discrepancy between the divergence in theory and the actual min-max objective function used in practice, but also the potential confounding between different divergences and different training and regularization strategies (Fedus et al., 2018; Mescheder et al., 2018). E.g., Mescheder et al. (2018) have provided a simple example where in theory the Wasserstein GAN is predicted to succeed while the vanilla GAN is predicted to fail, but in practice the Wasserstein GAN with a finite number of discriminator updates per generator update fails to converge while the vanilla GAN with the non-saturating loss can slowly converge. Fedus et al. (2018) provide a rich set of empirical evidence to discourage viewing GANs purely from the perspective of minimizing a specific divergence at each training step and emphasize the important role played by gradient penalties on stabilizing GAN training.\n\nDiffusion models. Due to the use of a forward diffusion chain, the proposed Diffusion-GAN can be related to diffusion-based (or score-based) deep generative models (Ho et al., 2020b; SohlDickstein et al., 2015; Song and Ermon, 2019) that employ both a forward (inference) and a reverse (generative) diffusion chain. These diffusion-based generative models are stable to train and can generate high-fidelity photo-realistic images (Dhariwal and Nichol, 2021; Ho et al., 2020b; Nichol et al., 2021; Ramesh et al., 2022; Song and Ermon, 2019; Song et al., 2021b). However, they are notoriously slow in generation due to the need to traverse the reverse diffusion chain, which involves going through the same U-Net-based generator network hundreds or even thousands of times (Song et al., 2021a). For this reason, a variety of methods have been proposed to reduce the generation cost of diffusion-based generative models (Kong and Ping, 2021; Luhman and Luhman, 2021; Pandey et al., 2022; San-Roman et al., 2021; Song et al., 2021a; Xiao et al., 2021; Zheng et al., 2022).\n\nA key distinction is that Diffusion-GAN needs a reverse diffusion chain during neither training nor generation. More specifically, its generator maps the noise to a generated sample in a single step. Diffusion-GAN can train and generate as quickly as a vanilla GAN does with the same generator size. For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM (Ho et al., 2020b) on an Nvidia 2080 Ti GPU, but would take less than a minute to do so from Diffusion-GAN.\n\nDifferentiable augmentation. As Diffusion-GAN transforms both the data and generated samples before sending them to the discriminator, we can also relate it to differentiable augmentation (Karras et al., 2020a; Zhao et al., 2020) proposed for data-efficient GAN training. Karras et al. (2020a) introduce a stochastic augmentation pipeline with 18 transformations and develop an adaptive mechanism for controlling the augmentation probability. Zhao et al. (2020) propose to use Color + Translation + Cutout as differentiable augmentations for both generated and real images.\n\nWhile providing good empirical results on some datasets, these augmentation methods are developed with domain-specific knowledge and have the risk of leaking augmentation into generation (Karras et al., 2020a). As observed in our experiments, they sometime worsen the results when applied to a new dataset, likely because the risk of augmentation leakage overpowers the benefits of enlarging the training set, which could happen especially if the training set size is already sufficiently large.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBy contrast, Diffusion-GAN uses a differentiable forward diffusion process to stochastically transform the data and can be considered as both a domain-agnostic and a model-agnostic augmentation method. In other words, Diffusion-GAN can be applied to non-image data or even latent features, for which appropriate data augmentation is difficult to be defined, and easily plugged into an existing GAN to improve its generation performance. Moreover, we prove in theory and show in experiments that augmentation leakage is not a concern for Diffusion-GAN. Tran et al. (2021) provide a theoretical analysis for deterministic non-leaking transformation with differentiable and invertible mapping functions. Bora et al. (2018) show similar theorems to us for specific stochastic transformations, such as Gaussian Projection, Convolve+Noise, and stochastic Block-Pixels, while our Theorem 2 includes more satisfying possibilities as discussed in Appendix B.\n\nB PROOF\n\nProof of Theorem 1. For simplicity, let x ∼ Pr, xg ∼ Pg, y ∼ Pr′,t, yg ∼ Pg′,t, at = bt = (1 − ̄αt)σ2. Then,\n\npr′,t(y) =\n\npg′,t(y) =\n\npr(x)N (y; atx, btI)dx\n\npg(x)N (y; atx, btI)dx\n\n(cid:90)\n\nX\n\n(cid:90)\n\nX\n\n√\n\n ̄αt and\n\nz ∼ p(z), xg = gθ(z), yg = atxg + btε, ε ∼ p(ε)\n\nDf (pr′,t(y)||pg′,t(y)) =\n\n(cid:90)\n\nX\n\npg′,t(y)f\n\n(cid:20)\n\n(cid:19)\n\ndy\n\n(cid:19)(cid:21)\n\n(cid:18) pr′,t(y) pg′,t(y) (cid:18) pr′,t(y) pg′,t(y) (cid:18) pr′,t(atgθ(z) + btε) pg′,t(atgθ(z) + btε)\n\n(cid:20)\n\nf\n\nf\n\n(cid:19)(cid:21)\n\n= Ey∼pg′ ,t(y)\n\n= Ez∼p(z),ε∼p(ε)\n\nSince N (y; atx, btI) is assumed to be an isotropic Gaussian distribution, for simplicity, in what follows we show the proof in uni-variate Gaussian, which could be easily extended to multi-variate Gaussian by the production rule. We first show that under mild conditions, the pr′,t(y) and pg′,t(y) are continuous functions over y.\n\nlim ∆y→0\n\npr′,t(y − ∆y) = lim\n\n(cid:90)\n\nX\n\npr(x)N (y − ∆y; atx, bt)dx\n\n∆y→0 (cid:90)\n\n=\n\n=\n\n=\n\nX\n\n(cid:90)\n\nX\n\n(cid:90)\n\npr(x) lim\n\nN (y − ∆y; atx, bt)dx\n\n∆y→0\n\npr(x) lim\n\n∆y→0\n\n1 C1\n\nexp\n\n(cid:18) ((y − ∆y) − atx)2 C2\n\n(cid:19)\n\ndx\n\npr(x)N (y; atx, bt)dx\n\nX = pr′,t(y),\n\nwhere C1 and C2 are constants. Hence, pr′,t(y) is a continuous function defined on y. The proof of continuity for pg′,t(y) is exactly the same proof. Then, given gθ is also a continuous function, it is clear to see that Df (pr′,t(y)||pg′,t(y)) is a continuous function over θ.\n\nNext, we show that Df (pr′,t(y)||pg′,t(y)) is differentiable. By the chain rule, showing Df (pr′,t(y)||pg′,t(y)) to be differentiable is equivalent to show pr′,t(y), pr′,t(y) and f are differentiable. Usually, f is defined with differentiability (Nowozin et al., 2016).\n\n∇θpr′,t(atgθ(z) + btε) = ∇θ\n\n(cid:90)\n\n=\n\nX\n\n(cid:90)\n\nX\n\npr(x)N (atgθ(z) + btε; atx, bt)dx\n\n(cid:18) ||atgθ(z) + btε − atx||2 C2\n\n2\n\n(cid:19)\n\ndx,\n\npr(x)\n\n1 C1\n\n∇θ exp\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n∇θpg′,t(atgθ(z) + btε) = ∇θ\n\n(cid:90)\n\nX\n\npg(x)N (atgθ(z) + btε; atx, bt)dx\n\n= ∇θEz′∼p(z′) [N (atgθ(z) + btε; atgθ(z′), bt)]\n\n= Ez′∼p(z′)\n\n(cid:20) 1 C1\n\n∇θ exp\n\n(cid:18) ||atgθ(z) + btε − atgθ(z′)||2 C2\n\n2\n\n(cid:19)(cid:21)\n\n,\n\nwhere C1 and C2 are constants. Hence, pr′,t(y) and pr′,t(y) are differentiable, which concludes the proof.\n\nProof of Theorem 2. We have p(y) = (cid:82) p(x)q(y | x)dx and pg(y) = (cid:82) pg(x)q(y | x)dx. ⇐ If p(x) = pg(x), then p(y) = pg(y) ⇒ Let y ∼ p(y) and yg ∼ pg(y). Given the assumption on q(y | x), we have\n\ny = f (x) + g(ε), x ∼ p(x), ε ∼ p(ε) yg = f (xg) + g(εg), xg ∼ pg(x), εg ∼ p(ε).\n\nSince f and g are one-to-one mapping functions, f (x) and g(ε) are identifiable, which indicates f (x) D= f (xg) ⇒ x D= xg. By the property of moment-generating functions (MGF), given f (x) is independent with g(ε), we have for ∀s\n\nMy(s) = Mf (x)(s) · Mg(ε)(s) Myg (s) = Mf (xg)(s) · Mg(εg)(s).\n\nwhere My(s) = Ey∼p(y)[esT y] denotes the MGF of random variable y and the others follow the same form. By the moment-generating function uniqueness theorem, given y D= yg and g(ε) D= g(εg), we have My(s) = Myg (s) and Mg(ε)(s) = Mg(εg)(s) for ∀s. Then, we could obtain Mf (x) = Mf (xg) for ∀s. Thus, Mf (x) = Mf (xg) ⇒ f (x) D= f (xg) ⇒ p(x) = p(xg), which concludes the proof.\n\nDiscussion. Next, we discuss which q(y | x) fits the assumption we made on it. We follow the discussion of reparameterization of distributions as used in Kingma and Welling (2014). Three basic approaches are:\n\n1. Tractable inverse CDF. In this case, let ε ∼ U(0, I), and ψ(ε, y, x) be the inverse CDF of q(y | x). From ψ(ε, y, x), if y = f (x) + g(ε), for example, y ∼ Cauchy(x, γ) and y ∼ Logistic(x, s), then Theorem 2 holds.\n\n2. Analogous to the Gaussian example, y ∼ N (x, σ2I) ⇒ y = x + σ · ε, ε ∼ N (0, I). For any “location-scale” family of distributions we can choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable ε, and let g(.) = location + scale · ε. Examples: Laplace, Elliptical, Student’s t, Logistic, Uniform, Triangular, and Gaussian distributions.\n\n3. Implicit distributions. q(y | x) could be modeled by neural networks, which implies y =\n\nf (x) + g(ε), ε ∼ p(ε), where f and g are one-to-one nonlinear transformations.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC DERIVATIONS\n\nDerivation of equality in JSD\n\nJSD(p(y, t), pg(y, t))\n\n=\n\n=\n\n=\n\n1 2\n\n1 2\n\n1 2\n\n(cid:20)\n\nDKL\n\np(y, t)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:20)\n\nEy,t∼p(y,t)\n\nlog\n\np(y, t) + pg(y, t) 2\n\n(cid:21)\n\n2 · p(y, t) p(y, t) + pg(y, t)\n\n+\n\n(cid:21)\n\n1 2\n\n+\n\n1 2\n\n(cid:20)\n\nDKL\n\npg(y, t)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\np(y, t) + pg(y, t) 2\n\n(cid:21)\n\nEy,t∼pg(y,t)\n\nlog\n\n2 · pg(y, t) p(y, t) + pg(y, t)\n\n(cid:21)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:20)\n\nEt∼pπ(t),y∼p(y | t)\n\nlog\n\n(cid:20)\n\n2 · p(y | t)pπ(t) p(y | t)pπ(t) + pg(y | t)pπ(t)\n\n(cid:21)\n\nEt∼pπ(t),y∼pg(y | t)\n\nlog\n\n(cid:20)\n\n+\n\n1 2\n\n2 · pg(y | t)pπ(t) p(y | t)pπ(t) + pg(y | t)pπ(t)\n\n(cid:21)\n\n= Et∼pπ(t)\n\n(cid:20)\n\nEy∼p(y | t)\n\nlog\n\n(cid:20) 1 2\n\n2 · p(y | t) p(y | t) + pg(y | t)\n\n(cid:21)\n\n+\n\n1 2\n\n(cid:20)\n\nEy∼pg(y | t)\n\nlog\n\n2 · pg(y | t) p(y | t) + pg(y | t)\n\n(cid:21)(cid:21)\n\n= Et∼pπ(t)[JSD(p(y | t), pg(y | t))].\n\nD DETAILS OF TOY EXAMPLE\n\nHere, we provide the detailed analysis of the JS divergence toy example.\n\nNotation. Let X be a compact metric set (such as the space of images [0, 1]d) and Prob(X ) denote the space of probability measures defined on X . Let Pr be the target data distribution and Pg 1 be the generator distribution. The JSD between the two distributions Pr, Pg ∈ Prob(X ) is defined as:\n\nDJS(Pr||Pg) =\n\n1 2\n\nDKL(Pr||Pm) +\n\n1 2\n\nDKL(Pg||Pm),\n\n(7)\n\nwhere Pm is the mixture (Pr + Pg)/2 and DKL denotes the Kullback-Leibler divergence, i.e., DKL(Pr||Pg) = (cid:82) pθ(x) )dx. More generally, the f -divergence (Nowozin et al., 2016) between Pr and Pg is defined as:\n\nX pr(x) log( pr(x)\n\nDf (Pr||Pg) =\n\n(cid:90)\n\nX\n\npg(x)f\n\n(cid:19)\n\n(cid:18) pr(x) pg(x)\n\ndx,\n\n(8)\n\nwhere the generator function f : R+ → R is a convex and lower-semicontinuous function satisfying f (1) = 0. We refer to Nowozin et al. (2016) for more details.\n\nWe recall the typical example introduced in Arjovsky and Bottou (2017) and follow the notations.\n\nExample. Let Z ∼ U [0, 1] be the uniform distribution on the unit interval. Let X ∼ Pr be the distribution of (0, Z) ∈ R2, which contains a 0 on the x-axis and a random variable Z on the y-axis. Let Xg ∼ Pg be the distribution of (θ, Z) ∈ R2, where θ is a single real parameter. In this case, the DJS(Pr||Pg) is not continuous,\n\nDJS(Pr||Pg) =\n\n(cid:26)0\n\nif θ = 0, log 2 if θ ̸= 0.\n\n1For notation simplicity, g and G both denote the generator network in GANs in this paper.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: We show the data distribution and DJS(Pr||Pg).\n\nwhich can not provide a usable gradient for training. The derivation is as follows:\n\nEx∼pr(x)\n\n(cid:20)\n\nlog\n\n2 · pr(x) pr(x) + pg(x)\n\n(cid:21)\n\n+\n\n1 2\n\nEy∼pg(y)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\n2 · pg(y) pr(y) + pg(y)\n\nEx1=0,x2∼U [0,1]\n\n(cid:20)\n\nlog\n\n2 · 1[x1 = 0] · U (x2) 1[x1 = 0] · U (x2) + 1[x1 = θ] · U (x2)\n\n(cid:21)\n\nEy1=θ,y2∼U [0,1]\n\n(cid:20)\n\nlog\n\nlog\n\n2 · 1[x1 = 0] 1[x1 = 0] + 1[x1 = θ]\n\n2 · 1[y1 = θ] · U (y2) 1[y1 = 0] · U (y2) + 1[y1 = θ] · U (y2) 1\n2\n\n(cid:21) (cid:12) (cid:12) (cid:12)x1 = 0\n\n2 · 1[y1 = θ] 1[y1 = 0] + 1[y1 = θ]\n\nlog\n\n+\n\n(cid:20)\n\n(cid:21)\n\n(cid:21)\n\n(cid:12) (cid:12) (cid:12)y1 = θ\n\nDJS(Pr||Pg) =\n\n=\n\n=\n\n=\n\n1 2\n\n1 2\n\n+\n\n1 2\n(cid:20)\n\n1 2\n(cid:26)0\n\nif θ = 0, log 2 if θ ̸= 0.\n\nAlthough this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero (Arjovsky and Bottou, 2017). This happens to be the case when two low dimensional manifolds intersect in general position (Arjovsky and Bottou, 2017). To avoid the potential issue caused by having non-overlapping distribution supports, a common remedy is to use Wasserstein-1 distance which in theory can still provide usable gradient (Arjovsky and Bottou, 2017; Arjovsky et al., 2017). In this case, the Wasserstein-1 distance is |θ|.\n\nDiffusion-based noise injection In general, with our diffusion noise injected, we could have,\n\npr′,t =\n\npg′,t =\n\n(cid:90)\n\nX\n\n(cid:90)\n\nX\n\n√\n\npr(x)N (y;\n\n ̄αtx, (1 − ̄αt)σ2I)dx\n\n√\n\npg(x)N (y;\n\n ̄αtx, (1 − ̄αt)σ2I)dx\n\nDJS(pr′,t||pg′,t) =\n\n1 2\n\nEpr′ ,t\n\n(cid:20)\n\nlog\n\n2pr′,t pr′,t + pg′,t\n\n(cid:21)\n\n+\n\n1 2\n\n(cid:20)\n\nlog\n\nEpg′ ,t\n\n2pg′,t pr′,t + pg′,t\n\n(cid:21)\n\nFor the previous example, we have Y ′\n\nt and Y ′\n\ng,t such that,\n\nY ′\n\nt = (y1, y2) ∼ pr′,t = N (y1 | 0, bt)f (y2), Y ′\n\ng,t = (yg,1, yg,2) ∼ pg′,t = N (yg,1 | atθ, bt)f (yg,2),\n\nwhere f (·) = (cid:82) 1 supports of Y ′ pending on t, as shown in Figure 2. As t increases, the high density region of Y ′\n\n ̄αt and (1 − ̄αt)σ2. The g,t are both the whole metric space R2 and they overlap with each other deg,t get closer\n\n0 N (· | atZ, bt)U (Z)dZ, at and bt are abbreviations for\n\nt and Y ′\n\nt and Y ′\n\n√\n\n18\n\n024x0.00.51.0yt=0024x01yt=20002x101yt=40002x101yt=5001012x101yt=600101x101yt=80010.07.55.02.50.02.55.07.510.00246DJS(q(y|t)||qg(y|t))t=0t=200t=400t=500t=600t=80020246x0.000.250.500.751.00Optimal Discriminator Value D*(x)t=0t=200t=400t=500t=600t=800Published as a conference paper at ICLR 2023\n\nsince the weight at is decreasing towards 0. Then, we derive the JS divergence,\n\nDJS(pr′,t||pg′,t)\n\n=\n\n=\n\n1 2\n\n+\n\n1 2\n\n+\n\nEy1∼N (y1 | 0,bt),y2∼f (y2)\n\n(cid:20)\n\nlog\n\n2 · N (y1 | 0, bt)f (y2) N (y1 | 0, bt)f (y2) + N (y1 | atθ, bt)f (y2)\n\n(cid:21)\n\n1 2\n\nEyg,1∼N (yg,1 | 0,bt),yg,2∼f (yg,2)\n\n(cid:20)\n\nlog\n\n2 · N (yg,1 | atθ, bt)f (yg,2) N (yg,1 | 0, bt)f (yg,2) + N (yg,1 | atθ, bt)f (yg,2)\n\n(cid:21)\n\nEy1∼N (0,bt)\n\n(cid:20)\n\nlog\n\n1 2\n\nEyg,1∼N (atθ,bt)\n\n(cid:21)\n\n2 · N (y1 | 0, bt) N (y1 | 0, bt) + N (y1 | atθ, bt) (cid:20)\n\nlog\n\n2 · N (yg,1 | atθ, bt) N (yg,1 | 0, bt) + N (yg,1 | atθ, bt)\n\n(cid:21)\n\nwhich is clearly continuous and differentiable.\n\nWe show this DJS(pr′,t||pg′,t) with respect to increasing t values and a θ grid in the second row of Figure 2. As shown in the left panel, the black line with t = 0 shows the origianl JSD, which is not even continuous, while as the diffusion level t increments, the lines become smoother and flatter. It is clear to see that these smooth curves provide good learning signals for θ. Recall that the Wasserstein-1 distance is |θ| in this case. Meanwhile, we could observe with an intense diffusion, e.g., t = 800, the curve becomes flatter, which indicates smaller gradients and a much slower learning process. This motivates us that an adaptive diffusion could provide different level of gradient smoothness and is possibly better for training. The right panel shows the optimal discriminator outputs over the space X . With diffusion, the optimal discriminator is well defined over the space and the gradient is smooth, while without diffusion the optimal discriminator is only valid on two star points. Interestingly, we find that smaller t drives the optimal discriminator to become more assertive while larger t makes discriminator become more neutral. The diffusion here works like a scale to balance the power of the discriminator.\n\nE DATASET DESCRIPTIONS\n\nThe CIFAR-10 dataset consists of 50k 32 × 32 training images in 10 categories. The STL-10 dataset originated from ImageNet (Deng et al., 2009) consists of 100k unlabeled images in 10 categories, and we resize them to 64 × 64 resolution. For LSUN datasets, we sample 200k images from LSUNBedroom, use the whole 125k images from LSUN-Church, and resize them to 256 × 256 resolution for training. The AFHQ datasets includes around 5k 512 × 512 images per category for dogs, cats, and wild life; we train a separate network for each of them. The FFHQ contains 70k images crawled from Flickr at 1024 × 1024 resolution and we use all of them for training.\n\nF ALGORITHM\n\nWe provide the Diffusion-GAN algorithm in Algorithm 1.\n\nG HYPERPARAMETERS\n\nDiffusion-GAN is built on GAN backbones, so we keep the learning hyperparameters of the original GAN backbones untouched. Diffusion-GAN introduces four new hyperparameters: noise standard deviation σ, Tmax, T increasing threshold dtarget, and t sampling distribution pπ.\n\nThe σ is fixed as 0.05 for images (pixel values rescaled to [-1 ,1]) in all our experiments and it shows good performance. Tmax could be fixed as 500 or 1000, which depends on the diversity of the dataset. We recommend a large Tmax for diverse datasets. dtarget is usually fixed as 0.6, which does not influence much about the performance. pπ has two choices, ‘uniform’ and ‘priority’. Generally, (σ = 0.05, Tmax = 500, dtarget = 0.6, pπ = ‘uniform’) is a good starting point for a new dataset.\n\nIn our experiment, we find StyleGAN2-based models are not sensitive to the values of dtarget, so we set dtarget = 0.6 for them across all dataset, only except that we set dtarget = 0.8 for FFHQ\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Diffusion-GAN\n\nwhile i ≤ number of training iterations do\n\nStep I: Update discriminator\n\n• •\n• •\n• •\n\nSample minibatch of m noise samples {z1, z2, . . . , zm} ∼ pz(z). Obtain generated samples {xg,1, xg,2, . . . , xg,m} by xg = G(z). Sample minibatch of m data examples {x1, x2, . . . , xm} ∼ p(x). Sample {t1, t2, . . . , tm} from tepl list uniformly with replacement. For j ∈ {1, 2, . . . , m}, sample yj ∼ q(yj|xj, tj) and yg,j ∼ q(yg,j|xg,j, tj) Update discriminator by maximizing Equation (3).\n\nStep II: Update generator\n\n• •\n• •\n•\n\nSample minibatch of m noise samples {z1, z2, . . . , zm} ∼ pz(z) Obtain generated samples {xg,1, xg,2, . . . , xg,m} by xg = G(z). Sample {t1, t2, . . . , tm} from tepl list with replacement. For j ∈ {1, 2, . . . , m}, sample yg,j ∼ q(yg,j|xg,j, tj) Update generator by minimizing Equation (3).\n\nStep III: Update diffusion if i mod 4 == 0 then\n\nUpdate T by Equation (5) Sample tepl = [0, . . . , 0, t1, . . . , t32], where tk ∼ pπ for k ∈ {1, . . . , 32}. pπ is in Equation (6). {tepl has 64 dimensions.}\n\nend if end while\n\nDatasets\n\nCIFAR-10 (32 × 32, 50k images) STL-10 (64 × 64, 100k images) LSUN-Church (256 × 256, 120k images) LSUN-Bedroom (256 × 256, 200k images)\n\nrd\n\n0.45 0.6 0.2 0.2\n\nTable 4: dtarget for Diffusion ProjectedGAN\n\n(dtarget = 0.8 for FFHQ is slightly better than 0.6 in FID). We report dtarget of Diffusion ProjectedGAN for our experiments in Table 4. We also evaluated two t sampling distribution pπ, [‘priority’, ‘uniform’], defined in Equation (6). In most cases, ‘priority’ works slightly better, while in some cases, such as FFHQ, ‘uniform’ is better. Overall, we didn’t modify anything in the model architectures and training hyperparameters, such as learning rate and batch size. The forward diffusion configuration and model training configurations are as follows.\n\nDiffusion config. For our diffusion-based noise injection, we set up a linearly increasing schedule for βt, where t ∈ {1, 2, . . . , T }. For pixel level injection in StyleGAN2, we follow Ho et al. (2020b) and set β0 = 0.0001 and βT = 0.02. We adaptively modify T ranging from Tmin = 5 to Tmax = 1000. The image pixels are usually rescaled to [−1, 1] so we set the Guassian noise standard deviation σ = 0.05. For feature level injection in Diffusion ProjectedGAN, we set β0 = 0.0001, βT = 0.01, Tmin = 5, Tmax = 500, and σ = 0.5. We list all these values in Table 5\n\nModel config. For StyleGAN2-based models, we borrow the config settings provided by Karras et al. (2020a), which include [‘auto’, ‘stylegan2’, ‘cifar’, ‘paper256’, ‘paper512’, ‘stylegan2’]. We create the ‘stl’ config based on ‘cifar’ with a small modification that we change the gamma term to be 0.01. For ProjectedGAN models, we use the recommended default config (Sauer et al., 2021), which is based on FastGAN (Liu et al., 2020). We report the config settings used for our experiments in Table 6.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nDiffusion config for pixel, priority Diffusion config for pixel, uniform β0 = 0.0001, βT = 0.02, Tmin = 5, Tmax = 500, σ = 0.05\n\nβ0 = 0.0001, βT = 0.02, Tmin = 5, Tmax = 1000, σ = 0.05\n\nDiffusion config for feature\n\nβ0 = 0.0001, βT = 0.01, Tmin = 5, Tmax = 500, σ = 0.5\n\nTable 5: Diffusion config.\n\nDataset\n\nModels\n\nCIFAR-10 (32 × 32)\n\nSTL-10 (64 × 64)\n\nLSUN-Bedroom (256 × 256)\n\nLSUN-Church (256 × 256)\n\nAFHQ-Cat/Dog/Wild (512 × 512)\n\nFFHQ (1024 × 1024)\n\nStyleGAN2 Diffusion StyleGAN2 ProjectedGAN Diffusion ProjectedGAN\n\nStyleGAN2 Diffusion StyleGAN2 ProjectedGAN Diffusion ProjectedGAN\n\nStyleGAN2 Diffusion StyleGAN2 ProjectedGAN Diffusion ProjectedGAN\n\nStyleGAN2 Diffusion StyleGAN2 ProjectedGAN Diffusion ProjectedGAN\n\nStyleGAN2 Diffusion StyleGAN2 InsGen Diffusion InsGen\n\nStyleGAN2 Diffusion StyleGAN2 InsGen Diffusion InsGen\n\nConfig\n\ncifar cifar default default\n\nstl stl default default\n\npaper256 paper256 default default\n\npaper256 paper256 default default\n\npaper512 paper512 default paper512\n\nstylegan2 stylegan2 default stylegan2\n\nSpecification\n\n- diffusion-pixel, dtarget = 0.6, ‘priority’ diffusion-feature diffusion-feature\n\n- diffusion-pixel, dtarget = 0.6, ‘priority’ diffusion-feature diffusion-feature\n\n- diffusion-pixel, dtarget = 0.6, ‘priority’ diffusion-feature diffusion-feature\n\n- diffusion-pixel, dtarget = 0.6, ‘priority’ diffusion-feature diffusion-feature\n\n- diffusion-pixel, dtarget = 0.6, ‘priority’ -\ndiffusion-pixel, dtarget = 0.6, ‘uniform’\n\n- diffusion-pixel, dtarget = 0.8, ‘uniform’ -\ndiffusion-pixel, dtarget = 0.6, ‘uniform’\n\nTable 6: The config setting of StyleGAN2-based models and ProjectedGAN-based models. For StyleGAN2based models, we borrow the config settings provided by Karras et al. (2020a), which includes [‘auto’, ‘stylegan2’, ‘cifar’, ‘paper256’, ‘paper512’, ‘paper1024’]. We create the ‘stl’ config based on ’cifar’ with small modifications that we change the gamma term to be 0.01. For ProjectedGAN models, we use the recommended default config (Sauer et al., 2021), which is based on FastGAN.\n\nH IMPLEMENTATION DETAILS\n\nWe implement an additional diffusion sampling pipeline, where the diffusion configurations are set in Appendix G. The T in the forward diffusion process is adaptively adjusted and clipped to [Tmin, Tmax]. As illustrated in Algorithm 1, at each update step, we sample t from tepl for each data point x, and then use the analytic Gaussian distribution at diffusion step t to sample y. Next, we use y and t instead of x for optimization.\n\nDiffusion StyleGAN2. We inherit all the network architectures from StyleGAN2 implemented by Karras et al. (2020a). We modify the original mapping network, which is there for label conditioning and unused for unconditional image generation tasks, inside the discriminator to inject t. Specifically, we change the original input of mapping network, the class label c, to our discrete value timestep t. Then, we train the generator and discriminator with diffused samples y and t.\n\nDiffuson ProjectedGAN. To simplify the implementation and minimize the modifications to ProjectedGAN, we construct the discriminator as Dφ(y), where t is ignored. Our method is plugged in as a data augmentation method. The only change in the optimization stage is that the discriminator is fed with diffused images y instead of original images x.\n\nDiffuson InsGen. To simplify the implementation and minimize the modifications to InsGen, we keep their contrastive learning part untouched. We modify the original discriminator network\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Ablation study on the T adaptiveness.\n\nto inject t similarly to Diffusion StyleGAN2. Then, we train the generator and discriminator with diffused samples y and t.\n\nI ABLATION ON THE MIXING PROCEDURE AND T ADAPTIVENESS\n\nNote the mixing procedure described in Equation (6), referred to as “priority mixing” in what follows, is designed based on our intuition. Here we conduct an ablation study on the mixing procedure by comparing the priority mixing with uniform mixing on three representative datasets. We report in Table 7 the FID results, which suggest that uniform mixing could work better than priority mixing in some dataset, and hence Diffusion-GAN may be further improved by optimizing its mixing procedure according to the training data. While optimizing the mixing procedure is beyond the focus of this paper, it is worth further investigation in future studies.\n\nTable 7: Ablation study on the mixing procedure. “Priority Mixing” refers to the mixing procedure in Equation (6) and “Uniform Mixing” refers to sample t uniformly at random.\n\nCIFAR-10\n\nSTL-10\n\nFFHQ\n\nPriority Mixing Uniform Mixing\n\n3.19 3.44\n\n11.43 11.75\n\n3.22 2.83\n\nWe further conduct ablation study on whether the T needs to be adaptively adjusted. As shown in Figure 7, we observe with adaptive diffusion strategy, the training curves of FIDs converge faster and reach lower final FIDs.\n\nJ MORE GAN VARIANTS\n\nTo further validate our noise injection via diffusion-based mixtures, we add our diffusion-based training into two more representative GAN variants: DCGAN (Radford et al., 2015) and SNGAN (Miyato et al., 2018b), which have quite different GAN architectures compared to StyleGAN2. We provide the FIDs for CIFAR-10 in Table 8. We observe that both Diffusion-DCGAN and DiffusionSNGAN clearly outperform their corresponding baseline GANs.\n\nTable 8: FIDs on CIFAR-10 for DCGAN, Diffusion-DCGAN, SNGAN, and Diffusion-SNGAN.\n\nDCGAN (Radford et al., 2015) Diffusion-DCGAN SNGAN (Miyato et al., 2018b) Diffusion-SNGAN\n\nCIFAR-10\n\n28.65\n\n24.67\n\n20.76\n\n17.23\n\n22\n\n5M10M20M30M40M50MTraining Progress (# million real images)510152550100FIDAblation on the adaptive schedule474849503.54.04.5uniform-adaTuniform-no_adaTpriority-adaTpriority-no_adaTPublished as a conference paper at ICLR 2023\n\nMethod DDPM (Ho et al., 2020a) DDIM (Song et al., 2020) Denoising Diffusion GAN (Xiao et al., 2021) StyleGAN2 (Karras et al., 2020a) StyleGAN2 + DiffAug (Zhao et al., 2020) StyleGAN2 + ADA (Karras et al., 2020a) Diffusion StyleGAN2\n\nIS ↑ 9.46 8.78 9.63 9.18 9.40 9.83 9.94\n\nFID ↓ Recall ↑ NFE ↓ 1000 0.57 3.21 50 0.53 4.67 4\n0.57 3.75 1\n0.41 8.32 1\n0.42 5.79 2.92 1\n0.49 0.58 1\n3.19\n\nTable 9: Inception Score for CIFAR-10. For sampling time, we use the number of function evaluations (NFE).\n\nK INCEPTION SCORE FOR CIFAR-10\n\nWe report the Inception Score (IS) (Salimans et al., 2016) of Diffusion StyleGAN2 for CIFAR-10 dataset in Table 9 and also include other state-of-the-art GANs and diffusion models as baselines. Note CIFAR-10 is a well-known dataset and tested by almost all baselines, so we pick CIFAR-10 here and we reference the reported IS values from their original papers for a fair comparison.\n\nL MORE GENERATED IMAGES\n\nWe provide more randomly generated images for LSUN-Bedroom, LSUN-Church, AFHQ, and FFHQ datasets in Figure 8, Figure 9, and Figure 10.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: More generated images for LSUN-Bedroom (FID 1.43, Recall 0.58) and LSUN-Church (FID 1.85, Recall 0.65) from Diffusion ProjectedGAN.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: More generated images for AFHQ-Cat (FID 2.40), AFHQ-Dog (FID 4.83) and AFHQ-Wild (FID 1.51) from Diffusion InsGen.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: More generated images for FFHQ from Diffusion StyleGAN2 (FID 3.71, Recall 0.43).\n\n26",
  "translations": [
    "# Summary Of The Paper\n\nThe authors propose a diffusion-based approach to stabilise the learning process of GANs in a instance noise fashion.\n\n# Strength And Weaknesses\n\nStrength:\n- The abstract and introduction give a nice overview and motivation for the problem.\n- The method is supported by a theoretical analysis.\n- The results are good and evaluation is reasonable.\n- The objective is still differentiable wrt the generator using the classical rewritting of the perturbed samples.\n\nWeaknesses:\n- 25-gaussians example (Fig. 5): the purpose of the figure is to exhibit the fact that the propose method is not prone to mode collapse but I am curious to know if the proposed method has shown some mesurable improvement wrt SotA methods on this \"toy example\"? \n- The authors did an ablation study on the adaptive diffusion process. But, reading Section 3.3, it feels like the adaptative diffusion strategy is key to the method whereas the ablation study does not clearly support it.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall the writting is good, the proposed method looks sound and in line with previously proposed methods.\n\n# Summary Of The Review\n\nOverall I think this is a good paper tackling a well motivated task.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper presents Diffusion-GAN, a novel framework designed to stabilize and enhance the training of Generative Adversarial Networks (GANs) by employing a forward diffusion process to introduce Gaussian-mixture distributed instance noise. The methodology includes an adaptive diffusion process, a timestep-dependent discriminator that distinguishes between diffused real and generated samples, and a generator that is updated based on feedback from the discriminator throughout the diffusion chain. Empirical results demonstrate that Diffusion-GAN outperforms established GAN baselines such as StyleGAN2 and Projected GAN on various datasets, showing significant improvements in image quality, stability, and data efficiency.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to integrating diffusion processes into GAN training, which addresses common issues of instability and non-convergence. The theoretical analysis provides a solid foundation for the framework, highlighting the continuity and differentiability of the objective function, which is critical for consistent training. However, a potential weakness could be the complexity of implementation, as the adaptive nature of the diffusion process might present challenges in practice. Additionally, while the empirical results are compelling, further exploration of the model's performance across a broader range of tasks and datasets would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing is concise, with sufficient detail provided for understanding the diffusion process and its implications for GAN training. The novelty of the approach is significant, particularly in how it leverages the adaptive diffusion process. However, the reproducibility of results may depend on the availability of code and detailed implementation guidelines, which are not explicitly mentioned in the summary.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of GANs through the introduction of Diffusion-GAN, which effectively addresses stability and quality issues in GAN training. The theoretical insights and empirical results are promising, though practical implementation may pose challenges. Further validation across diverse tasks would enhance the robustness of the findings.\n\n# Correctness\n4 - The theoretical and empirical claims are well-supported, though some practical aspects may require additional clarification.\n\n# Technical Novelty And Significance\n5 - The integration of a diffusion process into GAN training represents a significant advancement, introducing a new paradigm for addressing long-standing issues in the field.\n\n# Empirical Novelty And Significance\n4 - While the empirical results are strong and demonstrate clear advantages over existing models, broader validation across a wider range of datasets and tasks would enhance the significance of the findings.",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel framework aimed at addressing the issues of instability, non-convergence, and mode collapse commonly encountered in Generative Adversarial Networks (GANs). The methodology involves an adaptive forward diffusion process that generates instance noise, enhancing the training stability of GANs. The main contributions include the integration of a noise-to-data ratio that varies over diffusion timesteps, a timestep-dependent discriminator, and a generator that leverages feedback from the discriminator to improve sample quality. Experimental results demonstrate that Diffusion-GAN significantly outperforms state-of-the-art GANs across various datasets in terms of image fidelity and diversity.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to improving GAN stability and image quality through adaptive noise injection via diffusion, backed by a solid theoretical framework. The empirical results substantiate its claims, showing consistent improvements over established GAN variants. However, the paper does present limitations such as the added complexity of the adaptive diffusion process, which may complicate implementation and optimization. Additionally, while the results are promising, further validation across a broader range of applications and datasets would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and experimental results. The theoretical foundations are robust, providing clarity on the adaptive strategies employed. However, the complexity of the proposed method might pose challenges for reproducibility, particularly in terms of tuning the hyperparameters and adapting the diffusion process to different applications. Nonetheless, the quality of the experimental design and the thoroughness of the evaluations contribute positively to the overall clarity and quality of the work.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in GAN training through the introduction of Diffusion-GAN, which effectively leverages adaptive noise to enhance stability and image generation quality. While the contributions are noteworthy, the complexity and potential challenges in reproducibility warrant caution. Further validation in diverse settings would be beneficial.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Diffusion-GAN**, a novel framework for training Generative Adversarial Networks (GANs) that addresses issues of instability and mode collapse through the incorporation of a forward diffusion process. The methodology consists of generating Gaussian-mixture distributed instance noise, employing an adaptive diffusion process, and utilizing a timestep-dependent discriminator to differentiate between diffused real and generated data. Empirical results demonstrate that Diffusion-GAN significantly outperforms existing GAN baselines in terms of image quality and stability across various datasets, including CIFAR-10 and LSUN.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to tackling the long-standing challenges of GAN training, particularly through the use of a differentiable diffusion process which stabilizes training and enhances sample diversity. The theoretical analysis provided, including theorems that confirm valid gradient propagation and noise injection methods, adds robustness to the proposed methodology. However, the paper could be strengthened by a more comprehensive exploration of the computational complexity involved with the adaptive diffusion process and its practical implications in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and experimental results. The explanations of the diffusion process and the adversarial training framework are accessible, making it easy to understand the innovations introduced. The results are reproducible, as the authors provide sufficient details on datasets and evaluation protocols. However, the paper could benefit from clearer visualizations of the performance improvements and more extensive discussions around the limitations of the proposed approach.\n\n# Summary Of The Review\nOverall, Diffusion-GAN presents a significant advancement in the field of generative modeling by effectively addressing key challenges in GAN training. The novel use of diffusion processes provides both theoretical and empirical support for its effectiveness, although some aspects of computational complexity and limitations could be further elaborated.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"Diffusion-GAN: Training GANs with Diffusion\" presents a novel approach to stabilizing GAN training by integrating a diffusion process for instance noise injection. The authors propose a framework that employs a timestep-dependent strategy for the discriminator to guide the generator, providing both theoretical analysis and empirical results. The findings demonstrate that Diffusion-GAN achieves improved stability and higher quality image generation compared to strong GAN baselines, with notable advancements in data efficiency under limited data conditions.\n\n# Strengths And Weaknesses\nThe paper showcases several strengths, including its innovative methodology that introduces a fresh perspective on GAN training, along with a solid theoretical foundation that underpins the proposed approach. The empirical results are compelling, indicating superior performance in generating photorealistic images. However, the paper also has limitations: the theoretical aspects may be difficult for practitioners to apply without a strong mathematical background, and the empirical validation is primarily restricted to specific image datasets. Consequently, the generalizability of the findings across various data types remains uncertain. Additionally, while the method claims improved data efficiency, further exploration is needed to assess its performance in extremely limited data scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, and the methodology is articulated clearly, allowing for a reasonable understanding of the novel concepts introduced. The quality of the empirical evaluations is high, with comprehensive quantitative metrics reported. The detailed implementation information enhances the reproducibility of results, which is crucial for advancing research in this area. However, the reliance on specific architectures may hinder broader applicability, and the theoretical complexity could limit the accessibility of the method for some practitioners.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of generative models by introducing Diffusion-GAN, a novel framework that leverages diffusion processes to improve GAN training. While the methodology and empirical results are promising, further validation across diverse datasets and additional qualitative assessments would strengthen the paper's claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces **Diffusion-GAN**, a novel Generative Adversarial Network framework that employs a **forward diffusion chain** to introduce instance noise, aiming to enhance both training stability and image quality. The approach features three key components: an **adaptive diffusion process**, a **timestep-dependent discriminator**, and a **generator** that collaborates through the diffusion mechanism. The authors claim that Diffusion-GAN achieves superior performance in generating high-resolution images, demonstrating improved stability and efficiency compared to existing state-of-the-art GAN methodologies.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Mechanism**: The adaptive diffusion mechanism allows for a dynamic adjustment of noise, which is a significant improvement over static noise injection used in traditional GANs.\n2. **Enhanced Discriminator Performance**: The timestep-dependent discriminator leverages variability in noise, enabling it to better differentiate between real and generated samples.\n3. **Theoretical Foundations**: The authors provide a solid theoretical basis for their methodology, ensuring continuity and differentiability in the objective function, which is crucial for effective gradient descent.\n4. **Extensive Empirical Validation**: The results, backed by experiments on diverse datasets, convincingly show the advantages of Diffusion-GAN over numerous strong baselines.\n\n**Weaknesses:**\n1. **Scalability Concerns**: The paper does not fully address how well the adaptive diffusion approach scales with larger and more complex datasets.\n2. **Computational Overhead**: There is a lack of in-depth analysis regarding the computational cost associated with the diffusion process and its impact on overall training time, which could be a potential limitation for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it accessible for readers familiar with GANs. The quality of the writing supports clear communication of the complex ideas presented. The novelty of using a diffusion process for noise injection in GANs stands out, providing a fresh perspective in the field. The reproducibility of the results is enhanced by the extensive experimental setup and comparison against multiple baselines, although further details on computational costs would bolster this aspect.\n\n# Summary Of The Review\nThe proposed Diffusion-GAN framework represents a significant advancement in GAN training methodologies through innovative noise injection strategies and adaptive training mechanisms. The theoretical and empirical results substantiate the claims of improved generative performance, making this work a noteworthy contribution to the field of generative modeling.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Diffusion-GAN: Training GANs with Diffusion\" introduces a novel framework for adversarial training by integrating a forward diffusion process to enhance the stability and performance of Generative Adversarial Networks (GANs). The authors identify key issues in traditional GAN training, such as instability and mode collapse, and propose a method that generates Gaussian-mixture distributed instance noise through diffusion, which improves the training process. Additionally, they introduce a timestep-dependent discriminator that adapts based on the noise injected during training. The paper includes theoretical guarantees on the min-max objective function, ensuring meaningful gradients for the generator, and presents empirical results showing that Diffusion-GAN outperforms existing GAN baselines in terms of stability and image quality across various datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative approach of integrating a diffusion process into adversarial training, which directly addresses common issues faced in GANs. The robust theoretical foundation enhances the credibility of the proposed methods, and the comprehensive experiments clearly demonstrate the advantages of Diffusion-GAN over competing models. However, the complexity of implementing a diffusion process may pose challenges for practitioners seeking simpler methods. Additionally, the generalizability of the approach beyond image datasets remains uncertain, indicating a gap in applicability to other domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its ideas, making it accessible to readers. The theoretical analysis is presented in a manner that adds quality to the work, while the empirical validation provides reproducible results. The novelty of the approach is significant, as it introduces a new dimension to adversarial training that has not been extensively explored in prior literature. However, the implementation complexity could hinder reproducibility for those less familiar with diffusion processes.\n\n# Summary Of The Review\nOverall, the paper presents a substantial advancement in adversarial training methodologies through the introduction of a diffusion process for noise generation in GANs. Its theoretical and empirical contributions are noteworthy, although the complexity of the implementation may limit accessibility for some practitioners. The work is valuable for advancing the understanding and application of generative models.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel framework designed to address the inherent instability and mode collapse prevalent in Generative Adversarial Networks (GANs). By employing a forward diffusion chain to generate instance noise, the methodology posits that training GANs can be simplified to a level comparable to linear regression. The authors claim substantial improvements in training stability, image realism, and efficiency, supported by extensive empirical results that demonstrate superior performance metrics such as Fréchet Inception Distance (FID) and Recall scores. The theoretical foundation of the method is also robust, asserting guaranteed useful gradients for the generator and suggesting universal applicability across various GAN architectures and datasets.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its ambitious goal of redefining GAN training through the integration of diffusion processes, along with strong empirical results that suggest significant performance improvements over existing models. The theoretical justification provided is thorough and positions the method as a potential game-changer in the field of generative modeling. However, the paper may overstate its claims regarding the elimination of all previous limitations associated with GANs, as the complexities of generative modeling might still present challenges that are not fully addressed. Additionally, the universality claim requires more rigorous validation across diverse datasets and applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with varying levels of expertise. The quality of writing is high, with detailed explanations that support the methodology and findings. The novelty of the approach is significant, as it combines diffusion processes with GAN training in a way that has not been explored previously. However, the reproducibility of the results could be enhanced by providing more explicit details about the experimental setup and any potential limitations encountered during training.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to improving GAN training through the introduction of Diffusion-GAN, backed by strong theoretical and empirical evidence. While the claims made are ambitious and suggest a transformative impact on generative modeling, further validation and clarification are necessary to fully substantiate the universality and practicality of the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Diffusion-GAN, a novel framework for training Generative Adversarial Networks (GANs) that leverages a forward diffusion process to introduce Gaussian-mixture distributed instance noise. It comprises an adaptive diffusion process, a timestep-dependent discriminator, and a generator, aimed at improving stability and data efficiency in generating realistic images. The empirical results demonstrate that Diffusion-GAN shows improved diversity and stability in GAN training, although some image quality metrics exhibit mixed performance across datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to noise injection through the diffusion process, which addresses common challenges in GAN training, such as instability and mode collapse. The introduction of a timestep-dependent discriminator is a clever mechanism to provide consistent gradients, facilitating improved convergence. However, the paper's evaluation reveals a lack of universally superior performance, with some datasets showing lower fidelity results, which raises questions about the robustness of the proposed method across varying conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, presenting its contributions and methodology in an understandable manner. The novelty of the approach is notable, particularly in how it integrates diffusion processes into GAN training. However, reproducibility may be affected by the varying performance metrics across different datasets, as well as the absence of detailed implementation specifics that could guide other researchers in replicating the results.\n\n# Summary Of The Review\nOverall, Diffusion-GAN introduces a compelling framework that addresses important issues in GAN training and shows promise in improving model stability and diversity. However, the results are mixed, particularly in image fidelity, suggesting that while the approach is innovative, further refinement and exploration are necessary to establish its effectiveness across a broader range of applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the efficacy of injecting instance noise via a diffusion process to stabilize GAN training. The authors propose an adaptive diffusion approach that adjusts the number of diffusion steps based on learning dynamics. They argue that this strategy enhances the min-max objective function of GANs, leading to improved performance on benchmark datasets. While empirical results indicate some success, the theoretical underpinnings and generalization of these findings to diverse datasets remain questionable.\n\n# Strength And Weaknesses\nThe paper presents an interesting approach to mitigating the instability of GAN training through instance noise, which is a relevant problem in the field. However, several weaknesses undermine its contributions. The reliance on theoretical guarantees that may not hold in practice raises concerns, particularly regarding the Gaussian mixture assumption for instance noise, which may not be applicable across different datasets. Additionally, the paper fails to adequately address the inherent instability of the min-max objective function and does not robustly compare its method to existing techniques. The generalization of results is also not rigorously tested, limiting the paper's practical implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly; however, some sections could benefit from more thorough explanations, particularly regarding the theoretical foundations. The novelty of introducing an adaptive diffusion process is noteworthy, yet the practical significance is muddied by the lack of empirical validation across various datasets. The reproducibility of results is compromised by insufficient details on implementation and performance metrics, particularly concerning the potential computational overhead introduced by the proposed method.\n\n# Summary Of The Review\nOverall, while the paper introduces an innovative approach to stabilize GAN training through instance noise, significant concerns about its theoretical assumptions, generalizability, and empirical validation limit its impact. Addressing these issues could strengthen the paper’s contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Diffusion-GAN: Training GANs with Diffusion\" by Zhendong Wang et al. introduces a novel framework that integrates diffusion processes into the training of Generative Adversarial Networks (GANs). The authors propose a method that employs a forward diffusion process to generate Gaussian-mixture instance noise, which is injected into the GAN training to stabilize the learning dynamics. Key contributions include the implementation of a timestep-dependent discriminator and an adaptive generator, both of which enhance image realism and training stability. Empirical results demonstrate that Diffusion-GAN outperforms existing GAN architectures in terms of image fidelity and diversity across multiple datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the common challenges of GAN training, particularly instability and mode collapse. The theoretical analysis provided supports the effectiveness of the proposed method, and the extensive empirical evaluations lend credibility to the claims made. However, a potential weakness is the lack of a comprehensive comparison with all relevant state-of-the-art GAN methods; while the results are promising, additional benchmarks could strengthen the arguments further. Additionally, the paper could benefit from a more detailed discussion on the computational overhead introduced by the diffusion process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing quality is high, with technical concepts explained comprehensibly, making it accessible to a broad audience within the machine learning community. The novelty of the approach is significant as it combines techniques from two burgeoning areas—diffusion models and GANs—offering a fresh perspective on a long-standing problem. Reproducibility is supported by detailed descriptions of the methodology and experiments, although sharing code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of generative models by introducing Diffusion-GAN, which effectively enhances GAN training stability and output quality. The combination of theoretical insights and empirical validation solidifies the paper's standing as a noteworthy advancement in GAN methodologies.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework aimed at enhancing the efficiency of large-scale neural networks through a new pruning technique. The authors propose a method that leverages a combination of gradient-based optimization and statistical analysis to identify and remove redundant connections in neural architectures. Empirical evaluations demonstrate that the proposed approach not only preserves model accuracy but also significantly reduces computational costs, making it applicable to real-world scenarios where resources are constrained.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The pruning technique presented is a fresh perspective that integrates both gradient information and statistical metrics, which could lead to improvements over traditional pruning methods.\n2. **Robust Theoretical Background:** The paper provides a solid theoretical framework, elucidating the rationale behind the proposed method and grounding its effectiveness in established principles.\n3. **Significant Empirical Results:** The empirical section showcases a range of experiments across various datasets, illustrating the method's capability to maintain performance while achieving substantial reductions in model size and inference time.\n\n**Weaknesses:**\n1. **Limited Scope of Experiments:** While the results are promising, the experiments are primarily conducted on a narrow selection of benchmarks. Broader testing across different types of architectures and datasets would strengthen the findings.\n2. **Comparative Analysis:** The paper lacks a thorough comparison with existing state-of-the-art pruning methods, making it challenging to ascertain the relative advantages of the proposed technique.\n3. **Reproducibility Concerns:** Certain implementation details are not fully described, which could hinder other researchers' ability to replicate the results or build upon this work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical progression of ideas. The introduction effectively sets the stage, while the methodology is articulated with sufficient detail. However, the lack of exhaustive implementation specifics detracts from reproducibility. In terms of novelty, the proposed pruning technique introduces significant advancements in the field, although it could benefit from a deeper exploration of its implications and limitations.\n\n# Summary Of The Review\nThe paper presents a promising new pruning technique for neural networks that combines gradient-based optimization with statistical analysis, demonstrating significant efficiency gains while maintaining model performance. However, the empirical evaluation could be expanded, and clearer comparisons with existing methods are necessary to fully establish the method's significance in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel approach to training Generative Adversarial Networks (GANs) that addresses the common issues of instability and non-convergence. The methodology involves a forward diffusion chain to generate Gaussian-mixture distributed instance noise, which is injected into the training process. Key components include an adaptive diffusion process, a timestep-dependent discriminator, and a generator that learns through feedback from the discriminator, allowing it to effectively match the true data distribution. The findings demonstrate that Diffusion-GAN achieves superior performance in generating high-resolution, photo-realistic images with enhanced stability and data efficiency compared to state-of-the-art GANs.\n\n# Strength And Weaknesses\nThe primary strength of Diffusion-GAN lies in its innovative approach to stabilizing GAN training through the incorporation of a diffusion process that effectively generates instance noise. This method not only enhances the training dynamics but also mitigates issues like discriminator overfitting and mode collapse. The theoretical support provided by the authors is compelling, suggesting a strong foundation for their claims. However, the paper could benefit from a more detailed discussion of potential limitations or scenarios where the diffusion process might not perform as well. Additionally, while empirical results are promising, a broader comparison across diverse datasets would strengthen the validity of the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the problem, methodology, and findings. The concepts are articulated in a manner that is accessible to readers familiar with GANs and diffusion processes. The quality of the writing is high, although some sections could use additional elaboration to enhance understanding, particularly regarding the implementation details of the diffusion process. The novelty of the approach is significant, as it introduces a new mechanism for training GANs. The reproducibility of the results appears feasible, given that the authors outline their methodology, but further details about the experimental setup would be beneficial for researchers looking to replicate the findings.\n\n# Summary Of The Review\nOverall, Diffusion-GAN presents a significant contribution to the field of GAN training by introducing a forward diffusion process that enhances stability and performance. The theoretical and empirical results are strong, making a compelling case for the effectiveness of the proposed method. However, further exploration of potential limitations and additional experimental comparisons would enhance the robustness of the study.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel framework designed to stabilize the training of Generative Adversarial Networks (GANs) by employing a forward diffusion process to generate Gaussian-mixture instance noise. The methodology consists of three key components: an adaptive diffusion process that adjusts noise levels, a timestep-dependent discriminator that assesses noisy samples at each step, and a generator that learns to produce samples based on feedback from the discriminator. The theoretical analysis supports the effectiveness of this approach, demonstrating that the generator can receive informative gradients throughout training. Experimental results show that Diffusion-GAN outperforms several strong GAN baselines in generating realistic images across multiple datasets, affirming its advantages in stability and data efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing common challenges faced by GANs, such as instability and mode collapse, by integrating a diffusion process for noise generation. This method not only provides theoretical grounding but also exhibits empirical success, showing significant improvements in both image quality and training stability. A notable weakness, however, is the reliance on a complex architecture that may complicate the implementation and understanding of the proposed framework. Additionally, while the paper discusses the implications of its findings, it could benefit from a more thorough exploration of potential limitations and specific scenarios where the method may not perform as well.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with GANs and diffusion processes. The quality of the writing is high, with a coherent flow from the introduction through to the experimental results. The novelty of the approach is significant, as it introduces a unique perspective on utilizing diffusion processes for improving GAN training. However, reproducibility could be a concern due to the complexity of the proposed architecture and the specific configurations used in the experiments, which may not be easily replicated without detailed guidance.\n\n# Summary Of The Review\nOverall, Diffusion-GAN presents a compelling and innovative framework for addressing the challenges of GAN training through a diffusion process for instance noise generation. While the paper demonstrates strong theoretical and empirical contributions, its complexity may pose challenges for reproducibility and practical implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"Diffusion-GAN,\" a novel framework aimed at improving the training stability of Generative Adversarial Networks (GANs) through the integration of a diffusion process. The proposed methodology involves an adaptive diffusion mechanism that injects instance noise during training, thereby addressing common issues associated with GAN training such as mode collapse and instability. Empirical findings demonstrate that Diffusion-GAN achieves superior performance compared to traditional GANs across various datasets, with notable improvements in both qualitative and quantitative measures, including Fréchet Inception Distance (FID) and Recall.\n\n# Strength And Weaknesses\nStrengths of the paper include a clear articulation of the problem space, comprehensive methodology, and rigorous experimental validation. The integration of theoretical analysis to support the proposed approach adds depth to the work. However, a potential weakness lies in the lack of detailed exploration of the limitations of the proposed method and the need for further ablation studies to clarify the impact of specific components of the framework. This could enhance the understanding of which aspects of the diffusion process contribute most significantly to the improved training stability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and logically organized, with technical details adequately explained for comprehension. The clarity of figures and tables enhances the presentation of results, making it easier for readers to grasp the key findings. The originality of the approach is evident, as it introduces a unique perspective on GAN training by leveraging diffusion processes. However, while the methodology is described in detail, the reproducibility of results could be improved with additional information regarding hyperparameter settings and experimental conditions.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of generative models by presenting a novel technique for stabilizing GAN training through diffusion processes. While the empirical results are promising, further exploration of limitations and additional ablation studies could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Diffusion-GAN, a novel architecture that integrates a forward diffusion chain to address instability in training Generative Adversarial Networks (GANs). The proposed framework includes an adaptive diffusion process, a timestep-dependent discriminator, and a generator network. The methodology involves a stochastic minimax objective that incorporates instance noise through a diffusion mechanism, thereby improving the training stability and sample quality. Empirical results demonstrate that Diffusion-GAN achieves superior performance compared to state-of-the-art GANs, evidenced by improved metrics such as Fréchet Inception Distance (FID) and Recall scores on datasets like CIFAR-10 and FFHQ.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to enhancing GAN stability through the integration of a diffusion process, which is a significant contribution to the field. The theoretical guarantees provided for the continuity and differentiability of the minimax objective lend credibility to the methodology. However, the paper does not extensively analyze the computational complexity of the proposed method or its scalability, which could be a limitation when applied to larger datasets or in real-time applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates the problem being addressed, the proposed solution, and the empirical results. The mathematical formulations are presented with sufficient detail, making it accessible to readers familiar with the subject. The novelty of combining diffusion processes with GANs is significant, although the reproducibility could be enhanced by providing more implementation details and hyperparameter settings used in experiments.\n\n# Summary Of The Review\nOverall, Diffusion-GAN introduces a compelling framework that enhances the stability of GAN training through a novel diffusion process. While the theoretical and empirical contributions are strong, further exploration of computational efficiency and detailed experimental setups would improve the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel generative framework that integrates a forward diffusion chain with adaptive noise injection to enhance the performance of generative adversarial networks (GANs). The authors claim that their method outperforms existing GAN baselines on certain tasks, leveraging the noise injection strategy to improve sample quality. However, the empirical validation appears limited, focusing primarily on a narrow set of image datasets, which raises concerns regarding the generalizability of the findings.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach that combines diffusion processes with GAN architectures, potentially leading to improvements in sample quality. However, the weaknesses are significant; the complexity of the proposed method may hinder reproducibility and practical implementation. Additionally, the theoretical assumptions underpinning the model seem questionable, and the heavy reliance on instance noise injection may not yield consistent performance across diverse applications. The empirical evaluation is insufficient, with outdated comparisons and limited exploration of hyperparameters, leaving many questions about the robustness and applicability of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to vague contributions and inconsistent terminology, which may confuse readers. The novelty is present in the integration of diffusion processes within GANs, but it lacks sufficient empirical backing and a thorough exploration of underlying mechanisms. Reproducibility is at risk due to the complexity of the method and limited discussion of hyperparameters and practical implementations.\n\n# Summary Of The Review\nOverall, while Diffusion-GAN presents an interesting conceptual framework for enhancing GAN performance, its practical implications are unclear, and the empirical evidence is insufficiently robust. The paper's complexity and reliance on theoretical assumptions raise significant concerns about its applicability and reproducibility in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents Diffusion-GAN, a novel framework that integrates Generative Adversarial Networks (GANs) with a forward diffusion process to enhance image generation. This approach not only stabilizes GAN training but also improves the quality of generated images significantly. The authors demonstrate that Diffusion-GAN achieves state-of-the-art results on various benchmark datasets, outperforming existing GAN architectures in terms of image fidelity and diversity. Additionally, the method employs Gaussian-mixture distributed instance noise to augment data efficiently, ensuring robustness and adaptability across different generative tasks.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative integration of diffusion processes with GANs, which results in enhanced stability during training and superior image generation capabilities. The extensive theoretical support provided enhances the credibility of the proposed method. Moreover, the adaptability of Diffusion-GAN to various architectures and tasks highlights its potential for broad applications. However, the paper could benefit from a more detailed discussion on the computational costs associated with the proposed approach, as it may limit practical applications in resource-constrained environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, as it combines two established techniques in a manner that has not been explored in depth before. Reproducibility is supported by the clear description of the experimental setup and the provision of quantitative metrics, although further details on implementation specifics would enhance reproducibility.\n\n# Summary Of The Review\nOverall, Diffusion-GAN represents a notable advancement in generative modeling, combining the strengths of GANs and diffusion processes to achieve impressive results in image synthesis. Its stability, efficiency, and high-quality outputs position it as a promising foundation for future developments in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents Diffusion-GAN, a novel framework that integrates diffusion processes into the training of Generative Adversarial Networks (GANs). The main contributions include a theoretical exploration of how adaptive noise injection through a forward diffusion chain can enhance the training dynamics of GANs by providing a smoother optimization landscape and preventing issues such as gradient vanishing and overfitting. Key findings highlight that the new min-max objective function is continuous and differentiable, ensuring effective gradient flow, and that the adaptive nature of the noise injection allows the discriminator to learn progressively more complex features without degradation in generative performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical analysis and the novel application of diffusion processes to GAN training, which addresses common challenges such as mode collapse and instability. The framework's ability to maintain a consistent learning signal through gradient flow is particularly noteworthy. However, the paper could benefit from empirical validation of its theoretical claims, as the current focus is largely on theoretical implications without comprehensive experimental results. Additionally, while the theoretical insights are compelling, the practical implementation details and potential computational overhead of the diffusion process are not thoroughly explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, making the complex ideas accessible to readers. The quality of the theoretical framework is high, providing a solid foundation for the proposed methodology. The novelty of combining diffusion processes with GANs is significant, as it opens avenues for further research. However, reproducibility could be a concern due to the lack of detailed experimental results and implementation specifics, which should ideally accompany such theoretical advancements.\n\n# Summary Of The Review\nOverall, the paper makes a significant theoretical contribution to the field of generative modeling by introducing Diffusion-GAN, which leverages diffusion processes to improve GAN training dynamics. While the theoretical insights are robust and promising, the paper would benefit from empirical validation to strengthen its claims and practical applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents Diffusion-GAN, a novel Generative Adversarial Network (GAN) framework that incorporates a forward diffusion chain to facilitate the generation of Gaussian-mixture distributed instance noise. The methodology features key components such as an adaptive diffusion process, a timestep-dependent discriminator, and an innovative generator. The training process involves the generator learning from the discriminator’s feedback through backpropagation across the diffusion chain, with the objective of fooling the discriminator at various diffusion steps. The findings indicate that this approach enhances the discriminator's ability to differentiate between real and generated data, contributing to improved performance on benchmark datasets such as CIFAR-10, STL-10, and LSUN.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of an adaptive diffusion mechanism that dynamically adjusts the intensity of noise during training, which effectively mitigates overfitting. The use of a timestep-dependent discriminator enhances the model’s capability to handle varying levels of noise, leading to better generated outputs. However, a notable weakness is the potential complexity introduced by the adaptive parameters, which may hinder reproducibility. Additionally, while the paper provides extensive experimental results, it would benefit from a more thorough comparison with other state-of-the-art GAN architectures to contextualize its performance gains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with a well-structured presentation of the methodology and results. The quality of the writing is consistent, and the figures effectively illustrate the concepts discussed. In terms of novelty, the integration of a diffusion process within a GAN framework is significant and presents a fresh perspective on noise handling in generative models. The reproducibility is somewhat compromised due to the complexity of the adaptive components, although the provision of implementation details and code availability aids in this regard.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the field of generative models by introducing Diffusion-GAN, which combines diffusion processes with GAN architectures to enhance model performance. While the approach demonstrates strong empirical results, the complexity of the adaptive mechanisms may pose challenges for reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a new generative adversarial network aimed at addressing training instability issues commonly associated with GANs. The methodology focuses on a forward diffusion chain that applies instance noise adaptively, claiming improvements in stability, performance, and data efficiency over established models like StyleGAN2. The authors report superior Frechet Inception Distance (FID) scores and assert the ability to generate high-resolution, photo-realistic images with reduced risk of augmentation leakage. However, the paper lacks comparative depth against existing theoretical frameworks and established methodologies in the field.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its innovative approach to adaptively adjusting diffusion steps, which could offer new avenues for GAN training. Additionally, the reported performance improvements over traditional models are promising. However, the weaknesses are significant; the paper does not sufficiently engage with prior works, such as WGAN-GP, that have addressed GAN stability, nor does it provide a rigorous comparison to existing theoretical frameworks. The novelty of the diffusion process is diminished by its similarity to concepts already explored in existing literature, and the empirical results may be skewed due to the optimization of the competing baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, presenting its ideas in a structured manner. However, the novelty of the contributions is questionable, as many of the proposed ideas appear to be reiterations of existing methods. The quality of the theoretical analysis is robust but lacks direct comparisons that would allow for a clearer assessment of its significance. Reproducibility is not sufficiently addressed, as the experiments focus on a narrow set of benchmarks, leaving doubts about the model's performance across diverse datasets and tasks.\n\n# Summary Of The Review\nOverall, while Diffusion-GAN presents an interesting approach to GAN training and claims notable performance improvements, it falls short in addressing existing literature and providing a comprehensive evaluation of its contributions. The novelty is undermined by the similarities to prior works, and the empirical claims need more rigorous contextualization within the broader landscape of generative models.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Diffusion-GAN: Training GANs with Diffusion\" presents a novel approach to improve Generative Adversarial Networks (GANs) by integrating diffusion processes into the training paradigm. The authors propose a diffusion-based framework that incorporates a timestep-dependent discriminator to enhance training stability and sample quality. Key findings demonstrate that the Diffusion-GAN outperforms traditional GANs on standard benchmark datasets, achieving lower Fréchet Inception Distances (FID) and improved visual fidelity in generated samples.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach of combining diffusion processes with GANs, which addresses common issues of mode collapse and instability in GAN training. The empirical results are robust, with clear comparisons to baseline methods and a variety of datasets. However, weaknesses include some inconsistencies in notation and formatting throughout the document, which may detract from the overall clarity. Additionally, while the theoretical grounding is present, more in-depth discussions on the implications of the proposed method could enhance the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but certain sections suffer from inconsistent notation and formatting that could hinder reader comprehension. The novelty of the proposed method is significant, as it represents a meaningful advancement in the GAN training landscape. The experimental results are well-documented, but the reproducibility could be improved with more detailed descriptions of the implementation and hyperparameter settings used in the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of GANs through its integration of diffusion processes. Despite some clarity issues and inconsistencies in formatting, the novel approach and solid empirical results make a strong case for its significance in addressing GAN training challenges.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach termed Diffusion-GAN, which integrates diffusion processes into the training framework of Generative Adversarial Networks (GANs). The authors propose a specific adaptive diffusion mechanism to enhance the stability and performance of GAN training. Empirical results demonstrate improvements on selected benchmark datasets, yet the paper primarily focuses on Gaussian noise distributions and specific assumptions regarding discriminator behavior, limiting the generalizability of the findings.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to GAN training through the incorporation of diffusion processes, which shows promise in improving performance on benchmark datasets. However, the paper has several weaknesses, including a limited exploration of the implications for other generative models, a narrow focus on Gaussian noise, and a lack of thorough theoretical analysis concerning varying network architectures. Additionally, the empirical evaluation is constrained by a finite set of datasets, which diminishes the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the proposed methodology and results. However, the novelty is somewhat constrained by the lack of exploration into alternative noise distributions and the implications for broader generative modeling contexts. The reproducibility of the findings could be enhanced by including more diverse datasets and a comprehensive comparison with existing state-of-the-art methods. The theoretical justifications provided are based on simplistic examples, which could limit the generalizability of the results.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting method for improving GAN training through diffusion processes, yet it falls short in rigorously exploring the broader implications and robustness of its findings. A more comprehensive analysis, including diverse datasets and alternative noise types, is necessary to strengthen the contributions and significance of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents \"Diffusion-GAN,\" a novel approach to training Generative Adversarial Networks (GANs) by integrating a forward diffusion process that generates Gaussian-mixture distributed instance noise. The contributions include a comprehensive theoretical framework supported by rigorous proofs, which demonstrate the differentiability and smoothness of the noise injection process, thereby enhancing gradient propagation. The experimental results across various datasets, such as CIFAR-10 and LSUN, indicate that Diffusion-GAN significantly outperforms traditional GAN baselines in terms of Fréchet Inception Distance (FID) and recall scores, validating the effectiveness of the proposed methodology.\n\n# Strength And Weaknesses\nStrengths of the paper include its solid theoretical foundation, which is backed by theorems that establish crucial properties of the noise injection method. The empirical validation is thorough, demonstrating significant improvements over baseline models and providing strong metrics for evaluation. However, a potential weakness lies in the complexity of the model, which may pose challenges for reproducibility and practical implementation. While the adaptive mechanisms are promising, their effectiveness may vary across different datasets, potentially limiting generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the theoretical and empirical aspects accessible to readers. The quality of the writing is high, with a clear presentation of results and methodologies. The novelty of integrating diffusion processes into GAN training is significant, although the complex nature of the model raises concerns about reproducibility in practical scenarios. The provided experimental details are adequate, but additional information on implementation specifics would enhance reproducibility.\n\n# Summary Of The Review\nOverall, \"Diffusion-GAN\" makes a substantial contribution to the field of GANs by introducing an innovative training framework that combines statistical rigor with empirical validation. While the theoretical insights and experimental results are compelling, the complexity of the model may hinder its practical application and reproducibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel generative model that incorporates a diffusion process to stabilize GAN training. The authors propose an adaptive diffusion mechanism aimed at improving the quality and robustness of generated images. The methodology includes theoretical guarantees based on specific conditions and experimental validation across a limited set of image datasets. Findings indicate that Diffusion-GAN can outperform traditional GAN models in certain scenarios, although the paper does not thoroughly explore its effectiveness across diverse datasets or conditions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in the introduction of the adaptive diffusion process, which presents a potentially effective approach to mitigate issues commonly faced in GAN training, such as mode collapse and instability. However, several weaknesses are noted: the lack of a comprehensive comparison with existing GAN stabilization techniques limits the context for evaluating Diffusion-GAN's effectiveness. Additionally, the theoretical guarantees are condition-specific, raising questions about their generalizability. The paper also fails to address computational overheads in practical applications, as well as the robustness of the model against adversarial attacks. Furthermore, the study lacks detailed exploration of hyperparameter sensitivity and does not include an ablation study to isolate the contributions of individual components.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly; however, it lacks depth in several areas that could enhance clarity regarding the model's applicability and limitations. The novelty of the approach is significant in the context of generative modeling, but the empirical validation is limited, raising concerns about reproducibility across various conditions and datasets. The claims made regarding the domain-agnostic nature of Diffusion-GAN require further empirical validation across a wider range of applications.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to GAN stabilization through the introduction of Diffusion-GAN. While the proposed methodology demonstrates potential advantages, the lack of comprehensive comparisons, empirical validation across diverse datasets, and exploration of practical limitations diminishes the overall impact of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Diffusion-GAN: Training GANs with Diffusion\" proposes an innovative approach to training Generative Adversarial Networks (GANs) by integrating a diffusion process for noise injection into the discriminator. The authors argue that their method addresses common issues in GAN training, such as instability and mode collapse, by introducing a timestep-dependent discriminator and employing a forward diffusion chain that generates Gaussian-mixture noise. The findings suggest that their method leads to higher stability and data efficiency, achieving state-of-the-art results in generating high-resolution images.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to stabilize GAN training through the novel application of diffusion processes, which may contribute to improved image quality and training stability. However, the paper suffers from several weaknesses, including a lack of truly novel contributions as many of the techniques discussed, such as noise injection and data augmentation, have been extensively explored in previous literature. The empirical results, while showing improvements, rely on metrics like FID scores that may not fully capture the benefits of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, providing clear explanations of the proposed methodology and results. However, the novelty is questionable, as the contributions appear to be incremental rather than groundbreaking, combining existing techniques rather than presenting a fundamentally new approach. Reproducibility is addressed to some extent, but the reliance on specific hyperparameters and the lack of comprehensive benchmark comparisons may hinder the ability of other researchers to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to training GANs by incorporating diffusion processes; however, it falls short of delivering truly novel contributions to the field. While the results indicate improvements in training stability and image quality, the reliance on established techniques and metrics detracts from the paper's overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel generative model that integrates a diffusion process with Generative Adversarial Networks (GANs) to enhance stability and performance during training. The authors present a differentiable data augmentation method that leverages an adaptive diffusion process, allowing for more effective noise injection during training. Experimental results indicate that Diffusion-GAN outperforms traditional GANs in terms of sample quality and stability, particularly in settings with limited training data.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach that combines concepts from diffusion processes with GANs, leading to improved training stability and sample quality. The inclusion of theoretical insights into the diffusion process provides a solid foundation for understanding the model's mechanics. However, the paper could benefit from more comprehensive evaluations, such as additional performance metrics beyond FID and Recall, and more explicit comparisons with other generative models like VAEs and flow-based models. Furthermore, the exploration of data efficiency across different domains is somewhat limited, which raises questions about the model's generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation. The methodology is described in sufficient detail, allowing for reproducibility; however, certain implementation aspects, particularly regarding hyperparameter tuning and their impacts, could be elaborated upon. The novelty of the approach is significant, combining established concepts in a novel way, although its broad applicability remains to be fully demonstrated.\n\n# Summary Of The Review\nOverall, Diffusion-GAN presents a promising advancement in generative modeling by integrating diffusion processes with GANs, showcasing improved stability and sample quality. While the paper lays a strong foundation, additional evaluations and comparisons could further strengthen its contributions and applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Diffusion-GAN, a novel generative adversarial network (GAN) that demonstrates superior performance in image generation tasks compared to existing models. Diffusion-GAN is evaluated on several benchmark datasets including CIFAR-10, CelebA, STL-10, LSUN-Bedroom, LSUN-Church, and FFHQ, utilizing metrics such as Fréchet Inception Distance (FID) and Recall scores to quantify its performance. The findings reveal that Diffusion-GAN achieves state-of-the-art results, consistently outperforming strong GAN baselines in both image quality and diversity, particularly in scenarios with limited data.\n\n# Strength And Weaknesses\nThe strengths of Diffusion-GAN include its robust performance across various datasets, showcasing significant improvements in FID scores and higher Recall metrics that indicate greater diversity in generated images. Additionally, its adaptability across domains and efficiency in limited data scenarios are notable contributions. However, potential weaknesses include the lack of detailed explanations of the underlying architecture and training processes, which may hinder reproducibility and understanding of the model's efficacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation of results and methodology, yet it could benefit from more comprehensive explanations of the model's architecture and training procedures. The quality of empirical results is high, showcasing a rigorous evaluation against multiple benchmarks. In terms of novelty, while the concept of combining diffusion processes with GANs is compelling, the paper does not thoroughly explore the theoretical foundations or implications of this approach. Reproducibility may be challenging due to the insufficient details provided.\n\n# Summary Of The Review\nOverall, the paper presents Diffusion-GAN as a promising advancement in the field of generative models, demonstrating significant improvements in image generation quality and diversity. However, the clarity regarding the model's architecture and training could be enhanced to support reproducibility and deeper understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach to generative modeling through the introduction of the Diffusion-GAN framework, which merges the strengths of diffusion processes and Generative Adversarial Networks (GANs). The authors propose an innovative methodology that employs a diffusion process to improve sample quality and diversity. The findings demonstrate that Diffusion-GAN outperforms existing state-of-the-art models in various benchmark datasets, showcasing its potential for generating high-fidelity images.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by integrating diffusion processes with GANs, which is a relatively unexplored area in generative modeling. The experimental results are compelling, indicating clear advantages over traditional GAN methods in terms of output quality and diversity. However, the paper suffers from several clarity issues. The abstract is dense and convoluted, which may hinder initial understanding. Additionally, inconsistent terminology (e.g., using both \"Diffusion-GAN\" and \"Diffusion-GANs\") and lengthy paragraphs detract from the overall readability. The logical flow between sections could also be improved to guide the reader more effectively through the methodology and results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by the use of technical jargon and dense theoretical sections, which may alienate readers not well-versed in the subject matter. While the novelty of combining diffusion processes with GANs is noteworthy, the communication of this idea could be enhanced through clearer definitions and examples. The reproducibility of results is supported by detailed descriptions of experiments; however, the lengthy experimentation section could benefit from a more concise summary of findings and performance metrics. Overall, while the quality of the work is high, the delivery lacks accessibility.\n\n# Summary Of The Review\nIn summary, the paper introduces an innovative Diffusion-GAN framework that shows promise in advancing generative modeling. However, issues related to clarity, readability, and terminological consistency significantly detract from its overall impact. Improvements in these areas could enhance the paper's accessibility and effectiveness.\n\n# Correctness\n4/5 - The methodology appears sound, and results are presented accurately, though some minor inconsistencies in terminology raise questions about clarity.\n\n# Technical Novelty And Significance\n4/5 - The integration of diffusion processes with GANs presents a novel approach, contributing meaningfully to the generative modeling field, albeit with some prior work in related areas.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are significant, demonstrating superior performance compared to existing models; however, further exploration of the limitations and edge cases could strengthen the findings."
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.6264348559159765,
    -1.5350341922143464,
    -1.7052153742013423,
    -1.7998304001368206,
    -1.7724379967978734,
    -1.654442635902458,
    -1.739280067402507,
    -1.6986213830864825,
    -1.7864394991582566,
    -1.868775185540663,
    -1.4858252573277573,
    -1.4202856854188235,
    -1.5819213531643328,
    -1.5391301139180118,
    -1.5138896633520937,
    -1.6359702968944132,
    -1.8735544815614802,
    -1.4984794393770318,
    -1.5727480908954443,
    -1.6215906350420326,
    -1.957707179522752,
    -1.5057067024565038,
    -1.7049524824881581,
    -1.7813955206819687,
    -1.7572445013234297,
    -1.5743398985711194,
    -1.517363403481549,
    -1.492342709269321,
    -1.57413695042108
  ],
  "logp_cond": [
    [
      0.0,
      -2.1991997927803517,
      -2.2139654940014353,
      -2.23372501109643,
      -2.2856515144393876,
      -2.2160028935722234,
      -2.234937772041403,
      -2.254490827448148,
      -2.249224868835009,
      -2.2458259032467742,
      -2.231067038113432,
      -2.351924568697098,
      -2.2337954382417227,
      -2.2214845695312957,
      -2.226083334205603,
      -2.2310122402447274,
      -2.29736465668232,
      -2.284191292895565,
      -2.2948680744139827,
      -2.246788925509383,
      -2.2435614906796,
      -2.286573400128267,
      -2.2759705809190782,
      -2.2738309581937237,
      -2.2653653056063994,
      -2.280519504733369,
      -2.259381126519557,
      -2.334204582909312,
      -2.3076757831656893
    ],
    [
      -1.2364093181046196,
      0.0,
      -1.1625097294570772,
      -1.1170805128985286,
      -1.1970398255245245,
      -1.1425043870178484,
      -1.1373918275582127,
      -1.2064086757676697,
      -1.1218032766614123,
      -1.2347266805858013,
      -1.2075473125851774,
      -1.3032918222946537,
      -1.0950439597796673,
      -1.109505813272551,
      -1.2642481402632468,
      -1.1494516390865597,
      -1.2690804687298372,
      -1.2168270741537637,
      -1.220312277879909,
      -1.1455679915524297,
      -1.2277010976758507,
      -1.2642609124921091,
      -1.2815565311767845,
      -1.1479774861940883,
      -1.2661915215412078,
      -1.2066431883419657,
      -1.2410292282733002,
      -1.2884986142329238,
      -1.3026850261016256
    ],
    [
      -1.3322088276262287,
      -1.181074013229245,
      0.0,
      -1.2503397077120566,
      -1.318122173063799,
      -1.2672636251646494,
      -1.250183673159528,
      -1.2969522829431734,
      -1.2576015044321134,
      -1.3123710512794589,
      -1.269574063780994,
      -1.397926546974081,
      -1.1881130303579093,
      -1.176950185122052,
      -1.3516572066784227,
      -1.228568761418796,
      -1.307347698990097,
      -1.3679019397414776,
      -1.3298040743281108,
      -1.2630927088626158,
      -1.2986729791506857,
      -1.3396835735003187,
      -1.3526323258051796,
      -1.273264661268962,
      -1.345506868259942,
      -1.308939553602459,
      -1.3207420862608796,
      -1.436020098149703,
      -1.4490138329709794
    ],
    [
      -1.4165124764121841,
      -1.2208439438741583,
      -1.3211620352915916,
      0.0,
      -1.4464074990360094,
      -1.344246473645572,
      -1.2736771521062853,
      -1.3771343912283125,
      -1.2848139383628188,
      -1.410236248201694,
      -1.3742695542961658,
      -1.5121104938891845,
      -1.2851500785075585,
      -1.3023278445834114,
      -1.4189178223264347,
      -1.3261418795995594,
      -1.4433784605527336,
      -1.371317735917955,
      -1.4548773196856915,
      -1.3331636539782568,
      -1.412612879234836,
      -1.405319137624194,
      -1.4452320210889562,
      -1.3106404784662642,
      -1.4188039400879473,
      -1.412090707018845,
      -1.3995142692417597,
      -1.4910302953646866,
      -1.5221612348476135
    ],
    [
      -1.4773561328270914,
      -1.3772830822788722,
      -1.429103888533847,
      -1.4465061540519435,
      0.0,
      -1.43686000040268,
      -1.4076062757905503,
      -1.4214760043524266,
      -1.406762248571654,
      -1.431217086690618,
      -1.470335027715461,
      -1.4920861047445069,
      -1.400166461084581,
      -1.383197187790005,
      -1.4182396454896191,
      -1.4251155518142462,
      -1.4134128347693937,
      -1.4686819988420157,
      -1.4678220787208223,
      -1.4432824327777434,
      -1.4278196082750114,
      -1.4697514960051192,
      -1.4409121248035364,
      -1.4322446667339535,
      -1.4015425825540666,
      -1.3612014525395422,
      -1.378901867506557,
      -1.4478063375808592,
      -1.456329883186701
    ],
    [
      -1.3429981083640594,
      -1.2529906109959832,
      -1.2966571536185874,
      -1.2560942464555809,
      -1.3330581509308979,
      0.0,
      -1.3019260118372473,
      -1.292063348814192,
      -1.266811420727023,
      -1.3383674557776954,
      -1.2949270257249892,
      -1.4098470216812788,
      -1.2030604410951868,
      -1.2662390978260372,
      -1.3489779760259133,
      -1.2773533023367947,
      -1.342773842678891,
      -1.3727867940328222,
      -1.323578267840048,
      -1.2602877903567717,
      -1.326590950416835,
      -1.3642309428708295,
      -1.359882097779068,
      -1.3370992895262237,
      -1.3775949875653668,
      -1.3102659169373714,
      -1.3859168700371312,
      -1.4236067982981944,
      -1.4355298257834679
    ],
    [
      -1.4530263922969282,
      -1.3051182032314996,
      -1.3289681504341968,
      -1.3137415842270352,
      -1.3632921935627587,
      -1.329632176232549,
      0.0,
      -1.367464349851168,
      -1.3171104947535301,
      -1.3862998009882586,
      -1.3304457668162226,
      -1.4825561179038302,
      -1.3054664462640035,
      -1.284484770741914,
      -1.4293065800949312,
      -1.368420393882926,
      -1.3879035844842318,
      -1.3716952927919686,
      -1.3993303675122328,
      -1.3075171889551374,
      -1.3956744913464991,
      -1.4310571524998796,
      -1.439564832183698,
      -1.315703169365102,
      -1.4106048176056238,
      -1.3626967245086279,
      -1.3970762501921752,
      -1.4695565274192839,
      -1.4938565212754074
    ],
    [
      -1.4225908062041213,
      -1.322170515200735,
      -1.353470718763015,
      -1.3864982683074165,
      -1.4112761544417947,
      -1.3952617770441968,
      -1.3440439928632713,
      0.0,
      -1.3746473331440268,
      -1.3796027411700789,
      -1.3590290139181762,
      -1.4588029138453122,
      -1.3217875249372697,
      -1.3388170820093641,
      -1.356650883437549,
      -1.3132587961161406,
      -1.3856174149405251,
      -1.398892431391717,
      -1.3271398355105086,
      -1.3363227635986878,
      -1.3172396806651228,
      -1.4057269177089253,
      -1.394610140532251,
      -1.3265898488201626,
      -1.3742185697085252,
      -1.3970937292117849,
      -1.3904463307315544,
      -1.414491021077166,
      -1.4465311282578812
    ],
    [
      -1.4213627106483655,
      -1.203130169965907,
      -1.3248656730290835,
      -1.24634443244626,
      -1.432389409786848,
      -1.3202097674709965,
      -1.264265494894977,
      -1.3676895420227186,
      0.0,
      -1.416744961388203,
      -1.3295857918201213,
      -1.473960289111858,
      -1.2016032121468625,
      -1.2209196446688741,
      -1.4351397430381045,
      -1.3475170854641618,
      -1.391665108996601,
      -1.3486878698941904,
      -1.3732124333349862,
      -1.2634319845031357,
      -1.3697636719253081,
      -1.4194209640561068,
      -1.4456822246229732,
      -1.3112797763998876,
      -1.4223105659165025,
      -1.3283801143752059,
      -1.4224457068256966,
      -1.448427810462171,
      -1.5161782434479691
    ],
    [
      -1.5097833930434528,
      -1.5156762675860591,
      -1.5358920705976415,
      -1.5051203042164325,
      -1.5751199742193387,
      -1.5101831981677412,
      -1.484337138682223,
      -1.5036903353296986,
      -1.518247554907734,
      0.0,
      -1.5264777084229992,
      -1.6063354109837127,
      -1.5038865511633361,
      -1.512760810175419,
      -1.5404070139246064,
      -1.4754493918181901,
      -1.5150794824702856,
      -1.5317458999814386,
      -1.5299868939334504,
      -1.4989408343195305,
      -1.5469344417896411,
      -1.5842972812145928,
      -1.511577637218641,
      -1.4968790181549045,
      -1.488112723670312,
      -1.6009879019901962,
      -1.571885178701329,
      -1.6001436056156921,
      -1.6058337175212438
    ],
    [
      -1.211141149847918,
      -1.063260390125723,
      -1.0802198447329137,
      -1.069536567841442,
      -1.1224353177803266,
      -1.086336603226555,
      -1.0396439175474583,
      -1.1025933500397214,
      -1.076523607101779,
      -1.166710976160974,
      0.0,
      -1.2053541762632434,
      -1.0389477938654763,
      -1.0512684920406599,
      -1.1471255480436497,
      -1.0959021997142917,
      -1.1694591305847089,
      -1.0941009730679825,
      -1.1129712956709528,
      -1.0781032767821015,
      -1.1535835055139092,
      -1.1401680616698462,
      -1.1653485577523868,
      -1.1077108387351842,
      -1.150606886446075,
      -1.0770111247080523,
      -1.1624647576821405,
      -1.1931702892409275,
      -1.2119550788456899
    ],
    [
      -1.1552193442127219,
      -1.1899190735946537,
      -1.1603935227736488,
      -1.1486136271806877,
      -1.1685073393405443,
      -1.1582229360278242,
      -1.1484468271215458,
      -1.135016592473241,
      -1.1689174015214645,
      -1.138871678862106,
      -1.1562587174121695,
      0.0,
      -1.1387352111290976,
      -1.148201307935706,
      -1.1424330321422649,
      -1.1485480382427005,
      -1.1457794122915599,
      -1.1662206779765827,
      -1.1571682488753554,
      -1.1688224781420828,
      -1.155487685362286,
      -1.1511046312749071,
      -1.1713597618365414,
      -1.1578397988965132,
      -1.1276156914759476,
      -1.2023894574754743,
      -1.1455297932713244,
      -1.1630545813275797,
      -1.2163154728420322
    ],
    [
      -1.282299245683242,
      -1.097638200068968,
      -1.1961758284911328,
      -1.1840352322404866,
      -1.3052403147529668,
      -1.1883876762101167,
      -1.1837680853738644,
      -1.241975464086452,
      -1.1395391973380995,
      -1.2576199040899283,
      -1.2047442639467607,
      -1.3611395827041577,
      0.0,
      -1.1150906610885984,
      -1.2736193178088728,
      -1.1829949635116686,
      -1.2775599661794097,
      -1.2768655475515593,
      -1.2512552068371148,
      -1.1546968621962814,
      -1.2038674780370857,
      -1.2887934197500501,
      -1.300580453982494,
      -1.2172860021563674,
      -1.3018052901358101,
      -1.187917183373769,
      -1.295293556559502,
      -1.3282041455249396,
      -1.4049852878454563
    ],
    [
      -1.1792209376484182,
      -1.037602743119443,
      -1.1048658192212417,
      -1.0947514489030064,
      -1.1721334539054806,
      -1.1269281162864684,
      -1.0613204503085305,
      -1.1541972781085028,
      -1.0362138726357846,
      -1.1789545067602907,
      -1.1354882387055731,
      -1.2731530419315489,
      -1.0379707740592679,
      0.0,
      -1.2186949361468349,
      -1.1722041914708685,
      -1.1824176966872042,
      -1.217691019229784,
      -1.1946794463689843,
      -1.087377178664456,
      -1.153024086215919,
      -1.222610554675657,
      -1.2155844100186186,
      -1.106022651147771,
      -1.2036314313312655,
      -1.1303973189695578,
      -1.2232509886294278,
      -1.258469013555469,
      -1.2950415623209799
    ],
    [
      -1.119080855533348,
      -1.1072644858308922,
      -1.1356116565592496,
      -1.1176650913563853,
      -1.1453506341737272,
      -1.150818336425358,
      -1.1101509605221902,
      -1.081157765035399,
      -1.1491023513748795,
      -1.1366428476528496,
      -1.109398199169264,
      -1.2019473384529675,
      -1.0930342595937945,
      -1.1009229187738534,
      0.0,
      -1.066876501056601,
      -1.1743963177839931,
      -1.1768064773248808,
      -1.1658068024364743,
      -1.144047501800934,
      -1.1637998999648889,
      -1.1467524477228392,
      -1.1782504466802102,
      -1.0672787924158662,
      -1.1274306547949071,
      -1.1607203021901245,
      -1.0930933483168912,
      -1.1769456081467142,
      -1.2375188405564552
    ],
    [
      -1.2661276243203017,
      -1.114268472287437,
      -1.157249004265442,
      -1.1492501326907631,
      -1.2618350890914758,
      -1.1724143816271164,
      -1.1536002856062404,
      -1.1609671562116564,
      -1.1711795747968508,
      -1.2395650439182229,
      -1.2052016211001277,
      -1.3497860177964476,
      -1.1206101685222556,
      -1.1785882134917764,
      -1.1975266933940945,
      0.0,
      -1.228372778776695,
      -1.2699760324513898,
      -1.1839045528701324,
      -1.1789341710398347,
      -1.207666459774168,
      -1.253607269902297,
      -1.260817310291793,
      -1.1893988821565413,
      -1.2544070411678816,
      -1.2064384550910785,
      -1.2204638799126197,
      -1.264153977233112,
      -1.3226695045793713
    ],
    [
      -1.5026702303692412,
      -1.4515288058416518,
      -1.4368468808725956,
      -1.4512963664300476,
      -1.4743583781550806,
      -1.432196217058351,
      -1.403046399796463,
      -1.435319652575148,
      -1.474085082008284,
      -1.4901692084384468,
      -1.4948297532452295,
      -1.5441207522772373,
      -1.451970223144895,
      -1.435208211497794,
      -1.5022932564736289,
      -1.4331389734975453,
      0.0,
      -1.4605106585382477,
      -1.4442617579277828,
      -1.442701916921645,
      -1.4561300328340778,
      -1.4973693734653544,
      -1.4571661203323325,
      -1.402719295133159,
      -1.437161365717515,
      -1.5006176163580613,
      -1.5005889477204821,
      -1.537608895655629,
      -1.527315664441452
    ],
    [
      -1.1444773426453068,
      -1.0493256020840334,
      -1.136863716938107,
      -1.0221881871306868,
      -1.1632273968517093,
      -1.112891364717121,
      -1.0278193038279617,
      -1.0648145158906264,
      -1.0425294789827197,
      -1.1191792506804015,
      -1.0796969665958172,
      -1.178955160463564,
      -1.0277607815241894,
      -1.0634766425002073,
      -1.134899056090785,
      -1.1225664501158423,
      -1.117896411416354,
      0.0,
      -1.1182212986406206,
      -1.0929234131753234,
      -1.096343318621273,
      -1.1503969469080315,
      -1.154910468109528,
      -1.0199913414497286,
      -1.1220964980304968,
      -1.1028537393423192,
      -1.1296699854843366,
      -1.1511866020586647,
      -1.2183282960722686
    ],
    [
      -1.3214116092820396,
      -1.2273288477296151,
      -1.2231514497322875,
      -1.2813014281986779,
      -1.3479395651573443,
      -1.243822706049108,
      -1.2170036890522973,
      -1.2235161615640184,
      -1.2671293220881026,
      -1.318083636690254,
      -1.2650652248984158,
      -1.329410682737906,
      -1.2227273662706406,
      -1.2358715372688203,
      -1.2921055841327749,
      -1.2654427006068307,
      -1.2353174426713938,
      -1.2961255764854211,
      0.0,
      -1.2122125240664172,
      -1.2596537999854516,
      -1.2926813907740016,
      -1.274554201723483,
      -1.222731213752832,
      -1.297663638191409,
      -1.2503315189093653,
      -1.2816056248940233,
      -1.2530220110375536,
      -1.3418142382464564
    ],
    [
      -1.3220905729650327,
      -1.1948404110747148,
      -1.2500217404193674,
      -1.2278067797142593,
      -1.3290407709766892,
      -1.216956678910579,
      -1.2132944935554542,
      -1.2700584170306706,
      -1.1997087893697944,
      -1.31024178350671,
      -1.2512192411423422,
      -1.3577943864027353,
      -1.1683297006583417,
      -1.174482508663758,
      -1.3301855777170508,
      -1.2463334795727152,
      -1.2843247003607003,
      -1.2617884728321394,
      -1.2876560500192866,
      0.0,
      -1.2739482917098026,
      -1.30182489840056,
      -1.2939438223421273,
      -1.197528394991027,
      -1.309209712568319,
      -1.234252429204304,
      -1.310772972929482,
      -1.3301670934516872,
      -1.3689298857396108
    ],
    [
      -1.6614266423423243,
      -1.606426475690379,
      -1.5976013686632748,
      -1.6244377819427458,
      -1.7109354033739772,
      -1.6005867527518876,
      -1.605820008868782,
      -1.5658377614533021,
      -1.6276933448351556,
      -1.6391953599769822,
      -1.6593911808678827,
      -1.712895296732619,
      -1.5892674229752202,
      -1.6085101315567771,
      -1.6440167763493296,
      -1.6171472373521534,
      -1.5952946285160072,
      -1.6290682387628637,
      -1.6083218357535243,
      -1.6067566894717273,
      0.0,
      -1.6779554790105347,
      -1.6558402567290476,
      -1.6138504281785353,
      -1.619588905842727,
      -1.6015851129684797,
      -1.6586902973160116,
      -1.711063681504111,
      -1.7290794307930197
    ],
    [
      -1.193365412709591,
      -1.117067180339832,
      -1.1127488209485774,
      -1.1210138589821346,
      -1.1765798009994608,
      -1.1464663441827228,
      -1.1010301355488221,
      -1.136004213457095,
      -1.1113682303938361,
      -1.166229991879604,
      -1.1175583698223719,
      -1.1673627652606926,
      -1.1140920376370818,
      -1.0958631817568458,
      -1.0890050171554315,
      -1.062986265893161,
      -1.1391703609596822,
      -1.1787747170506173,
      -1.1212780125069632,
      -1.1227756972383358,
      -1.1286639938177692,
      0.0,
      -1.1258960933672033,
      -1.1119874520688866,
      -1.1543050295618287,
      -1.09343423176007,
      -1.128855206239897,
      -1.1403170275771317,
      -1.1697861699573666
    ],
    [
      -1.3391743934586295,
      -1.3242956433975492,
      -1.323474890077298,
      -1.3321904358846337,
      -1.3868899654121754,
      -1.3609397589590906,
      -1.3170845045219333,
      -1.3289658647193434,
      -1.3727778909020947,
      -1.2873200753997853,
      -1.3260755380382427,
      -1.4507295174537918,
      -1.3564066709380815,
      -1.3782803364442324,
      -1.3371113910453,
      -1.33289540125134,
      -1.3196629494035355,
      -1.3701450150851784,
      -1.3174686736545793,
      -1.3464252401701948,
      -1.3104792753135115,
      -1.3858163550680385,
      0.0,
      -1.3263294722223917,
      -1.2631202439174523,
      -1.3811296418673964,
      -1.3579270198584228,
      -1.432032410701871,
      -1.451817572656853
    ],
    [
      -1.4373547632694434,
      -1.2799001832543497,
      -1.3628333026039217,
      -1.252392357689127,
      -1.430208473128579,
      -1.382258962851838,
      -1.2825718673688253,
      -1.3529288366797085,
      -1.3082526034819335,
      -1.3886800817961158,
      -1.3599378288173771,
      -1.4742059106183725,
      -1.3225390924018867,
      -1.3239680713887008,
      -1.3788755747401173,
      -1.3390008227465235,
      -1.3464875672450742,
      -1.358096078519601,
      -1.356482823548142,
      -1.3341733053408897,
      -1.3788778826235149,
      -1.4046293731238366,
      -1.4316758728189152,
      0.0,
      -1.3859598685970211,
      -1.3670441902014656,
      -1.408264916221319,
      -1.414875735566379,
      -1.4983076924581362
    ],
    [
      -1.4155353585445893,
      -1.387964766628546,
      -1.3868996662761113,
      -1.3935456585715926,
      -1.444204430228335,
      -1.4089242079317343,
      -1.4036630159315282,
      -1.3686043719226826,
      -1.421538995536997,
      -1.3696027864943503,
      -1.4372110642558118,
      -1.47632221850038,
      -1.406613050808159,
      -1.4038442577016346,
      -1.3685007675400345,
      -1.430333102053247,
      -1.4214943768647117,
      -1.4128830471923446,
      -1.3888541992721513,
      -1.4223328212137512,
      -1.3501392835988129,
      -1.4701748597079038,
      -1.3732553978339068,
      -1.405843777175301,
      0.0,
      -1.4650536172999191,
      -1.4243521021001955,
      -1.4486559468872329,
      -1.4798990260268765
    ],
    [
      -1.2818351209313568,
      -1.1581338720490908,
      -1.217523326016271,
      -1.1887402414591894,
      -1.167667338341567,
      -1.1910287874001235,
      -1.1717178352126072,
      -1.2346294603512518,
      -1.1538985964660944,
      -1.2886998705535104,
      -1.1587693635667373,
      -1.303792795082262,
      -1.1129288573254343,
      -1.1653114886062703,
      -1.2773037873402615,
      -1.2129314209929158,
      -1.2331868665187615,
      -1.2301038470269003,
      -1.249522213642361,
      -1.16042619948246,
      -1.183548017870073,
      -1.223226068293741,
      -1.2322081895811483,
      -1.21322702611067,
      -1.254061100091603,
      0.0,
      -1.1949254078871419,
      -1.2693462970404044,
      -1.2479045287170956
    ],
    [
      -1.1905731423387076,
      -1.1978514115337573,
      -1.1761492818966648,
      -1.2102671355720498,
      -1.1583363620146472,
      -1.1896634366546486,
      -1.1795788057737013,
      -1.1775847336199299,
      -1.1814379208658177,
      -1.1857948013253128,
      -1.207298250300977,
      -1.2044513934964112,
      -1.1840321183848928,
      -1.2052434537620726,
      -1.1712779653870755,
      -1.1645425481456086,
      -1.1618116374682086,
      -1.2142733907995906,
      -1.1877031387363963,
      -1.1965180241540416,
      -1.1741235301081874,
      -1.1894882152346036,
      -1.1662038797132752,
      -1.1754298600881583,
      -1.1543619499224855,
      -1.157285430345217,
      0.0,
      -1.1964043916140414,
      -1.2194769859202297
    ],
    [
      -1.1949518274745965,
      -1.1776025070778005,
      -1.168217125206181,
      -1.1929195267943082,
      -1.2364015955470864,
      -1.195118714120071,
      -1.1706453366974265,
      -1.1675614615935608,
      -1.1743765445695165,
      -1.1657942213457633,
      -1.207049984723704,
      -1.182958034883379,
      -1.1850601907345883,
      -1.1666752003922756,
      -1.209235500637369,
      -1.1671518511483152,
      -1.1555699250481768,
      -1.20244643634152,
      -1.1520339770635246,
      -1.1702147985064855,
      -1.1589376130504805,
      -1.182124412804798,
      -1.1897283421996414,
      -1.1871081478827792,
      -1.176784785357763,
      -1.1673628123598883,
      -1.1841476070450467,
      0.0,
      -1.2429671862926668
    ],
    [
      -1.3296760456968315,
      -1.3141728783761784,
      -1.3088255848919415,
      -1.3197361469459308,
      -1.334898887308433,
      -1.3391685510549793,
      -1.3175609510173298,
      -1.3205609121480106,
      -1.3684322846217831,
      -1.3432928360299554,
      -1.3285597893858696,
      -1.3553099513110671,
      -1.3310229030925196,
      -1.3266989139245464,
      -1.328307524687551,
      -1.3432359650072379,
      -1.3272174421826892,
      -1.3180303744829158,
      -1.3492218451271538,
      -1.330891725538622,
      -1.3317095488893875,
      -1.3212302612628954,
      -1.3319508364347818,
      -1.3251327258490848,
      -1.326128264401959,
      -1.2902224969430527,
      -1.2966051990011522,
      -1.326557875604377,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.42723506313562476,
      0.4124693619145412,
      0.3927098448195463,
      0.34078334147658884,
      0.4104319623437531,
      0.39149708387457327,
      0.3719440284678286,
      0.3772099870809673,
      0.38060895266920225,
      0.3953678178025446,
      0.27451028721887827,
      0.3926394176742538,
      0.4049502863846808,
      0.4003515217103737,
      0.39542261567124903,
      0.32907019923365644,
      0.3422435630204115,
      0.3315667815019938,
      0.37964593040659356,
      0.3828733652363763,
      0.3398614557877093,
      0.35046427499689825,
      0.35260389772225276,
      0.3610695503095771,
      0.34591535118260763,
      0.36705372939641956,
      0.2922302730066644,
      0.31875907275028714
    ],
    [
      0.2986248741097268,
      0.0,
      0.37252446275726925,
      0.41795367931581784,
      0.33799436668982197,
      0.39252980519649805,
      0.3976423646561338,
      0.3286255164466767,
      0.4132309155529341,
      0.3003075116285452,
      0.327486879629169,
      0.23174236991969277,
      0.43999023243467916,
      0.4255283789417954,
      0.2707860519510996,
      0.3855825531277868,
      0.2659537234845093,
      0.31820711806058277,
      0.3147219143344375,
      0.38946620066191673,
      0.3073330945384958,
      0.2707732797222373,
      0.253477661037562,
      0.3870567060202581,
      0.2688426706731386,
      0.32839100387238074,
      0.29400496394104625,
      0.24653557798142267,
      0.23234916611272083
    ],
    [
      0.37300654657511356,
      0.5241413609720973,
      0.0,
      0.4548756664892857,
      0.3870932011375432,
      0.4379517490366929,
      0.45503170104181434,
      0.40826309125816884,
      0.4476138697692289,
      0.3928443229218834,
      0.43564131042034826,
      0.30728882722726136,
      0.517102343843433,
      0.5282651890792902,
      0.3535581675229196,
      0.4766466127825464,
      0.39786767521124533,
      0.3373134344598647,
      0.3754112998732315,
      0.44212266533872646,
      0.4065423950506566,
      0.36553180070102353,
      0.3525830483961627,
      0.4319507129323803,
      0.35970850594140025,
      0.3962758205988832,
      0.38447328794046265,
      0.2691952760516392,
      0.2562015412303629
    ],
    [
      0.38331792372463647,
      0.5789864562626623,
      0.478668364845229,
      0.0,
      0.3534229011008112,
      0.4555839264912487,
      0.5261532480305353,
      0.4226960089085081,
      0.5150164617740018,
      0.3895941519351267,
      0.4255608458406548,
      0.2877199062476361,
      0.5146803216292621,
      0.49750255555340916,
      0.3809125778103859,
      0.4736885205372612,
      0.35645193958408705,
      0.4285126642188657,
      0.34495308045112916,
      0.46666674615856385,
      0.3872175209019846,
      0.3945112625126266,
      0.3545983790478644,
      0.4891899216705564,
      0.3810264600488733,
      0.3877396931179755,
      0.4003161308950609,
      0.30880010477213404,
      0.27766916528920715
    ],
    [
      0.29508186397078195,
      0.3951549145190012,
      0.34333410826402644,
      0.3259318427459299,
      0.0,
      0.3355779963951935,
      0.3648317210073231,
      0.3509619924454468,
      0.3656757482262194,
      0.3412209101072554,
      0.3021029690824124,
      0.2803518920533665,
      0.3722715357132924,
      0.3892408090078685,
      0.3541983513082543,
      0.3473224449836272,
      0.35902516202847967,
      0.3037559979558577,
      0.30461591807705113,
      0.32915556402013,
      0.344618388522862,
      0.30268650079275417,
      0.33152587199433703,
      0.34019333006391994,
      0.3708954142438068,
      0.4112365442583312,
      0.3935361292913164,
      0.3246316592170142,
      0.3161081136111723
    ],
    [
      0.3114445275383986,
      0.4014520249064748,
      0.35778548228387064,
      0.3983483894468771,
      0.32138448497156014,
      0.0,
      0.35251662406521067,
      0.36237928708826606,
      0.3876312151754351,
      0.31607518012476254,
      0.3595156101774688,
      0.24459561422117915,
      0.45138219480727115,
      0.38820353807642083,
      0.3054646598765447,
      0.3770893335656633,
      0.3116687932235669,
      0.28165584186963577,
      0.33086436806240993,
      0.39415484554568625,
      0.32785168548562305,
      0.2902116930316285,
      0.29456053812339,
      0.3173433463762343,
      0.2768476483370912,
      0.3441767189650866,
      0.26852576586532684,
      0.23083583760426363,
      0.2189128101189901
    ],
    [
      0.28625367510557886,
      0.43416186417100744,
      0.41031191696831026,
      0.4255384831754718,
      0.3759878738397484,
      0.40964789116995814,
      0.0,
      0.371815717551339,
      0.4221695726489769,
      0.3529802664142485,
      0.40883430058628445,
      0.25672394949867683,
      0.4338136211385035,
      0.45479529666059304,
      0.3099734873075759,
      0.37085967351958105,
      0.3513764829182753,
      0.3675847746105385,
      0.33994969989027424,
      0.4317628784473697,
      0.34360557605600794,
      0.30822291490262743,
      0.29971523521880905,
      0.42357689803740506,
      0.32867524979688323,
      0.3765833428938792,
      0.3422038172103319,
      0.2697235399832232,
      0.2454235461270997
    ],
    [
      0.2760305768823612,
      0.37645086788574744,
      0.3451506643234674,
      0.31212311477906596,
      0.2873452286446878,
      0.3033596060422856,
      0.3545773902232112,
      0.0,
      0.3239740499424557,
      0.3190186419164036,
      0.33959236916830626,
      0.23981846924117023,
      0.3768338581492128,
      0.35980430107711836,
      0.34197049964893345,
      0.38536258697034187,
      0.3130039681459573,
      0.29972895169476543,
      0.37148154757597385,
      0.3622986194877946,
      0.3813817024213597,
      0.29289446537755714,
      0.3040112425542314,
      0.37203153426631985,
      0.3244028133779573,
      0.3015276538746976,
      0.30817505235492804,
      0.2841303620093165,
      0.25209025482860126
    ],
    [
      0.3650767885098911,
      0.5833093291923497,
      0.4615738261291731,
      0.5400950667119966,
      0.3540500893714087,
      0.4662297316872601,
      0.5221740042632796,
      0.418749957135538,
      0.0,
      0.36969453777005357,
      0.45685370733813535,
      0.31247921004639867,
      0.5848362870113941,
      0.5655198544893825,
      0.3512997561201521,
      0.4389224136940948,
      0.3947743901616556,
      0.4377516292640662,
      0.4132270658232704,
      0.523007514655121,
      0.4166758272329485,
      0.36701853510214977,
      0.34075727453528337,
      0.47515972275836904,
      0.3641289332417541,
      0.45805938478305075,
      0.36399379233256,
      0.33801168869608555,
      0.2702612557102875
    ],
    [
      0.35899179249721014,
      0.3530989179546038,
      0.33288311494302136,
      0.3636548813242304,
      0.2936552113213242,
      0.35859198737292175,
      0.38443804685843985,
      0.36508485021096426,
      0.3505276306329288,
      0.0,
      0.3422974771176637,
      0.2624397745569502,
      0.36488863437732677,
      0.35601437536524383,
      0.32836817161605647,
      0.3933257937224728,
      0.35369570307037734,
      0.3370292855592243,
      0.33878829160721247,
      0.3698343512211324,
      0.3218407437510218,
      0.2844779043260701,
      0.357197548322022,
      0.37189616738575837,
      0.3806624618703509,
      0.26778728355046666,
      0.29689000683933386,
      0.2686315799249708,
      0.2629414680194191
    ],
    [
      0.2746841074798392,
      0.42256486720203434,
      0.4056054125948436,
      0.41628868948631537,
      0.3633899395474307,
      0.3994886541012024,
      0.44618133978029895,
      0.3832319072880359,
      0.4093016502259783,
      0.3191142811667833,
      0.0,
      0.2804710810645139,
      0.44687746346228097,
      0.4345567652870974,
      0.33869970928410753,
      0.3899230576134656,
      0.3163661267430484,
      0.3917242842597748,
      0.3728539616568045,
      0.4077219805456558,
      0.3322417518138481,
      0.3456571956579111,
      0.3204766995753705,
      0.3781144185925731,
      0.33521837088168227,
      0.408814132619705,
      0.32336049964561675,
      0.2926549680868298,
      0.2738701784820674
    ],
    [
      0.2650663412061016,
      0.23036661182416984,
      0.2598921626451747,
      0.2716720582381358,
      0.2517783460782792,
      0.26206274939099927,
      0.2718388582972777,
      0.28526909294558256,
      0.251368283897359,
      0.28141400655671744,
      0.26402696800665404,
      0.0,
      0.2815504742897259,
      0.2720843774831174,
      0.2778526532765586,
      0.27173764717612303,
      0.2745062731272636,
      0.2540650074422408,
      0.2631174365434681,
      0.25146320727674065,
      0.26479800005653753,
      0.26918105414391635,
      0.24892592358228205,
      0.26244588652231027,
      0.29266999394287585,
      0.21789622794334917,
      0.27475589214749907,
      0.25723110409124383,
      0.20397021257679127
    ],
    [
      0.29962210748109075,
      0.48428315309536485,
      0.3857455246732,
      0.39788612092384623,
      0.27668103841136604,
      0.39353367695421615,
      0.39815326779046845,
      0.3399458890778808,
      0.4423821558262333,
      0.3243014490744045,
      0.3771770892175721,
      0.2207817704601751,
      0.0,
      0.46683069207573435,
      0.30830203535545997,
      0.39892638965266425,
      0.3043613869849231,
      0.30505580561277346,
      0.330666146327218,
      0.4272244909680514,
      0.3780538751272471,
      0.29312793341428267,
      0.2813408991818389,
      0.3646353510079654,
      0.28011606302852265,
      0.39400416979056385,
      0.28662779660483073,
      0.2537172076393932,
      0.1769360653188765
    ],
    [
      0.3599091762695936,
      0.5015273707985688,
      0.4342642946967701,
      0.44437866501500545,
      0.3669966600125312,
      0.4122019976315434,
      0.4778096636094813,
      0.384932835809509,
      0.5029162412822272,
      0.3601756071577211,
      0.40364187521243866,
      0.2659770719864629,
      0.501159339858744,
      0.0,
      0.3204351777711769,
      0.3669259224471433,
      0.3567124172308076,
      0.32143909468822773,
      0.34445066754902753,
      0.45175293525355587,
      0.3861060277020929,
      0.31651955924235486,
      0.3235457038993932,
      0.4331074627702407,
      0.3354986825867463,
      0.40873279494845405,
      0.315879125288584,
      0.28066110036254277,
      0.2440885515970319
    ],
    [
      0.3948088078187457,
      0.40662517752120153,
      0.3782780067928442,
      0.3962245719957085,
      0.36853902917836656,
      0.3630713269267358,
      0.40373870282990354,
      0.4327318983166948,
      0.36478731197721426,
      0.3772468156992441,
      0.40449146418282966,
      0.3119423248991262,
      0.4208554037582992,
      0.41296674457824034,
      0.0,
      0.4470131622954927,
      0.3394933455681006,
      0.337083186027213,
      0.34808286091561946,
      0.3698421615511598,
      0.3500897633872049,
      0.3671372156292545,
      0.33563921667188357,
      0.44661087093622753,
      0.3864590085571866,
      0.3531693611619693,
      0.42079631503520254,
      0.3369440552053795,
      0.2763708227956385
    ],
    [
      0.3698426725741115,
      0.5217018246069762,
      0.47872129262897123,
      0.48672016420365005,
      0.3741352078029374,
      0.4635559152672968,
      0.4823700112881728,
      0.47500314068275684,
      0.46479072209756245,
      0.39640525297619034,
      0.43076867579428546,
      0.2861842790979656,
      0.5153601283721576,
      0.45738208340263675,
      0.43844360350031875,
      0.0,
      0.40759751811771827,
      0.3659942644430234,
      0.4520657440242808,
      0.45703612585457853,
      0.4283038371202452,
      0.3823630269921161,
      0.3751529866026202,
      0.4465714147378719,
      0.3815632557265316,
      0.4295318418033347,
      0.41550641698179347,
      0.37181631966130113,
      0.31330079231504193
    ],
    [
      0.370884251192239,
      0.4220256757198284,
      0.4367076006888846,
      0.42225811513143263,
      0.39919610340639955,
      0.4413582645031291,
      0.4705080817650171,
      0.43823482898633226,
      0.39946939955319616,
      0.3833852731230334,
      0.3787247283162507,
      0.32943372928424286,
      0.4215842584165852,
      0.4383462700636862,
      0.3712612250878513,
      0.4404155080639349,
      0.0,
      0.4130438230232325,
      0.4292927236336974,
      0.4308525646398351,
      0.41742444872740236,
      0.3761851080961258,
      0.4163883612291477,
      0.4708351864283211,
      0.4363931158439651,
      0.37293686520341884,
      0.37296553384099806,
      0.3359455859058511,
      0.34623881712002813
    ],
    [
      0.354002096731725,
      0.4491538372929984,
      0.3616157224389247,
      0.47629125224634494,
      0.3352520425253225,
      0.38558807465991074,
      0.47066013554907005,
      0.4336649234864054,
      0.455949960394312,
      0.3793001886966303,
      0.4187824727812146,
      0.3195242789134678,
      0.4707186578528424,
      0.4350027968768244,
      0.36358038328624676,
      0.37591298926118943,
      0.38058302796067767,
      0.0,
      0.3802581407364112,
      0.4055560262017084,
      0.4021361207557588,
      0.34808249246900025,
      0.34356897126750385,
      0.4784880979273032,
      0.37638294134653494,
      0.39562570003471254,
      0.36880945389269515,
      0.347292837318367,
      0.2801511433047632
    ],
    [
      0.2513364816134047,
      0.34541924316582917,
      0.3495966411631568,
      0.29144666269676645,
      0.22480852573810006,
      0.32892538484633627,
      0.35574440184314704,
      0.3492319293314259,
      0.30561876880734173,
      0.25466445420519035,
      0.30768286599702854,
      0.24333740815753835,
      0.3500207246248037,
      0.33687655362662405,
      0.28064250676266944,
      0.3073053902886136,
      0.33743064822405056,
      0.27662251441002317,
      0.0,
      0.3605355668290271,
      0.31309429090999275,
      0.28006670012144275,
      0.2981938891719613,
      0.3500168771426124,
      0.27508445270403525,
      0.322416571986079,
      0.291142466001421,
      0.3197260798578907,
      0.23093385264898791
    ],
    [
      0.29950006207699986,
      0.42675022396731777,
      0.3715688946226652,
      0.39378385532777327,
      0.29254986406534345,
      0.4046339561314536,
      0.40829614148657845,
      0.35153221801136203,
      0.4218818456722382,
      0.31134885153532266,
      0.3703713938996904,
      0.2637962486392973,
      0.45326093438369086,
      0.4471081263782746,
      0.2914050573249818,
      0.37525715546931737,
      0.33726593468133226,
      0.35980216220989325,
      0.333934585022746,
      0.0,
      0.34764234333223,
      0.31976573664147256,
      0.32764681269990525,
      0.4240622400510057,
      0.3123809224737135,
      0.3873382058377286,
      0.3108176621125507,
      0.29142354159034545,
      0.25266074930242177
    ],
    [
      0.2962805371804278,
      0.3512807038323731,
      0.36010581085947724,
      0.3332693975800063,
      0.24677177614877488,
      0.35712042677086453,
      0.3518871706539701,
      0.39186941806944997,
      0.3300138346875965,
      0.3185118195457699,
      0.29831599865486935,
      0.2448118827901331,
      0.3684397565475319,
      0.34919704796597495,
      0.31369040317342245,
      0.3405599421705987,
      0.36241255100674485,
      0.3286389407598884,
      0.3493853437692278,
      0.35095049005102474,
      0.0,
      0.2797517005122174,
      0.30186692279370453,
      0.3438567513442168,
      0.338118273680025,
      0.35612206655427237,
      0.2990168822067405,
      0.24664349801864116,
      0.22862774872973235
    ],
    [
      0.3123412897469129,
      0.38863952211667185,
      0.39295788150792643,
      0.3846928434743693,
      0.32912690145704304,
      0.3592403582737811,
      0.4046765669076817,
      0.36970248899940894,
      0.3943384720626677,
      0.3394767105768999,
      0.38814833263413195,
      0.33834393719581124,
      0.3916146648194221,
      0.409843520699658,
      0.4167016853010723,
      0.4427204365633428,
      0.3665363414968217,
      0.3269319854058865,
      0.3844286899495406,
      0.38293100521816803,
      0.3770427086387347,
      0.0,
      0.37981060908930053,
      0.39371925038761724,
      0.35140167289467517,
      0.4122724706964338,
      0.37685149621660674,
      0.36538967487937213,
      0.3359205324991372
    ],
    [
      0.3657780890295286,
      0.3806568390906089,
      0.3814775924108602,
      0.3727620466035244,
      0.31806251707598276,
      0.34401272352906753,
      0.38786797796622485,
      0.3759866177688147,
      0.33217459158606344,
      0.4176324070883728,
      0.3788769444499154,
      0.25422296503436637,
      0.3485458115500766,
      0.3266721460439257,
      0.36784109144285804,
      0.372057081236818,
      0.38528953308462266,
      0.33480746740297973,
      0.3874838088335788,
      0.3585272423179633,
      0.3944732071746466,
      0.31913612742011965,
      0.0,
      0.3786230102657664,
      0.44183223857070586,
      0.3238228406207617,
      0.3470254626297353,
      0.2729200717862872,
      0.25313490983130515
    ],
    [
      0.34404075741252527,
      0.501495337427619,
      0.41856221807804705,
      0.5290031629928418,
      0.3511870475533898,
      0.3991365578301307,
      0.4988236533131434,
      0.4284666840022602,
      0.4731429172000352,
      0.3927154388858529,
      0.4214576918645916,
      0.30718961006359624,
      0.458856428280082,
      0.45742744929326795,
      0.40251994594185136,
      0.44239469793544517,
      0.4349079534368945,
      0.4232994421623677,
      0.4249126971338266,
      0.447222215341079,
      0.40251763805845386,
      0.3767661475581321,
      0.34971964786305354,
      0.0,
      0.3954356520849476,
      0.4143513304805031,
      0.3731306044606497,
      0.3665197851155897,
      0.2830878282238325
    ],
    [
      0.34170914277884035,
      0.36927973469488373,
      0.3703448350473184,
      0.36369884275183706,
      0.3130400710950947,
      0.34832029339169535,
      0.3535814853919015,
      0.3886401294007471,
      0.33570550578643266,
      0.3876417148290794,
      0.3200334370676179,
      0.28092228282304976,
      0.3506314505152708,
      0.3534002436217951,
      0.3887437337833952,
      0.32691139927018265,
      0.335750124458718,
      0.3443614541310851,
      0.36839030205127843,
      0.33491168010967853,
      0.40710521772461683,
      0.28706964161552584,
      0.38398910348952286,
      0.35140072414812873,
      0.0,
      0.29219088402351057,
      0.3328923992232342,
      0.3085885544361968,
      0.2773454752965532
    ],
    [
      0.2925047776397627,
      0.41620602652202865,
      0.35681657255484844,
      0.38559965711193,
      0.40667256022955245,
      0.3833111111709959,
      0.40262206335851225,
      0.3397104382198677,
      0.42044130210502506,
      0.285640028017609,
      0.4155705350043821,
      0.27054710348885735,
      0.4614110412456851,
      0.4090284099648491,
      0.29703611123085794,
      0.3614084775782036,
      0.3411530320523579,
      0.3442360515442191,
      0.3248176849287585,
      0.4139136990886594,
      0.3907918807010464,
      0.3511138302773784,
      0.34213170898997114,
      0.36111287246044954,
      0.32027879847951635,
      0.0,
      0.3794144906839776,
      0.3049936015307151,
      0.3264353698540239
    ],
    [
      0.3267902611428415,
      0.3195119919477918,
      0.3412141215848843,
      0.3070962679094993,
      0.3590270414669019,
      0.3276999668269005,
      0.33778459770784774,
      0.3397786698616192,
      0.3359254826157314,
      0.3315686021562363,
      0.31006515318057204,
      0.3129120099851379,
      0.3333312850966563,
      0.31211994971947643,
      0.34608543809447356,
      0.35282085533594043,
      0.3555517660133405,
      0.30309001268195845,
      0.32966026474515275,
      0.32084537932750745,
      0.3432398733733617,
      0.3278751882469455,
      0.35115952376827386,
      0.34193354339339077,
      0.36300145355906355,
      0.36007797313633216,
      0.0,
      0.32095901186750764,
      0.2978864175613194
    ],
    [
      0.2973908817947246,
      0.31474020219152066,
      0.32412558406314007,
      0.2994231824750129,
      0.2559411137222347,
      0.29722399514925013,
      0.32169737257189457,
      0.32478124767576033,
      0.31796616469980465,
      0.32654848792355784,
      0.28529272454561716,
      0.3093846743859421,
      0.30728251853473276,
      0.32566750887704554,
      0.28310720863195216,
      0.3251908581210059,
      0.3367727842211443,
      0.2898962729278012,
      0.3403087322057965,
      0.32212791076283565,
      0.3334050962188406,
      0.31021829646452304,
      0.3026143670696797,
      0.3052345613865419,
      0.315557923911558,
      0.32497989690943285,
      0.3081951022242744,
      0.0,
      0.24937552297665433
    ],
    [
      0.24446090472424853,
      0.2599640720449017,
      0.2653113655291386,
      0.25440080347514926,
      0.23923806311264695,
      0.2349683993661007,
      0.2565759994037502,
      0.2535760382730694,
      0.20570466579929692,
      0.23084411439112462,
      0.24557716103521043,
      0.2188269991100129,
      0.24311404732856046,
      0.24743803649653362,
      0.24582942573352895,
      0.23090098541384219,
      0.24691950823839082,
      0.2561065759381642,
      0.22491510529392622,
      0.24324522488245814,
      0.2424274015316925,
      0.2529066891581846,
      0.2421861139862982,
      0.24900422457199523,
      0.24800868601912107,
      0.28391445347802735,
      0.2775317514199278,
      0.24757907481670305,
      0.0
    ]
  ],
  "row_avgs": [
    0.36648175059985905,
    0.3292022515285127,
    0.4026607651358453,
    0.41646990140572493,
    0.34268727478239397,
    0.3293885021047978,
    0.3625811266374493,
    0.3253060854594368,
    0.4269175562059682,
    0.3364261948328114,
    0.36533762479089693,
    0.26189310181101766,
    0.34251498396700597,
    0.37934807223849926,
    0.3768228190075959,
    0.423863875667016,
    0.40651055167835953,
    0.3925690987931737,
    0.30492578045983926,
    0.3531352044624162,
    0.3227684677163456,
    0.37556435891818213,
    0.35434654863733867,
    0.41136751928550036,
    0.3434499951056139,
    0.36088997271550155,
    0.3324647179395237,
    0.30908750688008135,
    0.24612413894900018
  ],
  "col_avgs": [
    0.3217421898145486,
    0.4137922576450806,
    0.3781290298932137,
    0.3877902617302632,
    0.3258610909707569,
    0.3741199460163365,
    0.401774274126245,
    0.3745289587757043,
    0.38453316882426664,
    0.34215285639618653,
    0.364723528535995,
    0.2770099984861038,
    0.41332331556127777,
    0.4022061895389487,
    0.3374664514233956,
    0.3790215894452839,
    0.3447320824895988,
    0.33678505734944875,
    0.35195017366133363,
    0.3865991150772401,
    0.3618154921982603,
    0.3221111948541888,
    0.32725887625565253,
    0.3835991064752854,
    0.34291647193329416,
    0.3598532351544982,
    0.3390675723815401,
    0.2983475704077461,
    0.2678946922940127
  ],
  "combined_avgs": [
    0.34411197020720385,
    0.3714972545867966,
    0.3903948975145295,
    0.40213008156799407,
    0.3342741828765754,
    0.35175422406056717,
    0.3821777003818472,
    0.34991752211757055,
    0.4057253625151174,
    0.339289525614499,
    0.36503057666344596,
    0.2694515501485607,
    0.37791914976414187,
    0.390777130888724,
    0.3571446352154958,
    0.40144273255614993,
    0.37562131708397917,
    0.3646770780713112,
    0.3284379770605864,
    0.36986715976982815,
    0.342291979957303,
    0.34883777688618545,
    0.3408027124464956,
    0.39748331288039285,
    0.343183233519454,
    0.3603716039349999,
    0.33576614516053194,
    0.3037175386439137,
    0.2570094156215064
  ],
  "gppm": [
    589.9466596669807,
    553.862387562833,
    570.7127467767497,
    566.0750526111991,
    592.5330461251503,
    568.9989081841346,
    558.7309005160134,
    570.8564339763755,
    568.6631856819741,
    586.4566828292501,
    579.3679258819953,
    617.0801820728202,
    554.2093322350382,
    560.7344476767233,
    593.1112270169733,
    573.1921895327348,
    585.6146875955557,
    593.3207644789344,
    584.7997274841756,
    567.0307190738174,
    573.2562230637875,
    600.7594254031544,
    595.9451775162444,
    567.9767100816211,
    585.9417265289961,
    581.4259125498926,
    592.9913533040602,
    610.3143590054724,
    617.2641750563095
  ],
  "gppm_normalized": [
    1.3899919927067859,
    1.3230724145504813,
    1.3681076366274447,
    1.3599749562911188,
    1.4164153557287784,
    1.3648685219738792,
    1.3390914015795687,
    1.3755643952889398,
    1.3617830476092296,
    1.4076152898080552,
    1.386977997247856,
    1.497557209783903,
    1.3264318240606323,
    1.3411739026670964,
    1.42666576290576,
    1.3748568257506713,
    1.4058246783919741,
    1.4242294775201223,
    1.4026638132152573,
    1.3588985946456635,
    1.3776630924748223,
    1.4420688329382148,
    1.4283116485492715,
    1.3666241671018984,
    1.407553623826424,
    1.3934422360075005,
    1.42098451550674,
    1.4652031556703353,
    1.4802450991407992
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534
  ],
  "response_lengths": [
    1573,
    3076,
    2527,
    2401,
    2540,
    2913,
    2671,
    2699,
    2235,
    2343,
    2557,
    2973,
    2852,
    2774,
    2369,
    2252,
    2240,
    2312,
    2548,
    2609,
    2538,
    2151,
    2257,
    2405,
    2560,
    2332,
    2208,
    2260,
    2944
  ]
}