{
  "example_idx": 17,
  "reference": "Under review as a conference paper at ICLR 2023\n\nCAUSAL ATTENTION TO EXPLOIT TRANSIENT EMERGENCE OF CAUSAL EFFECT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe propose a causal reasoning mechanism called causal attention that can improve performance of machine learning models on a class of causal inference tasks by revealing the generation process behind the observed data. We consider the problem of reconstructing causal networks (e.g., biological neural networks) connecting large numbers of variables (e.g., nerve cells), of which evolution is governed by nonlinear dynamics consisting of weak coupling-drive (i.e., causal effect) and strong self-drive (dominants the evolution). The core difficulty is sparseness of causal effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in the neural activity sequence. Causal attention is designed to guide the model to make inference focusing on the critical regions of time series data where causality may manifest. Specifically, attention coefficients are assigned autonomously by a neural network trained to maximise the Attentionextended Transfer Entropy, which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, causal attention explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real causal networks using data generated by neuronal models widely used in neuroscience.\n\n1\n\nINTRODUCTION\n\nIn this work, our task is to infer causal relationships between observed variables based on time series data and reconstruct the causal network connecting large numbers of these variables. Assume the time series xit record the time evolution of variable i governed by coupled nonlinear dynamics, as represented by a general differential equation ̇xit = g(xit) + (cid:80) Bijf (xit, xjt), where g and f are self- and coupling functions respectively. The parent variable influences the dynamic evolution of the child variable via the coupling function f . Note that these two functions are hidden and usually unknown for real systems. The asymmetric adjacency matrix B represents the causal, i.e., directional coupling relationship between variables. Hence, the goal is to infer matrix B from observed time series xit, i = 1, 2, . . . , N where N is the number of variables in the system. If Bij = 1, the variable i is a coupling driver (parent variable) of variable j, otherwise it is zero.\n\nThe key challenge is that the causal effect in neural dynamics (e.g., biological neural systems observed via neuronal activity sequences) is too weak to be detected, rendering powerless classic unsupervised techniques of causal inference across multiple research communities Granger (1969); Schreiber (2000); Sugihara et al. (2012); Sun et al. (2015); Nauta et al. (2019); Runge et al. (2019); Gerhardus & Runge (2020); Tank et al. (2021); Mastakouri et al. (2021). This difficulty manifests in three aspects. First, the dynamics contains self-drive and coupling-drive. The strength of coupling f (xit, ·) is usually many orders of magnitude smaller than self-drive g(xit), and the latter dominates evolution. Second, the behavior of the coupling-drive is chaotic, unlike in linear models Shimizu et al. (2006); Xie et al. (2020). The resulting unpredictability and variability of system state means that coupling force can be significant momentarily and otherwise almost vanish, as illustrated in Figure 3 (gray lines). This dilutes the information in time series that can be useful for inferring the causal relationship. Third, in the heterogeneous networks common in applications, some variables are hubs coupled with many parent variables, among which it is difficult to distinguish individual causes. When causal effects are weak, we do not observe clearly the principle of Granger Causality, whereby the parent variable can help to explain the future change in its child variable Pfister et al.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(2019). Rather, when we train a machine learning model Nauta et al. (2019); Tank et al. (2021) for prediction task on the neuronal activity sequences, the model only exploits the historical information of the child variable itself and that from parent variables is ignored. We posit that coupling-drive makes a negligible contribution to dynamic evolution in the majority of samples of time series data. In other words, only in a small fraction of samples is the information of parent variables effective in predicting the evolution of child variables. Taking as an example the gradient algorithm to minimise the regression error over all samples (cid:80) t(xit − ˆxit)2, the adjustment of model parameters from the tiny samples corresponding to significant coupling force is negligible, but these are the only samples which could induce the model to exploit causal effects in reducing regression error. Similarly, for transfer entropy Schreiber (2000), which measures the reduction in uncertainty which a potential parent variable provides to a potential child variable, there is no significant difference in measured value between ordered pairs of variables with and without causality.\n\nTo overcome the difficulty, we propose a causal reasoning mechanism – causal attention – to identify the moments when causal effect emerges. We design an objective function, Attention-extended Transfer Entropy (AeTE), comprising a weighted generalisation of transfer entropy. In order to maximize AeTE, the causal attention mechanism trains neural networks to autonomously allocates high attention coefficients at at times t where information of parent variables effectively reduces the uncertainty of child variables, and ignores other positions by setting at close to zero. If we consider each value in a time series as a feature, the operation of attention allocation is also equivalent to removing the non-causal features Kusner et al. (2017); Hu et al. (2021).\n\nHowever, noise in empirical samples may also produce high transfer entropy regions, which leads to spurious causal effects even when using causal attention. We add a binary classification model to perform more sophisticated inference under the guidance of causal attention to focus on these critical regions and recognize different patterns between noisy and sparse emergence of causal effect. We deal with this class of causal inference task by way of small sample supervised learning. Although training and test data have a distribution shift in the setting of small samples, they arise through an identical underlying generation process. Thus, if the model provides an insight into the underlying dynamics – the coupling-drive for causal inference – then the understanding acquired from small samples can be effectively utilised in the test environment Bareinboim & Pearl (2014); Battaglia et al. (2016); Makhlouf et al. (2020); Pessach & Shmueli (2022). The role of causal attention is to help the classification model gain this insight. Our contributions are summarized as follows:\n\n1. We introduce causal attention, a causal reasoning mechanism to identify the positions of time series at which causal effect emerges and guide a classification model to infer causality focusing on these critical positions. Without any prior knowledge of dynamics, the mechanism determines the areas where the coupling force is substantially different from zero.\n\n2. By formulating Transfer Entropy as the difference between two types of mutual information, and based on the dual representation of Kullback-Liebler (KL) divergence, we design a differentiable metric, Attention-extended Transfer Entropy, as the objective function of the proposed causal attention mechanism.\n\n3. Our method significantly improves performance on synthetic and real causal networks using the data generated by five well-known neural dynamic models, and the number of labels required is very small compared to the size of the causal networks.\n\nOur methodology has limitations (i.e., cases for which performance improvement is less): 1. Dense networks, where a variable is coupled with many driving variables such that their causal effects overlap and are harder to distinguish. 2. Intense noise, which makes the casual attention mechanism falsely identify high transfer entropy regions. The downstream classifier then extracts non-causal features, leading to the reduction of its generalization. 3. Strongly coupled system, which is dominated by synchronization phenomena in which the dynamic behaviors of all variables are similar.\n\n2 BACKGROUND\n\n2.1 DEFINITION OF TRANSFER ENTROPY\n\nThe transfer entropy, an information-theoretic causality measure, is able to detect information flow between time series X and Y . Transfer Entropy measures the degree of non-symmetric dependence\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nof Y on X, defined as Schreiber (2000):\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\np\n\n(cid:17)\n\nlog\n\n(cid:16)\n\n(cid:17)\n\n, x(l)\n\nt\n\nyt+1 | y(k) (cid:16)\n\nt\n\nyt+1 | y(k)\n\nt\n\np\n\n(cid:17) ,\n\n(1)\n\nwhere x(l) t = (yt, ..., yt−k+1) and k, l are lengths. For an uncoupled system (X and Y independent) that can be approximated by a Markov process of order k,\n\nt = (xt, ..., xt−l+1) and y(k)\n\nthe conditional probability to find Y in state yt+1 at time t + 1 satisfies p\n\n(cid:16)\n\np\n\nyt+1 | y(k)\n\nt\n\n(cid:17)\n\n.\n\n(cid:16)\n\nyt+1 | y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n=\n\n2.2 MUTUAL INFORMATION NEURAL ESTIMATION\n\nThe mutual information is equivalent to the KL divergence between the joint distribution PXY and the product of the marginal distributions PX ⊗ PY Nowozin et al. (2016); Hjelm et al. (2018). The KL divergence DKL admits the neural dual representation Donsker & Varadhan (1983); Belghazi et al. (2018):\n\nM I(X, Y ) = DKL (PXY ∥PX , PY ) ≥ sup θ∈Θ\n\nEPXY [fθ] − log (cid:0)EPX ⊗PY\n\n(cid:2)efθ (cid:3)(cid:1) ,\n\n(2)\n\nwhere the supremum is taken over parameter space Θ and fθ is the family of functions parameterized by the neural network with parameters θ ∈ Θ. The mutual information neural estimator is strongly consistent and can approximate the actual value with arbitrary accuracy Belghazi et al. (2018).\n\n3 METHOD\n\n3.1 NEURAL ESTIMATOR OF TRANSFER ENTROPY\n\nFigure 1: Visual interpretation of transfer entropy and its attention-extended version. The Transfer Entropy is derived as the difference of two types of mutual information: M I (Yt+1, (Yt, Xt)) (blue area) quantifies the reduction in uncertainty of future state yt+1 from knowing current states (yt, xt), and M I (Yt+1, Yt) (green area) is same but only yt is known. The attention coefficients at (yellow area) are assigned to each position of time series by the causal attention mechanism to maximize the Attention-extended Transfer Entropy. For brevity, k = l = 1 here.\n\nBy the conditional Bayes formula and adding a marginal distribution of Y , we derive the transfer entropy as the difference between two types of mutual information. An intuitive description is provided in Figure 1, and the derivation is placed in Appendix A.\n\nT E(X → Y )\n\n(cid:88)\n\n(cid:16)\n\np\n\n=\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n=M I\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (l)\n\nt\n\n(cid:17)(cid:17)\n\n(cid:16)\n\np\n\nyt+1, y(k) (cid:16)\n\nt\n\n(cid:17)\n\n, x(l)\n\nt\n\ny(k)\n\nt\n\n, x(l)\n\nt\n\np (yr) p (cid:16)\n\n− M I\n\nYt+1, Y (k)\n\nt\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n(cid:17)\n\nlog\n\n(cid:17) −\n\n(cid:17)\n\n.\n\n(cid:17)\n\n(cid:16)\n\np\n\nyt+1, y(k) (cid:16)\n\nt\n\ny(k)\n\nt\n\np (yr) p\n\n(cid:17) (3)\n\n(4)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nIn these expressions, yr is sampled from Y randomly and independently of the time step t. The first term M I quantifies the reduction in the uncertainty of the future state\n\n, X (l)\n\nY (k)\n\n(cid:17)(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nYt+1,\n\nt\n\nt\n\nyt+1 from knowing the historical information y(k) the reduction in uncertainty simply from knowing y(k) differentiable estimator of transfer entropy as:\n\nt\n\nt\n\nand x(l)\n\nt\n\n. The second term M I\n\n(cid:16)\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\nis\n\n. By connecting Eq. 4 and Eq. 2, we define a\n\nT E(X → Y ) = sup\n\nE\n\nΘ\n\n(cid:16)\n\nP\n\nYt+1,Y (k)\n\nt\n\n,X (l)\n\nt\n\n(cid:17) [fθ] − log\n\nE\n\nP (Yt+1)⊗P\n\n(cid:18)\n\n(cid:19)\n\n(cid:17) (cid:2)efθ (cid:3)\n\n,X (l)\n\nt\n\n(cid:16)\n\nY (k)\n\nt\n\n− sup\n\nE\n\nΦ\n\n(cid:16)\n\nP\n\nYt+1,Y (k)\n\nt\n\n(cid:17) [fφ] − log\n\n(cid:16)\n\nEP (Yt+1)⊗P (Y (k)\n\nt\n\n)\n\n(cid:2)efφ(cid:3)(cid:17)\n\n.\n\n(5)\n\nTransfer entropy, and even mutual information, is difficult to compute Paninski (2003), especially for high-dimensional or noisy data. In Appendix B, we offer a theoretical proof for the consistency and convergence properties of Transfer Entropy Neural Estimation, and examine its bias on a linear dynamic system where the true values of transfer entropy can be determined analytically.\n\n3.2 ATTENTION-EXTENDED TRANSFER ENTROPY\n\nThe main difficulty in our task is that the causal effect in certain nonlinear dynamical systems is too weak to be recognized by classic techniques. We discuss the limitation of the iconic transfer entropy in detail that it works well when the three true distributions, i.e., one joint distribution and two conditional distributions in Eq. 1, can be estimated perfectly. However, sparse causal effects are easily masked if the estimated probability density deviates even slightly from the real distribution. These momentary sources of evidence of coupling drive are like outliers in the total distribution of a time series dominated by self-drive. In order to make the transfer entropy provide a clear distinction between causal and non-causal pairs, we need to highlight the positions where\n\n(cid:16)\n\np\n\nyt+1 | y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n(cid:16)\n\n> p\n\nyt+1 | y(k)\n\nt\n\n(cid:17)\n\nand filter out other times by adjusting at in Eq. 6, all while\n\navoiding the problem of distribution approximation. We do so by defining AeTE as: (cid:17)\n\n(cid:16)\n\nAeT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\nat · p\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n= M I\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (k)\n\nt\n\n(cid:17)\n\n(cid:17)\n\n| A\n\np\n\np\n\nyt+1 | y(k) (cid:16)\n\nt\n\n, x(l) t\n(cid:17)\n\nyt+1 | y(k) (cid:16)\n\nt\n\nYt+1, Y (k)\n\nt\n\n− M I\n\n(6)\n\n(7)\n\n(cid:17)\n\n.\n\n| A\n\nIn this expression, at ∈ [0, 1] is the attention coefficient at time step t and the collection A of attention coefficients is the attention series. Comparison of Eq.1 and Eq.6 reveals that the transfer entropy can be viewed as a simplified version of AeTE in which attention coefficients are uniformly set to one: ∀t, at = 1. Because each position has an equal contribution to estimation, the value of transfer entropy is dominated by the majority of positions where causal effect is negligible, i.e.,\n\n(cid:16)\n\nwhere p\n\nyt+1 | y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n(cid:16)\n\n≈ p\n\nyt+1 | y(k)\n\nt\n\n(cid:17)\n\n. Similarly to transfer entropy, AeTE is derived as the\n\ndifference of two mutual informations, but AeTE incorporates the scheme of attention assignment. By connecting Eq. 7 and Eq. 2, we define a differentiable estimator of AeTE as:\n\nAeT E(X → Y ) = sup\n\nΘ\n\n1 L\n\n(cid:88)\n\nat · fθ\n\n(cid:16)\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n− log\n\n(cid:18) 1 L\n\n(cid:88)\n\nat · efθ\n\n(cid:16)\n\nyr,y(k)\n\nt\n\n(cid:17)(cid:19)\n\n,x(l)\n\nt\n\n− sup\n\nΦ\n\n1 L\n\n(cid:88)\n\nat · fφ\n\n(cid:16)\n\nyt+1, y(k)\n\nt\n\n(cid:17)\n\n− log\n\n(cid:18) 1 L\n\n(cid:88)\n\nat · efφ\n\n(cid:16)\n\nyr,y(k)\n\nt\n\n(cid:17)(cid:19)\n\n,\n\n(8)\n\nwhere T is the total number of time steps and L = T −max(k, l). The expectation on the distribution of variables is adapted into the mean over time series.\n\n3.3 CAUSAL ATTENTION MECHANISM\n\nThe overall framework of our model is presented in Figure 2. In addition to two neural networks fθ and fφ for mutual information estimation, we employ another neural network gα for causal attention\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Graphical illustration of causal attention mechanism framework. An input sample is the time series on an order pair of variables with shape [2, L]. Stage 1: the neural network gα assigns the attention coefficients {ai}L i=1. The neural networks fθ and fφ forming transfer entropy estimator estimate mutual information M I1 and M I2 (first and second terms in Eq. 7). Stage 2: the inferred probability of causality is Sigmoid( 1 L\n\n(cid:80) ai ∗ logiti).\n\nassignment. Rather than approximating distributions, the neural network gα learns to maximize AeTE given by Eq. 8 via gradient descent. However, the occurrence of high transfer entropy regions may appear due to noise in empirical samples. For more sophisticated inference, we augment our method with a binary classifier hη guided by causal attention to focus on high transfer entropy regions and recognize different patterns between noise and sparse emergence of causal effect. The classifier takes causality and non-causality as class labels. Then, the training process is divided into two independent stages: causal attention learning and classification learning. The objectives in the first stage are:\n\nθ, φ ← argmax\n\nL1 + L2\n\nθ,φ|α\n\nα ← argmax\n\nL1 − L2,\n\nα|θ,φ\n\n(9)\n\n(10)\n\nwhere L1, L2 is the expectation of the first and second sup term of Eq.8 on training set respectively. We update (fθ, fφ) and gα alternately. A small learning rate is required to maintain training stability, otherwise the gα may fall into a trivial solution where attention is almost zero throughout the time series. The objective in the second stage is:\n\nη ← argmin\n\nL3,\n\nη|α\n\n(11)\n\nwhere L3 is the binary cross entropy and the notation η | α indicates that causal attention remains fixed during the second stage of training. The downstream classifier is sensitive to the upstream scheme of attention assignment. In experiments, there exists an optimal loss interval for training the attention model gα. We stop the first stage training when the loss value of objective Eq. 10 is stable in this interval, and then the downstream classifier hη usually obtains the best generalization performance. For different dynamics, their optimal intervals are different and we find them empirically1. Details on the implementation of causal attention mechanism are provided in Appendix C.\n\n4 EXPERIMENT\n\nWe describe our experiment setup and extensively evaluate the causal attention mechanism on neuronal dynamics coupled on both synthetic and real causal networks.\n\n4.1 SETUP\n\nCausal networks. For synthetic networks, we generate ten groups of Erd ̋os-R ́enyi (ER) and scalefree (SF) directed networks with one hundred nodes (i.e., variables) uniformly and with mean degree varying from 5 to 41 by adjusting the probability for edge creation in ER and the total number of\n\n1An alternative design, which we have not yet implemented, would involve joining the first and second stages. Attention model gα would be trained by not only maximizing AeTE but also responding to feedback from the classifier, and would find the balance between Eq. 10 and Eq. 11 automatically.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nedges in SF. Symmetric links (both xi → xj and xj → xi) can exist. For each set of network parameters we consider five independently generated instances. For real networks, we select five neurological connectivity datasets as presented in Table 1, each from a different species: Cat, Macaque, Mouse, Worm and Rat.\n\nDataset\n\nRegion\n\n#Nodes\n\n#Edges Mean degree\n\nCat Macaque Mouse Worm Rat\n\nBrain Brain Cortex Neural Brain\n\n65 242 195 272 503\n\n1139 4090 214 4451 30088\n\n17.5 16.9 1.1 16.4 59.8\n\nTable 1: Statistical information of five real networks: dataset name, type of network, number of nodes, number of edges and mean degree ⟨k⟩. Detailed introduction are provided in Appendix E.\n\nTable 2: Equations of the five dynamical models considered. B is the asymmetrical adjacency matrix of the causal network, recording causal relationships between nodes. Bij = 1 if variable i is the parent of variable j, otherwise Bij = 0. In these expressions, Γ describes the coupling-drive, while other terms represent self-drive. The detailed configuration of dynamical parameters is provided in Appendix D.\n\nDynamics\n\nEquations\n\nHindmarsh-Rose\n\nMorris-Lecar\n\nIzhikevich\n\nRulkov\n\n ̇pi = qi − ap3 ̇qi = c − dp2 Γ = gc (Vsyn − pi) (cid:80)N\n\ni + bp2 i − qi,\n\ni − n + Iext + Γ\n\n ̇ni = r [s (pi − p0) − ni]\n\nj=1Bij /(1 + exp(−λ (pj − Θsyn)))\n\nC ̇V =I − gL (V − VL) − gCam∞(V ) (V − VCa) − gK n (V − VK ) + Γ\n\n ̇n =λ(V ) (n∞(V ) − n) , Γ = gc\n\n(cid:80)N\n\nj=1 Bij (nj − ni)\n\n ̇vi = 0.04v2\n\n ̇ui = a(bvi − ui), Γ = gc\n\ni + 5vi + 140 − ui + I + Γ j=1Bij uj\n\n(cid:80)N\n\nF1(ui, wi) =\n\nβ 1 + u2 i\n\n+ wi + Γ (uj ) , F2(ui, wi) = wi − νui − σ\n\nΓ (uj ) = gc\n\n(cid:80)N\n\nj=1Bij / (1 + exp(λ (uj − Θs)))\n\nFitzHugh-Nagumo\n\n ̇v = a + bv + cv2 + dv3 − u + Γ\n\n ̇u = ε(ev − u), Γ = −gc\n\n(cid:80)N\n\nj=1 Bij (vj − vi) .\n\nDynamic models. We use five dynamic models for neural activity simulation widely used in the field of neuroscience: Hindmarsh-Rose (HR), Morris-Lecar (Morris), Izhikevich (Izh), Rulkov and FitzHugh-Nagumo (FHN). Dynamic equations are provided in Table 2, and segments of generated time series are represented in Figure 3.\n\nEvaluation metrics. We measure the following metrics: (1) the area under the receiver operating characteristic curve (AUROC); and (2) the area under the precision-recall curve (AUPRC).\n\nBaselines. We compared our method with seven baselines: (1) Granger causality test (Ganger) Granger (1969); (2) Transfer Entropy (TE), as in Eq. 5; (3) Convergent cross mapping (CCM) Sugihara et al. (2012); (4) Latent convergent cross mapping (Latent CCM) De Brouwer et al. (2020); (5) PCMCI Runge et al. (2019) and (6) PCMCI+ Runge (2020) using partial correlation to quantify causal strength; (7) Classification model with Convolutional Block Attention Module (TA) Woo et al. (2018).\n\nTraining details. We employ the convolutional neural network for model gα and hη, and the fullyconnected neural network for model fθ and fφ. We use the ADAM Kingma & Ba (2014) optimizer with the initial learning rate of 10−4 for classifier hη and 10−5 for the others. The batch size is 10. For synthetic networks, we select randomly twenty ordered pairs of variables as a training/validation set and four hundred ordered pairs as a test set. For real networks, the sample set scheme is provided in Table 3. All sets are composed of equal samples with causality and without causality. The total time step T of time series is 50,000. Gaussian measurement noise is added with mean zero and standard deviation 1%/10% that of the original time series for synthetic/real networks respectively.We run all experiments in this work on a local machine with two NVIDIA Tesla V100 32GB GPUs.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Insight into the coupling drive of dynamics. Top panel in each subplot: segment of time series from a ordered pair of variables with a causal relationship (blue is parent and green is child). Bottom panel in each subplot: the absolute value of coupling force (gray line), traditional attention (light pink line), and causal attention (orange line). (a) Hindmarsh-Rose; (b) Izhikevich; (c) FitzHugh-Nagumo; (d) Rulkov.\n\n4.2 RESULT\n\nInsight into the coupling-drive of underlying dynamics. The gray lines in Figure 3 represent the change of coupling force from parent to child variable over time, and are generated by the coupling term Γ in Table 2. The absolute value of the coupling force rises (the gray lines spike) at occasional moments when the behavior of a parent variable substantially influences the evolution of its child variable, and remains almost zero at other times. The orange lines representing the causal attention keep in step with the gray lines, indicating that the causal attention mechanism recognizes the effect of coupling force in reducing the uncertainty of the child variable and pays attention to the areas where coupling force is significant. In Figure 3(d), the causal attention focuses on two separated regions where the coupling forces have concentrated bursts. In contrast, the light pink lines representing the traditional attention remain close to their maximum value, indicating it is insensitive to changes in coupling force. This leads its classifier to extract features throughout the whole time series (instead of focusing on causal features). The traditional attention are not designed for causal reasoning and cannot accommodate the selection of features that correspond to the causal information.\n\nFigure 4: Comparison of classifiers on synthetic causal networks. CA: Causal Attention. Examples of dynamical model: (a) HR on ER; (b) Izh on ER; (c) HR on SF; (d) Izh on SF.\n\n7\n\n(a) Hindmarsh-Rose(b) Izhikevich(d) Rulkov(c) FitzHugh-Nagumo(a) HR in ER(b) Izh in ER(c) HR in SF(d) Izh in SFAUROC/AUPRCUnder review as a conference paper at ICLR 2023\n\nTable 3: Performance comparison on real causal networks. Values in the first column of each dataset are AUROC, and values in the second column are AUPRC. Each point contains the mean and standard deviation computed in five experiments with randomly sampled training/validation/test set in Cat (top, 10/10/500) and Mouse (bottom, 10/10/90) connectomes. Results on other three connectomes are shown in Appendix F.\n\nPerformance on test sets. Compared with the baselines, our method usually substantially improves reconstruction performance on both synthetic and real causal networks, as shown in Figure 4 and Table 3. In contrast, the classifier with traditional attention mechanism (TA) obtains low losses on training sets but has poor generalization on test sets, highlighting that mere statistical correlation for causal inference is unstable and can be spurious Cui & Athey (2022). The performance of classical unsupervised methods, for which all positions in the time series are treated equally, is also limited by the paucity of causal effects. These patterns demonstrate the importance of identifying and focusing on critical regions, which we achieve via the causal attention mechanism. In conclusion, our method slightly increases cost, due to the need for label collection, but obtains a substantial boost in performance compared with those unsupervised methods in this class of causal network reconstruction tasks.\n\nThe performance of all methods tends to decrease as the average network degree grows. Networks with larger average degree are more likely to exhibit synchronization of variables which makes it harder to distinguish cause and effect. Furthermore, a single variable in these denser networks can have many parent variables, and substantial coupling forces can emerge from distinct parents at overlapping times, making individual drivers harder to distinguish. In this circumstance, a slight variance in the scheme of causal attention assignment may cause fluctuations in the performance of the downstream classifier. Robustness of the proposed method to measurement noise and sequence length is presented in Appendix F.\n\n5 RELATED WORK\n\n5.1 CAUSAL NETWORK RECONSTRUCTION\n\nConventional frameworks assume separability, i.e., that information about causes are not contained in the caused variable itself. Several common methods Spirtes & Glymour (1991); Sun et al. (2015); Runge et al. (2019); Mastakouri et al. (2021) are based on conditional independence relations, but\n\n8\n\nHindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.57±0.01 0.57±0.01 0.59±0.02 0.59±0.02 0.65±0.01 0.65±0.01 0.53±0.01 0.53±0.010.59±0.01 0.54±0.01 0.44±0.01 0.49±0.01 0.40±0.03 0.44±0.02 0.74±0.01 0.73±0.02 0.57±0.02 0.62±0.010.75±0.01 0.75±0.02 0.58±0.02 0.58±0.02 0.51±0.01 0.51±0.01 0.57±0.01 0.57±0.01 0.68±0.02 0.68±0.020.69±0.02 0.69±0.01 0.55±0.02 0.55±0.02 0.48±0.03 0.48±0.03 0.51±0.01 0.51±0.02 0.64±0.02 0.64±0.010.53±0.01 0.53±0.01 0.49±0.01 0.49±0.01 0.49±0.01 0.49±0.01 0.66±0.01 0.66±0.01 0.53±0.01 0.53±0.010.57±0.01 0.57±0.01 0.52±0.02 0.52±0.01 0.52±0.02 0.52±0.01 0.63±0.02 0.63±0.02 0.56±0.01 0.56±0.010.76±0.01 0.76±0.02 0.61±0.01 0.59±0.01 0.53±0.01 0.52±0.01 0.53±0.01 0.52±0.01 0.67±0.01 0.64±0.020.91±0.01 0.88±0.01 0.80±0.01 0.76±0.01 0.61±0.02 0.62±0.01 0.92±0.01 0.89±0.01 0.70±0.01 0.67±0.01Hindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.62±0.03 0.62±0.02 0.51±0.02 0.51±0.01 0.54±0.03 0.54±0.02 0.89±0.01 0.89±0.02 0.21±0.04 0.21±0.010.40±0.03 0.44±0.02 0.53±0.05 0.62±0.08 0.57±0.02 0.56±0.03 0.33±0.04 0.40±0.02 0.30±0.03 0.33±0.010.55±0.05 0.55±0.04 0.24±0.05 0.24±0.01 0.47±0.04 0.47±0.04 0.58±0.01 0.58±0.02 0.58±0.05 0.58±0.040.47±0.01 0.47±0.01 0.51±0.02 0.51±0.01 0.51±0.01 0.51±0.02 0.53±0.02 0.53±0.02 0.53±0.01 0.53±0.010.50±0.02 0.50±0.01 0.46±0.03 0.46±0.01 0.50±0.02 0.50±0.01 0.77±0.03 0.77±0.03 0.13±0.02 0.13±0.010.53±0.03 0.50±0.01 0.38±0.04 0.38±0.01 0.50±0.05 0.50±0.02 0.79±0.02 0.79±0.01 0.21±0.09 0.21±0.010.97±0.02 0.95±0.04 0.51±0.02 0.53±0.02 0.51±0.03 0.51±0.02 0.89±0.01 0.83±0.05 0.94±0.01 0.89±0.010.98±0.01 0.96±0.02 0.85±0.01 0.82±0.01 0.92±0.03 0.91±0.03 0.98±0.01 0.96±0.02 0.89±0.04 0.85±0.05Under review as a conference paper at ICLR 2023\n\ndiffer in the design of condition-selection strategies or choice of conditional independence test. Granger Causality Granger (1969) is extended to nonlinear dynamics by using neural networks to represent nonlinear casual relationships Tank et al. (2021); Nauta et al. (2019). Many methods of causal discovery assume that the causal network is a directed acyclic graph. However, directed cyclic graphs are common in real systems. To address this non-separability issue, Convergent-cross mapping Sugihara et al. (2012) and its variations Clark et al. (2015); De Brouwer et al. (2020) measure the extent to which the historical record of child can reliably estimate states of the parent in reconstructed state space. However, sparse causal effect in neuronal dynamics, particularly in the presence of noise, may lead parent and child time series to appear statistically independent, so that their contribution to state estimation is hard to recognize.\n\n5.2 MUTUAL INFORMATION ESTIMATION\n\nBelghazi et al. Belghazi et al. (2018) built on a dual representation of KL divergence Donsker & Varadhan (1983) to offer a parametric mutual information neural estimator (MINE) which is linearly scalable in dimension as well as sample size, and is also trainable and strongly consistent. They also discussed another version of MINE based on the f -divergence representation Nguyen et al. (2010); Nowozin et al. (2016). Using the technique of Noise-Contrastive Estimation (NCE) Gutmann & Hyv ̈arinen (2010), based on comparing target and randomly chosen negative samples, Van den Oord et al. Van den Oord et al. (2018) proposed InfoNCE loss, minimization of which maximizes a mutual information lower bound. An important application of this contrastive learning approach has been extracting high-level representations of different data modalities Chen et al. (2020); Woo et al. (2021); Hu et al. (2021); Koch-Janusz & Ringel (2018). In our work, we extend MINE for transfer entropy estimation.\n\n6 CONCLUSION\n\nThe problem of reconstructing causal networks from observational data is fundamental in multiple disciplines of science including neuroscience, since it is a prerequisite foundation for the research about structure analysis and behavior control in causal networks. Especially, several countries have recently launched grand brain projects, and one important goal is to map the connectomes (i.e., directed links between neurons) of different species.\n\nWe proposed a novel mechanism, causal attention, to guide machine learning models to infer causal relationships while focusing on the specific areas where casual effect may emerge. We showed that this mechanism identifies weak causal effects ignored by classical techniques, and helps machine learning models gain insight into the coupling dynamics underlying time series data. Our method needs a small set of samples (i.e., a small number of known causal links), and thus raises an open problem worthy of future pursuit: for large complex systems, how to select the small number of ordered pairs of nodes that offer general pattern for identification of sparse causal effects.\n\nREFERENCES\n\nElias Bareinboim and Judea Pearl. Transportability from multiple environments with limited experiments: Completeness results. Advances in Neural Information Processing Systems, 27, 2014.\n\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. Advances in Neural Information Processing Systems, 29, 2016.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pp. 531–540. PMLR, 2018.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pp. 1597–1607. PMLR, 2020.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Thomas Clark, Hao Ye, Forest Isbell, Ethan R Deyle, Jane Cowles, G David Tilman, and George Sugihara. Spatial convergent cross mapping to detect causal relationships from short time series. Ecology, 96(5):1174–1181, 2015.\n\nPeng Cui and Susan Athey. Stable learning establishes some common ground between causal infer-\n\nence and machine learning. Nature Machine Intelligence, 4(2):110–115, 2022.\n\nEdward De Brouwer, Adam Arany, Jaak Simm, and Yves Moreau. Latent convergent cross mapping.\n\nIn International Conference on Learning Representations, 2020.\n\nMonroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on Pure and Applied mathematics, 36(2):183– 212, 1983.\n\nAndreas Gerhardus and Jakob Runge. High-recall causal discovery for autocorrelated time series with latent confounders. Advances in Neural Information Processing Systems, 33:12615–12625, 2020.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: Journal of the Econometric Society, pp. 424–438, 1969.\n\nMichael Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nYeping Hu, Xiaogang Jia, Masayoshi Tomizuka, and Wei Zhan. Causal-based time series domain generalization for vehicle intention prediction. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nMaciej Koch-Janusz and Zohar Ringel. Mutual information, neural networks and the renormaliza-\n\ntion group. Nature Physics, 14(6):578–582, 2018.\n\nMatt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances\n\nin Neural Information Processing Systems, 30, 2017.\n\nKarima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. Survey on causal-based machine learn-\n\ning fairness notions. arXiv preprint arXiv:2010.09553, 2020.\n\nAtalanti A Mastakouri, Bernhard Sch ̈olkopf, and Dominik Janzing. Necessary and sufficient conditions for causal feature selection in time series with latent common causes. In International Conference on Machine Learning, pp. 7502–7511. PMLR, 2021.\n\nMeike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolu-\n\ntional neural networks. Machine Learning and Knowledge Extraction, 1(1):312–340, 2019.\n\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. Advances in Neural Information Processing Systems, 29, 2016.\n\nLiam Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191–\n\n1253, 2003.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDana Pessach and Erez Shmueli. A review on fairness in machine learning. ACM Computing Surveys\n\n(CSUR), 55(3):1–44, 2022.\n\nNiklas Pfister, Peter B ̈uhlmann, and Jonas Peters. Invariant causal prediction for sequential data.\n\nJournal of the American Statistical Association, 114(527):1264–1276, 2019.\n\nJakob Runge. Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear In Conference on Uncertainty in Artificial Intelligence, pp. 1388–1397.\n\ntime series datasets. PMLR, 2020.\n\nJakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets. Science Advances, 5 (11):eaau4996, 2019.\n\nThomas Schreiber. Measuring information transfer. Physical Review Letters, 85(2):461, 2000.\n\nShohei Shimizu, Patrik O Hoyer, Aapo Hyv ̈arinen, Antti Kerminen, and Michael Jordan. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.\n\nPeter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. Social\n\nScience Computer Review, 9(1):62–72, 1991.\n\nGeorge Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty, and Stephan\n\nMunch. Detecting causality in complex ecosystems. science, 338(6106):496–500, 2012.\n\nJie Sun, Dane Taylor, and Erik M Bollt. Causal network inference by optimal causation entropy.\n\nSIAM Journal on Applied Dynamical Systems, 14(1):73–106, 2015.\n\nAlex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily B Fox. Neural Granger causality.\n\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4267–4279, 2021.\n\nAaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv e-prints, pp. arXiv–1807, 2018.\n\nGerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In International Conference on Learning Representations, 2021.\n\nSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block In Proceedings of the European Conference on Computer Vision, pp. 3–19,\n\nattention module. 2018.\n\nFeng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. Advances in Neural Information Processing Systems, 33:14891–14902, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA DERIVATION\n\nHere we present a derivation showing that the transfer entropy equals the difference between two types of mutual information:\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\np\n\n(cid:17)\n\nlog\n\n(cid:16)\n\n(cid:17)\n\n, x(l)\n\nt\n\nyt+1 | y(k) (cid:16)\n\nt\n\nyt+1 | y(k)\n\nt\n\np\n\n(cid:17) .\n\n(12)\n\nApplying the conditional Bayes formula p (y | x) = p(y,x) log term of equation 12:\n\np(x) on the numerator and denominator in the\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n(cid:16)\n\np\n\nyt+1,y(k)\n\nt\n\n(cid:17)\n\n,x(l) t\n(cid:17)\n\n(cid:16)\n\np\n\ny(k)\n\nt\n\n,x(l)\n\nt\n\n(cid:16)\n\np\n\nyt+1,y(k) t\n(cid:17)\n\n(cid:16)\n\ny(k)\n\nt\n\np\n\n.\n\n(cid:17)\n\n(13)\n\nAdding the marginal distribution of time series Y to the numerator and denominator simultaneously:\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n(cid:16)\n\np\n\nyt+1,y(k) (cid:16)\n\nt\n\ny(k)\n\nt\n\np(yr)p (cid:16)\n\np\n\n(cid:17)\n\n(cid:17)\n\n,x(l)\n\nt\n\n,x(l) t\n(cid:17)\n\nyt+1,y(k) (cid:16)\n\nt\n\n(cid:17)\n\ny(k)\n\nt\n\np(yr)p (cid:16)\n\nyt+1, y(k) (cid:16)\n\nt\n\np (yr) p\n\n(cid:17)\n\n, x(l)\n\nt\n\ny(k)\n\nt\n\n(cid:17)\n\n, x(l)\n\nt\n\n(cid:88)\n\n(cid:16)\n\np\n\n=\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\np\n\n(cid:17)\n\nlog\n\n(cid:88)\n\np\n\n−\n\n(cid:16)\n\nyt+1, y(k)\n\nt\n\n(cid:16)\n\np\n\n(cid:17)\n\nlog\n\nyt+1, y(k) (cid:16)\n\nt\n\n(cid:17)\n\n(cid:17)\n\np (yr) p (cid:17)\n\n− M I\n\nt\n\ny(k) (cid:16)\n\n(cid:16)\n\n= M I\n\nYt+1, Y (k)\n\nt\n\n, X (l)\n\nt\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\n.\n\n(14)\n\n(15)\n\n(16)\n\nIn these expressions, yr is sampled from time series Y randomly each time step and independently of the time step t.\n\nB NEURAL ESTIMATOR FOR TRANSFER ENTROPY\n\nB.1 CONSISTENCY\n\nDefinition. A neural estimator (cid:98)S(X, Y )n which uses n samples from the data distribution to estimate a statistic S(X, Y ) on variables X, Y is strongly consistent if for any ε > 0, there exists a positive integer N and a choice of neural network such that:\n\n∀n ≥ N,\n\n| S(X, Y ) − S(X, Y )n |≤ ε, almost everywhere (a.e.)\n\n(17)\n\nThe Mutual Information Neural Estimator (MINE) depends on a choice of a neural network and the number of samples n from the data distribution Belghazi et al. (2018). Let fθ be the family of functions parameterized by the neural network with parameters θ ∈ Θ. MINE is defined as:\n\n(cid:100)M I(X, Y )n = sup θ∈Θ\n\nEP (n)\n\nXY\n\n[fθ] − log\n\n(cid:16)\n\nEP (n)\n\nX ⊗P (n)\n\nY\n\n(cid:2)efθ (cid:3)(cid:17)\n\n.\n\n(18)\n\nTheorem 1 Belghazi et al. (2018). MINE is strongly consistent.\n\nThe Transfer Entropy Neural Estimator (TENE) consists of two independent MINE and depends on choice of neural network and sample number n. TENE is defined as:\n\n(cid:100)T E(X → Y )n = (cid:100)M I\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (l)\n\nt\n\n(cid:17)(cid:17)\n\n− (cid:100)M I\n\nn\n\n(cid:16)\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\n.\n\nn\n\n(19)\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nWe use M I [1], M I [2], (cid:100)M I\n\n(cid:16)\n\nM I\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\n, (cid:100)M I\n\n(cid:16)\n\nYt+1,\n\nn , (cid:100)M I (cid:16) Y (k)\n\nt\n\n, X (l)\n\nt\n\nas abbreviations of M I (cid:17) (cid:16) (cid:17)(cid:17)\n\nand (cid:100)M I\n\nYt+1, Y (k)\n\nt\n\n[1]\n\n[2]\n\nn\n\nn\n\nrespectively.\n\nn\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (l)\n\nt\n\n(cid:17)(cid:17)\n\nand\n\nWe will prove the following: Theorem 2. TENE is strongly consistent. Proof. Let ε > 0. By Theorem 1, we can choose neural networks and integers N1, N2 and such that\n\n∀n ≥ N1,\n\n∀n ≥ N2,\n\n(cid:12) (cid:12) M I [1] − (cid:100)M I (cid:12) (cid:12) (cid:12) (cid:12) M I [2] − (cid:100)M I (cid:12) (cid:12)\n\n[1]\n\nn\n\n[2]\n\nn\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ ε/2, a.e.\n\n≤ ε/2, a.e.\n\nLetting N = max {N1, N2}, for n ≥ N and for some neural network we have, a.e.,\n\n∀n ≥ N,\n\n(cid:12) (cid:12) (cid:12)T E(X → Y ) − (cid:100)T E(X → Y )n\n\n(cid:12) (cid:12) (cid:12) =\n\n=\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(M I [1] − M I [2]) − ((cid:100)M I\n\n[1]\n\nn − (cid:100)M I\n\n(M I [1] − (cid:100)M I\n\n[1]\n\nn ) − (M I [2] − (cid:100)M I\n\n[2]\n\n(cid:12) (cid:12) n ) (cid:12) (cid:12) (cid:12) (cid:12) n ) (cid:12) (cid:12)\n\n[2]\n\n(M I [1] − (cid:100)M I\n\n[1]\n\nn )\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(M I [2] − (cid:100)M I\n\n[2]\n\nn )\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nThe proof is complete.\n\n≤ ε/2 + ε/2 = ε.\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nB.2 VARIATION OF BIAS VARY WITH DIMENSION AND NOISE\n\nWe examine the performance of TENE for the considered class of neural networks on linear dynamic system, consisting of variables X and Y defined as:\n\nxt+1 = αxt + εx yt+1 = βyt + gcxt + εy\n\n(26)\n\n(27)\n\nWe set α = β = 0.5 and εx = εx ∼ N (0, σ2). The true values of transfer entropy T E(X → Y ) in this simple coupled system can be determined analytically Kaiser & Schreiber (2002). We can increase the dimension of the system by considering multiple independent copies of variables X and Y , in which case the mutual information and transfer entropy scale linearly with the dimension of the system. For each considered dimension, standard deviation σ, and coupling strength gc in an interval from -0.4 to 0.4, we generate a time series of length 50,000. We also consider an alternative non-parametric estimator of mutual information, the Kraskov estimator Kraskov et al. (2004) with k = 5 nearest neighbours. In Fig 5 we compare the results of MINE with the analytic formula and the Kraskov estimator. MINE shows marked improvement over the Kraskov estimator, especially when variables are high-dimensional. Comparing Fig 5(a,b) or (c,d) shows that the amplitude of the driving Gaussian noise has little influence on estimates. Interestingly, as coupling strength gc grows small, i.e., as X and Y become more independent, the Kraskov estimator can suggest a negative value of the mutual information, i.e., we estimate that M In (Yt+1, Yt, Xt) < M In (Yt+1, Yt). We deduce that irrelevant information about the nearly independent variable Xt interferes with the estimation of the mutual information by the Kraskov estimator.\n\nC ALGORITHM\n\nDetails on the implementation of causal attention mechanism are provided in Algorithm 1.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: True and estimated transfer entropy versus coupling strength gc. The dimension and standard deviation (std) σ of system noise is indicated in the titles of subplots.\n\nAlgorithm 1 Causal Inference by Causal Attention Input: Small samples S, part with causality ̄S ⊂ S, segment length for training ̄L ≪ L 1: θ, φ, α, η ← initialize network parameters 2: repeat until (L1 − L2) is stable in optimal loss interval 3: 4:\n\nfor all samples ̄S do\n\nChoose ts ∈ (cid:2)1, L − ̄L(cid:3) randomly Take segment s = {(xt, yt)}ts+ ̄L Produce joint samples in Eq. 8 (cid:110)(cid:16)\n\n(cid:17)(cid:111)ts+ ̄L\n\nt=ts\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n, etc.\n\nt=ts\n\n5: 6:\n\n7:\n\nend for Assign causal attention A Compute L1, L2 on ̄S Update parameters Recompute L1, L2 on ̄S Update parameters α ← α + ∇α (L1 − L2)\n\n8: 9: 10: 11: 12: 13: 14: repeat until L3 convergence Compute L3 on S 15: Update parameters 16:\n\nη ← η − ∇ηL3\n\nθ, φ ← θ + ∇θL1, φ + ∇φL2\n\n14\n\n(d)(b)(c)(a)Under review as a conference paper at ICLR 2023\n\nD MODEL BRAIN DYNAMICS\n\nHere we give detailed information about five neuronal dynamics applied to modeling membrane potential and relevant quantities in biological connectomes. We input to each causal discovery algorithm the coordinate corresponding to the neuron membrane voltage potential, because this variable is most likely to be experimentally accessible.\n\nD.1 HINDMARSH-ROSE DYNAMICS\n\nThe spikes of activity in neurons are considered an important part of the brain’s information processing Borges et al. (2018); Rabinovich et al. (2006). Hindmarsh and Rose Hindmarsh & Rose (1984) (HR) proposed a phenomenological neuron model that is a simplification of the Hodgkin-Huxley model Hodgkin & Huxley (1952). The HR model is described by\n\n ̇p = q − ap3 + bp2 − n + Iext ̇q = c − dp2 − q ̇n = r [s (p − p0) − n]\n\nwhere p(t) is the action potential of the membrane, q(t) is related to the fast current and n(t) is associated with the slow current. Presynaptic neurons with an action potential pj coupled by chemical synapses to neurons i modifying its action potential pi according to\n\n ̇pi = qi − ap3\n\ni + bp2\n\ni − n + Iext + Γ\n\nΓ = gc (Vsyn − pi) (cid:80)N\n\nj=1\n\nBij 1 + exp(−λ (pj − Θsyn))\n\nwhere i, j = 1, . . . , N , N is the number of neurons, gc is the chemical coupling strength and Bij describes neurons’ chemical connections. The chemical synapse function is modeled by the above sigmoidal function, with Θsyn = 1.0. We use parameters a = 1, b = 3, c = 1, u = 5, s = 4, r = 0.005, p0 = −1.60, coupling strength gc = 0.1, Vsyn = 2, λ = 10, and external current Iext = 3.24, for which HR neurons exhibits a chaotic burst behavior.\n\nD.2 MORRIS–LECAR DYNAMICS\n\nMorris and Lecar Morris & Lecar (1981) suggested a simple two variable model to describe oscillations in a barnacle’s giant muscle fiber. The Morris–Lecar model has became quite popular in computational neuroscience community due to its biophysically meaningful and measurable parameters, which consist of a membrane potential u receiving an instantaneously activated Ca current and a more slowly activated K current n evolving according to:\n\nC ̇V =I − gL (V − VL) − gCam∞(V ) (V − VCa) − gKn (V − VK) + Γ(V )\n\n ̇n =λ(V ) (n∞(V ) − n)\n\nwhere\n\nm∞(V ) =\n\nn∞(V ) =\n\n(cid:26)\n\n(cid:26)\n\n1 2\n\n1 2\n\n1 + tanh\n\n1 + tanh\n\n(cid:21)(cid:27)\n\n(cid:21)(cid:27)\n\n(cid:20) (V − V1) V2 (cid:20) (V − V3) V4 (cid:21)\n\nλ(V ) = ̄λ cosh\n\n(cid:20) (V − V3) (2V4)\n\nwith the coupling term\n\nΓ(Vi) = gc\n\n(cid:80)N\n\nj=1 Bij (nj − ni) ,\n\nwith parameters C = 20μF/cm2, gL = 2mmho/cm2, VL = −50mV, gCa = 4mmho/cm2, VCa = 100mV, gK = 8mmho/cm2, VK = −70mV, V1 = 0mV, V2 = 15mV, V3 = 10mV, V4 = 10mV, ̄λ = 0.1 s−1, and applied current I = 34μA/cm2.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nD.3\n\nIZHIKEVICH DYNAMICS\n\nIzhikevich dynamics reproduce spiking and bursting behavior of known types of cortical neurons, and combine the biological plausibility of Hodgkin–Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons Izhikevich (2003). The equations governing Izhikevich spike dynamics are:\n\n ̇v = 0.04v2 + 5v + 140 − u + I + gc\n\n ̇u = a(bv − u)\n\nwith the auxiliary after-spike resetting\n\n(cid:88)\n\nBijuj\n\nif v ≥ +30mV,\n\nthen\n\n(cid:26) v ← c\n\nu ← u + d\n\n.\n\nHere, variable v represents the membrane potential of the neuron and u represents a membrane recovery variable, which accounts for the activation of K +ionic currents and inactivation of Na+ ionic currents, and it provides negative feedback to v. Here, we use the parameters a = 0.2, b = 2, c = −56, d = −16, I = −99. After the spike reaches its apex (+30mV), the membrane voltage and the recovery variable are reset. If v skips over 30 , then it first is reset to 30 , and then to c so that all spikes have equal magnitudes.\n\nD.4 RULKOV DYNAMICS\n\nThe Rulkov model is a map-based neuron model with a surprising abundance of features, such as periodic and chaotic spiking, and bursting. The Rulkov map is an abstract mathematical model, although it shares some specific features with others neuron models closer to experimental observations. We use synthetic time series where each neuron is simulated using the Rulkov model Eroglu et al. (2020), which has two variables, u and w, evolving at different timescales as described by x(t + 1) = (u(t + 1), v(t + 1)) = F (x(t)) = (F1(u(t), w(t)), F2(u(t), w(t))), with\n\nF1(u, w) =\n\nβ\n\n1 + u2 + w + Γ (u)\n\nand F2(u, w) = w − νu − σ.\n\nThe two variables reflect the two important time scales of a neuron model. The variable u represents the fast dynamics of the system and usually models the membrane voltage of the neuron, whereas w is the slow variable and represents the variations of the ionic recovery currents. Different combinations of parameters σ and β give rise to different dynamical states of the neuron, such as resting, tonic spiking, and chaotic bursts. As for the coupling, we consider chemical synaptic coupling, that is, H (xi, xj) = (h (ui, uj) , 0) with h (ui, uj) = (ui − Vs) Γ (uj), where\n\nΓ (uj) =\n\n1 1 + exp {λ (uj − Θs)}\n\nand electrical synaptic coupling, H (xi, xj) = (h (ui, uj) , 0), with h (ui, uj) = uj − ui. In the chemical coupling, Vs is a parameter called the reverse potential. Here, we use the parameters with β = 4.4, σ = ν = 0.001,, Vs = 20, Θs = −0.25 and λ = 10.\n\nD.5 FITZHUGH-NAGUMO DYNAMICS\n\nA FitzHugh-Nagumo neuron comprises a two-dimensional system of smooth ODEs, so cannot exhibit autonomous chaotic dynamics and bursting. Adding noise allows for stochastic bursting FitzHugh (1961). The equations governing the FitzHugh-Nagumo neuronal network dynamics are\n\n ̇v = a + bv + cv2 + dv3 − u + Γ ̇u = ε(ev − u)\n\nwith the coupling term\n\nΓ(vi) = −gc\n\n(cid:80)N\n\nj=1 Bij (vj − vi) .\n\nThe FitzHugh-Nagumo dynamics capture the firing behaviors of neurons with two components. The first component v represents the membrane potential, which contains self- and interaction dynamics,\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nand the second component u represents a recovery variable. To simulate the shape of each spike, the time step in the model must be relatively small, e.g., τ = 0.25 ms. Here, we use the parameters a = 0.28, b = 1, c = 0, d = −1, ε = 0.04, e = 12.5. Moreover, the parameters in the FitzHugh–Nagumo model can be tuned so that the model describes spiking dynamics of many resonator neurons.\n\nD.6 TIME SERIES GENERATION\n\nTo obtain the time series from above neural dynamics, we use Runge-Kutta method with variablestep to solve the ordinary differential equation of Hindmarsh-Rose and Morris–Lecar dynamics with sample interval τ = 0.1. Izhikevich dynamics are solved by the Euler formula with time step h = 0.05. For the Rulkov map we consider a unit sample interval. The total time step of time series T = 50, 000 in both synthetic and real networks.\n\nE REAL BRAIN CONNECTOMES INFORMATION\n\nE.1 CAT CONNECTOME\n\nThe cat connectivity dataset comprises a description of cortical connections in the cat brain Scannell et al. (1995), a connectivity set resulting from a comprehensive literature search of anatomical tracing studies in the cat cortex. Detailed information on the delineated regions, including information on the used parcellation scheme, abbreviations and possible overlap with other parcellation schemes, as well as information on the physiological characteristics of these regions, is given in the appendix of the original study Ref. Scannell et al. (1995). The connectivity dataset incorporates data of one hemisphere, including 65 regions and 1139 interregional macroscopic axonal projections de Reus & van den Heuvel (2013).\n\nE.2 MACAQUE CONNECTOME\n\nThe macaque connectivity data set used in this study comprises anatomical data from 410 tract tracing studies collated in the online neuroinformatics data base CoCoMac (http://cocomac.org), first analyzed and made publicly available in Ref. Modha & Singh (2010). In the present study they focused primarily on an analysis of the connectivity among regions of the cerebral cortex. The cortical connection matrix was extracted from the primary connection data by removing all subcortical (thalamus, basal ganglia, brainstem) regions. In addition, regions that did not maintain at least one incoming and one outgoing connection were also removed to ensure that the network was strongly connected. The remaining connection data set used in this study consisted of 242 regions and 4090 directed projections represented in binary format (connection present = 1, connection absent = 0) Harriger et al. (2012).\n\nE.3 MOUSE CONNECTOME\n\nThe Allen Mouse Brain Connectivity Atlas uses enhanced green fluorescent protein (EGFP)- expressing adeno-associated viral vectors to trace axonal projections from defined regions and cell types, and high-throughput serial two-photon tomography to image the EGFP-labelled axons throughout the brain. This systematic and standardized approach allows spatial registration of individual experiments into a common three dimensional (3D) reference space, resulting in a wholebrain connectivity matrix. The Allen Mouse Brain Connectivity Atlas is a freely available, foundational resource for structural and functional investigations into the neural circuits that support behavioural and cognitive processes in health and disease Oh et al. (2014).\n\nE.4 WORM CONNECTOME\n\nAll the chemical and gap junction synapses, the connectome, in the posterior nervous system of the C. elegans adult male are identified by serial section electron microscopy Jarrell et al. (2012). The feasibility of comprehensive synapse-level nervous system reconstruction by this method was a primary reason for the initial selection of C. elegans as an experimental model. They developed a PC-based software platform to facilitate assembly of a connectome from electron micrographic\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nimages. The connectome is of a single adult animal and was produced from a series of 5000 serial thin sections of 70 to 90 nm encompassing the posterior half of the body.\n\nE.5 RAT CONNECTOME\n\nBecause resliceable 3D brain models for relating systematically and topographically different parcellation schemes are still in the first phases of development, it is necessary to rely on qualitative comparisons between regions and tracts that are either inserted directly by neuroanatomists or trained annotators, or are extracted or inferred by collators from the available literature. To address these challenges, Ref. Bota et al. (2012) developed a publicly available neuroinformatics system, the Brain Architecture Knowledge Management System, including an exemplar for constructing interrelated connectomes at different levels of the mammalian central nervous system organization, and presented the latest version of the BAMS rat macroconnectome.\n\nInformation about the above datasets is summarized in Table 4.\n\nTable 4: Statistical information of six real networks: dataset name, type of network, number of nodes, number of edges, mean degree ⟨k⟩, and data acquisition method.\n\nDataset\n\nCat Macaque Mouse Worm Rat\n\nRegion\n\n#Nodes\n\n#Edges Mean degree\n\nSensor\n\nBrain Brain Cerebral Cortex Neural Brain\n\n65 242 195 272 503\n\n1139 4090 214 4451 30088\n\n17.5 16.9 1.1 16.4 59.8\n\nTract tracing studies Tract tracing studies Electron microscopy Electron Microscopy Neuroanatomical experiments\n\nF ADDITIONAL EXPERIMENTS\n\nF.1 PERFORMANCE OF TRANSFER ENTROPY ON REAL CAUSAL NETWORKS\n\nExperiment Results on Macaque/C.elegans/Rat connectome are provided in Table 5/Table 6/Table 7 as the supplement of main text Table 3. Classic methods have limited performance across various neural dynamics unfolding on real causal networks especially in a noisy environment: we add Gaussian measurement noise with mean zero and standard deviation 10% that of the original time series. As we discuss in main text Sec. 3.1, sparse causal effects are easily masked in metric of iconic transfer entropy when noise causes estimated probability densities to deviate even slightly from the true distributions.\n\nTable 5: Performance train/validation/test set is 50/50/500.\n\ncomparison on Macaque\n\nconnectome. The\n\nsample number\n\nin\n\n18\n\nHindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.48±0.02 0.48±0.01 0.50±0.02 0.50±0.01 0.54±0.01 0.54±0.01 0.54±0.01 0.54±0.010.44±0.01 0.49±0.01 0.53±0.05 0.52±0.05 0.54±0.03 0.51±0.02 0.48±0.07 0.49±0.04 0.43±0.02 0.45±0.010.44±0.02 0.44±0.02 0.49±0.01 0.49±0.01 0.51±0.01 0.51±0.01 0.55±0.01 0.55±0.01 0.56±0.01 0.56±0.010.47±0.01 0.47±0.01 0.51±0.02 0.51±0.01 0.51±0.01 0.51±0.02 0.53±0.02 0.53±0.02 0.53±0.01 0.53±0.010.47±0.01 0.47±0.01 0.51±0.01 0.51±0.01 0.50±0.02 0.50±0.01 0.50±0.01 0.50±0.01 0.52±0.01 0.52±0.010.47±0.02 0.47±0.01 0.48±0.01 0.48±0.01 0.51±0.02 0.51±0.01 0.52±0.02 0.52±0.01 0.52±0.01 0.52±0.010.68±0.03 0.51±0.03 0.56±0.01 0.55±0.01 0.52±0.04 0.50±0.02 0.51±0.02 0.51±0.03 0.54±0.01 0.52±0.010.71±0.04 0.65±0.03 0.60±0.02 0.59±0.01 0.66±0.03 0.64±0.03 0.59±0.03 0.57±0.03 0.63±0.02 0.59±0.02Under review as a conference paper at ICLR 2023\n\nTable 6: Performance comparison on C.elegans connectome. The sample number train/validation/test set is 50/50/500.\n\nin\n\nTable 7: Performance comparison on Rat connectome. The sample number in train/validation/test set is 100/100/500.\n\nF.2 DATA EFFICIENCY OF CAUSAL ATTENTION MECHANISM AGAINST THE LENGTH OF TIME\n\nSERIES\n\nTaking Cat and Mouse connectome as examples, we show the results of our method trained on the time series data with different lengths, i.e. total time steps, in Figure 6. The size of training and test sets is the same as the scheme in the main text Table 4. Overall, the AUROC/AUPRC scores tend to get higher as the length increases, but this tendency is not significant. It indicates that the causal attention mechanism extracts sufficient causal features for causal inference even over short time series.\n\nF.3 ROBUSTNESS OF CAUSAL ATTENTION MECHANISM AGAINST THE INTENSITY OF NOISE\n\nWe show the results of our method trained on time series data added different intensities of noise (measurement noise rather than intrinsic noise of dynamics) in Figure 7. Except for dynamics of Izh and FHN on Cat connectome, the AUROC/AUPRC scores are stable within the range 2%-10% of standard deviation. It implies that causal attention mechanism is robust by the sample noise level.\n\n19\n\nHindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.56±0.01 0.56±0.01 0.64±0.01 0.64±0.02 0.71±0.01 0.71±0.01 0.64±0.02 0.64±0.010.74±0.01 0.73±0.02 0.61±0.01 0.58±0.02 0.55±0.01 0.56±0.02 0.80±0.01 0.80±0.01 0.52±0.01 0.48±0.010.79±0.02 0.79±0.02 0.54±0.02 0.54±0.02 0.62±0.02 0.62±0.02 0.67±0.02 0.67±0.02 0.74±0.01 0.74±0.010.78±0.01 0.78±0.01 0.52±0.01 0.52±0.01 0.55±0.04 0.55±0.03 0.53±0.04 0.53±0.03 0.68±0.01 0.68±0.010.53±0.01 0.53±0.01 0.50±0.02 0.50±0.01 0.53±0.02 0.53±0.01 0.66±0.01 0.66±0.01 0.57±0.01 0.59±0.010.53±0.02 0.53±0.01 0.52±0.01 0.52±0.01 0.56±0.01 0.56±0.01 0.69±0.02 0.69±0.02 0.63±0.01 0.63±0.010.92±0.01 0.91±0.01 0.67±0.03 0.65±0.03 0.52±0.01 0.54±0.01 0.56±0.01 0.54±0.01 0.85±0.01 0.84±0.010.93±0.01 0.92±0.01 0.81±0.01 0.76±0.01 0.76±0.01 0.71±0.01 0.97±0.01 0.97±0.01 0.78±0.01 0.72±0.02Hindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.43±0.02 0.43±0.01 0.50±0.01 0.50±0.01 0.46±0.01 0.46±0.01 0.52±0.02 0.52±0.010.57±0.02 0.62±0.01 0.33±0.02 0.41±0.01 0.48±0.09 0.54±0.08 0.37±0.01 0.43±0.01 0.41±0.03 0.47±0.030.63±0.02 0.63±0.02 0.48±0.02 0.48±0.02 0.54±0.05 0.54±0.05 0.50±0.01 0.50±0.01 0.52±0.03 0.52±0.020.63±0.02 0.63±0.02 0.40±0.01 0.40±0.01 0.55±0.01 0.55±0.02 0.53±0.02 0.53±0.01 0.51±0.03 0.51±0.020.51±0.01 0.51±0.01 0.44±0.02 0.44±0.01 0.53±0.01 0.53±0.01 0.46±0.01 0.46±0.01 0.49±0.02 0.49±0.010.51±0.02 0.51±0.01 0.42±0.02 0.42±0.01 0.55±0.01 0.55±0.01 0.44±0.02 0.44±0.01 0.49±0.01 0.49±0.010.82±0.03 0.78±0.02 0.70±0.01 0.67±0.01 0.62±0.02 0.58±0.03 0.73±0.02 0.74±0.01 0.74±0.04 0.74±0.020.83±0.03 0.83±0.03 0.78±0.01 0.78±0.01 0.65±0.04 0.64±0.03 0.83±0.01 0.79±0.01 0.84±0.02 0.81±0.01Under review as a conference paper at ICLR 2023\n\nFigure 6: AUROC/AUPRC scores of the causal attention mechanism trained on time series data of different lengths. The blue bar is AUPRC, the green bar is AUROC. The x-axis indicates the scores and y-axis represents the total time step of time series.\n\nF.4 HOW MUCH DOES CAUSAL ATTENTION MECHANISM BRING THE DISTANCE BETWEEN\n\nDISTRIBUTIONS OF TRAINING AND TEST SET CLOSER TOGETHER\n\nThe causal attention mechanism helps the classifier reveal the generation processing underlying the data, i.e., coupling-drive in problem of causal inference and alleviate the dilemma of distribution shift in scene of small samples. The causal attention mechanism refines the content of samples (critical regions) and thus reduces the distribution dimension of the entire dataset. To quantify this shortened distance, we ask, how many additional training samples does traditional machine learning need to achieve the same level of generalization as our method? Taking the Cat Connectome as example, we train the classifier with traditional attention mechanism by gradually expanding the size of training set, and provide the results in Figure8. The green horizontal lines represent the AUROC value of our method using ten ordered pairs with causality (0.8% of edges in Cat Connectome) while, to achieve the same performance, the traditional classifier needs approximately 10%/15%/20%/40% samples for HR/Izh/Morris/Rulkov dynamics respectively (the blue lines cross the green lines). It also indicates that our method provides significant saving in labels collection, which is significant given that the procedure for checking connections in organisms is cumbersome in practice.\n\n20\n\nHindmarsh-RoseMorris-LecarIzhikevich RulkovFitzHugh-NagumoCatCatHindmarsh-RoseMorris-LecarIzhikevich RulkovFitzHugh-NagumoMouseMouseUnder review as a conference paper at ICLR 2023\n\nFigure 7: AUROC/AUPRC scores of the causal attention mechanism trained on time series data added different intensity of noise. The blue bar is AUPRC, the green bar is AUROC, and the y-axis indicates the percentage of standard deviation of Gaussian measurement noise in the original time series.\n\nFigure 8: Size of training set that the traditional classifier requires to achieve same level of generalization as our method. The x-coordinate indicates the percentage of the ordered pairs with causality in training set to the total edges in causal network. Examples of dynamical model: (a) HR; (b) Izh; (c) Morris; (d) Rulkov.\n\n21\n\n(a) HR(b) Izh(c) Morris(d) RulkovUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pp. 531–540. PMLR, 2018.\n\nFS Borges, Ewandson L Lameu, Kelly C Iarosz, Paulo R Protachevicz, Iberˆe L Caldas, Ricardo L Viana, Elbert EN Macau, Antonio M Batista, and Murilo da Silva Baptista. Inference of topology and the nature of synapses, and the flow of information in neuronal networks. Physical Review E, 97(2):022303, 2018.\n\nMihail Bota, Hong-Wei Dong, and Larry W Swanson. Combining collation and annotation efforts toward completion of the rat and mouse connectomes in bams. Frontiers in neuroinformatics, 6: 2, 2012.\n\nMarcel A de Reus and Martijn P van den Heuvel. Rich club organization and intermodule commu-\n\nnication in the cat connectome. Journal of Neuroscience, 33(32):12929–12939, 2013.\n\nDeniz Eroglu, Matteo Tanzi, Sebastian van Strien, and Tiago Pereira. Revealing dynamics, commu-\n\nnities, and criticality from data. Physical Review X, 10(2):021047, 2020.\n\nRichard FitzHugh.\n\nImpulses and physiological states in theoretical models of nerve membrane.\n\nBiophysical journal, 1(6):445–466, 1961.\n\nLogan Harriger, Martijn P Van Den Heuvel, and Olaf Sporns. Rich club organization of macaque\n\ncerebral cortex and its role in network communication. PLoS ONE, 7(9):e46497, 2012.\n\nJames L Hindmarsh and RM Rose. A model of neuronal bursting using three coupled first order differential equations. Proceedings of the Royal Society of London. Series B. Biological Sciences, 221(1222):87–102, 1984.\n\nAlan Lloyd Hodgkin and Andrew Fielding Huxley. Propagation of electrical signals along giant nerve fibres. Proceedings of the Royal Society of London. Series B-Biological Sciences, 140 (899):177–183, 1952.\n\nEugene M Izhikevich. Simple model of spiking neurons. IEEE Transactions on neural networks, 14\n\n(6):1569–1572, 2003.\n\nTravis A Jarrell, Yi Wang, Adam E Bloniarz, Christopher A Brittin, Meng Xu, J Nichol Thomson, Donna G Albertson, David H Hall, and Scott W Emmons. The connectome of a decision-making neural network. Science, 337(6093):437–444, 2012.\n\nAndreas Kaiser and Thomas Schreiber. Information transfer in continuous processes. Physica D:\n\nNonlinear Phenomena, 166(1-2):43–62, 2002.\n\nAlexander Kraskov, Harald St ̈ogbauer, and Peter Grassberger. Estimating mutual information. Phys-\n\nical review E, 69(6):066138, 2004.\n\nDharmendra S Modha and Raghavendra Singh. Network architecture of the long-distance pathways in the macaque brain. Proceedings of the National Academy of Sciences, 107(30):13485–13490, 2010.\n\nCatherine Morris and Harold Lecar. Voltage oscillations in the barnacle giant muscle fiber. Biophys-\n\nical journal, 35(1):193–213, 1981.\n\nSeung Wook Oh, Julie A Harris, Lydia Ng, Brent Winslow, Nicholas Cain, Stefan Mihalas, Quanxin Wang, Chris Lau, Leonard Kuan, Alex M Henry, et al. A mesoscale connectome of the mouse brain. Nature, 508(7495):207–214, 2014.\n\nMikhail I Rabinovich, Pablo Varona, Allen I Selverston, and Henry DI Abarbanel. Dynamical\n\nprinciples in neuroscience. Reviews of Modern Physics, 78(4):1213, 2006.\n\nJack W Scannell, Colin Blakemore, and Malcolm P Young. Analysis of connectivity in the cat\n\ncerebral cortex. Journal of Neuroscience, 15(2):1463–1483, 1995.\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThis paper studies a cyclic causal model where values of every variable $X_i$ at time step t are decided by a differential equation $\\frac{d x_i}{d x}  = g(x_i) + \\sum_{j} B_{ij}f(x_i, x_j)$. The learner's goal is to recover the structural functions $f$ and $g$ and the coefficients $B_{ij}$ from the observational data. The authors propose a new loss function for this learning task, called attention-extended transfer entropy, which encourages certain conditional dependence in the model. Finally, simulations were performed on both synthetic and dynamical models to validate the proposed approach.\n\n# Strength And Weaknesses\n\nThis paper may have some interesting ideas. The definition of attention-extended transfer entropy (ATE) seems intriguing. However, due to the lack of clarity, it is unclear how this paper contributes to the existing literature and how novel the proposed method is. I will elaborate on the following.\n\nFirst, the authors claim that \"our task is to infer causal relationships between observed variables based on time series data and reconstruct the causal network connecting large numbers of these variables.\" However, this inference task is never formally defined. What is the performance measure of the learning task? How does one measure the quality of the reconstructed causal networks? Should we measure the L1 / L2 distance of the learned parameters with the actual parameters of the underlying model, or should we measure of the divergence between simulated and observed samples? Unfortunately, I tried to read through the paper but could not find answers.\n\nSecond, the paper describes the proposed algorithm. However, it does not perform any analysis of the algorithm's theoretical guarantee, concentration properties, and sample complexity. It is unclear how the proposed algorithm improves the existing baseline. To support the proposed method, the authors have to resort to empirical evaluation.\n\nAs for the experiments, I appreciate the authors' efforts in including various synthetic and dynamical models. Unfortunately, the clarity issue remains. For instance, the authors state \"compared with the baselines, our method usually substantially improves\nreconstruction performance on both synthetic and real causal networks, as shown in Figure 4.\" Again, it is unclear how the reconstruction performance is measured here. Without such information, it is hard to evaluate and compare the proposed method with the existing baseline.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease see my response above.\n\n# Summary Of The Review\n\nThis paper studies a cyclic causal model where values of every variable $X_i$ at time step t are decided by a differential equation $\\frac{d x_i}{d x}  = g(x_i) + \\sum_{j} B_{ij}f(x_i, x_j)$. The learner's goal is to recover the structural functions $f$ and $g$ and the coefficients $B_{ij}$ from the observational data. The authors propose a new loss function for this learning task, called attention-extended transfer entropy, which encourages certain conditional dependence in the model. However, this paper provides little theoretical guarantee for the proposed method. Due to the lack of clarity, it is unclear how this paper contributes to the existing literature and how novel the proposed method is.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThis paper introduces \"Causal Attention,\" a novel mechanism for enhancing causal inference tasks by revealing the underlying data generation processes. The authors focus on reconstructing causal networks governed by nonlinear dynamics, particularly addressing the challenge of detecting weak and transient causal effects in neural activity sequences. The proposed methodology involves an innovative neural network architecture that autonomously assigns attention coefficients, optimizing the Attention-extended Transfer Entropy (AeTE) to improve network reconstruction performance. Empirical results demonstrate that the Causal Attention mechanism outperforms traditional methods on both synthetic and real networks, effectively identifying significant causal relationships.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach to causal inference using a neural attention mechanism, which provides a clear advantage in identifying weak causal effects in complex systems. The experimental design is robust, employing both synthetic models and real-world datasets, and the results convincingly demonstrate the efficacy of the proposed method over existing baselines. However, a notable weakness is the limited discussion on the performance of the method in dense networks where synchronization may obscure causal relationships, which is crucial for understanding the method's applicability in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, effectively communicating complex ideas and methodologies. The quality of the proposed approach is high, with a systematic presentation of the neural architecture and training processes. The novelty of combining causal inference with an attention mechanism is significant, presenting a fresh perspective in the field. Reproducibility is facilitated by the detailed description of the methodology and the availability of metrics for evaluation, although the authors could enhance reproducibility by providing access to code or datasets.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in causal inference through the introduction of Causal Attention, demonstrating strong empirical results and a novel methodology. While it excels in clarity and quality, further exploration of its limitations in dense networks could strengthen the paper’s impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel causal reasoning mechanism called **causal attention**, aimed at improving machine learning models' capabilities in causal inference tasks, particularly in reconstructing causal networks influenced by nonlinear dynamics. The methodology centers around the introduction of **Attention-extended Transfer Entropy (AeTE)**, which enhances the detection of causal effects in time series data by autonomously assigning attention coefficients. The authors demonstrate the effectiveness of their approach through extensive experiments on both synthetic causal networks and real-world neurological datasets, showing significant performance improvements over traditional methods in terms of causal inference accuracy and data efficiency.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative contribution to causal inference with the introduction of causal attention and AeTE, offering a fresh perspective on leveraging deep learning for this purpose. The experimental design is robust, utilizing a mix of synthetic and real datasets, which strengthens the findings. The model's ability to generalize well on unseen data while requiring fewer labeled samples is particularly valuable for practical applications in fields like neuroscience. However, the paper also has limitations; the performance may degrade in densely connected networks, and there are concerns regarding noise sensitivity and generalization in strongly coupled systems. Additionally, the assumption of similar underlying processes for training and test data distributions may not always hold.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results, making it accessible to a broad audience. The quality of the experiments is high, and the results are meticulously detailed, allowing for a reasonable level of reproducibility. The novelty of the approach is significant, particularly in its integration of attention mechanisms into causal inference, which has not been widely explored in existing literature. However, the potential challenges regarding noise sensitivity and the model's assumptions may affect its reproducibility in more complex scenarios.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in causal inference methodologies through the introduction of causal attention. While the results are promising and the experimental design is robust, attention should be given to the model's limitations, particularly in terms of its performance in complex network structures and sensitivity to noise.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Causal Attention**, a novel mechanism aimed at enhancing machine learning models' performance in causal inference tasks by identifying critical moments within time series data where causality is significant. The methodology employs a neural network to maximize **Attention-extended Transfer Entropy (AeTE)**, which effectively reconstructs causal networks influenced by nonlinear dynamics with weak coupling and strong self-drive. The findings demonstrate that Causal Attention significantly improves the reconstruction of both synthetic and real causal networks, outperforming traditional methods by focusing on key time moments rather than treating all time points equally.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach to addressing the challenges of causal inference in complex systems, particularly the effective identification of weak causal effects masked by strong self-drive dynamics. The method's incorporation of attention mechanisms to enhance the detection of significant coupling moments is a notable advancement. However, the paper could benefit from a more thorough discussion of the limitations of the proposed method and a clearer explanation of the practical implications of the findings. Additionally, while the empirical results are promising, further validation on more diverse datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its concepts clearly, with sufficient background information to understand the proposed methodology. The theoretical foundations, including consistency and convergence proofs for the neural estimator of Transfer Entropy, add to the quality of the work. The novelty lies in the integration of attention mechanisms with causal inference, which is relatively unexplored. Reproducibility is supported by the inclusion of detailed experimental setups and appendices, although the paper could enhance this by providing additional resources or code.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in causal inference through the introduction of Causal Attention, which effectively identifies key moments of causality in time series data. While the methodology is innovative and empirically validated, a more comprehensive discussion of limitations and broader applicability would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel causal reasoning mechanism termed “causal attention,” designed to enhance the identification of weak causal effects within time series data. The authors introduce an attention-extended transfer entropy (AeTE) framework that prioritizes critical moments in time series, allowing for more effective causal relationship detection. Experimental results demonstrate significant performance improvements over traditional methods in reconstructing causal networks from both synthetic and real datasets, although challenges arise with increasing network density and noise.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to causal inference, particularly through the introduction of causal attention and the formulation of AeTE, which effectively target important temporal moments. The method exhibits notable performance gains, especially in scenarios with limited labeled data, addressing common data collection burdens. However, weaknesses include the method's sensitivity to network density, which may hinder its generalizability, and potential overfitting due to its reliance on small sample sizes. Additionally, while the method shows robustness to noise, extreme cases can still lead to misidentification of causal influences. The theoretical framework, though clear, may present a barrier for practitioners lacking expertise in causal inference.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a coherent presentation of concepts and methodologies. The theoretical foundation is solid, contributing to the reproducibility of the proposed approach. The novelty of the causal attention mechanism and the AeTE formulation is significant, positioning the work as a meaningful advancement in the field. However, the complexity of some theoretical concepts may impede comprehension among a broader audience, potentially limiting the practical applicability of the findings.\n\n# Summary Of The Review\nThis paper introduces a promising causal attention mechanism that enhances causal inference in time series data, showcasing substantial performance improvements over traditional methods. While the methodology demonstrates significant novelty and robustness, it also faces limitations related to network density and noise sensitivity that warrant further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe manuscript introduces a novel methodology called \"causal attention\" aimed at enhancing causal inference in machine learning, particularly for reconstructing causal networks by identifying transient causal effects that are frequently overlooked. The paper presents a causal attention mechanism that dynamically assigns weights to time series data, emphasizing critical moments of causal relationships. Additionally, the authors propose a new objective function, \"Causal-Weighted Transfer Entropy\" (CWTE), which integrates attention weights to improve causal inference. Experimental results demonstrate significant improvements in reconstruction accuracy across synthetic and real datasets, indicating the efficacy of the proposed approach without extensive prior knowledge of the causal dynamics.\n\n# Strength And Weaknesses\n**Strengths:**\n- The paper addresses a significant gap in causal inference by focusing on transient effects, which are often ignored by traditional methods.\n- The introduction of CWTE as an objective function is a notable contribution, providing a novel integration of attention mechanisms into causal analysis.\n- The experimental validation is robust, encompassing multiple datasets and demonstrating consistent performance improvements over existing techniques.\n\n**Weaknesses:**\n- The methodology lacks a thorough explanation of underlying assumptions regarding causal network dynamics, particularly concerning noise and its effects on model performance.\n- The potential computational overhead of the attention mechanism and implications for real-time applications are not adequately discussed.\n- The exploration of transient causal effects could be further extended to other domains beyond neuroscience, which is not sufficiently addressed in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methodologies and experiments. The quality of the writing facilitates understanding of complex concepts. In terms of novelty, the integration of attention mechanisms into causal inference is innovative. However, the reproducibility of the results could be enhanced by providing more detailed methodological descriptions, particularly regarding the implementation of the CWTE function and the training process.\n\n# Summary Of The Review\nOverall, the manuscript presents a compelling and innovative approach to causal inference through the application of attention mechanisms, significantly advancing the identification of transient causal effects in time series data. While the contributions are notable, the paper would benefit from addressing the computational aspects and broader applicability of the findings to enhance its overall robustness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing adversarial training in machine learning models through an attention mechanism termed \"adversarial attention.\" This mechanism identifies and prioritizes critical features in the input data that are vulnerable to adversarial attacks. The authors introduce a new optimization objective that integrates this attention mechanism, demonstrating significant improvements in model robustness against various adversarial attacks through extensive experiments on benchmark datasets like MNIST and CIFAR-10. The results indicate that models trained with adversarial attention outperform traditional adversarial training methods, while also providing insights into the features contributing to model vulnerabilities.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of an attention mechanism into the adversarial training framework, which enhances model robustness and sheds light on vulnerabilities. The experimental results are compelling, showing clear performance improvements over baseline methods. However, the paper has some weaknesses, including the increased model complexity which may lead to longer training times and higher computational costs. Additionally, while the authors demonstrate improved robustness against known adversarial attacks, the model's performance against unseen attacks remains unaddressed, leaving a gap in its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, presenting its methodology and findings clearly. The quality of the experiments is high, utilizing standard datasets and metrics, which aids reproducibility. The novelty of the approach is significant, as it combines adversarial training with an attention mechanism in a way that has not been extensively explored in existing literature. However, the complexity of the model may hinder its adoption in practical applications, and further validation across diverse adversarial scenarios would strengthen its claims.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method to enhance adversarial training through the use of an attention mechanism, resulting in improved robustness and valuable insights into model vulnerabilities. While the contributions are significant, the increased complexity and limited exploration of unseen adversarial attacks represent areas for further investigation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces **Causal Attention**, a novel mechanism designed to enhance causal inference tasks within machine learning, particularly for reconstructing causal networks from time series data. It addresses significant challenges in detecting weak causal effects that traditional methods have previously overlooked. The authors propose the **Attention-extended Transfer Entropy (AeTE)** as a new metric for establishing causality, claiming that it can autonomously identify critical causal relationships with minimal prior knowledge. Experimental results indicate substantial improvements in causal network reconstruction performance compared to existing techniques, although limitations in certain scenarios are acknowledged.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including the introduction of a potentially transformative methodology and the extensive evaluation demonstrating its superiority over traditional causal inference methods. The acknowledgment of limitations, although minimal, shows some awareness of the method's applicability. However, the paper tends to overstate the capabilities of **Causal Attention**, particularly its effectiveness in dense networks and high noise levels. The claims about its transformative impact could lead to misinterpretation of its practical applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting a clear narrative from the introduction to the conclusions. However, while the proposed methodology and its significance are described with enthusiasm, the clarity could be improved by providing a more balanced view of its limitations. The novelty of the approach is evident, particularly in the context of causal inference, but the reproducibility of the results remains questionable without sufficient detail regarding the experimental setups and data used.\n\n# Summary Of The Review\nOverall, the paper introduces a novel approach to causal inference through **Causal Attention**, claiming significant advancements over existing methods. While the contributions are promising, the exaggerated claims regarding its capabilities may obscure its practical limitations, warranting a more cautious interpretation of the results.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel causal reasoning mechanism termed \"causal attention,\" aimed at enhancing the performance of machine learning models in causal inference tasks, particularly when inferring causal relationships from time series data. The authors address the challenges associated with weak coupling effects and chaotic behavior in causal networks. The methodology involves utilizing neural networks to estimate mutual information while implementing causal attention to allocate attention coefficients that highlight critical time points for causal effects. Experimental results demonstrate significant improvements in performance metrics, such as AUROC and AUPRC scores, when comparing the proposed method to traditional approaches, particularly in less dense networks.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to causal inference through the causal attention mechanism, which effectively addresses the difficulties posed by weak coupling effects and chaotic behavior in causal networks. The experimental setup is robust, utilizing both synthetic and real-world datasets, which helps validate the proposed method’s applicability. However, the method's limitations are notable, particularly its struggles in dense networks where causal effects overlap, leading to potential false positives in causal inference. Additionally, while the method shows promise in sparser environments, the performance decline in denser networks raises concerns regarding its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its concepts clearly, making it accessible to readers familiar with machine learning and causal inference. The quality of the empirical results is high, with significant improvements in performance metrics clearly outlined. The proposed causal attention mechanism is novel and provides a significant advancement in the field; however, the reproducibility of the results may be challenged by the need for specific conditions outlined in the methodology, such as network density and measurement noise.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to causal inference methodologies through the introduction of causal attention. While the results demonstrate strong improvements in specific scenarios, the limitations in dense networks need to be addressed to enhance the method's robustness and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to causal inference in time series data through a causal attention mechanism, which aims to identify causal relationships and improve performance on various neural dynamics. The methodology involves using transfer entropy to measure causality while addressing the limitations of traditional methods, particularly in the context of sparse causal effects and noisy data. Key findings suggest that the proposed method can learn causal relationships effectively from limited labeled samples, although the assumptions regarding the generalizability, robustness, and interpretability of the attention mechanism raise important concerns.\n\n# Strength And Weaknesses\nThe primary contribution of the paper lies in the introduction of a causal attention mechanism that combines transfer entropy with neural network architectures to identify causal moments in time series data. However, several weaknesses undermine its validity. The assumptions regarding the ability to infer causality without accounting for confounding factors, the reliance on accurate estimation of probability distributions, and the generalizability of the approach across diverse dynamics may lead to overfitting and misinterpretation of causal relationships. Additionally, the paper does not adequately address the limitations of empirical validation, which could affect the applicability of the findings in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively well-structured, but some sections could benefit from clearer explanations, particularly regarding the interpretability of the attention mechanism and the implications of the identified causal relationships. While the proposed method demonstrates technical novelty by integrating attention mechanisms in causal inference, the reliance on specific neural network architectures may hinder reproducibility across different contexts and applications. Overall, the quality of the empirical evaluations is questionable, as they may not fully capture the complexities of real-world causal networks.\n\n# Summary Of The Review\nThe paper introduces an interesting approach to causal inference using a causal attention mechanism but is hindered by several critical assumptions that may compromise its validity and generalizability. While the methodology shows promise, the limitations in empirical validation and potential overfitting raise concerns about its practical applicability.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel mechanism known as causal attention to improve machine learning model performance in causal inference tasks, particularly focusing on identifying significant regions in time series data where causal effects manifest. The methodology involves the Attention-extended Transfer Entropy (AeTE) metric, which is enhanced through a binary classification model designed to differentiate genuine causal effects from noise. The findings demonstrate that the proposed causal attention mechanism outperforms traditional methods in reconstructing causal networks, especially under noisy conditions, highlighting its effectiveness in complex systems characterized by nonlinear dynamics.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the challenges of causal inference in the presence of noise and chaotic dynamics, leveraging attention mechanisms that prioritize critical regions of time series data. The integration of a classification model adds robustness to the causal identification process. However, a notable weakness is the reliance on synthetic datasets for some experiments, which may limit the generalizability of the findings. Additionally, while the method shows promise, the paper does not provide extensive discussion on practical implementation challenges when applied to real-world datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem statement, methodology, and results. The quality of writing is high, making it accessible to a broad audience. The novelty of the proposed causal attention mechanism is significant, as it introduces a fresh perspective on causal inference in nonlinear systems. However, reproducibility may be a concern, as the specific details regarding the implementation and parameter tuning of the causal attention mechanism are not thoroughly documented.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to enhancing causal inference in complex systems. The proposed causal attention mechanism demonstrates significant improvements over traditional methods, though some limitations exist regarding experimental validation with real-world data and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to improving model performance in the domain of causal inference within machine learning. The authors propose a new algorithm grounded in a robust theoretical framework that focuses on enhancing the understanding and application of causal relationships in data. Through a series of experiments, the findings demonstrate that the proposed method outperforms existing techniques on specific datasets, suggesting its potential viability in both academic and practical applications.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Novel Contribution:** The introduction of a new algorithm for causal inference is a significant advancement that has the potential to influence future research in the area.\n2. **Theoretical Foundation:** The paper establishes a strong theoretical basis for the proposed method, connecting it effectively with existing literature and justifying its relevance.\n3. **Practical Relevance:** The work addresses a pertinent challenge in machine learning, making it applicable to a variety of real-world scenarios.\n4. **Clarity of Presentation:** The writing is clear and well-organized, facilitating an easy understanding of the methodology and findings.\n\n**Weaknesses:**\n1. **Lack of Empirical Validation:** The experimental results are somewhat limited, with a lack of detailed analysis that could hinder the evaluation of the proposed method's effectiveness in diverse contexts.\n2. **Limited Comparison to Baselines:** The paper does not sufficiently compare the new method against a wider range of existing approaches, which could provide a clearer picture of its relative performance.\n3. **Generality of Findings:** There is insufficient discussion on the applicability of the findings to other datasets or contexts, raising concerns about the generalizability of the approach.\n4. **Potential Overfitting:** The risk of overfitting to the specific datasets used is noted, which could restrict the method's effectiveness in broader applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, featuring a logical structure that aids in the comprehension of complex ideas. The novelty of the proposed method is commendable, but the reproducibility may be compromised due to the lack of comprehensive experimental details. Overall, while the quality of the writing and the theoretical framework is high, the empirical aspects need more depth to support claims robustly.\n\n# Summary Of The Review\nThe paper introduces an innovative approach to causal inference that holds promise for future research and practical applications. However, the limited empirical validation and insufficient baseline comparisons currently detract from its overall impact. Addressing these issues could significantly enhance the paper's contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel causal reasoning mechanism termed **causal attention**, which enhances machine learning models for causal inference tasks, particularly in reconstructing causal networks influenced by nonlinear dynamics. The methodology employs attention coefficients derived from a neural network to identify critical time series data where causal effects may emerge, specifically addressing the challenge of detecting transient, weak causal relationships often overshadowed by stronger self-driven dynamics. The findings demonstrate that causal attention significantly improves network reconstruction performance in both synthetic and real causal networks, highlighting its effectiveness in identifying sparse causal effects without requiring prior knowledge.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to causal inference through the introduction of causal attention, which effectively addresses the shortcomings of traditional methods in detecting weak and transient causal effects. The methodology is well-structured, and the proposed objective function, Attention-extended Transfer Entropy (AeTE), provides a solid theoretical foundation for the approach. However, a notable weakness is that the paper raises questions regarding the optimal selection of samples for identifying sparse causal effects in larger systems, which remains inadequately addressed. Furthermore, while the empirical results are promising, additional validation across more diverse datasets could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-organized, making it accessible to readers familiar with causal inference and machine learning. The quality of the writing is high, and the methodology is described in sufficient detail to allow for reproducibility. The novelty of the causal attention mechanism is significant, as it introduces a new way of focusing on relevant time series data to uncover causal relationships, which has not been extensively explored in prior literature.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in causal inference by introducing the causal attention mechanism, which effectively identifies weak causal relationships in time series data. While the contributions are noteworthy and the methodology is robust, further exploration of sample selection strategies and validation across varied datasets would enhance the overall impact and applicability of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel causal reasoning mechanism called \"causal attention\" aimed at enhancing machine learning performance in causal inference tasks, particularly in reconstructing causal networks from time series data influenced by nonlinear dynamics. The proposed methodology involves a neural network that assigns attention coefficients to time series data, optimizing a new metric known as Attention-extended Transfer Entropy (AeTE) to identify significant causal effects. Experimental results demonstrate that the causal attention mechanism significantly outperforms traditional methods like Granger causality and Transfer Entropy in reconstructing causal networks, showing improved performance on both synthetic and real-world datasets without requiring prior knowledge of the underlying dynamics.\n\n# Strength And Weaknesses\nThe strength of this paper lies in its innovative approach to causal inference through the mechanism of causal attention, effectively addressing some of the key challenges in inferring causal relationships from observed data. The incorporation of AeTE as a novel metric is a significant contribution that enhances the capacity of traditional methods. However, the paper does have limitations, particularly regarding performance in dense networks and under conditions of high noise or synchronization, which may restrict its applicability in certain scenarios. Moreover, while the results are promising, further exploration of the effects of network density and noise would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing is generally clear, making complex ideas accessible, though some sections could benefit from additional clarity regarding the implementation details of the causal attention mechanism. The quality of the experiments appears robust, with appropriate evaluation metrics. In terms of novelty, the introduction of causal attention and the formulation of AeTE are significant, marking a notable advancement in the field. However, reproducibility may hinge on the availability of code and data, which should be provided to enhance transparency and allow for verification of results.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful advancement in causal inference through the introduction of causal attention and a new metric, AeTE. The findings indicate a clear improvement over traditional methods, though certain limitations and the need for further validation in challenging scenarios should be addressed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Causal Attention to Exploit Transient Emergence of Causal Effect\" introduces a novel mechanism for causal inference in time series data, termed causal attention. The methodology revolves around the Attention-extended Transfer Entropy (AeTE), which serves as the objective function to be maximized during the neural network training process. The authors present experimental results demonstrating the efficacy of the causal attention mechanism, particularly in the context of synthetic and real causal networks, showing improved performance against baseline methods with respect to evaluation metrics such as AUROC and AUPRC.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to causal inference, offering a fresh perspective through the causal attention mechanism and AeTE. The comprehensive evaluation using both synthetic and real-world datasets enhances the credibility of the findings. However, the work has some weaknesses, including a lack of explicit discussion on ethical considerations and potential implications of the research findings in neuroscience. Furthermore, while the methodology is well-defined, some technical details may require prior domain knowledge for full comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and logically organized, making it easy to follow the progression of ideas. The clarity of the technical details is generally good, though some portions may be challenging for readers without a strong background in causal inference. The novelty of the approach is significant, introducing an original neural network-based mechanism for causal inference. The reproducibility of the results is supported by detailed descriptions of the experimental setup, models, and datasets used, making it feasible for others to replicate the work.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in the field of causal inference, particularly for time series data. The proposed causal attention mechanism shows promising results, indicating its applicability in both theoretical and practical contexts. Minor clarifications regarding methodology and ethical implications would strengthen the paper further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel causal reasoning mechanism called **Causal Attention**, aimed at enhancing machine learning performance in causal inference tasks. The methodology revolves around reconstructing causal networks from observed time series data by focusing on critical temporal regions where causal dynamics emerge. The authors propose **Attention-extended Transfer Entropy (AeTE)** as a new metric to evaluate causal relationships, which is optimized using a neural architecture to dynamically generate attention coefficients. Empirical results demonstrate that Causal Attention effectively identifies significant causal influences, improving reconstruction accuracy for both synthetic and empirical causal networks derived from neural models.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of attention mechanisms within causal inference frameworks, which allows for a directed focus on transient causal effects often missed by traditional methods. The novelty of the AeTE metric contributes significantly to the existing literature on transfer entropy. However, the paper presents weaknesses, including diminished performance in dense causal networks and susceptibility to noise, which can lead to erroneous causal identifications. Furthermore, the complexity of the proposed method may limit its applicability in real-world scenarios with high levels of noise and overlapping causal effects.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, with a logical flow from the introduction to the methodology and results. The quality of the writing is high, making complex concepts accessible. The novelty of Causal Attention and the AeTE metric is substantial, marking a significant contribution to the field of causal inference. However, reproducibility may be a concern due to the reliance on specific neural network architectures and optimization strategies that may not be straightforward for other researchers to implement without further details on their training processes.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in causal inference through the introduction of Causal Attention, which effectively identifies transient causal relationships in complex systems. While the contributions are significant, the limitations regarding noise and dense networks should be addressed in future work to enhance the method's robustness and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel causal reasoning mechanism termed \"causal attention,\" which the authors claim enhances performance in causal inference tasks. The methodology relies on an Attention-extended Transfer Entropy (AeTE) metric to identify causal relationships from data, proposing that it reduces the need for extensive labeling compared to traditional approaches. However, the findings indicate significant limitations, particularly in environments characterized by dense networks and high noise levels, raising doubts about the generalizability and robustness of the proposed method.\n\n# Strength And Weaknesses\nWhile the paper attempts to contribute to the field of causal inference through the introduction of causal attention, its novelty is questionable as it seems to repackage existing attention mechanisms rather than provide a groundbreaking advancement. The methodology's reliance on transient causal effects represents a fundamental weakness, potentially undermining the model's robustness. Furthermore, the empirical validation is limited, with concerns that the improvements reported may not be statistically significant or practically meaningful. The acknowledgment of limitations, such as performance degradation in complex systems, further highlights the method's potential shortcomings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation; however, the theoretical grounding of the proposed AeTE metric is lacking, which affects the perceived quality and novelty of the contribution. Reproducibility may be hindered by the insufficient empirical data provided and the complexity of the proposed method, which may not yield tangible advantages over simpler, established techniques.\n\n# Summary Of The Review\nOverall, the paper presents a method that claims to enhance causal inference through causal attention, but it falls short of demonstrating substantial improvements over existing methods. The limitations outlined raise concerns about the robustness and generalizability of the approach, leading to skepticism regarding its contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents a revolutionary causal attention mechanism designed to enhance causal inference tasks in machine learning. The methodology involves the use of autonomous attention coefficients that maximize the Attention-extended Transfer Entropy, allowing models to effectively identify and reconstruct causal networks. The findings indicate that this approach not only significantly improves reconstruction accuracy on synthetic and real datasets derived from neuronal models but also demonstrates robustness against measurement noise and requires minimal labeled data for effective performance.\n\n# Strength And Weaknesses\nOne of the key strengths of this paper is its demonstration of exceptional performance improvements in causal inference, surpassing traditional methods. The focus on critical time series regions is particularly noteworthy, as it provides insights into complex neural dynamics that were previously elusive. Furthermore, the versatility of the approach across various fields—such as neuroscience, economics, and environmental science—highlights its broad applicability. However, while the results are impressive, further exploration of the limitations of the causal attention mechanism and its performance in diverse real-world scenarios would strengthen the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the contributions and methodology, making it accessible to a broad audience. The quality of the experimental results is high, with comprehensive comparisons to traditional causal inference techniques that underscore the novelty of the proposed approach. The reproducibility of the results is supported by the clear presentation of methodologies, although more information on the datasets and specific experimental setups would enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in causal inference methodologies through the introduction of a novel causal attention mechanism. Its impressive empirical results and broad applicability make it a valuable contribution to the field, although further exploration of its limitations and reproducibility could improve the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel causal attention mechanism designed to enhance causal inference within machine learning frameworks. It builds on the premise that traditional methods inadequately capture the complex dynamics of causal relationships, particularly in systems characterized by weak coupling drives and nonlinear behaviors. The methodology centers around the formulation of an attention-extended transfer entropy (AeTE), which serves as a metric to discern critical moments of causal influence and incorporates advanced neural estimation techniques rooted in Kullback-Leibler divergence. The findings suggest that this framework can significantly improve the detection of weak causal effects and has implications for various scientific fields, especially in understanding complex systems.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical advancements, particularly the introduction of the causal attention mechanism and the attention-extended transfer entropy. These constructs address the limitations of existing causal inference methodologies and highlight the importance of capturing nuanced causal dynamics. However, a notable weakness is the lack of empirical validation of the proposed framework in real-world applications, which raises questions about its practical utility. Additionally, while the theoretical underpinnings are robust, the complexity of the models may hinder reproducibility and ease of application.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical contributions clearly, making it accessible to readers familiar with causal inference and information theory. The quality of the theoretical analysis is high, providing a solid foundation for the proposed methods. In terms of novelty, the integration of attention mechanisms into causal inference is a significant contribution. However, the reproducibility of the findings could be challenged due to the complexity of the neural estimators and the theoretical models proposed, which may require specialized knowledge to implement effectively.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework for causal inference through the lens of a causal attention mechanism, with significant contributions to the field. While the theoretical advancements are noteworthy, the lack of empirical validation and potential challenges in reproducibility warrant consideration for further development.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach termed Causal Attention, which aims to enhance causal inference tasks by leveraging attention mechanisms to focus on key moments in time series data where causality emerges. Central to this method is the Attention-extended Transfer Entropy (AeTE), an objective function that autonomously assigns attention coefficients through a neural network, thereby identifying critical causal influences. The methodology involves a two-stage training process utilizing three neural networks, and it demonstrates data efficiency, requiring only a small number of labeled samples. The experimental results show that the proposed method outperforms seven baseline techniques across various synthetic and real-world datasets, while also exhibiting robustness against measurement noise.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative Causal Attention mechanism, which effectively identifies significant causal relationships in time series data, and its empirical validation against well-established causal inference methods. The two-stage training process is particularly noteworthy, as it allows for focused optimization of attention coefficients. However, the paper does have weaknesses, including reduced performance in dense networks with overlapping causal influences and the need for empirical tuning of hyperparameters, which may limit its applicability in certain scenarios. Additionally, the lack of clarity regarding code availability could hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation of methodologies and results. The quality of writing is high, making it accessible to readers familiar with causal inference and neural networks. The novelty of the proposed Causal Attention mechanism is significant, offering a fresh perspective on causal inference in time series data. However, the reproducibility aspect is somewhat compromised due to the unspecified code availability, which could deter other researchers from validating the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling method for causal inference through the innovative use of attention mechanisms. While it demonstrates strong empirical results and methodological rigor, concerns regarding performance in dense networks and code availability for reproducibility warrant consideration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel causal attention mechanism aimed at improving causal inference in neural networks. The authors propose the Attention-extended Transfer Entropy (AeTE) to estimate mutual information and claim it can identify weak causal effects that traditional methods overlook. They evaluate their approach on synthetic and real causal networks, reporting performance metrics such as AUROC and AUPRC, which they assert demonstrate superior performance compared to existing techniques.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambition to address the challenges of causal inference in neural dynamics, particularly regarding weak causal effects. However, the proposed causal attention mechanism lacks novelty, as it closely resembles established methods like the Granger causality test and the parametric mutual information estimator by Belghazi et al. (2018). Furthermore, the authors' claims regarding improved performance and efficiency appear overstated, given that traditional techniques like Transfer Entropy and Convergent Cross Mapping (CCM) have proven effective in similar scenarios. The paper also fails to convincingly demonstrate why a neural network approach would outperform classic statistical methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation, although the novelty aspect is questionable. The claims of superiority over traditional methods are not sufficiently backed by robust empirical validation, and the reproducibility of the results may be hampered by the reliance on neural networks, which often come with interpretability challenges.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting concept in causal inference, it does not substantiate its claims of novelty and superiority over established methods. The lack of robust comparative analysis and the overemphasis on the neural network approach diminish the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to causal inference using a mechanism termed \"causal attention,\" which aims to exploit transient causal effects in dynamic systems. The methodology involves a mathematical framework that integrates attention mechanisms with transfer entropy to quantify causal relationships between variables. Key findings indicate that the proposed method outperforms traditional causal inference techniques in identifying and quantifying causal effects in both simulated and real-world datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of attention mechanisms with causal inference, which addresses a significant gap in existing methodologies. The empirical validation on diverse datasets demonstrates robustness and practical applicability. However, weaknesses include some unclear mathematical notations and terminologies, which may hinder understanding for readers not deeply familiar with the field. Additionally, there is a lack of in-depth discussion on the limitations of the proposed method, which could provide a more balanced perspective.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits a generally high quality of writing, yet it suffers from several clarity issues, particularly in mathematical exposition and terminology consistency. While the novelty of combining causal attention with transfer entropy is commendable, improved clarity and coherence would enhance the overall comprehensibility, especially for interdisciplinary audiences. The reproducibility of the results is supported by empirical findings, although explicit details on implementation could be further detailed to facilitate replication.\n\n# Summary Of The Review\nOverall, the paper introduces a promising and novel approach to causal inference that leverages attention mechanisms. While the contributions are significant, enhancing the clarity of the mathematical framework and addressing the limitations more comprehensively would strengthen the paper. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to causal attention in biological neural networks. It introduces a framework that leverages causal inference methodologies to enhance the interpretability and performance of neural models. The authors conduct experiments using five specific neural dynamic models and report findings that suggest the effectiveness of their causal attention mechanism in extracting meaningful causal relationships from the data.\n\n# Strength And Weaknesses\nThe paper's main strengths lie in its innovative application of causal attention to neural networks and its empirical validation through multiple models. However, weaknesses include a limited exploration of the method's applicability in dense networks and high noise environments, and a lack of comprehensive analysis regarding the robustness of the causal attention mechanism under varying degrees of sparsity. Additionally, the paper could benefit from broader discussions on computational complexity, interpretability of causal coefficients, and ethical implications related to the deployment of such methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. However, certain aspects could be elaborated upon, such as the interpretability of causal attention coefficients and computational scalability. The novelty of the approach is significant, but the lack of diverse experimental validation raises concerns about its reproducibility and generalizability across different domains.\n\n# Summary Of The Review\nOverall, the paper offers a promising approach to integrating causal attention in neural networks but falls short in addressing several critical areas that could enhance its applicability and robustness. Further exploration of practical challenges, including noise handling and interpretability, would strengthen the contributions of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel causal attention mechanism designed to enhance performance in causal inference tasks, particularly in the context of time series data characterized by nonlinear dynamics. The methodology revolves around the Attention-extended Transfer Entropy (AeTE), which serves as a weighted generalization of transfer entropy. The findings indicate statistically significant improvements in reconstruction performance, both on synthetic and real causal networks, when employing the proposed method compared to traditional causal inference techniques.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to infer causal relationships using a statistically robust framework that extends existing methodologies. The introduction of AeTE provides a clear advancement in the ability to detect weak causal effects amidst chaotic dynamics, which is a notable contribution to the field. However, a potential weakness is the reliance on complex neural network architectures for estimating attention coefficients, which may hinder interpretability and accessibility for practitioners not well-versed in deep learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The statistical foundations are rigorously laid out, providing a solid basis for the proposed methods, which enhances the clarity of the work. The novelty of the approach is significant, introducing a new mechanism in causal inference. However, the reproducibility of results could be improved by providing more detailed descriptions of the computational setups and datasets used for empirical evaluations.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling advancement in causal inference through the development of a causal attention mechanism backed by robust statistical methods. While the contributions are significant, the complexity of the approach may limit its accessibility for broader applications in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a novel causal attention mechanism aimed at reconstructing causal networks from observational data. The methodology involves leveraging transfer entropy to identify causal relationships among variables, particularly in dense networks where traditional methods may falter. Key findings indicate that while the proposed mechanism shows promise in specific contexts, it faces significant challenges in accurately reconstructing causal networks when variables exhibit overlapping causal effects and in the presence of noise.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to causal inference using attention mechanisms, which could potentially enhance the understanding of complex relationships in data. However, the methodology faces notable weaknesses, such as its limited performance in high-noise environments and its inability to generalize to strongly coupled systems where variables exhibit synchronized behavior. Additionally, the reliance on a small number of labeled samples raises concerns regarding scalability and applicability to larger datasets. The lack of exploration of alternative training techniques and the limited assessment of computational complexity further detracts from the paper’s overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the concepts are presented understandably, the limitations of the proposed method are not adequately highlighted. The quality of the analysis is reasonable but lacks depth in exploring alternative scenarios and robustness under various conditions. In terms of novelty, the approach shows potential, yet it does not significantly advance the state of the art in causal inference due to the identified limitations. Reproducibility is also a concern, as the reliance on a small set of labeled data and specific models may hinder broader application.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to causal inference using attention mechanisms but suffers from significant limitations that restrict its applicability and effectiveness in more complex scenarios. Future work is needed to address these weaknesses and enhance the robustness and generalizability of the proposed method.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Causal Attention\" proposes a new mechanism to enhance machine learning models by focusing on critical regions of time series data to improve causal inference. The authors introduce a causal attention mechanism that highlights moments when causal effects emerge, along with a new metric termed Attention-extended Transfer Entropy (AeTE), which aims to address the limitations of classical transfer entropy. Through synthetic network generation and testing on real datasets, the authors claim to demonstrate substantial performance improvements over existing methods in identifying causal relationships.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address causal inference in time series analysis, a field with significant practical implications. However, the contributions are overshadowed by a lack of novelty, as many of the challenges discussed, such as weak coupling and noise in causal effects, are well-documented in prior literature. The proposed causal attention mechanism and AeTE, while interesting, do not represent a significant departure from established methods. The authors' framing of existing challenges as new discoveries detracts from the paper's credibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear in its presentation, but it suffers from overcomplication in some areas, making it difficult to discern genuine contributions. The methodology follows standard practices in causal inference and machine learning, which may limit reproducibility if the experimental details are not adequately specified. The novelty of the proposed approaches is questionable, as they appear to reiterate known concepts rather than introduce innovative solutions.\n\n# Summary Of The Review\nOverall, this paper fails to present significant advancements in the field of causal inference. While it addresses a relevant topic, the contributions lack originality and do not sufficiently build upon or challenge existing literature. The authors may have overlooked the depth of prior work, resulting in a superficial treatment of the subject.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel causal attention mechanism aimed at enhancing causal inference in dynamic systems. It employs an Attention-extended Transfer Entropy (AeTE) objective to quantify causal relationships while requiring a minimal number of labeled samples for training. The authors validate their approach using both synthetic and real datasets, demonstrating its effectiveness in identifying causal effects, particularly in environments with noise and complexity. \n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining attention mechanisms with causal inference, which is a critical area in machine learning. The authors effectively highlight the advantages of data efficiency and the potential for the model to operate without extensive prior knowledge. However, the paper could benefit from integrating causal attention with existing methods, evaluating a broader set of baselines, and exploring alternative objective functions. Furthermore, while the results are promising, the absence of a detailed empirical examination of various attention mechanisms leaves a gap in understanding the nuances of their applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. The novelty of combining causal inference with attention mechanisms is significant, although further exploration of alternative techniques could strengthen its contributions. The reproducibility of results appears reasonable, given the provided methodology and datasets, but additional details on the experimental setup and parameter tuning would enhance clarity.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in causal inference through the introduction of a causal attention mechanism, demonstrating its efficacy with minimal labeled data. While the contributions are noteworthy, the work could be improved by further empirical validation and a broader comparison against contemporary methods in causal inference.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel method termed \"causal attention,\" which aims to improve the performance of causal inference tasks in both synthetic and real networks. The methodology leverages a unique attention mechanism that enhances the identification of causal relationships, demonstrating marked improvements in reconstruction performance over several classical unsupervised techniques. The findings indicate that the proposed method consistently outperforms traditional approaches such as Granger causality and Transfer Entropy across various performance metrics, particularly in noisy conditions and with limited labeled data.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to causal inference and the robust empirical performance of the causal attention mechanism. The consistent improvements in AUROC and AUPRC scores across different scenarios highlight the method's effectiveness, particularly in real-world applications where traditional techniques often fail. However, the paper also acknowledges limitations, particularly in denser networks where performance declines with increasing average network degree. This limitation suggests the need for further refinement to handle overlapping causal effects more effectively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and its contributions. The results are effectively presented through figures and tables, enhancing the clarity of the findings. The novelty of the causal attention mechanism is significant, as it addresses key challenges in causal inference. The reproducibility of the results appears feasible, although details on datasets and implementation specifics could be elaborated to strengthen this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in causal inference through the introduction of the causal attention mechanism, demonstrating significant empirical advantages over traditional methods. While the methodology is robust and effective, there are limitations that warrant further investigation, particularly regarding performance in dense networks.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for understanding causal relationships in complex systems through the introduction of Attention-extended Transfer Entropy (AeTE) and causal attention mechanisms. The authors employ a combination of theoretical analysis and empirical evaluation to demonstrate the effectiveness of their approach in modeling dependencies among variables. Key findings indicate that AeTE outperforms traditional measures in capturing intricate causal dynamics, particularly in high-dimensional datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to causal inference, combining attention mechanisms with transfer entropy to enhance interpretability and precision. The empirical results validate the proposed method, indicating significant improvements over existing techniques. However, the paper suffers from a lack of clarity in certain sections, notably in the methodology and background, which may pose challenges for readers unfamiliar with the topic. Additionally, the dense language and technical jargon may limit accessibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a significant advancement in causal inference, its clarity is compromised by complex sentence structures and heavy jargon. The methodology, while robust, requires simplification and context to enhance understanding. The reproducibility of results appears feasible, but additional details on the implementation and data used would strengthen this aspect. Overall, the paper exhibits high quality in its contributions but needs refinement to improve clarity and accessibility.\n\n# Summary Of The Review\nThis paper offers a valuable contribution to the field of causal inference through the introduction of a novel framework that leverages attention mechanisms. However, the complexity of the writing and technical jargon may hinder comprehension by a broader audience. Further revisions focusing on clarity and organization are recommended to maximize the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -1.766628907138407,
    -1.8767405642126043,
    -1.8895642487934383,
    -1.843101010811126,
    -1.9808380951230358,
    -1.761604800838806,
    -1.4043313709542409,
    -2.0433955124371366,
    -1.8611503633560649,
    -1.7965561078869248,
    -1.7048592434870695,
    -1.5612407654436644,
    -1.8171969028670116,
    -1.7826199794091928,
    -1.8056995416478632,
    -1.8073439527755475,
    -1.9767739823072037,
    -1.9393441329766885,
    -1.7649716614863276,
    -1.8314176478283153,
    -1.8704836423617117,
    -1.6959057258903802,
    -1.8766601752098733,
    -1.9270652300062165,
    -1.7883823631808458,
    -1.9587333196197871,
    -1.936676086695223,
    -1.956350865178898,
    -1.7652617768068406
  ],
  "logp_cond": [
    [
      0.0,
      -1.5936666799089254,
      -1.5835536581148588,
      -1.5748870652133464,
      -1.6022786018373176,
      -1.6014636085380303,
      -1.65313985951023,
      -1.5788310517921023,
      -1.6218938403707293,
      -1.639176244721864,
      -1.5990709347529235,
      -1.6329739063805642,
      -1.581138300200469,
      -1.5875293594184496,
      -1.599624933453474,
      -1.5881673019973848,
      -1.6015484854478261,
      -1.585873740284063,
      -1.6201161433731754,
      -1.6101891968510609,
      -1.6003468704292065,
      -1.6298228446339886,
      -1.6359205415911837,
      -1.5779367324695857,
      -1.6093727499095312,
      -1.6077619963472267,
      -1.6043780035852442,
      -1.607759391557418,
      -1.6050897985250372
    ],
    [
      -1.4755579972151707,
      0.0,
      -1.311893837519035,
      -1.3462939998301402,
      -1.4022153320757005,
      -1.4423340034864784,
      -1.5535510689304588,
      -1.3293615261595015,
      -1.398650610357005,
      -1.5076619764434405,
      -1.4014487371174178,
      -1.5605039770500995,
      -1.2886856200734167,
      -1.3153096640802155,
      -1.3955324013636192,
      -1.3172115705663001,
      -1.3774745586796082,
      -1.3580475390926654,
      -1.4393782054250657,
      -1.3516176793108778,
      -1.3922514869435112,
      -1.4849466066111803,
      -1.487257315398386,
      -1.3796360330400619,
      -1.4540903284486517,
      -1.3910803476547093,
      -1.430340484508378,
      -1.4635817388906491,
      -1.478785459753716
    ],
    [
      -1.5682890122333037,
      -1.4580085506369778,
      0.0,
      -1.4066271525386773,
      -1.4539492325391028,
      -1.524554508326231,
      -1.593495778998436,
      -1.3936993251449907,
      -1.4450838171548432,
      -1.5503417412429432,
      -1.4734560674018087,
      -1.5905977998676457,
      -1.3787011653749575,
      -1.3636909038513496,
      -1.4622675276196395,
      -1.4051145231590974,
      -1.4320055011131338,
      -1.4548406724129215,
      -1.5095394738842798,
      -1.4173051797598861,
      -1.4852846757741356,
      -1.6063094245541958,
      -1.596792903221777,
      -1.4351502080367078,
      -1.4961413442449047,
      -1.4695183283103739,
      -1.5089719646451385,
      -1.547060680423838,
      -1.5667800872659048
    ],
    [
      -1.460998838881962,
      -1.3450544642941773,
      -1.3056019363736249,
      0.0,
      -1.377727652543779,
      -1.441372178555087,
      -1.5256115462508946,
      -1.328659774130663,
      -1.3945491641005383,
      -1.4788873286632804,
      -1.3412788546015149,
      -1.5204335736981693,
      -1.231338515383361,
      -1.3310844410055702,
      -1.4199337111098391,
      -1.3345196958374765,
      -1.3998118023980974,
      -1.3451684346221893,
      -1.3790447480391468,
      -1.3815814150132693,
      -1.3845170148564958,
      -1.4943553755630123,
      -1.539244852967703,
      -1.331563779746936,
      -1.4784297711595826,
      -1.3332281717576417,
      -1.4327793704906862,
      -1.466200109139347,
      -1.4429775394260733
    ],
    [
      -1.6299101824656157,
      -1.5708474722531862,
      -1.4985885007617041,
      -1.5240953428509256,
      0.0,
      -1.5747693567657528,
      -1.6408987168460827,
      -1.4709383978594601,
      -1.551578387947599,
      -1.5609297477678468,
      -1.5606407472932047,
      -1.6668042314540212,
      -1.4533649612333042,
      -1.4974267563864034,
      -1.5180639204158501,
      -1.5125094791662075,
      -1.5134005433848317,
      -1.5519319949661823,
      -1.5270805739157682,
      -1.5199508592713058,
      -1.6175397387345023,
      -1.6142538515092226,
      -1.6911335429078314,
      -1.5203792404757246,
      -1.6007012338120297,
      -1.5519633712783671,
      -1.6009465311489237,
      -1.5943470737530112,
      -1.6569215921649245
    ],
    [
      -1.5346495950904524,
      -1.4578126842661803,
      -1.4171923786022458,
      -1.4314485782327604,
      -1.4449668064138759,
      0.0,
      -1.4783298009828618,
      -1.4103462043392758,
      -1.4501513344070802,
      -1.4357433589312918,
      -1.4933300665726215,
      -1.4955292253759214,
      -1.3890443779892567,
      -1.391001844035494,
      -1.4108628099624927,
      -1.4154243239489475,
      -1.4557675883886785,
      -1.4267581216532244,
      -1.448156619914302,
      -1.4339027575613161,
      -1.5073688137145775,
      -1.4436696164708045,
      -1.5005385445045127,
      -1.4582412565852634,
      -1.4859199120131337,
      -1.4276587928222808,
      -1.4655243443381072,
      -1.4577848034862442,
      -1.5587687585494474
    ],
    [
      -1.1673766874512483,
      -1.1484842291729815,
      -1.0293286025450785,
      -1.118448767526252,
      -1.0508399760906735,
      -1.0871771123774503,
      0.0,
      -1.0954993456379514,
      -1.0565562484564541,
      -1.0658868165996427,
      -1.1118940305036515,
      -1.1448743839112843,
      -1.0427829957628327,
      -1.0733322059020318,
      -1.0742088116847113,
      -1.0909548349118003,
      -1.0936387835999501,
      -1.1311876444332476,
      -1.0720386239099093,
      -1.0926978561218827,
      -1.0987693680854171,
      -1.0769885270207016,
      -1.1182932000169954,
      -1.0790353119354428,
      -1.130691061073577,
      -1.0973494072985632,
      -1.0956003222120534,
      -1.1189106488156766,
      -1.1392426679953882
    ],
    [
      -1.665744891991081,
      -1.5528516524465397,
      -1.5148577771462857,
      -1.5169080276132636,
      -1.6114944760633394,
      -1.632641908328401,
      -1.7742908630310499,
      0.0,
      -1.6194891215344305,
      -1.6921493944242698,
      -1.570617156047333,
      -1.725556383413292,
      -1.5462239342664126,
      -1.4949358641879258,
      -1.6916018252557776,
      -1.4997306746581405,
      -1.5372024511804423,
      -1.573821567733382,
      -1.6196051361045918,
      -1.5930370845010673,
      -1.6302252464126656,
      -1.7370457890667859,
      -1.7144282365515602,
      -1.6005520478965678,
      -1.6613700993046923,
      -1.5856422746770267,
      -1.634204982048441,
      -1.6682356527621738,
      -1.6952513492434906
    ],
    [
      -1.545332600568835,
      -1.430919463351256,
      -1.3678519137506997,
      -1.4591347015406053,
      -1.3953673137975495,
      -1.480706570518643,
      -1.5508999251026336,
      -1.3893297727162959,
      0.0,
      -1.503884155456579,
      -1.4736970323958913,
      -1.5424164083849585,
      -1.38598093946575,
      -1.340258629124628,
      -1.4757976599725522,
      -1.395771178377339,
      -1.3910294049345724,
      -1.4392289336802122,
      -1.5004657176816028,
      -1.4091059745646162,
      -1.466342768185993,
      -1.5440140303855854,
      -1.502857687417312,
      -1.4949284253855535,
      -1.4654441218989527,
      -1.4230479248343346,
      -1.5096505510318703,
      -1.4102504609866515,
      -1.5309271306946886
    ],
    [
      -1.5015355765185097,
      -1.4403009960757702,
      -1.3922778976690735,
      -1.4200322724123664,
      -1.3812820299732436,
      -1.4139074990564353,
      -1.484444057362811,
      -1.387178606563082,
      -1.4412849761161306,
      0.0,
      -1.445858017587315,
      -1.5232061593399313,
      -1.3588143247928797,
      -1.4059719441964895,
      -1.4040218175792132,
      -1.4474040366077714,
      -1.419426315331264,
      -1.381129828805635,
      -1.4259135729003802,
      -1.4263090419123223,
      -1.426184707036682,
      -1.4602401896549737,
      -1.495573572613713,
      -1.3612366337223674,
      -1.4170350242036822,
      -1.4110680402967979,
      -1.4406565415331514,
      -1.4311849353781285,
      -1.501374364812438
    ],
    [
      -1.4069092234313527,
      -1.277165017615682,
      -1.245026954281342,
      -1.2192239588747964,
      -1.3072738609032426,
      -1.3243168193626547,
      -1.4521470533025125,
      -1.2025982052076092,
      -1.3082716072174951,
      -1.4184028298363116,
      0.0,
      -1.4035066055357481,
      -1.233211343270432,
      -1.1962820191615744,
      -1.3189082541596302,
      -1.2275135686818404,
      -1.27576634605526,
      -1.2102201660884306,
      -1.3200967956238123,
      -1.273022684073612,
      -1.3344763454797066,
      -1.4183656290328637,
      -1.3824462767184555,
      -1.2675895336700116,
      -1.3870105694702677,
      -1.2044025274884473,
      -1.3409067706913913,
      -1.350647699918399,
      -1.3737571358262102
    ],
    [
      -1.334336738719894,
      -1.304220791473305,
      -1.2555097909070485,
      -1.2848037098284062,
      -1.2880465359574267,
      -1.2640995752406325,
      -1.2904308665236095,
      -1.2394016561873682,
      -1.258096314031107,
      -1.2816650779328989,
      -1.2651707636416392,
      0.0,
      -1.2828603186758598,
      -1.2620904214852986,
      -1.2842091150949053,
      -1.2671388831412043,
      -1.2352165392690329,
      -1.2632693194843452,
      -1.2645542722121565,
      -1.2875292196235262,
      -1.2780553526686127,
      -1.2986918163985002,
      -1.2625124203251241,
      -1.300291457215273,
      -1.281726278956781,
      -1.2529057448770902,
      -1.2679048966265503,
      -1.294538190384178,
      -1.3115167101134764
    ],
    [
      -1.4878546186647288,
      -1.3313752379937303,
      -1.3005981488172287,
      -1.234668286091726,
      -1.3640042373413401,
      -1.394164748637957,
      -1.4999226909944936,
      -1.3507293466794297,
      -1.4043189462378436,
      -1.4476371768880885,
      -1.3720609569914286,
      -1.5532171729011828,
      0.0,
      -1.352614003033555,
      -1.3503389023173464,
      -1.3253073882528874,
      -1.3945328964887547,
      -1.4013610601356072,
      -1.388386244505818,
      -1.3775984002415715,
      -1.4366340946385794,
      -1.4905584243429106,
      -1.5441666476484455,
      -1.335739265709376,
      -1.5374933018739163,
      -1.3762283654041698,
      -1.4296414589987942,
      -1.4764779560383856,
      -1.5011945836450766
    ],
    [
      -1.484286005349439,
      -1.2981095981975148,
      -1.2288403262730923,
      -1.3047966992184505,
      -1.3475375066006017,
      -1.397883596233824,
      -1.573912880027351,
      -1.2820586495433088,
      -1.3308012002980094,
      -1.4770068200147815,
      -1.3160767550690817,
      -1.5045412156544546,
      -1.27585682583774,
      0.0,
      -1.4211726335405939,
      -1.256749011974299,
      -1.3363157811676099,
      -1.373921909804405,
      -1.4078239027906263,
      -1.3797846121572899,
      -1.3911005779238896,
      -1.5082404904054267,
      -1.4743849437387933,
      -1.3509394727784352,
      -1.40718383506428,
      -1.3413763682649005,
      -1.4242670465066767,
      -1.410559629613078,
      -1.46998189157113
    ],
    [
      -1.4709864066510636,
      -1.387265058765892,
      -1.351143402198066,
      -1.3443587472524745,
      -1.356738959852443,
      -1.401129408588268,
      -1.4975186552225999,
      -1.3937465995152567,
      -1.4356554826973151,
      -1.4469028249775486,
      -1.3815206401791833,
      -1.4795062908107846,
      -1.320505628132437,
      -1.3821791542958959,
      0.0,
      -1.4111144788562644,
      -1.420847189603278,
      -1.38128307612469,
      -1.4009261861861984,
      -1.4029038058281613,
      -1.3812215235159155,
      -1.4215918953742355,
      -1.5152121025747347,
      -1.3290265781162864,
      -1.4920832892586455,
      -1.384634337050532,
      -1.431219504339327,
      -1.4690577733864145,
      -1.443870287940255
    ],
    [
      -1.4581260221705759,
      -1.3320641400284294,
      -1.2947183346600437,
      -1.3222546178163272,
      -1.3856522144699404,
      -1.3898204672487373,
      -1.5380242689783012,
      -1.242893880817652,
      -1.377120675641292,
      -1.478350767121029,
      -1.3357719126653138,
      -1.5081777393624796,
      -1.2844980188244195,
      -1.268615909292687,
      -1.4082969224505044,
      0.0,
      -1.3134503250877574,
      -1.347993168454078,
      -1.3974362290311175,
      -1.3223355982132703,
      -1.403697257062722,
      -1.4771011250983124,
      -1.4644284445983808,
      -1.3798792564974753,
      -1.41213975790046,
      -1.3738230279165105,
      -1.4197588541443116,
      -1.4399020054490843,
      -1.4588329924575498
    ],
    [
      -1.6236159847413192,
      -1.538249534371558,
      -1.4044767586378006,
      -1.5210610782101546,
      -1.550621974726973,
      -1.643202483133922,
      -1.7111521978860793,
      -1.4378454174327684,
      -1.5084687160518573,
      -1.6324138395762353,
      -1.517339502838789,
      -1.6475852667161994,
      -1.5303398204831153,
      -1.4362923695918302,
      -1.5969262337702346,
      -1.4477364342040564,
      0.0,
      -1.5567961360809206,
      -1.5841917265732337,
      -1.4923523886346652,
      -1.5212395623612855,
      -1.661672553909561,
      -1.6340767675794285,
      -1.587774837572747,
      -1.5879862256247448,
      -1.5039224848496284,
      -1.5097990654345117,
      -1.5472599875726276,
      -1.6038236571711249
    ],
    [
      -1.538925037902661,
      -1.4758203940061372,
      -1.3790603628147209,
      -1.4086345078785238,
      -1.47788422233822,
      -1.5200749832361693,
      -1.6492520717315737,
      -1.4270041918826553,
      -1.4832258238543006,
      -1.5530951472851362,
      -1.4418931647501292,
      -1.614517517871368,
      -1.4671307802127305,
      -1.4161766629291568,
      -1.5352865899473482,
      -1.3736282678169145,
      -1.4760987593644117,
      0.0,
      -1.5190102528232081,
      -1.3931413130017836,
      -1.5056559965033316,
      -1.5992051528876514,
      -1.5946203534331265,
      -1.4297611115272,
      -1.518144231635369,
      -1.4795225594791026,
      -1.5244205894479406,
      -1.4927009435251266,
      -1.593889006275888
    ],
    [
      -1.451052407556045,
      -1.4226810916951242,
      -1.3922877115433916,
      -1.3213526968907254,
      -1.3544707391387267,
      -1.3801088415553948,
      -1.4712880809486921,
      -1.3233225163146058,
      -1.3981428091756491,
      -1.3736115240456719,
      -1.3707525799772653,
      -1.484426273624097,
      -1.3431170210270502,
      -1.3838395001861599,
      -1.3805335913969117,
      -1.3628874556187047,
      -1.3754578186515933,
      -1.4011665350841875,
      0.0,
      -1.396308520654846,
      -1.3892465054344791,
      -1.4435454960243301,
      -1.4637432901989487,
      -1.3554947446315386,
      -1.452226293558582,
      -1.3704692526042803,
      -1.4351606510237906,
      -1.4559525920504195,
      -1.4686375571642585
    ],
    [
      -1.4873221181654352,
      -1.3443442637519487,
      -1.3826202978951105,
      -1.4024513795188374,
      -1.4029392931967952,
      -1.4973622414393675,
      -1.5563160065170991,
      -1.3598226992474114,
      -1.4386153251442164,
      -1.484914781354121,
      -1.4060757806811246,
      -1.518920348493031,
      -1.3722191173825158,
      -1.3672167916451283,
      -1.4115299905727365,
      -1.3354678344922692,
      -1.3861659818572325,
      -1.3681783479819714,
      -1.4688886834933392,
      0.0,
      -1.4494728531256789,
      -1.5309559856593538,
      -1.4875008290456448,
      -1.433029882293933,
      -1.4726935274413102,
      -1.4204066350016464,
      -1.4201820671761078,
      -1.440419526821756,
      -1.4821279489864996
    ],
    [
      -1.5091016552098662,
      -1.3905943461059087,
      -1.3796505624716007,
      -1.3700071585777487,
      -1.4694609523071873,
      -1.4902646751914268,
      -1.6297303054528376,
      -1.3635015387022509,
      -1.435808381520558,
      -1.4457184842259636,
      -1.3930447847758562,
      -1.5546244107160962,
      -1.4118855773872234,
      -1.4261693386636463,
      -1.4273003542150542,
      -1.4206451888448484,
      -1.3638952758024254,
      -1.3906084379600998,
      -1.4366159107699683,
      -1.4314015772168396,
      0.0,
      -1.5644777277221558,
      -1.4954404165689492,
      -1.3993716917591918,
      -1.4790457588355315,
      -1.3842174803361902,
      -1.4151949695036687,
      -1.4193246581573637,
      -1.5005978175541828
    ],
    [
      -1.327425639004976,
      -1.3334428737142774,
      -1.2759895229010243,
      -1.2908287644314138,
      -1.2495436917391864,
      -1.2216376545485657,
      -1.293093580465483,
      -1.2547012792023509,
      -1.278771956726936,
      -1.2550754949119012,
      -1.2752860809973212,
      -1.3200249837830145,
      -1.2353249528089407,
      -1.2590951141833533,
      -1.1861991840956871,
      -1.2627503649736287,
      -1.2480377053813576,
      -1.2729625479437974,
      -1.244616053509967,
      -1.2793567006883648,
      -1.3036891507052462,
      0.0,
      -1.3130583780031349,
      -1.231521647550143,
      -1.3452548890486247,
      -1.2589219555217914,
      -1.2789430827087283,
      -1.2776288532564744,
      -1.2833044951452892
    ],
    [
      -1.5372379112715089,
      -1.4359588373527317,
      -1.4328074303792677,
      -1.471600102443363,
      -1.4923325204871662,
      -1.4913702189721973,
      -1.521477728558579,
      -1.3993886701792706,
      -1.4100789217443144,
      -1.4054273588353143,
      -1.4354206282200856,
      -1.4659973757683933,
      -1.4674045754191702,
      -1.4326688752201857,
      -1.487853400835336,
      -1.4233004145715644,
      -1.3779273363797802,
      -1.44745331808075,
      -1.4818551382997538,
      -1.391058180428424,
      -1.414742132430112,
      -1.5024032253410138,
      0.0,
      -1.478530919897106,
      -1.39738279921713,
      -1.4342117573044886,
      -1.4118698204454772,
      -1.4105086328075822,
      -1.5380328656541797
    ],
    [
      -1.5173743972535367,
      -1.4080214885334006,
      -1.37106257497418,
      -1.3739550037098067,
      -1.3956946838883262,
      -1.491226213864237,
      -1.583531168494625,
      -1.3812290761125574,
      -1.4812527468898424,
      -1.4625707457904282,
      -1.3950830341750862,
      -1.5976103924548366,
      -1.3326234000504755,
      -1.3643966138632382,
      -1.3484772516711636,
      -1.3888882972167254,
      -1.413191535863102,
      -1.4233785832626984,
      -1.3903637316661426,
      -1.4494908477252706,
      -1.4112579444922297,
      -1.5255749524948266,
      -1.6020008990136487,
      0.0,
      -1.5391794601662652,
      -1.4324347275553921,
      -1.4736535191306819,
      -1.481680861863622,
      -1.5025312158697148
    ],
    [
      -1.466841211950128,
      -1.453989843537949,
      -1.389152438110197,
      -1.443313228970342,
      -1.4826315062614996,
      -1.4782094029130912,
      -1.5273365166541208,
      -1.3913193592715007,
      -1.3787567235282345,
      -1.4620367281785196,
      -1.4302401969680552,
      -1.4844767452882568,
      -1.4797272712220992,
      -1.3918506875096581,
      -1.5069880101595485,
      -1.4108787786736998,
      -1.3990906065687203,
      -1.3930490531836957,
      -1.451168152246552,
      -1.407085827706317,
      -1.4328408221374227,
      -1.518263169202144,
      -1.430332985917367,
      -1.4864098483211445,
      0.0,
      -1.4132228904801245,
      -1.4105918352128233,
      -1.3857647786153657,
      -1.4953247390485687
    ],
    [
      -1.6139261217303804,
      -1.4997767187743671,
      -1.4601665789385583,
      -1.4560997983941053,
      -1.4946480776511526,
      -1.5664242594204731,
      -1.6435557463929336,
      -1.4779162155869714,
      -1.5048202599807636,
      -1.5936164520628493,
      -1.4877230967492765,
      -1.6308869313520689,
      -1.4323703688755594,
      -1.500033259185465,
      -1.4736356053170343,
      -1.495491357200609,
      -1.4729665729755224,
      -1.485696127532573,
      -1.5267097944599837,
      -1.5184531769603837,
      -1.4779216752868265,
      -1.6305457763109998,
      -1.631515423507335,
      -1.5093521209000502,
      -1.5740906039966638,
      0.0,
      -1.5201630644311057,
      -1.536627035756012,
      -1.5435684024063492
    ],
    [
      -1.6215229390642536,
      -1.5316355596630515,
      -1.4823622820997573,
      -1.4959262022345519,
      -1.551106601898225,
      -1.5414477305628407,
      -1.6213303182107872,
      -1.4979934737848455,
      -1.565014339421109,
      -1.5728734380152798,
      -1.5041975528444256,
      -1.6017314700858976,
      -1.5059726578221226,
      -1.510756863909249,
      -1.506322689188945,
      -1.5215405485475628,
      -1.448219091814804,
      -1.4990859967613988,
      -1.5303458518638144,
      -1.4630382325942741,
      -1.4676431476951382,
      -1.6040092369606391,
      -1.5874771869632696,
      -1.5139813767270178,
      -1.5463281375778588,
      -1.4844538157613094,
      0.0,
      -1.5995531190387597,
      -1.5735257249999342
    ],
    [
      -1.6009749720898652,
      -1.5158658043038489,
      -1.4693784381515727,
      -1.5411581704305806,
      -1.4883350005523672,
      -1.5898195118307024,
      -1.5967425602439378,
      -1.4545319672366412,
      -1.3886572812134979,
      -1.5401267448228193,
      -1.5540810648080248,
      -1.5914722084806214,
      -1.525535802166883,
      -1.431102504525514,
      -1.5398314594585116,
      -1.528143881943782,
      -1.464064204341024,
      -1.4546128136723464,
      -1.5995682218781246,
      -1.4499444087324957,
      -1.4751191020226149,
      -1.5673471562894377,
      -1.550112759420302,
      -1.5125441681075307,
      -1.477680262887635,
      -1.5608351439905632,
      -1.5403564975273516,
      0.0,
      -1.5419079120667931
    ],
    [
      -1.4002199767213155,
      -1.355884619326204,
      -1.2957499939842048,
      -1.3413380455378892,
      -1.3588148734780832,
      -1.404980855336301,
      -1.4744630255634787,
      -1.3033786439210073,
      -1.36302100122053,
      -1.3960139403232201,
      -1.339913686613302,
      -1.3965966649625197,
      -1.3487431064952704,
      -1.3135645397324829,
      -1.3513335109648923,
      -1.3239594350447348,
      -1.3268428064284874,
      -1.3301181203240646,
      -1.3610517485182978,
      -1.3139930786142997,
      -1.3007760464847637,
      -1.3918497659118878,
      -1.398390929358958,
      -1.3158484158128214,
      -1.3428121141262646,
      -1.3051450802648983,
      -1.3106531510960104,
      -1.357702234562728,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.1729622272294815,
      0.1830752490235481,
      0.1917418419250605,
      0.1643503053010893,
      0.16516529860037665,
      0.11348904762817691,
      0.18779785534630467,
      0.14473506676767767,
      0.12745266241654285,
      0.16755797238548342,
      0.13365500075784276,
      0.18549060693793784,
      0.17909954771995729,
      0.16700397368493292,
      0.17846160514102216,
      0.1650804216905808,
      0.1807551668543439,
      0.14651276376523148,
      0.15643971028734605,
      0.1662820367092004,
      0.1368060625044183,
      0.1307083655472232,
      0.18869217466882127,
      0.15725615722887576,
      0.1588669107911802,
      0.16225090355316274,
      0.15886951558098894,
      0.16153910861336973
    ],
    [
      0.4011825669974336,
      0.0,
      0.5648467266935693,
      0.5304465643824641,
      0.4745252321369038,
      0.4344065607261258,
      0.32318949528214547,
      0.5473790380531027,
      0.47808995385559916,
      0.3690785877691638,
      0.47529182709518647,
      0.3162365871625048,
      0.5880549441391876,
      0.5614309001323887,
      0.48120816284898504,
      0.5595289936463042,
      0.49926600553299605,
      0.5186930251199389,
      0.4373623587875386,
      0.5251228849017264,
      0.48448907726909307,
      0.391793957601424,
      0.38948324881421836,
      0.4971045311725424,
      0.42265023576395255,
      0.48566021655789493,
      0.44640007970422624,
      0.41315882532195514,
      0.39795510445888826
    ],
    [
      0.32127523656013457,
      0.43155569815646055,
      0.0,
      0.482937096254761,
      0.4356150162543355,
      0.36500974046720724,
      0.29606846979500223,
      0.4958649236484476,
      0.4444804316385951,
      0.3392225075504951,
      0.41610818139162964,
      0.2989664489257926,
      0.5108630834184809,
      0.5258733449420887,
      0.4272967211737988,
      0.48444972563434097,
      0.45755874768030447,
      0.43472357638051684,
      0.38002477490915854,
      0.4722590690335522,
      0.40427957301930273,
      0.28325482423924253,
      0.2927713455716614,
      0.45441404075673053,
      0.39342290454853357,
      0.42004592048306444,
      0.3805922841482998,
      0.3425035683696003,
      0.32278416152753353
    ],
    [
      0.3821021719291642,
      0.4980465465169488,
      0.5374990744375012,
      0.0,
      0.46537335826734716,
      0.40172883225603906,
      0.3174894645602315,
      0.5144412366804632,
      0.4485518467105878,
      0.36421368214784566,
      0.5018221562096112,
      0.32266743711295676,
      0.6117624954277652,
      0.5120165698055559,
      0.42316729970128697,
      0.5085813149736496,
      0.44328920841302866,
      0.4979325761889368,
      0.4640562627719793,
      0.46151959579785684,
      0.4585839959546303,
      0.34874563524811375,
      0.3038561578434231,
      0.5115372310641901,
      0.36467123965154347,
      0.5098728390534844,
      0.41032164032043994,
      0.37690090167177903,
      0.4001234713850528
    ],
    [
      0.35092791265742007,
      0.40999062286984955,
      0.48224959436133163,
      0.45674275227211014,
      0.0,
      0.40606873835728297,
      0.3399393782769531,
      0.5098996972635756,
      0.4292597071754367,
      0.419908347355189,
      0.4201973478298311,
      0.31403386366901453,
      0.5274731338897316,
      0.48341133873663233,
      0.4627741747071856,
      0.4683286159568283,
      0.4674375517382041,
      0.4289061001568535,
      0.45375752120726753,
      0.46088723585172997,
      0.36329835638853347,
      0.36658424361381314,
      0.2897045522152044,
      0.4604588546473112,
      0.3801368613110061,
      0.4288747238446686,
      0.37989156397411206,
      0.38649102137002456,
      0.32391650295811125
    ],
    [
      0.2269552057483537,
      0.30379211657262584,
      0.3444124222365603,
      0.3301562226060457,
      0.31663799442493024,
      0.0,
      0.2832749998559443,
      0.3512585964995303,
      0.3114534664317259,
      0.3258614419075143,
      0.26827473426618464,
      0.26607557546288474,
      0.3725604228495494,
      0.370602956803312,
      0.3507419908763134,
      0.34618047688985865,
      0.3058372124501276,
      0.33484667918558175,
      0.3134481809245042,
      0.32770204327749,
      0.25423598712422857,
      0.3179351843680016,
      0.26106625633429337,
      0.3033635442535427,
      0.27568488882567244,
      0.3339460080165253,
      0.2960804565006989,
      0.30381999735256193,
      0.2028360422893587
    ],
    [
      0.23695468350299254,
      0.25584714178125934,
      0.37500276840916236,
      0.2858826034279889,
      0.3534913948635674,
      0.3171542585767906,
      0.0,
      0.3088320253162895,
      0.34777512249778675,
      0.3384445543545982,
      0.29243734045058933,
      0.2594569870429566,
      0.36154837519140814,
      0.33099916505220905,
      0.3301225592695296,
      0.31337653604244053,
      0.31069258735429073,
      0.2731437265209933,
      0.33229274704433154,
      0.3116335148323581,
      0.30556200286882373,
      0.3273428439335393,
      0.2860381709372455,
      0.3252960590187981,
      0.2736403098806639,
      0.30698196365567765,
      0.3087310487421875,
      0.2854207221385643,
      0.2650887029588527
    ],
    [
      0.3776506204460557,
      0.4905438599905969,
      0.5285377352908509,
      0.526487484823873,
      0.4319010363737972,
      0.4107536041087356,
      0.2691046494060867,
      0.0,
      0.4239063909027061,
      0.35124611801286676,
      0.4727783563898036,
      0.3178391290238445,
      0.497171578170724,
      0.5484596482492108,
      0.35179368718135895,
      0.5436648377789961,
      0.5061930612566943,
      0.4695739447037546,
      0.4237903763325448,
      0.4503584279360693,
      0.41317026602447093,
      0.3063497233703507,
      0.3289672758855764,
      0.4428434645405688,
      0.3820254131324443,
      0.4577532377601099,
      0.4091905303886956,
      0.3751598596749628,
      0.34814416319364594
    ],
    [
      0.31581776278722984,
      0.4302309000048088,
      0.4932984496053652,
      0.40201566181545956,
      0.46578304955851535,
      0.3804437928374218,
      0.31025043825343124,
      0.471820590639769,
      0.0,
      0.35726620789948593,
      0.38745333096017354,
      0.31873395497110635,
      0.4751694238903148,
      0.520891734231437,
      0.3853527033835127,
      0.4653791849787259,
      0.4701209584214925,
      0.42192142967585267,
      0.3606846456744621,
      0.4520443887914487,
      0.3948075951700718,
      0.3171363329704795,
      0.3582926759387528,
      0.3662219379705114,
      0.3957062414571122,
      0.43810243852173025,
      0.3514998123241946,
      0.45089990236941335,
      0.3302232326613763
    ],
    [
      0.2950205313684151,
      0.35625511181115455,
      0.40427821021785126,
      0.37652383547455837,
      0.4152740779136812,
      0.38264860883048946,
      0.3121120505241137,
      0.40937750132384276,
      0.35527113177079417,
      0.0,
      0.35069809029960974,
      0.27334994854699346,
      0.4377417830940451,
      0.39058416369043525,
      0.3925342903077116,
      0.3491520712791534,
      0.3771297925556607,
      0.4154262790812897,
      0.37064253498654454,
      0.37024706597460244,
      0.3703714008502428,
      0.33631591823195106,
      0.30098253527321184,
      0.4353194741645574,
      0.37952108368324255,
      0.3854880675901269,
      0.3558995663537734,
      0.36537117250879625,
      0.29518174307448675
    ],
    [
      0.2979500200557168,
      0.4276942258713876,
      0.4598322892057276,
      0.4856352846122731,
      0.3975853825838269,
      0.38054242412441486,
      0.25271219018455704,
      0.5022610382794603,
      0.3965876362695744,
      0.2864564136507579,
      0.0,
      0.3013526379513214,
      0.4716479002166376,
      0.5085772243254951,
      0.3859509893274393,
      0.47734567480522916,
      0.42909289743180956,
      0.4946390773986389,
      0.3847624478632572,
      0.43183655941345744,
      0.3703828980073629,
      0.28649361445420585,
      0.322412966768614,
      0.4372697098170579,
      0.31784867401680184,
      0.5004567159986222,
      0.3639524727956782,
      0.3542115435686706,
      0.33110210766085935
    ],
    [
      0.2269040267237703,
      0.25701997397035936,
      0.3057309745366159,
      0.2764370556152582,
      0.2731942294862377,
      0.29714119020303187,
      0.2708098989200549,
      0.3218391092562962,
      0.3031444514125574,
      0.27957568751076556,
      0.29607000180202525,
      0.0,
      0.2783804467678046,
      0.2991503439583658,
      0.2770316503487591,
      0.29410188230246015,
      0.32602422617463156,
      0.29797144595931924,
      0.29668649323150786,
      0.27371154582013824,
      0.2831854127750517,
      0.26254894904516424,
      0.2987283451185403,
      0.26094930822839135,
      0.2795144864868835,
      0.30833502056657425,
      0.29333586881711415,
      0.26670257505948647,
      0.24972405533018804
    ],
    [
      0.32934228420228284,
      0.4858216648732814,
      0.516598754049783,
      0.5825286167752857,
      0.4531926655256715,
      0.4230321542290547,
      0.31727421187251803,
      0.46646755618758196,
      0.41287795662916804,
      0.36955972597892317,
      0.44513594587558303,
      0.26397972996582886,
      0.0,
      0.4645828998334567,
      0.4668580005496652,
      0.4918895146141242,
      0.42266400637825696,
      0.4158358427314044,
      0.4288106583611937,
      0.4395985026254401,
      0.38056280822843225,
      0.326638478524101,
      0.27303025521856616,
      0.48145763715763557,
      0.2797036009930953,
      0.4409685374628418,
      0.3875554438682174,
      0.340718946828626,
      0.316002319221935
    ],
    [
      0.2983339740597537,
      0.484510381211678,
      0.5537796531361006,
      0.47782328019074227,
      0.43508247280859114,
      0.38473638317536873,
      0.20870709938184184,
      0.500561329865884,
      0.4518187791111834,
      0.30561315939441136,
      0.4665432243401111,
      0.27807876375473817,
      0.5067631535714527,
      0.0,
      0.36144734586859895,
      0.5258709674348938,
      0.44630419824158296,
      0.4086980696047877,
      0.3747960766185665,
      0.40283536725190294,
      0.3915194014853032,
      0.2743794890037661,
      0.30823503567039956,
      0.43168050663075763,
      0.3754361443449128,
      0.44124361114429234,
      0.35835293290251613,
      0.3720603497961148,
      0.3126380878380628
    ],
    [
      0.33471313499679956,
      0.41843448288197127,
      0.4545561394497972,
      0.46134079439538866,
      0.4489605817954201,
      0.4045701330595952,
      0.3081808864252633,
      0.41195294213260647,
      0.37004405895054804,
      0.35879671667031454,
      0.42417890146867987,
      0.3261932508370786,
      0.4851939135154262,
      0.4235203873519673,
      0.0,
      0.3945850627915988,
      0.38485235204458523,
      0.42441646552317325,
      0.4047733554616648,
      0.40279573581970185,
      0.42447801813194763,
      0.3841076462736277,
      0.29048743907312846,
      0.47667296353157673,
      0.31361625238921764,
      0.4210652045973311,
      0.3744800373085362,
      0.33664176826144865,
      0.3618292537076082
    ],
    [
      0.34921793060497164,
      0.47527981274711806,
      0.5126256181155038,
      0.48508933495922024,
      0.42169173830560713,
      0.4175234855268102,
      0.26931968379724625,
      0.5644500719578955,
      0.43022327713425557,
      0.3289931856545185,
      0.4715720401102337,
      0.2991662134130679,
      0.522845933951128,
      0.5387280434828605,
      0.3990470303250431,
      0.0,
      0.4938936276877901,
      0.4593507843214695,
      0.40990772374442996,
      0.4850083545622772,
      0.4036466957128255,
      0.3302428276772351,
      0.34291550817716665,
      0.42746469627807215,
      0.3952041948750875,
      0.43352092485903704,
      0.3875850986312359,
      0.3674419473264632,
      0.34851096031799766
    ],
    [
      0.3531579975658845,
      0.4385244479356456,
      0.572297223669403,
      0.4557129040970491,
      0.42615200758023075,
      0.33357149917328166,
      0.2656217844211244,
      0.5389285648744353,
      0.4683052662553464,
      0.3443601427309684,
      0.45943447946841465,
      0.32918871559100427,
      0.44643416182408835,
      0.5404816127153735,
      0.3798477485369691,
      0.5290375481031473,
      0.0,
      0.4199778462262831,
      0.39258225573397,
      0.48442159367253845,
      0.45553441994591815,
      0.31510142839764277,
      0.34269721472777515,
      0.38899914473445674,
      0.3887877566824589,
      0.47285149745757526,
      0.466974916872692,
      0.4295139947345761,
      0.37295032513607884
    ],
    [
      0.4004190950740274,
      0.4635237389705513,
      0.5602837701619676,
      0.5307096250981647,
      0.4614599106384685,
      0.4192691497405192,
      0.2900920612451148,
      0.5123399410940332,
      0.45611830912238793,
      0.3862489856915523,
      0.4974509682265593,
      0.3248266151053205,
      0.47221335276395804,
      0.5231674700475317,
      0.40405754302934027,
      0.565715865159774,
      0.4632453736122768,
      0.0,
      0.4203338801534804,
      0.5462028199749049,
      0.4336881364733569,
      0.3401389800890371,
      0.34472377954356204,
      0.5095830214494885,
      0.42119990134131946,
      0.4598215734975859,
      0.41492354352874794,
      0.4466431894515619,
      0.3454551267008006
    ],
    [
      0.31391925393028264,
      0.3422905697912033,
      0.37268394994293597,
      0.4436189645956021,
      0.4105009223476008,
      0.38486281993093274,
      0.29368358053763544,
      0.4416491451717217,
      0.3668288523106784,
      0.3913601374406557,
      0.39421908150906226,
      0.2805453878622306,
      0.42185464045927734,
      0.3811321613001677,
      0.3844380700894159,
      0.40208420586762283,
      0.3895138428347342,
      0.36380512640214,
      0.0,
      0.36866314083148155,
      0.3757251560518484,
      0.3214261654619974,
      0.3012283712873789,
      0.40947691685478893,
      0.31274536792774565,
      0.39450240888204724,
      0.3298110104625369,
      0.30901906943590807,
      0.2963341043220691
    ],
    [
      0.34409552966288004,
      0.4870733840763666,
      0.44879734993320475,
      0.4289662683094779,
      0.42847835463152006,
      0.3340554063889478,
      0.27510164131121617,
      0.4715949485809039,
      0.39280232268409887,
      0.34650286647419426,
      0.4253418671471907,
      0.31249729933528436,
      0.4591985304457995,
      0.46420085618318696,
      0.41988765725557875,
      0.49594981333604604,
      0.4452516659710828,
      0.46323929984634393,
      0.36252896433497606,
      0.0,
      0.3819447947026364,
      0.30046166216896153,
      0.34391681878267044,
      0.3983877655343824,
      0.3587241203870051,
      0.41101101282666885,
      0.41123558065220744,
      0.3909981210065594,
      0.3492896988418157
    ],
    [
      0.36138198715184555,
      0.479889296255803,
      0.49083307989011105,
      0.500476483783963,
      0.4010226900545244,
      0.3802189671702849,
      0.24075333690887413,
      0.5069821036594608,
      0.4346752608411537,
      0.4247651581357481,
      0.47743885758585547,
      0.31585923164561547,
      0.4585980649744883,
      0.44431430369806546,
      0.44318328814665753,
      0.44983845351686336,
      0.5065883665592863,
      0.4798752044016119,
      0.4338677315917434,
      0.43908206514487214,
      0.0,
      0.30600591463955595,
      0.3750432257927625,
      0.47111195060251987,
      0.39143788352618025,
      0.48626616202552153,
      0.455288672858043,
      0.45115898420434797,
      0.36988582480752896
    ],
    [
      0.36848008688540435,
      0.36246285217610286,
      0.41991620298935595,
      0.40507696145896643,
      0.4463620341511938,
      0.4742680713418146,
      0.4028121454248972,
      0.4412044466880294,
      0.41713376916344425,
      0.44083023097847907,
      0.42061964489305903,
      0.3758807421073658,
      0.4605807730814395,
      0.43681061170702695,
      0.5097065417946931,
      0.43315536091675155,
      0.44786802050902264,
      0.42294317794658287,
      0.4512896723804132,
      0.4165490252020154,
      0.3922165751851341,
      0.0,
      0.3828473478872454,
      0.4643840783402373,
      0.3506508368417556,
      0.4369837703685888,
      0.416962643181652,
      0.4182768726339059,
      0.41260123074509103
    ],
    [
      0.3394222639383644,
      0.4407013378571416,
      0.44385274483060555,
      0.40506007276651035,
      0.384327654722707,
      0.3852899562376759,
      0.3551824466512943,
      0.47727150503060267,
      0.4665812534655589,
      0.47123281637455894,
      0.4412395469897876,
      0.41066279944148,
      0.40925559979070303,
      0.44399129998968756,
      0.38880677437453737,
      0.45335976063830885,
      0.4987328388300931,
      0.4292068571291232,
      0.3948050369101195,
      0.48560199478144916,
      0.46191804277976134,
      0.3742569498688595,
      0.0,
      0.3981292553127673,
      0.47927737599274334,
      0.4424484179053847,
      0.4647903547643961,
      0.46615154240229106,
      0.33862730955569353
    ],
    [
      0.4096908327526798,
      0.519043741472816,
      0.5560026550320365,
      0.5531102262964098,
      0.5313705461178904,
      0.4358390161419796,
      0.3435340615115916,
      0.5458361538936591,
      0.4458124831163741,
      0.4644944842157883,
      0.5319821958311304,
      0.32945483755137994,
      0.594441829955741,
      0.5626686161429784,
      0.5785879783350529,
      0.5381769327894912,
      0.5138736941431146,
      0.5036866467435182,
      0.536701498340074,
      0.47757438228094595,
      0.5158072855139868,
      0.40149027751139,
      0.3250643309925678,
      0.0,
      0.3878857698399514,
      0.4946305024508244,
      0.45341171087553467,
      0.44538436814259463,
      0.4245340141365017
    ],
    [
      0.3215411512307178,
      0.33439251964289674,
      0.3992299250706488,
      0.34506913421050367,
      0.3057508569193461,
      0.31017296026775454,
      0.2610458465267249,
      0.397063003909345,
      0.40962563965261123,
      0.32634563500232616,
      0.3581421662127906,
      0.30390561789258896,
      0.3086550919587465,
      0.39653167567118763,
      0.2813943530212972,
      0.37750358450714594,
      0.38929175661212545,
      0.3953333099971501,
      0.3372142109342937,
      0.3812965354745288,
      0.35554154104342306,
      0.27011919397870177,
      0.35804937726347874,
      0.3019725148597012,
      0.0,
      0.37515947270072125,
      0.3777905279680225,
      0.40261758456548,
      0.2930576241322771
    ],
    [
      0.3448071978894067,
      0.45895660084542,
      0.49856674068122886,
      0.5026335212256818,
      0.46408524196863454,
      0.392309060199314,
      0.3151775732268536,
      0.48081710403281575,
      0.4539130596390235,
      0.36511686755693784,
      0.47101022287051064,
      0.3278463882677183,
      0.5263629507442278,
      0.4587000604343221,
      0.48509771430275284,
      0.46324196241917814,
      0.4857667466442648,
      0.47303719208721406,
      0.43202352515980347,
      0.4402801426594034,
      0.48081164433296064,
      0.3281875433087873,
      0.3272178961124521,
      0.4493811987197369,
      0.38464271562312335,
      0.0,
      0.4385702551886814,
      0.42210628386377524,
      0.41516491721343796
    ],
    [
      0.31515314763096947,
      0.4050405270321715,
      0.4543138045954658,
      0.4407498844606712,
      0.38556948479699815,
      0.3952283561323824,
      0.31534576848443585,
      0.4386826129103776,
      0.37166174727411416,
      0.3638026486799433,
      0.4324785338507975,
      0.3349446166093255,
      0.4307034288731004,
      0.42591922278597405,
      0.4303533975062781,
      0.41513553814766024,
      0.48845699488041916,
      0.4375900899338243,
      0.4063302348314086,
      0.47363785410094894,
      0.4690329390000849,
      0.33266684973458394,
      0.3491988997319535,
      0.42269470996820524,
      0.39034794911736426,
      0.45222227093391365,
      0.0,
      0.3371229676564633,
      0.3631503616952889
    ],
    [
      0.3553758930890327,
      0.440485060875049,
      0.48697242702732524,
      0.4151926947483173,
      0.46801586462653066,
      0.36653135334819553,
      0.3596083049349601,
      0.5018188979422566,
      0.5676935839654,
      0.4162241203560786,
      0.4022698003708731,
      0.36487865669827646,
      0.4308150630120149,
      0.525248360653384,
      0.41651940572038626,
      0.42820698323511586,
      0.492286660837874,
      0.5017380515065515,
      0.35678264330077325,
      0.5064064564464021,
      0.481231763156283,
      0.38900370888946023,
      0.406238105758596,
      0.4438066970713672,
      0.4786706022912628,
      0.39551572118833467,
      0.4159943676515463,
      0.0,
      0.4144429531121048
    ],
    [
      0.3650418000855251,
      0.40937715748063663,
      0.46951178282263584,
      0.42392373126895144,
      0.40644690332875744,
      0.3602809214705396,
      0.29079875124336185,
      0.46188313288583327,
      0.40224077558631066,
      0.3692478364836205,
      0.4253480901935387,
      0.36866511184432094,
      0.4165186703115702,
      0.4516972370743577,
      0.4139282658419483,
      0.4413023417621058,
      0.4384189703783532,
      0.435143656482776,
      0.4042100282885428,
      0.4512686981925409,
      0.46448573032207685,
      0.3734120108949528,
      0.36687084744788256,
      0.4494133609940192,
      0.42244966268057604,
      0.4601166965419423,
      0.45460862571083016,
      0.4075595422441125,
      0.0
    ]
  ],
  "row_avgs": [
    0.16186062709500634,
    0.46478698899740917,
    0.40407933630282394,
    0.4414597943607647,
    0.41684108266625763,
    0.3071086108690686,
    0.30782821127378196,
    0.42340566001247837,
    0.4031274563487019,
    0.36638278717076206,
    0.3948783220235305,
    0.2840695948366912,
    0.4079531685272127,
    0.39778032992279694,
    0.39376563853021446,
    0.42037381229487736,
    0.42183744638801185,
    0.4433519902494783,
    0.3642115151372036,
    0.39826905717182887,
    0.42770866248476025,
    0.42245977596355955,
    0.42679227890472166,
    0.47928896686171435,
    0.34549331468666195,
    0.4316368688292024,
    0.4027691014769688,
    0.4367133643504912,
    0.4144346549950934
  ],
  "col_avgs": [
    0.33345836784026833,
    0.4099909286035995,
    0.46034233269343555,
    0.4357888893518128,
    0.4097214645529972,
    0.3765293836650844,
    0.29288140237827326,
    0.45643839546873305,
    0.40720041429766785,
    0.3582936045855087,
    0.4140391037865824,
    0.3096050553053515,
    0.452439262615241,
    0.45402827702566484,
    0.3999335470538939,
    0.44262874338103,
    0.43088327810231003,
    0.4215146660039276,
    0.3896778072729921,
    0.42482088253354033,
    0.3977426269366782,
    0.32317665771440585,
    0.32145629820412686,
    0.4145745267265263,
    0.36260209395859044,
    0.41973970884579537,
    0.3845172125017135,
    0.37224732634075114,
    0.33439616098557196
  ],
  "combined_avgs": [
    0.24765949746763732,
    0.4373889588005043,
    0.43221083449812975,
    0.43862434185628874,
    0.4132812736096274,
    0.34181899726707654,
    0.3003548068260276,
    0.43992202774060574,
    0.40516393532318484,
    0.36233819587813537,
    0.4044587129050564,
    0.29683732507102134,
    0.43019621557122684,
    0.4259043034742309,
    0.3968495927920542,
    0.43150127783795367,
    0.42636036224516094,
    0.432433328126703,
    0.37694466120509784,
    0.41154496985268463,
    0.41272564471071926,
    0.3728182168389827,
    0.37412428855442426,
    0.4469317467941203,
    0.3540477043226262,
    0.4256882888374989,
    0.39364315698934116,
    0.40448034534562116,
    0.3744154079903327
  ],
  "gppm": [
    622.749251724628,
    606.7005958077885,
    583.4738724475887,
    595.7281413871475,
    605.191210718794,
    619.2877730796808,
    662.4075520571014,
    586.0593620537117,
    608.1330680241912,
    629.712860749412,
    608.8590818521059,
    648.6726905604226,
    588.4021728748108,
    587.7498875192978,
    612.2942370971567,
    593.2568645848784,
    599.1623236911184,
    603.1859438474266,
    616.3182471175195,
    601.451790272322,
    614.7983824655097,
    648.8232502625362,
    648.5559841941451,
    606.4788700422715,
    629.3081875039396,
    603.0521714927166,
    620.4382582719774,
    624.2985574704599,
    643.2452537295578
  ],
  "gppm_normalized": [
    1.4355159402922435,
    1.3976157989464344,
    1.3460871079494903,
    1.374792935686062,
    1.388768012723485,
    1.4222450835541551,
    1.527896963430491,
    1.3473006926984785,
    1.3993161998426642,
    1.4484943597159206,
    1.399957400236921,
    1.506445427872406,
    1.3586080672480503,
    1.3616344436545387,
    1.4085205934399878,
    1.3655258908799675,
    1.3784185359934524,
    1.3837065695581028,
    1.413088920937483,
    1.3809670778723226,
    1.4172564667184024,
    1.4892696109382189,
    1.4943119520194792,
    1.3943725869552799,
    1.441907347719533,
    1.3887769858408316,
    1.4260870064517004,
    1.4313227294087836,
    1.471225032206286
  ],
  "token_counts": [
    449,
    442,
    457,
    469,
    405,
    411,
    478,
    424,
    432,
    434,
    418,
    576,
    471,
    536,
    433,
    435,
    431,
    402,
    393,
    417,
    463,
    400,
    456,
    424,
    387,
    443,
    422,
    396,
    359,
    580,
    438,
    467,
    431,
    435,
    409,
    435,
    418,
    448,
    403,
    426,
    581,
    423,
    449,
    432,
    432,
    425,
    368,
    421,
    434,
    418,
    424,
    385,
    435,
    408,
    359,
    468,
    462,
    400,
    583,
    461,
    409,
    488,
    417,
    454,
    485,
    445,
    429,
    436,
    411,
    551,
    480,
    483,
    477,
    458,
    399,
    435,
    428,
    419,
    435,
    425,
    421,
    410,
    448,
    435,
    409,
    421,
    399,
    735,
    426,
    464,
    434,
    424,
    480,
    439,
    400,
    432,
    435,
    405,
    538,
    444,
    467,
    422,
    442,
    385,
    385,
    442,
    434,
    384,
    362,
    341,
    376,
    405,
    400,
    363,
    381,
    368
  ],
  "response_lengths": [
    3661,
    2543,
    2751,
    2548,
    2482,
    2881,
    2571,
    2400,
    2579,
    2599,
    2413,
    2948,
    2664,
    2739,
    2374,
    2619,
    2245,
    2340,
    2616,
    2558,
    2124,
    2149,
    2023,
    2176,
    2412,
    2251,
    2171,
    2319,
    2165
  ]
}