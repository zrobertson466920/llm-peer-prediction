{
  "example_idx": 43,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSPENCNN: ORCHESTRATING AND SPARSITY FOR FAST HOMOMORPHICALLY ENCRYPTED NEURAL NETWORK INFERENCE\n\nENCODING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHomomorphic Encryption (HE) is a promising technology for protecting user’s data privacy for Machine Learning as a Service (MLaaS) on public clouds. However, the computation overheads associated with the HE operations, which can be orders of magnitude slower than their counterparts for plaintexts, can lead to extremely high latency in neural network inference, seriously hindering its application in practice. While extensive neural network optimization techniques have been proposed, such as sparsification and pruning for plaintext domain, they cannot address this problem effectively. In this paper, we propose an HE-based CNN inference framework, i.e., SpENCNN, that can effectively exploit the single-instruction-multiple-data (SIMD) feature of the HE scheme to improve the CNN inference latency. In particular, we first develop a HE-group convolution technique that can partition channels among different groups based on the data size and ciphertext size, and then encode them into the same ciphertext in an interleaved manner, so as to dramatically reduce the bottlenecked operations in HE convolution. We further develop a sub-block weight pruning technique that can reduce more costly HE-operations for CNN convolutions. Our experiment results show that the SpENCNN-optimized CNN models can achieve overall speedups of 8.37x, 12.11x, and 19.26x for LeNet, VGG-5, and HEFNet, respectively, with negligible accuracy loss.\n\n1\n\nINTRODUCTION\n\nFor the past decade, we have witnessed the tremendous progress of the machine-learning technology and the great success achieved in practical applications. Convolution Neural Network (CNN) models, for example, have been widely used for many cognitive tasks such as face recognition, medical imaging, and human action recognition. Meanwhile, there is a growing interest to deploy machine learning models on the cloud as a service (MLaaS). While cloud computing has been well recognized as an attractive solution, especially for computation intensive applications such as the MLaaS, outsourcing sensitive data and data processing on cloud can pose a severe threat to user’s privacy.\n\nHomomorphic Encryption (HE) is a promising technology for protecting user’s privacy when deploying MLaaS on cloud. HE allows computations be performed on encrypted inputs and the decrypted output matches the corresponding results computed from the original inputs. Thus, a client can encrypt the sensitive data locally and send the encrypted ciphertexts to the cloud. All intermediate results will maintain encrypted, and the encrypted results sent from cloud can be correctly decrypted using the secret key hold by the client. Whlie HE can help to maintain the confidentiality for computation process on cloud effectively, one major problem has to deal with is the excessive computational cost associated with the operations over the encrypted data: HE operations (e.g. HE multiplication, additions on encrypted data) can be several (i.e., three to seven) orders of magnitude slower than the corresponding operations on plaintexts. The tremendous computational cost of HE has been the largest bottleneck that hinders its applications on cloud.\n\nOne of the most effective approaches (e.g. (Gilad-Bachrach et al., 2016; Brutzkus et al., 2019; Dathathri et al., 2019; Kim et al., 2022)) to reduce the HE computational cost is to take advantage of the single-instruction-multiple-data (SIMD) capability, supported by HE schemes, e.g. CKKS and BFV. Smart & Vercauteren (2010) initially proposed to pack multiple data elements in the plaintext\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) Comparison of different HE-operations’ latency (b) Comparison of the HE convolution latency under different pruning methods. (c) Illustration for different pruning methods for plaintext domain. (d) Multi-channel convolution process in HE domain. Notation definitions refer to section 3.1. pt(ki) indicate the weight plaintext. The convolution layer used here has 64 input- and 64 output-channel, with a 3 × 3 kernel. The input feature map size of the convolution layer is 32 × 32.\n\ndomain to different “slots” in the same ciphertext and thus computations for data elements at the same slot of two encoded messages can be performed in parallel. The challenge is how to pack data based on the characteristics of the applications so that computation can be conducted effectively in a SIMD scheme manner. In particular, the problem rises when the computation needs to be performed on data elements at different slots of the messages. To re-arrange the location of each individual data element in an encrypted message is out of question due to its large overhead. A more reasonable solution is to employ the HE-rotation1operation that can move the data element cyclically in the same message. However, HE-rotation has a high latency cost due to required permutation and key-switching operation, compared with other HE-operations such as the HE multiplication of a ciphertext with a plaintext (HE-PMult) and HE addition of two ciphertexts (HE-Add), as shown in Figure 1 (a). Therefore, how to judiciously encode the inputs and perform the SIMD operations plays the key role in reducing the HE computation complexity.\n\nIn this paper, we study the problem on how to improve the HE-based inference latency when deploying a privacy-preserving machine learning (PPML) platform based on CNN models on cloud. It is well-known that the major computation workload for CNN inference comes from the convolution layers. Assuming the user inputs (e.g. images) are encrypted as the ciphertexts and associated CNN models are encoded as plaintext messages, the major HE computations are therefore HE-PMult, HE-Add, and HE-rotation operations. Traditional neural network optimization techniques such as sparsification and pruning (Han et al., 2015b; Wen et al., 2016) help to reduce the computation demand for CNN inference for plaintext domain. However, they may not be effective here as reducing the computation demand does not necessarily imply the reduction of SIMD computations. In addition, note that, as shown in Figure 1(a), the computation cost for an HE-rotation can be over 43× of that for an HE-Pmult or an HE-Add operation. Simply reducing the HE operations without optimizing the HE-rotations may not be effective at all in reducing the computational cost.\n\nTo this end, we develop an HE-based CNN inference framework, i.e., SpENCNN, with the goal to effectively exploit the SIMD feature of the HE scheme to improve the CNN inference latency. In particular, we develop two techniques to reduce the HE computational cost. First, we develop HE-group convolution and associated group-interleaved encoding to optimize channel locations on ciphertexts based on the number of convolutional groups and ciphertext size, thus significantly reducing the number of costly HE-rotations. Second, we further optimize the model architecture by pruning and training the weights in the sub-blocks iteratively with the goal to minimize HE-rotations and accuracy loss. We have conducted extensive experiments based on three CNN models on MNIST dataset and CIFAR-10 dataset and results show that the optimized CNN models can achieve overall speedups of 8.37x, 12.11x, and 19.26x for LeNet, VGG-5, and HEFNet, respectively, with negligible accuracy loss. To our best knowledge, this is the first work to that builds optimizing framework for CNN model architecture from the aspect of structural sparsity and data packing in HE to benefit HE-based PPML inference.\n\n1For\n\ninstance, Rot(ct, k) transforms an encryption of (v0, ..., vN/2−1) into an encryption of\n\n(vk, ..., vN/2−1, v0, ..., vk−1)\n\n2\n\nCh1Ch21.3575%0%Original Non-StructuralPruningSparsity (c) Different Pruning Methods for Plaintext domain50%50%Structural Pruning (Filter) Structural Pruning (Channel) ABDTypeCLatency(ms)HE-PmultBCDLatency (ms)HE-rotationHE-Pmult & Add (b)48591ABaselineLatency(ms)HE-rotationHE-Add631.5(a)--------------------00000000000000000000000000000000 Inner-rotationsCh1Ch2Ch2Ch1Out1Out2=+Outer-rotations(d) Convolution in HE domainCh2Ch1Out1Out2Under review as a conference paper at ICLR 2023\n\n2 PRELIMINARIES\n\n2.1 CKKS HOMOMORPHIC ENCRYPTION\n\nHomomorphic Encryption (HE) allows computations to be performed on encrypted data without decryption. Among various HE schemes, the levelled HE– Cheon-Kim-Kim-Song (CKKS) (Cheon et al., 2017) is widely adopted in the encrypted neural network inference because of supporting the fixed-point real number arithmetic and potentially avoiding the prohibitively expensive bootstrapping. The CKKS-based HE operations mainly consist of ciphertext addition HE-Add (ct1 + ct2), ciphertext multiplication HE-Cmult (ct1 × ct2), scalar multiplication HE-Pmult (pt1 × ct2), ciphertext roation HE-rotation Rot(ct, k), etc. For MLaaS that only encrypts clients’ data, HE-Add, HE-Pmult and HErotation often dominate the computations of an encrypted inference. Among these three operations, HE-rotation costs much longer latency than the other two, e.g. ∼ 43× as our profiling result in Figure 1 (a) shows, due to the complex automorphism operation and a key-switching operation. The detailed calculation process of HE-rotation can be described as:\n\nRot(ct, k) = (c(X ik), 0) + P −1(a(X ik) · evkk\n\nrot)\n\n(1)\n\nwhere the evaluation key (evkk rot) is a public key with a larger modulus P Q, and P is greater than Q. Assume ct = (c(X i), a(X i)) represents a ciphertext before rotation, then the automorphism (c(X ik) and a(X ik)) maps each polynomial coefficient index i to output polynomial coefficient index ik mod N , where N is the polynomial degree. The second term on the right side of Equation 1 represents the key-switching operation to ensure the final ciphertext can be still decrypted by the same secret key. It is very expensive and could take over 90% of all operations in practice (Samardzic et al., 2022)\n\n2.2 THREAT MODEL\n\nWe assume the cloud-based machine learning service, of which a trained convolutional neural network (CNN) model with plaintext weights, is hosted in a cloud server. A client could upload his/her private and sensitive data to the public cloud for obtaining an online inference service. The cloud server is semi-honest (e.g. honest but curious). To ensue the confidentiality of clients’ data against such a cloud server, the client utilizes HE to encrypt the data and then send it to cloud for performing encrypted inference without decrypting the data or accessing the private key. Finally the client can decrypt the returned encrypted inference results from cloud using a private key. In this work, we focus on encrypting the client’s data and others like model parameters, are assumed as plaintext.\n\n2.3 MOTIVATION EXAMPLE\n\nTo identify the computation bottleneck in HE inference, we analyze the computation pattern of the convolutional layer, which often dominates CNN inference’s memory and computational overheads, in the encryption process. Here the input and output activation feature maps are encrypted as ciphertext, while the convolutional kernels are assumed as plaintext. We also assume the state-of-the-art ciphertext encoding–row-major (Dathathri et al., 2019; Kim et al., 2022) is adopted here. This allows efficient multi-channel ciphertext packing to take advantage of CPU’s single-instruction-multiple-data (SIMD) architecture for fast HE inference. Figure 1 (d) shows the typical HE convolution process of a convolution layer which consists of 2-input/output channels with 3 × 3 kernels. To compute a ciphertext output feature map, two types of cipertext rotations need to be performed sequentially. First, inner-rotation rotates each input channel’s ciphertext feature map 8 times (or K 2 − 1, here kernel size K = 3). Each rotated version will need to be multiplied with its corresponding weight plaintext, and then such results will be summed up to obtain an intermediate ciphertext from each input channel, which will further be concatenated as a whole ciphertext (e.g. Ch1 and Ch2 as ct1). Second, outer-rotation rotates the concatenated ciphertext multiple times (in this simple example, 1 time because of packing 2 output channels as a ciphertext). Finally all ciphertext output feature maps can be obtained in parallel by the summation of these rotated copies. Apparently, compared to non-encrypted convolution, HE convolution significantly escalates the memory and computation overheads. Moreover, since the latency of HE-rotation can be much higher than other operations due to complex automorphism and key switching operations (see our profiling result in Figure 1(a) 63ms for HE-Rot v.s. 1.5ms for HE-Pmult, detailed setting in Sec. 4.1), and the multi-channel convolutions in deep CNNs would involve a huge volume of HE-rotation 2. As a result, the long-latency HE-rotation quickly becomes a bottleneck of the encrypted inference.\n\n2The matrix-vector multiplications in fully-connected layers also require a substantial amount of HE-rotation.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The overall flow of SpENCNN framework for optimizing HE-based CNN inference, mainly consists of two orthogonal techniques to generate a tailored CNN model: (1) HE-group Convolution (outer-rotation optimization), and (2) the Sub-block Pruning (inner-rotation optimization).\n\nOne straightforward solution to accelerating HE inference is to reduce the number of rotations through zeroing out (or pruning) the plaintext weights. As Figure 1(c) shows, if any weight plaintext–pt(ki) contains all zero values, then the corresponding ciphertext rotation–Rot(ct1, k) and its associated multiplication and summation can be safely eliminated. Since existing pruning techniques have been proved to be effective in reducing the computation and memory overhead to speedup the nonencrypted inference without accuracy drop, we apply two representative pruning methods–nonstructured pruning(Han et al., 2015a) (zeros appear randomly in a kernel, see Fig. 1 (c)–B, 75% sparsity) and structured pruning(Wen et al., 2016) (structured zeros in a kernel, see filter pruning and channel pruning in Fig. 1 (c)–C and D, 50% sparsity). For HE operations in an example convolutional layer with 64 input/output channels, feature map size 32 × 32 and kernel size 3 × 3, as Fig. 1 (d) shows, the existing pruning achieves very marginal or even no reduction of the HE-rotation latency which dominates the convolution computation. In the worst case, it even cannot remove any HE-rotation despite the high model sparsity, e.g. the non-structured pruning with 75% sparsity ratio. The underlying reason is two-fold: 1) pruning is unable to address the outer-rotation since computing an output ciphertext by convolution needs to sum all channels’ feature maps belonging to the same ciphertext if using the state-of-the-art ciphertext encoding (see Fig. 3 (a)); 2) existing pruning techniques are designed for non-encryted inference, and the special channel-wise ciphertext operations involved in HE convolution are ignored. This prompts the need of jointly optimizing ciphertext encoding and encryption-aware model sparsity to accelerate HE inference.\n\n3 THE SPENCNN FRAMEWORK\n\nIn this section, we present the technical details of our proposed SpENCNN framework. Figure 2 depicts its overall flow. The SpENCNN framework takes an initial CNN model as input and outputs a tailored CNN model after two processing stages–(1) HE-group Convolution is designed to reduce the outer-rotations caused by multi-channel convolution. In particular, we design an adjustable method and determine a theoretically optimal group number Gbase based on the size of the ciphertext and data packed in the CKKS HE scheme, to ensure that all outer-rotations can be eliminated while keeping model accuracy. (2) Sub-block Pruning is further proposed to reduce the number of inner-rotations. However, this is not trivial. We observe that to reduce as many inner-rotations as possible, we must precisely identify and completely prune selected sub-blocks. This, unfortunately, would result in an considerable accuracy drop. To address this issue, we develop a set of sub-steps which include identifying subblocks, pruning the subblocks, updating the remaining subblocks, and retraining the model for accuracy recovery.\n\n3.1 HE-GROUP CONVOLUTION\n\nTwo intuitions. Our proposed HE-group convolution is based on two intuitions. We observe that the number of required outer-rotations for a ciphertext is Rotouter = N/2×(h×w)pad − 1, where N is the polynomial degree defined in the cryptographic parameters, h and w are the height and width of the input feature map, and pad rounds a number to the next power of two. Each ciphertext generated by the outer-rotation further requires a set of inner-rotations. The number of inner-rotations is Rotinner = K 2 − 1, where k is the convolutional kernel size. Apparently, increasing the number of outer-rotation–Rotouter by just 1 can bring an extra Rotinner = K 2 − 1 inner-rotations. This gives us the first intuition–reducing the number of outer-rotations will fundamentally reduce the computational overhead.\n\n4\n\nInitial CNNmodelGroupConvolution Substitution Identify all sub-blocks in the modelPrune the sub-block with leastweights in restRetrain themodel to recoverthe accuracySub-block PruningHE-group ConvolutionTailoredCNN modelRepeatUpdate rest validsub-blocksUnder review as a conference paper at ICLR 2023\n\nFigure 3: (a) an example of HE convolution which generates 4 output channels from the 4 input channels with 3 × 3 kernels. By HE-group convolution, we only need 2 group-interleaved encoded cts multiply with corresponding pts and sum up to get the output channels data. (b) proposed HE-group convolution by a group-interleaved format.\n\nThe reason behind the outer-rotation is that multiple channels on the same ciphertext are involved in the same multichannel convolution. In our study, we find that convolution (e.g., depthwise convolution (Howard et al., 2017)) can be also performed individually within each single channel. We also find that the group convolution technique (Krizhevsky et al., 2012; Zhang et al., 2018; Ioannou et al., 2017) can reduce the number of channels involved in each group. This gives us the second intuition–reducing the number of channels in the same group can eliminate the outer-rotation.\n\nBased on these intuitions, we design the HE-group convolution. Given the group number G, we have the upper bound of the number of channels in the same group as ⌈(N/2·(h·w)pad) × 1/G⌉. Then the relationship between Rotouter and G can be expressed as:\n\nRotouter = ⌈(N/2·(h·w)pad) × 1/G⌉ − 1 (2) Accordingly, we can use an appropriate G value to cancel the first term, i.e., the Gbase = N/2·(h·w)pad in our design. Theoretically, this optimized value indicates zero outer-rotations.\n\nGroup-interleaved encoding. However, we find that the traditional ciphertext encoding format is not compatible when we implement our grouping idea. This is because the row-major format is mainly designed to perform convolution in the SIMD manner without considering the channel positions on the ciphertext. To address this issue, we propose a group-interleaved encoding format–the channel data from different groups are placed on the same ciphertext in an interleaved manner. This new encoding facilitates fast HE-group convolution without involving any outer-rotation.\n\nFigure 3 shows an example of proposed HE-group convolution and group-interleaved encoding. We assume that two ciphertexts contain 4 channels of data, and each ciphertext cti contains 2 channels. As shown in Figure 3 (a)–left, in general HE convolution, each cti has to do 1 outer-rotation to cover the 2 different channels, i.e., {ch1, ch2} and {ch2, ch1} for ct1. Convolution will be performed individually on each outer-rotated case for all ciphertexts. The encrypted output channels {out1, out2}, {out3, out4} can be generated after a summation.\n\nFor our HE group convolution, as Figure 3 (a)–right shows, sibling channels {ch1, ch2} and {ch3, ch4} from the same cti are in different convolution groups, which are encoded by our groupinterleaved format, i.e., {ch1, ch3} in ct1 and {ch2, ch4} in ct2. Now, the outer rotation is eliminated because each cti can perform 2 groups of convolution individually without rotating the channels. The encrypted output channels after summation, i.e., {out1, out3}, {out2, out4}, are naturally groupinterleaved and can immediately send to the next HE-group convolution. Figure 3 (b) further shows the generalized group-interleaved encoding, in which a ciphertext can encrypt M channels using the adjustable convolution group number G, with constraints G ≤ M and M %G = 0.\n\n3.2 SUB-BLOCK PRUNING\n\nWe design the sub-block pruning to further remove the remaining inner-rotations after the HE-group convolution. Our idea is to prune (zero out) a whole set of weights corresponding to specific innerrotations, so that the computational overhead of these inner-rotations can be eliminated. This reminds us of the weight sparsity in CNN models. In HE-convolution, an inner-rotated ciphertext will be\n\n5\n\nCh2Ch1out1out2out3out4out1out3out2out4Ch1Ch2Ch3Ch4Ch2Ch1Ch4Ch3HE-rotationCh1Ch3Ch2Ch4Ch1Ch2Ch3Ch4Group 1Group 2HE-group ConvolutionGeneral HE ConvolutionCh2Ch4Ch1Ch3Ch4Ch3ChChChCh ChChChChChGroup 1Group GGroup 2GGMGGroup-Interleaved Format(a)(b)HE-rotationUnder review as a conference paper at ICLR 2023\n\nFigure 4: (a) The weight sparse pattern in convolutional layers. For the same ct, their weight sparse patterns must be same. For different ct, the weight sparse pattern may change. (b) The weight sparse pattern in FC layers is in a diagonal-wise shape.\n\nmultiplied with the weights at the same position, namely “sub-block”, from all relevant kernels. Therefore, our design tends to cut out the same sparse pattern on these sub-blocks for all relevant kernels of the same ciphertext.\n\nAs the example in Figure 4 (a) shows, the 4 kernels of ct1 shares the same sparse pattern, thus eliminating 6 inner-rotations. This pruning scheme can be also extended to the FC layer. As shown in Figure 4 (b), the multiplication of ciphertext on FC layer is equivalent to the convolution using a diagonal-wise encoding method (Halevi & Shoup, 2014). Multiplying with the weights in one diagonal line requires one HE-rotated copy of ciphertext. (see Appendix A.3 for more details.)\n\nTo obtain the desired sparse pattern, we propose the sub-block pruning. The more sub-blocks pi being pruned, the less inner-rotations needed. This is actually an optimization problem, in which, we need to minimize the total number of sub-blocks while maintaining the prediction accuracy concurrently:\n\nmin{P =\n\n(cid:88)\n\npi · Ii} s.t. Acc(f (x; (W, P ))) ≥ Acc(f (x; W )) where Ii =\n\ni\n\nWe would like to find the sub-block with the minimum weight importance to the model at each iteration and prune, then, retrain the model for a few epochs to recover the accuracy. However, the sizes of sub-blocks are different. To measure the weight importance of each sub-block in a fair way. ||wpi || we define the weight importance metric as an average L2 norm dim(wpi ) . As described in algorithm 1, at each iteration, we would like to prune the sub-block with the least weight importance. The iterative algorithm would stop when the model accuracy is lower than the initial accuracy.\n\n(cid:26)0 pi is pruned other\n\n1\n\n(3)\n\n4 EVALUATION\n\n4.1 EXPERIMENT SETUP\n\nSetup. We conduct our experiments on a workstation equipped with an AMD Ryzen Threadripper 3975WX CPU, an NVIDIA RTX 3090 GPU, and 256GB of RAM. To evaluate our proposed SpENCNN, we select three baseline CNN models that are often adopted in HE inference performance evaluation, and implement them using PyTorch on GPU. This includes LeNet-like for MNIST\n\nModel\n\nLeNet-like VGG-5 HEFNet\n\nModel\n\nLeNet-like VGG-5 HEFNet\n\n# Layers FC 2\n3 1\n\nConv 2\n3 4\n\nAct 3\n5 4\n\nGroups Accuracy (Gbase) 4\n8 8\n\nEncryption Parameters Mult Level 10 16 13\n\nN 8192 16384 16384\n\nP 264 529 436\n\nQ 24 31 31\n\n(%) 98.95 84.06 83.67 Security Level >128 bit > 80 bit >128 bit\n\nTable 1: Three baseline Convolutional Neural Networks models and corresponding Encryption Parameters. LeNet-like is for the MNIST dataset. VGG-5 and HEFNet are for CIFAR-10 dataset.\n\n6\n\nAlgorithm 1 Sub-blocks Iterative Pruning\n\n1: Input: CNN model:f (x; (W, P )), 2: Remark: x-Data, W-Weights, P-HE blocks 3: Output: HE-friendly model:f (x; (W ′, P ′)) 4: P ′ = P = (cid:80) 5: While Accuracy loss ≤ 0 :\n\ni pi · Ii\n\n6:\n\ni ← argmin\n\n||wpi || dim(wpi )\n\ni\n\nIi = 0 prune weights in pi from current P update P ′ retrain model with P ′ and update W ′\n\n7: 8: 9: 10: 11: end While 12: Return f (x; (W ′, P ′))\n\n--------sub-blocksub-block(a) Weight sparsity in convolutional layers(b) Weight sparsity in FC layersUnder review as a conference paper at ICLR 2023\n\ndataset (2016), VGG-5 (Rathi et al., 2020) and the HE-friendly Net (HEFNet) for CIFAR10 dataset (Details of layer size is in Appendix A.2). Since the non-linear activation function like ReLU cannot be evaluated in HE, we replace ReLU with the adaptive quadratic polynomial function f (x) = ax2 + bx + c following the related works (Dathathri et al., 2019; Kim et al., 2022), where a, b, c are trainable parameters to maintain the model accuracy. Table 1 lists the specifications of these three models and the corresponding accuracy. In particular, the test accuracy for LeNet-like-MINST, VGG-5-CIFAR10, HEFNET-CIFAR10, is 98.95%, 84.06%. and 83.67%, respectively, which are consistent with their original versions.\n\nWe use Microsoft SEAL library v3.4.5 (SEAL) to implement the RNS-CKKS HE computation on these networks. Table 1 also lists the key parameters used in our RNS-CKKS encryption, including the polynomial degree N , the total modulus in bit-length Q, the scale factor in bit-length P to maintain the HE evaluation accuracy, and total multiplication level. These parameters can guarantee a security level of 80 bit for VGG-5, 128 bit for LeNet-like and HEFNet.\n\nMethodology. We first perform an ablation study to evaluate each individual technique’s effectiveness, and then compare the whole SpENCNN framework with the state-of-the-art method. y We adopt the average inference latency (in seconds) as the main measurement. An image set containing 20 different samples is used to measure and report the latency for these models. A lower latency indicates better performance. In addition, we measure the left holomorphic operation count (HOC, in %), sparsity (in %), and accuracy (in %) on the tailored models. The lower HOC and lower sparsity while offering higher accuracy are desired on all models.\n\n4.2 RESULTS\n\n4.2.1 EVALUATON ON HE-GROUP CONVOLUTION\n\nTable 2 lists our evaluation results for the HE-group convolution. We apply the HE group convolution alone (in ablation) to each baseline model and evaluate its effectiveness and scalability. In particular, we adjust the number of groups from its default (i.e., 1-baseline) until it exceed its Gbase according to our design (i.e., the highlighted 4, 8, and 8 for LeNet-like, VGG-5, and HEFNet, respectively). For detailed analysis, we breakdown HOC into “Rot” (HE-rotation) and “Others” (other operations including HE-Pmult and HE-add).\n\nOur HE-group convolution can be scaled to any convolutional model. As the number of groups increases, it can effectively reduce the number of HOC and maintain the accuracy, thus reducing the latency of HE inference and improving the performance. As listed in Table 2, the number of HE-rotation is reduced from 100% to 27.27%, 85.45%, and 11.95% on LeNet-like, VGG-5, and HEFNet, respectively. Once the Gbase is reached, the number of HE-rotation does not decrease further in spite of increasing the number of groups. This is because the outer-rotation is completely eliminated in HE-group convolution after the group number reaches Gbase. We also find that HE group convolution can reduce other HOC such as HE-Pmult and HE-add even after exceeding Gbase. This also contributes to the performance improvement.\n\nFor example, the HE-group convolution is particularly effective on our HEFNet (i.e., ∼ 88% and ∼ 86% reduction for HE-rotation and others, respectively). This is because it has the largest volume of convolution layers among the three models. Such a dramatic reduction in HOC further shortens the\n\nModel\n\nGroups\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\n1-baseline 2\n4 8\n1-baseline 4\n8 16 1-baseline 4\n8 16\n\nHOC Left (%) Others Rot -\n- 52.91 51.52 28.24 27.27 16.47 27.27 -\n- 84.08 87.53 81.42 85.45 80.10 85.45 -\n- 25.74 24.53 13.36 11.95 7.18 11.95\n\nAccuracy (%)\n\nLatency (s)\n\nSpeedup (×)\n\n98.95 98.95 98.95 98.67 85.16 84.53 84.06 82.23 84.91 84.35 83.67 80.06\n\n1.2658 0.6806 0.3807 0.3044 53.909 46.539 45.311 45.053 24.113 6.2491 3.2718 2.3627\n\n- 1.86 3.32 4.16 -\n1.16 1.19 1.20 -\n3.86 7.37 10.21\n\nTable 2: Ablation study of HE-group convolution with different number of convolution groups.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ninference latency from 24.11s to 3.27s, which represents a 7.37× speedup. In contrast, the HE group convolution is the least effective in VGG-5, as it contains three large FC layers (size of 8192×4096), which cannot be substantially optimized using HE group convolution alone. There are more than 85% of HE-rotations and 80% of other operations that cannot be eliminated. And this number saturates as a lower bound after reaching the Gbase, resulting in a limited speedup of 1.19×.\n\nWe also observe that the model accuracy slightly decreases as the number of groups increases. This is due to the fact that fewer channels are involved in the HE-group convolution compared to the general convolution (see Figure 3). Fortunately, as long as the number of groups does not exceed our suggested Gbase, the loss of accuracy is marginal (i.e., 0%, 1.1%, and 1.2% on LeNet-like, VGG-5, and HEFNet, respectively). Also, our design is adjustable, allowing a trade-off between the accuracy and the optimized number of convolution groups.\n\n4.2.2 EVALUATION ON SUB-BLOCK PRUNING\n\nHere, we apply the sub-block pruning alone (in ablation) and compare it with other pruning methods such as Non-structural prune (Han et al., 2015a), and Structural-prune (Wen et al., 2016). Table 3 lists our evaluation results. We do not include accuracy in this evaluation since we prune each baseline model using each pruning method under the constraint of maintaining the original model accuracy. Instead, we include the sparsity (i.e., the percentage of pruned weights) for comparison.\n\nOur sub-block pruning can effectively improve the HE inference performance on all baseline models (i.e., the speedup of 2.62×, 6.15×, and 2.57× on LeNet-like, VGG-5, and HEFNet, respectively), which significantly outperforms other traditional pruning methods (i.e., marginal ∼ 1.1× speedup on most cases). The reason is obvious but significant. Our design is more HE-oriented and effective in the ciphertext domain while traditional pruning is more sparsity-oriented and for the plaintext computation efficiency only. For example, although NS-prune can prune ∼ 92% of the weights on VGG-5, it cannot eliminate the HE overhead (i.e., ∼ 96% HOC) caused by the remaining ∼ 8% of the weights.\n\nOur sub-block pruning method performs the best (i.e., ∼ 16% HOC) on VGG-5 because it effectively eliminates the inner-rotations caused by the large number of redundant weights in the FC layers. Together with the previous results (see Table 2), sub-block pruning can be a good complement to the HE-group convolution that performs weakly on the FC layers. We also note that our sub-block pruning method on LeNet-like (i.e., 35.21% Rot left) slightly outperforms HEFNet (i.e., 41.88% Rot left). This is because the larger convolutional kernel (i.e., 5 × 5) in LeNet-like gives our pruning method more space to optimize the inner-rotations.\n\n4.2.3 COMPARE WITH THE STATE-OF-THE-ART\n\nWe compare our method with the state-of-the-art HE-prune method–Hunter (Cai et al., 2022). The comparison results are presented in Table 4. In this evaluation, our method combines the HE-group convolution and the sub-block pruning, and uses the proposed Gbase as the group number (i.e., highlighted data). For a fair comparison, pruning is well controlled to ensure that our method and Hunter have the same level of accuracy (i.e., error≤ ±0.04%) on all baseline models. We can see from Table 4, our method outperforms the state-of-the-art significantly in terms of HOC, sparsity, and\n\nNetwork\n\nGroups\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\nDense-Baseline NS-prune S-prune (channel) Sub-block prune Dense-Baseline NS-prune S-prune (channel) Sub-block prune Dense-Baseline NS-prune S-prune (channel) Sub-block prune\n\nHOC Left (%) Others Rot -\n- 96.23 96.12 92.82 88.03 34.07 35.21 -\n- 97.14 97.59 98.08 98.47 16.11 15.89 -\n- 88.97 85.60 95.24 94.69 36.11 41.88\n\nSparsity (%)\n\nLatency (s)\n\nSpeedup (×)\n\n0.00 91.00 53.77 63.83 0.00 91.88 90.48 89.87 0.00 72.95 51.91 63.90\n\n1.2658 1.2190 1.1202 0.4644 53.909 52.5280 50.7178 8.7659 24.113 21.1660 22.9240 9.3709\n\n- 1.04 1.13 2.62 -\n1.03 1.06 6.15 -\n1.14 1.05 2.57\n\nTable 3: Ablation study of sub-block prune and comparison with other pruning methods.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nNetwork\n\nMethod\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\nBaseline Hunter Ours-4 Baseline Hunter Ours-8 Baseline Hunter Ours-8\n\nHOC Left (%) Others Rot -\n- 39.91 40.95 9.88 8.54 -\n- 18.93 17.86 7.72 7.86 -\n- 42.20 48.27 4.61 3.99\n\nSparsity Accuracy\n\n(%) 0\n59.99 62.62 0\n89.81 91.97 0\n57.82 65.62\n\n(%) 98.95 98.95 98.95 85.16 84.03 84.07 84.91 83.63 83.67\n\nLatency (s) 1.2658 0.5353 0.1535 53.909 9.9916 4.3830 24.113 10.855 1.2520\n\nSpeedup (×) -\n2.36 8.37 -\n5.40 12.11 -\n2.22 19.26\n\nTable 4: Comparison with Hunter on model HOC left, sparsity, accuracy,latency ,and speedup.\n\nlatency, on all baseline models. For example, our method eliminates 96% HE-rotations on HEFNet, achieving a 19.26× speedup. In contrast, the Hunter-optimized model still has 51% HE-rotations left behind and achieves only 2.22× speedup compared to the un-pruned baseline. This is because our method is designed to eliminate both outer and inner HE-rotations by synthetically applying the HE-group convolution and sub-block pruning, while the state-of-the-art is solely built upon the fixed structure pruning. We also print out the sparse pattern in a convolutional layer of LeNet-like, included in Appendix A.1.\n\n5 RELATED WORK\n\nCryptoNets (Gilad-Bachrach et al., 2016) is an initial attempt to realize HE-inference. After that, many subsequent works are proposed to improve the HE-inference latency from different aspects. Faster-CryptoNets (Chou et al., 2018) combines weight pruning and quantization to obtain a sparse polynomial representation to speed up the PMult operation, which achieves 6.38× latency reduction. LoLa (Brutzkus et al., 2019) successfully demonstrates the HE inference on a simple 3-layer model (1 convolutional layer and 2 FC layers) and achieves a 2.2s inference latency on the MNIST sample by leveraging HE schemes, data encoding format, and rotation techniques. CHET and HEAR (Dathathri et al., 2019; Kim et al., 2022) further refine the row-major coding format to achieve the same level of inference latency, but on a larger network (3 convolutional layers and minimum 64 channels).\n\nLou & Jiang (2021) propose a neural architecture search (NAS) based method to reduce the encryption parameters and speed up the HE-inference. Further, Ghodsi et al. (2020); Jha et al. (2021); Mishra et al. (2020); Lou et al. (2020) propose to reduce the cost of non-linear operations in NAS based HE-inference since operations like ReLU dominate the latency in the multi-party computation (MPC) setting. HE-PEx (Aharoni et al., 2022) and Hunter (Cai et al., 2022) attempt to structurally prune the weights to accelerate the HE-inference. In Hunter, a structural pruning method is proposed to facilitate HE in the MPC setting. HE-PEx adpots Hunter’s method to prune the weights in the FC layer only. It can reduce memory requirement and latency by 60% on the tested autoencoder models. Our work–SpENCNN is the first to orchestrate the ciphertext encoding and model sparsity design for HE inference acceleration, significantly outperforming these works.\n\n6 CONCLUSION\n\nIn this paper, we propose a fast LHE-based encrypted inference framework-SpENCNN built upon two novel techniques–HE-group convolution and sub-block weight pruning. Experimental results show that our solution can speed up the privacy-preserving inference by 8.37×, 12.11×, and 19.26× on LeNet-like, VGG-5, and HEFNet, respectively, greatly outperforming the state-of-the-art solutions. In the future, we would like to extend our work to deeper models and complex classification tasks in a no-client-interaction setting by leveraging bootstrapping. We hope to apply our framework to these deep models. There shall exist potential optimizations for the trade-off between model sparsification, data encoding, bootstrapping, and cryptographic parameters.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nTensorFlow 2016.\n\nLenet-like for convolutional mnist model example.\n\nhttps:\n\n//github.com/tensorflow/models/blob/v1.9.0/tutorials/image/ mnist/convolutional.py.\n\nEhud Aharoni, Moran Baruch, Pradip Bose, Alper Buyuktosunoglu, Nir Drucker, Subhankar Pal, Tomer Pelleg, Kanthi Sarpatwar, Hayim Shaul, Omri Soceanu, et al. He-pex: Efficient machine learning under homomorphic encryption using pruning, permutation and expansion. arXiv preprint arXiv:2207.03384, 2022.\n\nAlon Brutzkus, Ran Gilad-Bachrach, and Oren Elisha. Low latency privacy preserving inference. In\n\nInternational Conference on Machine Learning, pp. 812–821. PMLR, 2019.\n\nYifei Cai, Qiao Zhang, Rui Ning, Chunsheng Xin, and Hongyi Wu. Hunter: He-friendly structured pruning for efficient privacy-preserving deep learning. In Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security, pp. 931–945, 2022.\n\nJung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. Homomorphic encryption for arithmetic of approximate numbers. In International conference on the theory and application of cryptology and information security, pp. 409–437. Springer, 2017.\n\nEdward Chou, Josh Beal, Daniel Levy, Serena Yeung, Albert Haque, and Li Fei-Fei. Faster cryptonets: Leveraging sparsity for real-world encrypted inference. arXiv preprint arXiv:1811.09953, 2018.\n\nRoshan Dathathri, Olli Saarikivi, Hao Chen, Kim Laine, Kristin Lauter, Saeed Maleki, Madanlal Musuvathi, and Todd Mytkowicz. Chet: an optimizing compiler for fully-homomorphic neuralnetwork inferencing. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 142–156, 2019.\n\nZahra Ghodsi, Akshaj Kumar Veldanda, Brandon Reagen, and Siddharth Garg. Cryptonas: Private inference on a relu budget. Advances in Neural Information Processing Systems, 33:16961–16971, 2020.\n\nRan Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In International conference on machine learning, pp. 201–210. PMLR, 2016.\n\nShai Halevi and Victor Shoup. Algorithms in helib. In Annual Cryptology Conference, pp. 554–571.\n\nSpringer, 2014.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015b.\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nYani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi. Deep roots: Improving cnn efficiency with hierarchical filter groups. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1231–1240, 2017.\n\nNandan Kumar Jha, Zahra Ghodsi, Siddharth Garg, and Brandon Reagen. Deepreduce: Relu reduction for fast private inference. In International Conference on Machine Learning, pp. 4839–4849. PMLR, 2021.\n\nMiran Kim, Xiaoqian Jiang, Kristin Lauter, Elkhan Ismayilzada, and Shayan Shams. Secure human action recognition by encrypted neural network inference. Nature communications, 13(1):1–13, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nA Krizhevsky, I Sutskever, and GE Hinton. Imagenet classification with deep convolutional neural networks. 2012 advances in neural information processing systems (nips). Neural Information Processing Systems Foundation, La Jolla, CA, 2012.\n\nQian Lou and Lei Jiang. Hemet: A homomorphic-encryption-friendly privacy-preserving mobile neural network architecture. In International conference on machine learning, pp. 7102–7110. PMLR, 2021.\n\nQian Lou, Song Bian, and Lei Jiang. Autoprivacy: Automated layer-wise parameter selection for secure neural network inference. Advances in Neural Information Processing Systems, 33: 8638–8647, 2020.\n\nPratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa. Delphi: A cryptographic inference service for neural networks. In 29th USENIX Security Symposium (USENIX Security 20), pp. 2505–2522, 2020.\n\nNitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy. Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=B1xSperKvH.\n\nNikola Samardzic, Axel Feldmann, Aleksandar Krastev, Nathan Manohar, Nicholas Genise, Srinivas Devadas, Karim Eldefrawy, Chris Peikert, and Daniel Sanchez. Craterlake: a hardware accelerator for efficient unbounded computation on encrypted data. In ISCA, pp. 173–187, 2022.\n\nSEAL. Microsoft SEAL (release 3.4). https://github.com/Microsoft/SEAL, October\n\n2019. Microsoft Research, Redmond, WA.\n\nNigel P Smart and Frederik Vercauteren. Fully homomorphic encryption with relatively small key and ciphertext sizes. In International Workshop on Public Key Cryptography, pp. 420–443. Springer, 2010.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\n\ndeep neural networks. Advances in neural information processing systems, 29, 2016.\n\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6848–6856, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 SPARSE PATTERNS\n\nWe present the weights after optimized in binary representation. In each group, it contains 64 weight kernels with size 5 × 5. Kernels in same group indicate that they are associated with the same ciphertext. Within the same group, across different ciphertexts, the sparse patterns are different.\n\nFigure 5: The sparse patterns for weight kernels in LeNet-like 2nd Convolutional layer.\n\nA.2 BASELINE MODEL ARCHITECTURE IN DETAIL\n\nThe following Table 5 contain the detailed convolutional kernels size, weight matrix size and number of channels. These three baseline models has different property. The LeNet-like model is a tiny model designed for simple classification task so that the channel number and weight matrix size is small, and has the least number of layers. The VGG-5 model has much more weights in FC layers and the max number of layers. The HEFNet contain the most convolutional layers and the widest channel size.\n\nNetwork\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\nLayer Conv1 Conv2 FC1 FC2 Conv1 Conv2 Conv3 FC1 FC2 FC3 Conv1 Conv2 Conv3 Conv4 FC1\n\n# Input channel 1\n32 64 32 3\n64 128 8192 4096 4096 3\n64 128 256 1024\n\n# Output channel Kernel size (Matrix size in FC)\n\n32 64 32 10 64 128 128 4096 4096 10 64 128 256 256 10\n\n5×5 5×5 64×32 32×10 3×3 3×3 3×3 8192×4096 4096×4096 4096×10 3×3 3×3 3×3 3×3 1024×10\n\nTable 5: Convolutional layer and Fully-connected layer size in three baseline models.\n\nA.3 MATRIX MULTIPLICATION IN FC LAYER\n\nHalevi & Shoup (2014) proposed a diagonal-wise multiplication of ciphertexts on FC layers. Given a N-element ciphertext ct = {x1..N }, the weights of FC layer can be reshaped into a M × N × N tensor { ⃗W } (will pad with zero if needed). For each N × N matrix W , we can multiply the diagonal N elements (as plaintext) with the rotated copies of ciphertext. As the example shown in Figure 6, we have ct = {x1..4} and a 4 × 4 matrix W . We have pt = {a11, a22, a33, a44},\n\n12\n\nGroup 1Group 2Under review as a conference paper at ICLR 2023\n\npt1 = {a41, a12, a23, a34}, pt2 = {a31, a42, a13, a24}, and pt3 = {a21, a32, a43, a14}. They will be multiplied by ct = {x1, x2, x3, x4}, Rot(ct, 1) = {x2, x3, x4, x1}, Rot(ct, 2) = {x3, x4, x1, x2}, and Rot(ct, 3) = {x4, x1, x2, x3}, respectively and then summed.\n\nFigure 6: Ciphertext multiply with a matrix.\n\n13\n\nMatrix Wct copies",
  "translations": [
    "# Summary Of The Paper\n\nThe paper seeks to improve the computational efficiency of convolution layers in FHE. Since homomorphic rotations are the primary computational bottleneck of convolutions in FHE, the paper seeks to reduce the number of rotations. This is achieved in two steps, 1) adopting group convolutions, which reduces out-level rotations, and 2) weight pruning, which reduces inner-level rotations.\n\nThe efficiency of the proposed convolution is evaluated on *shallow* CNNs designed for MNIST and CIFAR-10. The proposed approach shows appreciable speed-up over *naive* implementations of convolutions in FHE.\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper rightly identifies the main bottleneck of *naive* implementations of convolution in FHE, namely rotations. As such, efforts to improve efficiency of convolution is necessary. Leveraging alternative convolutions, such as group convolutions or in the extreme depth-wise convolutions is interesting.\n- Adopting pruning for sparsifying the convolution and optimizing the sparsity pattern for reducing number of homomorphic rotations.\n\nWeaknesses:\n- The main drawback of the paper is the lack of comparisons to prior work that improve efficiency of convolutions. These include multiplexed convolutions [1], mobile networks explored HEMET [2].\n- The main premise of the paper is that rotations in convolutional layers are the main computational bottleneck of networks in FHE. So the paper considers shallow networks only, which are not likely to be practically useful. For instance 85% accuracy on CIFAR-10 is quite poor by the standards of the best plaintext models which achieve ~99% accuracy.\n- As networks become deeper, the main accuracy bottleneck is low-degree polynomial approximations of non-linear functions like ReLU and the main computational bottleneck is the bootstrapping operations required for evaluating high-multiplicative depth circuits. So improving efficiency of convolutional layers does not benefit deeper networks since convolution is not the main bottleneck for such networks.\n\nOther Clarification Questions:\n- The paper does not mention how pooling operations or strided convolutions are handled. Strided convolutions result in wasted slots. How  does that affect the proposed convolutions?\n\n[1] HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021\n[2] Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The paper is clear for the most part. The figures are a bit challenging to understand, but becomes clear with the description in the text.\n\n- The quality of the paper is good for the most part, however there are important baselines that are missing. Such methods have not been cited, discussed, or compared against.\n\n- The proposed method is fairly novel. Most existing CNN implementations in FHE using standard dense convolutional layers. This paper proposes to use group convolutions which are more HE friendly. Sparsity pattern is also optimized for minimizing rotations as opposed to other criterion used in standard networks.\n\n- The proposed approach is not reproducible based on the descriptions in the paper. There is missing information, hyper-parameters etc. And the paper does not provide code, nor do the authors promise to release code publicly later on.\n\n# Summary Of The Review\n\nThe paper proposed to use group convolutions and a weight pruning to mitigate the computational bottlenecks of convolutional layers, namely homomorphic rotations. The paper, however, does not compare to or discuss existing attempts toward HE-friendly CNNs. Furthermore, the experiments are conducted on shallow networks. The proposed approach will not provide much computational benefit for deeper networks since convolution is not the main bottleneck for such networks. Reproducibility is also limited.\n\nOverall, the paper has good ideas, but an evaluation, comparison, and discussion of the broader utility of the proposed approach are missing.\n\n**Update After Rebuttal:** The author's rebuttal does not adequately address the comments from the initial review. In theory, the method may have promise in achieving the claims in the rebuttal, but I do not believe it is straightforward and needs to be demonstrated. I will maintain the original rating.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a novel framework designed to enhance the inference speed of convolutional neural networks (CNNs) under homomorphic encryption (HE), addressing the significant latency challenges associated with HE in machine learning applications. The authors propose two primary techniques: HE-group convolution, which optimizes channel partitioning to reduce costly outer-rotations, and sub-block weight pruning, which iteratively eliminates less important weights to minimize inner-rotations. Experimental results demonstrate substantial speedups of 8.37x, 12.11x, and 19.26x for different CNN architectures (LeNet, VGG-5, and HEFNet) while maintaining comparable accuracy to existing methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to optimizing CNN inference in the context of HE, which is a critical area given the growing importance of privacy in machine learning. The proposed techniques are well-motivated and effectively tackle the unique computational challenges posed by HE, particularly in terms of reducing latency without sacrificing accuracy. Additionally, the extensive empirical evaluation showcases the performance improvements of SpENCNN over existing state-of-the-art methods. However, the paper could improve its clarity in certain sections, particularly in the theoretical aspects of the HE-group convolution, which may not be easily accessible to all readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, with a clear delineation of contributions and methodologies. However, some technical details, especially related to the theoretical derivations, could benefit from more detailed explanations to enhance understanding. The quality of the experimental setup is solid, using reputable benchmarks and libraries, which supports reproducibility. The novelty of the proposed framework is significant, as it introduces tailored techniques that are specifically designed for the constraints of HE in CNNs, filling a notable gap in existing literature.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of privacy-preserving machine learning by presenting SpENCNN, a framework that significantly enhances the inference speed of CNNs under homomorphic encryption. While the novelty and empirical results are commendable, some aspects of the theoretical explanations could be improved for clarity.\n\n# Correctness\n4/5 - The methods and results appear to be correct, but some theoretical aspects may require further validation or clarification.\n\n# Technical Novelty And Significance\n5/5 - The paper introduces innovative techniques specifically designed for the challenges of HE in CNNs, making a significant advancement in the field.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results show substantial performance improvements over existing methods, although the paper could explore additional datasets or architectures to further substantiate the findings.",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a novel framework designed to accelerate neural network inference while maintaining the confidentiality of data through Homomorphic Encryption (HE). By leveraging SIMD (Single Instruction, Multiple Data) features of HE schemes, the authors introduce two key techniques: HE-group convolution and sub-block weight pruning. These methods significantly reduce the computational overhead associated with HE operations, resulting in marked improvements in inference speed across various CNN architectures, including LeNet-like, VGG-5, and HEFNet, with minimal accuracy loss.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative combination of HE-specific optimizations with existing pruning strategies, effectively addressing both outer and inner HE-rotations. The reported performance gains, with speedups of up to 19.26×, underscore the efficacy of the proposed techniques while maintaining high accuracy levels, a crucial aspect for practical applications. However, the study does exhibit weaknesses, such as the dependency on specific CNN models, which raises concerns about the generalizability of the results. Additionally, the potential complexity of implementation could hinder practical adoption, particularly for practitioners unfamiliar with HE.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and findings clearly, making it accessible to readers with a background in machine learning and cryptography. The quality of the experimental design is commendable, with thorough ablation studies and detailed performance metrics. The novelty is evident in the approach to optimize HE for CNNs; however, reproducibility may be somewhat limited due to the focus on specific architectures and the inherent complexity of implementing the proposed techniques.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of privacy-preserving machine learning by introducing effective techniques for optimizing HE operations in CNNs. While the findings are promising, the limitations regarding model dependency and implementation complexity warrant consideration for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents **SpENCNN**, a novel framework designed to enhance the inference speed of convolutional neural networks (CNNs) using homomorphic encryption (HE). The authors identify the significant latency issues associated with HE operations, particularly in the context of CNNs, and propose two main techniques: **HE-Group Convolution**, which reduces outer-rotations during convolutions, and **Sub-Block Pruning**, which minimizes inner-rotations by eliminating non-essential weights. Experimental results demonstrate substantial speedups in inference latency across various CNN architectures, with the most notable improvement seen in HEFNet, achieving a speedup of 7.37x.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the computational bottlenecks of HE in neural network inference, particularly through the introduction of HE-Group Convolution and Sub-Block Pruning. These techniques are well-articulated and supported by empirical results demonstrating their effectiveness. However, a potential weakness is the limited exploration of the trade-offs between model accuracy and the degree of sparsification, which could provide deeper insights into the practical applicability of the proposed methods in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The experimental setup is detailed, allowing for reproducibility, and the results are comprehensively analyzed. The novelty is significant, as the proposed techniques specifically target the unique challenges posed by HE in CNN inference. However, the paper could benefit from more explicit discussions regarding the model's limitations and potential extensions.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of privacy-preserving machine learning by effectively addressing the latency challenges of HE-enabled CNN inference. The proposed techniques are innovative and empirically validated, although more exploration of accuracy trade-offs would enhance the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SpENCNN, a novel inference framework for convolutional neural networks (CNNs) that leverages homomorphic encryption (HE) to ensure privacy in machine learning applications. The authors propose an innovative approach that incorporates SIMD (Single Instruction, Multiple Data) capabilities of HE schemes to achieve substantial improvements in inference latency. The findings indicate significant speedups of 8.37x, 12.11x, and 19.26x across different CNN models when using their framework, while claiming negligible accuracy loss. The experimental setup includes a robust evaluation across multiple baseline CNN models and datasets, specifically MNIST and CIFAR-10, and includes thorough ablation studies to assess the contributions of various components of the framework.\n\n# Strength And Weaknesses\n**Strengths:**\n\n1. **Innovative Framework:** The introduction of SpENCNN as a novel HE-based CNN inference framework represents a meaningful advancement in privacy-preserving machine learning.\n   \n2. **Effective Use of SIMD:** The exploitation of SIMD features of HE schemes is a well-thought-out strategy that contributes to significant improvements in inference speed.\n\n3. **Substantial Speedups Achieved:** The reported speedups for different models are impressive, indicating practical applicability and potential for real-world deployment.\n\n4. **Comprehensive Experimental Setup:** The inclusion of multiple baseline CNN models and datasets demonstrates a rigorous evaluation approach.\n\n5. **Ablation Studies Included:** The detailed ablation studies enhance the credibility of the results by evaluating the effectiveness of individual components.\n\n6. **Clear Technical Contributions:** The paper clearly outlines its main contributions, which provide valuable insights for future research in the field.\n\n**Weaknesses:**\n\n1. **Limited Comparative Analysis:** The paper lacks extensive comparisons with existing frameworks, which could provide a clearer understanding of its relative strengths and weaknesses.\n\n2. **Dependency on Network Structure:** The effectiveness of SIMD is contingent on the neural network architecture, and the paper does not explore how performance gains may vary across different architectures.\n\n3. **Lack of Broader Dataset Testing:** The focus on only two datasets may hinder the generalizability of the findings, as additional testing on diverse datasets would be beneficial.\n\n4. **Justification of Parameter Choices:** The rationale behind the selection of specific configurations for parameters and hyperparameters is not deeply justified, which may raise questions regarding their optimality.\n\n5. **Limited Discussion of Limitations:** While the contributions are clear, the paper could benefit from a more detailed discussion of potential limitations or failure modes in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of the writing is high, with a logical flow that guides the reader through the framework and experimental results. The novelty of the proposed SpENCNN framework is significant, particularly in its application of HE to CNN inference. However, reproducibility may be limited due to the lack of detailed discussions on parameter choices and comparative analyses with existing frameworks.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of HE-based neural network inference through the introduction of the SpENCNN framework. Despite its promising results and innovative contributions, the paper could be strengthened by addressing its limitations, particularly in comparative analysis, broader applicability, and a deeper exploration of the implications of its findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a novel framework designed to enhance the efficiency of homomorphically encrypted neural network inference, specifically targeting convolutional neural networks (CNNs). The authors introduce two main techniques: HE-group convolution and a new sub-block weight pruning method, which collectively aim to minimize computational overhead and improve inference latency while preserving accuracy. Experimental results indicate that SpENCNN achieves significant speedups in inference latency across various models, demonstrating the framework's effectiveness in reducing computational demands associated with homomorphic encryption.\n\n# Strength And Weaknesses\nThe strengths of this paper are its clear and innovative approach to a pressing issue in the field of privacy-preserving machine learning, particularly in the context of encrypted data. The experimental results are compelling, showcasing substantial improvements in inference speed without a significant loss in accuracy. The introduction of group-interleaved encoding is a notable contribution, potentially impacting future research beyond this study. However, the paper falls short in providing a comprehensive comparison with existing methods, particularly regarding practical applicability and scalability. Additionally, a deeper exploration of the trade-offs associated with the proposed methods, especially concerning model complexity and real-world deployment, would enhance the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its methodology and findings. The quality of the writing and presentation is high, with thorough explanations of the techniques introduced. The novelty of the proposed methods is significant, especially in the context of homomorphic encryption applied to neural networks. However, the reproducibility of the results could be improved with more detailed descriptions of experimental setups and parameters, as well as a clearer discussion of the limitations and potential challenges in applying these methods in practical scenarios.\n\n# Summary Of The Review\nOverall, the SpENCNN framework represents a notable advancement in optimizing homomorphic encryption for neural network inference, with promising results that could facilitate broader adoption of privacy-preserving techniques. While the contributions are substantial, further exploration of the methods' scalability and trade-offs would strengthen the paper's impact in the community.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"SPENCNN: Orchestrating and Sparsity for Fast Homomorphically Encrypted Neural Network Inference\" introduces a novel framework called SpENCNN, which aims to enhance the robustness of convolutional neural networks (CNNs) against adversarial attacks while maintaining high inference speed and minimal accuracy loss. The key contributions include an innovative adversarial training framework that efficiently generates adversarial examples, integration of sparsity techniques to prune less significant weights in the network, and comprehensive empirical evaluations demonstrating improved robustness and reduced inference times on benchmark datasets such as MNIST and CIFAR-10. Additionally, the authors provide theoretical insights into the effects of adversarial training on model generalization and robustness.\n\n# Strength And Weaknesses\nThe strengths of the paper include its novelty, as the integration of adversarial training with sparsity techniques addresses a significant challenge in machine learning security. The practical relevance of the findings is notable, as the results show substantial improvements in robustness without sacrificing inference speed, making the approach applicable in real-world scenarios. Furthermore, the comprehensive evaluation across multiple datasets enhances the credibility of the results. However, the complexity of implementing the proposed techniques may pose challenges for practitioners, and the generalizability of the methods requires further validation on larger, more complex datasets to ensure effectiveness across various domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the writing is high, making it accessible to readers familiar with the topic. The novelty of combining adversarial training with sparsity techniques is evident, and the experimental results are reproducible given the detailed methodologies provided. However, the complexity involved in executing the proposed framework may hinder reproducibility for some practitioners.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial machine learning by proposing a framework that successfully enhances the robustness of CNNs while ensuring computational efficiency. The integration of sparsity techniques represents an innovative contribution that is both relevant and promising for real-world applications.\n\n# Correctness\n4/5 - The methodology and results appear sound, but there may be some concerns regarding the generalizability of the findings to more complex datasets.\n\n# Technical Novelty And Significance\n5/5 - The combination of adversarial training and sparsity techniques is a highly novel approach that addresses critical issues in the field, contributing meaningfully to ongoing research in machine learning security.\n\n# Empirical Novelty And Significance\n4/5 - While the empirical results are solid and demonstrate the effectiveness of the proposed framework, further validation on larger and more diverse datasets would strengthen the significance of the findings.",
    "# Summary Of The Paper\nThe paper introduces SpENCNN, a framework designed to enhance the speed of convolutional neural network (CNN) inference under homomorphic encryption (HE) while maintaining data privacy. The authors claim significant contributions including HE-group convolution, which purportedly eliminates the need for HE-rotation, and a sub-block weight pruning technique that enhances accuracy recovery. Experimental results indicate impressive speedups of 8.37x to 19.26x over existing methods, with minimal accuracy loss. The methodology is presented as accessible for implementation, suggesting a democratization of advanced machine learning techniques.\n\n# Strength And Weaknesses\nStrengths include the ambitious claims of speed improvements and the potential for real-time applications of HE in machine learning, which can significantly enhance data privacy. However, the paper downplays the challenges associated with HE and makes exaggerated claims regarding the obsolescence of existing optimization techniques. The effectiveness of the pruning method lacks substantial empirical validation, and the broad assertions about its transformative potential may mislead readers about the current landscape of HE.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, providing a clear overview of the proposed methods and their implications. However, certain claims, particularly regarding the ease of implementation and the transformative nature of the contributions, may not be fully substantiated. While the novelty of combining HE with CNNs is notable, the reproducibility of the results is questionable due to the lack of detailed experimental protocols and validation against a broader range of models.\n\n# Summary Of The Review\nThe paper presents a novel approach to enhancing CNN inference under homomorphic encryption with promising results; however, it overstates the impact and feasibility of its contributions, potentially misleading the audience about the practical state of HE applications. While the findings are significant, they do not represent a paradigm shift as claimed.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a Homomorphic Encryption (HE)-based Convolutional Neural Network (CNN) inference framework designed to enhance inference latency by leveraging SIMD features of HE. The authors introduce two key innovations: HE-group convolution and sub-block weight pruning, which aim to mitigate the computational overhead associated with HE operations. Experimental results indicate significant speedups—originally reported as 8.37x, 12.11x, and 19.26x for LeNet, VGG-5, and HEFNet, respectively—while maintaining minimal accuracy loss. The paper also includes a comparison with existing state-of-the-art methods, demonstrating substantial improvements in inference speed.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its practical approach to addressing the latency issues associated with HE in deep learning frameworks. The proposed techniques, HE-group convolution and sub-block pruning, are well-justified and effectively implemented, leading to impressive performance gains. However, the reported speedups were altered in the final results, showing a decrease from the original claims, which raises concerns about the robustness of the findings. Additionally, while the paper provides a solid theoretical foundation, it could benefit from a more comprehensive exploration of the trade-offs between speed and accuracy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly, with a logical flow from introduction to conclusion. The methodology is detailed enough to allow for reproducibility, although the altered results do suggest that further validation may be necessary. In terms of novelty, while the combination of HE and CNNs is not entirely new, the specific methods introduced offer fresh perspectives on optimizing HE operations. Overall, the quality of the writing and the clarity of the results is commendable, though the discrepancies in reported speedups could confuse readers.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of HE-based CNN inference by introducing effective techniques to minimize latency while preserving accuracy. However, the reduced performance metrics in the final results necessitate caution in interpreting the findings, suggesting a need for further validation of the proposed methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a framework for enhancing the efficiency of Homomorphic Encryption (HE) in privacy-preserving machine learning, particularly within cloud environments. The authors propose techniques aimed at optimizing HE operations, leveraging SIMD capabilities, and employing pruning strategies to reduce computational costs while maintaining model accuracy. The findings suggest significant speedups in HE operations; however, the paper does not sufficiently address the trade-offs and assumptions underlying these optimizations, including their applicability across various neural network architectures and real-world datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to optimizing HE for machine learning tasks, which is a relevant and timely topic given the increasing concern over data privacy. The proposed techniques could be beneficial in specific scenarios. However, the paper has several weaknesses: it makes several critical assumptions regarding the efficacy of HE optimizations that may not hold universally, such as the generalizability of results from specific models and datasets to broader applications. Additionally, it neglects the trade-offs involved in using HE, particularly regarding computational overhead and the trustworthiness of clients in key management. These weaknesses limit the practical applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-organized, but the clarity suffers due to the lack of critical evaluation of the assumptions made. While the novelty of the proposed techniques is apparent, the authors do not provide enough empirical evidence to support their claims about negligible accuracy loss and significant performance improvements across a range of scenarios. The reproducibility of the results is also questionable, as the experiments are limited to specific models and datasets without comprehensive validation against diverse conditions.\n\n# Summary Of The Review\nOverall, the paper presents interesting techniques for optimizing HE in machine learning but is hindered by significant assumptions and a lack of critical analysis of its findings. The contributions, while potentially valuable, need to be substantiated with broader empirical validation and clearer discussions on the limitations and trade-offs involved.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces SpENCNN, a novel framework designed to optimize convolutional neural network (CNN) inference using Homomorphic Encryption (HE). The authors present two primary techniques: HE-group convolution, which minimizes the need for costly outer-rotations during computation, and sub-block weight pruning, aimed at reducing the frequency of inner-rotations. The evaluation of SpENCNN demonstrates substantial improvements in inference speed while preserving accuracy across various CNN architectures, addressing the unique challenges posed by HE in machine learning applications.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to combining structural sparsity with ciphertext encoding, which is shown to significantly enhance the performance of HE-based CNN inference. The methodology is clearly articulated, and the experimental results provide robust evidence supporting the claims made. However, a potential weakness is the lack of extensive comparison with a broader range of baseline models, which could provide deeper insights into the effectiveness of SpENCNN across different scenarios. Additionally, the paper could benefit from a more detailed discussion on the trade-offs involved in the proposed optimizations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodologies and findings, allowing for easy comprehension of the proposed techniques. The quality of the writing is high, with rigorous attention to detail in the experimental evaluation. The novelty lies in the specific combination of HE techniques tailored for CNNs, which is addressed thoroughly. However, while the methodology is presented in a reproducible manner, the practical implementation details could be elaborated further to enhance replicability in real-world applications.\n\n# Summary Of The Review\nOverall, SpENCNN presents a significant advancement in optimizing CNN inference using Homomorphic Encryption, showcasing impressive speed improvements with minimal accuracy loss. The paper is well-written and provides a solid foundation for future research in this area, though it could benefit from broader empirical comparisons and more detailed implementation guidance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework designed to enhance the performance of neural network architectures by addressing the issue of overfitting in complex models. The authors propose a unique regularization technique that integrates adaptive dropout mechanisms with a novel loss function aimed at improving generalization. Through extensive experiments on benchmark datasets, the authors demonstrate significant improvements in model accuracy while maintaining robustness against overfitting.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Relevance:** The focus on overfitting is highly pertinent given the increasing complexity of modern neural networks and their applications.\n2. **Novelty:** The introduction of an adaptive dropout mechanism combined with a tailored loss function offers a fresh approach to regularization.\n3. **Empirical Validation:** The paper includes thorough experiments across multiple datasets, providing robust evidence of the proposed method's effectiveness.\n4. **Clarity of Presentation:** The writing is clear and organized, making it accessible for a broad audience, from practitioners to researchers.\n5. **Potential Impact:** The proposed framework has the potential to influence future research on regularization techniques in deep learning.\n\n**Weaknesses:**\n1. **Limited Baseline Comparison:** While the results are promising, the paper could benefit from comparisons with a broader range of state-of-the-art regularization techniques.\n2. **Methodological Ambiguities:** Some aspects of the adaptive dropout mechanism lack sufficient detail, which may hinder reproducibility.\n3. **Scalability Concerns:** The scalability of the proposed approach in larger datasets or real-world scenarios remains unaddressed.\n4. **Theoretical Justification:** The paper would benefit from a stronger theoretical foundation explaining why the specific choices of the loss function and dropout mechanism are effective.\n5. **Discussion of Limitations:** The authors do not adequately address scenarios in which the proposed method may fail or underperform.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is overall well-written and presents its ideas in a logical sequence. The quality of the experiments is commendable; however, the reproducibility of certain methodological components could be improved with more detailed descriptions. The novelty of the approach is significant, although more comprehensive comparisons with existing methods would enhance its standing.\n\n# Summary Of The Review\nThe paper makes a valuable contribution to the field by introducing an innovative regularization framework that effectively addresses overfitting in neural networks. While the empirical results are strong, further improvements in methodological clarity and comparative analysis with existing techniques are necessary for a more robust evaluation of the proposed approach.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a novel framework designed to enhance the efficiency of homomorphically encrypted neural network inference, specifically for convolutional neural networks (CNNs). The authors address the significant latency issues associated with homomorphic encryption (HE) operations by introducing two key innovations: HE-group convolution for optimized channel partitioning and sub-block weight pruning. The methodology leverages the SIMD (Single Instruction, Multiple Data) capabilities of HE to improve inference speed. Experimental results demonstrate that SpENCNN achieves substantial speedups—ranging from 8.37x to 19.26x—across various CNN architectures (LeNet, VGG-5, HEFNet) with only minimal accuracy degradation.\n\n# Strength And Weaknesses\nThe main strength of the paper is its focus on a critical issue in the application of HE to machine learning—latency. The introduction of HE-group convolution and sub-block weight pruning represents a significant step forward in optimizing HE operations specifically for CNNs. The empirical results are compelling, showcasing notable performance improvements without sacrificing accuracy. However, a potential weakness lies in the narrow focus on CNN architectures; the paper does not explore the application of the framework to other types of neural networks or tasks. Additionally, further details on the experimental setup and the specific parameters used in the pruning techniques could enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The language is technical yet accessible, making it suitable for a diverse audience. The novel aspects of the work are clearly articulated, and the empirical results are presented in a straightforward manner. However, the reproducibility of the results could be improved by providing more comprehensive details regarding the experimental setups, such as dataset specifications and hyperparameters utilized in training the models.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the intersection of homomorphic encryption and neural network inference, providing significant performance enhancements for CNNs while maintaining data privacy. While the proposed SpENCNN framework demonstrates promising results, broader applicability and more detailed methodological descriptions would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "## Summary Of The Paper\nThe paper presents SpENCNN, a framework designed to enhance the inference speed of convolutional neural networks (CNNs) under homomorphic encryption (HE). The authors introduce two key methodologies: HE-group convolution, which optimizes channel allocation to minimize computational costs associated with HE-rotations, and sub-block pruning, which targets specific weight sub-blocks to reduce overhead while maintaining accuracy. Experimental results demonstrate significant speed improvements—ranging from 8.37x to 19.26x—across different CNN models when compared to existing state-of-the-art approaches, with only minimal accuracy degradation.\n\n## Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to tackling the computational challenges of HE in neural network inference, particularly through the use of HE-group convolution and sub-block pruning. These contributions are well-supported by empirical results demonstrating substantial performance gains. However, a potential weakness is the limited exploration of the trade-offs between model complexity and efficiency, particularly in the context of deeper networks, which might limit the applicability of the proposed methods in more complex scenarios.\n\n## Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to readers with varying levels of familiarity with HE and CNNs. The quality of the experimental setup is high, utilizing Microsoft SEAL for HE operations, which enhances reproducibility. However, the novelty could be further highlighted by providing more contextual comparisons with prior art. While the methodologies are sound and the results compelling, a more thorough discussion on the limitations and potential extensions could improve the overall clarity and impact of the findings.\n\n## Summary Of The Review\nOverall, the paper presents a significant advancement in optimizing CNN inference under homomorphic encryption through the SpENCNN framework. The methodologies introduced are both innovative and effective, leading to impressive empirical results; however, further exploration of trade-offs in model complexity would strengthen the contributions.\n\n## Correctness\n4\n\n## Technical Novelty And Significance\n4\n\n## Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents SpENCNN, a novel framework aimed at optimizing the latency of homomorphically encrypted (HE) neural network inference. The authors introduce two primary contributions: HE-group convolution, which optimizes channel locations to reduce HE-rotation operations, and sub-block weight pruning, which minimizes inner-rotations while maintaining model accuracy through iterative pruning and retraining. The evaluation conducted on LeNet, VGG-5, and HEFNet models using MNIST and CIFAR-10 datasets demonstrates significant speedups of up to 19.26x in inference latency with negligible accuracy loss compared to existing methods.\n\n# Strength And Weaknesses\nThe paper makes several notable contributions, particularly in the context of improving the efficiency of HE in machine learning applications. The introduction of HE-group convolution is a strong point, as it effectively addresses the computational overhead associated with HE operations. Additionally, the sub-block weight pruning technique showcases innovation in model optimization. However, the paper's reliance on specific models (LeNet, VGG-5, HEFNet) may limit the generalizability of the findings. Furthermore, while the results are promising, the extent of accuracy retention across a broader range of models remains to be explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, making it accessible to readers. The evaluation metrics are defined systematically, which supports reproducibility. However, some sections could benefit from additional detail, particularly in the explanation of the experimental setup and the specific parameters used during the pruning process. The novelty of combining HE with model sparsity is significant, although it would be beneficial for the authors to discuss potential limitations or challenges in applying these techniques to more complex architectures.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to reducing latency in homomorphically encrypted neural network inference through innovative techniques. While the results are impressive, further validation across a wider array of models would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a novel framework designed to enhance the efficiency of Convolutional Neural Network (CNN) inference under the constraints of Homomorphic Encryption (HE). It introduces two primary techniques: HE-group convolution and sub-block pruning, which address the significant latency caused by HE operations, particularly in multi-channel CNN layers. Experimental results demonstrate substantial speedups in inference times—8.37x for LeNet, 12.11x for VGG-5, and 19.26x for HEFNet—while maintaining acceptable levels of accuracy.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to optimizing CNNs for HE contexts, specifically through the introduction of HE-group convolution and sub-block pruning. These techniques are well-motivated and effectively tackle the high computational overhead associated with HE. The experimental validation is thorough, showcasing significant performance improvements across multiple architectures and datasets. However, a potential weakness is the limited discussion on the implications of these techniques for more complex or deeper networks beyond those tested, as well as the scalability of the proposed methods in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents a clear methodology, with mathematical formulations that thoroughly explain the proposed techniques. The quality of the figures and results is high, allowing for easy comprehension of the experimental outcomes. The novelty of the contributions is evident, particularly in addressing the gap in HE optimization methodologies. However, the reproducibility of the results may depend on the availability of the implementation details, which are not extensively covered in the paper.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of privacy-preserving machine learning by optimizing CNN inference under homomorphic encryption. The proposed techniques yield impressive speed improvements with minimal accuracy loss, although further exploration into their application for deeper networks would enhance the work's applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces SpENCNN, a framework aimed at enhancing the efficiency of homomorphic encryption (HE) for convolutional neural network (CNN) inference. It proposes a novel HE-group convolution technique that integrates concepts from SIMD and group convolutions. The authors claim that their method achieves speedups with negligible accuracy loss, although the specific criteria for assessing accuracy loss are not clearly defined. Experimental results suggest improvements over existing methods, but the paper lacks comprehensive evaluation across different model architectures and real-world scenarios.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its ambitious goal of making HE more efficient for CNN applications, and it does present some theoretical advancements. However, the reliance on established concepts without sufficient originality raises concerns about the novelty of the contributions. The methodology is complicated, potentially leading to confusion in implementation. Furthermore, the dismissal of existing optimization techniques like pruning as ineffective in the encrypted domain lacks justification and depth. The lack of ablation studies and comprehensive evaluation weakens the overall contribution, as it is difficult to isolate the effectiveness of specific components of the SpENCNN framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the proposed HE-group convolution technique is undermined by its complexity, which may hinder reproducibility and implementation in practical applications. The paper's novelty is questionable due to its heavy reliance on pre-existing concepts, and the implications of the findings are not well articulated. The experimental setup does not provide a thorough analysis of trade-offs, particularly regarding speedup metrics and accuracy recovery, which diminishes the overall quality of the contribution.\n\n# Summary Of The Review\nWhile the paper presents an interesting theoretical approach to improving HE for CNN inference, it suffers from implementation complexities and a lack of originality. The findings are not sufficiently validated across diverse scenarios, raising doubts about the practical applicability of the proposed method.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a novel framework designed to facilitate efficient convolutional neural network (CNN) inference on homomorphically encrypted data. The main contributions include the introduction of HE-Group Convolution, which leverages SIMD features of homomorphic encryption to minimize costly HE-rotation operations, and Sub-Block Weight Pruning, which reduces computational overhead by selectively pruning weights. The authors demonstrate significant performance improvements, achieving inference speedups of 8.37x, 12.11x, and 19.26x for LeNet, VGG-5, and HEFNet, respectively. The framework emphasizes privacy-preserving machine learning (PPML) and broad applicability across various CNN architectures, highlighting its potential for deployment in sensitive fields such as healthcare and finance.\n\n# Strength And Weaknesses\nThe strengths of SpENCNN lie in its innovative approach to combining homomorphic encryption with deep learning, resulting in substantial efficiency gains without compromising model accuracy. The use of HE-Group Convolution and Sub-Block Weight Pruning are particularly noteworthy, as they address critical challenges in encrypted inference. However, one potential weakness is the scalability of the approach to deeper models and more complex tasks, which remains to be validated. Additionally, while the performance metrics are impressive, the paper could benefit from a more extensive evaluation across diverse datasets and real-world scenarios to reinforce its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings, making it accessible to both researchers and practitioners in the field. The quality of the methodology is high, with a thorough explanation of the techniques employed. The novelty of the proposed methods is significant, particularly in their application to homomorphic encryption in neural networks. However, reproducibility may be a concern, as detailed implementation guidelines and code availability are not explicitly mentioned, which could hinder other researchers from replicating the results.\n\n# Summary Of The Review\nSpENCNN makes a significant contribution to the intersection of homomorphic encryption and deep learning by providing an efficient framework for secure CNN inference. Its innovative techniques yield impressive performance gains while maintaining privacy, although further validation across a broader range of applications would enhance the credibility of its findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the SpENCNN framework, which aims to optimize convolutional neural network (CNN) inference in the context of homomorphic encryption (HE). It highlights the challenges posed by HE, particularly the computational overhead and latency associated with HE operations, especially HE-rotation. The authors propose innovative techniques such as HE-group convolution and sub-block weight pruning, which theoretically minimize unnecessary computations and reduce the HE operational burden while maintaining model accuracy. The framework theoretically demonstrates that structured sparsity and data encoding can lead to significant reductions in computational complexity in HE-based systems.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its theoretical contributions, which provide a novel synthesis of HE principles and CNN architecture that addresses key computational challenges. The proposed techniques, particularly the grouping of channels to optimize HE-rotation and targeted weight pruning, present a fresh perspective on improving HE efficiency. However, the paper's reliance on theoretical constructs over empirical validation is a notable weakness, as it leaves open questions regarding the practical implementation and performance of the proposed methods in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical foundations, making complex concepts accessible to readers. The quality of the theoretical analysis is high, providing detailed explanations of the proposed techniques and their implications. However, the lack of empirical results may hinder reproducibility and the practical application of the findings. The theoretical novelty is strong, but the absence of experimental validation raises concerns about the applicability of the proposed framework.\n\n# Summary Of The Review\nThe SpENCNN framework presents an innovative theoretical approach to optimizing HE-based CNN inference by addressing computational challenges through novel techniques. While the theoretical contributions are significant and well-articulated, the lack of empirical validation limits the practical implications of the work. Overall, the paper lays a solid foundation for future research but requires further investigation into its real-world applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the SpENCNN framework, which optimizes convolutional neural network (CNN) inference under homomorphic encryption (HE) using two primary techniques: HE-group convolution and sub-block weight pruning. HE-group convolution minimizes costly HE rotation operations by partitioning channels into groups and employing a new group-interleaved encoding format to enhance SIMD operations. Sub-block pruning identifies and removes less critical weights to decrease computational overhead while maintaining model accuracy. The framework demonstrates significant performance improvements, achieving speedups of 8.37x, 12.11x, and 19.26x for various CNN architectures with minimal accuracy loss. The results are validated through extensive experiments on well-known datasets, and the code is made available for reproducibility.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach to reducing the computational burden of HE in CNNs, which is crucial for practical applications involving sensitive data. The use of group convolution and weight pruning illustrates a thoughtful integration of techniques to address the specific challenges posed by HE. However, the paper could benefit from a more detailed discussion on the limitations of the proposed methods, particularly regarding the scalability to deeper models and more complex tasks. Additionally, while the empirical results are promising, a broader comparison with a wider range of state-of-the-art methods could reinforce the claims of superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly conveys both the methodology and results, making it accessible to readers with a solid understanding of deep learning and homomorphic encryption. The quality of the writing is high, with detailed explanations of the techniques employed and their implications for performance. The novelty lies in the combination of HE-group convolution with sub-block pruning, which presents a fresh perspective on optimizing HE-based CNNs. Reproducibility is adequately addressed, as the authors have provided the implementation details and code on GitHub, allowing others to replicate their experiments.\n\n# Summary Of The Review\nOverall, the SpENCNN framework presents a compelling approach to optimizing CNN inference under homomorphic encryption, demonstrating significant performance gains with minimal loss in accuracy. The methodologies are novel and well-executed, although further exploration of limitations and broader comparisons could strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a new framework aimed at enhancing homomorphic encryption (HE) based convolutional neural network (CNN) inference. The authors propose novel techniques such as HE-group convolution and sub-block pruning to improve computational efficiency and reduce latency. They report substantial speedups in performance, claiming reductions of 8.37x, 12.11x, and 19.26x in different scenarios. However, the paper lacks a comprehensive comparison with existing benchmarks, which is crucial to validate their claims of superiority.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the introduction of innovative methodologies like HE-group convolution and sub-block pruning, which could have potential implications for HE-based applications. However, the weaknesses are significant: the authors fail to provide a robust comparison with existing methods, such as CryptoNets or Faster-CryptoNets, which undermines the claimed performance improvements. Additionally, the novelty of their approaches is questionable, as they do not sufficiently differentiate their techniques from similar strategies explored in prior works. The lack of a critical discussion on related works further diminishes the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear in its presentation, but the novelty of the proposed methods is not convincingly established. The authors do not sufficiently justify how their techniques advance the current state of research in HE-based CNNs. Furthermore, the reproducibility of their results could be hindered due to the absence of comprehensive comparisons and benchmarks, which are vital for validating their claims.\n\n# Summary Of The Review\nOverall, while the paper introduces potentially valuable methodologies for HE-based CNN inference, it falls short in providing a critical context for its contributions. The lack of direct comparisons with existing state-of-the-art methods and insufficient engagement with prior research limits the assessment of its true significance in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents \"SPENCNN,\" a novel approach that integrates Convolutional Neural Networks (CNNs) with Homomorphic Encryption (HE) to enhance user data privacy while maintaining computational efficiency. The authors propose a framework that allows for encrypted image classification, thereby ensuring that users' data privacy is preserved when using cloud-based services. The methodology involves the development of a specialized HE-rotation operation that enables CNNs to process encrypted data without decryption. Experimental results demonstrate that SPENCNN achieves competitive accuracy on benchmark datasets while significantly reducing the computational overhead typically associated with HE.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative combination of HE and CNNs, which addresses a critical gap in privacy-preserving machine learning. The proposed HE-rotation operation is a noteworthy contribution that enhances the practical applicability of HE in deep learning contexts. However, the paper could benefit from a more thorough exploration of the limitations of the proposed method, particularly in terms of scalability and the potential trade-offs between accuracy and encryption overhead. Additionally, the experimental validation could be expanded to include more diverse datasets to strengthen the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of complex concepts such as HE and CNNs. However, there are several instances of typographical errors and inconsistent formatting, particularly in the presentation of mathematical notation and figure references, which could hinder understanding. The novelty of the approach is significant, as it presents a unique solution to the challenge of processing encrypted data with deep learning models. Reproducibility is somewhat compromised by the lack of detailed descriptions of the experimental setup and hyperparameter tuning, which would be beneficial for other researchers looking to replicate the results.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework that effectively integrates HE with CNNs for privacy-preserving image classification. While the contributions are noteworthy, the clarity of presentation and reproducibility of results could be improved to enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents SpENCNN, a framework aimed at enhancing the latency of homomorphic encryption (HE)-based convolutional neural network (CNN) inference through two main techniques: sub-block pruning and HE-group convolution. The authors evaluate these methods primarily on the MNIST and CIFAR-10 datasets, demonstrating improvements in inference speed while achieving negligible accuracy loss. However, the paper does not address potential security implications of the proposed optimizations or explore their applicability to other neural network architectures.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its approach to optimizing HE-based CNN inference, which is increasingly relevant given the rise of privacy-preserving machine learning. The proposed methods show promise in reducing latency, which is a critical issue in deploying HE systems. However, the paper has several weaknesses, including a lack of comprehensive evaluations on larger datasets, limited exploration of architectural variations, and insufficient comparison with existing optimization techniques. Additionally, the absence of discussions on scalability, computational overhead, and environmental impacts detracts from the overall robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the proposed methods. However, it lacks depth in statistical analysis of the experimental results, which would enhance the reproducibility and credibility of the claims made. The novelty of the techniques is notable, but the limited scope of tested architectures and datasets raises concerns about the generalizability of the results. Overall, while the methodology is sound, it would benefit from more rigorous validation and broader applicability.\n\n# Summary Of The Review\nOverall, the paper presents interesting methods for optimizing HE-based CNN inference, but it is limited by its narrow focus on specific architectures and datasets. The lack of discussion on security implications and practical deployment considerations are significant drawbacks that need to be addressed for a more comprehensive contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents a novel approach for optimizing homomorphic encryption (HE) inference through a combination of techniques aimed at improving performance metrics such as latency, speedup, sparsity, and holomorphic operation count (HOC). The authors conduct a series of experiments using three baseline convolutional neural network (CNN) models and report significant improvements in inference latency and model efficiency. However, the findings highlight the need for rigorous statistical validation to confirm the observed performance enhancements.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive experimental setup and the clear articulation of performance metrics, such as speedup ratios and average inference latency. The inclusion of ablation studies to isolate the effects of specific techniques is commendable. However, the paper lacks rigorous statistical validation of its results, such as significance testing and confidence intervals, which are essential for substantiating the claimed improvements. Additionally, the sample size for latency measurements may not be sufficient to ensure the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to the reader. The novelty of the proposed methods is evident, especially in terms of their potential impact on HE inference optimization. However, the reproducibility of results is questionable due to the absence of detailed statistical analyses. The authors should provide more rigorous justifications for their experimental choices to enhance the reproducibility of their findings.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to optimizing HE inference with several innovative techniques that show potential for performance improvements. However, the lack of rigorous statistical validation and the need for larger sample sizes undermine the conclusiveness of the results. Future work should focus on addressing these statistical shortcomings to strengthen the claims made.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes a new framework, SpENCNN, aimed at improving the efficiency of convolutional neural network (CNN) inference in the context of homomorphic encryption (HE). The authors focus on enhancing specific aspects of latency associated with HE operations on CNN models. While they present methods intended to mitigate the computational overhead and maintain accuracy, their experiments are limited to two datasets (MNIST and CIFAR-10), and the implications of their findings are not thoroughly quantified or generalized across different neural network architectures.\n\n# Strength And Weaknesses\nThe main strength of this work lies in its focus on CNNs and the exploration of latency improvements for HE operations. However, the paper has significant weaknesses. It does not address foundational issues of HE, such as the inherent slowness of HE operations, nor does it consider the implications of stronger adversarial models beyond semi-honest assumptions. Its narrow focus on CNNs limits applicability, and the experimental validation is insufficiently broad. Furthermore, the lack of quantification regarding accuracy trade-offs and the omission of discussions related to larger architectures and bootstrapping present notable gaps in the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is mixed; while the methodology is described, the absence of detailed discussions on broader implications, model scaling, and practical challenges in maintaining encrypted models detracts from its quality. The novelty is moderate; although the framework introduces some interesting approaches, it does not sufficiently advance the state-of-the-art in HE or CNNs. Reproducibility may be compromised due to the lack of comprehensive details on pruning strategies and the absence of a wider variety of datasets for validation.\n\n# Summary Of The Review\nOverall, while the SpENCNN framework presents some innovative ideas for enhancing CNN inference under homomorphic encryption, it is limited by its narrow focus, insufficient empirical validation, and lack of thorough exploration into the broader implications of its findings. The paper would benefit from addressing these shortcomings to enhance its relevance and applicability in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces SpENCNN, a framework aimed at optimizing homomorphic encryption (HE) for neural network inference. The authors propose methods such as HE-group convolution and sub-block weight pruning, as well as a technique they call \"group-interleaved encoding\" to improve data management during inference. Experimental results claim significant speedups over existing methods, although the improvements appear marginal and incremental rather than groundbreaking.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its systematic approach to addressing the performance challenges associated with HE in neural networks. However, the weaknesses are pronounced; many contributions seem to be reiterations of well-established concepts, lacking true novelty. The claim of significant performance improvements is not sufficiently substantiated, and the paper does not convincingly position itself against current state-of-the-art methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably clear in its presentation, although the depth of technical detail could be overwhelming for those less familiar with HE. The novelty is limited, as many of the proposed techniques are common in the field. Reproducibility may be a concern, as the experimental setup lacks detailed descriptions that would allow for easy replication of results.\n\n# Summary Of The Review\nOverall, SpENCNN presents an incremental approach to optimizing HE for neural networks but does not significantly advance the state of research in this area. The contributions are largely derivative, and while the authors cite speed improvements, the results are modest and fail to demonstrate a substantial breakthrough.\n\n# Correctness\nRating: 3/5\n\n# Technical Novelty And Significance\nRating: 2/5\n\n# Empirical Novelty And Significance\nRating: 2/5",
    "# Summary Of The Paper\nThe paper introduces SpENCNN, a novel framework designed to optimize convolutional neural network (CNN) inference utilizing Homomorphic Encryption (HE). The authors propose methods including HE-group convolution and sub-block pruning to enhance latency during inference. Experimental results demonstrate substantial improvements in inference speed across various CNN models. However, the authors suggest potential extensions of the framework to other model architectures, such as transformers, and emphasize the importance of efficient encoding strategies in the context of HE.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of structural sparsity and data packing for HE-based inference, marking a significant contribution to the field of privacy-preserving machine learning. The results showing notable speedups in inference latency are compelling. However, several weaknesses are evident. The proposed methods could be further enriched by incorporating more advanced pruning techniques and comparing different encoding strategies. Additionally, the evaluation setup relies heavily on standard datasets, which may not fully capture the framework's potential in more complex applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a coherent presentation of methodology and results. However, some sections, particularly those discussing encoding strategies and computational overheads, could benefit from more in-depth analysis and comparative studies. The technical novelty is strong, but the empirical novelty is somewhat limited due to the reliance on well-known datasets. The reproducibility of the results could be enhanced with clearer details on the experimental setup and parameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to optimizing CNN inference with HE, showcasing significant advancements in speed. However, the work would benefit from a more comprehensive exploration of various pruning techniques, encoding strategies, and the application of the framework to more complex datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the SpENCNN framework, designed to optimize homomorphically encrypted neural network inference. It leverages innovative techniques such as HE-group convolution and sub-block pruning to significantly reduce homomorphic operation counts (HOC) and improve inference speed. The experimental results show impressive speedups across various architectures, with notable performance metrics indicating enhancements of up to 19.26x for HEFNet, alongside substantial reductions in HE-rotations and maintenance of high accuracy.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its comprehensive evaluation of the SpENCNN framework across multiple well-established CNN architectures, including LeNet, VGG-5, and HEFNet, on benchmark datasets such as MNIST and CIFAR-10. The reported performance improvements over baseline models and the state-of-the-art method (Hunter) are particularly compelling, showcasing the effectiveness of the proposed methods. However, while the results are promising, the paper could benefit from a deeper exploration of potential limitations or trade-offs associated with the proposed techniques, such as the impact on model complexity or the generalization to other architectures not tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, results, and implications. The quality of the writing is high, with sufficient detail provided to understand the proposed framework and its contributions. The novelty of the HE-group convolution and sub-block pruning techniques is apparent, contributing to the existing body of knowledge in privacy-preserving machine learning. However, reproducibility could be enhanced by providing more details on the experimental setup and code availability, which are crucial for enabling other researchers to validate the findings.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in optimizing homomorphically encrypted neural network inference through the SpENCNN framework. The substantial performance improvements demonstrated across multiple architectures and datasets underscore its relevance in the field of privacy-preserving machine learning, although further clarification on the limitations of the proposed methods would strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach to enhancing the efficiency of convolutional neural networks (CNNs) through a combination of homomorphic encryption (HE) operations and sub-block weight pruning. The authors propose a methodology that leverages HE to maintain data privacy while performing computations, and they introduce a framework that facilitates efficient CNN inference. The findings demonstrate that the proposed method significantly reduces computational overhead and improves the privacy-preserving capabilities of CNNs, providing a promising direction for secure machine learning applications.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of HE operations with CNN architecture, addressing the critical concern of data privacy in machine learning. The empirical results show considerable improvements in computational efficiency, supporting the proposed methodology's effectiveness. However, the paper has weaknesses, particularly in its clarity and presentation. Technical terms are used without adequate explanation, and the writing contains grammatical inconsistencies and typographical errors that detract from the overall professionalism of the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a significant contribution to the field, its clarity suffers due to convoluted language and complex sentence structures. Key technical terms lack sufficient context, which may hinder understanding for a broader audience. Additionally, the paper could benefit from a more structured presentation, including clearer figures and legends, more subheadings, and smoother transitions between sections. Although the methodology appears reproducible, the lack of clear definitions and explanations may pose challenges for readers attempting to replicate the work.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the intersection of privacy and efficiency in CNNs, but it requires significant improvements in clarity and presentation to enhance its impact. The innovative approach is promising, yet the execution falls short in terms of readability and accessibility for diverse audiences.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.116258274869956,
    -1.7387331524882028,
    -2.0042280966769743,
    -1.638063742297309,
    -1.5917362766908596,
    -1.6964558613902903,
    -1.6013387479368553,
    -2.1632011481818028,
    -1.8083262756055238,
    -1.8014873514713359,
    -1.7679581275323188,
    -1.336280830018164,
    -1.7306794276843103,
    -1.856197995888293,
    -1.7659802673642564,
    -1.803616279725068,
    -2.0772816013796884,
    -1.6900048485697803,
    -1.8382058730830624,
    -1.7101331219804061,
    -1.8690315217313873,
    -1.4845492652139527,
    -1.7287601313099947,
    -1.7015801923873133,
    -1.9422186369610974,
    -1.995483245046466,
    -1.910189105447685,
    -1.817010945898285,
    -1.6783407834996011
  ],
  "logp_cond": [
    [
      0.0,
      -1.9959875291920102,
      -2.0120567690903486,
      -1.9954644346954136,
      -2.0063833690726245,
      -2.006736857814569,
      -2.032175165945008,
      -1.9986403197937856,
      -2.0250206436269864,
      -2.03101155646627,
      -2.0077522782822297,
      -2.0507033351494703,
      -2.016547276843878,
      -1.9846426105443902,
      -1.9906212241774475,
      -2.0234337955932253,
      -2.0254203932484,
      -1.9969287208482636,
      -2.0187831180923914,
      -2.005599006555335,
      -2.0243361291989563,
      -2.0430028303537453,
      -2.0243298239375114,
      -2.039403548997452,
      -2.0144877894841415,
      -2.0210596086728576,
      -2.0213834431015076,
      -2.007235384419596,
      -2.0282745828459565
    ],
    [
      -1.4763878061945117,
      0.0,
      -1.2261040684951117,
      -1.207046337823193,
      -1.368585441248842,
      -1.3412111769257802,
      -1.380587208284232,
      -1.2866616930605064,
      -1.1961027176690966,
      -1.4804310773765266,
      -1.2737111845425397,
      -1.5233957851723707,
      -1.204720881570133,
      -1.2324605812612721,
      -1.2128923597912116,
      -1.1978738653860446,
      -1.3931401273483013,
      -1.254494405705712,
      -1.3515102901943739,
      -1.2853663564270863,
      -1.3321331431024783,
      -1.454953356760874,
      -1.3787934356858151,
      -1.4991342982659832,
      -1.4028524030629994,
      -1.3538148304320499,
      -1.3840883639653148,
      -1.2964205157651258,
      -1.4192182071667165
    ],
    [
      -1.5934736118644328,
      -1.3119345922104053,
      0.0,
      -1.3499941094475294,
      -1.4544301413785716,
      -1.4180661905986327,
      -1.600673016367428,
      -1.3916187646424734,
      -1.2920590942723469,
      -1.5547624504035933,
      -1.3448113604359522,
      -1.7546138229999872,
      -1.3170790019526895,
      -1.3368802005538598,
      -1.300540271921427,
      -1.3470094781473125,
      -1.5147495210671997,
      -1.3142606620288309,
      -1.4125400255246605,
      -1.3905297526390412,
      -1.4624950960322314,
      -1.6384292646764955,
      -1.4752103964520462,
      -1.59030506559261,
      -1.5228220214414183,
      -1.4659154327667896,
      -1.455257820423922,
      -1.3312985018992616,
      -1.623535049991418
    ],
    [
      -1.2978971832997337,
      -1.007756330563078,
      -1.0322555427844584,
      0.0,
      -1.2292730008448958,
      -1.11003505378233,
      -1.29540924924278,
      -1.1536010588804522,
      -1.0762351270339778,
      -1.2932707754746697,
      -1.0461150358627833,
      -1.3783539746603632,
      -1.0900796905412138,
      -1.085506142624754,
      -1.068281053354026,
      -1.0757084087116295,
      -1.2100529987932853,
      -1.1068288640025339,
      -1.166252243733604,
      -1.1928275025985753,
      -1.1553772010378525,
      -1.2866016139524894,
      -1.1913829527905513,
      -1.2748494462441127,
      -1.2133179141085635,
      -1.194247527088677,
      -1.108607851484003,
      -1.1200720468425613,
      -1.2700628134241398
    ],
    [
      -1.390131322331351,
      -1.3024108674793011,
      -1.3023044372144175,
      -1.3153921259238333,
      0.0,
      -1.298758816414945,
      -1.3766247026172447,
      -1.3030589515112645,
      -1.2587776630194167,
      -1.351179586227767,
      -1.3620516737894621,
      -1.4071431688303682,
      -1.279963212708917,
      -1.3022169114292503,
      -1.2893225506318713,
      -1.3029616632537324,
      -1.3237648793604015,
      -1.2757539375638711,
      -1.3476426564923376,
      -1.3048574318639568,
      -1.2758389150690594,
      -1.3831546459955344,
      -1.300659623807906,
      -1.3532143066576892,
      -1.3126662656495978,
      -1.349784461416007,
      -1.3194867134013004,
      -1.3181558067282733,
      -1.4026238979068657
    ],
    [
      -1.415398118827194,
      -1.2287040118955384,
      -1.1913508395398802,
      -1.151779505516074,
      -1.2850289063197742,
      0.0,
      -1.3714459655560975,
      -1.1800747530057087,
      -1.1763099988204246,
      -1.3895274880183277,
      -1.166056879165658,
      -1.4758285270194376,
      -1.1432814147646868,
      -1.1774809530332593,
      -1.180509000909536,
      -1.205485924920137,
      -1.2463778555059692,
      -1.1711709333778824,
      -1.2154606600405105,
      -1.145435785288397,
      -1.2192481961314403,
      -1.3701642420059417,
      -1.2286727989591115,
      -1.3665700750149297,
      -1.2883096047392246,
      -1.191063005733892,
      -1.190573850103365,
      -1.1606384189047976,
      -1.3835284070775231
    ],
    [
      -1.3835828233894911,
      -1.2337480340460412,
      -1.2545470797077107,
      -1.2656018231980761,
      -1.2386586087584905,
      -1.2857818889135713,
      0.0,
      -1.261273544553713,
      -1.2709722074901662,
      -1.3608709897120526,
      -1.2827000332193963,
      -1.3779508449826092,
      -1.2862035097993918,
      -1.2850521619735564,
      -1.2704284657722742,
      -1.277263763463697,
      -1.2764282878879771,
      -1.2994846386039698,
      -1.302808833375383,
      -1.3066953912081776,
      -1.2957099559299754,
      -1.296129699912803,
      -1.2830461761532919,
      -1.3628217854347748,
      -1.294158333315119,
      -1.284706523319227,
      -1.2820269201666903,
      -1.3018819104636385,
      -1.3594485072875557
    ],
    [
      -1.8190230106719687,
      -1.5292448151059856,
      -1.5129571970740292,
      -1.5889764289304125,
      -1.6490264607508454,
      -1.568057086265998,
      -1.755430546349018,
      0.0,
      -1.5588178268752737,
      -1.8001084629384665,
      -1.5699574747032836,
      -1.9056278373988262,
      -1.4883099524871712,
      -1.4860478693690067,
      -1.5212124399109412,
      -1.5590585192757305,
      -1.5876318660828692,
      -1.5409931781043569,
      -1.5803623715257498,
      -1.5026592087790223,
      -1.572291794363628,
      -1.8032689315487738,
      -1.6301300445929037,
      -1.773960319794363,
      -1.6993457270935204,
      -1.6339393966201377,
      -1.6134647648416365,
      -1.5948810168757506,
      -1.7607368771972398
    ],
    [
      -1.5674436632455984,
      -1.2014983049821872,
      -1.218210132536438,
      -1.2692930323372464,
      -1.3560690099094506,
      -1.3508556515880352,
      -1.4539538299321915,
      -1.2954916945915371,
      0.0,
      -1.5110370052213247,
      -1.3356548660467364,
      -1.5739997021931302,
      -1.1996615395455252,
      -1.246335969434832,
      -1.2225036818654786,
      -1.210943879944581,
      -1.3830462838565396,
      -1.224876845819238,
      -1.3749588441788445,
      -1.2748355152358957,
      -1.2878931113085093,
      -1.4935339596784731,
      -1.3585051595155933,
      -1.514378566571417,
      -1.4493788624634736,
      -1.3544382162627668,
      -1.3619380478530523,
      -1.26708095792022,
      -1.459959418378205
    ],
    [
      -1.5386010382911517,
      -1.521233914463481,
      -1.4123627253684459,
      -1.4721144473878507,
      -1.460446623924452,
      -1.502890953661538,
      -1.5600553520045128,
      -1.4279255968455888,
      -1.5004625021579023,
      0.0,
      -1.4933272800640112,
      -1.5863226021973342,
      -1.4751732088300618,
      -1.4719909401311688,
      -1.4768782887636356,
      -1.4796128531444193,
      -1.4781378006503008,
      -1.4644900256239244,
      -1.5369394987901384,
      -1.4766501454118612,
      -1.4585235269433274,
      -1.5560279081080053,
      -1.496707115091937,
      -1.4644822170103629,
      -1.4830251358928508,
      -1.53307155039551,
      -1.522157127448385,
      -1.4994647257291598,
      -1.5304814165986427
    ],
    [
      -1.4453024160031358,
      -1.1822755306965347,
      -1.162829325860817,
      -1.2045890885014983,
      -1.3619806057062855,
      -1.228843741145331,
      -1.423217061309139,
      -1.2265064694409966,
      -1.2386308542826783,
      -1.3957707596049973,
      0.0,
      -1.5432506331297877,
      -1.2186481085240988,
      -1.2469446920208642,
      -1.2018350106104916,
      -1.2688437443841079,
      -1.2962778568016768,
      -1.2523816879338978,
      -1.2432076200524713,
      -1.2481264181261957,
      -1.2831327795671497,
      -1.4362884543918117,
      -1.2710346834655553,
      -1.4059949423627816,
      -1.331939988693931,
      -1.2928663690153732,
      -1.2025013996251355,
      -1.2956089902393095,
      -1.4199980183428211
    ],
    [
      -1.1453158056974373,
      -1.1113107827022688,
      -1.1402366557302173,
      -1.1138486584136909,
      -1.077385506547769,
      -1.1395457243393872,
      -1.1453374206627447,
      -1.1097686691497157,
      -1.1062929508482853,
      -1.1211170532017416,
      -1.1405596248232042,
      0.0,
      -1.1443084786903122,
      -1.1004553331828633,
      -1.1285785895620883,
      -1.151749556303726,
      -1.134383900258996,
      -1.1495851661502983,
      -1.1410426007091752,
      -1.1497997027773263,
      -1.137171144006716,
      -1.1269599860873505,
      -1.1151606824381697,
      -1.1237339194829867,
      -1.1321326034373862,
      -1.1262714234514593,
      -1.1341377015361431,
      -1.1551148962943474,
      -1.1063653233389779
    ],
    [
      -1.467580705270922,
      -1.125156665771597,
      -1.1140422381118604,
      -1.1819586555863142,
      -1.2569733823416822,
      -1.2162357279129987,
      -1.4355441750519091,
      -1.1942915643286451,
      -1.0979936250640618,
      -1.406969396271883,
      -1.2472897982162188,
      -1.4772672330192738,
      0.0,
      -1.1739725374167398,
      -1.1202740479225584,
      -1.125286948317996,
      -1.3198568708324103,
      -1.106566340146375,
      -1.299938145593331,
      -1.1974646738097576,
      -1.2898243647172936,
      -1.4015724721827025,
      -1.3437010463234829,
      -1.4395322796511723,
      -1.3507822799357294,
      -1.3403191561647858,
      -1.2882563580517632,
      -1.1539782120562587,
      -1.3617754632756618
    ],
    [
      -1.5478442467868931,
      -1.2280073167672259,
      -1.272621919365042,
      -1.2795115016510377,
      -1.3854205834950881,
      -1.3545276134744753,
      -1.534011517329787,
      -1.261454738797494,
      -1.2828057166284976,
      -1.5309029715011289,
      -1.334717611948983,
      -1.6279175360151925,
      -1.2713133777482017,
      0.0,
      -1.232152736888325,
      -1.2631038638282193,
      -1.4121509256987161,
      -1.3116669905288108,
      -1.3262578835466237,
      -1.2480138940924705,
      -1.282723941231339,
      -1.536362901822191,
      -1.3713596003663204,
      -1.4990206375337567,
      -1.4638305390518505,
      -1.4066249706417486,
      -1.362580061704094,
      -1.2648525898994167,
      -1.5209998057514975
    ],
    [
      -1.4113599634434209,
      -1.1395904639204557,
      -1.183665406467526,
      -1.2092353998261858,
      -1.3472725286054188,
      -1.28050484625292,
      -1.4031620115833805,
      -1.2073794149583885,
      -1.1882442615430096,
      -1.4710622312590331,
      -1.2034431936206922,
      -1.5281657929669354,
      -1.176666244522973,
      -1.2259293672841272,
      0.0,
      -1.2296800203996803,
      -1.3488731449025215,
      -1.203904506376618,
      -1.2519150422282836,
      -1.26126787773656,
      -1.318103587664527,
      -1.416344243269573,
      -1.327067817338363,
      -1.456237263338151,
      -1.3746815727795534,
      -1.319130371846744,
      -1.2988366287129784,
      -1.1749783483885072,
      -1.4415437369408346
    ],
    [
      -1.53580064093824,
      -1.1134312381125868,
      -1.1325211109389315,
      -1.1687828438416141,
      -1.265866973708948,
      -1.240848402739296,
      -1.430498784425835,
      -1.1666261072759763,
      -1.0955117700329189,
      -1.4316299045456051,
      -1.2490211364294947,
      -1.5975052235700895,
      -1.1077028687495905,
      -1.1235156710957368,
      -1.1410282213742462,
      0.0,
      -1.3245368122648988,
      -1.0862206889349548,
      -1.2864201119885044,
      -1.1759931771502476,
      -1.1868854697821956,
      -1.4706522211905109,
      -1.2560587619282884,
      -1.4290878841337986,
      -1.3438750122921048,
      -1.3629048737548761,
      -1.2676414362119988,
      -1.1289679958039442,
      -1.4630782352597143
    ],
    [
      -1.7694452644398422,
      -1.6406460024993064,
      -1.6090328946431516,
      -1.5723178888002463,
      -1.6446904902073234,
      -1.5556973894473316,
      -1.7320687878799284,
      -1.5238582785700956,
      -1.6239087893796664,
      -1.7255218841429594,
      -1.616486360077649,
      -1.8370242997784998,
      -1.6106112576148308,
      -1.5874844314343866,
      -1.6133291395752072,
      -1.596153864164908,
      0.0,
      -1.5697438805677104,
      -1.6110353940595348,
      -1.5998698292737195,
      -1.5805879841662933,
      -1.763330752422834,
      -1.5973051109616694,
      -1.735165372080483,
      -1.6100984712748514,
      -1.6152291885910268,
      -1.6019334104132854,
      -1.6242730680162027,
      -1.7633636469289624
    ],
    [
      -1.3877227436724138,
      -1.100326632370743,
      -1.0616784442598675,
      -1.1691513703334222,
      -1.2227927260286278,
      -1.1901788180216808,
      -1.365724887739836,
      -1.1197517289379728,
      -1.0429920269666653,
      -1.3577335215667992,
      -1.1679673106486972,
      -1.451096252385919,
      -1.0878804443570962,
      -1.1486927179768696,
      -1.1118805168603447,
      -1.0617997036674447,
      -1.2321850619113441,
      0.0,
      -1.1780680820317098,
      -1.1095161584150381,
      -1.1860434755567297,
      -1.3694051604645257,
      -1.25108757183817,
      -1.3706895700705009,
      -1.2863977379434761,
      -1.2319693749995944,
      -1.2154904134209819,
      -1.128880814048152,
      -1.359715660324437
    ],
    [
      -1.527071897927375,
      -1.350531475287081,
      -1.2765589083291788,
      -1.3154970745861005,
      -1.4462403599513363,
      -1.3048835477097025,
      -1.460893708715013,
      -1.278756172993105,
      -1.338337525665727,
      -1.5262570726961626,
      -1.2584460759019038,
      -1.5920214831091486,
      -1.308539971486716,
      -1.2404326131340662,
      -1.2580742005735919,
      -1.3342887710555114,
      -1.3621950654399255,
      -1.2487818721901611,
      0.0,
      -1.2285685830975184,
      -1.3363990541252295,
      -1.452603022090598,
      -1.3628155896226706,
      -1.4848975965935747,
      -1.4534361071351385,
      -1.3615549561930964,
      -1.3218100356803142,
      -1.3030988136702797,
      -1.4727256265929094
    ],
    [
      -1.4156086851780003,
      -1.1536229273603809,
      -1.1528754331064583,
      -1.2533821727642538,
      -1.2630945523109485,
      -1.1694597224624863,
      -1.359758174106051,
      -1.1355989361489784,
      -1.1460428446382702,
      -1.3581784902928828,
      -1.1811740766796601,
      -1.523427579763834,
      -1.160359871999208,
      -1.1014289993085735,
      -1.1758702399891718,
      -1.1950635182728429,
      -1.2463739173033959,
      -1.0966439125540204,
      -1.2117482506472936,
      0.0,
      -1.1751888604998773,
      -1.377118174436653,
      -1.2671628565891493,
      -1.3857633728846277,
      -1.3291522940530809,
      -1.2264021396215523,
      -1.2353825160576017,
      -1.1947178140331514,
      -1.3822046723525925
    ],
    [
      -1.568064368913668,
      -1.3410305976616728,
      -1.3232748706792776,
      -1.3444764440936068,
      -1.404686896675101,
      -1.3449377641845826,
      -1.5331647879963133,
      -1.2831514710803047,
      -1.294882408238808,
      -1.4913516240093267,
      -1.3756446162248714,
      -1.630467470787541,
      -1.3267058294612308,
      -1.2964274523721664,
      -1.3508668655384555,
      -1.285483231589047,
      -1.3584204141049967,
      -1.3182000402807879,
      -1.3572093509603378,
      -1.2963553067124582,
      0.0,
      -1.592217883163511,
      -1.347070336921098,
      -1.5172238300669671,
      -1.4156927287578207,
      -1.3680970379210453,
      -1.3496541207102026,
      -1.3393150164350798,
      -1.5288147857276322
    ],
    [
      -1.2810696119786744,
      -1.11595943037379,
      -1.130534198216278,
      -1.098520335664255,
      -1.1377576545323527,
      -1.1006772493523067,
      -1.1070010894383149,
      -1.1032562681164133,
      -1.0370765594413756,
      -1.1592302785443227,
      -1.1389198769865887,
      -1.213215428235786,
      -1.1187861725896349,
      -1.08089881140912,
      -1.1013405531313918,
      -1.1237227934317082,
      -1.1101481627405339,
      -1.1278140765193083,
      -1.1289014040184755,
      -1.115255397455655,
      -1.1193867958457673,
      0.0,
      -1.1269124089958809,
      -1.1921544107701323,
      -1.1281176355578983,
      -1.1120708923620803,
      -1.1385768467068045,
      -1.1383394195611114,
      -1.1345992697672134
    ],
    [
      -1.4164705769641974,
      -1.2572204250766303,
      -1.212096248887075,
      -1.174963861423048,
      -1.2989821791086782,
      -1.2292844215450878,
      -1.3645395066638748,
      -1.2308852839687092,
      -1.1973876783943507,
      -1.340517242704485,
      -1.2215384034140107,
      -1.4647383388677175,
      -1.238841860870387,
      -1.2036829570214025,
      -1.2090644942437816,
      -1.2010796367762393,
      -1.228676771503571,
      -1.2592957026361113,
      -1.2557313983498484,
      -1.2435938392031765,
      -1.171040271999601,
      -1.4007835561419948,
      0.0,
      -1.3857432938430716,
      -1.258311571455323,
      -1.254188287731711,
      -1.1957006635971728,
      -1.2363993322760904,
      -1.4098846645291045
    ],
    [
      -1.4328397956374583,
      -1.4105928226355902,
      -1.3733337634163338,
      -1.3448419178163122,
      -1.3479745321828445,
      -1.3628090754169004,
      -1.434295236428822,
      -1.3541690295820776,
      -1.4108030295144498,
      -1.362912537090247,
      -1.3594697441240267,
      -1.4761810885157427,
      -1.3871046745696127,
      -1.372421943657206,
      -1.3518870605721063,
      -1.3751846670900219,
      -1.365221525347737,
      -1.3953968543382649,
      -1.3597994856375257,
      -1.3729745910371722,
      -1.3332265390775668,
      -1.4620690151405862,
      -1.3327148723878803,
      0.0,
      -1.3720295600986878,
      -1.3638906770202242,
      -1.3451102576709828,
      -1.3474158383073112,
      -1.4237755802576264
    ],
    [
      -1.6685509522857047,
      -1.5926665817978372,
      -1.5334288999059151,
      -1.518775217455745,
      -1.5136521021625988,
      -1.5015071184367292,
      -1.5800870647045986,
      -1.5266289265255806,
      -1.5676859807510475,
      -1.5829935653076084,
      -1.5245931554723047,
      -1.7269073199763334,
      -1.5016085578536043,
      -1.5128775397283192,
      -1.5206605903373205,
      -1.5416213707825042,
      -1.4936992529390896,
      -1.5264316082641152,
      -1.5169817908477599,
      -1.5245031164559866,
      -1.5413403582962404,
      -1.6613352749637003,
      -1.518339017586796,
      -1.626640504631547,
      0.0,
      -1.5715807115912022,
      -1.5058652923006788,
      -1.550214083145538,
      -1.686066913329571
    ],
    [
      -1.6276238133262502,
      -1.4221118351208062,
      -1.4313389594164116,
      -1.4826679488225374,
      -1.5813723259348194,
      -1.3626348468061285,
      -1.5749538354500545,
      -1.4133277127212742,
      -1.426787869801783,
      -1.6329461139520904,
      -1.4288603238997852,
      -1.7184769681587686,
      -1.4142580965930953,
      -1.461348225561874,
      -1.4255190485877407,
      -1.4887457404239999,
      -1.447360427877226,
      -1.4492006594910591,
      -1.4257579219826892,
      -1.3832385774897908,
      -1.4008021225679712,
      -1.6096928790590148,
      -1.4838819101734633,
      -1.619613854211651,
      -1.568342696184997,
      0.0,
      -1.4673214392881022,
      -1.4699399762034178,
      -1.5840150749690365
    ],
    [
      -1.555438131642185,
      -1.4099083258934544,
      -1.3431573545296658,
      -1.3167785669956453,
      -1.4237859429331863,
      -1.3176285923082178,
      -1.5300445278822685,
      -1.3572059124064857,
      -1.3438999428095686,
      -1.4989559837367064,
      -1.3088694659135374,
      -1.6411735345395035,
      -1.3263636340447014,
      -1.3533546652030024,
      -1.3453439709481994,
      -1.3448942202830239,
      -1.395246930466529,
      -1.3903552714286778,
      -1.37804879594562,
      -1.3748007897251429,
      -1.358699359606537,
      -1.5315459941758882,
      -1.2781062074946665,
      -1.4810377859501413,
      -1.4154525193750722,
      -1.3971488518484512,
      0.0,
      -1.3645283685637606,
      -1.527442736041909
    ],
    [
      -1.5173299645633467,
      -1.2557551164737453,
      -1.2345329626994475,
      -1.3118974038543596,
      -1.390383272104916,
      -1.3464334132267517,
      -1.4570435676499207,
      -1.3143598507229084,
      -1.302272284550964,
      -1.4818850878086036,
      -1.3604519530278054,
      -1.6008675991596129,
      -1.2838210438329616,
      -1.2807335193365228,
      -1.2284128491334492,
      -1.236774339282104,
      -1.4034830353993437,
      -1.2064614438634025,
      -1.3525968762643243,
      -1.3183805397122481,
      -1.3164302778087837,
      -1.4716641884727313,
      -1.357547218638116,
      -1.467926277813953,
      -1.4658186269859694,
      -1.4119451434343635,
      -1.372510932094689,
      0.0,
      -1.499141800838246
    ],
    [
      -1.4214229940588483,
      -1.3053687652233856,
      -1.3442498610263451,
      -1.3208283377511691,
      -1.369049686686506,
      -1.333286655390708,
      -1.3504280487954463,
      -1.309373756345469,
      -1.2664193248682856,
      -1.3696934364641178,
      -1.3064804131307355,
      -1.368004777200981,
      -1.3070290752850138,
      -1.309961219957365,
      -1.3042065616761505,
      -1.3692874700118223,
      -1.339870560457409,
      -1.3320466737719472,
      -1.363014052936141,
      -1.3352783702196913,
      -1.336373491745843,
      -1.3339635224147122,
      -1.338148164426075,
      -1.3603065511021248,
      -1.395896255234231,
      -1.308691874776826,
      -1.349556597602856,
      -1.3661524819060293,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.12027074567794571,
      0.1042015057796073,
      0.12079384017454231,
      0.10987490579733139,
      0.1095214170553871,
      0.08408310892494786,
      0.11761795507617023,
      0.09123763124296946,
      0.08524671840368603,
      0.1085059965877262,
      0.06555493972048554,
      0.09971099802607775,
      0.1316156643255657,
      0.1256370506925084,
      0.09282447927673054,
      0.09083788162155582,
      0.11932955402169232,
      0.09747515677756446,
      0.1106592683146208,
      0.09192214567099954,
      0.07325544451621058,
      0.0919284509324445,
      0.07685472587250386,
      0.10177048538581435,
      0.09519866619709827,
      0.09487483176844824,
      0.10902289045036007,
      0.08798369202399936
    ],
    [
      0.2623453462936911,
      0.0,
      0.5126290839930912,
      0.5316868146650098,
      0.37014771123936074,
      0.39752197556242264,
      0.3581459442039707,
      0.45207145942769644,
      0.5426304348191062,
      0.25830207511167624,
      0.4650219679456631,
      0.21533736731583208,
      0.5340122709180699,
      0.5062725712269307,
      0.5258407926969912,
      0.5408592871021582,
      0.34559302513990153,
      0.4842387467824909,
      0.38722286229382896,
      0.4533667960611165,
      0.4066000093857245,
      0.2837797957273289,
      0.3599397168023877,
      0.2395988542222196,
      0.3358807494252034,
      0.38491832205615295,
      0.35464478852288805,
      0.44231263672307697,
      0.3195149453214863
    ],
    [
      0.4107544848125415,
      0.692293504466569,
      0.0,
      0.6542339872294449,
      0.5497979552984027,
      0.5861619060783416,
      0.40355508030954623,
      0.612609332034501,
      0.7121690024046274,
      0.44946564627338104,
      0.6594167362410222,
      0.24961427367698708,
      0.6871490947242849,
      0.6673478961231145,
      0.7036878247555474,
      0.6572186185296618,
      0.4894785756097746,
      0.6899674346481435,
      0.5916880711523138,
      0.6136983440379331,
      0.541733000644743,
      0.3657988320004788,
      0.5290177002249281,
      0.41392303108436423,
      0.481406075235556,
      0.5383126639101847,
      0.5489702762530524,
      0.6729295947777127,
      0.38069304668555626
    ],
    [
      0.3401665589975753,
      0.6303074117342311,
      0.6058081995128506,
      0.0,
      0.40879074145241323,
      0.528028688514979,
      0.342654493054529,
      0.48446268341685683,
      0.5618286152633312,
      0.34479296682263927,
      0.5919487064345257,
      0.25970976763694575,
      0.5479840517560952,
      0.5525575996725549,
      0.5697826889432831,
      0.5623553335856795,
      0.4280107435040237,
      0.5312348782947751,
      0.47181149856370497,
      0.4452362396987337,
      0.48268654125945654,
      0.3514621283448196,
      0.4466807895067577,
      0.36321429605319633,
      0.4247458281887455,
      0.443816215208632,
      0.529455890813306,
      0.5179916954547477,
      0.36800092887316915
    ],
    [
      0.20160495435950865,
      0.28932540921155847,
      0.28943183947644213,
      0.27634415076702634,
      0.0,
      0.29297746027591454,
      0.21511157407361492,
      0.2886773251795951,
      0.33295861367144286,
      0.2405566904630927,
      0.22968460290139747,
      0.18459310786049143,
      0.3117730639819425,
      0.28951936526160926,
      0.3024137260589883,
      0.2887746134371272,
      0.26797139733045805,
      0.3159823391269885,
      0.244093620198522,
      0.28687884482690285,
      0.31589736162180015,
      0.2085816306953252,
      0.2910766528829536,
      0.23852197003317044,
      0.27907001104126183,
      0.24195181527485254,
      0.2722495632895592,
      0.27358046996258634,
      0.1891123787839939
    ],
    [
      0.28105774256309624,
      0.4677518494947519,
      0.5051050218504101,
      0.5446763558742163,
      0.4114269550705161,
      0.0,
      0.3250098958341927,
      0.5163811083845815,
      0.5201458625698656,
      0.30692837337196255,
      0.5303989822246322,
      0.2206273343708527,
      0.5531744466256034,
      0.518974908357031,
      0.5159468604807542,
      0.4909699364701532,
      0.450078005884321,
      0.5252849280124079,
      0.4809952013497798,
      0.5510200761018933,
      0.47720766525884994,
      0.3262916193843486,
      0.46778306243117873,
      0.3298857863753606,
      0.40814625665106563,
      0.5053928556563982,
      0.5058820112869253,
      0.5358174424854927,
      0.31292745431276714
    ],
    [
      0.21775592454736414,
      0.3675907138908141,
      0.34679166822914453,
      0.33573692473877914,
      0.3626801391783647,
      0.315556859023284,
      0.0,
      0.34006520338314217,
      0.3303665404466891,
      0.2404677582248027,
      0.31863871471745897,
      0.22338790295424604,
      0.31513523813746347,
      0.31628658596329884,
      0.33091028216458107,
      0.3240749844731583,
      0.32491046004887814,
      0.3018541093328855,
      0.2985299145614724,
      0.2946433567286777,
      0.30562879200687987,
      0.30520904802405235,
      0.3182925717835634,
      0.23851696250208043,
      0.3071804146217363,
      0.3166322246176283,
      0.319311827770165,
      0.2994568374732167,
      0.24189024064929954
    ],
    [
      0.34417813750983406,
      0.6339563330758171,
      0.6502439511077736,
      0.5742247192513903,
      0.5141746874309574,
      0.5951440619158048,
      0.4077706018327847,
      0.0,
      0.604383321306529,
      0.36309268524333627,
      0.5932436734785191,
      0.2575733107829765,
      0.6748911956946315,
      0.6771532788127961,
      0.6419887082708615,
      0.6041426289060723,
      0.5755692820989335,
      0.6222079700774459,
      0.582838776656053,
      0.6605419394027805,
      0.5909093538181747,
      0.359932216633029,
      0.5330711035888991,
      0.3892408283874398,
      0.46385542108828237,
      0.5292617515616651,
      0.5497363833401663,
      0.5683201313060522,
      0.40246427098456294
    ],
    [
      0.24088261235992547,
      0.6068279706233366,
      0.5901161430690858,
      0.5390332432682774,
      0.4522572656960733,
      0.4574706240174886,
      0.35437244567333237,
      0.5128345810139867,
      0.0,
      0.29728927038419917,
      0.4726714095587874,
      0.2343265734123936,
      0.6086647360599986,
      0.5619903061706919,
      0.5858225937400452,
      0.5973823956609428,
      0.42527999174898423,
      0.5834494297862858,
      0.43336743142667933,
      0.5334907603696282,
      0.5204331642970146,
      0.3147923159270507,
      0.4498211160899306,
      0.29394770903410694,
      0.35894741314205025,
      0.453888059342757,
      0.44638822775247156,
      0.5412453176853038,
      0.34836685722731886
    ],
    [
      0.2628863131801842,
      0.2802534370078549,
      0.38912462610289,
      0.3293729040834852,
      0.34104072754688386,
      0.29859639780979785,
      0.24143199946682303,
      0.37356175462574703,
      0.30102484931343354,
      0.0,
      0.3081600714073247,
      0.21516474927400164,
      0.3263141426412741,
      0.32949641134016705,
      0.3246090627077003,
      0.3218744983269166,
      0.3233495508210351,
      0.3369973258474115,
      0.2645478526811975,
      0.32483720605947464,
      0.34296382452800844,
      0.2454594433633306,
      0.3047802363793988,
      0.337005134460973,
      0.3184622155784851,
      0.26841580107582597,
      0.27933022402295093,
      0.3020226257421761,
      0.2710059348726932
    ],
    [
      0.32265571152918304,
      0.5856825968357842,
      0.6051288016715017,
      0.5633690390308206,
      0.4059775218260333,
      0.5391143863869878,
      0.3447410662231798,
      0.5414516580913222,
      0.5293272732496406,
      0.37218736792732154,
      0.0,
      0.22470749440253113,
      0.5493100190082201,
      0.5210134355114546,
      0.5661231169218273,
      0.499114383148211,
      0.471680270730642,
      0.515576439598421,
      0.5247505074798475,
      0.5198317094061231,
      0.4848253479651692,
      0.3316696731405071,
      0.4969234440667636,
      0.3619631851695373,
      0.4360181388383879,
      0.47509175851694563,
      0.5654567279071834,
      0.4723491372930093,
      0.3479601091894977
    ],
    [
      0.1909650243207266,
      0.2249700473158951,
      0.19604417428794663,
      0.22243217160447304,
      0.2588953234703948,
      0.19673510567877672,
      0.1909434093554192,
      0.22651216086844816,
      0.22998787916987862,
      0.2151637768164223,
      0.19572120519495972,
      0.0,
      0.19197235132785173,
      0.23582549683530063,
      0.20770224045607555,
      0.18453127371443778,
      0.20189692975916795,
      0.18669566386786562,
      0.1952382293089887,
      0.18648112724083754,
      0.19910968601144785,
      0.2093208439308134,
      0.2211201475799942,
      0.21254691053517716,
      0.2041482265807777,
      0.21000940656670464,
      0.20214312848202076,
      0.1811659337238165,
      0.22991550667918603
    ],
    [
      0.2630987224133883,
      0.6055227619127133,
      0.6166371895724498,
      0.5487207720979961,
      0.47370604534262806,
      0.5144436997713115,
      0.29513525263240115,
      0.5363878633556651,
      0.6326858026202484,
      0.32371003141242727,
      0.4833896294680915,
      0.2534121946650365,
      0.0,
      0.5567068902675705,
      0.6104053797617519,
      0.6053924793663144,
      0.41082255685189994,
      0.6241130875379353,
      0.4307412820909793,
      0.5332147538745526,
      0.4408550629670167,
      0.32910695550160773,
      0.3869783813608274,
      0.291147148033138,
      0.37989714774858085,
      0.39036027151952446,
      0.4424230696325471,
      0.5767012156280515,
      0.3689039644086485
    ],
    [
      0.3083537491013999,
      0.6281906791210672,
      0.5835760765232509,
      0.5766864942372554,
      0.4707774123932049,
      0.5016703824138178,
      0.322186478558506,
      0.5947432570907991,
      0.5733922792597954,
      0.3252950243871642,
      0.5214803839393101,
      0.22828045987310053,
      0.5848846181400913,
      0.0,
      0.6240452589999681,
      0.5930941320600738,
      0.4440470701895769,
      0.5445310053594823,
      0.5299401123416694,
      0.6081841017958225,
      0.5734740546569541,
      0.3198350940661021,
      0.4848383955219726,
      0.35717735835453635,
      0.3923674568364426,
      0.4495730252465444,
      0.4936179341841991,
      0.5913454059888763,
      0.3351981901367955
    ],
    [
      0.35462030392083554,
      0.6263898034438007,
      0.5823148608967303,
      0.5567448675380706,
      0.4187077387588376,
      0.48547542111133635,
      0.3628182557808759,
      0.5586008524058679,
      0.5777360058212468,
      0.29491803610522327,
      0.5625370737435642,
      0.23781447439732095,
      0.5893140228412834,
      0.5400509000801292,
      0.0,
      0.5363002469645761,
      0.41710712246173487,
      0.5620757609876383,
      0.5140652251359727,
      0.5047123896276964,
      0.44787667969972933,
      0.3496360240946834,
      0.4389124500258934,
      0.30974300402610533,
      0.391298694584703,
      0.4468498955175124,
      0.46714363865127795,
      0.5910019189757492,
      0.3244365304234218
    ],
    [
      0.267815638786828,
      0.6901850416124813,
      0.6710951687861366,
      0.634833435883454,
      0.53774930601612,
      0.562767876985772,
      0.3731174952992331,
      0.6369901724490918,
      0.7081045096921492,
      0.371986375179463,
      0.5545951432955734,
      0.2061110561549786,
      0.6959134109754777,
      0.6801006086293313,
      0.6625880583508219,
      0.0,
      0.4790794674601693,
      0.7173955907901133,
      0.5171961677365637,
      0.6276231025748205,
      0.6167308099428725,
      0.33296405853455724,
      0.5475575177967797,
      0.3745283955912695,
      0.4597412674329633,
      0.440711405970192,
      0.5359748435130693,
      0.6746482839211239,
      0.34053804446535385
    ],
    [
      0.30783633693984624,
      0.43663559888038206,
      0.46824870673653685,
      0.5049637125794422,
      0.43259111117236504,
      0.5215842119323568,
      0.3452128134997601,
      0.5534233228095928,
      0.453372812000022,
      0.35175971723672905,
      0.4607952413020395,
      0.24025730160118863,
      0.4666703437648576,
      0.48979716994530187,
      0.46395246180448124,
      0.4811277372147804,
      0.0,
      0.507537720811978,
      0.46624620732015365,
      0.47741177210596897,
      0.4966936172133951,
      0.3139508489568543,
      0.47997649041801904,
      0.3421162292992055,
      0.46718313010483703,
      0.4620524127886616,
      0.475348190966403,
      0.4530085333634857,
      0.313917954450726
    ],
    [
      0.3022821048973665,
      0.5896782161990373,
      0.6283264043099128,
      0.5208534782363581,
      0.4672121225411525,
      0.4998260305480995,
      0.32427996082994426,
      0.5702531196318075,
      0.647012821603115,
      0.3322713270029811,
      0.5220375379210831,
      0.2389085961838613,
      0.6021244042126841,
      0.5413121305929107,
      0.5781243317094356,
      0.6282051449023356,
      0.45781978665843615,
      0.0,
      0.5119367665380705,
      0.5804886901547421,
      0.5039613730130506,
      0.32059968810525463,
      0.4389172767316103,
      0.31931527849927943,
      0.40360711062630417,
      0.4580354735701859,
      0.47451443514879843,
      0.5611240345216282,
      0.33028918824534337
    ],
    [
      0.3111339751556874,
      0.48767439779598143,
      0.5616469647538835,
      0.5227087984969618,
      0.3919655131317261,
      0.5333223253733599,
      0.3773121643680495,
      0.5594497000899574,
      0.4998683474173353,
      0.31194880038689976,
      0.5797597971811586,
      0.2461843899739138,
      0.5296659015963463,
      0.5977732599489962,
      0.5801316725094705,
      0.503917102027551,
      0.47601080764313686,
      0.5894240008929013,
      0.0,
      0.609637289985544,
      0.5018068189578329,
      0.3856028509924643,
      0.4753902834603918,
      0.3533082764894877,
      0.3847697659479239,
      0.47665091688996597,
      0.5163958374027482,
      0.5351070594127827,
      0.365480246490153
    ],
    [
      0.2945244368024058,
      0.5565101946200253,
      0.5572576888739478,
      0.45675094921615234,
      0.44703856966945765,
      0.5406733995179198,
      0.3503749478743552,
      0.5745341858314277,
      0.5640902773421359,
      0.3519546316875233,
      0.528959045300746,
      0.1867055422165722,
      0.549773249981198,
      0.6087041226718326,
      0.5342628819912343,
      0.5150696037075633,
      0.46375920467701026,
      0.6134892094263857,
      0.4983848713331125,
      0.0,
      0.5349442614805289,
      0.33301494754375316,
      0.44297026539125683,
      0.3243697490957784,
      0.38098082792732524,
      0.4837309823588538,
      0.4747506059228044,
      0.5154153079472548,
      0.32792844962781365
    ],
    [
      0.30096715281771935,
      0.5280009240697146,
      0.5457566510521097,
      0.5245550776377805,
      0.4643446250562864,
      0.5240937575468048,
      0.33586673373507403,
      0.5858800506510826,
      0.5741491134925794,
      0.37767989772206056,
      0.49338690550651587,
      0.23856405094384625,
      0.5423256922701565,
      0.5726040693592209,
      0.5181646561929318,
      0.5835482901423403,
      0.5106111076263906,
      0.5508314814505995,
      0.5118221707710495,
      0.5726762150189291,
      0.0,
      0.27681363856787633,
      0.5219611848102894,
      0.35180769166442016,
      0.4533387929735666,
      0.500934483810342,
      0.5193774010211847,
      0.5297165052963075,
      0.3402167360037551
    ],
    [
      0.20347965323527828,
      0.3685898348401626,
      0.3540150669976747,
      0.3860289295496977,
      0.3467916106816,
      0.383872015861646,
      0.3775481757756378,
      0.3812929970975394,
      0.44747270577257714,
      0.32531898666963,
      0.345629388227364,
      0.2713338369781666,
      0.36576309262431783,
      0.40365045380483267,
      0.3832087120825609,
      0.3608264717822445,
      0.37440110247341885,
      0.3567351886946444,
      0.3556478611954772,
      0.36929386775829776,
      0.3651624693681854,
      0.0,
      0.35763685621807184,
      0.2923948544438204,
      0.35643162965605435,
      0.37247837285187235,
      0.3459724185071482,
      0.3462098456528413,
      0.3499499954467393
    ],
    [
      0.3122895543457973,
      0.4715397062333644,
      0.5166638824229197,
      0.5537962698869467,
      0.42977795220131654,
      0.49947570976490696,
      0.36422062464611993,
      0.4978748473412855,
      0.5313724529156441,
      0.3882428886055098,
      0.5072217278959841,
      0.2640217924422772,
      0.4899182704396077,
      0.5250771742885922,
      0.5196956370662131,
      0.5276804945337554,
      0.5000833598064238,
      0.4694644286738834,
      0.47302873296014636,
      0.4851662921068183,
      0.5577198593103938,
      0.32797657516799994,
      0.0,
      0.34301683746692313,
      0.4704485598546717,
      0.47457184357828375,
      0.533059467712822,
      0.4923607990339043,
      0.3188754667808902
    ],
    [
      0.26874039674985495,
      0.2909873697517231,
      0.32824642897097944,
      0.35673827457100105,
      0.3536056602044688,
      0.33877111697041284,
      0.26728495595849133,
      0.34741116280523565,
      0.2907771628728635,
      0.3386676552970662,
      0.3421104482632866,
      0.22539910387157058,
      0.31447551781770056,
      0.32915824873010724,
      0.349693131815207,
      0.32639552529729143,
      0.3363586670395762,
      0.3061833380490484,
      0.34178070674978756,
      0.32860560135014105,
      0.36835365330974645,
      0.2395111772467271,
      0.36886531999943295,
      0.0,
      0.32955063228862547,
      0.33768951536708913,
      0.3564699347163305,
      0.35416435408000213,
      0.2778046121296869
    ],
    [
      0.2736676846753927,
      0.34955205516326027,
      0.4087897370551823,
      0.42344341950535247,
      0.4285665347984986,
      0.4407115185243682,
      0.36213157225649883,
      0.41558971043551685,
      0.37453265621004994,
      0.35922507165348905,
      0.41762548148879275,
      0.21531131698476402,
      0.44061007910749317,
      0.42934109723277825,
      0.42155804662377694,
      0.40059726617859326,
      0.44851938402200786,
      0.41578702869698225,
      0.42523684611333756,
      0.41771552050511085,
      0.40087827866485704,
      0.28088336199739716,
      0.4238796193743015,
      0.31557813232955034,
      0.0,
      0.37063792536989526,
      0.43635334466041864,
      0.39200455381555943,
      0.2561517236315265
    ],
    [
      0.36785943172021573,
      0.5733714099256597,
      0.5641442856300543,
      0.5128152962239285,
      0.41411091911164655,
      0.6328483982403375,
      0.4205294095964114,
      0.5821555323251917,
      0.5686953752446828,
      0.36253713109437546,
      0.5666229211466807,
      0.2770062768876973,
      0.5812251484533706,
      0.534135019484592,
      0.5699641964587252,
      0.506737504622466,
      0.5481228171692398,
      0.5462825855554068,
      0.5697253230637767,
      0.6122446675566751,
      0.5946811224784947,
      0.3857903659874511,
      0.5116013348730026,
      0.37586939083481496,
      0.4271405488614688,
      0.0,
      0.5281618057583637,
      0.5255432688430481,
      0.4114681700774294
    ],
    [
      0.3547509738054999,
      0.5002807795542306,
      0.5670317509180192,
      0.5934105384520396,
      0.48640316251449867,
      0.5925605131394671,
      0.3801445775654164,
      0.5529831930411993,
      0.5662891626381164,
      0.4112331217109786,
      0.6013196395341476,
      0.2690155709081814,
      0.5838254714029836,
      0.5568344402446825,
      0.5648451344994856,
      0.5652948851646611,
      0.514942174981156,
      0.5198338340190072,
      0.5321403095020649,
      0.5353883157225421,
      0.5514897458411478,
      0.37864311127179673,
      0.6320828979530184,
      0.4291513194975436,
      0.4947365860726127,
      0.5130402535992338,
      0.0,
      0.5456607368839244,
      0.382746369405776
    ],
    [
      0.29968098133493837,
      0.5612558294245398,
      0.5824779831988376,
      0.5051135420439254,
      0.42662767379336897,
      0.47057753267153335,
      0.35996737824836433,
      0.5026510951753766,
      0.514738661347321,
      0.3351258580896814,
      0.45655899287047963,
      0.2161433467386722,
      0.5331899020653235,
      0.5362774265617622,
      0.5885980967648359,
      0.580236606616181,
      0.41352791049894133,
      0.6105495020348826,
      0.4644140696339607,
      0.49863040618603693,
      0.5005806680895013,
      0.34534675742555376,
      0.45946372726016915,
      0.34908466808433203,
      0.3511923189123156,
      0.40506580246392154,
      0.44450001380359594,
      0.0,
      0.31786914506003905
    ],
    [
      0.25691778944075283,
      0.3729720182762155,
      0.334090922473256,
      0.357512445748432,
      0.3092910968130951,
      0.34505412810889324,
      0.32791273470415483,
      0.3689670271541321,
      0.4119214586313156,
      0.30864734703548335,
      0.37186037036886566,
      0.31033600629862024,
      0.37131170821458737,
      0.3683795635422362,
      0.3741342218234507,
      0.30905331348777887,
      0.3384702230421921,
      0.34629410972765395,
      0.31532673056346017,
      0.34306241327990983,
      0.3419672917537582,
      0.344377261084889,
      0.34019261907352605,
      0.3180342323974763,
      0.28244452826537003,
      0.3696489087227752,
      0.3287841858967451,
      0.3121883015935718,
      0.0
    ]
  ],
  "row_avgs": [
    0.10027893393982121,
    0.402515583963767,
    0.5554675710436684,
    0.46912593502009126,
    0.2663826625731474,
    0.449474571383336,
    0.30562507857832594,
    0.534432525841556,
    0.4576914273049338,
    0.30578890429526595,
    0.4705000114666448,
    0.2074354778815644,
    0.4615935575648322,
    0.48431378181349205,
    0.4660429356436363,
    0.5328086519223844,
    0.43691684668640607,
    0.4769041726119568,
    0.47371597374187896,
    0.4646758003584419,
    0.47678553775753335,
    0.3552548712231251,
    0.45873718598155,
    0.32192141686690906,
    0.38374567739552684,
    0.502549630615186,
    0.5062885203515511,
    0.4510516391570854,
    0.3385411770543785
  ],
  "col_avgs": [
    0.2901168463077442,
    0.48344880857710415,
    0.4880337422947366,
    0.4724135875925807,
    0.4101546781502501,
    0.4537341757429118,
    0.3290665410814869,
    0.4744797611140292,
    0.4890097010120968,
    0.32300054393997163,
    0.4567607783623465,
    0.2312643620902683,
    0.48718151581460684,
    0.48492700339233535,
    0.49085131522655406,
    0.4709856870249912,
    0.4117263884606781,
    0.4819052390037627,
    0.4296497323392691,
    0.4673121809947261,
    0.4484675949719904,
    0.3085573467225808,
    0.4222021290192059,
    0.31935221285135046,
    0.37659857978111194,
    0.40767575105748927,
    0.43004610745384264,
    0.46115767278698794,
    0.31648607687098673
  ],
  "combined_avgs": [
    0.1951978901237827,
    0.44298219627043556,
    0.5217506566692025,
    0.47076976130633597,
    0.3382686703616987,
    0.4516043735631239,
    0.31734580982990646,
    0.5044561434777926,
    0.4733505641585153,
    0.3143947241176188,
    0.46363039491449565,
    0.21934991998591635,
    0.47438753668971956,
    0.4846203926029137,
    0.47844712543509516,
    0.5018971694736878,
    0.4243216175735421,
    0.47940470580785977,
    0.45168285304057404,
    0.465993990676584,
    0.46262656636476185,
    0.33190610897285294,
    0.4404696575003779,
    0.32063681485912976,
    0.3801721285883194,
    0.45511269083633765,
    0.4681673139026969,
    0.45610465597203664,
    0.3275136269626826
  ],
  "gppm": [
    662.8385922179042,
    614.7410445295938,
    615.5718005566594,
    627.5553113877716,
    641.5987980068452,
    634.777085733008,
    685.7532975825178,
    620.1335694900721,
    614.9551606476107,
    691.5175582293206,
    633.8374197912864,
    733.9233884891773,
    617.8755883391872,
    618.5762495246033,
    616.4800064501417,
    625.8352670640769,
    649.6183847334371,
    619.7211186778642,
    644.8055371765141,
    626.7046321150386,
    636.1910662691026,
    701.5100121944901,
    649.929586525274,
    696.8289715646794,
    664.429482741264,
    655.3809401886789,
    644.8644721374288,
    629.3639655074891,
    697.3627058725059
  ],
  "gppm_normalized": [
    1.5316677632534659,
    1.3475253701401206,
    1.344751192399341,
    1.3715906008156875,
    1.4036881977533306,
    1.384271151013252,
    1.504339503860749,
    1.3541361297227164,
    1.3389688980232164,
    1.515668795813816,
    1.3832845104730471,
    1.6121250475813855,
    1.3516263578079681,
    1.3535514767585635,
    1.3449913100585296,
    1.3701877226086498,
    1.4192108436046469,
    1.3525029152328274,
    1.409207094515267,
    1.3620845024828674,
    1.3844685066776936,
    1.533018993746578,
    1.4195393727295538,
    1.5192553471968304,
    1.4494549869668776,
    1.4348057479832845,
    1.4064217659570666,
    1.3810478999720848,
    1.5232429225054527
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400
  ],
  "response_lengths": [
    4869,
    3058,
    2336,
    2263,
    3930,
    2651,
    3192,
    2233,
    2499,
    2499,
    2379,
    3024,
    2579,
    2345,
    2360,
    2303,
    2370,
    2660,
    2493,
    2718,
    2208,
    2538,
    2319,
    2221,
    2404,
    1862,
    2266,
    2464,
    2330
  ]
}